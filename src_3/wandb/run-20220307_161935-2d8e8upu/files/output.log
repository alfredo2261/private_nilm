LSTM(
  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))
  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))
  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))
  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))
  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))
  (linear1): Linear(in_features=23500, out_features=1024, bias=True)
  (linear2): Linear(in_features=1024, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (dropout): Dropout(p=0.2, inplace=False)
)
Window Length:  499
trigger times: 0
Loss after 131100 batches: 0.8410
trigger times: 0
Loss after 262200 batches: 0.4159
trigger times: 0
Loss after 393300 batches: 0.2996
trigger times: 0
Loss after 524400 batches: 0.2298
trigger times: 0
Loss after 655500 batches: 0.1958
trigger times: 0
Loss after 786600 batches: 0.1697
trigger times: 0
Loss after 917700 batches: 0.1467
trigger times: 0
Loss after 1048800 batches: 0.1270
trigger times: 0
Loss after 1179900 batches: 0.1109
trigger times: 0
Loss after 1311000 batches: 0.0968
trigger times: 1
Loss after 1442100 batches: 0.0854
trigger times: 2
Loss after 1573200 batches: 0.0757
trigger times: 0
Loss after 1704300 batches: 0.0693
trigger times: 1
Loss after 1835400 batches: 0.0626
trigger times: 2
Loss after 1966500 batches: 0.0569
trigger times: 3
Loss after 2097600 batches: 0.0524
trigger times: 4
Loss after 2228700 batches: 0.0483
trigger times: 5
Loss after 2359800 batches: 0.0454
trigger times: 0
Loss after 2490900 batches: 0.0429
trigger times: 1
Loss after 2622000 batches: 0.0403
trigger times: 2
Loss after 2753100 batches: 0.0386
trigger times: 3
Loss after 2884200 batches: 0.0355
trigger times: 4
Loss after 3015300 batches: 0.0349
trigger times: 0
Loss after 3146400 batches: 0.0332
trigger times: 0
Loss after 3277500 batches: 0.0324
trigger times: 1
Loss after 3408600 batches: 0.0302
trigger times: 2
Loss after 3539700 batches: 0.0289
trigger times: 3
Loss after 3670800 batches: 0.0283
trigger times: 4
Loss after 3801900 batches: 0.0277
trigger times: 0
Loss after 3933000 batches: 0.0262
trigger times: 1
Loss after 4064100 batches: 0.0256
trigger times: 2
Loss after 4195200 batches: 0.0253
trigger times: 3
Loss after 4326300 batches: 0.0236
trigger times: 4
Loss after 4457400 batches: 0.0231
trigger times: 5
Loss after 4588500 batches: 0.0228
trigger times: 6
Loss after 4719600 batches: 0.0223
trigger times: 7
Loss after 4850700 batches: 0.0219
trigger times: 8
Loss after 4981800 batches: 0.0220
trigger times: 9
Loss after 5112900 batches: 0.0206
trigger times: 10
Loss after 5244000 batches: 0.0205
trigger times: 0
Loss after 5375100 batches: 0.0198
trigger times: 1
Loss after 5506200 batches: 0.0199
trigger times: 2
Loss after 5637300 batches: 0.0191
trigger times: 3
Loss after 5768400 batches: 0.0189
trigger times: 4
Loss after 5899500 batches: 0.0189
trigger times: 5
Loss after 6030600 batches: 0.0176
trigger times: 6
Loss after 6161700 batches: 0.0175
trigger times: 7
Loss after 6292800 batches: 0.0177
trigger times: 8
Loss after 6423900 batches: 0.0170
trigger times: 9
Loss after 6555000 batches: 0.0165
trigger times: 10
Loss after 6686100 batches: 0.0164
trigger times: 11
Loss after 6817200 batches: 0.0168
trigger times: 12
Loss after 6948300 batches: 0.0159
trigger times: 13
Loss after 7079400 batches: 0.0159
trigger times: 14
Loss after 7210500 batches: 0.0154
trigger times: 15
Loss after 7341600 batches: 0.0154
trigger times: 16
Loss after 7472700 batches: 0.0151
trigger times: 17
Loss after 7603800 batches: 0.0152
trigger times: 18
Loss after 7734900 batches: 0.0147
trigger times: 19
Loss after 7866000 batches: 0.0147
trigger times: 20
Early stopping!
Start to test process.
Loss after 7997100 batches: 0.0143
Time to train on one home:  455.7002806663513
trigger times: 0
Loss after 8099700 batches: 0.9866
trigger times: 0
Loss after 8202300 batches: 0.7993
trigger times: 0
Loss after 8304900 batches: 0.6542
trigger times: 0
Loss after 8407500 batches: 0.5612
trigger times: 0
Loss after 8510100 batches: 0.5137
trigger times: 0
Loss after 8612700 batches: 0.4911
trigger times: 1
Loss after 8715300 batches: 0.4194
trigger times: 2
Loss after 8817900 batches: 0.3880
trigger times: 3
Loss after 8920500 batches: 0.3383
trigger times: 4
Loss after 9023100 batches: 0.2908
trigger times: 5
Loss after 9125700 batches: 0.2870
trigger times: 6
Loss after 9228300 batches: 0.2822
trigger times: 7
Loss after 9330900 batches: 0.2313
trigger times: 8
Loss after 9433500 batches: 0.2219
trigger times: 9
Loss after 9536100 batches: 0.1940
trigger times: 10
Loss after 9638700 batches: 0.1821
trigger times: 11
Loss after 9741300 batches: 0.1664
trigger times: 12
Loss after 9843900 batches: 0.1575
trigger times: 13
Loss after 9946500 batches: 0.1467
trigger times: 14
Loss after 10049100 batches: 0.1401
trigger times: 15
Loss after 10151700 batches: 0.1363
trigger times: 16
Loss after 10254300 batches: 0.1278
trigger times: 17
Loss after 10356900 batches: 0.1218
trigger times: 18
Loss after 10459500 batches: 0.1154
trigger times: 19
Loss after 10562100 batches: 0.1168
trigger times: 20
Early stopping!
Start to test process.
Loss after 10664700 batches: 0.1133
Time to train on one home:  165.5740566253662
train_results:  [0.0638206151882245]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523]]
Round_0_results:  [0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523]
trigger times: 0
Loss after 10795800 batches: 0.2133
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 88 < 89; dropping {'Training_Loss': 0.2133422610613535, 'Validation_Loss': 0.27486909760369194, 'Training_R2': 0.7852955810473445, 'Validation_R2': 0.7444803251205169, 'Training_F1': 0.7471070604820955, 'Validation_F1': 0.6872640898969439, 'Training_NEP': 0.5077785292543763, 'Validation_NEP': 0.588963474174187, 'Training_NDE': 0.16118241831710176, 'Validation_NDE': 0.20348239958188696, 'Training_MAE': 16.816617855491028, 'Validation_MAE': 16.15182192163228, 'Training_MSE': 709.17737, 'Validation_MSE': 751.4549}.
trigger times: 0
Loss after 10926900 batches: 0.0630
trigger times: 1
Loss after 11058000 batches: 0.0406
trigger times: 0
Loss after 11189100 batches: 0.0338
trigger times: 0
Loss after 11320200 batches: 0.0302
trigger times: 1
Loss after 11451300 batches: 0.0268
trigger times: 2
Loss after 11582400 batches: 0.0251
trigger times: 3
Loss after 11713500 batches: 0.0239
trigger times: 0
Loss after 11844600 batches: 0.0225
trigger times: 1
Loss after 11975700 batches: 0.0212
trigger times: 2
Loss after 12106800 batches: 0.0203
trigger times: 0
Loss after 12237900 batches: 0.0198
trigger times: 1
Loss after 12369000 batches: 0.0189
trigger times: 2
Loss after 12500100 batches: 0.0185
trigger times: 3
Loss after 12631200 batches: 0.0175
trigger times: 4
Loss after 12762300 batches: 0.0177
trigger times: 5
Loss after 12893400 batches: 0.0172
trigger times: 6
Loss after 13024500 batches: 0.0168
trigger times: 7
Loss after 13155600 batches: 0.0163
trigger times: 8
Loss after 13286700 batches: 0.0158
trigger times: 9
Loss after 13417800 batches: 0.0154
trigger times: 10
Loss after 13548900 batches: 0.0153
trigger times: 0
Loss after 13680000 batches: 0.0150
trigger times: 1
Loss after 13811100 batches: 0.0147
trigger times: 2
Loss after 13942200 batches: 0.0148
trigger times: 3
Loss after 14073300 batches: 0.0142
trigger times: 4
Loss after 14204400 batches: 0.0142
trigger times: 5
Loss after 14335500 batches: 0.0140
trigger times: 0
Loss after 14466600 batches: 0.0139
trigger times: 1
Loss after 14597700 batches: 0.0137
trigger times: 2
Loss after 14728800 batches: 0.0134
trigger times: 3
Loss after 14859900 batches: 0.0137
trigger times: 4
Loss after 14991000 batches: 0.0134
trigger times: 5
Loss after 15122100 batches: 0.0130
trigger times: 6
Loss after 15253200 batches: 0.0128
trigger times: 7
Loss after 15384300 batches: 0.0131
trigger times: 8
Loss after 15515400 batches: 0.0130
trigger times: 9
Loss after 15646500 batches: 0.0126
trigger times: 10
Loss after 15777600 batches: 0.0125
trigger times: 11
Loss after 15908700 batches: 0.0121
trigger times: 12
Loss after 16039800 batches: 0.0120
trigger times: 13
Loss after 16170900 batches: 0.0119
trigger times: 14
Loss after 16302000 batches: 0.0122
trigger times: 15
Loss after 16433100 batches: 0.0116
trigger times: 16
Loss after 16564200 batches: 0.0119
trigger times: 17
Loss after 16695300 batches: 0.0116
trigger times: 0
Loss after 16826400 batches: 0.0114
trigger times: 1
Loss after 16957500 batches: 0.0114
trigger times: 2
Loss after 17088600 batches: 0.0114
trigger times: 3
Loss after 17219700 batches: 0.0114
trigger times: 4
Loss after 17350800 batches: 0.0114
trigger times: 5
Loss after 17481900 batches: 0.0112
trigger times: 6
Loss after 17613000 batches: 0.0111
trigger times: 7
Loss after 17744100 batches: 0.0109
trigger times: 8
Loss after 17875200 batches: 0.0112
trigger times: 9
Loss after 18006300 batches: 0.0108
trigger times: 10
Loss after 18137400 batches: 0.0111
trigger times: 11
Loss after 18268500 batches: 0.0107
trigger times: 0
Loss after 18399600 batches: 0.0104
trigger times: 1
Loss after 18530700 batches: 0.0108
trigger times: 2
Loss after 18661800 batches: 0.0105
trigger times: 3
Loss after 18792900 batches: 0.0106
trigger times: 4
Loss after 18924000 batches: 0.0107
trigger times: 5
Loss after 19055100 batches: 0.0103
trigger times: 6
Loss after 19186200 batches: 0.0103
trigger times: 0
Loss after 19317300 batches: 0.0100
trigger times: 1
Loss after 19448400 batches: 0.0101
trigger times: 2
Loss after 19579500 batches: 0.0102
trigger times: 3
Loss after 19710600 batches: 0.0100
trigger times: 4
Loss after 19841700 batches: 0.0102
trigger times: 5
Loss after 19972800 batches: 0.0098
trigger times: 6
Loss after 20103900 batches: 0.0100
trigger times: 7
Loss after 20235000 batches: 0.0098
trigger times: 8
Loss after 20366100 batches: 0.0096
trigger times: 9
Loss after 20497200 batches: 0.0094
trigger times: 10
Loss after 20628300 batches: 0.0094
trigger times: 11
Loss after 20759400 batches: 0.0095
trigger times: 12
Loss after 20890500 batches: 0.0095
trigger times: 13
Loss after 21021600 batches: 0.0092
trigger times: 14
Loss after 21152700 batches: 0.0093
trigger times: 15
Loss after 21283800 batches: 0.0094
trigger times: 16
Loss after 21414900 batches: 0.0091
trigger times: 17
Loss after 21546000 batches: 0.0092
trigger times: 18
Loss after 21677100 batches: 0.0090
trigger times: 19
Loss after 21808200 batches: 0.0091
trigger times: 20
Early stopping!
Start to test process.
Loss after 21939300 batches: 0.0089
Time to train on one home:  638.085734128952
trigger times: 0
Loss after 22041900 batches: 0.5277
trigger times: 1
Loss after 22144500 batches: 0.2591
trigger times: 2
Loss after 22247100 batches: 0.1788
trigger times: 3
Loss after 22349700 batches: 0.1683
trigger times: 4
Loss after 22452300 batches: 0.1306
trigger times: 5
Loss after 22554900 batches: 0.1127
trigger times: 6
Loss after 22657500 batches: 0.1084
trigger times: 7
Loss after 22760100 batches: 0.1086
trigger times: 8
Loss after 22862700 batches: 0.0965
trigger times: 9
Loss after 22965300 batches: 0.0907
trigger times: 10
Loss after 23067900 batches: 0.0915
trigger times: 11
Loss after 23170500 batches: 0.0890
trigger times: 12
Loss after 23273100 batches: 0.0783
trigger times: 13
Loss after 23375700 batches: 0.0734
trigger times: 14
Loss after 23478300 batches: 0.0681
trigger times: 15
Loss after 23580900 batches: 0.0732
trigger times: 16
Loss after 23683500 batches: 0.0635
trigger times: 17
Loss after 23786100 batches: 0.0671
trigger times: 18
Loss after 23888700 batches: 0.0844
trigger times: 19
Loss after 23991300 batches: 0.0632
trigger times: 20
Early stopping!
Start to test process.
Loss after 24093900 batches: 0.0562
Time to train on one home:  135.84960913658142
train_results:  [0.0638206151882245, 0.032527834880738946]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745]]
Round_1_results:  [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 195 < 196; dropping {'Training_Loss': 0.0620399331146816, 'Validation_Loss': 0.25121117217673194, 'Training_R2': 0.9375900958426914, 'Validation_R2': 0.7665308657311565, 'Training_F1': 0.8620270165246032, 'Validation_F1': 0.746405368289927, 'Training_NEP': 0.2768196990773706, 'Validation_NEP': 0.4995249378429564, 'Training_NDE': 0.04685222282840704, 'Validation_NDE': 0.18592251141418706, 'Training_MAE': 9.167719440780278, 'Validation_MAE': 13.699046197672471, 'Training_MSE': 206.14243, 'Validation_MSE': 686.60675}.
trigger times: 0
Loss after 24225000 batches: 0.0620
trigger times: 1
Loss after 24356100 batches: 0.0205
trigger times: 0
Loss after 24487200 batches: 0.0171
trigger times: 1
Loss after 24618300 batches: 0.0151
trigger times: 0
Loss after 24749400 batches: 0.0140
trigger times: 1
Loss after 24880500 batches: 0.0133
trigger times: 2
Loss after 25011600 batches: 0.0129
trigger times: 0
Loss after 25142700 batches: 0.0125
trigger times: 1
Loss after 25273800 batches: 0.0119
trigger times: 2
Loss after 25404900 batches: 0.0121
trigger times: 3
Loss after 25536000 batches: 0.0119
trigger times: 0
Loss after 25667100 batches: 0.0112
trigger times: 1
Loss after 25798200 batches: 0.0111
trigger times: 2
Loss after 25929300 batches: 0.0111
trigger times: 3
Loss after 26060400 batches: 0.0107
trigger times: 4
Loss after 26191500 batches: 0.0105
trigger times: 0
Loss after 26322600 batches: 0.0105
trigger times: 1
Loss after 26453700 batches: 0.0105
trigger times: 2
Loss after 26584800 batches: 0.0103
trigger times: 3
Loss after 26715900 batches: 0.0102
trigger times: 4
Loss after 26847000 batches: 0.0102
trigger times: 5
Loss after 26978100 batches: 0.0103
trigger times: 6
Loss after 27109200 batches: 0.0100
trigger times: 7
Loss after 27240300 batches: 0.0099
trigger times: 0
Loss after 27371400 batches: 0.0099
trigger times: 1
Loss after 27502500 batches: 0.0096
trigger times: 2
Loss after 27633600 batches: 0.0099
trigger times: 3
Loss after 27764700 batches: 0.0097
trigger times: 4
Loss after 27895800 batches: 0.0094
trigger times: 5
Loss after 28026900 batches: 0.0093
trigger times: 6
Loss after 28158000 batches: 0.0095
trigger times: 7
Loss after 28289100 batches: 0.0092
trigger times: 8
Loss after 28420200 batches: 0.0093
trigger times: 9
Loss after 28551300 batches: 0.0092
trigger times: 10
Loss after 28682400 batches: 0.0094
trigger times: 11
Loss after 28813500 batches: 0.0092
trigger times: 12
Loss after 28944600 batches: 0.0092
trigger times: 13
Loss after 29075700 batches: 0.0091
trigger times: 0
Loss after 29206800 batches: 0.0090
trigger times: 1
Loss after 29337900 batches: 0.0086
trigger times: 2
Loss after 29469000 batches: 0.0087
trigger times: 3
Loss after 29600100 batches: 0.0089
trigger times: 4
Loss after 29731200 batches: 0.0087
trigger times: 5
Loss after 29862300 batches: 0.0090
trigger times: 0
Loss after 29993400 batches: 0.0086
trigger times: 1
Loss after 30124500 batches: 0.0087
trigger times: 2
Loss after 30255600 batches: 0.0087
trigger times: 3
Loss after 30386700 batches: 0.0084
trigger times: 4
Loss after 30517800 batches: 0.0083
trigger times: 5
Loss after 30648900 batches: 0.0084
trigger times: 6
Loss after 30780000 batches: 0.0084
trigger times: 0
Loss after 30911100 batches: 0.0083
trigger times: 1
Loss after 31042200 batches: 0.0083
trigger times: 2
Loss after 31173300 batches: 0.0083
trigger times: 3
Loss after 31304400 batches: 0.0081
trigger times: 4
Loss after 31435500 batches: 0.0083
trigger times: 5
Loss after 31566600 batches: 0.0082
trigger times: 6
Loss after 31697700 batches: 0.0081
trigger times: 7
Loss after 31828800 batches: 0.0081
trigger times: 8
Loss after 31959900 batches: 0.0080
trigger times: 9
Loss after 32091000 batches: 0.0081
trigger times: 10
Loss after 32222100 batches: 0.0080
trigger times: 11
Loss after 32353200 batches: 0.0083
trigger times: 12
Loss after 32484300 batches: 0.0080
trigger times: 13
Loss after 32615400 batches: 0.0079
trigger times: 14
Loss after 32746500 batches: 0.0077
trigger times: 15
Loss after 32877600 batches: 0.0083
trigger times: 16
Loss after 33008700 batches: 0.0077
trigger times: 17
Loss after 33139800 batches: 0.0075
trigger times: 18
Loss after 33270900 batches: 0.0077
trigger times: 19
Loss after 33402000 batches: 0.0076
trigger times: 20
Early stopping!
Start to test process.
Loss after 33533100 batches: 0.0079
Time to train on one home:  536.8273768424988
trigger times: 0
Loss after 33635700 batches: 0.3435
trigger times: 1
Loss after 33738300 batches: 0.1426
trigger times: 2
Loss after 33840900 batches: 0.1025
trigger times: 3
Loss after 33943500 batches: 0.0914
trigger times: 4
Loss after 34046100 batches: 0.0836
trigger times: 5
Loss after 34148700 batches: 0.0707
trigger times: 6
Loss after 34251300 batches: 0.0646
trigger times: 7
Loss after 34353900 batches: 0.0601
trigger times: 8
Loss after 34456500 batches: 0.0643
trigger times: 9
Loss after 34559100 batches: 0.0599
trigger times: 10
Loss after 34661700 batches: 0.0541
trigger times: 11
Loss after 34764300 batches: 0.0512
trigger times: 12
Loss after 34866900 batches: 0.0486
trigger times: 13
Loss after 34969500 batches: 0.0460
trigger times: 14
Loss after 35072100 batches: 0.0439
trigger times: 15
Loss after 35174700 batches: 0.0422
trigger times: 16
Loss after 35277300 batches: 0.0434
trigger times: 17
Loss after 35379900 batches: 0.0400
trigger times: 18
Loss after 35482500 batches: 0.0384
trigger times: 19
Loss after 35585100 batches: 0.0368
trigger times: 20
Early stopping!
Start to test process.
Loss after 35687700 batches: 0.0363
Time to train on one home:  135.94958448410034
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926]]
Round_2_results:  [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926]
trigger times: 0
Loss after 35818800 batches: 0.0264
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 288 < 289; dropping {'Training_Loss': 0.026368887040693807, 'Validation_Loss': 0.23425027231375375, 'Training_R2': 0.9735275513944555, 'Validation_R2': 0.7822246872269834, 'Training_F1': 0.908710359432995, 'Validation_F1': 0.7418504864114736, 'Training_NEP': 0.1827914485242363, 'Validation_NEP': 0.5068992682444274, 'Training_NDE': 0.019873337054873184, 'Validation_NDE': 0.173424779260736, 'Training_MAE': 6.053690260589623, 'Validation_MAE': 13.901280931503543, 'Training_MSE': 87.43957, 'Validation_MSE': 640.45294}.
trigger times: 0
Loss after 35949900 batches: 0.0133
trigger times: 1
Loss after 36081000 batches: 0.0115
trigger times: 2
Loss after 36212100 batches: 0.0107
trigger times: 3
Loss after 36343200 batches: 0.0104
trigger times: 4
Loss after 36474300 batches: 0.0101
trigger times: 5
Loss after 36605400 batches: 0.0096
trigger times: 6
Loss after 36736500 batches: 0.0092
trigger times: 7
Loss after 36867600 batches: 0.0094
trigger times: 8
Loss after 36998700 batches: 0.0094
trigger times: 9
Loss after 37129800 batches: 0.0093
trigger times: 0
Loss after 37260900 batches: 0.0090
trigger times: 1
Loss after 37392000 batches: 0.0088
trigger times: 0
Loss after 37523100 batches: 0.0086
trigger times: 1
Loss after 37654200 batches: 0.0086
trigger times: 2
Loss after 37785300 batches: 0.0088
trigger times: 3
Loss after 37916400 batches: 0.0087
trigger times: 4
Loss after 38047500 batches: 0.0083
trigger times: 5
Loss after 38178600 batches: 0.0086
trigger times: 6
Loss after 38309700 batches: 0.0083
trigger times: 0
Loss after 38440800 batches: 0.0082
trigger times: 1
Loss after 38571900 batches: 0.0083
trigger times: 2
Loss after 38703000 batches: 0.0080
trigger times: 3
Loss after 38834100 batches: 0.0080
trigger times: 4
Loss after 38965200 batches: 0.0079
trigger times: 5
Loss after 39096300 batches: 0.0081
trigger times: 6
Loss after 39227400 batches: 0.0081
trigger times: 7
Loss after 39358500 batches: 0.0080
trigger times: 8
Loss after 39489600 batches: 0.0079
trigger times: 9
Loss after 39620700 batches: 0.0079
trigger times: 10
Loss after 39751800 batches: 0.0079
trigger times: 11
Loss after 39882900 batches: 0.0078
trigger times: 12
Loss after 40014000 batches: 0.0076
trigger times: 13
Loss after 40145100 batches: 0.0074
trigger times: 14
Loss after 40276200 batches: 0.0073
trigger times: 15
Loss after 40407300 batches: 0.0075
trigger times: 16
Loss after 40538400 batches: 0.0076
trigger times: 17
Loss after 40669500 batches: 0.0076
trigger times: 18
Loss after 40800600 batches: 0.0079
trigger times: 19
Loss after 40931700 batches: 0.0075
trigger times: 20
Early stopping!
Start to test process.
Loss after 41062800 batches: 0.0074
Time to train on one home:  311.1416611671448
trigger times: 0
Loss after 41165400 batches: 0.2756
trigger times: 0
Loss after 41268000 batches: 0.0986
trigger times: 0
Loss after 41370600 batches: 0.0686
trigger times: 1
Loss after 41473200 batches: 0.0587
trigger times: 2
Loss after 41575800 batches: 0.0591
trigger times: 0
Loss after 41678400 batches: 0.0630
trigger times: 1
Loss after 41781000 batches: 0.0549
trigger times: 2
Loss after 41883600 batches: 0.0529
trigger times: 3
Loss after 41986200 batches: 0.0454
trigger times: 4
Loss after 42088800 batches: 0.0448
trigger times: 5
Loss after 42191400 batches: 0.0426
trigger times: 6
Loss after 42294000 batches: 0.0390
trigger times: 7
Loss after 42396600 batches: 0.0396
trigger times: 8
Loss after 42499200 batches: 0.0377
trigger times: 9
Loss after 42601800 batches: 0.0350
trigger times: 10
Loss after 42704400 batches: 0.0354
trigger times: 11
Loss after 42807000 batches: 0.0335
trigger times: 12
Loss after 42909600 batches: 0.0330
trigger times: 13
Loss after 43012200 batches: 0.0317
trigger times: 14
Loss after 43114800 batches: 0.0307
trigger times: 15
Loss after 43217400 batches: 0.0318
trigger times: 16
Loss after 43320000 batches: 0.0303
trigger times: 17
Loss after 43422600 batches: 0.0314
trigger times: 18
Loss after 43525200 batches: 0.0299
trigger times: 19
Loss after 43627800 batches: 0.0291
trigger times: 20
Early stopping!
Start to test process.
Loss after 43730400 batches: 0.0297
Time to train on one home:  166.8547122478485
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296]]
Round_3_results:  [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 355 < 356; dropping {'Training_Loss': 0.024927775674271135, 'Validation_Loss': 0.21331765088770124, 'Training_R2': 0.9749321390306579, 'Validation_R2': 0.8017459304723439, 'Training_F1': 0.9120279050471338, 'Validation_F1': 0.7515203552370352, 'Training_NEP': 0.17618624727393897, 'Validation_NEP': 0.47900391327848524, 'Training_NDE': 0.01881888818490688, 'Validation_NDE': 0.15787909018508578, 'Training_MAE': 5.834939094706404, 'Validation_MAE': 13.136274567598937, 'Training_MSE': 82.80016, 'Validation_MSE': 583.04315}.
trigger times: 0
Loss after 43861500 batches: 0.0249
trigger times: 1
Loss after 43992600 batches: 0.0116
trigger times: 0
Loss after 44123700 batches: 0.0098
trigger times: 1
Loss after 44254800 batches: 0.0096
trigger times: 2
Loss after 44385900 batches: 0.0089
trigger times: 3
Loss after 44517000 batches: 0.0087
trigger times: 4
Loss after 44648100 batches: 0.0085
trigger times: 5
Loss after 44779200 batches: 0.0084
trigger times: 6
Loss after 44910300 batches: 0.0084
trigger times: 0
Loss after 45041400 batches: 0.0081
trigger times: 1
Loss after 45172500 batches: 0.0082
trigger times: 2
Loss after 45303600 batches: 0.0079
trigger times: 3
Loss after 45434700 batches: 0.0080
trigger times: 4
Loss after 45565800 batches: 0.0081
trigger times: 5
Loss after 45696900 batches: 0.0082
trigger times: 6
Loss after 45828000 batches: 0.0079
trigger times: 7
Loss after 45959100 batches: 0.0078
trigger times: 8
Loss after 46090200 batches: 0.0077
trigger times: 9
Loss after 46221300 batches: 0.0077
trigger times: 0
Loss after 46352400 batches: 0.0076
trigger times: 1
Loss after 46483500 batches: 0.0076
trigger times: 2
Loss after 46614600 batches: 0.0075
trigger times: 3
Loss after 46745700 batches: 0.0076
trigger times: 4
Loss after 46876800 batches: 0.0075
trigger times: 5
Loss after 47007900 batches: 0.0073
trigger times: 6
Loss after 47139000 batches: 0.0072
trigger times: 7
Loss after 47270100 batches: 0.0074
trigger times: 8
Loss after 47401200 batches: 0.0072
trigger times: 9
Loss after 47532300 batches: 0.0072
trigger times: 10
Loss after 47663400 batches: 0.0071
trigger times: 11
Loss after 47794500 batches: 0.0072
trigger times: 0
Loss after 47925600 batches: 0.0070
trigger times: 1
Loss after 48056700 batches: 0.0069
trigger times: 2
Loss after 48187800 batches: 0.0070
trigger times: 3
Loss after 48318900 batches: 0.0069
trigger times: 4
Loss after 48450000 batches: 0.0071
trigger times: 5
Loss after 48581100 batches: 0.0072
trigger times: 6
Loss after 48712200 batches: 0.0071
trigger times: 7
Loss after 48843300 batches: 0.0071
trigger times: 8
Loss after 48974400 batches: 0.0068
trigger times: 9
Loss after 49105500 batches: 0.0068
trigger times: 10
Loss after 49236600 batches: 0.0068
trigger times: 11
Loss after 49367700 batches: 0.0070
trigger times: 12
Loss after 49498800 batches: 0.0068
trigger times: 13
Loss after 49629900 batches: 0.0070
trigger times: 14
Loss after 49761000 batches: 0.0068
trigger times: 15
Loss after 49892100 batches: 0.0067
trigger times: 16
Loss after 50023200 batches: 0.0066
trigger times: 17
Loss after 50154300 batches: 0.0067
trigger times: 18
Loss after 50285400 batches: 0.0069
trigger times: 19
Loss after 50416500 batches: 0.0067
trigger times: 20
Early stopping!
Start to test process.
Loss after 50547600 batches: 0.0067
Time to train on one home:  392.16317987442017
trigger times: 0
Loss after 50650200 batches: 0.1902
trigger times: 0
Loss after 50752800 batches: 0.0591
trigger times: 1
Loss after 50855400 batches: 0.0496
trigger times: 2
Loss after 50958000 batches: 0.0477
trigger times: 3
Loss after 51060600 batches: 0.0408
trigger times: 4
Loss after 51163200 batches: 0.0379
trigger times: 5
Loss after 51265800 batches: 0.0374
trigger times: 6
Loss after 51368400 batches: 0.0368
trigger times: 7
Loss after 51471000 batches: 0.0359
trigger times: 8
Loss after 51573600 batches: 0.0332
trigger times: 9
Loss after 51676200 batches: 0.0333
trigger times: 10
Loss after 51778800 batches: 0.0331
trigger times: 11
Loss after 51881400 batches: 0.0344
trigger times: 12
Loss after 51984000 batches: 0.0320
trigger times: 13
Loss after 52086600 batches: 0.0298
trigger times: 14
Loss after 52189200 batches: 0.0294
trigger times: 15
Loss after 52291800 batches: 0.0337
trigger times: 16
Loss after 52394400 batches: 0.0302
trigger times: 17
Loss after 52497000 batches: 0.0286
trigger times: 18
Loss after 52599600 batches: 0.0273
trigger times: 19
Loss after 52702200 batches: 0.0267
trigger times: 20
Early stopping!
Start to test process.
Loss after 52804800 batches: 0.0280
Time to train on one home:  142.3696506023407
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758]]
Round_4_results:  [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758]
trigger times: 0
Loss after 52935900 batches: 0.0223
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 429 < 430; dropping {'Training_Loss': 0.02232906930500044, 'Validation_Loss': 0.21545974330769646, 'Training_R2': 0.9775557097045031, 'Validation_R2': 0.7997849690842901, 'Training_F1': 0.9168189254405298, 'Validation_F1': 0.7583368707541132, 'Training_NEP': 0.16663016329443542, 'Validation_NEP': 0.47167023803741165, 'Training_NDE': 0.016849327111599605, 'Validation_NDE': 0.1594406964641984, 'Training_MAE': 5.518460545063382, 'Validation_MAE': 12.935154766933893, 'Training_MSE': 74.13439, 'Validation_MSE': 588.8101}.
trigger times: 1
Loss after 53067000 batches: 0.0103
trigger times: 0
Loss after 53198100 batches: 0.0090
trigger times: 1
Loss after 53329200 batches: 0.0083
trigger times: 2
Loss after 53460300 batches: 0.0079
trigger times: 3
Loss after 53591400 batches: 0.0079
trigger times: 4
Loss after 53722500 batches: 0.0075
trigger times: 0
Loss after 53853600 batches: 0.0076
trigger times: 1
Loss after 53984700 batches: 0.0072
trigger times: 2
Loss after 54115800 batches: 0.0075
trigger times: 0
Loss after 54246900 batches: 0.0073
trigger times: 1
Loss after 54378000 batches: 0.0073
trigger times: 0
Loss after 54509100 batches: 0.0073
trigger times: 0
Loss after 54640200 batches: 0.0072
trigger times: 1
Loss after 54771300 batches: 0.0070
trigger times: 2
Loss after 54902400 batches: 0.0070
trigger times: 3
Loss after 55033500 batches: 0.0070
trigger times: 4
Loss after 55164600 batches: 0.0069
trigger times: 5
Loss after 55295700 batches: 0.0069
trigger times: 6
Loss after 55426800 batches: 0.0069
trigger times: 7
Loss after 55557900 batches: 0.0068
trigger times: 8
Loss after 55689000 batches: 0.0068
trigger times: 9
Loss after 55820100 batches: 0.0069
trigger times: 10
Loss after 55951200 batches: 0.0068
trigger times: 11
Loss after 56082300 batches: 0.0069
trigger times: 12
Loss after 56213400 batches: 0.0067
trigger times: 13
Loss after 56344500 batches: 0.0069
trigger times: 14
Loss after 56475600 batches: 0.0067
trigger times: 15
Loss after 56606700 batches: 0.0066
trigger times: 16
Loss after 56737800 batches: 0.0066
trigger times: 17
Loss after 56868900 batches: 0.0066
trigger times: 18
Loss after 57000000 batches: 0.0067
trigger times: 19
Loss after 57131100 batches: 0.0065
trigger times: 20
Early stopping!
Start to test process.
Loss after 57262200 batches: 0.0066
Time to train on one home:  260.10861110687256
trigger times: 0
Loss after 57364800 batches: 0.1657
trigger times: 0
Loss after 57467400 batches: 0.0538
trigger times: 0
Loss after 57570000 batches: 0.0424
trigger times: 1
Loss after 57672600 batches: 0.0386
trigger times: 0
Loss after 57775200 batches: 0.0401
trigger times: 1
Loss after 57877800 batches: 0.0364
trigger times: 0
Loss after 57980400 batches: 0.0346
trigger times: 1
Loss after 58083000 batches: 0.0328
trigger times: 2
Loss after 58185600 batches: 0.0322
trigger times: 3
Loss after 58288200 batches: 0.0307
trigger times: 4
Loss after 58390800 batches: 0.0295
trigger times: 5
Loss after 58493400 batches: 0.0281
trigger times: 6
Loss after 58596000 batches: 0.0296
trigger times: 7
Loss after 58698600 batches: 0.0331
trigger times: 8
Loss after 58801200 batches: 0.0281
trigger times: 9
Loss after 58903800 batches: 0.0267
trigger times: 10
Loss after 59006400 batches: 0.0280
trigger times: 11
Loss after 59109000 batches: 0.0282
trigger times: 12
Loss after 59211600 batches: 0.0266
trigger times: 13
Loss after 59314200 batches: 0.0266
trigger times: 14
Loss after 59416800 batches: 0.0263
trigger times: 15
Loss after 59519400 batches: 0.0259
trigger times: 16
Loss after 59622000 batches: 0.0236
trigger times: 17
Loss after 59724600 batches: 0.0240
trigger times: 18
Loss after 59827200 batches: 0.0240
trigger times: 0
Loss after 59929800 batches: 0.0339
trigger times: 1
Loss after 60032400 batches: 0.0263
trigger times: 2
Loss after 60135000 batches: 0.0228
trigger times: 3
Loss after 60237600 batches: 0.0233
trigger times: 4
Loss after 60340200 batches: 0.0238
trigger times: 5
Loss after 60442800 batches: 0.0254
trigger times: 6
Loss after 60545400 batches: 0.0228
trigger times: 7
Loss after 60648000 batches: 0.0221
trigger times: 8
Loss after 60750600 batches: 0.0234
trigger times: 9
Loss after 60853200 batches: 0.0221
trigger times: 10
Loss after 60955800 batches: 0.0243
trigger times: 11
Loss after 61058400 batches: 0.0223
trigger times: 12
Loss after 61161000 batches: 0.0214
trigger times: 13
Loss after 61263600 batches: 0.0218
trigger times: 14
Loss after 61366200 batches: 0.0241
trigger times: 15
Loss after 61468800 batches: 0.0209
trigger times: 16
Loss after 61571400 batches: 0.0206
trigger times: 17
Loss after 61674000 batches: 0.0196
trigger times: 18
Loss after 61776600 batches: 0.0214
trigger times: 19
Loss after 61879200 batches: 0.0213
trigger times: 20
Early stopping!
Start to test process.
Loss after 61981800 batches: 0.0205
Time to train on one home:  283.9719908237457
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768]]
Round_5_results:  [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 509 < 510; dropping {'Training_Loss': 0.023095174481705675, 'Validation_Loss': 0.21336764842271805, 'Training_R2': 0.9767999408689907, 'Validation_R2': 0.8016427265122292, 'Training_F1': 0.9160200214805886, 'Validation_F1': 0.7546524970405831, 'Training_NEP': 0.16818291305453417, 'Validation_NEP': 0.479373005865543, 'Training_NDE': 0.01741669619133643, 'Validation_NDE': 0.1579612763786163, 'Training_MAE': 5.569884537683017, 'Validation_MAE': 13.146396617607405, 'Training_MSE': 76.63074, 'Validation_MSE': 583.3466}.
trigger times: 0
Loss after 62112900 batches: 0.0231
trigger times: 1
Loss after 62244000 batches: 0.0098
trigger times: 0
Loss after 62375100 batches: 0.0083
trigger times: 1
Loss after 62506200 batches: 0.0081
trigger times: 2
Loss after 62637300 batches: 0.0077
trigger times: 3
Loss after 62768400 batches: 0.0074
trigger times: 4
Loss after 62899500 batches: 0.0074
trigger times: 5
Loss after 63030600 batches: 0.0074
trigger times: 6
Loss after 63161700 batches: 0.0071
trigger times: 7
Loss after 63292800 batches: 0.0071
trigger times: 8
Loss after 63423900 batches: 0.0071
trigger times: 9
Loss after 63555000 batches: 0.0071
trigger times: 10
Loss after 63686100 batches: 0.0067
trigger times: 11
Loss after 63817200 batches: 0.0068
trigger times: 12
Loss after 63948300 batches: 0.0068
trigger times: 13
Loss after 64079400 batches: 0.0068
trigger times: 14
Loss after 64210500 batches: 0.0066
trigger times: 15
Loss after 64341600 batches: 0.0067
trigger times: 16
Loss after 64472700 batches: 0.0065
trigger times: 0
Loss after 64603800 batches: 0.0066
trigger times: 1
Loss after 64734900 batches: 0.0068
trigger times: 2
Loss after 64866000 batches: 0.0067
trigger times: 3
Loss after 64997100 batches: 0.0067
trigger times: 4
Loss after 65128200 batches: 0.0065
trigger times: 5
Loss after 65259300 batches: 0.0062
trigger times: 6
Loss after 65390400 batches: 0.0065
trigger times: 7
Loss after 65521500 batches: 0.0065
trigger times: 8
Loss after 65652600 batches: 0.0065
trigger times: 9
Loss after 65783700 batches: 0.0066
trigger times: 10
Loss after 65914800 batches: 0.0064
trigger times: 11
Loss after 66045900 batches: 0.0063
trigger times: 12
Loss after 66177000 batches: 0.0062
trigger times: 13
Loss after 66308100 batches: 0.0066
trigger times: 14
Loss after 66439200 batches: 0.0066
trigger times: 15
Loss after 66570300 batches: 0.0063
trigger times: 16
Loss after 66701400 batches: 0.0062
trigger times: 17
Loss after 66832500 batches: 0.0062
trigger times: 18
Loss after 66963600 batches: 0.0064
trigger times: 19
Loss after 67094700 batches: 0.0063
trigger times: 20
Early stopping!
Start to test process.
Loss after 67225800 batches: 0.0063
Time to train on one home:  303.39015007019043
trigger times: 0
Loss after 67328400 batches: 0.1247
trigger times: 1
Loss after 67431000 batches: 0.0397
trigger times: 0
Loss after 67533600 batches: 0.0329
trigger times: 1
Loss after 67636200 batches: 0.0291
trigger times: 2
Loss after 67738800 batches: 0.0290
trigger times: 0
Loss after 67841400 batches: 0.0273
trigger times: 1
Loss after 67944000 batches: 0.0269
trigger times: 2
Loss after 68046600 batches: 0.0258
trigger times: 3
Loss after 68149200 batches: 0.0247
trigger times: 4
Loss after 68251800 batches: 0.0271
trigger times: 5
Loss after 68354400 batches: 0.0239
trigger times: 6
Loss after 68457000 batches: 0.0229
trigger times: 7
Loss after 68559600 batches: 0.0234
trigger times: 8
Loss after 68662200 batches: 0.0222
trigger times: 9
Loss after 68764800 batches: 0.0243
trigger times: 10
Loss after 68867400 batches: 0.0271
trigger times: 11
Loss after 68970000 batches: 0.0230
trigger times: 12
Loss after 69072600 batches: 0.0218
trigger times: 13
Loss after 69175200 batches: 0.0208
trigger times: 14
Loss after 69277800 batches: 0.0211
trigger times: 15
Loss after 69380400 batches: 0.0205
trigger times: 16
Loss after 69483000 batches: 0.0211
trigger times: 17
Loss after 69585600 batches: 0.0204
trigger times: 18
Loss after 69688200 batches: 0.0219
trigger times: 19
Loss after 69790800 batches: 0.0211
trigger times: 20
Early stopping!
Start to test process.
Loss after 69893400 batches: 0.0209
Time to train on one home:  166.36870431900024
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625]]
Round_6_results:  [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 575 < 576; dropping {'Training_Loss': 0.021279996876024974, 'Validation_Loss': 0.23014110989040798, 'Training_R2': 0.9786120966869515, 'Validation_R2': 0.7861328146747222, 'Training_F1': 0.9179540117291668, 'Validation_F1': 0.7523732199217188, 'Training_NEP': 0.1643570460773091, 'Validation_NEP': 0.4862646166290313, 'Training_NDE': 0.016056278652977617, 'Validation_NDE': 0.17031255257481523, 'Training_MAE': 5.443179410909715, 'Validation_MAE': 13.335393176283887, 'Training_MSE': 70.64511, 'Validation_MSE': 628.9596}.
trigger times: 0
Loss after 70024500 batches: 0.0213
trigger times: 0
Loss after 70155600 batches: 0.0089
trigger times: 0
Loss after 70286700 batches: 0.0079
trigger times: 1
Loss after 70417800 batches: 0.0075
trigger times: 2
Loss after 70548900 batches: 0.0074
trigger times: 3
Loss after 70680000 batches: 0.0072
trigger times: 4
Loss after 70811100 batches: 0.0070
trigger times: 0
Loss after 70942200 batches: 0.0070
trigger times: 1
Loss after 71073300 batches: 0.0068
trigger times: 2
Loss after 71204400 batches: 0.0070
trigger times: 3
Loss after 71335500 batches: 0.0067
trigger times: 0
Loss after 71466600 batches: 0.0066
trigger times: 1
Loss after 71597700 batches: 0.0065
trigger times: 2
Loss after 71728800 batches: 0.0065
trigger times: 3
Loss after 71859900 batches: 0.0065
trigger times: 4
Loss after 71991000 batches: 0.0065
trigger times: 5
Loss after 72122100 batches: 0.0063
trigger times: 6
Loss after 72253200 batches: 0.0064
trigger times: 7
Loss after 72384300 batches: 0.0063
trigger times: 8
Loss after 72515400 batches: 0.0062
trigger times: 9
Loss after 72646500 batches: 0.0063
trigger times: 0
Loss after 72777600 batches: 0.0063
trigger times: 1
Loss after 72908700 batches: 0.0062
trigger times: 2
Loss after 73039800 batches: 0.0064
trigger times: 3
Loss after 73170900 batches: 0.0062
trigger times: 4
Loss after 73302000 batches: 0.0066
trigger times: 5
Loss after 73433100 batches: 0.0063
trigger times: 6
Loss after 73564200 batches: 0.0061
trigger times: 7
Loss after 73695300 batches: 0.0061
trigger times: 8
Loss after 73826400 batches: 0.0062
trigger times: 9
Loss after 73957500 batches: 0.0060
trigger times: 10
Loss after 74088600 batches: 0.0063
trigger times: 11
Loss after 74219700 batches: 0.0060
trigger times: 12
Loss after 74350800 batches: 0.0060
trigger times: 13
Loss after 74481900 batches: 0.0060
trigger times: 14
Loss after 74613000 batches: 0.0060
trigger times: 15
Loss after 74744100 batches: 0.0060
trigger times: 16
Loss after 74875200 batches: 0.0059
trigger times: 17
Loss after 75006300 batches: 0.0060
trigger times: 18
Loss after 75137400 batches: 0.0060
trigger times: 19
Loss after 75268500 batches: 0.0060
trigger times: 20
Early stopping!
Start to test process.
Loss after 75399600 batches: 0.0059
Time to train on one home:  319.08974862098694
trigger times: 0
Loss after 75502200 batches: 0.1021
trigger times: 0
Loss after 75604800 batches: 0.0362
trigger times: 1
Loss after 75707400 batches: 0.0283
trigger times: 2
Loss after 75810000 batches: 0.0274
trigger times: 3
Loss after 75912600 batches: 0.0271
trigger times: 0
Loss after 76015200 batches: 0.0267
trigger times: 1
Loss after 76117800 batches: 0.0265
trigger times: 2
Loss after 76220400 batches: 0.0253
trigger times: 3
Loss after 76323000 batches: 0.0243
trigger times: 4
Loss after 76425600 batches: 0.0229
trigger times: 5
Loss after 76528200 batches: 0.0210
trigger times: 6
Loss after 76630800 batches: 0.0219
trigger times: 0
Loss after 76733400 batches: 0.0226
trigger times: 1
Loss after 76836000 batches: 0.0223
trigger times: 2
Loss after 76938600 batches: 0.0205
trigger times: 3
Loss after 77041200 batches: 0.0207
trigger times: 4
Loss after 77143800 batches: 0.0206
trigger times: 5
Loss after 77246400 batches: 0.0201
trigger times: 6
Loss after 77349000 batches: 0.0228
trigger times: 7
Loss after 77451600 batches: 0.0201
trigger times: 8
Loss after 77554200 batches: 0.0201
trigger times: 9
Loss after 77656800 batches: 0.0193
trigger times: 10
Loss after 77759400 batches: 0.0192
trigger times: 11
Loss after 77862000 batches: 0.0218
trigger times: 12
Loss after 77964600 batches: 0.0198
trigger times: 13
Loss after 78067200 batches: 0.0192
trigger times: 14
Loss after 78169800 batches: 0.0188
trigger times: 15
Loss after 78272400 batches: 0.0188
trigger times: 16
Loss after 78375000 batches: 0.0195
trigger times: 17
Loss after 78477600 batches: 0.0183
trigger times: 18
Loss after 78580200 batches: 0.0180
trigger times: 19
Loss after 78682800 batches: 0.0176
trigger times: 20
Early stopping!
Start to test process.
Loss after 78785400 batches: 0.0177
Time to train on one home:  207.72429943084717
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193]]
Round_7_results:  [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193]
trigger times: 0
Loss after 78916500 batches: 0.0240
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 650 < 651; dropping {'Training_Loss': 0.023952782250729936, 'Validation_Loss': 0.2189110608564483, 'Training_R2': 0.9758707824094491, 'Validation_R2': 0.7963933607965137, 'Training_F1': 0.9141794214762499, 'Validation_F1': 0.7438458666732437, 'Training_NEP': 0.17188491145150042, 'Validation_NEP': 0.49956768650409283, 'Training_NDE': 0.01811423193950246, 'Validation_NDE': 0.1621415945189728, 'Training_MAE': 5.692487382736031, 'Validation_MAE': 13.700218543314165, 'Training_MSE': 79.69978, 'Validation_MSE': 598.7844}.
trigger times: 0
Loss after 79047600 batches: 0.0089
trigger times: 1
Loss after 79178700 batches: 0.0075
trigger times: 2
Loss after 79309800 batches: 0.0070
trigger times: 0
Loss after 79440900 batches: 0.0070
trigger times: 1
Loss after 79572000 batches: 0.0068
trigger times: 2
Loss after 79703100 batches: 0.0067
trigger times: 3
Loss after 79834200 batches: 0.0065
trigger times: 4
Loss after 79965300 batches: 0.0064
trigger times: 0
Loss after 80096400 batches: 0.0064
trigger times: 1
Loss after 80227500 batches: 0.0063
trigger times: 2
Loss after 80358600 batches: 0.0063
trigger times: 3
Loss after 80489700 batches: 0.0063
trigger times: 4
Loss after 80620800 batches: 0.0062
trigger times: 5
Loss after 80751900 batches: 0.0061
trigger times: 6
Loss after 80883000 batches: 0.0062
trigger times: 7
Loss after 81014100 batches: 0.0060
trigger times: 8
Loss after 81145200 batches: 0.0060
trigger times: 9
Loss after 81276300 batches: 0.0061
trigger times: 10
Loss after 81407400 batches: 0.0060
trigger times: 0
Loss after 81538500 batches: 0.0059
trigger times: 1
Loss after 81669600 batches: 0.0060
trigger times: 2
Loss after 81800700 batches: 0.0060
trigger times: 3
Loss after 81931800 batches: 0.0059
trigger times: 4
Loss after 82062900 batches: 0.0060
trigger times: 5
Loss after 82194000 batches: 0.0059
trigger times: 6
Loss after 82325100 batches: 0.0059
trigger times: 7
Loss after 82456200 batches: 0.0060
trigger times: 8
Loss after 82587300 batches: 0.0058
trigger times: 9
Loss after 82718400 batches: 0.0058
trigger times: 10
Loss after 82849500 batches: 0.0058
trigger times: 11
Loss after 82980600 batches: 0.0058
trigger times: 12
Loss after 83111700 batches: 0.0059
trigger times: 13
Loss after 83242800 batches: 0.0059
trigger times: 14
Loss after 83373900 batches: 0.0059
trigger times: 15
Loss after 83505000 batches: 0.0059
trigger times: 16
Loss after 83636100 batches: 0.0058
trigger times: 17
Loss after 83767200 batches: 0.0057
trigger times: 18
Loss after 83898300 batches: 0.0057
trigger times: 19
Loss after 84029400 batches: 0.0057
trigger times: 20
Early stopping!
Start to test process.
Loss after 84160500 batches: 0.0057
Time to train on one home:  311.31513929367065
trigger times: 0
Loss after 84263100 batches: 0.1011
trigger times: 1
Loss after 84365700 batches: 0.0331
trigger times: 2
Loss after 84468300 batches: 0.0254
trigger times: 3
Loss after 84570900 batches: 0.0264
trigger times: 4
Loss after 84673500 batches: 0.0249
trigger times: 5
Loss after 84776100 batches: 0.0233
trigger times: 6
Loss after 84878700 batches: 0.0224
trigger times: 7
Loss after 84981300 batches: 0.0226
trigger times: 8
Loss after 85083900 batches: 0.0213
trigger times: 9
Loss after 85186500 batches: 0.0205
trigger times: 10
Loss after 85289100 batches: 0.0211
trigger times: 11
Loss after 85391700 batches: 0.0192
trigger times: 12
Loss after 85494300 batches: 0.0195
trigger times: 13
Loss after 85596900 batches: 0.0197
trigger times: 14
Loss after 85699500 batches: 0.0193
trigger times: 15
Loss after 85802100 batches: 0.0190
trigger times: 16
Loss after 85904700 batches: 0.0182
trigger times: 17
Loss after 86007300 batches: 0.0185
trigger times: 18
Loss after 86109900 batches: 0.0195
trigger times: 19
Loss after 86212500 batches: 0.0187
trigger times: 20
Early stopping!
Start to test process.
Loss after 86315100 batches: 0.0194
Time to train on one home:  136.557297706604
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468, 0.012550091408176377]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193], [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377]]
Round_8_results:  [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 712 < 713; dropping {'Training_Loss': 0.018503005097989203, 'Validation_Loss': 0.23149088439014223, 'Training_R2': 0.9813797442751664, 'Validation_R2': 0.7849362755496707, 'Training_F1': 0.9229085194755005, 'Validation_F1': 0.7539381902754969, 'Training_NEP': 0.15438640916033547, 'Validation_NEP': 0.47313165245053934, 'Training_NDE': 0.01397855648268394, 'Validation_NDE': 0.17126541325951167, 'Training_MAE': 5.112971690124814, 'Validation_MAE': 12.975232813178852, 'Training_MSE': 61.50346, 'Validation_MSE': 632.4785}.
trigger times: 0
Loss after 86446200 batches: 0.0185
trigger times: 0
Loss after 86577300 batches: 0.0077
trigger times: 0
Loss after 86708400 batches: 0.0069
trigger times: 1
Loss after 86839500 batches: 0.0066
trigger times: 2
Loss after 86970600 batches: 0.0064
trigger times: 0
Loss after 87101700 batches: 0.0065
trigger times: 1
Loss after 87232800 batches: 0.0062
trigger times: 2
Loss after 87363900 batches: 0.0062
trigger times: 3
Loss after 87495000 batches: 0.0062
trigger times: 4
Loss after 87626100 batches: 0.0062
trigger times: 5
Loss after 87757200 batches: 0.0061
trigger times: 6
Loss after 87888300 batches: 0.0059
trigger times: 7
Loss after 88019400 batches: 0.0060
trigger times: 8
Loss after 88150500 batches: 0.0060
trigger times: 9
Loss after 88281600 batches: 0.0058
trigger times: 10
Loss after 88412700 batches: 0.0058
trigger times: 11
Loss after 88543800 batches: 0.0059
trigger times: 12
Loss after 88674900 batches: 0.0059
trigger times: 13
Loss after 88806000 batches: 0.0058
trigger times: 14
Loss after 88937100 batches: 0.0059
trigger times: 15
Loss after 89068200 batches: 0.0058
trigger times: 16
Loss after 89199300 batches: 0.0059
trigger times: 0
Loss after 89330400 batches: 0.0058
trigger times: 1
Loss after 89461500 batches: 0.0057
trigger times: 2
Loss after 89592600 batches: 0.0059
trigger times: 3
Loss after 89723700 batches: 0.0058
trigger times: 0
Loss after 89854800 batches: 0.0057
trigger times: 1
Loss after 89985900 batches: 0.0058
trigger times: 2
Loss after 90117000 batches: 0.0056
trigger times: 3
Loss after 90248100 batches: 0.0058
trigger times: 4
Loss after 90379200 batches: 0.0057
trigger times: 5
Loss after 90510300 batches: 0.0056
trigger times: 6
Loss after 90641400 batches: 0.0056
trigger times: 7
Loss after 90772500 batches: 0.0056
trigger times: 8
Loss after 90903600 batches: 0.0056
trigger times: 9
Loss after 91034700 batches: 0.0056
trigger times: 10
Loss after 91165800 batches: 0.0055
trigger times: 11
Loss after 91296900 batches: 0.0054
trigger times: 12
Loss after 91428000 batches: 0.0056
trigger times: 13
Loss after 91559100 batches: 0.0055
trigger times: 14
Loss after 91690200 batches: 0.0055
trigger times: 15
Loss after 91821300 batches: 0.0057
trigger times: 16
Loss after 91952400 batches: 0.0054
trigger times: 17
Loss after 92083500 batches: 0.0056
trigger times: 18
Loss after 92214600 batches: 0.0054
trigger times: 19
Loss after 92345700 batches: 0.0055
trigger times: 20
Early stopping!
Start to test process.
Loss after 92476800 batches: 0.0055
Time to train on one home:  353.9916684627533
trigger times: 0
Loss after 92579400 batches: 0.0930
trigger times: 0
Loss after 92682000 batches: 0.0308
trigger times: 1
Loss after 92784600 batches: 0.0249
trigger times: 2
Loss after 92887200 batches: 0.0237
trigger times: 3
Loss after 92989800 batches: 0.0255
trigger times: 4
Loss after 93092400 batches: 0.0213
trigger times: 5
Loss after 93195000 batches: 0.0203
trigger times: 6
Loss after 93297600 batches: 0.0194
trigger times: 7
Loss after 93400200 batches: 0.0196
trigger times: 8
Loss after 93502800 batches: 0.0194
trigger times: 9
Loss after 93605400 batches: 0.0207
trigger times: 10
Loss after 93708000 batches: 0.0212
trigger times: 11
Loss after 93810600 batches: 0.0189
trigger times: 12
Loss after 93913200 batches: 0.0175
trigger times: 13
Loss after 94015800 batches: 0.0176
trigger times: 14
Loss after 94118400 batches: 0.0182
trigger times: 15
Loss after 94221000 batches: 0.0214
trigger times: 16
Loss after 94323600 batches: 0.0219
trigger times: 17
Loss after 94426200 batches: 0.0183
trigger times: 18
Loss after 94528800 batches: 0.0182
trigger times: 19
Loss after 94631400 batches: 0.0170
trigger times: 20
Early stopping!
Start to test process.
Loss after 94734000 batches: 0.0167
Time to train on one home:  144.25537300109863
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468, 0.012550091408176377, 0.011063836768490445]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193], [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377], [0.2663021981716156, 0.71310717149598, 0.7055370444827307, 0.6282820545928078, 0.23502105231165174, 14.843388730992553, 725.5246]]
Round_9_results:  [0.2663021981716156, 0.71310717149598, 0.7055370444827307, 0.6282820545928078, 0.23502105231165174, 14.843388730992553, 725.5246]
trigger times: 0
Loss after 94865100 batches: 0.0189
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 781 < 782; dropping {'Training_Loss': 0.01892673322614634, 'Validation_Loss': 0.21397247993283802, 'Training_R2': 0.9809801503257869, 'Validation_R2': 0.8010989182003332, 'Training_F1': 0.9237289283937056, 'Validation_F1': 0.7520054749043541, 'Training_NEP': 0.15276408358096086, 'Validation_NEP': 0.476355047140104, 'Training_NDE': 0.014278538753286708, 'Validation_NDE': 0.15839433665183936, 'Training_MAE': 5.059243484354492, 'Validation_MAE': 13.063631668612095, 'Training_MSE': 62.82333, 'Validation_MSE': 584.9459}.
trigger times: 0
Loss after 94996200 batches: 0.0077
trigger times: 1
Loss after 95127300 batches: 0.0069
trigger times: 0
Loss after 95258400 batches: 0.0065
trigger times: 0
Loss after 95389500 batches: 0.0061
trigger times: 1
Loss after 95520600 batches: 0.0061
trigger times: 2
Loss after 95651700 batches: 0.0059
trigger times: 0
Loss after 95782800 batches: 0.0058
trigger times: 1
Loss after 95913900 batches: 0.0058
trigger times: 2
Loss after 96045000 batches: 0.0057
trigger times: 3
Loss after 96176100 batches: 0.0057
trigger times: 4
Loss after 96307200 batches: 0.0058
trigger times: 5
Loss after 96438300 batches: 0.0056
trigger times: 6
Loss after 96569400 batches: 0.0057
trigger times: 7
Loss after 96700500 batches: 0.0057
trigger times: 8
Loss after 96831600 batches: 0.0058
trigger times: 9
Loss after 96962700 batches: 0.0057
trigger times: 10
Loss after 97093800 batches: 0.0056
trigger times: 11
Loss after 97224900 batches: 0.0056
trigger times: 12
Loss after 97356000 batches: 0.0056
trigger times: 13
Loss after 97487100 batches: 0.0054
trigger times: 14
Loss after 97618200 batches: 0.0055
trigger times: 15
Loss after 97749300 batches: 0.0055
trigger times: 16
Loss after 97880400 batches: 0.0055
trigger times: 17
Loss after 98011500 batches: 0.0055
trigger times: 18
Loss after 98142600 batches: 0.0054
trigger times: 19
Loss after 98273700 batches: 0.0055
trigger times: 20
Early stopping!
Start to test process.
Loss after 98404800 batches: 0.0054
Time to train on one home:  216.4638946056366
trigger times: 0
Loss after 98507400 batches: 0.0940
trigger times: 0
Loss after 98610000 batches: 0.0325
trigger times: 1
Loss after 98712600 batches: 0.0299
trigger times: 0
Loss after 98815200 batches: 0.0252
trigger times: 1
Loss after 98917800 batches: 0.0233
trigger times: 2
Loss after 99020400 batches: 0.0228
trigger times: 3
Loss after 99123000 batches: 0.0219
trigger times: 4
Loss after 99225600 batches: 0.0208
trigger times: 5
Loss after 99328200 batches: 0.0193
trigger times: 6
Loss after 99430800 batches: 0.0188
trigger times: 7
Loss after 99533400 batches: 0.0183
trigger times: 8
Loss after 99636000 batches: 0.0183
trigger times: 9
Loss after 99738600 batches: 0.0187
trigger times: 10
Loss after 99841200 batches: 0.0186
trigger times: 11
Loss after 99943800 batches: 0.0177
trigger times: 12
Loss after 100046400 batches: 0.0176
trigger times: 13
Loss after 100149000 batches: 0.0173
trigger times: 14
Loss after 100251600 batches: 0.0167
trigger times: 15
Loss after 100354200 batches: 0.0173
trigger times: 16
Loss after 100456800 batches: 0.0193
trigger times: 17
Loss after 100559400 batches: 0.0178
trigger times: 18
Loss after 100662000 batches: 0.0188
trigger times: 19
Loss after 100764600 batches: 0.0173
trigger times: 20
Early stopping!
Start to test process.
Loss after 100867200 batches: 0.0179
Time to train on one home:  154.09028124809265
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468, 0.012550091408176377, 0.011063836768490445, 0.01163185285241155]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193], [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377], [0.2663021981716156, 0.71310717149598, 0.7055370444827307, 0.6282820545928078, 0.23502105231165174, 14.843388730992553, 725.5246], [0.25270352098676896, 0.727851446888989, 0.7089626089401181, 0.6079155827932633, 0.22294262171264745, 14.362223534900227, 688.2378]]
Round_10_results:  [0.25270352098676896, 0.727851446888989, 0.7089626089401181, 0.6079155827932633, 0.22294262171264745, 14.362223534900227, 688.2378]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 833 < 834; dropping {'Training_Loss': 0.01792584310443896, 'Validation_Loss': 0.23099295960532296, 'Training_R2': 0.9819954798858245, 'Validation_R2': 0.7854219064442025, 'Training_F1': 0.9252434542147087, 'Validation_F1': 0.7529826603122598, 'Training_NEP': 0.14972658332440972, 'Validation_NEP': 0.4773323362066573, 'Training_NDE': 0.013516312830438894, 'Validation_NDE': 0.17087868241470663, 'Training_MAE': 4.958647499870108, 'Validation_MAE': 13.090432989340956, 'Training_MSE': 59.469658, 'Validation_MSE': 631.0503}.
trigger times: 0
Loss after 100998300 batches: 0.0179
trigger times: 0
Loss after 101129400 batches: 0.0074
trigger times: 1
Loss after 101260500 batches: 0.0065
trigger times: 2
Loss after 101391600 batches: 0.0062
trigger times: 3
Loss after 101522700 batches: 0.0061
trigger times: 4
Loss after 101653800 batches: 0.0060
trigger times: 5
Loss after 101784900 batches: 0.0060
trigger times: 6
Loss after 101916000 batches: 0.0058
trigger times: 7
Loss after 102047100 batches: 0.0058
trigger times: 8
Loss after 102178200 batches: 0.0057
trigger times: 9
Loss after 102309300 batches: 0.0057
trigger times: 10
Loss after 102440400 batches: 0.0057
trigger times: 11
Loss after 102571500 batches: 0.0057
trigger times: 12
Loss after 102702600 batches: 0.0056
trigger times: 13
Loss after 102833700 batches: 0.0055
trigger times: 14
Loss after 102964800 batches: 0.0057
trigger times: 15
Loss after 103095900 batches: 0.0056
trigger times: 16
Loss after 103227000 batches: 0.0055
trigger times: 17
Loss after 103358100 batches: 0.0054
trigger times: 18
Loss after 103489200 batches: 0.0054
trigger times: 19
Loss after 103620300 batches: 0.0055
trigger times: 20
Early stopping!
Start to test process.
Loss after 103751400 batches: 0.0055
Time to train on one home:  173.45230984687805
trigger times: 0
Loss after 103854000 batches: 0.0828
trigger times: 1
Loss after 103956600 batches: 0.0259
trigger times: 2
Loss after 104059200 batches: 0.0208
trigger times: 3
Loss after 104161800 batches: 0.0203
trigger times: 4
Loss after 104264400 batches: 0.0192
trigger times: 5
Loss after 104367000 batches: 0.0190
trigger times: 6
Loss after 104469600 batches: 0.0194
trigger times: 7
Loss after 104572200 batches: 0.0185
trigger times: 8
Loss after 104674800 batches: 0.0184
trigger times: 9
Loss after 104777400 batches: 0.0196
trigger times: 10
Loss after 104880000 batches: 0.0180
trigger times: 11
Loss after 104982600 batches: 0.0185
trigger times: 12
Loss after 105085200 batches: 0.0186
trigger times: 13
Loss after 105187800 batches: 0.0176
trigger times: 14
Loss after 105290400 batches: 0.0174
trigger times: 15
Loss after 105393000 batches: 0.0171
trigger times: 16
Loss after 105495600 batches: 0.0172
trigger times: 17
Loss after 105598200 batches: 0.0166
trigger times: 18
Loss after 105700800 batches: 0.0165
trigger times: 19
Loss after 105803400 batches: 0.0160
trigger times: 20
Early stopping!
Start to test process.
Loss after 105906000 batches: 0.0181
Time to train on one home:  136.36070466041565
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468, 0.012550091408176377, 0.011063836768490445, 0.01163185285241155, 0.011824783068719284]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193], [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377], [0.2663021981716156, 0.71310717149598, 0.7055370444827307, 0.6282820545928078, 0.23502105231165174, 14.843388730992553, 725.5246], [0.25270352098676896, 0.727851446888989, 0.7089626089401181, 0.6079155827932633, 0.22294262171264745, 14.362223534900227, 688.2378], [0.2544779148366716, 0.725871599561609, 0.7061137267750279, 0.6131617949134658, 0.22456450192737296, 14.486167176607262, 693.24457]]
Round_11_results:  [0.2544779148366716, 0.725871599561609, 0.7061137267750279, 0.6131617949134658, 0.22456450192737296, 14.486167176607262, 693.24457]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 876 < 877; dropping {'Training_Loss': 0.018593536048494983, 'Validation_Loss': 0.21277638524770737, 'Training_R2': 0.9812960200855095, 'Validation_R2': 0.8021628390328241, 'Training_F1': 0.923411466591808, 'Validation_F1': 0.7599270386844884, 'Training_NEP': 0.15334951305310346, 'Validation_NEP': 0.4711058348022177, 'Training_NDE': 0.014041409717965992, 'Validation_NDE': 0.15754708618448274, 'Training_MAE': 5.078631747440015, 'Validation_MAE': 12.919676488659288, 'Training_MSE': 61.78, 'Validation_MSE': 581.817}.
trigger times: 0
Loss after 106037100 batches: 0.0186
trigger times: 0
Loss after 106168200 batches: 0.0075
trigger times: 1
Loss after 106299300 batches: 0.0067
trigger times: 0
Loss after 106430400 batches: 0.0063
trigger times: 0
Loss after 106561500 batches: 0.0061
trigger times: 1
Loss after 106692600 batches: 0.0059
trigger times: 2
Loss after 106823700 batches: 0.0057
trigger times: 3
Loss after 106954800 batches: 0.0056
trigger times: 4
Loss after 107085900 batches: 0.0057
trigger times: 5
Loss after 107217000 batches: 0.0057
trigger times: 6
Loss after 107348100 batches: 0.0057
trigger times: 7
Loss after 107479200 batches: 0.0056
trigger times: 8
Loss after 107610300 batches: 0.0057
trigger times: 9
Loss after 107741400 batches: 0.0055
trigger times: 10
Loss after 107872500 batches: 0.0055
trigger times: 11
Loss after 108003600 batches: 0.0055
trigger times: 12
Loss after 108134700 batches: 0.0055
trigger times: 13
Loss after 108265800 batches: 0.0054
trigger times: 14
Loss after 108396900 batches: 0.0054
trigger times: 15
Loss after 108528000 batches: 0.0055
trigger times: 16
Loss after 108659100 batches: 0.0055
trigger times: 17
Loss after 108790200 batches: 0.0054
trigger times: 18
Loss after 108921300 batches: 0.0053
trigger times: 19
Loss after 109052400 batches: 0.0053
trigger times: 20
Early stopping!
Start to test process.
Loss after 109183500 batches: 0.0054
Time to train on one home:  194.26874804496765
trigger times: 0
Loss after 109286100 batches: 0.0767
trigger times: 1
Loss after 109388700 batches: 0.0266
trigger times: 2
Loss after 109491300 batches: 0.0231
trigger times: 3
Loss after 109593900 batches: 0.0217
trigger times: 4
Loss after 109696500 batches: 0.0223
trigger times: 5
Loss after 109799100 batches: 0.0202
trigger times: 6
Loss after 109901700 batches: 0.0199
trigger times: 7
Loss after 110004300 batches: 0.0200
trigger times: 8
Loss after 110106900 batches: 0.0186
trigger times: 9
Loss after 110209500 batches: 0.0183
trigger times: 10
Loss after 110312100 batches: 0.0181
trigger times: 11
Loss after 110414700 batches: 0.0178
trigger times: 12
Loss after 110517300 batches: 0.0174
trigger times: 13
Loss after 110619900 batches: 0.0184
trigger times: 14
Loss after 110722500 batches: 0.0172
trigger times: 15
Loss after 110825100 batches: 0.0181
trigger times: 16
Loss after 110927700 batches: 0.0193
trigger times: 17
Loss after 111030300 batches: 0.0170
trigger times: 18
Loss after 111132900 batches: 0.0161
trigger times: 19
Loss after 111235500 batches: 0.0165
trigger times: 20
Early stopping!
Start to test process.
Loss after 111338100 batches: 0.0197
Time to train on one home:  136.85879230499268
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468, 0.012550091408176377, 0.011063836768490445, 0.01163185285241155, 0.011824783068719284, 0.0125261837206965]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193], [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377], [0.2663021981716156, 0.71310717149598, 0.7055370444827307, 0.6282820545928078, 0.23502105231165174, 14.843388730992553, 725.5246], [0.25270352098676896, 0.727851446888989, 0.7089626089401181, 0.6079155827932633, 0.22294262171264745, 14.362223534900227, 688.2378], [0.2544779148366716, 0.725871599561609, 0.7061137267750279, 0.6131617949134658, 0.22456450192737296, 14.486167176607262, 693.24457], [0.2666938371128506, 0.7126538057200025, 0.705986548011326, 0.6121637948704038, 0.23539244710150373, 14.462589067882961, 726.67114]]
Round_12_results:  [0.2666938371128506, 0.7126538057200025, 0.705986548011326, 0.6121637948704038, 0.23539244710150373, 14.462589067882961, 726.67114]
trigger times: 0
Loss after 111469200 batches: 0.0178
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 922 < 923; dropping {'Training_Loss': 0.01777720852878015, 'Validation_Loss': 0.1978780147102144, 'Training_R2': 0.9821250135157561, 'Validation_R2': 0.8159940218225349, 'Training_F1': 0.9258315837128076, 'Validation_F1': 0.7605457244165266, 'Training_NEP': 0.14848109104647628, 'Validation_NEP': 0.46190942070079566, 'Training_NDE': 0.013419069635223718, 'Validation_NDE': 0.14653266130924192, 'Training_MAE': 4.9173992657024925, 'Validation_MAE': 12.667472660413353, 'Training_MSE': 59.0418, 'Validation_MSE': 541.14105}.
trigger times: 0
Loss after 111600300 batches: 0.0072
trigger times: 0
Loss after 111731400 batches: 0.0065
trigger times: 1
Loss after 111862500 batches: 0.0061
trigger times: 2
Loss after 111993600 batches: 0.0058
trigger times: 3
Loss after 112124700 batches: 0.0058
trigger times: 4
Loss after 112255800 batches: 0.0060
trigger times: 5
Loss after 112386900 batches: 0.0056
trigger times: 6
Loss after 112518000 batches: 0.0056
trigger times: 7
Loss after 112649100 batches: 0.0055
trigger times: 8
Loss after 112780200 batches: 0.0056
trigger times: 9
Loss after 112911300 batches: 0.0055
trigger times: 10
Loss after 113042400 batches: 0.0055
trigger times: 11
Loss after 113173500 batches: 0.0055
trigger times: 12
Loss after 113304600 batches: 0.0055
trigger times: 13
Loss after 113435700 batches: 0.0054
trigger times: 14
Loss after 113566800 batches: 0.0054
trigger times: 15
Loss after 113697900 batches: 0.0053
trigger times: 16
Loss after 113829000 batches: 0.0054
trigger times: 17
Loss after 113960100 batches: 0.0054
trigger times: 18
Loss after 114091200 batches: 0.0054
trigger times: 19
Loss after 114222300 batches: 0.0053
trigger times: 20
Early stopping!
Start to test process.
Loss after 114353400 batches: 0.0053
Time to train on one home:  179.92067170143127
trigger times: 0
Loss after 114456000 batches: 0.0742
trigger times: 0
Loss after 114558600 batches: 0.0224
trigger times: 1
Loss after 114661200 batches: 0.0196
trigger times: 2
Loss after 114763800 batches: 0.0182
trigger times: 3
Loss after 114866400 batches: 0.0176
trigger times: 4
Loss after 114969000 batches: 0.0181
trigger times: 5
Loss after 115071600 batches: 0.0177
trigger times: 6
Loss after 115174200 batches: 0.0173
trigger times: 7
Loss after 115276800 batches: 0.0172
trigger times: 8
Loss after 115379400 batches: 0.0166
trigger times: 9
Loss after 115482000 batches: 0.0162
trigger times: 10
Loss after 115584600 batches: 0.0161
trigger times: 11
Loss after 115687200 batches: 0.0176
trigger times: 12
Loss after 115789800 batches: 0.0169
trigger times: 13
Loss after 115892400 batches: 0.0166
trigger times: 14
Loss after 115995000 batches: 0.0174
trigger times: 15
Loss after 116097600 batches: 0.0160
trigger times: 16
Loss after 116200200 batches: 0.0161
trigger times: 17
Loss after 116302800 batches: 0.0187
trigger times: 18
Loss after 116405400 batches: 0.0177
trigger times: 19
Loss after 116508000 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 116610600 batches: 0.0153
Time to train on one home:  143.16987562179565
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468, 0.012550091408176377, 0.011063836768490445, 0.01163185285241155, 0.011824783068719284, 0.0125261837206965, 0.010276443009312464]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193], [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377], [0.2663021981716156, 0.71310717149598, 0.7055370444827307, 0.6282820545928078, 0.23502105231165174, 14.843388730992553, 725.5246], [0.25270352098676896, 0.727851446888989, 0.7089626089401181, 0.6079155827932633, 0.22294262171264745, 14.362223534900227, 688.2378], [0.2544779148366716, 0.725871599561609, 0.7061137267750279, 0.6131617949134658, 0.22456450192737296, 14.486167176607262, 693.24457], [0.2666938371128506, 0.7126538057200025, 0.705986548011326, 0.6121637948704038, 0.23539244710150373, 14.462589067882961, 726.67114], [0.25190316140651703, 0.7286495802638416, 0.7092482351055476, 0.5931849219628168, 0.22228879517184127, 14.01420639296159, 686.21936]]
Round_13_results:  [0.25190316140651703, 0.7286495802638416, 0.7092482351055476, 0.5931849219628168, 0.22228879517184127, 14.01420639296159, 686.21936]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 967 < 968; dropping {'Training_Loss': 0.019000436943727283, 'Validation_Loss': 0.1936454251408577, 'Training_R2': 0.9808716317046163, 'Validation_R2': 0.8198783749738764, 'Training_F1': 0.9229677542680453, 'Validation_F1': 0.7697124277693835, 'Training_NEP': 0.15420086825993978, 'Validation_NEP': 0.454088242638071, 'Training_NDE': 0.014360005608407918, 'Validation_NDE': 0.14343936721973122, 'Training_MAE': 5.106826943470994, 'Validation_MAE': 12.452983509853322, 'Training_MSE': 63.181774, 'Validation_MSE': 529.71765}.
trigger times: 0
Loss after 116741700 batches: 0.0190
trigger times: 0
Loss after 116872800 batches: 0.0072
trigger times: 1
Loss after 117003900 batches: 0.0065
trigger times: 2
Loss after 117135000 batches: 0.0061
trigger times: 3
Loss after 117266100 batches: 0.0059
trigger times: 4
Loss after 117397200 batches: 0.0059
trigger times: 5
Loss after 117528300 batches: 0.0058
trigger times: 6
Loss after 117659400 batches: 0.0056
trigger times: 7
Loss after 117790500 batches: 0.0055
trigger times: 8
Loss after 117921600 batches: 0.0055
trigger times: 9
Loss after 118052700 batches: 0.0054
trigger times: 10
Loss after 118183800 batches: 0.0055
trigger times: 11
Loss after 118314900 batches: 0.0055
trigger times: 12
Loss after 118446000 batches: 0.0054
trigger times: 13
Loss after 118577100 batches: 0.0054
trigger times: 14
Loss after 118708200 batches: 0.0054
trigger times: 15
Loss after 118839300 batches: 0.0053
trigger times: 16
Loss after 118970400 batches: 0.0052
trigger times: 17
Loss after 119101500 batches: 0.0053
trigger times: 18
Loss after 119232600 batches: 0.0053
trigger times: 19
Loss after 119363700 batches: 0.0053
trigger times: 20
Early stopping!
Start to test process.
Loss after 119494800 batches: 0.0052
Time to train on one home:  172.73281383514404
trigger times: 0
Loss after 119597400 batches: 0.0692
trigger times: 1
Loss after 119700000 batches: 0.0219
trigger times: 2
Loss after 119802600 batches: 0.0185
trigger times: 3
Loss after 119905200 batches: 0.0172
trigger times: 4
Loss after 120007800 batches: 0.0168
trigger times: 5
Loss after 120110400 batches: 0.0174
trigger times: 6
Loss after 120213000 batches: 0.0173
trigger times: 7
Loss after 120315600 batches: 0.0167
trigger times: 8
Loss after 120418200 batches: 0.0168
trigger times: 9
Loss after 120520800 batches: 0.0160
trigger times: 10
Loss after 120623400 batches: 0.0161
trigger times: 11
Loss after 120726000 batches: 0.0159
trigger times: 12
Loss after 120828600 batches: 0.0156
trigger times: 13
Loss after 120931200 batches: 0.0169
trigger times: 14
Loss after 121033800 batches: 0.0157
trigger times: 15
Loss after 121136400 batches: 0.0170
trigger times: 16
Loss after 121239000 batches: 0.0156
trigger times: 17
Loss after 121341600 batches: 0.0158
trigger times: 18
Loss after 121444200 batches: 0.0166
trigger times: 19
Loss after 121546800 batches: 0.0150
trigger times: 20
Early stopping!
Start to test process.
Loss after 121649400 batches: 0.0147
Time to train on one home:  137.2798662185669
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468, 0.012550091408176377, 0.011063836768490445, 0.01163185285241155, 0.011824783068719284, 0.0125261837206965, 0.010276443009312464, 0.009969955873579416]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193], [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377], [0.2663021981716156, 0.71310717149598, 0.7055370444827307, 0.6282820545928078, 0.23502105231165174, 14.843388730992553, 725.5246], [0.25270352098676896, 0.727851446888989, 0.7089626089401181, 0.6079155827932633, 0.22294262171264745, 14.362223534900227, 688.2378], [0.2544779148366716, 0.725871599561609, 0.7061137267750279, 0.6131617949134658, 0.22456450192737296, 14.486167176607262, 693.24457], [0.2666938371128506, 0.7126538057200025, 0.705986548011326, 0.6121637948704038, 0.23539244710150373, 14.462589067882961, 726.67114], [0.25190316140651703, 0.7286495802638416, 0.7092482351055476, 0.5931849219628168, 0.22228879517184127, 14.01420639296159, 686.21936], [0.24904666013187832, 0.7318019064498353, 0.7116258247043286, 0.5983909341545783, 0.21970642662214626, 14.137200296950597, 678.2474]]
Round_14_results:  [0.24904666013187832, 0.7318019064498353, 0.7116258247043286, 0.5983909341545783, 0.21970642662214626, 14.137200296950597, 678.2474]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1010 < 1011; dropping {'Training_Loss': 0.018544114247526763, 'Validation_Loss': 0.23123860938681495, 'Training_R2': 0.9813305458645031, 'Validation_R2': 0.7851633376897578, 'Training_F1': 0.9244901545983247, 'Validation_F1': 0.7583353896314512, 'Training_NEP': 0.15117078210660717, 'Validation_NEP': 0.46984919249223955, 'Training_NDE': 0.014015490602841968, 'Validation_NDE': 0.17108459294052486, 'Training_MAE': 5.006476499381455, 'Validation_MAE': 12.885214143030105, 'Training_MSE': 61.66596, 'Validation_MSE': 631.8107}.
trigger times: 0
Loss after 121780500 batches: 0.0185
trigger times: 1
Loss after 121911600 batches: 0.0070
trigger times: 2
Loss after 122042700 batches: 0.0063
trigger times: 0
Loss after 122173800 batches: 0.0060
trigger times: 0
Loss after 122304900 batches: 0.0058
trigger times: 0
Loss after 122436000 batches: 0.0057
trigger times: 0
Loss after 122567100 batches: 0.0057
trigger times: 1
Loss after 122698200 batches: 0.0057
trigger times: 2
Loss after 122829300 batches: 0.0056
trigger times: 3
Loss after 122960400 batches: 0.0054
trigger times: 4
Loss after 123091500 batches: 0.0055
trigger times: 5
Loss after 123222600 batches: 0.0054
trigger times: 6
Loss after 123353700 batches: 0.0054
trigger times: 7
Loss after 123484800 batches: 0.0052
trigger times: 8
Loss after 123615900 batches: 0.0054
trigger times: 9
Loss after 123747000 batches: 0.0054
trigger times: 0
Loss after 123878100 batches: 0.0052
trigger times: 1
Loss after 124009200 batches: 0.0055
trigger times: 2
Loss after 124140300 batches: 0.0053
trigger times: 3
Loss after 124271400 batches: 0.0053
trigger times: 4
Loss after 124402500 batches: 0.0053
trigger times: 5
Loss after 124533600 batches: 0.0051
trigger times: 6
Loss after 124664700 batches: 0.0052
trigger times: 7
Loss after 124795800 batches: 0.0052
trigger times: 8
Loss after 124926900 batches: 0.0051
trigger times: 9
Loss after 125058000 batches: 0.0053
trigger times: 10
Loss after 125189100 batches: 0.0052
trigger times: 11
Loss after 125320200 batches: 0.0051
trigger times: 12
Loss after 125451300 batches: 0.0051
trigger times: 13
Loss after 125582400 batches: 0.0051
trigger times: 14
Loss after 125713500 batches: 0.0051
trigger times: 15
Loss after 125844600 batches: 0.0052
trigger times: 16
Loss after 125975700 batches: 0.0051
trigger times: 17
Loss after 126106800 batches: 0.0052
trigger times: 18
Loss after 126237900 batches: 0.0052
trigger times: 19
Loss after 126369000 batches: 0.0051
trigger times: 20
Early stopping!
Start to test process.
Loss after 126500100 batches: 0.0051
Time to train on one home:  282.56556010246277
trigger times: 0
Loss after 126602700 batches: 0.0677
trigger times: 1
Loss after 126705300 batches: 0.0207
trigger times: 2
Loss after 126807900 batches: 0.0190
trigger times: 3
Loss after 126910500 batches: 0.0176
trigger times: 4
Loss after 127013100 batches: 0.0175
trigger times: 5
Loss after 127115700 batches: 0.0201
trigger times: 6
Loss after 127218300 batches: 0.0160
trigger times: 7
Loss after 127320900 batches: 0.0162
trigger times: 8
Loss after 127423500 batches: 0.0173
trigger times: 9
Loss after 127526100 batches: 0.0155
trigger times: 10
Loss after 127628700 batches: 0.0153
trigger times: 11
Loss after 127731300 batches: 0.0151
trigger times: 12
Loss after 127833900 batches: 0.0148
trigger times: 13
Loss after 127936500 batches: 0.0147
trigger times: 14
Loss after 128039100 batches: 0.0149
trigger times: 15
Loss after 128141700 batches: 0.0146
trigger times: 16
Loss after 128244300 batches: 0.0160
trigger times: 17
Loss after 128346900 batches: 0.0145
trigger times: 18
Loss after 128449500 batches: 0.0157
trigger times: 19
Loss after 128552100 batches: 0.0157
trigger times: 20
Early stopping!
Start to test process.
Loss after 128654700 batches: 0.0145
Time to train on one home:  136.20245552062988
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468, 0.012550091408176377, 0.011063836768490445, 0.01163185285241155, 0.011824783068719284, 0.0125261837206965, 0.010276443009312464, 0.009969955873579416, 0.0098096743874128]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193], [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377], [0.2663021981716156, 0.71310717149598, 0.7055370444827307, 0.6282820545928078, 0.23502105231165174, 14.843388730992553, 725.5246], [0.25270352098676896, 0.727851446888989, 0.7089626089401181, 0.6079155827932633, 0.22294262171264745, 14.362223534900227, 688.2378], [0.2544779148366716, 0.725871599561609, 0.7061137267750279, 0.6131617949134658, 0.22456450192737296, 14.486167176607262, 693.24457], [0.2666938371128506, 0.7126538057200025, 0.705986548011326, 0.6121637948704038, 0.23539244710150373, 14.462589067882961, 726.67114], [0.25190316140651703, 0.7286495802638416, 0.7092482351055476, 0.5931849219628168, 0.22228879517184127, 14.01420639296159, 686.21936], [0.24904666013187832, 0.7318019064498353, 0.7116258247043286, 0.5983909341545783, 0.21970642662214626, 14.137200296950597, 678.2474], [0.2613627256618606, 0.7183849008913448, 0.7072632416319652, 0.5987265228520742, 0.23069756495652122, 14.145128700211911, 712.17773]]
Round_15_results:  [0.2613627256618606, 0.7183849008913448, 0.7072632416319652, 0.5987265228520742, 0.23069756495652122, 14.145128700211911, 712.17773]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1068 < 1069; dropping {'Training_Loss': 0.01793739392053125, 'Validation_Loss': 0.21456805037127602, 'Training_R2': 0.9819572485100594, 'Validation_R2': 0.8004969112481258, 'Training_F1': 0.9248746426301544, 'Validation_F1': 0.7681692745852641, 'Training_NEP': 0.150402740627402, 'Validation_NEP': 0.46012555733593963, 'Training_NDE': 0.013545013802833659, 'Validation_NDE': 0.15887374325431675, 'Training_MAE': 4.981040488780682, 'Validation_MAE': 12.618551726153248, 'Training_MSE': 59.595947, 'Validation_MSE': 586.7164}.
trigger times: 0
Loss after 128785800 batches: 0.0179
trigger times: 0
Loss after 128916900 batches: 0.0067
trigger times: 1
Loss after 129048000 batches: 0.0061
trigger times: 2
Loss after 129179100 batches: 0.0058
trigger times: 3
Loss after 129310200 batches: 0.0058
trigger times: 4
Loss after 129441300 batches: 0.0055
trigger times: 5
Loss after 129572400 batches: 0.0055
trigger times: 6
Loss after 129703500 batches: 0.0054
trigger times: 7
Loss after 129834600 batches: 0.0054
trigger times: 8
Loss after 129965700 batches: 0.0054
trigger times: 9
Loss after 130096800 batches: 0.0054
trigger times: 10
Loss after 130227900 batches: 0.0052
trigger times: 11
Loss after 130359000 batches: 0.0053
trigger times: 0
Loss after 130490100 batches: 0.0054
trigger times: 1
Loss after 130621200 batches: 0.0053
trigger times: 2
Loss after 130752300 batches: 0.0052
trigger times: 3
Loss after 130883400 batches: 0.0052
trigger times: 4
Loss after 131014500 batches: 0.0050
trigger times: 5
Loss after 131145600 batches: 0.0052
trigger times: 6
Loss after 131276700 batches: 0.0051
trigger times: 7
Loss after 131407800 batches: 0.0053
trigger times: 8
Loss after 131538900 batches: 0.0051
trigger times: 9
Loss after 131670000 batches: 0.0050
trigger times: 10
Loss after 131801100 batches: 0.0050
trigger times: 11
Loss after 131932200 batches: 0.0051
trigger times: 12
Loss after 132063300 batches: 0.0051
trigger times: 13
Loss after 132194400 batches: 0.0052
trigger times: 14
Loss after 132325500 batches: 0.0050
trigger times: 15
Loss after 132456600 batches: 0.0049
trigger times: 16
Loss after 132587700 batches: 0.0050
trigger times: 17
Loss after 132718800 batches: 0.0050
trigger times: 18
Loss after 132849900 batches: 0.0050
trigger times: 19
Loss after 132981000 batches: 0.0050
trigger times: 20
Early stopping!
Start to test process.
Loss after 133112100 batches: 0.0050
Time to train on one home:  259.9213228225708
trigger times: 0
Loss after 133214700 batches: 0.0646
trigger times: 1
Loss after 133317300 batches: 0.0201
trigger times: 2
Loss after 133419900 batches: 0.0167
trigger times: 3
Loss after 133522500 batches: 0.0163
trigger times: 4
Loss after 133625100 batches: 0.0162
trigger times: 5
Loss after 133727700 batches: 0.0165
trigger times: 6
Loss after 133830300 batches: 0.0160
trigger times: 7
Loss after 133932900 batches: 0.0159
trigger times: 8
Loss after 134035500 batches: 0.0166
trigger times: 9
Loss after 134138100 batches: 0.0183
trigger times: 10
Loss after 134240700 batches: 0.0173
trigger times: 11
Loss after 134343300 batches: 0.0164
trigger times: 12
Loss after 134445900 batches: 0.0151
trigger times: 13
Loss after 134548500 batches: 0.0148
trigger times: 14
Loss after 134651100 batches: 0.0146
trigger times: 15
Loss after 134753700 batches: 0.0142
trigger times: 16
Loss after 134856300 batches: 0.0140
trigger times: 17
Loss after 134958900 batches: 0.0143
trigger times: 18
Loss after 135061500 batches: 0.0146
trigger times: 19
Loss after 135164100 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 135266700 batches: 0.0168
Time to train on one home:  137.51890301704407
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468, 0.012550091408176377, 0.011063836768490445, 0.01163185285241155, 0.011824783068719284, 0.0125261837206965, 0.010276443009312464, 0.009969955873579416, 0.0098096743874128, 0.010941623763649022]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193], [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377], [0.2663021981716156, 0.71310717149598, 0.7055370444827307, 0.6282820545928078, 0.23502105231165174, 14.843388730992553, 725.5246], [0.25270352098676896, 0.727851446888989, 0.7089626089401181, 0.6079155827932633, 0.22294262171264745, 14.362223534900227, 688.2378], [0.2544779148366716, 0.725871599561609, 0.7061137267750279, 0.6131617949134658, 0.22456450192737296, 14.486167176607262, 693.24457], [0.2666938371128506, 0.7126538057200025, 0.705986548011326, 0.6121637948704038, 0.23539244710150373, 14.462589067882961, 726.67114], [0.25190316140651703, 0.7286495802638416, 0.7092482351055476, 0.5931849219628168, 0.22228879517184127, 14.01420639296159, 686.21936], [0.24904666013187832, 0.7318019064498353, 0.7116258247043286, 0.5983909341545783, 0.21970642662214626, 14.137200296950597, 678.2474], [0.2613627256618606, 0.7183849008913448, 0.7072632416319652, 0.5987265228520742, 0.23069756495652122, 14.145128700211911, 712.17773], [0.2656688574287627, 0.7137178347788815, 0.7042097535131658, 0.6113799423464334, 0.23452080025549546, 14.444070271052768, 723.9803]]
Round_16_results:  [0.2656688574287627, 0.7137178347788815, 0.7042097535131658, 0.6113799423464334, 0.23452080025549546, 14.444070271052768, 723.9803]
trigger times: 0
Loss after 135397800 batches: 0.0176
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1123 < 1124; dropping {'Training_Loss': 0.017606022671075923, 'Validation_Loss': 0.23604049368037117, 'Training_R2': 0.9822874977992316, 'Validation_R2': 0.7807499892934486, 'Training_F1': 0.925948885036908, 'Validation_F1': 0.7584301075268817, 'Training_NEP': 0.14826673192999298, 'Validation_NEP': 0.4662821818560895, 'Training_NDE': 0.013297089799518104, 'Validation_NDE': 0.17459915095761455, 'Training_MAE': 4.910300116884535, 'Validation_MAE': 12.787391912766255, 'Training_MSE': 58.505116, 'Validation_MSE': 644.78986}.
trigger times: 0
Loss after 135528900 batches: 0.0067
trigger times: 1
Loss after 135660000 batches: 0.0060
trigger times: 2
Loss after 135791100 batches: 0.0059
trigger times: 3
Loss after 135922200 batches: 0.0055
trigger times: 4
Loss after 136053300 batches: 0.0055
trigger times: 5
Loss after 136184400 batches: 0.0054
trigger times: 6
Loss after 136315500 batches: 0.0053
trigger times: 7
Loss after 136446600 batches: 0.0052
trigger times: 8
Loss after 136577700 batches: 0.0052
trigger times: 9
Loss after 136708800 batches: 0.0051
trigger times: 10
Loss after 136839900 batches: 0.0050
trigger times: 11
Loss after 136971000 batches: 0.0052
trigger times: 12
Loss after 137102100 batches: 0.0051
trigger times: 13
Loss after 137233200 batches: 0.0051
trigger times: 14
Loss after 137364300 batches: 0.0051
trigger times: 15
Loss after 137495400 batches: 0.0050
trigger times: 16
Loss after 137626500 batches: 0.0051
trigger times: 17
Loss after 137757600 batches: 0.0050
trigger times: 18
Loss after 137888700 batches: 0.0050
trigger times: 19
Loss after 138019800 batches: 0.0050
trigger times: 20
Early stopping!
Start to test process.
Loss after 138150900 batches: 0.0050
Time to train on one home:  173.3536605834961
trigger times: 0
Loss after 138253500 batches: 0.0694
trigger times: 0
Loss after 138356100 batches: 0.0194
trigger times: 1
Loss after 138458700 batches: 0.0171
trigger times: 2
Loss after 138561300 batches: 0.0173
trigger times: 3
Loss after 138663900 batches: 0.0164
trigger times: 4
Loss after 138766500 batches: 0.0166
trigger times: 5
Loss after 138869100 batches: 0.0157
trigger times: 6
Loss after 138971700 batches: 0.0162
trigger times: 7
Loss after 139074300 batches: 0.0154
trigger times: 8
Loss after 139176900 batches: 0.0167
trigger times: 9
Loss after 139279500 batches: 0.0159
trigger times: 10
Loss after 139382100 batches: 0.0148
trigger times: 11
Loss after 139484700 batches: 0.0148
trigger times: 12
Loss after 139587300 batches: 0.0143
trigger times: 13
Loss after 139689900 batches: 0.0144
trigger times: 14
Loss after 139792500 batches: 0.0143
trigger times: 15
Loss after 139895100 batches: 0.0150
trigger times: 16
Loss after 139997700 batches: 0.0143
trigger times: 17
Loss after 140100300 batches: 0.0146
trigger times: 18
Loss after 140202900 batches: 0.0144
trigger times: 19
Loss after 140305500 batches: 0.0148
trigger times: 20
Early stopping!
Start to test process.
Loss after 140408100 batches: 0.0149
Time to train on one home:  142.6037871837616
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468, 0.012550091408176377, 0.011063836768490445, 0.01163185285241155, 0.011824783068719284, 0.0125261837206965, 0.010276443009312464, 0.009969955873579416, 0.0098096743874128, 0.010941623763649022, 0.009987550931342569]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193], [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377], [0.2663021981716156, 0.71310717149598, 0.7055370444827307, 0.6282820545928078, 0.23502105231165174, 14.843388730992553, 725.5246], [0.25270352098676896, 0.727851446888989, 0.7089626089401181, 0.6079155827932633, 0.22294262171264745, 14.362223534900227, 688.2378], [0.2544779148366716, 0.725871599561609, 0.7061137267750279, 0.6131617949134658, 0.22456450192737296, 14.486167176607262, 693.24457], [0.2666938371128506, 0.7126538057200025, 0.705986548011326, 0.6121637948704038, 0.23539244710150373, 14.462589067882961, 726.67114], [0.25190316140651703, 0.7286495802638416, 0.7092482351055476, 0.5931849219628168, 0.22228879517184127, 14.01420639296159, 686.21936], [0.24904666013187832, 0.7318019064498353, 0.7116258247043286, 0.5983909341545783, 0.21970642662214626, 14.137200296950597, 678.2474], [0.2613627256618606, 0.7183849008913448, 0.7072632416319652, 0.5987265228520742, 0.23069756495652122, 14.145128700211911, 712.17773], [0.2656688574287627, 0.7137178347788815, 0.7042097535131658, 0.6113799423464334, 0.23452080025549546, 14.444070271052768, 723.9803], [0.261268078453011, 0.7184715882165533, 0.7060757603468095, 0.6189808345958958, 0.23062655116890263, 14.623644074786949, 711.9585]]
Round_17_results:  [0.261268078453011, 0.7184715882165533, 0.7060757603468095, 0.6189808345958958, 0.23062655116890263, 14.623644074786949, 711.9585]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1167 < 1168; dropping {'Training_Loss': 0.01716365346263321, 'Validation_Loss': 0.19587134652667576, 'Training_R2': 0.9827361761061454, 'Validation_R2': 0.8178528061157242, 'Training_F1': 0.9276735619206548, 'Validation_F1': 0.7629384061262068, 'Training_NEP': 0.14480403858200544, 'Validation_NEP': 0.4585743260490391, 'Training_NDE': 0.012960259030465602, 'Validation_NDE': 0.1450524234823049, 'Training_MAE': 4.795622580460607, 'Validation_MAE': 12.576010528602053, 'Training_MSE': 57.023106, 'Validation_MSE': 535.6746}.
trigger times: 0
Loss after 140539200 batches: 0.0172
trigger times: 0
Loss after 140670300 batches: 0.0066
trigger times: 1
Loss after 140801400 batches: 0.0059
trigger times: 2
Loss after 140932500 batches: 0.0057
trigger times: 3
Loss after 141063600 batches: 0.0055
trigger times: 4
Loss after 141194700 batches: 0.0054
trigger times: 0
Loss after 141325800 batches: 0.0053
trigger times: 1
Loss after 141456900 batches: 0.0053
trigger times: 2
Loss after 141588000 batches: 0.0053
trigger times: 3
Loss after 141719100 batches: 0.0052
trigger times: 4
Loss after 141850200 batches: 0.0051
trigger times: 5
Loss after 141981300 batches: 0.0051
trigger times: 6
Loss after 142112400 batches: 0.0052
trigger times: 7
Loss after 142243500 batches: 0.0050
trigger times: 8
Loss after 142374600 batches: 0.0052
trigger times: 9
Loss after 142505700 batches: 0.0050
trigger times: 10
Loss after 142636800 batches: 0.0051
trigger times: 11
Loss after 142767900 batches: 0.0050
trigger times: 12
Loss after 142899000 batches: 0.0051
trigger times: 13
Loss after 143030100 batches: 0.0049
trigger times: 14
Loss after 143161200 batches: 0.0050
trigger times: 15
Loss after 143292300 batches: 0.0049
trigger times: 16
Loss after 143423400 batches: 0.0051
trigger times: 17
Loss after 143554500 batches: 0.0049
trigger times: 18
Loss after 143685600 batches: 0.0049
trigger times: 19
Loss after 143816700 batches: 0.0049
trigger times: 20
Early stopping!
Start to test process.
Loss after 143947800 batches: 0.0049
Time to train on one home:  208.57805466651917
trigger times: 0
Loss after 144050400 batches: 0.0626
trigger times: 1
Loss after 144153000 batches: 0.0193
trigger times: 2
Loss after 144255600 batches: 0.0177
trigger times: 3
Loss after 144358200 batches: 0.0165
trigger times: 4
Loss after 144460800 batches: 0.0160
trigger times: 5
Loss after 144563400 batches: 0.0151
trigger times: 6
Loss after 144666000 batches: 0.0155
trigger times: 7
Loss after 144768600 batches: 0.0160
trigger times: 8
Loss after 144871200 batches: 0.0148
trigger times: 9
Loss after 144973800 batches: 0.0145
trigger times: 10
Loss after 145076400 batches: 0.0146
trigger times: 11
Loss after 145179000 batches: 0.0149
trigger times: 12
Loss after 145281600 batches: 0.0150
trigger times: 13
Loss after 145384200 batches: 0.0147
trigger times: 14
Loss after 145486800 batches: 0.0137
trigger times: 15
Loss after 145589400 batches: 0.0139
trigger times: 16
Loss after 145692000 batches: 0.0138
trigger times: 17
Loss after 145794600 batches: 0.0139
trigger times: 18
Loss after 145897200 batches: 0.0148
trigger times: 19
Loss after 145999800 batches: 0.0135
trigger times: 20
Early stopping!
Start to test process.
Loss after 146102400 batches: 0.0137
Time to train on one home:  136.61458945274353
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468, 0.012550091408176377, 0.011063836768490445, 0.01163185285241155, 0.011824783068719284, 0.0125261837206965, 0.010276443009312464, 0.009969955873579416, 0.0098096743874128, 0.010941623763649022, 0.009987550931342569, 0.009317491026988516]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193], [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377], [0.2663021981716156, 0.71310717149598, 0.7055370444827307, 0.6282820545928078, 0.23502105231165174, 14.843388730992553, 725.5246], [0.25270352098676896, 0.727851446888989, 0.7089626089401181, 0.6079155827932633, 0.22294262171264745, 14.362223534900227, 688.2378], [0.2544779148366716, 0.725871599561609, 0.7061137267750279, 0.6131617949134658, 0.22456450192737296, 14.486167176607262, 693.24457], [0.2666938371128506, 0.7126538057200025, 0.705986548011326, 0.6121637948704038, 0.23539244710150373, 14.462589067882961, 726.67114], [0.25190316140651703, 0.7286495802638416, 0.7092482351055476, 0.5931849219628168, 0.22228879517184127, 14.01420639296159, 686.21936], [0.24904666013187832, 0.7318019064498353, 0.7116258247043286, 0.5983909341545783, 0.21970642662214626, 14.137200296950597, 678.2474], [0.2613627256618606, 0.7183849008913448, 0.7072632416319652, 0.5987265228520742, 0.23069756495652122, 14.145128700211911, 712.17773], [0.2656688574287627, 0.7137178347788815, 0.7042097535131658, 0.6113799423464334, 0.23452080025549546, 14.444070271052768, 723.9803], [0.261268078453011, 0.7184715882165533, 0.7060757603468095, 0.6189808345958958, 0.23062655116890263, 14.623644074786949, 711.9585], [0.26291131393777, 0.7166957594452226, 0.706760277816635, 0.6034175301961993, 0.23208130048675668, 14.255955430085514, 716.4494]]
Round_18_results:  [0.26291131393777, 0.7166957594452226, 0.706760277816635, 0.6034175301961993, 0.23208130048675668, 14.255955430085514, 716.4494]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1215 < 1216; dropping {'Training_Loss': 0.01662436060009981, 'Validation_Loss': 0.20501919421884748, 'Training_R2': 0.9832689478388763, 'Validation_R2': 0.8093487382576339, 'Training_F1': 0.9278437325448098, 'Validation_F1': 0.7617533938720602, 'Training_NEP': 0.14443149672624847, 'Validation_NEP': 0.4613990099507768, 'Training_NDE': 0.012560297833991702, 'Validation_NDE': 0.15182461484012336, 'Training_MAE': 4.783284733028107, 'Validation_MAE': 12.653475080083348, 'Training_MSE': 55.263336, 'Validation_MSE': 560.68414}.
trigger times: 0
Loss after 146233500 batches: 0.0166
trigger times: 0
Loss after 146364600 batches: 0.0068
trigger times: 1
Loss after 146495700 batches: 0.0056
trigger times: 2
Loss after 146626800 batches: 0.0055
trigger times: 0
Loss after 146757900 batches: 0.0054
trigger times: 1
Loss after 146889000 batches: 0.0053
trigger times: 2
Loss after 147020100 batches: 0.0051
trigger times: 3
Loss after 147151200 batches: 0.0051
trigger times: 4
Loss after 147282300 batches: 0.0052
trigger times: 5
Loss after 147413400 batches: 0.0051
trigger times: 6
Loss after 147544500 batches: 0.0050
trigger times: 7
Loss after 147675600 batches: 0.0052
trigger times: 8
Loss after 147806700 batches: 0.0051
trigger times: 9
Loss after 147937800 batches: 0.0052
trigger times: 10
Loss after 148068900 batches: 0.0050
trigger times: 0
Loss after 148200000 batches: 0.0050
trigger times: 1
Loss after 148331100 batches: 0.0051
trigger times: 2
Loss after 148462200 batches: 0.0049
trigger times: 3
Loss after 148593300 batches: 0.0050
trigger times: 4
Loss after 148724400 batches: 0.0049
trigger times: 5
Loss after 148855500 batches: 0.0049
trigger times: 6
Loss after 148986600 batches: 0.0050
trigger times: 7
Loss after 149117700 batches: 0.0049
trigger times: 8
Loss after 149248800 batches: 0.0049
trigger times: 9
Loss after 149379900 batches: 0.0049
trigger times: 10
Loss after 149511000 batches: 0.0050
trigger times: 11
Loss after 149642100 batches: 0.0050
trigger times: 12
Loss after 149773200 batches: 0.0049
trigger times: 13
Loss after 149904300 batches: 0.0049
trigger times: 14
Loss after 150035400 batches: 0.0049
trigger times: 15
Loss after 150166500 batches: 0.0049
trigger times: 16
Loss after 150297600 batches: 0.0049
trigger times: 17
Loss after 150428700 batches: 0.0048
trigger times: 18
Loss after 150559800 batches: 0.0047
trigger times: 19
Loss after 150690900 batches: 0.0048
trigger times: 20
Early stopping!
Start to test process.
Loss after 150822000 batches: 0.0048
Time to train on one home:  274.95091676712036
trigger times: 0
Loss after 150924600 batches: 0.0595
trigger times: 1
Loss after 151027200 batches: 0.0185
trigger times: 2
Loss after 151129800 batches: 0.0161
trigger times: 3
Loss after 151232400 batches: 0.0157
trigger times: 4
Loss after 151335000 batches: 0.0158
trigger times: 5
Loss after 151437600 batches: 0.0149
trigger times: 6
Loss after 151540200 batches: 0.0155
trigger times: 7
Loss after 151642800 batches: 0.0158
trigger times: 8
Loss after 151745400 batches: 0.0147
trigger times: 9
Loss after 151848000 batches: 0.0140
trigger times: 10
Loss after 151950600 batches: 0.0142
trigger times: 11
Loss after 152053200 batches: 0.0147
trigger times: 12
Loss after 152155800 batches: 0.0141
trigger times: 13
Loss after 152258400 batches: 0.0140
trigger times: 14
Loss after 152361000 batches: 0.0136
trigger times: 15
Loss after 152463600 batches: 0.0138
trigger times: 16
Loss after 152566200 batches: 0.0137
trigger times: 17
Loss after 152668800 batches: 0.0133
trigger times: 18
Loss after 152771400 batches: 0.0133
trigger times: 19
Loss after 152874000 batches: 0.0148
trigger times: 20
Early stopping!
Start to test process.
Loss after 152976600 batches: 0.0137
Time to train on one home:  138.50781059265137
train_results:  [0.0638206151882245, 0.032527834880738946, 0.022083185005871125, 0.01851950877225604, 0.017348830304288465, 0.013561092562904884, 0.013607770681710912, 0.011835530528092468, 0.012550091408176377, 0.011063836768490445, 0.01163185285241155, 0.011824783068719284, 0.0125261837206965, 0.010276443009312464, 0.009969955873579416, 0.0098096743874128, 0.010941623763649022, 0.009987550931342569, 0.009317491026988516, 0.009257666147644535]
test_results:  [[0.7138209640979767, 0.22798497501446957, 0.3239613431718226, 1.3151683789028992, 0.6324305299599462, 31.071324339220602, 1952.3523], [0.32718564569950104, 0.6470260629842683, 0.6555292015118758, 0.6939285875150697, 0.2891543387424252, 16.394311600560357, 892.63745], [0.24663883282078636, 0.7343240223843921, 0.719567906757355, 0.6130987378033798, 0.2176403228994343, 14.48467742978031, 671.86926], [0.25381826361020404, 0.7266235669104825, 0.7089009687291224, 0.6312272518633478, 0.22394849434517644, 14.91297007213784, 691.34296], [0.25387324227227104, 0.7265710111770418, 0.711556934623273, 0.6294055522424549, 0.22399154771755467, 14.869931765653753, 691.4758], [0.2734965624080764, 0.7053361556491324, 0.6901657023511311, 0.6599735553709012, 0.2413870264329985, 15.592111795862943, 745.1768], [0.2532128517826398, 0.727290525127706, 0.7051332238982758, 0.6362753619636657, 0.2234021257835184, 15.032233482620622, 689.65625], [0.25473642266458935, 0.7256443480439025, 0.6984297852087163, 0.633622533615287, 0.2247506651406861, 14.969559462053374, 693.8193], [0.2626446510354678, 0.7170958188080905, 0.7010098927195321, 0.6406213925795432, 0.23175357402200472, 15.134910013641507, 715.4377], [0.2663021981716156, 0.71310717149598, 0.7055370444827307, 0.6282820545928078, 0.23502105231165174, 14.843388730992553, 725.5246], [0.25270352098676896, 0.727851446888989, 0.7089626089401181, 0.6079155827932633, 0.22294262171264745, 14.362223534900227, 688.2378], [0.2544779148366716, 0.725871599561609, 0.7061137267750279, 0.6131617949134658, 0.22456450192737296, 14.486167176607262, 693.24457], [0.2666938371128506, 0.7126538057200025, 0.705986548011326, 0.6121637948704038, 0.23539244710150373, 14.462589067882961, 726.67114], [0.25190316140651703, 0.7286495802638416, 0.7092482351055476, 0.5931849219628168, 0.22228879517184127, 14.01420639296159, 686.21936], [0.24904666013187832, 0.7318019064498353, 0.7116258247043286, 0.5983909341545783, 0.21970642662214626, 14.137200296950597, 678.2474], [0.2613627256618606, 0.7183849008913448, 0.7072632416319652, 0.5987265228520742, 0.23069756495652122, 14.145128700211911, 712.17773], [0.2656688574287627, 0.7137178347788815, 0.7042097535131658, 0.6113799423464334, 0.23452080025549546, 14.444070271052768, 723.9803], [0.261268078453011, 0.7184715882165533, 0.7060757603468095, 0.6189808345958958, 0.23062655116890263, 14.623644074786949, 711.9585], [0.26291131393777, 0.7166957594452226, 0.706760277816635, 0.6034175301961993, 0.23208130048675668, 14.255955430085514, 716.4494], [0.25484493954314125, 0.7254594789146087, 0.7123783658060041, 0.6011202137747423, 0.22490210893080487, 14.201680506215729, 694.2868]]
Round_19_results:  [0.25484493954314125, 0.7254594789146087, 0.7123783658060041, 0.6011202137747423, 0.22490210893080487, 14.201680506215729, 694.2868]