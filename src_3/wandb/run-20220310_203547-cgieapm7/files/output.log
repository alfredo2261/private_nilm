LSTM(
  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))
  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))
  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))
  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))
  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))
  (linear1): Linear(in_features=23500, out_features=1024, bias=True)
  (linear2): Linear(in_features=1024, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (dropout): Dropout(p=0.2, inplace=False)
)
Window Length:  499
trigger times: 0
Loss after 131100 batches: 0.7274
trigger times: 1
Loss after 262200 batches: 0.3482
trigger times: 0
Loss after 393300 batches: 0.2558
trigger times: 1
Loss after 524400 batches: 0.2092
trigger times: 2
Loss after 655500 batches: 0.1791
trigger times: 0
Loss after 786600 batches: 0.1578
trigger times: 0
Loss after 917700 batches: 0.1404
trigger times: 0
Loss after 1048800 batches: 0.1214
trigger times: 1
Loss after 1179900 batches: 0.1069
trigger times: 2
Loss after 1311000 batches: 0.0958
trigger times: 0
Loss after 1442100 batches: 0.0869
trigger times: 1
Loss after 1573200 batches: 0.0780
trigger times: 2
Loss after 1704300 batches: 0.0688
trigger times: 3
Loss after 1835400 batches: 0.0625
trigger times: 0
Loss after 1966500 batches: 0.0602
trigger times: 1
Loss after 2097600 batches: 0.0541
trigger times: 0
Loss after 2228700 batches: 0.0510
trigger times: 0
Loss after 2359800 batches: 0.0459
trigger times: 1
Loss after 2490900 batches: 0.0435
trigger times: 2
Loss after 2622000 batches: 0.0409
trigger times: 3
Loss after 2753100 batches: 0.0397
trigger times: 0
Loss after 2884200 batches: 0.0362
trigger times: 1
Loss after 3015300 batches: 0.0353
trigger times: 2
Loss after 3146400 batches: 0.0335
trigger times: 3
Loss after 3277500 batches: 0.0343
trigger times: 4
Loss after 3408600 batches: 0.0306
trigger times: 5
Loss after 3539700 batches: 0.0308
trigger times: 0
Loss after 3670800 batches: 0.0286
trigger times: 1
Loss after 3801900 batches: 0.0278
trigger times: 2
Loss after 3933000 batches: 0.0265
trigger times: 3
Loss after 4064100 batches: 0.0276
trigger times: 4
Loss after 4195200 batches: 0.0257
trigger times: 0
Loss after 4326300 batches: 0.0243
trigger times: 1
Loss after 4457400 batches: 0.0239
trigger times: 2
Loss after 4588500 batches: 0.0228
trigger times: 3
Loss after 4719600 batches: 0.0224
trigger times: 4
Loss after 4850700 batches: 0.0220
trigger times: 5
Loss after 4981800 batches: 0.0212
trigger times: 6
Loss after 5112900 batches: 0.0211
trigger times: 7
Loss after 5244000 batches: 0.0206
trigger times: 8
Loss after 5375100 batches: 0.0200
trigger times: 9
Loss after 5506200 batches: 0.0193
trigger times: 0
Loss after 5637300 batches: 0.0190
trigger times: 1
Loss after 5768400 batches: 0.0187
trigger times: 2
Loss after 5899500 batches: 0.0181
trigger times: 3
Loss after 6030600 batches: 0.0177
trigger times: 4
Loss after 6161700 batches: 0.0176
trigger times: 0
Loss after 6292800 batches: 0.0171
trigger times: 1
Loss after 6423900 batches: 0.0166
trigger times: 2
Loss after 6555000 batches: 0.0169
trigger times: 0
Loss after 6686100 batches: 0.0164
trigger times: 1
Loss after 6817200 batches: 0.0165
trigger times: 2
Loss after 6948300 batches: 0.0156
trigger times: 3
Loss after 7079400 batches: 0.0159
trigger times: 0
Loss after 7210500 batches: 0.0153
trigger times: 1
Loss after 7341600 batches: 0.0152
trigger times: 2
Loss after 7472700 batches: 0.0146
trigger times: 3
Loss after 7603800 batches: 0.0147
trigger times: 4
Loss after 7734900 batches: 0.0148
trigger times: 5
Loss after 7866000 batches: 0.0142
trigger times: 6
Loss after 7997100 batches: 0.0143
trigger times: 7
Loss after 8128200 batches: 0.0137
trigger times: 8
Loss after 8259300 batches: 0.0139
trigger times: 0
Loss after 8390400 batches: 0.0137
trigger times: 1
Loss after 8521500 batches: 0.0132
trigger times: 2
Loss after 8652600 batches: 0.0138
trigger times: 3
Loss after 8783700 batches: 0.0131
trigger times: 4
Loss after 8914800 batches: 0.0125
trigger times: 5
Loss after 9045900 batches: 0.0124
trigger times: 6
Loss after 9177000 batches: 0.0125
trigger times: 7
Loss after 9308100 batches: 0.0124
trigger times: 8
Loss after 9439200 batches: 0.0123
trigger times: 9
Loss after 9570300 batches: 0.0125
trigger times: 0
Loss after 9701400 batches: 0.0122
trigger times: 1
Loss after 9832500 batches: 0.0117
trigger times: 2
Loss after 9963600 batches: 0.0121
trigger times: 3
Loss after 10094700 batches: 0.0122
trigger times: 4
Loss after 10225800 batches: 0.0116
trigger times: 5
Loss after 10356900 batches: 0.0117
trigger times: 6
Loss after 10488000 batches: 0.0115
trigger times: 7
Loss after 10619100 batches: 0.0112
trigger times: 8
Loss after 10750200 batches: 0.0113
trigger times: 0
Loss after 10881300 batches: 0.0113
trigger times: 1
Loss after 11012400 batches: 0.0113
trigger times: 0
Loss after 11143500 batches: 0.0110
trigger times: 1
Loss after 11274600 batches: 0.0108
trigger times: 2
Loss after 11405700 batches: 0.0109
trigger times: 3
Loss after 11536800 batches: 0.0108
trigger times: 4
Loss after 11667900 batches: 0.0104
trigger times: 5
Loss after 11799000 batches: 0.0108
trigger times: 6
Loss after 11930100 batches: 0.0106
trigger times: 7
Loss after 12061200 batches: 0.0105
trigger times: 8
Loss after 12192300 batches: 0.0104
trigger times: 0
Loss after 12323400 batches: 0.0103
trigger times: 1
Loss after 12454500 batches: 0.0105
trigger times: 2
Loss after 12585600 batches: 0.0101
trigger times: 3
Loss after 12716700 batches: 0.0102
trigger times: 4
Loss after 12847800 batches: 0.0103
trigger times: 5
Loss after 12978900 batches: 0.0100
trigger times: 6
Loss after 13110000 batches: 0.0101
trigger times: 7
Loss after 13241100 batches: 0.0098
trigger times: 8
Loss after 13372200 batches: 0.0099
trigger times: 9
Loss after 13503300 batches: 0.0100
trigger times: 10
Loss after 13634400 batches: 0.0097
trigger times: 11
Loss after 13765500 batches: 0.0099
trigger times: 12
Loss after 13896600 batches: 0.0098
trigger times: 13
Loss after 14027700 batches: 0.0096
trigger times: 14
Loss after 14158800 batches: 0.0096
trigger times: 15
Loss after 14289900 batches: 0.0095
trigger times: 16
Loss after 14421000 batches: 0.0091
trigger times: 17
Loss after 14552100 batches: 0.0094
trigger times: 18
Loss after 14683200 batches: 0.0094
trigger times: 19
Loss after 14814300 batches: 0.0094
trigger times: 20
Early stopping!
Start to test process.
Loss after 14945400 batches: 0.0092
Time to train on one home:  828.9589035511017
trigger times: 0
Loss after 15048000 batches: 0.9846
trigger times: 0
Loss after 15150600 batches: 0.8284
trigger times: 1
Loss after 15253200 batches: 0.6646
trigger times: 0
Loss after 15355800 batches: 0.5676
trigger times: 0
Loss after 15458400 batches: 0.5080
trigger times: 0
Loss after 15561000 batches: 0.4727
trigger times: 1
Loss after 15663600 batches: 0.4263
trigger times: 2
Loss after 15766200 batches: 0.3798
trigger times: 3
Loss after 15868800 batches: 0.3412
trigger times: 4
Loss after 15971400 batches: 0.3275
trigger times: 5
Loss after 16074000 batches: 0.3102
trigger times: 6
Loss after 16176600 batches: 0.2617
trigger times: 7
Loss after 16279200 batches: 0.2339
trigger times: 8
Loss after 16381800 batches: 0.2107
trigger times: 9
Loss after 16484400 batches: 0.2011
trigger times: 10
Loss after 16587000 batches: 0.1873
trigger times: 11
Loss after 16689600 batches: 0.1684
trigger times: 12
Loss after 16792200 batches: 0.1571
trigger times: 13
Loss after 16894800 batches: 0.1667
trigger times: 14
Loss after 16997400 batches: 0.1453
trigger times: 15
Loss after 17100000 batches: 0.1331
trigger times: 16
Loss after 17202600 batches: 0.1230
trigger times: 17
Loss after 17305200 batches: 0.1208
trigger times: 18
Loss after 17407800 batches: 0.1131
trigger times: 19
Loss after 17510400 batches: 0.1080
trigger times: 20
Early stopping!
Start to test process.
Loss after 17613000 batches: 0.1068
Time to train on one home:  161.28162121772766
trigger times: 0
Loss after 17744100 batches: 0.8525
trigger times: 0
Loss after 17875200 batches: 0.5213
trigger times: 1
Loss after 18006300 batches: 0.4337
trigger times: 2
Loss after 18137400 batches: 0.3593
trigger times: 3
Loss after 18268500 batches: 0.3065
trigger times: 4
Loss after 18399600 batches: 0.2727
trigger times: 5
Loss after 18530700 batches: 0.2404
trigger times: 6
Loss after 18661800 batches: 0.2181
trigger times: 7
Loss after 18792900 batches: 0.2013
trigger times: 8
Loss after 18924000 batches: 0.1846
trigger times: 9
Loss after 19055100 batches: 0.1719
trigger times: 10
Loss after 19186200 batches: 0.1576
trigger times: 11
Loss after 19317300 batches: 0.1472
trigger times: 12
Loss after 19448400 batches: 0.1374
trigger times: 13
Loss after 19579500 batches: 0.1253
trigger times: 14
Loss after 19710600 batches: 0.1179
trigger times: 15
Loss after 19841700 batches: 0.1084
trigger times: 16
Loss after 19972800 batches: 0.1006
trigger times: 17
Loss after 20103900 batches: 0.0929
trigger times: 18
Loss after 20235000 batches: 0.0900
trigger times: 19
Loss after 20366100 batches: 0.0820
trigger times: 20
Early stopping!
Start to test process.
Loss after 20497200 batches: 0.0774
Time to train on one home:  169.58104610443115
trigger times: 0
Loss after 20628300 batches: 0.9648
trigger times: 0
Loss after 20759400 batches: 0.7561
trigger times: 0
Loss after 20890500 batches: 0.6707
trigger times: 1
Loss after 21021600 batches: 0.6239
trigger times: 2
Loss after 21152700 batches: 0.5697
trigger times: 3
Loss after 21283800 batches: 0.4886
trigger times: 4
Loss after 21414900 batches: 0.4067
trigger times: 5
Loss after 21546000 batches: 0.3261
trigger times: 6
Loss after 21677100 batches: 0.2604
trigger times: 7
Loss after 21808200 batches: 0.2116
trigger times: 8
Loss after 21939300 batches: 0.1734
trigger times: 9
Loss after 22070400 batches: 0.1503
trigger times: 10
Loss after 22201500 batches: 0.1345
trigger times: 11
Loss after 22332600 batches: 0.1198
trigger times: 12
Loss after 22463700 batches: 0.1125
trigger times: 13
Loss after 22594800 batches: 0.1043
trigger times: 14
Loss after 22725900 batches: 0.0970
trigger times: 15
Loss after 22857000 batches: 0.0910
trigger times: 16
Loss after 22988100 batches: 0.0868
trigger times: 17
Loss after 23119200 batches: 0.0846
trigger times: 18
Loss after 23250300 batches: 0.0810
trigger times: 19
Loss after 23381400 batches: 0.0783
trigger times: 20
Early stopping!
Start to test process.
Loss after 23512500 batches: 0.0752
Time to train on one home:  176.12783432006836
trigger times: 0
Loss after 23641140 batches: 0.7746
trigger times: 0
Loss after 23769780 batches: 0.4737
trigger times: 0
Loss after 23898420 batches: 0.4074
trigger times: 0
Loss after 24027060 batches: 0.3587
trigger times: 0
Loss after 24155700 batches: 0.3071
trigger times: 0
Loss after 24284340 batches: 0.2573
trigger times: 1
Loss after 24412980 batches: 0.2117
trigger times: 2
Loss after 24541620 batches: 0.1797
trigger times: 3
Loss after 24670260 batches: 0.1574
trigger times: 4
Loss after 24798900 batches: 0.1444
trigger times: 5
Loss after 24927540 batches: 0.1260
trigger times: 6
Loss after 25056180 batches: 0.1139
trigger times: 7
Loss after 25184820 batches: 0.1037
trigger times: 8
Loss after 25313460 batches: 0.0984
trigger times: 9
Loss after 25442100 batches: 0.0880
trigger times: 10
Loss after 25570740 batches: 0.0842
trigger times: 11
Loss after 25699380 batches: 0.0782
trigger times: 12
Loss after 25828020 batches: 0.0743
trigger times: 13
Loss after 25956660 batches: 0.0701
trigger times: 14
Loss after 26085300 batches: 0.0673
trigger times: 15
Loss after 26213940 batches: 0.0642
trigger times: 16
Loss after 26342580 batches: 0.0597
trigger times: 17
Loss after 26471220 batches: 0.0590
trigger times: 18
Loss after 26599860 batches: 0.0552
trigger times: 19
Loss after 26728500 batches: 0.0527
trigger times: 20
Early stopping!
Start to test process.
Loss after 26857140 batches: 0.0518
Time to train on one home:  195.04672980308533
trigger times: 0
Loss after 26988240 batches: 0.9364
trigger times: 0
Loss after 27119340 batches: 0.7786
trigger times: 1
Loss after 27250440 batches: 0.7079
trigger times: 2
Loss after 27381540 batches: 0.6287
trigger times: 3
Loss after 27512640 batches: 0.5443
trigger times: 4
Loss after 27643740 batches: 0.4653
trigger times: 5
Loss after 27774840 batches: 0.3710
trigger times: 6
Loss after 27905940 batches: 0.2854
trigger times: 7
Loss after 28037040 batches: 0.2197
trigger times: 8
Loss after 28168140 batches: 0.1802
trigger times: 9
Loss after 28299240 batches: 0.1522
trigger times: 10
Loss after 28430340 batches: 0.1336
trigger times: 11
Loss after 28561440 batches: 0.1201
trigger times: 12
Loss after 28692540 batches: 0.1088
trigger times: 13
Loss after 28823640 batches: 0.1034
trigger times: 14
Loss after 28954740 batches: 0.0975
trigger times: 15
Loss after 29085840 batches: 0.0918
trigger times: 16
Loss after 29216940 batches: 0.0866
trigger times: 17
Loss after 29348040 batches: 0.0822
trigger times: 18
Loss after 29479140 batches: 0.0803
trigger times: 19
Loss after 29610240 batches: 0.0766
trigger times: 20
Early stopping!
Start to test process.
Loss after 29741340 batches: 0.0737
Time to train on one home:  169.281503200531
trigger times: 0
Loss after 29872440 batches: 0.9999
trigger times: 0
Loss after 30003540 batches: 0.8758
trigger times: 0
Loss after 30134640 batches: 0.7987
trigger times: 0
Loss after 30265740 batches: 0.7434
trigger times: 1
Loss after 30396840 batches: 0.6852
trigger times: 2
Loss after 30527940 batches: 0.5934
trigger times: 3
Loss after 30659040 batches: 0.5153
trigger times: 4
Loss after 30790140 batches: 0.4603
trigger times: 5
Loss after 30921240 batches: 0.4125
trigger times: 6
Loss after 31052340 batches: 0.3824
trigger times: 7
Loss after 31183440 batches: 0.3379
trigger times: 8
Loss after 31314540 batches: 0.3115
trigger times: 9
Loss after 31445640 batches: 0.2845
trigger times: 10
Loss after 31576740 batches: 0.2644
trigger times: 11
Loss after 31707840 batches: 0.2584
trigger times: 12
Loss after 31838940 batches: 0.2469
trigger times: 13
Loss after 31970040 batches: 0.2191
trigger times: 14
Loss after 32101140 batches: 0.2093
trigger times: 15
Loss after 32232240 batches: 0.2016
trigger times: 16
Loss after 32363340 batches: 0.1912
trigger times: 17
Loss after 32494440 batches: 0.1767
trigger times: 18
Loss after 32625540 batches: 0.1659
trigger times: 19
Loss after 32756640 batches: 0.1554
trigger times: 20
Early stopping!
Start to test process.
Loss after 32887740 batches: 0.1509
Time to train on one home:  183.48134088516235
trigger times: 0
Loss after 33018840 batches: 0.8613
trigger times: 1
Loss after 33149940 batches: 0.4371
trigger times: 0
Loss after 33281040 batches: 0.3207
trigger times: 0
Loss after 33412140 batches: 0.2516
trigger times: 0
Loss after 33543240 batches: 0.2020
trigger times: 1
Loss after 33674340 batches: 0.1658
trigger times: 0
Loss after 33805440 batches: 0.1377
trigger times: 1
Loss after 33936540 batches: 0.1152
trigger times: 2
Loss after 34067640 batches: 0.0971
trigger times: 0
Loss after 34198740 batches: 0.0864
trigger times: 0
Loss after 34329840 batches: 0.0731
trigger times: 1
Loss after 34460940 batches: 0.0652
trigger times: 2
Loss after 34592040 batches: 0.0593
trigger times: 0
Loss after 34723140 batches: 0.0555
trigger times: 0
Loss after 34854240 batches: 0.0511
trigger times: 1
Loss after 34985340 batches: 0.0462
trigger times: 0
Loss after 35116440 batches: 0.0435
trigger times: 0
Loss after 35247540 batches: 0.0404
trigger times: 1
Loss after 35378640 batches: 0.0386
trigger times: 0
Loss after 35509740 batches: 0.0366
trigger times: 1
Loss after 35640840 batches: 0.0351
trigger times: 0
Loss after 35771940 batches: 0.0348
trigger times: 1
Loss after 35903040 batches: 0.0324
trigger times: 0
Loss after 36034140 batches: 0.0312
trigger times: 1
Loss after 36165240 batches: 0.0302
trigger times: 0
Loss after 36296340 batches: 0.0286
trigger times: 1
Loss after 36427440 batches: 0.0281
trigger times: 2
Loss after 36558540 batches: 0.0262
trigger times: 3
Loss after 36689640 batches: 0.0255
trigger times: 4
Loss after 36820740 batches: 0.0259
trigger times: 5
Loss after 36951840 batches: 0.0247
trigger times: 6
Loss after 37082940 batches: 0.0235
trigger times: 7
Loss after 37214040 batches: 0.0225
trigger times: 8
Loss after 37345140 batches: 0.0218
trigger times: 0
Loss after 37476240 batches: 0.0221
trigger times: 1
Loss after 37607340 batches: 0.0213
trigger times: 2
Loss after 37738440 batches: 0.0204
trigger times: 3
Loss after 37869540 batches: 0.0202
trigger times: 0
Loss after 38000640 batches: 0.0200
trigger times: 1
Loss after 38131740 batches: 0.0193
trigger times: 2
Loss after 38262840 batches: 0.0188
trigger times: 3
Loss after 38393940 batches: 0.0187
trigger times: 4
Loss after 38525040 batches: 0.0179
trigger times: 5
Loss after 38656140 batches: 0.0178
trigger times: 6
Loss after 38787240 batches: 0.0177
trigger times: 7
Loss after 38918340 batches: 0.0175
trigger times: 8
Loss after 39049440 batches: 0.0169
trigger times: 9
Loss after 39180540 batches: 0.0161
trigger times: 10
Loss after 39311640 batches: 0.0159
trigger times: 11
Loss after 39442740 batches: 0.0155
trigger times: 12
Loss after 39573840 batches: 0.0156
trigger times: 13
Loss after 39704940 batches: 0.0153
trigger times: 14
Loss after 39836040 batches: 0.0146
trigger times: 15
Loss after 39967140 batches: 0.0144
trigger times: 16
Loss after 40098240 batches: 0.0143
trigger times: 17
Loss after 40229340 batches: 0.0142
trigger times: 18
Loss after 40360440 batches: 0.0138
trigger times: 19
Loss after 40491540 batches: 0.0134
trigger times: 20
Early stopping!
Start to test process.
Loss after 40622640 batches: 0.0132
Time to train on one home:  435.30011439323425
trigger times: 0
Loss after 40701240 batches: 0.9784
trigger times: 0
Loss after 40779840 batches: 0.7706
trigger times: 0
Loss after 40858440 batches: 0.6234
trigger times: 1
Loss after 40937040 batches: 0.5239
trigger times: 2
Loss after 41015640 batches: 0.4523
trigger times: 3
Loss after 41094240 batches: 0.3879
trigger times: 4
Loss after 41172840 batches: 0.3166
trigger times: 5
Loss after 41251440 batches: 0.2717
trigger times: 6
Loss after 41330040 batches: 0.2270
trigger times: 7
Loss after 41408640 batches: 0.1925
trigger times: 8
Loss after 41487240 batches: 0.1675
trigger times: 9
Loss after 41565840 batches: 0.1456
trigger times: 10
Loss after 41644440 batches: 0.1306
trigger times: 11
Loss after 41723040 batches: 0.1199
trigger times: 12
Loss after 41801640 batches: 0.1120
trigger times: 13
Loss after 41880240 batches: 0.1003
trigger times: 14
Loss after 41958840 batches: 0.0920
trigger times: 15
Loss after 42037440 batches: 0.0866
trigger times: 16
Loss after 42116040 batches: 0.0840
trigger times: 17
Loss after 42194640 batches: 0.0852
trigger times: 18
Loss after 42273240 batches: 0.0809
trigger times: 19
Loss after 42351840 batches: 0.0728
trigger times: 20
Early stopping!
Start to test process.
Loss after 42430440 batches: 0.0737
Time to train on one home:  118.23862314224243
trigger times: 0
Loss after 42561540 batches: 0.7994
trigger times: 0
Loss after 42692640 batches: 0.3839
trigger times: 0
Loss after 42823740 batches: 0.2898
trigger times: 1
Loss after 42954840 batches: 0.2332
trigger times: 2
Loss after 43085940 batches: 0.1901
trigger times: 3
Loss after 43217040 batches: 0.1595
trigger times: 4
Loss after 43348140 batches: 0.1275
trigger times: 5
Loss after 43479240 batches: 0.1043
trigger times: 6
Loss after 43610340 batches: 0.0885
trigger times: 7
Loss after 43741440 batches: 0.0766
trigger times: 8
Loss after 43872540 batches: 0.0672
trigger times: 9
Loss after 44003640 batches: 0.0613
trigger times: 10
Loss after 44134740 batches: 0.0542
trigger times: 11
Loss after 44265840 batches: 0.0494
trigger times: 12
Loss after 44396940 batches: 0.0463
trigger times: 13
Loss after 44528040 batches: 0.0440
trigger times: 14
Loss after 44659140 batches: 0.0414
trigger times: 15
Loss after 44790240 batches: 0.0379
trigger times: 16
Loss after 44921340 batches: 0.0375
trigger times: 17
Loss after 45052440 batches: 0.0346
trigger times: 18
Loss after 45183540 batches: 0.0332
trigger times: 19
Loss after 45314640 batches: 0.0321
trigger times: 20
Early stopping!
Start to test process.
Loss after 45445740 batches: 0.0303
Time to train on one home:  177.06621527671814
train_results:  [0.06620264916472092]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545]]
Round_0_results:  [0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 363 < 364; dropping {'Training_Loss': 0.5811203363931404, 'Validation_Loss': 0.3375679850578308, 'Training_R2': 0.4156689506813548, 'Validation_R2': 0.6858713580856519, 'Training_F1': 0.54462907934579, 'Validation_F1': 0.6980227221622556, 'Training_NEP': 0.9097284975517015, 'Validation_NEP': 0.6449502265899931, 'Training_NDE': 0.43866769061570765, 'Validation_NDE': 0.25015549140894455, 'Training_MAE': 30.128403652752745, 'Validation_MAE': 17.68721094768107, 'Training_MSE': 1930.0692, 'Validation_MSE': 923.8174}.
trigger times: 0
Loss after 45576840 batches: 0.5811
trigger times: 1
Loss after 45707940 batches: 0.2749
trigger times: 0
Loss after 45839040 batches: 0.2206
trigger times: 1
Loss after 45970140 batches: 0.1915
trigger times: 2
Loss after 46101240 batches: 0.1701
trigger times: 3
Loss after 46232340 batches: 0.1515
trigger times: 4
Loss after 46363440 batches: 0.1368
trigger times: 0
Loss after 46494540 batches: 0.1243
trigger times: 0
Loss after 46625640 batches: 0.1142
trigger times: 1
Loss after 46756740 batches: 0.1086
trigger times: 2
Loss after 46887840 batches: 0.0991
trigger times: 3
Loss after 47018940 batches: 0.0937
trigger times: 0
Loss after 47150040 batches: 0.0891
trigger times: 0
Loss after 47281140 batches: 0.0836
trigger times: 1
Loss after 47412240 batches: 0.0792
trigger times: 2
Loss after 47543340 batches: 0.0761
trigger times: 0
Loss after 47674440 batches: 0.0716
trigger times: 1
Loss after 47805540 batches: 0.0674
trigger times: 2
Loss after 47936640 batches: 0.0645
trigger times: 0
Loss after 48067740 batches: 0.0618
trigger times: 1
Loss after 48198840 batches: 0.0593
trigger times: 2
Loss after 48329940 batches: 0.0559
trigger times: 3
Loss after 48461040 batches: 0.0549
trigger times: 4
Loss after 48592140 batches: 0.0514
trigger times: 5
Loss after 48723240 batches: 0.0499
trigger times: 6
Loss after 48854340 batches: 0.0487
trigger times: 7
Loss after 48985440 batches: 0.0471
trigger times: 8
Loss after 49116540 batches: 0.0466
trigger times: 9
Loss after 49247640 batches: 0.0446
trigger times: 10
Loss after 49378740 batches: 0.0427
trigger times: 0
Loss after 49509840 batches: 0.0418
trigger times: 1
Loss after 49640940 batches: 0.0399
trigger times: 2
Loss after 49772040 batches: 0.0397
trigger times: 3
Loss after 49903140 batches: 0.0380
trigger times: 0
Loss after 50034240 batches: 0.0386
trigger times: 0
Loss after 50165340 batches: 0.0376
trigger times: 1
Loss after 50296440 batches: 0.0358
trigger times: 2
Loss after 50427540 batches: 0.0358
trigger times: 3
Loss after 50558640 batches: 0.0343
trigger times: 4
Loss after 50689740 batches: 0.0335
trigger times: 5
Loss after 50820840 batches: 0.0334
trigger times: 6
Loss after 50951940 batches: 0.0319
trigger times: 7
Loss after 51083040 batches: 0.0323
trigger times: 8
Loss after 51214140 batches: 0.0312
trigger times: 9
Loss after 51345240 batches: 0.0304
trigger times: 10
Loss after 51476340 batches: 0.0296
trigger times: 11
Loss after 51607440 batches: 0.0299
trigger times: 0
Loss after 51738540 batches: 0.0285
trigger times: 1
Loss after 51869640 batches: 0.0287
trigger times: 2
Loss after 52000740 batches: 0.0279
trigger times: 3
Loss after 52131840 batches: 0.0275
trigger times: 4
Loss after 52262940 batches: 0.0271
trigger times: 5
Loss after 52394040 batches: 0.0265
trigger times: 0
Loss after 52525140 batches: 0.0264
trigger times: 1
Loss after 52656240 batches: 0.0253
trigger times: 2
Loss after 52787340 batches: 0.0250
trigger times: 3
Loss after 52918440 batches: 0.0250
trigger times: 4
Loss after 53049540 batches: 0.0248
trigger times: 5
Loss after 53180640 batches: 0.0241
trigger times: 6
Loss after 53311740 batches: 0.0243
trigger times: 7
Loss after 53442840 batches: 0.0237
trigger times: 8
Loss after 53573940 batches: 0.0238
trigger times: 9
Loss after 53705040 batches: 0.0235
trigger times: 10
Loss after 53836140 batches: 0.0228
trigger times: 11
Loss after 53967240 batches: 0.0227
trigger times: 12
Loss after 54098340 batches: 0.0224
trigger times: 13
Loss after 54229440 batches: 0.0219
trigger times: 14
Loss after 54360540 batches: 0.0217
trigger times: 15
Loss after 54491640 batches: 0.0218
trigger times: 16
Loss after 54622740 batches: 0.0213
trigger times: 17
Loss after 54753840 batches: 0.0214
trigger times: 18
Loss after 54884940 batches: 0.0208
trigger times: 19
Loss after 55016040 batches: 0.0206
trigger times: 20
Early stopping!
Start to test process.
Loss after 55147140 batches: 0.0205
Time to train on one home:  540.5206573009491
trigger times: 0
Loss after 55249740 batches: 0.7490
trigger times: 0
Loss after 55352340 batches: 0.5455
trigger times: 0
Loss after 55454940 batches: 0.4694
trigger times: 1
Loss after 55557540 batches: 0.4123
trigger times: 2
Loss after 55660140 batches: 0.3586
trigger times: 3
Loss after 55762740 batches: 0.3193
trigger times: 4
Loss after 55865340 batches: 0.2860
trigger times: 5
Loss after 55967940 batches: 0.2539
trigger times: 6
Loss after 56070540 batches: 0.2238
trigger times: 7
Loss after 56173140 batches: 0.2142
trigger times: 8
Loss after 56275740 batches: 0.1884
trigger times: 9
Loss after 56378340 batches: 0.1778
trigger times: 10
Loss after 56480940 batches: 0.1689
trigger times: 11
Loss after 56583540 batches: 0.1676
trigger times: 12
Loss after 56686140 batches: 0.1527
trigger times: 13
Loss after 56788740 batches: 0.1453
trigger times: 14
Loss after 56891340 batches: 0.1449
trigger times: 15
Loss after 56993940 batches: 0.1324
trigger times: 16
Loss after 57096540 batches: 0.1348
trigger times: 17
Loss after 57199140 batches: 0.1425
trigger times: 18
Loss after 57301740 batches: 0.1173
trigger times: 19
Loss after 57404340 batches: 0.1120
trigger times: 20
Early stopping!
Start to test process.
Loss after 57506940 batches: 0.1155
Time to train on one home:  143.9690704345703
trigger times: 0
Loss after 57638040 batches: 0.5107
trigger times: 1
Loss after 57769140 batches: 0.3291
trigger times: 2
Loss after 57900240 batches: 0.2717
trigger times: 3
Loss after 58031340 batches: 0.2266
trigger times: 4
Loss after 58162440 batches: 0.1961
trigger times: 5
Loss after 58293540 batches: 0.1693
trigger times: 6
Loss after 58424640 batches: 0.1508
trigger times: 7
Loss after 58555740 batches: 0.1380
trigger times: 8
Loss after 58686840 batches: 0.1251
trigger times: 9
Loss after 58817940 batches: 0.1157
trigger times: 10
Loss after 58949040 batches: 0.1084
trigger times: 11
Loss after 59080140 batches: 0.1024
trigger times: 12
Loss after 59211240 batches: 0.0954
trigger times: 13
Loss after 59342340 batches: 0.0907
trigger times: 14
Loss after 59473440 batches: 0.0860
trigger times: 15
Loss after 59604540 batches: 0.0817
trigger times: 16
Loss after 59735640 batches: 0.0788
trigger times: 17
Loss after 59866740 batches: 0.0754
trigger times: 18
Loss after 59997840 batches: 0.0729
trigger times: 19
Loss after 60128940 batches: 0.0711
trigger times: 20
Early stopping!
Start to test process.
Loss after 60260040 batches: 0.0678
Time to train on one home:  161.8460624217987
trigger times: 0
Loss after 60391140 batches: 0.7817
trigger times: 1
Loss after 60522240 batches: 0.5973
trigger times: 2
Loss after 60653340 batches: 0.4471
trigger times: 3
Loss after 60784440 batches: 0.3240
trigger times: 4
Loss after 60915540 batches: 0.2500
trigger times: 5
Loss after 61046640 batches: 0.2060
trigger times: 6
Loss after 61177740 batches: 0.1773
trigger times: 7
Loss after 61308840 batches: 0.1545
trigger times: 8
Loss after 61439940 batches: 0.1422
trigger times: 9
Loss after 61571040 batches: 0.1283
trigger times: 10
Loss after 61702140 batches: 0.1197
trigger times: 11
Loss after 61833240 batches: 0.1119
trigger times: 12
Loss after 61964340 batches: 0.1074
trigger times: 13
Loss after 62095440 batches: 0.1013
trigger times: 14
Loss after 62226540 batches: 0.0967
trigger times: 15
Loss after 62357640 batches: 0.0934
trigger times: 16
Loss after 62488740 batches: 0.0901
trigger times: 17
Loss after 62619840 batches: 0.0873
trigger times: 18
Loss after 62750940 batches: 0.0835
trigger times: 19
Loss after 62882040 batches: 0.0806
trigger times: 20
Early stopping!
Start to test process.
Loss after 63013140 batches: 0.0788
Time to train on one home:  162.43972516059875
trigger times: 0
Loss after 63141780 batches: 0.5008
trigger times: 0
Loss after 63270420 batches: 0.3275
trigger times: 1
Loss after 63399060 batches: 0.2474
trigger times: 2
Loss after 63527700 batches: 0.1992
trigger times: 3
Loss after 63656340 batches: 0.1688
trigger times: 4
Loss after 63784980 batches: 0.1502
trigger times: 5
Loss after 63913620 batches: 0.1335
trigger times: 6
Loss after 64042260 batches: 0.1218
trigger times: 7
Loss after 64170900 batches: 0.1096
trigger times: 8
Loss after 64299540 batches: 0.1028
trigger times: 9
Loss after 64428180 batches: 0.0970
trigger times: 10
Loss after 64556820 batches: 0.0903
trigger times: 11
Loss after 64685460 batches: 0.0840
trigger times: 12
Loss after 64814100 batches: 0.0816
trigger times: 13
Loss after 64942740 batches: 0.0772
trigger times: 14
Loss after 65071380 batches: 0.0746
trigger times: 15
Loss after 65200020 batches: 0.0715
trigger times: 16
Loss after 65328660 batches: 0.0676
trigger times: 17
Loss after 65457300 batches: 0.0653
trigger times: 18
Loss after 65585940 batches: 0.0627
trigger times: 19
Loss after 65714580 batches: 0.0605
trigger times: 20
Early stopping!
Start to test process.
Loss after 65843220 batches: 0.0589
Time to train on one home:  167.0398256778717
trigger times: 0
Loss after 65974320 batches: 0.7924
trigger times: 1
Loss after 66105420 batches: 0.5615
trigger times: 2
Loss after 66236520 batches: 0.4334
trigger times: 3
Loss after 66367620 batches: 0.3245
trigger times: 4
Loss after 66498720 batches: 0.2499
trigger times: 5
Loss after 66629820 batches: 0.1981
trigger times: 6
Loss after 66760920 batches: 0.1685
trigger times: 7
Loss after 66892020 batches: 0.1481
trigger times: 8
Loss after 67023120 batches: 0.1320
trigger times: 9
Loss after 67154220 batches: 0.1223
trigger times: 10
Loss after 67285320 batches: 0.1119
trigger times: 11
Loss after 67416420 batches: 0.1073
trigger times: 12
Loss after 67547520 batches: 0.1009
trigger times: 13
Loss after 67678620 batches: 0.0959
trigger times: 14
Loss after 67809720 batches: 0.0920
trigger times: 15
Loss after 67940820 batches: 0.0886
trigger times: 16
Loss after 68071920 batches: 0.0834
trigger times: 17
Loss after 68203020 batches: 0.0820
trigger times: 18
Loss after 68334120 batches: 0.0791
trigger times: 19
Loss after 68465220 batches: 0.0775
trigger times: 20
Early stopping!
Start to test process.
Loss after 68596320 batches: 0.0751
Time to train on one home:  162.1733763217926
trigger times: 0
Loss after 68727420 batches: 0.8369
trigger times: 1
Loss after 68858520 batches: 0.7039
trigger times: 2
Loss after 68989620 batches: 0.6036
trigger times: 3
Loss after 69120720 batches: 0.5337
trigger times: 4
Loss after 69251820 batches: 0.4639
trigger times: 5
Loss after 69382920 batches: 0.4049
trigger times: 6
Loss after 69514020 batches: 0.3568
trigger times: 7
Loss after 69645120 batches: 0.3157
trigger times: 8
Loss after 69776220 batches: 0.2837
trigger times: 9
Loss after 69907320 batches: 0.2635
trigger times: 10
Loss after 70038420 batches: 0.2369
trigger times: 11
Loss after 70169520 batches: 0.2274
trigger times: 12
Loss after 70300620 batches: 0.2070
trigger times: 13
Loss after 70431720 batches: 0.1879
trigger times: 14
Loss after 70562820 batches: 0.1786
trigger times: 15
Loss after 70693920 batches: 0.1619
trigger times: 16
Loss after 70825020 batches: 0.1580
trigger times: 17
Loss after 70956120 batches: 0.1510
trigger times: 18
Loss after 71087220 batches: 0.1519
trigger times: 19
Loss after 71218320 batches: 0.1291
trigger times: 20
Early stopping!
Start to test process.
Loss after 71349420 batches: 0.1261
Time to train on one home:  161.8390669822693
trigger times: 0
Loss after 71480520 batches: 0.4933
trigger times: 0
Loss after 71611620 batches: 0.2292
trigger times: 1
Loss after 71742720 batches: 0.1769
trigger times: 2
Loss after 71873820 batches: 0.1417
trigger times: 3
Loss after 72004920 batches: 0.1176
trigger times: 4
Loss after 72136020 batches: 0.0992
trigger times: 5
Loss after 72267120 batches: 0.0837
trigger times: 6
Loss after 72398220 batches: 0.0750
trigger times: 7
Loss after 72529320 batches: 0.0680
trigger times: 8
Loss after 72660420 batches: 0.0622
trigger times: 9
Loss after 72791520 batches: 0.0585
trigger times: 10
Loss after 72922620 batches: 0.0536
trigger times: 0
Loss after 73053720 batches: 0.0512
trigger times: 1
Loss after 73184820 batches: 0.0482
trigger times: 2
Loss after 73315920 batches: 0.0454
trigger times: 3
Loss after 73447020 batches: 0.0443
trigger times: 4
Loss after 73578120 batches: 0.0434
trigger times: 5
Loss after 73709220 batches: 0.0413
trigger times: 6
Loss after 73840320 batches: 0.0400
trigger times: 7
Loss after 73971420 batches: 0.0385
trigger times: 8
Loss after 74102520 batches: 0.0371
trigger times: 9
Loss after 74233620 batches: 0.0372
trigger times: 10
Loss after 74364720 batches: 0.0349
trigger times: 11
Loss after 74495820 batches: 0.0341
trigger times: 12
Loss after 74626920 batches: 0.0330
trigger times: 13
Loss after 74758020 batches: 0.0321
trigger times: 14
Loss after 74889120 batches: 0.0312
trigger times: 15
Loss after 75020220 batches: 0.0308
trigger times: 16
Loss after 75151320 batches: 0.0308
trigger times: 17
Loss after 75282420 batches: 0.0293
trigger times: 18
Loss after 75413520 batches: 0.0288
trigger times: 19
Loss after 75544620 batches: 0.0279
trigger times: 20
Early stopping!
Start to test process.
Loss after 75675720 batches: 0.0274
Time to train on one home:  247.83079957962036
trigger times: 0
Loss after 75754320 batches: 0.7795
trigger times: 0
Loss after 75832920 batches: 0.5626
trigger times: 1
Loss after 75911520 batches: 0.4568
trigger times: 2
Loss after 75990120 batches: 0.3775
trigger times: 3
Loss after 76068720 batches: 0.3130
trigger times: 4
Loss after 76147320 batches: 0.2638
trigger times: 5
Loss after 76225920 batches: 0.2259
trigger times: 6
Loss after 76304520 batches: 0.2007
trigger times: 7
Loss after 76383120 batches: 0.1809
trigger times: 8
Loss after 76461720 batches: 0.1636
trigger times: 9
Loss after 76540320 batches: 0.1508
trigger times: 10
Loss after 76618920 batches: 0.1400
trigger times: 11
Loss after 76697520 batches: 0.1309
trigger times: 12
Loss after 76776120 batches: 0.1221
trigger times: 13
Loss after 76854720 batches: 0.1175
trigger times: 14
Loss after 76933320 batches: 0.1119
trigger times: 15
Loss after 77011920 batches: 0.1096
trigger times: 16
Loss after 77090520 batches: 0.1049
trigger times: 17
Loss after 77169120 batches: 0.1022
trigger times: 18
Loss after 77247720 batches: 0.0958
trigger times: 19
Loss after 77326320 batches: 0.0938
trigger times: 20
Early stopping!
Start to test process.
Loss after 77404920 batches: 0.0914
Time to train on one home:  112.87241387367249
trigger times: 0
Loss after 77536020 batches: 0.5174
trigger times: 1
Loss after 77667120 batches: 0.2778
trigger times: 2
Loss after 77798220 batches: 0.2236
trigger times: 3
Loss after 77929320 batches: 0.1842
trigger times: 4
Loss after 78060420 batches: 0.1575
trigger times: 5
Loss after 78191520 batches: 0.1366
trigger times: 6
Loss after 78322620 batches: 0.1213
trigger times: 7
Loss after 78453720 batches: 0.1081
trigger times: 8
Loss after 78584820 batches: 0.0941
trigger times: 9
Loss after 78715920 batches: 0.0860
trigger times: 10
Loss after 78847020 batches: 0.0797
trigger times: 11
Loss after 78978120 batches: 0.0734
trigger times: 12
Loss after 79109220 batches: 0.0690
trigger times: 13
Loss after 79240320 batches: 0.0648
trigger times: 14
Loss after 79371420 batches: 0.0626
trigger times: 15
Loss after 79502520 batches: 0.0592
trigger times: 16
Loss after 79633620 batches: 0.0567
trigger times: 17
Loss after 79764720 batches: 0.0537
trigger times: 18
Loss after 79895820 batches: 0.0530
trigger times: 19
Loss after 80026920 batches: 0.0506
trigger times: 20
Early stopping!
Start to test process.
Loss after 80158020 batches: 0.0499
Time to train on one home:  161.96230959892273
train_results:  [0.06620264916472092, 0.07114762963109807]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446]]
Round_1_results:  [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446]
trigger times: 0
Loss after 80289120 batches: 0.3863
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 642 < 643; dropping {'Training_Loss': 0.3863198071155908, 'Validation_Loss': 0.33353449238671196, 'Training_R2': 0.6120501622010244, 'Validation_R2': 0.6900061085021845, 'Training_F1': 0.6667184739774425, 'Validation_F1': 0.7095893789906158, 'Training_NEP': 0.6666578486238806, 'Validation_NEP': 0.6475198035321056, 'Training_NDE': 0.29124082935598444, 'Validation_NDE': 0.2468627941369046, 'Training_MAE': 22.07838582134174, 'Validation_MAE': 17.757679407955496, 'Training_MSE': 1281.4141, 'Validation_MSE': 911.65753}.
trigger times: 0
Loss after 80420220 batches: 0.1853
trigger times: 0
Loss after 80551320 batches: 0.1376
trigger times: 0
Loss after 80682420 batches: 0.1100
trigger times: 0
Loss after 80813520 batches: 0.0913
trigger times: 0
Loss after 80944620 batches: 0.0794
trigger times: 0
Loss after 81075720 batches: 0.0704
trigger times: 0
Loss after 81206820 batches: 0.0629
trigger times: 1
Loss after 81337920 batches: 0.0587
trigger times: 2
Loss after 81469020 batches: 0.0557
trigger times: 3
Loss after 81600120 batches: 0.0522
trigger times: 4
Loss after 81731220 batches: 0.0489
trigger times: 5
Loss after 81862320 batches: 0.0470
trigger times: 6
Loss after 81993420 batches: 0.0446
trigger times: 0
Loss after 82124520 batches: 0.0425
trigger times: 0
Loss after 82255620 batches: 0.0405
trigger times: 0
Loss after 82386720 batches: 0.0392
trigger times: 0
Loss after 82517820 batches: 0.0385
trigger times: 1
Loss after 82648920 batches: 0.0374
trigger times: 2
Loss after 82780020 batches: 0.0359
trigger times: 3
Loss after 82911120 batches: 0.0349
trigger times: 4
Loss after 83042220 batches: 0.0339
trigger times: 5
Loss after 83173320 batches: 0.0330
trigger times: 0
Loss after 83304420 batches: 0.0328
trigger times: 1
Loss after 83435520 batches: 0.0313
trigger times: 2
Loss after 83566620 batches: 0.0313
trigger times: 3
Loss after 83697720 batches: 0.0302
trigger times: 0
Loss after 83828820 batches: 0.0307
trigger times: 1
Loss after 83959920 batches: 0.0287
trigger times: 2
Loss after 84091020 batches: 0.0281
trigger times: 3
Loss after 84222120 batches: 0.0276
trigger times: 4
Loss after 84353220 batches: 0.0275
trigger times: 5
Loss after 84484320 batches: 0.0264
trigger times: 6
Loss after 84615420 batches: 0.0259
trigger times: 7
Loss after 84746520 batches: 0.0261
trigger times: 8
Loss after 84877620 batches: 0.0255
trigger times: 9
Loss after 85008720 batches: 0.0253
trigger times: 10
Loss after 85139820 batches: 0.0244
trigger times: 11
Loss after 85270920 batches: 0.0240
trigger times: 12
Loss after 85402020 batches: 0.0235
trigger times: 13
Loss after 85533120 batches: 0.0229
trigger times: 14
Loss after 85664220 batches: 0.0226
trigger times: 15
Loss after 85795320 batches: 0.0225
trigger times: 16
Loss after 85926420 batches: 0.0221
trigger times: 0
Loss after 86057520 batches: 0.0223
trigger times: 1
Loss after 86188620 batches: 0.0216
trigger times: 2
Loss after 86319720 batches: 0.0211
trigger times: 3
Loss after 86450820 batches: 0.0208
trigger times: 4
Loss after 86581920 batches: 0.0207
trigger times: 5
Loss after 86713020 batches: 0.0205
trigger times: 6
Loss after 86844120 batches: 0.0200
trigger times: 7
Loss after 86975220 batches: 0.0199
trigger times: 8
Loss after 87106320 batches: 0.0193
trigger times: 9
Loss after 87237420 batches: 0.0190
trigger times: 10
Loss after 87368520 batches: 0.0188
trigger times: 0
Loss after 87499620 batches: 0.0189
trigger times: 1
Loss after 87630720 batches: 0.0189
trigger times: 2
Loss after 87761820 batches: 0.0186
trigger times: 3
Loss after 87892920 batches: 0.0187
trigger times: 4
Loss after 88024020 batches: 0.0180
trigger times: 5
Loss after 88155120 batches: 0.0180
trigger times: 6
Loss after 88286220 batches: 0.0177
trigger times: 7
Loss after 88417320 batches: 0.0172
trigger times: 8
Loss after 88548420 batches: 0.0174
trigger times: 9
Loss after 88679520 batches: 0.0169
trigger times: 10
Loss after 88810620 batches: 0.0172
trigger times: 11
Loss after 88941720 batches: 0.0165
trigger times: 12
Loss after 89072820 batches: 0.0167
trigger times: 13
Loss after 89203920 batches: 0.0163
trigger times: 14
Loss after 89335020 batches: 0.0162
trigger times: 15
Loss after 89466120 batches: 0.0163
trigger times: 16
Loss after 89597220 batches: 0.0158
trigger times: 17
Loss after 89728320 batches: 0.0158
trigger times: 18
Loss after 89859420 batches: 0.0157
trigger times: 19
Loss after 89990520 batches: 0.0158
trigger times: 20
Early stopping!
Start to test process.
Loss after 90121620 batches: 0.0154
Time to train on one home:  557.0447287559509
trigger times: 0
Loss after 90224220 batches: 0.6073
trigger times: 1
Loss after 90326820 batches: 0.4012
trigger times: 2
Loss after 90429420 batches: 0.3241
trigger times: 3
Loss after 90532020 batches: 0.2505
trigger times: 4
Loss after 90634620 batches: 0.2109
trigger times: 5
Loss after 90737220 batches: 0.1874
trigger times: 6
Loss after 90839820 batches: 0.1745
trigger times: 7
Loss after 90942420 batches: 0.1490
trigger times: 8
Loss after 91045020 batches: 0.1295
trigger times: 9
Loss after 91147620 batches: 0.1241
trigger times: 10
Loss after 91250220 batches: 0.1117
trigger times: 11
Loss after 91352820 batches: 0.1108
trigger times: 12
Loss after 91455420 batches: 0.1032
trigger times: 13
Loss after 91558020 batches: 0.1097
trigger times: 14
Loss after 91660620 batches: 0.0989
trigger times: 15
Loss after 91763220 batches: 0.0931
trigger times: 16
Loss after 91865820 batches: 0.0872
trigger times: 17
Loss after 91968420 batches: 0.0858
trigger times: 18
Loss after 92071020 batches: 0.0806
trigger times: 19
Loss after 92173620 batches: 0.0739
trigger times: 20
Early stopping!
Start to test process.
Loss after 92276220 batches: 0.0730
Time to train on one home:  132.59720492362976
trigger times: 0
Loss after 92407320 batches: 0.4332
trigger times: 1
Loss after 92538420 batches: 0.2690
trigger times: 2
Loss after 92669520 batches: 0.1966
trigger times: 3
Loss after 92800620 batches: 0.1548
trigger times: 4
Loss after 92931720 batches: 0.1297
trigger times: 5
Loss after 93062820 batches: 0.1128
trigger times: 6
Loss after 93193920 batches: 0.1023
trigger times: 7
Loss after 93325020 batches: 0.0935
trigger times: 8
Loss after 93456120 batches: 0.0858
trigger times: 9
Loss after 93587220 batches: 0.0795
trigger times: 10
Loss after 93718320 batches: 0.0745
trigger times: 11
Loss after 93849420 batches: 0.0709
trigger times: 12
Loss after 93980520 batches: 0.0668
trigger times: 13
Loss after 94111620 batches: 0.0648
trigger times: 14
Loss after 94242720 batches: 0.0619
trigger times: 15
Loss after 94373820 batches: 0.0601
trigger times: 16
Loss after 94504920 batches: 0.0576
trigger times: 17
Loss after 94636020 batches: 0.0556
trigger times: 18
Loss after 94767120 batches: 0.0538
trigger times: 19
Loss after 94898220 batches: 0.0530
trigger times: 20
Early stopping!
Start to test process.
Loss after 95029320 batches: 0.0511
Time to train on one home:  162.59922981262207
trigger times: 0
Loss after 95160420 batches: 0.6765
trigger times: 1
Loss after 95291520 batches: 0.4390
trigger times: 2
Loss after 95422620 batches: 0.3099
trigger times: 3
Loss after 95553720 batches: 0.2434
trigger times: 4
Loss after 95684820 batches: 0.1988
trigger times: 5
Loss after 95815920 batches: 0.1694
trigger times: 6
Loss after 95947020 batches: 0.1501
trigger times: 7
Loss after 96078120 batches: 0.1359
trigger times: 8
Loss after 96209220 batches: 0.1248
trigger times: 9
Loss after 96340320 batches: 0.1177
trigger times: 10
Loss after 96471420 batches: 0.1094
trigger times: 11
Loss after 96602520 batches: 0.1043
trigger times: 12
Loss after 96733620 batches: 0.0991
trigger times: 13
Loss after 96864720 batches: 0.0953
trigger times: 14
Loss after 96995820 batches: 0.0918
trigger times: 15
Loss after 97126920 batches: 0.0885
trigger times: 16
Loss after 97258020 batches: 0.0861
trigger times: 17
Loss after 97389120 batches: 0.0833
trigger times: 18
Loss after 97520220 batches: 0.0800
trigger times: 19
Loss after 97651320 batches: 0.0780
trigger times: 20
Early stopping!
Start to test process.
Loss after 97782420 batches: 0.0756
Time to train on one home:  162.0959448814392
trigger times: 0
Loss after 97911060 batches: 0.4021
trigger times: 1
Loss after 98039700 batches: 0.2059
trigger times: 2
Loss after 98168340 batches: 0.1447
trigger times: 3
Loss after 98296980 batches: 0.1127
trigger times: 4
Loss after 98425620 batches: 0.0949
trigger times: 5
Loss after 98554260 batches: 0.0816
trigger times: 6
Loss after 98682900 batches: 0.0764
trigger times: 7
Loss after 98811540 batches: 0.0688
trigger times: 8
Loss after 98940180 batches: 0.0627
trigger times: 9
Loss after 99068820 batches: 0.0585
trigger times: 10
Loss after 99197460 batches: 0.0556
trigger times: 11
Loss after 99326100 batches: 0.0528
trigger times: 12
Loss after 99454740 batches: 0.0510
trigger times: 13
Loss after 99583380 batches: 0.0494
trigger times: 14
Loss after 99712020 batches: 0.0468
trigger times: 15
Loss after 99840660 batches: 0.0461
trigger times: 16
Loss after 99969300 batches: 0.0450
trigger times: 17
Loss after 100097940 batches: 0.0429
trigger times: 18
Loss after 100226580 batches: 0.0416
trigger times: 19
Loss after 100355220 batches: 0.0412
trigger times: 20
Early stopping!
Start to test process.
Loss after 100483860 batches: 0.0391
Time to train on one home:  159.3774013519287
trigger times: 0
Loss after 100614960 batches: 0.6941
trigger times: 1
Loss after 100746060 batches: 0.4408
trigger times: 2
Loss after 100877160 batches: 0.2844
trigger times: 3
Loss after 101008260 batches: 0.2019
trigger times: 4
Loss after 101139360 batches: 0.1599
trigger times: 5
Loss after 101270460 batches: 0.1341
trigger times: 6
Loss after 101401560 batches: 0.1204
trigger times: 7
Loss after 101532660 batches: 0.1091
trigger times: 8
Loss after 101663760 batches: 0.1021
trigger times: 9
Loss after 101794860 batches: 0.0956
trigger times: 10
Loss after 101925960 batches: 0.0894
trigger times: 11
Loss after 102057060 batches: 0.0856
trigger times: 12
Loss after 102188160 batches: 0.0808
trigger times: 13
Loss after 102319260 batches: 0.0785
trigger times: 14
Loss after 102450360 batches: 0.0758
trigger times: 15
Loss after 102581460 batches: 0.0722
trigger times: 16
Loss after 102712560 batches: 0.0698
trigger times: 17
Loss after 102843660 batches: 0.0688
trigger times: 18
Loss after 102974760 batches: 0.0662
trigger times: 19
Loss after 103105860 batches: 0.0660
trigger times: 20
Early stopping!
Start to test process.
Loss after 103236960 batches: 0.0635
Time to train on one home:  162.3230836391449
trigger times: 0
Loss after 103368060 batches: 0.7519
trigger times: 1
Loss after 103499160 batches: 0.5699
trigger times: 2
Loss after 103630260 batches: 0.4484
trigger times: 3
Loss after 103761360 batches: 0.3639
trigger times: 4
Loss after 103892460 batches: 0.3098
trigger times: 5
Loss after 104023560 batches: 0.2689
trigger times: 6
Loss after 104154660 batches: 0.2398
trigger times: 7
Loss after 104285760 batches: 0.2075
trigger times: 8
Loss after 104416860 batches: 0.1965
trigger times: 9
Loss after 104547960 batches: 0.1861
trigger times: 10
Loss after 104679060 batches: 0.1632
trigger times: 11
Loss after 104810160 batches: 0.1489
trigger times: 12
Loss after 104941260 batches: 0.1369
trigger times: 13
Loss after 105072360 batches: 0.1291
trigger times: 14
Loss after 105203460 batches: 0.1237
trigger times: 15
Loss after 105334560 batches: 0.1152
trigger times: 16
Loss after 105465660 batches: 0.1079
trigger times: 17
Loss after 105596760 batches: 0.1006
trigger times: 18
Loss after 105727860 batches: 0.0967
trigger times: 19
Loss after 105858960 batches: 0.0935
trigger times: 20
Early stopping!
Start to test process.
Loss after 105990060 batches: 0.0894
Time to train on one home:  161.52126169204712
trigger times: 0
Loss after 106121160 batches: 0.3127
trigger times: 1
Loss after 106252260 batches: 0.1335
trigger times: 2
Loss after 106383360 batches: 0.0881
trigger times: 3
Loss after 106514460 batches: 0.0657
trigger times: 4
Loss after 106645560 batches: 0.0549
trigger times: 5
Loss after 106776660 batches: 0.0487
trigger times: 6
Loss after 106907760 batches: 0.0436
trigger times: 7
Loss after 107038860 batches: 0.0402
trigger times: 8
Loss after 107169960 batches: 0.0377
trigger times: 9
Loss after 107301060 batches: 0.0356
trigger times: 10
Loss after 107432160 batches: 0.0336
trigger times: 11
Loss after 107563260 batches: 0.0318
trigger times: 12
Loss after 107694360 batches: 0.0309
trigger times: 13
Loss after 107825460 batches: 0.0296
trigger times: 14
Loss after 107956560 batches: 0.0287
trigger times: 15
Loss after 108087660 batches: 0.0275
trigger times: 16
Loss after 108218760 batches: 0.0270
trigger times: 17
Loss after 108349860 batches: 0.0260
trigger times: 18
Loss after 108480960 batches: 0.0250
trigger times: 19
Loss after 108612060 batches: 0.0245
trigger times: 20
Early stopping!
Start to test process.
Loss after 108743160 batches: 0.0237
Time to train on one home:  161.6119372844696
trigger times: 0
Loss after 108821760 batches: 0.6823
trigger times: 1
Loss after 108900360 batches: 0.4409
trigger times: 2
Loss after 108978960 batches: 0.3085
trigger times: 3
Loss after 109057560 batches: 0.2344
trigger times: 4
Loss after 109136160 batches: 0.1907
trigger times: 5
Loss after 109214760 batches: 0.1634
trigger times: 6
Loss after 109293360 batches: 0.1475
trigger times: 7
Loss after 109371960 batches: 0.1317
trigger times: 8
Loss after 109450560 batches: 0.1213
trigger times: 9
Loss after 109529160 batches: 0.1143
trigger times: 10
Loss after 109607760 batches: 0.1064
trigger times: 11
Loss after 109686360 batches: 0.1012
trigger times: 12
Loss after 109764960 batches: 0.0978
trigger times: 0
Loss after 109843560 batches: 0.0898
trigger times: 0
Loss after 109922160 batches: 0.0894
trigger times: 1
Loss after 110000760 batches: 0.0836
trigger times: 2
Loss after 110079360 batches: 0.0831
trigger times: 3
Loss after 110157960 batches: 0.0788
trigger times: 4
Loss after 110236560 batches: 0.0752
trigger times: 5
Loss after 110315160 batches: 0.0727
trigger times: 6
Loss after 110393760 batches: 0.0708
trigger times: 7
Loss after 110472360 batches: 0.0712
trigger times: 0
Loss after 110550960 batches: 0.0689
trigger times: 1
Loss after 110629560 batches: 0.0673
trigger times: 2
Loss after 110708160 batches: 0.0659
trigger times: 3
Loss after 110786760 batches: 0.0652
trigger times: 4
Loss after 110865360 batches: 0.0621
trigger times: 5
Loss after 110943960 batches: 0.0624
trigger times: 6
Loss after 111022560 batches: 0.0611
trigger times: 7
Loss after 111101160 batches: 0.0594
trigger times: 8
Loss after 111179760 batches: 0.0583
trigger times: 9
Loss after 111258360 batches: 0.0573
trigger times: 10
Loss after 111336960 batches: 0.0571
trigger times: 11
Loss after 111415560 batches: 0.0554
trigger times: 12
Loss after 111494160 batches: 0.0545
trigger times: 13
Loss after 111572760 batches: 0.0536
trigger times: 14
Loss after 111651360 batches: 0.0517
trigger times: 15
Loss after 111729960 batches: 0.0521
trigger times: 16
Loss after 111808560 batches: 0.0508
trigger times: 17
Loss after 111887160 batches: 0.0518
trigger times: 18
Loss after 111965760 batches: 0.0493
trigger times: 19
Loss after 112044360 batches: 0.0484
trigger times: 20
Early stopping!
Start to test process.
Loss after 112122960 batches: 0.0474
Time to train on one home:  211.72120904922485
trigger times: 0
Loss after 112254060 batches: 0.3151
trigger times: 1
Loss after 112385160 batches: 0.1534
trigger times: 2
Loss after 112516260 batches: 0.0998
trigger times: 3
Loss after 112647360 batches: 0.0753
trigger times: 4
Loss after 112778460 batches: 0.0655
trigger times: 5
Loss after 112909560 batches: 0.0576
trigger times: 6
Loss after 113040660 batches: 0.0525
trigger times: 7
Loss after 113171760 batches: 0.0501
trigger times: 8
Loss after 113302860 batches: 0.0465
trigger times: 9
Loss after 113433960 batches: 0.0430
trigger times: 10
Loss after 113565060 batches: 0.0408
trigger times: 11
Loss after 113696160 batches: 0.0388
trigger times: 12
Loss after 113827260 batches: 0.0385
trigger times: 13
Loss after 113958360 batches: 0.0366
trigger times: 14
Loss after 114089460 batches: 0.0350
trigger times: 15
Loss after 114220560 batches: 0.0340
trigger times: 16
Loss after 114351660 batches: 0.0326
trigger times: 17
Loss after 114482760 batches: 0.0317
trigger times: 18
Loss after 114613860 batches: 0.0308
trigger times: 19
Loss after 114744960 batches: 0.0300
trigger times: 20
Early stopping!
Start to test process.
Loss after 114876060 batches: 0.0293
Time to train on one home:  161.98309206962585
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953]]
Round_2_results:  [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953]
trigger times: 0
Loss after 115007160 batches: 0.3424
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 929 < 930; dropping {'Training_Loss': 0.3423571167689449, 'Validation_Loss': 0.30142124328348374, 'Training_R2': 0.6567018374345461, 'Validation_R2': 0.7202511455733217, 'Training_F1': 0.6922961951362978, 'Validation_F1': 0.7073716753421796, 'Training_NEP': 0.6136580965993632, 'Validation_NEP': 0.6012659784472938, 'Training_NDE': 0.25772002418971557, 'Validation_NDE': 0.22277724095364826, 'Training_MAE': 20.323139144132195, 'Validation_MAE': 16.48920762876456, 'Training_MSE': 1133.9279, 'Validation_MSE': 822.7102}.
trigger times: 0
Loss after 115138260 batches: 0.1521
trigger times: 0
Loss after 115269360 batches: 0.1094
trigger times: 1
Loss after 115400460 batches: 0.0860
trigger times: 0
Loss after 115531560 batches: 0.0714
trigger times: 1
Loss after 115662660 batches: 0.0622
trigger times: 2
Loss after 115793760 batches: 0.0564
trigger times: 0
Loss after 115924860 batches: 0.0519
trigger times: 1
Loss after 116055960 batches: 0.0477
trigger times: 0
Loss after 116187060 batches: 0.0439
trigger times: 1
Loss after 116318160 batches: 0.0416
trigger times: 2
Loss after 116449260 batches: 0.0390
trigger times: 0
Loss after 116580360 batches: 0.0372
trigger times: 1
Loss after 116711460 batches: 0.0357
trigger times: 2
Loss after 116842560 batches: 0.0337
trigger times: 3
Loss after 116973660 batches: 0.0334
trigger times: 4
Loss after 117104760 batches: 0.0312
trigger times: 5
Loss after 117235860 batches: 0.0309
trigger times: 6
Loss after 117366960 batches: 0.0303
trigger times: 7
Loss after 117498060 batches: 0.0292
trigger times: 8
Loss after 117629160 batches: 0.0282
trigger times: 9
Loss after 117760260 batches: 0.0275
trigger times: 10
Loss after 117891360 batches: 0.0272
trigger times: 0
Loss after 118022460 batches: 0.0257
trigger times: 1
Loss after 118153560 batches: 0.0260
trigger times: 0
Loss after 118284660 batches: 0.0244
trigger times: 1
Loss after 118415760 batches: 0.0243
trigger times: 2
Loss after 118546860 batches: 0.0237
trigger times: 3
Loss after 118677960 batches: 0.0234
trigger times: 4
Loss after 118809060 batches: 0.0230
trigger times: 0
Loss after 118940160 batches: 0.0220
trigger times: 0
Loss after 119071260 batches: 0.0220
trigger times: 1
Loss after 119202360 batches: 0.0220
trigger times: 2
Loss after 119333460 batches: 0.0215
trigger times: 3
Loss after 119464560 batches: 0.0208
trigger times: 4
Loss after 119595660 batches: 0.0209
trigger times: 5
Loss after 119726760 batches: 0.0206
trigger times: 0
Loss after 119857860 batches: 0.0200
trigger times: 1
Loss after 119988960 batches: 0.0198
trigger times: 2
Loss after 120120060 batches: 0.0193
trigger times: 3
Loss after 120251160 batches: 0.0188
trigger times: 4
Loss after 120382260 batches: 0.0191
trigger times: 5
Loss after 120513360 batches: 0.0188
trigger times: 6
Loss after 120644460 batches: 0.0182
trigger times: 7
Loss after 120775560 batches: 0.0183
trigger times: 8
Loss after 120906660 batches: 0.0182
trigger times: 9
Loss after 121037760 batches: 0.0181
trigger times: 10
Loss after 121168860 batches: 0.0177
trigger times: 11
Loss after 121299960 batches: 0.0175
trigger times: 12
Loss after 121431060 batches: 0.0171
trigger times: 13
Loss after 121562160 batches: 0.0171
trigger times: 14
Loss after 121693260 batches: 0.0169
trigger times: 15
Loss after 121824360 batches: 0.0166
trigger times: 16
Loss after 121955460 batches: 0.0168
trigger times: 17
Loss after 122086560 batches: 0.0168
trigger times: 18
Loss after 122217660 batches: 0.0161
trigger times: 19
Loss after 122348760 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 122479860 batches: 0.0161
Time to train on one home:  426.9437837600708
trigger times: 0
Loss after 122582460 batches: 0.5368
trigger times: 1
Loss after 122685060 batches: 0.2981
trigger times: 2
Loss after 122787660 batches: 0.2146
trigger times: 3
Loss after 122890260 batches: 0.1656
trigger times: 4
Loss after 122992860 batches: 0.1374
trigger times: 5
Loss after 123095460 batches: 0.1256
trigger times: 6
Loss after 123198060 batches: 0.1229
trigger times: 7
Loss after 123300660 batches: 0.1073
trigger times: 8
Loss after 123403260 batches: 0.0915
trigger times: 9
Loss after 123505860 batches: 0.0833
trigger times: 10
Loss after 123608460 batches: 0.0799
trigger times: 11
Loss after 123711060 batches: 0.0798
trigger times: 12
Loss after 123813660 batches: 0.0696
trigger times: 13
Loss after 123916260 batches: 0.0685
trigger times: 14
Loss after 124018860 batches: 0.0667
trigger times: 15
Loss after 124121460 batches: 0.0613
trigger times: 16
Loss after 124224060 batches: 0.0617
trigger times: 17
Loss after 124326660 batches: 0.0635
trigger times: 18
Loss after 124429260 batches: 0.0583
trigger times: 19
Loss after 124531860 batches: 0.0610
trigger times: 20
Early stopping!
Start to test process.
Loss after 124634460 batches: 0.0542
Time to train on one home:  132.05942034721375
trigger times: 0
Loss after 124765560 batches: 0.3472
trigger times: 1
Loss after 124896660 batches: 0.1657
trigger times: 2
Loss after 125027760 batches: 0.1107
trigger times: 3
Loss after 125158860 batches: 0.0874
trigger times: 0
Loss after 125289960 batches: 0.0749
trigger times: 1
Loss after 125421060 batches: 0.0670
trigger times: 2
Loss after 125552160 batches: 0.0616
trigger times: 3
Loss after 125683260 batches: 0.0561
trigger times: 4
Loss after 125814360 batches: 0.0531
trigger times: 5
Loss after 125945460 batches: 0.0503
trigger times: 6
Loss after 126076560 batches: 0.0473
trigger times: 7
Loss after 126207660 batches: 0.0454
trigger times: 0
Loss after 126338760 batches: 0.0438
trigger times: 1
Loss after 126469860 batches: 0.0416
trigger times: 2
Loss after 126600960 batches: 0.0405
trigger times: 0
Loss after 126732060 batches: 0.0398
trigger times: 1
Loss after 126863160 batches: 0.0380
trigger times: 2
Loss after 126994260 batches: 0.0371
trigger times: 3
Loss after 127125360 batches: 0.0359
trigger times: 4
Loss after 127256460 batches: 0.0353
trigger times: 5
Loss after 127387560 batches: 0.0345
trigger times: 0
Loss after 127518660 batches: 0.0338
trigger times: 1
Loss after 127649760 batches: 0.0333
trigger times: 2
Loss after 127780860 batches: 0.0326
trigger times: 3
Loss after 127911960 batches: 0.0317
trigger times: 4
Loss after 128043060 batches: 0.0314
trigger times: 5
Loss after 128174160 batches: 0.0306
trigger times: 6
Loss after 128305260 batches: 0.0304
trigger times: 7
Loss after 128436360 batches: 0.0299
trigger times: 8
Loss after 128567460 batches: 0.0295
trigger times: 9
Loss after 128698560 batches: 0.0287
trigger times: 10
Loss after 128829660 batches: 0.0284
trigger times: 11
Loss after 128960760 batches: 0.0282
trigger times: 12
Loss after 129091860 batches: 0.0278
trigger times: 13
Loss after 129222960 batches: 0.0273
trigger times: 14
Loss after 129354060 batches: 0.0270
trigger times: 15
Loss after 129485160 batches: 0.0270
trigger times: 16
Loss after 129616260 batches: 0.0266
trigger times: 17
Loss after 129747360 batches: 0.0268
trigger times: 18
Loss after 129878460 batches: 0.0263
trigger times: 19
Loss after 130009560 batches: 0.0258
trigger times: 20
Early stopping!
Start to test process.
Loss after 130140660 batches: 0.0257
Time to train on one home:  312.5420506000519
trigger times: 0
Loss after 130271760 batches: 0.6032
trigger times: 1
Loss after 130402860 batches: 0.3002
trigger times: 2
Loss after 130533960 batches: 0.1842
trigger times: 3
Loss after 130665060 batches: 0.1385
trigger times: 4
Loss after 130796160 batches: 0.1166
trigger times: 5
Loss after 130927260 batches: 0.1033
trigger times: 6
Loss after 131058360 batches: 0.0947
trigger times: 7
Loss after 131189460 batches: 0.0888
trigger times: 8
Loss after 131320560 batches: 0.0824
trigger times: 9
Loss after 131451660 batches: 0.0772
trigger times: 10
Loss after 131582760 batches: 0.0734
trigger times: 11
Loss after 131713860 batches: 0.0701
trigger times: 12
Loss after 131844960 batches: 0.0676
trigger times: 13
Loss after 131976060 batches: 0.0653
trigger times: 14
Loss after 132107160 batches: 0.0631
trigger times: 15
Loss after 132238260 batches: 0.0611
trigger times: 16
Loss after 132369360 batches: 0.0593
trigger times: 17
Loss after 132500460 batches: 0.0583
trigger times: 18
Loss after 132631560 batches: 0.0571
trigger times: 19
Loss after 132762660 batches: 0.0553
trigger times: 20
Early stopping!
Start to test process.
Loss after 132893760 batches: 0.0539
Time to train on one home:  162.3337926864624
trigger times: 0
Loss after 133022400 batches: 0.3065
trigger times: 1
Loss after 133151040 batches: 0.1250
trigger times: 0
Loss after 133279680 batches: 0.0864
trigger times: 0
Loss after 133408320 batches: 0.0697
trigger times: 1
Loss after 133536960 batches: 0.0600
trigger times: 0
Loss after 133665600 batches: 0.0545
trigger times: 0
Loss after 133794240 batches: 0.0497
trigger times: 1
Loss after 133922880 batches: 0.0466
trigger times: 0
Loss after 134051520 batches: 0.0440
trigger times: 1
Loss after 134180160 batches: 0.0412
trigger times: 0
Loss after 134308800 batches: 0.0399
trigger times: 1
Loss after 134437440 batches: 0.0376
trigger times: 2
Loss after 134566080 batches: 0.0372
trigger times: 3
Loss after 134694720 batches: 0.0352
trigger times: 4
Loss after 134823360 batches: 0.0343
trigger times: 5
Loss after 134952000 batches: 0.0334
trigger times: 6
Loss after 135080640 batches: 0.0324
trigger times: 7
Loss after 135209280 batches: 0.0327
trigger times: 8
Loss after 135337920 batches: 0.0313
trigger times: 9
Loss after 135466560 batches: 0.0303
trigger times: 10
Loss after 135595200 batches: 0.0295
trigger times: 0
Loss after 135723840 batches: 0.0292
trigger times: 1
Loss after 135852480 batches: 0.0294
trigger times: 0
Loss after 135981120 batches: 0.0277
trigger times: 1
Loss after 136109760 batches: 0.0276
trigger times: 2
Loss after 136238400 batches: 0.0272
trigger times: 3
Loss after 136367040 batches: 0.0265
trigger times: 4
Loss after 136495680 batches: 0.0263
trigger times: 5
Loss after 136624320 batches: 0.0261
trigger times: 6
Loss after 136752960 batches: 0.0255
trigger times: 0
Loss after 136881600 batches: 0.0245
trigger times: 1
Loss after 137010240 batches: 0.0241
trigger times: 2
Loss after 137138880 batches: 0.0242
trigger times: 3
Loss after 137267520 batches: 0.0237
trigger times: 4
Loss after 137396160 batches: 0.0237
trigger times: 5
Loss after 137524800 batches: 0.0231
trigger times: 6
Loss after 137653440 batches: 0.0232
trigger times: 7
Loss after 137782080 batches: 0.0221
trigger times: 8
Loss after 137910720 batches: 0.0217
trigger times: 9
Loss after 138039360 batches: 0.0220
trigger times: 10
Loss after 138168000 batches: 0.0216
trigger times: 11
Loss after 138296640 batches: 0.0216
trigger times: 12
Loss after 138425280 batches: 0.0212
trigger times: 13
Loss after 138553920 batches: 0.0216
trigger times: 14
Loss after 138682560 batches: 0.0214
trigger times: 15
Loss after 138811200 batches: 0.0210
trigger times: 16
Loss after 138939840 batches: 0.0206
trigger times: 17
Loss after 139068480 batches: 0.0204
trigger times: 18
Loss after 139197120 batches: 0.0201
trigger times: 19
Loss after 139325760 batches: 0.0197
trigger times: 20
Early stopping!
Start to test process.
Loss after 139454400 batches: 0.0196
Time to train on one home:  371.6779594421387
trigger times: 0
Loss after 139585500 batches: 0.6089
trigger times: 1
Loss after 139716600 batches: 0.2757
trigger times: 2
Loss after 139847700 batches: 0.1553
trigger times: 3
Loss after 139978800 batches: 0.1145
trigger times: 4
Loss after 140109900 batches: 0.0960
trigger times: 5
Loss after 140241000 batches: 0.0861
trigger times: 6
Loss after 140372100 batches: 0.0776
trigger times: 7
Loss after 140503200 batches: 0.0723
trigger times: 8
Loss after 140634300 batches: 0.0669
trigger times: 9
Loss after 140765400 batches: 0.0639
trigger times: 10
Loss after 140896500 batches: 0.0612
trigger times: 11
Loss after 141027600 batches: 0.0583
trigger times: 12
Loss after 141158700 batches: 0.0556
trigger times: 13
Loss after 141289800 batches: 0.0540
trigger times: 14
Loss after 141420900 batches: 0.0522
trigger times: 15
Loss after 141552000 batches: 0.0513
trigger times: 16
Loss after 141683100 batches: 0.0500
trigger times: 17
Loss after 141814200 batches: 0.0490
trigger times: 18
Loss after 141945300 batches: 0.0473
trigger times: 19
Loss after 142076400 batches: 0.0463
trigger times: 20
Early stopping!
Start to test process.
Loss after 142207500 batches: 0.0454
Time to train on one home:  161.41226243972778
trigger times: 0
Loss after 142338600 batches: 0.6646
trigger times: 1
Loss after 142469700 batches: 0.4257
trigger times: 2
Loss after 142600800 batches: 0.3064
trigger times: 3
Loss after 142731900 batches: 0.2438
trigger times: 4
Loss after 142863000 batches: 0.1957
trigger times: 5
Loss after 142994100 batches: 0.1639
trigger times: 6
Loss after 143125200 batches: 0.1443
trigger times: 7
Loss after 143256300 batches: 0.1254
trigger times: 8
Loss after 143387400 batches: 0.1176
trigger times: 9
Loss after 143518500 batches: 0.1065
trigger times: 10
Loss after 143649600 batches: 0.0995
trigger times: 11
Loss after 143780700 batches: 0.0893
trigger times: 12
Loss after 143911800 batches: 0.0846
trigger times: 13
Loss after 144042900 batches: 0.0854
trigger times: 14
Loss after 144174000 batches: 0.0768
trigger times: 15
Loss after 144305100 batches: 0.0745
trigger times: 16
Loss after 144436200 batches: 0.0691
trigger times: 17
Loss after 144567300 batches: 0.0670
trigger times: 18
Loss after 144698400 batches: 0.0677
trigger times: 19
Loss after 144829500 batches: 0.0629
trigger times: 20
Early stopping!
Start to test process.
Loss after 144960600 batches: 0.0610
Time to train on one home:  161.99457025527954
trigger times: 0
Loss after 145091700 batches: 0.2177
trigger times: 0
Loss after 145222800 batches: 0.0781
trigger times: 1
Loss after 145353900 batches: 0.0531
trigger times: 2
Loss after 145485000 batches: 0.0447
trigger times: 0
Loss after 145616100 batches: 0.0383
trigger times: 1
Loss after 145747200 batches: 0.0339
trigger times: 0
Loss after 145878300 batches: 0.0311
trigger times: 1
Loss after 146009400 batches: 0.0299
trigger times: 2
Loss after 146140500 batches: 0.0280
trigger times: 3
Loss after 146271600 batches: 0.0271
trigger times: 4
Loss after 146402700 batches: 0.0255
trigger times: 5
Loss after 146533800 batches: 0.0246
trigger times: 6
Loss after 146664900 batches: 0.0238
trigger times: 0
Loss after 146796000 batches: 0.0228
trigger times: 1
Loss after 146927100 batches: 0.0222
trigger times: 2
Loss after 147058200 batches: 0.0220
trigger times: 3
Loss after 147189300 batches: 0.0214
trigger times: 4
Loss after 147320400 batches: 0.0205
trigger times: 5
Loss after 147451500 batches: 0.0200
trigger times: 6
Loss after 147582600 batches: 0.0196
trigger times: 7
Loss after 147713700 batches: 0.0196
trigger times: 8
Loss after 147844800 batches: 0.0186
trigger times: 9
Loss after 147975900 batches: 0.0178
trigger times: 10
Loss after 148107000 batches: 0.0175
trigger times: 11
Loss after 148238100 batches: 0.0177
trigger times: 12
Loss after 148369200 batches: 0.0172
trigger times: 13
Loss after 148500300 batches: 0.0167
trigger times: 14
Loss after 148631400 batches: 0.0169
trigger times: 15
Loss after 148762500 batches: 0.0166
trigger times: 16
Loss after 148893600 batches: 0.0162
trigger times: 17
Loss after 149024700 batches: 0.0161
trigger times: 18
Loss after 149155800 batches: 0.0160
trigger times: 19
Loss after 149286900 batches: 0.0156
trigger times: 20
Early stopping!
Start to test process.
Loss after 149418000 batches: 0.0151
Time to train on one home:  255.95008730888367
trigger times: 0
Loss after 149496600 batches: 0.5983
trigger times: 1
Loss after 149575200 batches: 0.2794
trigger times: 2
Loss after 149653800 batches: 0.1671
trigger times: 3
Loss after 149732400 batches: 0.1223
trigger times: 4
Loss after 149811000 batches: 0.1026
trigger times: 5
Loss after 149889600 batches: 0.0879
trigger times: 6
Loss after 149968200 batches: 0.0789
trigger times: 7
Loss after 150046800 batches: 0.0740
trigger times: 8
Loss after 150125400 batches: 0.0668
trigger times: 9
Loss after 150204000 batches: 0.0628
trigger times: 10
Loss after 150282600 batches: 0.0607
trigger times: 11
Loss after 150361200 batches: 0.0583
trigger times: 12
Loss after 150439800 batches: 0.0581
trigger times: 13
Loss after 150518400 batches: 0.0552
trigger times: 14
Loss after 150597000 batches: 0.0537
trigger times: 15
Loss after 150675600 batches: 0.0520
trigger times: 16
Loss after 150754200 batches: 0.0500
trigger times: 17
Loss after 150832800 batches: 0.0481
trigger times: 18
Loss after 150911400 batches: 0.0475
trigger times: 19
Loss after 150990000 batches: 0.0456
trigger times: 20
Early stopping!
Start to test process.
Loss after 151068600 batches: 0.0447
Time to train on one home:  108.78809785842896
trigger times: 0
Loss after 151199700 batches: 0.2457
trigger times: 0
Loss after 151330800 batches: 0.0951
trigger times: 0
Loss after 151461900 batches: 0.0626
trigger times: 1
Loss after 151593000 batches: 0.0505
trigger times: 0
Loss after 151724100 batches: 0.0445
trigger times: 1
Loss after 151855200 batches: 0.0404
trigger times: 2
Loss after 151986300 batches: 0.0365
trigger times: 3
Loss after 152117400 batches: 0.0338
trigger times: 4
Loss after 152248500 batches: 0.0320
trigger times: 5
Loss after 152379600 batches: 0.0312
trigger times: 6
Loss after 152510700 batches: 0.0298
trigger times: 7
Loss after 152641800 batches: 0.0283
trigger times: 8
Loss after 152772900 batches: 0.0278
trigger times: 9
Loss after 152904000 batches: 0.0262
trigger times: 10
Loss after 153035100 batches: 0.0258
trigger times: 0
Loss after 153166200 batches: 0.0250
trigger times: 1
Loss after 153297300 batches: 0.0246
trigger times: 0
Loss after 153428400 batches: 0.0238
trigger times: 1
Loss after 153559500 batches: 0.0232
trigger times: 2
Loss after 153690600 batches: 0.0224
trigger times: 3
Loss after 153821700 batches: 0.0223
trigger times: 4
Loss after 153952800 batches: 0.0218
trigger times: 5
Loss after 154083900 batches: 0.0216
trigger times: 6
Loss after 154215000 batches: 0.0212
trigger times: 7
Loss after 154346100 batches: 0.0210
trigger times: 8
Loss after 154477200 batches: 0.0204
trigger times: 9
Loss after 154608300 batches: 0.0201
trigger times: 10
Loss after 154739400 batches: 0.0199
trigger times: 11
Loss after 154870500 batches: 0.0194
trigger times: 12
Loss after 155001600 batches: 0.0189
trigger times: 13
Loss after 155132700 batches: 0.0187
trigger times: 14
Loss after 155263800 batches: 0.0189
trigger times: 15
Loss after 155394900 batches: 0.0184
trigger times: 16
Loss after 155526000 batches: 0.0181
trigger times: 17
Loss after 155657100 batches: 0.0179
trigger times: 18
Loss after 155788200 batches: 0.0178
trigger times: 19
Loss after 155919300 batches: 0.0175
trigger times: 20
Early stopping!
Start to test process.
Loss after 156050400 batches: 0.0176
Time to train on one home:  283.2851231098175
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251]]
Round_3_results:  [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251]
trigger times: 0
Loss after 156181500 batches: 0.1943
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1257 < 1258; dropping {'Training_Loss': 0.19433000894649974, 'Validation_Loss': 0.22146943045987022, 'Training_R2': 0.8046012096303174, 'Validation_R2': 0.7938709109408046, 'Training_F1': 0.7638327979977236, 'Validation_F1': 0.7516646707496462, 'Training_NEP': 0.4716419515007909, 'Validation_NEP': 0.5213380317379396, 'Training_NDE': 0.14668934026442537, 'Validation_NDE': 0.16415034061536135, 'Training_MAE': 15.619846066853912, 'Validation_MAE': 14.297251729255937, 'Training_MSE': 645.4101, 'Validation_MSE': 606.20264}.
trigger times: 1
Loss after 156312600 batches: 0.0727
trigger times: 0
Loss after 156443700 batches: 0.0494
trigger times: 1
Loss after 156574800 batches: 0.0410
trigger times: 2
Loss after 156705900 batches: 0.0359
trigger times: 3
Loss after 156837000 batches: 0.0322
trigger times: 0
Loss after 156968100 batches: 0.0293
trigger times: 1
Loss after 157099200 batches: 0.0279
trigger times: 0
Loss after 157230300 batches: 0.0260
trigger times: 1
Loss after 157361400 batches: 0.0249
trigger times: 2
Loss after 157492500 batches: 0.0242
trigger times: 3
Loss after 157623600 batches: 0.0228
trigger times: 4
Loss after 157754700 batches: 0.0220
trigger times: 5
Loss after 157885800 batches: 0.0218
trigger times: 6
Loss after 158016900 batches: 0.0211
trigger times: 7
Loss after 158148000 batches: 0.0201
trigger times: 8
Loss after 158279100 batches: 0.0197
trigger times: 9
Loss after 158410200 batches: 0.0191
trigger times: 10
Loss after 158541300 batches: 0.0188
trigger times: 11
Loss after 158672400 batches: 0.0187
trigger times: 12
Loss after 158803500 batches: 0.0183
trigger times: 13
Loss after 158934600 batches: 0.0179
trigger times: 14
Loss after 159065700 batches: 0.0176
trigger times: 15
Loss after 159196800 batches: 0.0171
trigger times: 0
Loss after 159327900 batches: 0.0168
trigger times: 1
Loss after 159459000 batches: 0.0169
trigger times: 2
Loss after 159590100 batches: 0.0165
trigger times: 3
Loss after 159721200 batches: 0.0163
trigger times: 4
Loss after 159852300 batches: 0.0166
trigger times: 5
Loss after 159983400 batches: 0.0166
trigger times: 6
Loss after 160114500 batches: 0.0159
trigger times: 7
Loss after 160245600 batches: 0.0153
trigger times: 8
Loss after 160376700 batches: 0.0151
trigger times: 9
Loss after 160507800 batches: 0.0150
trigger times: 10
Loss after 160638900 batches: 0.0149
trigger times: 11
Loss after 160770000 batches: 0.0148
trigger times: 0
Loss after 160901100 batches: 0.0146
trigger times: 1
Loss after 161032200 batches: 0.0143
trigger times: 2
Loss after 161163300 batches: 0.0143
trigger times: 3
Loss after 161294400 batches: 0.0143
trigger times: 4
Loss after 161425500 batches: 0.0140
trigger times: 0
Loss after 161556600 batches: 0.0142
trigger times: 0
Loss after 161687700 batches: 0.0137
trigger times: 1
Loss after 161818800 batches: 0.0135
trigger times: 2
Loss after 161949900 batches: 0.0133
trigger times: 3
Loss after 162081000 batches: 0.0135
trigger times: 4
Loss after 162212100 batches: 0.0135
trigger times: 5
Loss after 162343200 batches: 0.0133
trigger times: 6
Loss after 162474300 batches: 0.0134
trigger times: 7
Loss after 162605400 batches: 0.0131
trigger times: 8
Loss after 162736500 batches: 0.0129
trigger times: 9
Loss after 162867600 batches: 0.0128
trigger times: 10
Loss after 162998700 batches: 0.0131
trigger times: 11
Loss after 163129800 batches: 0.0125
trigger times: 12
Loss after 163260900 batches: 0.0123
trigger times: 13
Loss after 163392000 batches: 0.0127
trigger times: 14
Loss after 163523100 batches: 0.0123
trigger times: 15
Loss after 163654200 batches: 0.0124
trigger times: 16
Loss after 163785300 batches: 0.0120
trigger times: 17
Loss after 163916400 batches: 0.0118
trigger times: 18
Loss after 164047500 batches: 0.0119
trigger times: 19
Loss after 164178600 batches: 0.0119
trigger times: 20
Early stopping!
Start to test process.
Loss after 164309700 batches: 0.0119
Time to train on one home:  463.42061257362366
trigger times: 0
Loss after 164412300 batches: 0.4804
trigger times: 1
Loss after 164514900 batches: 0.2226
trigger times: 2
Loss after 164617500 batches: 0.1620
trigger times: 3
Loss after 164720100 batches: 0.1191
trigger times: 4
Loss after 164822700 batches: 0.1080
trigger times: 5
Loss after 164925300 batches: 0.0873
trigger times: 6
Loss after 165027900 batches: 0.0757
trigger times: 7
Loss after 165130500 batches: 0.0686
trigger times: 8
Loss after 165233100 batches: 0.0641
trigger times: 9
Loss after 165335700 batches: 0.0615
trigger times: 10
Loss after 165438300 batches: 0.0594
trigger times: 11
Loss after 165540900 batches: 0.0545
trigger times: 12
Loss after 165643500 batches: 0.0588
trigger times: 13
Loss after 165746100 batches: 0.0537
trigger times: 14
Loss after 165848700 batches: 0.0477
trigger times: 15
Loss after 165951300 batches: 0.0483
trigger times: 16
Loss after 166053900 batches: 0.0476
trigger times: 17
Loss after 166156500 batches: 0.0435
trigger times: 18
Loss after 166259100 batches: 0.0425
trigger times: 19
Loss after 166361700 batches: 0.0402
trigger times: 20
Early stopping!
Start to test process.
Loss after 166464300 batches: 0.0399
Time to train on one home:  132.98760294914246
trigger times: 0
Loss after 166595400 batches: 0.2793
trigger times: 0
Loss after 166726500 batches: 0.1120
trigger times: 1
Loss after 166857600 batches: 0.0763
trigger times: 0
Loss after 166988700 batches: 0.0621
trigger times: 1
Loss after 167119800 batches: 0.0532
trigger times: 2
Loss after 167250900 batches: 0.0484
trigger times: 3
Loss after 167382000 batches: 0.0448
trigger times: 0
Loss after 167513100 batches: 0.0424
trigger times: 1
Loss after 167644200 batches: 0.0396
trigger times: 2
Loss after 167775300 batches: 0.0379
trigger times: 3
Loss after 167906400 batches: 0.0359
trigger times: 4
Loss after 168037500 batches: 0.0350
trigger times: 5
Loss after 168168600 batches: 0.0339
trigger times: 6
Loss after 168299700 batches: 0.0325
trigger times: 7
Loss after 168430800 batches: 0.0315
trigger times: 8
Loss after 168561900 batches: 0.0304
trigger times: 9
Loss after 168693000 batches: 0.0299
trigger times: 10
Loss after 168824100 batches: 0.0291
trigger times: 11
Loss after 168955200 batches: 0.0285
trigger times: 12
Loss after 169086300 batches: 0.0277
trigger times: 13
Loss after 169217400 batches: 0.0277
trigger times: 14
Loss after 169348500 batches: 0.0269
trigger times: 15
Loss after 169479600 batches: 0.0267
trigger times: 16
Loss after 169610700 batches: 0.0262
trigger times: 17
Loss after 169741800 batches: 0.0257
trigger times: 18
Loss after 169872900 batches: 0.0255
trigger times: 19
Loss after 170004000 batches: 0.0252
trigger times: 20
Early stopping!
Start to test process.
Loss after 170135100 batches: 0.0250
Time to train on one home:  213.78928017616272
trigger times: 0
Loss after 170266200 batches: 0.5341
trigger times: 1
Loss after 170397300 batches: 0.2116
trigger times: 2
Loss after 170528400 batches: 0.1354
trigger times: 3
Loss after 170659500 batches: 0.1082
trigger times: 4
Loss after 170790600 batches: 0.0939
trigger times: 5
Loss after 170921700 batches: 0.0829
trigger times: 6
Loss after 171052800 batches: 0.0765
trigger times: 7
Loss after 171183900 batches: 0.0724
trigger times: 8
Loss after 171315000 batches: 0.0674
trigger times: 9
Loss after 171446100 batches: 0.0638
trigger times: 10
Loss after 171577200 batches: 0.0613
trigger times: 11
Loss after 171708300 batches: 0.0588
trigger times: 12
Loss after 171839400 batches: 0.0557
trigger times: 13
Loss after 171970500 batches: 0.0544
trigger times: 14
Loss after 172101600 batches: 0.0528
trigger times: 15
Loss after 172232700 batches: 0.0512
trigger times: 16
Loss after 172363800 batches: 0.0492
trigger times: 17
Loss after 172494900 batches: 0.0484
trigger times: 18
Loss after 172626000 batches: 0.0476
trigger times: 19
Loss after 172757100 batches: 0.0470
trigger times: 20
Early stopping!
Start to test process.
Loss after 172888200 batches: 0.0455
Time to train on one home:  162.86827778816223
trigger times: 0
Loss after 173016840 batches: 0.2395
trigger times: 0
Loss after 173145480 batches: 0.0803
trigger times: 0
Loss after 173274120 batches: 0.0550
trigger times: 1
Loss after 173402760 batches: 0.0451
trigger times: 0
Loss after 173531400 batches: 0.0399
trigger times: 1
Loss after 173660040 batches: 0.0371
trigger times: 2
Loss after 173788680 batches: 0.0343
trigger times: 3
Loss after 173917320 batches: 0.0320
trigger times: 4
Loss after 174045960 batches: 0.0305
trigger times: 5
Loss after 174174600 batches: 0.0287
trigger times: 6
Loss after 174303240 batches: 0.0277
trigger times: 7
Loss after 174431880 batches: 0.0269
trigger times: 8
Loss after 174560520 batches: 0.0264
trigger times: 0
Loss after 174689160 batches: 0.0255
trigger times: 1
Loss after 174817800 batches: 0.0243
trigger times: 2
Loss after 174946440 batches: 0.0243
trigger times: 3
Loss after 175075080 batches: 0.0234
trigger times: 4
Loss after 175203720 batches: 0.0225
trigger times: 0
Loss after 175332360 batches: 0.0225
trigger times: 1
Loss after 175461000 batches: 0.0220
trigger times: 2
Loss after 175589640 batches: 0.0213
trigger times: 3
Loss after 175718280 batches: 0.0208
trigger times: 4
Loss after 175846920 batches: 0.0204
trigger times: 5
Loss after 175975560 batches: 0.0206
trigger times: 6
Loss after 176104200 batches: 0.0202
trigger times: 7
Loss after 176232840 batches: 0.0203
trigger times: 8
Loss after 176361480 batches: 0.0198
trigger times: 9
Loss after 176490120 batches: 0.0195
trigger times: 10
Loss after 176618760 batches: 0.0192
trigger times: 0
Loss after 176747400 batches: 0.0187
trigger times: 1
Loss after 176876040 batches: 0.0186
trigger times: 0
Loss after 177004680 batches: 0.0185
trigger times: 0
Loss after 177133320 batches: 0.0183
trigger times: 0
Loss after 177261960 batches: 0.0179
trigger times: 1
Loss after 177390600 batches: 0.0176
trigger times: 2
Loss after 177519240 batches: 0.0179
trigger times: 3
Loss after 177647880 batches: 0.0178
trigger times: 4
Loss after 177776520 batches: 0.0179
trigger times: 0
Loss after 177905160 batches: 0.0176
trigger times: 1
Loss after 178033800 batches: 0.0171
trigger times: 2
Loss after 178162440 batches: 0.0171
trigger times: 3
Loss after 178291080 batches: 0.0166
trigger times: 4
Loss after 178419720 batches: 0.0164
trigger times: 5
Loss after 178548360 batches: 0.0161
trigger times: 6
Loss after 178677000 batches: 0.0166
trigger times: 7
Loss after 178805640 batches: 0.0162
trigger times: 8
Loss after 178934280 batches: 0.0159
trigger times: 9
Loss after 179062920 batches: 0.0157
trigger times: 10
Loss after 179191560 batches: 0.0160
trigger times: 11
Loss after 179320200 batches: 0.0157
trigger times: 12
Loss after 179448840 batches: 0.0154
trigger times: 13
Loss after 179577480 batches: 0.0152
trigger times: 14
Loss after 179706120 batches: 0.0155
trigger times: 15
Loss after 179834760 batches: 0.0152
trigger times: 16
Loss after 179963400 batches: 0.0151
trigger times: 17
Loss after 180092040 batches: 0.0153
trigger times: 18
Loss after 180220680 batches: 0.0151
trigger times: 19
Loss after 180349320 batches: 0.0149
trigger times: 20
Early stopping!
Start to test process.
Loss after 180477960 batches: 0.0147
Time to train on one home:  426.7659435272217
trigger times: 0
Loss after 180609060 batches: 0.5152
trigger times: 1
Loss after 180740160 batches: 0.1860
trigger times: 2
Loss after 180871260 batches: 0.1135
trigger times: 3
Loss after 181002360 batches: 0.0886
trigger times: 4
Loss after 181133460 batches: 0.0769
trigger times: 5
Loss after 181264560 batches: 0.0696
trigger times: 6
Loss after 181395660 batches: 0.0631
trigger times: 7
Loss after 181526760 batches: 0.0595
trigger times: 8
Loss after 181657860 batches: 0.0562
trigger times: 9
Loss after 181788960 batches: 0.0536
trigger times: 10
Loss after 181920060 batches: 0.0509
trigger times: 11
Loss after 182051160 batches: 0.0490
trigger times: 12
Loss after 182182260 batches: 0.0470
trigger times: 13
Loss after 182313360 batches: 0.0461
trigger times: 14
Loss after 182444460 batches: 0.0443
trigger times: 15
Loss after 182575560 batches: 0.0430
trigger times: 16
Loss after 182706660 batches: 0.0423
trigger times: 17
Loss after 182837760 batches: 0.0411
trigger times: 18
Loss after 182968860 batches: 0.0400
trigger times: 19
Loss after 183099960 batches: 0.0396
trigger times: 20
Early stopping!
Start to test process.
Loss after 183231060 batches: 0.0392
Time to train on one home:  162.04052734375
trigger times: 0
Loss after 183362160 batches: 0.6157
trigger times: 1
Loss after 183493260 batches: 0.3336
trigger times: 2
Loss after 183624360 batches: 0.2296
trigger times: 3
Loss after 183755460 batches: 0.1673
trigger times: 0
Loss after 183886560 batches: 0.1362
trigger times: 1
Loss after 184017660 batches: 0.1158
trigger times: 2
Loss after 184148760 batches: 0.1010
trigger times: 0
Loss after 184279860 batches: 0.0906
trigger times: 0
Loss after 184410960 batches: 0.0855
trigger times: 1
Loss after 184542060 batches: 0.0764
trigger times: 0
Loss after 184673160 batches: 0.0731
trigger times: 0
Loss after 184804260 batches: 0.0651
trigger times: 1
Loss after 184935360 batches: 0.0623
trigger times: 2
Loss after 185066460 batches: 0.0597
trigger times: 0
Loss after 185197560 batches: 0.0584
trigger times: 0
Loss after 185328660 batches: 0.0551
trigger times: 1
Loss after 185459760 batches: 0.0550
trigger times: 0
Loss after 185590860 batches: 0.0537
trigger times: 0
Loss after 185721960 batches: 0.0508
trigger times: 1
Loss after 185853060 batches: 0.0497
trigger times: 2
Loss after 185984160 batches: 0.0513
trigger times: 3
Loss after 186115260 batches: 0.0483
trigger times: 4
Loss after 186246360 batches: 0.0478
trigger times: 5
Loss after 186377460 batches: 0.0470
trigger times: 6
Loss after 186508560 batches: 0.0461
trigger times: 0
Loss after 186639660 batches: 0.0455
trigger times: 1
Loss after 186770760 batches: 0.0447
trigger times: 2
Loss after 186901860 batches: 0.0432
trigger times: 3
Loss after 187032960 batches: 0.0433
trigger times: 0
Loss after 187164060 batches: 0.0440
trigger times: 1
Loss after 187295160 batches: 0.0446
trigger times: 2
Loss after 187426260 batches: 0.0418
trigger times: 3
Loss after 187557360 batches: 0.0418
trigger times: 0
Loss after 187688460 batches: 0.0420
trigger times: 1
Loss after 187819560 batches: 0.0402
trigger times: 2
Loss after 187950660 batches: 0.0391
trigger times: 3
Loss after 188081760 batches: 0.0392
trigger times: 4
Loss after 188212860 batches: 0.0386
trigger times: 5
Loss after 188343960 batches: 0.0406
trigger times: 6
Loss after 188475060 batches: 0.0401
trigger times: 0
Loss after 188606160 batches: 0.0380
trigger times: 1
Loss after 188737260 batches: 0.0376
trigger times: 2
Loss after 188868360 batches: 0.0374
trigger times: 3
Loss after 188999460 batches: 0.0362
trigger times: 4
Loss after 189130560 batches: 0.0350
trigger times: 5
Loss after 189261660 batches: 0.0352
trigger times: 6
Loss after 189392760 batches: 0.0353
trigger times: 7
Loss after 189523860 batches: 0.0349
trigger times: 8
Loss after 189654960 batches: 0.0345
trigger times: 9
Loss after 189786060 batches: 0.0347
trigger times: 10
Loss after 189917160 batches: 0.0356
trigger times: 11
Loss after 190048260 batches: 0.0355
trigger times: 0
Loss after 190179360 batches: 0.0345
trigger times: 1
Loss after 190310460 batches: 0.0337
trigger times: 0
Loss after 190441560 batches: 0.0333
trigger times: 1
Loss after 190572660 batches: 0.0334
trigger times: 0
Loss after 190703760 batches: 0.0344
trigger times: 1
Loss after 190834860 batches: 0.0347
trigger times: 2
Loss after 190965960 batches: 0.0318
trigger times: 3
Loss after 191097060 batches: 0.0318
trigger times: 4
Loss after 191228160 batches: 0.0315
trigger times: 5
Loss after 191359260 batches: 0.0317
trigger times: 6
Loss after 191490360 batches: 0.0309
trigger times: 7
Loss after 191621460 batches: 0.0331
trigger times: 8
Loss after 191752560 batches: 0.0328
trigger times: 9
Loss after 191883660 batches: 0.0319
trigger times: 10
Loss after 192014760 batches: 0.0311
trigger times: 11
Loss after 192145860 batches: 0.0311
trigger times: 12
Loss after 192276960 batches: 0.0302
trigger times: 13
Loss after 192408060 batches: 0.0314
trigger times: 14
Loss after 192539160 batches: 0.0308
trigger times: 15
Loss after 192670260 batches: 0.0314
trigger times: 16
Loss after 192801360 batches: 0.0285
trigger times: 17
Loss after 192932460 batches: 0.0295
trigger times: 18
Loss after 193063560 batches: 0.0294
trigger times: 19
Loss after 193194660 batches: 0.0297
trigger times: 20
Early stopping!
Start to test process.
Loss after 193325760 batches: 0.0297
Time to train on one home:  563.3716490268707
trigger times: 0
Loss after 193456860 batches: 0.1569
trigger times: 0
Loss after 193587960 batches: 0.0542
trigger times: 0
Loss after 193719060 batches: 0.0387
trigger times: 1
Loss after 193850160 batches: 0.0322
trigger times: 0
Loss after 193981260 batches: 0.0287
trigger times: 0
Loss after 194112360 batches: 0.0263
trigger times: 1
Loss after 194243460 batches: 0.0245
trigger times: 2
Loss after 194374560 batches: 0.0231
trigger times: 3
Loss after 194505660 batches: 0.0219
trigger times: 0
Loss after 194636760 batches: 0.0211
trigger times: 1
Loss after 194767860 batches: 0.0205
trigger times: 0
Loss after 194898960 batches: 0.0197
trigger times: 1
Loss after 195030060 batches: 0.0184
trigger times: 2
Loss after 195161160 batches: 0.0182
trigger times: 3
Loss after 195292260 batches: 0.0177
trigger times: 4
Loss after 195423360 batches: 0.0175
trigger times: 5
Loss after 195554460 batches: 0.0169
trigger times: 6
Loss after 195685560 batches: 0.0166
trigger times: 7
Loss after 195816660 batches: 0.0165
trigger times: 0
Loss after 195947760 batches: 0.0156
trigger times: 1
Loss after 196078860 batches: 0.0153
trigger times: 0
Loss after 196209960 batches: 0.0150
trigger times: 1
Loss after 196341060 batches: 0.0151
trigger times: 2
Loss after 196472160 batches: 0.0147
trigger times: 3
Loss after 196603260 batches: 0.0146
trigger times: 4
Loss after 196734360 batches: 0.0142
trigger times: 0
Loss after 196865460 batches: 0.0142
trigger times: 1
Loss after 196996560 batches: 0.0140
trigger times: 2
Loss after 197127660 batches: 0.0138
trigger times: 3
Loss after 197258760 batches: 0.0136
trigger times: 4
Loss after 197389860 batches: 0.0137
trigger times: 5
Loss after 197520960 batches: 0.0132
trigger times: 6
Loss after 197652060 batches: 0.0132
trigger times: 7
Loss after 197783160 batches: 0.0131
trigger times: 8
Loss after 197914260 batches: 0.0129
trigger times: 9
Loss after 198045360 batches: 0.0126
trigger times: 10
Loss after 198176460 batches: 0.0125
trigger times: 11
Loss after 198307560 batches: 0.0125
trigger times: 12
Loss after 198438660 batches: 0.0123
trigger times: 13
Loss after 198569760 batches: 0.0122
trigger times: 0
Loss after 198700860 batches: 0.0119
trigger times: 1
Loss after 198831960 batches: 0.0121
trigger times: 2
Loss after 198963060 batches: 0.0119
trigger times: 0
Loss after 199094160 batches: 0.0118
trigger times: 1
Loss after 199225260 batches: 0.0118
trigger times: 2
Loss after 199356360 batches: 0.0116
trigger times: 3
Loss after 199487460 batches: 0.0115
trigger times: 4
Loss after 199618560 batches: 0.0112
trigger times: 5
Loss after 199749660 batches: 0.0112
trigger times: 0
Loss after 199880760 batches: 0.0114
trigger times: 0
Loss after 200011860 batches: 0.0112
trigger times: 1
Loss after 200142960 batches: 0.0110
trigger times: 0
Loss after 200274060 batches: 0.0109
trigger times: 1
Loss after 200405160 batches: 0.0109
trigger times: 2
Loss after 200536260 batches: 0.0108
trigger times: 3
Loss after 200667360 batches: 0.0107
trigger times: 4
Loss after 200798460 batches: 0.0105
trigger times: 5
Loss after 200929560 batches: 0.0106
trigger times: 6
Loss after 201060660 batches: 0.0104
trigger times: 7
Loss after 201191760 batches: 0.0105
trigger times: 8
Loss after 201322860 batches: 0.0104
trigger times: 9
Loss after 201453960 batches: 0.0102
trigger times: 10
Loss after 201585060 batches: 0.0100
trigger times: 11
Loss after 201716160 batches: 0.0102
trigger times: 12
Loss after 201847260 batches: 0.0099
trigger times: 13
Loss after 201978360 batches: 0.0099
trigger times: 14
Loss after 202109460 batches: 0.0100
trigger times: 15
Loss after 202240560 batches: 0.0098
trigger times: 16
Loss after 202371660 batches: 0.0099
trigger times: 17
Loss after 202502760 batches: 0.0098
trigger times: 18
Loss after 202633860 batches: 0.0098
trigger times: 19
Loss after 202764960 batches: 0.0097
trigger times: 20
Early stopping!
Start to test process.
Loss after 202896060 batches: 0.0096
Time to train on one home:  533.59419465065
trigger times: 0
Loss after 202974660 batches: 0.5137
trigger times: 1
Loss after 203053260 batches: 0.1932
trigger times: 2
Loss after 203131860 batches: 0.1110
trigger times: 3
Loss after 203210460 batches: 0.0859
trigger times: 4
Loss after 203289060 batches: 0.0716
trigger times: 5
Loss after 203367660 batches: 0.0628
trigger times: 6
Loss after 203446260 batches: 0.0583
trigger times: 7
Loss after 203524860 batches: 0.0547
trigger times: 8
Loss after 203603460 batches: 0.0533
trigger times: 9
Loss after 203682060 batches: 0.0497
trigger times: 10
Loss after 203760660 batches: 0.0466
trigger times: 11
Loss after 203839260 batches: 0.0448
trigger times: 12
Loss after 203917860 batches: 0.0434
trigger times: 13
Loss after 203996460 batches: 0.0420
trigger times: 14
Loss after 204075060 batches: 0.0410
trigger times: 15
Loss after 204153660 batches: 0.0396
trigger times: 16
Loss after 204232260 batches: 0.0383
trigger times: 17
Loss after 204310860 batches: 0.0377
trigger times: 18
Loss after 204389460 batches: 0.0368
trigger times: 19
Loss after 204468060 batches: 0.0363
trigger times: 20
Early stopping!
Start to test process.
Loss after 204546660 batches: 0.0356
Time to train on one home:  108.89857125282288
trigger times: 0
Loss after 204677760 batches: 0.1817
trigger times: 1
Loss after 204808860 batches: 0.0606
trigger times: 2
Loss after 204939960 batches: 0.0427
trigger times: 3
Loss after 205071060 batches: 0.0358
trigger times: 4
Loss after 205202160 batches: 0.0321
trigger times: 5
Loss after 205333260 batches: 0.0288
trigger times: 6
Loss after 205464360 batches: 0.0274
trigger times: 7
Loss after 205595460 batches: 0.0259
trigger times: 0
Loss after 205726560 batches: 0.0248
trigger times: 1
Loss after 205857660 batches: 0.0234
trigger times: 2
Loss after 205988760 batches: 0.0227
trigger times: 3
Loss after 206119860 batches: 0.0222
trigger times: 4
Loss after 206250960 batches: 0.0216
trigger times: 5
Loss after 206382060 batches: 0.0206
trigger times: 6
Loss after 206513160 batches: 0.0201
trigger times: 0
Loss after 206644260 batches: 0.0196
trigger times: 0
Loss after 206775360 batches: 0.0191
trigger times: 0
Loss after 206906460 batches: 0.0186
trigger times: 1
Loss after 207037560 batches: 0.0183
trigger times: 2
Loss after 207168660 batches: 0.0183
trigger times: 3
Loss after 207299760 batches: 0.0179
trigger times: 4
Loss after 207430860 batches: 0.0173
trigger times: 5
Loss after 207561960 batches: 0.0172
trigger times: 0
Loss after 207693060 batches: 0.0169
trigger times: 1
Loss after 207824160 batches: 0.0165
trigger times: 2
Loss after 207955260 batches: 0.0164
trigger times: 3
Loss after 208086360 batches: 0.0160
trigger times: 4
Loss after 208217460 batches: 0.0163
trigger times: 5
Loss after 208348560 batches: 0.0157
trigger times: 6
Loss after 208479660 batches: 0.0153
trigger times: 7
Loss after 208610760 batches: 0.0154
trigger times: 8
Loss after 208741860 batches: 0.0149
trigger times: 9
Loss after 208872960 batches: 0.0147
trigger times: 10
Loss after 209004060 batches: 0.0150
trigger times: 11
Loss after 209135160 batches: 0.0148
trigger times: 12
Loss after 209266260 batches: 0.0145
trigger times: 13
Loss after 209397360 batches: 0.0144
trigger times: 14
Loss after 209528460 batches: 0.0143
trigger times: 15
Loss after 209659560 batches: 0.0144
trigger times: 16
Loss after 209790660 batches: 0.0141
trigger times: 17
Loss after 209921760 batches: 0.0136
trigger times: 18
Loss after 210052860 batches: 0.0136
trigger times: 19
Loss after 210183960 batches: 0.0134
trigger times: 20
Early stopping!
Start to test process.
Loss after 210315060 batches: 0.0133
Time to train on one home:  327.3177411556244
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407]]
Round_4_results:  [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407]
trigger times: 0
Loss after 210446160 batches: 0.1838
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1685 < 1686; dropping {'Training_Loss': 0.18377551000635578, 'Validation_Loss': 0.2276346915298038, 'Training_R2': 0.8153650145123092, 'Validation_R2': 0.788195091798996, 'Training_F1': 0.7651215713243832, 'Validation_F1': 0.7166403747963753, 'Training_NEP': 0.47069761248273767, 'Validation_NEP': 0.5449873224498263, 'Training_NDE': 0.1386087608816811, 'Validation_NDE': 0.16867026378414565, 'Training_MAE': 15.58857143140221, 'Validation_MAE': 14.945813395472838, 'Training_MSE': 609.8569, 'Validation_MSE': 622.89465}.
trigger times: 0
Loss after 210577260 batches: 0.0678
trigger times: 0
Loss after 210708360 batches: 0.0470
trigger times: 1
Loss after 210839460 batches: 0.0389
trigger times: 0
Loss after 210970560 batches: 0.0344
trigger times: 0
Loss after 211101660 batches: 0.0311
trigger times: 1
Loss after 211232760 batches: 0.0289
trigger times: 0
Loss after 211363860 batches: 0.0273
trigger times: 0
Loss after 211494960 batches: 0.0263
trigger times: 0
Loss after 211626060 batches: 0.0249
trigger times: 1
Loss after 211757160 batches: 0.0241
trigger times: 2
Loss after 211888260 batches: 0.0226
trigger times: 0
Loss after 212019360 batches: 0.0222
trigger times: 1
Loss after 212150460 batches: 0.0216
trigger times: 2
Loss after 212281560 batches: 0.0208
trigger times: 3
Loss after 212412660 batches: 0.0200
trigger times: 4
Loss after 212543760 batches: 0.0196
trigger times: 5
Loss after 212674860 batches: 0.0194
trigger times: 6
Loss after 212805960 batches: 0.0190
trigger times: 7
Loss after 212937060 batches: 0.0188
trigger times: 8
Loss after 213068160 batches: 0.0182
trigger times: 9
Loss after 213199260 batches: 0.0180
trigger times: 10
Loss after 213330360 batches: 0.0173
trigger times: 11
Loss after 213461460 batches: 0.0176
trigger times: 12
Loss after 213592560 batches: 0.0173
trigger times: 13
Loss after 213723660 batches: 0.0168
trigger times: 0
Loss after 213854760 batches: 0.0165
trigger times: 1
Loss after 213985860 batches: 0.0161
trigger times: 2
Loss after 214116960 batches: 0.0161
trigger times: 3
Loss after 214248060 batches: 0.0157
trigger times: 4
Loss after 214379160 batches: 0.0155
trigger times: 5
Loss after 214510260 batches: 0.0154
trigger times: 6
Loss after 214641360 batches: 0.0155
trigger times: 7
Loss after 214772460 batches: 0.0154
trigger times: 8
Loss after 214903560 batches: 0.0152
trigger times: 9
Loss after 215034660 batches: 0.0149
trigger times: 10
Loss after 215165760 batches: 0.0147
trigger times: 11
Loss after 215296860 batches: 0.0146
trigger times: 12
Loss after 215427960 batches: 0.0146
trigger times: 13
Loss after 215559060 batches: 0.0148
trigger times: 14
Loss after 215690160 batches: 0.0144
trigger times: 15
Loss after 215821260 batches: 0.0142
trigger times: 16
Loss after 215952360 batches: 0.0140
trigger times: 17
Loss after 216083460 batches: 0.0137
trigger times: 18
Loss after 216214560 batches: 0.0137
trigger times: 19
Loss after 216345660 batches: 0.0136
trigger times: 20
Early stopping!
Start to test process.
Loss after 216476760 batches: 0.0131
Time to train on one home:  350.09974360466003
trigger times: 0
Loss after 216579360 batches: 0.4250
trigger times: 1
Loss after 216681960 batches: 0.1823
trigger times: 2
Loss after 216784560 batches: 0.1216
trigger times: 3
Loss after 216887160 batches: 0.1008
trigger times: 4
Loss after 216989760 batches: 0.0993
trigger times: 5
Loss after 217092360 batches: 0.0812
trigger times: 6
Loss after 217194960 batches: 0.0739
trigger times: 7
Loss after 217297560 batches: 0.0586
trigger times: 8
Loss after 217400160 batches: 0.0531
trigger times: 9
Loss after 217502760 batches: 0.0499
trigger times: 10
Loss after 217605360 batches: 0.0468
trigger times: 11
Loss after 217707960 batches: 0.0468
trigger times: 12
Loss after 217810560 batches: 0.0474
trigger times: 13
Loss after 217913160 batches: 0.0434
trigger times: 14
Loss after 218015760 batches: 0.0406
trigger times: 15
Loss after 218118360 batches: 0.0378
trigger times: 16
Loss after 218220960 batches: 0.0367
trigger times: 17
Loss after 218323560 batches: 0.0366
trigger times: 18
Loss after 218426160 batches: 0.0363
trigger times: 19
Loss after 218528760 batches: 0.0334
trigger times: 20
Early stopping!
Start to test process.
Loss after 218631360 batches: 0.0321
Time to train on one home:  132.41284799575806
trigger times: 0
Loss after 218762460 batches: 0.2695
trigger times: 0
Loss after 218893560 batches: 0.0996
trigger times: 0
Loss after 219024660 batches: 0.0666
trigger times: 0
Loss after 219155760 batches: 0.0544
trigger times: 0
Loss after 219286860 batches: 0.0474
trigger times: 0
Loss after 219417960 batches: 0.0427
trigger times: 1
Loss after 219549060 batches: 0.0400
trigger times: 2
Loss after 219680160 batches: 0.0373
trigger times: 3
Loss after 219811260 batches: 0.0357
trigger times: 4
Loss after 219942360 batches: 0.0341
trigger times: 5
Loss after 220073460 batches: 0.0324
trigger times: 6
Loss after 220204560 batches: 0.0312
trigger times: 7
Loss after 220335660 batches: 0.0302
trigger times: 8
Loss after 220466760 batches: 0.0298
trigger times: 9
Loss after 220597860 batches: 0.0285
trigger times: 10
Loss after 220728960 batches: 0.0280
trigger times: 11
Loss after 220860060 batches: 0.0275
trigger times: 12
Loss after 220991160 batches: 0.0273
trigger times: 13
Loss after 221122260 batches: 0.0262
trigger times: 14
Loss after 221253360 batches: 0.0258
trigger times: 15
Loss after 221384460 batches: 0.0252
trigger times: 16
Loss after 221515560 batches: 0.0249
trigger times: 17
Loss after 221646660 batches: 0.0248
trigger times: 18
Loss after 221777760 batches: 0.0242
trigger times: 19
Loss after 221908860 batches: 0.0241
trigger times: 20
Early stopping!
Start to test process.
Loss after 222039960 batches: 0.0236
Time to train on one home:  198.0622580051422
trigger times: 0
Loss after 222171060 batches: 0.4715
trigger times: 1
Loss after 222302160 batches: 0.1688
trigger times: 2
Loss after 222433260 batches: 0.1090
trigger times: 3
Loss after 222564360 batches: 0.0883
trigger times: 4
Loss after 222695460 batches: 0.0769
trigger times: 5
Loss after 222826560 batches: 0.0705
trigger times: 6
Loss after 222957660 batches: 0.0652
trigger times: 7
Loss after 223088760 batches: 0.0600
trigger times: 8
Loss after 223219860 batches: 0.0573
trigger times: 9
Loss after 223350960 batches: 0.0548
trigger times: 10
Loss after 223482060 batches: 0.0528
trigger times: 11
Loss after 223613160 batches: 0.0507
trigger times: 12
Loss after 223744260 batches: 0.0486
trigger times: 13
Loss after 223875360 batches: 0.0468
trigger times: 14
Loss after 224006460 batches: 0.0459
trigger times: 15
Loss after 224137560 batches: 0.0440
trigger times: 16
Loss after 224268660 batches: 0.0432
trigger times: 17
Loss after 224399760 batches: 0.0424
trigger times: 18
Loss after 224530860 batches: 0.0412
trigger times: 19
Loss after 224661960 batches: 0.0407
trigger times: 20
Early stopping!
Start to test process.
Loss after 224793060 batches: 0.0397
Time to train on one home:  161.7217502593994
trigger times: 0
Loss after 224921700 batches: 0.1809
trigger times: 0
Loss after 225050340 batches: 0.0621
trigger times: 0
Loss after 225178980 batches: 0.0434
trigger times: 0
Loss after 225307620 batches: 0.0360
trigger times: 0
Loss after 225436260 batches: 0.0322
trigger times: 0
Loss after 225564900 batches: 0.0291
trigger times: 1
Loss after 225693540 batches: 0.0273
trigger times: 2
Loss after 225822180 batches: 0.0256
trigger times: 3
Loss after 225950820 batches: 0.0247
trigger times: 4
Loss after 226079460 batches: 0.0237
trigger times: 5
Loss after 226208100 batches: 0.0231
trigger times: 6
Loss after 226336740 batches: 0.0215
trigger times: 7
Loss after 226465380 batches: 0.0219
trigger times: 8
Loss after 226594020 batches: 0.0208
trigger times: 0
Loss after 226722660 batches: 0.0202
trigger times: 1
Loss after 226851300 batches: 0.0196
trigger times: 0
Loss after 226979940 batches: 0.0189
trigger times: 0
Loss after 227108580 batches: 0.0192
trigger times: 1
Loss after 227237220 batches: 0.0186
trigger times: 2
Loss after 227365860 batches: 0.0185
trigger times: 3
Loss after 227494500 batches: 0.0180
trigger times: 4
Loss after 227623140 batches: 0.0181
trigger times: 5
Loss after 227751780 batches: 0.0174
trigger times: 6
Loss after 227880420 batches: 0.0171
trigger times: 7
Loss after 228009060 batches: 0.0171
trigger times: 8
Loss after 228137700 batches: 0.0169
trigger times: 9
Loss after 228266340 batches: 0.0166
trigger times: 10
Loss after 228394980 batches: 0.0166
trigger times: 11
Loss after 228523620 batches: 0.0164
trigger times: 12
Loss after 228652260 batches: 0.0163
trigger times: 13
Loss after 228780900 batches: 0.0159
trigger times: 14
Loss after 228909540 batches: 0.0155
trigger times: 15
Loss after 229038180 batches: 0.0160
trigger times: 16
Loss after 229166820 batches: 0.0156
trigger times: 17
Loss after 229295460 batches: 0.0151
trigger times: 0
Loss after 229424100 batches: 0.0151
trigger times: 0
Loss after 229552740 batches: 0.0152
trigger times: 1
Loss after 229681380 batches: 0.0151
trigger times: 2
Loss after 229810020 batches: 0.0146
trigger times: 3
Loss after 229938660 batches: 0.0146
trigger times: 4
Loss after 230067300 batches: 0.0145
trigger times: 5
Loss after 230195940 batches: 0.0145
trigger times: 6
Loss after 230324580 batches: 0.0147
trigger times: 7
Loss after 230453220 batches: 0.0146
trigger times: 8
Loss after 230581860 batches: 0.0140
trigger times: 9
Loss after 230710500 batches: 0.0141
trigger times: 0
Loss after 230839140 batches: 0.0141
trigger times: 0
Loss after 230967780 batches: 0.0140
trigger times: 1
Loss after 231096420 batches: 0.0141
trigger times: 2
Loss after 231225060 batches: 0.0133
trigger times: 3
Loss after 231353700 batches: 0.0136
trigger times: 4
Loss after 231482340 batches: 0.0135
trigger times: 5
Loss after 231610980 batches: 0.0134
trigger times: 6
Loss after 231739620 batches: 0.0138
trigger times: 7
Loss after 231868260 batches: 0.0131
trigger times: 8
Loss after 231996900 batches: 0.0131
trigger times: 9
Loss after 232125540 batches: 0.0130
trigger times: 10
Loss after 232254180 batches: 0.0131
trigger times: 11
Loss after 232382820 batches: 0.0131
trigger times: 12
Loss after 232511460 batches: 0.0130
trigger times: 13
Loss after 232640100 batches: 0.0134
trigger times: 14
Loss after 232768740 batches: 0.0129
trigger times: 15
Loss after 232897380 batches: 0.0131
trigger times: 16
Loss after 233026020 batches: 0.0126
trigger times: 17
Loss after 233154660 batches: 0.0125
trigger times: 18
Loss after 233283300 batches: 0.0125
trigger times: 19
Loss after 233411940 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 233540580 batches: 0.0126
Time to train on one home:  491.90354561805725
trigger times: 0
Loss after 233671680 batches: 0.4702
trigger times: 1
Loss after 233802780 batches: 0.1470
trigger times: 2
Loss after 233933880 batches: 0.0935
trigger times: 3
Loss after 234064980 batches: 0.0764
trigger times: 4
Loss after 234196080 batches: 0.0663
trigger times: 5
Loss after 234327180 batches: 0.0600
trigger times: 6
Loss after 234458280 batches: 0.0556
trigger times: 7
Loss after 234589380 batches: 0.0518
trigger times: 8
Loss after 234720480 batches: 0.0497
trigger times: 9
Loss after 234851580 batches: 0.0469
trigger times: 10
Loss after 234982680 batches: 0.0459
trigger times: 11
Loss after 235113780 batches: 0.0440
trigger times: 12
Loss after 235244880 batches: 0.0420
trigger times: 13
Loss after 235375980 batches: 0.0414
trigger times: 14
Loss after 235507080 batches: 0.0399
trigger times: 15
Loss after 235638180 batches: 0.0386
trigger times: 16
Loss after 235769280 batches: 0.0387
trigger times: 17
Loss after 235900380 batches: 0.0371
trigger times: 18
Loss after 236031480 batches: 0.0365
trigger times: 19
Loss after 236162580 batches: 0.0354
trigger times: 20
Early stopping!
Start to test process.
Loss after 236293680 batches: 0.0349
Time to train on one home:  161.96556496620178
trigger times: 0
Loss after 236424780 batches: 0.4963
trigger times: 0
Loss after 236555880 batches: 0.2249
trigger times: 0
Loss after 236686980 batches: 0.1451
trigger times: 1
Loss after 236818080 batches: 0.1068
trigger times: 2
Loss after 236949180 batches: 0.0811
trigger times: 3
Loss after 237080280 batches: 0.0705
trigger times: 4
Loss after 237211380 batches: 0.0639
trigger times: 5
Loss after 237342480 batches: 0.0590
trigger times: 0
Loss after 237473580 batches: 0.0568
trigger times: 1
Loss after 237604680 batches: 0.0525
trigger times: 0
Loss after 237735780 batches: 0.0515
trigger times: 1
Loss after 237866880 batches: 0.0491
trigger times: 2
Loss after 237997980 batches: 0.0466
trigger times: 3
Loss after 238129080 batches: 0.0452
trigger times: 0
Loss after 238260180 batches: 0.0446
trigger times: 1
Loss after 238391280 batches: 0.0427
trigger times: 0
Loss after 238522380 batches: 0.0416
trigger times: 1
Loss after 238653480 batches: 0.0394
trigger times: 2
Loss after 238784580 batches: 0.0407
trigger times: 0
Loss after 238915680 batches: 0.0395
trigger times: 1
Loss after 239046780 batches: 0.0388
trigger times: 2
Loss after 239177880 batches: 0.0394
trigger times: 0
Loss after 239308980 batches: 0.0369
trigger times: 0
Loss after 239440080 batches: 0.0370
trigger times: 1
Loss after 239571180 batches: 0.0368
trigger times: 2
Loss after 239702280 batches: 0.0361
trigger times: 0
Loss after 239833380 batches: 0.0368
trigger times: 1
Loss after 239964480 batches: 0.0347
trigger times: 2
Loss after 240095580 batches: 0.0353
trigger times: 3
Loss after 240226680 batches: 0.0346
trigger times: 4
Loss after 240357780 batches: 0.0342
trigger times: 5
Loss after 240488880 batches: 0.0351
trigger times: 6
Loss after 240619980 batches: 0.0328
trigger times: 7
Loss after 240751080 batches: 0.0326
trigger times: 0
Loss after 240882180 batches: 0.0316
trigger times: 1
Loss after 241013280 batches: 0.0326
trigger times: 2
Loss after 241144380 batches: 0.0333
trigger times: 0
Loss after 241275480 batches: 0.0320
trigger times: 1
Loss after 241406580 batches: 0.0317
trigger times: 2
Loss after 241537680 batches: 0.0311
trigger times: 3
Loss after 241668780 batches: 0.0310
trigger times: 4
Loss after 241799880 batches: 0.0319
trigger times: 5
Loss after 241930980 batches: 0.0303
trigger times: 6
Loss after 242062080 batches: 0.0307
trigger times: 7
Loss after 242193180 batches: 0.0320
trigger times: 8
Loss after 242324280 batches: 0.0301
trigger times: 9
Loss after 242455380 batches: 0.0299
trigger times: 10
Loss after 242586480 batches: 0.0290
trigger times: 11
Loss after 242717580 batches: 0.0282
trigger times: 12
Loss after 242848680 batches: 0.0292
trigger times: 13
Loss after 242979780 batches: 0.0291
trigger times: 14
Loss after 243110880 batches: 0.0296
trigger times: 0
Loss after 243241980 batches: 0.0304
trigger times: 1
Loss after 243373080 batches: 0.0302
trigger times: 2
Loss after 243504180 batches: 0.0294
trigger times: 3
Loss after 243635280 batches: 0.0289
trigger times: 4
Loss after 243766380 batches: 0.0278
trigger times: 0
Loss after 243897480 batches: 0.0277
trigger times: 1
Loss after 244028580 batches: 0.0274
trigger times: 0
Loss after 244159680 batches: 0.0277
trigger times: 1
Loss after 244290780 batches: 0.0289
trigger times: 2
Loss after 244421880 batches: 0.0272
trigger times: 3
Loss after 244552980 batches: 0.0278
trigger times: 4
Loss after 244684080 batches: 0.0284
trigger times: 5
Loss after 244815180 batches: 0.0266
trigger times: 6
Loss after 244946280 batches: 0.0271
trigger times: 7
Loss after 245077380 batches: 0.0266
trigger times: 8
Loss after 245208480 batches: 0.0261
trigger times: 9
Loss after 245339580 batches: 0.0259
trigger times: 10
Loss after 245470680 batches: 0.0262
trigger times: 11
Loss after 245601780 batches: 0.0266
trigger times: 12
Loss after 245732880 batches: 0.0259
trigger times: 13
Loss after 245863980 batches: 0.0260
trigger times: 14
Loss after 245995080 batches: 0.0253
trigger times: 15
Loss after 246126180 batches: 0.0254
trigger times: 16
Loss after 246257280 batches: 0.0260
trigger times: 17
Loss after 246388380 batches: 0.0255
trigger times: 18
Loss after 246519480 batches: 0.0237
trigger times: 0
Loss after 246650580 batches: 0.0238
trigger times: 1
Loss after 246781680 batches: 0.0243
trigger times: 0
Loss after 246912780 batches: 0.0248
trigger times: 1
Loss after 247043880 batches: 0.0254
trigger times: 2
Loss after 247174980 batches: 0.0256
trigger times: 3
Loss after 247306080 batches: 0.0251
trigger times: 4
Loss after 247437180 batches: 0.0246
trigger times: 5
Loss after 247568280 batches: 0.0245
trigger times: 6
Loss after 247699380 batches: 0.0242
trigger times: 7
Loss after 247830480 batches: 0.0249
trigger times: 8
Loss after 247961580 batches: 0.0238
trigger times: 9
Loss after 248092680 batches: 0.0233
trigger times: 10
Loss after 248223780 batches: 0.0234
trigger times: 11
Loss after 248354880 batches: 0.0245
trigger times: 12
Loss after 248485980 batches: 0.0240
trigger times: 13
Loss after 248617080 batches: 0.0241
trigger times: 14
Loss after 248748180 batches: 0.0234
trigger times: 15
Loss after 248879280 batches: 0.0238
trigger times: 16
Loss after 249010380 batches: 0.0246
trigger times: 17
Loss after 249141480 batches: 0.0245
trigger times: 18
Loss after 249272580 batches: 0.0248
trigger times: 19
Loss after 249403680 batches: 0.0252
trigger times: 20
Early stopping!
Start to test process.
Loss after 249534780 batches: 0.0228
Time to train on one home:  734.7488572597504
trigger times: 0
Loss after 249665880 batches: 0.1282
trigger times: 0
Loss after 249796980 batches: 0.0416
trigger times: 1
Loss after 249928080 batches: 0.0315
trigger times: 2
Loss after 250059180 batches: 0.0259
trigger times: 3
Loss after 250190280 batches: 0.0233
trigger times: 4
Loss after 250321380 batches: 0.0216
trigger times: 5
Loss after 250452480 batches: 0.0200
trigger times: 6
Loss after 250583580 batches: 0.0184
trigger times: 7
Loss after 250714680 batches: 0.0178
trigger times: 8
Loss after 250845780 batches: 0.0173
trigger times: 9
Loss after 250976880 batches: 0.0163
trigger times: 10
Loss after 251107980 batches: 0.0160
trigger times: 11
Loss after 251239080 batches: 0.0155
trigger times: 12
Loss after 251370180 batches: 0.0149
trigger times: 13
Loss after 251501280 batches: 0.0146
trigger times: 14
Loss after 251632380 batches: 0.0145
trigger times: 15
Loss after 251763480 batches: 0.0142
trigger times: 16
Loss after 251894580 batches: 0.0139
trigger times: 17
Loss after 252025680 batches: 0.0135
trigger times: 18
Loss after 252156780 batches: 0.0133
trigger times: 19
Loss after 252287880 batches: 0.0130
trigger times: 20
Early stopping!
Start to test process.
Loss after 252418980 batches: 0.0130
Time to train on one home:  169.14850187301636
trigger times: 0
Loss after 252497580 batches: 0.4642
trigger times: 1
Loss after 252576180 batches: 0.1507
trigger times: 0
Loss after 252654780 batches: 0.0876
trigger times: 1
Loss after 252733380 batches: 0.0670
trigger times: 0
Loss after 252811980 batches: 0.0585
trigger times: 0
Loss after 252890580 batches: 0.0524
trigger times: 1
Loss after 252969180 batches: 0.0493
trigger times: 2
Loss after 253047780 batches: 0.0445
trigger times: 3
Loss after 253126380 batches: 0.0414
trigger times: 4
Loss after 253204980 batches: 0.0400
trigger times: 0
Loss after 253283580 batches: 0.0389
trigger times: 1
Loss after 253362180 batches: 0.0369
trigger times: 2
Loss after 253440780 batches: 0.0351
trigger times: 3
Loss after 253519380 batches: 0.0350
trigger times: 4
Loss after 253597980 batches: 0.0336
trigger times: 5
Loss after 253676580 batches: 0.0330
trigger times: 6
Loss after 253755180 batches: 0.0323
trigger times: 7
Loss after 253833780 batches: 0.0315
trigger times: 8
Loss after 253912380 batches: 0.0305
trigger times: 9
Loss after 253990980 batches: 0.0307
trigger times: 10
Loss after 254069580 batches: 0.0292
trigger times: 11
Loss after 254148180 batches: 0.0285
trigger times: 12
Loss after 254226780 batches: 0.0288
trigger times: 13
Loss after 254305380 batches: 0.0283
trigger times: 14
Loss after 254383980 batches: 0.0271
trigger times: 15
Loss after 254462580 batches: 0.0275
trigger times: 16
Loss after 254541180 batches: 0.0265
trigger times: 17
Loss after 254619780 batches: 0.0272
trigger times: 18
Loss after 254698380 batches: 0.0264
trigger times: 19
Loss after 254776980 batches: 0.0262
trigger times: 20
Early stopping!
Start to test process.
Loss after 254855580 batches: 0.0262
Time to train on one home:  155.15509724617004
trigger times: 0
Loss after 254986680 batches: 0.1430
trigger times: 0
Loss after 255117780 batches: 0.0457
trigger times: 0
Loss after 255248880 batches: 0.0339
trigger times: 0
Loss after 255379980 batches: 0.0282
trigger times: 1
Loss after 255511080 batches: 0.0252
trigger times: 2
Loss after 255642180 batches: 0.0234
trigger times: 0
Loss after 255773280 batches: 0.0221
trigger times: 1
Loss after 255904380 batches: 0.0209
trigger times: 0
Loss after 256035480 batches: 0.0199
trigger times: 1
Loss after 256166580 batches: 0.0190
trigger times: 2
Loss after 256297680 batches: 0.0188
trigger times: 0
Loss after 256428780 batches: 0.0178
trigger times: 1
Loss after 256559880 batches: 0.0174
trigger times: 2
Loss after 256690980 batches: 0.0168
trigger times: 3
Loss after 256822080 batches: 0.0167
trigger times: 4
Loss after 256953180 batches: 0.0164
trigger times: 5
Loss after 257084280 batches: 0.0156
trigger times: 6
Loss after 257215380 batches: 0.0155
trigger times: 7
Loss after 257346480 batches: 0.0152
trigger times: 8
Loss after 257477580 batches: 0.0150
trigger times: 9
Loss after 257608680 batches: 0.0148
trigger times: 10
Loss after 257739780 batches: 0.0145
trigger times: 11
Loss after 257870880 batches: 0.0144
trigger times: 12
Loss after 258001980 batches: 0.0144
trigger times: 13
Loss after 258133080 batches: 0.0141
trigger times: 14
Loss after 258264180 batches: 0.0142
trigger times: 15
Loss after 258395280 batches: 0.0141
trigger times: 16
Loss after 258526380 batches: 0.0135
trigger times: 17
Loss after 258657480 batches: 0.0135
trigger times: 18
Loss after 258788580 batches: 0.0132
trigger times: 19
Loss after 258919680 batches: 0.0130
trigger times: 20
Early stopping!
Start to test process.
Loss after 259050780 batches: 0.0130
Time to train on one home:  240.73069620132446
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516]]
Round_5_results:  [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2075 < 2076; dropping {'Training_Loss': 0.14393449673112832, 'Validation_Loss': 0.1978473969631725, 'Training_R2': 0.8552696609862646, 'Validation_R2': 0.8160529481290957, 'Training_F1': 0.8015709967542004, 'Validation_F1': 0.7413922355273903, 'Training_NEP': 0.3963572608464278, 'Validation_NEP': 0.5013502287545551, 'Training_NDE': 0.10865163446511, 'Validation_NDE': 0.14648573550494484, 'Training_MAE': 13.126566418022895, 'Validation_MAE': 13.749103247136626, 'Training_MSE': 478.0502, 'Validation_MSE': 540.9678}.
trigger times: 0
Loss after 259181880 batches: 0.1439
trigger times: 0
Loss after 259312980 batches: 0.0439
trigger times: 0
Loss after 259444080 batches: 0.0313
trigger times: 0
Loss after 259575180 batches: 0.0271
trigger times: 0
Loss after 259706280 batches: 0.0246
trigger times: 0
Loss after 259837380 batches: 0.0228
trigger times: 1
Loss after 259968480 batches: 0.0219
trigger times: 2
Loss after 260099580 batches: 0.0196
trigger times: 3
Loss after 260230680 batches: 0.0192
trigger times: 0
Loss after 260361780 batches: 0.0187
trigger times: 1
Loss after 260492880 batches: 0.0179
trigger times: 0
Loss after 260623980 batches: 0.0174
trigger times: 1
Loss after 260755080 batches: 0.0172
trigger times: 2
Loss after 260886180 batches: 0.0168
trigger times: 0
Loss after 261017280 batches: 0.0166
trigger times: 1
Loss after 261148380 batches: 0.0162
trigger times: 2
Loss after 261279480 batches: 0.0155
trigger times: 3
Loss after 261410580 batches: 0.0155
trigger times: 4
Loss after 261541680 batches: 0.0150
trigger times: 5
Loss after 261672780 batches: 0.0145
trigger times: 6
Loss after 261803880 batches: 0.0142
trigger times: 7
Loss after 261934980 batches: 0.0143
trigger times: 8
Loss after 262066080 batches: 0.0147
trigger times: 9
Loss after 262197180 batches: 0.0141
trigger times: 10
Loss after 262328280 batches: 0.0138
trigger times: 11
Loss after 262459380 batches: 0.0138
trigger times: 12
Loss after 262590480 batches: 0.0136
trigger times: 13
Loss after 262721580 batches: 0.0135
trigger times: 14
Loss after 262852680 batches: 0.0133
trigger times: 15
Loss after 262983780 batches: 0.0133
trigger times: 16
Loss after 263114880 batches: 0.0129
trigger times: 17
Loss after 263245980 batches: 0.0130
trigger times: 18
Loss after 263377080 batches: 0.0129
trigger times: 19
Loss after 263508180 batches: 0.0131
trigger times: 20
Early stopping!
Start to test process.
Loss after 263639280 batches: 0.0128
Time to train on one home:  261.97122383117676
trigger times: 0
Loss after 263741880 batches: 0.3994
trigger times: 0
Loss after 263844480 batches: 0.1625
trigger times: 1
Loss after 263947080 batches: 0.1084
trigger times: 2
Loss after 264049680 batches: 0.0801
trigger times: 3
Loss after 264152280 batches: 0.0674
trigger times: 4
Loss after 264254880 batches: 0.0598
trigger times: 5
Loss after 264357480 batches: 0.0659
trigger times: 6
Loss after 264460080 batches: 0.0500
trigger times: 7
Loss after 264562680 batches: 0.0458
trigger times: 8
Loss after 264665280 batches: 0.0470
trigger times: 9
Loss after 264767880 batches: 0.0427
trigger times: 10
Loss after 264870480 batches: 0.0390
trigger times: 11
Loss after 264973080 batches: 0.0388
trigger times: 12
Loss after 265075680 batches: 0.0378
trigger times: 13
Loss after 265178280 batches: 0.0360
trigger times: 14
Loss after 265280880 batches: 0.0345
trigger times: 15
Loss after 265383480 batches: 0.0336
trigger times: 16
Loss after 265486080 batches: 0.0325
trigger times: 17
Loss after 265588680 batches: 0.0328
trigger times: 18
Loss after 265691280 batches: 0.0316
trigger times: 0
Loss after 265793880 batches: 0.0299
trigger times: 1
Loss after 265896480 batches: 0.0305
trigger times: 0
Loss after 265999080 batches: 0.0291
trigger times: 0
Loss after 266101680 batches: 0.0291
trigger times: 1
Loss after 266204280 batches: 0.0281
trigger times: 0
Loss after 266306880 batches: 0.0277
trigger times: 1
Loss after 266409480 batches: 0.0272
trigger times: 0
Loss after 266512080 batches: 0.0279
trigger times: 1
Loss after 266614680 batches: 0.0279
trigger times: 2
Loss after 266717280 batches: 0.0262
trigger times: 3
Loss after 266819880 batches: 0.0250
trigger times: 4
Loss after 266922480 batches: 0.0252
trigger times: 5
Loss after 267025080 batches: 0.0252
trigger times: 6
Loss after 267127680 batches: 0.0250
trigger times: 0
Loss after 267230280 batches: 0.0244
trigger times: 1
Loss after 267332880 batches: 0.0247
trigger times: 0
Loss after 267435480 batches: 0.0251
trigger times: 1
Loss after 267538080 batches: 0.0240
trigger times: 2
Loss after 267640680 batches: 0.0228
trigger times: 3
Loss after 267743280 batches: 0.0233
trigger times: 4
Loss after 267845880 batches: 0.0250
trigger times: 0
Loss after 267948480 batches: 0.0260
trigger times: 0
Loss after 268051080 batches: 0.0243
trigger times: 1
Loss after 268153680 batches: 0.0246
trigger times: 2
Loss after 268256280 batches: 0.0234
trigger times: 3
Loss after 268358880 batches: 0.0243
trigger times: 4
Loss after 268461480 batches: 0.0242
trigger times: 5
Loss after 268564080 batches: 0.0248
trigger times: 6
Loss after 268666680 batches: 0.0228
trigger times: 0
Loss after 268769280 batches: 0.0240
trigger times: 1
Loss after 268871880 batches: 0.0230
trigger times: 2
Loss after 268974480 batches: 0.0233
trigger times: 3
Loss after 269077080 batches: 0.0213
trigger times: 0
Loss after 269179680 batches: 0.0226
trigger times: 1
Loss after 269282280 batches: 0.0229
trigger times: 2
Loss after 269384880 batches: 0.0236
trigger times: 3
Loss after 269487480 batches: 0.0248
trigger times: 0
Loss after 269590080 batches: 0.0252
trigger times: 0
Loss after 269692680 batches: 0.0216
trigger times: 0
Loss after 269795280 batches: 0.0224
trigger times: 1
Loss after 269897880 batches: 0.0217
trigger times: 2
Loss after 270000480 batches: 0.0215
trigger times: 3
Loss after 270103080 batches: 0.0226
trigger times: 4
Loss after 270205680 batches: 0.0220
trigger times: 5
Loss after 270308280 batches: 0.0222
trigger times: 6
Loss after 270410880 batches: 0.0224
trigger times: 7
Loss after 270513480 batches: 0.0208
trigger times: 8
Loss after 270616080 batches: 0.0207
trigger times: 9
Loss after 270718680 batches: 0.0221
trigger times: 10
Loss after 270821280 batches: 0.0212
trigger times: 11
Loss after 270923880 batches: 0.0202
trigger times: 12
Loss after 271026480 batches: 0.0202
trigger times: 13
Loss after 271129080 batches: 0.0220
trigger times: 14
Loss after 271231680 batches: 0.0213
trigger times: 15
Loss after 271334280 batches: 0.0196
trigger times: 16
Loss after 271436880 batches: 0.0200
trigger times: 17
Loss after 271539480 batches: 0.0194
trigger times: 18
Loss after 271642080 batches: 0.0193
trigger times: 19
Loss after 271744680 batches: 0.0196
trigger times: 0
Loss after 271847280 batches: 0.0200
trigger times: 1
Loss after 271949880 batches: 0.0193
trigger times: 2
Loss after 272052480 batches: 0.0189
trigger times: 3
Loss after 272155080 batches: 0.0184
trigger times: 4
Loss after 272257680 batches: 0.0178
trigger times: 5
Loss after 272360280 batches: 0.0185
trigger times: 6
Loss after 272462880 batches: 0.0183
trigger times: 7
Loss after 272565480 batches: 0.0183
trigger times: 8
Loss after 272668080 batches: 0.0184
trigger times: 9
Loss after 272770680 batches: 0.0188
trigger times: 10
Loss after 272873280 batches: 0.0213
trigger times: 11
Loss after 272975880 batches: 0.0198
trigger times: 12
Loss after 273078480 batches: 0.0212
trigger times: 13
Loss after 273181080 batches: 0.0216
trigger times: 14
Loss after 273283680 batches: 0.0249
trigger times: 0
Loss after 273386280 batches: 0.0213
trigger times: 1
Loss after 273488880 batches: 0.0238
trigger times: 2
Loss after 273591480 batches: 0.0203
trigger times: 3
Loss after 273694080 batches: 0.0201
trigger times: 4
Loss after 273796680 batches: 0.0229
trigger times: 5
Loss after 273899280 batches: 0.0197
trigger times: 6
Loss after 274001880 batches: 0.0189
trigger times: 7
Loss after 274104480 batches: 0.0200
trigger times: 8
Loss after 274207080 batches: 0.0184
trigger times: 9
Loss after 274309680 batches: 0.0177
trigger times: 10
Loss after 274412280 batches: 0.0175
trigger times: 11
Loss after 274514880 batches: 0.0170
trigger times: 12
Loss after 274617480 batches: 0.0174
trigger times: 13
Loss after 274720080 batches: 0.0178
trigger times: 14
Loss after 274822680 batches: 0.0173
trigger times: 15
Loss after 274925280 batches: 0.0172
trigger times: 16
Loss after 275027880 batches: 0.0177
trigger times: 17
Loss after 275130480 batches: 0.0177
trigger times: 18
Loss after 275233080 batches: 0.0167
trigger times: 19
Loss after 275335680 batches: 0.0167
trigger times: 20
Early stopping!
Start to test process.
Loss after 275438280 batches: 0.0171
Time to train on one home:  672.3460340499878
trigger times: 0
Loss after 275569380 batches: 0.2172
trigger times: 0
Loss after 275700480 batches: 0.0757
trigger times: 0
Loss after 275831580 batches: 0.0536
trigger times: 0
Loss after 275962680 batches: 0.0453
trigger times: 1
Loss after 276093780 batches: 0.0403
trigger times: 2
Loss after 276224880 batches: 0.0366
trigger times: 3
Loss after 276355980 batches: 0.0343
trigger times: 4
Loss after 276487080 batches: 0.0323
trigger times: 5
Loss after 276618180 batches: 0.0314
trigger times: 6
Loss after 276749280 batches: 0.0295
trigger times: 7
Loss after 276880380 batches: 0.0290
trigger times: 8
Loss after 277011480 batches: 0.0276
trigger times: 9
Loss after 277142580 batches: 0.0269
trigger times: 10
Loss after 277273680 batches: 0.0261
trigger times: 11
Loss after 277404780 batches: 0.0253
trigger times: 12
Loss after 277535880 batches: 0.0251
trigger times: 13
Loss after 277666980 batches: 0.0243
trigger times: 14
Loss after 277798080 batches: 0.0240
trigger times: 15
Loss after 277929180 batches: 0.0236
trigger times: 16
Loss after 278060280 batches: 0.0233
trigger times: 17
Loss after 278191380 batches: 0.0228
trigger times: 18
Loss after 278322480 batches: 0.0225
trigger times: 19
Loss after 278453580 batches: 0.0223
trigger times: 20
Early stopping!
Start to test process.
Loss after 278584680 batches: 0.0219
Time to train on one home:  184.1719365119934
trigger times: 0
Loss after 278715780 batches: 0.4387
trigger times: 1
Loss after 278846880 batches: 0.1433
trigger times: 0
Loss after 278977980 batches: 0.0949
trigger times: 1
Loss after 279109080 batches: 0.0781
trigger times: 2
Loss after 279240180 batches: 0.0684
trigger times: 3
Loss after 279371280 batches: 0.0629
trigger times: 0
Loss after 279502380 batches: 0.0581
trigger times: 1
Loss after 279633480 batches: 0.0544
trigger times: 0
Loss after 279764580 batches: 0.0515
trigger times: 1
Loss after 279895680 batches: 0.0494
trigger times: 2
Loss after 280026780 batches: 0.0476
trigger times: 3
Loss after 280157880 batches: 0.0457
trigger times: 0
Loss after 280288980 batches: 0.0440
trigger times: 1
Loss after 280420080 batches: 0.0426
trigger times: 0
Loss after 280551180 batches: 0.0413
trigger times: 0
Loss after 280682280 batches: 0.0406
trigger times: 1
Loss after 280813380 batches: 0.0399
trigger times: 0
Loss after 280944480 batches: 0.0391
trigger times: 1
Loss after 281075580 batches: 0.0381
trigger times: 2
Loss after 281206680 batches: 0.0373
trigger times: 0
Loss after 281337780 batches: 0.0369
trigger times: 1
Loss after 281468880 batches: 0.0362
trigger times: 2
Loss after 281599980 batches: 0.0363
trigger times: 3
Loss after 281731080 batches: 0.0348
trigger times: 4
Loss after 281862180 batches: 0.0345
trigger times: 5
Loss after 281993280 batches: 0.0340
trigger times: 6
Loss after 282124380 batches: 0.0335
trigger times: 0
Loss after 282255480 batches: 0.0328
trigger times: 1
Loss after 282386580 batches: 0.0329
trigger times: 0
Loss after 282517680 batches: 0.0325
trigger times: 0
Loss after 282648780 batches: 0.0324
trigger times: 1
Loss after 282779880 batches: 0.0324
trigger times: 2
Loss after 282910980 batches: 0.0313
trigger times: 3
Loss after 283042080 batches: 0.0309
trigger times: 4
Loss after 283173180 batches: 0.0307
trigger times: 5
Loss after 283304280 batches: 0.0308
trigger times: 6
Loss after 283435380 batches: 0.0306
trigger times: 7
Loss after 283566480 batches: 0.0297
trigger times: 8
Loss after 283697580 batches: 0.0297
trigger times: 0
Loss after 283828680 batches: 0.0296
trigger times: 1
Loss after 283959780 batches: 0.0294
trigger times: 2
Loss after 284090880 batches: 0.0295
trigger times: 0
Loss after 284221980 batches: 0.0289
trigger times: 1
Loss after 284353080 batches: 0.0289
trigger times: 2
Loss after 284484180 batches: 0.0283
trigger times: 3
Loss after 284615280 batches: 0.0285
trigger times: 4
Loss after 284746380 batches: 0.0279
trigger times: 5
Loss after 284877480 batches: 0.0278
trigger times: 6
Loss after 285008580 batches: 0.0278
trigger times: 7
Loss after 285139680 batches: 0.0275
trigger times: 8
Loss after 285270780 batches: 0.0272
trigger times: 9
Loss after 285401880 batches: 0.0268
trigger times: 10
Loss after 285532980 batches: 0.0265
trigger times: 0
Loss after 285664080 batches: 0.0262
trigger times: 1
Loss after 285795180 batches: 0.0267
trigger times: 2
Loss after 285926280 batches: 0.0265
trigger times: 3
Loss after 286057380 batches: 0.0262
trigger times: 4
Loss after 286188480 batches: 0.0259
trigger times: 5
Loss after 286319580 batches: 0.0261
trigger times: 6
Loss after 286450680 batches: 0.0259
trigger times: 7
Loss after 286581780 batches: 0.0255
trigger times: 8
Loss after 286712880 batches: 0.0258
trigger times: 0
Loss after 286843980 batches: 0.0251
trigger times: 1
Loss after 286975080 batches: 0.0251
trigger times: 2
Loss after 287106180 batches: 0.0250
trigger times: 3
Loss after 287237280 batches: 0.0250
trigger times: 4
Loss after 287368380 batches: 0.0248
trigger times: 5
Loss after 287499480 batches: 0.0248
trigger times: 6
Loss after 287630580 batches: 0.0250
trigger times: 0
Loss after 287761680 batches: 0.0247
trigger times: 1
Loss after 287892780 batches: 0.0244
trigger times: 2
Loss after 288023880 batches: 0.0246
trigger times: 3
Loss after 288154980 batches: 0.0242
trigger times: 4
Loss after 288286080 batches: 0.0241
trigger times: 5
Loss after 288417180 batches: 0.0238
trigger times: 6
Loss after 288548280 batches: 0.0238
trigger times: 7
Loss after 288679380 batches: 0.0236
trigger times: 8
Loss after 288810480 batches: 0.0233
trigger times: 9
Loss after 288941580 batches: 0.0235
trigger times: 10
Loss after 289072680 batches: 0.0238
trigger times: 11
Loss after 289203780 batches: 0.0234
trigger times: 12
Loss after 289334880 batches: 0.0230
trigger times: 13
Loss after 289465980 batches: 0.0229
trigger times: 0
Loss after 289597080 batches: 0.0233
trigger times: 1
Loss after 289728180 batches: 0.0234
trigger times: 2
Loss after 289859280 batches: 0.0231
trigger times: 3
Loss after 289990380 batches: 0.0229
trigger times: 4
Loss after 290121480 batches: 0.0227
trigger times: 5
Loss after 290252580 batches: 0.0226
trigger times: 6
Loss after 290383680 batches: 0.0226
trigger times: 0
Loss after 290514780 batches: 0.0226
trigger times: 1
Loss after 290645880 batches: 0.0227
trigger times: 2
Loss after 290776980 batches: 0.0223
trigger times: 3
Loss after 290908080 batches: 0.0221
trigger times: 4
Loss after 291039180 batches: 0.0222
trigger times: 0
Loss after 291170280 batches: 0.0224
trigger times: 1
Loss after 291301380 batches: 0.0222
trigger times: 2
Loss after 291432480 batches: 0.0222
trigger times: 3
Loss after 291563580 batches: 0.0220
trigger times: 4
Loss after 291694680 batches: 0.0220
trigger times: 5
Loss after 291825780 batches: 0.0218
trigger times: 6
Loss after 291956880 batches: 0.0216
trigger times: 7
Loss after 292087980 batches: 0.0216
trigger times: 8
Loss after 292219080 batches: 0.0216
trigger times: 9
Loss after 292350180 batches: 0.0218
trigger times: 10
Loss after 292481280 batches: 0.0218
trigger times: 11
Loss after 292612380 batches: 0.0218
trigger times: 12
Loss after 292743480 batches: 0.0215
trigger times: 13
Loss after 292874580 batches: 0.0213
trigger times: 14
Loss after 293005680 batches: 0.0215
trigger times: 15
Loss after 293136780 batches: 0.0215
trigger times: 16
Loss after 293267880 batches: 0.0211
trigger times: 17
Loss after 293398980 batches: 0.0209
trigger times: 18
Loss after 293530080 batches: 0.0210
trigger times: 19
Loss after 293661180 batches: 0.0209
trigger times: 20
Early stopping!
Start to test process.
Loss after 293792280 batches: 0.0207
Time to train on one home:  842.8826744556427
trigger times: 0
Loss after 293920920 batches: 0.1458
trigger times: 0
Loss after 294049560 batches: 0.0486
trigger times: 0
Loss after 294178200 batches: 0.0354
trigger times: 0
Loss after 294306840 batches: 0.0294
trigger times: 0
Loss after 294435480 batches: 0.0274
trigger times: 0
Loss after 294564120 batches: 0.0243
trigger times: 0
Loss after 294692760 batches: 0.0229
trigger times: 0
Loss after 294821400 batches: 0.0220
trigger times: 0
Loss after 294950040 batches: 0.0207
trigger times: 1
Loss after 295078680 batches: 0.0197
trigger times: 2
Loss after 295207320 batches: 0.0194
trigger times: 0
Loss after 295335960 batches: 0.0190
trigger times: 1
Loss after 295464600 batches: 0.0183
trigger times: 0
Loss after 295593240 batches: 0.0179
trigger times: 1
Loss after 295721880 batches: 0.0179
trigger times: 2
Loss after 295850520 batches: 0.0170
trigger times: 0
Loss after 295979160 batches: 0.0169
trigger times: 1
Loss after 296107800 batches: 0.0167
trigger times: 2
Loss after 296236440 batches: 0.0172
trigger times: 0
Loss after 296365080 batches: 0.0160
trigger times: 0
Loss after 296493720 batches: 0.0156
trigger times: 1
Loss after 296622360 batches: 0.0160
trigger times: 2
Loss after 296751000 batches: 0.0155
trigger times: 3
Loss after 296879640 batches: 0.0150
trigger times: 4
Loss after 297008280 batches: 0.0151
trigger times: 5
Loss after 297136920 batches: 0.0149
trigger times: 6
Loss after 297265560 batches: 0.0147
trigger times: 7
Loss after 297394200 batches: 0.0147
trigger times: 8
Loss after 297522840 batches: 0.0144
trigger times: 9
Loss after 297651480 batches: 0.0145
trigger times: 10
Loss after 297780120 batches: 0.0145
trigger times: 11
Loss after 297908760 batches: 0.0142
trigger times: 12
Loss after 298037400 batches: 0.0139
trigger times: 13
Loss after 298166040 batches: 0.0141
trigger times: 14
Loss after 298294680 batches: 0.0137
trigger times: 15
Loss after 298423320 batches: 0.0139
trigger times: 16
Loss after 298551960 batches: 0.0136
trigger times: 17
Loss after 298680600 batches: 0.0135
trigger times: 18
Loss after 298809240 batches: 0.0134
trigger times: 19
Loss after 298937880 batches: 0.0133
trigger times: 20
Early stopping!
Start to test process.
Loss after 299066520 batches: 0.0133
Time to train on one home:  301.286824464798
trigger times: 0
Loss after 299197620 batches: 0.4280
trigger times: 0
Loss after 299328720 batches: 0.1255
trigger times: 0
Loss after 299459820 batches: 0.0831
trigger times: 0
Loss after 299590920 batches: 0.0671
trigger times: 0
Loss after 299722020 batches: 0.0597
trigger times: 1
Loss after 299853120 batches: 0.0544
trigger times: 2
Loss after 299984220 batches: 0.0503
trigger times: 3
Loss after 300115320 batches: 0.0474
trigger times: 0
Loss after 300246420 batches: 0.0459
trigger times: 1
Loss after 300377520 batches: 0.0428
trigger times: 0
Loss after 300508620 batches: 0.0415
trigger times: 1
Loss after 300639720 batches: 0.0398
trigger times: 2
Loss after 300770820 batches: 0.0388
trigger times: 0
Loss after 300901920 batches: 0.0380
trigger times: 1
Loss after 301033020 batches: 0.0368
trigger times: 2
Loss after 301164120 batches: 0.0360
trigger times: 3
Loss after 301295220 batches: 0.0360
trigger times: 4
Loss after 301426320 batches: 0.0343
trigger times: 5
Loss after 301557420 batches: 0.0336
trigger times: 6
Loss after 301688520 batches: 0.0332
trigger times: 7
Loss after 301819620 batches: 0.0323
trigger times: 0
Loss after 301950720 batches: 0.0320
trigger times: 0
Loss after 302081820 batches: 0.0317
trigger times: 1
Loss after 302212920 batches: 0.0314
trigger times: 0
Loss after 302344020 batches: 0.0312
trigger times: 1
Loss after 302475120 batches: 0.0305
trigger times: 2
Loss after 302606220 batches: 0.0294
trigger times: 3
Loss after 302737320 batches: 0.0296
trigger times: 4
Loss after 302868420 batches: 0.0294
trigger times: 5
Loss after 302999520 batches: 0.0289
trigger times: 6
Loss after 303130620 batches: 0.0289
trigger times: 7
Loss after 303261720 batches: 0.0284
trigger times: 8
Loss after 303392820 batches: 0.0282
trigger times: 9
Loss after 303523920 batches: 0.0276
trigger times: 0
Loss after 303655020 batches: 0.0276
trigger times: 1
Loss after 303786120 batches: 0.0272
trigger times: 2
Loss after 303917220 batches: 0.0276
trigger times: 3
Loss after 304048320 batches: 0.0266
trigger times: 0
Loss after 304179420 batches: 0.0266
trigger times: 1
Loss after 304310520 batches: 0.0264
trigger times: 2
Loss after 304441620 batches: 0.0259
trigger times: 3
Loss after 304572720 batches: 0.0260
trigger times: 4
Loss after 304703820 batches: 0.0259
trigger times: 5
Loss after 304834920 batches: 0.0253
trigger times: 0
Loss after 304966020 batches: 0.0254
trigger times: 1
Loss after 305097120 batches: 0.0252
trigger times: 2
Loss after 305228220 batches: 0.0251
trigger times: 3
Loss after 305359320 batches: 0.0250
trigger times: 4
Loss after 305490420 batches: 0.0246
trigger times: 5
Loss after 305621520 batches: 0.0244
trigger times: 6
Loss after 305752620 batches: 0.0241
trigger times: 7
Loss after 305883720 batches: 0.0243
trigger times: 8
Loss after 306014820 batches: 0.0239
trigger times: 9
Loss after 306145920 batches: 0.0237
trigger times: 10
Loss after 306277020 batches: 0.0237
trigger times: 11
Loss after 306408120 batches: 0.0236
trigger times: 12
Loss after 306539220 batches: 0.0233
trigger times: 13
Loss after 306670320 batches: 0.0233
trigger times: 14
Loss after 306801420 batches: 0.0234
trigger times: 15
Loss after 306932520 batches: 0.0230
trigger times: 16
Loss after 307063620 batches: 0.0229
trigger times: 17
Loss after 307194720 batches: 0.0224
trigger times: 18
Loss after 307325820 batches: 0.0224
trigger times: 19
Loss after 307456920 batches: 0.0225
trigger times: 20
Early stopping!
Start to test process.
Loss after 307588020 batches: 0.0227
Time to train on one home:  477.35092639923096
trigger times: 0
Loss after 307719120 batches: 0.4104
trigger times: 0
Loss after 307850220 batches: 0.1630
trigger times: 0
Loss after 307981320 batches: 0.0955
trigger times: 0
Loss after 308112420 batches: 0.0713
trigger times: 1
Loss after 308243520 batches: 0.0606
trigger times: 0
Loss after 308374620 batches: 0.0551
trigger times: 1
Loss after 308505720 batches: 0.0501
trigger times: 0
Loss after 308636820 batches: 0.0494
trigger times: 1
Loss after 308767920 batches: 0.0465
trigger times: 2
Loss after 308899020 batches: 0.0431
trigger times: 3
Loss after 309030120 batches: 0.0399
trigger times: 4
Loss after 309161220 batches: 0.0408
trigger times: 0
Loss after 309292320 batches: 0.0399
trigger times: 1
Loss after 309423420 batches: 0.0390
trigger times: 0
Loss after 309554520 batches: 0.0379
trigger times: 1
Loss after 309685620 batches: 0.0382
trigger times: 2
Loss after 309816720 batches: 0.0367
trigger times: 0
Loss after 309947820 batches: 0.0350
trigger times: 1
Loss after 310078920 batches: 0.0342
trigger times: 2
Loss after 310210020 batches: 0.0339
trigger times: 0
Loss after 310341120 batches: 0.0342
trigger times: 1
Loss after 310472220 batches: 0.0336
trigger times: 2
Loss after 310603320 batches: 0.0332
trigger times: 0
Loss after 310734420 batches: 0.0310
trigger times: 1
Loss after 310865520 batches: 0.0315
trigger times: 2
Loss after 310996620 batches: 0.0310
trigger times: 3
Loss after 311127720 batches: 0.0299
trigger times: 4
Loss after 311258820 batches: 0.0308
trigger times: 5
Loss after 311389920 batches: 0.0295
trigger times: 6
Loss after 311521020 batches: 0.0291
trigger times: 7
Loss after 311652120 batches: 0.0311
trigger times: 8
Loss after 311783220 batches: 0.0288
trigger times: 9
Loss after 311914320 batches: 0.0289
trigger times: 10
Loss after 312045420 batches: 0.0310
trigger times: 0
Loss after 312176520 batches: 0.0289
trigger times: 0
Loss after 312307620 batches: 0.0288
trigger times: 1
Loss after 312438720 batches: 0.0293
trigger times: 2
Loss after 312569820 batches: 0.0294
trigger times: 3
Loss after 312700920 batches: 0.0283
trigger times: 4
Loss after 312832020 batches: 0.0280
trigger times: 0
Loss after 312963120 batches: 0.0281
trigger times: 1
Loss after 313094220 batches: 0.0292
trigger times: 2
Loss after 313225320 batches: 0.0284
trigger times: 3
Loss after 313356420 batches: 0.0272
trigger times: 0
Loss after 313487520 batches: 0.0292
trigger times: 1
Loss after 313618620 batches: 0.0283
trigger times: 2
Loss after 313749720 batches: 0.0272
trigger times: 3
Loss after 313880820 batches: 0.0269
trigger times: 4
Loss after 314011920 batches: 0.0257
trigger times: 5
Loss after 314143020 batches: 0.0261
trigger times: 6
Loss after 314274120 batches: 0.0257
trigger times: 7
Loss after 314405220 batches: 0.0258
trigger times: 8
Loss after 314536320 batches: 0.0257
trigger times: 9
Loss after 314667420 batches: 0.0262
trigger times: 10
Loss after 314798520 batches: 0.0252
trigger times: 11
Loss after 314929620 batches: 0.0251
trigger times: 12
Loss after 315060720 batches: 0.0248
trigger times: 13
Loss after 315191820 batches: 0.0238
trigger times: 14
Loss after 315322920 batches: 0.0247
trigger times: 15
Loss after 315454020 batches: 0.0247
trigger times: 16
Loss after 315585120 batches: 0.0237
trigger times: 17
Loss after 315716220 batches: 0.0242
trigger times: 18
Loss after 315847320 batches: 0.0248
trigger times: 0
Loss after 315978420 batches: 0.0245
trigger times: 1
Loss after 316109520 batches: 0.0246
trigger times: 2
Loss after 316240620 batches: 0.0243
trigger times: 3
Loss after 316371720 batches: 0.0236
trigger times: 4
Loss after 316502820 batches: 0.0243
trigger times: 5
Loss after 316633920 batches: 0.0239
trigger times: 0
Loss after 316765020 batches: 0.0233
trigger times: 1
Loss after 316896120 batches: 0.0245
trigger times: 2
Loss after 317027220 batches: 0.0239
trigger times: 3
Loss after 317158320 batches: 0.0230
trigger times: 4
Loss after 317289420 batches: 0.0233
trigger times: 5
Loss after 317420520 batches: 0.0231
trigger times: 6
Loss after 317551620 batches: 0.0233
trigger times: 7
Loss after 317682720 batches: 0.0237
trigger times: 8
Loss after 317813820 batches: 0.0228
trigger times: 9
Loss after 317944920 batches: 0.0236
trigger times: 10
Loss after 318076020 batches: 0.0223
trigger times: 11
Loss after 318207120 batches: 0.0236
trigger times: 12
Loss after 318338220 batches: 0.0236
trigger times: 13
Loss after 318469320 batches: 0.0230
trigger times: 14
Loss after 318600420 batches: 0.0227
trigger times: 15
Loss after 318731520 batches: 0.0240
trigger times: 16
Loss after 318862620 batches: 0.0224
trigger times: 0
Loss after 318993720 batches: 0.0228
trigger times: 1
Loss after 319124820 batches: 0.0229
trigger times: 2
Loss after 319255920 batches: 0.0226
trigger times: 3
Loss after 319387020 batches: 0.0221
trigger times: 4
Loss after 319518120 batches: 0.0227
trigger times: 5
Loss after 319649220 batches: 0.0236
trigger times: 6
Loss after 319780320 batches: 0.0228
trigger times: 7
Loss after 319911420 batches: 0.0230
trigger times: 8
Loss after 320042520 batches: 0.0223
trigger times: 9
Loss after 320173620 batches: 0.0229
trigger times: 10
Loss after 320304720 batches: 0.0235
trigger times: 11
Loss after 320435820 batches: 0.0219
trigger times: 12
Loss after 320566920 batches: 0.0213
trigger times: 13
Loss after 320698020 batches: 0.0216
trigger times: 14
Loss after 320829120 batches: 0.0212
trigger times: 15
Loss after 320960220 batches: 0.0211
trigger times: 16
Loss after 321091320 batches: 0.0206
trigger times: 17
Loss after 321222420 batches: 0.0222
trigger times: 18
Loss after 321353520 batches: 0.0209
trigger times: 19
Loss after 321484620 batches: 0.0215
trigger times: 20
Early stopping!
Start to test process.
Loss after 321615720 batches: 0.0221
Time to train on one home:  777.044379234314
trigger times: 0
Loss after 321746820 batches: 0.1111
trigger times: 1
Loss after 321877920 batches: 0.0350
trigger times: 2
Loss after 322009020 batches: 0.0255
trigger times: 3
Loss after 322140120 batches: 0.0219
trigger times: 0
Loss after 322271220 batches: 0.0199
trigger times: 1
Loss after 322402320 batches: 0.0187
trigger times: 2
Loss after 322533420 batches: 0.0175
trigger times: 0
Loss after 322664520 batches: 0.0168
trigger times: 1
Loss after 322795620 batches: 0.0163
trigger times: 2
Loss after 322926720 batches: 0.0156
trigger times: 3
Loss after 323057820 batches: 0.0147
trigger times: 4
Loss after 323188920 batches: 0.0146
trigger times: 5
Loss after 323320020 batches: 0.0142
trigger times: 6
Loss after 323451120 batches: 0.0135
trigger times: 0
Loss after 323582220 batches: 0.0134
trigger times: 1
Loss after 323713320 batches: 0.0131
trigger times: 2
Loss after 323844420 batches: 0.0129
trigger times: 3
Loss after 323975520 batches: 0.0127
trigger times: 4
Loss after 324106620 batches: 0.0124
trigger times: 5
Loss after 324237720 batches: 0.0122
trigger times: 6
Loss after 324368820 batches: 0.0121
trigger times: 7
Loss after 324499920 batches: 0.0119
trigger times: 8
Loss after 324631020 batches: 0.0116
trigger times: 0
Loss after 324762120 batches: 0.0117
trigger times: 1
Loss after 324893220 batches: 0.0113
trigger times: 2
Loss after 325024320 batches: 0.0116
trigger times: 0
Loss after 325155420 batches: 0.0113
trigger times: 1
Loss after 325286520 batches: 0.0111
trigger times: 2
Loss after 325417620 batches: 0.0109
trigger times: 3
Loss after 325548720 batches: 0.0106
trigger times: 0
Loss after 325679820 batches: 0.0107
trigger times: 0
Loss after 325810920 batches: 0.0109
trigger times: 1
Loss after 325942020 batches: 0.0105
trigger times: 0
Loss after 326073120 batches: 0.0104
trigger times: 1
Loss after 326204220 batches: 0.0105
trigger times: 2
Loss after 326335320 batches: 0.0103
trigger times: 3
Loss after 326466420 batches: 0.0103
trigger times: 4
Loss after 326597520 batches: 0.0102
trigger times: 5
Loss after 326728620 batches: 0.0100
trigger times: 6
Loss after 326859720 batches: 0.0103
trigger times: 7
Loss after 326990820 batches: 0.0097
trigger times: 8
Loss after 327121920 batches: 0.0097
trigger times: 9
Loss after 327253020 batches: 0.0098
trigger times: 10
Loss after 327384120 batches: 0.0097
trigger times: 11
Loss after 327515220 batches: 0.0097
trigger times: 12
Loss after 327646320 batches: 0.0098
trigger times: 13
Loss after 327777420 batches: 0.0095
trigger times: 0
Loss after 327908520 batches: 0.0094
trigger times: 1
Loss after 328039620 batches: 0.0092
trigger times: 2
Loss after 328170720 batches: 0.0094
trigger times: 3
Loss after 328301820 batches: 0.0093
trigger times: 4
Loss after 328432920 batches: 0.0091
trigger times: 5
Loss after 328564020 batches: 0.0093
trigger times: 6
Loss after 328695120 batches: 0.0092
trigger times: 7
Loss after 328826220 batches: 0.0090
trigger times: 8
Loss after 328957320 batches: 0.0089
trigger times: 9
Loss after 329088420 batches: 0.0088
trigger times: 0
Loss after 329219520 batches: 0.0089
trigger times: 1
Loss after 329350620 batches: 0.0088
trigger times: 2
Loss after 329481720 batches: 0.0088
trigger times: 3
Loss after 329612820 batches: 0.0089
trigger times: 4
Loss after 329743920 batches: 0.0086
trigger times: 5
Loss after 329875020 batches: 0.0087
trigger times: 6
Loss after 330006120 batches: 0.0090
trigger times: 0
Loss after 330137220 batches: 0.0085
trigger times: 1
Loss after 330268320 batches: 0.0085
trigger times: 2
Loss after 330399420 batches: 0.0086
trigger times: 3
Loss after 330530520 batches: 0.0086
trigger times: 4
Loss after 330661620 batches: 0.0083
trigger times: 5
Loss after 330792720 batches: 0.0082
trigger times: 6
Loss after 330923820 batches: 0.0083
trigger times: 7
Loss after 331054920 batches: 0.0082
trigger times: 8
Loss after 331186020 batches: 0.0083
trigger times: 9
Loss after 331317120 batches: 0.0081
trigger times: 10
Loss after 331448220 batches: 0.0081
trigger times: 11
Loss after 331579320 batches: 0.0081
trigger times: 12
Loss after 331710420 batches: 0.0080
trigger times: 13
Loss after 331841520 batches: 0.0083
trigger times: 14
Loss after 331972620 batches: 0.0080
trigger times: 15
Loss after 332103720 batches: 0.0080
trigger times: 16
Loss after 332234820 batches: 0.0079
trigger times: 17
Loss after 332365920 batches: 0.0079
trigger times: 18
Loss after 332497020 batches: 0.0078
trigger times: 19
Loss after 332628120 batches: 0.0081
trigger times: 20
Early stopping!
Start to test process.
Loss after 332759220 batches: 0.0078
Time to train on one home:  620.6167268753052
trigger times: 0
Loss after 332837820 batches: 0.4131
trigger times: 0
Loss after 332916420 batches: 0.1210
trigger times: 0
Loss after 332995020 batches: 0.0711
trigger times: 0
Loss after 333073620 batches: 0.0563
trigger times: 1
Loss after 333152220 batches: 0.0499
trigger times: 0
Loss after 333230820 batches: 0.0446
trigger times: 1
Loss after 333309420 batches: 0.0412
trigger times: 0
Loss after 333388020 batches: 0.0385
trigger times: 1
Loss after 333466620 batches: 0.0362
trigger times: 2
Loss after 333545220 batches: 0.0338
trigger times: 3
Loss after 333623820 batches: 0.0329
trigger times: 4
Loss after 333702420 batches: 0.0316
trigger times: 5
Loss after 333781020 batches: 0.0320
trigger times: 6
Loss after 333859620 batches: 0.0298
trigger times: 7
Loss after 333938220 batches: 0.0293
trigger times: 8
Loss after 334016820 batches: 0.0289
trigger times: 9
Loss after 334095420 batches: 0.0281
trigger times: 10
Loss after 334174020 batches: 0.0274
trigger times: 11
Loss after 334252620 batches: 0.0271
trigger times: 12
Loss after 334331220 batches: 0.0271
trigger times: 13
Loss after 334409820 batches: 0.0261
trigger times: 14
Loss after 334488420 batches: 0.0254
trigger times: 15
Loss after 334567020 batches: 0.0247
trigger times: 16
Loss after 334645620 batches: 0.0248
trigger times: 17
Loss after 334724220 batches: 0.0240
trigger times: 18
Loss after 334802820 batches: 0.0236
trigger times: 19
Loss after 334881420 batches: 0.0246
trigger times: 20
Early stopping!
Start to test process.
Loss after 334960020 batches: 0.0233
Time to train on one home:  140.87471103668213
trigger times: 0
Loss after 335091120 batches: 0.1254
trigger times: 0
Loss after 335222220 batches: 0.0400
trigger times: 0
Loss after 335353320 batches: 0.0294
trigger times: 0
Loss after 335484420 batches: 0.0255
trigger times: 1
Loss after 335615520 batches: 0.0229
trigger times: 2
Loss after 335746620 batches: 0.0206
trigger times: 3
Loss after 335877720 batches: 0.0200
trigger times: 4
Loss after 336008820 batches: 0.0185
trigger times: 5
Loss after 336139920 batches: 0.0180
trigger times: 6
Loss after 336271020 batches: 0.0169
trigger times: 7
Loss after 336402120 batches: 0.0168
trigger times: 8
Loss after 336533220 batches: 0.0164
trigger times: 9
Loss after 336664320 batches: 0.0160
trigger times: 10
Loss after 336795420 batches: 0.0156
trigger times: 11
Loss after 336926520 batches: 0.0153
trigger times: 12
Loss after 337057620 batches: 0.0145
trigger times: 13
Loss after 337188720 batches: 0.0145
trigger times: 14
Loss after 337319820 batches: 0.0142
trigger times: 15
Loss after 337450920 batches: 0.0140
trigger times: 16
Loss after 337582020 batches: 0.0140
trigger times: 17
Loss after 337713120 batches: 0.0133
trigger times: 18
Loss after 337844220 batches: 0.0133
trigger times: 19
Loss after 337975320 batches: 0.0131
trigger times: 20
Early stopping!
Start to test process.
Loss after 338106420 batches: 0.0132
Time to train on one home:  183.17277097702026
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237]]
Round_6_results:  [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237]
trigger times: 0
Loss after 338237520 batches: 0.1358
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2715 < 2716; dropping {'Training_Loss': 0.13578883235184652, 'Validation_Loss': 0.1752877417537901, 'Training_R2': 0.8634492206547532, 'Validation_R2': 0.8369619521265496, 'Training_F1': 0.8049348317096119, 'Validation_F1': 0.7221027351642275, 'Training_NEP': 0.3898130060432625, 'Validation_NEP': 0.5214211621458409, 'Training_NDE': 0.10251109383456627, 'Validation_NDE': 0.12983490692089975, 'Training_MAE': 12.90983367759885, 'Validation_MAE': 14.299531509927524, 'Training_MSE': 451.03268, 'Validation_MSE': 479.47675}.
trigger times: 0
Loss after 338368620 batches: 0.0403
trigger times: 1
Loss after 338499720 batches: 0.0286
trigger times: 0
Loss after 338630820 batches: 0.0237
trigger times: 0
Loss after 338761920 batches: 0.0217
trigger times: 1
Loss after 338893020 batches: 0.0202
trigger times: 2
Loss after 339024120 batches: 0.0194
trigger times: 3
Loss after 339155220 batches: 0.0181
trigger times: 4
Loss after 339286320 batches: 0.0178
trigger times: 5
Loss after 339417420 batches: 0.0171
trigger times: 6
Loss after 339548520 batches: 0.0163
trigger times: 7
Loss after 339679620 batches: 0.0158
trigger times: 0
Loss after 339810720 batches: 0.0156
trigger times: 1
Loss after 339941820 batches: 0.0151
trigger times: 2
Loss after 340072920 batches: 0.0148
trigger times: 3
Loss after 340204020 batches: 0.0146
trigger times: 4
Loss after 340335120 batches: 0.0144
trigger times: 5
Loss after 340466220 batches: 0.0139
trigger times: 6
Loss after 340597320 batches: 0.0140
trigger times: 7
Loss after 340728420 batches: 0.0136
trigger times: 8
Loss after 340859520 batches: 0.0137
trigger times: 9
Loss after 340990620 batches: 0.0133
trigger times: 10
Loss after 341121720 batches: 0.0132
trigger times: 11
Loss after 341252820 batches: 0.0134
trigger times: 12
Loss after 341383920 batches: 0.0130
trigger times: 13
Loss after 341515020 batches: 0.0126
trigger times: 14
Loss after 341646120 batches: 0.0125
trigger times: 15
Loss after 341777220 batches: 0.0121
trigger times: 16
Loss after 341908320 batches: 0.0124
trigger times: 17
Loss after 342039420 batches: 0.0121
trigger times: 18
Loss after 342170520 batches: 0.0118
trigger times: 19
Loss after 342301620 batches: 0.0119
trigger times: 20
Early stopping!
Start to test process.
Loss after 342432720 batches: 0.0115
Time to train on one home:  247.37770915031433
trigger times: 0
Loss after 342535320 batches: 0.3265
trigger times: 1
Loss after 342637920 batches: 0.1145
trigger times: 2
Loss after 342740520 batches: 0.0825
trigger times: 3
Loss after 342843120 batches: 0.0622
trigger times: 4
Loss after 342945720 batches: 0.0530
trigger times: 5
Loss after 343048320 batches: 0.0470
trigger times: 6
Loss after 343150920 batches: 0.0410
trigger times: 7
Loss after 343253520 batches: 0.0441
trigger times: 0
Loss after 343356120 batches: 0.0372
trigger times: 0
Loss after 343458720 batches: 0.0334
trigger times: 1
Loss after 343561320 batches: 0.0314
trigger times: 2
Loss after 343663920 batches: 0.0302
trigger times: 3
Loss after 343766520 batches: 0.0294
trigger times: 0
Loss after 343869120 batches: 0.0285
trigger times: 0
Loss after 343971720 batches: 0.0281
trigger times: 1
Loss after 344074320 batches: 0.0280
trigger times: 2
Loss after 344176920 batches: 0.0256
trigger times: 3
Loss after 344279520 batches: 0.0258
trigger times: 4
Loss after 344382120 batches: 0.0273
trigger times: 5
Loss after 344484720 batches: 0.0265
trigger times: 6
Loss after 344587320 batches: 0.0238
trigger times: 7
Loss after 344689920 batches: 0.0257
trigger times: 8
Loss after 344792520 batches: 0.0236
trigger times: 9
Loss after 344895120 batches: 0.0248
trigger times: 10
Loss after 344997720 batches: 0.0255
trigger times: 11
Loss after 345100320 batches: 0.0233
trigger times: 12
Loss after 345202920 batches: 0.0229
trigger times: 13
Loss after 345305520 batches: 0.0226
trigger times: 14
Loss after 345408120 batches: 0.0233
trigger times: 15
Loss after 345510720 batches: 0.0219
trigger times: 16
Loss after 345613320 batches: 0.0219
trigger times: 17
Loss after 345715920 batches: 0.0213
trigger times: 18
Loss after 345818520 batches: 0.0213
trigger times: 19
Loss after 345921120 batches: 0.0217
trigger times: 20
Early stopping!
Start to test process.
Loss after 346023720 batches: 0.0215
Time to train on one home:  212.70896697044373
trigger times: 0
Loss after 346154820 batches: 0.2562
trigger times: 0
Loss after 346285920 batches: 0.0861
trigger times: 1
Loss after 346417020 batches: 0.0580
trigger times: 0
Loss after 346548120 batches: 0.0462
trigger times: 1
Loss after 346679220 batches: 0.0410
trigger times: 2
Loss after 346810320 batches: 0.0367
trigger times: 3
Loss after 346941420 batches: 0.0340
trigger times: 4
Loss after 347072520 batches: 0.0324
trigger times: 5
Loss after 347203620 batches: 0.0312
trigger times: 0
Loss after 347334720 batches: 0.0295
trigger times: 1
Loss after 347465820 batches: 0.0288
trigger times: 2
Loss after 347596920 batches: 0.0273
trigger times: 3
Loss after 347728020 batches: 0.0266
trigger times: 4
Loss after 347859120 batches: 0.0260
trigger times: 5
Loss after 347990220 batches: 0.0257
trigger times: 6
Loss after 348121320 batches: 0.0249
trigger times: 7
Loss after 348252420 batches: 0.0241
trigger times: 8
Loss after 348383520 batches: 0.0240
trigger times: 9
Loss after 348514620 batches: 0.0234
trigger times: 10
Loss after 348645720 batches: 0.0228
trigger times: 11
Loss after 348776820 batches: 0.0224
trigger times: 12
Loss after 348907920 batches: 0.0220
trigger times: 13
Loss after 349039020 batches: 0.0217
trigger times: 14
Loss after 349170120 batches: 0.0216
trigger times: 15
Loss after 349301220 batches: 0.0213
trigger times: 16
Loss after 349432320 batches: 0.0208
trigger times: 17
Loss after 349563420 batches: 0.0207
trigger times: 18
Loss after 349694520 batches: 0.0203
trigger times: 19
Loss after 349825620 batches: 0.0203
trigger times: 20
Early stopping!
Start to test process.
Loss after 349956720 batches: 0.0200
Time to train on one home:  226.67571687698364
trigger times: 0
Loss after 350087820 batches: 0.3473
trigger times: 0
Loss after 350218920 batches: 0.1098
trigger times: 0
Loss after 350350020 batches: 0.0761
trigger times: 0
Loss after 350481120 batches: 0.0621
trigger times: 1
Loss after 350612220 batches: 0.0549
trigger times: 2
Loss after 350743320 batches: 0.0493
trigger times: 3
Loss after 350874420 batches: 0.0462
trigger times: 4
Loss after 351005520 batches: 0.0433
trigger times: 0
Loss after 351136620 batches: 0.0413
trigger times: 1
Loss after 351267720 batches: 0.0395
trigger times: 0
Loss after 351398820 batches: 0.0379
trigger times: 1
Loss after 351529920 batches: 0.0365
trigger times: 0
Loss after 351661020 batches: 0.0353
trigger times: 0
Loss after 351792120 batches: 0.0352
trigger times: 1
Loss after 351923220 batches: 0.0337
trigger times: 0
Loss after 352054320 batches: 0.0334
trigger times: 1
Loss after 352185420 batches: 0.0324
trigger times: 2
Loss after 352316520 batches: 0.0317
trigger times: 3
Loss after 352447620 batches: 0.0314
trigger times: 4
Loss after 352578720 batches: 0.0308
trigger times: 5
Loss after 352709820 batches: 0.0303
trigger times: 6
Loss after 352840920 batches: 0.0296
trigger times: 7
Loss after 352972020 batches: 0.0293
trigger times: 8
Loss after 353103120 batches: 0.0289
trigger times: 9
Loss after 353234220 batches: 0.0287
trigger times: 10
Loss after 353365320 batches: 0.0280
trigger times: 11
Loss after 353496420 batches: 0.0279
trigger times: 12
Loss after 353627520 batches: 0.0272
trigger times: 13
Loss after 353758620 batches: 0.0271
trigger times: 14
Loss after 353889720 batches: 0.0267
trigger times: 15
Loss after 354020820 batches: 0.0266
trigger times: 16
Loss after 354151920 batches: 0.0266
trigger times: 17
Loss after 354283020 batches: 0.0265
trigger times: 18
Loss after 354414120 batches: 0.0262
trigger times: 19
Loss after 354545220 batches: 0.0257
trigger times: 20
Early stopping!
Start to test process.
Loss after 354676320 batches: 0.0257
Time to train on one home:  270.09588408470154
trigger times: 0
Loss after 354804960 batches: 0.1735
trigger times: 0
Loss after 354933600 batches: 0.0502
trigger times: 0
Loss after 355062240 batches: 0.0357
trigger times: 0
Loss after 355190880 batches: 0.0295
trigger times: 1
Loss after 355319520 batches: 0.0263
trigger times: 2
Loss after 355448160 batches: 0.0246
trigger times: 3
Loss after 355576800 batches: 0.0230
trigger times: 0
Loss after 355705440 batches: 0.0220
trigger times: 1
Loss after 355834080 batches: 0.0213
trigger times: 0
Loss after 355962720 batches: 0.0202
trigger times: 0
Loss after 356091360 batches: 0.0194
trigger times: 1
Loss after 356220000 batches: 0.0192
trigger times: 2
Loss after 356348640 batches: 0.0182
trigger times: 3
Loss after 356477280 batches: 0.0177
trigger times: 4
Loss after 356605920 batches: 0.0175
trigger times: 0
Loss after 356734560 batches: 0.0171
trigger times: 1
Loss after 356863200 batches: 0.0167
trigger times: 2
Loss after 356991840 batches: 0.0166
trigger times: 3
Loss after 357120480 batches: 0.0166
trigger times: 4
Loss after 357249120 batches: 0.0159
trigger times: 5
Loss after 357377760 batches: 0.0159
trigger times: 6
Loss after 357506400 batches: 0.0160
trigger times: 7
Loss after 357635040 batches: 0.0158
trigger times: 8
Loss after 357763680 batches: 0.0152
trigger times: 9
Loss after 357892320 batches: 0.0151
trigger times: 10
Loss after 358020960 batches: 0.0149
trigger times: 11
Loss after 358149600 batches: 0.0144
trigger times: 12
Loss after 358278240 batches: 0.0143
trigger times: 13
Loss after 358406880 batches: 0.0143
trigger times: 0
Loss after 358535520 batches: 0.0141
trigger times: 0
Loss after 358664160 batches: 0.0140
trigger times: 1
Loss after 358792800 batches: 0.0140
trigger times: 2
Loss after 358921440 batches: 0.0139
trigger times: 3
Loss after 359050080 batches: 0.0137
trigger times: 4
Loss after 359178720 batches: 0.0136
trigger times: 5
Loss after 359307360 batches: 0.0136
trigger times: 6
Loss after 359436000 batches: 0.0137
trigger times: 7
Loss after 359564640 batches: 0.0134
trigger times: 8
Loss after 359693280 batches: 0.0135
trigger times: 0
Loss after 359821920 batches: 0.0133
trigger times: 1
Loss after 359950560 batches: 0.0131
trigger times: 2
Loss after 360079200 batches: 0.0130
trigger times: 3
Loss after 360207840 batches: 0.0129
trigger times: 4
Loss after 360336480 batches: 0.0128
trigger times: 5
Loss after 360465120 batches: 0.0127
trigger times: 6
Loss after 360593760 batches: 0.0127
trigger times: 7
Loss after 360722400 batches: 0.0126
trigger times: 0
Loss after 360851040 batches: 0.0128
trigger times: 1
Loss after 360979680 batches: 0.0125
trigger times: 2
Loss after 361108320 batches: 0.0125
trigger times: 3
Loss after 361236960 batches: 0.0121
trigger times: 4
Loss after 361365600 batches: 0.0124
trigger times: 0
Loss after 361494240 batches: 0.0125
trigger times: 1
Loss after 361622880 batches: 0.0121
trigger times: 2
Loss after 361751520 batches: 0.0120
trigger times: 3
Loss after 361880160 batches: 0.0122
trigger times: 4
Loss after 362008800 batches: 0.0118
trigger times: 5
Loss after 362137440 batches: 0.0119
trigger times: 6
Loss after 362266080 batches: 0.0118
trigger times: 7
Loss after 362394720 batches: 0.0117
trigger times: 0
Loss after 362523360 batches: 0.0118
trigger times: 1
Loss after 362652000 batches: 0.0120
trigger times: 2
Loss after 362780640 batches: 0.0119
trigger times: 3
Loss after 362909280 batches: 0.0115
trigger times: 4
Loss after 363037920 batches: 0.0112
trigger times: 5
Loss after 363166560 batches: 0.0114
trigger times: 6
Loss after 363295200 batches: 0.0111
trigger times: 7
Loss after 363423840 batches: 0.0112
trigger times: 8
Loss after 363552480 batches: 0.0114
trigger times: 9
Loss after 363681120 batches: 0.0111
trigger times: 0
Loss after 363809760 batches: 0.0108
trigger times: 0
Loss after 363938400 batches: 0.0111
trigger times: 1
Loss after 364067040 batches: 0.0112
trigger times: 2
Loss after 364195680 batches: 0.0109
trigger times: 3
Loss after 364324320 batches: 0.0112
trigger times: 4
Loss after 364452960 batches: 0.0113
trigger times: 5
Loss after 364581600 batches: 0.0110
trigger times: 6
Loss after 364710240 batches: 0.0109
trigger times: 7
Loss after 364838880 batches: 0.0110
trigger times: 8
Loss after 364967520 batches: 0.0110
trigger times: 9
Loss after 365096160 batches: 0.0107
trigger times: 10
Loss after 365224800 batches: 0.0106
trigger times: 0
Loss after 365353440 batches: 0.0109
trigger times: 1
Loss after 365482080 batches: 0.0108
trigger times: 2
Loss after 365610720 batches: 0.0107
trigger times: 3
Loss after 365739360 batches: 0.0106
trigger times: 4
Loss after 365868000 batches: 0.0105
trigger times: 5
Loss after 365996640 batches: 0.0106
trigger times: 6
Loss after 366125280 batches: 0.0104
trigger times: 7
Loss after 366253920 batches: 0.0103
trigger times: 8
Loss after 366382560 batches: 0.0105
trigger times: 9
Loss after 366511200 batches: 0.0105
trigger times: 10
Loss after 366639840 batches: 0.0104
trigger times: 11
Loss after 366768480 batches: 0.0101
trigger times: 12
Loss after 366897120 batches: 0.0102
trigger times: 13
Loss after 367025760 batches: 0.0103
trigger times: 14
Loss after 367154400 batches: 0.0105
trigger times: 15
Loss after 367283040 batches: 0.0102
trigger times: 16
Loss after 367411680 batches: 0.0101
trigger times: 17
Loss after 367540320 batches: 0.0101
trigger times: 18
Loss after 367668960 batches: 0.0102
trigger times: 19
Loss after 367797600 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 367926240 batches: 0.0101
Time to train on one home:  738.853039264679
trigger times: 0
Loss after 368057340 batches: 0.3812
trigger times: 0
Loss after 368188440 batches: 0.1065
trigger times: 0
Loss after 368319540 batches: 0.0703
trigger times: 0
Loss after 368450640 batches: 0.0577
trigger times: 1
Loss after 368581740 batches: 0.0519
trigger times: 0
Loss after 368712840 batches: 0.0472
trigger times: 0
Loss after 368843940 batches: 0.0441
trigger times: 1
Loss after 368975040 batches: 0.0427
trigger times: 2
Loss after 369106140 batches: 0.0395
trigger times: 3
Loss after 369237240 batches: 0.0384
trigger times: 4
Loss after 369368340 batches: 0.0364
trigger times: 5
Loss after 369499440 batches: 0.0359
trigger times: 6
Loss after 369630540 batches: 0.0350
trigger times: 0
Loss after 369761640 batches: 0.0337
trigger times: 1
Loss after 369892740 batches: 0.0330
trigger times: 2
Loss after 370023840 batches: 0.0319
trigger times: 3
Loss after 370154940 batches: 0.0314
trigger times: 4
Loss after 370286040 batches: 0.0310
trigger times: 5
Loss after 370417140 batches: 0.0304
trigger times: 6
Loss after 370548240 batches: 0.0298
trigger times: 7
Loss after 370679340 batches: 0.0288
trigger times: 8
Loss after 370810440 batches: 0.0289
trigger times: 0
Loss after 370941540 batches: 0.0284
trigger times: 1
Loss after 371072640 batches: 0.0280
trigger times: 0
Loss after 371203740 batches: 0.0280
trigger times: 1
Loss after 371334840 batches: 0.0277
trigger times: 2
Loss after 371465940 batches: 0.0271
trigger times: 3
Loss after 371597040 batches: 0.0268
trigger times: 4
Loss after 371728140 batches: 0.0264
trigger times: 5
Loss after 371859240 batches: 0.0261
trigger times: 6
Loss after 371990340 batches: 0.0262
trigger times: 7
Loss after 372121440 batches: 0.0261
trigger times: 8
Loss after 372252540 batches: 0.0254
trigger times: 9
Loss after 372383640 batches: 0.0252
trigger times: 10
Loss after 372514740 batches: 0.0254
trigger times: 11
Loss after 372645840 batches: 0.0250
trigger times: 12
Loss after 372776940 batches: 0.0246
trigger times: 13
Loss after 372908040 batches: 0.0243
trigger times: 14
Loss after 373039140 batches: 0.0244
trigger times: 15
Loss after 373170240 batches: 0.0242
trigger times: 16
Loss after 373301340 batches: 0.0243
trigger times: 17
Loss after 373432440 batches: 0.0240
trigger times: 18
Loss after 373563540 batches: 0.0234
trigger times: 19
Loss after 373694640 batches: 0.0235
trigger times: 20
Early stopping!
Start to test process.
Loss after 373825740 batches: 0.0234
Time to train on one home:  334.15188360214233
trigger times: 0
Loss after 373956840 batches: 0.3682
trigger times: 0
Loss after 374087940 batches: 0.1357
trigger times: 0
Loss after 374219040 batches: 0.0820
trigger times: 0
Loss after 374350140 batches: 0.0617
trigger times: 0
Loss after 374481240 batches: 0.0504
trigger times: 1
Loss after 374612340 batches: 0.0456
trigger times: 2
Loss after 374743440 batches: 0.0422
trigger times: 3
Loss after 374874540 batches: 0.0420
trigger times: 0
Loss after 375005640 batches: 0.0393
trigger times: 0
Loss after 375136740 batches: 0.0373
trigger times: 1
Loss after 375267840 batches: 0.0362
trigger times: 0
Loss after 375398940 batches: 0.0346
trigger times: 0
Loss after 375530040 batches: 0.0340
trigger times: 0
Loss after 375661140 batches: 0.0339
trigger times: 1
Loss after 375792240 batches: 0.0326
trigger times: 0
Loss after 375923340 batches: 0.0321
trigger times: 1
Loss after 376054440 batches: 0.0316
trigger times: 0
Loss after 376185540 batches: 0.0303
trigger times: 1
Loss after 376316640 batches: 0.0308
trigger times: 2
Loss after 376447740 batches: 0.0301
trigger times: 3
Loss after 376578840 batches: 0.0294
trigger times: 4
Loss after 376709940 batches: 0.0292
trigger times: 5
Loss after 376841040 batches: 0.0292
trigger times: 0
Loss after 376972140 batches: 0.0286
trigger times: 1
Loss after 377103240 batches: 0.0294
trigger times: 2
Loss after 377234340 batches: 0.0288
trigger times: 3
Loss after 377365440 batches: 0.0269
trigger times: 4
Loss after 377496540 batches: 0.0284
trigger times: 5
Loss after 377627640 batches: 0.0266
trigger times: 6
Loss after 377758740 batches: 0.0259
trigger times: 0
Loss after 377889840 batches: 0.0271
trigger times: 0
Loss after 378020940 batches: 0.0287
trigger times: 1
Loss after 378152040 batches: 0.0269
trigger times: 2
Loss after 378283140 batches: 0.0279
trigger times: 0
Loss after 378414240 batches: 0.0260
trigger times: 0
Loss after 378545340 batches: 0.0250
trigger times: 1
Loss after 378676440 batches: 0.0263
trigger times: 2
Loss after 378807540 batches: 0.0253
trigger times: 3
Loss after 378938640 batches: 0.0246
trigger times: 4
Loss after 379069740 batches: 0.0267
trigger times: 5
Loss after 379200840 batches: 0.0255
trigger times: 6
Loss after 379331940 batches: 0.0261
trigger times: 7
Loss after 379463040 batches: 0.0248
trigger times: 8
Loss after 379594140 batches: 0.0247
trigger times: 9
Loss after 379725240 batches: 0.0245
trigger times: 10
Loss after 379856340 batches: 0.0258
trigger times: 11
Loss after 379987440 batches: 0.0250
trigger times: 12
Loss after 380118540 batches: 0.0244
trigger times: 13
Loss after 380249640 batches: 0.0246
trigger times: 14
Loss after 380380740 batches: 0.0252
trigger times: 15
Loss after 380511840 batches: 0.0229
trigger times: 16
Loss after 380642940 batches: 0.0234
trigger times: 17
Loss after 380774040 batches: 0.0230
trigger times: 18
Loss after 380905140 batches: 0.0229
trigger times: 19
Loss after 381036240 batches: 0.0236
trigger times: 20
Early stopping!
Start to test process.
Loss after 381167340 batches: 0.0230
Time to train on one home:  413.66529417037964
trigger times: 0
Loss after 381298440 batches: 0.1017
trigger times: 1
Loss after 381429540 batches: 0.0321
trigger times: 2
Loss after 381560640 batches: 0.0237
trigger times: 3
Loss after 381691740 batches: 0.0198
trigger times: 4
Loss after 381822840 batches: 0.0176
trigger times: 5
Loss after 381953940 batches: 0.0160
trigger times: 6
Loss after 382085040 batches: 0.0155
trigger times: 7
Loss after 382216140 batches: 0.0150
trigger times: 8
Loss after 382347240 batches: 0.0141
trigger times: 9
Loss after 382478340 batches: 0.0134
trigger times: 10
Loss after 382609440 batches: 0.0131
trigger times: 11
Loss after 382740540 batches: 0.0125
trigger times: 12
Loss after 382871640 batches: 0.0126
trigger times: 13
Loss after 383002740 batches: 0.0120
trigger times: 14
Loss after 383133840 batches: 0.0120
trigger times: 15
Loss after 383264940 batches: 0.0115
trigger times: 16
Loss after 383396040 batches: 0.0116
trigger times: 17
Loss after 383527140 batches: 0.0112
trigger times: 18
Loss after 383658240 batches: 0.0110
trigger times: 19
Loss after 383789340 batches: 0.0108
trigger times: 20
Early stopping!
Start to test process.
Loss after 383920440 batches: 0.0106
Time to train on one home:  162.37324595451355
trigger times: 0
Loss after 383999040 batches: 0.4427
trigger times: 0
Loss after 384077640 batches: 0.1257
trigger times: 0
Loss after 384156240 batches: 0.0708
trigger times: 0
Loss after 384234840 batches: 0.0536
trigger times: 1
Loss after 384313440 batches: 0.0462
trigger times: 2
Loss after 384392040 batches: 0.0417
trigger times: 3
Loss after 384470640 batches: 0.0388
trigger times: 0
Loss after 384549240 batches: 0.0356
trigger times: 1
Loss after 384627840 batches: 0.0340
trigger times: 2
Loss after 384706440 batches: 0.0321
trigger times: 3
Loss after 384785040 batches: 0.0308
trigger times: 0
Loss after 384863640 batches: 0.0300
trigger times: 1
Loss after 384942240 batches: 0.0287
trigger times: 2
Loss after 385020840 batches: 0.0281
trigger times: 3
Loss after 385099440 batches: 0.0275
trigger times: 4
Loss after 385178040 batches: 0.0270
trigger times: 5
Loss after 385256640 batches: 0.0269
trigger times: 0
Loss after 385335240 batches: 0.0261
trigger times: 1
Loss after 385413840 batches: 0.0249
trigger times: 2
Loss after 385492440 batches: 0.0245
trigger times: 3
Loss after 385571040 batches: 0.0243
trigger times: 4
Loss after 385649640 batches: 0.0237
trigger times: 0
Loss after 385728240 batches: 0.0233
trigger times: 1
Loss after 385806840 batches: 0.0234
trigger times: 0
Loss after 385885440 batches: 0.0224
trigger times: 1
Loss after 385964040 batches: 0.0222
trigger times: 2
Loss after 386042640 batches: 0.0225
trigger times: 3
Loss after 386121240 batches: 0.0218
trigger times: 4
Loss after 386199840 batches: 0.0221
trigger times: 5
Loss after 386278440 batches: 0.0214
trigger times: 6
Loss after 386357040 batches: 0.0208
trigger times: 7
Loss after 386435640 batches: 0.0206
trigger times: 0
Loss after 386514240 batches: 0.0208
trigger times: 1
Loss after 386592840 batches: 0.0206
trigger times: 2
Loss after 386671440 batches: 0.0202
trigger times: 3
Loss after 386750040 batches: 0.0198
trigger times: 4
Loss after 386828640 batches: 0.0193
trigger times: 5
Loss after 386907240 batches: 0.0194
trigger times: 6
Loss after 386985840 batches: 0.0195
trigger times: 7
Loss after 387064440 batches: 0.0193
trigger times: 8
Loss after 387143040 batches: 0.0193
trigger times: 0
Loss after 387221640 batches: 0.0190
trigger times: 0
Loss after 387300240 batches: 0.0187
trigger times: 1
Loss after 387378840 batches: 0.0196
trigger times: 2
Loss after 387457440 batches: 0.0190
trigger times: 3
Loss after 387536040 batches: 0.0189
trigger times: 0
Loss after 387614640 batches: 0.0185
trigger times: 1
Loss after 387693240 batches: 0.0181
trigger times: 2
Loss after 387771840 batches: 0.0182
trigger times: 3
Loss after 387850440 batches: 0.0182
trigger times: 4
Loss after 387929040 batches: 0.0179
trigger times: 5
Loss after 388007640 batches: 0.0177
trigger times: 6
Loss after 388086240 batches: 0.0173
trigger times: 0
Loss after 388164840 batches: 0.0178
trigger times: 1
Loss after 388243440 batches: 0.0183
trigger times: 2
Loss after 388322040 batches: 0.0175
trigger times: 3
Loss after 388400640 batches: 0.0172
trigger times: 4
Loss after 388479240 batches: 0.0175
trigger times: 5
Loss after 388557840 batches: 0.0169
trigger times: 6
Loss after 388636440 batches: 0.0171
trigger times: 7
Loss after 388715040 batches: 0.0171
trigger times: 8
Loss after 388793640 batches: 0.0175
trigger times: 9
Loss after 388872240 batches: 0.0166
trigger times: 10
Loss after 388950840 batches: 0.0169
trigger times: 11
Loss after 389029440 batches: 0.0169
trigger times: 12
Loss after 389108040 batches: 0.0170
trigger times: 13
Loss after 389186640 batches: 0.0166
trigger times: 14
Loss after 389265240 batches: 0.0163
trigger times: 15
Loss after 389343840 batches: 0.0164
trigger times: 16
Loss after 389422440 batches: 0.0164
trigger times: 17
Loss after 389501040 batches: 0.0160
trigger times: 18
Loss after 389579640 batches: 0.0161
trigger times: 19
Loss after 389658240 batches: 0.0167
trigger times: 20
Early stopping!
Start to test process.
Loss after 389736840 batches: 0.0170
Time to train on one home:  353.4545021057129
trigger times: 0
Loss after 389867940 batches: 0.1218
trigger times: 1
Loss after 389999040 batches: 0.0372
trigger times: 2
Loss after 390130140 batches: 0.0266
trigger times: 3
Loss after 390261240 batches: 0.0227
trigger times: 4
Loss after 390392340 batches: 0.0208
trigger times: 0
Loss after 390523440 batches: 0.0194
trigger times: 1
Loss after 390654540 batches: 0.0181
trigger times: 2
Loss after 390785640 batches: 0.0175
trigger times: 3
Loss after 390916740 batches: 0.0170
trigger times: 4
Loss after 391047840 batches: 0.0161
trigger times: 5
Loss after 391178940 batches: 0.0154
trigger times: 6
Loss after 391310040 batches: 0.0152
trigger times: 7
Loss after 391441140 batches: 0.0151
trigger times: 8
Loss after 391572240 batches: 0.0143
trigger times: 9
Loss after 391703340 batches: 0.0141
trigger times: 10
Loss after 391834440 batches: 0.0137
trigger times: 11
Loss after 391965540 batches: 0.0136
trigger times: 12
Loss after 392096640 batches: 0.0133
trigger times: 13
Loss after 392227740 batches: 0.0132
trigger times: 14
Loss after 392358840 batches: 0.0129
trigger times: 15
Loss after 392489940 batches: 0.0131
trigger times: 16
Loss after 392621040 batches: 0.0124
trigger times: 17
Loss after 392752140 batches: 0.0124
trigger times: 18
Loss after 392883240 batches: 0.0122
trigger times: 19
Loss after 393014340 batches: 0.0119
trigger times: 20
Early stopping!
Start to test process.
Loss after 393145440 batches: 0.0121
Time to train on one home:  198.80576825141907
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165]]
Round_7_results:  [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3174 < 3175; dropping {'Training_Loss': 0.1269425538913259, 'Validation_Loss': 0.19028794020414352, 'Training_R2': 0.8722166765341677, 'Validation_R2': 0.8230073284944626, 'Training_F1': 0.8135932542525761, 'Validation_F1': 0.7371726949658796, 'Training_NEP': 0.3724429764397368, 'Validation_NEP': 0.5072066383598423, 'Training_NDE': 0.09592920908330708, 'Validation_NDE': 0.14094763357593498, 'Training_MAE': 12.33457274561343, 'Validation_MAE': 13.909710295268711, 'Training_MSE': 422.07343, 'Validation_MSE': 520.51575}.
trigger times: 0
Loss after 393276540 batches: 0.1269
trigger times: 0
Loss after 393407640 batches: 0.0359
trigger times: 0
Loss after 393538740 batches: 0.0257
trigger times: 1
Loss after 393669840 batches: 0.0222
trigger times: 2
Loss after 393800940 batches: 0.0207
trigger times: 3
Loss after 393932040 batches: 0.0189
trigger times: 4
Loss after 394063140 batches: 0.0177
trigger times: 5
Loss after 394194240 batches: 0.0173
trigger times: 6
Loss after 394325340 batches: 0.0162
trigger times: 0
Loss after 394456440 batches: 0.0156
trigger times: 1
Loss after 394587540 batches: 0.0157
trigger times: 2
Loss after 394718640 batches: 0.0147
trigger times: 3
Loss after 394849740 batches: 0.0146
trigger times: 4
Loss after 394980840 batches: 0.0141
trigger times: 5
Loss after 395111940 batches: 0.0141
trigger times: 6
Loss after 395243040 batches: 0.0137
trigger times: 7
Loss after 395374140 batches: 0.0132
trigger times: 8
Loss after 395505240 batches: 0.0132
trigger times: 9
Loss after 395636340 batches: 0.0129
trigger times: 10
Loss after 395767440 batches: 0.0129
trigger times: 11
Loss after 395898540 batches: 0.0125
trigger times: 12
Loss after 396029640 batches: 0.0125
trigger times: 13
Loss after 396160740 batches: 0.0125
trigger times: 14
Loss after 396291840 batches: 0.0123
trigger times: 15
Loss after 396422940 batches: 0.0120
trigger times: 16
Loss after 396554040 batches: 0.0119
trigger times: 17
Loss after 396685140 batches: 0.0119
trigger times: 18
Loss after 396816240 batches: 0.0118
trigger times: 19
Loss after 396947340 batches: 0.0119
trigger times: 20
Early stopping!
Start to test process.
Loss after 397078440 batches: 0.0118
Time to train on one home:  228.36245226860046
trigger times: 0
Loss after 397181040 batches: 0.3018
trigger times: 1
Loss after 397283640 batches: 0.0962
trigger times: 2
Loss after 397386240 batches: 0.0697
trigger times: 3
Loss after 397488840 batches: 0.0568
trigger times: 4
Loss after 397591440 batches: 0.0519
trigger times: 5
Loss after 397694040 batches: 0.0446
trigger times: 6
Loss after 397796640 batches: 0.0378
trigger times: 7
Loss after 397899240 batches: 0.0340
trigger times: 8
Loss after 398001840 batches: 0.0331
trigger times: 9
Loss after 398104440 batches: 0.0317
trigger times: 10
Loss after 398207040 batches: 0.0293
trigger times: 0
Loss after 398309640 batches: 0.0278
trigger times: 1
Loss after 398412240 batches: 0.0280
trigger times: 2
Loss after 398514840 batches: 0.0272
trigger times: 3
Loss after 398617440 batches: 0.0280
trigger times: 4
Loss after 398720040 batches: 0.0268
trigger times: 5
Loss after 398822640 batches: 0.0264
trigger times: 0
Loss after 398925240 batches: 0.0426
trigger times: 1
Loss after 399027840 batches: 0.0479
trigger times: 2
Loss after 399130440 batches: 0.0278
trigger times: 3
Loss after 399233040 batches: 0.0260
trigger times: 4
Loss after 399335640 batches: 0.0242
trigger times: 5
Loss after 399438240 batches: 0.0246
trigger times: 6
Loss after 399540840 batches: 0.0224
trigger times: 7
Loss after 399643440 batches: 0.0231
trigger times: 8
Loss after 399746040 batches: 0.0243
trigger times: 9
Loss after 399848640 batches: 0.0223
trigger times: 10
Loss after 399951240 batches: 0.0240
trigger times: 11
Loss after 400053840 batches: 0.0267
trigger times: 12
Loss after 400156440 batches: 0.0227
trigger times: 13
Loss after 400259040 batches: 0.0236
trigger times: 14
Loss after 400361640 batches: 0.0212
trigger times: 15
Loss after 400464240 batches: 0.0219
trigger times: 16
Loss after 400566840 batches: 0.0232
trigger times: 17
Loss after 400669440 batches: 0.0218
trigger times: 18
Loss after 400772040 batches: 0.0221
trigger times: 19
Loss after 400874640 batches: 0.0213
trigger times: 20
Early stopping!
Start to test process.
Loss after 400977240 batches: 0.0222
Time to train on one home:  229.97025442123413
trigger times: 0
Loss after 401108340 batches: 0.1944
trigger times: 0
Loss after 401239440 batches: 0.0627
trigger times: 0
Loss after 401370540 batches: 0.0449
trigger times: 1
Loss after 401501640 batches: 0.0380
trigger times: 2
Loss after 401632740 batches: 0.0343
trigger times: 3
Loss after 401763840 batches: 0.0318
trigger times: 4
Loss after 401894940 batches: 0.0300
trigger times: 5
Loss after 402026040 batches: 0.0283
trigger times: 6
Loss after 402157140 batches: 0.0268
trigger times: 7
Loss after 402288240 batches: 0.0258
trigger times: 8
Loss after 402419340 batches: 0.0249
trigger times: 9
Loss after 402550440 batches: 0.0244
trigger times: 10
Loss after 402681540 batches: 0.0238
trigger times: 11
Loss after 402812640 batches: 0.0233
trigger times: 12
Loss after 402943740 batches: 0.0227
trigger times: 13
Loss after 403074840 batches: 0.0224
trigger times: 14
Loss after 403205940 batches: 0.0219
trigger times: 15
Loss after 403337040 batches: 0.0215
trigger times: 16
Loss after 403468140 batches: 0.0208
trigger times: 17
Loss after 403599240 batches: 0.0209
trigger times: 18
Loss after 403730340 batches: 0.0204
trigger times: 19
Loss after 403861440 batches: 0.0199
trigger times: 20
Early stopping!
Start to test process.
Loss after 403992540 batches: 0.0196
Time to train on one home:  176.07069325447083
trigger times: 0
Loss after 404123640 batches: 0.3257
trigger times: 0
Loss after 404254740 batches: 0.0972
trigger times: 0
Loss after 404385840 batches: 0.0675
trigger times: 1
Loss after 404516940 batches: 0.0565
trigger times: 0
Loss after 404648040 batches: 0.0513
trigger times: 1
Loss after 404779140 batches: 0.0468
trigger times: 2
Loss after 404910240 batches: 0.0437
trigger times: 3
Loss after 405041340 batches: 0.0413
trigger times: 4
Loss after 405172440 batches: 0.0388
trigger times: 5
Loss after 405303540 batches: 0.0377
trigger times: 0
Loss after 405434640 batches: 0.0362
trigger times: 0
Loss after 405565740 batches: 0.0351
trigger times: 1
Loss after 405696840 batches: 0.0341
trigger times: 2
Loss after 405827940 batches: 0.0333
trigger times: 3
Loss after 405959040 batches: 0.0326
trigger times: 4
Loss after 406090140 batches: 0.0316
trigger times: 5
Loss after 406221240 batches: 0.0312
trigger times: 6
Loss after 406352340 batches: 0.0307
trigger times: 7
Loss after 406483440 batches: 0.0301
trigger times: 0
Loss after 406614540 batches: 0.0299
trigger times: 1
Loss after 406745640 batches: 0.0296
trigger times: 0
Loss after 406876740 batches: 0.0285
trigger times: 1
Loss after 407007840 batches: 0.0290
trigger times: 2
Loss after 407138940 batches: 0.0281
trigger times: 3
Loss after 407270040 batches: 0.0277
trigger times: 4
Loss after 407401140 batches: 0.0271
trigger times: 5
Loss after 407532240 batches: 0.0272
trigger times: 6
Loss after 407663340 batches: 0.0268
trigger times: 7
Loss after 407794440 batches: 0.0263
trigger times: 8
Loss after 407925540 batches: 0.0260
trigger times: 9
Loss after 408056640 batches: 0.0258
trigger times: 10
Loss after 408187740 batches: 0.0260
trigger times: 11
Loss after 408318840 batches: 0.0252
trigger times: 0
Loss after 408449940 batches: 0.0251
trigger times: 1
Loss after 408581040 batches: 0.0257
trigger times: 2
Loss after 408712140 batches: 0.0257
trigger times: 3
Loss after 408843240 batches: 0.0250
trigger times: 4
Loss after 408974340 batches: 0.0248
trigger times: 0
Loss after 409105440 batches: 0.0244
trigger times: 1
Loss after 409236540 batches: 0.0240
trigger times: 2
Loss after 409367640 batches: 0.0240
trigger times: 3
Loss after 409498740 batches: 0.0238
trigger times: 4
Loss after 409629840 batches: 0.0234
trigger times: 5
Loss after 409760940 batches: 0.0237
trigger times: 6
Loss after 409892040 batches: 0.0231
trigger times: 7
Loss after 410023140 batches: 0.0235
trigger times: 0
Loss after 410154240 batches: 0.0229
trigger times: 1
Loss after 410285340 batches: 0.0230
trigger times: 2
Loss after 410416440 batches: 0.0230
trigger times: 3
Loss after 410547540 batches: 0.0229
trigger times: 4
Loss after 410678640 batches: 0.0226
trigger times: 5
Loss after 410809740 batches: 0.0224
trigger times: 6
Loss after 410940840 batches: 0.0225
trigger times: 0
Loss after 411071940 batches: 0.0223
trigger times: 1
Loss after 411203040 batches: 0.0225
trigger times: 2
Loss after 411334140 batches: 0.0217
trigger times: 3
Loss after 411465240 batches: 0.0221
trigger times: 4
Loss after 411596340 batches: 0.0220
trigger times: 5
Loss after 411727440 batches: 0.0217
trigger times: 6
Loss after 411858540 batches: 0.0214
trigger times: 7
Loss after 411989640 batches: 0.0216
trigger times: 8
Loss after 412120740 batches: 0.0215
trigger times: 0
Loss after 412251840 batches: 0.0213
trigger times: 1
Loss after 412382940 batches: 0.0214
trigger times: 2
Loss after 412514040 batches: 0.0212
trigger times: 3
Loss after 412645140 batches: 0.0212
trigger times: 4
Loss after 412776240 batches: 0.0210
trigger times: 5
Loss after 412907340 batches: 0.0210
trigger times: 6
Loss after 413038440 batches: 0.0208
trigger times: 7
Loss after 413169540 batches: 0.0207
trigger times: 8
Loss after 413300640 batches: 0.0204
trigger times: 9
Loss after 413431740 batches: 0.0204
trigger times: 10
Loss after 413562840 batches: 0.0205
trigger times: 11
Loss after 413693940 batches: 0.0205
trigger times: 12
Loss after 413825040 batches: 0.0205
trigger times: 13
Loss after 413956140 batches: 0.0202
trigger times: 14
Loss after 414087240 batches: 0.0204
trigger times: 15
Loss after 414218340 batches: 0.0202
trigger times: 16
Loss after 414349440 batches: 0.0199
trigger times: 17
Loss after 414480540 batches: 0.0199
trigger times: 18
Loss after 414611640 batches: 0.0198
trigger times: 19
Loss after 414742740 batches: 0.0201
trigger times: 20
Early stopping!
Start to test process.
Loss after 414873840 batches: 0.0197
Time to train on one home:  606.0130515098572
trigger times: 0
Loss after 415002480 batches: 0.1758
trigger times: 0
Loss after 415131120 batches: 0.0574
trigger times: 0
Loss after 415259760 batches: 0.0385
trigger times: 1
Loss after 415388400 batches: 0.0320
trigger times: 0
Loss after 415517040 batches: 0.0288
trigger times: 0
Loss after 415645680 batches: 0.0256
trigger times: 0
Loss after 415774320 batches: 0.0240
trigger times: 0
Loss after 415902960 batches: 0.0228
trigger times: 0
Loss after 416031600 batches: 0.0216
trigger times: 1
Loss after 416160240 batches: 0.0210
trigger times: 2
Loss after 416288880 batches: 0.0204
trigger times: 3
Loss after 416417520 batches: 0.0197
trigger times: 0
Loss after 416546160 batches: 0.0190
trigger times: 1
Loss after 416674800 batches: 0.0186
trigger times: 0
Loss after 416803440 batches: 0.0180
trigger times: 1
Loss after 416932080 batches: 0.0175
trigger times: 2
Loss after 417060720 batches: 0.0171
trigger times: 3
Loss after 417189360 batches: 0.0169
trigger times: 4
Loss after 417318000 batches: 0.0166
trigger times: 0
Loss after 417446640 batches: 0.0166
trigger times: 1
Loss after 417575280 batches: 0.0158
trigger times: 2
Loss after 417703920 batches: 0.0159
trigger times: 3
Loss after 417832560 batches: 0.0157
trigger times: 4
Loss after 417961200 batches: 0.0154
trigger times: 5
Loss after 418089840 batches: 0.0154
trigger times: 6
Loss after 418218480 batches: 0.0148
trigger times: 7
Loss after 418347120 batches: 0.0149
trigger times: 8
Loss after 418475760 batches: 0.0150
trigger times: 9
Loss after 418604400 batches: 0.0145
trigger times: 0
Loss after 418733040 batches: 0.0143
trigger times: 1
Loss after 418861680 batches: 0.0140
trigger times: 2
Loss after 418990320 batches: 0.0140
trigger times: 3
Loss after 419118960 batches: 0.0140
trigger times: 4
Loss after 419247600 batches: 0.0140
trigger times: 0
Loss after 419376240 batches: 0.0137
trigger times: 1
Loss after 419504880 batches: 0.0134
trigger times: 2
Loss after 419633520 batches: 0.0136
trigger times: 0
Loss after 419762160 batches: 0.0135
trigger times: 1
Loss after 419890800 batches: 0.0135
trigger times: 2
Loss after 420019440 batches: 0.0131
trigger times: 3
Loss after 420148080 batches: 0.0132
trigger times: 4
Loss after 420276720 batches: 0.0130
trigger times: 5
Loss after 420405360 batches: 0.0132
trigger times: 6
Loss after 420534000 batches: 0.0130
trigger times: 7
Loss after 420662640 batches: 0.0127
trigger times: 8
Loss after 420791280 batches: 0.0127
trigger times: 9
Loss after 420919920 batches: 0.0127
trigger times: 10
Loss after 421048560 batches: 0.0127
trigger times: 11
Loss after 421177200 batches: 0.0127
trigger times: 12
Loss after 421305840 batches: 0.0125
trigger times: 13
Loss after 421434480 batches: 0.0125
trigger times: 14
Loss after 421563120 batches: 0.0121
trigger times: 15
Loss after 421691760 batches: 0.0120
trigger times: 0
Loss after 421820400 batches: 0.0121
trigger times: 1
Loss after 421949040 batches: 0.0122
trigger times: 2
Loss after 422077680 batches: 0.0120
trigger times: 3
Loss after 422206320 batches: 0.0117
trigger times: 4
Loss after 422334960 batches: 0.0120
trigger times: 5
Loss after 422463600 batches: 0.0118
trigger times: 6
Loss after 422592240 batches: 0.0117
trigger times: 7
Loss after 422720880 batches: 0.0119
trigger times: 8
Loss after 422849520 batches: 0.0116
trigger times: 9
Loss after 422978160 batches: 0.0115
trigger times: 10
Loss after 423106800 batches: 0.0114
trigger times: 11
Loss after 423235440 batches: 0.0112
trigger times: 12
Loss after 423364080 batches: 0.0113
trigger times: 13
Loss after 423492720 batches: 0.0112
trigger times: 14
Loss after 423621360 batches: 0.0109
trigger times: 15
Loss after 423750000 batches: 0.0110
trigger times: 16
Loss after 423878640 batches: 0.0113
trigger times: 17
Loss after 424007280 batches: 0.0109
trigger times: 18
Loss after 424135920 batches: 0.0110
trigger times: 19
Loss after 424264560 batches: 0.0110
trigger times: 20
Early stopping!
Start to test process.
Loss after 424393200 batches: 0.0108
Time to train on one home:  534.8610603809357
trigger times: 0
Loss after 424524300 batches: 0.3611
trigger times: 0
Loss after 424655400 batches: 0.0959
trigger times: 0
Loss after 424786500 batches: 0.0648
trigger times: 0
Loss after 424917600 batches: 0.0544
trigger times: 0
Loss after 425048700 batches: 0.0485
trigger times: 0
Loss after 425179800 batches: 0.0441
trigger times: 1
Loss after 425310900 batches: 0.0416
trigger times: 2
Loss after 425442000 batches: 0.0395
trigger times: 3
Loss after 425573100 batches: 0.0374
trigger times: 4
Loss after 425704200 batches: 0.0363
trigger times: 5
Loss after 425835300 batches: 0.0354
trigger times: 6
Loss after 425966400 batches: 0.0340
trigger times: 7
Loss after 426097500 batches: 0.0325
trigger times: 8
Loss after 426228600 batches: 0.0319
trigger times: 0
Loss after 426359700 batches: 0.0311
trigger times: 0
Loss after 426490800 batches: 0.0305
trigger times: 1
Loss after 426621900 batches: 0.0303
trigger times: 2
Loss after 426753000 batches: 0.0294
trigger times: 3
Loss after 426884100 batches: 0.0293
trigger times: 4
Loss after 427015200 batches: 0.0288
trigger times: 5
Loss after 427146300 batches: 0.0283
trigger times: 6
Loss after 427277400 batches: 0.0282
trigger times: 7
Loss after 427408500 batches: 0.0276
trigger times: 8
Loss after 427539600 batches: 0.0272
trigger times: 9
Loss after 427670700 batches: 0.0267
trigger times: 10
Loss after 427801800 batches: 0.0264
trigger times: 11
Loss after 427932900 batches: 0.0262
trigger times: 12
Loss after 428064000 batches: 0.0259
trigger times: 0
Loss after 428195100 batches: 0.0255
trigger times: 1
Loss after 428326200 batches: 0.0259
trigger times: 2
Loss after 428457300 batches: 0.0251
trigger times: 3
Loss after 428588400 batches: 0.0249
trigger times: 4
Loss after 428719500 batches: 0.0249
trigger times: 5
Loss after 428850600 batches: 0.0244
trigger times: 6
Loss after 428981700 batches: 0.0242
trigger times: 7
Loss after 429112800 batches: 0.0240
trigger times: 8
Loss after 429243900 batches: 0.0242
trigger times: 9
Loss after 429375000 batches: 0.0242
trigger times: 10
Loss after 429506100 batches: 0.0237
trigger times: 11
Loss after 429637200 batches: 0.0234
trigger times: 12
Loss after 429768300 batches: 0.0231
trigger times: 13
Loss after 429899400 batches: 0.0234
trigger times: 14
Loss after 430030500 batches: 0.0227
trigger times: 15
Loss after 430161600 batches: 0.0227
trigger times: 16
Loss after 430292700 batches: 0.0227
trigger times: 17
Loss after 430423800 batches: 0.0225
trigger times: 18
Loss after 430554900 batches: 0.0223
trigger times: 19
Loss after 430686000 batches: 0.0222
trigger times: 20
Early stopping!
Start to test process.
Loss after 430817100 batches: 0.0220
Time to train on one home:  363.43101596832275
trigger times: 0
Loss after 430948200 batches: 0.3462
trigger times: 0
Loss after 431079300 batches: 0.1130
trigger times: 1
Loss after 431210400 batches: 0.0721
trigger times: 0
Loss after 431341500 batches: 0.0581
trigger times: 0
Loss after 431472600 batches: 0.0506
trigger times: 0
Loss after 431603700 batches: 0.0454
trigger times: 0
Loss after 431734800 batches: 0.0412
trigger times: 0
Loss after 431865900 batches: 0.0379
trigger times: 0
Loss after 431997000 batches: 0.0373
trigger times: 0
Loss after 432128100 batches: 0.0371
trigger times: 0
Loss after 432259200 batches: 0.0353
trigger times: 1
Loss after 432390300 batches: 0.0342
trigger times: 2
Loss after 432521400 batches: 0.0325
trigger times: 0
Loss after 432652500 batches: 0.0315
trigger times: 1
Loss after 432783600 batches: 0.0310
trigger times: 2
Loss after 432914700 batches: 0.0314
trigger times: 3
Loss after 433045800 batches: 0.0316
trigger times: 0
Loss after 433176900 batches: 0.0291
trigger times: 1
Loss after 433308000 batches: 0.0290
trigger times: 0
Loss after 433439100 batches: 0.0295
trigger times: 0
Loss after 433570200 batches: 0.0293
trigger times: 1
Loss after 433701300 batches: 0.0293
trigger times: 0
Loss after 433832400 batches: 0.0289
trigger times: 1
Loss after 433963500 batches: 0.0283
trigger times: 2
Loss after 434094600 batches: 0.0275
trigger times: 0
Loss after 434225700 batches: 0.0269
trigger times: 1
Loss after 434356800 batches: 0.0264
trigger times: 2
Loss after 434487900 batches: 0.0271
trigger times: 3
Loss after 434619000 batches: 0.0271
trigger times: 4
Loss after 434750100 batches: 0.0275
trigger times: 5
Loss after 434881200 batches: 0.0274
trigger times: 6
Loss after 435012300 batches: 0.0265
trigger times: 7
Loss after 435143400 batches: 0.0248
trigger times: 8
Loss after 435274500 batches: 0.0250
trigger times: 0
Loss after 435405600 batches: 0.0248
trigger times: 1
Loss after 435536700 batches: 0.0261
trigger times: 0
Loss after 435667800 batches: 0.0252
trigger times: 1
Loss after 435798900 batches: 0.0258
trigger times: 0
Loss after 435930000 batches: 0.0248
trigger times: 0
Loss after 436061100 batches: 0.0246
trigger times: 1
Loss after 436192200 batches: 0.0246
trigger times: 2
Loss after 436323300 batches: 0.0247
trigger times: 3
Loss after 436454400 batches: 0.0244
trigger times: 4
Loss after 436585500 batches: 0.0245
trigger times: 5
Loss after 436716600 batches: 0.0257
trigger times: 6
Loss after 436847700 batches: 0.0251
trigger times: 7
Loss after 436978800 batches: 0.0241
trigger times: 8
Loss after 437109900 batches: 0.0232
trigger times: 9
Loss after 437241000 batches: 0.0234
trigger times: 10
Loss after 437372100 batches: 0.0246
trigger times: 11
Loss after 437503200 batches: 0.0243
trigger times: 12
Loss after 437634300 batches: 0.0235
trigger times: 13
Loss after 437765400 batches: 0.0235
trigger times: 14
Loss after 437896500 batches: 0.0229
trigger times: 15
Loss after 438027600 batches: 0.0234
trigger times: 16
Loss after 438158700 batches: 0.0233
trigger times: 17
Loss after 438289800 batches: 0.0230
trigger times: 18
Loss after 438420900 batches: 0.0238
trigger times: 19
Loss after 438552000 batches: 0.0241
trigger times: 0
Loss after 438683100 batches: 0.0233
trigger times: 1
Loss after 438814200 batches: 0.0223
trigger times: 2
Loss after 438945300 batches: 0.0219
trigger times: 3
Loss after 439076400 batches: 0.0222
trigger times: 4
Loss after 439207500 batches: 0.0221
trigger times: 5
Loss after 439338600 batches: 0.0224
trigger times: 6
Loss after 439469700 batches: 0.0216
trigger times: 7
Loss after 439600800 batches: 0.0214
trigger times: 8
Loss after 439731900 batches: 0.0210
trigger times: 9
Loss after 439863000 batches: 0.0220
trigger times: 10
Loss after 439994100 batches: 0.0215
trigger times: 11
Loss after 440125200 batches: 0.0224
trigger times: 12
Loss after 440256300 batches: 0.0218
trigger times: 13
Loss after 440387400 batches: 0.0198
trigger times: 14
Loss after 440518500 batches: 0.0212
trigger times: 15
Loss after 440649600 batches: 0.0208
trigger times: 16
Loss after 440780700 batches: 0.0208
trigger times: 17
Loss after 440911800 batches: 0.0216
trigger times: 18
Loss after 441042900 batches: 0.0204
trigger times: 19
Loss after 441174000 batches: 0.0218
trigger times: 20
Early stopping!
Start to test process.
Loss after 441305100 batches: 0.0221
Time to train on one home:  584.4341309070587
trigger times: 0
Loss after 441436200 batches: 0.0946
trigger times: 0
Loss after 441567300 batches: 0.0294
trigger times: 0
Loss after 441698400 batches: 0.0217
trigger times: 1
Loss after 441829500 batches: 0.0185
trigger times: 2
Loss after 441960600 batches: 0.0166
trigger times: 0
Loss after 442091700 batches: 0.0156
trigger times: 1
Loss after 442222800 batches: 0.0148
trigger times: 2
Loss after 442353900 batches: 0.0141
trigger times: 3
Loss after 442485000 batches: 0.0136
trigger times: 4
Loss after 442616100 batches: 0.0130
trigger times: 5
Loss after 442747200 batches: 0.0125
trigger times: 6
Loss after 442878300 batches: 0.0126
trigger times: 7
Loss after 443009400 batches: 0.0121
trigger times: 8
Loss after 443140500 batches: 0.0116
trigger times: 9
Loss after 443271600 batches: 0.0114
trigger times: 10
Loss after 443402700 batches: 0.0115
trigger times: 11
Loss after 443533800 batches: 0.0111
trigger times: 12
Loss after 443664900 batches: 0.0109
trigger times: 13
Loss after 443796000 batches: 0.0106
trigger times: 14
Loss after 443927100 batches: 0.0107
trigger times: 15
Loss after 444058200 batches: 0.0105
trigger times: 16
Loss after 444189300 batches: 0.0102
trigger times: 17
Loss after 444320400 batches: 0.0102
trigger times: 18
Loss after 444451500 batches: 0.0101
trigger times: 19
Loss after 444582600 batches: 0.0101
trigger times: 20
Early stopping!
Start to test process.
Loss after 444713700 batches: 0.0099
Time to train on one home:  198.0242829322815
trigger times: 0
Loss after 444792300 batches: 0.3236
trigger times: 1
Loss after 444870900 batches: 0.0882
trigger times: 2
Loss after 444949500 batches: 0.0540
trigger times: 0
Loss after 445028100 batches: 0.0432
trigger times: 0
Loss after 445106700 batches: 0.0372
trigger times: 1
Loss after 445185300 batches: 0.0338
trigger times: 2
Loss after 445263900 batches: 0.0323
trigger times: 3
Loss after 445342500 batches: 0.0301
trigger times: 0
Loss after 445421100 batches: 0.0286
trigger times: 1
Loss after 445499700 batches: 0.0271
trigger times: 2
Loss after 445578300 batches: 0.0257
trigger times: 0
Loss after 445656900 batches: 0.0250
trigger times: 1
Loss after 445735500 batches: 0.0252
trigger times: 2
Loss after 445814100 batches: 0.0244
trigger times: 3
Loss after 445892700 batches: 0.0243
trigger times: 4
Loss after 445971300 batches: 0.0233
trigger times: 0
Loss after 446049900 batches: 0.0226
trigger times: 1
Loss after 446128500 batches: 0.0223
trigger times: 2
Loss after 446207100 batches: 0.0220
trigger times: 3
Loss after 446285700 batches: 0.0214
trigger times: 4
Loss after 446364300 batches: 0.0217
trigger times: 5
Loss after 446442900 batches: 0.0209
trigger times: 6
Loss after 446521500 batches: 0.0206
trigger times: 7
Loss after 446600100 batches: 0.0201
trigger times: 8
Loss after 446678700 batches: 0.0200
trigger times: 9
Loss after 446757300 batches: 0.0198
trigger times: 10
Loss after 446835900 batches: 0.0196
trigger times: 11
Loss after 446914500 batches: 0.0191
trigger times: 0
Loss after 446993100 batches: 0.0188
trigger times: 1
Loss after 447071700 batches: 0.0186
trigger times: 2
Loss after 447150300 batches: 0.0181
trigger times: 3
Loss after 447228900 batches: 0.0184
trigger times: 4
Loss after 447307500 batches: 0.0181
trigger times: 5
Loss after 447386100 batches: 0.0185
trigger times: 6
Loss after 447464700 batches: 0.0176
trigger times: 7
Loss after 447543300 batches: 0.0181
trigger times: 8
Loss after 447621900 batches: 0.0177
trigger times: 0
Loss after 447700500 batches: 0.0175
trigger times: 1
Loss after 447779100 batches: 0.0175
trigger times: 0
Loss after 447857700 batches: 0.0171
trigger times: 1
Loss after 447936300 batches: 0.0170
trigger times: 2
Loss after 448014900 batches: 0.0171
trigger times: 3
Loss after 448093500 batches: 0.0172
trigger times: 4
Loss after 448172100 batches: 0.0169
trigger times: 5
Loss after 448250700 batches: 0.0168
trigger times: 6
Loss after 448329300 batches: 0.0166
trigger times: 7
Loss after 448407900 batches: 0.0159
trigger times: 8
Loss after 448486500 batches: 0.0163
trigger times: 9
Loss after 448565100 batches: 0.0163
trigger times: 10
Loss after 448643700 batches: 0.0164
trigger times: 11
Loss after 448722300 batches: 0.0165
trigger times: 12
Loss after 448800900 batches: 0.0156
trigger times: 13
Loss after 448879500 batches: 0.0158
trigger times: 14
Loss after 448958100 batches: 0.0160
trigger times: 15
Loss after 449036700 batches: 0.0154
trigger times: 0
Loss after 449115300 batches: 0.0158
trigger times: 1
Loss after 449193900 batches: 0.0152
trigger times: 2
Loss after 449272500 batches: 0.0155
trigger times: 3
Loss after 449351100 batches: 0.0154
trigger times: 4
Loss after 449429700 batches: 0.0153
trigger times: 5
Loss after 449508300 batches: 0.0151
trigger times: 6
Loss after 449586900 batches: 0.0150
trigger times: 7
Loss after 449665500 batches: 0.0150
trigger times: 8
Loss after 449744100 batches: 0.0152
trigger times: 9
Loss after 449822700 batches: 0.0147
trigger times: 10
Loss after 449901300 batches: 0.0147
trigger times: 11
Loss after 449979900 batches: 0.0148
trigger times: 12
Loss after 450058500 batches: 0.0150
trigger times: 13
Loss after 450137100 batches: 0.0147
trigger times: 14
Loss after 450215700 batches: 0.0145
trigger times: 15
Loss after 450294300 batches: 0.0153
trigger times: 16
Loss after 450372900 batches: 0.0145
trigger times: 17
Loss after 450451500 batches: 0.0143
trigger times: 18
Loss after 450530100 batches: 0.0146
trigger times: 19
Loss after 450608700 batches: 0.0143
trigger times: 20
Early stopping!
Start to test process.
Loss after 450687300 batches: 0.0144
Time to train on one home:  362.7624719142914
trigger times: 0
Loss after 450818400 batches: 0.1100
trigger times: 0
Loss after 450949500 batches: 0.0335
trigger times: 1
Loss after 451080600 batches: 0.0251
trigger times: 0
Loss after 451211700 batches: 0.0215
trigger times: 1
Loss after 451342800 batches: 0.0196
trigger times: 0
Loss after 451473900 batches: 0.0184
trigger times: 1
Loss after 451605000 batches: 0.0169
trigger times: 0
Loss after 451736100 batches: 0.0162
trigger times: 0
Loss after 451867200 batches: 0.0155
trigger times: 1
Loss after 451998300 batches: 0.0152
trigger times: 2
Loss after 452129400 batches: 0.0150
trigger times: 3
Loss after 452260500 batches: 0.0144
trigger times: 4
Loss after 452391600 batches: 0.0139
trigger times: 5
Loss after 452522700 batches: 0.0136
trigger times: 0
Loss after 452653800 batches: 0.0137
trigger times: 1
Loss after 452784900 batches: 0.0131
trigger times: 2
Loss after 452916000 batches: 0.0132
trigger times: 3
Loss after 453047100 batches: 0.0128
trigger times: 0
Loss after 453178200 batches: 0.0126
trigger times: 1
Loss after 453309300 batches: 0.0123
trigger times: 2
Loss after 453440400 batches: 0.0121
trigger times: 3
Loss after 453571500 batches: 0.0121
trigger times: 4
Loss after 453702600 batches: 0.0120
trigger times: 5
Loss after 453833700 batches: 0.0118
trigger times: 6
Loss after 453964800 batches: 0.0116
trigger times: 7
Loss after 454095900 batches: 0.0116
trigger times: 8
Loss after 454227000 batches: 0.0113
trigger times: 9
Loss after 454358100 batches: 0.0113
trigger times: 10
Loss after 454489200 batches: 0.0111
trigger times: 11
Loss after 454620300 batches: 0.0109
trigger times: 12
Loss after 454751400 batches: 0.0109
trigger times: 13
Loss after 454882500 batches: 0.0108
trigger times: 0
Loss after 455013600 batches: 0.0109
trigger times: 1
Loss after 455144700 batches: 0.0106
trigger times: 2
Loss after 455275800 batches: 0.0105
trigger times: 3
Loss after 455406900 batches: 0.0104
trigger times: 4
Loss after 455538000 batches: 0.0105
trigger times: 5
Loss after 455669100 batches: 0.0105
trigger times: 6
Loss after 455800200 batches: 0.0105
trigger times: 7
Loss after 455931300 batches: 0.0102
trigger times: 8
Loss after 456062400 batches: 0.0102
trigger times: 9
Loss after 456193500 batches: 0.0100
trigger times: 10
Loss after 456324600 batches: 0.0101
trigger times: 11
Loss after 456455700 batches: 0.0100
trigger times: 12
Loss after 456586800 batches: 0.0099
trigger times: 13
Loss after 456717900 batches: 0.0097
trigger times: 14
Loss after 456849000 batches: 0.0098
trigger times: 15
Loss after 456980100 batches: 0.0098
trigger times: 16
Loss after 457111200 batches: 0.0097
trigger times: 17
Loss after 457242300 batches: 0.0095
trigger times: 18
Loss after 457373400 batches: 0.0095
trigger times: 19
Loss after 457504500 batches: 0.0096
trigger times: 20
Early stopping!
Start to test process.
Loss after 457635600 batches: 0.0094
Time to train on one home:  391.38872504234314
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318, 0.016183730855002625]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165], [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777]]
Round_8_results:  [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3706 < 3707; dropping {'Training_Loss': 0.12111000836176693, 'Validation_Loss': 0.1765920321146647, 'Training_R2': 0.8780667630877401, 'Validation_R2': 0.8359044737924135, 'Training_F1': 0.8188797721572463, 'Validation_F1': 0.7574309764187946, 'Training_NEP': 0.3613932284546909, 'Validation_NEP': 0.47425457796021014, 'Training_NDE': 0.09153744526834286, 'Validation_NDE': 0.1306770269221769, 'Training_MAE': 11.968627006361995, 'Validation_MAE': 13.006028089386588, 'Training_MSE': 402.75046, 'Validation_MSE': 482.58664}.
trigger times: 0
Loss after 457766700 batches: 0.1211
trigger times: 1
Loss after 457897800 batches: 0.0334
trigger times: 2
Loss after 458028900 batches: 0.0251
trigger times: 0
Loss after 458160000 batches: 0.0209
trigger times: 0
Loss after 458291100 batches: 0.0189
trigger times: 0
Loss after 458422200 batches: 0.0176
trigger times: 1
Loss after 458553300 batches: 0.0168
trigger times: 2
Loss after 458684400 batches: 0.0161
trigger times: 3
Loss after 458815500 batches: 0.0153
trigger times: 4
Loss after 458946600 batches: 0.0149
trigger times: 5
Loss after 459077700 batches: 0.0150
trigger times: 6
Loss after 459208800 batches: 0.0145
trigger times: 0
Loss after 459339900 batches: 0.0139
trigger times: 1
Loss after 459471000 batches: 0.0135
trigger times: 2
Loss after 459602100 batches: 0.0135
trigger times: 3
Loss after 459733200 batches: 0.0132
trigger times: 4
Loss after 459864300 batches: 0.0129
trigger times: 5
Loss after 459995400 batches: 0.0128
trigger times: 6
Loss after 460126500 batches: 0.0122
trigger times: 7
Loss after 460257600 batches: 0.0126
trigger times: 0
Loss after 460388700 batches: 0.0122
trigger times: 1
Loss after 460519800 batches: 0.0117
trigger times: 2
Loss after 460650900 batches: 0.0118
trigger times: 3
Loss after 460782000 batches: 0.0116
trigger times: 4
Loss after 460913100 batches: 0.0118
trigger times: 5
Loss after 461044200 batches: 0.0114
trigger times: 6
Loss after 461175300 batches: 0.0115
trigger times: 7
Loss after 461306400 batches: 0.0113
trigger times: 8
Loss after 461437500 batches: 0.0109
trigger times: 9
Loss after 461568600 batches: 0.0109
trigger times: 10
Loss after 461699700 batches: 0.0108
trigger times: 11
Loss after 461830800 batches: 0.0107
trigger times: 12
Loss after 461961900 batches: 0.0107
trigger times: 13
Loss after 462093000 batches: 0.0108
trigger times: 14
Loss after 462224100 batches: 0.0104
trigger times: 15
Loss after 462355200 batches: 0.0103
trigger times: 16
Loss after 462486300 batches: 0.0101
trigger times: 17
Loss after 462617400 batches: 0.0103
trigger times: 18
Loss after 462748500 batches: 0.0103
trigger times: 19
Loss after 462879600 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 463010700 batches: 0.0102
Time to train on one home:  306.417848110199
trigger times: 0
Loss after 463113300 batches: 0.2941
trigger times: 1
Loss after 463215900 batches: 0.1031
trigger times: 2
Loss after 463318500 batches: 0.0623
trigger times: 3
Loss after 463421100 batches: 0.0477
trigger times: 4
Loss after 463523700 batches: 0.0410
trigger times: 5
Loss after 463626300 batches: 0.0366
trigger times: 6
Loss after 463728900 batches: 0.0352
trigger times: 7
Loss after 463831500 batches: 0.0351
trigger times: 8
Loss after 463934100 batches: 0.0319
trigger times: 0
Loss after 464036700 batches: 0.0314
trigger times: 1
Loss after 464139300 batches: 0.0292
trigger times: 2
Loss after 464241900 batches: 0.0291
trigger times: 0
Loss after 464344500 batches: 0.0277
trigger times: 0
Loss after 464447100 batches: 0.0262
trigger times: 1
Loss after 464549700 batches: 0.0256
trigger times: 2
Loss after 464652300 batches: 0.0247
trigger times: 0
Loss after 464754900 batches: 0.0259
trigger times: 0
Loss after 464857500 batches: 0.0257
trigger times: 1
Loss after 464960100 batches: 0.0242
trigger times: 2
Loss after 465062700 batches: 0.0229
trigger times: 3
Loss after 465165300 batches: 0.0226
trigger times: 0
Loss after 465267900 batches: 0.0226
trigger times: 1
Loss after 465370500 batches: 0.0239
trigger times: 2
Loss after 465473100 batches: 0.0238
trigger times: 3
Loss after 465575700 batches: 0.0223
trigger times: 0
Loss after 465678300 batches: 0.0215
trigger times: 1
Loss after 465780900 batches: 0.0217
trigger times: 2
Loss after 465883500 batches: 0.0236
trigger times: 3
Loss after 465986100 batches: 0.0234
trigger times: 4
Loss after 466088700 batches: 0.0238
trigger times: 5
Loss after 466191300 batches: 0.0207
trigger times: 6
Loss after 466293900 batches: 0.0203
trigger times: 7
Loss after 466396500 batches: 0.0209
trigger times: 8
Loss after 466499100 batches: 0.0201
trigger times: 9
Loss after 466601700 batches: 0.0190
trigger times: 10
Loss after 466704300 batches: 0.0193
trigger times: 11
Loss after 466806900 batches: 0.0194
trigger times: 0
Loss after 466909500 batches: 0.0232
trigger times: 1
Loss after 467012100 batches: 0.0196
trigger times: 2
Loss after 467114700 batches: 0.0197
trigger times: 3
Loss after 467217300 batches: 0.0197
trigger times: 4
Loss after 467319900 batches: 0.0198
trigger times: 5
Loss after 467422500 batches: 0.0194
trigger times: 6
Loss after 467525100 batches: 0.0190
trigger times: 0
Loss after 467627700 batches: 0.0189
trigger times: 1
Loss after 467730300 batches: 0.0183
trigger times: 2
Loss after 467832900 batches: 0.0183
trigger times: 0
Loss after 467935500 batches: 0.0192
trigger times: 1
Loss after 468038100 batches: 0.0225
trigger times: 2
Loss after 468140700 batches: 0.0203
trigger times: 3
Loss after 468243300 batches: 0.0197
trigger times: 4
Loss after 468345900 batches: 0.0186
trigger times: 5
Loss after 468448500 batches: 0.0184
trigger times: 6
Loss after 468551100 batches: 0.0180
trigger times: 7
Loss after 468653700 batches: 0.0189
trigger times: 8
Loss after 468756300 batches: 0.0178
trigger times: 9
Loss after 468858900 batches: 0.0179
trigger times: 10
Loss after 468961500 batches: 0.0191
trigger times: 11
Loss after 469064100 batches: 0.0183
trigger times: 12
Loss after 469166700 batches: 0.0181
trigger times: 13
Loss after 469269300 batches: 0.0193
trigger times: 14
Loss after 469371900 batches: 0.0181
trigger times: 15
Loss after 469474500 batches: 0.0187
trigger times: 16
Loss after 469577100 batches: 0.0174
trigger times: 17
Loss after 469679700 batches: 0.0164
trigger times: 18
Loss after 469782300 batches: 0.0175
trigger times: 19
Loss after 469884900 batches: 0.0166
trigger times: 20
Early stopping!
Start to test process.
Loss after 469987500 batches: 0.0166
Time to train on one home:  402.7186667919159
trigger times: 0
Loss after 470118600 batches: 0.1907
trigger times: 1
Loss after 470249700 batches: 0.0600
trigger times: 2
Loss after 470380800 batches: 0.0429
trigger times: 3
Loss after 470511900 batches: 0.0362
trigger times: 4
Loss after 470643000 batches: 0.0327
trigger times: 5
Loss after 470774100 batches: 0.0305
trigger times: 6
Loss after 470905200 batches: 0.0285
trigger times: 7
Loss after 471036300 batches: 0.0267
trigger times: 8
Loss after 471167400 batches: 0.0254
trigger times: 9
Loss after 471298500 batches: 0.0245
trigger times: 10
Loss after 471429600 batches: 0.0242
trigger times: 11
Loss after 471560700 batches: 0.0231
trigger times: 12
Loss after 471691800 batches: 0.0229
trigger times: 13
Loss after 471822900 batches: 0.0222
trigger times: 14
Loss after 471954000 batches: 0.0218
trigger times: 15
Loss after 472085100 batches: 0.0213
trigger times: 16
Loss after 472216200 batches: 0.0208
trigger times: 17
Loss after 472347300 batches: 0.0202
trigger times: 18
Loss after 472478400 batches: 0.0202
trigger times: 19
Loss after 472609500 batches: 0.0200
trigger times: 20
Early stopping!
Start to test process.
Loss after 472740600 batches: 0.0197
Time to train on one home:  162.16344285011292
trigger times: 0
Loss after 472871700 batches: 0.2774
trigger times: 0
Loss after 473002800 batches: 0.0847
trigger times: 0
Loss after 473133900 batches: 0.0598
trigger times: 1
Loss after 473265000 batches: 0.0498
trigger times: 0
Loss after 473396100 batches: 0.0449
trigger times: 1
Loss after 473527200 batches: 0.0412
trigger times: 2
Loss after 473658300 batches: 0.0387
trigger times: 3
Loss after 473789400 batches: 0.0365
trigger times: 4
Loss after 473920500 batches: 0.0354
trigger times: 5
Loss after 474051600 batches: 0.0342
trigger times: 6
Loss after 474182700 batches: 0.0333
trigger times: 7
Loss after 474313800 batches: 0.0316
trigger times: 8
Loss after 474444900 batches: 0.0311
trigger times: 9
Loss after 474576000 batches: 0.0301
trigger times: 10
Loss after 474707100 batches: 0.0296
trigger times: 11
Loss after 474838200 batches: 0.0289
trigger times: 12
Loss after 474969300 batches: 0.0286
trigger times: 0
Loss after 475100400 batches: 0.0277
trigger times: 1
Loss after 475231500 batches: 0.0277
trigger times: 2
Loss after 475362600 batches: 0.0274
trigger times: 3
Loss after 475493700 batches: 0.0267
trigger times: 4
Loss after 475624800 batches: 0.0264
trigger times: 5
Loss after 475755900 batches: 0.0258
trigger times: 0
Loss after 475887000 batches: 0.0257
trigger times: 1
Loss after 476018100 batches: 0.0256
trigger times: 0
Loss after 476149200 batches: 0.0251
trigger times: 1
Loss after 476280300 batches: 0.0249
trigger times: 2
Loss after 476411400 batches: 0.0247
trigger times: 3
Loss after 476542500 batches: 0.0243
trigger times: 4
Loss after 476673600 batches: 0.0245
trigger times: 5
Loss after 476804700 batches: 0.0244
trigger times: 6
Loss after 476935800 batches: 0.0243
trigger times: 7
Loss after 477066900 batches: 0.0239
trigger times: 0
Loss after 477198000 batches: 0.0236
trigger times: 1
Loss after 477329100 batches: 0.0233
trigger times: 2
Loss after 477460200 batches: 0.0237
trigger times: 0
Loss after 477591300 batches: 0.0233
trigger times: 1
Loss after 477722400 batches: 0.0228
trigger times: 2
Loss after 477853500 batches: 0.0228
trigger times: 3
Loss after 477984600 batches: 0.0230
trigger times: 4
Loss after 478115700 batches: 0.0226
trigger times: 5
Loss after 478246800 batches: 0.0225
trigger times: 6
Loss after 478377900 batches: 0.0220
trigger times: 7
Loss after 478509000 batches: 0.0221
trigger times: 8
Loss after 478640100 batches: 0.0223
trigger times: 9
Loss after 478771200 batches: 0.0215
trigger times: 0
Loss after 478902300 batches: 0.0213
trigger times: 1
Loss after 479033400 batches: 0.0217
trigger times: 2
Loss after 479164500 batches: 0.0214
trigger times: 0
Loss after 479295600 batches: 0.0211
trigger times: 1
Loss after 479426700 batches: 0.0210
trigger times: 2
Loss after 479557800 batches: 0.0212
trigger times: 3
Loss after 479688900 batches: 0.0210
trigger times: 4
Loss after 479820000 batches: 0.0211
trigger times: 5
Loss after 479951100 batches: 0.0209
trigger times: 6
Loss after 480082200 batches: 0.0207
trigger times: 0
Loss after 480213300 batches: 0.0209
trigger times: 1
Loss after 480344400 batches: 0.0209
trigger times: 2
Loss after 480475500 batches: 0.0205
trigger times: 3
Loss after 480606600 batches: 0.0204
trigger times: 4
Loss after 480737700 batches: 0.0197
trigger times: 5
Loss after 480868800 batches: 0.0205
trigger times: 6
Loss after 480999900 batches: 0.0203
trigger times: 7
Loss after 481131000 batches: 0.0200
trigger times: 0
Loss after 481262100 batches: 0.0205
trigger times: 1
Loss after 481393200 batches: 0.0200
trigger times: 2
Loss after 481524300 batches: 0.0194
trigger times: 3
Loss after 481655400 batches: 0.0194
trigger times: 4
Loss after 481786500 batches: 0.0193
trigger times: 5
Loss after 481917600 batches: 0.0192
trigger times: 6
Loss after 482048700 batches: 0.0193
trigger times: 7
Loss after 482179800 batches: 0.0194
trigger times: 8
Loss after 482310900 batches: 0.0190
trigger times: 9
Loss after 482442000 batches: 0.0190
trigger times: 10
Loss after 482573100 batches: 0.0190
trigger times: 11
Loss after 482704200 batches: 0.0191
trigger times: 12
Loss after 482835300 batches: 0.0189
trigger times: 13
Loss after 482966400 batches: 0.0191
trigger times: 14
Loss after 483097500 batches: 0.0190
trigger times: 15
Loss after 483228600 batches: 0.0187
trigger times: 16
Loss after 483359700 batches: 0.0191
trigger times: 17
Loss after 483490800 batches: 0.0188
trigger times: 18
Loss after 483621900 batches: 0.0188
trigger times: 19
Loss after 483753000 batches: 0.0187
trigger times: 20
Early stopping!
Start to test process.
Loss after 483884100 batches: 0.0187
Time to train on one home:  622.4135701656342
trigger times: 0
Loss after 484012740 batches: 0.1193
trigger times: 0
Loss after 484141380 batches: 0.0350
trigger times: 0
Loss after 484270020 batches: 0.0260
trigger times: 0
Loss after 484398660 batches: 0.0222
trigger times: 0
Loss after 484527300 batches: 0.0210
trigger times: 1
Loss after 484655940 batches: 0.0190
trigger times: 2
Loss after 484784580 batches: 0.0178
trigger times: 3
Loss after 484913220 batches: 0.0172
trigger times: 4
Loss after 485041860 batches: 0.0167
trigger times: 5
Loss after 485170500 batches: 0.0165
trigger times: 6
Loss after 485299140 batches: 0.0158
trigger times: 0
Loss after 485427780 batches: 0.0154
trigger times: 1
Loss after 485556420 batches: 0.0151
trigger times: 2
Loss after 485685060 batches: 0.0145
trigger times: 3
Loss after 485813700 batches: 0.0143
trigger times: 4
Loss after 485942340 batches: 0.0144
trigger times: 5
Loss after 486070980 batches: 0.0142
trigger times: 6
Loss after 486199620 batches: 0.0138
trigger times: 7
Loss after 486328260 batches: 0.0138
trigger times: 8
Loss after 486456900 batches: 0.0135
trigger times: 9
Loss after 486585540 batches: 0.0134
trigger times: 10
Loss after 486714180 batches: 0.0130
trigger times: 11
Loss after 486842820 batches: 0.0129
trigger times: 12
Loss after 486971460 batches: 0.0126
trigger times: 13
Loss after 487100100 batches: 0.0127
trigger times: 14
Loss after 487228740 batches: 0.0129
trigger times: 0
Loss after 487357380 batches: 0.0125
trigger times: 1
Loss after 487486020 batches: 0.0122
trigger times: 2
Loss after 487614660 batches: 0.0123
trigger times: 3
Loss after 487743300 batches: 0.0120
trigger times: 4
Loss after 487871940 batches: 0.0122
trigger times: 0
Loss after 488000580 batches: 0.0119
trigger times: 1
Loss after 488129220 batches: 0.0118
trigger times: 2
Loss after 488257860 batches: 0.0117
trigger times: 3
Loss after 488386500 batches: 0.0118
trigger times: 4
Loss after 488515140 batches: 0.0118
trigger times: 5
Loss after 488643780 batches: 0.0116
trigger times: 0
Loss after 488772420 batches: 0.0116
trigger times: 1
Loss after 488901060 batches: 0.0115
trigger times: 2
Loss after 489029700 batches: 0.0113
trigger times: 3
Loss after 489158340 batches: 0.0113
trigger times: 4
Loss after 489286980 batches: 0.0113
trigger times: 0
Loss after 489415620 batches: 0.0112
trigger times: 1
Loss after 489544260 batches: 0.0112
trigger times: 2
Loss after 489672900 batches: 0.0110
trigger times: 3
Loss after 489801540 batches: 0.0111
trigger times: 4
Loss after 489930180 batches: 0.0109
trigger times: 5
Loss after 490058820 batches: 0.0115
trigger times: 6
Loss after 490187460 batches: 0.0110
trigger times: 7
Loss after 490316100 batches: 0.0107
trigger times: 8
Loss after 490444740 batches: 0.0110
trigger times: 9
Loss after 490573380 batches: 0.0108
trigger times: 10
Loss after 490702020 batches: 0.0106
trigger times: 11
Loss after 490830660 batches: 0.0106
trigger times: 12
Loss after 490959300 batches: 0.0105
trigger times: 13
Loss after 491087940 batches: 0.0104
trigger times: 14
Loss after 491216580 batches: 0.0103
trigger times: 15
Loss after 491345220 batches: 0.0104
trigger times: 16
Loss after 491473860 batches: 0.0102
trigger times: 17
Loss after 491602500 batches: 0.0103
trigger times: 18
Loss after 491731140 batches: 0.0103
trigger times: 19
Loss after 491859780 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 491988420 batches: 0.0101
Time to train on one home:  456.79904198646545
trigger times: 0
Loss after 492119520 batches: 0.3394
trigger times: 0
Loss after 492250620 batches: 0.0849
trigger times: 0
Loss after 492381720 batches: 0.0578
trigger times: 1
Loss after 492512820 batches: 0.0492
trigger times: 2
Loss after 492643920 batches: 0.0438
trigger times: 3
Loss after 492775020 batches: 0.0402
trigger times: 4
Loss after 492906120 batches: 0.0385
trigger times: 5
Loss after 493037220 batches: 0.0368
trigger times: 0
Loss after 493168320 batches: 0.0350
trigger times: 1
Loss after 493299420 batches: 0.0334
trigger times: 2
Loss after 493430520 batches: 0.0325
trigger times: 3
Loss after 493561620 batches: 0.0313
trigger times: 4
Loss after 493692720 batches: 0.0309
trigger times: 5
Loss after 493823820 batches: 0.0302
trigger times: 6
Loss after 493954920 batches: 0.0296
trigger times: 7
Loss after 494086020 batches: 0.0287
trigger times: 8
Loss after 494217120 batches: 0.0283
trigger times: 9
Loss after 494348220 batches: 0.0281
trigger times: 10
Loss after 494479320 batches: 0.0275
trigger times: 11
Loss after 494610420 batches: 0.0272
trigger times: 12
Loss after 494741520 batches: 0.0269
trigger times: 13
Loss after 494872620 batches: 0.0263
trigger times: 14
Loss after 495003720 batches: 0.0261
trigger times: 15
Loss after 495134820 batches: 0.0258
trigger times: 16
Loss after 495265920 batches: 0.0252
trigger times: 17
Loss after 495397020 batches: 0.0252
trigger times: 18
Loss after 495528120 batches: 0.0245
trigger times: 19
Loss after 495659220 batches: 0.0245
trigger times: 20
Early stopping!
Start to test process.
Loss after 495790320 batches: 0.0242
Time to train on one home:  219.91114473342896
trigger times: 0
Loss after 495921420 batches: 0.3246
trigger times: 0
Loss after 496052520 batches: 0.1011
trigger times: 1
Loss after 496183620 batches: 0.0677
trigger times: 0
Loss after 496314720 batches: 0.0520
trigger times: 0
Loss after 496445820 batches: 0.0478
trigger times: 1
Loss after 496576920 batches: 0.0421
trigger times: 2
Loss after 496708020 batches: 0.0385
trigger times: 3
Loss after 496839120 batches: 0.0365
trigger times: 0
Loss after 496970220 batches: 0.0344
trigger times: 1
Loss after 497101320 batches: 0.0328
trigger times: 2
Loss after 497232420 batches: 0.0323
trigger times: 0
Loss after 497363520 batches: 0.0318
trigger times: 0
Loss after 497494620 batches: 0.0306
trigger times: 1
Loss after 497625720 batches: 0.0299
trigger times: 0
Loss after 497756820 batches: 0.0298
trigger times: 1
Loss after 497887920 batches: 0.0299
trigger times: 2
Loss after 498019020 batches: 0.0297
trigger times: 0
Loss after 498150120 batches: 0.0309
trigger times: 1
Loss after 498281220 batches: 0.0277
trigger times: 2
Loss after 498412320 batches: 0.0278
trigger times: 3
Loss after 498543420 batches: 0.0255
trigger times: 4
Loss after 498674520 batches: 0.0265
trigger times: 5
Loss after 498805620 batches: 0.0260
trigger times: 6
Loss after 498936720 batches: 0.0262
trigger times: 0
Loss after 499067820 batches: 0.0257
trigger times: 1
Loss after 499198920 batches: 0.0255
trigger times: 2
Loss after 499330020 batches: 0.0258
trigger times: 0
Loss after 499461120 batches: 0.0260
trigger times: 1
Loss after 499592220 batches: 0.0253
trigger times: 2
Loss after 499723320 batches: 0.0247
trigger times: 3
Loss after 499854420 batches: 0.0255
trigger times: 4
Loss after 499985520 batches: 0.0249
trigger times: 5
Loss after 500116620 batches: 0.0248
trigger times: 6
Loss after 500247720 batches: 0.0248
trigger times: 7
Loss after 500378820 batches: 0.0238
trigger times: 0
Loss after 500509920 batches: 0.0244
trigger times: 1
Loss after 500641020 batches: 0.0233
trigger times: 2
Loss after 500772120 batches: 0.0243
trigger times: 3
Loss after 500903220 batches: 0.0244
trigger times: 4
Loss after 501034320 batches: 0.0235
trigger times: 0
Loss after 501165420 batches: 0.0237
trigger times: 1
Loss after 501296520 batches: 0.0237
trigger times: 0
Loss after 501427620 batches: 0.0230
trigger times: 1
Loss after 501558720 batches: 0.0231
trigger times: 0
Loss after 501689820 batches: 0.0249
trigger times: 1
Loss after 501820920 batches: 0.0230
trigger times: 2
Loss after 501952020 batches: 0.0231
trigger times: 3
Loss after 502083120 batches: 0.0218
trigger times: 4
Loss after 502214220 batches: 0.0209
trigger times: 5
Loss after 502345320 batches: 0.0222
trigger times: 6
Loss after 502476420 batches: 0.0230
trigger times: 7
Loss after 502607520 batches: 0.0231
trigger times: 8
Loss after 502738620 batches: 0.0225
trigger times: 9
Loss after 502869720 batches: 0.0223
trigger times: 10
Loss after 503000820 batches: 0.0219
trigger times: 11
Loss after 503131920 batches: 0.0230
trigger times: 12
Loss after 503263020 batches: 0.0210
trigger times: 13
Loss after 503394120 batches: 0.0214
trigger times: 14
Loss after 503525220 batches: 0.0214
trigger times: 15
Loss after 503656320 batches: 0.0216
trigger times: 16
Loss after 503787420 batches: 0.0213
trigger times: 17
Loss after 503918520 batches: 0.0218
trigger times: 18
Loss after 504049620 batches: 0.0217
trigger times: 19
Loss after 504180720 batches: 0.0215
trigger times: 20
Early stopping!
Start to test process.
Loss after 504311820 batches: 0.0210
Time to train on one home:  477.80534052848816
trigger times: 0
Loss after 504442920 batches: 0.0992
trigger times: 0
Loss after 504574020 batches: 0.0299
trigger times: 0
Loss after 504705120 batches: 0.0206
trigger times: 1
Loss after 504836220 batches: 0.0178
trigger times: 2
Loss after 504967320 batches: 0.0162
trigger times: 0
Loss after 505098420 batches: 0.0151
trigger times: 1
Loss after 505229520 batches: 0.0142
trigger times: 0
Loss after 505360620 batches: 0.0137
trigger times: 1
Loss after 505491720 batches: 0.0132
trigger times: 2
Loss after 505622820 batches: 0.0128
trigger times: 3
Loss after 505753920 batches: 0.0123
trigger times: 0
Loss after 505885020 batches: 0.0120
trigger times: 1
Loss after 506016120 batches: 0.0120
trigger times: 2
Loss after 506147220 batches: 0.0116
trigger times: 0
Loss after 506278320 batches: 0.0114
trigger times: 1
Loss after 506409420 batches: 0.0109
trigger times: 2
Loss after 506540520 batches: 0.0110
trigger times: 3
Loss after 506671620 batches: 0.0107
trigger times: 4
Loss after 506802720 batches: 0.0107
trigger times: 5
Loss after 506933820 batches: 0.0103
trigger times: 6
Loss after 507064920 batches: 0.0102
trigger times: 7
Loss after 507196020 batches: 0.0101
trigger times: 8
Loss after 507327120 batches: 0.0099
trigger times: 9
Loss after 507458220 batches: 0.0098
trigger times: 10
Loss after 507589320 batches: 0.0097
trigger times: 11
Loss after 507720420 batches: 0.0096
trigger times: 12
Loss after 507851520 batches: 0.0097
trigger times: 13
Loss after 507982620 batches: 0.0095
trigger times: 14
Loss after 508113720 batches: 0.0095
trigger times: 15
Loss after 508244820 batches: 0.0092
trigger times: 16
Loss after 508375920 batches: 0.0093
trigger times: 17
Loss after 508507020 batches: 0.0090
trigger times: 18
Loss after 508638120 batches: 0.0091
trigger times: 19
Loss after 508769220 batches: 0.0090
trigger times: 20
Early stopping!
Start to test process.
Loss after 508900320 batches: 0.0087
Time to train on one home:  263.28056955337524
trigger times: 0
Loss after 508978920 batches: 0.3900
trigger times: 1
Loss after 509057520 batches: 0.1058
trigger times: 0
Loss after 509136120 batches: 0.0572
trigger times: 0
Loss after 509214720 batches: 0.0451
trigger times: 1
Loss after 509293320 batches: 0.0392
trigger times: 2
Loss after 509371920 batches: 0.0350
trigger times: 3
Loss after 509450520 batches: 0.0327
trigger times: 4
Loss after 509529120 batches: 0.0311
trigger times: 5
Loss after 509607720 batches: 0.0289
trigger times: 0
Loss after 509686320 batches: 0.0276
trigger times: 1
Loss after 509764920 batches: 0.0268
trigger times: 2
Loss after 509843520 batches: 0.0256
trigger times: 3
Loss after 509922120 batches: 0.0248
trigger times: 0
Loss after 510000720 batches: 0.0237
trigger times: 1
Loss after 510079320 batches: 0.0231
trigger times: 2
Loss after 510157920 batches: 0.0237
trigger times: 3
Loss after 510236520 batches: 0.0226
trigger times: 4
Loss after 510315120 batches: 0.0218
trigger times: 5
Loss after 510393720 batches: 0.0222
trigger times: 6
Loss after 510472320 batches: 0.0214
trigger times: 7
Loss after 510550920 batches: 0.0213
trigger times: 8
Loss after 510629520 batches: 0.0205
trigger times: 0
Loss after 510708120 batches: 0.0207
trigger times: 1
Loss after 510786720 batches: 0.0198
trigger times: 2
Loss after 510865320 batches: 0.0203
trigger times: 3
Loss after 510943920 batches: 0.0198
trigger times: 4
Loss after 511022520 batches: 0.0191
trigger times: 5
Loss after 511101120 batches: 0.0193
trigger times: 6
Loss after 511179720 batches: 0.0191
trigger times: 7
Loss after 511258320 batches: 0.0194
trigger times: 8
Loss after 511336920 batches: 0.0188
trigger times: 0
Loss after 511415520 batches: 0.0181
trigger times: 1
Loss after 511494120 batches: 0.0181
trigger times: 0
Loss after 511572720 batches: 0.0172
trigger times: 1
Loss after 511651320 batches: 0.0173
trigger times: 2
Loss after 511729920 batches: 0.0178
trigger times: 3
Loss after 511808520 batches: 0.0174
trigger times: 4
Loss after 511887120 batches: 0.0177
trigger times: 5
Loss after 511965720 batches: 0.0173
trigger times: 6
Loss after 512044320 batches: 0.0173
trigger times: 7
Loss after 512122920 batches: 0.0169
trigger times: 8
Loss after 512201520 batches: 0.0168
trigger times: 9
Loss after 512280120 batches: 0.0167
trigger times: 10
Loss after 512358720 batches: 0.0168
trigger times: 11
Loss after 512437320 batches: 0.0166
trigger times: 12
Loss after 512515920 batches: 0.0165
trigger times: 13
Loss after 512594520 batches: 0.0164
trigger times: 14
Loss after 512673120 batches: 0.0161
trigger times: 15
Loss after 512751720 batches: 0.0162
trigger times: 16
Loss after 512830320 batches: 0.0156
trigger times: 17
Loss after 512908920 batches: 0.0161
trigger times: 18
Loss after 512987520 batches: 0.0157
trigger times: 19
Loss after 513066120 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 513144720 batches: 0.0158
Time to train on one home:  261.0265130996704
trigger times: 0
Loss after 513275820 batches: 0.1057
trigger times: 1
Loss after 513406920 batches: 0.0318
trigger times: 0
Loss after 513538020 batches: 0.0232
trigger times: 1
Loss after 513669120 batches: 0.0198
trigger times: 0
Loss after 513800220 batches: 0.0181
trigger times: 1
Loss after 513931320 batches: 0.0165
trigger times: 2
Loss after 514062420 batches: 0.0159
trigger times: 3
Loss after 514193520 batches: 0.0153
trigger times: 4
Loss after 514324620 batches: 0.0147
trigger times: 5
Loss after 514455720 batches: 0.0141
trigger times: 6
Loss after 514586820 batches: 0.0140
trigger times: 7
Loss after 514717920 batches: 0.0135
trigger times: 8
Loss after 514849020 batches: 0.0130
trigger times: 9
Loss after 514980120 batches: 0.0129
trigger times: 10
Loss after 515111220 batches: 0.0126
trigger times: 11
Loss after 515242320 batches: 0.0121
trigger times: 12
Loss after 515373420 batches: 0.0122
trigger times: 13
Loss after 515504520 batches: 0.0119
trigger times: 14
Loss after 515635620 batches: 0.0119
trigger times: 15
Loss after 515766720 batches: 0.0119
trigger times: 16
Loss after 515897820 batches: 0.0114
trigger times: 17
Loss after 516028920 batches: 0.0113
trigger times: 18
Loss after 516160020 batches: 0.0110
trigger times: 19
Loss after 516291120 batches: 0.0109
trigger times: 20
Early stopping!
Start to test process.
Loss after 516422220 batches: 0.0112
Time to train on one home:  190.7868230342865
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318, 0.016183730855002625, 0.015608215882864637]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165], [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777], [0.5659677750534482, 0.3881937670236807, 0.47736113706344846, 1.0466814163269424, 0.5011883546712889, 24.728223616247963, 1547.1996]]
Round_9_results:  [0.5659677750534482, 0.3881937670236807, 0.47736113706344846, 1.0466814163269424, 0.5011883546712889, 24.728223616247963, 1547.1996]
trigger times: 0
Loss after 516553320 batches: 0.1229
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4192 < 4193; dropping {'Training_Loss': 0.12286886664212875, 'Validation_Loss': 0.1954710235198339, 'Training_R2': 0.8764271808135542, 'Validation_R2': 0.8183004261486844, 'Training_F1': 0.817807311633602, 'Validation_F1': 0.7522760158477847, 'Training_NEP': 0.3635672996272289, 'Validation_NEP': 0.4885768855389177, 'Training_NDE': 0.09276830878420472, 'Validation_NDE': 0.14469596248394687, 'Training_MAE': 12.040627931948384, 'Validation_MAE': 13.398805182809044, 'Training_MSE': 408.16608, 'Validation_MSE': 534.35815}.
trigger times: 0
Loss after 516684420 batches: 0.0322
trigger times: 0
Loss after 516815520 batches: 0.0235
trigger times: 1
Loss after 516946620 batches: 0.0199
trigger times: 2
Loss after 517077720 batches: 0.0182
trigger times: 3
Loss after 517208820 batches: 0.0172
trigger times: 0
Loss after 517339920 batches: 0.0158
trigger times: 1
Loss after 517471020 batches: 0.0154
trigger times: 2
Loss after 517602120 batches: 0.0147
trigger times: 3
Loss after 517733220 batches: 0.0143
trigger times: 4
Loss after 517864320 batches: 0.0138
trigger times: 5
Loss after 517995420 batches: 0.0136
trigger times: 6
Loss after 518126520 batches: 0.0133
trigger times: 7
Loss after 518257620 batches: 0.0131
trigger times: 8
Loss after 518388720 batches: 0.0129
trigger times: 9
Loss after 518519820 batches: 0.0128
trigger times: 10
Loss after 518650920 batches: 0.0122
trigger times: 11
Loss after 518782020 batches: 0.0122
trigger times: 12
Loss after 518913120 batches: 0.0119
trigger times: 13
Loss after 519044220 batches: 0.0117
trigger times: 14
Loss after 519175320 batches: 0.0116
trigger times: 15
Loss after 519306420 batches: 0.0117
trigger times: 16
Loss after 519437520 batches: 0.0113
trigger times: 17
Loss after 519568620 batches: 0.0113
trigger times: 18
Loss after 519699720 batches: 0.0113
trigger times: 19
Loss after 519830820 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 519961920 batches: 0.0108
Time to train on one home:  205.80935835838318
trigger times: 0
Loss after 520064520 batches: 0.2836
trigger times: 1
Loss after 520167120 batches: 0.0890
trigger times: 2
Loss after 520269720 batches: 0.0555
trigger times: 3
Loss after 520372320 batches: 0.0485
trigger times: 4
Loss after 520474920 batches: 0.0386
trigger times: 5
Loss after 520577520 batches: 0.0355
trigger times: 6
Loss after 520680120 batches: 0.0344
trigger times: 7
Loss after 520782720 batches: 0.0314
trigger times: 8
Loss after 520885320 batches: 0.0291
trigger times: 9
Loss after 520987920 batches: 0.0306
trigger times: 10
Loss after 521090520 batches: 0.0333
trigger times: 11
Loss after 521193120 batches: 0.0267
trigger times: 12
Loss after 521295720 batches: 0.0248
trigger times: 13
Loss after 521398320 batches: 0.0245
trigger times: 14
Loss after 521500920 batches: 0.0240
trigger times: 15
Loss after 521603520 batches: 0.0232
trigger times: 16
Loss after 521706120 batches: 0.0228
trigger times: 17
Loss after 521808720 batches: 0.0234
trigger times: 18
Loss after 521911320 batches: 0.0254
trigger times: 19
Loss after 522013920 batches: 0.0235
trigger times: 20
Early stopping!
Start to test process.
Loss after 522116520 batches: 0.0226
Time to train on one home:  132.9473431110382
trigger times: 0
Loss after 522247620 batches: 0.1725
trigger times: 0
Loss after 522378720 batches: 0.0553
trigger times: 0
Loss after 522509820 batches: 0.0401
trigger times: 1
Loss after 522640920 batches: 0.0340
trigger times: 2
Loss after 522772020 batches: 0.0309
trigger times: 3
Loss after 522903120 batches: 0.0289
trigger times: 4
Loss after 523034220 batches: 0.0273
trigger times: 5
Loss after 523165320 batches: 0.0259
trigger times: 6
Loss after 523296420 batches: 0.0248
trigger times: 7
Loss after 523427520 batches: 0.0241
trigger times: 8
Loss after 523558620 batches: 0.0235
trigger times: 9
Loss after 523689720 batches: 0.0226
trigger times: 10
Loss after 523820820 batches: 0.0224
trigger times: 11
Loss after 523951920 batches: 0.0215
trigger times: 12
Loss after 524083020 batches: 0.0212
trigger times: 13
Loss after 524214120 batches: 0.0206
trigger times: 14
Loss after 524345220 batches: 0.0203
trigger times: 15
Loss after 524476320 batches: 0.0196
trigger times: 16
Loss after 524607420 batches: 0.0197
trigger times: 17
Loss after 524738520 batches: 0.0192
trigger times: 18
Loss after 524869620 batches: 0.0190
trigger times: 19
Loss after 525000720 batches: 0.0186
trigger times: 20
Early stopping!
Start to test process.
Loss after 525131820 batches: 0.0186
Time to train on one home:  177.04210209846497
trigger times: 0
Loss after 525262920 batches: 0.3727
trigger times: 0
Loss after 525394020 batches: 0.1087
trigger times: 0
Loss after 525525120 batches: 0.0704
trigger times: 0
Loss after 525656220 batches: 0.0573
trigger times: 1
Loss after 525787320 batches: 0.0502
trigger times: 2
Loss after 525918420 batches: 0.0453
trigger times: 3
Loss after 526049520 batches: 0.0428
trigger times: 0
Loss after 526180620 batches: 0.0401
trigger times: 1
Loss after 526311720 batches: 0.0387
trigger times: 2
Loss after 526442820 batches: 0.0366
trigger times: 0
Loss after 526573920 batches: 0.0351
trigger times: 1
Loss after 526705020 batches: 0.0340
trigger times: 2
Loss after 526836120 batches: 0.0326
trigger times: 3
Loss after 526967220 batches: 0.0318
trigger times: 4
Loss after 527098320 batches: 0.0313
trigger times: 0
Loss after 527229420 batches: 0.0308
trigger times: 0
Loss after 527360520 batches: 0.0302
trigger times: 0
Loss after 527491620 batches: 0.0297
trigger times: 0
Loss after 527622720 batches: 0.0288
trigger times: 1
Loss after 527753820 batches: 0.0283
trigger times: 2
Loss after 527884920 batches: 0.0280
trigger times: 3
Loss after 528016020 batches: 0.0274
trigger times: 4
Loss after 528147120 batches: 0.0268
trigger times: 0
Loss after 528278220 batches: 0.0268
trigger times: 1
Loss after 528409320 batches: 0.0268
trigger times: 2
Loss after 528540420 batches: 0.0263
trigger times: 3
Loss after 528671520 batches: 0.0259
trigger times: 4
Loss after 528802620 batches: 0.0255
trigger times: 5
Loss after 528933720 batches: 0.0255
trigger times: 6
Loss after 529064820 batches: 0.0254
trigger times: 7
Loss after 529195920 batches: 0.0250
trigger times: 8
Loss after 529327020 batches: 0.0249
trigger times: 9
Loss after 529458120 batches: 0.0242
trigger times: 10
Loss after 529589220 batches: 0.0241
trigger times: 11
Loss after 529720320 batches: 0.0243
trigger times: 12
Loss after 529851420 batches: 0.0240
trigger times: 0
Loss after 529982520 batches: 0.0242
trigger times: 0
Loss after 530113620 batches: 0.0236
trigger times: 1
Loss after 530244720 batches: 0.0232
trigger times: 2
Loss after 530375820 batches: 0.0233
trigger times: 3
Loss after 530506920 batches: 0.0229
trigger times: 0
Loss after 530638020 batches: 0.0233
trigger times: 1
Loss after 530769120 batches: 0.0228
trigger times: 0
Loss after 530900220 batches: 0.0223
trigger times: 1
Loss after 531031320 batches: 0.0226
trigger times: 2
Loss after 531162420 batches: 0.0220
trigger times: 3
Loss after 531293520 batches: 0.0220
trigger times: 4
Loss after 531424620 batches: 0.0221
trigger times: 5
Loss after 531555720 batches: 0.0216
trigger times: 6
Loss after 531686820 batches: 0.0217
trigger times: 7
Loss after 531817920 batches: 0.0218
trigger times: 8
Loss after 531949020 batches: 0.0214
trigger times: 9
Loss after 532080120 batches: 0.0215
trigger times: 10
Loss after 532211220 batches: 0.0213
trigger times: 11
Loss after 532342320 batches: 0.0211
trigger times: 12
Loss after 532473420 batches: 0.0211
trigger times: 13
Loss after 532604520 batches: 0.0208
trigger times: 14
Loss after 532735620 batches: 0.0207
trigger times: 15
Loss after 532866720 batches: 0.0209
trigger times: 0
Loss after 532997820 batches: 0.0210
trigger times: 1
Loss after 533128920 batches: 0.0207
trigger times: 2
Loss after 533260020 batches: 0.0205
trigger times: 3
Loss after 533391120 batches: 0.0204
trigger times: 4
Loss after 533522220 batches: 0.0202
trigger times: 5
Loss after 533653320 batches: 0.0198
trigger times: 6
Loss after 533784420 batches: 0.0203
trigger times: 7
Loss after 533915520 batches: 0.0201
trigger times: 8
Loss after 534046620 batches: 0.0200
trigger times: 9
Loss after 534177720 batches: 0.0202
trigger times: 0
Loss after 534308820 batches: 0.0200
trigger times: 1
Loss after 534439920 batches: 0.0199
trigger times: 2
Loss after 534571020 batches: 0.0199
trigger times: 3
Loss after 534702120 batches: 0.0197
trigger times: 4
Loss after 534833220 batches: 0.0194
trigger times: 0
Loss after 534964320 batches: 0.0195
trigger times: 1
Loss after 535095420 batches: 0.0192
trigger times: 2
Loss after 535226520 batches: 0.0191
trigger times: 0
Loss after 535357620 batches: 0.0190
trigger times: 1
Loss after 535488720 batches: 0.0190
trigger times: 0
Loss after 535619820 batches: 0.0193
trigger times: 0
Loss after 535750920 batches: 0.0191
trigger times: 1
Loss after 535882020 batches: 0.0191
trigger times: 2
Loss after 536013120 batches: 0.0188
trigger times: 3
Loss after 536144220 batches: 0.0190
trigger times: 4
Loss after 536275320 batches: 0.0189
trigger times: 5
Loss after 536406420 batches: 0.0188
trigger times: 6
Loss after 536537520 batches: 0.0188
trigger times: 7
Loss after 536668620 batches: 0.0189
trigger times: 0
Loss after 536799720 batches: 0.0187
trigger times: 1
Loss after 536930820 batches: 0.0186
trigger times: 2
Loss after 537061920 batches: 0.0186
trigger times: 3
Loss after 537193020 batches: 0.0184
trigger times: 4
Loss after 537324120 batches: 0.0186
trigger times: 0
Loss after 537455220 batches: 0.0185
trigger times: 1
Loss after 537586320 batches: 0.0185
trigger times: 2
Loss after 537717420 batches: 0.0181
trigger times: 0
Loss after 537848520 batches: 0.0182
trigger times: 1
Loss after 537979620 batches: 0.0179
trigger times: 2
Loss after 538110720 batches: 0.0182
trigger times: 3
Loss after 538241820 batches: 0.0181
trigger times: 4
Loss after 538372920 batches: 0.0182
trigger times: 5
Loss after 538504020 batches: 0.0182
trigger times: 6
Loss after 538635120 batches: 0.0180
trigger times: 7
Loss after 538766220 batches: 0.0178
trigger times: 8
Loss after 538897320 batches: 0.0178
trigger times: 9
Loss after 539028420 batches: 0.0178
trigger times: 10
Loss after 539159520 batches: 0.0181
trigger times: 11
Loss after 539290620 batches: 0.0178
trigger times: 12
Loss after 539421720 batches: 0.0179
trigger times: 13
Loss after 539552820 batches: 0.0176
trigger times: 14
Loss after 539683920 batches: 0.0177
trigger times: 15
Loss after 539815020 batches: 0.0178
trigger times: 16
Loss after 539946120 batches: 0.0177
trigger times: 0
Loss after 540077220 batches: 0.0175
trigger times: 1
Loss after 540208320 batches: 0.0173
trigger times: 2
Loss after 540339420 batches: 0.0174
trigger times: 3
Loss after 540470520 batches: 0.0176
trigger times: 4
Loss after 540601620 batches: 0.0172
trigger times: 5
Loss after 540732720 batches: 0.0171
trigger times: 6
Loss after 540863820 batches: 0.0172
trigger times: 7
Loss after 540994920 batches: 0.0170
trigger times: 8
Loss after 541126020 batches: 0.0170
trigger times: 9
Loss after 541257120 batches: 0.0171
trigger times: 10
Loss after 541388220 batches: 0.0171
trigger times: 11
Loss after 541519320 batches: 0.0169
trigger times: 12
Loss after 541650420 batches: 0.0171
trigger times: 13
Loss after 541781520 batches: 0.0171
trigger times: 14
Loss after 541912620 batches: 0.0169
trigger times: 15
Loss after 542043720 batches: 0.0168
trigger times: 16
Loss after 542174820 batches: 0.0166
trigger times: 17
Loss after 542305920 batches: 0.0166
trigger times: 18
Loss after 542437020 batches: 0.0169
trigger times: 19
Loss after 542568120 batches: 0.0165
trigger times: 20
Early stopping!
Start to test process.
Loss after 542699220 batches: 0.0167
Time to train on one home:  972.3898322582245
trigger times: 0
Loss after 542827860 batches: 0.1118
trigger times: 0
Loss after 542956500 batches: 0.0347
trigger times: 1
Loss after 543085140 batches: 0.0257
trigger times: 2
Loss after 543213780 batches: 0.0217
trigger times: 3
Loss after 543342420 batches: 0.0202
trigger times: 0
Loss after 543471060 batches: 0.0190
trigger times: 1
Loss after 543599700 batches: 0.0173
trigger times: 2
Loss after 543728340 batches: 0.0167
trigger times: 3
Loss after 543856980 batches: 0.0165
trigger times: 4
Loss after 543985620 batches: 0.0157
trigger times: 0
Loss after 544114260 batches: 0.0152
trigger times: 0
Loss after 544242900 batches: 0.0151
trigger times: 1
Loss after 544371540 batches: 0.0145
trigger times: 2
Loss after 544500180 batches: 0.0142
trigger times: 3
Loss after 544628820 batches: 0.0138
trigger times: 4
Loss after 544757460 batches: 0.0142
trigger times: 5
Loss after 544886100 batches: 0.0133
trigger times: 6
Loss after 545014740 batches: 0.0132
trigger times: 0
Loss after 545143380 batches: 0.0127
trigger times: 0
Loss after 545272020 batches: 0.0128
trigger times: 0
Loss after 545400660 batches: 0.0128
trigger times: 1
Loss after 545529300 batches: 0.0127
trigger times: 2
Loss after 545657940 batches: 0.0126
trigger times: 3
Loss after 545786580 batches: 0.0125
trigger times: 4
Loss after 545915220 batches: 0.0125
trigger times: 5
Loss after 546043860 batches: 0.0121
trigger times: 6
Loss after 546172500 batches: 0.0122
trigger times: 7
Loss after 546301140 batches: 0.0119
trigger times: 8
Loss after 546429780 batches: 0.0117
trigger times: 9
Loss after 546558420 batches: 0.0118
trigger times: 10
Loss after 546687060 batches: 0.0116
trigger times: 11
Loss after 546815700 batches: 0.0118
trigger times: 12
Loss after 546944340 batches: 0.0118
trigger times: 13
Loss after 547072980 batches: 0.0114
trigger times: 14
Loss after 547201620 batches: 0.0114
trigger times: 15
Loss after 547330260 batches: 0.0115
trigger times: 16
Loss after 547458900 batches: 0.0114
trigger times: 17
Loss after 547587540 batches: 0.0112
trigger times: 18
Loss after 547716180 batches: 0.0114
trigger times: 19
Loss after 547844820 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 547973460 batches: 0.0111
Time to train on one home:  300.5741455554962
trigger times: 0
Loss after 548104560 batches: 0.3196
trigger times: 0
Loss after 548235660 batches: 0.0799
trigger times: 0
Loss after 548366760 batches: 0.0558
trigger times: 0
Loss after 548497860 batches: 0.0478
trigger times: 0
Loss after 548628960 batches: 0.0432
trigger times: 0
Loss after 548760060 batches: 0.0395
trigger times: 1
Loss after 548891160 batches: 0.0379
trigger times: 2
Loss after 549022260 batches: 0.0355
trigger times: 3
Loss after 549153360 batches: 0.0345
trigger times: 4
Loss after 549284460 batches: 0.0331
trigger times: 5
Loss after 549415560 batches: 0.0318
trigger times: 6
Loss after 549546660 batches: 0.0308
trigger times: 7
Loss after 549677760 batches: 0.0300
trigger times: 0
Loss after 549808860 batches: 0.0296
trigger times: 1
Loss after 549939960 batches: 0.0289
trigger times: 2
Loss after 550071060 batches: 0.0286
trigger times: 3
Loss after 550202160 batches: 0.0281
trigger times: 4
Loss after 550333260 batches: 0.0276
trigger times: 5
Loss after 550464360 batches: 0.0272
trigger times: 6
Loss after 550595460 batches: 0.0267
trigger times: 7
Loss after 550726560 batches: 0.0264
trigger times: 8
Loss after 550857660 batches: 0.0259
trigger times: 9
Loss after 550988760 batches: 0.0255
trigger times: 10
Loss after 551119860 batches: 0.0254
trigger times: 11
Loss after 551250960 batches: 0.0250
trigger times: 12
Loss after 551382060 batches: 0.0248
trigger times: 13
Loss after 551513160 batches: 0.0245
trigger times: 14
Loss after 551644260 batches: 0.0241
trigger times: 15
Loss after 551775360 batches: 0.0237
trigger times: 16
Loss after 551906460 batches: 0.0238
trigger times: 17
Loss after 552037560 batches: 0.0236
trigger times: 18
Loss after 552168660 batches: 0.0236
trigger times: 19
Loss after 552299760 batches: 0.0235
trigger times: 20
Early stopping!
Start to test process.
Loss after 552430860 batches: 0.0231
Time to train on one home:  255.58545899391174
trigger times: 0
Loss after 552561960 batches: 0.3021
trigger times: 0
Loss after 552693060 batches: 0.0942
trigger times: 0
Loss after 552824160 batches: 0.0618
trigger times: 1
Loss after 552955260 batches: 0.0477
trigger times: 0
Loss after 553086360 batches: 0.0422
trigger times: 1
Loss after 553217460 batches: 0.0392
trigger times: 0
Loss after 553348560 batches: 0.0369
trigger times: 1
Loss after 553479660 batches: 0.0359
trigger times: 0
Loss after 553610760 batches: 0.0327
trigger times: 0
Loss after 553741860 batches: 0.0327
trigger times: 1
Loss after 553872960 batches: 0.0317
trigger times: 0
Loss after 554004060 batches: 0.0314
trigger times: 0
Loss after 554135160 batches: 0.0301
trigger times: 1
Loss after 554266260 batches: 0.0299
trigger times: 2
Loss after 554397360 batches: 0.0294
trigger times: 0
Loss after 554528460 batches: 0.0276
trigger times: 1
Loss after 554659560 batches: 0.0267
trigger times: 2
Loss after 554790660 batches: 0.0267
trigger times: 0
Loss after 554921760 batches: 0.0262
trigger times: 1
Loss after 555052860 batches: 0.0262
trigger times: 2
Loss after 555183960 batches: 0.0264
trigger times: 3
Loss after 555315060 batches: 0.0271
trigger times: 0
Loss after 555446160 batches: 0.0267
trigger times: 1
Loss after 555577260 batches: 0.0258
trigger times: 2
Loss after 555708360 batches: 0.0256
trigger times: 3
Loss after 555839460 batches: 0.0257
trigger times: 4
Loss after 555970560 batches: 0.0254
trigger times: 0
Loss after 556101660 batches: 0.0255
trigger times: 1
Loss after 556232760 batches: 0.0249
trigger times: 2
Loss after 556363860 batches: 0.0239
trigger times: 0
Loss after 556494960 batches: 0.0241
trigger times: 1
Loss after 556626060 batches: 0.0248
trigger times: 2
Loss after 556757160 batches: 0.0255
trigger times: 3
Loss after 556888260 batches: 0.0235
trigger times: 0
Loss after 557019360 batches: 0.0240
trigger times: 1
Loss after 557150460 batches: 0.0229
trigger times: 2
Loss after 557281560 batches: 0.0233
trigger times: 3
Loss after 557412660 batches: 0.0243
trigger times: 4
Loss after 557543760 batches: 0.0231
trigger times: 0
Loss after 557674860 batches: 0.0228
trigger times: 1
Loss after 557805960 batches: 0.0224
trigger times: 2
Loss after 557937060 batches: 0.0226
trigger times: 3
Loss after 558068160 batches: 0.0226
trigger times: 0
Loss after 558199260 batches: 0.0223
trigger times: 1
Loss after 558330360 batches: 0.0226
trigger times: 2
Loss after 558461460 batches: 0.0223
trigger times: 3
Loss after 558592560 batches: 0.0215
trigger times: 4
Loss after 558723660 batches: 0.0214
trigger times: 5
Loss after 558854760 batches: 0.0213
trigger times: 0
Loss after 558985860 batches: 0.0222
trigger times: 0
Loss after 559116960 batches: 0.0222
trigger times: 0
Loss after 559248060 batches: 0.0218
trigger times: 1
Loss after 559379160 batches: 0.0219
trigger times: 2
Loss after 559510260 batches: 0.0219
trigger times: 3
Loss after 559641360 batches: 0.0225
trigger times: 4
Loss after 559772460 batches: 0.0221
trigger times: 5
Loss after 559903560 batches: 0.0225
trigger times: 6
Loss after 560034660 batches: 0.0219
trigger times: 7
Loss after 560165760 batches: 0.0214
trigger times: 8
Loss after 560296860 batches: 0.0206
trigger times: 9
Loss after 560427960 batches: 0.0206
trigger times: 10
Loss after 560559060 batches: 0.0214
trigger times: 11
Loss after 560690160 batches: 0.0205
trigger times: 12
Loss after 560821260 batches: 0.0203
trigger times: 13
Loss after 560952360 batches: 0.0214
trigger times: 14
Loss after 561083460 batches: 0.0206
trigger times: 15
Loss after 561214560 batches: 0.0202
trigger times: 16
Loss after 561345660 batches: 0.0213
trigger times: 17
Loss after 561476760 batches: 0.0197
trigger times: 18
Loss after 561607860 batches: 0.0191
trigger times: 19
Loss after 561738960 batches: 0.0202
trigger times: 20
Early stopping!
Start to test process.
Loss after 561870060 batches: 0.0209
Time to train on one home:  528.2989933490753
trigger times: 0
Loss after 562001160 batches: 0.0860
trigger times: 1
Loss after 562132260 batches: 0.0258
trigger times: 2
Loss after 562263360 batches: 0.0196
trigger times: 0
Loss after 562394460 batches: 0.0172
trigger times: 1
Loss after 562525560 batches: 0.0152
trigger times: 2
Loss after 562656660 batches: 0.0142
trigger times: 3
Loss after 562787760 batches: 0.0135
trigger times: 4
Loss after 562918860 batches: 0.0127
trigger times: 0
Loss after 563049960 batches: 0.0125
trigger times: 1
Loss after 563181060 batches: 0.0119
trigger times: 2
Loss after 563312160 batches: 0.0116
trigger times: 3
Loss after 563443260 batches: 0.0113
trigger times: 4
Loss after 563574360 batches: 0.0111
trigger times: 5
Loss after 563705460 batches: 0.0108
trigger times: 6
Loss after 563836560 batches: 0.0108
trigger times: 7
Loss after 563967660 batches: 0.0106
trigger times: 8
Loss after 564098760 batches: 0.0104
trigger times: 9
Loss after 564229860 batches: 0.0102
trigger times: 10
Loss after 564360960 batches: 0.0100
trigger times: 11
Loss after 564492060 batches: 0.0099
trigger times: 12
Loss after 564623160 batches: 0.0096
trigger times: 13
Loss after 564754260 batches: 0.0095
trigger times: 14
Loss after 564885360 batches: 0.0096
trigger times: 15
Loss after 565016460 batches: 0.0095
trigger times: 16
Loss after 565147560 batches: 0.0093
trigger times: 17
Loss after 565278660 batches: 0.0092
trigger times: 18
Loss after 565409760 batches: 0.0091
trigger times: 19
Loss after 565540860 batches: 0.0090
trigger times: 20
Early stopping!
Start to test process.
Loss after 565671960 batches: 0.0089
Time to train on one home:  219.82352304458618
trigger times: 0
Loss after 565750560 batches: 0.2682
trigger times: 0
Loss after 565829160 batches: 0.0703
trigger times: 0
Loss after 565907760 batches: 0.0436
trigger times: 0
Loss after 565986360 batches: 0.0358
trigger times: 0
Loss after 566064960 batches: 0.0314
trigger times: 1
Loss after 566143560 batches: 0.0298
trigger times: 2
Loss after 566222160 batches: 0.0273
trigger times: 0
Loss after 566300760 batches: 0.0256
trigger times: 0
Loss after 566379360 batches: 0.0253
trigger times: 1
Loss after 566457960 batches: 0.0239
trigger times: 0
Loss after 566536560 batches: 0.0229
trigger times: 1
Loss after 566615160 batches: 0.0222
trigger times: 2
Loss after 566693760 batches: 0.0214
trigger times: 3
Loss after 566772360 batches: 0.0210
trigger times: 4
Loss after 566850960 batches: 0.0205
trigger times: 0
Loss after 566929560 batches: 0.0207
trigger times: 1
Loss after 567008160 batches: 0.0199
trigger times: 0
Loss after 567086760 batches: 0.0195
trigger times: 1
Loss after 567165360 batches: 0.0189
trigger times: 2
Loss after 567243960 batches: 0.0188
trigger times: 3
Loss after 567322560 batches: 0.0187
trigger times: 4
Loss after 567401160 batches: 0.0183
trigger times: 0
Loss after 567479760 batches: 0.0179
trigger times: 1
Loss after 567558360 batches: 0.0180
trigger times: 2
Loss after 567636960 batches: 0.0179
trigger times: 3
Loss after 567715560 batches: 0.0177
trigger times: 4
Loss after 567794160 batches: 0.0177
trigger times: 5
Loss after 567872760 batches: 0.0171
trigger times: 6
Loss after 567951360 batches: 0.0170
trigger times: 7
Loss after 568029960 batches: 0.0171
trigger times: 8
Loss after 568108560 batches: 0.0169
trigger times: 9
Loss after 568187160 batches: 0.0165
trigger times: 10
Loss after 568265760 batches: 0.0162
trigger times: 11
Loss after 568344360 batches: 0.0166
trigger times: 12
Loss after 568422960 batches: 0.0160
trigger times: 13
Loss after 568501560 batches: 0.0159
trigger times: 14
Loss after 568580160 batches: 0.0162
trigger times: 15
Loss after 568658760 batches: 0.0166
trigger times: 16
Loss after 568737360 batches: 0.0164
trigger times: 17
Loss after 568815960 batches: 0.0154
trigger times: 18
Loss after 568894560 batches: 0.0155
trigger times: 19
Loss after 568973160 batches: 0.0156
trigger times: 20
Early stopping!
Start to test process.
Loss after 569051760 batches: 0.0151
Time to train on one home:  210.6182415485382
trigger times: 0
Loss after 569182860 batches: 0.1026
trigger times: 0
Loss after 569313960 batches: 0.0292
trigger times: 0
Loss after 569445060 batches: 0.0216
trigger times: 0
Loss after 569576160 batches: 0.0189
trigger times: 1
Loss after 569707260 batches: 0.0172
trigger times: 2
Loss after 569838360 batches: 0.0161
trigger times: 3
Loss after 569969460 batches: 0.0155
trigger times: 4
Loss after 570100560 batches: 0.0146
trigger times: 5
Loss after 570231660 batches: 0.0143
trigger times: 6
Loss after 570362760 batches: 0.0136
trigger times: 7
Loss after 570493860 batches: 0.0133
trigger times: 8
Loss after 570624960 batches: 0.0131
trigger times: 9
Loss after 570756060 batches: 0.0125
trigger times: 10
Loss after 570887160 batches: 0.0123
trigger times: 11
Loss after 571018260 batches: 0.0121
trigger times: 12
Loss after 571149360 batches: 0.0119
trigger times: 13
Loss after 571280460 batches: 0.0118
trigger times: 14
Loss after 571411560 batches: 0.0117
trigger times: 15
Loss after 571542660 batches: 0.0115
trigger times: 16
Loss after 571673760 batches: 0.0111
trigger times: 17
Loss after 571804860 batches: 0.0113
trigger times: 18
Loss after 571935960 batches: 0.0110
trigger times: 19
Loss after 572067060 batches: 0.0108
trigger times: 20
Early stopping!
Start to test process.
Loss after 572198160 batches: 0.0108
Time to train on one home:  184.30568552017212
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318, 0.016183730855002625, 0.015608215882864637, 0.015855816283001423]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165], [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777], [0.5659677750534482, 0.3881937670236807, 0.47736113706344846, 1.0466814163269424, 0.5011883546712889, 24.728223616247963, 1547.1996], [0.5552795496251848, 0.39972320473570555, 0.4869199067938419, 1.0432319579028642, 0.4917435017003341, 24.646728924611136, 1518.0426]]
Round_10_results:  [0.5552795496251848, 0.39972320473570555, 0.4869199067938419, 1.0432319579028642, 0.4917435017003341, 24.646728924611136, 1518.0426]
trigger times: 0
Loss after 572329260 batches: 0.1147
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4640 < 4641; dropping {'Training_Loss': 0.11470732254520902, 'Validation_Loss': 0.1850043824977345, 'Training_R2': 0.8845421184407465, 'Validation_R2': 0.828045788562604, 'Training_F1': 0.8242547021773557, 'Validation_F1': 0.721797822226413, 'Training_NEP': 0.35144548434100614, 'Validation_NEP': 0.5086896712396574, 'Training_NDE': 0.08667628106710552, 'Validation_NDE': 0.13693526957560317, 'Training_MAE': 11.639177449820698, 'Validation_MAE': 13.950381209559765, 'Training_MSE': 381.36206, 'Validation_MSE': 505.69815}.
trigger times: 0
Loss after 572460360 batches: 0.0314
trigger times: 1
Loss after 572591460 batches: 0.0223
trigger times: 2
Loss after 572722560 batches: 0.0194
trigger times: 3
Loss after 572853660 batches: 0.0177
trigger times: 4
Loss after 572984760 batches: 0.0166
trigger times: 5
Loss after 573115860 batches: 0.0155
trigger times: 6
Loss after 573246960 batches: 0.0148
trigger times: 7
Loss after 573378060 batches: 0.0145
trigger times: 8
Loss after 573509160 batches: 0.0140
trigger times: 9
Loss after 573640260 batches: 0.0137
trigger times: 10
Loss after 573771360 batches: 0.0130
trigger times: 11
Loss after 573902460 batches: 0.0130
trigger times: 0
Loss after 574033560 batches: 0.0126
trigger times: 1
Loss after 574164660 batches: 0.0129
trigger times: 2
Loss after 574295760 batches: 0.0122
trigger times: 3
Loss after 574426860 batches: 0.0118
trigger times: 4
Loss after 574557960 batches: 0.0117
trigger times: 5
Loss after 574689060 batches: 0.0118
trigger times: 6
Loss after 574820160 batches: 0.0116
trigger times: 7
Loss after 574951260 batches: 0.0113
trigger times: 0
Loss after 575082360 batches: 0.0109
trigger times: 1
Loss after 575213460 batches: 0.0112
trigger times: 2
Loss after 575344560 batches: 0.0108
trigger times: 3
Loss after 575475660 batches: 0.0109
trigger times: 0
Loss after 575606760 batches: 0.0110
trigger times: 1
Loss after 575737860 batches: 0.0107
trigger times: 2
Loss after 575868960 batches: 0.0108
trigger times: 3
Loss after 576000060 batches: 0.0103
trigger times: 4
Loss after 576131160 batches: 0.0104
trigger times: 5
Loss after 576262260 batches: 0.0103
trigger times: 6
Loss after 576393360 batches: 0.0104
trigger times: 7
Loss after 576524460 batches: 0.0102
trigger times: 8
Loss after 576655560 batches: 0.0102
trigger times: 9
Loss after 576786660 batches: 0.0103
trigger times: 10
Loss after 576917760 batches: 0.0100
trigger times: 11
Loss after 577048860 batches: 0.0099
trigger times: 12
Loss after 577179960 batches: 0.0099
trigger times: 13
Loss after 577311060 batches: 0.0098
trigger times: 14
Loss after 577442160 batches: 0.0097
trigger times: 15
Loss after 577573260 batches: 0.0097
trigger times: 16
Loss after 577704360 batches: 0.0094
trigger times: 17
Loss after 577835460 batches: 0.0093
trigger times: 18
Loss after 577966560 batches: 0.0092
trigger times: 19
Loss after 578097660 batches: 0.0093
trigger times: 20
Early stopping!
Start to test process.
Loss after 578228760 batches: 0.0094
Time to train on one home:  342.75278997421265
trigger times: 0
Loss after 578331360 batches: 0.2612
trigger times: 1
Loss after 578433960 batches: 0.0826
trigger times: 0
Loss after 578536560 batches: 0.0540
trigger times: 1
Loss after 578639160 batches: 0.0438
trigger times: 0
Loss after 578741760 batches: 0.0386
trigger times: 1
Loss after 578844360 batches: 0.0399
trigger times: 2
Loss after 578946960 batches: 0.0338
trigger times: 3
Loss after 579049560 batches: 0.0345
trigger times: 4
Loss after 579152160 batches: 0.0294
trigger times: 5
Loss after 579254760 batches: 0.0281
trigger times: 6
Loss after 579357360 batches: 0.0269
trigger times: 7
Loss after 579459960 batches: 0.0263
trigger times: 8
Loss after 579562560 batches: 0.0255
trigger times: 9
Loss after 579665160 batches: 0.0269
trigger times: 10
Loss after 579767760 batches: 0.0256
trigger times: 11
Loss after 579870360 batches: 0.0247
trigger times: 12
Loss after 579972960 batches: 0.0243
trigger times: 13
Loss after 580075560 batches: 0.0230
trigger times: 14
Loss after 580178160 batches: 0.0231
trigger times: 15
Loss after 580280760 batches: 0.0224
trigger times: 16
Loss after 580383360 batches: 0.0225
trigger times: 17
Loss after 580485960 batches: 0.0216
trigger times: 18
Loss after 580588560 batches: 0.0213
trigger times: 19
Loss after 580691160 batches: 0.0212
trigger times: 20
Early stopping!
Start to test process.
Loss after 580793760 batches: 0.0213
Time to train on one home:  156.69038319587708
trigger times: 0
Loss after 580924860 batches: 0.1843
trigger times: 0
Loss after 581055960 batches: 0.0551
trigger times: 1
Loss after 581187060 batches: 0.0398
trigger times: 2
Loss after 581318160 batches: 0.0337
trigger times: 0
Loss after 581449260 batches: 0.0302
trigger times: 1
Loss after 581580360 batches: 0.0284
trigger times: 2
Loss after 581711460 batches: 0.0265
trigger times: 3
Loss after 581842560 batches: 0.0254
trigger times: 4
Loss after 581973660 batches: 0.0245
trigger times: 5
Loss after 582104760 batches: 0.0236
trigger times: 6
Loss after 582235860 batches: 0.0229
trigger times: 0
Loss after 582366960 batches: 0.0220
trigger times: 1
Loss after 582498060 batches: 0.0217
trigger times: 2
Loss after 582629160 batches: 0.0212
trigger times: 3
Loss after 582760260 batches: 0.0207
trigger times: 4
Loss after 582891360 batches: 0.0201
trigger times: 5
Loss after 583022460 batches: 0.0197
trigger times: 6
Loss after 583153560 batches: 0.0197
trigger times: 7
Loss after 583284660 batches: 0.0194
trigger times: 8
Loss after 583415760 batches: 0.0188
trigger times: 9
Loss after 583546860 batches: 0.0186
trigger times: 10
Loss after 583677960 batches: 0.0185
trigger times: 11
Loss after 583809060 batches: 0.0181
trigger times: 12
Loss after 583940160 batches: 0.0179
trigger times: 13
Loss after 584071260 batches: 0.0179
trigger times: 14
Loss after 584202360 batches: 0.0175
trigger times: 15
Loss after 584333460 batches: 0.0174
trigger times: 16
Loss after 584464560 batches: 0.0173
trigger times: 17
Loss after 584595660 batches: 0.0170
trigger times: 18
Loss after 584726760 batches: 0.0169
trigger times: 19
Loss after 584857860 batches: 0.0169
trigger times: 20
Early stopping!
Start to test process.
Loss after 584988960 batches: 0.0166
Time to train on one home:  241.4094307422638
trigger times: 0
Loss after 585120060 batches: 0.2287
trigger times: 0
Loss after 585251160 batches: 0.0693
trigger times: 0
Loss after 585382260 batches: 0.0485
trigger times: 1
Loss after 585513360 batches: 0.0413
trigger times: 2
Loss after 585644460 batches: 0.0379
trigger times: 3
Loss after 585775560 batches: 0.0348
trigger times: 0
Loss after 585906660 batches: 0.0338
trigger times: 1
Loss after 586037760 batches: 0.0318
trigger times: 2
Loss after 586168860 batches: 0.0304
trigger times: 3
Loss after 586299960 batches: 0.0291
trigger times: 0
Loss after 586431060 batches: 0.0281
trigger times: 1
Loss after 586562160 batches: 0.0274
trigger times: 0
Loss after 586693260 batches: 0.0273
trigger times: 0
Loss after 586824360 batches: 0.0266
trigger times: 1
Loss after 586955460 batches: 0.0260
trigger times: 2
Loss after 587086560 batches: 0.0255
trigger times: 3
Loss after 587217660 batches: 0.0251
trigger times: 4
Loss after 587348760 batches: 0.0248
trigger times: 5
Loss after 587479860 batches: 0.0242
trigger times: 6
Loss after 587610960 batches: 0.0243
trigger times: 7
Loss after 587742060 batches: 0.0238
trigger times: 8
Loss after 587873160 batches: 0.0239
trigger times: 9
Loss after 588004260 batches: 0.0234
trigger times: 10
Loss after 588135360 batches: 0.0235
trigger times: 11
Loss after 588266460 batches: 0.0232
trigger times: 12
Loss after 588397560 batches: 0.0229
trigger times: 13
Loss after 588528660 batches: 0.0226
trigger times: 14
Loss after 588659760 batches: 0.0225
trigger times: 15
Loss after 588790860 batches: 0.0221
trigger times: 16
Loss after 588921960 batches: 0.0216
trigger times: 17
Loss after 589053060 batches: 0.0217
trigger times: 18
Loss after 589184160 batches: 0.0212
trigger times: 19
Loss after 589315260 batches: 0.0214
trigger times: 0
Loss after 589446360 batches: 0.0213
trigger times: 1
Loss after 589577460 batches: 0.0211
trigger times: 2
Loss after 589708560 batches: 0.0211
trigger times: 3
Loss after 589839660 batches: 0.0208
trigger times: 4
Loss after 589970760 batches: 0.0201
trigger times: 5
Loss after 590101860 batches: 0.0208
trigger times: 6
Loss after 590232960 batches: 0.0206
trigger times: 7
Loss after 590364060 batches: 0.0201
trigger times: 8
Loss after 590495160 batches: 0.0206
trigger times: 9
Loss after 590626260 batches: 0.0202
trigger times: 10
Loss after 590757360 batches: 0.0201
trigger times: 11
Loss after 590888460 batches: 0.0203
trigger times: 12
Loss after 591019560 batches: 0.0199
trigger times: 13
Loss after 591150660 batches: 0.0199
trigger times: 14
Loss after 591281760 batches: 0.0196
trigger times: 15
Loss after 591412860 batches: 0.0193
trigger times: 16
Loss after 591543960 batches: 0.0194
trigger times: 17
Loss after 591675060 batches: 0.0197
trigger times: 18
Loss after 591806160 batches: 0.0191
trigger times: 19
Loss after 591937260 batches: 0.0193
trigger times: 20
Early stopping!
Start to test process.
Loss after 592068360 batches: 0.0191
Time to train on one home:  398.7908990383148
trigger times: 0
Loss after 592197000 batches: 0.1347
trigger times: 0
Loss after 592325640 batches: 0.0374
trigger times: 1
Loss after 592454280 batches: 0.0269
trigger times: 0
Loss after 592582920 batches: 0.0232
trigger times: 1
Loss after 592711560 batches: 0.0209
trigger times: 2
Loss after 592840200 batches: 0.0193
trigger times: 3
Loss after 592968840 batches: 0.0183
trigger times: 0
Loss after 593097480 batches: 0.0173
trigger times: 0
Loss after 593226120 batches: 0.0166
trigger times: 1
Loss after 593354760 batches: 0.0165
trigger times: 2
Loss after 593483400 batches: 0.0157
trigger times: 3
Loss after 593612040 batches: 0.0151
trigger times: 0
Loss after 593740680 batches: 0.0150
trigger times: 0
Loss after 593869320 batches: 0.0147
trigger times: 1
Loss after 593997960 batches: 0.0144
trigger times: 2
Loss after 594126600 batches: 0.0140
trigger times: 3
Loss after 594255240 batches: 0.0140
trigger times: 4
Loss after 594383880 batches: 0.0136
trigger times: 0
Loss after 594512520 batches: 0.0135
trigger times: 1
Loss after 594641160 batches: 0.0129
trigger times: 0
Loss after 594769800 batches: 0.0131
trigger times: 0
Loss after 594898440 batches: 0.0129
trigger times: 1
Loss after 595027080 batches: 0.0127
trigger times: 2
Loss after 595155720 batches: 0.0127
trigger times: 3
Loss after 595284360 batches: 0.0127
trigger times: 4
Loss after 595413000 batches: 0.0125
trigger times: 5
Loss after 595541640 batches: 0.0123
trigger times: 6
Loss after 595670280 batches: 0.0121
trigger times: 7
Loss after 595798920 batches: 0.0119
trigger times: 8
Loss after 595927560 batches: 0.0119
trigger times: 9
Loss after 596056200 batches: 0.0120
trigger times: 10
Loss after 596184840 batches: 0.0118
trigger times: 11
Loss after 596313480 batches: 0.0116
trigger times: 12
Loss after 596442120 batches: 0.0117
trigger times: 13
Loss after 596570760 batches: 0.0118
trigger times: 14
Loss after 596699400 batches: 0.0113
trigger times: 15
Loss after 596828040 batches: 0.0114
trigger times: 16
Loss after 596956680 batches: 0.0114
trigger times: 17
Loss after 597085320 batches: 0.0111
trigger times: 18
Loss after 597213960 batches: 0.0113
trigger times: 19
Loss after 597342600 batches: 0.0113
trigger times: 20
Early stopping!
Start to test process.
Loss after 597471240 batches: 0.0111
Time to train on one home:  307.58704686164856
trigger times: 0
Loss after 597602340 batches: 0.2825
trigger times: 0
Loss after 597733440 batches: 0.0727
trigger times: 1
Loss after 597864540 batches: 0.0523
trigger times: 2
Loss after 597995640 batches: 0.0449
trigger times: 0
Loss after 598126740 batches: 0.0410
trigger times: 1
Loss after 598257840 batches: 0.0379
trigger times: 2
Loss after 598388940 batches: 0.0355
trigger times: 3
Loss after 598520040 batches: 0.0341
trigger times: 4
Loss after 598651140 batches: 0.0327
trigger times: 5
Loss after 598782240 batches: 0.0324
trigger times: 6
Loss after 598913340 batches: 0.0310
trigger times: 7
Loss after 599044440 batches: 0.0297
trigger times: 8
Loss after 599175540 batches: 0.0288
trigger times: 9
Loss after 599306640 batches: 0.0289
trigger times: 0
Loss after 599437740 batches: 0.0282
trigger times: 1
Loss after 599568840 batches: 0.0275
trigger times: 2
Loss after 599699940 batches: 0.0270
trigger times: 3
Loss after 599831040 batches: 0.0268
trigger times: 4
Loss after 599962140 batches: 0.0262
trigger times: 5
Loss after 600093240 batches: 0.0258
trigger times: 6
Loss after 600224340 batches: 0.0256
trigger times: 7
Loss after 600355440 batches: 0.0253
trigger times: 8
Loss after 600486540 batches: 0.0247
trigger times: 9
Loss after 600617640 batches: 0.0244
trigger times: 10
Loss after 600748740 batches: 0.0241
trigger times: 11
Loss after 600879840 batches: 0.0239
trigger times: 12
Loss after 601010940 batches: 0.0237
trigger times: 13
Loss after 601142040 batches: 0.0235
trigger times: 0
Loss after 601273140 batches: 0.0237
trigger times: 1
Loss after 601404240 batches: 0.0231
trigger times: 2
Loss after 601535340 batches: 0.0234
trigger times: 3
Loss after 601666440 batches: 0.0228
trigger times: 4
Loss after 601797540 batches: 0.0225
trigger times: 5
Loss after 601928640 batches: 0.0223
trigger times: 6
Loss after 602059740 batches: 0.0224
trigger times: 7
Loss after 602190840 batches: 0.0223
trigger times: 8
Loss after 602321940 batches: 0.0218
trigger times: 9
Loss after 602453040 batches: 0.0216
trigger times: 10
Loss after 602584140 batches: 0.0218
trigger times: 11
Loss after 602715240 batches: 0.0213
trigger times: 12
Loss after 602846340 batches: 0.0213
trigger times: 13
Loss after 602977440 batches: 0.0214
trigger times: 14
Loss after 603108540 batches: 0.0212
trigger times: 15
Loss after 603239640 batches: 0.0207
trigger times: 16
Loss after 603370740 batches: 0.0209
trigger times: 17
Loss after 603501840 batches: 0.0207
trigger times: 18
Loss after 603632940 batches: 0.0205
trigger times: 19
Loss after 603764040 batches: 0.0205
trigger times: 20
Early stopping!
Start to test process.
Loss after 603895140 batches: 0.0203
Time to train on one home:  363.73175978660583
trigger times: 0
Loss after 604026240 batches: 0.2955
trigger times: 0
Loss after 604157340 batches: 0.0937
trigger times: 1
Loss after 604288440 batches: 0.0598
trigger times: 0
Loss after 604419540 batches: 0.0458
trigger times: 1
Loss after 604550640 batches: 0.0394
trigger times: 2
Loss after 604681740 batches: 0.0373
trigger times: 0
Loss after 604812840 batches: 0.0350
trigger times: 0
Loss after 604943940 batches: 0.0324
trigger times: 1
Loss after 605075040 batches: 0.0328
trigger times: 2
Loss after 605206140 batches: 0.0316
trigger times: 0
Loss after 605337240 batches: 0.0300
trigger times: 1
Loss after 605468340 batches: 0.0292
trigger times: 2
Loss after 605599440 batches: 0.0281
trigger times: 0
Loss after 605730540 batches: 0.0295
trigger times: 1
Loss after 605861640 batches: 0.0286
trigger times: 2
Loss after 605992740 batches: 0.0271
trigger times: 0
Loss after 606123840 batches: 0.0277
trigger times: 1
Loss after 606254940 batches: 0.0268
trigger times: 2
Loss after 606386040 batches: 0.0272
trigger times: 3
Loss after 606517140 batches: 0.0268
trigger times: 4
Loss after 606648240 batches: 0.0260
trigger times: 0
Loss after 606779340 batches: 0.0256
trigger times: 1
Loss after 606910440 batches: 0.0252
trigger times: 2
Loss after 607041540 batches: 0.0250
trigger times: 3
Loss after 607172640 batches: 0.0253
trigger times: 0
Loss after 607303740 batches: 0.0249
trigger times: 0
Loss after 607434840 batches: 0.0238
trigger times: 1
Loss after 607565940 batches: 0.0241
trigger times: 2
Loss after 607697040 batches: 0.0239
trigger times: 3
Loss after 607828140 batches: 0.0238
trigger times: 4
Loss after 607959240 batches: 0.0230
trigger times: 0
Loss after 608090340 batches: 0.0238
trigger times: 1
Loss after 608221440 batches: 0.0236
trigger times: 2
Loss after 608352540 batches: 0.0236
trigger times: 3
Loss after 608483640 batches: 0.0228
trigger times: 4
Loss after 608614740 batches: 0.0233
trigger times: 5
Loss after 608745840 batches: 0.0236
trigger times: 6
Loss after 608876940 batches: 0.0229
trigger times: 7
Loss after 609008040 batches: 0.0230
trigger times: 8
Loss after 609139140 batches: 0.0229
trigger times: 9
Loss after 609270240 batches: 0.0227
trigger times: 10
Loss after 609401340 batches: 0.0223
trigger times: 11
Loss after 609532440 batches: 0.0221
trigger times: 12
Loss after 609663540 batches: 0.0231
trigger times: 13
Loss after 609794640 batches: 0.0219
trigger times: 14
Loss after 609925740 batches: 0.0218
trigger times: 15
Loss after 610056840 batches: 0.0221
trigger times: 16
Loss after 610187940 batches: 0.0219
trigger times: 17
Loss after 610319040 batches: 0.0208
trigger times: 18
Loss after 610450140 batches: 0.0204
trigger times: 19
Loss after 610581240 batches: 0.0217
trigger times: 20
Early stopping!
Start to test process.
Loss after 610712340 batches: 0.0219
Time to train on one home:  385.68870639801025
trigger times: 0
Loss after 610843440 batches: 0.0768
trigger times: 0
Loss after 610974540 batches: 0.0243
trigger times: 0
Loss after 611105640 batches: 0.0180
trigger times: 0
Loss after 611236740 batches: 0.0157
trigger times: 0
Loss after 611367840 batches: 0.0143
trigger times: 1
Loss after 611498940 batches: 0.0135
trigger times: 2
Loss after 611630040 batches: 0.0125
trigger times: 3
Loss after 611761140 batches: 0.0123
trigger times: 0
Loss after 611892240 batches: 0.0120
trigger times: 1
Loss after 612023340 batches: 0.0112
trigger times: 2
Loss after 612154440 batches: 0.0111
trigger times: 3
Loss after 612285540 batches: 0.0108
trigger times: 4
Loss after 612416640 batches: 0.0106
trigger times: 5
Loss after 612547740 batches: 0.0105
trigger times: 6
Loss after 612678840 batches: 0.0102
trigger times: 7
Loss after 612809940 batches: 0.0101
trigger times: 8
Loss after 612941040 batches: 0.0098
trigger times: 9
Loss after 613072140 batches: 0.0098
trigger times: 10
Loss after 613203240 batches: 0.0096
trigger times: 11
Loss after 613334340 batches: 0.0093
trigger times: 12
Loss after 613465440 batches: 0.0095
trigger times: 13
Loss after 613596540 batches: 0.0094
trigger times: 14
Loss after 613727640 batches: 0.0092
trigger times: 15
Loss after 613858740 batches: 0.0090
trigger times: 16
Loss after 613989840 batches: 0.0092
trigger times: 17
Loss after 614120940 batches: 0.0091
trigger times: 18
Loss after 614252040 batches: 0.0091
trigger times: 19
Loss after 614383140 batches: 0.0088
trigger times: 20
Early stopping!
Start to test process.
Loss after 614514240 batches: 0.0086
Time to train on one home:  219.5067744255066
trigger times: 0
Loss after 614592840 batches: 0.2565
trigger times: 0
Loss after 614671440 batches: 0.0632
trigger times: 0
Loss after 614750040 batches: 0.0422
trigger times: 0
Loss after 614828640 batches: 0.0342
trigger times: 1
Loss after 614907240 batches: 0.0304
trigger times: 2
Loss after 614985840 batches: 0.0282
trigger times: 0
Loss after 615064440 batches: 0.0267
trigger times: 0
Loss after 615143040 batches: 0.0251
trigger times: 1
Loss after 615221640 batches: 0.0241
trigger times: 0
Loss after 615300240 batches: 0.0228
trigger times: 1
Loss after 615378840 batches: 0.0218
trigger times: 2
Loss after 615457440 batches: 0.0216
trigger times: 3
Loss after 615536040 batches: 0.0209
trigger times: 4
Loss after 615614640 batches: 0.0203
trigger times: 5
Loss after 615693240 batches: 0.0199
trigger times: 0
Loss after 615771840 batches: 0.0194
trigger times: 1
Loss after 615850440 batches: 0.0190
trigger times: 0
Loss after 615929040 batches: 0.0193
trigger times: 1
Loss after 616007640 batches: 0.0195
trigger times: 2
Loss after 616086240 batches: 0.0187
trigger times: 3
Loss after 616164840 batches: 0.0184
trigger times: 4
Loss after 616243440 batches: 0.0188
trigger times: 5
Loss after 616322040 batches: 0.0181
trigger times: 6
Loss after 616400640 batches: 0.0180
trigger times: 7
Loss after 616479240 batches: 0.0170
trigger times: 8
Loss after 616557840 batches: 0.0173
trigger times: 9
Loss after 616636440 batches: 0.0171
trigger times: 10
Loss after 616715040 batches: 0.0174
trigger times: 11
Loss after 616793640 batches: 0.0165
trigger times: 12
Loss after 616872240 batches: 0.0166
trigger times: 13
Loss after 616950840 batches: 0.0164
trigger times: 14
Loss after 617029440 batches: 0.0162
trigger times: 15
Loss after 617108040 batches: 0.0157
trigger times: 16
Loss after 617186640 batches: 0.0160
trigger times: 17
Loss after 617265240 batches: 0.0156
trigger times: 0
Loss after 617343840 batches: 0.0151
trigger times: 1
Loss after 617422440 batches: 0.0156
trigger times: 2
Loss after 617501040 batches: 0.0153
trigger times: 3
Loss after 617579640 batches: 0.0155
trigger times: 4
Loss after 617658240 batches: 0.0154
trigger times: 5
Loss after 617736840 batches: 0.0152
trigger times: 6
Loss after 617815440 batches: 0.0154
trigger times: 7
Loss after 617894040 batches: 0.0147
trigger times: 8
Loss after 617972640 batches: 0.0148
trigger times: 9
Loss after 618051240 batches: 0.0146
trigger times: 10
Loss after 618129840 batches: 0.0152
trigger times: 11
Loss after 618208440 batches: 0.0150
trigger times: 12
Loss after 618287040 batches: 0.0147
trigger times: 13
Loss after 618365640 batches: 0.0147
trigger times: 14
Loss after 618444240 batches: 0.0143
trigger times: 15
Loss after 618522840 batches: 0.0140
trigger times: 16
Loss after 618601440 batches: 0.0143
trigger times: 17
Loss after 618680040 batches: 0.0137
trigger times: 18
Loss after 618758640 batches: 0.0147
trigger times: 19
Loss after 618837240 batches: 0.0141
trigger times: 20
Early stopping!
Start to test process.
Loss after 618915840 batches: 0.0139
Time to train on one home:  270.65237259864807
trigger times: 0
Loss after 619046940 batches: 0.0919
trigger times: 0
Loss after 619178040 batches: 0.0275
trigger times: 0
Loss after 619309140 batches: 0.0211
trigger times: 1
Loss after 619440240 batches: 0.0181
trigger times: 2
Loss after 619571340 batches: 0.0168
trigger times: 3
Loss after 619702440 batches: 0.0156
trigger times: 4
Loss after 619833540 batches: 0.0150
trigger times: 5
Loss after 619964640 batches: 0.0142
trigger times: 6
Loss after 620095740 batches: 0.0139
trigger times: 7
Loss after 620226840 batches: 0.0131
trigger times: 8
Loss after 620357940 batches: 0.0128
trigger times: 9
Loss after 620489040 batches: 0.0127
trigger times: 10
Loss after 620620140 batches: 0.0127
trigger times: 11
Loss after 620751240 batches: 0.0122
trigger times: 12
Loss after 620882340 batches: 0.0120
trigger times: 13
Loss after 621013440 batches: 0.0117
trigger times: 14
Loss after 621144540 batches: 0.0117
trigger times: 15
Loss after 621275640 batches: 0.0113
trigger times: 16
Loss after 621406740 batches: 0.0113
trigger times: 17
Loss after 621537840 batches: 0.0109
trigger times: 18
Loss after 621668940 batches: 0.0109
trigger times: 19
Loss after 621800040 batches: 0.0107
trigger times: 20
Early stopping!
Start to test process.
Loss after 621931140 batches: 0.0106
Time to train on one home:  177.4011209011078
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318, 0.016183730855002625, 0.015608215882864637, 0.015855816283001423, 0.015272240943982671]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165], [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777], [0.5659677750534482, 0.3881937670236807, 0.47736113706344846, 1.0466814163269424, 0.5011883546712889, 24.728223616247963, 1547.1996], [0.5552795496251848, 0.39972320473570555, 0.4869199067938419, 1.0432319579028642, 0.4917435017003341, 24.646728924611136, 1518.0426], [0.556764519876904, 0.3981234608309019, 0.4873119444985629, 1.0354352452336422, 0.4930540032485836, 24.462528793273435, 1522.0884]]
Round_11_results:  [0.556764519876904, 0.3981234608309019, 0.4873119444985629, 1.0354352452336422, 0.4930540032485836, 24.462528793273435, 1522.0884]
trigger times: 0
Loss after 622062240 batches: 0.1068
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5048 < 5049; dropping {'Training_Loss': 0.1067666194911273, 'Validation_Loss': 0.18783266262875664, 'Training_R2': 0.892531707604674, 'Validation_R2': 0.8255443530569332, 'Training_F1': 0.8315576362952813, 'Validation_F1': 0.7507095182903323, 'Training_NEP': 0.3365668663189431, 'Validation_NEP': 0.4710002944166008, 'Training_NDE': 0.08067835466631754, 'Validation_NDE': 0.1389272809513742, 'Training_MAE': 11.146427128411357, 'Validation_MAE': 12.916782133425436, 'Training_MSE': 354.97214, 'Validation_MSE': 513.05457}.
trigger times: 1
Loss after 622193340 batches: 0.0287
trigger times: 0
Loss after 622324440 batches: 0.0216
trigger times: 1
Loss after 622455540 batches: 0.0186
trigger times: 0
Loss after 622586640 batches: 0.0171
trigger times: 0
Loss after 622717740 batches: 0.0161
trigger times: 1
Loss after 622848840 batches: 0.0152
trigger times: 0
Loss after 622979940 batches: 0.0145
trigger times: 1
Loss after 623111040 batches: 0.0139
trigger times: 2
Loss after 623242140 batches: 0.0135
trigger times: 3
Loss after 623373240 batches: 0.0132
trigger times: 4
Loss after 623504340 batches: 0.0128
trigger times: 0
Loss after 623635440 batches: 0.0124
trigger times: 1
Loss after 623766540 batches: 0.0122
trigger times: 2
Loss after 623897640 batches: 0.0122
trigger times: 0
Loss after 624028740 batches: 0.0117
trigger times: 1
Loss after 624159840 batches: 0.0116
trigger times: 2
Loss after 624290940 batches: 0.0115
trigger times: 3
Loss after 624422040 batches: 0.0112
trigger times: 4
Loss after 624553140 batches: 0.0108
trigger times: 0
Loss after 624684240 batches: 0.0108
trigger times: 1
Loss after 624815340 batches: 0.0109
trigger times: 2
Loss after 624946440 batches: 0.0107
trigger times: 3
Loss after 625077540 batches: 0.0109
trigger times: 4
Loss after 625208640 batches: 0.0105
trigger times: 5
Loss after 625339740 batches: 0.0105
trigger times: 6
Loss after 625470840 batches: 0.0104
trigger times: 7
Loss after 625601940 batches: 0.0102
trigger times: 8
Loss after 625733040 batches: 0.0100
trigger times: 9
Loss after 625864140 batches: 0.0102
trigger times: 10
Loss after 625995240 batches: 0.0101
trigger times: 11
Loss after 626126340 batches: 0.0099
trigger times: 12
Loss after 626257440 batches: 0.0096
trigger times: 13
Loss after 626388540 batches: 0.0098
trigger times: 14
Loss after 626519640 batches: 0.0096
trigger times: 15
Loss after 626650740 batches: 0.0097
trigger times: 16
Loss after 626781840 batches: 0.0096
trigger times: 17
Loss after 626912940 batches: 0.0093
trigger times: 18
Loss after 627044040 batches: 0.0096
trigger times: 19
Loss after 627175140 batches: 0.0095
trigger times: 20
Early stopping!
Start to test process.
Loss after 627306240 batches: 0.0094
Time to train on one home:  304.68265438079834
trigger times: 0
Loss after 627408840 batches: 0.2580
trigger times: 1
Loss after 627511440 batches: 0.0811
trigger times: 2
Loss after 627614040 batches: 0.0510
trigger times: 3
Loss after 627716640 batches: 0.0431
trigger times: 4
Loss after 627819240 batches: 0.0381
trigger times: 5
Loss after 627921840 batches: 0.0366
trigger times: 6
Loss after 628024440 batches: 0.0315
trigger times: 7
Loss after 628127040 batches: 0.0300
trigger times: 8
Loss after 628229640 batches: 0.0285
trigger times: 9
Loss after 628332240 batches: 0.0289
trigger times: 10
Loss after 628434840 batches: 0.0268
trigger times: 11
Loss after 628537440 batches: 0.0279
trigger times: 12
Loss after 628640040 batches: 0.0272
trigger times: 13
Loss after 628742640 batches: 0.0255
trigger times: 14
Loss after 628845240 batches: 0.0248
trigger times: 15
Loss after 628947840 batches: 0.0240
trigger times: 0
Loss after 629050440 batches: 0.0252
trigger times: 1
Loss after 629153040 batches: 0.0260
trigger times: 2
Loss after 629255640 batches: 0.0228
trigger times: 3
Loss after 629358240 batches: 0.0219
trigger times: 4
Loss after 629460840 batches: 0.0222
trigger times: 5
Loss after 629563440 batches: 0.0217
trigger times: 0
Loss after 629666040 batches: 0.0212
trigger times: 1
Loss after 629768640 batches: 0.0216
trigger times: 2
Loss after 629871240 batches: 0.0219
trigger times: 3
Loss after 629973840 batches: 0.0209
trigger times: 0
Loss after 630076440 batches: 0.0243
trigger times: 1
Loss after 630179040 batches: 0.0207
trigger times: 2
Loss after 630281640 batches: 0.0194
trigger times: 3
Loss after 630384240 batches: 0.0192
trigger times: 0
Loss after 630486840 batches: 0.0189
trigger times: 1
Loss after 630589440 batches: 0.0195
trigger times: 2
Loss after 630692040 batches: 0.0214
trigger times: 3
Loss after 630794640 batches: 0.0195
trigger times: 4
Loss after 630897240 batches: 0.0198
trigger times: 5
Loss after 630999840 batches: 0.0192
trigger times: 6
Loss after 631102440 batches: 0.0186
trigger times: 7
Loss after 631205040 batches: 0.0206
trigger times: 8
Loss after 631307640 batches: 0.0204
trigger times: 9
Loss after 631410240 batches: 0.0186
trigger times: 10
Loss after 631512840 batches: 0.0191
trigger times: 11
Loss after 631615440 batches: 0.0178
trigger times: 12
Loss after 631718040 batches: 0.0181
trigger times: 13
Loss after 631820640 batches: 0.0179
trigger times: 14
Loss after 631923240 batches: 0.0176
trigger times: 15
Loss after 632025840 batches: 0.0187
trigger times: 16
Loss after 632128440 batches: 0.0176
trigger times: 17
Loss after 632231040 batches: 0.0178
trigger times: 18
Loss after 632333640 batches: 0.0180
trigger times: 0
Loss after 632436240 batches: 0.0177
trigger times: 1
Loss after 632538840 batches: 0.0178
trigger times: 2
Loss after 632641440 batches: 0.0189
trigger times: 3
Loss after 632744040 batches: 0.0209
trigger times: 4
Loss after 632846640 batches: 0.0207
trigger times: 5
Loss after 632949240 batches: 0.0175
trigger times: 6
Loss after 633051840 batches: 0.0169
trigger times: 7
Loss after 633154440 batches: 0.0170
trigger times: 8
Loss after 633257040 batches: 0.0163
trigger times: 9
Loss after 633359640 batches: 0.0162
trigger times: 10
Loss after 633462240 batches: 0.0160
trigger times: 11
Loss after 633564840 batches: 0.0160
trigger times: 12
Loss after 633667440 batches: 0.0164
trigger times: 13
Loss after 633770040 batches: 0.0167
trigger times: 14
Loss after 633872640 batches: 0.0172
trigger times: 15
Loss after 633975240 batches: 0.0165
trigger times: 16
Loss after 634077840 batches: 0.0161
trigger times: 17
Loss after 634180440 batches: 0.0170
trigger times: 18
Loss after 634283040 batches: 0.0163
trigger times: 19
Loss after 634385640 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 634488240 batches: 0.0157
Time to train on one home:  413.76073122024536
trigger times: 0
Loss after 634619340 batches: 0.1864
trigger times: 1
Loss after 634750440 batches: 0.0547
trigger times: 2
Loss after 634881540 batches: 0.0386
trigger times: 3
Loss after 635012640 batches: 0.0331
trigger times: 4
Loss after 635143740 batches: 0.0298
trigger times: 5
Loss after 635274840 batches: 0.0277
trigger times: 6
Loss after 635405940 batches: 0.0261
trigger times: 7
Loss after 635537040 batches: 0.0246
trigger times: 8
Loss after 635668140 batches: 0.0240
trigger times: 9
Loss after 635799240 batches: 0.0232
trigger times: 10
Loss after 635930340 batches: 0.0221
trigger times: 11
Loss after 636061440 batches: 0.0220
trigger times: 12
Loss after 636192540 batches: 0.0214
trigger times: 13
Loss after 636323640 batches: 0.0205
trigger times: 14
Loss after 636454740 batches: 0.0204
trigger times: 15
Loss after 636585840 batches: 0.0201
trigger times: 16
Loss after 636716940 batches: 0.0195
trigger times: 17
Loss after 636848040 batches: 0.0193
trigger times: 18
Loss after 636979140 batches: 0.0191
trigger times: 19
Loss after 637110240 batches: 0.0186
trigger times: 20
Early stopping!
Start to test process.
Loss after 637241340 batches: 0.0185
Time to train on one home:  162.85185599327087
trigger times: 0
Loss after 637372440 batches: 0.2233
trigger times: 0
Loss after 637503540 batches: 0.0636
trigger times: 0
Loss after 637634640 batches: 0.0465
trigger times: 1
Loss after 637765740 batches: 0.0400
trigger times: 0
Loss after 637896840 batches: 0.0361
trigger times: 0
Loss after 638027940 batches: 0.0336
trigger times: 0
Loss after 638159040 batches: 0.0322
trigger times: 1
Loss after 638290140 batches: 0.0306
trigger times: 2
Loss after 638421240 batches: 0.0297
trigger times: 3
Loss after 638552340 batches: 0.0286
trigger times: 0
Loss after 638683440 batches: 0.0278
trigger times: 1
Loss after 638814540 batches: 0.0273
trigger times: 0
Loss after 638945640 batches: 0.0270
trigger times: 1
Loss after 639076740 batches: 0.0263
trigger times: 2
Loss after 639207840 batches: 0.0252
trigger times: 3
Loss after 639338940 batches: 0.0252
trigger times: 0
Loss after 639470040 batches: 0.0245
trigger times: 1
Loss after 639601140 batches: 0.0242
trigger times: 2
Loss after 639732240 batches: 0.0243
trigger times: 3
Loss after 639863340 batches: 0.0238
trigger times: 4
Loss after 639994440 batches: 0.0233
trigger times: 0
Loss after 640125540 batches: 0.0232
trigger times: 1
Loss after 640256640 batches: 0.0231
trigger times: 2
Loss after 640387740 batches: 0.0228
trigger times: 3
Loss after 640518840 batches: 0.0224
trigger times: 4
Loss after 640649940 batches: 0.0223
trigger times: 5
Loss after 640781040 batches: 0.0220
trigger times: 6
Loss after 640912140 batches: 0.0215
trigger times: 7
Loss after 641043240 batches: 0.0217
trigger times: 8
Loss after 641174340 batches: 0.0216
trigger times: 9
Loss after 641305440 batches: 0.0211
trigger times: 10
Loss after 641436540 batches: 0.0213
trigger times: 11
Loss after 641567640 batches: 0.0210
trigger times: 12
Loss after 641698740 batches: 0.0211
trigger times: 13
Loss after 641829840 batches: 0.0208
trigger times: 14
Loss after 641960940 batches: 0.0209
trigger times: 15
Loss after 642092040 batches: 0.0205
trigger times: 16
Loss after 642223140 batches: 0.0201
trigger times: 17
Loss after 642354240 batches: 0.0203
trigger times: 18
Loss after 642485340 batches: 0.0201
trigger times: 0
Loss after 642616440 batches: 0.0199
trigger times: 1
Loss after 642747540 batches: 0.0197
trigger times: 0
Loss after 642878640 batches: 0.0197
trigger times: 1
Loss after 643009740 batches: 0.0196
trigger times: 2
Loss after 643140840 batches: 0.0196
trigger times: 0
Loss after 643271940 batches: 0.0197
trigger times: 1
Loss after 643403040 batches: 0.0196
trigger times: 0
Loss after 643534140 batches: 0.0196
trigger times: 1
Loss after 643665240 batches: 0.0194
trigger times: 2
Loss after 643796340 batches: 0.0190
trigger times: 3
Loss after 643927440 batches: 0.0188
trigger times: 4
Loss after 644058540 batches: 0.0191
trigger times: 0
Loss after 644189640 batches: 0.0190
trigger times: 1
Loss after 644320740 batches: 0.0188
trigger times: 2
Loss after 644451840 batches: 0.0185
trigger times: 3
Loss after 644582940 batches: 0.0187
trigger times: 4
Loss after 644714040 batches: 0.0189
trigger times: 5
Loss after 644845140 batches: 0.0187
trigger times: 6
Loss after 644976240 batches: 0.0187
trigger times: 7
Loss after 645107340 batches: 0.0186
trigger times: 8
Loss after 645238440 batches: 0.0183
trigger times: 9
Loss after 645369540 batches: 0.0183
trigger times: 10
Loss after 645500640 batches: 0.0180
trigger times: 11
Loss after 645631740 batches: 0.0185
trigger times: 12
Loss after 645762840 batches: 0.0180
trigger times: 13
Loss after 645893940 batches: 0.0180
trigger times: 14
Loss after 646025040 batches: 0.0178
trigger times: 15
Loss after 646156140 batches: 0.0180
trigger times: 16
Loss after 646287240 batches: 0.0176
trigger times: 17
Loss after 646418340 batches: 0.0180
trigger times: 18
Loss after 646549440 batches: 0.0180
trigger times: 19
Loss after 646680540 batches: 0.0175
trigger times: 20
Early stopping!
Start to test process.
Loss after 646811640 batches: 0.0174
Time to train on one home:  536.0006201267242
trigger times: 0
Loss after 646940280 batches: 0.1005
trigger times: 0
Loss after 647068920 batches: 0.0300
trigger times: 1
Loss after 647197560 batches: 0.0229
trigger times: 2
Loss after 647326200 batches: 0.0200
trigger times: 3
Loss after 647454840 batches: 0.0186
trigger times: 0
Loss after 647583480 batches: 0.0175
trigger times: 1
Loss after 647712120 batches: 0.0165
trigger times: 2
Loss after 647840760 batches: 0.0157
trigger times: 3
Loss after 647969400 batches: 0.0156
trigger times: 4
Loss after 648098040 batches: 0.0150
trigger times: 5
Loss after 648226680 batches: 0.0147
trigger times: 6
Loss after 648355320 batches: 0.0141
trigger times: 7
Loss after 648483960 batches: 0.0140
trigger times: 8
Loss after 648612600 batches: 0.0135
trigger times: 9
Loss after 648741240 batches: 0.0133
trigger times: 10
Loss after 648869880 batches: 0.0130
trigger times: 0
Loss after 648998520 batches: 0.0131
trigger times: 1
Loss after 649127160 batches: 0.0128
trigger times: 2
Loss after 649255800 batches: 0.0127
trigger times: 3
Loss after 649384440 batches: 0.0126
trigger times: 4
Loss after 649513080 batches: 0.0121
trigger times: 5
Loss after 649641720 batches: 0.0122
trigger times: 6
Loss after 649770360 batches: 0.0120
trigger times: 7
Loss after 649899000 batches: 0.0119
trigger times: 8
Loss after 650027640 batches: 0.0118
trigger times: 9
Loss after 650156280 batches: 0.0118
trigger times: 10
Loss after 650284920 batches: 0.0114
trigger times: 11
Loss after 650413560 batches: 0.0115
trigger times: 12
Loss after 650542200 batches: 0.0113
trigger times: 13
Loss after 650670840 batches: 0.0113
trigger times: 14
Loss after 650799480 batches: 0.0116
trigger times: 15
Loss after 650928120 batches: 0.0110
trigger times: 16
Loss after 651056760 batches: 0.0112
trigger times: 17
Loss after 651185400 batches: 0.0108
trigger times: 18
Loss after 651314040 batches: 0.0111
trigger times: 19
Loss after 651442680 batches: 0.0110
trigger times: 20
Early stopping!
Start to test process.
Loss after 651571320 batches: 0.0110
Time to train on one home:  272.5438723564148
trigger times: 0
Loss after 651702420 batches: 0.3009
trigger times: 0
Loss after 651833520 batches: 0.0724
trigger times: 0
Loss after 651964620 batches: 0.0510
trigger times: 1
Loss after 652095720 batches: 0.0438
trigger times: 2
Loss after 652226820 batches: 0.0393
trigger times: 3
Loss after 652357920 batches: 0.0363
trigger times: 4
Loss after 652489020 batches: 0.0347
trigger times: 5
Loss after 652620120 batches: 0.0333
trigger times: 0
Loss after 652751220 batches: 0.0321
trigger times: 1
Loss after 652882320 batches: 0.0307
trigger times: 2
Loss after 653013420 batches: 0.0301
trigger times: 3
Loss after 653144520 batches: 0.0294
trigger times: 4
Loss after 653275620 batches: 0.0284
trigger times: 5
Loss after 653406720 batches: 0.0279
trigger times: 6
Loss after 653537820 batches: 0.0270
trigger times: 7
Loss after 653668920 batches: 0.0269
trigger times: 8
Loss after 653800020 batches: 0.0264
trigger times: 9
Loss after 653931120 batches: 0.0257
trigger times: 10
Loss after 654062220 batches: 0.0251
trigger times: 11
Loss after 654193320 batches: 0.0249
trigger times: 12
Loss after 654324420 batches: 0.0247
trigger times: 13
Loss after 654455520 batches: 0.0244
trigger times: 14
Loss after 654586620 batches: 0.0242
trigger times: 15
Loss after 654717720 batches: 0.0239
trigger times: 16
Loss after 654848820 batches: 0.0235
trigger times: 17
Loss after 654979920 batches: 0.0237
trigger times: 18
Loss after 655111020 batches: 0.0232
trigger times: 19
Loss after 655242120 batches: 0.0226
trigger times: 20
Early stopping!
Start to test process.
Loss after 655373220 batches: 0.0230
Time to train on one home:  220.38207006454468
trigger times: 0
Loss after 655504320 batches: 0.2456
trigger times: 0
Loss after 655635420 batches: 0.0838
trigger times: 0
Loss after 655766520 batches: 0.0550
trigger times: 0
Loss after 655897620 batches: 0.0451
trigger times: 0
Loss after 656028720 batches: 0.0393
trigger times: 0
Loss after 656159820 batches: 0.0356
trigger times: 0
Loss after 656290920 batches: 0.0333
trigger times: 0
Loss after 656422020 batches: 0.0330
trigger times: 1
Loss after 656553120 batches: 0.0329
trigger times: 0
Loss after 656684220 batches: 0.0318
trigger times: 1
Loss after 656815320 batches: 0.0301
trigger times: 0
Loss after 656946420 batches: 0.0303
trigger times: 1
Loss after 657077520 batches: 0.0281
trigger times: 2
Loss after 657208620 batches: 0.0270
trigger times: 0
Loss after 657339720 batches: 0.0274
trigger times: 0
Loss after 657470820 batches: 0.0280
trigger times: 1
Loss after 657601920 batches: 0.0286
trigger times: 2
Loss after 657733020 batches: 0.0261
trigger times: 3
Loss after 657864120 batches: 0.0254
trigger times: 4
Loss after 657995220 batches: 0.0248
trigger times: 0
Loss after 658126320 batches: 0.0254
trigger times: 1
Loss after 658257420 batches: 0.0245
trigger times: 2
Loss after 658388520 batches: 0.0237
trigger times: 3
Loss after 658519620 batches: 0.0253
trigger times: 4
Loss after 658650720 batches: 0.0245
trigger times: 5
Loss after 658781820 batches: 0.0250
trigger times: 6
Loss after 658912920 batches: 0.0247
trigger times: 0
Loss after 659044020 batches: 0.0243
trigger times: 1
Loss after 659175120 batches: 0.0233
trigger times: 2
Loss after 659306220 batches: 0.0243
trigger times: 3
Loss after 659437320 batches: 0.0228
trigger times: 4
Loss after 659568420 batches: 0.0226
trigger times: 5
Loss after 659699520 batches: 0.0233
trigger times: 6
Loss after 659830620 batches: 0.0224
trigger times: 7
Loss after 659961720 batches: 0.0223
trigger times: 8
Loss after 660092820 batches: 0.0224
trigger times: 9
Loss after 660223920 batches: 0.0226
trigger times: 10
Loss after 660355020 batches: 0.0217
trigger times: 11
Loss after 660486120 batches: 0.0221
trigger times: 0
Loss after 660617220 batches: 0.0220
trigger times: 1
Loss after 660748320 batches: 0.0227
trigger times: 2
Loss after 660879420 batches: 0.0220
trigger times: 3
Loss after 661010520 batches: 0.0219
trigger times: 4
Loss after 661141620 batches: 0.0212
trigger times: 5
Loss after 661272720 batches: 0.0215
trigger times: 6
Loss after 661403820 batches: 0.0216
trigger times: 7
Loss after 661534920 batches: 0.0217
trigger times: 8
Loss after 661666020 batches: 0.0212
trigger times: 9
Loss after 661797120 batches: 0.0213
trigger times: 10
Loss after 661928220 batches: 0.0205
trigger times: 11
Loss after 662059320 batches: 0.0210
trigger times: 12
Loss after 662190420 batches: 0.0222
trigger times: 13
Loss after 662321520 batches: 0.0206
trigger times: 14
Loss after 662452620 batches: 0.0201
trigger times: 15
Loss after 662583720 batches: 0.0207
trigger times: 16
Loss after 662714820 batches: 0.0211
trigger times: 17
Loss after 662845920 batches: 0.0208
trigger times: 18
Loss after 662977020 batches: 0.0208
trigger times: 19
Loss after 663108120 batches: 0.0206
trigger times: 20
Early stopping!
Start to test process.
Loss after 663239220 batches: 0.0216
Time to train on one home:  443.3539752960205
trigger times: 0
Loss after 663370320 batches: 0.0712
trigger times: 0
Loss after 663501420 batches: 0.0231
trigger times: 1
Loss after 663632520 batches: 0.0175
trigger times: 0
Loss after 663763620 batches: 0.0154
trigger times: 0
Loss after 663894720 batches: 0.0141
trigger times: 1
Loss after 664025820 batches: 0.0133
trigger times: 2
Loss after 664156920 batches: 0.0123
trigger times: 3
Loss after 664288020 batches: 0.0118
trigger times: 4
Loss after 664419120 batches: 0.0115
trigger times: 5
Loss after 664550220 batches: 0.0110
trigger times: 0
Loss after 664681320 batches: 0.0108
trigger times: 1
Loss after 664812420 batches: 0.0105
trigger times: 2
Loss after 664943520 batches: 0.0103
trigger times: 3
Loss after 665074620 batches: 0.0099
trigger times: 4
Loss after 665205720 batches: 0.0100
trigger times: 5
Loss after 665336820 batches: 0.0097
trigger times: 6
Loss after 665467920 batches: 0.0096
trigger times: 7
Loss after 665599020 batches: 0.0093
trigger times: 8
Loss after 665730120 batches: 0.0093
trigger times: 9
Loss after 665861220 batches: 0.0093
trigger times: 10
Loss after 665992320 batches: 0.0091
trigger times: 11
Loss after 666123420 batches: 0.0091
trigger times: 12
Loss after 666254520 batches: 0.0087
trigger times: 13
Loss after 666385620 batches: 0.0088
trigger times: 14
Loss after 666516720 batches: 0.0087
trigger times: 15
Loss after 666647820 batches: 0.0087
trigger times: 16
Loss after 666778920 batches: 0.0086
trigger times: 17
Loss after 666910020 batches: 0.0086
trigger times: 18
Loss after 667041120 batches: 0.0084
trigger times: 19
Loss after 667172220 batches: 0.0082
trigger times: 20
Early stopping!
Start to test process.
Loss after 667303320 batches: 0.0082
Time to train on one home:  233.27620315551758
trigger times: 0
Loss after 667381920 batches: 0.2430
trigger times: 1
Loss after 667460520 batches: 0.0615
trigger times: 0
Loss after 667539120 batches: 0.0397
trigger times: 1
Loss after 667617720 batches: 0.0316
trigger times: 2
Loss after 667696320 batches: 0.0284
trigger times: 3
Loss after 667774920 batches: 0.0265
trigger times: 0
Loss after 667853520 batches: 0.0243
trigger times: 1
Loss after 667932120 batches: 0.0237
trigger times: 2
Loss after 668010720 batches: 0.0221
trigger times: 3
Loss after 668089320 batches: 0.0218
trigger times: 4
Loss after 668167920 batches: 0.0208
trigger times: 0
Loss after 668246520 batches: 0.0208
trigger times: 1
Loss after 668325120 batches: 0.0201
trigger times: 2
Loss after 668403720 batches: 0.0197
trigger times: 3
Loss after 668482320 batches: 0.0192
trigger times: 4
Loss after 668560920 batches: 0.0189
trigger times: 5
Loss after 668639520 batches: 0.0189
trigger times: 6
Loss after 668718120 batches: 0.0186
trigger times: 0
Loss after 668796720 batches: 0.0180
trigger times: 1
Loss after 668875320 batches: 0.0177
trigger times: 2
Loss after 668953920 batches: 0.0178
trigger times: 0
Loss after 669032520 batches: 0.0175
trigger times: 1
Loss after 669111120 batches: 0.0173
trigger times: 0
Loss after 669189720 batches: 0.0169
trigger times: 0
Loss after 669268320 batches: 0.0162
trigger times: 1
Loss after 669346920 batches: 0.0171
trigger times: 2
Loss after 669425520 batches: 0.0163
trigger times: 3
Loss after 669504120 batches: 0.0161
trigger times: 4
Loss after 669582720 batches: 0.0155
trigger times: 5
Loss after 669661320 batches: 0.0159
trigger times: 0
Loss after 669739920 batches: 0.0160
trigger times: 1
Loss after 669818520 batches: 0.0158
trigger times: 2
Loss after 669897120 batches: 0.0155
trigger times: 3
Loss after 669975720 batches: 0.0152
trigger times: 4
Loss after 670054320 batches: 0.0155
trigger times: 5
Loss after 670132920 batches: 0.0158
trigger times: 6
Loss after 670211520 batches: 0.0154
trigger times: 7
Loss after 670290120 batches: 0.0151
trigger times: 8
Loss after 670368720 batches: 0.0147
trigger times: 9
Loss after 670447320 batches: 0.0148
trigger times: 10
Loss after 670525920 batches: 0.0144
trigger times: 11
Loss after 670604520 batches: 0.0146
trigger times: 12
Loss after 670683120 batches: 0.0145
trigger times: 13
Loss after 670761720 batches: 0.0142
trigger times: 14
Loss after 670840320 batches: 0.0141
trigger times: 15
Loss after 670918920 batches: 0.0142
trigger times: 16
Loss after 670997520 batches: 0.0140
trigger times: 17
Loss after 671076120 batches: 0.0138
trigger times: 18
Loss after 671154720 batches: 0.0138
trigger times: 19
Loss after 671233320 batches: 0.0136
trigger times: 20
Early stopping!
Start to test process.
Loss after 671311920 batches: 0.0136
Time to train on one home:  246.9656581878662
trigger times: 0
Loss after 671443020 batches: 0.0917
trigger times: 0
Loss after 671574120 batches: 0.0267
trigger times: 1
Loss after 671705220 batches: 0.0201
trigger times: 2
Loss after 671836320 batches: 0.0178
trigger times: 3
Loss after 671967420 batches: 0.0161
trigger times: 4
Loss after 672098520 batches: 0.0156
trigger times: 5
Loss after 672229620 batches: 0.0143
trigger times: 6
Loss after 672360720 batches: 0.0137
trigger times: 7
Loss after 672491820 batches: 0.0135
trigger times: 8
Loss after 672622920 batches: 0.0133
trigger times: 9
Loss after 672754020 batches: 0.0128
trigger times: 10
Loss after 672885120 batches: 0.0123
trigger times: 11
Loss after 673016220 batches: 0.0121
trigger times: 12
Loss after 673147320 batches: 0.0121
trigger times: 13
Loss after 673278420 batches: 0.0118
trigger times: 14
Loss after 673409520 batches: 0.0116
trigger times: 15
Loss after 673540620 batches: 0.0114
trigger times: 16
Loss after 673671720 batches: 0.0112
trigger times: 17
Loss after 673802820 batches: 0.0110
trigger times: 18
Loss after 673933920 batches: 0.0109
trigger times: 19
Loss after 674065020 batches: 0.0107
trigger times: 20
Early stopping!
Start to test process.
Loss after 674196120 batches: 0.0106
Time to train on one home:  169.20621800422668
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318, 0.016183730855002625, 0.015608215882864637, 0.015855816283001423, 0.015272240943982671, 0.014904168019127074]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165], [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777], [0.5659677750534482, 0.3881937670236807, 0.47736113706344846, 1.0466814163269424, 0.5011883546712889, 24.728223616247963, 1547.1996], [0.5552795496251848, 0.39972320473570555, 0.4869199067938419, 1.0432319579028642, 0.4917435017003341, 24.646728924611136, 1518.0426], [0.556764519876904, 0.3981234608309019, 0.4873119444985629, 1.0354352452336422, 0.4930540032485836, 24.462528793273435, 1522.0884], [0.5499979423152076, 0.40547709897063045, 0.49151309919991676, 1.0193683799804445, 0.4870299426858642, 24.082943342919844, 1503.4916]]
Round_12_results:  [0.5499979423152076, 0.40547709897063045, 0.49151309919991676, 1.0193683799804445, 0.4870299426858642, 24.082943342919844, 1503.4916]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5483 < 5484; dropping {'Training_Loss': 0.11360908130992134, 'Validation_Loss': 0.19416187786393696, 'Training_R2': 0.8855759632640682, 'Validation_R2': 0.8195463567998853, 'Training_F1': 0.8249154918596675, 'Validation_F1': 0.7159069389874462, 'Training_NEP': 0.3499444082200864, 'Validation_NEP': 0.5122016560859666, 'Training_NDE': 0.08590015540746375, 'Validation_NDE': 0.14370376899145518, 'Training_MAE': 11.589464785650794, 'Validation_MAE': 14.04669440437816, 'Training_MSE': 377.94727, 'Validation_MSE': 530.69403}.
trigger times: 0
Loss after 674327220 batches: 0.1136
trigger times: 0
Loss after 674458320 batches: 0.0282
trigger times: 0
Loss after 674589420 batches: 0.0214
trigger times: 1
Loss after 674720520 batches: 0.0186
trigger times: 2
Loss after 674851620 batches: 0.0168
trigger times: 3
Loss after 674982720 batches: 0.0157
trigger times: 4
Loss after 675113820 batches: 0.0151
trigger times: 5
Loss after 675244920 batches: 0.0142
trigger times: 6
Loss after 675376020 batches: 0.0139
trigger times: 7
Loss after 675507120 batches: 0.0133
trigger times: 8
Loss after 675638220 batches: 0.0131
trigger times: 9
Loss after 675769320 batches: 0.0127
trigger times: 10
Loss after 675900420 batches: 0.0123
trigger times: 11
Loss after 676031520 batches: 0.0121
trigger times: 0
Loss after 676162620 batches: 0.0119
trigger times: 1
Loss after 676293720 batches: 0.0119
trigger times: 2
Loss after 676424820 batches: 0.0116
trigger times: 3
Loss after 676555920 batches: 0.0116
trigger times: 4
Loss after 676687020 batches: 0.0110
trigger times: 5
Loss after 676818120 batches: 0.0110
trigger times: 6
Loss after 676949220 batches: 0.0108
trigger times: 7
Loss after 677080320 batches: 0.0107
trigger times: 8
Loss after 677211420 batches: 0.0106
trigger times: 9
Loss after 677342520 batches: 0.0104
trigger times: 10
Loss after 677473620 batches: 0.0104
trigger times: 11
Loss after 677604720 batches: 0.0103
trigger times: 12
Loss after 677735820 batches: 0.0099
trigger times: 13
Loss after 677866920 batches: 0.0098
trigger times: 14
Loss after 677998020 batches: 0.0100
trigger times: 15
Loss after 678129120 batches: 0.0098
trigger times: 16
Loss after 678260220 batches: 0.0097
trigger times: 17
Loss after 678391320 batches: 0.0098
trigger times: 18
Loss after 678522420 batches: 0.0097
trigger times: 19
Loss after 678653520 batches: 0.0096
trigger times: 20
Early stopping!
Start to test process.
Loss after 678784620 batches: 0.0095
Time to train on one home:  262.8893139362335
trigger times: 0
Loss after 678887220 batches: 0.2233
trigger times: 0
Loss after 678989820 batches: 0.0725
trigger times: 1
Loss after 679092420 batches: 0.0511
trigger times: 0
Loss after 679195020 batches: 0.0428
trigger times: 0
Loss after 679297620 batches: 0.0361
trigger times: 0
Loss after 679400220 batches: 0.0310
trigger times: 1
Loss after 679502820 batches: 0.0288
trigger times: 2
Loss after 679605420 batches: 0.0286
trigger times: 0
Loss after 679708020 batches: 0.0285
trigger times: 1
Loss after 679810620 batches: 0.0262
trigger times: 2
Loss after 679913220 batches: 0.0247
trigger times: 3
Loss after 680015820 batches: 0.0247
trigger times: 4
Loss after 680118420 batches: 0.0241
trigger times: 0
Loss after 680221020 batches: 0.0246
trigger times: 1
Loss after 680323620 batches: 0.0226
trigger times: 2
Loss after 680426220 batches: 0.0219
trigger times: 3
Loss after 680528820 batches: 0.0213
trigger times: 4
Loss after 680631420 batches: 0.0220
trigger times: 5
Loss after 680734020 batches: 0.0218
trigger times: 6
Loss after 680836620 batches: 0.0211
trigger times: 7
Loss after 680939220 batches: 0.0225
trigger times: 8
Loss after 681041820 batches: 0.0221
trigger times: 9
Loss after 681144420 batches: 0.0218
trigger times: 10
Loss after 681247020 batches: 0.0205
trigger times: 0
Loss after 681349620 batches: 0.0213
trigger times: 0
Loss after 681452220 batches: 0.0194
trigger times: 0
Loss after 681554820 batches: 0.0204
trigger times: 1
Loss after 681657420 batches: 0.0198
trigger times: 2
Loss after 681760020 batches: 0.0201
trigger times: 3
Loss after 681862620 batches: 0.0220
trigger times: 4
Loss after 681965220 batches: 0.0212
trigger times: 5
Loss after 682067820 batches: 0.0187
trigger times: 6
Loss after 682170420 batches: 0.0181
trigger times: 7
Loss after 682273020 batches: 0.0180
trigger times: 8
Loss after 682375620 batches: 0.0174
trigger times: 9
Loss after 682478220 batches: 0.0176
trigger times: 10
Loss after 682580820 batches: 0.0177
trigger times: 11
Loss after 682683420 batches: 0.0188
trigger times: 12
Loss after 682786020 batches: 0.0185
trigger times: 13
Loss after 682888620 batches: 0.0197
trigger times: 14
Loss after 682991220 batches: 0.0178
trigger times: 15
Loss after 683093820 batches: 0.0172
trigger times: 16
Loss after 683196420 batches: 0.0164
trigger times: 17
Loss after 683299020 batches: 0.0165
trigger times: 18
Loss after 683401620 batches: 0.0171
trigger times: 19
Loss after 683504220 batches: 0.0173
trigger times: 20
Early stopping!
Start to test process.
Loss after 683606820 batches: 0.0174
Time to train on one home:  282.87886238098145
trigger times: 0
Loss after 683737920 batches: 0.1547
trigger times: 0
Loss after 683869020 batches: 0.0483
trigger times: 0
Loss after 684000120 batches: 0.0353
trigger times: 1
Loss after 684131220 batches: 0.0308
trigger times: 2
Loss after 684262320 batches: 0.0284
trigger times: 0
Loss after 684393420 batches: 0.0263
trigger times: 0
Loss after 684524520 batches: 0.0248
trigger times: 1
Loss after 684655620 batches: 0.0237
trigger times: 2
Loss after 684786720 batches: 0.0228
trigger times: 3
Loss after 684917820 batches: 0.0219
trigger times: 4
Loss after 685048920 batches: 0.0215
trigger times: 5
Loss after 685180020 batches: 0.0209
trigger times: 6
Loss after 685311120 batches: 0.0206
trigger times: 7
Loss after 685442220 batches: 0.0201
trigger times: 8
Loss after 685573320 batches: 0.0195
trigger times: 9
Loss after 685704420 batches: 0.0193
trigger times: 10
Loss after 685835520 batches: 0.0188
trigger times: 11
Loss after 685966620 batches: 0.0186
trigger times: 12
Loss after 686097720 batches: 0.0185
trigger times: 13
Loss after 686228820 batches: 0.0178
trigger times: 14
Loss after 686359920 batches: 0.0179
trigger times: 15
Loss after 686491020 batches: 0.0174
trigger times: 16
Loss after 686622120 batches: 0.0174
trigger times: 17
Loss after 686753220 batches: 0.0173
trigger times: 18
Loss after 686884320 batches: 0.0170
trigger times: 19
Loss after 687015420 batches: 0.0167
trigger times: 20
Early stopping!
Start to test process.
Loss after 687146520 batches: 0.0165
Time to train on one home:  205.4827117919922
trigger times: 0
Loss after 687277620 batches: 0.2073
trigger times: 0
Loss after 687408720 batches: 0.0605
trigger times: 1
Loss after 687539820 batches: 0.0438
trigger times: 2
Loss after 687670920 batches: 0.0381
trigger times: 3
Loss after 687802020 batches: 0.0346
trigger times: 4
Loss after 687933120 batches: 0.0323
trigger times: 5
Loss after 688064220 batches: 0.0302
trigger times: 0
Loss after 688195320 batches: 0.0296
trigger times: 1
Loss after 688326420 batches: 0.0285
trigger times: 2
Loss after 688457520 batches: 0.0278
trigger times: 0
Loss after 688588620 batches: 0.0268
trigger times: 1
Loss after 688719720 batches: 0.0262
trigger times: 2
Loss after 688850820 batches: 0.0255
trigger times: 3
Loss after 688981920 batches: 0.0252
trigger times: 4
Loss after 689113020 batches: 0.0247
trigger times: 5
Loss after 689244120 batches: 0.0243
trigger times: 6
Loss after 689375220 batches: 0.0244
trigger times: 7
Loss after 689506320 batches: 0.0233
trigger times: 8
Loss after 689637420 batches: 0.0233
trigger times: 0
Loss after 689768520 batches: 0.0229
trigger times: 1
Loss after 689899620 batches: 0.0227
trigger times: 2
Loss after 690030720 batches: 0.0224
trigger times: 0
Loss after 690161820 batches: 0.0220
trigger times: 1
Loss after 690292920 batches: 0.0224
trigger times: 2
Loss after 690424020 batches: 0.0217
trigger times: 3
Loss after 690555120 batches: 0.0215
trigger times: 4
Loss after 690686220 batches: 0.0214
trigger times: 5
Loss after 690817320 batches: 0.0211
trigger times: 6
Loss after 690948420 batches: 0.0209
trigger times: 7
Loss after 691079520 batches: 0.0210
trigger times: 8
Loss after 691210620 batches: 0.0205
trigger times: 9
Loss after 691341720 batches: 0.0205
trigger times: 10
Loss after 691472820 batches: 0.0205
trigger times: 11
Loss after 691603920 batches: 0.0201
trigger times: 12
Loss after 691735020 batches: 0.0200
trigger times: 13
Loss after 691866120 batches: 0.0200
trigger times: 14
Loss after 691997220 batches: 0.0201
trigger times: 15
Loss after 692128320 batches: 0.0203
trigger times: 16
Loss after 692259420 batches: 0.0199
trigger times: 17
Loss after 692390520 batches: 0.0198
trigger times: 18
Loss after 692521620 batches: 0.0196
trigger times: 19
Loss after 692652720 batches: 0.0194
trigger times: 20
Early stopping!
Start to test process.
Loss after 692783820 batches: 0.0194
Time to train on one home:  320.3492751121521
trigger times: 0
Loss after 692912460 batches: 0.1071
trigger times: 0
Loss after 693041100 batches: 0.0300
trigger times: 0
Loss after 693169740 batches: 0.0226
trigger times: 1
Loss after 693298380 batches: 0.0194
trigger times: 2
Loss after 693427020 batches: 0.0181
trigger times: 3
Loss after 693555660 batches: 0.0172
trigger times: 4
Loss after 693684300 batches: 0.0165
trigger times: 5
Loss after 693812940 batches: 0.0157
trigger times: 0
Loss after 693941580 batches: 0.0152
trigger times: 1
Loss after 694070220 batches: 0.0149
trigger times: 2
Loss after 694198860 batches: 0.0141
trigger times: 3
Loss after 694327500 batches: 0.0141
trigger times: 4
Loss after 694456140 batches: 0.0139
trigger times: 5
Loss after 694584780 batches: 0.0137
trigger times: 6
Loss after 694713420 batches: 0.0133
trigger times: 7
Loss after 694842060 batches: 0.0131
trigger times: 8
Loss after 694970700 batches: 0.0132
trigger times: 0
Loss after 695099340 batches: 0.0129
trigger times: 1
Loss after 695227980 batches: 0.0126
trigger times: 2
Loss after 695356620 batches: 0.0123
trigger times: 3
Loss after 695485260 batches: 0.0126
trigger times: 4
Loss after 695613900 batches: 0.0120
trigger times: 5
Loss after 695742540 batches: 0.0119
trigger times: 6
Loss after 695871180 batches: 0.0119
trigger times: 7
Loss after 695999820 batches: 0.0115
trigger times: 8
Loss after 696128460 batches: 0.0114
trigger times: 9
Loss after 696257100 batches: 0.0117
trigger times: 10
Loss after 696385740 batches: 0.0114
trigger times: 11
Loss after 696514380 batches: 0.0115
trigger times: 12
Loss after 696643020 batches: 0.0116
trigger times: 13
Loss after 696771660 batches: 0.0114
trigger times: 14
Loss after 696900300 batches: 0.0113
trigger times: 15
Loss after 697028940 batches: 0.0111
trigger times: 16
Loss after 697157580 batches: 0.0111
trigger times: 17
Loss after 697286220 batches: 0.0110
trigger times: 18
Loss after 697414860 batches: 0.0109
trigger times: 19
Loss after 697543500 batches: 0.0105
trigger times: 20
Early stopping!
Start to test process.
Loss after 697672140 batches: 0.0106
Time to train on one home:  280.121591091156
trigger times: 0
Loss after 697803240 batches: 0.2634
trigger times: 0
Loss after 697934340 batches: 0.0668
trigger times: 1
Loss after 698065440 batches: 0.0480
trigger times: 0
Loss after 698196540 batches: 0.0420
trigger times: 1
Loss after 698327640 batches: 0.0383
trigger times: 0
Loss after 698458740 batches: 0.0358
trigger times: 1
Loss after 698589840 batches: 0.0338
trigger times: 2
Loss after 698720940 batches: 0.0319
trigger times: 3
Loss after 698852040 batches: 0.0315
trigger times: 4
Loss after 698983140 batches: 0.0302
trigger times: 0
Loss after 699114240 batches: 0.0294
trigger times: 1
Loss after 699245340 batches: 0.0288
trigger times: 0
Loss after 699376440 batches: 0.0281
trigger times: 1
Loss after 699507540 batches: 0.0271
trigger times: 2
Loss after 699638640 batches: 0.0268
trigger times: 0
Loss after 699769740 batches: 0.0264
trigger times: 1
Loss after 699900840 batches: 0.0260
trigger times: 2
Loss after 700031940 batches: 0.0255
trigger times: 3
Loss after 700163040 batches: 0.0246
trigger times: 0
Loss after 700294140 batches: 0.0249
trigger times: 1
Loss after 700425240 batches: 0.0247
trigger times: 2
Loss after 700556340 batches: 0.0243
trigger times: 3
Loss after 700687440 batches: 0.0237
trigger times: 0
Loss after 700818540 batches: 0.0235
trigger times: 1
Loss after 700949640 batches: 0.0237
trigger times: 2
Loss after 701080740 batches: 0.0231
trigger times: 3
Loss after 701211840 batches: 0.0228
trigger times: 4
Loss after 701342940 batches: 0.0229
trigger times: 5
Loss after 701474040 batches: 0.0223
trigger times: 6
Loss after 701605140 batches: 0.0222
trigger times: 7
Loss after 701736240 batches: 0.0222
trigger times: 8
Loss after 701867340 batches: 0.0222
trigger times: 9
Loss after 701998440 batches: 0.0218
trigger times: 10
Loss after 702129540 batches: 0.0216
trigger times: 11
Loss after 702260640 batches: 0.0215
trigger times: 12
Loss after 702391740 batches: 0.0212
trigger times: 13
Loss after 702522840 batches: 0.0212
trigger times: 14
Loss after 702653940 batches: 0.0211
trigger times: 15
Loss after 702785040 batches: 0.0209
trigger times: 16
Loss after 702916140 batches: 0.0210
trigger times: 17
Loss after 703047240 batches: 0.0208
trigger times: 18
Loss after 703178340 batches: 0.0204
trigger times: 19
Loss after 703309440 batches: 0.0203
trigger times: 20
Early stopping!
Start to test process.
Loss after 703440540 batches: 0.0200
Time to train on one home:  329.1000454425812
trigger times: 0
Loss after 703571640 batches: 0.2569
trigger times: 0
Loss after 703702740 batches: 0.0785
trigger times: 0
Loss after 703833840 batches: 0.0528
trigger times: 0
Loss after 703964940 batches: 0.0427
trigger times: 0
Loss after 704096040 batches: 0.0383
trigger times: 0
Loss after 704227140 batches: 0.0358
trigger times: 0
Loss after 704358240 batches: 0.0332
trigger times: 0
Loss after 704489340 batches: 0.0306
trigger times: 1
Loss after 704620440 batches: 0.0296
trigger times: 0
Loss after 704751540 batches: 0.0297
trigger times: 0
Loss after 704882640 batches: 0.0286
trigger times: 1
Loss after 705013740 batches: 0.0281
trigger times: 0
Loss after 705144840 batches: 0.0267
trigger times: 0
Loss after 705275940 batches: 0.0271
trigger times: 1
Loss after 705407040 batches: 0.0266
trigger times: 2
Loss after 705538140 batches: 0.0276
trigger times: 3
Loss after 705669240 batches: 0.0262
trigger times: 0
Loss after 705800340 batches: 0.0247
trigger times: 1
Loss after 705931440 batches: 0.0252
trigger times: 0
Loss after 706062540 batches: 0.0248
trigger times: 0
Loss after 706193640 batches: 0.0243
trigger times: 0
Loss after 706324740 batches: 0.0245
trigger times: 1
Loss after 706455840 batches: 0.0243
trigger times: 0
Loss after 706586940 batches: 0.0242
trigger times: 1
Loss after 706718040 batches: 0.0252
trigger times: 2
Loss after 706849140 batches: 0.0241
trigger times: 3
Loss after 706980240 batches: 0.0236
trigger times: 4
Loss after 707111340 batches: 0.0228
trigger times: 5
Loss after 707242440 batches: 0.0237
trigger times: 6
Loss after 707373540 batches: 0.0232
trigger times: 7
Loss after 707504640 batches: 0.0227
trigger times: 8
Loss after 707635740 batches: 0.0219
trigger times: 9
Loss after 707766840 batches: 0.0222
trigger times: 10
Loss after 707897940 batches: 0.0221
trigger times: 11
Loss after 708029040 batches: 0.0230
trigger times: 0
Loss after 708160140 batches: 0.0226
trigger times: 1
Loss after 708291240 batches: 0.0215
trigger times: 2
Loss after 708422340 batches: 0.0220
trigger times: 0
Loss after 708553440 batches: 0.0222
trigger times: 1
Loss after 708684540 batches: 0.0220
trigger times: 2
Loss after 708815640 batches: 0.0209
trigger times: 3
Loss after 708946740 batches: 0.0212
trigger times: 4
Loss after 709077840 batches: 0.0210
trigger times: 5
Loss after 709208940 batches: 0.0206
trigger times: 0
Loss after 709340040 batches: 0.0213
trigger times: 0
Loss after 709471140 batches: 0.0212
trigger times: 1
Loss after 709602240 batches: 0.0226
trigger times: 2
Loss after 709733340 batches: 0.0205
trigger times: 3
Loss after 709864440 batches: 0.0208
trigger times: 0
Loss after 709995540 batches: 0.0198
trigger times: 1
Loss after 710126640 batches: 0.0211
trigger times: 2
Loss after 710257740 batches: 0.0212
trigger times: 3
Loss after 710388840 batches: 0.0215
trigger times: 4
Loss after 710519940 batches: 0.0208
trigger times: 0
Loss after 710651040 batches: 0.0195
trigger times: 1
Loss after 710782140 batches: 0.0199
trigger times: 2
Loss after 710913240 batches: 0.0199
trigger times: 3
Loss after 711044340 batches: 0.0199
trigger times: 4
Loss after 711175440 batches: 0.0201
trigger times: 5
Loss after 711306540 batches: 0.0198
trigger times: 6
Loss after 711437640 batches: 0.0198
trigger times: 7
Loss after 711568740 batches: 0.0188
trigger times: 8
Loss after 711699840 batches: 0.0195
trigger times: 9
Loss after 711830940 batches: 0.0193
trigger times: 10
Loss after 711962040 batches: 0.0194
trigger times: 11
Loss after 712093140 batches: 0.0200
trigger times: 12
Loss after 712224240 batches: 0.0205
trigger times: 13
Loss after 712355340 batches: 0.0192
trigger times: 14
Loss after 712486440 batches: 0.0195
trigger times: 15
Loss after 712617540 batches: 0.0196
trigger times: 16
Loss after 712748640 batches: 0.0190
trigger times: 17
Loss after 712879740 batches: 0.0191
trigger times: 18
Loss after 713010840 batches: 0.0188
trigger times: 19
Loss after 713141940 batches: 0.0191
trigger times: 20
Early stopping!
Start to test process.
Loss after 713273040 batches: 0.0188
Time to train on one home:  549.2212159633636
trigger times: 0
Loss after 713404140 batches: 0.0685
trigger times: 0
Loss after 713535240 batches: 0.0229
trigger times: 1
Loss after 713666340 batches: 0.0169
trigger times: 2
Loss after 713797440 batches: 0.0151
trigger times: 3
Loss after 713928540 batches: 0.0136
trigger times: 4
Loss after 714059640 batches: 0.0127
trigger times: 5
Loss after 714190740 batches: 0.0120
trigger times: 6
Loss after 714321840 batches: 0.0114
trigger times: 7
Loss after 714452940 batches: 0.0112
trigger times: 8
Loss after 714584040 batches: 0.0111
trigger times: 9
Loss after 714715140 batches: 0.0107
trigger times: 10
Loss after 714846240 batches: 0.0104
trigger times: 11
Loss after 714977340 batches: 0.0100
trigger times: 12
Loss after 715108440 batches: 0.0098
trigger times: 13
Loss after 715239540 batches: 0.0098
trigger times: 14
Loss after 715370640 batches: 0.0095
trigger times: 15
Loss after 715501740 batches: 0.0095
trigger times: 16
Loss after 715632840 batches: 0.0092
trigger times: 17
Loss after 715763940 batches: 0.0090
trigger times: 18
Loss after 715895040 batches: 0.0090
trigger times: 19
Loss after 716026140 batches: 0.0087
trigger times: 20
Early stopping!
Start to test process.
Loss after 716157240 batches: 0.0089
Time to train on one home:  169.71065855026245
trigger times: 0
Loss after 716235840 batches: 0.2463
trigger times: 0
Loss after 716314440 batches: 0.0610
trigger times: 1
Loss after 716393040 batches: 0.0381
trigger times: 0
Loss after 716471640 batches: 0.0307
trigger times: 0
Loss after 716550240 batches: 0.0272
trigger times: 1
Loss after 716628840 batches: 0.0248
trigger times: 0
Loss after 716707440 batches: 0.0236
trigger times: 0
Loss after 716786040 batches: 0.0225
trigger times: 0
Loss after 716864640 batches: 0.0221
trigger times: 1
Loss after 716943240 batches: 0.0212
trigger times: 2
Loss after 717021840 batches: 0.0210
trigger times: 3
Loss after 717100440 batches: 0.0201
trigger times: 4
Loss after 717179040 batches: 0.0203
trigger times: 5
Loss after 717257640 batches: 0.0193
trigger times: 6
Loss after 717336240 batches: 0.0185
trigger times: 7
Loss after 717414840 batches: 0.0184
trigger times: 8
Loss after 717493440 batches: 0.0183
trigger times: 9
Loss after 717572040 batches: 0.0180
trigger times: 10
Loss after 717650640 batches: 0.0175
trigger times: 11
Loss after 717729240 batches: 0.0172
trigger times: 12
Loss after 717807840 batches: 0.0171
trigger times: 13
Loss after 717886440 batches: 0.0165
trigger times: 14
Loss after 717965040 batches: 0.0173
trigger times: 15
Loss after 718043640 batches: 0.0164
trigger times: 0
Loss after 718122240 batches: 0.0162
trigger times: 0
Loss after 718200840 batches: 0.0163
trigger times: 1
Loss after 718279440 batches: 0.0163
trigger times: 2
Loss after 718358040 batches: 0.0157
trigger times: 3
Loss after 718436640 batches: 0.0157
trigger times: 4
Loss after 718515240 batches: 0.0158
trigger times: 0
Loss after 718593840 batches: 0.0154
trigger times: 0
Loss after 718672440 batches: 0.0154
trigger times: 1
Loss after 718751040 batches: 0.0154
trigger times: 0
Loss after 718829640 batches: 0.0150
trigger times: 1
Loss after 718908240 batches: 0.0149
trigger times: 2
Loss after 718986840 batches: 0.0153
trigger times: 0
Loss after 719065440 batches: 0.0148
trigger times: 1
Loss after 719144040 batches: 0.0148
trigger times: 2
Loss after 719222640 batches: 0.0147
trigger times: 3
Loss after 719301240 batches: 0.0148
trigger times: 4
Loss after 719379840 batches: 0.0140
trigger times: 5
Loss after 719458440 batches: 0.0140
trigger times: 6
Loss after 719537040 batches: 0.0141
trigger times: 7
Loss after 719615640 batches: 0.0139
trigger times: 8
Loss after 719694240 batches: 0.0141
trigger times: 9
Loss after 719772840 batches: 0.0138
trigger times: 10
Loss after 719851440 batches: 0.0139
trigger times: 11
Loss after 719930040 batches: 0.0139
trigger times: 12
Loss after 720008640 batches: 0.0141
trigger times: 13
Loss after 720087240 batches: 0.0140
trigger times: 14
Loss after 720165840 batches: 0.0137
trigger times: 15
Loss after 720244440 batches: 0.0140
trigger times: 16
Loss after 720323040 batches: 0.0134
trigger times: 17
Loss after 720401640 batches: 0.0136
trigger times: 18
Loss after 720480240 batches: 0.0132
trigger times: 19
Loss after 720558840 batches: 0.0131
trigger times: 20
Early stopping!
Start to test process.
Loss after 720637440 batches: 0.0129
Time to train on one home:  276.68675112724304
trigger times: 0
Loss after 720768540 batches: 0.0866
trigger times: 0
Loss after 720899640 batches: 0.0253
trigger times: 1
Loss after 721030740 batches: 0.0193
trigger times: 2
Loss after 721161840 batches: 0.0172
trigger times: 0
Loss after 721292940 batches: 0.0161
trigger times: 1
Loss after 721424040 batches: 0.0151
trigger times: 2
Loss after 721555140 batches: 0.0144
trigger times: 3
Loss after 721686240 batches: 0.0135
trigger times: 4
Loss after 721817340 batches: 0.0131
trigger times: 5
Loss after 721948440 batches: 0.0127
trigger times: 6
Loss after 722079540 batches: 0.0123
trigger times: 7
Loss after 722210640 batches: 0.0120
trigger times: 8
Loss after 722341740 batches: 0.0116
trigger times: 9
Loss after 722472840 batches: 0.0115
trigger times: 10
Loss after 722603940 batches: 0.0112
trigger times: 11
Loss after 722735040 batches: 0.0112
trigger times: 0
Loss after 722866140 batches: 0.0112
trigger times: 1
Loss after 722997240 batches: 0.0109
trigger times: 2
Loss after 723128340 batches: 0.0107
trigger times: 3
Loss after 723259440 batches: 0.0105
trigger times: 4
Loss after 723390540 batches: 0.0101
trigger times: 5
Loss after 723521640 batches: 0.0101
trigger times: 0
Loss after 723652740 batches: 0.0102
trigger times: 1
Loss after 723783840 batches: 0.0102
trigger times: 2
Loss after 723914940 batches: 0.0099
trigger times: 3
Loss after 724046040 batches: 0.0098
trigger times: 4
Loss after 724177140 batches: 0.0099
trigger times: 5
Loss after 724308240 batches: 0.0096
trigger times: 6
Loss after 724439340 batches: 0.0096
trigger times: 7
Loss after 724570440 batches: 0.0097
trigger times: 8
Loss after 724701540 batches: 0.0095
trigger times: 0
Loss after 724832640 batches: 0.0093
trigger times: 1
Loss after 724963740 batches: 0.0093
trigger times: 2
Loss after 725094840 batches: 0.0093
trigger times: 3
Loss after 725225940 batches: 0.0094
trigger times: 4
Loss after 725357040 batches: 0.0091
trigger times: 5
Loss after 725488140 batches: 0.0091
trigger times: 6
Loss after 725619240 batches: 0.0089
trigger times: 7
Loss after 725750340 batches: 0.0089
trigger times: 8
Loss after 725881440 batches: 0.0088
trigger times: 9
Loss after 726012540 batches: 0.0088
trigger times: 10
Loss after 726143640 batches: 0.0088
trigger times: 11
Loss after 726274740 batches: 0.0087
trigger times: 12
Loss after 726405840 batches: 0.0086
trigger times: 13
Loss after 726536940 batches: 0.0088
trigger times: 14
Loss after 726668040 batches: 0.0087
trigger times: 15
Loss after 726799140 batches: 0.0084
trigger times: 16
Loss after 726930240 batches: 0.0083
trigger times: 17
Loss after 727061340 batches: 0.0084
trigger times: 18
Loss after 727192440 batches: 0.0084
trigger times: 19
Loss after 727323540 batches: 0.0082
trigger times: 20
Early stopping!
Start to test process.
Loss after 727454640 batches: 0.0081
Time to train on one home:  385.89973044395447
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318, 0.016183730855002625, 0.015608215882864637, 0.015855816283001423, 0.015272240943982671, 0.014904168019127074, 0.014211993132470634]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165], [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777], [0.5659677750534482, 0.3881937670236807, 0.47736113706344846, 1.0466814163269424, 0.5011883546712889, 24.728223616247963, 1547.1996], [0.5552795496251848, 0.39972320473570555, 0.4869199067938419, 1.0432319579028642, 0.4917435017003341, 24.646728924611136, 1518.0426], [0.556764519876904, 0.3981234608309019, 0.4873119444985629, 1.0354352452336422, 0.4930540032485836, 24.462528793273435, 1522.0884], [0.5499979423152076, 0.40547709897063045, 0.49151309919991676, 1.0193683799804445, 0.4870299426858642, 24.082943342919844, 1503.4916], [0.5525510013103485, 0.4027306139686573, 0.4951615175873931, 1.0298212095954027, 0.48927984833421284, 24.32989518911608, 1510.4373]]
Round_13_results:  [0.5525510013103485, 0.4027306139686573, 0.4951615175873931, 1.0298212095954027, 0.48927984833421284, 24.32989518911608, 1510.4373]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5923 < 5924; dropping {'Training_Loss': 0.10704692326626687, 'Validation_Loss': 0.17279845393366283, 'Training_R2': 0.8921844456154305, 'Validation_R2': 0.8394323188133832, 'Training_F1': 0.8310497550768006, 'Validation_F1': 0.7613273381756323, 'Training_NEP': 0.3373596174284015, 'Validation_NEP': 0.4621562806983313, 'Training_NDE': 0.08093905040555231, 'Validation_NDE': 0.12786763711468555, 'Training_MAE': 11.172681472961635, 'Validation_MAE': 12.674242585705173, 'Training_MSE': 356.1191, 'Validation_MSE': 472.21164}.
trigger times: 0
Loss after 727585740 batches: 0.1070
trigger times: 1
Loss after 727716840 batches: 0.0270
trigger times: 0
Loss after 727847940 batches: 0.0198
trigger times: 0
Loss after 727979040 batches: 0.0170
trigger times: 1
Loss after 728110140 batches: 0.0160
trigger times: 0
Loss after 728241240 batches: 0.0148
trigger times: 0
Loss after 728372340 batches: 0.0143
trigger times: 0
Loss after 728503440 batches: 0.0140
trigger times: 1
Loss after 728634540 batches: 0.0131
trigger times: 2
Loss after 728765640 batches: 0.0129
trigger times: 3
Loss after 728896740 batches: 0.0126
trigger times: 4
Loss after 729027840 batches: 0.0125
trigger times: 5
Loss after 729158940 batches: 0.0119
trigger times: 0
Loss after 729290040 batches: 0.0116
trigger times: 1
Loss after 729421140 batches: 0.0116
trigger times: 2
Loss after 729552240 batches: 0.0113
trigger times: 0
Loss after 729683340 batches: 0.0110
trigger times: 1
Loss after 729814440 batches: 0.0108
trigger times: 2
Loss after 729945540 batches: 0.0108
trigger times: 3
Loss after 730076640 batches: 0.0107
trigger times: 4
Loss after 730207740 batches: 0.0107
trigger times: 5
Loss after 730338840 batches: 0.0105
trigger times: 6
Loss after 730469940 batches: 0.0103
trigger times: 7
Loss after 730601040 batches: 0.0103
trigger times: 8
Loss after 730732140 batches: 0.0099
trigger times: 9
Loss after 730863240 batches: 0.0100
trigger times: 10
Loss after 730994340 batches: 0.0099
trigger times: 11
Loss after 731125440 batches: 0.0097
trigger times: 12
Loss after 731256540 batches: 0.0096
trigger times: 13
Loss after 731387640 batches: 0.0098
trigger times: 14
Loss after 731518740 batches: 0.0096
trigger times: 15
Loss after 731649840 batches: 0.0095
trigger times: 16
Loss after 731780940 batches: 0.0094
trigger times: 17
Loss after 731912040 batches: 0.0094
trigger times: 18
Loss after 732043140 batches: 0.0092
trigger times: 19
Loss after 732174240 batches: 0.0091
trigger times: 20
Early stopping!
Start to test process.
Loss after 732305340 batches: 0.0092
Time to train on one home:  277.5022511482239
trigger times: 0
Loss after 732407940 batches: 0.2220
trigger times: 0
Loss after 732510540 batches: 0.0750
trigger times: 0
Loss after 732613140 batches: 0.0470
trigger times: 0
Loss after 732715740 batches: 0.0374
trigger times: 1
Loss after 732818340 batches: 0.0327
trigger times: 2
Loss after 732920940 batches: 0.0314
trigger times: 0
Loss after 733023540 batches: 0.0322
trigger times: 0
Loss after 733126140 batches: 0.0300
trigger times: 0
Loss after 733228740 batches: 0.0285
trigger times: 0
Loss after 733331340 batches: 0.0259
trigger times: 1
Loss after 733433940 batches: 0.0248
trigger times: 2
Loss after 733536540 batches: 0.0251
trigger times: 0
Loss after 733639140 batches: 0.0259
trigger times: 1
Loss after 733741740 batches: 0.0233
trigger times: 2
Loss after 733844340 batches: 0.0228
trigger times: 3
Loss after 733946940 batches: 0.0230
trigger times: 4
Loss after 734049540 batches: 0.0211
trigger times: 5
Loss after 734152140 batches: 0.0211
trigger times: 6
Loss after 734254740 batches: 0.0210
trigger times: 7
Loss after 734357340 batches: 0.0218
trigger times: 0
Loss after 734459940 batches: 0.0204
trigger times: 1
Loss after 734562540 batches: 0.0194
trigger times: 2
Loss after 734665140 batches: 0.0199
trigger times: 0
Loss after 734767740 batches: 0.0201
trigger times: 1
Loss after 734870340 batches: 0.0198
trigger times: 2
Loss after 734972940 batches: 0.0187
trigger times: 3
Loss after 735075540 batches: 0.0191
trigger times: 4
Loss after 735178140 batches: 0.0190
trigger times: 5
Loss after 735280740 batches: 0.0180
trigger times: 0
Loss after 735383340 batches: 0.0185
trigger times: 1
Loss after 735485940 batches: 0.0189
trigger times: 0
Loss after 735588540 batches: 0.0186
trigger times: 0
Loss after 735691140 batches: 0.0188
trigger times: 1
Loss after 735793740 batches: 0.0198
trigger times: 2
Loss after 735896340 batches: 0.0183
trigger times: 3
Loss after 735998940 batches: 0.0183
trigger times: 4
Loss after 736101540 batches: 0.0198
trigger times: 5
Loss after 736204140 batches: 0.0183
trigger times: 6
Loss after 736306740 batches: 0.0177
trigger times: 7
Loss after 736409340 batches: 0.0178
trigger times: 8
Loss after 736511940 batches: 0.0169
trigger times: 9
Loss after 736614540 batches: 0.0168
trigger times: 0
Loss after 736717140 batches: 0.0179
trigger times: 0
Loss after 736819740 batches: 0.0171
trigger times: 1
Loss after 736922340 batches: 0.0199
trigger times: 2
Loss after 737024940 batches: 0.0172
trigger times: 3
Loss after 737127540 batches: 0.0158
trigger times: 0
Loss after 737230140 batches: 0.0167
trigger times: 1
Loss after 737332740 batches: 0.0165
trigger times: 2
Loss after 737435340 batches: 0.0166
trigger times: 3
Loss after 737537940 batches: 0.0159
trigger times: 4
Loss after 737640540 batches: 0.0157
trigger times: 5
Loss after 737743140 batches: 0.0162
trigger times: 6
Loss after 737845740 batches: 0.0166
trigger times: 7
Loss after 737948340 batches: 0.0169
trigger times: 8
Loss after 738050940 batches: 0.0166
trigger times: 9
Loss after 738153540 batches: 0.0163
trigger times: 10
Loss after 738256140 batches: 0.0189
trigger times: 11
Loss after 738358740 batches: 0.0184
trigger times: 12
Loss after 738461340 batches: 0.0156
trigger times: 13
Loss after 738563940 batches: 0.0156
trigger times: 14
Loss after 738666540 batches: 0.0164
trigger times: 15
Loss after 738769140 batches: 0.0173
trigger times: 16
Loss after 738871740 batches: 0.0161
trigger times: 17
Loss after 738974340 batches: 0.0157
trigger times: 18
Loss after 739076940 batches: 0.0155
trigger times: 19
Loss after 739179540 batches: 0.0150
trigger times: 20
Early stopping!
Start to test process.
Loss after 739282140 batches: 0.0153
Time to train on one home:  402.05241537094116
trigger times: 0
Loss after 739413240 batches: 0.1588
trigger times: 1
Loss after 739544340 batches: 0.0474
trigger times: 2
Loss after 739675440 batches: 0.0350
trigger times: 3
Loss after 739806540 batches: 0.0305
trigger times: 4
Loss after 739937640 batches: 0.0277
trigger times: 5
Loss after 740068740 batches: 0.0258
trigger times: 6
Loss after 740199840 batches: 0.0245
trigger times: 7
Loss after 740330940 batches: 0.0236
trigger times: 8
Loss after 740462040 batches: 0.0222
trigger times: 9
Loss after 740593140 batches: 0.0219
trigger times: 10
Loss after 740724240 batches: 0.0212
trigger times: 11
Loss after 740855340 batches: 0.0204
trigger times: 12
Loss after 740986440 batches: 0.0201
trigger times: 13
Loss after 741117540 batches: 0.0198
trigger times: 14
Loss after 741248640 batches: 0.0193
trigger times: 15
Loss after 741379740 batches: 0.0189
trigger times: 16
Loss after 741510840 batches: 0.0187
trigger times: 17
Loss after 741641940 batches: 0.0184
trigger times: 18
Loss after 741773040 batches: 0.0182
trigger times: 19
Loss after 741904140 batches: 0.0178
trigger times: 20
Early stopping!
Start to test process.
Loss after 742035240 batches: 0.0177
Time to train on one home:  162.16286373138428
trigger times: 0
Loss after 742166340 batches: 0.2020
trigger times: 0
Loss after 742297440 batches: 0.0572
trigger times: 0
Loss after 742428540 batches: 0.0421
trigger times: 0
Loss after 742559640 batches: 0.0367
trigger times: 1
Loss after 742690740 batches: 0.0338
trigger times: 2
Loss after 742821840 batches: 0.0321
trigger times: 3
Loss after 742952940 batches: 0.0303
trigger times: 0
Loss after 743084040 batches: 0.0290
trigger times: 0
Loss after 743215140 batches: 0.0282
trigger times: 1
Loss after 743346240 batches: 0.0275
trigger times: 0
Loss after 743477340 batches: 0.0266
trigger times: 1
Loss after 743608440 batches: 0.0260
trigger times: 0
Loss after 743739540 batches: 0.0251
trigger times: 1
Loss after 743870640 batches: 0.0251
trigger times: 2
Loss after 744001740 batches: 0.0246
trigger times: 3
Loss after 744132840 batches: 0.0240
trigger times: 4
Loss after 744263940 batches: 0.0239
trigger times: 5
Loss after 744395040 batches: 0.0235
trigger times: 6
Loss after 744526140 batches: 0.0229
trigger times: 7
Loss after 744657240 batches: 0.0226
trigger times: 8
Loss after 744788340 batches: 0.0226
trigger times: 9
Loss after 744919440 batches: 0.0222
trigger times: 10
Loss after 745050540 batches: 0.0218
trigger times: 11
Loss after 745181640 batches: 0.0216
trigger times: 12
Loss after 745312740 batches: 0.0215
trigger times: 13
Loss after 745443840 batches: 0.0212
trigger times: 14
Loss after 745574940 batches: 0.0213
trigger times: 15
Loss after 745706040 batches: 0.0213
trigger times: 16
Loss after 745837140 batches: 0.0209
trigger times: 17
Loss after 745968240 batches: 0.0208
trigger times: 18
Loss after 746099340 batches: 0.0208
trigger times: 19
Loss after 746230440 batches: 0.0207
trigger times: 20
Early stopping!
Start to test process.
Loss after 746361540 batches: 0.0206
Time to train on one home:  247.32733273506165
trigger times: 0
Loss after 746490180 batches: 0.1059
trigger times: 0
Loss after 746618820 batches: 0.0315
trigger times: 1
Loss after 746747460 batches: 0.0228
trigger times: 0
Loss after 746876100 batches: 0.0202
trigger times: 1
Loss after 747004740 batches: 0.0183
trigger times: 0
Loss after 747133380 batches: 0.0175
trigger times: 0
Loss after 747262020 batches: 0.0165
trigger times: 1
Loss after 747390660 batches: 0.0156
trigger times: 2
Loss after 747519300 batches: 0.0150
trigger times: 3
Loss after 747647940 batches: 0.0152
trigger times: 4
Loss after 747776580 batches: 0.0143
trigger times: 5
Loss after 747905220 batches: 0.0141
trigger times: 6
Loss after 748033860 batches: 0.0136
trigger times: 7
Loss after 748162500 batches: 0.0135
trigger times: 8
Loss after 748291140 batches: 0.0134
trigger times: 9
Loss after 748419780 batches: 0.0134
trigger times: 10
Loss after 748548420 batches: 0.0133
trigger times: 11
Loss after 748677060 batches: 0.0128
trigger times: 12
Loss after 748805700 batches: 0.0125
trigger times: 13
Loss after 748934340 batches: 0.0124
trigger times: 14
Loss after 749062980 batches: 0.0123
trigger times: 15
Loss after 749191620 batches: 0.0121
trigger times: 16
Loss after 749320260 batches: 0.0120
trigger times: 17
Loss after 749448900 batches: 0.0117
trigger times: 18
Loss after 749577540 batches: 0.0116
trigger times: 19
Loss after 749706180 batches: 0.0115
trigger times: 20
Early stopping!
Start to test process.
Loss after 749834820 batches: 0.0114
Time to train on one home:  201.54450845718384
trigger times: 0
Loss after 749965920 batches: 0.2452
trigger times: 0
Loss after 750097020 batches: 0.0618
trigger times: 0
Loss after 750228120 batches: 0.0463
trigger times: 1
Loss after 750359220 batches: 0.0401
trigger times: 2
Loss after 750490320 batches: 0.0364
trigger times: 0
Loss after 750621420 batches: 0.0343
trigger times: 0
Loss after 750752520 batches: 0.0326
trigger times: 1
Loss after 750883620 batches: 0.0311
trigger times: 2
Loss after 751014720 batches: 0.0299
trigger times: 3
Loss after 751145820 batches: 0.0290
trigger times: 4
Loss after 751276920 batches: 0.0288
trigger times: 5
Loss after 751408020 batches: 0.0273
trigger times: 6
Loss after 751539120 batches: 0.0266
trigger times: 7
Loss after 751670220 batches: 0.0268
trigger times: 8
Loss after 751801320 batches: 0.0259
trigger times: 9
Loss after 751932420 batches: 0.0257
trigger times: 10
Loss after 752063520 batches: 0.0250
trigger times: 11
Loss after 752194620 batches: 0.0246
trigger times: 12
Loss after 752325720 batches: 0.0243
trigger times: 13
Loss after 752456820 batches: 0.0243
trigger times: 14
Loss after 752587920 batches: 0.0238
trigger times: 15
Loss after 752719020 batches: 0.0234
trigger times: 16
Loss after 752850120 batches: 0.0234
trigger times: 17
Loss after 752981220 batches: 0.0228
trigger times: 18
Loss after 753112320 batches: 0.0221
trigger times: 19
Loss after 753243420 batches: 0.0226
trigger times: 20
Early stopping!
Start to test process.
Loss after 753374520 batches: 0.0222
Time to train on one home:  204.5285129547119
trigger times: 0
Loss after 753505620 batches: 0.2275
trigger times: 0
Loss after 753636720 batches: 0.0735
trigger times: 0
Loss after 753767820 batches: 0.0514
trigger times: 1
Loss after 753898920 batches: 0.0435
trigger times: 2
Loss after 754030020 batches: 0.0399
trigger times: 0
Loss after 754161120 batches: 0.0351
trigger times: 1
Loss after 754292220 batches: 0.0316
trigger times: 2
Loss after 754423320 batches: 0.0296
trigger times: 3
Loss after 754554420 batches: 0.0297
trigger times: 0
Loss after 754685520 batches: 0.0294
trigger times: 0
Loss after 754816620 batches: 0.0285
trigger times: 0
Loss after 754947720 batches: 0.0277
trigger times: 1
Loss after 755078820 batches: 0.0269
trigger times: 2
Loss after 755209920 batches: 0.0260
trigger times: 3
Loss after 755341020 batches: 0.0247
trigger times: 4
Loss after 755472120 batches: 0.0247
trigger times: 5
Loss after 755603220 batches: 0.0252
trigger times: 6
Loss after 755734320 batches: 0.0244
trigger times: 7
Loss after 755865420 batches: 0.0250
trigger times: 0
Loss after 755996520 batches: 0.0247
trigger times: 1
Loss after 756127620 batches: 0.0244
trigger times: 0
Loss after 756258720 batches: 0.0237
trigger times: 0
Loss after 756389820 batches: 0.0228
trigger times: 0
Loss after 756520920 batches: 0.0235
trigger times: 1
Loss after 756652020 batches: 0.0224
trigger times: 2
Loss after 756783120 batches: 0.0230
trigger times: 3
Loss after 756914220 batches: 0.0233
trigger times: 4
Loss after 757045320 batches: 0.0223
trigger times: 5
Loss after 757176420 batches: 0.0238
trigger times: 6
Loss after 757307520 batches: 0.0230
trigger times: 7
Loss after 757438620 batches: 0.0218
trigger times: 8
Loss after 757569720 batches: 0.0221
trigger times: 9
Loss after 757700820 batches: 0.0223
trigger times: 10
Loss after 757831920 batches: 0.0219
trigger times: 11
Loss after 757963020 batches: 0.0220
trigger times: 12
Loss after 758094120 batches: 0.0215
trigger times: 0
Loss after 758225220 batches: 0.0218
trigger times: 1
Loss after 758356320 batches: 0.0212
trigger times: 2
Loss after 758487420 batches: 0.0215
trigger times: 3
Loss after 758618520 batches: 0.0218
trigger times: 4
Loss after 758749620 batches: 0.0219
trigger times: 5
Loss after 758880720 batches: 0.0212
trigger times: 6
Loss after 759011820 batches: 0.0205
trigger times: 7
Loss after 759142920 batches: 0.0212
trigger times: 8
Loss after 759274020 batches: 0.0202
trigger times: 9
Loss after 759405120 batches: 0.0203
trigger times: 10
Loss after 759536220 batches: 0.0205
trigger times: 11
Loss after 759667320 batches: 0.0197
trigger times: 12
Loss after 759798420 batches: 0.0202
trigger times: 13
Loss after 759929520 batches: 0.0212
trigger times: 14
Loss after 760060620 batches: 0.0202
trigger times: 15
Loss after 760191720 batches: 0.0195
trigger times: 16
Loss after 760322820 batches: 0.0191
trigger times: 17
Loss after 760453920 batches: 0.0191
trigger times: 18
Loss after 760585020 batches: 0.0199
trigger times: 19
Loss after 760716120 batches: 0.0186
trigger times: 20
Early stopping!
Start to test process.
Loss after 760847220 batches: 0.0186
Time to train on one home:  419.26315355300903
trigger times: 0
Loss after 760978320 batches: 0.0724
trigger times: 0
Loss after 761109420 batches: 0.0220
trigger times: 0
Loss after 761240520 batches: 0.0165
trigger times: 1
Loss after 761371620 batches: 0.0147
trigger times: 2
Loss after 761502720 batches: 0.0133
trigger times: 3
Loss after 761633820 batches: 0.0125
trigger times: 0
Loss after 761764920 batches: 0.0119
trigger times: 1
Loss after 761896020 batches: 0.0114
trigger times: 2
Loss after 762027120 batches: 0.0111
trigger times: 3
Loss after 762158220 batches: 0.0108
trigger times: 4
Loss after 762289320 batches: 0.0103
trigger times: 5
Loss after 762420420 batches: 0.0102
trigger times: 6
Loss after 762551520 batches: 0.0098
trigger times: 7
Loss after 762682620 batches: 0.0098
trigger times: 8
Loss after 762813720 batches: 0.0096
trigger times: 9
Loss after 762944820 batches: 0.0094
trigger times: 10
Loss after 763075920 batches: 0.0095
trigger times: 11
Loss after 763207020 batches: 0.0093
trigger times: 12
Loss after 763338120 batches: 0.0090
trigger times: 13
Loss after 763469220 batches: 0.0089
trigger times: 14
Loss after 763600320 batches: 0.0088
trigger times: 15
Loss after 763731420 batches: 0.0086
trigger times: 16
Loss after 763862520 batches: 0.0085
trigger times: 17
Loss after 763993620 batches: 0.0088
trigger times: 18
Loss after 764124720 batches: 0.0085
trigger times: 19
Loss after 764255820 batches: 0.0084
trigger times: 20
Early stopping!
Start to test process.
Loss after 764386920 batches: 0.0082
Time to train on one home:  205.17888259887695
trigger times: 0
Loss after 764465520 batches: 0.2132
trigger times: 1
Loss after 764544120 batches: 0.0542
trigger times: 0
Loss after 764622720 batches: 0.0365
trigger times: 0
Loss after 764701320 batches: 0.0305
trigger times: 1
Loss after 764779920 batches: 0.0267
trigger times: 0
Loss after 764858520 batches: 0.0246
trigger times: 1
Loss after 764937120 batches: 0.0237
trigger times: 0
Loss after 765015720 batches: 0.0226
trigger times: 1
Loss after 765094320 batches: 0.0214
trigger times: 2
Loss after 765172920 batches: 0.0206
trigger times: 3
Loss after 765251520 batches: 0.0198
trigger times: 4
Loss after 765330120 batches: 0.0192
trigger times: 5
Loss after 765408720 batches: 0.0188
trigger times: 6
Loss after 765487320 batches: 0.0186
trigger times: 7
Loss after 765565920 batches: 0.0183
trigger times: 0
Loss after 765644520 batches: 0.0179
trigger times: 1
Loss after 765723120 batches: 0.0174
trigger times: 0
Loss after 765801720 batches: 0.0171
trigger times: 1
Loss after 765880320 batches: 0.0171
trigger times: 2
Loss after 765958920 batches: 0.0168
trigger times: 3
Loss after 766037520 batches: 0.0166
trigger times: 4
Loss after 766116120 batches: 0.0165
trigger times: 5
Loss after 766194720 batches: 0.0162
trigger times: 6
Loss after 766273320 batches: 0.0160
trigger times: 7
Loss after 766351920 batches: 0.0161
trigger times: 8
Loss after 766430520 batches: 0.0154
trigger times: 9
Loss after 766509120 batches: 0.0154
trigger times: 10
Loss after 766587720 batches: 0.0153
trigger times: 11
Loss after 766666320 batches: 0.0151
trigger times: 12
Loss after 766744920 batches: 0.0148
trigger times: 13
Loss after 766823520 batches: 0.0149
trigger times: 14
Loss after 766902120 batches: 0.0145
trigger times: 15
Loss after 766980720 batches: 0.0148
trigger times: 16
Loss after 767059320 batches: 0.0146
trigger times: 17
Loss after 767137920 batches: 0.0145
trigger times: 18
Loss after 767216520 batches: 0.0145
trigger times: 19
Loss after 767295120 batches: 0.0144
trigger times: 20
Early stopping!
Start to test process.
Loss after 767373720 batches: 0.0145
Time to train on one home:  187.23465132713318
trigger times: 0
Loss after 767504820 batches: 0.0791
trigger times: 1
Loss after 767635920 batches: 0.0245
trigger times: 0
Loss after 767767020 batches: 0.0189
trigger times: 1
Loss after 767898120 batches: 0.0165
trigger times: 2
Loss after 768029220 batches: 0.0151
trigger times: 0
Loss after 768160320 batches: 0.0141
trigger times: 1
Loss after 768291420 batches: 0.0136
trigger times: 2
Loss after 768422520 batches: 0.0127
trigger times: 3
Loss after 768553620 batches: 0.0126
trigger times: 4
Loss after 768684720 batches: 0.0121
trigger times: 5
Loss after 768815820 batches: 0.0117
trigger times: 6
Loss after 768946920 batches: 0.0116
trigger times: 7
Loss after 769078020 batches: 0.0113
trigger times: 8
Loss after 769209120 batches: 0.0110
trigger times: 9
Loss after 769340220 batches: 0.0108
trigger times: 10
Loss after 769471320 batches: 0.0109
trigger times: 11
Loss after 769602420 batches: 0.0104
trigger times: 12
Loss after 769733520 batches: 0.0103
trigger times: 13
Loss after 769864620 batches: 0.0101
trigger times: 14
Loss after 769995720 batches: 0.0100
trigger times: 15
Loss after 770126820 batches: 0.0099
trigger times: 16
Loss after 770257920 batches: 0.0099
trigger times: 17
Loss after 770389020 batches: 0.0097
trigger times: 18
Loss after 770520120 batches: 0.0096
trigger times: 19
Loss after 770651220 batches: 0.0093
trigger times: 20
Early stopping!
Start to test process.
Loss after 770782320 batches: 0.0094
Time to train on one home:  198.10540652275085
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318, 0.016183730855002625, 0.015608215882864637, 0.015855816283001423, 0.015272240943982671, 0.014904168019127074, 0.014211993132470634, 0.014714711150355778]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165], [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777], [0.5659677750534482, 0.3881937670236807, 0.47736113706344846, 1.0466814163269424, 0.5011883546712889, 24.728223616247963, 1547.1996], [0.5552795496251848, 0.39972320473570555, 0.4869199067938419, 1.0432319579028642, 0.4917435017003341, 24.646728924611136, 1518.0426], [0.556764519876904, 0.3981234608309019, 0.4873119444985629, 1.0354352452336422, 0.4930540032485836, 24.462528793273435, 1522.0884], [0.5499979423152076, 0.40547709897063045, 0.49151309919991676, 1.0193683799804445, 0.4870299426858642, 24.082943342919844, 1503.4916], [0.5525510013103485, 0.4027306139686573, 0.4951615175873931, 1.0298212095954027, 0.48927984833421284, 24.32989518911608, 1510.4373], [0.5425989263587527, 0.41345946401828226, 0.5050210962352305, 1.0162617040097164, 0.48049083244313984, 24.009547009604805, 1483.3052]]
Round_14_results:  [0.5425989263587527, 0.41345946401828226, 0.5050210962352305, 1.0162617040097164, 0.48049083244313984, 24.009547009604805, 1483.3052]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 6284 < 6285; dropping {'Training_Loss': 0.0935727873781942, 'Validation_Loss': 0.17145506623718473, 'Training_R2': 0.9057944296932278, 'Validation_R2': 0.8406681304234844, 'Training_F1': 0.8420787420167789, 'Validation_F1': 0.7585455051453204, 'Training_NEP': 0.3154788113002161, 'Validation_NEP': 0.4634044558732113, 'Training_NDE': 0.07072179378075856, 'Validation_NDE': 0.12688350189310965, 'Training_MAE': 10.448032568313982, 'Validation_MAE': 12.708472727361974, 'Training_MSE': 311.16476, 'Validation_MSE': 468.57727}.
trigger times: 0
Loss after 770913420 batches: 0.0936
trigger times: 0
Loss after 771044520 batches: 0.0258
trigger times: 0
Loss after 771175620 batches: 0.0195
trigger times: 1
Loss after 771306720 batches: 0.0168
trigger times: 2
Loss after 771437820 batches: 0.0157
trigger times: 0
Loss after 771568920 batches: 0.0149
trigger times: 1
Loss after 771700020 batches: 0.0137
trigger times: 2
Loss after 771831120 batches: 0.0135
trigger times: 3
Loss after 771962220 batches: 0.0128
trigger times: 0
Loss after 772093320 batches: 0.0125
trigger times: 1
Loss after 772224420 batches: 0.0121
trigger times: 2
Loss after 772355520 batches: 0.0118
trigger times: 3
Loss after 772486620 batches: 0.0118
trigger times: 0
Loss after 772617720 batches: 0.0115
trigger times: 1
Loss after 772748820 batches: 0.0111
trigger times: 2
Loss after 772879920 batches: 0.0111
trigger times: 3
Loss after 773011020 batches: 0.0108
trigger times: 4
Loss after 773142120 batches: 0.0109
trigger times: 5
Loss after 773273220 batches: 0.0107
trigger times: 6
Loss after 773404320 batches: 0.0103
trigger times: 7
Loss after 773535420 batches: 0.0102
trigger times: 8
Loss after 773666520 batches: 0.0102
trigger times: 9
Loss after 773797620 batches: 0.0101
trigger times: 10
Loss after 773928720 batches: 0.0099
trigger times: 11
Loss after 774059820 batches: 0.0099
trigger times: 12
Loss after 774190920 batches: 0.0098
trigger times: 13
Loss after 774322020 batches: 0.0096
trigger times: 14
Loss after 774453120 batches: 0.0093
trigger times: 15
Loss after 774584220 batches: 0.0095
trigger times: 16
Loss after 774715320 batches: 0.0095
trigger times: 17
Loss after 774846420 batches: 0.0093
trigger times: 18
Loss after 774977520 batches: 0.0092
trigger times: 19
Loss after 775108620 batches: 0.0091
trigger times: 20
Early stopping!
Start to test process.
Loss after 775239720 batches: 0.0090
Time to train on one home:  255.79457831382751
trigger times: 0
Loss after 775342320 batches: 0.1941
trigger times: 0
Loss after 775444920 batches: 0.0687
trigger times: 0
Loss after 775547520 batches: 0.0497
trigger times: 1
Loss after 775650120 batches: 0.0375
trigger times: 2
Loss after 775752720 batches: 0.0335
trigger times: 3
Loss after 775855320 batches: 0.0307
trigger times: 4
Loss after 775957920 batches: 0.0319
trigger times: 0
Loss after 776060520 batches: 0.0276
trigger times: 1
Loss after 776163120 batches: 0.0246
trigger times: 0
Loss after 776265720 batches: 0.0243
trigger times: 1
Loss after 776368320 batches: 0.0257
trigger times: 2
Loss after 776470920 batches: 0.0232
trigger times: 0
Loss after 776573520 batches: 0.0229
trigger times: 1
Loss after 776676120 batches: 0.0214
trigger times: 2
Loss after 776778720 batches: 0.0233
trigger times: 3
Loss after 776881320 batches: 0.0220
trigger times: 4
Loss after 776983920 batches: 0.0206
trigger times: 5
Loss after 777086520 batches: 0.0203
trigger times: 6
Loss after 777189120 batches: 0.0200
trigger times: 7
Loss after 777291720 batches: 0.0195
trigger times: 8
Loss after 777394320 batches: 0.0210
trigger times: 9
Loss after 777496920 batches: 0.0232
trigger times: 0
Loss after 777599520 batches: 0.0195
trigger times: 1
Loss after 777702120 batches: 0.0191
trigger times: 2
Loss after 777804720 batches: 0.0189
trigger times: 3
Loss after 777907320 batches: 0.0183
trigger times: 4
Loss after 778009920 batches: 0.0180
trigger times: 5
Loss after 778112520 batches: 0.0185
trigger times: 6
Loss after 778215120 batches: 0.0182
trigger times: 7
Loss after 778317720 batches: 0.0181
trigger times: 8
Loss after 778420320 batches: 0.0176
trigger times: 9
Loss after 778522920 batches: 0.0178
trigger times: 10
Loss after 778625520 batches: 0.0174
trigger times: 11
Loss after 778728120 batches: 0.0176
trigger times: 12
Loss after 778830720 batches: 0.0179
trigger times: 13
Loss after 778933320 batches: 0.0195
trigger times: 14
Loss after 779035920 batches: 0.0175
trigger times: 15
Loss after 779138520 batches: 0.0170
trigger times: 16
Loss after 779241120 batches: 0.0169
trigger times: 17
Loss after 779343720 batches: 0.0166
trigger times: 18
Loss after 779446320 batches: 0.0168
trigger times: 19
Loss after 779548920 batches: 0.0170
trigger times: 20
Early stopping!
Start to test process.
Loss after 779651520 batches: 0.0165
Time to train on one home:  259.5198247432709
trigger times: 0
Loss after 779782620 batches: 0.1341
trigger times: 1
Loss after 779913720 batches: 0.0435
trigger times: 0
Loss after 780044820 batches: 0.0334
trigger times: 1
Loss after 780175920 batches: 0.0292
trigger times: 2
Loss after 780307020 batches: 0.0268
trigger times: 3
Loss after 780438120 batches: 0.0247
trigger times: 4
Loss after 780569220 batches: 0.0240
trigger times: 5
Loss after 780700320 batches: 0.0226
trigger times: 6
Loss after 780831420 batches: 0.0218
trigger times: 7
Loss after 780962520 batches: 0.0216
trigger times: 8
Loss after 781093620 batches: 0.0209
trigger times: 9
Loss after 781224720 batches: 0.0205
trigger times: 10
Loss after 781355820 batches: 0.0197
trigger times: 11
Loss after 781486920 batches: 0.0193
trigger times: 12
Loss after 781618020 batches: 0.0190
trigger times: 13
Loss after 781749120 batches: 0.0187
trigger times: 14
Loss after 781880220 batches: 0.0184
trigger times: 15
Loss after 782011320 batches: 0.0182
trigger times: 16
Loss after 782142420 batches: 0.0177
trigger times: 17
Loss after 782273520 batches: 0.0174
trigger times: 18
Loss after 782404620 batches: 0.0173
trigger times: 19
Loss after 782535720 batches: 0.0169
trigger times: 20
Early stopping!
Start to test process.
Loss after 782666820 batches: 0.0170
Time to train on one home:  175.455983877182
trigger times: 0
Loss after 782797920 batches: 0.1986
trigger times: 0
Loss after 782929020 batches: 0.0573
trigger times: 1
Loss after 783060120 batches: 0.0423
trigger times: 2
Loss after 783191220 batches: 0.0374
trigger times: 3
Loss after 783322320 batches: 0.0338
trigger times: 4
Loss after 783453420 batches: 0.0317
trigger times: 0
Loss after 783584520 batches: 0.0302
trigger times: 0
Loss after 783715620 batches: 0.0292
trigger times: 1
Loss after 783846720 batches: 0.0282
trigger times: 2
Loss after 783977820 batches: 0.0275
trigger times: 3
Loss after 784108920 batches: 0.0263
trigger times: 4
Loss after 784240020 batches: 0.0260
trigger times: 5
Loss after 784371120 batches: 0.0254
trigger times: 6
Loss after 784502220 batches: 0.0250
trigger times: 7
Loss after 784633320 batches: 0.0243
trigger times: 8
Loss after 784764420 batches: 0.0243
trigger times: 0
Loss after 784895520 batches: 0.0241
trigger times: 1
Loss after 785026620 batches: 0.0234
trigger times: 2
Loss after 785157720 batches: 0.0231
trigger times: 3
Loss after 785288820 batches: 0.0226
trigger times: 4
Loss after 785419920 batches: 0.0223
trigger times: 0
Loss after 785551020 batches: 0.0222
trigger times: 1
Loss after 785682120 batches: 0.0217
trigger times: 2
Loss after 785813220 batches: 0.0219
trigger times: 3
Loss after 785944320 batches: 0.0214
trigger times: 4
Loss after 786075420 batches: 0.0215
trigger times: 5
Loss after 786206520 batches: 0.0213
trigger times: 6
Loss after 786337620 batches: 0.0211
trigger times: 7
Loss after 786468720 batches: 0.0209
trigger times: 8
Loss after 786599820 batches: 0.0211
trigger times: 9
Loss after 786730920 batches: 0.0206
trigger times: 10
Loss after 786862020 batches: 0.0206
trigger times: 0
Loss after 786993120 batches: 0.0205
trigger times: 1
Loss after 787124220 batches: 0.0203
trigger times: 2
Loss after 787255320 batches: 0.0204
trigger times: 3
Loss after 787386420 batches: 0.0200
trigger times: 4
Loss after 787517520 batches: 0.0197
trigger times: 5
Loss after 787648620 batches: 0.0201
trigger times: 6
Loss after 787779720 batches: 0.0196
trigger times: 7
Loss after 787910820 batches: 0.0195
trigger times: 8
Loss after 788041920 batches: 0.0193
trigger times: 9
Loss after 788173020 batches: 0.0192
trigger times: 10
Loss after 788304120 batches: 0.0190
trigger times: 11
Loss after 788435220 batches: 0.0193
trigger times: 12
Loss after 788566320 batches: 0.0190
trigger times: 13
Loss after 788697420 batches: 0.0191
trigger times: 14
Loss after 788828520 batches: 0.0190
trigger times: 15
Loss after 788959620 batches: 0.0191
trigger times: 16
Loss after 789090720 batches: 0.0186
trigger times: 17
Loss after 789221820 batches: 0.0184
trigger times: 0
Loss after 789352920 batches: 0.0185
trigger times: 1
Loss after 789484020 batches: 0.0184
trigger times: 2
Loss after 789615120 batches: 0.0183
trigger times: 3
Loss after 789746220 batches: 0.0180
trigger times: 4
Loss after 789877320 batches: 0.0179
trigger times: 5
Loss after 790008420 batches: 0.0181
trigger times: 6
Loss after 790139520 batches: 0.0178
trigger times: 7
Loss after 790270620 batches: 0.0183
trigger times: 8
Loss after 790401720 batches: 0.0179
trigger times: 9
Loss after 790532820 batches: 0.0179
trigger times: 10
Loss after 790663920 batches: 0.0176
trigger times: 11
Loss after 790795020 batches: 0.0176
trigger times: 12
Loss after 790926120 batches: 0.0178
trigger times: 13
Loss after 791057220 batches: 0.0173
trigger times: 14
Loss after 791188320 batches: 0.0171
trigger times: 15
Loss after 791319420 batches: 0.0172
trigger times: 16
Loss after 791450520 batches: 0.0174
trigger times: 17
Loss after 791581620 batches: 0.0176
trigger times: 18
Loss after 791712720 batches: 0.0176
trigger times: 19
Loss after 791843820 batches: 0.0172
trigger times: 20
Early stopping!
Start to test process.
Loss after 791974920 batches: 0.0173
Time to train on one home:  519.9009370803833
trigger times: 0
Loss after 792103560 batches: 0.0964
trigger times: 0
Loss after 792232200 batches: 0.0287
trigger times: 1
Loss after 792360840 batches: 0.0215
trigger times: 2
Loss after 792489480 batches: 0.0190
trigger times: 3
Loss after 792618120 batches: 0.0181
trigger times: 4
Loss after 792746760 batches: 0.0169
trigger times: 5
Loss after 792875400 batches: 0.0157
trigger times: 6
Loss after 793004040 batches: 0.0150
trigger times: 7
Loss after 793132680 batches: 0.0150
trigger times: 8
Loss after 793261320 batches: 0.0146
trigger times: 9
Loss after 793389960 batches: 0.0140
trigger times: 10
Loss after 793518600 batches: 0.0141
trigger times: 11
Loss after 793647240 batches: 0.0134
trigger times: 12
Loss after 793775880 batches: 0.0132
trigger times: 13
Loss after 793904520 batches: 0.0130
trigger times: 14
Loss after 794033160 batches: 0.0128
trigger times: 15
Loss after 794161800 batches: 0.0128
trigger times: 16
Loss after 794290440 batches: 0.0127
trigger times: 17
Loss after 794419080 batches: 0.0122
trigger times: 18
Loss after 794547720 batches: 0.0120
trigger times: 19
Loss after 794676360 batches: 0.0120
trigger times: 20
Early stopping!
Start to test process.
Loss after 794805000 batches: 0.0121
Time to train on one home:  167.44245386123657
trigger times: 0
Loss after 794936100 batches: 0.2570
trigger times: 0
Loss after 795067200 batches: 0.0617
trigger times: 0
Loss after 795198300 batches: 0.0456
trigger times: 1
Loss after 795329400 batches: 0.0394
trigger times: 2
Loss after 795460500 batches: 0.0366
trigger times: 3
Loss after 795591600 batches: 0.0343
trigger times: 4
Loss after 795722700 batches: 0.0326
trigger times: 5
Loss after 795853800 batches: 0.0310
trigger times: 6
Loss after 795984900 batches: 0.0303
trigger times: 7
Loss after 796116000 batches: 0.0291
trigger times: 8
Loss after 796247100 batches: 0.0285
trigger times: 9
Loss after 796378200 batches: 0.0279
trigger times: 10
Loss after 796509300 batches: 0.0272
trigger times: 0
Loss after 796640400 batches: 0.0264
trigger times: 1
Loss after 796771500 batches: 0.0258
trigger times: 2
Loss after 796902600 batches: 0.0255
trigger times: 0
Loss after 797033700 batches: 0.0251
trigger times: 1
Loss after 797164800 batches: 0.0250
trigger times: 2
Loss after 797295900 batches: 0.0245
trigger times: 3
Loss after 797427000 batches: 0.0243
trigger times: 4
Loss after 797558100 batches: 0.0236
trigger times: 5
Loss after 797689200 batches: 0.0233
trigger times: 6
Loss after 797820300 batches: 0.0233
trigger times: 7
Loss after 797951400 batches: 0.0229
trigger times: 8
Loss after 798082500 batches: 0.0230
trigger times: 9
Loss after 798213600 batches: 0.0225
trigger times: 10
Loss after 798344700 batches: 0.0223
trigger times: 11
Loss after 798475800 batches: 0.0223
trigger times: 12
Loss after 798606900 batches: 0.0218
trigger times: 13
Loss after 798738000 batches: 0.0217
trigger times: 14
Loss after 798869100 batches: 0.0216
trigger times: 15
Loss after 799000200 batches: 0.0215
trigger times: 16
Loss after 799131300 batches: 0.0213
trigger times: 17
Loss after 799262400 batches: 0.0213
trigger times: 18
Loss after 799393500 batches: 0.0209
trigger times: 0
Loss after 799524600 batches: 0.0209
trigger times: 1
Loss after 799655700 batches: 0.0207
trigger times: 2
Loss after 799786800 batches: 0.0205
trigger times: 3
Loss after 799917900 batches: 0.0207
trigger times: 4
Loss after 800049000 batches: 0.0203
trigger times: 5
Loss after 800180100 batches: 0.0199
trigger times: 6
Loss after 800311200 batches: 0.0198
trigger times: 0
Loss after 800442300 batches: 0.0200
trigger times: 1
Loss after 800573400 batches: 0.0196
trigger times: 2
Loss after 800704500 batches: 0.0195
trigger times: 0
Loss after 800835600 batches: 0.0195
trigger times: 1
Loss after 800966700 batches: 0.0196
trigger times: 2
Loss after 801097800 batches: 0.0194
trigger times: 3
Loss after 801228900 batches: 0.0193
trigger times: 0
Loss after 801360000 batches: 0.0192
trigger times: 1
Loss after 801491100 batches: 0.0190
trigger times: 0
Loss after 801622200 batches: 0.0190
trigger times: 1
Loss after 801753300 batches: 0.0189
trigger times: 2
Loss after 801884400 batches: 0.0189
trigger times: 3
Loss after 802015500 batches: 0.0186
trigger times: 4
Loss after 802146600 batches: 0.0186
trigger times: 5
Loss after 802277700 batches: 0.0184
trigger times: 6
Loss after 802408800 batches: 0.0187
trigger times: 0
Loss after 802539900 batches: 0.0183
trigger times: 1
Loss after 802671000 batches: 0.0182
trigger times: 2
Loss after 802802100 batches: 0.0183
trigger times: 3
Loss after 802933200 batches: 0.0183
trigger times: 4
Loss after 803064300 batches: 0.0182
trigger times: 5
Loss after 803195400 batches: 0.0180
trigger times: 6
Loss after 803326500 batches: 0.0181
trigger times: 7
Loss after 803457600 batches: 0.0178
trigger times: 8
Loss after 803588700 batches: 0.0179
trigger times: 9
Loss after 803719800 batches: 0.0175
trigger times: 10
Loss after 803850900 batches: 0.0177
trigger times: 11
Loss after 803982000 batches: 0.0177
trigger times: 12
Loss after 804113100 batches: 0.0178
trigger times: 13
Loss after 804244200 batches: 0.0175
trigger times: 14
Loss after 804375300 batches: 0.0175
trigger times: 15
Loss after 804506400 batches: 0.0175
trigger times: 16
Loss after 804637500 batches: 0.0174
trigger times: 17
Loss after 804768600 batches: 0.0172
trigger times: 18
Loss after 804899700 batches: 0.0170
trigger times: 19
Loss after 805030800 batches: 0.0171
trigger times: 20
Early stopping!
Start to test process.
Loss after 805161900 batches: 0.0171
Time to train on one home:  580.5059289932251
trigger times: 0
Loss after 805293000 batches: 0.2133
trigger times: 0
Loss after 805424100 batches: 0.0684
trigger times: 0
Loss after 805555200 batches: 0.0489
trigger times: 0
Loss after 805686300 batches: 0.0414
trigger times: 0
Loss after 805817400 batches: 0.0351
trigger times: 1
Loss after 805948500 batches: 0.0334
trigger times: 2
Loss after 806079600 batches: 0.0312
trigger times: 0
Loss after 806210700 batches: 0.0302
trigger times: 1
Loss after 806341800 batches: 0.0286
trigger times: 2
Loss after 806472900 batches: 0.0273
trigger times: 3
Loss after 806604000 batches: 0.0281
trigger times: 4
Loss after 806735100 batches: 0.0280
trigger times: 0
Loss after 806866200 batches: 0.0277
trigger times: 0
Loss after 806997300 batches: 0.0258
trigger times: 1
Loss after 807128400 batches: 0.0261
trigger times: 0
Loss after 807259500 batches: 0.0252
trigger times: 1
Loss after 807390600 batches: 0.0246
trigger times: 2
Loss after 807521700 batches: 0.0245
trigger times: 3
Loss after 807652800 batches: 0.0238
trigger times: 4
Loss after 807783900 batches: 0.0243
trigger times: 0
Loss after 807915000 batches: 0.0234
trigger times: 1
Loss after 808046100 batches: 0.0240
trigger times: 2
Loss after 808177200 batches: 0.0238
trigger times: 3
Loss after 808308300 batches: 0.0232
trigger times: 4
Loss after 808439400 batches: 0.0237
trigger times: 0
Loss after 808570500 batches: 0.0232
trigger times: 1
Loss after 808701600 batches: 0.0227
trigger times: 2
Loss after 808832700 batches: 0.0219
trigger times: 3
Loss after 808963800 batches: 0.0221
trigger times: 4
Loss after 809094900 batches: 0.0222
trigger times: 5
Loss after 809226000 batches: 0.0215
trigger times: 0
Loss after 809357100 batches: 0.0217
trigger times: 0
Loss after 809488200 batches: 0.0216
trigger times: 1
Loss after 809619300 batches: 0.0223
trigger times: 0
Loss after 809750400 batches: 0.0212
trigger times: 1
Loss after 809881500 batches: 0.0213
trigger times: 0
Loss after 810012600 batches: 0.0216
trigger times: 1
Loss after 810143700 batches: 0.0208
trigger times: 2
Loss after 810274800 batches: 0.0212
trigger times: 3
Loss after 810405900 batches: 0.0207
trigger times: 4
Loss after 810537000 batches: 0.0204
trigger times: 5
Loss after 810668100 batches: 0.0210
trigger times: 6
Loss after 810799200 batches: 0.0216
trigger times: 7
Loss after 810930300 batches: 0.0203
trigger times: 8
Loss after 811061400 batches: 0.0192
trigger times: 9
Loss after 811192500 batches: 0.0202
trigger times: 10
Loss after 811323600 batches: 0.0201
trigger times: 11
Loss after 811454700 batches: 0.0196
trigger times: 0
Loss after 811585800 batches: 0.0209
trigger times: 1
Loss after 811716900 batches: 0.0198
trigger times: 0
Loss after 811848000 batches: 0.0198
trigger times: 0
Loss after 811979100 batches: 0.0210
trigger times: 1
Loss after 812110200 batches: 0.0200
trigger times: 2
Loss after 812241300 batches: 0.0207
trigger times: 3
Loss after 812372400 batches: 0.0195
trigger times: 4
Loss after 812503500 batches: 0.0192
trigger times: 5
Loss after 812634600 batches: 0.0197
trigger times: 6
Loss after 812765700 batches: 0.0202
trigger times: 0
Loss after 812896800 batches: 0.0189
trigger times: 1
Loss after 813027900 batches: 0.0197
trigger times: 2
Loss after 813159000 batches: 0.0183
trigger times: 3
Loss after 813290100 batches: 0.0183
trigger times: 4
Loss after 813421200 batches: 0.0179
trigger times: 5
Loss after 813552300 batches: 0.0189
trigger times: 6
Loss after 813683400 batches: 0.0188
trigger times: 7
Loss after 813814500 batches: 0.0180
trigger times: 8
Loss after 813945600 batches: 0.0189
trigger times: 9
Loss after 814076700 batches: 0.0189
trigger times: 10
Loss after 814207800 batches: 0.0189
trigger times: 11
Loss after 814338900 batches: 0.0187
trigger times: 12
Loss after 814470000 batches: 0.0181
trigger times: 13
Loss after 814601100 batches: 0.0185
trigger times: 14
Loss after 814732200 batches: 0.0188
trigger times: 15
Loss after 814863300 batches: 0.0183
trigger times: 16
Loss after 814994400 batches: 0.0179
trigger times: 17
Loss after 815125500 batches: 0.0181
trigger times: 18
Loss after 815256600 batches: 0.0192
trigger times: 19
Loss after 815387700 batches: 0.0180
trigger times: 20
Early stopping!
Start to test process.
Loss after 815518800 batches: 0.0177
Time to train on one home:  575.9875304698944
trigger times: 0
Loss after 815649900 batches: 0.0653
trigger times: 0
Loss after 815781000 batches: 0.0214
trigger times: 0
Loss after 815912100 batches: 0.0163
trigger times: 0
Loss after 816043200 batches: 0.0144
trigger times: 1
Loss after 816174300 batches: 0.0131
trigger times: 2
Loss after 816305400 batches: 0.0124
trigger times: 3
Loss after 816436500 batches: 0.0117
trigger times: 4
Loss after 816567600 batches: 0.0113
trigger times: 5
Loss after 816698700 batches: 0.0109
trigger times: 6
Loss after 816829800 batches: 0.0103
trigger times: 7
Loss after 816960900 batches: 0.0101
trigger times: 0
Loss after 817092000 batches: 0.0102
trigger times: 1
Loss after 817223100 batches: 0.0098
trigger times: 2
Loss after 817354200 batches: 0.0095
trigger times: 3
Loss after 817485300 batches: 0.0095
trigger times: 4
Loss after 817616400 batches: 0.0093
trigger times: 5
Loss after 817747500 batches: 0.0095
trigger times: 6
Loss after 817878600 batches: 0.0090
trigger times: 7
Loss after 818009700 batches: 0.0088
trigger times: 8
Loss after 818140800 batches: 0.0087
trigger times: 9
Loss after 818271900 batches: 0.0087
trigger times: 10
Loss after 818403000 batches: 0.0086
trigger times: 11
Loss after 818534100 batches: 0.0086
trigger times: 12
Loss after 818665200 batches: 0.0087
trigger times: 13
Loss after 818796300 batches: 0.0084
trigger times: 14
Loss after 818927400 batches: 0.0082
trigger times: 15
Loss after 819058500 batches: 0.0083
trigger times: 16
Loss after 819189600 batches: 0.0082
trigger times: 17
Loss after 819320700 batches: 0.0083
trigger times: 18
Loss after 819451800 batches: 0.0079
trigger times: 19
Loss after 819582900 batches: 0.0079
trigger times: 20
Early stopping!
Start to test process.
Loss after 819714000 batches: 0.0078
Time to train on one home:  240.41936659812927
trigger times: 0
Loss after 819792600 batches: 0.2147
trigger times: 0
Loss after 819871200 batches: 0.0529
trigger times: 0
Loss after 819949800 batches: 0.0349
trigger times: 0
Loss after 820028400 batches: 0.0290
trigger times: 1
Loss after 820107000 batches: 0.0258
trigger times: 0
Loss after 820185600 batches: 0.0238
trigger times: 1
Loss after 820264200 batches: 0.0228
trigger times: 0
Loss after 820342800 batches: 0.0218
trigger times: 1
Loss after 820421400 batches: 0.0215
trigger times: 0
Loss after 820500000 batches: 0.0205
trigger times: 1
Loss after 820578600 batches: 0.0205
trigger times: 2
Loss after 820657200 batches: 0.0191
trigger times: 3
Loss after 820735800 batches: 0.0193
trigger times: 4
Loss after 820814400 batches: 0.0183
trigger times: 5
Loss after 820893000 batches: 0.0180
trigger times: 6
Loss after 820971600 batches: 0.0176
trigger times: 7
Loss after 821050200 batches: 0.0170
trigger times: 8
Loss after 821128800 batches: 0.0173
trigger times: 9
Loss after 821207400 batches: 0.0172
trigger times: 0
Loss after 821286000 batches: 0.0167
trigger times: 1
Loss after 821364600 batches: 0.0162
trigger times: 2
Loss after 821443200 batches: 0.0164
trigger times: 3
Loss after 821521800 batches: 0.0162
trigger times: 0
Loss after 821600400 batches: 0.0157
trigger times: 1
Loss after 821679000 batches: 0.0159
trigger times: 2
Loss after 821757600 batches: 0.0158
trigger times: 3
Loss after 821836200 batches: 0.0151
trigger times: 4
Loss after 821914800 batches: 0.0147
trigger times: 5
Loss after 821993400 batches: 0.0148
trigger times: 6
Loss after 822072000 batches: 0.0151
trigger times: 7
Loss after 822150600 batches: 0.0147
trigger times: 8
Loss after 822229200 batches: 0.0145
trigger times: 9
Loss after 822307800 batches: 0.0142
trigger times: 10
Loss after 822386400 batches: 0.0145
trigger times: 11
Loss after 822465000 batches: 0.0148
trigger times: 12
Loss after 822543600 batches: 0.0147
trigger times: 13
Loss after 822622200 batches: 0.0144
trigger times: 0
Loss after 822700800 batches: 0.0142
trigger times: 1
Loss after 822779400 batches: 0.0141
trigger times: 2
Loss after 822858000 batches: 0.0138
trigger times: 3
Loss after 822936600 batches: 0.0137
trigger times: 4
Loss after 823015200 batches: 0.0140
trigger times: 5
Loss after 823093800 batches: 0.0139
trigger times: 6
Loss after 823172400 batches: 0.0136
trigger times: 7
Loss after 823251000 batches: 0.0133
trigger times: 8
Loss after 823329600 batches: 0.0136
trigger times: 9
Loss after 823408200 batches: 0.0134
trigger times: 10
Loss after 823486800 batches: 0.0134
trigger times: 11
Loss after 823565400 batches: 0.0133
trigger times: 12
Loss after 823644000 batches: 0.0133
trigger times: 13
Loss after 823722600 batches: 0.0137
trigger times: 14
Loss after 823801200 batches: 0.0132
trigger times: 15
Loss after 823879800 batches: 0.0134
trigger times: 16
Loss after 823958400 batches: 0.0128
trigger times: 17
Loss after 824037000 batches: 0.0129
trigger times: 18
Loss after 824115600 batches: 0.0130
trigger times: 19
Loss after 824194200 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 824272800 batches: 0.0128
Time to train on one home:  280.3131191730499
trigger times: 0
Loss after 824403900 batches: 0.0733
trigger times: 0
Loss after 824535000 batches: 0.0239
trigger times: 0
Loss after 824666100 batches: 0.0180
trigger times: 1
Loss after 824797200 batches: 0.0157
trigger times: 2
Loss after 824928300 batches: 0.0147
trigger times: 3
Loss after 825059400 batches: 0.0138
trigger times: 4
Loss after 825190500 batches: 0.0131
trigger times: 5
Loss after 825321600 batches: 0.0124
trigger times: 6
Loss after 825452700 batches: 0.0122
trigger times: 7
Loss after 825583800 batches: 0.0118
trigger times: 8
Loss after 825714900 batches: 0.0115
trigger times: 0
Loss after 825846000 batches: 0.0115
trigger times: 1
Loss after 825977100 batches: 0.0108
trigger times: 0
Loss after 826108200 batches: 0.0108
trigger times: 0
Loss after 826239300 batches: 0.0108
trigger times: 0
Loss after 826370400 batches: 0.0105
trigger times: 0
Loss after 826501500 batches: 0.0103
trigger times: 1
Loss after 826632600 batches: 0.0102
trigger times: 2
Loss after 826763700 batches: 0.0100
trigger times: 3
Loss after 826894800 batches: 0.0098
trigger times: 0
Loss after 827025900 batches: 0.0099
trigger times: 1
Loss after 827157000 batches: 0.0098
trigger times: 2
Loss after 827288100 batches: 0.0095
trigger times: 3
Loss after 827419200 batches: 0.0095
trigger times: 4
Loss after 827550300 batches: 0.0095
trigger times: 5
Loss after 827681400 batches: 0.0094
trigger times: 6
Loss after 827812500 batches: 0.0092
trigger times: 7
Loss after 827943600 batches: 0.0092
trigger times: 8
Loss after 828074700 batches: 0.0091
trigger times: 9
Loss after 828205800 batches: 0.0091
trigger times: 10
Loss after 828336900 batches: 0.0089
trigger times: 11
Loss after 828468000 batches: 0.0088
trigger times: 12
Loss after 828599100 batches: 0.0089
trigger times: 13
Loss after 828730200 batches: 0.0087
trigger times: 14
Loss after 828861300 batches: 0.0088
trigger times: 15
Loss after 828992400 batches: 0.0086
trigger times: 16
Loss after 829123500 batches: 0.0085
trigger times: 17
Loss after 829254600 batches: 0.0086
trigger times: 18
Loss after 829385700 batches: 0.0086
trigger times: 19
Loss after 829516800 batches: 0.0083
trigger times: 20
Early stopping!
Start to test process.
Loss after 829647900 batches: 0.0084
Time to train on one home:  304.8516209125519
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318, 0.016183730855002625, 0.015608215882864637, 0.015855816283001423, 0.015272240943982671, 0.014904168019127074, 0.014211993132470634, 0.014714711150355778, 0.01356970331741986]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165], [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777], [0.5659677750534482, 0.3881937670236807, 0.47736113706344846, 1.0466814163269424, 0.5011883546712889, 24.728223616247963, 1547.1996], [0.5552795496251848, 0.39972320473570555, 0.4869199067938419, 1.0432319579028642, 0.4917435017003341, 24.646728924611136, 1518.0426], [0.556764519876904, 0.3981234608309019, 0.4873119444985629, 1.0354352452336422, 0.4930540032485836, 24.462528793273435, 1522.0884], [0.5499979423152076, 0.40547709897063045, 0.49151309919991676, 1.0193683799804445, 0.4870299426858642, 24.082943342919844, 1503.4916], [0.5525510013103485, 0.4027306139686573, 0.4951615175873931, 1.0298212095954027, 0.48927984833421284, 24.32989518911608, 1510.4373], [0.5425989263587527, 0.41345946401828226, 0.5050210962352305, 1.0162617040097164, 0.48049083244313984, 24.009547009604805, 1483.3052], [0.5446713169415792, 0.41122514224627915, 0.5011693979571417, 1.011856964658698, 0.482321176745565, 23.905483463673608, 1488.9554]]
Round_15_results:  [0.5446713169415792, 0.41122514224627915, 0.5011693979571417, 1.011856964658698, 0.482321176745565, 23.905483463673608, 1488.9554]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 6766 < 6767; dropping {'Training_Loss': 0.09784155433892079, 'Validation_Loss': 0.18132434868150288, 'Training_R2': 0.9014410052298814, 'Validation_R2': 0.8315129538353044, 'Training_F1': 0.8386187338749227, 'Validation_F1': 0.7354085232132558, 'Training_NEP': 0.32245488986286586, 'Validation_NEP': 0.5012624720274513, 'Training_NDE': 0.07398998679879665, 'Validation_NDE': 0.13417420192095453, 'Training_MAE': 10.679066455253293, 'Validation_MAE': 13.746696593599081, 'Training_MSE': 325.5443, 'Validation_MSE': 495.50162}.
trigger times: 0
Loss after 829779000 batches: 0.0978
trigger times: 0
Loss after 829910100 batches: 0.0264
trigger times: 0
Loss after 830041200 batches: 0.0190
trigger times: 0
Loss after 830172300 batches: 0.0166
trigger times: 0
Loss after 830303400 batches: 0.0152
trigger times: 0
Loss after 830434500 batches: 0.0140
trigger times: 0
Loss after 830565600 batches: 0.0135
trigger times: 1
Loss after 830696700 batches: 0.0133
trigger times: 2
Loss after 830827800 batches: 0.0126
trigger times: 3
Loss after 830958900 batches: 0.0124
trigger times: 0
Loss after 831090000 batches: 0.0121
trigger times: 0
Loss after 831221100 batches: 0.0115
trigger times: 1
Loss after 831352200 batches: 0.0116
trigger times: 2
Loss after 831483300 batches: 0.0111
trigger times: 0
Loss after 831614400 batches: 0.0108
trigger times: 1
Loss after 831745500 batches: 0.0109
trigger times: 2
Loss after 831876600 batches: 0.0106
trigger times: 3
Loss after 832007700 batches: 0.0104
trigger times: 4
Loss after 832138800 batches: 0.0103
trigger times: 5
Loss after 832269900 batches: 0.0102
trigger times: 6
Loss after 832401000 batches: 0.0100
trigger times: 0
Loss after 832532100 batches: 0.0100
trigger times: 0
Loss after 832663200 batches: 0.0100
trigger times: 1
Loss after 832794300 batches: 0.0098
trigger times: 2
Loss after 832925400 batches: 0.0097
trigger times: 3
Loss after 833056500 batches: 0.0096
trigger times: 4
Loss after 833187600 batches: 0.0094
trigger times: 5
Loss after 833318700 batches: 0.0094
trigger times: 6
Loss after 833449800 batches: 0.0093
trigger times: 7
Loss after 833580900 batches: 0.0092
trigger times: 8
Loss after 833712000 batches: 0.0094
trigger times: 0
Loss after 833843100 batches: 0.0092
trigger times: 1
Loss after 833974200 batches: 0.0092
trigger times: 2
Loss after 834105300 batches: 0.0090
trigger times: 3
Loss after 834236400 batches: 0.0092
trigger times: 4
Loss after 834367500 batches: 0.0088
trigger times: 5
Loss after 834498600 batches: 0.0089
trigger times: 6
Loss after 834629700 batches: 0.0088
trigger times: 7
Loss after 834760800 batches: 0.0087
trigger times: 8
Loss after 834891900 batches: 0.0089
trigger times: 9
Loss after 835023000 batches: 0.0087
trigger times: 10
Loss after 835154100 batches: 0.0086
trigger times: 11
Loss after 835285200 batches: 0.0086
trigger times: 12
Loss after 835416300 batches: 0.0085
trigger times: 13
Loss after 835547400 batches: 0.0084
trigger times: 14
Loss after 835678500 batches: 0.0083
trigger times: 15
Loss after 835809600 batches: 0.0082
trigger times: 16
Loss after 835940700 batches: 0.0080
trigger times: 17
Loss after 836071800 batches: 0.0082
trigger times: 18
Loss after 836202900 batches: 0.0081
trigger times: 19
Loss after 836334000 batches: 0.0082
trigger times: 20
Early stopping!
Start to test process.
Loss after 836465100 batches: 0.0082
Time to train on one home:  383.8254623413086
trigger times: 0
Loss after 836567700 batches: 0.2230
trigger times: 0
Loss after 836670300 batches: 0.0736
trigger times: 0
Loss after 836772900 batches: 0.0459
trigger times: 0
Loss after 836875500 batches: 0.0369
trigger times: 1
Loss after 836978100 batches: 0.0303
trigger times: 2
Loss after 837080700 batches: 0.0279
trigger times: 3
Loss after 837183300 batches: 0.0274
trigger times: 4
Loss after 837285900 batches: 0.0286
trigger times: 5
Loss after 837388500 batches: 0.0283
trigger times: 0
Loss after 837491100 batches: 0.0253
trigger times: 0
Loss after 837593700 batches: 0.0247
trigger times: 0
Loss after 837696300 batches: 0.0235
trigger times: 0
Loss after 837798900 batches: 0.0229
trigger times: 1
Loss after 837901500 batches: 0.0227
trigger times: 2
Loss after 838004100 batches: 0.0223
trigger times: 3
Loss after 838106700 batches: 0.0209
trigger times: 4
Loss after 838209300 batches: 0.0197
trigger times: 0
Loss after 838311900 batches: 0.0205
trigger times: 1
Loss after 838414500 batches: 0.0221
trigger times: 2
Loss after 838517100 batches: 0.0198
trigger times: 3
Loss after 838619700 batches: 0.0197
trigger times: 4
Loss after 838722300 batches: 0.0195
trigger times: 5
Loss after 838824900 batches: 0.0194
trigger times: 6
Loss after 838927500 batches: 0.0217
trigger times: 0
Loss after 839030100 batches: 0.0207
trigger times: 1
Loss after 839132700 batches: 0.0241
trigger times: 0
Loss after 839235300 batches: 0.0228
trigger times: 1
Loss after 839337900 batches: 0.0193
trigger times: 2
Loss after 839440500 batches: 0.0188
trigger times: 3
Loss after 839543100 batches: 0.0196
trigger times: 4
Loss after 839645700 batches: 0.0185
trigger times: 5
Loss after 839748300 batches: 0.0188
trigger times: 6
Loss after 839850900 batches: 0.0175
trigger times: 7
Loss after 839953500 batches: 0.0166
trigger times: 8
Loss after 840056100 batches: 0.0162
trigger times: 9
Loss after 840158700 batches: 0.0164
trigger times: 10
Loss after 840261300 batches: 0.0164
trigger times: 11
Loss after 840363900 batches: 0.0168
trigger times: 0
Loss after 840466500 batches: 0.0175
trigger times: 1
Loss after 840569100 batches: 0.0176
trigger times: 2
Loss after 840671700 batches: 0.0169
trigger times: 3
Loss after 840774300 batches: 0.0179
trigger times: 4
Loss after 840876900 batches: 0.0173
trigger times: 5
Loss after 840979500 batches: 0.0174
trigger times: 0
Loss after 841082100 batches: 0.0172
trigger times: 1
Loss after 841184700 batches: 0.0182
trigger times: 2
Loss after 841287300 batches: 0.0178
trigger times: 3
Loss after 841389900 batches: 0.0169
trigger times: 4
Loss after 841492500 batches: 0.0160
trigger times: 5
Loss after 841595100 batches: 0.0161
trigger times: 6
Loss after 841697700 batches: 0.0163
trigger times: 7
Loss after 841800300 batches: 0.0197
trigger times: 8
Loss after 841902900 batches: 0.0164
trigger times: 9
Loss after 842005500 batches: 0.0157
trigger times: 10
Loss after 842108100 batches: 0.0155
trigger times: 11
Loss after 842210700 batches: 0.0149
trigger times: 12
Loss after 842313300 batches: 0.0151
trigger times: 13
Loss after 842415900 batches: 0.0153
trigger times: 14
Loss after 842518500 batches: 0.0174
trigger times: 15
Loss after 842621100 batches: 0.0156
trigger times: 16
Loss after 842723700 batches: 0.0147
trigger times: 17
Loss after 842826300 batches: 0.0151
trigger times: 18
Loss after 842928900 batches: 0.0171
trigger times: 19
Loss after 843031500 batches: 0.0151
trigger times: 0
Loss after 843134100 batches: 0.0156
trigger times: 1
Loss after 843236700 batches: 0.0154
trigger times: 2
Loss after 843339300 batches: 0.0148
trigger times: 3
Loss after 843441900 batches: 0.0147
trigger times: 4
Loss after 843544500 batches: 0.0143
trigger times: 5
Loss after 843647100 batches: 0.0149
trigger times: 6
Loss after 843749700 batches: 0.0150
trigger times: 7
Loss after 843852300 batches: 0.0154
trigger times: 0
Loss after 843954900 batches: 0.0150
trigger times: 1
Loss after 844057500 batches: 0.0163
trigger times: 2
Loss after 844160100 batches: 0.0149
trigger times: 3
Loss after 844262700 batches: 0.0149
trigger times: 4
Loss after 844365300 batches: 0.0141
trigger times: 5
Loss after 844467900 batches: 0.0144
trigger times: 6
Loss after 844570500 batches: 0.0141
trigger times: 7
Loss after 844673100 batches: 0.0142
trigger times: 8
Loss after 844775700 batches: 0.0144
trigger times: 9
Loss after 844878300 batches: 0.0146
trigger times: 10
Loss after 844980900 batches: 0.0151
trigger times: 0
Loss after 845083500 batches: 0.0151
trigger times: 1
Loss after 845186100 batches: 0.0138
trigger times: 2
Loss after 845288700 batches: 0.0144
trigger times: 3
Loss after 845391300 batches: 0.0150
trigger times: 4
Loss after 845493900 batches: 0.0157
trigger times: 5
Loss after 845596500 batches: 0.0139
trigger times: 6
Loss after 845699100 batches: 0.0146
trigger times: 7
Loss after 845801700 batches: 0.0139
trigger times: 0
Loss after 845904300 batches: 0.0137
trigger times: 1
Loss after 846006900 batches: 0.0141
trigger times: 2
Loss after 846109500 batches: 0.0133
trigger times: 0
Loss after 846212100 batches: 0.0136
trigger times: 1
Loss after 846314700 batches: 0.0141
trigger times: 2
Loss after 846417300 batches: 0.0140
trigger times: 3
Loss after 846519900 batches: 0.0134
trigger times: 4
Loss after 846622500 batches: 0.0134
trigger times: 5
Loss after 846725100 batches: 0.0137
trigger times: 6
Loss after 846827700 batches: 0.0142
trigger times: 7
Loss after 846930300 batches: 0.0141
trigger times: 8
Loss after 847032900 batches: 0.0139
trigger times: 9
Loss after 847135500 batches: 0.0149
trigger times: 10
Loss after 847238100 batches: 0.0143
trigger times: 11
Loss after 847340700 batches: 0.0135
trigger times: 12
Loss after 847443300 batches: 0.0131
trigger times: 13
Loss after 847545900 batches: 0.0133
trigger times: 14
Loss after 847648500 batches: 0.0138
trigger times: 15
Loss after 847751100 batches: 0.0142
trigger times: 16
Loss after 847853700 batches: 0.0138
trigger times: 17
Loss after 847956300 batches: 0.0172
trigger times: 18
Loss after 848058900 batches: 0.0138
trigger times: 19
Loss after 848161500 batches: 0.0129
trigger times: 20
Early stopping!
Start to test process.
Loss after 848264100 batches: 0.0124
Time to train on one home:  670.7255494594574
trigger times: 0
Loss after 848395200 batches: 0.1560
trigger times: 0
Loss after 848526300 batches: 0.0449
trigger times: 1
Loss after 848657400 batches: 0.0336
trigger times: 2
Loss after 848788500 batches: 0.0293
trigger times: 3
Loss after 848919600 batches: 0.0268
trigger times: 4
Loss after 849050700 batches: 0.0249
trigger times: 5
Loss after 849181800 batches: 0.0240
trigger times: 6
Loss after 849312900 batches: 0.0229
trigger times: 7
Loss after 849444000 batches: 0.0220
trigger times: 8
Loss after 849575100 batches: 0.0214
trigger times: 9
Loss after 849706200 batches: 0.0208
trigger times: 10
Loss after 849837300 batches: 0.0203
trigger times: 11
Loss after 849968400 batches: 0.0198
trigger times: 12
Loss after 850099500 batches: 0.0192
trigger times: 13
Loss after 850230600 batches: 0.0188
trigger times: 14
Loss after 850361700 batches: 0.0185
trigger times: 15
Loss after 850492800 batches: 0.0182
trigger times: 16
Loss after 850623900 batches: 0.0180
trigger times: 17
Loss after 850755000 batches: 0.0176
trigger times: 18
Loss after 850886100 batches: 0.0173
trigger times: 19
Loss after 851017200 batches: 0.0171
trigger times: 20
Early stopping!
Start to test process.
Loss after 851148300 batches: 0.0169
Time to train on one home:  168.79478430747986
trigger times: 0
Loss after 851279400 batches: 0.1877
trigger times: 0
Loss after 851410500 batches: 0.0555
trigger times: 1
Loss after 851541600 batches: 0.0410
trigger times: 0
Loss after 851672700 batches: 0.0354
trigger times: 1
Loss after 851803800 batches: 0.0327
trigger times: 2
Loss after 851934900 batches: 0.0307
trigger times: 0
Loss after 852066000 batches: 0.0289
trigger times: 1
Loss after 852197100 batches: 0.0278
trigger times: 2
Loss after 852328200 batches: 0.0270
trigger times: 3
Loss after 852459300 batches: 0.0264
trigger times: 0
Loss after 852590400 batches: 0.0258
trigger times: 1
Loss after 852721500 batches: 0.0249
trigger times: 2
Loss after 852852600 batches: 0.0244
trigger times: 3
Loss after 852983700 batches: 0.0241
trigger times: 4
Loss after 853114800 batches: 0.0237
trigger times: 0
Loss after 853245900 batches: 0.0234
trigger times: 1
Loss after 853377000 batches: 0.0230
trigger times: 2
Loss after 853508100 batches: 0.0227
trigger times: 3
Loss after 853639200 batches: 0.0220
trigger times: 4
Loss after 853770300 batches: 0.0223
trigger times: 5
Loss after 853901400 batches: 0.0216
trigger times: 0
Loss after 854032500 batches: 0.0219
trigger times: 1
Loss after 854163600 batches: 0.0213
trigger times: 2
Loss after 854294700 batches: 0.0207
trigger times: 0
Loss after 854425800 batches: 0.0211
trigger times: 1
Loss after 854556900 batches: 0.0208
trigger times: 2
Loss after 854688000 batches: 0.0205
trigger times: 0
Loss after 854819100 batches: 0.0205
trigger times: 1
Loss after 854950200 batches: 0.0202
trigger times: 2
Loss after 855081300 batches: 0.0198
trigger times: 3
Loss after 855212400 batches: 0.0198
trigger times: 4
Loss after 855343500 batches: 0.0198
trigger times: 5
Loss after 855474600 batches: 0.0198
trigger times: 6
Loss after 855605700 batches: 0.0194
trigger times: 7
Loss after 855736800 batches: 0.0196
trigger times: 8
Loss after 855867900 batches: 0.0194
trigger times: 9
Loss after 855999000 batches: 0.0192
trigger times: 10
Loss after 856130100 batches: 0.0194
trigger times: 11
Loss after 856261200 batches: 0.0191
trigger times: 12
Loss after 856392300 batches: 0.0190
trigger times: 13
Loss after 856523400 batches: 0.0192
trigger times: 14
Loss after 856654500 batches: 0.0188
trigger times: 15
Loss after 856785600 batches: 0.0185
trigger times: 16
Loss after 856916700 batches: 0.0183
trigger times: 17
Loss after 857047800 batches: 0.0187
trigger times: 18
Loss after 857178900 batches: 0.0194
trigger times: 19
Loss after 857310000 batches: 0.0185
trigger times: 20
Early stopping!
Start to test process.
Loss after 857441100 batches: 0.0185
Time to train on one home:  353.633261680603
trigger times: 0
Loss after 857569740 batches: 0.1103
trigger times: 0
Loss after 857698380 batches: 0.0295
trigger times: 0
Loss after 857827020 batches: 0.0223
trigger times: 1
Loss after 857955660 batches: 0.0196
trigger times: 0
Loss after 858084300 batches: 0.0182
trigger times: 1
Loss after 858212940 batches: 0.0171
trigger times: 2
Loss after 858341580 batches: 0.0160
trigger times: 0
Loss after 858470220 batches: 0.0158
trigger times: 1
Loss after 858598860 batches: 0.0150
trigger times: 2
Loss after 858727500 batches: 0.0148
trigger times: 3
Loss after 858856140 batches: 0.0145
trigger times: 0
Loss after 858984780 batches: 0.0139
trigger times: 1
Loss after 859113420 batches: 0.0137
trigger times: 2
Loss after 859242060 batches: 0.0134
trigger times: 0
Loss after 859370700 batches: 0.0128
trigger times: 1
Loss after 859499340 batches: 0.0131
trigger times: 2
Loss after 859627980 batches: 0.0127
trigger times: 3
Loss after 859756620 batches: 0.0128
trigger times: 4
Loss after 859885260 batches: 0.0126
trigger times: 5
Loss after 860013900 batches: 0.0123
trigger times: 6
Loss after 860142540 batches: 0.0121
trigger times: 7
Loss after 860271180 batches: 0.0119
trigger times: 8
Loss after 860399820 batches: 0.0119
trigger times: 0
Loss after 860528460 batches: 0.0116
trigger times: 1
Loss after 860657100 batches: 0.0116
trigger times: 0
Loss after 860785740 batches: 0.0114
trigger times: 1
Loss after 860914380 batches: 0.0115
trigger times: 2
Loss after 861043020 batches: 0.0113
trigger times: 3
Loss after 861171660 batches: 0.0112
trigger times: 4
Loss after 861300300 batches: 0.0109
trigger times: 5
Loss after 861428940 batches: 0.0110
trigger times: 6
Loss after 861557580 batches: 0.0111
trigger times: 7
Loss after 861686220 batches: 0.0113
trigger times: 8
Loss after 861814860 batches: 0.0108
trigger times: 9
Loss after 861943500 batches: 0.0105
trigger times: 10
Loss after 862072140 batches: 0.0107
trigger times: 11
Loss after 862200780 batches: 0.0107
trigger times: 12
Loss after 862329420 batches: 0.0105
trigger times: 13
Loss after 862458060 batches: 0.0104
trigger times: 14
Loss after 862586700 batches: 0.0104
trigger times: 15
Loss after 862715340 batches: 0.0104
trigger times: 16
Loss after 862843980 batches: 0.0104
trigger times: 17
Loss after 862972620 batches: 0.0104
trigger times: 18
Loss after 863101260 batches: 0.0103
trigger times: 19
Loss after 863229900 batches: 0.0101
trigger times: 20
Early stopping!
Start to test process.
Loss after 863358540 batches: 0.0099
Time to train on one home:  335.552392244339
trigger times: 0
Loss after 863489640 batches: 0.2292
trigger times: 0
Loss after 863620740 batches: 0.0581
trigger times: 0
Loss after 863751840 batches: 0.0426
trigger times: 1
Loss after 863882940 batches: 0.0376
trigger times: 0
Loss after 864014040 batches: 0.0337
trigger times: 1
Loss after 864145140 batches: 0.0322
trigger times: 2
Loss after 864276240 batches: 0.0306
trigger times: 3
Loss after 864407340 batches: 0.0293
trigger times: 4
Loss after 864538440 batches: 0.0282
trigger times: 5
Loss after 864669540 batches: 0.0273
trigger times: 6
Loss after 864800640 batches: 0.0267
trigger times: 7
Loss after 864931740 batches: 0.0259
trigger times: 8
Loss after 865062840 batches: 0.0250
trigger times: 9
Loss after 865193940 batches: 0.0246
trigger times: 10
Loss after 865325040 batches: 0.0243
trigger times: 11
Loss after 865456140 batches: 0.0239
trigger times: 12
Loss after 865587240 batches: 0.0233
trigger times: 13
Loss after 865718340 batches: 0.0232
trigger times: 14
Loss after 865849440 batches: 0.0231
trigger times: 15
Loss after 865980540 batches: 0.0227
trigger times: 16
Loss after 866111640 batches: 0.0224
trigger times: 17
Loss after 866242740 batches: 0.0219
trigger times: 18
Loss after 866373840 batches: 0.0218
trigger times: 19
Loss after 866504940 batches: 0.0217
trigger times: 0
Loss after 866636040 batches: 0.0214
trigger times: 1
Loss after 866767140 batches: 0.0212
trigger times: 2
Loss after 866898240 batches: 0.0213
trigger times: 3
Loss after 867029340 batches: 0.0209
trigger times: 4
Loss after 867160440 batches: 0.0209
trigger times: 0
Loss after 867291540 batches: 0.0204
trigger times: 1
Loss after 867422640 batches: 0.0203
trigger times: 2
Loss after 867553740 batches: 0.0201
trigger times: 3
Loss after 867684840 batches: 0.0200
trigger times: 4
Loss after 867815940 batches: 0.0201
trigger times: 5
Loss after 867947040 batches: 0.0201
trigger times: 6
Loss after 868078140 batches: 0.0199
trigger times: 7
Loss after 868209240 batches: 0.0196
trigger times: 8
Loss after 868340340 batches: 0.0195
trigger times: 9
Loss after 868471440 batches: 0.0192
trigger times: 10
Loss after 868602540 batches: 0.0192
trigger times: 11
Loss after 868733640 batches: 0.0189
trigger times: 12
Loss after 868864740 batches: 0.0188
trigger times: 13
Loss after 868995840 batches: 0.0191
trigger times: 14
Loss after 869126940 batches: 0.0187
trigger times: 15
Loss after 869258040 batches: 0.0187
trigger times: 16
Loss after 869389140 batches: 0.0185
trigger times: 17
Loss after 869520240 batches: 0.0182
trigger times: 0
Loss after 869651340 batches: 0.0184
trigger times: 1
Loss after 869782440 batches: 0.0184
trigger times: 2
Loss after 869913540 batches: 0.0182
trigger times: 3
Loss after 870044640 batches: 0.0181
trigger times: 4
Loss after 870175740 batches: 0.0183
trigger times: 5
Loss after 870306840 batches: 0.0180
trigger times: 6
Loss after 870437940 batches: 0.0182
trigger times: 7
Loss after 870569040 batches: 0.0178
trigger times: 8
Loss after 870700140 batches: 0.0180
trigger times: 9
Loss after 870831240 batches: 0.0177
trigger times: 10
Loss after 870962340 batches: 0.0173
trigger times: 11
Loss after 871093440 batches: 0.0175
trigger times: 12
Loss after 871224540 batches: 0.0175
trigger times: 0
Loss after 871355640 batches: 0.0176
trigger times: 1
Loss after 871486740 batches: 0.0172
trigger times: 2
Loss after 871617840 batches: 0.0175
trigger times: 3
Loss after 871748940 batches: 0.0173
trigger times: 4
Loss after 871880040 batches: 0.0172
trigger times: 5
Loss after 872011140 batches: 0.0170
trigger times: 6
Loss after 872142240 batches: 0.0170
trigger times: 7
Loss after 872273340 batches: 0.0169
trigger times: 8
Loss after 872404440 batches: 0.0168
trigger times: 9
Loss after 872535540 batches: 0.0167
trigger times: 10
Loss after 872666640 batches: 0.0169
trigger times: 11
Loss after 872797740 batches: 0.0168
trigger times: 12
Loss after 872928840 batches: 0.0165
trigger times: 13
Loss after 873059940 batches: 0.0165
trigger times: 14
Loss after 873191040 batches: 0.0164
trigger times: 15
Loss after 873322140 batches: 0.0165
trigger times: 16
Loss after 873453240 batches: 0.0163
trigger times: 17
Loss after 873584340 batches: 0.0162
trigger times: 18
Loss after 873715440 batches: 0.0164
trigger times: 19
Loss after 873846540 batches: 0.0162
trigger times: 0
Loss after 873977640 batches: 0.0164
trigger times: 1
Loss after 874108740 batches: 0.0164
trigger times: 2
Loss after 874239840 batches: 0.0160
trigger times: 3
Loss after 874370940 batches: 0.0159
trigger times: 4
Loss after 874502040 batches: 0.0160
trigger times: 5
Loss after 874633140 batches: 0.0158
trigger times: 6
Loss after 874764240 batches: 0.0157
trigger times: 7
Loss after 874895340 batches: 0.0159
trigger times: 8
Loss after 875026440 batches: 0.0159
trigger times: 9
Loss after 875157540 batches: 0.0157
trigger times: 10
Loss after 875288640 batches: 0.0158
trigger times: 11
Loss after 875419740 batches: 0.0158
trigger times: 12
Loss after 875550840 batches: 0.0160
trigger times: 13
Loss after 875681940 batches: 0.0159
trigger times: 14
Loss after 875813040 batches: 0.0157
trigger times: 15
Loss after 875944140 batches: 0.0160
trigger times: 16
Loss after 876075240 batches: 0.0158
trigger times: 17
Loss after 876206340 batches: 0.0156
trigger times: 18
Loss after 876337440 batches: 0.0158
trigger times: 19
Loss after 876468540 batches: 0.0153
trigger times: 20
Early stopping!
Start to test process.
Loss after 876599640 batches: 0.0153
Time to train on one home:  734.4306592941284
trigger times: 0
Loss after 876730740 batches: 0.2061
trigger times: 0
Loss after 876861840 batches: 0.0637
trigger times: 0
Loss after 876992940 batches: 0.0459
trigger times: 0
Loss after 877124040 batches: 0.0413
trigger times: 0
Loss after 877255140 batches: 0.0345
trigger times: 0
Loss after 877386240 batches: 0.0319
trigger times: 0
Loss after 877517340 batches: 0.0310
trigger times: 1
Loss after 877648440 batches: 0.0298
trigger times: 2
Loss after 877779540 batches: 0.0281
trigger times: 3
Loss after 877910640 batches: 0.0282
trigger times: 4
Loss after 878041740 batches: 0.0274
trigger times: 0
Loss after 878172840 batches: 0.0268
trigger times: 1
Loss after 878303940 batches: 0.0259
trigger times: 2
Loss after 878435040 batches: 0.0251
trigger times: 3
Loss after 878566140 batches: 0.0244
trigger times: 0
Loss after 878697240 batches: 0.0245
trigger times: 1
Loss after 878828340 batches: 0.0245
trigger times: 2
Loss after 878959440 batches: 0.0235
trigger times: 3
Loss after 879090540 batches: 0.0234
trigger times: 4
Loss after 879221640 batches: 0.0227
trigger times: 0
Loss after 879352740 batches: 0.0232
trigger times: 0
Loss after 879483840 batches: 0.0227
trigger times: 1
Loss after 879614940 batches: 0.0230
trigger times: 2
Loss after 879746040 batches: 0.0234
trigger times: 3
Loss after 879877140 batches: 0.0237
trigger times: 4
Loss after 880008240 batches: 0.0214
trigger times: 5
Loss after 880139340 batches: 0.0212
trigger times: 6
Loss after 880270440 batches: 0.0219
trigger times: 7
Loss after 880401540 batches: 0.0211
trigger times: 8
Loss after 880532640 batches: 0.0212
trigger times: 9
Loss after 880663740 batches: 0.0202
trigger times: 10
Loss after 880794840 batches: 0.0207
trigger times: 11
Loss after 880925940 batches: 0.0214
trigger times: 12
Loss after 881057040 batches: 0.0213
trigger times: 13
Loss after 881188140 batches: 0.0215
trigger times: 14
Loss after 881319240 batches: 0.0206
trigger times: 15
Loss after 881450340 batches: 0.0209
trigger times: 16
Loss after 881581440 batches: 0.0205
trigger times: 17
Loss after 881712540 batches: 0.0211
trigger times: 18
Loss after 881843640 batches: 0.0202
trigger times: 19
Loss after 881974740 batches: 0.0210
trigger times: 20
Early stopping!
Start to test process.
Loss after 882105840 batches: 0.0205
Time to train on one home:  311.88238620758057
trigger times: 0
Loss after 882236940 batches: 0.0634
trigger times: 0
Loss after 882368040 batches: 0.0202
trigger times: 1
Loss after 882499140 batches: 0.0157
trigger times: 2
Loss after 882630240 batches: 0.0136
trigger times: 0
Loss after 882761340 batches: 0.0128
trigger times: 1
Loss after 882892440 batches: 0.0117
trigger times: 2
Loss after 883023540 batches: 0.0114
trigger times: 3
Loss after 883154640 batches: 0.0108
trigger times: 4
Loss after 883285740 batches: 0.0106
trigger times: 5
Loss after 883416840 batches: 0.0103
trigger times: 6
Loss after 883547940 batches: 0.0100
trigger times: 7
Loss after 883679040 batches: 0.0099
trigger times: 8
Loss after 883810140 batches: 0.0096
trigger times: 9
Loss after 883941240 batches: 0.0096
trigger times: 10
Loss after 884072340 batches: 0.0092
trigger times: 11
Loss after 884203440 batches: 0.0090
trigger times: 12
Loss after 884334540 batches: 0.0089
trigger times: 13
Loss after 884465640 batches: 0.0088
trigger times: 14
Loss after 884596740 batches: 0.0085
trigger times: 15
Loss after 884727840 batches: 0.0086
trigger times: 16
Loss after 884858940 batches: 0.0084
trigger times: 17
Loss after 884990040 batches: 0.0083
trigger times: 18
Loss after 885121140 batches: 0.0085
trigger times: 19
Loss after 885252240 batches: 0.0081
trigger times: 20
Early stopping!
Start to test process.
Loss after 885383340 batches: 0.0081
Time to train on one home:  190.11073875427246
trigger times: 0
Loss after 885461940 batches: 0.2069
trigger times: 1
Loss after 885540540 batches: 0.0498
trigger times: 2
Loss after 885619140 batches: 0.0334
trigger times: 3
Loss after 885697740 batches: 0.0275
trigger times: 4
Loss after 885776340 batches: 0.0251
trigger times: 0
Loss after 885854940 batches: 0.0237
trigger times: 0
Loss after 885933540 batches: 0.0217
trigger times: 1
Loss after 886012140 batches: 0.0213
trigger times: 0
Loss after 886090740 batches: 0.0209
trigger times: 0
Loss after 886169340 batches: 0.0201
trigger times: 0
Loss after 886247940 batches: 0.0193
trigger times: 1
Loss after 886326540 batches: 0.0183
trigger times: 2
Loss after 886405140 batches: 0.0182
trigger times: 3
Loss after 886483740 batches: 0.0176
trigger times: 4
Loss after 886562340 batches: 0.0179
trigger times: 5
Loss after 886640940 batches: 0.0175
trigger times: 6
Loss after 886719540 batches: 0.0170
trigger times: 7
Loss after 886798140 batches: 0.0167
trigger times: 8
Loss after 886876740 batches: 0.0164
trigger times: 9
Loss after 886955340 batches: 0.0165
trigger times: 0
Loss after 887033940 batches: 0.0162
trigger times: 1
Loss after 887112540 batches: 0.0158
trigger times: 2
Loss after 887191140 batches: 0.0158
trigger times: 3
Loss after 887269740 batches: 0.0155
trigger times: 4
Loss after 887348340 batches: 0.0152
trigger times: 5
Loss after 887426940 batches: 0.0158
trigger times: 6
Loss after 887505540 batches: 0.0152
trigger times: 0
Loss after 887584140 batches: 0.0147
trigger times: 1
Loss after 887662740 batches: 0.0146
trigger times: 2
Loss after 887741340 batches: 0.0143
trigger times: 3
Loss after 887819940 batches: 0.0147
trigger times: 4
Loss after 887898540 batches: 0.0143
trigger times: 5
Loss after 887977140 batches: 0.0144
trigger times: 6
Loss after 888055740 batches: 0.0142
trigger times: 7
Loss after 888134340 batches: 0.0139
trigger times: 8
Loss after 888212940 batches: 0.0140
trigger times: 9
Loss after 888291540 batches: 0.0142
trigger times: 10
Loss after 888370140 batches: 0.0138
trigger times: 11
Loss after 888448740 batches: 0.0137
trigger times: 12
Loss after 888527340 batches: 0.0133
trigger times: 13
Loss after 888605940 batches: 0.0138
trigger times: 14
Loss after 888684540 batches: 0.0132
trigger times: 15
Loss after 888763140 batches: 0.0131
trigger times: 16
Loss after 888841740 batches: 0.0134
trigger times: 17
Loss after 888920340 batches: 0.0132
trigger times: 18
Loss after 888998940 batches: 0.0130
trigger times: 19
Loss after 889077540 batches: 0.0131
trigger times: 20
Early stopping!
Start to test process.
Loss after 889156140 batches: 0.0130
Time to train on one home:  233.03774118423462
trigger times: 0
Loss after 889287240 batches: 0.0782
trigger times: 0
Loss after 889418340 batches: 0.0240
trigger times: 0
Loss after 889549440 batches: 0.0179
trigger times: 1
Loss after 889680540 batches: 0.0150
trigger times: 2
Loss after 889811640 batches: 0.0140
trigger times: 3
Loss after 889942740 batches: 0.0134
trigger times: 4
Loss after 890073840 batches: 0.0127
trigger times: 5
Loss after 890204940 batches: 0.0125
trigger times: 6
Loss after 890336040 batches: 0.0117
trigger times: 7
Loss after 890467140 batches: 0.0116
trigger times: 0
Loss after 890598240 batches: 0.0114
trigger times: 1
Loss after 890729340 batches: 0.0112
trigger times: 2
Loss after 890860440 batches: 0.0108
trigger times: 3
Loss after 890991540 batches: 0.0105
trigger times: 4
Loss after 891122640 batches: 0.0102
trigger times: 5
Loss after 891253740 batches: 0.0102
trigger times: 6
Loss after 891384840 batches: 0.0101
trigger times: 7
Loss after 891515940 batches: 0.0099
trigger times: 8
Loss after 891647040 batches: 0.0097
trigger times: 9
Loss after 891778140 batches: 0.0095
trigger times: 10
Loss after 891909240 batches: 0.0097
trigger times: 11
Loss after 892040340 batches: 0.0096
trigger times: 12
Loss after 892171440 batches: 0.0099
trigger times: 13
Loss after 892302540 batches: 0.0095
trigger times: 14
Loss after 892433640 batches: 0.0092
trigger times: 15
Loss after 892564740 batches: 0.0090
trigger times: 16
Loss after 892695840 batches: 0.0090
trigger times: 17
Loss after 892826940 batches: 0.0090
trigger times: 18
Loss after 892958040 batches: 0.0088
trigger times: 19
Loss after 893089140 batches: 0.0088
trigger times: 20
Early stopping!
Start to test process.
Loss after 893220240 batches: 0.0088
Time to train on one home:  233.25494194030762
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318, 0.016183730855002625, 0.015608215882864637, 0.015855816283001423, 0.015272240943982671, 0.014904168019127074, 0.014211993132470634, 0.014714711150355778, 0.01356970331741986, 0.013157077172457638]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165], [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777], [0.5659677750534482, 0.3881937670236807, 0.47736113706344846, 1.0466814163269424, 0.5011883546712889, 24.728223616247963, 1547.1996], [0.5552795496251848, 0.39972320473570555, 0.4869199067938419, 1.0432319579028642, 0.4917435017003341, 24.646728924611136, 1518.0426], [0.556764519876904, 0.3981234608309019, 0.4873119444985629, 1.0354352452336422, 0.4930540032485836, 24.462528793273435, 1522.0884], [0.5499979423152076, 0.40547709897063045, 0.49151309919991676, 1.0193683799804445, 0.4870299426858642, 24.082943342919844, 1503.4916], [0.5525510013103485, 0.4027306139686573, 0.4951615175873931, 1.0298212095954027, 0.48927984833421284, 24.32989518911608, 1510.4373], [0.5425989263587527, 0.41345946401828226, 0.5050210962352305, 1.0162617040097164, 0.48049083244313984, 24.009547009604805, 1483.3052], [0.5446713169415792, 0.41122514224627915, 0.5011693979571417, 1.011856964658698, 0.482321176745565, 23.905483463673608, 1488.9554], [0.5494886537392935, 0.4060339214979838, 0.5010704622538157, 1.0286816103891239, 0.48657379668524176, 24.30297174940827, 1502.0835]]
Round_16_results:  [0.5494886537392935, 0.4060339214979838, 0.5010704622538157, 1.0286816103891239, 0.48657379668524176, 24.30297174940827, 1502.0835]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 7296 < 7297; dropping {'Training_Loss': 0.10040546138331576, 'Validation_Loss': 0.17216200629870096, 'Training_R2': 0.8988635086377091, 'Validation_R2': 0.8400234955714289, 'Training_F1': 0.8371474845071432, 'Validation_F1': 0.7619529158086534, 'Training_NEP': 0.32535384448727517, 'Validation_NEP': 0.4553193148186486, 'Training_NDE': 0.0759249592411754, 'Validation_NDE': 0.12739685510793447, 'Training_MAE': 10.775074083166755, 'Validation_MAE': 12.486744616450375, 'Training_MSE': 334.05792, 'Validation_MSE': 470.47308}.
trigger times: 0
Loss after 893351340 batches: 0.1004
trigger times: 0
Loss after 893482440 batches: 0.0244
trigger times: 1
Loss after 893613540 batches: 0.0180
trigger times: 0
Loss after 893744640 batches: 0.0159
trigger times: 0
Loss after 893875740 batches: 0.0145
trigger times: 1
Loss after 894006840 batches: 0.0138
trigger times: 0
Loss after 894137940 batches: 0.0132
trigger times: 1
Loss after 894269040 batches: 0.0125
trigger times: 2
Loss after 894400140 batches: 0.0122
trigger times: 3
Loss after 894531240 batches: 0.0121
trigger times: 0
Loss after 894662340 batches: 0.0121
trigger times: 0
Loss after 894793440 batches: 0.0113
trigger times: 1
Loss after 894924540 batches: 0.0111
trigger times: 2
Loss after 895055640 batches: 0.0111
trigger times: 0
Loss after 895186740 batches: 0.0108
trigger times: 1
Loss after 895317840 batches: 0.0107
trigger times: 2
Loss after 895448940 batches: 0.0105
trigger times: 3
Loss after 895580040 batches: 0.0103
trigger times: 0
Loss after 895711140 batches: 0.0101
trigger times: 1
Loss after 895842240 batches: 0.0098
trigger times: 2
Loss after 895973340 batches: 0.0098
trigger times: 3
Loss after 896104440 batches: 0.0096
trigger times: 4
Loss after 896235540 batches: 0.0095
trigger times: 5
Loss after 896366640 batches: 0.0095
trigger times: 6
Loss after 896497740 batches: 0.0093
trigger times: 7
Loss after 896628840 batches: 0.0092
trigger times: 8
Loss after 896759940 batches: 0.0092
trigger times: 9
Loss after 896891040 batches: 0.0090
trigger times: 10
Loss after 897022140 batches: 0.0090
trigger times: 11
Loss after 897153240 batches: 0.0090
trigger times: 12
Loss after 897284340 batches: 0.0089
trigger times: 13
Loss after 897415440 batches: 0.0088
trigger times: 14
Loss after 897546540 batches: 0.0090
trigger times: 15
Loss after 897677640 batches: 0.0087
trigger times: 16
Loss after 897808740 batches: 0.0087
trigger times: 17
Loss after 897939840 batches: 0.0088
trigger times: 18
Loss after 898070940 batches: 0.0087
trigger times: 19
Loss after 898202040 batches: 0.0085
trigger times: 20
Early stopping!
Start to test process.
Loss after 898333140 batches: 0.0085
Time to train on one home:  290.2222137451172
trigger times: 0
Loss after 898435740 batches: 0.1977
trigger times: 0
Loss after 898538340 batches: 0.0617
trigger times: 0
Loss after 898640940 batches: 0.0460
trigger times: 1
Loss after 898743540 batches: 0.0369
trigger times: 2
Loss after 898846140 batches: 0.0311
trigger times: 3
Loss after 898948740 batches: 0.0291
trigger times: 0
Loss after 899051340 batches: 0.0310
trigger times: 1
Loss after 899153940 batches: 0.0263
trigger times: 2
Loss after 899256540 batches: 0.0256
trigger times: 3
Loss after 899359140 batches: 0.0248
trigger times: 0
Loss after 899461740 batches: 0.0225
trigger times: 1
Loss after 899564340 batches: 0.0230
trigger times: 2
Loss after 899666940 batches: 0.0216
trigger times: 3
Loss after 899769540 batches: 0.0205
trigger times: 4
Loss after 899872140 batches: 0.0197
trigger times: 5
Loss after 899974740 batches: 0.0196
trigger times: 6
Loss after 900077340 batches: 0.0193
trigger times: 7
Loss after 900179940 batches: 0.0191
trigger times: 8
Loss after 900282540 batches: 0.0190
trigger times: 9
Loss after 900385140 batches: 0.0185
trigger times: 10
Loss after 900487740 batches: 0.0186
trigger times: 11
Loss after 900590340 batches: 0.0179
trigger times: 12
Loss after 900692940 batches: 0.0183
trigger times: 0
Loss after 900795540 batches: 0.0178
trigger times: 1
Loss after 900898140 batches: 0.0190
trigger times: 2
Loss after 901000740 batches: 0.0179
trigger times: 3
Loss after 901103340 batches: 0.0174
trigger times: 4
Loss after 901205940 batches: 0.0186
trigger times: 5
Loss after 901308540 batches: 0.0201
trigger times: 6
Loss after 901411140 batches: 0.0190
trigger times: 0
Loss after 901513740 batches: 0.0234
trigger times: 1
Loss after 901616340 batches: 0.0194
trigger times: 2
Loss after 901718940 batches: 0.0169
trigger times: 3
Loss after 901821540 batches: 0.0164
trigger times: 4
Loss after 901924140 batches: 0.0163
trigger times: 5
Loss after 902026740 batches: 0.0160
trigger times: 6
Loss after 902129340 batches: 0.0157
trigger times: 7
Loss after 902231940 batches: 0.0158
trigger times: 8
Loss after 902334540 batches: 0.0173
trigger times: 9
Loss after 902437140 batches: 0.0168
trigger times: 10
Loss after 902539740 batches: 0.0156
trigger times: 11
Loss after 902642340 batches: 0.0152
trigger times: 12
Loss after 902744940 batches: 0.0148
trigger times: 13
Loss after 902847540 batches: 0.0157
trigger times: 14
Loss after 902950140 batches: 0.0150
trigger times: 15
Loss after 903052740 batches: 0.0151
trigger times: 16
Loss after 903155340 batches: 0.0155
trigger times: 17
Loss after 903257940 batches: 0.0159
trigger times: 0
Loss after 903360540 batches: 0.0165
trigger times: 1
Loss after 903463140 batches: 0.0153
trigger times: 2
Loss after 903565740 batches: 0.0152
trigger times: 3
Loss after 903668340 batches: 0.0148
trigger times: 4
Loss after 903770940 batches: 0.0143
trigger times: 5
Loss after 903873540 batches: 0.0143
trigger times: 6
Loss after 903976140 batches: 0.0149
trigger times: 7
Loss after 904078740 batches: 0.0150
trigger times: 8
Loss after 904181340 batches: 0.0146
trigger times: 9
Loss after 904283940 batches: 0.0152
trigger times: 10
Loss after 904386540 batches: 0.0161
trigger times: 11
Loss after 904489140 batches: 0.0157
trigger times: 12
Loss after 904591740 batches: 0.0146
trigger times: 13
Loss after 904694340 batches: 0.0145
trigger times: 14
Loss after 904796940 batches: 0.0146
trigger times: 15
Loss after 904899540 batches: 0.0142
trigger times: 16
Loss after 905002140 batches: 0.0147
trigger times: 17
Loss after 905104740 batches: 0.0175
trigger times: 18
Loss after 905207340 batches: 0.0150
trigger times: 19
Loss after 905309940 batches: 0.0155
trigger times: 20
Early stopping!
Start to test process.
Loss after 905412540 batches: 0.0139
Time to train on one home:  407.03925609588623
trigger times: 0
Loss after 905543640 batches: 0.1504
trigger times: 0
Loss after 905674740 batches: 0.0440
trigger times: 1
Loss after 905805840 batches: 0.0330
trigger times: 0
Loss after 905936940 batches: 0.0288
trigger times: 0
Loss after 906068040 batches: 0.0266
trigger times: 1
Loss after 906199140 batches: 0.0248
trigger times: 2
Loss after 906330240 batches: 0.0232
trigger times: 3
Loss after 906461340 batches: 0.0228
trigger times: 4
Loss after 906592440 batches: 0.0215
trigger times: 5
Loss after 906723540 batches: 0.0208
trigger times: 6
Loss after 906854640 batches: 0.0203
trigger times: 7
Loss after 906985740 batches: 0.0197
trigger times: 8
Loss after 907116840 batches: 0.0196
trigger times: 9
Loss after 907247940 batches: 0.0189
trigger times: 10
Loss after 907379040 batches: 0.0187
trigger times: 11
Loss after 907510140 batches: 0.0183
trigger times: 12
Loss after 907641240 batches: 0.0180
trigger times: 13
Loss after 907772340 batches: 0.0179
trigger times: 14
Loss after 907903440 batches: 0.0175
trigger times: 15
Loss after 908034540 batches: 0.0171
trigger times: 16
Loss after 908165640 batches: 0.0169
trigger times: 17
Loss after 908296740 batches: 0.0167
trigger times: 18
Loss after 908427840 batches: 0.0164
trigger times: 19
Loss after 908558940 batches: 0.0165
trigger times: 20
Early stopping!
Start to test process.
Loss after 908690040 batches: 0.0164
Time to train on one home:  190.65067100524902
trigger times: 0
Loss after 908821140 batches: 0.2166
trigger times: 0
Loss after 908952240 batches: 0.0566
trigger times: 1
Loss after 909083340 batches: 0.0421
trigger times: 2
Loss after 909214440 batches: 0.0361
trigger times: 0
Loss after 909345540 batches: 0.0331
trigger times: 1
Loss after 909476640 batches: 0.0316
trigger times: 0
Loss after 909607740 batches: 0.0293
trigger times: 1
Loss after 909738840 batches: 0.0282
trigger times: 0
Loss after 909869940 batches: 0.0271
trigger times: 1
Loss after 910001040 batches: 0.0264
trigger times: 0
Loss after 910132140 batches: 0.0258
trigger times: 1
Loss after 910263240 batches: 0.0251
trigger times: 2
Loss after 910394340 batches: 0.0252
trigger times: 3
Loss after 910525440 batches: 0.0244
trigger times: 4
Loss after 910656540 batches: 0.0239
trigger times: 5
Loss after 910787640 batches: 0.0235
trigger times: 6
Loss after 910918740 batches: 0.0230
trigger times: 7
Loss after 911049840 batches: 0.0230
trigger times: 8
Loss after 911180940 batches: 0.0227
trigger times: 9
Loss after 911312040 batches: 0.0222
trigger times: 10
Loss after 911443140 batches: 0.0220
trigger times: 11
Loss after 911574240 batches: 0.0218
trigger times: 12
Loss after 911705340 batches: 0.0214
trigger times: 13
Loss after 911836440 batches: 0.0214
trigger times: 14
Loss after 911967540 batches: 0.0210
trigger times: 15
Loss after 912098640 batches: 0.0209
trigger times: 0
Loss after 912229740 batches: 0.0206
trigger times: 1
Loss after 912360840 batches: 0.0204
trigger times: 2
Loss after 912491940 batches: 0.0203
trigger times: 3
Loss after 912623040 batches: 0.0204
trigger times: 4
Loss after 912754140 batches: 0.0200
trigger times: 5
Loss after 912885240 batches: 0.0199
trigger times: 6
Loss after 913016340 batches: 0.0197
trigger times: 7
Loss after 913147440 batches: 0.0195
trigger times: 8
Loss after 913278540 batches: 0.0197
trigger times: 9
Loss after 913409640 batches: 0.0191
trigger times: 10
Loss after 913540740 batches: 0.0193
trigger times: 11
Loss after 913671840 batches: 0.0192
trigger times: 12
Loss after 913802940 batches: 0.0190
trigger times: 13
Loss after 913934040 batches: 0.0190
trigger times: 14
Loss after 914065140 batches: 0.0190
trigger times: 15
Loss after 914196240 batches: 0.0186
trigger times: 16
Loss after 914327340 batches: 0.0186
trigger times: 17
Loss after 914458440 batches: 0.0185
trigger times: 18
Loss after 914589540 batches: 0.0186
trigger times: 19
Loss after 914720640 batches: 0.0186
trigger times: 20
Early stopping!
Start to test process.
Loss after 914851740 batches: 0.0181
Time to train on one home:  347.2798924446106
trigger times: 0
Loss after 914980380 batches: 0.1019
trigger times: 0
Loss after 915109020 batches: 0.0283
trigger times: 0
Loss after 915237660 batches: 0.0213
trigger times: 1
Loss after 915366300 batches: 0.0188
trigger times: 0
Loss after 915494940 batches: 0.0175
trigger times: 1
Loss after 915623580 batches: 0.0163
trigger times: 2
Loss after 915752220 batches: 0.0155
trigger times: 3
Loss after 915880860 batches: 0.0149
trigger times: 4
Loss after 916009500 batches: 0.0145
trigger times: 5
Loss after 916138140 batches: 0.0142
trigger times: 0
Loss after 916266780 batches: 0.0140
trigger times: 1
Loss after 916395420 batches: 0.0135
trigger times: 2
Loss after 916524060 batches: 0.0131
trigger times: 3
Loss after 916652700 batches: 0.0127
trigger times: 4
Loss after 916781340 batches: 0.0126
trigger times: 5
Loss after 916909980 batches: 0.0123
trigger times: 6
Loss after 917038620 batches: 0.0124
trigger times: 7
Loss after 917167260 batches: 0.0119
trigger times: 8
Loss after 917295900 batches: 0.0122
trigger times: 9
Loss after 917424540 batches: 0.0118
trigger times: 10
Loss after 917553180 batches: 0.0117
trigger times: 11
Loss after 917681820 batches: 0.0115
trigger times: 12
Loss after 917810460 batches: 0.0116
trigger times: 13
Loss after 917939100 batches: 0.0115
trigger times: 0
Loss after 918067740 batches: 0.0113
trigger times: 1
Loss after 918196380 batches: 0.0113
trigger times: 2
Loss after 918325020 batches: 0.0111
trigger times: 3
Loss after 918453660 batches: 0.0110
trigger times: 4
Loss after 918582300 batches: 0.0108
trigger times: 5
Loss after 918710940 batches: 0.0111
trigger times: 6
Loss after 918839580 batches: 0.0106
trigger times: 7
Loss after 918968220 batches: 0.0106
trigger times: 8
Loss after 919096860 batches: 0.0106
trigger times: 9
Loss after 919225500 batches: 0.0105
trigger times: 10
Loss after 919354140 batches: 0.0103
trigger times: 11
Loss after 919482780 batches: 0.0104
trigger times: 12
Loss after 919611420 batches: 0.0102
trigger times: 13
Loss after 919740060 batches: 0.0103
trigger times: 14
Loss after 919868700 batches: 0.0100
trigger times: 15
Loss after 919997340 batches: 0.0100
trigger times: 16
Loss after 920125980 batches: 0.0101
trigger times: 17
Loss after 920254620 batches: 0.0100
trigger times: 18
Loss after 920383260 batches: 0.0099
trigger times: 19
Loss after 920511900 batches: 0.0100
trigger times: 20
Early stopping!
Start to test process.
Loss after 920640540 batches: 0.0098
Time to train on one home:  328.5685873031616
trigger times: 0
Loss after 920771640 batches: 0.2249
trigger times: 0
Loss after 920902740 batches: 0.0530
trigger times: 0
Loss after 921033840 batches: 0.0390
trigger times: 1
Loss after 921164940 batches: 0.0349
trigger times: 2
Loss after 921296040 batches: 0.0317
trigger times: 3
Loss after 921427140 batches: 0.0302
trigger times: 4
Loss after 921558240 batches: 0.0285
trigger times: 5
Loss after 921689340 batches: 0.0274
trigger times: 6
Loss after 921820440 batches: 0.0262
trigger times: 7
Loss after 921951540 batches: 0.0257
trigger times: 8
Loss after 922082640 batches: 0.0251
trigger times: 9
Loss after 922213740 batches: 0.0243
trigger times: 10
Loss after 922344840 batches: 0.0240
trigger times: 11
Loss after 922475940 batches: 0.0234
trigger times: 12
Loss after 922607040 batches: 0.0233
trigger times: 13
Loss after 922738140 batches: 0.0228
trigger times: 14
Loss after 922869240 batches: 0.0222
trigger times: 15
Loss after 923000340 batches: 0.0220
trigger times: 16
Loss after 923131440 batches: 0.0219
trigger times: 17
Loss after 923262540 batches: 0.0216
trigger times: 18
Loss after 923393640 batches: 0.0216
trigger times: 19
Loss after 923524740 batches: 0.0209
trigger times: 20
Early stopping!
Start to test process.
Loss after 923655840 batches: 0.0206
Time to train on one home:  177.03471279144287
trigger times: 0
Loss after 923786940 batches: 0.2234
trigger times: 0
Loss after 923918040 batches: 0.0641
trigger times: 0
Loss after 924049140 batches: 0.0449
trigger times: 0
Loss after 924180240 batches: 0.0386
trigger times: 0
Loss after 924311340 batches: 0.0336
trigger times: 0
Loss after 924442440 batches: 0.0314
trigger times: 0
Loss after 924573540 batches: 0.0303
trigger times: 0
Loss after 924704640 batches: 0.0302
trigger times: 1
Loss after 924835740 batches: 0.0282
trigger times: 2
Loss after 924966840 batches: 0.0262
trigger times: 0
Loss after 925097940 batches: 0.0263
trigger times: 0
Loss after 925229040 batches: 0.0267
trigger times: 1
Loss after 925360140 batches: 0.0258
trigger times: 0
Loss after 925491240 batches: 0.0252
trigger times: 1
Loss after 925622340 batches: 0.0244
trigger times: 0
Loss after 925753440 batches: 0.0240
trigger times: 1
Loss after 925884540 batches: 0.0245
trigger times: 0
Loss after 926015640 batches: 0.0242
trigger times: 1
Loss after 926146740 batches: 0.0234
trigger times: 0
Loss after 926277840 batches: 0.0237
trigger times: 0
Loss after 926408940 batches: 0.0231
trigger times: 0
Loss after 926540040 batches: 0.0231
trigger times: 1
Loss after 926671140 batches: 0.0228
trigger times: 0
Loss after 926802240 batches: 0.0219
trigger times: 1
Loss after 926933340 batches: 0.0217
trigger times: 2
Loss after 927064440 batches: 0.0216
trigger times: 0
Loss after 927195540 batches: 0.0212
trigger times: 1
Loss after 927326640 batches: 0.0225
trigger times: 0
Loss after 927457740 batches: 0.0223
trigger times: 1
Loss after 927588840 batches: 0.0215
trigger times: 2
Loss after 927719940 batches: 0.0209
trigger times: 3
Loss after 927851040 batches: 0.0220
trigger times: 4
Loss after 927982140 batches: 0.0217
trigger times: 5
Loss after 928113240 batches: 0.0206
trigger times: 6
Loss after 928244340 batches: 0.0208
trigger times: 7
Loss after 928375440 batches: 0.0203
trigger times: 8
Loss after 928506540 batches: 0.0197
trigger times: 9
Loss after 928637640 batches: 0.0206
trigger times: 10
Loss after 928768740 batches: 0.0202
trigger times: 11
Loss after 928899840 batches: 0.0196
trigger times: 12
Loss after 929030940 batches: 0.0198
trigger times: 13
Loss after 929162040 batches: 0.0195
trigger times: 14
Loss after 929293140 batches: 0.0198
trigger times: 15
Loss after 929424240 batches: 0.0211
trigger times: 16
Loss after 929555340 batches: 0.0196
trigger times: 17
Loss after 929686440 batches: 0.0203
trigger times: 18
Loss after 929817540 batches: 0.0196
trigger times: 19
Loss after 929948640 batches: 0.0202
trigger times: 20
Early stopping!
Start to test process.
Loss after 930079740 batches: 0.0203
Time to train on one home:  363.3382534980774
trigger times: 0
Loss after 930210840 batches: 0.0702
trigger times: 0
Loss after 930341940 batches: 0.0210
trigger times: 1
Loss after 930473040 batches: 0.0155
trigger times: 2
Loss after 930604140 batches: 0.0137
trigger times: 3
Loss after 930735240 batches: 0.0128
trigger times: 4
Loss after 930866340 batches: 0.0120
trigger times: 5
Loss after 930997440 batches: 0.0115
trigger times: 6
Loss after 931128540 batches: 0.0109
trigger times: 0
Loss after 931259640 batches: 0.0104
trigger times: 0
Loss after 931390740 batches: 0.0101
trigger times: 0
Loss after 931521840 batches: 0.0104
trigger times: 1
Loss after 931652940 batches: 0.0098
trigger times: 2
Loss after 931784040 batches: 0.0097
trigger times: 3
Loss after 931915140 batches: 0.0094
trigger times: 4
Loss after 932046240 batches: 0.0092
trigger times: 5
Loss after 932177340 batches: 0.0089
trigger times: 6
Loss after 932308440 batches: 0.0088
trigger times: 7
Loss after 932439540 batches: 0.0086
trigger times: 8
Loss after 932570640 batches: 0.0087
trigger times: 9
Loss after 932701740 batches: 0.0085
trigger times: 10
Loss after 932832840 batches: 0.0086
trigger times: 11
Loss after 932963940 batches: 0.0084
trigger times: 12
Loss after 933095040 batches: 0.0084
trigger times: 13
Loss after 933226140 batches: 0.0084
trigger times: 14
Loss after 933357240 batches: 0.0081
trigger times: 15
Loss after 933488340 batches: 0.0081
trigger times: 16
Loss after 933619440 batches: 0.0081
trigger times: 17
Loss after 933750540 batches: 0.0077
trigger times: 18
Loss after 933881640 batches: 0.0080
trigger times: 19
Loss after 934012740 batches: 0.0077
trigger times: 20
Early stopping!
Start to test process.
Loss after 934143840 batches: 0.0077
Time to train on one home:  237.1441252231598
trigger times: 0
Loss after 934222440 batches: 0.1930
trigger times: 0
Loss after 934301040 batches: 0.0482
trigger times: 0
Loss after 934379640 batches: 0.0326
trigger times: 0
Loss after 934458240 batches: 0.0274
trigger times: 1
Loss after 934536840 batches: 0.0245
trigger times: 0
Loss after 934615440 batches: 0.0238
trigger times: 1
Loss after 934694040 batches: 0.0216
trigger times: 2
Loss after 934772640 batches: 0.0205
trigger times: 3
Loss after 934851240 batches: 0.0202
trigger times: 4
Loss after 934929840 batches: 0.0190
trigger times: 0
Loss after 935008440 batches: 0.0190
trigger times: 0
Loss after 935087040 batches: 0.0183
trigger times: 0
Loss after 935165640 batches: 0.0185
trigger times: 1
Loss after 935244240 batches: 0.0177
trigger times: 2
Loss after 935322840 batches: 0.0170
trigger times: 0
Loss after 935401440 batches: 0.0172
trigger times: 1
Loss after 935480040 batches: 0.0168
trigger times: 2
Loss after 935558640 batches: 0.0163
trigger times: 3
Loss after 935637240 batches: 0.0161
trigger times: 4
Loss after 935715840 batches: 0.0162
trigger times: 5
Loss after 935794440 batches: 0.0156
trigger times: 6
Loss after 935873040 batches: 0.0154
trigger times: 7
Loss after 935951640 batches: 0.0156
trigger times: 0
Loss after 936030240 batches: 0.0150
trigger times: 0
Loss after 936108840 batches: 0.0147
trigger times: 1
Loss after 936187440 batches: 0.0148
trigger times: 2
Loss after 936266040 batches: 0.0149
trigger times: 3
Loss after 936344640 batches: 0.0144
trigger times: 4
Loss after 936423240 batches: 0.0142
trigger times: 5
Loss after 936501840 batches: 0.0144
trigger times: 6
Loss after 936580440 batches: 0.0143
trigger times: 7
Loss after 936659040 batches: 0.0136
trigger times: 8
Loss after 936737640 batches: 0.0139
trigger times: 9
Loss after 936816240 batches: 0.0144
trigger times: 10
Loss after 936894840 batches: 0.0140
trigger times: 11
Loss after 936973440 batches: 0.0138
trigger times: 12
Loss after 937052040 batches: 0.0136
trigger times: 13
Loss after 937130640 batches: 0.0133
trigger times: 14
Loss after 937209240 batches: 0.0133
trigger times: 15
Loss after 937287840 batches: 0.0132
trigger times: 16
Loss after 937366440 batches: 0.0133
trigger times: 17
Loss after 937445040 batches: 0.0133
trigger times: 18
Loss after 937523640 batches: 0.0132
trigger times: 19
Loss after 937602240 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 937680840 batches: 0.0129
Time to train on one home:  223.40833163261414
trigger times: 0
Loss after 937811940 batches: 0.0757
trigger times: 0
Loss after 937943040 batches: 0.0225
trigger times: 0
Loss after 938074140 batches: 0.0173
trigger times: 0
Loss after 938205240 batches: 0.0152
trigger times: 1
Loss after 938336340 batches: 0.0139
trigger times: 2
Loss after 938467440 batches: 0.0130
trigger times: 3
Loss after 938598540 batches: 0.0124
trigger times: 4
Loss after 938729640 batches: 0.0120
trigger times: 5
Loss after 938860740 batches: 0.0116
trigger times: 6
Loss after 938991840 batches: 0.0110
trigger times: 7
Loss after 939122940 batches: 0.0110
trigger times: 0
Loss after 939254040 batches: 0.0108
trigger times: 1
Loss after 939385140 batches: 0.0106
trigger times: 2
Loss after 939516240 batches: 0.0103
trigger times: 3
Loss after 939647340 batches: 0.0101
trigger times: 4
Loss after 939778440 batches: 0.0100
trigger times: 5
Loss after 939909540 batches: 0.0098
trigger times: 6
Loss after 940040640 batches: 0.0100
trigger times: 7
Loss after 940171740 batches: 0.0097
trigger times: 8
Loss after 940302840 batches: 0.0095
trigger times: 9
Loss after 940433940 batches: 0.0093
trigger times: 10
Loss after 940565040 batches: 0.0093
trigger times: 11
Loss after 940696140 batches: 0.0092
trigger times: 12
Loss after 940827240 batches: 0.0091
trigger times: 13
Loss after 940958340 batches: 0.0089
trigger times: 14
Loss after 941089440 batches: 0.0091
trigger times: 15
Loss after 941220540 batches: 0.0087
trigger times: 16
Loss after 941351640 batches: 0.0087
trigger times: 0
Loss after 941482740 batches: 0.0088
trigger times: 1
Loss after 941613840 batches: 0.0087
trigger times: 2
Loss after 941744940 batches: 0.0085
trigger times: 3
Loss after 941876040 batches: 0.0084
trigger times: 4
Loss after 942007140 batches: 0.0086
trigger times: 5
Loss after 942138240 batches: 0.0082
trigger times: 6
Loss after 942269340 batches: 0.0085
trigger times: 7
Loss after 942400440 batches: 0.0083
trigger times: 8
Loss after 942531540 batches: 0.0084
trigger times: 9
Loss after 942662640 batches: 0.0081
trigger times: 10
Loss after 942793740 batches: 0.0080
trigger times: 11
Loss after 942924840 batches: 0.0080
trigger times: 12
Loss after 943055940 batches: 0.0080
trigger times: 13
Loss after 943187040 batches: 0.0078
trigger times: 14
Loss after 943318140 batches: 0.0079
trigger times: 15
Loss after 943449240 batches: 0.0079
trigger times: 16
Loss after 943580340 batches: 0.0079
trigger times: 17
Loss after 943711440 batches: 0.0078
trigger times: 18
Loss after 943842540 batches: 0.0079
trigger times: 19
Loss after 943973640 batches: 0.0076
trigger times: 20
Early stopping!
Start to test process.
Loss after 944104740 batches: 0.0077
Time to train on one home:  367.3993124961853
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318, 0.016183730855002625, 0.015608215882864637, 0.015855816283001423, 0.015272240943982671, 0.014904168019127074, 0.014211993132470634, 0.014714711150355778, 0.01356970331741986, 0.013157077172457638, 0.013606505732267123]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165], [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777], [0.5659677750534482, 0.3881937670236807, 0.47736113706344846, 1.0466814163269424, 0.5011883546712889, 24.728223616247963, 1547.1996], [0.5552795496251848, 0.39972320473570555, 0.4869199067938419, 1.0432319579028642, 0.4917435017003341, 24.646728924611136, 1518.0426], [0.556764519876904, 0.3981234608309019, 0.4873119444985629, 1.0354352452336422, 0.4930540032485836, 24.462528793273435, 1522.0884], [0.5499979423152076, 0.40547709897063045, 0.49151309919991676, 1.0193683799804445, 0.4870299426858642, 24.082943342919844, 1503.4916], [0.5525510013103485, 0.4027306139686573, 0.4951615175873931, 1.0298212095954027, 0.48927984833421284, 24.32989518911608, 1510.4373], [0.5425989263587527, 0.41345946401828226, 0.5050210962352305, 1.0162617040097164, 0.48049083244313984, 24.009547009604805, 1483.3052], [0.5446713169415792, 0.41122514224627915, 0.5011693979571417, 1.011856964658698, 0.482321176745565, 23.905483463673608, 1488.9554], [0.5494886537392935, 0.4060339214979838, 0.5010704622538157, 1.0286816103891239, 0.48657379668524176, 24.30297174940827, 1502.0835], [0.5506102244059244, 0.4048432294636072, 0.5077384865654669, 1.0249413038763329, 0.4875492051552179, 24.21460566742873, 1505.0948]]
Round_17_results:  [0.5506102244059244, 0.4048432294636072, 0.5077384865654669, 1.0249413038763329, 0.4875492051552179, 24.21460566742873, 1505.0948]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 7718 < 7719; dropping {'Training_Loss': 0.09289276578797484, 'Validation_Loss': 0.20472992873854107, 'Training_R2': 0.9064409200933717, 'Validation_R2': 0.8099425982997568, 'Training_F1': 0.844288999988775, 'Validation_F1': 0.738481721395979, 'Training_NEP': 0.31112825874422706, 'Validation_NEP': 0.48880472541383313, 'Training_NDE': 0.07023646196214771, 'Validation_NDE': 0.1513516960073799, 'Training_MAE': 10.303950895735728, 'Validation_MAE': 13.405053497428963, 'Training_MSE': 309.02945, 'Validation_MSE': 558.9377}.
trigger times: 0
Loss after 944235840 batches: 0.0929
trigger times: 0
Loss after 944366940 batches: 0.0233
trigger times: 0
Loss after 944498040 batches: 0.0181
trigger times: 1
Loss after 944629140 batches: 0.0156
trigger times: 2
Loss after 944760240 batches: 0.0143
trigger times: 0
Loss after 944891340 batches: 0.0135
trigger times: 1
Loss after 945022440 batches: 0.0131
trigger times: 2
Loss after 945153540 batches: 0.0123
trigger times: 0
Loss after 945284640 batches: 0.0119
trigger times: 0
Loss after 945415740 batches: 0.0116
trigger times: 1
Loss after 945546840 batches: 0.0112
trigger times: 2
Loss after 945677940 batches: 0.0112
trigger times: 0
Loss after 945809040 batches: 0.0107
trigger times: 1
Loss after 945940140 batches: 0.0106
trigger times: 2
Loss after 946071240 batches: 0.0108
trigger times: 3
Loss after 946202340 batches: 0.0104
trigger times: 4
Loss after 946333440 batches: 0.0102
trigger times: 5
Loss after 946464540 batches: 0.0102
trigger times: 6
Loss after 946595640 batches: 0.0100
trigger times: 7
Loss after 946726740 batches: 0.0098
trigger times: 8
Loss after 946857840 batches: 0.0099
trigger times: 9
Loss after 946988940 batches: 0.0098
trigger times: 10
Loss after 947120040 batches: 0.0094
trigger times: 11
Loss after 947251140 batches: 0.0093
trigger times: 12
Loss after 947382240 batches: 0.0091
trigger times: 13
Loss after 947513340 batches: 0.0095
trigger times: 14
Loss after 947644440 batches: 0.0092
trigger times: 15
Loss after 947775540 batches: 0.0089
trigger times: 16
Loss after 947906640 batches: 0.0088
trigger times: 17
Loss after 948037740 batches: 0.0088
trigger times: 18
Loss after 948168840 batches: 0.0088
trigger times: 19
Loss after 948299940 batches: 0.0086
trigger times: 20
Early stopping!
Start to test process.
Loss after 948431040 batches: 0.0087
Time to train on one home:  251.4113049507141
trigger times: 0
Loss after 948533640 batches: 0.1881
trigger times: 0
Loss after 948636240 batches: 0.0616
trigger times: 0
Loss after 948738840 batches: 0.0427
trigger times: 1
Loss after 948841440 batches: 0.0318
trigger times: 0
Loss after 948944040 batches: 0.0280
trigger times: 0
Loss after 949046640 batches: 0.0304
trigger times: 1
Loss after 949149240 batches: 0.0323
trigger times: 2
Loss after 949251840 batches: 0.0248
trigger times: 3
Loss after 949354440 batches: 0.0239
trigger times: 4
Loss after 949457040 batches: 0.0223
trigger times: 5
Loss after 949559640 batches: 0.0214
trigger times: 6
Loss after 949662240 batches: 0.0218
trigger times: 7
Loss after 949764840 batches: 0.0206
trigger times: 8
Loss after 949867440 batches: 0.0195
trigger times: 0
Loss after 949970040 batches: 0.0189
trigger times: 1
Loss after 950072640 batches: 0.0190
trigger times: 2
Loss after 950175240 batches: 0.0192
trigger times: 0
Loss after 950277840 batches: 0.0193
trigger times: 0
Loss after 950380440 batches: 0.0185
trigger times: 1
Loss after 950483040 batches: 0.0180
trigger times: 2
Loss after 950585640 batches: 0.0180
trigger times: 0
Loss after 950688240 batches: 0.0197
trigger times: 1
Loss after 950790840 batches: 0.0197
trigger times: 2
Loss after 950893440 batches: 0.0176
trigger times: 3
Loss after 950996040 batches: 0.0187
trigger times: 4
Loss after 951098640 batches: 0.0187
trigger times: 5
Loss after 951201240 batches: 0.0183
trigger times: 0
Loss after 951303840 batches: 0.0174
trigger times: 1
Loss after 951406440 batches: 0.0176
trigger times: 2
Loss after 951509040 batches: 0.0172
trigger times: 3
Loss after 951611640 batches: 0.0172
trigger times: 4
Loss after 951714240 batches: 0.0179
trigger times: 5
Loss after 951816840 batches: 0.0178
trigger times: 0
Loss after 951919440 batches: 0.0168
trigger times: 1
Loss after 952022040 batches: 0.0171
trigger times: 2
Loss after 952124640 batches: 0.0174
trigger times: 3
Loss after 952227240 batches: 0.0171
trigger times: 0
Loss after 952329840 batches: 0.0182
trigger times: 1
Loss after 952432440 batches: 0.0165
trigger times: 2
Loss after 952535040 batches: 0.0163
trigger times: 3
Loss after 952637640 batches: 0.0152
trigger times: 4
Loss after 952740240 batches: 0.0153
trigger times: 5
Loss after 952842840 batches: 0.0151
trigger times: 6
Loss after 952945440 batches: 0.0161
trigger times: 7
Loss after 953048040 batches: 0.0150
trigger times: 8
Loss after 953150640 batches: 0.0159
trigger times: 9
Loss after 953253240 batches: 0.0147
trigger times: 10
Loss after 953355840 batches: 0.0148
trigger times: 11
Loss after 953458440 batches: 0.0152
trigger times: 12
Loss after 953561040 batches: 0.0158
trigger times: 13
Loss after 953663640 batches: 0.0151
trigger times: 14
Loss after 953766240 batches: 0.0147
trigger times: 15
Loss after 953868840 batches: 0.0144
trigger times: 16
Loss after 953971440 batches: 0.0154
trigger times: 17
Loss after 954074040 batches: 0.0159
trigger times: 0
Loss after 954176640 batches: 0.0217
trigger times: 0
Loss after 954279240 batches: 0.0162
trigger times: 1
Loss after 954381840 batches: 0.0157
trigger times: 0
Loss after 954484440 batches: 0.0150
trigger times: 1
Loss after 954587040 batches: 0.0158
trigger times: 2
Loss after 954689640 batches: 0.0156
trigger times: 0
Loss after 954792240 batches: 0.0142
trigger times: 1
Loss after 954894840 batches: 0.0147
trigger times: 2
Loss after 954997440 batches: 0.0149
trigger times: 3
Loss after 955100040 batches: 0.0158
trigger times: 4
Loss after 955202640 batches: 0.0144
trigger times: 0
Loss after 955305240 batches: 0.0143
trigger times: 1
Loss after 955407840 batches: 0.0146
trigger times: 2
Loss after 955510440 batches: 0.0148
trigger times: 3
Loss after 955613040 batches: 0.0138
trigger times: 4
Loss after 955715640 batches: 0.0140
trigger times: 5
Loss after 955818240 batches: 0.0139
trigger times: 6
Loss after 955920840 batches: 0.0136
trigger times: 7
Loss after 956023440 batches: 0.0180
trigger times: 8
Loss after 956126040 batches: 0.0140
trigger times: 9
Loss after 956228640 batches: 0.0139
trigger times: 10
Loss after 956331240 batches: 0.0142
trigger times: 11
Loss after 956433840 batches: 0.0137
trigger times: 12
Loss after 956536440 batches: 0.0129
trigger times: 13
Loss after 956639040 batches: 0.0131
trigger times: 14
Loss after 956741640 batches: 0.0142
trigger times: 15
Loss after 956844240 batches: 0.0134
trigger times: 16
Loss after 956946840 batches: 0.0130
trigger times: 17
Loss after 957049440 batches: 0.0122
trigger times: 18
Loss after 957152040 batches: 0.0128
trigger times: 19
Loss after 957254640 batches: 0.0134
trigger times: 20
Early stopping!
Start to test process.
Loss after 957357240 batches: 0.0129
Time to train on one home:  527.0524518489838
trigger times: 0
Loss after 957488340 batches: 0.1291
trigger times: 0
Loss after 957619440 batches: 0.0416
trigger times: 1
Loss after 957750540 batches: 0.0318
trigger times: 0
Loss after 957881640 batches: 0.0278
trigger times: 1
Loss after 958012740 batches: 0.0256
trigger times: 2
Loss after 958143840 batches: 0.0239
trigger times: 3
Loss after 958274940 batches: 0.0233
trigger times: 4
Loss after 958406040 batches: 0.0220
trigger times: 5
Loss after 958537140 batches: 0.0211
trigger times: 6
Loss after 958668240 batches: 0.0206
trigger times: 7
Loss after 958799340 batches: 0.0201
trigger times: 8
Loss after 958930440 batches: 0.0194
trigger times: 9
Loss after 959061540 batches: 0.0190
trigger times: 10
Loss after 959192640 batches: 0.0187
trigger times: 11
Loss after 959323740 batches: 0.0184
trigger times: 12
Loss after 959454840 batches: 0.0178
trigger times: 13
Loss after 959585940 batches: 0.0176
trigger times: 14
Loss after 959717040 batches: 0.0175
trigger times: 15
Loss after 959848140 batches: 0.0171
trigger times: 16
Loss after 959979240 batches: 0.0167
trigger times: 17
Loss after 960110340 batches: 0.0168
trigger times: 18
Loss after 960241440 batches: 0.0165
trigger times: 19
Loss after 960372540 batches: 0.0163
trigger times: 20
Early stopping!
Start to test process.
Loss after 960503640 batches: 0.0161
Time to train on one home:  196.65519261360168
trigger times: 0
Loss after 960634740 batches: 0.1924
trigger times: 0
Loss after 960765840 batches: 0.0519
trigger times: 0
Loss after 960896940 batches: 0.0392
trigger times: 1
Loss after 961028040 batches: 0.0344
trigger times: 2
Loss after 961159140 batches: 0.0317
trigger times: 3
Loss after 961290240 batches: 0.0304
trigger times: 4
Loss after 961421340 batches: 0.0287
trigger times: 5
Loss after 961552440 batches: 0.0277
trigger times: 6
Loss after 961683540 batches: 0.0265
trigger times: 7
Loss after 961814640 batches: 0.0259
trigger times: 8
Loss after 961945740 batches: 0.0252
trigger times: 9
Loss after 962076840 batches: 0.0244
trigger times: 0
Loss after 962207940 batches: 0.0242
trigger times: 1
Loss after 962339040 batches: 0.0235
trigger times: 2
Loss after 962470140 batches: 0.0232
trigger times: 3
Loss after 962601240 batches: 0.0229
trigger times: 4
Loss after 962732340 batches: 0.0225
trigger times: 0
Loss after 962863440 batches: 0.0224
trigger times: 1
Loss after 962994540 batches: 0.0222
trigger times: 2
Loss after 963125640 batches: 0.0220
trigger times: 3
Loss after 963256740 batches: 0.0214
trigger times: 4
Loss after 963387840 batches: 0.0214
trigger times: 5
Loss after 963518940 batches: 0.0212
trigger times: 6
Loss after 963650040 batches: 0.0210
trigger times: 7
Loss after 963781140 batches: 0.0206
trigger times: 8
Loss after 963912240 batches: 0.0206
trigger times: 9
Loss after 964043340 batches: 0.0205
trigger times: 10
Loss after 964174440 batches: 0.0203
trigger times: 11
Loss after 964305540 batches: 0.0201
trigger times: 12
Loss after 964436640 batches: 0.0200
trigger times: 13
Loss after 964567740 batches: 0.0198
trigger times: 14
Loss after 964698840 batches: 0.0196
trigger times: 15
Loss after 964829940 batches: 0.0196
trigger times: 16
Loss after 964961040 batches: 0.0191
trigger times: 17
Loss after 965092140 batches: 0.0191
trigger times: 18
Loss after 965223240 batches: 0.0193
trigger times: 19
Loss after 965354340 batches: 0.0193
trigger times: 20
Early stopping!
Start to test process.
Loss after 965485440 batches: 0.0192
Time to train on one home:  303.99958205223083
trigger times: 0
Loss after 965614080 batches: 0.0894
trigger times: 0
Loss after 965742720 batches: 0.0266
trigger times: 1
Loss after 965871360 batches: 0.0205
trigger times: 2
Loss after 966000000 batches: 0.0179
trigger times: 0
Loss after 966128640 batches: 0.0168
trigger times: 1
Loss after 966257280 batches: 0.0156
trigger times: 2
Loss after 966385920 batches: 0.0150
trigger times: 3
Loss after 966514560 batches: 0.0146
trigger times: 4
Loss after 966643200 batches: 0.0139
trigger times: 5
Loss after 966771840 batches: 0.0135
trigger times: 6
Loss after 966900480 batches: 0.0136
trigger times: 7
Loss after 967029120 batches: 0.0132
trigger times: 8
Loss after 967157760 batches: 0.0128
trigger times: 9
Loss after 967286400 batches: 0.0125
trigger times: 10
Loss after 967415040 batches: 0.0122
trigger times: 11
Loss after 967543680 batches: 0.0121
trigger times: 12
Loss after 967672320 batches: 0.0123
trigger times: 13
Loss after 967800960 batches: 0.0116
trigger times: 14
Loss after 967929600 batches: 0.0114
trigger times: 15
Loss after 968058240 batches: 0.0118
trigger times: 16
Loss after 968186880 batches: 0.0116
trigger times: 17
Loss after 968315520 batches: 0.0116
trigger times: 18
Loss after 968444160 batches: 0.0112
trigger times: 19
Loss after 968572800 batches: 0.0112
trigger times: 20
Early stopping!
Start to test process.
Loss after 968701440 batches: 0.0112
Time to train on one home:  200.84365677833557
trigger times: 0
Loss after 968832540 batches: 0.2428
trigger times: 1
Loss after 968963640 batches: 0.0533
trigger times: 2
Loss after 969094740 batches: 0.0399
trigger times: 0
Loss after 969225840 batches: 0.0355
trigger times: 1
Loss after 969356940 batches: 0.0320
trigger times: 2
Loss after 969488040 batches: 0.0302
trigger times: 3
Loss after 969619140 batches: 0.0292
trigger times: 4
Loss after 969750240 batches: 0.0282
trigger times: 5
Loss after 969881340 batches: 0.0271
trigger times: 6
Loss after 970012440 batches: 0.0265
trigger times: 7
Loss after 970143540 batches: 0.0257
trigger times: 8
Loss after 970274640 batches: 0.0250
trigger times: 9
Loss after 970405740 batches: 0.0246
trigger times: 10
Loss after 970536840 batches: 0.0239
trigger times: 11
Loss after 970667940 batches: 0.0235
trigger times: 0
Loss after 970799040 batches: 0.0234
trigger times: 1
Loss after 970930140 batches: 0.0229
trigger times: 2
Loss after 971061240 batches: 0.0225
trigger times: 3
Loss after 971192340 batches: 0.0224
trigger times: 4
Loss after 971323440 batches: 0.0221
trigger times: 5
Loss after 971454540 batches: 0.0216
trigger times: 6
Loss after 971585640 batches: 0.0213
trigger times: 7
Loss after 971716740 batches: 0.0215
trigger times: 8
Loss after 971847840 batches: 0.0210
trigger times: 9
Loss after 971978940 batches: 0.0208
trigger times: 10
Loss after 972110040 batches: 0.0208
trigger times: 11
Loss after 972241140 batches: 0.0205
trigger times: 12
Loss after 972372240 batches: 0.0205
trigger times: 13
Loss after 972503340 batches: 0.0202
trigger times: 14
Loss after 972634440 batches: 0.0199
trigger times: 15
Loss after 972765540 batches: 0.0201
trigger times: 16
Loss after 972896640 batches: 0.0199
trigger times: 17
Loss after 973027740 batches: 0.0197
trigger times: 0
Loss after 973158840 batches: 0.0196
trigger times: 1
Loss after 973289940 batches: 0.0195
trigger times: 2
Loss after 973421040 batches: 0.0194
trigger times: 3
Loss after 973552140 batches: 0.0193
trigger times: 4
Loss after 973683240 batches: 0.0191
trigger times: 5
Loss after 973814340 batches: 0.0189
trigger times: 6
Loss after 973945440 batches: 0.0188
trigger times: 7
Loss after 974076540 batches: 0.0187
trigger times: 8
Loss after 974207640 batches: 0.0186
trigger times: 0
Loss after 974338740 batches: 0.0186
trigger times: 1
Loss after 974469840 batches: 0.0185
trigger times: 2
Loss after 974600940 batches: 0.0182
trigger times: 3
Loss after 974732040 batches: 0.0184
trigger times: 4
Loss after 974863140 batches: 0.0182
trigger times: 5
Loss after 974994240 batches: 0.0177
trigger times: 0
Loss after 975125340 batches: 0.0180
trigger times: 1
Loss after 975256440 batches: 0.0178
trigger times: 2
Loss after 975387540 batches: 0.0178
trigger times: 3
Loss after 975518640 batches: 0.0176
trigger times: 4
Loss after 975649740 batches: 0.0177
trigger times: 5
Loss after 975780840 batches: 0.0177
trigger times: 6
Loss after 975911940 batches: 0.0175
trigger times: 0
Loss after 976043040 batches: 0.0175
trigger times: 1
Loss after 976174140 batches: 0.0173
trigger times: 2
Loss after 976305240 batches: 0.0174
trigger times: 3
Loss after 976436340 batches: 0.0171
trigger times: 4
Loss after 976567440 batches: 0.0170
trigger times: 5
Loss after 976698540 batches: 0.0169
trigger times: 6
Loss after 976829640 batches: 0.0169
trigger times: 7
Loss after 976960740 batches: 0.0170
trigger times: 8
Loss after 977091840 batches: 0.0171
trigger times: 0
Loss after 977222940 batches: 0.0171
trigger times: 1
Loss after 977354040 batches: 0.0168
trigger times: 2
Loss after 977485140 batches: 0.0165
trigger times: 3
Loss after 977616240 batches: 0.0163
trigger times: 4
Loss after 977747340 batches: 0.0167
trigger times: 0
Loss after 977878440 batches: 0.0164
trigger times: 0
Loss after 978009540 batches: 0.0166
trigger times: 1
Loss after 978140640 batches: 0.0167
trigger times: 2
Loss after 978271740 batches: 0.0163
trigger times: 3
Loss after 978402840 batches: 0.0164
trigger times: 0
Loss after 978533940 batches: 0.0160
trigger times: 1
Loss after 978665040 batches: 0.0160
trigger times: 2
Loss after 978796140 batches: 0.0161
trigger times: 3
Loss after 978927240 batches: 0.0162
trigger times: 4
Loss after 979058340 batches: 0.0160
trigger times: 5
Loss after 979189440 batches: 0.0162
trigger times: 6
Loss after 979320540 batches: 0.0161
trigger times: 7
Loss after 979451640 batches: 0.0159
trigger times: 8
Loss after 979582740 batches: 0.0158
trigger times: 9
Loss after 979713840 batches: 0.0158
trigger times: 10
Loss after 979844940 batches: 0.0157
trigger times: 11
Loss after 979976040 batches: 0.0159
trigger times: 12
Loss after 980107140 batches: 0.0157
trigger times: 13
Loss after 980238240 batches: 0.0155
trigger times: 14
Loss after 980369340 batches: 0.0157
trigger times: 15
Loss after 980500440 batches: 0.0156
trigger times: 16
Loss after 980631540 batches: 0.0156
trigger times: 17
Loss after 980762640 batches: 0.0155
trigger times: 18
Loss after 980893740 batches: 0.0157
trigger times: 19
Loss after 981024840 batches: 0.0158
trigger times: 20
Early stopping!
Start to test process.
Loss after 981155940 batches: 0.0153
Time to train on one home:  738.8517379760742
trigger times: 0
Loss after 981287040 batches: 0.2113
trigger times: 0
Loss after 981418140 batches: 0.0620
trigger times: 0
Loss after 981549240 batches: 0.0449
trigger times: 0
Loss after 981680340 batches: 0.0370
trigger times: 0
Loss after 981811440 batches: 0.0330
trigger times: 0
Loss after 981942540 batches: 0.0308
trigger times: 0
Loss after 982073640 batches: 0.0303
trigger times: 1
Loss after 982204740 batches: 0.0288
trigger times: 2
Loss after 982335840 batches: 0.0279
trigger times: 0
Loss after 982466940 batches: 0.0273
trigger times: 0
Loss after 982598040 batches: 0.0256
trigger times: 1
Loss after 982729140 batches: 0.0256
trigger times: 0
Loss after 982860240 batches: 0.0261
trigger times: 0
Loss after 982991340 batches: 0.0257
trigger times: 1
Loss after 983122440 batches: 0.0263
trigger times: 2
Loss after 983253540 batches: 0.0242
trigger times: 0
Loss after 983384640 batches: 0.0240
trigger times: 1
Loss after 983515740 batches: 0.0241
trigger times: 0
Loss after 983646840 batches: 0.0232
trigger times: 1
Loss after 983777940 batches: 0.0230
trigger times: 2
Loss after 983909040 batches: 0.0233
trigger times: 3
Loss after 984040140 batches: 0.0224
trigger times: 4
Loss after 984171240 batches: 0.0226
trigger times: 5
Loss after 984302340 batches: 0.0218
trigger times: 6
Loss after 984433440 batches: 0.0220
trigger times: 7
Loss after 984564540 batches: 0.0228
trigger times: 8
Loss after 984695640 batches: 0.0232
trigger times: 0
Loss after 984826740 batches: 0.0222
trigger times: 1
Loss after 984957840 batches: 0.0212
trigger times: 2
Loss after 985088940 batches: 0.0210
trigger times: 3
Loss after 985220040 batches: 0.0205
trigger times: 4
Loss after 985351140 batches: 0.0202
trigger times: 5
Loss after 985482240 batches: 0.0201
trigger times: 6
Loss after 985613340 batches: 0.0192
trigger times: 0
Loss after 985744440 batches: 0.0209
trigger times: 1
Loss after 985875540 batches: 0.0217
trigger times: 2
Loss after 986006640 batches: 0.0207
trigger times: 3
Loss after 986137740 batches: 0.0212
trigger times: 4
Loss after 986268840 batches: 0.0204
trigger times: 5
Loss after 986399940 batches: 0.0196
trigger times: 6
Loss after 986531040 batches: 0.0199
trigger times: 7
Loss after 986662140 batches: 0.0213
trigger times: 8
Loss after 986793240 batches: 0.0205
trigger times: 9
Loss after 986924340 batches: 0.0206
trigger times: 10
Loss after 987055440 batches: 0.0190
trigger times: 11
Loss after 987186540 batches: 0.0188
trigger times: 12
Loss after 987317640 batches: 0.0198
trigger times: 13
Loss after 987448740 batches: 0.0195
trigger times: 14
Loss after 987579840 batches: 0.0191
trigger times: 15
Loss after 987710940 batches: 0.0197
trigger times: 16
Loss after 987842040 batches: 0.0192
trigger times: 17
Loss after 987973140 batches: 0.0196
trigger times: 18
Loss after 988104240 batches: 0.0195
trigger times: 19
Loss after 988235340 batches: 0.0190
trigger times: 20
Early stopping!
Start to test process.
Loss after 988366440 batches: 0.0183
Time to train on one home:  433.6570746898651
trigger times: 0
Loss after 988497540 batches: 0.0589
trigger times: 0
Loss after 988628640 batches: 0.0192
trigger times: 1
Loss after 988759740 batches: 0.0148
trigger times: 2
Loss after 988890840 batches: 0.0133
trigger times: 3
Loss after 989021940 batches: 0.0123
trigger times: 4
Loss after 989153040 batches: 0.0115
trigger times: 0
Loss after 989284140 batches: 0.0109
trigger times: 1
Loss after 989415240 batches: 0.0105
trigger times: 2
Loss after 989546340 batches: 0.0103
trigger times: 3
Loss after 989677440 batches: 0.0101
trigger times: 4
Loss after 989808540 batches: 0.0098
trigger times: 5
Loss after 989939640 batches: 0.0094
trigger times: 6
Loss after 990070740 batches: 0.0093
trigger times: 7
Loss after 990201840 batches: 0.0090
trigger times: 8
Loss after 990332940 batches: 0.0090
trigger times: 9
Loss after 990464040 batches: 0.0089
trigger times: 10
Loss after 990595140 batches: 0.0086
trigger times: 11
Loss after 990726240 batches: 0.0084
trigger times: 12
Loss after 990857340 batches: 0.0082
trigger times: 13
Loss after 990988440 batches: 0.0081
trigger times: 14
Loss after 991119540 batches: 0.0084
trigger times: 15
Loss after 991250640 batches: 0.0081
trigger times: 16
Loss after 991381740 batches: 0.0079
trigger times: 17
Loss after 991512840 batches: 0.0078
trigger times: 18
Loss after 991643940 batches: 0.0078
trigger times: 0
Loss after 991775040 batches: 0.0079
trigger times: 1
Loss after 991906140 batches: 0.0078
trigger times: 2
Loss after 992037240 batches: 0.0078
trigger times: 3
Loss after 992168340 batches: 0.0077
trigger times: 4
Loss after 992299440 batches: 0.0076
trigger times: 5
Loss after 992430540 batches: 0.0077
trigger times: 6
Loss after 992561640 batches: 0.0074
trigger times: 7
Loss after 992692740 batches: 0.0074
trigger times: 8
Loss after 992823840 batches: 0.0075
trigger times: 9
Loss after 992954940 batches: 0.0072
trigger times: 10
Loss after 993086040 batches: 0.0075
trigger times: 11
Loss after 993217140 batches: 0.0074
trigger times: 12
Loss after 993348240 batches: 0.0072
trigger times: 13
Loss after 993479340 batches: 0.0072
trigger times: 14
Loss after 993610440 batches: 0.0071
trigger times: 15
Loss after 993741540 batches: 0.0069
trigger times: 16
Loss after 993872640 batches: 0.0069
trigger times: 17
Loss after 994003740 batches: 0.0070
trigger times: 18
Loss after 994134840 batches: 0.0070
trigger times: 19
Loss after 994265940 batches: 0.0069
trigger times: 20
Early stopping!
Start to test process.
Loss after 994397040 batches: 0.0069
Time to train on one home:  366.56592655181885
trigger times: 0
Loss after 994475640 batches: 0.1853
trigger times: 0
Loss after 994554240 batches: 0.0469
trigger times: 0
Loss after 994632840 batches: 0.0317
trigger times: 0
Loss after 994711440 batches: 0.0275
trigger times: 0
Loss after 994790040 batches: 0.0240
trigger times: 1
Loss after 994868640 batches: 0.0225
trigger times: 0
Loss after 994947240 batches: 0.0216
trigger times: 1
Loss after 995025840 batches: 0.0205
trigger times: 2
Loss after 995104440 batches: 0.0199
trigger times: 3
Loss after 995183040 batches: 0.0191
trigger times: 4
Loss after 995261640 batches: 0.0186
trigger times: 5
Loss after 995340240 batches: 0.0174
trigger times: 6
Loss after 995418840 batches: 0.0174
trigger times: 0
Loss after 995497440 batches: 0.0173
trigger times: 1
Loss after 995576040 batches: 0.0168
trigger times: 0
Loss after 995654640 batches: 0.0166
trigger times: 1
Loss after 995733240 batches: 0.0166
trigger times: 0
Loss after 995811840 batches: 0.0166
trigger times: 1
Loss after 995890440 batches: 0.0159
trigger times: 0
Loss after 995969040 batches: 0.0161
trigger times: 1
Loss after 996047640 batches: 0.0157
trigger times: 2
Loss after 996126240 batches: 0.0150
trigger times: 0
Loss after 996204840 batches: 0.0149
trigger times: 1
Loss after 996283440 batches: 0.0150
trigger times: 2
Loss after 996362040 batches: 0.0150
trigger times: 3
Loss after 996440640 batches: 0.0149
trigger times: 4
Loss after 996519240 batches: 0.0148
trigger times: 5
Loss after 996597840 batches: 0.0141
trigger times: 6
Loss after 996676440 batches: 0.0140
trigger times: 7
Loss after 996755040 batches: 0.0143
trigger times: 8
Loss after 996833640 batches: 0.0138
trigger times: 9
Loss after 996912240 batches: 0.0143
trigger times: 10
Loss after 996990840 batches: 0.0140
trigger times: 11
Loss after 997069440 batches: 0.0138
trigger times: 12
Loss after 997148040 batches: 0.0133
trigger times: 13
Loss after 997226640 batches: 0.0136
trigger times: 14
Loss after 997305240 batches: 0.0135
trigger times: 15
Loss after 997383840 batches: 0.0137
trigger times: 16
Loss after 997462440 batches: 0.0132
trigger times: 17
Loss after 997541040 batches: 0.0127
trigger times: 18
Loss after 997619640 batches: 0.0128
trigger times: 19
Loss after 997698240 batches: 0.0129
trigger times: 20
Early stopping!
Start to test process.
Loss after 997776840 batches: 0.0130
Time to train on one home:  226.40123558044434
trigger times: 0
Loss after 997907940 batches: 0.0732
trigger times: 0
Loss after 998039040 batches: 0.0220
trigger times: 0
Loss after 998170140 batches: 0.0168
trigger times: 0
Loss after 998301240 batches: 0.0150
trigger times: 1
Loss after 998432340 batches: 0.0136
trigger times: 2
Loss after 998563440 batches: 0.0128
trigger times: 3
Loss after 998694540 batches: 0.0121
trigger times: 4
Loss after 998825640 batches: 0.0116
trigger times: 5
Loss after 998956740 batches: 0.0112
trigger times: 6
Loss after 999087840 batches: 0.0109
trigger times: 7
Loss after 999218940 batches: 0.0106
trigger times: 8
Loss after 999350040 batches: 0.0103
trigger times: 9
Loss after 999481140 batches: 0.0103
trigger times: 10
Loss after 999612240 batches: 0.0101
trigger times: 11
Loss after 999743340 batches: 0.0100
trigger times: 12
Loss after 999874440 batches: 0.0099
trigger times: 13
Loss after 1000005540 batches: 0.0096
trigger times: 14
Loss after 1000136640 batches: 0.0095
trigger times: 15
Loss after 1000267740 batches: 0.0093
trigger times: 16
Loss after 1000398840 batches: 0.0090
trigger times: 17
Loss after 1000529940 batches: 0.0091
trigger times: 18
Loss after 1000661040 batches: 0.0090
trigger times: 19
Loss after 1000792140 batches: 0.0089
trigger times: 20
Early stopping!
Start to test process.
Loss after 1000923240 batches: 0.0089
Time to train on one home:  197.45584321022034
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318, 0.016183730855002625, 0.015608215882864637, 0.015855816283001423, 0.015272240943982671, 0.014904168019127074, 0.014211993132470634, 0.014714711150355778, 0.01356970331741986, 0.013157077172457638, 0.013606505732267123, 0.013057062050878846]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165], [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777], [0.5659677750534482, 0.3881937670236807, 0.47736113706344846, 1.0466814163269424, 0.5011883546712889, 24.728223616247963, 1547.1996], [0.5552795496251848, 0.39972320473570555, 0.4869199067938419, 1.0432319579028642, 0.4917435017003341, 24.646728924611136, 1518.0426], [0.556764519876904, 0.3981234608309019, 0.4873119444985629, 1.0354352452336422, 0.4930540032485836, 24.462528793273435, 1522.0884], [0.5499979423152076, 0.40547709897063045, 0.49151309919991676, 1.0193683799804445, 0.4870299426858642, 24.082943342919844, 1503.4916], [0.5525510013103485, 0.4027306139686573, 0.4951615175873931, 1.0298212095954027, 0.48927984833421284, 24.32989518911608, 1510.4373], [0.5425989263587527, 0.41345946401828226, 0.5050210962352305, 1.0162617040097164, 0.48049083244313984, 24.009547009604805, 1483.3052], [0.5446713169415792, 0.41122514224627915, 0.5011693979571417, 1.011856964658698, 0.482321176745565, 23.905483463673608, 1488.9554], [0.5494886537392935, 0.4060339214979838, 0.5010704622538157, 1.0286816103891239, 0.48657379668524176, 24.30297174940827, 1502.0835], [0.5506102244059244, 0.4048432294636072, 0.5077384865654669, 1.0249413038763329, 0.4875492051552179, 24.21460566742873, 1505.0948], [0.5427450504567888, 0.4133636447515222, 0.510553041876086, 1.0128563948605698, 0.48056932706785044, 23.929095360412393, 1483.5474]]
Round_18_results:  [0.5427450504567888, 0.4133636447515222, 0.510553041876086, 1.0128563948605698, 0.48056932706785044, 23.929095360412393, 1483.5474]
trigger times: 0
Loss after 1001054340 batches: 0.0947
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 8188 < 8189; dropping {'Training_Loss': 0.09468770877651449, 'Validation_Loss': 0.1843420300218794, 'Training_R2': 0.9046134965934955, 'Validation_R2': 0.8288196927055878, 'Training_F1': 0.8440903107339007, 'Validation_F1': 0.7465800780285446, 'Training_NEP': 0.31144175435008914, 'Validation_NEP': 0.48365686120800355, 'Training_NDE': 0.0716083412203222, 'Validation_NDE': 0.13631897311179855, 'Training_MAE': 10.31433324847305, 'Validation_MAE': 13.263877703724797, 'Training_MSE': 315.06543, 'Validation_MSE': 503.4222}.
trigger times: 0
Loss after 1001185440 batches: 0.0226
trigger times: 0
Loss after 1001316540 batches: 0.0178
trigger times: 0
Loss after 1001447640 batches: 0.0158
trigger times: 0
Loss after 1001578740 batches: 0.0145
trigger times: 0
Loss after 1001709840 batches: 0.0136
trigger times: 1
Loss after 1001840940 batches: 0.0129
trigger times: 2
Loss after 1001972040 batches: 0.0126
trigger times: 3
Loss after 1002103140 batches: 0.0118
trigger times: 4
Loss after 1002234240 batches: 0.0117
trigger times: 5
Loss after 1002365340 batches: 0.0113
trigger times: 6
Loss after 1002496440 batches: 0.0108
trigger times: 7
Loss after 1002627540 batches: 0.0108
trigger times: 8
Loss after 1002758640 batches: 0.0104
trigger times: 9
Loss after 1002889740 batches: 0.0103
trigger times: 10
Loss after 1003020840 batches: 0.0101
trigger times: 0
Loss after 1003151940 batches: 0.0101
trigger times: 1
Loss after 1003283040 batches: 0.0098
trigger times: 2
Loss after 1003414140 batches: 0.0097
trigger times: 3
Loss after 1003545240 batches: 0.0098
trigger times: 4
Loss after 1003676340 batches: 0.0095
trigger times: 5
Loss after 1003807440 batches: 0.0093
trigger times: 6
Loss after 1003938540 batches: 0.0093
trigger times: 7
Loss after 1004069640 batches: 0.0093
trigger times: 8
Loss after 1004200740 batches: 0.0090
trigger times: 9
Loss after 1004331840 batches: 0.0090
trigger times: 10
Loss after 1004462940 batches: 0.0090
trigger times: 11
Loss after 1004594040 batches: 0.0087
trigger times: 12
Loss after 1004725140 batches: 0.0088
trigger times: 13
Loss after 1004856240 batches: 0.0089
trigger times: 14
Loss after 1004987340 batches: 0.0089
trigger times: 15
Loss after 1005118440 batches: 0.0086
trigger times: 16
Loss after 1005249540 batches: 0.0087
trigger times: 17
Loss after 1005380640 batches: 0.0086
trigger times: 0
Loss after 1005511740 batches: 0.0084
trigger times: 1
Loss after 1005642840 batches: 0.0085
trigger times: 2
Loss after 1005773940 batches: 0.0082
trigger times: 3
Loss after 1005905040 batches: 0.0083
trigger times: 4
Loss after 1006036140 batches: 0.0084
trigger times: 5
Loss after 1006167240 batches: 0.0083
trigger times: 6
Loss after 1006298340 batches: 0.0081
trigger times: 7
Loss after 1006429440 batches: 0.0083
trigger times: 8
Loss after 1006560540 batches: 0.0080
trigger times: 9
Loss after 1006691640 batches: 0.0079
trigger times: 10
Loss after 1006822740 batches: 0.0081
trigger times: 11
Loss after 1006953840 batches: 0.0081
trigger times: 12
Loss after 1007084940 batches: 0.0080
trigger times: 13
Loss after 1007216040 batches: 0.0077
trigger times: 14
Loss after 1007347140 batches: 0.0077
trigger times: 15
Loss after 1007478240 batches: 0.0076
trigger times: 16
Loss after 1007609340 batches: 0.0077
trigger times: 17
Loss after 1007740440 batches: 0.0076
trigger times: 18
Loss after 1007871540 batches: 0.0077
trigger times: 19
Loss after 1008002640 batches: 0.0075
trigger times: 20
Early stopping!
Start to test process.
Loss after 1008133740 batches: 0.0074
Time to train on one home:  433.6544849872589
trigger times: 0
Loss after 1008236340 batches: 0.1650
trigger times: 0
Loss after 1008338940 batches: 0.0561
trigger times: 1
Loss after 1008441540 batches: 0.0376
trigger times: 2
Loss after 1008544140 batches: 0.0312
trigger times: 3
Loss after 1008646740 batches: 0.0273
trigger times: 4
Loss after 1008749340 batches: 0.0250
trigger times: 5
Loss after 1008851940 batches: 0.0245
trigger times: 0
Loss after 1008954540 batches: 0.0234
trigger times: 1
Loss after 1009057140 batches: 0.0234
trigger times: 2
Loss after 1009159740 batches: 0.0215
trigger times: 3
Loss after 1009262340 batches: 0.0211
trigger times: 4
Loss after 1009364940 batches: 0.0210
trigger times: 0
Loss after 1009467540 batches: 0.0200
trigger times: 1
Loss after 1009570140 batches: 0.0201
trigger times: 2
Loss after 1009672740 batches: 0.0211
trigger times: 3
Loss after 1009775340 batches: 0.0199
trigger times: 4
Loss after 1009877940 batches: 0.0193
trigger times: 0
Loss after 1009980540 batches: 0.0187
trigger times: 1
Loss after 1010083140 batches: 0.0181
trigger times: 2
Loss after 1010185740 batches: 0.0184
trigger times: 0
Loss after 1010288340 batches: 0.0182
trigger times: 1
Loss after 1010390940 batches: 0.0178
trigger times: 2
Loss after 1010493540 batches: 0.0168
trigger times: 0
Loss after 1010596140 batches: 0.0164
trigger times: 1
Loss after 1010698740 batches: 0.0166
trigger times: 2
Loss after 1010801340 batches: 0.0177
trigger times: 3
Loss after 1010903940 batches: 0.0170
trigger times: 4
Loss after 1011006540 batches: 0.0168
trigger times: 5
Loss after 1011109140 batches: 0.0165
trigger times: 0
Loss after 1011211740 batches: 0.0241
trigger times: 1
Loss after 1011314340 batches: 0.0252
trigger times: 2
Loss after 1011416940 batches: 0.0176
trigger times: 3
Loss after 1011519540 batches: 0.0160
trigger times: 4
Loss after 1011622140 batches: 0.0159
trigger times: 5
Loss after 1011724740 batches: 0.0153
trigger times: 6
Loss after 1011827340 batches: 0.0153
trigger times: 7
Loss after 1011929940 batches: 0.0166
trigger times: 8
Loss after 1012032540 batches: 0.0152
trigger times: 9
Loss after 1012135140 batches: 0.0155
trigger times: 10
Loss after 1012237740 batches: 0.0158
trigger times: 11
Loss after 1012340340 batches: 0.0156
trigger times: 12
Loss after 1012442940 batches: 0.0150
trigger times: 13
Loss after 1012545540 batches: 0.0148
trigger times: 14
Loss after 1012648140 batches: 0.0147
trigger times: 15
Loss after 1012750740 batches: 0.0145
trigger times: 16
Loss after 1012853340 batches: 0.0154
trigger times: 17
Loss after 1012955940 batches: 0.0149
trigger times: 18
Loss after 1013058540 batches: 0.0145
trigger times: 19
Loss after 1013161140 batches: 0.0150
trigger times: 20
Early stopping!
Start to test process.
Loss after 1013263740 batches: 0.0192
Time to train on one home:  320.480117559433
trigger times: 0
Loss after 1013394840 batches: 0.1331
trigger times: 0
Loss after 1013525940 batches: 0.0412
trigger times: 0
Loss after 1013657040 batches: 0.0308
trigger times: 1
Loss after 1013788140 batches: 0.0274
trigger times: 2
Loss after 1013919240 batches: 0.0251
trigger times: 0
Loss after 1014050340 batches: 0.0236
trigger times: 0
Loss after 1014181440 batches: 0.0226
trigger times: 1
Loss after 1014312540 batches: 0.0215
trigger times: 2
Loss after 1014443640 batches: 0.0207
trigger times: 3
Loss after 1014574740 batches: 0.0201
trigger times: 4
Loss after 1014705840 batches: 0.0197
trigger times: 5
Loss after 1014836940 batches: 0.0192
trigger times: 6
Loss after 1014968040 batches: 0.0186
trigger times: 7
Loss after 1015099140 batches: 0.0184
trigger times: 8
Loss after 1015230240 batches: 0.0181
trigger times: 9
Loss after 1015361340 batches: 0.0176
trigger times: 10
Loss after 1015492440 batches: 0.0175
trigger times: 11
Loss after 1015623540 batches: 0.0170
trigger times: 12
Loss after 1015754640 batches: 0.0168
trigger times: 13
Loss after 1015885740 batches: 0.0169
trigger times: 14
Loss after 1016016840 batches: 0.0164
trigger times: 15
Loss after 1016147940 batches: 0.0163
trigger times: 16
Loss after 1016279040 batches: 0.0160
trigger times: 17
Loss after 1016410140 batches: 0.0160
trigger times: 18
Loss after 1016541240 batches: 0.0158
trigger times: 19
Loss after 1016672340 batches: 0.0156
trigger times: 20
Early stopping!
Start to test process.
Loss after 1016803440 batches: 0.0155
Time to train on one home:  219.2358570098877
trigger times: 0
Loss after 1016934540 batches: 0.1944
trigger times: 0
Loss after 1017065640 batches: 0.0515
trigger times: 1
Loss after 1017196740 batches: 0.0392
trigger times: 2
Loss after 1017327840 batches: 0.0345
trigger times: 3
Loss after 1017458940 batches: 0.0319
trigger times: 0
Loss after 1017590040 batches: 0.0297
trigger times: 1
Loss after 1017721140 batches: 0.0286
trigger times: 2
Loss after 1017852240 batches: 0.0276
trigger times: 0
Loss after 1017983340 batches: 0.0265
trigger times: 1
Loss after 1018114440 batches: 0.0258
trigger times: 2
Loss after 1018245540 batches: 0.0253
trigger times: 3
Loss after 1018376640 batches: 0.0246
trigger times: 4
Loss after 1018507740 batches: 0.0241
trigger times: 5
Loss after 1018638840 batches: 0.0237
trigger times: 6
Loss after 1018769940 batches: 0.0233
trigger times: 7
Loss after 1018901040 batches: 0.0229
trigger times: 8
Loss after 1019032140 batches: 0.0223
trigger times: 9
Loss after 1019163240 batches: 0.0223
trigger times: 10
Loss after 1019294340 batches: 0.0219
trigger times: 11
Loss after 1019425440 batches: 0.0217
trigger times: 12
Loss after 1019556540 batches: 0.0215
trigger times: 13
Loss after 1019687640 batches: 0.0212
trigger times: 0
Loss after 1019818740 batches: 0.0210
trigger times: 1
Loss after 1019949840 batches: 0.0208
trigger times: 2
Loss after 1020080940 batches: 0.0207
trigger times: 0
Loss after 1020212040 batches: 0.0206
trigger times: 0
Loss after 1020343140 batches: 0.0204
trigger times: 1
Loss after 1020474240 batches: 0.0206
trigger times: 2
Loss after 1020605340 batches: 0.0198
trigger times: 3
Loss after 1020736440 batches: 0.0202
trigger times: 0
Loss after 1020867540 batches: 0.0198
trigger times: 1
Loss after 1020998640 batches: 0.0196
trigger times: 2
Loss after 1021129740 batches: 0.0196
trigger times: 0
Loss after 1021260840 batches: 0.0195
trigger times: 1
Loss after 1021391940 batches: 0.0194
trigger times: 2
Loss after 1021523040 batches: 0.0190
trigger times: 3
Loss after 1021654140 batches: 0.0190
trigger times: 4
Loss after 1021785240 batches: 0.0188
trigger times: 5
Loss after 1021916340 batches: 0.0187
trigger times: 6
Loss after 1022047440 batches: 0.0187
trigger times: 7
Loss after 1022178540 batches: 0.0185
trigger times: 8
Loss after 1022309640 batches: 0.0184
trigger times: 9
Loss after 1022440740 batches: 0.0183
trigger times: 10
Loss after 1022571840 batches: 0.0184
trigger times: 11
Loss after 1022702940 batches: 0.0187
trigger times: 12
Loss after 1022834040 batches: 0.0183
trigger times: 0
Loss after 1022965140 batches: 0.0179
trigger times: 1
Loss after 1023096240 batches: 0.0178
trigger times: 2
Loss after 1023227340 batches: 0.0179
trigger times: 3
Loss after 1023358440 batches: 0.0179
trigger times: 4
Loss after 1023489540 batches: 0.0178
trigger times: 5
Loss after 1023620640 batches: 0.0177
trigger times: 6
Loss after 1023751740 batches: 0.0174
trigger times: 7
Loss after 1023882840 batches: 0.0175
trigger times: 8
Loss after 1024013940 batches: 0.0177
trigger times: 9
Loss after 1024145040 batches: 0.0173
trigger times: 10
Loss after 1024276140 batches: 0.0171
trigger times: 11
Loss after 1024407240 batches: 0.0173
trigger times: 12
Loss after 1024538340 batches: 0.0173
trigger times: 13
Loss after 1024669440 batches: 0.0173
trigger times: 0
Loss after 1024800540 batches: 0.0168
trigger times: 1
Loss after 1024931640 batches: 0.0167
trigger times: 2
Loss after 1025062740 batches: 0.0169
trigger times: 3
Loss after 1025193840 batches: 0.0168
trigger times: 4
Loss after 1025324940 batches: 0.0168
trigger times: 5
Loss after 1025456040 batches: 0.0166
trigger times: 6
Loss after 1025587140 batches: 0.0167
trigger times: 7
Loss after 1025718240 batches: 0.0169
trigger times: 8
Loss after 1025849340 batches: 0.0166
trigger times: 9
Loss after 1025980440 batches: 0.0164
trigger times: 10
Loss after 1026111540 batches: 0.0163
trigger times: 11
Loss after 1026242640 batches: 0.0164
trigger times: 12
Loss after 1026373740 batches: 0.0162
trigger times: 13
Loss after 1026504840 batches: 0.0161
trigger times: 14
Loss after 1026635940 batches: 0.0161
trigger times: 15
Loss after 1026767040 batches: 0.0161
trigger times: 16
Loss after 1026898140 batches: 0.0162
trigger times: 17
Loss after 1027029240 batches: 0.0161
trigger times: 18
Loss after 1027160340 batches: 0.0159
trigger times: 19
Loss after 1027291440 batches: 0.0156
trigger times: 20
Early stopping!
Start to test process.
Loss after 1027422540 batches: 0.0157
Time to train on one home:  625.9402697086334
trigger times: 0
Loss after 1027551180 batches: 0.0898
trigger times: 0
Loss after 1027679820 batches: 0.0262
trigger times: 1
Loss after 1027808460 batches: 0.0198
trigger times: 0
Loss after 1027937100 batches: 0.0174
trigger times: 1
Loss after 1028065740 batches: 0.0162
trigger times: 2
Loss after 1028194380 batches: 0.0157
trigger times: 3
Loss after 1028323020 batches: 0.0149
trigger times: 4
Loss after 1028451660 batches: 0.0144
trigger times: 5
Loss after 1028580300 batches: 0.0144
trigger times: 6
Loss after 1028708940 batches: 0.0139
trigger times: 7
Loss after 1028837580 batches: 0.0136
trigger times: 8
Loss after 1028966220 batches: 0.0133
trigger times: 9
Loss after 1029094860 batches: 0.0129
trigger times: 10
Loss after 1029223500 batches: 0.0127
trigger times: 11
Loss after 1029352140 batches: 0.0126
trigger times: 12
Loss after 1029480780 batches: 0.0123
trigger times: 13
Loss after 1029609420 batches: 0.0119
trigger times: 14
Loss after 1029738060 batches: 0.0119
trigger times: 15
Loss after 1029866700 batches: 0.0116
trigger times: 16
Loss after 1029995340 batches: 0.0115
trigger times: 17
Loss after 1030123980 batches: 0.0115
trigger times: 18
Loss after 1030252620 batches: 0.0114
trigger times: 19
Loss after 1030381260 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 1030509900 batches: 0.0111
Time to train on one home:  196.00925254821777
trigger times: 0
Loss after 1030641000 batches: 0.2189
trigger times: 0
Loss after 1030772100 batches: 0.0504
trigger times: 0
Loss after 1030903200 batches: 0.0377
trigger times: 1
Loss after 1031034300 batches: 0.0332
trigger times: 2
Loss after 1031165400 batches: 0.0303
trigger times: 0
Loss after 1031296500 batches: 0.0291
trigger times: 1
Loss after 1031427600 batches: 0.0276
trigger times: 0
Loss after 1031558700 batches: 0.0264
trigger times: 1
Loss after 1031689800 batches: 0.0257
trigger times: 2
Loss after 1031820900 batches: 0.0252
trigger times: 3
Loss after 1031952000 batches: 0.0243
trigger times: 4
Loss after 1032083100 batches: 0.0238
trigger times: 5
Loss after 1032214200 batches: 0.0233
trigger times: 6
Loss after 1032345300 batches: 0.0229
trigger times: 7
Loss after 1032476400 batches: 0.0223
trigger times: 8
Loss after 1032607500 batches: 0.0222
trigger times: 9
Loss after 1032738600 batches: 0.0218
trigger times: 10
Loss after 1032869700 batches: 0.0215
trigger times: 11
Loss after 1033000800 batches: 0.0213
trigger times: 12
Loss after 1033131900 batches: 0.0211
trigger times: 13
Loss after 1033263000 batches: 0.0209
trigger times: 14
Loss after 1033394100 batches: 0.0206
trigger times: 15
Loss after 1033525200 batches: 0.0204
trigger times: 16
Loss after 1033656300 batches: 0.0200
trigger times: 0
Loss after 1033787400 batches: 0.0199
trigger times: 1
Loss after 1033918500 batches: 0.0198
trigger times: 2
Loss after 1034049600 batches: 0.0194
trigger times: 3
Loss after 1034180700 batches: 0.0195
trigger times: 4
Loss after 1034311800 batches: 0.0192
trigger times: 5
Loss after 1034442900 batches: 0.0192
trigger times: 0
Loss after 1034574000 batches: 0.0190
trigger times: 0
Loss after 1034705100 batches: 0.0189
trigger times: 1
Loss after 1034836200 batches: 0.0189
trigger times: 2
Loss after 1034967300 batches: 0.0187
trigger times: 3
Loss after 1035098400 batches: 0.0185
trigger times: 4
Loss after 1035229500 batches: 0.0184
trigger times: 5
Loss after 1035360600 batches: 0.0185
trigger times: 6
Loss after 1035491700 batches: 0.0182
trigger times: 7
Loss after 1035622800 batches: 0.0180
trigger times: 8
Loss after 1035753900 batches: 0.0180
trigger times: 9
Loss after 1035885000 batches: 0.0180
trigger times: 10
Loss after 1036016100 batches: 0.0180
trigger times: 11
Loss after 1036147200 batches: 0.0178
trigger times: 12
Loss after 1036278300 batches: 0.0178
trigger times: 13
Loss after 1036409400 batches: 0.0176
trigger times: 14
Loss after 1036540500 batches: 0.0173
trigger times: 15
Loss after 1036671600 batches: 0.0173
trigger times: 16
Loss after 1036802700 batches: 0.0172
trigger times: 17
Loss after 1036933800 batches: 0.0173
trigger times: 18
Loss after 1037064900 batches: 0.0174
trigger times: 19
Loss after 1037196000 batches: 0.0168
trigger times: 0
Loss after 1037327100 batches: 0.0169
trigger times: 1
Loss after 1037458200 batches: 0.0169
trigger times: 0
Loss after 1037589300 batches: 0.0170
trigger times: 1
Loss after 1037720400 batches: 0.0169
trigger times: 2
Loss after 1037851500 batches: 0.0167
trigger times: 3
Loss after 1037982600 batches: 0.0165
trigger times: 4
Loss after 1038113700 batches: 0.0166
trigger times: 5
Loss after 1038244800 batches: 0.0163
trigger times: 6
Loss after 1038375900 batches: 0.0163
trigger times: 7
Loss after 1038507000 batches: 0.0165
trigger times: 8
Loss after 1038638100 batches: 0.0164
trigger times: 9
Loss after 1038769200 batches: 0.0164
trigger times: 10
Loss after 1038900300 batches: 0.0161
trigger times: 11
Loss after 1039031400 batches: 0.0161
trigger times: 12
Loss after 1039162500 batches: 0.0162
trigger times: 0
Loss after 1039293600 batches: 0.0161
trigger times: 1
Loss after 1039424700 batches: 0.0160
trigger times: 2
Loss after 1039555800 batches: 0.0159
trigger times: 3
Loss after 1039686900 batches: 0.0158
trigger times: 4
Loss after 1039818000 batches: 0.0161
trigger times: 5
Loss after 1039949100 batches: 0.0156
trigger times: 6
Loss after 1040080200 batches: 0.0157
trigger times: 7
Loss after 1040211300 batches: 0.0156
trigger times: 8
Loss after 1040342400 batches: 0.0156
trigger times: 9
Loss after 1040473500 batches: 0.0156
trigger times: 10
Loss after 1040604600 batches: 0.0156
trigger times: 11
Loss after 1040735700 batches: 0.0156
trigger times: 12
Loss after 1040866800 batches: 0.0155
trigger times: 13
Loss after 1040997900 batches: 0.0154
trigger times: 14
Loss after 1041129000 batches: 0.0152
trigger times: 15
Loss after 1041260100 batches: 0.0154
trigger times: 16
Loss after 1041391200 batches: 0.0155
trigger times: 17
Loss after 1041522300 batches: 0.0154
trigger times: 18
Loss after 1041653400 batches: 0.0150
trigger times: 19
Loss after 1041784500 batches: 0.0154
trigger times: 20
Early stopping!
Start to test process.
Loss after 1041915600 batches: 0.0153
Time to train on one home:  679.422687292099
trigger times: 0
Loss after 1042046700 batches: 0.1914
trigger times: 0
Loss after 1042177800 batches: 0.0657
trigger times: 0
Loss after 1042308900 batches: 0.0456
trigger times: 1
Loss after 1042440000 batches: 0.0390
trigger times: 0
Loss after 1042571100 batches: 0.0332
trigger times: 1
Loss after 1042702200 batches: 0.0310
trigger times: 2
Loss after 1042833300 batches: 0.0296
trigger times: 3
Loss after 1042964400 batches: 0.0282
trigger times: 0
Loss after 1043095500 batches: 0.0275
trigger times: 1
Loss after 1043226600 batches: 0.0264
trigger times: 0
Loss after 1043357700 batches: 0.0260
trigger times: 1
Loss after 1043488800 batches: 0.0261
trigger times: 2
Loss after 1043619900 batches: 0.0266
trigger times: 3
Loss after 1043751000 batches: 0.0258
trigger times: 0
Loss after 1043882100 batches: 0.0245
trigger times: 1
Loss after 1044013200 batches: 0.0240
trigger times: 2
Loss after 1044144300 batches: 0.0244
trigger times: 3
Loss after 1044275400 batches: 0.0239
trigger times: 4
Loss after 1044406500 batches: 0.0231
trigger times: 5
Loss after 1044537600 batches: 0.0229
trigger times: 6
Loss after 1044668700 batches: 0.0232
trigger times: 7
Loss after 1044799800 batches: 0.0225
trigger times: 8
Loss after 1044930900 batches: 0.0219
trigger times: 9
Loss after 1045062000 batches: 0.0227
trigger times: 0
Loss after 1045193100 batches: 0.0224
trigger times: 1
Loss after 1045324200 batches: 0.0214
trigger times: 0
Loss after 1045455300 batches: 0.0217
trigger times: 1
Loss after 1045586400 batches: 0.0214
trigger times: 2
Loss after 1045717500 batches: 0.0215
trigger times: 3
Loss after 1045848600 batches: 0.0211
trigger times: 4
Loss after 1045979700 batches: 0.0215
trigger times: 5
Loss after 1046110800 batches: 0.0209
trigger times: 6
Loss after 1046241900 batches: 0.0204
trigger times: 7
Loss after 1046373000 batches: 0.0201
trigger times: 8
Loss after 1046504100 batches: 0.0199
trigger times: 9
Loss after 1046635200 batches: 0.0198
trigger times: 10
Loss after 1046766300 batches: 0.0196
trigger times: 11
Loss after 1046897400 batches: 0.0196
trigger times: 0
Loss after 1047028500 batches: 0.0197
trigger times: 1
Loss after 1047159600 batches: 0.0195
trigger times: 2
Loss after 1047290700 batches: 0.0189
trigger times: 3
Loss after 1047421800 batches: 0.0198
trigger times: 4
Loss after 1047552900 batches: 0.0192
trigger times: 5
Loss after 1047684000 batches: 0.0194
trigger times: 6
Loss after 1047815100 batches: 0.0200
trigger times: 7
Loss after 1047946200 batches: 0.0194
trigger times: 0
Loss after 1048077300 batches: 0.0187
trigger times: 1
Loss after 1048208400 batches: 0.0189
trigger times: 2
Loss after 1048339500 batches: 0.0190
trigger times: 0
Loss after 1048470600 batches: 0.0188
trigger times: 1
Loss after 1048601700 batches: 0.0183
trigger times: 2
Loss after 1048732800 batches: 0.0189
trigger times: 3
Loss after 1048863900 batches: 0.0195
trigger times: 4
Loss after 1048995000 batches: 0.0186
trigger times: 5
Loss after 1049126100 batches: 0.0184
trigger times: 6
Loss after 1049257200 batches: 0.0192
trigger times: 7
Loss after 1049388300 batches: 0.0192
trigger times: 8
Loss after 1049519400 batches: 0.0184
trigger times: 9
Loss after 1049650500 batches: 0.0186
trigger times: 10
Loss after 1049781600 batches: 0.0176
trigger times: 11
Loss after 1049912700 batches: 0.0177
trigger times: 12
Loss after 1050043800 batches: 0.0175
trigger times: 13
Loss after 1050174900 batches: 0.0173
trigger times: 14
Loss after 1050306000 batches: 0.0176
trigger times: 15
Loss after 1050437100 batches: 0.0188
trigger times: 16
Loss after 1050568200 batches: 0.0181
trigger times: 17
Loss after 1050699300 batches: 0.0181
trigger times: 18
Loss after 1050830400 batches: 0.0199
trigger times: 19
Loss after 1050961500 batches: 0.0174
trigger times: 20
Early stopping!
Start to test process.
Loss after 1051092600 batches: 0.0171
Time to train on one home:  548.9797656536102
trigger times: 0
Loss after 1051223700 batches: 0.0693
trigger times: 0
Loss after 1051354800 batches: 0.0214
trigger times: 0
Loss after 1051485900 batches: 0.0160
trigger times: 1
Loss after 1051617000 batches: 0.0136
trigger times: 0
Loss after 1051748100 batches: 0.0125
trigger times: 1
Loss after 1051879200 batches: 0.0119
trigger times: 2
Loss after 1052010300 batches: 0.0114
trigger times: 0
Loss after 1052141400 batches: 0.0107
trigger times: 0
Loss after 1052272500 batches: 0.0102
trigger times: 1
Loss after 1052403600 batches: 0.0099
trigger times: 2
Loss after 1052534700 batches: 0.0099
trigger times: 3
Loss after 1052665800 batches: 0.0097
trigger times: 4
Loss after 1052796900 batches: 0.0096
trigger times: 5
Loss after 1052928000 batches: 0.0093
trigger times: 6
Loss after 1053059100 batches: 0.0089
trigger times: 0
Loss after 1053190200 batches: 0.0089
trigger times: 1
Loss after 1053321300 batches: 0.0089
trigger times: 2
Loss after 1053452400 batches: 0.0087
trigger times: 3
Loss after 1053583500 batches: 0.0084
trigger times: 4
Loss after 1053714600 batches: 0.0083
trigger times: 5
Loss after 1053845700 batches: 0.0083
trigger times: 6
Loss after 1053976800 batches: 0.0082
trigger times: 7
Loss after 1054107900 batches: 0.0081
trigger times: 8
Loss after 1054239000 batches: 0.0079
trigger times: 9
Loss after 1054370100 batches: 0.0079
trigger times: 10
Loss after 1054501200 batches: 0.0080
trigger times: 11
Loss after 1054632300 batches: 0.0079
trigger times: 12
Loss after 1054763400 batches: 0.0078
trigger times: 13
Loss after 1054894500 batches: 0.0078
trigger times: 14
Loss after 1055025600 batches: 0.0077
trigger times: 15
Loss after 1055156700 batches: 0.0074
trigger times: 16
Loss after 1055287800 batches: 0.0075
trigger times: 17
Loss after 1055418900 batches: 0.0075
trigger times: 18
Loss after 1055550000 batches: 0.0074
trigger times: 19
Loss after 1055681100 batches: 0.0074
trigger times: 20
Early stopping!
Start to test process.
Loss after 1055812200 batches: 0.0072
Time to train on one home:  284.5258939266205
trigger times: 0
Loss after 1055890800 batches: 0.1873
trigger times: 0
Loss after 1055969400 batches: 0.0467
trigger times: 0
Loss after 1056048000 batches: 0.0316
trigger times: 0
Loss after 1056126600 batches: 0.0259
trigger times: 0
Loss after 1056205200 batches: 0.0240
trigger times: 0
Loss after 1056283800 batches: 0.0223
trigger times: 1
Loss after 1056362400 batches: 0.0211
trigger times: 0
Loss after 1056441000 batches: 0.0201
trigger times: 1
Loss after 1056519600 batches: 0.0191
trigger times: 2
Loss after 1056598200 batches: 0.0189
trigger times: 3
Loss after 1056676800 batches: 0.0186
trigger times: 0
Loss after 1056755400 batches: 0.0180
trigger times: 0
Loss after 1056834000 batches: 0.0180
trigger times: 1
Loss after 1056912600 batches: 0.0172
trigger times: 2
Loss after 1056991200 batches: 0.0165
trigger times: 3
Loss after 1057069800 batches: 0.0169
trigger times: 0
Loss after 1057148400 batches: 0.0166
trigger times: 1
Loss after 1057227000 batches: 0.0160
trigger times: 2
Loss after 1057305600 batches: 0.0157
trigger times: 3
Loss after 1057384200 batches: 0.0160
trigger times: 4
Loss after 1057462800 batches: 0.0155
trigger times: 5
Loss after 1057541400 batches: 0.0152
trigger times: 6
Loss after 1057620000 batches: 0.0151
trigger times: 0
Loss after 1057698600 batches: 0.0152
trigger times: 1
Loss after 1057777200 batches: 0.0149
trigger times: 2
Loss after 1057855800 batches: 0.0150
trigger times: 3
Loss after 1057934400 batches: 0.0144
trigger times: 4
Loss after 1058013000 batches: 0.0142
trigger times: 5
Loss after 1058091600 batches: 0.0142
trigger times: 6
Loss after 1058170200 batches: 0.0139
trigger times: 7
Loss after 1058248800 batches: 0.0138
trigger times: 8
Loss after 1058327400 batches: 0.0139
trigger times: 9
Loss after 1058406000 batches: 0.0138
trigger times: 10
Loss after 1058484600 batches: 0.0136
trigger times: 11
Loss after 1058563200 batches: 0.0136
trigger times: 12
Loss after 1058641800 batches: 0.0137
trigger times: 13
Loss after 1058720400 batches: 0.0135
trigger times: 14
Loss after 1058799000 batches: 0.0132
trigger times: 0
Loss after 1058877600 batches: 0.0131
trigger times: 1
Loss after 1058956200 batches: 0.0137
trigger times: 2
Loss after 1059034800 batches: 0.0130
trigger times: 3
Loss after 1059113400 batches: 0.0132
trigger times: 4
Loss after 1059192000 batches: 0.0130
trigger times: 5
Loss after 1059270600 batches: 0.0126
trigger times: 6
Loss after 1059349200 batches: 0.0128
trigger times: 7
Loss after 1059427800 batches: 0.0131
trigger times: 8
Loss after 1059506400 batches: 0.0127
trigger times: 9
Loss after 1059585000 batches: 0.0128
trigger times: 10
Loss after 1059663600 batches: 0.0122
trigger times: 11
Loss after 1059742200 batches: 0.0123
trigger times: 12
Loss after 1059820800 batches: 0.0119
trigger times: 13
Loss after 1059899400 batches: 0.0120
trigger times: 14
Loss after 1059978000 batches: 0.0125
trigger times: 15
Loss after 1060056600 batches: 0.0120
trigger times: 16
Loss after 1060135200 batches: 0.0117
trigger times: 17
Loss after 1060213800 batches: 0.0119
trigger times: 18
Loss after 1060292400 batches: 0.0120
trigger times: 19
Loss after 1060371000 batches: 0.0119
trigger times: 20
Early stopping!
Start to test process.
Loss after 1060449600 batches: 0.0118
Time to train on one home:  306.09649753570557
trigger times: 0
Loss after 1060580700 batches: 0.0696
trigger times: 0
Loss after 1060711800 batches: 0.0205
trigger times: 1
Loss after 1060842900 batches: 0.0165
trigger times: 0
Loss after 1060974000 batches: 0.0143
trigger times: 1
Loss after 1061105100 batches: 0.0131
trigger times: 0
Loss after 1061236200 batches: 0.0127
trigger times: 1
Loss after 1061367300 batches: 0.0122
trigger times: 0
Loss after 1061498400 batches: 0.0116
trigger times: 0
Loss after 1061629500 batches: 0.0112
trigger times: 0
Loss after 1061760600 batches: 0.0111
trigger times: 1
Loss after 1061891700 batches: 0.0105
trigger times: 2
Loss after 1062022800 batches: 0.0102
trigger times: 3
Loss after 1062153900 batches: 0.0102
trigger times: 0
Loss after 1062285000 batches: 0.0102
trigger times: 1
Loss after 1062416100 batches: 0.0096
trigger times: 2
Loss after 1062547200 batches: 0.0097
trigger times: 3
Loss after 1062678300 batches: 0.0094
trigger times: 4
Loss after 1062809400 batches: 0.0094
trigger times: 5
Loss after 1062940500 batches: 0.0094
trigger times: 6
Loss after 1063071600 batches: 0.0091
trigger times: 7
Loss after 1063202700 batches: 0.0089
trigger times: 0
Loss after 1063333800 batches: 0.0090
trigger times: 0
Loss after 1063464900 batches: 0.0089
trigger times: 1
Loss after 1063596000 batches: 0.0087
trigger times: 2
Loss after 1063727100 batches: 0.0087
trigger times: 3
Loss after 1063858200 batches: 0.0085
trigger times: 4
Loss after 1063989300 batches: 0.0086
trigger times: 5
Loss after 1064120400 batches: 0.0084
trigger times: 6
Loss after 1064251500 batches: 0.0083
trigger times: 7
Loss after 1064382600 batches: 0.0082
trigger times: 8
Loss after 1064513700 batches: 0.0083
trigger times: 9
Loss after 1064644800 batches: 0.0082
trigger times: 10
Loss after 1064775900 batches: 0.0082
trigger times: 11
Loss after 1064907000 batches: 0.0082
trigger times: 12
Loss after 1065038100 batches: 0.0082
trigger times: 13
Loss after 1065169200 batches: 0.0080
trigger times: 14
Loss after 1065300300 batches: 0.0081
trigger times: 15
Loss after 1065431400 batches: 0.0079
trigger times: 16
Loss after 1065562500 batches: 0.0078
trigger times: 17
Loss after 1065693600 batches: 0.0077
trigger times: 18
Loss after 1065824700 batches: 0.0078
trigger times: 19
Loss after 1065955800 batches: 0.0076
trigger times: 20
Early stopping!
Start to test process.
Loss after 1066086900 batches: 0.0076
Time to train on one home:  342.8242723941803
train_results:  [0.06620264916472092, 0.07114762963109807, 0.05074607514801087, 0.03531084037457569, 0.026422186497829014, 0.0231037815293141, 0.01748454932976234, 0.017476125338373318, 0.016183730855002625, 0.015608215882864637, 0.015855816283001423, 0.015272240943982671, 0.014904168019127074, 0.014211993132470634, 0.014714711150355778, 0.01356970331741986, 0.013157077172457638, 0.013606505732267123, 0.013057062050878846, 0.012793329836550646]
test_results:  [[0.8723314338260226, 0.05636832825504434, 0.23109074257267026, 1.478654265961097, 0.7730179581153068, 34.933737018202365, 2386.3545], [0.684922284550137, 0.2593849767499886, 0.35793733927892385, 1.201213078505687, 0.606707818489778, 28.379089523045394, 1872.9446], [0.6635878748363919, 0.28257658818490883, 0.23295601757676512, 1.102422140290355, 0.5877093759261937, 26.045118198685323, 1814.2953], [0.6288664009835985, 0.32024910717848565, 0.33751496614036297, 1.0788380236882524, 0.5568482522680361, 25.48793499085211, 1719.0251], [0.5802757839361826, 0.372755776558624, 0.42731957086113614, 1.0534418364510614, 0.5138350729026026, 24.8879409647757, 1586.2407], [0.5914222399393717, 0.36065144412651207, 0.4126166155772403, 1.0631863460026936, 0.5237508765166518, 25.118158495597086, 1616.8516], [0.5945181813504961, 0.3572617906647779, 0.43949714903311865, 1.0536405213752047, 0.5265276622861058, 24.89263496732199, 1625.4237], [0.5840653578440348, 0.3685739076459248, 0.4424198101003499, 1.0535551824370888, 0.517260837281645, 24.890618804321495, 1596.8165], [0.574888312154346, 0.3785144309696068, 0.46105928251401523, 1.0545535158641235, 0.5091176143776694, 24.914204789361648, 1571.6777], [0.5659677750534482, 0.3881937670236807, 0.47736113706344846, 1.0466814163269424, 0.5011883546712889, 24.728223616247963, 1547.1996], [0.5552795496251848, 0.39972320473570555, 0.4869199067938419, 1.0432319579028642, 0.4917435017003341, 24.646728924611136, 1518.0426], [0.556764519876904, 0.3981234608309019, 0.4873119444985629, 1.0354352452336422, 0.4930540032485836, 24.462528793273435, 1522.0884], [0.5499979423152076, 0.40547709897063045, 0.49151309919991676, 1.0193683799804445, 0.4870299426858642, 24.082943342919844, 1503.4916], [0.5525510013103485, 0.4027306139686573, 0.4951615175873931, 1.0298212095954027, 0.48927984833421284, 24.32989518911608, 1510.4373], [0.5425989263587527, 0.41345946401828226, 0.5050210962352305, 1.0162617040097164, 0.48049083244313984, 24.009547009604805, 1483.3052], [0.5446713169415792, 0.41122514224627915, 0.5011693979571417, 1.011856964658698, 0.482321176745565, 23.905483463673608, 1488.9554], [0.5494886537392935, 0.4060339214979838, 0.5010704622538157, 1.0286816103891239, 0.48657379668524176, 24.30297174940827, 1502.0835], [0.5506102244059244, 0.4048432294636072, 0.5077384865654669, 1.0249413038763329, 0.4875492051552179, 24.21460566742873, 1505.0948], [0.5427450504567888, 0.4133636447515222, 0.510553041876086, 1.0128563948605698, 0.48056932706785044, 23.929095360412393, 1483.5474], [0.5468276871575249, 0.4089692332662531, 0.5116635011366601, 1.0186256023444504, 0.48416920517196177, 24.065394955040308, 1494.6604]]
Round_19_results:  [0.5468276871575249, 0.4089692332662531, 0.5116635011366601, 1.0186256023444504, 0.48416920517196177, 24.065394955040308, 1494.6604]