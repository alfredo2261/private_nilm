LSTM(
  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))
  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))
  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))
  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))
  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))
  (linear1): Linear(in_features=23500, out_features=1024, bias=True)
  (linear2): Linear(in_features=1024, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (dropout): Dropout(p=0.2, inplace=False)
)
Window Length:  499
trigger times: 0
Loss after 131100 batches: 0.9062
trigger times: 0
Loss after 262200 batches: 0.4364
trigger times: 0
Loss after 393300 batches: 0.2882
trigger times: 0
Loss after 524400 batches: 0.2277
trigger times: 0
Loss after 655500 batches: 0.1982
trigger times: 0
Loss after 786600 batches: 0.1716
trigger times: 0
Loss after 917700 batches: 0.1516
trigger times: 1
Loss after 1048800 batches: 0.1357
trigger times: 2
Loss after 1179900 batches: 0.1215
trigger times: 0
Loss after 1311000 batches: 0.1087
trigger times: 1
Loss after 1442100 batches: 0.0980
trigger times: 2
Loss after 1573200 batches: 0.0880
trigger times: 0
Loss after 1704300 batches: 0.0816
trigger times: 0
Loss after 1835400 batches: 0.0749
trigger times: 1
Loss after 1966500 batches: 0.0679
trigger times: 0
Loss after 2097600 batches: 0.0627
trigger times: 1
Loss after 2228700 batches: 0.0594
trigger times: 2
Loss after 2359800 batches: 0.0553
trigger times: 0
Loss after 2490900 batches: 0.0514
trigger times: 0
Loss after 2622000 batches: 0.0488
trigger times: 1
Loss after 2753100 batches: 0.0454
trigger times: 0
Loss after 2884200 batches: 0.0438
trigger times: 1
Loss after 3015300 batches: 0.0412
trigger times: 2
Loss after 3146400 batches: 0.0400
trigger times: 3
Loss after 3277500 batches: 0.0380
trigger times: 4
Loss after 3408600 batches: 0.0365
trigger times: 5
Loss after 3539700 batches: 0.0341
trigger times: 6
Loss after 3670800 batches: 0.0330
trigger times: 0
Loss after 3801900 batches: 0.0325
trigger times: 1
Loss after 3933000 batches: 0.0321
trigger times: 2
Loss after 4064100 batches: 0.0301
trigger times: 3
Loss after 4195200 batches: 0.0286
trigger times: 0
Loss after 4326300 batches: 0.0284
trigger times: 1
Loss after 4457400 batches: 0.0276
trigger times: 2
Loss after 4588500 batches: 0.0274
trigger times: 3
Loss after 4719600 batches: 0.0262
trigger times: 0
Loss after 4850700 batches: 0.0256
trigger times: 1
Loss after 4981800 batches: 0.0251
trigger times: 0
Loss after 5112900 batches: 0.0247
trigger times: 1
Loss after 5244000 batches: 0.0236
trigger times: 2
Loss after 5375100 batches: 0.0232
trigger times: 0
Loss after 5506200 batches: 0.0224
trigger times: 1
Loss after 5637300 batches: 0.0227
trigger times: 2
Loss after 5768400 batches: 0.0214
trigger times: 3
Loss after 5899500 batches: 0.0215
trigger times: 4
Loss after 6030600 batches: 0.0210
trigger times: 5
Loss after 6161700 batches: 0.0209
trigger times: 6
Loss after 6292800 batches: 0.0203
trigger times: 7
Loss after 6423900 batches: 0.0205
trigger times: 8
Loss after 6555000 batches: 0.0199
trigger times: 0
Loss after 6686100 batches: 0.0195
trigger times: 1
Loss after 6817200 batches: 0.0191
trigger times: 0
Loss after 6948300 batches: 0.0187
trigger times: 1
Loss after 7079400 batches: 0.0182
trigger times: 2
Loss after 7210500 batches: 0.0176
trigger times: 3
Loss after 7341600 batches: 0.0179
trigger times: 4
Loss after 7472700 batches: 0.0178
trigger times: 0
Loss after 7603800 batches: 0.0176
trigger times: 1
Loss after 7734900 batches: 0.0170
trigger times: 2
Loss after 7866000 batches: 0.0170
trigger times: 0
Loss after 7997100 batches: 0.0169
trigger times: 1
Loss after 8128200 batches: 0.0164
trigger times: 2
Loss after 8259300 batches: 0.0160
trigger times: 3
Loss after 8390400 batches: 0.0158
trigger times: 0
Loss after 8521500 batches: 0.0161
trigger times: 1
Loss after 8652600 batches: 0.0157
trigger times: 2
Loss after 8783700 batches: 0.0151
trigger times: 0
Loss after 8914800 batches: 0.0150
trigger times: 1
Loss after 9045900 batches: 0.0151
trigger times: 2
Loss after 9177000 batches: 0.0150
trigger times: 3
Loss after 9308100 batches: 0.0148
trigger times: 4
Loss after 9439200 batches: 0.0143
trigger times: 5
Loss after 9570300 batches: 0.0144
trigger times: 6
Loss after 9701400 batches: 0.0138
trigger times: 7
Loss after 9832500 batches: 0.0139
trigger times: 8
Loss after 9963600 batches: 0.0137
trigger times: 9
Loss after 10094700 batches: 0.0140
trigger times: 10
Loss after 10225800 batches: 0.0135
trigger times: 11
Loss after 10356900 batches: 0.0136
trigger times: 0
Loss after 10488000 batches: 0.0133
trigger times: 0
Loss after 10619100 batches: 0.0140
trigger times: 1
Loss after 10750200 batches: 0.0135
trigger times: 2
Loss after 10881300 batches: 0.0132
trigger times: 3
Loss after 11012400 batches: 0.0127
trigger times: 4
Loss after 11143500 batches: 0.0132
trigger times: 5
Loss after 11274600 batches: 0.0130
trigger times: 6
Loss after 11405700 batches: 0.0132
trigger times: 7
Loss after 11536800 batches: 0.0126
trigger times: 8
Loss after 11667900 batches: 0.0127
trigger times: 9
Loss after 11799000 batches: 0.0122
trigger times: 10
Loss after 11930100 batches: 0.0120
trigger times: 11
Loss after 12061200 batches: 0.0122
trigger times: 12
Loss after 12192300 batches: 0.0119
trigger times: 13
Loss after 12323400 batches: 0.0121
trigger times: 14
Loss after 12454500 batches: 0.0121
trigger times: 15
Loss after 12585600 batches: 0.0120
trigger times: 16
Loss after 12716700 batches: 0.0118
trigger times: 17
Loss after 12847800 batches: 0.0113
trigger times: 18
Loss after 12978900 batches: 0.0115
trigger times: 19
Loss after 13110000 batches: 0.0116
trigger times: 20
Early stopping!
Start to test process.
Loss after 13241100 batches: 0.0117
Time to train on one home:  725.6756339073181
trigger times: 0
Loss after 13343700 batches: 1.0222
trigger times: 1
Loss after 13446300 batches: 0.8874
trigger times: 0
Loss after 13548900 batches: 0.7577
trigger times: 0
Loss after 13651500 batches: 0.6324
trigger times: 0
Loss after 13754100 batches: 0.5861
trigger times: 0
Loss after 13856700 batches: 0.5174
trigger times: 1
Loss after 13959300 batches: 0.4839
trigger times: 2
Loss after 14061900 batches: 0.4526
trigger times: 3
Loss after 14164500 batches: 0.4102
trigger times: 4
Loss after 14267100 batches: 0.3675
trigger times: 5
Loss after 14369700 batches: 0.3448
trigger times: 6
Loss after 14472300 batches: 0.3210
trigger times: 7
Loss after 14574900 batches: 0.2871
trigger times: 8
Loss after 14677500 batches: 0.2614
trigger times: 9
Loss after 14780100 batches: 0.2528
trigger times: 10
Loss after 14882700 batches: 0.2250
trigger times: 11
Loss after 14985300 batches: 0.2040
trigger times: 12
Loss after 15087900 batches: 0.2036
trigger times: 13
Loss after 15190500 batches: 0.1873
trigger times: 14
Loss after 15293100 batches: 0.1896
trigger times: 15
Loss after 15395700 batches: 0.1628
trigger times: 16
Loss after 15498300 batches: 0.1506
trigger times: 17
Loss after 15600900 batches: 0.1442
trigger times: 18
Loss after 15703500 batches: 0.1364
trigger times: 19
Loss after 15806100 batches: 0.1270
trigger times: 20
Early stopping!
Start to test process.
Loss after 15908700 batches: 0.1271
Time to train on one home:  158.74739599227905
trigger times: 0
Loss after 16039800 batches: 0.9577
trigger times: 0
Loss after 16170900 batches: 0.6122
trigger times: 0
Loss after 16302000 batches: 0.4866
trigger times: 1
Loss after 16433100 batches: 0.3888
trigger times: 2
Loss after 16564200 batches: 0.3286
trigger times: 3
Loss after 16695300 batches: 0.2892
trigger times: 4
Loss after 16826400 batches: 0.2567
trigger times: 5
Loss after 16957500 batches: 0.2239
trigger times: 6
Loss after 17088600 batches: 0.1987
trigger times: 7
Loss after 17219700 batches: 0.1778
trigger times: 8
Loss after 17350800 batches: 0.1589
trigger times: 9
Loss after 17481900 batches: 0.1417
trigger times: 10
Loss after 17613000 batches: 0.1281
trigger times: 11
Loss after 17744100 batches: 0.1172
trigger times: 12
Loss after 17875200 batches: 0.1084
trigger times: 13
Loss after 18006300 batches: 0.0987
trigger times: 14
Loss after 18137400 batches: 0.0917
trigger times: 15
Loss after 18268500 batches: 0.0868
trigger times: 16
Loss after 18399600 batches: 0.0814
trigger times: 17
Loss after 18530700 batches: 0.0764
trigger times: 18
Loss after 18661800 batches: 0.0716
trigger times: 19
Loss after 18792900 batches: 0.0685
trigger times: 20
Early stopping!
Start to test process.
Loss after 18924000 batches: 0.0651
Time to train on one home:  174.2393832206726
trigger times: 0
Loss after 19055100 batches: 1.0028
trigger times: 0
Loss after 19186200 batches: 0.8331
trigger times: 1
Loss after 19317300 batches: 0.7255
trigger times: 2
Loss after 19448400 batches: 0.6536
trigger times: 3
Loss after 19579500 batches: 0.5913
trigger times: 4
Loss after 19710600 batches: 0.5107
trigger times: 5
Loss after 19841700 batches: 0.4116
trigger times: 6
Loss after 19972800 batches: 0.3135
trigger times: 7
Loss after 20103900 batches: 0.2488
trigger times: 8
Loss after 20235000 batches: 0.2059
trigger times: 9
Loss after 20366100 batches: 0.1763
trigger times: 10
Loss after 20497200 batches: 0.1504
trigger times: 11
Loss after 20628300 batches: 0.1383
trigger times: 12
Loss after 20759400 batches: 0.1252
trigger times: 13
Loss after 20890500 batches: 0.1162
trigger times: 14
Loss after 21021600 batches: 0.1096
trigger times: 15
Loss after 21152700 batches: 0.1033
trigger times: 16
Loss after 21283800 batches: 0.0978
trigger times: 17
Loss after 21414900 batches: 0.0932
trigger times: 18
Loss after 21546000 batches: 0.0882
trigger times: 19
Loss after 21677100 batches: 0.0854
trigger times: 20
Early stopping!
Start to test process.
Loss after 21808200 batches: 0.0833
Time to train on one home:  167.43595170974731
trigger times: 0
Loss after 21936840 batches: 0.8405
trigger times: 0
Loss after 22065480 batches: 0.4818
trigger times: 0
Loss after 22194120 batches: 0.4270
trigger times: 0
Loss after 22322760 batches: 0.3821
trigger times: 1
Loss after 22451400 batches: 0.3275
trigger times: 2
Loss after 22580040 batches: 0.2734
trigger times: 0
Loss after 22708680 batches: 0.2287
trigger times: 1
Loss after 22837320 batches: 0.1972
trigger times: 2
Loss after 22965960 batches: 0.1714
trigger times: 3
Loss after 23094600 batches: 0.1535
trigger times: 4
Loss after 23223240 batches: 0.1384
trigger times: 5
Loss after 23351880 batches: 0.1254
trigger times: 6
Loss after 23480520 batches: 0.1148
trigger times: 7
Loss after 23609160 batches: 0.1044
trigger times: 8
Loss after 23737800 batches: 0.0969
trigger times: 9
Loss after 23866440 batches: 0.0904
trigger times: 10
Loss after 23995080 batches: 0.0841
trigger times: 11
Loss after 24123720 batches: 0.0797
trigger times: 12
Loss after 24252360 batches: 0.0740
trigger times: 13
Loss after 24381000 batches: 0.0720
trigger times: 14
Loss after 24509640 batches: 0.0678
trigger times: 15
Loss after 24638280 batches: 0.0647
trigger times: 16
Loss after 24766920 batches: 0.0610
trigger times: 17
Loss after 24895560 batches: 0.0585
trigger times: 18
Loss after 25024200 batches: 0.0571
trigger times: 19
Loss after 25152840 batches: 0.0545
trigger times: 20
Early stopping!
Start to test process.
Loss after 25281480 batches: 0.0528
Time to train on one home:  199.47649145126343
trigger times: 0
Loss after 25412580 batches: 0.9678
trigger times: 1
Loss after 25543680 batches: 0.8177
trigger times: 2
Loss after 25674780 batches: 0.7393
trigger times: 3
Loss after 25805880 batches: 0.6790
trigger times: 4
Loss after 25936980 batches: 0.6060
trigger times: 5
Loss after 26068080 batches: 0.5334
trigger times: 6
Loss after 26199180 batches: 0.4571
trigger times: 7
Loss after 26330280 batches: 0.3755
trigger times: 8
Loss after 26461380 batches: 0.2990
trigger times: 9
Loss after 26592480 batches: 0.2488
trigger times: 10
Loss after 26723580 batches: 0.2048
trigger times: 11
Loss after 26854680 batches: 0.1766
trigger times: 12
Loss after 26985780 batches: 0.1566
trigger times: 13
Loss after 27116880 batches: 0.1391
trigger times: 14
Loss after 27247980 batches: 0.1277
trigger times: 15
Loss after 27379080 batches: 0.1184
trigger times: 16
Loss after 27510180 batches: 0.1115
trigger times: 17
Loss after 27641280 batches: 0.1069
trigger times: 18
Loss after 27772380 batches: 0.0994
trigger times: 19
Loss after 27903480 batches: 0.0952
trigger times: 20
Early stopping!
Start to test process.
Loss after 28034580 batches: 0.0909
Time to train on one home:  159.9507532119751
trigger times: 0
Loss after 28165680 batches: 1.0081
trigger times: 0
Loss after 28296780 batches: 0.8692
trigger times: 1
Loss after 28427880 batches: 0.8071
trigger times: 0
Loss after 28558980 batches: 0.7688
trigger times: 1
Loss after 28690080 batches: 0.7063
trigger times: 2
Loss after 28821180 batches: 0.6081
trigger times: 3
Loss after 28952280 batches: 0.5251
trigger times: 4
Loss after 29083380 batches: 0.4492
trigger times: 5
Loss after 29214480 batches: 0.3957
trigger times: 6
Loss after 29345580 batches: 0.3589
trigger times: 7
Loss after 29476680 batches: 0.3161
trigger times: 8
Loss after 29607780 batches: 0.2786
trigger times: 9
Loss after 29738880 batches: 0.2510
trigger times: 10
Loss after 29869980 batches: 0.2294
trigger times: 11
Loss after 30001080 batches: 0.2137
trigger times: 12
Loss after 30132180 batches: 0.1953
trigger times: 13
Loss after 30263280 batches: 0.1768
trigger times: 14
Loss after 30394380 batches: 0.1676
trigger times: 15
Loss after 30525480 batches: 0.1508
trigger times: 16
Loss after 30656580 batches: 0.1444
trigger times: 17
Loss after 30787680 batches: 0.1328
trigger times: 18
Loss after 30918780 batches: 0.1235
trigger times: 19
Loss after 31049880 batches: 0.1212
trigger times: 20
Early stopping!
Start to test process.
Loss after 31180980 batches: 0.1129
Time to train on one home:  181.16792798042297
trigger times: 0
Loss after 31312080 batches: 0.8869
trigger times: 1
Loss after 31443180 batches: 0.4403
trigger times: 2
Loss after 31574280 batches: 0.3239
trigger times: 0
Loss after 31705380 batches: 0.2548
trigger times: 0
Loss after 31836480 batches: 0.2133
trigger times: 1
Loss after 31967580 batches: 0.1777
trigger times: 2
Loss after 32098680 batches: 0.1488
trigger times: 0
Loss after 32229780 batches: 0.1239
trigger times: 0
Loss after 32360880 batches: 0.1090
trigger times: 1
Loss after 32491980 batches: 0.0897
trigger times: 0
Loss after 32623080 batches: 0.0793
trigger times: 0
Loss after 32754180 batches: 0.0719
trigger times: 1
Loss after 32885280 batches: 0.0636
trigger times: 0
Loss after 33016380 batches: 0.0597
trigger times: 0
Loss after 33147480 batches: 0.0549
trigger times: 0
Loss after 33278580 batches: 0.0512
trigger times: 0
Loss after 33409680 batches: 0.0472
trigger times: 1
Loss after 33540780 batches: 0.0452
trigger times: 0
Loss after 33671880 batches: 0.0436
trigger times: 1
Loss after 33802980 batches: 0.0404
trigger times: 0
Loss after 33934080 batches: 0.0378
trigger times: 1
Loss after 34065180 batches: 0.0378
trigger times: 2
Loss after 34196280 batches: 0.0363
trigger times: 3
Loss after 34327380 batches: 0.0357
trigger times: 0
Loss after 34458480 batches: 0.0334
trigger times: 1
Loss after 34589580 batches: 0.0319
trigger times: 2
Loss after 34720680 batches: 0.0304
trigger times: 3
Loss after 34851780 batches: 0.0293
trigger times: 0
Loss after 34982880 batches: 0.0281
trigger times: 1
Loss after 35113980 batches: 0.0273
trigger times: 2
Loss after 35245080 batches: 0.0274
trigger times: 3
Loss after 35376180 batches: 0.0263
trigger times: 0
Loss after 35507280 batches: 0.0250
trigger times: 1
Loss after 35638380 batches: 0.0247
trigger times: 0
Loss after 35769480 batches: 0.0242
trigger times: 1
Loss after 35900580 batches: 0.0242
trigger times: 2
Loss after 36031680 batches: 0.0230
trigger times: 3
Loss after 36162780 batches: 0.0229
trigger times: 0
Loss after 36293880 batches: 0.0228
trigger times: 1
Loss after 36424980 batches: 0.0212
trigger times: 0
Loss after 36556080 batches: 0.0213
trigger times: 1
Loss after 36687180 batches: 0.0201
trigger times: 0
Loss after 36818280 batches: 0.0198
trigger times: 1
Loss after 36949380 batches: 0.0195
trigger times: 2
Loss after 37080480 batches: 0.0191
trigger times: 3
Loss after 37211580 batches: 0.0190
trigger times: 0
Loss after 37342680 batches: 0.0189
trigger times: 1
Loss after 37473780 batches: 0.0184
trigger times: 2
Loss after 37604880 batches: 0.0182
trigger times: 3
Loss after 37735980 batches: 0.0176
trigger times: 4
Loss after 37867080 batches: 0.0172
trigger times: 5
Loss after 37998180 batches: 0.0169
trigger times: 6
Loss after 38129280 batches: 0.0166
trigger times: 7
Loss after 38260380 batches: 0.0166
trigger times: 8
Loss after 38391480 batches: 0.0161
trigger times: 9
Loss after 38522580 batches: 0.0164
trigger times: 10
Loss after 38653680 batches: 0.0163
trigger times: 11
Loss after 38784780 batches: 0.0155
trigger times: 0
Loss after 38915880 batches: 0.0150
trigger times: 0
Loss after 39046980 batches: 0.0149
trigger times: 1
Loss after 39178080 batches: 0.0144
trigger times: 2
Loss after 39309180 batches: 0.0146
trigger times: 3
Loss after 39440280 batches: 0.0143
trigger times: 4
Loss after 39571380 batches: 0.0141
trigger times: 5
Loss after 39702480 batches: 0.0142
trigger times: 6
Loss after 39833580 batches: 0.0141
trigger times: 7
Loss after 39964680 batches: 0.0138
trigger times: 8
Loss after 40095780 batches: 0.0136
trigger times: 9
Loss after 40226880 batches: 0.0137
trigger times: 10
Loss after 40357980 batches: 0.0132
trigger times: 11
Loss after 40489080 batches: 0.0134
trigger times: 12
Loss after 40620180 batches: 0.0131
trigger times: 13
Loss after 40751280 batches: 0.0126
trigger times: 14
Loss after 40882380 batches: 0.0127
trigger times: 15
Loss after 41013480 batches: 0.0125
trigger times: 16
Loss after 41144580 batches: 0.0129
trigger times: 17
Loss after 41275680 batches: 0.0123
trigger times: 18
Loss after 41406780 batches: 0.0123
trigger times: 19
Loss after 41537880 batches: 0.0122
trigger times: 20
Early stopping!
Start to test process.
Loss after 41668980 batches: 0.0118
Time to train on one home:  577.7230489253998
trigger times: 0
Loss after 41747580 batches: 1.0309
trigger times: 0
Loss after 41826180 batches: 0.8417
trigger times: 0
Loss after 41904780 batches: 0.6764
trigger times: 1
Loss after 41983380 batches: 0.5853
trigger times: 2
Loss after 42061980 batches: 0.5116
trigger times: 3
Loss after 42140580 batches: 0.4360
trigger times: 4
Loss after 42219180 batches: 0.3737
trigger times: 5
Loss after 42297780 batches: 0.3133
trigger times: 6
Loss after 42376380 batches: 0.2621
trigger times: 7
Loss after 42454980 batches: 0.2228
trigger times: 8
Loss after 42533580 batches: 0.1883
trigger times: 9
Loss after 42612180 batches: 0.1652
trigger times: 10
Loss after 42690780 batches: 0.1462
trigger times: 11
Loss after 42769380 batches: 0.1330
trigger times: 12
Loss after 42847980 batches: 0.1156
trigger times: 13
Loss after 42926580 batches: 0.1083
trigger times: 14
Loss after 43005180 batches: 0.1009
trigger times: 15
Loss after 43083780 batches: 0.0945
trigger times: 16
Loss after 43162380 batches: 0.0898
trigger times: 17
Loss after 43240980 batches: 0.0845
trigger times: 18
Loss after 43319580 batches: 0.0820
trigger times: 19
Loss after 43398180 batches: 0.0777
trigger times: 20
Early stopping!
Start to test process.
Loss after 43476780 batches: 0.0749
Time to train on one home:  116.06358599662781
train_results:  [0.07004044145543929]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648]]
Round_0_results:  [0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648]
trigger times: 0
Loss after 43607880 batches: 0.5075
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 348 < 349; dropping {'Training_Loss': 0.5074594355416748, 'Validation_Loss': 0.39967019359270733, 'Training_R2': 0.48938881754821084, 'Validation_R2': 0.6282054748212822, 'Training_F1': 0.5983771292636176, 'Validation_F1': 0.6820430614406141, 'Training_NEP': 0.8202869155503526, 'Validation_NEP': 0.7245179075409948, 'Training_NDE': 0.38332487802909354, 'Validation_NDE': 0.2960775610350011, 'Training_MAE': 27.16627583865261, 'Validation_MAE': 19.869286865444593, 'Training_MSE': 1686.5696, 'Validation_MSE': 1093.4061}.
trigger times: 0
Loss after 43738980 batches: 0.2114
trigger times: 0
Loss after 43870080 batches: 0.1655
trigger times: 0
Loss after 44001180 batches: 0.1362
trigger times: 0
Loss after 44132280 batches: 0.1141
trigger times: 1
Loss after 44263380 batches: 0.0974
trigger times: 2
Loss after 44394480 batches: 0.0843
trigger times: 3
Loss after 44525580 batches: 0.0758
trigger times: 4
Loss after 44656680 batches: 0.0679
trigger times: 5
Loss after 44787780 batches: 0.0636
trigger times: 6
Loss after 44918880 batches: 0.0586
trigger times: 7
Loss after 45049980 batches: 0.0540
trigger times: 8
Loss after 45181080 batches: 0.0509
trigger times: 9
Loss after 45312180 batches: 0.0489
trigger times: 10
Loss after 45443280 batches: 0.0462
trigger times: 0
Loss after 45574380 batches: 0.0437
trigger times: 1
Loss after 45705480 batches: 0.0421
trigger times: 2
Loss after 45836580 batches: 0.0403
trigger times: 3
Loss after 45967680 batches: 0.0389
trigger times: 4
Loss after 46098780 batches: 0.0375
trigger times: 0
Loss after 46229880 batches: 0.0357
trigger times: 1
Loss after 46360980 batches: 0.0359
trigger times: 2
Loss after 46492080 batches: 0.0343
trigger times: 3
Loss after 46623180 batches: 0.0346
trigger times: 4
Loss after 46754280 batches: 0.0327
trigger times: 0
Loss after 46885380 batches: 0.0314
trigger times: 1
Loss after 47016480 batches: 0.0310
trigger times: 0
Loss after 47147580 batches: 0.0301
trigger times: 1
Loss after 47278680 batches: 0.0292
trigger times: 2
Loss after 47409780 batches: 0.0294
trigger times: 3
Loss after 47540880 batches: 0.0288
trigger times: 0
Loss after 47671980 batches: 0.0278
trigger times: 1
Loss after 47803080 batches: 0.0272
trigger times: 2
Loss after 47934180 batches: 0.0268
trigger times: 3
Loss after 48065280 batches: 0.0266
trigger times: 0
Loss after 48196380 batches: 0.0258
trigger times: 1
Loss after 48327480 batches: 0.0257
trigger times: 2
Loss after 48458580 batches: 0.0248
trigger times: 3
Loss after 48589680 batches: 0.0248
trigger times: 4
Loss after 48720780 batches: 0.0239
trigger times: 0
Loss after 48851880 batches: 0.0236
trigger times: 1
Loss after 48982980 batches: 0.0238
trigger times: 2
Loss after 49114080 batches: 0.0231
trigger times: 3
Loss after 49245180 batches: 0.0227
trigger times: 4
Loss after 49376280 batches: 0.0222
trigger times: 5
Loss after 49507380 batches: 0.0220
trigger times: 6
Loss after 49638480 batches: 0.0214
trigger times: 7
Loss after 49769580 batches: 0.0212
trigger times: 8
Loss after 49900680 batches: 0.0207
trigger times: 9
Loss after 50031780 batches: 0.0210
trigger times: 10
Loss after 50162880 batches: 0.0205
trigger times: 11
Loss after 50293980 batches: 0.0202
trigger times: 12
Loss after 50425080 batches: 0.0200
trigger times: 13
Loss after 50556180 batches: 0.0198
trigger times: 14
Loss after 50687280 batches: 0.0190
trigger times: 15
Loss after 50818380 batches: 0.0189
trigger times: 16
Loss after 50949480 batches: 0.0191
trigger times: 17
Loss after 51080580 batches: 0.0188
trigger times: 18
Loss after 51211680 batches: 0.0184
trigger times: 19
Loss after 51342780 batches: 0.0185
trigger times: 20
Early stopping!
Start to test process.
Loss after 51473880 batches: 0.0179
Time to train on one home:  443.17737221717834
trigger times: 0
Loss after 51576480 batches: 0.8042
trigger times: 0
Loss after 51679080 batches: 0.5790
trigger times: 1
Loss after 51781680 batches: 0.4811
trigger times: 2
Loss after 51884280 batches: 0.4142
trigger times: 3
Loss after 51986880 batches: 0.3633
trigger times: 4
Loss after 52089480 batches: 0.3166
trigger times: 5
Loss after 52192080 batches: 0.2886
trigger times: 6
Loss after 52294680 batches: 0.2462
trigger times: 7
Loss after 52397280 batches: 0.2227
trigger times: 8
Loss after 52499880 batches: 0.2183
trigger times: 9
Loss after 52602480 batches: 0.1897
trigger times: 10
Loss after 52705080 batches: 0.1747
trigger times: 11
Loss after 52807680 batches: 0.1648
trigger times: 12
Loss after 52910280 batches: 0.1574
trigger times: 13
Loss after 53012880 batches: 0.1600
trigger times: 14
Loss after 53115480 batches: 0.1418
trigger times: 15
Loss after 53218080 batches: 0.1347
trigger times: 16
Loss after 53320680 batches: 0.1323
trigger times: 17
Loss after 53423280 batches: 0.1220
trigger times: 18
Loss after 53525880 batches: 0.1181
trigger times: 19
Loss after 53628480 batches: 0.1138
trigger times: 20
Early stopping!
Start to test process.
Loss after 53731080 batches: 0.1076
Time to train on one home:  136.33613753318787
trigger times: 0
Loss after 53862180 batches: 0.5807
trigger times: 1
Loss after 53993280 batches: 0.3639
trigger times: 2
Loss after 54124380 batches: 0.2970
trigger times: 3
Loss after 54255480 batches: 0.2511
trigger times: 4
Loss after 54386580 batches: 0.2130
trigger times: 5
Loss after 54517680 batches: 0.1866
trigger times: 6
Loss after 54648780 batches: 0.1654
trigger times: 7
Loss after 54779880 batches: 0.1483
trigger times: 8
Loss after 54910980 batches: 0.1340
trigger times: 9
Loss after 55042080 batches: 0.1242
trigger times: 10
Loss after 55173180 batches: 0.1166
trigger times: 11
Loss after 55304280 batches: 0.1093
trigger times: 12
Loss after 55435380 batches: 0.1034
trigger times: 13
Loss after 55566480 batches: 0.0971
trigger times: 14
Loss after 55697580 batches: 0.0926
trigger times: 15
Loss after 55828680 batches: 0.0882
trigger times: 16
Loss after 55959780 batches: 0.0849
trigger times: 17
Loss after 56090880 batches: 0.0814
trigger times: 18
Loss after 56221980 batches: 0.0772
trigger times: 19
Loss after 56353080 batches: 0.0745
trigger times: 20
Early stopping!
Start to test process.
Loss after 56484180 batches: 0.0725
Time to train on one home:  160.28551626205444
trigger times: 0
Loss after 56615280 batches: 0.8771
trigger times: 1
Loss after 56746380 batches: 0.6854
trigger times: 2
Loss after 56877480 batches: 0.6203
trigger times: 3
Loss after 57008580 batches: 0.5506
trigger times: 4
Loss after 57139680 batches: 0.4552
trigger times: 5
Loss after 57270780 batches: 0.3569
trigger times: 6
Loss after 57401880 batches: 0.2818
trigger times: 7
Loss after 57532980 batches: 0.2353
trigger times: 8
Loss after 57664080 batches: 0.2016
trigger times: 9
Loss after 57795180 batches: 0.1791
trigger times: 10
Loss after 57926280 batches: 0.1616
trigger times: 11
Loss after 58057380 batches: 0.1492
trigger times: 12
Loss after 58188480 batches: 0.1388
trigger times: 13
Loss after 58319580 batches: 0.1298
trigger times: 14
Loss after 58450680 batches: 0.1222
trigger times: 15
Loss after 58581780 batches: 0.1160
trigger times: 16
Loss after 58712880 batches: 0.1115
trigger times: 17
Loss after 58843980 batches: 0.1067
trigger times: 18
Loss after 58975080 batches: 0.1017
trigger times: 19
Loss after 59106180 batches: 0.0977
trigger times: 20
Early stopping!
Start to test process.
Loss after 59237280 batches: 0.0965
Time to train on one home:  160.5042552947998
trigger times: 0
Loss after 59365920 batches: 0.5523
trigger times: 0
Loss after 59494560 batches: 0.3884
trigger times: 0
Loss after 59623200 batches: 0.3367
trigger times: 1
Loss after 59751840 batches: 0.2782
trigger times: 2
Loss after 59880480 batches: 0.2295
trigger times: 3
Loss after 60009120 batches: 0.1942
trigger times: 4
Loss after 60137760 batches: 0.1696
trigger times: 5
Loss after 60266400 batches: 0.1526
trigger times: 6
Loss after 60395040 batches: 0.1378
trigger times: 7
Loss after 60523680 batches: 0.1288
trigger times: 8
Loss after 60652320 batches: 0.1176
trigger times: 9
Loss after 60780960 batches: 0.1103
trigger times: 10
Loss after 60909600 batches: 0.1042
trigger times: 11
Loss after 61038240 batches: 0.0990
trigger times: 12
Loss after 61166880 batches: 0.0935
trigger times: 13
Loss after 61295520 batches: 0.0900
trigger times: 14
Loss after 61424160 batches: 0.0857
trigger times: 15
Loss after 61552800 batches: 0.0818
trigger times: 16
Loss after 61681440 batches: 0.0807
trigger times: 17
Loss after 61810080 batches: 0.0773
trigger times: 18
Loss after 61938720 batches: 0.0745
trigger times: 19
Loss after 62067360 batches: 0.0709
trigger times: 20
Early stopping!
Start to test process.
Loss after 62196000 batches: 0.0684
Time to train on one home:  171.87695598602295
trigger times: 0
Loss after 62327100 batches: 0.7986
trigger times: 1
Loss after 62458200 batches: 0.5821
trigger times: 2
Loss after 62589300 batches: 0.4729
trigger times: 3
Loss after 62720400 batches: 0.3666
trigger times: 4
Loss after 62851500 batches: 0.2810
trigger times: 5
Loss after 62982600 batches: 0.2228
trigger times: 6
Loss after 63113700 batches: 0.1870
trigger times: 7
Loss after 63244800 batches: 0.1600
trigger times: 8
Loss after 63375900 batches: 0.1424
trigger times: 9
Loss after 63507000 batches: 0.1305
trigger times: 10
Loss after 63638100 batches: 0.1204
trigger times: 11
Loss after 63769200 batches: 0.1114
trigger times: 12
Loss after 63900300 batches: 0.1057
trigger times: 13
Loss after 64031400 batches: 0.1018
trigger times: 14
Loss after 64162500 batches: 0.0969
trigger times: 15
Loss after 64293600 batches: 0.0917
trigger times: 16
Loss after 64424700 batches: 0.0883
trigger times: 17
Loss after 64555800 batches: 0.0855
trigger times: 18
Loss after 64686900 batches: 0.0830
trigger times: 19
Loss after 64818000 batches: 0.0796
trigger times: 20
Early stopping!
Start to test process.
Loss after 64949100 batches: 0.0785
Time to train on one home:  160.2047164440155
trigger times: 0
Loss after 65080200 batches: 0.8733
trigger times: 0
Loss after 65211300 batches: 0.7471
trigger times: 1
Loss after 65342400 batches: 0.6661
trigger times: 2
Loss after 65473500 batches: 0.5658
trigger times: 3
Loss after 65604600 batches: 0.4986
trigger times: 4
Loss after 65735700 batches: 0.4307
trigger times: 5
Loss after 65866800 batches: 0.3921
trigger times: 6
Loss after 65997900 batches: 0.3532
trigger times: 7
Loss after 66129000 batches: 0.3266
trigger times: 8
Loss after 66260100 batches: 0.2940
trigger times: 9
Loss after 66391200 batches: 0.2700
trigger times: 10
Loss after 66522300 batches: 0.2450
trigger times: 11
Loss after 66653400 batches: 0.2280
trigger times: 12
Loss after 66784500 batches: 0.2113
trigger times: 13
Loss after 66915600 batches: 0.1938
trigger times: 14
Loss after 67046700 batches: 0.1836
trigger times: 15
Loss after 67177800 batches: 0.1673
trigger times: 16
Loss after 67308900 batches: 0.1558
trigger times: 17
Loss after 67440000 batches: 0.1511
trigger times: 18
Loss after 67571100 batches: 0.1463
trigger times: 19
Loss after 67702200 batches: 0.1381
trigger times: 20
Early stopping!
Start to test process.
Loss after 67833300 batches: 0.1291
Time to train on one home:  167.36021828651428
trigger times: 0
Loss after 67964400 batches: 0.8135
trigger times: 0
Loss after 68095500 batches: 0.3496
trigger times: 0
Loss after 68226600 batches: 0.2706
trigger times: 1
Loss after 68357700 batches: 0.2334
trigger times: 2
Loss after 68488800 batches: 0.2027
trigger times: 3
Loss after 68619900 batches: 0.1803
trigger times: 4
Loss after 68751000 batches: 0.1638
trigger times: 5
Loss after 68882100 batches: 0.1481
trigger times: 6
Loss after 69013200 batches: 0.1369
trigger times: 7
Loss after 69144300 batches: 0.1273
trigger times: 8
Loss after 69275400 batches: 0.1196
trigger times: 9
Loss after 69406500 batches: 0.1117
trigger times: 10
Loss after 69537600 batches: 0.1046
trigger times: 11
Loss after 69668700 batches: 0.0993
trigger times: 12
Loss after 69799800 batches: 0.0963
trigger times: 13
Loss after 69930900 batches: 0.0926
trigger times: 14
Loss after 70062000 batches: 0.0897
trigger times: 15
Loss after 70193100 batches: 0.0850
trigger times: 16
Loss after 70324200 batches: 0.0818
trigger times: 17
Loss after 70455300 batches: 0.0800
trigger times: 18
Loss after 70586400 batches: 0.0769
trigger times: 19
Loss after 70717500 batches: 0.0747
trigger times: 20
Early stopping!
Start to test process.
Loss after 70848600 batches: 0.0722
Time to train on one home:  173.78194665908813
trigger times: 0
Loss after 70927200 batches: 0.8319
trigger times: 0
Loss after 71005800 batches: 0.6130
trigger times: 1
Loss after 71084400 batches: 0.5048
trigger times: 2
Loss after 71163000 batches: 0.4329
trigger times: 3
Loss after 71241600 batches: 0.3606
trigger times: 4
Loss after 71320200 batches: 0.3093
trigger times: 5
Loss after 71398800 batches: 0.2586
trigger times: 6
Loss after 71477400 batches: 0.2301
trigger times: 7
Loss after 71556000 batches: 0.2039
trigger times: 8
Loss after 71634600 batches: 0.1903
trigger times: 9
Loss after 71713200 batches: 0.1716
trigger times: 10
Loss after 71791800 batches: 0.1636
trigger times: 11
Loss after 71870400 batches: 0.1539
trigger times: 12
Loss after 71949000 batches: 0.1464
trigger times: 13
Loss after 72027600 batches: 0.1376
trigger times: 14
Loss after 72106200 batches: 0.1320
trigger times: 15
Loss after 72184800 batches: 0.1286
trigger times: 16
Loss after 72263400 batches: 0.1227
trigger times: 17
Loss after 72342000 batches: 0.1171
trigger times: 18
Loss after 72420600 batches: 0.1139
trigger times: 19
Loss after 72499200 batches: 0.1109
trigger times: 20
Early stopping!
Start to test process.
Loss after 72577800 batches: 0.1084
Time to train on one home:  111.77235746383667
train_results:  [0.07004044145543929, 0.0834501805535458]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824]]
Round_1_results:  [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824]
trigger times: 0
Loss after 72708900 batches: 0.3323
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 584 < 585; dropping {'Training_Loss': 0.3323269528600405, 'Validation_Loss': 0.3230527490377426, 'Training_R2': 0.6660304254300115, 'Validation_R2': 0.6995200606381975, 'Training_F1': 0.6815526075750991, 'Validation_F1': 0.6929834984523081, 'Training_NEP': 0.6375959516288717, 'Validation_NEP': 0.6754514948189599, 'Training_NDE': 0.250716887598826, 'Validation_NDE': 0.23928638417529902, 'Training_MAE': 21.115913428823788, 'Validation_MAE': 18.523682264529675, 'Training_MSE': 1103.115, 'Validation_MSE': 883.678}.
trigger times: 0
Loss after 72840000 batches: 0.1598
trigger times: 0
Loss after 72971100 batches: 0.1136
trigger times: 1
Loss after 73102200 batches: 0.0888
trigger times: 0
Loss after 73233300 batches: 0.0728
trigger times: 0
Loss after 73364400 batches: 0.0631
trigger times: 1
Loss after 73495500 batches: 0.0566
trigger times: 2
Loss after 73626600 batches: 0.0520
trigger times: 0
Loss after 73757700 batches: 0.0471
trigger times: 1
Loss after 73888800 batches: 0.0444
trigger times: 0
Loss after 74019900 batches: 0.0418
trigger times: 0
Loss after 74151000 batches: 0.0387
trigger times: 0
Loss after 74282100 batches: 0.0381
trigger times: 0
Loss after 74413200 batches: 0.0355
trigger times: 1
Loss after 74544300 batches: 0.0345
trigger times: 0
Loss after 74675400 batches: 0.0331
trigger times: 0
Loss after 74806500 batches: 0.0322
trigger times: 0
Loss after 74937600 batches: 0.0313
trigger times: 0
Loss after 75068700 batches: 0.0298
trigger times: 0
Loss after 75199800 batches: 0.0289
trigger times: 1
Loss after 75330900 batches: 0.0282
trigger times: 0
Loss after 75462000 batches: 0.0273
trigger times: 0
Loss after 75593100 batches: 0.0267
trigger times: 1
Loss after 75724200 batches: 0.0267
trigger times: 2
Loss after 75855300 batches: 0.0253
trigger times: 3
Loss after 75986400 batches: 0.0256
trigger times: 4
Loss after 76117500 batches: 0.0250
trigger times: 5
Loss after 76248600 batches: 0.0238
trigger times: 6
Loss after 76379700 batches: 0.0237
trigger times: 0
Loss after 76510800 batches: 0.0233
trigger times: 0
Loss after 76641900 batches: 0.0230
trigger times: 1
Loss after 76773000 batches: 0.0228
trigger times: 2
Loss after 76904100 batches: 0.0222
trigger times: 3
Loss after 77035200 batches: 0.0218
trigger times: 4
Loss after 77166300 batches: 0.0215
trigger times: 5
Loss after 77297400 batches: 0.0212
trigger times: 0
Loss after 77428500 batches: 0.0208
trigger times: 1
Loss after 77559600 batches: 0.0206
trigger times: 2
Loss after 77690700 batches: 0.0203
trigger times: 3
Loss after 77821800 batches: 0.0199
trigger times: 4
Loss after 77952900 batches: 0.0196
trigger times: 5
Loss after 78084000 batches: 0.0196
trigger times: 6
Loss after 78215100 batches: 0.0191
trigger times: 7
Loss after 78346200 batches: 0.0193
trigger times: 8
Loss after 78477300 batches: 0.0186
trigger times: 9
Loss after 78608400 batches: 0.0182
trigger times: 10
Loss after 78739500 batches: 0.0183
trigger times: 11
Loss after 78870600 batches: 0.0182
trigger times: 12
Loss after 79001700 batches: 0.0179
trigger times: 13
Loss after 79132800 batches: 0.0179
trigger times: 14
Loss after 79263900 batches: 0.0174
trigger times: 15
Loss after 79395000 batches: 0.0176
trigger times: 16
Loss after 79526100 batches: 0.0174
trigger times: 0
Loss after 79657200 batches: 0.0173
trigger times: 1
Loss after 79788300 batches: 0.0172
trigger times: 2
Loss after 79919400 batches: 0.0166
trigger times: 3
Loss after 80050500 batches: 0.0164
trigger times: 4
Loss after 80181600 batches: 0.0162
trigger times: 5
Loss after 80312700 batches: 0.0162
trigger times: 6
Loss after 80443800 batches: 0.0158
trigger times: 7
Loss after 80574900 batches: 0.0160
trigger times: 8
Loss after 80706000 batches: 0.0157
trigger times: 9
Loss after 80837100 batches: 0.0152
trigger times: 10
Loss after 80968200 batches: 0.0151
trigger times: 0
Loss after 81099300 batches: 0.0150
trigger times: 1
Loss after 81230400 batches: 0.0151
trigger times: 2
Loss after 81361500 batches: 0.0151
trigger times: 3
Loss after 81492600 batches: 0.0148
trigger times: 4
Loss after 81623700 batches: 0.0149
trigger times: 5
Loss after 81754800 batches: 0.0149
trigger times: 6
Loss after 81885900 batches: 0.0143
trigger times: 7
Loss after 82017000 batches: 0.0146
trigger times: 8
Loss after 82148100 batches: 0.0146
trigger times: 9
Loss after 82279200 batches: 0.0143
trigger times: 0
Loss after 82410300 batches: 0.0141
trigger times: 1
Loss after 82541400 batches: 0.0137
trigger times: 2
Loss after 82672500 batches: 0.0138
trigger times: 0
Loss after 82803600 batches: 0.0138
trigger times: 0
Loss after 82934700 batches: 0.0141
trigger times: 0
Loss after 83065800 batches: 0.0135
trigger times: 1
Loss after 83196900 batches: 0.0139
trigger times: 2
Loss after 83328000 batches: 0.0134
trigger times: 0
Loss after 83459100 batches: 0.0133
trigger times: 1
Loss after 83590200 batches: 0.0135
trigger times: 2
Loss after 83721300 batches: 0.0132
trigger times: 3
Loss after 83852400 batches: 0.0133
trigger times: 4
Loss after 83983500 batches: 0.0130
trigger times: 0
Loss after 84114600 batches: 0.0132
trigger times: 1
Loss after 84245700 batches: 0.0127
trigger times: 2
Loss after 84376800 batches: 0.0125
trigger times: 3
Loss after 84507900 batches: 0.0127
trigger times: 4
Loss after 84639000 batches: 0.0125
trigger times: 5
Loss after 84770100 batches: 0.0128
trigger times: 6
Loss after 84901200 batches: 0.0127
trigger times: 7
Loss after 85032300 batches: 0.0126
trigger times: 8
Loss after 85163400 batches: 0.0127
trigger times: 9
Loss after 85294500 batches: 0.0122
trigger times: 10
Loss after 85425600 batches: 0.0118
trigger times: 11
Loss after 85556700 batches: 0.0121
trigger times: 12
Loss after 85687800 batches: 0.0125
trigger times: 13
Loss after 85818900 batches: 0.0122
trigger times: 14
Loss after 85950000 batches: 0.0121
trigger times: 15
Loss after 86081100 batches: 0.0117
trigger times: 16
Loss after 86212200 batches: 0.0117
trigger times: 17
Loss after 86343300 batches: 0.0120
trigger times: 18
Loss after 86474400 batches: 0.0119
trigger times: 19
Loss after 86605500 batches: 0.0116
trigger times: 20
Early stopping!
Start to test process.
Loss after 86736600 batches: 0.0114
Time to train on one home:  775.9412546157837
trigger times: 0
Loss after 86839200 batches: 0.6482
trigger times: 1
Loss after 86941800 batches: 0.4353
trigger times: 2
Loss after 87044400 batches: 0.3249
trigger times: 3
Loss after 87147000 batches: 0.2484
trigger times: 4
Loss after 87249600 batches: 0.2141
trigger times: 5
Loss after 87352200 batches: 0.1832
trigger times: 6
Loss after 87454800 batches: 0.1649
trigger times: 7
Loss after 87557400 batches: 0.1475
trigger times: 8
Loss after 87660000 batches: 0.1377
trigger times: 9
Loss after 87762600 batches: 0.1289
trigger times: 10
Loss after 87865200 batches: 0.1208
trigger times: 11
Loss after 87967800 batches: 0.1150
trigger times: 12
Loss after 88070400 batches: 0.1129
trigger times: 13
Loss after 88173000 batches: 0.1056
trigger times: 14
Loss after 88275600 batches: 0.1001
trigger times: 15
Loss after 88378200 batches: 0.1011
trigger times: 16
Loss after 88480800 batches: 0.0903
trigger times: 17
Loss after 88583400 batches: 0.0902
trigger times: 18
Loss after 88686000 batches: 0.0824
trigger times: 19
Loss after 88788600 batches: 0.0778
trigger times: 20
Early stopping!
Start to test process.
Loss after 88891200 batches: 0.0751
Time to train on one home:  130.46445560455322
trigger times: 0
Loss after 89022300 batches: 0.4644
trigger times: 0
Loss after 89153400 batches: 0.2953
trigger times: 1
Loss after 89284500 batches: 0.2173
trigger times: 2
Loss after 89415600 batches: 0.1743
trigger times: 3
Loss after 89546700 batches: 0.1458
trigger times: 4
Loss after 89677800 batches: 0.1283
trigger times: 5
Loss after 89808900 batches: 0.1145
trigger times: 6
Loss after 89940000 batches: 0.1062
trigger times: 7
Loss after 90071100 batches: 0.0972
trigger times: 8
Loss after 90202200 batches: 0.0907
trigger times: 9
Loss after 90333300 batches: 0.0845
trigger times: 10
Loss after 90464400 batches: 0.0809
trigger times: 11
Loss after 90595500 batches: 0.0760
trigger times: 12
Loss after 90726600 batches: 0.0731
trigger times: 13
Loss after 90857700 batches: 0.0695
trigger times: 14
Loss after 90988800 batches: 0.0676
trigger times: 15
Loss after 91119900 batches: 0.0651
trigger times: 16
Loss after 91251000 batches: 0.0620
trigger times: 17
Loss after 91382100 batches: 0.0603
trigger times: 18
Loss after 91513200 batches: 0.0582
trigger times: 19
Loss after 91644300 batches: 0.0567
trigger times: 20
Early stopping!
Start to test process.
Loss after 91775400 batches: 0.0556
Time to train on one home:  167.45636796951294
trigger times: 0
Loss after 91906500 batches: 0.7188
trigger times: 1
Loss after 92037600 batches: 0.5075
trigger times: 2
Loss after 92168700 batches: 0.3835
trigger times: 3
Loss after 92299800 batches: 0.3038
trigger times: 4
Loss after 92430900 batches: 0.2443
trigger times: 5
Loss after 92562000 batches: 0.2111
trigger times: 6
Loss after 92693100 batches: 0.1827
trigger times: 7
Loss after 92824200 batches: 0.1654
trigger times: 8
Loss after 92955300 batches: 0.1503
trigger times: 9
Loss after 93086400 batches: 0.1381
trigger times: 10
Loss after 93217500 batches: 0.1301
trigger times: 11
Loss after 93348600 batches: 0.1240
trigger times: 12
Loss after 93479700 batches: 0.1161
trigger times: 13
Loss after 93610800 batches: 0.1105
trigger times: 14
Loss after 93741900 batches: 0.1050
trigger times: 15
Loss after 93873000 batches: 0.1021
trigger times: 16
Loss after 94004100 batches: 0.0978
trigger times: 17
Loss after 94135200 batches: 0.0936
trigger times: 18
Loss after 94266300 batches: 0.0901
trigger times: 19
Loss after 94397400 batches: 0.0885
trigger times: 20
Early stopping!
Start to test process.
Loss after 94528500 batches: 0.0854
Time to train on one home:  160.3078691959381
trigger times: 0
Loss after 94657140 batches: 0.4505
trigger times: 1
Loss after 94785780 batches: 0.2611
trigger times: 2
Loss after 94914420 batches: 0.1838
trigger times: 3
Loss after 95043060 batches: 0.1367
trigger times: 4
Loss after 95171700 batches: 0.1131
trigger times: 5
Loss after 95300340 batches: 0.0983
trigger times: 6
Loss after 95428980 batches: 0.0864
trigger times: 7
Loss after 95557620 batches: 0.0785
trigger times: 8
Loss after 95686260 batches: 0.0734
trigger times: 9
Loss after 95814900 batches: 0.0691
trigger times: 10
Loss after 95943540 batches: 0.0653
trigger times: 11
Loss after 96072180 batches: 0.0610
trigger times: 12
Loss after 96200820 batches: 0.0589
trigger times: 13
Loss after 96329460 batches: 0.0563
trigger times: 14
Loss after 96458100 batches: 0.0550
trigger times: 15
Loss after 96586740 batches: 0.0526
trigger times: 16
Loss after 96715380 batches: 0.0515
trigger times: 17
Loss after 96844020 batches: 0.0492
trigger times: 18
Loss after 96972660 batches: 0.0483
trigger times: 19
Loss after 97101300 batches: 0.0464
trigger times: 20
Early stopping!
Start to test process.
Loss after 97229940 batches: 0.0450
Time to train on one home:  157.6041178703308
trigger times: 0
Loss after 97361040 batches: 0.7172
trigger times: 1
Loss after 97492140 batches: 0.4783
trigger times: 2
Loss after 97623240 batches: 0.3277
trigger times: 3
Loss after 97754340 batches: 0.2234
trigger times: 4
Loss after 97885440 batches: 0.1716
trigger times: 5
Loss after 98016540 batches: 0.1448
trigger times: 6
Loss after 98147640 batches: 0.1264
trigger times: 7
Loss after 98278740 batches: 0.1149
trigger times: 8
Loss after 98409840 batches: 0.1047
trigger times: 9
Loss after 98540940 batches: 0.0964
trigger times: 10
Loss after 98672040 batches: 0.0910
trigger times: 11
Loss after 98803140 batches: 0.0868
trigger times: 12
Loss after 98934240 batches: 0.0837
trigger times: 13
Loss after 99065340 batches: 0.0784
trigger times: 14
Loss after 99196440 batches: 0.0757
trigger times: 15
Loss after 99327540 batches: 0.0728
trigger times: 16
Loss after 99458640 batches: 0.0716
trigger times: 17
Loss after 99589740 batches: 0.0683
trigger times: 18
Loss after 99720840 batches: 0.0664
trigger times: 19
Loss after 99851940 batches: 0.0649
trigger times: 20
Early stopping!
Start to test process.
Loss after 99983040 batches: 0.0627
Time to train on one home:  160.18128156661987
trigger times: 0
Loss after 100114140 batches: 0.7925
trigger times: 1
Loss after 100245240 batches: 0.6133
trigger times: 2
Loss after 100376340 batches: 0.4688
trigger times: 3
Loss after 100507440 batches: 0.3845
trigger times: 4
Loss after 100638540 batches: 0.3233
trigger times: 5
Loss after 100769640 batches: 0.2845
trigger times: 6
Loss after 100900740 batches: 0.2468
trigger times: 7
Loss after 101031840 batches: 0.2216
trigger times: 8
Loss after 101162940 batches: 0.2013
trigger times: 9
Loss after 101294040 batches: 0.1767
trigger times: 10
Loss after 101425140 batches: 0.1636
trigger times: 11
Loss after 101556240 batches: 0.1468
trigger times: 12
Loss after 101687340 batches: 0.1394
trigger times: 13
Loss after 101818440 batches: 0.1259
trigger times: 14
Loss after 101949540 batches: 0.1207
trigger times: 15
Loss after 102080640 batches: 0.1109
trigger times: 16
Loss after 102211740 batches: 0.1068
trigger times: 17
Loss after 102342840 batches: 0.0991
trigger times: 18
Loss after 102473940 batches: 0.0946
trigger times: 19
Loss after 102605040 batches: 0.0930
trigger times: 20
Early stopping!
Start to test process.
Loss after 102736140 batches: 0.0894
Time to train on one home:  160.446307182312
trigger times: 0
Loss after 102867240 batches: 0.3159
trigger times: 1
Loss after 102998340 batches: 0.1327
trigger times: 2
Loss after 103129440 batches: 0.0808
trigger times: 3
Loss after 103260540 batches: 0.0595
trigger times: 4
Loss after 103391640 batches: 0.0485
trigger times: 5
Loss after 103522740 batches: 0.0425
trigger times: 6
Loss after 103653840 batches: 0.0380
trigger times: 7
Loss after 103784940 batches: 0.0346
trigger times: 8
Loss after 103916040 batches: 0.0329
trigger times: 9
Loss after 104047140 batches: 0.0311
trigger times: 10
Loss after 104178240 batches: 0.0290
trigger times: 11
Loss after 104309340 batches: 0.0277
trigger times: 12
Loss after 104440440 batches: 0.0266
trigger times: 13
Loss after 104571540 batches: 0.0257
trigger times: 14
Loss after 104702640 batches: 0.0247
trigger times: 15
Loss after 104833740 batches: 0.0237
trigger times: 16
Loss after 104964840 batches: 0.0231
trigger times: 17
Loss after 105095940 batches: 0.0225
trigger times: 18
Loss after 105227040 batches: 0.0218
trigger times: 19
Loss after 105358140 batches: 0.0217
trigger times: 20
Early stopping!
Start to test process.
Loss after 105489240 batches: 0.0211
Time to train on one home:  160.2858054637909
trigger times: 0
Loss after 105567840 batches: 0.7233
trigger times: 1
Loss after 105646440 batches: 0.4874
trigger times: 2
Loss after 105725040 batches: 0.3546
trigger times: 3
Loss after 105803640 batches: 0.2676
trigger times: 4
Loss after 105882240 batches: 0.2184
trigger times: 5
Loss after 105960840 batches: 0.1833
trigger times: 6
Loss after 106039440 batches: 0.1626
trigger times: 7
Loss after 106118040 batches: 0.1445
trigger times: 8
Loss after 106196640 batches: 0.1334
trigger times: 9
Loss after 106275240 batches: 0.1217
trigger times: 10
Loss after 106353840 batches: 0.1138
trigger times: 11
Loss after 106432440 batches: 0.1077
trigger times: 12
Loss after 106511040 batches: 0.1016
trigger times: 13
Loss after 106589640 batches: 0.0985
trigger times: 14
Loss after 106668240 batches: 0.0919
trigger times: 15
Loss after 106746840 batches: 0.0902
trigger times: 16
Loss after 106825440 batches: 0.0867
trigger times: 17
Loss after 106904040 batches: 0.0827
trigger times: 18
Loss after 106982640 batches: 0.0812
trigger times: 19
Loss after 107061240 batches: 0.0775
trigger times: 20
Early stopping!
Start to test process.
Loss after 107139840 batches: 0.0761
Time to train on one home:  107.2219717502594
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525]]
Round_2_results:  [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525]
trigger times: 0
Loss after 107270940 batches: 0.2655
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 861 < 862; dropping {'Training_Loss': 0.2655467264494806, 'Validation_Loss': 0.23512715266810524, 'Training_R2': 0.7330880186370414, 'Validation_R2': 0.7812396004848533, 'Training_F1': 0.7186549828664358, 'Validation_F1': 0.6789739955698494, 'Training_NEP': 0.5630926896282749, 'Validation_NEP': 0.6026327908585727, 'Training_NDE': 0.200375562104185, 'Validation_NDE': 0.17420925041419794, 'Training_MAE': 18.64851314726538, 'Validation_MAE': 16.52669129563916, 'Training_MSE': 881.621, 'Validation_MSE': 643.3499}.
trigger times: 1
Loss after 107402040 batches: 0.0965
trigger times: 0
Loss after 107533140 batches: 0.0633
trigger times: 1
Loss after 107664240 batches: 0.0505
trigger times: 0
Loss after 107795340 batches: 0.0436
trigger times: 1
Loss after 107926440 batches: 0.0372
trigger times: 2
Loss after 108057540 batches: 0.0351
trigger times: 3
Loss after 108188640 batches: 0.0319
trigger times: 4
Loss after 108319740 batches: 0.0306
trigger times: 5
Loss after 108450840 batches: 0.0290
trigger times: 6
Loss after 108581940 batches: 0.0278
trigger times: 0
Loss after 108713040 batches: 0.0265
trigger times: 1
Loss after 108844140 batches: 0.0255
trigger times: 2
Loss after 108975240 batches: 0.0248
trigger times: 3
Loss after 109106340 batches: 0.0235
trigger times: 0
Loss after 109237440 batches: 0.0230
trigger times: 0
Loss after 109368540 batches: 0.0222
trigger times: 1
Loss after 109499640 batches: 0.0217
trigger times: 2
Loss after 109630740 batches: 0.0214
trigger times: 3
Loss after 109761840 batches: 0.0205
trigger times: 4
Loss after 109892940 batches: 0.0205
trigger times: 5
Loss after 110024040 batches: 0.0197
trigger times: 6
Loss after 110155140 batches: 0.0195
trigger times: 7
Loss after 110286240 batches: 0.0193
trigger times: 8
Loss after 110417340 batches: 0.0187
trigger times: 9
Loss after 110548440 batches: 0.0182
trigger times: 0
Loss after 110679540 batches: 0.0183
trigger times: 1
Loss after 110810640 batches: 0.0183
trigger times: 2
Loss after 110941740 batches: 0.0174
trigger times: 3
Loss after 111072840 batches: 0.0175
trigger times: 0
Loss after 111203940 batches: 0.0171
trigger times: 1
Loss after 111335040 batches: 0.0169
trigger times: 2
Loss after 111466140 batches: 0.0166
trigger times: 3
Loss after 111597240 batches: 0.0166
trigger times: 0
Loss after 111728340 batches: 0.0163
trigger times: 1
Loss after 111859440 batches: 0.0161
trigger times: 2
Loss after 111990540 batches: 0.0159
trigger times: 3
Loss after 112121640 batches: 0.0156
trigger times: 4
Loss after 112252740 batches: 0.0156
trigger times: 5
Loss after 112383840 batches: 0.0153
trigger times: 6
Loss after 112514940 batches: 0.0157
trigger times: 7
Loss after 112646040 batches: 0.0151
trigger times: 0
Loss after 112777140 batches: 0.0146
trigger times: 1
Loss after 112908240 batches: 0.0148
trigger times: 2
Loss after 113039340 batches: 0.0147
trigger times: 3
Loss after 113170440 batches: 0.0144
trigger times: 4
Loss after 113301540 batches: 0.0146
trigger times: 5
Loss after 113432640 batches: 0.0143
trigger times: 6
Loss after 113563740 batches: 0.0143
trigger times: 7
Loss after 113694840 batches: 0.0142
trigger times: 8
Loss after 113825940 batches: 0.0136
trigger times: 9
Loss after 113957040 batches: 0.0134
trigger times: 0
Loss after 114088140 batches: 0.0133
trigger times: 1
Loss after 114219240 batches: 0.0133
trigger times: 2
Loss after 114350340 batches: 0.0138
trigger times: 3
Loss after 114481440 batches: 0.0131
trigger times: 4
Loss after 114612540 batches: 0.0130
trigger times: 0
Loss after 114743640 batches: 0.0130
trigger times: 1
Loss after 114874740 batches: 0.0130
trigger times: 0
Loss after 115005840 batches: 0.0129
trigger times: 1
Loss after 115136940 batches: 0.0129
trigger times: 2
Loss after 115268040 batches: 0.0126
trigger times: 3
Loss after 115399140 batches: 0.0124
trigger times: 4
Loss after 115530240 batches: 0.0125
trigger times: 5
Loss after 115661340 batches: 0.0127
trigger times: 6
Loss after 115792440 batches: 0.0125
trigger times: 7
Loss after 115923540 batches: 0.0120
trigger times: 8
Loss after 116054640 batches: 0.0123
trigger times: 9
Loss after 116185740 batches: 0.0122
trigger times: 10
Loss after 116316840 batches: 0.0119
trigger times: 11
Loss after 116447940 batches: 0.0118
trigger times: 12
Loss after 116579040 batches: 0.0117
trigger times: 13
Loss after 116710140 batches: 0.0116
trigger times: 14
Loss after 116841240 batches: 0.0114
trigger times: 15
Loss after 116972340 batches: 0.0120
trigger times: 16
Loss after 117103440 batches: 0.0112
trigger times: 17
Loss after 117234540 batches: 0.0115
trigger times: 18
Loss after 117365640 batches: 0.0117
trigger times: 19
Loss after 117496740 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 117627840 batches: 0.0115
Time to train on one home:  577.2560422420502
trigger times: 0
Loss after 117730440 batches: 0.5697
trigger times: 1
Loss after 117833040 batches: 0.3076
trigger times: 2
Loss after 117935640 batches: 0.2055
trigger times: 3
Loss after 118038240 batches: 0.1586
trigger times: 4
Loss after 118140840 batches: 0.1303
trigger times: 5
Loss after 118243440 batches: 0.1147
trigger times: 6
Loss after 118346040 batches: 0.1063
trigger times: 7
Loss after 118448640 batches: 0.1025
trigger times: 8
Loss after 118551240 batches: 0.0886
trigger times: 9
Loss after 118653840 batches: 0.0825
trigger times: 10
Loss after 118756440 batches: 0.0778
trigger times: 11
Loss after 118859040 batches: 0.0741
trigger times: 12
Loss after 118961640 batches: 0.0701
trigger times: 13
Loss after 119064240 batches: 0.0655
trigger times: 14
Loss after 119166840 batches: 0.0631
trigger times: 15
Loss after 119269440 batches: 0.0606
trigger times: 16
Loss after 119372040 batches: 0.0594
trigger times: 17
Loss after 119474640 batches: 0.0556
trigger times: 18
Loss after 119577240 batches: 0.0584
trigger times: 19
Loss after 119679840 batches: 0.0667
trigger times: 20
Early stopping!
Start to test process.
Loss after 119782440 batches: 0.0545
Time to train on one home:  130.32806992530823
trigger times: 0
Loss after 119913540 batches: 0.3771
trigger times: 1
Loss after 120044640 batches: 0.1718
trigger times: 2
Loss after 120175740 batches: 0.1128
trigger times: 3
Loss after 120306840 batches: 0.0884
trigger times: 4
Loss after 120437940 batches: 0.0755
trigger times: 5
Loss after 120569040 batches: 0.0662
trigger times: 6
Loss after 120700140 batches: 0.0611
trigger times: 7
Loss after 120831240 batches: 0.0564
trigger times: 8
Loss after 120962340 batches: 0.0531
trigger times: 9
Loss after 121093440 batches: 0.0495
trigger times: 10
Loss after 121224540 batches: 0.0471
trigger times: 11
Loss after 121355640 batches: 0.0446
trigger times: 12
Loss after 121486740 batches: 0.0432
trigger times: 13
Loss after 121617840 batches: 0.0418
trigger times: 14
Loss after 121748940 batches: 0.0401
trigger times: 15
Loss after 121880040 batches: 0.0394
trigger times: 16
Loss after 122011140 batches: 0.0377
trigger times: 17
Loss after 122142240 batches: 0.0374
trigger times: 18
Loss after 122273340 batches: 0.0359
trigger times: 19
Loss after 122404440 batches: 0.0351
trigger times: 20
Early stopping!
Start to test process.
Loss after 122535540 batches: 0.0341
Time to train on one home:  159.63835954666138
trigger times: 0
Loss after 122666640 batches: 0.6307
trigger times: 1
Loss after 122797740 batches: 0.3205
trigger times: 2
Loss after 122928840 batches: 0.1907
trigger times: 3
Loss after 123059940 batches: 0.1418
trigger times: 4
Loss after 123191040 batches: 0.1176
trigger times: 5
Loss after 123322140 batches: 0.1027
trigger times: 6
Loss after 123453240 batches: 0.0946
trigger times: 7
Loss after 123584340 batches: 0.0867
trigger times: 8
Loss after 123715440 batches: 0.0821
trigger times: 9
Loss after 123846540 batches: 0.0762
trigger times: 10
Loss after 123977640 batches: 0.0724
trigger times: 11
Loss after 124108740 batches: 0.0693
trigger times: 12
Loss after 124239840 batches: 0.0664
trigger times: 13
Loss after 124370940 batches: 0.0637
trigger times: 14
Loss after 124502040 batches: 0.0612
trigger times: 15
Loss after 124633140 batches: 0.0590
trigger times: 16
Loss after 124764240 batches: 0.0576
trigger times: 17
Loss after 124895340 batches: 0.0559
trigger times: 18
Loss after 125026440 batches: 0.0544
trigger times: 19
Loss after 125157540 batches: 0.0537
trigger times: 20
Early stopping!
Start to test process.
Loss after 125288640 batches: 0.0524
Time to train on one home:  159.79065465927124
trigger times: 0
Loss after 125417280 batches: 0.3335
trigger times: 1
Loss after 125545920 batches: 0.1298
trigger times: 2
Loss after 125674560 batches: 0.0855
trigger times: 3
Loss after 125803200 batches: 0.0680
trigger times: 4
Loss after 125931840 batches: 0.0586
trigger times: 5
Loss after 126060480 batches: 0.0518
trigger times: 6
Loss after 126189120 batches: 0.0476
trigger times: 7
Loss after 126317760 batches: 0.0451
trigger times: 8
Loss after 126446400 batches: 0.0425
trigger times: 0
Loss after 126575040 batches: 0.0405
trigger times: 1
Loss after 126703680 batches: 0.0377
trigger times: 0
Loss after 126832320 batches: 0.0376
trigger times: 1
Loss after 126960960 batches: 0.0348
trigger times: 0
Loss after 127089600 batches: 0.0336
trigger times: 1
Loss after 127218240 batches: 0.0329
trigger times: 2
Loss after 127346880 batches: 0.0323
trigger times: 0
Loss after 127475520 batches: 0.0311
trigger times: 1
Loss after 127604160 batches: 0.0301
trigger times: 2
Loss after 127732800 batches: 0.0294
trigger times: 3
Loss after 127861440 batches: 0.0291
trigger times: 0
Loss after 127990080 batches: 0.0282
trigger times: 1
Loss after 128118720 batches: 0.0274
trigger times: 2
Loss after 128247360 batches: 0.0270
trigger times: 3
Loss after 128376000 batches: 0.0259
trigger times: 4
Loss after 128504640 batches: 0.0255
trigger times: 0
Loss after 128633280 batches: 0.0250
trigger times: 1
Loss after 128761920 batches: 0.0248
trigger times: 2
Loss after 128890560 batches: 0.0248
trigger times: 3
Loss after 129019200 batches: 0.0242
trigger times: 4
Loss after 129147840 batches: 0.0235
trigger times: 5
Loss after 129276480 batches: 0.0234
trigger times: 6
Loss after 129405120 batches: 0.0228
trigger times: 7
Loss after 129533760 batches: 0.0224
trigger times: 0
Loss after 129662400 batches: 0.0223
trigger times: 1
Loss after 129791040 batches: 0.0221
trigger times: 2
Loss after 129919680 batches: 0.0220
trigger times: 3
Loss after 130048320 batches: 0.0216
trigger times: 0
Loss after 130176960 batches: 0.0213
trigger times: 1
Loss after 130305600 batches: 0.0216
trigger times: 2
Loss after 130434240 batches: 0.0207
trigger times: 3
Loss after 130562880 batches: 0.0209
trigger times: 4
Loss after 130691520 batches: 0.0206
trigger times: 5
Loss after 130820160 batches: 0.0201
trigger times: 6
Loss after 130948800 batches: 0.0200
trigger times: 7
Loss after 131077440 batches: 0.0199
trigger times: 8
Loss after 131206080 batches: 0.0196
trigger times: 9
Loss after 131334720 batches: 0.0194
trigger times: 10
Loss after 131463360 batches: 0.0194
trigger times: 11
Loss after 131592000 batches: 0.0188
trigger times: 12
Loss after 131720640 batches: 0.0189
trigger times: 13
Loss after 131849280 batches: 0.0188
trigger times: 14
Loss after 131977920 batches: 0.0187
trigger times: 15
Loss after 132106560 batches: 0.0185
trigger times: 16
Loss after 132235200 batches: 0.0181
trigger times: 17
Loss after 132363840 batches: 0.0177
trigger times: 0
Loss after 132492480 batches: 0.0179
trigger times: 1
Loss after 132621120 batches: 0.0176
trigger times: 2
Loss after 132749760 batches: 0.0178
trigger times: 3
Loss after 132878400 batches: 0.0173
trigger times: 4
Loss after 133007040 batches: 0.0171
trigger times: 5
Loss after 133135680 batches: 0.0171
trigger times: 6
Loss after 133264320 batches: 0.0173
trigger times: 7
Loss after 133392960 batches: 0.0171
trigger times: 8
Loss after 133521600 batches: 0.0166
trigger times: 9
Loss after 133650240 batches: 0.0167
trigger times: 10
Loss after 133778880 batches: 0.0166
trigger times: 11
Loss after 133907520 batches: 0.0163
trigger times: 12
Loss after 134036160 batches: 0.0164
trigger times: 13
Loss after 134164800 batches: 0.0165
trigger times: 14
Loss after 134293440 batches: 0.0161
trigger times: 15
Loss after 134422080 batches: 0.0158
trigger times: 16
Loss after 134550720 batches: 0.0158
trigger times: 17
Loss after 134679360 batches: 0.0162
trigger times: 18
Loss after 134808000 batches: 0.0159
trigger times: 19
Loss after 134936640 batches: 0.0157
trigger times: 20
Early stopping!
Start to test process.
Loss after 135065280 batches: 0.0153
Time to train on one home:  540.0633609294891
trigger times: 0
Loss after 135196380 batches: 0.6274
trigger times: 1
Loss after 135327480 batches: 0.2933
trigger times: 2
Loss after 135458580 batches: 0.1601
trigger times: 3
Loss after 135589680 batches: 0.1187
trigger times: 4
Loss after 135720780 batches: 0.0971
trigger times: 5
Loss after 135851880 batches: 0.0861
trigger times: 6
Loss after 135982980 batches: 0.0780
trigger times: 7
Loss after 136114080 batches: 0.0729
trigger times: 8
Loss after 136245180 batches: 0.0673
trigger times: 9
Loss after 136376280 batches: 0.0640
trigger times: 10
Loss after 136507380 batches: 0.0595
trigger times: 11
Loss after 136638480 batches: 0.0584
trigger times: 12
Loss after 136769580 batches: 0.0555
trigger times: 13
Loss after 136900680 batches: 0.0539
trigger times: 14
Loss after 137031780 batches: 0.0522
trigger times: 15
Loss after 137162880 batches: 0.0502
trigger times: 16
Loss after 137293980 batches: 0.0487
trigger times: 17
Loss after 137425080 batches: 0.0478
trigger times: 18
Loss after 137556180 batches: 0.0460
trigger times: 19
Loss after 137687280 batches: 0.0452
trigger times: 20
Early stopping!
Start to test process.
Loss after 137818380 batches: 0.0450
Time to train on one home:  160.17865133285522
trigger times: 0
Loss after 137949480 batches: 0.6876
trigger times: 1
Loss after 138080580 batches: 0.4218
trigger times: 2
Loss after 138211680 batches: 0.2946
trigger times: 3
Loss after 138342780 batches: 0.2227
trigger times: 4
Loss after 138473880 batches: 0.1894
trigger times: 5
Loss after 138604980 batches: 0.1669
trigger times: 6
Loss after 138736080 batches: 0.1324
trigger times: 7
Loss after 138867180 batches: 0.1176
trigger times: 8
Loss after 138998280 batches: 0.1056
trigger times: 9
Loss after 139129380 batches: 0.0977
trigger times: 10
Loss after 139260480 batches: 0.0920
trigger times: 11
Loss after 139391580 batches: 0.0835
trigger times: 12
Loss after 139522680 batches: 0.0785
trigger times: 13
Loss after 139653780 batches: 0.0751
trigger times: 14
Loss after 139784880 batches: 0.0741
trigger times: 15
Loss after 139915980 batches: 0.0703
trigger times: 16
Loss after 140047080 batches: 0.0664
trigger times: 17
Loss after 140178180 batches: 0.0633
trigger times: 18
Loss after 140309280 batches: 0.0595
trigger times: 19
Loss after 140440380 batches: 0.0593
trigger times: 20
Early stopping!
Start to test process.
Loss after 140571480 batches: 0.0596
Time to train on one home:  160.14536833763123
trigger times: 0
Loss after 140702580 batches: 0.2180
trigger times: 0
Loss after 140833680 batches: 0.0741
trigger times: 1
Loss after 140964780 batches: 0.0475
trigger times: 2
Loss after 141095880 batches: 0.0398
trigger times: 0
Loss after 141226980 batches: 0.0342
trigger times: 1
Loss after 141358080 batches: 0.0310
trigger times: 2
Loss after 141489180 batches: 0.0289
trigger times: 0
Loss after 141620280 batches: 0.0269
trigger times: 1
Loss after 141751380 batches: 0.0259
trigger times: 0
Loss after 141882480 batches: 0.0248
trigger times: 0
Loss after 142013580 batches: 0.0235
trigger times: 1
Loss after 142144680 batches: 0.0228
trigger times: 0
Loss after 142275780 batches: 0.0221
trigger times: 0
Loss after 142406880 batches: 0.0208
trigger times: 0
Loss after 142537980 batches: 0.0208
trigger times: 1
Loss after 142669080 batches: 0.0201
trigger times: 2
Loss after 142800180 batches: 0.0195
trigger times: 3
Loss after 142931280 batches: 0.0189
trigger times: 4
Loss after 143062380 batches: 0.0185
trigger times: 5
Loss after 143193480 batches: 0.0179
trigger times: 6
Loss after 143324580 batches: 0.0179
trigger times: 7
Loss after 143455680 batches: 0.0172
trigger times: 8
Loss after 143586780 batches: 0.0172
trigger times: 9
Loss after 143717880 batches: 0.0166
trigger times: 0
Loss after 143848980 batches: 0.0163
trigger times: 1
Loss after 143980080 batches: 0.0163
trigger times: 2
Loss after 144111180 batches: 0.0160
trigger times: 3
Loss after 144242280 batches: 0.0157
trigger times: 4
Loss after 144373380 batches: 0.0155
trigger times: 5
Loss after 144504480 batches: 0.0151
trigger times: 0
Loss after 144635580 batches: 0.0152
trigger times: 0
Loss after 144766680 batches: 0.0149
trigger times: 1
Loss after 144897780 batches: 0.0147
trigger times: 2
Loss after 145028880 batches: 0.0150
trigger times: 3
Loss after 145159980 batches: 0.0143
trigger times: 4
Loss after 145291080 batches: 0.0142
trigger times: 5
Loss after 145422180 batches: 0.0140
trigger times: 6
Loss after 145553280 batches: 0.0138
trigger times: 0
Loss after 145684380 batches: 0.0134
trigger times: 1
Loss after 145815480 batches: 0.0137
trigger times: 2
Loss after 145946580 batches: 0.0134
trigger times: 3
Loss after 146077680 batches: 0.0131
trigger times: 4
Loss after 146208780 batches: 0.0133
trigger times: 5
Loss after 146339880 batches: 0.0127
trigger times: 0
Loss after 146470980 batches: 0.0127
trigger times: 0
Loss after 146602080 batches: 0.0129
trigger times: 1
Loss after 146733180 batches: 0.0125
trigger times: 2
Loss after 146864280 batches: 0.0123
trigger times: 3
Loss after 146995380 batches: 0.0123
trigger times: 4
Loss after 147126480 batches: 0.0122
trigger times: 5
Loss after 147257580 batches: 0.0121
trigger times: 6
Loss after 147388680 batches: 0.0118
trigger times: 7
Loss after 147519780 batches: 0.0120
trigger times: 8
Loss after 147650880 batches: 0.0119
trigger times: 9
Loss after 147781980 batches: 0.0116
trigger times: 10
Loss after 147913080 batches: 0.0116
trigger times: 11
Loss after 148044180 batches: 0.0116
trigger times: 12
Loss after 148175280 batches: 0.0115
trigger times: 13
Loss after 148306380 batches: 0.0115
trigger times: 14
Loss after 148437480 batches: 0.0114
trigger times: 15
Loss after 148568580 batches: 0.0111
trigger times: 16
Loss after 148699680 batches: 0.0112
trigger times: 17
Loss after 148830780 batches: 0.0110
trigger times: 18
Loss after 148961880 batches: 0.0110
trigger times: 19
Loss after 149092980 batches: 0.0110
trigger times: 20
Early stopping!
Start to test process.
Loss after 149224080 batches: 0.0110
Time to train on one home:  477.76655411720276
trigger times: 0
Loss after 149302680 batches: 0.6507
trigger times: 1
Loss after 149381280 batches: 0.3156
trigger times: 0
Loss after 149459880 batches: 0.1815
trigger times: 1
Loss after 149538480 batches: 0.1261
trigger times: 2
Loss after 149617080 batches: 0.1028
trigger times: 0
Loss after 149695680 batches: 0.0891
trigger times: 1
Loss after 149774280 batches: 0.0796
trigger times: 0
Loss after 149852880 batches: 0.0742
trigger times: 1
Loss after 149931480 batches: 0.0685
trigger times: 2
Loss after 150010080 batches: 0.0650
trigger times: 0
Loss after 150088680 batches: 0.0621
trigger times: 1
Loss after 150167280 batches: 0.0599
trigger times: 2
Loss after 150245880 batches: 0.0569
trigger times: 3
Loss after 150324480 batches: 0.0544
trigger times: 0
Loss after 150403080 batches: 0.0530
trigger times: 1
Loss after 150481680 batches: 0.0534
trigger times: 2
Loss after 150560280 batches: 0.0495
trigger times: 3
Loss after 150638880 batches: 0.0493
trigger times: 4
Loss after 150717480 batches: 0.0488
trigger times: 5
Loss after 150796080 batches: 0.0461
trigger times: 6
Loss after 150874680 batches: 0.0465
trigger times: 7
Loss after 150953280 batches: 0.0453
trigger times: 8
Loss after 151031880 batches: 0.0431
trigger times: 9
Loss after 151110480 batches: 0.0424
trigger times: 10
Loss after 151189080 batches: 0.0406
trigger times: 11
Loss after 151267680 batches: 0.0413
trigger times: 12
Loss after 151346280 batches: 0.0398
trigger times: 13
Loss after 151424880 batches: 0.0399
trigger times: 14
Loss after 151503480 batches: 0.0394
trigger times: 15
Loss after 151582080 batches: 0.0386
trigger times: 16
Loss after 151660680 batches: 0.0375
trigger times: 17
Loss after 151739280 batches: 0.0366
trigger times: 18
Loss after 151817880 batches: 0.0359
trigger times: 19
Loss after 151896480 batches: 0.0354
trigger times: 20
Early stopping!
Start to test process.
Loss after 151975080 batches: 0.0370
Time to train on one home:  170.87500834465027
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833]]
Round_3_results:  [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1223 < 1224; dropping {'Training_Loss': 0.20865647382331345, 'Validation_Loss': 0.2777462982469135, 'Training_R2': 0.7903493979913747, 'Validation_R2': 0.7420123136149128, 'Training_F1': 0.7523297387675629, 'Validation_F1': 0.6725650142184648, 'Training_NEP': 0.49589803267462496, 'Validation_NEP': 0.6034758454683427, 'Training_NDE': 0.15738842823182822, 'Validation_NDE': 0.2054477938459212, 'Training_MAE': 16.42315937033507, 'Validation_MAE': 16.549811350658377, 'Training_MSE': 692.48444, 'Validation_MSE': 758.7131}.
trigger times: 0
Loss after 152106180 batches: 0.2087
trigger times: 0
Loss after 152237280 batches: 0.0638
trigger times: 0
Loss after 152368380 batches: 0.0430
trigger times: 0
Loss after 152499480 batches: 0.0357
trigger times: 1
Loss after 152630580 batches: 0.0307
trigger times: 0
Loss after 152761680 batches: 0.0281
trigger times: 0
Loss after 152892780 batches: 0.0265
trigger times: 0
Loss after 153023880 batches: 0.0245
trigger times: 1
Loss after 153154980 batches: 0.0230
trigger times: 2
Loss after 153286080 batches: 0.0220
trigger times: 0
Loss after 153417180 batches: 0.0214
trigger times: 1
Loss after 153548280 batches: 0.0208
trigger times: 2
Loss after 153679380 batches: 0.0199
trigger times: 0
Loss after 153810480 batches: 0.0193
trigger times: 0
Loss after 153941580 batches: 0.0191
trigger times: 1
Loss after 154072680 batches: 0.0187
trigger times: 2
Loss after 154203780 batches: 0.0181
trigger times: 0
Loss after 154334880 batches: 0.0179
trigger times: 1
Loss after 154465980 batches: 0.0172
trigger times: 2
Loss after 154597080 batches: 0.0168
trigger times: 3
Loss after 154728180 batches: 0.0162
trigger times: 4
Loss after 154859280 batches: 0.0162
trigger times: 5
Loss after 154990380 batches: 0.0165
trigger times: 6
Loss after 155121480 batches: 0.0159
trigger times: 7
Loss after 155252580 batches: 0.0157
trigger times: 8
Loss after 155383680 batches: 0.0154
trigger times: 9
Loss after 155514780 batches: 0.0151
trigger times: 10
Loss after 155645880 batches: 0.0151
trigger times: 0
Loss after 155776980 batches: 0.0149
trigger times: 1
Loss after 155908080 batches: 0.0148
trigger times: 2
Loss after 156039180 batches: 0.0142
trigger times: 3
Loss after 156170280 batches: 0.0143
trigger times: 4
Loss after 156301380 batches: 0.0141
trigger times: 5
Loss after 156432480 batches: 0.0140
trigger times: 6
Loss after 156563580 batches: 0.0139
trigger times: 7
Loss after 156694680 batches: 0.0137
trigger times: 8
Loss after 156825780 batches: 0.0135
trigger times: 9
Loss after 156956880 batches: 0.0138
trigger times: 10
Loss after 157087980 batches: 0.0134
trigger times: 11
Loss after 157219080 batches: 0.0133
trigger times: 12
Loss after 157350180 batches: 0.0132
trigger times: 13
Loss after 157481280 batches: 0.0131
trigger times: 14
Loss after 157612380 batches: 0.0126
trigger times: 15
Loss after 157743480 batches: 0.0128
trigger times: 16
Loss after 157874580 batches: 0.0124
trigger times: 0
Loss after 158005680 batches: 0.0125
trigger times: 1
Loss after 158136780 batches: 0.0125
trigger times: 2
Loss after 158267880 batches: 0.0125
trigger times: 3
Loss after 158398980 batches: 0.0125
trigger times: 4
Loss after 158530080 batches: 0.0123
trigger times: 5
Loss after 158661180 batches: 0.0119
trigger times: 6
Loss after 158792280 batches: 0.0121
trigger times: 7
Loss after 158923380 batches: 0.0121
trigger times: 8
Loss after 159054480 batches: 0.0118
trigger times: 9
Loss after 159185580 batches: 0.0116
trigger times: 10
Loss after 159316680 batches: 0.0121
trigger times: 11
Loss after 159447780 batches: 0.0116
trigger times: 12
Loss after 159578880 batches: 0.0116
trigger times: 13
Loss after 159709980 batches: 0.0114
trigger times: 14
Loss after 159841080 batches: 0.0115
trigger times: 15
Loss after 159972180 batches: 0.0117
trigger times: 16
Loss after 160103280 batches: 0.0112
trigger times: 17
Loss after 160234380 batches: 0.0111
trigger times: 18
Loss after 160365480 batches: 0.0111
trigger times: 19
Loss after 160496580 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 160627680 batches: 0.0110
Time to train on one home:  478.4842936992645
trigger times: 0
Loss after 160730280 batches: 0.4727
trigger times: 1
Loss after 160832880 batches: 0.2189
trigger times: 2
Loss after 160935480 batches: 0.1438
trigger times: 3
Loss after 161038080 batches: 0.1049
trigger times: 4
Loss after 161140680 batches: 0.0994
trigger times: 5
Loss after 161243280 batches: 0.0904
trigger times: 6
Loss after 161345880 batches: 0.0768
trigger times: 7
Loss after 161448480 batches: 0.0715
trigger times: 8
Loss after 161551080 batches: 0.0634
trigger times: 9
Loss after 161653680 batches: 0.0575
trigger times: 10
Loss after 161756280 batches: 0.0551
trigger times: 11
Loss after 161858880 batches: 0.0576
trigger times: 12
Loss after 161961480 batches: 0.0515
trigger times: 13
Loss after 162064080 batches: 0.0498
trigger times: 14
Loss after 162166680 batches: 0.0474
trigger times: 15
Loss after 162269280 batches: 0.0434
trigger times: 16
Loss after 162371880 batches: 0.0418
trigger times: 17
Loss after 162474480 batches: 0.0423
trigger times: 18
Loss after 162577080 batches: 0.0432
trigger times: 19
Loss after 162679680 batches: 0.0406
trigger times: 20
Early stopping!
Start to test process.
Loss after 162782280 batches: 0.0381
Time to train on one home:  130.62877345085144
trigger times: 0
Loss after 162913380 batches: 0.2932
trigger times: 0
Loss after 163044480 batches: 0.1113
trigger times: 0
Loss after 163175580 batches: 0.0751
trigger times: 1
Loss after 163306680 batches: 0.0605
trigger times: 2
Loss after 163437780 batches: 0.0526
trigger times: 0
Loss after 163568880 batches: 0.0486
trigger times: 1
Loss after 163699980 batches: 0.0441
trigger times: 2
Loss after 163831080 batches: 0.0403
trigger times: 0
Loss after 163962180 batches: 0.0388
trigger times: 1
Loss after 164093280 batches: 0.0372
trigger times: 0
Loss after 164224380 batches: 0.0349
trigger times: 0
Loss after 164355480 batches: 0.0340
trigger times: 1
Loss after 164486580 batches: 0.0327
trigger times: 2
Loss after 164617680 batches: 0.0319
trigger times: 3
Loss after 164748780 batches: 0.0312
trigger times: 4
Loss after 164879880 batches: 0.0305
trigger times: 5
Loss after 165010980 batches: 0.0296
trigger times: 6
Loss after 165142080 batches: 0.0289
trigger times: 7
Loss after 165273180 batches: 0.0283
trigger times: 8
Loss after 165404280 batches: 0.0281
trigger times: 9
Loss after 165535380 batches: 0.0269
trigger times: 10
Loss after 165666480 batches: 0.0270
trigger times: 11
Loss after 165797580 batches: 0.0261
trigger times: 12
Loss after 165928680 batches: 0.0259
trigger times: 0
Loss after 166059780 batches: 0.0255
trigger times: 0
Loss after 166190880 batches: 0.0250
trigger times: 1
Loss after 166321980 batches: 0.0248
trigger times: 2
Loss after 166453080 batches: 0.0244
trigger times: 3
Loss after 166584180 batches: 0.0240
trigger times: 4
Loss after 166715280 batches: 0.0238
trigger times: 5
Loss after 166846380 batches: 0.0235
trigger times: 6
Loss after 166977480 batches: 0.0234
trigger times: 7
Loss after 167108580 batches: 0.0231
trigger times: 8
Loss after 167239680 batches: 0.0229
trigger times: 9
Loss after 167370780 batches: 0.0228
trigger times: 10
Loss after 167501880 batches: 0.0226
trigger times: 11
Loss after 167632980 batches: 0.0222
trigger times: 12
Loss after 167764080 batches: 0.0221
trigger times: 13
Loss after 167895180 batches: 0.0220
trigger times: 14
Loss after 168026280 batches: 0.0214
trigger times: 0
Loss after 168157380 batches: 0.0215
trigger times: 1
Loss after 168288480 batches: 0.0214
trigger times: 2
Loss after 168419580 batches: 0.0210
trigger times: 3
Loss after 168550680 batches: 0.0210
trigger times: 4
Loss after 168681780 batches: 0.0207
trigger times: 5
Loss after 168812880 batches: 0.0205
trigger times: 6
Loss after 168943980 batches: 0.0206
trigger times: 0
Loss after 169075080 batches: 0.0202
trigger times: 1
Loss after 169206180 batches: 0.0199
trigger times: 2
Loss after 169337280 batches: 0.0201
trigger times: 3
Loss after 169468380 batches: 0.0199
trigger times: 4
Loss after 169599480 batches: 0.0194
trigger times: 5
Loss after 169730580 batches: 0.0198
trigger times: 6
Loss after 169861680 batches: 0.0194
trigger times: 7
Loss after 169992780 batches: 0.0190
trigger times: 8
Loss after 170123880 batches: 0.0191
trigger times: 9
Loss after 170254980 batches: 0.0191
trigger times: 10
Loss after 170386080 batches: 0.0195
trigger times: 11
Loss after 170517180 batches: 0.0192
trigger times: 12
Loss after 170648280 batches: 0.0189
trigger times: 13
Loss after 170779380 batches: 0.0187
trigger times: 14
Loss after 170910480 batches: 0.0186
trigger times: 15
Loss after 171041580 batches: 0.0185
trigger times: 16
Loss after 171172680 batches: 0.0182
trigger times: 17
Loss after 171303780 batches: 0.0181
trigger times: 18
Loss after 171434880 batches: 0.0179
trigger times: 19
Loss after 171565980 batches: 0.0178
trigger times: 20
Early stopping!
Start to test process.
Loss after 171697080 batches: 0.0179
Time to train on one home:  492.33780169487
trigger times: 0
Loss after 171828180 batches: 0.5408
trigger times: 1
Loss after 171959280 batches: 0.2042
trigger times: 2
Loss after 172090380 batches: 0.1250
trigger times: 3
Loss after 172221480 batches: 0.0988
trigger times: 4
Loss after 172352580 batches: 0.0853
trigger times: 5
Loss after 172483680 batches: 0.0771
trigger times: 6
Loss after 172614780 batches: 0.0705
trigger times: 7
Loss after 172745880 batches: 0.0650
trigger times: 8
Loss after 172876980 batches: 0.0614
trigger times: 9
Loss after 173008080 batches: 0.0589
trigger times: 10
Loss after 173139180 batches: 0.0563
trigger times: 11
Loss after 173270280 batches: 0.0536
trigger times: 12
Loss after 173401380 batches: 0.0527
trigger times: 13
Loss after 173532480 batches: 0.0503
trigger times: 14
Loss after 173663580 batches: 0.0490
trigger times: 15
Loss after 173794680 batches: 0.0475
trigger times: 16
Loss after 173925780 batches: 0.0459
trigger times: 17
Loss after 174056880 batches: 0.0451
trigger times: 18
Loss after 174187980 batches: 0.0448
trigger times: 19
Loss after 174319080 batches: 0.0435
trigger times: 20
Early stopping!
Start to test process.
Loss after 174450180 batches: 0.0423
Time to train on one home:  159.72457098960876
trigger times: 0
Loss after 174578820 batches: 0.2170
trigger times: 0
Loss after 174707460 batches: 0.0723
trigger times: 0
Loss after 174836100 batches: 0.0505
trigger times: 0
Loss after 174964740 batches: 0.0402
trigger times: 1
Loss after 175093380 batches: 0.0364
trigger times: 2
Loss after 175222020 batches: 0.0334
trigger times: 3
Loss after 175350660 batches: 0.0307
trigger times: 0
Loss after 175479300 batches: 0.0288
trigger times: 0
Loss after 175607940 batches: 0.0274
trigger times: 0
Loss after 175736580 batches: 0.0263
trigger times: 0
Loss after 175865220 batches: 0.0252
trigger times: 1
Loss after 175993860 batches: 0.0244
trigger times: 2
Loss after 176122500 batches: 0.0240
trigger times: 3
Loss after 176251140 batches: 0.0228
trigger times: 4
Loss after 176379780 batches: 0.0221
trigger times: 5
Loss after 176508420 batches: 0.0215
trigger times: 0
Loss after 176637060 batches: 0.0215
trigger times: 1
Loss after 176765700 batches: 0.0207
trigger times: 2
Loss after 176894340 batches: 0.0203
trigger times: 3
Loss after 177022980 batches: 0.0195
trigger times: 4
Loss after 177151620 batches: 0.0195
trigger times: 5
Loss after 177280260 batches: 0.0193
trigger times: 6
Loss after 177408900 batches: 0.0187
trigger times: 7
Loss after 177537540 batches: 0.0186
trigger times: 8
Loss after 177666180 batches: 0.0181
trigger times: 9
Loss after 177794820 batches: 0.0181
trigger times: 10
Loss after 177923460 batches: 0.0181
trigger times: 0
Loss after 178052100 batches: 0.0177
trigger times: 1
Loss after 178180740 batches: 0.0178
trigger times: 2
Loss after 178309380 batches: 0.0174
trigger times: 3
Loss after 178438020 batches: 0.0169
trigger times: 4
Loss after 178566660 batches: 0.0170
trigger times: 0
Loss after 178695300 batches: 0.0167
trigger times: 1
Loss after 178823940 batches: 0.0167
trigger times: 2
Loss after 178952580 batches: 0.0164
trigger times: 3
Loss after 179081220 batches: 0.0162
trigger times: 4
Loss after 179209860 batches: 0.0162
trigger times: 5
Loss after 179338500 batches: 0.0158
trigger times: 6
Loss after 179467140 batches: 0.0156
trigger times: 7
Loss after 179595780 batches: 0.0154
trigger times: 8
Loss after 179724420 batches: 0.0156
trigger times: 9
Loss after 179853060 batches: 0.0154
trigger times: 10
Loss after 179981700 batches: 0.0152
trigger times: 11
Loss after 180110340 batches: 0.0150
trigger times: 12
Loss after 180238980 batches: 0.0149
trigger times: 13
Loss after 180367620 batches: 0.0151
trigger times: 14
Loss after 180496260 batches: 0.0149
trigger times: 15
Loss after 180624900 batches: 0.0151
trigger times: 16
Loss after 180753540 batches: 0.0151
trigger times: 17
Loss after 180882180 batches: 0.0145
trigger times: 18
Loss after 181010820 batches: 0.0143
trigger times: 19
Loss after 181139460 batches: 0.0143
trigger times: 20
Early stopping!
Start to test process.
Loss after 181268100 batches: 0.0140
Time to train on one home:  380.47060894966125
trigger times: 0
Loss after 181399200 batches: 0.5397
trigger times: 1
Loss after 181530300 batches: 0.1919
trigger times: 2
Loss after 181661400 batches: 0.1124
trigger times: 3
Loss after 181792500 batches: 0.0888
trigger times: 4
Loss after 181923600 batches: 0.0756
trigger times: 5
Loss after 182054700 batches: 0.0676
trigger times: 6
Loss after 182185800 batches: 0.0624
trigger times: 7
Loss after 182316900 batches: 0.0580
trigger times: 8
Loss after 182448000 batches: 0.0558
trigger times: 9
Loss after 182579100 batches: 0.0510
trigger times: 10
Loss after 182710200 batches: 0.0495
trigger times: 11
Loss after 182841300 batches: 0.0478
trigger times: 12
Loss after 182972400 batches: 0.0462
trigger times: 13
Loss after 183103500 batches: 0.0457
trigger times: 14
Loss after 183234600 batches: 0.0436
trigger times: 15
Loss after 183365700 batches: 0.0428
trigger times: 16
Loss after 183496800 batches: 0.0416
trigger times: 17
Loss after 183627900 batches: 0.0405
trigger times: 18
Loss after 183759000 batches: 0.0399
trigger times: 19
Loss after 183890100 batches: 0.0391
trigger times: 20
Early stopping!
Start to test process.
Loss after 184021200 batches: 0.0382
Time to train on one home:  160.16351056098938
trigger times: 0
Loss after 184152300 batches: 0.5992
trigger times: 1
Loss after 184283400 batches: 0.2991
trigger times: 2
Loss after 184414500 batches: 0.2041
trigger times: 3
Loss after 184545600 batches: 0.1516
trigger times: 4
Loss after 184676700 batches: 0.1198
trigger times: 5
Loss after 184807800 batches: 0.1016
trigger times: 6
Loss after 184938900 batches: 0.0866
trigger times: 7
Loss after 185070000 batches: 0.0772
trigger times: 8
Loss after 185201100 batches: 0.0714
trigger times: 9
Loss after 185332200 batches: 0.0677
trigger times: 10
Loss after 185463300 batches: 0.0643
trigger times: 11
Loss after 185594400 batches: 0.0604
trigger times: 12
Loss after 185725500 batches: 0.0592
trigger times: 13
Loss after 185856600 batches: 0.0575
trigger times: 14
Loss after 185987700 batches: 0.0539
trigger times: 15
Loss after 186118800 batches: 0.0509
trigger times: 16
Loss after 186249900 batches: 0.0491
trigger times: 17
Loss after 186381000 batches: 0.0491
trigger times: 18
Loss after 186512100 batches: 0.0492
trigger times: 19
Loss after 186643200 batches: 0.0471
trigger times: 20
Early stopping!
Start to test process.
Loss after 186774300 batches: 0.0463
Time to train on one home:  160.40246200561523
trigger times: 0
Loss after 186905400 batches: 0.1783
trigger times: 1
Loss after 187036500 batches: 0.0520
trigger times: 0
Loss after 187167600 batches: 0.0374
trigger times: 1
Loss after 187298700 batches: 0.0310
trigger times: 2
Loss after 187429800 batches: 0.0269
trigger times: 3
Loss after 187560900 batches: 0.0247
trigger times: 4
Loss after 187692000 batches: 0.0231
trigger times: 0
Loss after 187823100 batches: 0.0218
trigger times: 0
Loss after 187954200 batches: 0.0204
trigger times: 1
Loss after 188085300 batches: 0.0198
trigger times: 2
Loss after 188216400 batches: 0.0191
trigger times: 3
Loss after 188347500 batches: 0.0185
trigger times: 4
Loss after 188478600 batches: 0.0178
trigger times: 5
Loss after 188609700 batches: 0.0172
trigger times: 6
Loss after 188740800 batches: 0.0166
trigger times: 7
Loss after 188871900 batches: 0.0161
trigger times: 8
Loss after 189003000 batches: 0.0156
trigger times: 9
Loss after 189134100 batches: 0.0155
trigger times: 10
Loss after 189265200 batches: 0.0152
trigger times: 11
Loss after 189396300 batches: 0.0149
trigger times: 12
Loss after 189527400 batches: 0.0142
trigger times: 13
Loss after 189658500 batches: 0.0142
trigger times: 14
Loss after 189789600 batches: 0.0139
trigger times: 15
Loss after 189920700 batches: 0.0139
trigger times: 16
Loss after 190051800 batches: 0.0140
trigger times: 17
Loss after 190182900 batches: 0.0136
trigger times: 0
Loss after 190314000 batches: 0.0133
trigger times: 1
Loss after 190445100 batches: 0.0133
trigger times: 2
Loss after 190576200 batches: 0.0131
trigger times: 3
Loss after 190707300 batches: 0.0129
trigger times: 4
Loss after 190838400 batches: 0.0125
trigger times: 5
Loss after 190969500 batches: 0.0127
trigger times: 6
Loss after 191100600 batches: 0.0122
trigger times: 7
Loss after 191231700 batches: 0.0123
trigger times: 8
Loss after 191362800 batches: 0.0122
trigger times: 9
Loss after 191493900 batches: 0.0119
trigger times: 10
Loss after 191625000 batches: 0.0120
trigger times: 11
Loss after 191756100 batches: 0.0117
trigger times: 12
Loss after 191887200 batches: 0.0119
trigger times: 13
Loss after 192018300 batches: 0.0117
trigger times: 14
Loss after 192149400 batches: 0.0115
trigger times: 15
Loss after 192280500 batches: 0.0113
trigger times: 16
Loss after 192411600 batches: 0.0113
trigger times: 17
Loss after 192542700 batches: 0.0112
trigger times: 18
Loss after 192673800 batches: 0.0111
trigger times: 19
Loss after 192804900 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 192936000 batches: 0.0110
Time to train on one home:  343.4018278121948
trigger times: 0
Loss after 193014600 batches: 0.5162
trigger times: 0
Loss after 193093200 batches: 0.1760
trigger times: 0
Loss after 193171800 batches: 0.0967
trigger times: 1
Loss after 193250400 batches: 0.0752
trigger times: 2
Loss after 193329000 batches: 0.0631
trigger times: 3
Loss after 193407600 batches: 0.0573
trigger times: 4
Loss after 193486200 batches: 0.0525
trigger times: 5
Loss after 193564800 batches: 0.0486
trigger times: 6
Loss after 193643400 batches: 0.0463
trigger times: 7
Loss after 193722000 batches: 0.0437
trigger times: 8
Loss after 193800600 batches: 0.0433
trigger times: 0
Loss after 193879200 batches: 0.0413
trigger times: 1
Loss after 193957800 batches: 0.0398
trigger times: 2
Loss after 194036400 batches: 0.0373
trigger times: 3
Loss after 194115000 batches: 0.0370
trigger times: 4
Loss after 194193600 batches: 0.0355
trigger times: 5
Loss after 194272200 batches: 0.0343
trigger times: 6
Loss after 194350800 batches: 0.0340
trigger times: 7
Loss after 194429400 batches: 0.0329
trigger times: 8
Loss after 194508000 batches: 0.0326
trigger times: 9
Loss after 194586600 batches: 0.0326
trigger times: 10
Loss after 194665200 batches: 0.0313
trigger times: 11
Loss after 194743800 batches: 0.0303
trigger times: 12
Loss after 194822400 batches: 0.0303
trigger times: 13
Loss after 194901000 batches: 0.0297
trigger times: 14
Loss after 194979600 batches: 0.0289
trigger times: 15
Loss after 195058200 batches: 0.0290
trigger times: 16
Loss after 195136800 batches: 0.0280
trigger times: 17
Loss after 195215400 batches: 0.0285
trigger times: 18
Loss after 195294000 batches: 0.0284
trigger times: 19
Loss after 195372600 batches: 0.0273
trigger times: 20
Early stopping!
Start to test process.
Loss after 195451200 batches: 0.0269
Time to train on one home:  157.47002863883972
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756]]
Round_4_results:  [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756]
trigger times: 0
Loss after 195582300 batches: 0.1641
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1573 < 1574; dropping {'Training_Loss': 0.1640661116338001, 'Validation_Loss': 0.25755323635207283, 'Training_R2': 0.8349843230688444, 'Validation_R2': 0.7607246446057601, 'Training_F1': 0.7814744446825826, 'Validation_F1': 0.7128711354174283, 'Training_NEP': 0.43727734128464635, 'Validation_NEP': 0.5400350008313006, 'Training_NDE': 0.12388019770502365, 'Validation_NDE': 0.19054627984867617, 'Training_MAE': 14.481758328866267, 'Validation_MAE': 14.810000190769046, 'Training_MSE': 545.05347, 'Validation_MSE': 703.6822}.
trigger times: 0
Loss after 195713400 batches: 0.0465
trigger times: 0
Loss after 195844500 batches: 0.0337
trigger times: 0
Loss after 195975600 batches: 0.0277
trigger times: 0
Loss after 196106700 batches: 0.0257
trigger times: 1
Loss after 196237800 batches: 0.0229
trigger times: 2
Loss after 196368900 batches: 0.0217
trigger times: 3
Loss after 196500000 batches: 0.0203
trigger times: 4
Loss after 196631100 batches: 0.0195
trigger times: 5
Loss after 196762200 batches: 0.0186
trigger times: 6
Loss after 196893300 batches: 0.0179
trigger times: 7
Loss after 197024400 batches: 0.0177
trigger times: 8
Loss after 197155500 batches: 0.0172
trigger times: 0
Loss after 197286600 batches: 0.0168
trigger times: 1
Loss after 197417700 batches: 0.0161
trigger times: 2
Loss after 197548800 batches: 0.0158
trigger times: 0
Loss after 197679900 batches: 0.0160
trigger times: 1
Loss after 197811000 batches: 0.0156
trigger times: 2
Loss after 197942100 batches: 0.0152
trigger times: 3
Loss after 198073200 batches: 0.0148
trigger times: 4
Loss after 198204300 batches: 0.0145
trigger times: 5
Loss after 198335400 batches: 0.0146
trigger times: 6
Loss after 198466500 batches: 0.0140
trigger times: 7
Loss after 198597600 batches: 0.0139
trigger times: 8
Loss after 198728700 batches: 0.0136
trigger times: 9
Loss after 198859800 batches: 0.0137
trigger times: 10
Loss after 198990900 batches: 0.0137
trigger times: 11
Loss after 199122000 batches: 0.0133
trigger times: 12
Loss after 199253100 batches: 0.0134
trigger times: 13
Loss after 199384200 batches: 0.0128
trigger times: 14
Loss after 199515300 batches: 0.0130
trigger times: 15
Loss after 199646400 batches: 0.0129
trigger times: 16
Loss after 199777500 batches: 0.0125
trigger times: 17
Loss after 199908600 batches: 0.0125
trigger times: 18
Loss after 200039700 batches: 0.0126
trigger times: 19
Loss after 200170800 batches: 0.0121
trigger times: 20
Early stopping!
Start to test process.
Loss after 200301900 batches: 0.0121
Time to train on one home:  274.1169362068176
trigger times: 0
Loss after 200404500 batches: 0.4042
trigger times: 0
Loss after 200507100 batches: 0.1746
trigger times: 0
Loss after 200609700 batches: 0.1256
trigger times: 1
Loss after 200712300 batches: 0.0892
trigger times: 2
Loss after 200814900 batches: 0.0696
trigger times: 3
Loss after 200917500 batches: 0.0611
trigger times: 0
Loss after 201020100 batches: 0.0621
trigger times: 1
Loss after 201122700 batches: 0.0681
trigger times: 2
Loss after 201225300 batches: 0.0517
trigger times: 3
Loss after 201327900 batches: 0.0512
trigger times: 4
Loss after 201430500 batches: 0.0436
trigger times: 5
Loss after 201533100 batches: 0.0407
trigger times: 6
Loss after 201635700 batches: 0.0374
trigger times: 7
Loss after 201738300 batches: 0.0372
trigger times: 8
Loss after 201840900 batches: 0.0373
trigger times: 9
Loss after 201943500 batches: 0.0361
trigger times: 10
Loss after 202046100 batches: 0.0354
trigger times: 11
Loss after 202148700 batches: 0.0353
trigger times: 12
Loss after 202251300 batches: 0.0345
trigger times: 13
Loss after 202353900 batches: 0.0351
trigger times: 14
Loss after 202456500 batches: 0.0374
trigger times: 15
Loss after 202559100 batches: 0.0336
trigger times: 16
Loss after 202661700 batches: 0.0312
trigger times: 17
Loss after 202764300 batches: 0.0313
trigger times: 18
Loss after 202866900 batches: 0.0314
trigger times: 19
Loss after 202969500 batches: 0.0296
trigger times: 20
Early stopping!
Start to test process.
Loss after 203072100 batches: 0.0309
Time to train on one home:  165.32598447799683
trigger times: 0
Loss after 203203200 batches: 0.2118
trigger times: 1
Loss after 203334300 batches: 0.0773
trigger times: 2
Loss after 203465400 batches: 0.0541
trigger times: 3
Loss after 203596500 batches: 0.0452
trigger times: 0
Loss after 203727600 batches: 0.0399
trigger times: 0
Loss after 203858700 batches: 0.0365
trigger times: 0
Loss after 203989800 batches: 0.0340
trigger times: 1
Loss after 204120900 batches: 0.0315
trigger times: 2
Loss after 204252000 batches: 0.0301
trigger times: 3
Loss after 204383100 batches: 0.0287
trigger times: 4
Loss after 204514200 batches: 0.0279
trigger times: 5
Loss after 204645300 batches: 0.0277
trigger times: 0
Loss after 204776400 batches: 0.0262
trigger times: 1
Loss after 204907500 batches: 0.0255
trigger times: 2
Loss after 205038600 batches: 0.0249
trigger times: 0
Loss after 205169700 batches: 0.0242
trigger times: 1
Loss after 205300800 batches: 0.0234
trigger times: 2
Loss after 205431900 batches: 0.0232
trigger times: 3
Loss after 205563000 batches: 0.0230
trigger times: 4
Loss after 205694100 batches: 0.0225
trigger times: 5
Loss after 205825200 batches: 0.0220
trigger times: 6
Loss after 205956300 batches: 0.0217
trigger times: 7
Loss after 206087400 batches: 0.0216
trigger times: 8
Loss after 206218500 batches: 0.0213
trigger times: 9
Loss after 206349600 batches: 0.0210
trigger times: 10
Loss after 206480700 batches: 0.0207
trigger times: 11
Loss after 206611800 batches: 0.0203
trigger times: 12
Loss after 206742900 batches: 0.0203
trigger times: 13
Loss after 206874000 batches: 0.0200
trigger times: 14
Loss after 207005100 batches: 0.0195
trigger times: 15
Loss after 207136200 batches: 0.0193
trigger times: 16
Loss after 207267300 batches: 0.0194
trigger times: 17
Loss after 207398400 batches: 0.0195
trigger times: 18
Loss after 207529500 batches: 0.0191
trigger times: 19
Loss after 207660600 batches: 0.0191
trigger times: 20
Early stopping!
Start to test process.
Loss after 207791700 batches: 0.0187
Time to train on one home:  267.131450176239
trigger times: 0
Loss after 207922800 batches: 0.4417
trigger times: 1
Loss after 208053900 batches: 0.1455
trigger times: 2
Loss after 208185000 batches: 0.0983
trigger times: 3
Loss after 208316100 batches: 0.0818
trigger times: 4
Loss after 208447200 batches: 0.0720
trigger times: 5
Loss after 208578300 batches: 0.0658
trigger times: 6
Loss after 208709400 batches: 0.0596
trigger times: 7
Loss after 208840500 batches: 0.0559
trigger times: 8
Loss after 208971600 batches: 0.0530
trigger times: 9
Loss after 209102700 batches: 0.0504
trigger times: 10
Loss after 209233800 batches: 0.0490
trigger times: 11
Loss after 209364900 batches: 0.0475
trigger times: 12
Loss after 209496000 batches: 0.0457
trigger times: 13
Loss after 209627100 batches: 0.0439
trigger times: 14
Loss after 209758200 batches: 0.0420
trigger times: 15
Loss after 209889300 batches: 0.0406
trigger times: 16
Loss after 210020400 batches: 0.0405
trigger times: 17
Loss after 210151500 batches: 0.0394
trigger times: 18
Loss after 210282600 batches: 0.0390
trigger times: 19
Loss after 210413700 batches: 0.0381
trigger times: 20
Early stopping!
Start to test process.
Loss after 210544800 batches: 0.0373
Time to train on one home:  160.66357493400574
trigger times: 0
Loss after 210673440 batches: 0.3165
trigger times: 0
Loss after 210802080 batches: 0.1282
trigger times: 0
Loss after 210930720 batches: 0.0834
trigger times: 0
Loss after 211059360 batches: 0.0652
trigger times: 1
Loss after 211188000 batches: 0.0567
trigger times: 0
Loss after 211316640 batches: 0.0507
trigger times: 0
Loss after 211445280 batches: 0.0460
trigger times: 1
Loss after 211573920 batches: 0.0430
trigger times: 2
Loss after 211702560 batches: 0.0402
trigger times: 3
Loss after 211831200 batches: 0.0385
trigger times: 4
Loss after 211959840 batches: 0.0358
trigger times: 5
Loss after 212088480 batches: 0.0350
trigger times: 6
Loss after 212217120 batches: 0.0333
trigger times: 0
Loss after 212345760 batches: 0.0319
trigger times: 0
Loss after 212474400 batches: 0.0314
trigger times: 1
Loss after 212603040 batches: 0.0307
trigger times: 2
Loss after 212731680 batches: 0.0295
trigger times: 3
Loss after 212860320 batches: 0.0282
trigger times: 4
Loss after 212988960 batches: 0.0274
trigger times: 5
Loss after 213117600 batches: 0.0267
trigger times: 6
Loss after 213246240 batches: 0.0269
trigger times: 0
Loss after 213374880 batches: 0.0262
trigger times: 0
Loss after 213503520 batches: 0.0257
trigger times: 0
Loss after 213632160 batches: 0.0252
trigger times: 1
Loss after 213760800 batches: 0.0245
trigger times: 2
Loss after 213889440 batches: 0.0241
trigger times: 3
Loss after 214018080 batches: 0.0234
trigger times: 4
Loss after 214146720 batches: 0.0232
trigger times: 5
Loss after 214275360 batches: 0.0228
trigger times: 6
Loss after 214404000 batches: 0.0226
trigger times: 7
Loss after 214532640 batches: 0.0222
trigger times: 8
Loss after 214661280 batches: 0.0222
trigger times: 9
Loss after 214789920 batches: 0.0219
trigger times: 10
Loss after 214918560 batches: 0.0211
trigger times: 11
Loss after 215047200 batches: 0.0208
trigger times: 12
Loss after 215175840 batches: 0.0208
trigger times: 13
Loss after 215304480 batches: 0.0206
trigger times: 14
Loss after 215433120 batches: 0.0204
trigger times: 15
Loss after 215561760 batches: 0.0200
trigger times: 16
Loss after 215690400 batches: 0.0196
trigger times: 17
Loss after 215819040 batches: 0.0193
trigger times: 18
Loss after 215947680 batches: 0.0195
trigger times: 19
Loss after 216076320 batches: 0.0192
trigger times: 20
Early stopping!
Start to test process.
Loss after 216204960 batches: 0.0195
Time to train on one home:  319.0264296531677
trigger times: 0
Loss after 216336060 batches: 0.4553
trigger times: 1
Loss after 216467160 batches: 0.1372
trigger times: 2
Loss after 216598260 batches: 0.0871
trigger times: 3
Loss after 216729360 batches: 0.0713
trigger times: 4
Loss after 216860460 batches: 0.0628
trigger times: 5
Loss after 216991560 batches: 0.0565
trigger times: 6
Loss after 217122660 batches: 0.0530
trigger times: 7
Loss after 217253760 batches: 0.0495
trigger times: 8
Loss after 217384860 batches: 0.0467
trigger times: 9
Loss after 217515960 batches: 0.0447
trigger times: 10
Loss after 217647060 batches: 0.0429
trigger times: 11
Loss after 217778160 batches: 0.0417
trigger times: 12
Loss after 217909260 batches: 0.0404
trigger times: 13
Loss after 218040360 batches: 0.0387
trigger times: 14
Loss after 218171460 batches: 0.0377
trigger times: 15
Loss after 218302560 batches: 0.0376
trigger times: 16
Loss after 218433660 batches: 0.0365
trigger times: 17
Loss after 218564760 batches: 0.0355
trigger times: 18
Loss after 218695860 batches: 0.0346
trigger times: 19
Loss after 218826960 batches: 0.0338
trigger times: 20
Early stopping!
Start to test process.
Loss after 218958060 batches: 0.0332
Time to train on one home:  160.63035464286804
trigger times: 0
Loss after 219089160 batches: 0.5377
trigger times: 0
Loss after 219220260 batches: 0.2333
trigger times: 1
Loss after 219351360 batches: 0.1494
trigger times: 2
Loss after 219482460 batches: 0.1095
trigger times: 3
Loss after 219613560 batches: 0.0897
trigger times: 4
Loss after 219744660 batches: 0.0796
trigger times: 5
Loss after 219875760 batches: 0.0713
trigger times: 6
Loss after 220006860 batches: 0.0676
trigger times: 7
Loss after 220137960 batches: 0.0609
trigger times: 8
Loss after 220269060 batches: 0.0574
trigger times: 9
Loss after 220400160 batches: 0.0531
trigger times: 10
Loss after 220531260 batches: 0.0503
trigger times: 11
Loss after 220662360 batches: 0.0484
trigger times: 12
Loss after 220793460 batches: 0.0478
trigger times: 13
Loss after 220924560 batches: 0.0471
trigger times: 14
Loss after 221055660 batches: 0.0466
trigger times: 15
Loss after 221186760 batches: 0.0451
trigger times: 16
Loss after 221317860 batches: 0.0443
trigger times: 17
Loss after 221448960 batches: 0.0431
trigger times: 18
Loss after 221580060 batches: 0.0418
trigger times: 19
Loss after 221711160 batches: 0.0414
trigger times: 20
Early stopping!
Start to test process.
Loss after 221842260 batches: 0.0404
Time to train on one home:  167.75734543800354
trigger times: 0
Loss after 221973360 batches: 0.1428
trigger times: 0
Loss after 222104460 batches: 0.0421
trigger times: 0
Loss after 222235560 batches: 0.0296
trigger times: 1
Loss after 222366660 batches: 0.0253
trigger times: 2
Loss after 222497760 batches: 0.0233
trigger times: 3
Loss after 222628860 batches: 0.0217
trigger times: 4
Loss after 222759960 batches: 0.0198
trigger times: 5
Loss after 222891060 batches: 0.0186
trigger times: 6
Loss after 223022160 batches: 0.0180
trigger times: 7
Loss after 223153260 batches: 0.0174
trigger times: 8
Loss after 223284360 batches: 0.0168
trigger times: 9
Loss after 223415460 batches: 0.0162
trigger times: 10
Loss after 223546560 batches: 0.0151
trigger times: 11
Loss after 223677660 batches: 0.0149
trigger times: 12
Loss after 223808760 batches: 0.0147
trigger times: 13
Loss after 223939860 batches: 0.0144
trigger times: 14
Loss after 224070960 batches: 0.0139
trigger times: 15
Loss after 224202060 batches: 0.0137
trigger times: 16
Loss after 224333160 batches: 0.0134
trigger times: 17
Loss after 224464260 batches: 0.0132
trigger times: 18
Loss after 224595360 batches: 0.0134
trigger times: 19
Loss after 224726460 batches: 0.0129
trigger times: 20
Early stopping!
Start to test process.
Loss after 224857560 batches: 0.0131
Time to train on one home:  174.51923632621765
trigger times: 0
Loss after 224936160 batches: 0.4216
trigger times: 0
Loss after 225014760 batches: 0.1232
trigger times: 1
Loss after 225093360 batches: 0.0753
trigger times: 2
Loss after 225171960 batches: 0.0595
trigger times: 3
Loss after 225250560 batches: 0.0509
trigger times: 0
Loss after 225329160 batches: 0.0460
trigger times: 1
Loss after 225407760 batches: 0.0438
trigger times: 2
Loss after 225486360 batches: 0.0407
trigger times: 3
Loss after 225564960 batches: 0.0389
trigger times: 0
Loss after 225643560 batches: 0.0369
trigger times: 1
Loss after 225722160 batches: 0.0355
trigger times: 2
Loss after 225800760 batches: 0.0337
trigger times: 3
Loss after 225879360 batches: 0.0322
trigger times: 4
Loss after 225957960 batches: 0.0313
trigger times: 5
Loss after 226036560 batches: 0.0308
trigger times: 6
Loss after 226115160 batches: 0.0301
trigger times: 7
Loss after 226193760 batches: 0.0296
trigger times: 8
Loss after 226272360 batches: 0.0293
trigger times: 9
Loss after 226350960 batches: 0.0291
trigger times: 10
Loss after 226429560 batches: 0.0284
trigger times: 11
Loss after 226508160 batches: 0.0272
trigger times: 12
Loss after 226586760 batches: 0.0264
trigger times: 13
Loss after 226665360 batches: 0.0261
trigger times: 14
Loss after 226743960 batches: 0.0254
trigger times: 15
Loss after 226822560 batches: 0.0255
trigger times: 16
Loss after 226901160 batches: 0.0254
trigger times: 17
Loss after 226979760 batches: 0.0250
trigger times: 0
Loss after 227058360 batches: 0.0247
trigger times: 0
Loss after 227136960 batches: 0.0246
trigger times: 1
Loss after 227215560 batches: 0.0239
trigger times: 2
Loss after 227294160 batches: 0.0238
trigger times: 0
Loss after 227372760 batches: 0.0234
trigger times: 1
Loss after 227451360 batches: 0.0229
trigger times: 2
Loss after 227529960 batches: 0.0235
trigger times: 0
Loss after 227608560 batches: 0.0232
trigger times: 1
Loss after 227687160 batches: 0.0231
trigger times: 2
Loss after 227765760 batches: 0.0224
trigger times: 3
Loss after 227844360 batches: 0.0226
trigger times: 4
Loss after 227922960 batches: 0.0219
trigger times: 5
Loss after 228001560 batches: 0.0222
trigger times: 6
Loss after 228080160 batches: 0.0218
trigger times: 7
Loss after 228158760 batches: 0.0218
trigger times: 8
Loss after 228237360 batches: 0.0222
trigger times: 9
Loss after 228315960 batches: 0.0215
trigger times: 10
Loss after 228394560 batches: 0.0207
trigger times: 11
Loss after 228473160 batches: 0.0209
trigger times: 12
Loss after 228551760 batches: 0.0214
trigger times: 0
Loss after 228630360 batches: 0.0209
trigger times: 1
Loss after 228708960 batches: 0.0215
trigger times: 2
Loss after 228787560 batches: 0.0202
trigger times: 3
Loss after 228866160 batches: 0.0208
trigger times: 4
Loss after 228944760 batches: 0.0206
trigger times: 5
Loss after 229023360 batches: 0.0198
trigger times: 6
Loss after 229101960 batches: 0.0200
trigger times: 7
Loss after 229180560 batches: 0.0200
trigger times: 8
Loss after 229259160 batches: 0.0199
trigger times: 9
Loss after 229337760 batches: 0.0197
trigger times: 10
Loss after 229416360 batches: 0.0199
trigger times: 11
Loss after 229494960 batches: 0.0196
trigger times: 12
Loss after 229573560 batches: 0.0198
trigger times: 13
Loss after 229652160 batches: 0.0191
trigger times: 14
Loss after 229730760 batches: 0.0192
trigger times: 15
Loss after 229809360 batches: 0.0192
trigger times: 16
Loss after 229887960 batches: 0.0190
trigger times: 17
Loss after 229966560 batches: 0.0192
trigger times: 18
Loss after 230045160 batches: 0.0188
trigger times: 19
Loss after 230123760 batches: 0.0188
trigger times: 20
Early stopping!
Start to test process.
Loss after 230202360 batches: 0.0188
Time to train on one home:  321.59717631340027
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945]]
Round_5_results:  [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1872 < 1873; dropping {'Training_Loss': 0.14576920923197045, 'Validation_Loss': 0.22448344859811994, 'Training_R2': 0.8532970174499268, 'Validation_R2': 0.7911842781091968, 'Training_F1': 0.7939350691665629, 'Validation_F1': 0.7178950559165352, 'Training_NEP': 0.41205767579151215, 'Validation_NEP': 0.5211234698366273, 'Training_NDE': 0.11013253298231598, 'Validation_NDE': 0.1662898333789965, 'Training_MAE': 13.646533023723658, 'Validation_MAE': 14.291367551759153, 'Training_MSE': 484.56592, 'Validation_MSE': 614.10376}.
trigger times: 0
Loss after 230333460 batches: 0.1458
trigger times: 0
Loss after 230464560 batches: 0.0411
trigger times: 0
Loss after 230595660 batches: 0.0304
trigger times: 1
Loss after 230726760 batches: 0.0254
trigger times: 0
Loss after 230857860 batches: 0.0231
trigger times: 0
Loss after 230988960 batches: 0.0209
trigger times: 1
Loss after 231120060 batches: 0.0203
trigger times: 2
Loss after 231251160 batches: 0.0188
trigger times: 3
Loss after 231382260 batches: 0.0181
trigger times: 4
Loss after 231513360 batches: 0.0175
trigger times: 5
Loss after 231644460 batches: 0.0172
trigger times: 6
Loss after 231775560 batches: 0.0161
trigger times: 7
Loss after 231906660 batches: 0.0159
trigger times: 8
Loss after 232037760 batches: 0.0156
trigger times: 9
Loss after 232168860 batches: 0.0152
trigger times: 10
Loss after 232299960 batches: 0.0148
trigger times: 11
Loss after 232431060 batches: 0.0147
trigger times: 12
Loss after 232562160 batches: 0.0144
trigger times: 13
Loss after 232693260 batches: 0.0140
trigger times: 14
Loss after 232824360 batches: 0.0141
trigger times: 15
Loss after 232955460 batches: 0.0137
trigger times: 16
Loss after 233086560 batches: 0.0139
trigger times: 17
Loss after 233217660 batches: 0.0133
trigger times: 18
Loss after 233348760 batches: 0.0134
trigger times: 19
Loss after 233479860 batches: 0.0130
trigger times: 20
Early stopping!
Start to test process.
Loss after 233610960 batches: 0.0133
Time to train on one home:  195.77203345298767
trigger times: 0
Loss after 233713560 batches: 0.3662
trigger times: 1
Loss after 233816160 batches: 0.1344
trigger times: 0
Loss after 233918760 batches: 0.0852
trigger times: 0
Loss after 234021360 batches: 0.0727
trigger times: 0
Loss after 234123960 batches: 0.0695
trigger times: 1
Loss after 234226560 batches: 0.0576
trigger times: 2
Loss after 234329160 batches: 0.0482
trigger times: 3
Loss after 234431760 batches: 0.0448
trigger times: 4
Loss after 234534360 batches: 0.0412
trigger times: 5
Loss after 234636960 batches: 0.0401
trigger times: 6
Loss after 234739560 batches: 0.0463
trigger times: 7
Loss after 234842160 batches: 0.0386
trigger times: 8
Loss after 234944760 batches: 0.0367
trigger times: 9
Loss after 235047360 batches: 0.0360
trigger times: 10
Loss after 235149960 batches: 0.0340
trigger times: 11
Loss after 235252560 batches: 0.0335
trigger times: 0
Loss after 235355160 batches: 0.0318
trigger times: 1
Loss after 235457760 batches: 0.0308
trigger times: 2
Loss after 235560360 batches: 0.0327
trigger times: 3
Loss after 235662960 batches: 0.0301
trigger times: 4
Loss after 235765560 batches: 0.0302
trigger times: 0
Loss after 235868160 batches: 0.0302
trigger times: 1
Loss after 235970760 batches: 0.0286
trigger times: 2
Loss after 236073360 batches: 0.0285
trigger times: 3
Loss after 236175960 batches: 0.0300
trigger times: 0
Loss after 236278560 batches: 0.0300
trigger times: 1
Loss after 236381160 batches: 0.0327
trigger times: 2
Loss after 236483760 batches: 0.0266
trigger times: 3
Loss after 236586360 batches: 0.0271
trigger times: 4
Loss after 236688960 batches: 0.0256
trigger times: 5
Loss after 236791560 batches: 0.0271
trigger times: 6
Loss after 236894160 batches: 0.0272
trigger times: 7
Loss after 236996760 batches: 0.0255
trigger times: 8
Loss after 237099360 batches: 0.0244
trigger times: 9
Loss after 237201960 batches: 0.0257
trigger times: 10
Loss after 237304560 batches: 0.0239
trigger times: 11
Loss after 237407160 batches: 0.0234
trigger times: 12
Loss after 237509760 batches: 0.0247
trigger times: 13
Loss after 237612360 batches: 0.0245
trigger times: 14
Loss after 237714960 batches: 0.0230
trigger times: 15
Loss after 237817560 batches: 0.0233
trigger times: 16
Loss after 237920160 batches: 0.0226
trigger times: 17
Loss after 238022760 batches: 0.0247
trigger times: 18
Loss after 238125360 batches: 0.0251
trigger times: 19
Loss after 238227960 batches: 0.0243
trigger times: 0
Loss after 238330560 batches: 0.0270
trigger times: 1
Loss after 238433160 batches: 0.0247
trigger times: 0
Loss after 238535760 batches: 0.0222
trigger times: 1
Loss after 238638360 batches: 0.0220
trigger times: 2
Loss after 238740960 batches: 0.0229
trigger times: 3
Loss after 238843560 batches: 0.0252
trigger times: 4
Loss after 238946160 batches: 0.0220
trigger times: 5
Loss after 239048760 batches: 0.0217
trigger times: 6
Loss after 239151360 batches: 0.0212
trigger times: 7
Loss after 239253960 batches: 0.0216
trigger times: 0
Loss after 239356560 batches: 0.0220
trigger times: 0
Loss after 239459160 batches: 0.0219
trigger times: 1
Loss after 239561760 batches: 0.0222
trigger times: 2
Loss after 239664360 batches: 0.0211
trigger times: 3
Loss after 239766960 batches: 0.0213
trigger times: 4
Loss after 239869560 batches: 0.0242
trigger times: 5
Loss after 239972160 batches: 0.0235
trigger times: 6
Loss after 240074760 batches: 0.0210
trigger times: 7
Loss after 240177360 batches: 0.0209
trigger times: 8
Loss after 240279960 batches: 0.0214
trigger times: 9
Loss after 240382560 batches: 0.0206
trigger times: 10
Loss after 240485160 batches: 0.0208
trigger times: 11
Loss after 240587760 batches: 0.0207
trigger times: 12
Loss after 240690360 batches: 0.0207
trigger times: 13
Loss after 240792960 batches: 0.0224
trigger times: 14
Loss after 240895560 batches: 0.0207
trigger times: 15
Loss after 240998160 batches: 0.0220
trigger times: 16
Loss after 241100760 batches: 0.0199
trigger times: 17
Loss after 241203360 batches: 0.0198
trigger times: 18
Loss after 241305960 batches: 0.0199
trigger times: 0
Loss after 241408560 batches: 0.0230
trigger times: 1
Loss after 241511160 batches: 0.0218
trigger times: 2
Loss after 241613760 batches: 0.0194
trigger times: 3
Loss after 241716360 batches: 0.0191
trigger times: 4
Loss after 241818960 batches: 0.0192
trigger times: 5
Loss after 241921560 batches: 0.0210
trigger times: 0
Loss after 242024160 batches: 0.0222
trigger times: 1
Loss after 242126760 batches: 0.0226
trigger times: 2
Loss after 242229360 batches: 0.0196
trigger times: 3
Loss after 242331960 batches: 0.0208
trigger times: 4
Loss after 242434560 batches: 0.0190
trigger times: 5
Loss after 242537160 batches: 0.0208
trigger times: 6
Loss after 242639760 batches: 0.0265
trigger times: 7
Loss after 242742360 batches: 0.0209
trigger times: 8
Loss after 242844960 batches: 0.0205
trigger times: 0
Loss after 242947560 batches: 0.0195
trigger times: 1
Loss after 243050160 batches: 0.0204
trigger times: 2
Loss after 243152760 batches: 0.0195
trigger times: 3
Loss after 243255360 batches: 0.0204
trigger times: 4
Loss after 243357960 batches: 0.0186
trigger times: 5
Loss after 243460560 batches: 0.0193
trigger times: 6
Loss after 243563160 batches: 0.0189
trigger times: 7
Loss after 243665760 batches: 0.0199
trigger times: 8
Loss after 243768360 batches: 0.0183
trigger times: 9
Loss after 243870960 batches: 0.0183
trigger times: 10
Loss after 243973560 batches: 0.0184
trigger times: 11
Loss after 244076160 batches: 0.0181
trigger times: 12
Loss after 244178760 batches: 0.0208
trigger times: 13
Loss after 244281360 batches: 0.0190
trigger times: 14
Loss after 244383960 batches: 0.0185
trigger times: 0
Loss after 244486560 batches: 0.0174
trigger times: 1
Loss after 244589160 batches: 0.0176
trigger times: 0
Loss after 244691760 batches: 0.0195
trigger times: 1
Loss after 244794360 batches: 0.0192
trigger times: 2
Loss after 244896960 batches: 0.0179
trigger times: 3
Loss after 244999560 batches: 0.0201
trigger times: 4
Loss after 245102160 batches: 0.0194
trigger times: 5
Loss after 245204760 batches: 0.0199
trigger times: 6
Loss after 245307360 batches: 0.0189
trigger times: 7
Loss after 245409960 batches: 0.0183
trigger times: 8
Loss after 245512560 batches: 0.0179
trigger times: 9
Loss after 245615160 batches: 0.0194
trigger times: 10
Loss after 245717760 batches: 0.0195
trigger times: 11
Loss after 245820360 batches: 0.0185
trigger times: 12
Loss after 245922960 batches: 0.0209
trigger times: 13
Loss after 246025560 batches: 0.0186
trigger times: 14
Loss after 246128160 batches: 0.0172
trigger times: 15
Loss after 246230760 batches: 0.0179
trigger times: 16
Loss after 246333360 batches: 0.0167
trigger times: 17
Loss after 246435960 batches: 0.0165
trigger times: 18
Loss after 246538560 batches: 0.0166
trigger times: 19
Loss after 246641160 batches: 0.0171
trigger times: 20
Early stopping!
Start to test process.
Loss after 246743760 batches: 0.0166
Time to train on one home:  738.080326795578
trigger times: 0
Loss after 246874860 batches: 0.1920
trigger times: 1
Loss after 247005960 batches: 0.0696
trigger times: 2
Loss after 247137060 batches: 0.0485
trigger times: 3
Loss after 247268160 batches: 0.0401
trigger times: 4
Loss after 247399260 batches: 0.0360
trigger times: 5
Loss after 247530360 batches: 0.0328
trigger times: 6
Loss after 247661460 batches: 0.0303
trigger times: 7
Loss after 247792560 batches: 0.0294
trigger times: 8
Loss after 247923660 batches: 0.0281
trigger times: 9
Loss after 248054760 batches: 0.0272
trigger times: 10
Loss after 248185860 batches: 0.0261
trigger times: 11
Loss after 248316960 batches: 0.0254
trigger times: 12
Loss after 248448060 batches: 0.0247
trigger times: 13
Loss after 248579160 batches: 0.0241
trigger times: 14
Loss after 248710260 batches: 0.0233
trigger times: 15
Loss after 248841360 batches: 0.0224
trigger times: 16
Loss after 248972460 batches: 0.0221
trigger times: 17
Loss after 249103560 batches: 0.0219
trigger times: 18
Loss after 249234660 batches: 0.0216
trigger times: 19
Loss after 249365760 batches: 0.0218
trigger times: 20
Early stopping!
Start to test process.
Loss after 249496860 batches: 0.0208
Time to train on one home:  160.18153285980225
trigger times: 0
Loss after 249627960 batches: 0.3964
trigger times: 1
Loss after 249759060 batches: 0.1226
trigger times: 2
Loss after 249890160 batches: 0.0848
trigger times: 3
Loss after 250021260 batches: 0.0706
trigger times: 4
Loss after 250152360 batches: 0.0638
trigger times: 5
Loss after 250283460 batches: 0.0584
trigger times: 6
Loss after 250414560 batches: 0.0536
trigger times: 7
Loss after 250545660 batches: 0.0507
trigger times: 8
Loss after 250676760 batches: 0.0490
trigger times: 9
Loss after 250807860 batches: 0.0460
trigger times: 0
Loss after 250938960 batches: 0.0438
trigger times: 1
Loss after 251070060 batches: 0.0426
trigger times: 2
Loss after 251201160 batches: 0.0411
trigger times: 3
Loss after 251332260 batches: 0.0397
trigger times: 4
Loss after 251463360 batches: 0.0389
trigger times: 5
Loss after 251594460 batches: 0.0381
trigger times: 0
Loss after 251725560 batches: 0.0370
trigger times: 0
Loss after 251856660 batches: 0.0368
trigger times: 1
Loss after 251987760 batches: 0.0361
trigger times: 2
Loss after 252118860 batches: 0.0355
trigger times: 3
Loss after 252249960 batches: 0.0345
trigger times: 4
Loss after 252381060 batches: 0.0338
trigger times: 0
Loss after 252512160 batches: 0.0335
trigger times: 1
Loss after 252643260 batches: 0.0334
trigger times: 2
Loss after 252774360 batches: 0.0325
trigger times: 3
Loss after 252905460 batches: 0.0322
trigger times: 4
Loss after 253036560 batches: 0.0320
trigger times: 5
Loss after 253167660 batches: 0.0311
trigger times: 6
Loss after 253298760 batches: 0.0310
trigger times: 7
Loss after 253429860 batches: 0.0308
trigger times: 8
Loss after 253560960 batches: 0.0303
trigger times: 9
Loss after 253692060 batches: 0.0298
trigger times: 0
Loss after 253823160 batches: 0.0297
trigger times: 1
Loss after 253954260 batches: 0.0294
trigger times: 2
Loss after 254085360 batches: 0.0291
trigger times: 0
Loss after 254216460 batches: 0.0289
trigger times: 1
Loss after 254347560 batches: 0.0281
trigger times: 2
Loss after 254478660 batches: 0.0285
trigger times: 3
Loss after 254609760 batches: 0.0283
trigger times: 4
Loss after 254740860 batches: 0.0282
trigger times: 5
Loss after 254871960 batches: 0.0279
trigger times: 6
Loss after 255003060 batches: 0.0273
trigger times: 7
Loss after 255134160 batches: 0.0274
trigger times: 8
Loss after 255265260 batches: 0.0274
trigger times: 0
Loss after 255396360 batches: 0.0274
trigger times: 1
Loss after 255527460 batches: 0.0269
trigger times: 2
Loss after 255658560 batches: 0.0267
trigger times: 3
Loss after 255789660 batches: 0.0262
trigger times: 4
Loss after 255920760 batches: 0.0261
trigger times: 5
Loss after 256051860 batches: 0.0261
trigger times: 6
Loss after 256182960 batches: 0.0259
trigger times: 7
Loss after 256314060 batches: 0.0258
trigger times: 8
Loss after 256445160 batches: 0.0255
trigger times: 9
Loss after 256576260 batches: 0.0255
trigger times: 10
Loss after 256707360 batches: 0.0256
trigger times: 11
Loss after 256838460 batches: 0.0248
trigger times: 12
Loss after 256969560 batches: 0.0252
trigger times: 13
Loss after 257100660 batches: 0.0246
trigger times: 14
Loss after 257231760 batches: 0.0244
trigger times: 15
Loss after 257362860 batches: 0.0246
trigger times: 16
Loss after 257493960 batches: 0.0242
trigger times: 17
Loss after 257625060 batches: 0.0242
trigger times: 18
Loss after 257756160 batches: 0.0237
trigger times: 0
Loss after 257887260 batches: 0.0237
trigger times: 1
Loss after 258018360 batches: 0.0237
trigger times: 2
Loss after 258149460 batches: 0.0238
trigger times: 3
Loss after 258280560 batches: 0.0233
trigger times: 4
Loss after 258411660 batches: 0.0232
trigger times: 5
Loss after 258542760 batches: 0.0234
trigger times: 6
Loss after 258673860 batches: 0.0234
trigger times: 7
Loss after 258804960 batches: 0.0231
trigger times: 0
Loss after 258936060 batches: 0.0231
trigger times: 1
Loss after 259067160 batches: 0.0229
trigger times: 2
Loss after 259198260 batches: 0.0229
trigger times: 3
Loss after 259329360 batches: 0.0230
trigger times: 4
Loss after 259460460 batches: 0.0228
trigger times: 0
Loss after 259591560 batches: 0.0227
trigger times: 1
Loss after 259722660 batches: 0.0223
trigger times: 2
Loss after 259853760 batches: 0.0225
trigger times: 3
Loss after 259984860 batches: 0.0220
trigger times: 4
Loss after 260115960 batches: 0.0222
trigger times: 5
Loss after 260247060 batches: 0.0220
trigger times: 6
Loss after 260378160 batches: 0.0221
trigger times: 0
Loss after 260509260 batches: 0.0220
trigger times: 1
Loss after 260640360 batches: 0.0217
trigger times: 2
Loss after 260771460 batches: 0.0219
trigger times: 3
Loss after 260902560 batches: 0.0218
trigger times: 4
Loss after 261033660 batches: 0.0220
trigger times: 5
Loss after 261164760 batches: 0.0213
trigger times: 6
Loss after 261295860 batches: 0.0213
trigger times: 7
Loss after 261426960 batches: 0.0220
trigger times: 8
Loss after 261558060 batches: 0.0214
trigger times: 9
Loss after 261689160 batches: 0.0212
trigger times: 10
Loss after 261820260 batches: 0.0213
trigger times: 11
Loss after 261951360 batches: 0.0212
trigger times: 12
Loss after 262082460 batches: 0.0212
trigger times: 13
Loss after 262213560 batches: 0.0212
trigger times: 14
Loss after 262344660 batches: 0.0208
trigger times: 0
Loss after 262475760 batches: 0.0206
trigger times: 0
Loss after 262606860 batches: 0.0210
trigger times: 1
Loss after 262737960 batches: 0.0207
trigger times: 2
Loss after 262869060 batches: 0.0205
trigger times: 0
Loss after 263000160 batches: 0.0206
trigger times: 1
Loss after 263131260 batches: 0.0206
trigger times: 2
Loss after 263262360 batches: 0.0204
trigger times: 0
Loss after 263393460 batches: 0.0204
trigger times: 0
Loss after 263524560 batches: 0.0207
trigger times: 1
Loss after 263655660 batches: 0.0201
trigger times: 0
Loss after 263786760 batches: 0.0203
trigger times: 1
Loss after 263917860 batches: 0.0202
trigger times: 0
Loss after 264048960 batches: 0.0201
trigger times: 1
Loss after 264180060 batches: 0.0203
trigger times: 2
Loss after 264311160 batches: 0.0201
trigger times: 0
Loss after 264442260 batches: 0.0202
trigger times: 1
Loss after 264573360 batches: 0.0201
trigger times: 2
Loss after 264704460 batches: 0.0199
trigger times: 3
Loss after 264835560 batches: 0.0200
trigger times: 4
Loss after 264966660 batches: 0.0199
trigger times: 5
Loss after 265097760 batches: 0.0198
trigger times: 6
Loss after 265228860 batches: 0.0198
trigger times: 7
Loss after 265359960 batches: 0.0195
trigger times: 8
Loss after 265491060 batches: 0.0194
trigger times: 9
Loss after 265622160 batches: 0.0193
trigger times: 10
Loss after 265753260 batches: 0.0192
trigger times: 11
Loss after 265884360 batches: 0.0198
trigger times: 12
Loss after 266015460 batches: 0.0194
trigger times: 13
Loss after 266146560 batches: 0.0192
trigger times: 14
Loss after 266277660 batches: 0.0191
trigger times: 15
Loss after 266408760 batches: 0.0190
trigger times: 16
Loss after 266539860 batches: 0.0186
trigger times: 17
Loss after 266670960 batches: 0.0186
trigger times: 18
Loss after 266802060 batches: 0.0187
trigger times: 19
Loss after 266933160 batches: 0.0186
trigger times: 20
Early stopping!
Start to test process.
Loss after 267064260 batches: 0.0185
Time to train on one home:  958.69376039505
trigger times: 0
Loss after 267192900 batches: 0.1614
trigger times: 0
Loss after 267321540 batches: 0.0520
trigger times: 0
Loss after 267450180 batches: 0.0383
trigger times: 1
Loss after 267578820 batches: 0.0317
trigger times: 0
Loss after 267707460 batches: 0.0279
trigger times: 1
Loss after 267836100 batches: 0.0264
trigger times: 2
Loss after 267964740 batches: 0.0250
trigger times: 3
Loss after 268093380 batches: 0.0237
trigger times: 4
Loss after 268222020 batches: 0.0228
trigger times: 0
Loss after 268350660 batches: 0.0218
trigger times: 1
Loss after 268479300 batches: 0.0210
trigger times: 2
Loss after 268607940 batches: 0.0205
trigger times: 3
Loss after 268736580 batches: 0.0195
trigger times: 4
Loss after 268865220 batches: 0.0195
trigger times: 5
Loss after 268993860 batches: 0.0193
trigger times: 6
Loss after 269122500 batches: 0.0186
trigger times: 0
Loss after 269251140 batches: 0.0179
trigger times: 0
Loss after 269379780 batches: 0.0176
trigger times: 1
Loss after 269508420 batches: 0.0176
trigger times: 2
Loss after 269637060 batches: 0.0175
trigger times: 3
Loss after 269765700 batches: 0.0173
trigger times: 4
Loss after 269894340 batches: 0.0167
trigger times: 5
Loss after 270022980 batches: 0.0167
trigger times: 6
Loss after 270151620 batches: 0.0166
trigger times: 0
Loss after 270280260 batches: 0.0169
trigger times: 0
Loss after 270408900 batches: 0.0160
trigger times: 1
Loss after 270537540 batches: 0.0159
trigger times: 2
Loss after 270666180 batches: 0.0157
trigger times: 3
Loss after 270794820 batches: 0.0156
trigger times: 4
Loss after 270923460 batches: 0.0153
trigger times: 5
Loss after 271052100 batches: 0.0155
trigger times: 6
Loss after 271180740 batches: 0.0153
trigger times: 7
Loss after 271309380 batches: 0.0149
trigger times: 8
Loss after 271438020 batches: 0.0147
trigger times: 9
Loss after 271566660 batches: 0.0148
trigger times: 10
Loss after 271695300 batches: 0.0148
trigger times: 11
Loss after 271823940 batches: 0.0145
trigger times: 12
Loss after 271952580 batches: 0.0144
trigger times: 13
Loss after 272081220 batches: 0.0142
trigger times: 14
Loss after 272209860 batches: 0.0144
trigger times: 15
Loss after 272338500 batches: 0.0143
trigger times: 16
Loss after 272467140 batches: 0.0142
trigger times: 17
Loss after 272595780 batches: 0.0140
trigger times: 18
Loss after 272724420 batches: 0.0140
trigger times: 19
Loss after 272853060 batches: 0.0140
trigger times: 20
Early stopping!
Start to test process.
Loss after 272981700 batches: 0.0139
Time to train on one home:  331.94994258880615
trigger times: 0
Loss after 273112800 batches: 0.4026
trigger times: 1
Loss after 273243900 batches: 0.1147
trigger times: 2
Loss after 273375000 batches: 0.0758
trigger times: 3
Loss after 273506100 batches: 0.0624
trigger times: 4
Loss after 273637200 batches: 0.0555
trigger times: 5
Loss after 273768300 batches: 0.0504
trigger times: 6
Loss after 273899400 batches: 0.0475
trigger times: 7
Loss after 274030500 batches: 0.0447
trigger times: 8
Loss after 274161600 batches: 0.0426
trigger times: 9
Loss after 274292700 batches: 0.0415
trigger times: 10
Loss after 274423800 batches: 0.0396
trigger times: 11
Loss after 274554900 batches: 0.0379
trigger times: 12
Loss after 274686000 batches: 0.0370
trigger times: 13
Loss after 274817100 batches: 0.0356
trigger times: 14
Loss after 274948200 batches: 0.0351
trigger times: 15
Loss after 275079300 batches: 0.0341
trigger times: 16
Loss after 275210400 batches: 0.0337
trigger times: 17
Loss after 275341500 batches: 0.0330
trigger times: 18
Loss after 275472600 batches: 0.0323
trigger times: 19
Loss after 275603700 batches: 0.0318
trigger times: 20
Early stopping!
Start to test process.
Loss after 275734800 batches: 0.0310
Time to train on one home:  160.4927041530609
trigger times: 0
Loss after 275865900 batches: 0.4816
trigger times: 1
Loss after 275997000 batches: 0.1932
trigger times: 2
Loss after 276128100 batches: 0.1173
trigger times: 3
Loss after 276259200 batches: 0.0897
trigger times: 4
Loss after 276390300 batches: 0.0726
trigger times: 5
Loss after 276521400 batches: 0.0672
trigger times: 6
Loss after 276652500 batches: 0.0588
trigger times: 7
Loss after 276783600 batches: 0.0576
trigger times: 8
Loss after 276914700 batches: 0.0525
trigger times: 9
Loss after 277045800 batches: 0.0516
trigger times: 10
Loss after 277176900 batches: 0.0491
trigger times: 11
Loss after 277308000 batches: 0.0467
trigger times: 12
Loss after 277439100 batches: 0.0458
trigger times: 13
Loss after 277570200 batches: 0.0442
trigger times: 14
Loss after 277701300 batches: 0.0413
trigger times: 0
Loss after 277832400 batches: 0.0421
trigger times: 1
Loss after 277963500 batches: 0.0413
trigger times: 0
Loss after 278094600 batches: 0.0405
trigger times: 0
Loss after 278225700 batches: 0.0399
trigger times: 1
Loss after 278356800 batches: 0.0386
trigger times: 2
Loss after 278487900 batches: 0.0377
trigger times: 3
Loss after 278619000 batches: 0.0376
trigger times: 4
Loss after 278750100 batches: 0.0370
trigger times: 5
Loss after 278881200 batches: 0.0378
trigger times: 6
Loss after 279012300 batches: 0.0368
trigger times: 7
Loss after 279143400 batches: 0.0350
trigger times: 8
Loss after 279274500 batches: 0.0345
trigger times: 9
Loss after 279405600 batches: 0.0342
trigger times: 10
Loss after 279536700 batches: 0.0343
trigger times: 11
Loss after 279667800 batches: 0.0341
trigger times: 12
Loss after 279798900 batches: 0.0343
trigger times: 13
Loss after 279930000 batches: 0.0337
trigger times: 14
Loss after 280061100 batches: 0.0329
trigger times: 15
Loss after 280192200 batches: 0.0331
trigger times: 16
Loss after 280323300 batches: 0.0333
trigger times: 17
Loss after 280454400 batches: 0.0318
trigger times: 18
Loss after 280585500 batches: 0.0324
trigger times: 0
Loss after 280716600 batches: 0.0312
trigger times: 1
Loss after 280847700 batches: 0.0309
trigger times: 2
Loss after 280978800 batches: 0.0300
trigger times: 3
Loss after 281109900 batches: 0.0304
trigger times: 4
Loss after 281241000 batches: 0.0301
trigger times: 5
Loss after 281372100 batches: 0.0311
trigger times: 6
Loss after 281503200 batches: 0.0309
trigger times: 7
Loss after 281634300 batches: 0.0293
trigger times: 8
Loss after 281765400 batches: 0.0302
trigger times: 9
Loss after 281896500 batches: 0.0302
trigger times: 10
Loss after 282027600 batches: 0.0297
trigger times: 11
Loss after 282158700 batches: 0.0299
trigger times: 12
Loss after 282289800 batches: 0.0291
trigger times: 13
Loss after 282420900 batches: 0.0305
trigger times: 14
Loss after 282552000 batches: 0.0300
trigger times: 15
Loss after 282683100 batches: 0.0287
trigger times: 0
Loss after 282814200 batches: 0.0289
trigger times: 1
Loss after 282945300 batches: 0.0281
trigger times: 0
Loss after 283076400 batches: 0.0280
trigger times: 1
Loss after 283207500 batches: 0.0271
trigger times: 2
Loss after 283338600 batches: 0.0274
trigger times: 3
Loss after 283469700 batches: 0.0276
trigger times: 4
Loss after 283600800 batches: 0.0270
trigger times: 5
Loss after 283731900 batches: 0.0270
trigger times: 6
Loss after 283863000 batches: 0.0276
trigger times: 7
Loss after 283994100 batches: 0.0273
trigger times: 8
Loss after 284125200 batches: 0.0259
trigger times: 9
Loss after 284256300 batches: 0.0265
trigger times: 10
Loss after 284387400 batches: 0.0267
trigger times: 11
Loss after 284518500 batches: 0.0261
trigger times: 12
Loss after 284649600 batches: 0.0268
trigger times: 0
Loss after 284780700 batches: 0.0257
trigger times: 1
Loss after 284911800 batches: 0.0261
trigger times: 2
Loss after 285042900 batches: 0.0261
trigger times: 3
Loss after 285174000 batches: 0.0270
trigger times: 4
Loss after 285305100 batches: 0.0265
trigger times: 5
Loss after 285436200 batches: 0.0265
trigger times: 6
Loss after 285567300 batches: 0.0251
trigger times: 7
Loss after 285698400 batches: 0.0262
trigger times: 8
Loss after 285829500 batches: 0.0265
trigger times: 9
Loss after 285960600 batches: 0.0259
trigger times: 10
Loss after 286091700 batches: 0.0255
trigger times: 11
Loss after 286222800 batches: 0.0256
trigger times: 12
Loss after 286353900 batches: 0.0252
trigger times: 13
Loss after 286485000 batches: 0.0249
trigger times: 14
Loss after 286616100 batches: 0.0252
trigger times: 15
Loss after 286747200 batches: 0.0253
trigger times: 0
Loss after 286878300 batches: 0.0260
trigger times: 1
Loss after 287009400 batches: 0.0250
trigger times: 2
Loss after 287140500 batches: 0.0250
trigger times: 3
Loss after 287271600 batches: 0.0257
trigger times: 4
Loss after 287402700 batches: 0.0245
trigger times: 5
Loss after 287533800 batches: 0.0256
trigger times: 6
Loss after 287664900 batches: 0.0250
trigger times: 7
Loss after 287796000 batches: 0.0243
trigger times: 8
Loss after 287927100 batches: 0.0228
trigger times: 9
Loss after 288058200 batches: 0.0230
trigger times: 10
Loss after 288189300 batches: 0.0251
trigger times: 11
Loss after 288320400 batches: 0.0249
trigger times: 12
Loss after 288451500 batches: 0.0236
trigger times: 13
Loss after 288582600 batches: 0.0238
trigger times: 14
Loss after 288713700 batches: 0.0242
trigger times: 15
Loss after 288844800 batches: 0.0245
trigger times: 16
Loss after 288975900 batches: 0.0237
trigger times: 17
Loss after 289107000 batches: 0.0241
trigger times: 18
Loss after 289238100 batches: 0.0225
trigger times: 19
Loss after 289369200 batches: 0.0238
trigger times: 20
Early stopping!
Start to test process.
Loss after 289500300 batches: 0.0238
Time to train on one home:  754.2810580730438
trigger times: 0
Loss after 289631400 batches: 0.1107
trigger times: 1
Loss after 289762500 batches: 0.0348
trigger times: 0
Loss after 289893600 batches: 0.0256
trigger times: 0
Loss after 290024700 batches: 0.0221
trigger times: 1
Loss after 290155800 batches: 0.0199
trigger times: 0
Loss after 290286900 batches: 0.0185
trigger times: 1
Loss after 290418000 batches: 0.0171
trigger times: 2
Loss after 290549100 batches: 0.0162
trigger times: 3
Loss after 290680200 batches: 0.0158
trigger times: 4
Loss after 290811300 batches: 0.0151
trigger times: 5
Loss after 290942400 batches: 0.0148
trigger times: 6
Loss after 291073500 batches: 0.0145
trigger times: 7
Loss after 291204600 batches: 0.0140
trigger times: 8
Loss after 291335700 batches: 0.0137
trigger times: 9
Loss after 291466800 batches: 0.0133
trigger times: 10
Loss after 291597900 batches: 0.0130
trigger times: 11
Loss after 291729000 batches: 0.0129
trigger times: 12
Loss after 291860100 batches: 0.0126
trigger times: 13
Loss after 291991200 batches: 0.0126
trigger times: 14
Loss after 292122300 batches: 0.0123
trigger times: 15
Loss after 292253400 batches: 0.0121
trigger times: 16
Loss after 292384500 batches: 0.0122
trigger times: 17
Loss after 292515600 batches: 0.0118
trigger times: 18
Loss after 292646700 batches: 0.0118
trigger times: 19
Loss after 292777800 batches: 0.0115
trigger times: 20
Early stopping!
Start to test process.
Loss after 292908900 batches: 0.0113
Time to train on one home:  195.41338276863098
trigger times: 0
Loss after 292987500 batches: 0.3969
trigger times: 0
Loss after 293066100 batches: 0.1144
trigger times: 0
Loss after 293144700 batches: 0.0659
trigger times: 0
Loss after 293223300 batches: 0.0542
trigger times: 0
Loss after 293301900 batches: 0.0465
trigger times: 0
Loss after 293380500 batches: 0.0411
trigger times: 1
Loss after 293459100 batches: 0.0390
trigger times: 0
Loss after 293537700 batches: 0.0368
trigger times: 1
Loss after 293616300 batches: 0.0348
trigger times: 2
Loss after 293694900 batches: 0.0325
trigger times: 0
Loss after 293773500 batches: 0.0316
trigger times: 0
Loss after 293852100 batches: 0.0311
trigger times: 1
Loss after 293930700 batches: 0.0307
trigger times: 2
Loss after 294009300 batches: 0.0295
trigger times: 3
Loss after 294087900 batches: 0.0288
trigger times: 4
Loss after 294166500 batches: 0.0278
trigger times: 5
Loss after 294245100 batches: 0.0267
trigger times: 6
Loss after 294323700 batches: 0.0256
trigger times: 7
Loss after 294402300 batches: 0.0260
trigger times: 8
Loss after 294480900 batches: 0.0256
trigger times: 9
Loss after 294559500 batches: 0.0253
trigger times: 10
Loss after 294638100 batches: 0.0246
trigger times: 11
Loss after 294716700 batches: 0.0246
trigger times: 0
Loss after 294795300 batches: 0.0244
trigger times: 1
Loss after 294873900 batches: 0.0238
trigger times: 2
Loss after 294952500 batches: 0.0233
trigger times: 3
Loss after 295031100 batches: 0.0234
trigger times: 4
Loss after 295109700 batches: 0.0232
trigger times: 5
Loss after 295188300 batches: 0.0232
trigger times: 6
Loss after 295266900 batches: 0.0227
trigger times: 7
Loss after 295345500 batches: 0.0225
trigger times: 8
Loss after 295424100 batches: 0.0221
trigger times: 9
Loss after 295502700 batches: 0.0216
trigger times: 10
Loss after 295581300 batches: 0.0218
trigger times: 11
Loss after 295659900 batches: 0.0208
trigger times: 12
Loss after 295738500 batches: 0.0209
trigger times: 13
Loss after 295817100 batches: 0.0209
trigger times: 14
Loss after 295895700 batches: 0.0209
trigger times: 0
Loss after 295974300 batches: 0.0208
trigger times: 1
Loss after 296052900 batches: 0.0204
trigger times: 2
Loss after 296131500 batches: 0.0205
trigger times: 3
Loss after 296210100 batches: 0.0200
trigger times: 4
Loss after 296288700 batches: 0.0205
trigger times: 5
Loss after 296367300 batches: 0.0195
trigger times: 6
Loss after 296445900 batches: 0.0190
trigger times: 7
Loss after 296524500 batches: 0.0196
trigger times: 8
Loss after 296603100 batches: 0.0199
trigger times: 9
Loss after 296681700 batches: 0.0200
trigger times: 10
Loss after 296760300 batches: 0.0197
trigger times: 11
Loss after 296838900 batches: 0.0193
trigger times: 12
Loss after 296917500 batches: 0.0190
trigger times: 13
Loss after 296996100 batches: 0.0189
trigger times: 14
Loss after 297074700 batches: 0.0185
trigger times: 15
Loss after 297153300 batches: 0.0187
trigger times: 16
Loss after 297231900 batches: 0.0182
trigger times: 17
Loss after 297310500 batches: 0.0187
trigger times: 18
Loss after 297389100 batches: 0.0184
trigger times: 0
Loss after 297467700 batches: 0.0183
trigger times: 0
Loss after 297546300 batches: 0.0177
trigger times: 1
Loss after 297624900 batches: 0.0181
trigger times: 2
Loss after 297703500 batches: 0.0183
trigger times: 3
Loss after 297782100 batches: 0.0179
trigger times: 4
Loss after 297860700 batches: 0.0179
trigger times: 5
Loss after 297939300 batches: 0.0176
trigger times: 6
Loss after 298017900 batches: 0.0177
trigger times: 7
Loss after 298096500 batches: 0.0173
trigger times: 8
Loss after 298175100 batches: 0.0178
trigger times: 9
Loss after 298253700 batches: 0.0178
trigger times: 10
Loss after 298332300 batches: 0.0174
trigger times: 11
Loss after 298410900 batches: 0.0172
trigger times: 12
Loss after 298489500 batches: 0.0173
trigger times: 13
Loss after 298568100 batches: 0.0167
trigger times: 14
Loss after 298646700 batches: 0.0165
trigger times: 15
Loss after 298725300 batches: 0.0168
trigger times: 16
Loss after 298803900 batches: 0.0165
trigger times: 17
Loss after 298882500 batches: 0.0170
trigger times: 18
Loss after 298961100 batches: 0.0168
trigger times: 19
Loss after 299039700 batches: 0.0170
trigger times: 20
Early stopping!
Start to test process.
Loss after 299118300 batches: 0.0169
Time to train on one home:  372.1148934364319
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353]]
Round_6_results:  [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353]
trigger times: 0
Loss after 299249400 batches: 0.1383
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2458 < 2459; dropping {'Training_Loss': 0.13831979451033305, 'Validation_Loss': 0.23448357151614296, 'Training_R2': 0.8606477980007369, 'Validation_R2': 0.7820682266293595, 'Training_F1': 0.8015925644633326, 'Validation_F1': 0.7272769799584357, 'Training_NEP': 0.39651225707080556, 'Validation_NEP': 0.5135463081126105, 'Training_NDE': 0.10461417154626568, 'Validation_NDE': 0.17354937623300254, 'Training_MAE': 13.131699585583627, 'Validation_MAE': 14.083570341568267, 'Training_MSE': 460.28595, 'Validation_MSE': 640.9131}.
trigger times: 0
Loss after 299380500 batches: 0.0363
trigger times: 0
Loss after 299511600 batches: 0.0273
trigger times: 0
Loss after 299642700 batches: 0.0236
trigger times: 0
Loss after 299773800 batches: 0.0213
trigger times: 1
Loss after 299904900 batches: 0.0196
trigger times: 2
Loss after 300036000 batches: 0.0181
trigger times: 3
Loss after 300167100 batches: 0.0178
trigger times: 0
Loss after 300298200 batches: 0.0170
trigger times: 1
Loss after 300429300 batches: 0.0164
trigger times: 2
Loss after 300560400 batches: 0.0158
trigger times: 3
Loss after 300691500 batches: 0.0157
trigger times: 4
Loss after 300822600 batches: 0.0152
trigger times: 5
Loss after 300953700 batches: 0.0147
trigger times: 6
Loss after 301084800 batches: 0.0143
trigger times: 7
Loss after 301215900 batches: 0.0143
trigger times: 8
Loss after 301347000 batches: 0.0137
trigger times: 9
Loss after 301478100 batches: 0.0139
trigger times: 10
Loss after 301609200 batches: 0.0134
trigger times: 11
Loss after 301740300 batches: 0.0133
trigger times: 12
Loss after 301871400 batches: 0.0128
trigger times: 13
Loss after 302002500 batches: 0.0127
trigger times: 0
Loss after 302133600 batches: 0.0127
trigger times: 1
Loss after 302264700 batches: 0.0125
trigger times: 2
Loss after 302395800 batches: 0.0125
trigger times: 3
Loss after 302526900 batches: 0.0123
trigger times: 4
Loss after 302658000 batches: 0.0123
trigger times: 5
Loss after 302789100 batches: 0.0121
trigger times: 6
Loss after 302920200 batches: 0.0122
trigger times: 7
Loss after 303051300 batches: 0.0118
trigger times: 8
Loss after 303182400 batches: 0.0115
trigger times: 9
Loss after 303313500 batches: 0.0117
trigger times: 10
Loss after 303444600 batches: 0.0114
trigger times: 11
Loss after 303575700 batches: 0.0113
trigger times: 12
Loss after 303706800 batches: 0.0113
trigger times: 13
Loss after 303837900 batches: 0.0111
trigger times: 14
Loss after 303969000 batches: 0.0111
trigger times: 15
Loss after 304100100 batches: 0.0111
trigger times: 16
Loss after 304231200 batches: 0.0110
trigger times: 17
Loss after 304362300 batches: 0.0108
trigger times: 18
Loss after 304493400 batches: 0.0108
trigger times: 19
Loss after 304624500 batches: 0.0106
trigger times: 20
Early stopping!
Start to test process.
Loss after 304755600 batches: 0.0107
Time to train on one home:  315.81822967529297
trigger times: 0
Loss after 304858200 batches: 0.6817
trigger times: 0
Loss after 304960800 batches: 0.3246
trigger times: 1
Loss after 305063400 batches: 0.2195
trigger times: 2
Loss after 305166000 batches: 0.1590
trigger times: 3
Loss after 305268600 batches: 0.1279
trigger times: 4
Loss after 305371200 batches: 0.1096
trigger times: 5
Loss after 305473800 batches: 0.0965
trigger times: 6
Loss after 305576400 batches: 0.0905
trigger times: 7
Loss after 305679000 batches: 0.0828
trigger times: 8
Loss after 305781600 batches: 0.0758
trigger times: 9
Loss after 305884200 batches: 0.0687
trigger times: 10
Loss after 305986800 batches: 0.0631
trigger times: 11
Loss after 306089400 batches: 0.0597
trigger times: 12
Loss after 306192000 batches: 0.0588
trigger times: 13
Loss after 306294600 batches: 0.0542
trigger times: 14
Loss after 306397200 batches: 0.0522
trigger times: 15
Loss after 306499800 batches: 0.0528
trigger times: 16
Loss after 306602400 batches: 0.0488
trigger times: 17
Loss after 306705000 batches: 0.0467
trigger times: 18
Loss after 306807600 batches: 0.0432
trigger times: 19
Loss after 306910200 batches: 0.0447
trigger times: 20
Early stopping!
Start to test process.
Loss after 307012800 batches: 0.0445
Time to train on one home:  136.2525451183319
trigger times: 0
Loss after 307143900 batches: 0.1841
trigger times: 1
Loss after 307275000 batches: 0.0600
trigger times: 2
Loss after 307406100 batches: 0.0440
trigger times: 3
Loss after 307537200 batches: 0.0364
trigger times: 4
Loss after 307668300 batches: 0.0329
trigger times: 5
Loss after 307799400 batches: 0.0306
trigger times: 6
Loss after 307930500 batches: 0.0289
trigger times: 7
Loss after 308061600 batches: 0.0272
trigger times: 8
Loss after 308192700 batches: 0.0261
trigger times: 9
Loss after 308323800 batches: 0.0251
trigger times: 10
Loss after 308454900 batches: 0.0240
trigger times: 11
Loss after 308586000 batches: 0.0235
trigger times: 12
Loss after 308717100 batches: 0.0230
trigger times: 13
Loss after 308848200 batches: 0.0225
trigger times: 14
Loss after 308979300 batches: 0.0221
trigger times: 15
Loss after 309110400 batches: 0.0216
trigger times: 16
Loss after 309241500 batches: 0.0209
trigger times: 17
Loss after 309372600 batches: 0.0208
trigger times: 18
Loss after 309503700 batches: 0.0207
trigger times: 19
Loss after 309634800 batches: 0.0206
trigger times: 20
Early stopping!
Start to test process.
Loss after 309765900 batches: 0.0199
Time to train on one home:  160.23105096817017
trigger times: 0
Loss after 309897000 batches: 0.3403
trigger times: 1
Loss after 310028100 batches: 0.1074
trigger times: 0
Loss after 310159200 batches: 0.0716
trigger times: 1
Loss after 310290300 batches: 0.0591
trigger times: 2
Loss after 310421400 batches: 0.0523
trigger times: 3
Loss after 310552500 batches: 0.0479
trigger times: 0
Loss after 310683600 batches: 0.0443
trigger times: 1
Loss after 310814700 batches: 0.0424
trigger times: 2
Loss after 310945800 batches: 0.0400
trigger times: 3
Loss after 311076900 batches: 0.0385
trigger times: 4
Loss after 311208000 batches: 0.0371
trigger times: 5
Loss after 311339100 batches: 0.0350
trigger times: 6
Loss after 311470200 batches: 0.0345
trigger times: 7
Loss after 311601300 batches: 0.0336
trigger times: 8
Loss after 311732400 batches: 0.0325
trigger times: 9
Loss after 311863500 batches: 0.0317
trigger times: 10
Loss after 311994600 batches: 0.0318
trigger times: 11
Loss after 312125700 batches: 0.0310
trigger times: 12
Loss after 312256800 batches: 0.0304
trigger times: 13
Loss after 312387900 batches: 0.0300
trigger times: 14
Loss after 312519000 batches: 0.0292
trigger times: 15
Loss after 312650100 batches: 0.0284
trigger times: 16
Loss after 312781200 batches: 0.0284
trigger times: 17
Loss after 312912300 batches: 0.0279
trigger times: 18
Loss after 313043400 batches: 0.0274
trigger times: 19
Loss after 313174500 batches: 0.0275
trigger times: 20
Early stopping!
Start to test process.
Loss after 313305600 batches: 0.0271
Time to train on one home:  202.67030501365662
trigger times: 0
Loss after 313434240 batches: 0.1516
trigger times: 0
Loss after 313562880 batches: 0.0455
trigger times: 1
Loss after 313691520 batches: 0.0330
trigger times: 0
Loss after 313820160 batches: 0.0280
trigger times: 0
Loss after 313948800 batches: 0.0251
trigger times: 1
Loss after 314077440 batches: 0.0234
trigger times: 2
Loss after 314206080 batches: 0.0220
trigger times: 3
Loss after 314334720 batches: 0.0211
trigger times: 4
Loss after 314463360 batches: 0.0202
trigger times: 5
Loss after 314592000 batches: 0.0193
trigger times: 6
Loss after 314720640 batches: 0.0192
trigger times: 7
Loss after 314849280 batches: 0.0179
trigger times: 8
Loss after 314977920 batches: 0.0177
trigger times: 9
Loss after 315106560 batches: 0.0172
trigger times: 10
Loss after 315235200 batches: 0.0173
trigger times: 11
Loss after 315363840 batches: 0.0168
trigger times: 12
Loss after 315492480 batches: 0.0163
trigger times: 13
Loss after 315621120 batches: 0.0160
trigger times: 14
Loss after 315749760 batches: 0.0160
trigger times: 15
Loss after 315878400 batches: 0.0159
trigger times: 16
Loss after 316007040 batches: 0.0153
trigger times: 17
Loss after 316135680 batches: 0.0155
trigger times: 18
Loss after 316264320 batches: 0.0150
trigger times: 19
Loss after 316392960 batches: 0.0152
trigger times: 20
Early stopping!
Start to test process.
Loss after 316521600 batches: 0.0153
Time to train on one home:  185.33495545387268
trigger times: 0
Loss after 316652700 batches: 0.3914
trigger times: 1
Loss after 316783800 batches: 0.1041
trigger times: 2
Loss after 316914900 batches: 0.0701
trigger times: 3
Loss after 317046000 batches: 0.0583
trigger times: 4
Loss after 317177100 batches: 0.0512
trigger times: 5
Loss after 317308200 batches: 0.0483
trigger times: 6
Loss after 317439300 batches: 0.0451
trigger times: 7
Loss after 317570400 batches: 0.0425
trigger times: 8
Loss after 317701500 batches: 0.0407
trigger times: 9
Loss after 317832600 batches: 0.0387
trigger times: 10
Loss after 317963700 batches: 0.0377
trigger times: 11
Loss after 318094800 batches: 0.0363
trigger times: 12
Loss after 318225900 batches: 0.0353
trigger times: 0
Loss after 318357000 batches: 0.0340
trigger times: 1
Loss after 318488100 batches: 0.0336
trigger times: 2
Loss after 318619200 batches: 0.0332
trigger times: 0
Loss after 318750300 batches: 0.0322
trigger times: 1
Loss after 318881400 batches: 0.0322
trigger times: 0
Loss after 319012500 batches: 0.0313
trigger times: 1
Loss after 319143600 batches: 0.0305
trigger times: 2
Loss after 319274700 batches: 0.0298
trigger times: 3
Loss after 319405800 batches: 0.0297
trigger times: 0
Loss after 319536900 batches: 0.0290
trigger times: 1
Loss after 319668000 batches: 0.0291
trigger times: 2
Loss after 319799100 batches: 0.0283
trigger times: 0
Loss after 319930200 batches: 0.0281
trigger times: 1
Loss after 320061300 batches: 0.0282
trigger times: 2
Loss after 320192400 batches: 0.0273
trigger times: 3
Loss after 320323500 batches: 0.0271
trigger times: 4
Loss after 320454600 batches: 0.0270
trigger times: 5
Loss after 320585700 batches: 0.0264
trigger times: 6
Loss after 320716800 batches: 0.0262
trigger times: 7
Loss after 320847900 batches: 0.0268
trigger times: 8
Loss after 320979000 batches: 0.0259
trigger times: 9
Loss after 321110100 batches: 0.0257
trigger times: 10
Loss after 321241200 batches: 0.0256
trigger times: 0
Loss after 321372300 batches: 0.0253
trigger times: 1
Loss after 321503400 batches: 0.0252
trigger times: 2
Loss after 321634500 batches: 0.0250
trigger times: 3
Loss after 321765600 batches: 0.0253
trigger times: 4
Loss after 321896700 batches: 0.0246
trigger times: 5
Loss after 322027800 batches: 0.0242
trigger times: 6
Loss after 322158900 batches: 0.0240
trigger times: 0
Loss after 322290000 batches: 0.0238
trigger times: 1
Loss after 322421100 batches: 0.0241
trigger times: 2
Loss after 322552200 batches: 0.0236
trigger times: 3
Loss after 322683300 batches: 0.0234
trigger times: 4
Loss after 322814400 batches: 0.0232
trigger times: 5
Loss after 322945500 batches: 0.0230
trigger times: 6
Loss after 323076600 batches: 0.0233
trigger times: 7
Loss after 323207700 batches: 0.0228
trigger times: 8
Loss after 323338800 batches: 0.0227
trigger times: 9
Loss after 323469900 batches: 0.0224
trigger times: 10
Loss after 323601000 batches: 0.0225
trigger times: 11
Loss after 323732100 batches: 0.0223
trigger times: 12
Loss after 323863200 batches: 0.0223
trigger times: 13
Loss after 323994300 batches: 0.0220
trigger times: 14
Loss after 324125400 batches: 0.0219
trigger times: 15
Loss after 324256500 batches: 0.0220
trigger times: 16
Loss after 324387600 batches: 0.0216
trigger times: 17
Loss after 324518700 batches: 0.0215
trigger times: 18
Loss after 324649800 batches: 0.0215
trigger times: 0
Loss after 324780900 batches: 0.0215
trigger times: 1
Loss after 324912000 batches: 0.0213
trigger times: 2
Loss after 325043100 batches: 0.0209
trigger times: 0
Loss after 325174200 batches: 0.0212
trigger times: 1
Loss after 325305300 batches: 0.0210
trigger times: 2
Loss after 325436400 batches: 0.0209
trigger times: 3
Loss after 325567500 batches: 0.0208
trigger times: 4
Loss after 325698600 batches: 0.0208
trigger times: 5
Loss after 325829700 batches: 0.0205
trigger times: 6
Loss after 325960800 batches: 0.0204
trigger times: 7
Loss after 326091900 batches: 0.0204
trigger times: 8
Loss after 326223000 batches: 0.0202
trigger times: 9
Loss after 326354100 batches: 0.0202
trigger times: 10
Loss after 326485200 batches: 0.0201
trigger times: 11
Loss after 326616300 batches: 0.0199
trigger times: 12
Loss after 326747400 batches: 0.0202
trigger times: 13
Loss after 326878500 batches: 0.0199
trigger times: 14
Loss after 327009600 batches: 0.0198
trigger times: 15
Loss after 327140700 batches: 0.0197
trigger times: 16
Loss after 327271800 batches: 0.0197
trigger times: 17
Loss after 327402900 batches: 0.0200
trigger times: 0
Loss after 327534000 batches: 0.0194
trigger times: 1
Loss after 327665100 batches: 0.0197
trigger times: 2
Loss after 327796200 batches: 0.0194
trigger times: 3
Loss after 327927300 batches: 0.0192
trigger times: 4
Loss after 328058400 batches: 0.0192
trigger times: 5
Loss after 328189500 batches: 0.0191
trigger times: 6
Loss after 328320600 batches: 0.0193
trigger times: 7
Loss after 328451700 batches: 0.0192
trigger times: 8
Loss after 328582800 batches: 0.0188
trigger times: 9
Loss after 328713900 batches: 0.0191
trigger times: 10
Loss after 328845000 batches: 0.0190
trigger times: 11
Loss after 328976100 batches: 0.0194
trigger times: 12
Loss after 329107200 batches: 0.0188
trigger times: 13
Loss after 329238300 batches: 0.0189
trigger times: 14
Loss after 329369400 batches: 0.0188
trigger times: 15
Loss after 329500500 batches: 0.0186
trigger times: 16
Loss after 329631600 batches: 0.0185
trigger times: 0
Loss after 329762700 batches: 0.0186
trigger times: 1
Loss after 329893800 batches: 0.0183
trigger times: 2
Loss after 330024900 batches: 0.0186
trigger times: 3
Loss after 330156000 batches: 0.0182
trigger times: 4
Loss after 330287100 batches: 0.0188
trigger times: 5
Loss after 330418200 batches: 0.0188
trigger times: 6
Loss after 330549300 batches: 0.0182
trigger times: 7
Loss after 330680400 batches: 0.0182
trigger times: 8
Loss after 330811500 batches: 0.0182
trigger times: 9
Loss after 330942600 batches: 0.0179
trigger times: 10
Loss after 331073700 batches: 0.0182
trigger times: 11
Loss after 331204800 batches: 0.0179
trigger times: 12
Loss after 331335900 batches: 0.0178
trigger times: 0
Loss after 331467000 batches: 0.0180
trigger times: 1
Loss after 331598100 batches: 0.0177
trigger times: 2
Loss after 331729200 batches: 0.0178
trigger times: 3
Loss after 331860300 batches: 0.0177
trigger times: 4
Loss after 331991400 batches: 0.0175
trigger times: 5
Loss after 332122500 batches: 0.0175
trigger times: 6
Loss after 332253600 batches: 0.0176
trigger times: 7
Loss after 332384700 batches: 0.0176
trigger times: 8
Loss after 332515800 batches: 0.0175
trigger times: 9
Loss after 332646900 batches: 0.0172
trigger times: 10
Loss after 332778000 batches: 0.0171
trigger times: 11
Loss after 332909100 batches: 0.0170
trigger times: 12
Loss after 333040200 batches: 0.0174
trigger times: 13
Loss after 333171300 batches: 0.0172
trigger times: 14
Loss after 333302400 batches: 0.0172
trigger times: 15
Loss after 333433500 batches: 0.0173
trigger times: 16
Loss after 333564600 batches: 0.0170
trigger times: 17
Loss after 333695700 batches: 0.0170
trigger times: 18
Loss after 333826800 batches: 0.0167
trigger times: 19
Loss after 333957900 batches: 0.0167
trigger times: 20
Early stopping!
Start to test process.
Loss after 334089000 batches: 0.0168
Time to train on one home:  960.167008638382
trigger times: 0
Loss after 334220100 batches: 0.4691
trigger times: 0
Loss after 334351200 batches: 0.1775
trigger times: 1
Loss after 334482300 batches: 0.1037
trigger times: 2
Loss after 334613400 batches: 0.0757
trigger times: 0
Loss after 334744500 batches: 0.0651
trigger times: 1
Loss after 334875600 batches: 0.0572
trigger times: 0
Loss after 335006700 batches: 0.0529
trigger times: 0
Loss after 335137800 batches: 0.0480
trigger times: 0
Loss after 335268900 batches: 0.0487
trigger times: 0
Loss after 335400000 batches: 0.0437
trigger times: 0
Loss after 335531100 batches: 0.0417
trigger times: 1
Loss after 335662200 batches: 0.0412
trigger times: 2
Loss after 335793300 batches: 0.0411
trigger times: 0
Loss after 335924400 batches: 0.0393
trigger times: 1
Loss after 336055500 batches: 0.0376
trigger times: 0
Loss after 336186600 batches: 0.0381
trigger times: 1
Loss after 336317700 batches: 0.0365
trigger times: 2
Loss after 336448800 batches: 0.0362
trigger times: 0
Loss after 336579900 batches: 0.0354
trigger times: 0
Loss after 336711000 batches: 0.0337
trigger times: 1
Loss after 336842100 batches: 0.0331
trigger times: 2
Loss after 336973200 batches: 0.0330
trigger times: 0
Loss after 337104300 batches: 0.0336
trigger times: 1
Loss after 337235400 batches: 0.0327
trigger times: 2
Loss after 337366500 batches: 0.0322
trigger times: 3
Loss after 337497600 batches: 0.0323
trigger times: 4
Loss after 337628700 batches: 0.0316
trigger times: 5
Loss after 337759800 batches: 0.0311
trigger times: 6
Loss after 337890900 batches: 0.0317
trigger times: 7
Loss after 338022000 batches: 0.0307
trigger times: 8
Loss after 338153100 batches: 0.0311
trigger times: 9
Loss after 338284200 batches: 0.0307
trigger times: 10
Loss after 338415300 batches: 0.0305
trigger times: 11
Loss after 338546400 batches: 0.0288
trigger times: 12
Loss after 338677500 batches: 0.0289
trigger times: 13
Loss after 338808600 batches: 0.0305
trigger times: 14
Loss after 338939700 batches: 0.0301
trigger times: 15
Loss after 339070800 batches: 0.0290
trigger times: 16
Loss after 339201900 batches: 0.0289
trigger times: 17
Loss after 339333000 batches: 0.0273
trigger times: 0
Loss after 339464100 batches: 0.0278
trigger times: 1
Loss after 339595200 batches: 0.0280
trigger times: 0
Loss after 339726300 batches: 0.0272
trigger times: 1
Loss after 339857400 batches: 0.0270
trigger times: 2
Loss after 339988500 batches: 0.0264
trigger times: 3
Loss after 340119600 batches: 0.0263
trigger times: 4
Loss after 340250700 batches: 0.0264
trigger times: 5
Loss after 340381800 batches: 0.0263
trigger times: 6
Loss after 340512900 batches: 0.0264
trigger times: 7
Loss after 340644000 batches: 0.0261
trigger times: 8
Loss after 340775100 batches: 0.0262
trigger times: 9
Loss after 340906200 batches: 0.0271
trigger times: 10
Loss after 341037300 batches: 0.0264
trigger times: 11
Loss after 341168400 batches: 0.0266
trigger times: 12
Loss after 341299500 batches: 0.0257
trigger times: 13
Loss after 341430600 batches: 0.0260
trigger times: 14
Loss after 341561700 batches: 0.0250
trigger times: 15
Loss after 341692800 batches: 0.0249
trigger times: 16
Loss after 341823900 batches: 0.0245
trigger times: 0
Loss after 341955000 batches: 0.0254
trigger times: 1
Loss after 342086100 batches: 0.0257
trigger times: 0
Loss after 342217200 batches: 0.0244
trigger times: 1
Loss after 342348300 batches: 0.0250
trigger times: 2
Loss after 342479400 batches: 0.0243
trigger times: 3
Loss after 342610500 batches: 0.0243
trigger times: 4
Loss after 342741600 batches: 0.0244
trigger times: 5
Loss after 342872700 batches: 0.0251
trigger times: 6
Loss after 343003800 batches: 0.0249
trigger times: 7
Loss after 343134900 batches: 0.0247
trigger times: 8
Loss after 343266000 batches: 0.0242
trigger times: 9
Loss after 343397100 batches: 0.0248
trigger times: 10
Loss after 343528200 batches: 0.0232
trigger times: 11
Loss after 343659300 batches: 0.0233
trigger times: 12
Loss after 343790400 batches: 0.0232
trigger times: 13
Loss after 343921500 batches: 0.0223
trigger times: 14
Loss after 344052600 batches: 0.0230
trigger times: 15
Loss after 344183700 batches: 0.0235
trigger times: 16
Loss after 344314800 batches: 0.0237
trigger times: 17
Loss after 344445900 batches: 0.0242
trigger times: 18
Loss after 344577000 batches: 0.0236
trigger times: 19
Loss after 344708100 batches: 0.0234
trigger times: 20
Early stopping!
Start to test process.
Loss after 344839200 batches: 0.0218
Time to train on one home:  591.7012872695923
trigger times: 0
Loss after 344970300 batches: 0.1112
trigger times: 0
Loss after 345101400 batches: 0.0323
trigger times: 0
Loss after 345232500 batches: 0.0237
trigger times: 0
Loss after 345363600 batches: 0.0205
trigger times: 1
Loss after 345494700 batches: 0.0187
trigger times: 0
Loss after 345625800 batches: 0.0173
trigger times: 1
Loss after 345756900 batches: 0.0166
trigger times: 0
Loss after 345888000 batches: 0.0155
trigger times: 1
Loss after 346019100 batches: 0.0151
trigger times: 0
Loss after 346150200 batches: 0.0146
trigger times: 0
Loss after 346281300 batches: 0.0140
trigger times: 1
Loss after 346412400 batches: 0.0138
trigger times: 2
Loss after 346543500 batches: 0.0135
trigger times: 3
Loss after 346674600 batches: 0.0129
trigger times: 4
Loss after 346805700 batches: 0.0125
trigger times: 5
Loss after 346936800 batches: 0.0125
trigger times: 0
Loss after 347067900 batches: 0.0123
trigger times: 1
Loss after 347199000 batches: 0.0119
trigger times: 2
Loss after 347330100 batches: 0.0119
trigger times: 3
Loss after 347461200 batches: 0.0116
trigger times: 4
Loss after 347592300 batches: 0.0115
trigger times: 5
Loss after 347723400 batches: 0.0113
trigger times: 6
Loss after 347854500 batches: 0.0112
trigger times: 7
Loss after 347985600 batches: 0.0112
trigger times: 8
Loss after 348116700 batches: 0.0110
trigger times: 0
Loss after 348247800 batches: 0.0108
trigger times: 1
Loss after 348378900 batches: 0.0105
trigger times: 2
Loss after 348510000 batches: 0.0107
trigger times: 3
Loss after 348641100 batches: 0.0107
trigger times: 4
Loss after 348772200 batches: 0.0103
trigger times: 5
Loss after 348903300 batches: 0.0100
trigger times: 6
Loss after 349034400 batches: 0.0100
trigger times: 7
Loss after 349165500 batches: 0.0101
trigger times: 8
Loss after 349296600 batches: 0.0101
trigger times: 9
Loss after 349427700 batches: 0.0099
trigger times: 0
Loss after 349558800 batches: 0.0099
trigger times: 1
Loss after 349689900 batches: 0.0097
trigger times: 2
Loss after 349821000 batches: 0.0097
trigger times: 3
Loss after 349952100 batches: 0.0096
trigger times: 4
Loss after 350083200 batches: 0.0095
trigger times: 5
Loss after 350214300 batches: 0.0095
trigger times: 0
Loss after 350345400 batches: 0.0092
trigger times: 1
Loss after 350476500 batches: 0.0092
trigger times: 2
Loss after 350607600 batches: 0.0093
trigger times: 3
Loss after 350738700 batches: 0.0093
trigger times: 0
Loss after 350869800 batches: 0.0094
trigger times: 1
Loss after 351000900 batches: 0.0093
trigger times: 2
Loss after 351132000 batches: 0.0089
trigger times: 3
Loss after 351263100 batches: 0.0089
trigger times: 4
Loss after 351394200 batches: 0.0088
trigger times: 0
Loss after 351525300 batches: 0.0087
trigger times: 1
Loss after 351656400 batches: 0.0089
trigger times: 2
Loss after 351787500 batches: 0.0087
trigger times: 3
Loss after 351918600 batches: 0.0088
trigger times: 4
Loss after 352049700 batches: 0.0089
trigger times: 5
Loss after 352180800 batches: 0.0088
trigger times: 0
Loss after 352311900 batches: 0.0085
trigger times: 1
Loss after 352443000 batches: 0.0085
trigger times: 2
Loss after 352574100 batches: 0.0084
trigger times: 3
Loss after 352705200 batches: 0.0086
trigger times: 4
Loss after 352836300 batches: 0.0082
trigger times: 0
Loss after 352967400 batches: 0.0083
trigger times: 1
Loss after 353098500 batches: 0.0082
trigger times: 2
Loss after 353229600 batches: 0.0084
trigger times: 3
Loss after 353360700 batches: 0.0083
trigger times: 4
Loss after 353491800 batches: 0.0085
trigger times: 0
Loss after 353622900 batches: 0.0083
trigger times: 1
Loss after 353754000 batches: 0.0082
trigger times: 0
Loss after 353885100 batches: 0.0081
trigger times: 1
Loss after 354016200 batches: 0.0080
trigger times: 2
Loss after 354147300 batches: 0.0079
trigger times: 3
Loss after 354278400 batches: 0.0080
trigger times: 4
Loss after 354409500 batches: 0.0080
trigger times: 5
Loss after 354540600 batches: 0.0079
trigger times: 6
Loss after 354671700 batches: 0.0079
trigger times: 7
Loss after 354802800 batches: 0.0077
trigger times: 8
Loss after 354933900 batches: 0.0080
trigger times: 9
Loss after 355065000 batches: 0.0078
trigger times: 10
Loss after 355196100 batches: 0.0076
trigger times: 11
Loss after 355327200 batches: 0.0074
trigger times: 12
Loss after 355458300 batches: 0.0076
trigger times: 13
Loss after 355589400 batches: 0.0077
trigger times: 14
Loss after 355720500 batches: 0.0077
trigger times: 15
Loss after 355851600 batches: 0.0076
trigger times: 16
Loss after 355982700 batches: 0.0076
trigger times: 17
Loss after 356113800 batches: 0.0076
trigger times: 18
Loss after 356244900 batches: 0.0075
trigger times: 19
Loss after 356376000 batches: 0.0073
trigger times: 20
Early stopping!
Start to test process.
Loss after 356507100 batches: 0.0074
Time to train on one home:  640.4733936786652
trigger times: 0
Loss after 356585700 batches: 0.3103
trigger times: 0
Loss after 356664300 batches: 0.0867
trigger times: 1
Loss after 356742900 batches: 0.0533
trigger times: 0
Loss after 356821500 batches: 0.0431
trigger times: 1
Loss after 356900100 batches: 0.0378
trigger times: 2
Loss after 356978700 batches: 0.0337
trigger times: 0
Loss after 357057300 batches: 0.0322
trigger times: 0
Loss after 357135900 batches: 0.0298
trigger times: 1
Loss after 357214500 batches: 0.0290
trigger times: 2
Loss after 357293100 batches: 0.0274
trigger times: 3
Loss after 357371700 batches: 0.0265
trigger times: 4
Loss after 357450300 batches: 0.0260
trigger times: 5
Loss after 357528900 batches: 0.0247
trigger times: 0
Loss after 357607500 batches: 0.0248
trigger times: 1
Loss after 357686100 batches: 0.0241
trigger times: 2
Loss after 357764700 batches: 0.0237
trigger times: 3
Loss after 357843300 batches: 0.0232
trigger times: 0
Loss after 357921900 batches: 0.0231
trigger times: 1
Loss after 358000500 batches: 0.0226
trigger times: 2
Loss after 358079100 batches: 0.0226
trigger times: 3
Loss after 358157700 batches: 0.0217
trigger times: 4
Loss after 358236300 batches: 0.0218
trigger times: 5
Loss after 358314900 batches: 0.0216
trigger times: 6
Loss after 358393500 batches: 0.0205
trigger times: 7
Loss after 358472100 batches: 0.0208
trigger times: 0
Loss after 358550700 batches: 0.0200
trigger times: 1
Loss after 358629300 batches: 0.0196
trigger times: 2
Loss after 358707900 batches: 0.0200
trigger times: 3
Loss after 358786500 batches: 0.0189
trigger times: 4
Loss after 358865100 batches: 0.0194
trigger times: 5
Loss after 358943700 batches: 0.0187
trigger times: 6
Loss after 359022300 batches: 0.0192
trigger times: 7
Loss after 359100900 batches: 0.0192
trigger times: 8
Loss after 359179500 batches: 0.0187
trigger times: 9
Loss after 359258100 batches: 0.0182
trigger times: 10
Loss after 359336700 batches: 0.0181
trigger times: 11
Loss after 359415300 batches: 0.0181
trigger times: 12
Loss after 359493900 batches: 0.0181
trigger times: 13
Loss after 359572500 batches: 0.0181
trigger times: 14
Loss after 359651100 batches: 0.0176
trigger times: 15
Loss after 359729700 batches: 0.0181
trigger times: 16
Loss after 359808300 batches: 0.0176
trigger times: 17
Loss after 359886900 batches: 0.0173
trigger times: 0
Loss after 359965500 batches: 0.0173
trigger times: 1
Loss after 360044100 batches: 0.0169
trigger times: 2
Loss after 360122700 batches: 0.0173
trigger times: 3
Loss after 360201300 batches: 0.0179
trigger times: 4
Loss after 360279900 batches: 0.0172
trigger times: 0
Loss after 360358500 batches: 0.0168
trigger times: 1
Loss after 360437100 batches: 0.0171
trigger times: 2
Loss after 360515700 batches: 0.0164
trigger times: 3
Loss after 360594300 batches: 0.0165
trigger times: 4
Loss after 360672900 batches: 0.0166
trigger times: 5
Loss after 360751500 batches: 0.0170
trigger times: 6
Loss after 360830100 batches: 0.0164
trigger times: 7
Loss after 360908700 batches: 0.0160
trigger times: 8
Loss after 360987300 batches: 0.0161
trigger times: 9
Loss after 361065900 batches: 0.0163
trigger times: 10
Loss after 361144500 batches: 0.0160
trigger times: 11
Loss after 361223100 batches: 0.0163
trigger times: 12
Loss after 361301700 batches: 0.0161
trigger times: 13
Loss after 361380300 batches: 0.0159
trigger times: 14
Loss after 361458900 batches: 0.0160
trigger times: 15
Loss after 361537500 batches: 0.0162
trigger times: 16
Loss after 361616100 batches: 0.0158
trigger times: 17
Loss after 361694700 batches: 0.0156
trigger times: 18
Loss after 361773300 batches: 0.0155
trigger times: 19
Loss after 361851900 batches: 0.0155
trigger times: 20
Early stopping!
Start to test process.
Loss after 361930500 batches: 0.0153
Time to train on one home:  326.3520634174347
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983]]
Round_7_results:  [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2970 < 2971; dropping {'Training_Loss': 0.13276003926711263, 'Validation_Loss': 0.21394949158032736, 'Training_R2': 0.8663886035116308, 'Validation_R2': 0.8010654312048014, 'Training_F1': 0.8074649458065476, 'Validation_F1': 0.7414636959064421, 'Training_NEP': 0.3847155779441941, 'Validation_NEP': 0.4871478540514721, 'Training_NDE': 0.10030444694978168, 'Validation_NDE': 0.1584210039298436, 'Training_MAE': 12.74101696824773, 'Validation_MAE': 13.359615210734814, 'Training_MSE': 441.32382, 'Validation_MSE': 585.0444}.
trigger times: 0
Loss after 362061600 batches: 0.1328
trigger times: 0
Loss after 362192700 batches: 0.0358
trigger times: 0
Loss after 362323800 batches: 0.0259
trigger times: 1
Loss after 362454900 batches: 0.0223
trigger times: 2
Loss after 362586000 batches: 0.0200
trigger times: 3
Loss after 362717100 batches: 0.0184
trigger times: 4
Loss after 362848200 batches: 0.0173
trigger times: 5
Loss after 362979300 batches: 0.0166
trigger times: 0
Loss after 363110400 batches: 0.0162
trigger times: 1
Loss after 363241500 batches: 0.0153
trigger times: 2
Loss after 363372600 batches: 0.0152
trigger times: 3
Loss after 363503700 batches: 0.0145
trigger times: 4
Loss after 363634800 batches: 0.0140
trigger times: 5
Loss after 363765900 batches: 0.0137
trigger times: 6
Loss after 363897000 batches: 0.0136
trigger times: 7
Loss after 364028100 batches: 0.0133
trigger times: 8
Loss after 364159200 batches: 0.0127
trigger times: 9
Loss after 364290300 batches: 0.0129
trigger times: 10
Loss after 364421400 batches: 0.0130
trigger times: 11
Loss after 364552500 batches: 0.0123
trigger times: 12
Loss after 364683600 batches: 0.0123
trigger times: 13
Loss after 364814700 batches: 0.0121
trigger times: 0
Loss after 364945800 batches: 0.0123
trigger times: 1
Loss after 365076900 batches: 0.0119
trigger times: 2
Loss after 365208000 batches: 0.0116
trigger times: 3
Loss after 365339100 batches: 0.0117
trigger times: 4
Loss after 365470200 batches: 0.0115
trigger times: 5
Loss after 365601300 batches: 0.0112
trigger times: 6
Loss after 365732400 batches: 0.0113
trigger times: 7
Loss after 365863500 batches: 0.0113
trigger times: 8
Loss after 365994600 batches: 0.0111
trigger times: 9
Loss after 366125700 batches: 0.0108
trigger times: 10
Loss after 366256800 batches: 0.0109
trigger times: 11
Loss after 366387900 batches: 0.0107
trigger times: 12
Loss after 366519000 batches: 0.0106
trigger times: 13
Loss after 366650100 batches: 0.0106
trigger times: 14
Loss after 366781200 batches: 0.0103
trigger times: 15
Loss after 366912300 batches: 0.0101
trigger times: 16
Loss after 367043400 batches: 0.0103
trigger times: 17
Loss after 367174500 batches: 0.0103
trigger times: 18
Loss after 367305600 batches: 0.0100
trigger times: 19
Loss after 367436700 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 367567800 batches: 0.0101
Time to train on one home:  316.1957516670227
trigger times: 0
Loss after 367670400 batches: 0.4066
trigger times: 1
Loss after 367773000 batches: 0.1451
trigger times: 2
Loss after 367875600 batches: 0.0797
trigger times: 3
Loss after 367978200 batches: 0.0600
trigger times: 4
Loss after 368080800 batches: 0.0501
trigger times: 5
Loss after 368183400 batches: 0.0447
trigger times: 6
Loss after 368286000 batches: 0.0413
trigger times: 7
Loss after 368388600 batches: 0.0387
trigger times: 8
Loss after 368491200 batches: 0.0366
trigger times: 9
Loss after 368593800 batches: 0.0338
trigger times: 10
Loss after 368696400 batches: 0.0318
trigger times: 11
Loss after 368799000 batches: 0.0317
trigger times: 12
Loss after 368901600 batches: 0.0309
trigger times: 13
Loss after 369004200 batches: 0.0301
trigger times: 14
Loss after 369106800 batches: 0.0287
trigger times: 15
Loss after 369209400 batches: 0.0283
trigger times: 16
Loss after 369312000 batches: 0.0276
trigger times: 17
Loss after 369414600 batches: 0.0293
trigger times: 18
Loss after 369517200 batches: 0.0292
trigger times: 19
Loss after 369619800 batches: 0.0299
trigger times: 20
Early stopping!
Start to test process.
Loss after 369722400 batches: 0.0259
Time to train on one home:  130.73712682724
trigger times: 0
Loss after 369853500 batches: 0.1906
trigger times: 1
Loss after 369984600 batches: 0.0584
trigger times: 2
Loss after 370115700 batches: 0.0423
trigger times: 3
Loss after 370246800 batches: 0.0356
trigger times: 4
Loss after 370377900 batches: 0.0318
trigger times: 5
Loss after 370509000 batches: 0.0294
trigger times: 6
Loss after 370640100 batches: 0.0279
trigger times: 7
Loss after 370771200 batches: 0.0263
trigger times: 8
Loss after 370902300 batches: 0.0250
trigger times: 9
Loss after 371033400 batches: 0.0245
trigger times: 10
Loss after 371164500 batches: 0.0236
trigger times: 11
Loss after 371295600 batches: 0.0230
trigger times: 12
Loss after 371426700 batches: 0.0221
trigger times: 13
Loss after 371557800 batches: 0.0216
trigger times: 14
Loss after 371688900 batches: 0.0211
trigger times: 15
Loss after 371820000 batches: 0.0209
trigger times: 16
Loss after 371951100 batches: 0.0206
trigger times: 17
Loss after 372082200 batches: 0.0204
trigger times: 18
Loss after 372213300 batches: 0.0200
trigger times: 19
Loss after 372344400 batches: 0.0196
trigger times: 20
Early stopping!
Start to test process.
Loss after 372475500 batches: 0.0193
Time to train on one home:  160.33928203582764
trigger times: 0
Loss after 372606600 batches: 0.3202
trigger times: 0
Loss after 372737700 batches: 0.0900
trigger times: 0
Loss after 372868800 batches: 0.0634
trigger times: 1
Loss after 372999900 batches: 0.0526
trigger times: 0
Loss after 373131000 batches: 0.0473
trigger times: 1
Loss after 373262100 batches: 0.0438
trigger times: 2
Loss after 373393200 batches: 0.0408
trigger times: 3
Loss after 373524300 batches: 0.0388
trigger times: 4
Loss after 373655400 batches: 0.0378
trigger times: 5
Loss after 373786500 batches: 0.0360
trigger times: 6
Loss after 373917600 batches: 0.0342
trigger times: 0
Loss after 374048700 batches: 0.0335
trigger times: 1
Loss after 374179800 batches: 0.0328
trigger times: 2
Loss after 374310900 batches: 0.0317
trigger times: 3
Loss after 374442000 batches: 0.0307
trigger times: 4
Loss after 374573100 batches: 0.0310
trigger times: 0
Loss after 374704200 batches: 0.0301
trigger times: 1
Loss after 374835300 batches: 0.0293
trigger times: 2
Loss after 374966400 batches: 0.0291
trigger times: 3
Loss after 375097500 batches: 0.0284
trigger times: 4
Loss after 375228600 batches: 0.0278
trigger times: 5
Loss after 375359700 batches: 0.0271
trigger times: 6
Loss after 375490800 batches: 0.0276
trigger times: 7
Loss after 375621900 batches: 0.0267
trigger times: 8
Loss after 375753000 batches: 0.0267
trigger times: 0
Loss after 375884100 batches: 0.0261
trigger times: 1
Loss after 376015200 batches: 0.0259
trigger times: 2
Loss after 376146300 batches: 0.0254
trigger times: 3
Loss after 376277400 batches: 0.0254
trigger times: 4
Loss after 376408500 batches: 0.0252
trigger times: 5
Loss after 376539600 batches: 0.0250
trigger times: 6
Loss after 376670700 batches: 0.0246
trigger times: 7
Loss after 376801800 batches: 0.0245
trigger times: 8
Loss after 376932900 batches: 0.0244
trigger times: 9
Loss after 377064000 batches: 0.0244
trigger times: 10
Loss after 377195100 batches: 0.0238
trigger times: 11
Loss after 377326200 batches: 0.0239
trigger times: 12
Loss after 377457300 batches: 0.0236
trigger times: 13
Loss after 377588400 batches: 0.0237
trigger times: 14
Loss after 377719500 batches: 0.0235
trigger times: 15
Loss after 377850600 batches: 0.0234
trigger times: 0
Loss after 377981700 batches: 0.0232
trigger times: 1
Loss after 378112800 batches: 0.0230
trigger times: 2
Loss after 378243900 batches: 0.0234
trigger times: 3
Loss after 378375000 batches: 0.0228
trigger times: 4
Loss after 378506100 batches: 0.0227
trigger times: 0
Loss after 378637200 batches: 0.0227
trigger times: 0
Loss after 378768300 batches: 0.0222
trigger times: 0
Loss after 378899400 batches: 0.0222
trigger times: 1
Loss after 379030500 batches: 0.0220
trigger times: 2
Loss after 379161600 batches: 0.0218
trigger times: 0
Loss after 379292700 batches: 0.0218
trigger times: 1
Loss after 379423800 batches: 0.0217
trigger times: 2
Loss after 379554900 batches: 0.0216
trigger times: 3
Loss after 379686000 batches: 0.0212
trigger times: 4
Loss after 379817100 batches: 0.0212
trigger times: 5
Loss after 379948200 batches: 0.0211
trigger times: 6
Loss after 380079300 batches: 0.0212
trigger times: 7
Loss after 380210400 batches: 0.0209
trigger times: 8
Loss after 380341500 batches: 0.0210
trigger times: 9
Loss after 380472600 batches: 0.0206
trigger times: 10
Loss after 380603700 batches: 0.0206
trigger times: 11
Loss after 380734800 batches: 0.0206
trigger times: 12
Loss after 380865900 batches: 0.0208
trigger times: 13
Loss after 380997000 batches: 0.0207
trigger times: 14
Loss after 381128100 batches: 0.0205
trigger times: 15
Loss after 381259200 batches: 0.0203
trigger times: 16
Loss after 381390300 batches: 0.0200
trigger times: 17
Loss after 381521400 batches: 0.0201
trigger times: 18
Loss after 381652500 batches: 0.0199
trigger times: 19
Loss after 381783600 batches: 0.0198
trigger times: 20
Early stopping!
Start to test process.
Loss after 381914700 batches: 0.0198
Time to train on one home:  521.4370119571686
trigger times: 0
Loss after 382043340 batches: 0.1817
trigger times: 0
Loss after 382171980 batches: 0.0481
trigger times: 0
Loss after 382300620 batches: 0.0332
trigger times: 0
Loss after 382429260 batches: 0.0279
trigger times: 1
Loss after 382557900 batches: 0.0253
trigger times: 2
Loss after 382686540 batches: 0.0235
trigger times: 3
Loss after 382815180 batches: 0.0219
trigger times: 4
Loss after 382943820 batches: 0.0213
trigger times: 0
Loss after 383072460 batches: 0.0203
trigger times: 1
Loss after 383201100 batches: 0.0195
trigger times: 2
Loss after 383329740 batches: 0.0193
trigger times: 3
Loss after 383458380 batches: 0.0187
trigger times: 4
Loss after 383587020 batches: 0.0179
trigger times: 5
Loss after 383715660 batches: 0.0177
trigger times: 6
Loss after 383844300 batches: 0.0172
trigger times: 7
Loss after 383972940 batches: 0.0167
trigger times: 8
Loss after 384101580 batches: 0.0169
trigger times: 9
Loss after 384230220 batches: 0.0163
trigger times: 10
Loss after 384358860 batches: 0.0161
trigger times: 11
Loss after 384487500 batches: 0.0154
trigger times: 12
Loss after 384616140 batches: 0.0153
trigger times: 13
Loss after 384744780 batches: 0.0150
trigger times: 14
Loss after 384873420 batches: 0.0152
trigger times: 15
Loss after 385002060 batches: 0.0153
trigger times: 16
Loss after 385130700 batches: 0.0150
trigger times: 17
Loss after 385259340 batches: 0.0146
trigger times: 18
Loss after 385387980 batches: 0.0147
trigger times: 19
Loss after 385516620 batches: 0.0144
trigger times: 20
Early stopping!
Start to test process.
Loss after 385645260 batches: 0.0142
Time to train on one home:  213.59507656097412
trigger times: 0
Loss after 385776360 batches: 0.3468
trigger times: 0
Loss after 385907460 batches: 0.0911
trigger times: 0
Loss after 386038560 batches: 0.0608
trigger times: 1
Loss after 386169660 batches: 0.0505
trigger times: 0
Loss after 386300760 batches: 0.0447
trigger times: 1
Loss after 386431860 batches: 0.0418
trigger times: 0
Loss after 386562960 batches: 0.0387
trigger times: 1
Loss after 386694060 batches: 0.0367
trigger times: 2
Loss after 386825160 batches: 0.0353
trigger times: 0
Loss after 386956260 batches: 0.0343
trigger times: 1
Loss after 387087360 batches: 0.0330
trigger times: 2
Loss after 387218460 batches: 0.0314
trigger times: 3
Loss after 387349560 batches: 0.0307
trigger times: 4
Loss after 387480660 batches: 0.0301
trigger times: 0
Loss after 387611760 batches: 0.0295
trigger times: 1
Loss after 387742860 batches: 0.0288
trigger times: 0
Loss after 387873960 batches: 0.0284
trigger times: 1
Loss after 388005060 batches: 0.0277
trigger times: 2
Loss after 388136160 batches: 0.0275
trigger times: 0
Loss after 388267260 batches: 0.0270
trigger times: 1
Loss after 388398360 batches: 0.0266
trigger times: 0
Loss after 388529460 batches: 0.0257
trigger times: 1
Loss after 388660560 batches: 0.0257
trigger times: 2
Loss after 388791660 batches: 0.0258
trigger times: 0
Loss after 388922760 batches: 0.0252
trigger times: 1
Loss after 389053860 batches: 0.0246
trigger times: 2
Loss after 389184960 batches: 0.0245
trigger times: 3
Loss after 389316060 batches: 0.0243
trigger times: 4
Loss after 389447160 batches: 0.0241
trigger times: 5
Loss after 389578260 batches: 0.0239
trigger times: 0
Loss after 389709360 batches: 0.0236
trigger times: 1
Loss after 389840460 batches: 0.0235
trigger times: 2
Loss after 389971560 batches: 0.0232
trigger times: 3
Loss after 390102660 batches: 0.0236
trigger times: 4
Loss after 390233760 batches: 0.0231
trigger times: 5
Loss after 390364860 batches: 0.0226
trigger times: 6
Loss after 390495960 batches: 0.0225
trigger times: 7
Loss after 390627060 batches: 0.0225
trigger times: 8
Loss after 390758160 batches: 0.0221
trigger times: 0
Loss after 390889260 batches: 0.0223
trigger times: 1
Loss after 391020360 batches: 0.0221
trigger times: 2
Loss after 391151460 batches: 0.0216
trigger times: 3
Loss after 391282560 batches: 0.0214
trigger times: 0
Loss after 391413660 batches: 0.0215
trigger times: 1
Loss after 391544760 batches: 0.0215
trigger times: 2
Loss after 391675860 batches: 0.0212
trigger times: 3
Loss after 391806960 batches: 0.0210
trigger times: 4
Loss after 391938060 batches: 0.0212
trigger times: 5
Loss after 392069160 batches: 0.0207
trigger times: 6
Loss after 392200260 batches: 0.0206
trigger times: 7
Loss after 392331360 batches: 0.0205
trigger times: 0
Loss after 392462460 batches: 0.0204
trigger times: 1
Loss after 392593560 batches: 0.0204
trigger times: 0
Loss after 392724660 batches: 0.0199
trigger times: 1
Loss after 392855760 batches: 0.0201
trigger times: 2
Loss after 392986860 batches: 0.0198
trigger times: 3
Loss after 393117960 batches: 0.0196
trigger times: 4
Loss after 393249060 batches: 0.0201
trigger times: 5
Loss after 393380160 batches: 0.0197
trigger times: 6
Loss after 393511260 batches: 0.0196
trigger times: 0
Loss after 393642360 batches: 0.0197
trigger times: 1
Loss after 393773460 batches: 0.0196
trigger times: 2
Loss after 393904560 batches: 0.0196
trigger times: 3
Loss after 394035660 batches: 0.0193
trigger times: 4
Loss after 394166760 batches: 0.0193
trigger times: 5
Loss after 394297860 batches: 0.0191
trigger times: 6
Loss after 394428960 batches: 0.0190
trigger times: 7
Loss after 394560060 batches: 0.0189
trigger times: 8
Loss after 394691160 batches: 0.0190
trigger times: 9
Loss after 394822260 batches: 0.0190
trigger times: 10
Loss after 394953360 batches: 0.0185
trigger times: 11
Loss after 395084460 batches: 0.0185
trigger times: 12
Loss after 395215560 batches: 0.0184
trigger times: 13
Loss after 395346660 batches: 0.0186
trigger times: 14
Loss after 395477760 batches: 0.0181
trigger times: 15
Loss after 395608860 batches: 0.0184
trigger times: 0
Loss after 395739960 batches: 0.0180
trigger times: 1
Loss after 395871060 batches: 0.0183
trigger times: 0
Loss after 396002160 batches: 0.0182
trigger times: 1
Loss after 396133260 batches: 0.0180
trigger times: 2
Loss after 396264360 batches: 0.0179
trigger times: 3
Loss after 396395460 batches: 0.0178
trigger times: 0
Loss after 396526560 batches: 0.0179
trigger times: 1
Loss after 396657660 batches: 0.0179
trigger times: 2
Loss after 396788760 batches: 0.0178
trigger times: 3
Loss after 396919860 batches: 0.0179
trigger times: 4
Loss after 397050960 batches: 0.0179
trigger times: 5
Loss after 397182060 batches: 0.0179
trigger times: 6
Loss after 397313160 batches: 0.0175
trigger times: 7
Loss after 397444260 batches: 0.0175
trigger times: 8
Loss after 397575360 batches: 0.0174
trigger times: 9
Loss after 397706460 batches: 0.0173
trigger times: 10
Loss after 397837560 batches: 0.0176
trigger times: 11
Loss after 397968660 batches: 0.0175
trigger times: 12
Loss after 398099760 batches: 0.0172
trigger times: 13
Loss after 398230860 batches: 0.0172
trigger times: 14
Loss after 398361960 batches: 0.0175
trigger times: 15
Loss after 398493060 batches: 0.0174
trigger times: 16
Loss after 398624160 batches: 0.0173
trigger times: 17
Loss after 398755260 batches: 0.0171
trigger times: 18
Loss after 398886360 batches: 0.0171
trigger times: 19
Loss after 399017460 batches: 0.0172
trigger times: 20
Early stopping!
Start to test process.
Loss after 399148560 batches: 0.0170
Time to train on one home:  740.0657482147217
trigger times: 0
Loss after 399279660 batches: 0.3581
trigger times: 0
Loss after 399410760 batches: 0.1364
trigger times: 0
Loss after 399541860 batches: 0.0770
trigger times: 1
Loss after 399672960 batches: 0.0583
trigger times: 2
Loss after 399804060 batches: 0.0516
trigger times: 3
Loss after 399935160 batches: 0.0460
trigger times: 0
Loss after 400066260 batches: 0.0433
trigger times: 0
Loss after 400197360 batches: 0.0427
trigger times: 1
Loss after 400328460 batches: 0.0376
trigger times: 2
Loss after 400459560 batches: 0.0373
trigger times: 0
Loss after 400590660 batches: 0.0356
trigger times: 1
Loss after 400721760 batches: 0.0353
trigger times: 2
Loss after 400852860 batches: 0.0337
trigger times: 0
Loss after 400983960 batches: 0.0318
trigger times: 0
Loss after 401115060 batches: 0.0318
trigger times: 1
Loss after 401246160 batches: 0.0320
trigger times: 0
Loss after 401377260 batches: 0.0317
trigger times: 1
Loss after 401508360 batches: 0.0312
trigger times: 2
Loss after 401639460 batches: 0.0301
trigger times: 3
Loss after 401770560 batches: 0.0289
trigger times: 0
Loss after 401901660 batches: 0.0295
trigger times: 0
Loss after 402032760 batches: 0.0293
trigger times: 1
Loss after 402163860 batches: 0.0287
trigger times: 0
Loss after 402294960 batches: 0.0288
trigger times: 0
Loss after 402426060 batches: 0.0273
trigger times: 1
Loss after 402557160 batches: 0.0271
trigger times: 2
Loss after 402688260 batches: 0.0275
trigger times: 3
Loss after 402819360 batches: 0.0293
trigger times: 4
Loss after 402950460 batches: 0.0284
trigger times: 5
Loss after 403081560 batches: 0.0270
trigger times: 6
Loss after 403212660 batches: 0.0275
trigger times: 7
Loss after 403343760 batches: 0.0264
trigger times: 8
Loss after 403474860 batches: 0.0264
trigger times: 9
Loss after 403605960 batches: 0.0285
trigger times: 10
Loss after 403737060 batches: 0.0260
trigger times: 11
Loss after 403868160 batches: 0.0264
trigger times: 12
Loss after 403999260 batches: 0.0253
trigger times: 13
Loss after 404130360 batches: 0.0261
trigger times: 14
Loss after 404261460 batches: 0.0248
trigger times: 15
Loss after 404392560 batches: 0.0254
trigger times: 16
Loss after 404523660 batches: 0.0251
trigger times: 17
Loss after 404654760 batches: 0.0247
trigger times: 0
Loss after 404785860 batches: 0.0249
trigger times: 1
Loss after 404916960 batches: 0.0240
trigger times: 2
Loss after 405048060 batches: 0.0242
trigger times: 3
Loss after 405179160 batches: 0.0244
trigger times: 4
Loss after 405310260 batches: 0.0243
trigger times: 5
Loss after 405441360 batches: 0.0243
trigger times: 6
Loss after 405572460 batches: 0.0239
trigger times: 7
Loss after 405703560 batches: 0.0244
trigger times: 8
Loss after 405834660 batches: 0.0241
trigger times: 9
Loss after 405965760 batches: 0.0235
trigger times: 10
Loss after 406096860 batches: 0.0232
trigger times: 11
Loss after 406227960 batches: 0.0246
trigger times: 12
Loss after 406359060 batches: 0.0237
trigger times: 13
Loss after 406490160 batches: 0.0232
trigger times: 14
Loss after 406621260 batches: 0.0230
trigger times: 15
Loss after 406752360 batches: 0.0246
trigger times: 16
Loss after 406883460 batches: 0.0227
trigger times: 17
Loss after 407014560 batches: 0.0228
trigger times: 18
Loss after 407145660 batches: 0.0221
trigger times: 19
Loss after 407276760 batches: 0.0220
trigger times: 20
Early stopping!
Start to test process.
Loss after 407407860 batches: 0.0222
Time to train on one home:  457.04925322532654
trigger times: 0
Loss after 407538960 batches: 0.0979
trigger times: 0
Loss after 407670060 batches: 0.0293
trigger times: 0
Loss after 407801160 batches: 0.0213
trigger times: 0
Loss after 407932260 batches: 0.0181
trigger times: 0
Loss after 408063360 batches: 0.0165
trigger times: 1
Loss after 408194460 batches: 0.0154
trigger times: 0
Loss after 408325560 batches: 0.0148
trigger times: 1
Loss after 408456660 batches: 0.0138
trigger times: 0
Loss after 408587760 batches: 0.0130
trigger times: 1
Loss after 408718860 batches: 0.0126
trigger times: 2
Loss after 408849960 batches: 0.0126
trigger times: 3
Loss after 408981060 batches: 0.0119
trigger times: 4
Loss after 409112160 batches: 0.0116
trigger times: 0
Loss after 409243260 batches: 0.0116
trigger times: 1
Loss after 409374360 batches: 0.0113
trigger times: 2
Loss after 409505460 batches: 0.0109
trigger times: 0
Loss after 409636560 batches: 0.0109
trigger times: 1
Loss after 409767660 batches: 0.0107
trigger times: 2
Loss after 409898760 batches: 0.0104
trigger times: 3
Loss after 410029860 batches: 0.0101
trigger times: 4
Loss after 410160960 batches: 0.0100
trigger times: 5
Loss after 410292060 batches: 0.0099
trigger times: 6
Loss after 410423160 batches: 0.0101
trigger times: 0
Loss after 410554260 batches: 0.0099
trigger times: 1
Loss after 410685360 batches: 0.0098
trigger times: 2
Loss after 410816460 batches: 0.0095
trigger times: 3
Loss after 410947560 batches: 0.0095
trigger times: 4
Loss after 411078660 batches: 0.0092
trigger times: 0
Loss after 411209760 batches: 0.0093
trigger times: 1
Loss after 411340860 batches: 0.0092
trigger times: 0
Loss after 411471960 batches: 0.0092
trigger times: 1
Loss after 411603060 batches: 0.0090
trigger times: 2
Loss after 411734160 batches: 0.0092
trigger times: 3
Loss after 411865260 batches: 0.0091
trigger times: 4
Loss after 411996360 batches: 0.0090
trigger times: 5
Loss after 412127460 batches: 0.0088
trigger times: 0
Loss after 412258560 batches: 0.0086
trigger times: 1
Loss after 412389660 batches: 0.0086
trigger times: 2
Loss after 412520760 batches: 0.0085
trigger times: 3
Loss after 412651860 batches: 0.0087
trigger times: 4
Loss after 412782960 batches: 0.0087
trigger times: 5
Loss after 412914060 batches: 0.0085
trigger times: 6
Loss after 413045160 batches: 0.0084
trigger times: 7
Loss after 413176260 batches: 0.0082
trigger times: 0
Loss after 413307360 batches: 0.0081
trigger times: 0
Loss after 413438460 batches: 0.0082
trigger times: 0
Loss after 413569560 batches: 0.0081
trigger times: 1
Loss after 413700660 batches: 0.0083
trigger times: 2
Loss after 413831760 batches: 0.0083
trigger times: 3
Loss after 413962860 batches: 0.0082
trigger times: 4
Loss after 414093960 batches: 0.0079
trigger times: 0
Loss after 414225060 batches: 0.0080
trigger times: 1
Loss after 414356160 batches: 0.0078
trigger times: 2
Loss after 414487260 batches: 0.0081
trigger times: 3
Loss after 414618360 batches: 0.0078
trigger times: 4
Loss after 414749460 batches: 0.0077
trigger times: 5
Loss after 414880560 batches: 0.0078
trigger times: 6
Loss after 415011660 batches: 0.0080
trigger times: 7
Loss after 415142760 batches: 0.0079
trigger times: 8
Loss after 415273860 batches: 0.0078
trigger times: 9
Loss after 415404960 batches: 0.0075
trigger times: 10
Loss after 415536060 batches: 0.0076
trigger times: 11
Loss after 415667160 batches: 0.0074
trigger times: 12
Loss after 415798260 batches: 0.0074
trigger times: 13
Loss after 415929360 batches: 0.0075
trigger times: 14
Loss after 416060460 batches: 0.0074
trigger times: 15
Loss after 416191560 batches: 0.0074
trigger times: 16
Loss after 416322660 batches: 0.0072
trigger times: 17
Loss after 416453760 batches: 0.0072
trigger times: 18
Loss after 416584860 batches: 0.0074
trigger times: 19
Loss after 416715960 batches: 0.0071
trigger times: 20
Early stopping!
Start to test process.
Loss after 416847060 batches: 0.0071
Time to train on one home:  520.7925634384155
trigger times: 0
Loss after 416925660 batches: 0.3121
trigger times: 0
Loss after 417004260 batches: 0.0792
trigger times: 1
Loss after 417082860 batches: 0.0494
trigger times: 0
Loss after 417161460 batches: 0.0397
trigger times: 1
Loss after 417240060 batches: 0.0357
trigger times: 0
Loss after 417318660 batches: 0.0315
trigger times: 0
Loss after 417397260 batches: 0.0298
trigger times: 0
Loss after 417475860 batches: 0.0282
trigger times: 1
Loss after 417554460 batches: 0.0269
trigger times: 2
Loss after 417633060 batches: 0.0257
trigger times: 3
Loss after 417711660 batches: 0.0246
trigger times: 4
Loss after 417790260 batches: 0.0244
trigger times: 5
Loss after 417868860 batches: 0.0233
trigger times: 0
Loss after 417947460 batches: 0.0228
trigger times: 1
Loss after 418026060 batches: 0.0223
trigger times: 2
Loss after 418104660 batches: 0.0222
trigger times: 3
Loss after 418183260 batches: 0.0210
trigger times: 4
Loss after 418261860 batches: 0.0208
trigger times: 5
Loss after 418340460 batches: 0.0212
trigger times: 6
Loss after 418419060 batches: 0.0199
trigger times: 7
Loss after 418497660 batches: 0.0199
trigger times: 8
Loss after 418576260 batches: 0.0199
trigger times: 9
Loss after 418654860 batches: 0.0194
trigger times: 10
Loss after 418733460 batches: 0.0192
trigger times: 0
Loss after 418812060 batches: 0.0188
trigger times: 1
Loss after 418890660 batches: 0.0189
trigger times: 2
Loss after 418969260 batches: 0.0184
trigger times: 3
Loss after 419047860 batches: 0.0186
trigger times: 4
Loss after 419126460 batches: 0.0183
trigger times: 5
Loss after 419205060 batches: 0.0183
trigger times: 6
Loss after 419283660 batches: 0.0176
trigger times: 7
Loss after 419362260 batches: 0.0179
trigger times: 8
Loss after 419440860 batches: 0.0178
trigger times: 9
Loss after 419519460 batches: 0.0172
trigger times: 10
Loss after 419598060 batches: 0.0174
trigger times: 11
Loss after 419676660 batches: 0.0176
trigger times: 12
Loss after 419755260 batches: 0.0173
trigger times: 13
Loss after 419833860 batches: 0.0169
trigger times: 14
Loss after 419912460 batches: 0.0167
trigger times: 15
Loss after 419991060 batches: 0.0167
trigger times: 16
Loss after 420069660 batches: 0.0166
trigger times: 17
Loss after 420148260 batches: 0.0164
trigger times: 18
Loss after 420226860 batches: 0.0164
trigger times: 19
Loss after 420305460 batches: 0.0165
trigger times: 20
Early stopping!
Start to test process.
Loss after 420384060 batches: 0.0161
Time to train on one home:  216.60453462600708
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097, 0.016851266692967333]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983], [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013]]
Round_8_results:  [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013]
trigger times: 0
Loss after 420515160 batches: 0.1263
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3439 < 3440; dropping {'Training_Loss': 0.12632094056538815, 'Validation_Loss': 0.23314134197102654, 'Training_R2': 0.872765190546565, 'Validation_R2': 0.7833020411948983, 'Training_F1': 0.814029797261671, 'Validation_F1': 0.7277940475068845, 'Training_NEP': 0.37166158052977605, 'Validation_NEP': 0.5090548013622566, 'Training_NDE': 0.09551742987806203, 'Validation_NDE': 0.17256683135244372, 'Training_MAE': 12.308694462750724, 'Validation_MAE': 13.96039459235352, 'Training_MSE': 420.26175, 'Validation_MSE': 637.28455}.
trigger times: 0
Loss after 420646260 batches: 0.0336
trigger times: 0
Loss after 420777360 batches: 0.0241
trigger times: 0
Loss after 420908460 batches: 0.0207
trigger times: 0
Loss after 421039560 batches: 0.0183
trigger times: 0
Loss after 421170660 batches: 0.0172
trigger times: 1
Loss after 421301760 batches: 0.0166
trigger times: 0
Loss after 421432860 batches: 0.0157
trigger times: 1
Loss after 421563960 batches: 0.0151
trigger times: 2
Loss after 421695060 batches: 0.0145
trigger times: 3
Loss after 421826160 batches: 0.0140
trigger times: 4
Loss after 421957260 batches: 0.0139
trigger times: 5
Loss after 422088360 batches: 0.0135
trigger times: 6
Loss after 422219460 batches: 0.0130
trigger times: 7
Loss after 422350560 batches: 0.0128
trigger times: 8
Loss after 422481660 batches: 0.0126
trigger times: 9
Loss after 422612760 batches: 0.0125
trigger times: 10
Loss after 422743860 batches: 0.0122
trigger times: 0
Loss after 422874960 batches: 0.0120
trigger times: 1
Loss after 423006060 batches: 0.0119
trigger times: 2
Loss after 423137160 batches: 0.0116
trigger times: 3
Loss after 423268260 batches: 0.0115
trigger times: 4
Loss after 423399360 batches: 0.0116
trigger times: 5
Loss after 423530460 batches: 0.0113
trigger times: 6
Loss after 423661560 batches: 0.0112
trigger times: 7
Loss after 423792660 batches: 0.0111
trigger times: 8
Loss after 423923760 batches: 0.0111
trigger times: 9
Loss after 424054860 batches: 0.0109
trigger times: 10
Loss after 424185960 batches: 0.0106
trigger times: 11
Loss after 424317060 batches: 0.0105
trigger times: 12
Loss after 424448160 batches: 0.0105
trigger times: 13
Loss after 424579260 batches: 0.0107
trigger times: 14
Loss after 424710360 batches: 0.0106
trigger times: 15
Loss after 424841460 batches: 0.0104
trigger times: 16
Loss after 424972560 batches: 0.0104
trigger times: 17
Loss after 425103660 batches: 0.0101
trigger times: 18
Loss after 425234760 batches: 0.0101
trigger times: 19
Loss after 425365860 batches: 0.0103
trigger times: 20
Early stopping!
Start to test process.
Loss after 425496960 batches: 0.0100
Time to train on one home:  287.62764978408813
trigger times: 0
Loss after 425599560 batches: 0.3538
trigger times: 1
Loss after 425702160 batches: 0.1127
trigger times: 2
Loss after 425804760 batches: 0.0733
trigger times: 3
Loss after 425907360 batches: 0.0565
trigger times: 4
Loss after 426009960 batches: 0.0477
trigger times: 5
Loss after 426112560 batches: 0.0430
trigger times: 6
Loss after 426215160 batches: 0.0374
trigger times: 7
Loss after 426317760 batches: 0.0356
trigger times: 8
Loss after 426420360 batches: 0.0326
trigger times: 9
Loss after 426522960 batches: 0.0321
trigger times: 10
Loss after 426625560 batches: 0.0311
trigger times: 11
Loss after 426728160 batches: 0.0328
trigger times: 12
Loss after 426830760 batches: 0.0293
trigger times: 13
Loss after 426933360 batches: 0.0280
trigger times: 14
Loss after 427035960 batches: 0.0268
trigger times: 15
Loss after 427138560 batches: 0.0266
trigger times: 16
Loss after 427241160 batches: 0.0263
trigger times: 17
Loss after 427343760 batches: 0.0280
trigger times: 18
Loss after 427446360 batches: 0.0274
trigger times: 19
Loss after 427548960 batches: 0.0244
trigger times: 20
Early stopping!
Start to test process.
Loss after 427651560 batches: 0.0253
Time to train on one home:  130.71097874641418
trigger times: 0
Loss after 427782660 batches: 0.1959
trigger times: 1
Loss after 427913760 batches: 0.0603
trigger times: 2
Loss after 428044860 batches: 0.0424
trigger times: 3
Loss after 428175960 batches: 0.0353
trigger times: 4
Loss after 428307060 batches: 0.0311
trigger times: 5
Loss after 428438160 batches: 0.0290
trigger times: 6
Loss after 428569260 batches: 0.0277
trigger times: 0
Loss after 428700360 batches: 0.0262
trigger times: 0
Loss after 428831460 batches: 0.0250
trigger times: 1
Loss after 428962560 batches: 0.0240
trigger times: 2
Loss after 429093660 batches: 0.0234
trigger times: 3
Loss after 429224760 batches: 0.0226
trigger times: 4
Loss after 429355860 batches: 0.0219
trigger times: 5
Loss after 429486960 batches: 0.0217
trigger times: 6
Loss after 429618060 batches: 0.0209
trigger times: 7
Loss after 429749160 batches: 0.0203
trigger times: 8
Loss after 429880260 batches: 0.0198
trigger times: 9
Loss after 430011360 batches: 0.0199
trigger times: 10
Loss after 430142460 batches: 0.0196
trigger times: 11
Loss after 430273560 batches: 0.0194
trigger times: 12
Loss after 430404660 batches: 0.0190
trigger times: 13
Loss after 430535760 batches: 0.0187
trigger times: 14
Loss after 430666860 batches: 0.0187
trigger times: 15
Loss after 430797960 batches: 0.0185
trigger times: 16
Loss after 430929060 batches: 0.0180
trigger times: 17
Loss after 431060160 batches: 0.0178
trigger times: 18
Loss after 431191260 batches: 0.0179
trigger times: 19
Loss after 431322360 batches: 0.0175
trigger times: 20
Early stopping!
Start to test process.
Loss after 431453460 batches: 0.0174
Time to train on one home:  216.60579228401184
trigger times: 0
Loss after 431584560 batches: 0.2915
trigger times: 0
Loss after 431715660 batches: 0.0852
trigger times: 0
Loss after 431846760 batches: 0.0583
trigger times: 0
Loss after 431977860 batches: 0.0490
trigger times: 1
Loss after 432108960 batches: 0.0440
trigger times: 2
Loss after 432240060 batches: 0.0406
trigger times: 3
Loss after 432371160 batches: 0.0382
trigger times: 0
Loss after 432502260 batches: 0.0363
trigger times: 1
Loss after 432633360 batches: 0.0341
trigger times: 2
Loss after 432764460 batches: 0.0331
trigger times: 3
Loss after 432895560 batches: 0.0320
trigger times: 4
Loss after 433026660 batches: 0.0310
trigger times: 0
Loss after 433157760 batches: 0.0303
trigger times: 0
Loss after 433288860 batches: 0.0296
trigger times: 1
Loss after 433419960 batches: 0.0287
trigger times: 2
Loss after 433551060 batches: 0.0287
trigger times: 3
Loss after 433682160 batches: 0.0278
trigger times: 4
Loss after 433813260 batches: 0.0279
trigger times: 0
Loss after 433944360 batches: 0.0270
trigger times: 1
Loss after 434075460 batches: 0.0265
trigger times: 0
Loss after 434206560 batches: 0.0260
trigger times: 1
Loss after 434337660 batches: 0.0258
trigger times: 2
Loss after 434468760 batches: 0.0256
trigger times: 3
Loss after 434599860 batches: 0.0255
trigger times: 4
Loss after 434730960 batches: 0.0252
trigger times: 5
Loss after 434862060 batches: 0.0247
trigger times: 0
Loss after 434993160 batches: 0.0246
trigger times: 1
Loss after 435124260 batches: 0.0244
trigger times: 2
Loss after 435255360 batches: 0.0243
trigger times: 3
Loss after 435386460 batches: 0.0240
trigger times: 0
Loss after 435517560 batches: 0.0235
trigger times: 1
Loss after 435648660 batches: 0.0232
trigger times: 2
Loss after 435779760 batches: 0.0230
trigger times: 3
Loss after 435910860 batches: 0.0231
trigger times: 0
Loss after 436041960 batches: 0.0229
trigger times: 1
Loss after 436173060 batches: 0.0228
trigger times: 2
Loss after 436304160 batches: 0.0226
trigger times: 3
Loss after 436435260 batches: 0.0224
trigger times: 4
Loss after 436566360 batches: 0.0223
trigger times: 5
Loss after 436697460 batches: 0.0223
trigger times: 6
Loss after 436828560 batches: 0.0219
trigger times: 7
Loss after 436959660 batches: 0.0217
trigger times: 8
Loss after 437090760 batches: 0.0218
trigger times: 9
Loss after 437221860 batches: 0.0217
trigger times: 10
Loss after 437352960 batches: 0.0214
trigger times: 11
Loss after 437484060 batches: 0.0214
trigger times: 12
Loss after 437615160 batches: 0.0214
trigger times: 0
Loss after 437746260 batches: 0.0212
trigger times: 1
Loss after 437877360 batches: 0.0210
trigger times: 2
Loss after 438008460 batches: 0.0208
trigger times: 3
Loss after 438139560 batches: 0.0208
trigger times: 4
Loss after 438270660 batches: 0.0207
trigger times: 5
Loss after 438401760 batches: 0.0205
trigger times: 6
Loss after 438532860 batches: 0.0208
trigger times: 7
Loss after 438663960 batches: 0.0204
trigger times: 8
Loss after 438795060 batches: 0.0203
trigger times: 9
Loss after 438926160 batches: 0.0200
trigger times: 10
Loss after 439057260 batches: 0.0203
trigger times: 11
Loss after 439188360 batches: 0.0198
trigger times: 12
Loss after 439319460 batches: 0.0201
trigger times: 0
Loss after 439450560 batches: 0.0202
trigger times: 1
Loss after 439581660 batches: 0.0196
trigger times: 2
Loss after 439712760 batches: 0.0194
trigger times: 3
Loss after 439843860 batches: 0.0194
trigger times: 4
Loss after 439974960 batches: 0.0196
trigger times: 5
Loss after 440106060 batches: 0.0194
trigger times: 6
Loss after 440237160 batches: 0.0194
trigger times: 7
Loss after 440368260 batches: 0.0193
trigger times: 0
Loss after 440499360 batches: 0.0191
trigger times: 1
Loss after 440630460 batches: 0.0187
trigger times: 2
Loss after 440761560 batches: 0.0190
trigger times: 3
Loss after 440892660 batches: 0.0191
trigger times: 4
Loss after 441023760 batches: 0.0190
trigger times: 5
Loss after 441154860 batches: 0.0185
trigger times: 6
Loss after 441285960 batches: 0.0183
trigger times: 7
Loss after 441417060 batches: 0.0184
trigger times: 8
Loss after 441548160 batches: 0.0187
trigger times: 0
Loss after 441679260 batches: 0.0186
trigger times: 1
Loss after 441810360 batches: 0.0185
trigger times: 2
Loss after 441941460 batches: 0.0185
trigger times: 3
Loss after 442072560 batches: 0.0183
trigger times: 4
Loss after 442203660 batches: 0.0184
trigger times: 5
Loss after 442334760 batches: 0.0181
trigger times: 6
Loss after 442465860 batches: 0.0182
trigger times: 7
Loss after 442596960 batches: 0.0184
trigger times: 8
Loss after 442728060 batches: 0.0181
trigger times: 9
Loss after 442859160 batches: 0.0182
trigger times: 10
Loss after 442990260 batches: 0.0181
trigger times: 11
Loss after 443121360 batches: 0.0181
trigger times: 12
Loss after 443252460 batches: 0.0182
trigger times: 13
Loss after 443383560 batches: 0.0180
trigger times: 14
Loss after 443514660 batches: 0.0179
trigger times: 15
Loss after 443645760 batches: 0.0180
trigger times: 16
Loss after 443776860 batches: 0.0180
trigger times: 0
Loss after 443907960 batches: 0.0177
trigger times: 1
Loss after 444039060 batches: 0.0177
trigger times: 2
Loss after 444170160 batches: 0.0178
trigger times: 3
Loss after 444301260 batches: 0.0175
trigger times: 4
Loss after 444432360 batches: 0.0176
trigger times: 5
Loss after 444563460 batches: 0.0173
trigger times: 6
Loss after 444694560 batches: 0.0174
trigger times: 7
Loss after 444825660 batches: 0.0173
trigger times: 8
Loss after 444956760 batches: 0.0174
trigger times: 9
Loss after 445087860 batches: 0.0170
trigger times: 10
Loss after 445218960 batches: 0.0172
trigger times: 11
Loss after 445350060 batches: 0.0172
trigger times: 12
Loss after 445481160 batches: 0.0172
trigger times: 13
Loss after 445612260 batches: 0.0171
trigger times: 14
Loss after 445743360 batches: 0.0171
trigger times: 15
Loss after 445874460 batches: 0.0170
trigger times: 16
Loss after 446005560 batches: 0.0174
trigger times: 0
Loss after 446136660 batches: 0.0171
trigger times: 1
Loss after 446267760 batches: 0.0171
trigger times: 2
Loss after 446398860 batches: 0.0169
trigger times: 3
Loss after 446529960 batches: 0.0172
trigger times: 4
Loss after 446661060 batches: 0.0170
trigger times: 5
Loss after 446792160 batches: 0.0167
trigger times: 6
Loss after 446923260 batches: 0.0165
trigger times: 7
Loss after 447054360 batches: 0.0168
trigger times: 8
Loss after 447185460 batches: 0.0168
trigger times: 9
Loss after 447316560 batches: 0.0166
trigger times: 0
Loss after 447447660 batches: 0.0165
trigger times: 1
Loss after 447578760 batches: 0.0165
trigger times: 0
Loss after 447709860 batches: 0.0165
trigger times: 0
Loss after 447840960 batches: 0.0164
trigger times: 1
Loss after 447972060 batches: 0.0165
trigger times: 2
Loss after 448103160 batches: 0.0163
trigger times: 3
Loss after 448234260 batches: 0.0163
trigger times: 4
Loss after 448365360 batches: 0.0163
trigger times: 5
Loss after 448496460 batches: 0.0164
trigger times: 6
Loss after 448627560 batches: 0.0160
trigger times: 7
Loss after 448758660 batches: 0.0160
trigger times: 8
Loss after 448889760 batches: 0.0160
trigger times: 9
Loss after 449020860 batches: 0.0161
trigger times: 10
Loss after 449151960 batches: 0.0159
trigger times: 11
Loss after 449283060 batches: 0.0160
trigger times: 12
Loss after 449414160 batches: 0.0161
trigger times: 13
Loss after 449545260 batches: 0.0157
trigger times: 0
Loss after 449676360 batches: 0.0157
trigger times: 1
Loss after 449807460 batches: 0.0157
trigger times: 2
Loss after 449938560 batches: 0.0159
trigger times: 3
Loss after 450069660 batches: 0.0157
trigger times: 0
Loss after 450200760 batches: 0.0158
trigger times: 1
Loss after 450331860 batches: 0.0158
trigger times: 2
Loss after 450462960 batches: 0.0158
trigger times: 3
Loss after 450594060 batches: 0.0156
trigger times: 4
Loss after 450725160 batches: 0.0158
trigger times: 5
Loss after 450856260 batches: 0.0154
trigger times: 6
Loss after 450987360 batches: 0.0154
trigger times: 7
Loss after 451118460 batches: 0.0153
trigger times: 8
Loss after 451249560 batches: 0.0155
trigger times: 9
Loss after 451380660 batches: 0.0155
trigger times: 10
Loss after 451511760 batches: 0.0153
trigger times: 11
Loss after 451642860 batches: 0.0154
trigger times: 12
Loss after 451773960 batches: 0.0152
trigger times: 13
Loss after 451905060 batches: 0.0152
trigger times: 14
Loss after 452036160 batches: 0.0152
trigger times: 15
Loss after 452167260 batches: 0.0153
trigger times: 16
Loss after 452298360 batches: 0.0154
trigger times: 17
Loss after 452429460 batches: 0.0151
trigger times: 18
Loss after 452560560 batches: 0.0151
trigger times: 19
Loss after 452691660 batches: 0.0150
trigger times: 20
Early stopping!
Start to test process.
Loss after 452822760 batches: 0.0151
Time to train on one home:  1164.6781511306763
trigger times: 0
Loss after 452951400 batches: 0.1531
trigger times: 0
Loss after 453080040 batches: 0.0421
trigger times: 0
Loss after 453208680 batches: 0.0303
trigger times: 1
Loss after 453337320 batches: 0.0251
trigger times: 0
Loss after 453465960 batches: 0.0238
trigger times: 0
Loss after 453594600 batches: 0.0217
trigger times: 1
Loss after 453723240 batches: 0.0207
trigger times: 0
Loss after 453851880 batches: 0.0196
trigger times: 0
Loss after 453980520 batches: 0.0190
trigger times: 1
Loss after 454109160 batches: 0.0183
trigger times: 2
Loss after 454237800 batches: 0.0174
trigger times: 0
Loss after 454366440 batches: 0.0173
trigger times: 1
Loss after 454495080 batches: 0.0167
trigger times: 2
Loss after 454623720 batches: 0.0166
trigger times: 3
Loss after 454752360 batches: 0.0165
trigger times: 4
Loss after 454881000 batches: 0.0158
trigger times: 5
Loss after 455009640 batches: 0.0156
trigger times: 6
Loss after 455138280 batches: 0.0151
trigger times: 7
Loss after 455266920 batches: 0.0150
trigger times: 8
Loss after 455395560 batches: 0.0147
trigger times: 9
Loss after 455524200 batches: 0.0145
trigger times: 0
Loss after 455652840 batches: 0.0145
trigger times: 1
Loss after 455781480 batches: 0.0141
trigger times: 2
Loss after 455910120 batches: 0.0139
trigger times: 3
Loss after 456038760 batches: 0.0143
trigger times: 4
Loss after 456167400 batches: 0.0138
trigger times: 5
Loss after 456296040 batches: 0.0138
trigger times: 6
Loss after 456424680 batches: 0.0137
trigger times: 7
Loss after 456553320 batches: 0.0137
trigger times: 8
Loss after 456681960 batches: 0.0137
trigger times: 9
Loss after 456810600 batches: 0.0132
trigger times: 10
Loss after 456939240 batches: 0.0132
trigger times: 11
Loss after 457067880 batches: 0.0131
trigger times: 12
Loss after 457196520 batches: 0.0129
trigger times: 13
Loss after 457325160 batches: 0.0130
trigger times: 14
Loss after 457453800 batches: 0.0127
trigger times: 15
Loss after 457582440 batches: 0.0125
trigger times: 16
Loss after 457711080 batches: 0.0127
trigger times: 17
Loss after 457839720 batches: 0.0126
trigger times: 18
Loss after 457968360 batches: 0.0127
trigger times: 19
Loss after 458097000 batches: 0.0126
trigger times: 20
Early stopping!
Start to test process.
Loss after 458225640 batches: 0.0124
Time to train on one home:  304.06683564186096
trigger times: 0
Loss after 458356740 batches: 0.3523
trigger times: 1
Loss after 458487840 batches: 0.0849
trigger times: 2
Loss after 458618940 batches: 0.0567
trigger times: 0
Loss after 458750040 batches: 0.0469
trigger times: 0
Loss after 458881140 batches: 0.0415
trigger times: 0
Loss after 459012240 batches: 0.0382
trigger times: 1
Loss after 459143340 batches: 0.0360
trigger times: 2
Loss after 459274440 batches: 0.0343
trigger times: 0
Loss after 459405540 batches: 0.0325
trigger times: 0
Loss after 459536640 batches: 0.0312
trigger times: 0
Loss after 459667740 batches: 0.0306
trigger times: 0
Loss after 459798840 batches: 0.0297
trigger times: 1
Loss after 459929940 batches: 0.0289
trigger times: 2
Loss after 460061040 batches: 0.0280
trigger times: 0
Loss after 460192140 batches: 0.0273
trigger times: 1
Loss after 460323240 batches: 0.0270
trigger times: 0
Loss after 460454340 batches: 0.0267
trigger times: 1
Loss after 460585440 batches: 0.0257
trigger times: 2
Loss after 460716540 batches: 0.0256
trigger times: 0
Loss after 460847640 batches: 0.0249
trigger times: 0
Loss after 460978740 batches: 0.0249
trigger times: 1
Loss after 461109840 batches: 0.0242
trigger times: 2
Loss after 461240940 batches: 0.0245
trigger times: 3
Loss after 461372040 batches: 0.0237
trigger times: 0
Loss after 461503140 batches: 0.0232
trigger times: 1
Loss after 461634240 batches: 0.0232
trigger times: 0
Loss after 461765340 batches: 0.0229
trigger times: 0
Loss after 461896440 batches: 0.0227
trigger times: 1
Loss after 462027540 batches: 0.0226
trigger times: 2
Loss after 462158640 batches: 0.0223
trigger times: 3
Loss after 462289740 batches: 0.0225
trigger times: 4
Loss after 462420840 batches: 0.0221
trigger times: 5
Loss after 462551940 batches: 0.0223
trigger times: 6
Loss after 462683040 batches: 0.0219
trigger times: 7
Loss after 462814140 batches: 0.0214
trigger times: 0
Loss after 462945240 batches: 0.0215
trigger times: 0
Loss after 463076340 batches: 0.0214
trigger times: 1
Loss after 463207440 batches: 0.0209
trigger times: 0
Loss after 463338540 batches: 0.0211
trigger times: 0
Loss after 463469640 batches: 0.0209
trigger times: 1
Loss after 463600740 batches: 0.0208
trigger times: 2
Loss after 463731840 batches: 0.0207
trigger times: 3
Loss after 463862940 batches: 0.0203
trigger times: 0
Loss after 463994040 batches: 0.0206
trigger times: 1
Loss after 464125140 batches: 0.0201
trigger times: 0
Loss after 464256240 batches: 0.0202
trigger times: 1
Loss after 464387340 batches: 0.0202
trigger times: 2
Loss after 464518440 batches: 0.0198
trigger times: 3
Loss after 464649540 batches: 0.0193
trigger times: 4
Loss after 464780640 batches: 0.0198
trigger times: 5
Loss after 464911740 batches: 0.0196
trigger times: 6
Loss after 465042840 batches: 0.0196
trigger times: 7
Loss after 465173940 batches: 0.0192
trigger times: 8
Loss after 465305040 batches: 0.0192
trigger times: 9
Loss after 465436140 batches: 0.0190
trigger times: 10
Loss after 465567240 batches: 0.0189
trigger times: 11
Loss after 465698340 batches: 0.0187
trigger times: 12
Loss after 465829440 batches: 0.0190
trigger times: 13
Loss after 465960540 batches: 0.0188
trigger times: 14
Loss after 466091640 batches: 0.0187
trigger times: 15
Loss after 466222740 batches: 0.0187
trigger times: 16
Loss after 466353840 batches: 0.0187
trigger times: 0
Loss after 466484940 batches: 0.0181
trigger times: 1
Loss after 466616040 batches: 0.0182
trigger times: 2
Loss after 466747140 batches: 0.0181
trigger times: 3
Loss after 466878240 batches: 0.0181
trigger times: 4
Loss after 467009340 batches: 0.0182
trigger times: 5
Loss after 467140440 batches: 0.0179
trigger times: 6
Loss after 467271540 batches: 0.0180
trigger times: 7
Loss after 467402640 batches: 0.0180
trigger times: 8
Loss after 467533740 batches: 0.0177
trigger times: 9
Loss after 467664840 batches: 0.0177
trigger times: 10
Loss after 467795940 batches: 0.0176
trigger times: 0
Loss after 467927040 batches: 0.0177
trigger times: 1
Loss after 468058140 batches: 0.0176
trigger times: 2
Loss after 468189240 batches: 0.0176
trigger times: 0
Loss after 468320340 batches: 0.0177
trigger times: 1
Loss after 468451440 batches: 0.0173
trigger times: 2
Loss after 468582540 batches: 0.0173
trigger times: 3
Loss after 468713640 batches: 0.0171
trigger times: 4
Loss after 468844740 batches: 0.0170
trigger times: 5
Loss after 468975840 batches: 0.0171
trigger times: 6
Loss after 469106940 batches: 0.0173
trigger times: 7
Loss after 469238040 batches: 0.0171
trigger times: 8
Loss after 469369140 batches: 0.0174
trigger times: 9
Loss after 469500240 batches: 0.0173
trigger times: 10
Loss after 469631340 batches: 0.0170
trigger times: 11
Loss after 469762440 batches: 0.0168
trigger times: 12
Loss after 469893540 batches: 0.0170
trigger times: 0
Loss after 470024640 batches: 0.0169
trigger times: 1
Loss after 470155740 batches: 0.0168
trigger times: 2
Loss after 470286840 batches: 0.0167
trigger times: 3
Loss after 470417940 batches: 0.0167
trigger times: 4
Loss after 470549040 batches: 0.0165
trigger times: 5
Loss after 470680140 batches: 0.0169
trigger times: 6
Loss after 470811240 batches: 0.0164
trigger times: 7
Loss after 470942340 batches: 0.0167
trigger times: 8
Loss after 471073440 batches: 0.0167
trigger times: 9
Loss after 471204540 batches: 0.0164
trigger times: 10
Loss after 471335640 batches: 0.0166
trigger times: 0
Loss after 471466740 batches: 0.0163
trigger times: 1
Loss after 471597840 batches: 0.0164
trigger times: 2
Loss after 471728940 batches: 0.0163
trigger times: 3
Loss after 471860040 batches: 0.0163
trigger times: 4
Loss after 471991140 batches: 0.0163
trigger times: 5
Loss after 472122240 batches: 0.0162
trigger times: 6
Loss after 472253340 batches: 0.0162
trigger times: 7
Loss after 472384440 batches: 0.0161
trigger times: 8
Loss after 472515540 batches: 0.0159
trigger times: 9
Loss after 472646640 batches: 0.0159
trigger times: 10
Loss after 472777740 batches: 0.0164
trigger times: 11
Loss after 472908840 batches: 0.0160
trigger times: 12
Loss after 473039940 batches: 0.0158
trigger times: 13
Loss after 473171040 batches: 0.0159
trigger times: 14
Loss after 473302140 batches: 0.0160
trigger times: 15
Loss after 473433240 batches: 0.0157
trigger times: 16
Loss after 473564340 batches: 0.0160
trigger times: 0
Loss after 473695440 batches: 0.0158
trigger times: 1
Loss after 473826540 batches: 0.0159
trigger times: 2
Loss after 473957640 batches: 0.0156
trigger times: 3
Loss after 474088740 batches: 0.0156
trigger times: 4
Loss after 474219840 batches: 0.0157
trigger times: 0
Loss after 474350940 batches: 0.0154
trigger times: 1
Loss after 474482040 batches: 0.0153
trigger times: 2
Loss after 474613140 batches: 0.0156
trigger times: 3
Loss after 474744240 batches: 0.0157
trigger times: 4
Loss after 474875340 batches: 0.0156
trigger times: 5
Loss after 475006440 batches: 0.0152
trigger times: 6
Loss after 475137540 batches: 0.0154
trigger times: 7
Loss after 475268640 batches: 0.0152
trigger times: 8
Loss after 475399740 batches: 0.0152
trigger times: 9
Loss after 475530840 batches: 0.0152
trigger times: 10
Loss after 475661940 batches: 0.0150
trigger times: 11
Loss after 475793040 batches: 0.0152
trigger times: 12
Loss after 475924140 batches: 0.0151
trigger times: 13
Loss after 476055240 batches: 0.0150
trigger times: 14
Loss after 476186340 batches: 0.0150
trigger times: 15
Loss after 476317440 batches: 0.0148
trigger times: 16
Loss after 476448540 batches: 0.0150
trigger times: 17
Loss after 476579640 batches: 0.0149
trigger times: 18
Loss after 476710740 batches: 0.0149
trigger times: 19
Loss after 476841840 batches: 0.0150
trigger times: 20
Early stopping!
Start to test process.
Loss after 476972940 batches: 0.0146
Time to train on one home:  1023.7152323722839
trigger times: 0
Loss after 477104040 batches: 0.3115
trigger times: 0
Loss after 477235140 batches: 0.1235
trigger times: 1
Loss after 477366240 batches: 0.0712
trigger times: 0
Loss after 477497340 batches: 0.0556
trigger times: 0
Loss after 477628440 batches: 0.0471
trigger times: 0
Loss after 477759540 batches: 0.0425
trigger times: 0
Loss after 477890640 batches: 0.0392
trigger times: 1
Loss after 478021740 batches: 0.0380
trigger times: 2
Loss after 478152840 batches: 0.0363
trigger times: 0
Loss after 478283940 batches: 0.0333
trigger times: 0
Loss after 478415040 batches: 0.0329
trigger times: 1
Loss after 478546140 batches: 0.0324
trigger times: 2
Loss after 478677240 batches: 0.0324
trigger times: 3
Loss after 478808340 batches: 0.0317
trigger times: 4
Loss after 478939440 batches: 0.0314
trigger times: 0
Loss after 479070540 batches: 0.0299
trigger times: 1
Loss after 479201640 batches: 0.0298
trigger times: 2
Loss after 479332740 batches: 0.0299
trigger times: 0
Loss after 479463840 batches: 0.0295
trigger times: 1
Loss after 479594940 batches: 0.0286
trigger times: 2
Loss after 479726040 batches: 0.0282
trigger times: 0
Loss after 479857140 batches: 0.0276
trigger times: 1
Loss after 479988240 batches: 0.0262
trigger times: 2
Loss after 480119340 batches: 0.0277
trigger times: 3
Loss after 480250440 batches: 0.0273
trigger times: 4
Loss after 480381540 batches: 0.0255
trigger times: 0
Loss after 480512640 batches: 0.0257
trigger times: 1
Loss after 480643740 batches: 0.0265
trigger times: 2
Loss after 480774840 batches: 0.0263
trigger times: 3
Loss after 480905940 batches: 0.0282
trigger times: 4
Loss after 481037040 batches: 0.0248
trigger times: 5
Loss after 481168140 batches: 0.0259
trigger times: 0
Loss after 481299240 batches: 0.0266
trigger times: 0
Loss after 481430340 batches: 0.0280
trigger times: 1
Loss after 481561440 batches: 0.0255
trigger times: 2
Loss after 481692540 batches: 0.0243
trigger times: 3
Loss after 481823640 batches: 0.0263
trigger times: 4
Loss after 481954740 batches: 0.0252
trigger times: 5
Loss after 482085840 batches: 0.0239
trigger times: 6
Loss after 482216940 batches: 0.0244
trigger times: 7
Loss after 482348040 batches: 0.0252
trigger times: 8
Loss after 482479140 batches: 0.0243
trigger times: 0
Loss after 482610240 batches: 0.0246
trigger times: 1
Loss after 482741340 batches: 0.0249
trigger times: 2
Loss after 482872440 batches: 0.0252
trigger times: 3
Loss after 483003540 batches: 0.0233
trigger times: 4
Loss after 483134640 batches: 0.0244
trigger times: 5
Loss after 483265740 batches: 0.0233
trigger times: 6
Loss after 483396840 batches: 0.0233
trigger times: 7
Loss after 483527940 batches: 0.0225
trigger times: 8
Loss after 483659040 batches: 0.0237
trigger times: 9
Loss after 483790140 batches: 0.0229
trigger times: 10
Loss after 483921240 batches: 0.0228
trigger times: 11
Loss after 484052340 batches: 0.0231
trigger times: 12
Loss after 484183440 batches: 0.0220
trigger times: 13
Loss after 484314540 batches: 0.0227
trigger times: 0
Loss after 484445640 batches: 0.0226
trigger times: 1
Loss after 484576740 batches: 0.0225
trigger times: 2
Loss after 484707840 batches: 0.0221
trigger times: 3
Loss after 484838940 batches: 0.0214
trigger times: 4
Loss after 484970040 batches: 0.0224
trigger times: 5
Loss after 485101140 batches: 0.0218
trigger times: 6
Loss after 485232240 batches: 0.0224
trigger times: 7
Loss after 485363340 batches: 0.0217
trigger times: 8
Loss after 485494440 batches: 0.0215
trigger times: 9
Loss after 485625540 batches: 0.0216
trigger times: 10
Loss after 485756640 batches: 0.0215
trigger times: 11
Loss after 485887740 batches: 0.0214
trigger times: 12
Loss after 486018840 batches: 0.0214
trigger times: 0
Loss after 486149940 batches: 0.0226
trigger times: 1
Loss after 486281040 batches: 0.0223
trigger times: 2
Loss after 486412140 batches: 0.0216
trigger times: 3
Loss after 486543240 batches: 0.0205
trigger times: 4
Loss after 486674340 batches: 0.0211
trigger times: 5
Loss after 486805440 batches: 0.0215
trigger times: 0
Loss after 486936540 batches: 0.0208
trigger times: 1
Loss after 487067640 batches: 0.0205
trigger times: 2
Loss after 487198740 batches: 0.0202
trigger times: 3
Loss after 487329840 batches: 0.0208
trigger times: 4
Loss after 487460940 batches: 0.0200
trigger times: 5
Loss after 487592040 batches: 0.0211
trigger times: 6
Loss after 487723140 batches: 0.0207
trigger times: 7
Loss after 487854240 batches: 0.0212
trigger times: 8
Loss after 487985340 batches: 0.0202
trigger times: 9
Loss after 488116440 batches: 0.0215
trigger times: 10
Loss after 488247540 batches: 0.0198
trigger times: 11
Loss after 488378640 batches: 0.0192
trigger times: 12
Loss after 488509740 batches: 0.0195
trigger times: 13
Loss after 488640840 batches: 0.0195
trigger times: 14
Loss after 488771940 batches: 0.0202
trigger times: 15
Loss after 488903040 batches: 0.0190
trigger times: 16
Loss after 489034140 batches: 0.0201
trigger times: 17
Loss after 489165240 batches: 0.0199
trigger times: 18
Loss after 489296340 batches: 0.0204
trigger times: 19
Loss after 489427440 batches: 0.0210
trigger times: 20
Early stopping!
Start to test process.
Loss after 489558540 batches: 0.0198
Time to train on one home:  691.6073043346405
trigger times: 0
Loss after 489689640 batches: 0.0875
trigger times: 0
Loss after 489820740 batches: 0.0279
trigger times: 1
Loss after 489951840 batches: 0.0200
trigger times: 2
Loss after 490082940 batches: 0.0170
trigger times: 3
Loss after 490214040 batches: 0.0157
trigger times: 4
Loss after 490345140 batches: 0.0143
trigger times: 5
Loss after 490476240 batches: 0.0133
trigger times: 0
Loss after 490607340 batches: 0.0129
trigger times: 1
Loss after 490738440 batches: 0.0125
trigger times: 2
Loss after 490869540 batches: 0.0119
trigger times: 3
Loss after 491000640 batches: 0.0118
trigger times: 4
Loss after 491131740 batches: 0.0116
trigger times: 5
Loss after 491262840 batches: 0.0112
trigger times: 6
Loss after 491393940 batches: 0.0107
trigger times: 7
Loss after 491525040 batches: 0.0104
trigger times: 8
Loss after 491656140 batches: 0.0104
trigger times: 9
Loss after 491787240 batches: 0.0102
trigger times: 10
Loss after 491918340 batches: 0.0101
trigger times: 0
Loss after 492049440 batches: 0.0100
trigger times: 1
Loss after 492180540 batches: 0.0099
trigger times: 0
Loss after 492311640 batches: 0.0099
trigger times: 1
Loss after 492442740 batches: 0.0095
trigger times: 2
Loss after 492573840 batches: 0.0094
trigger times: 0
Loss after 492704940 batches: 0.0094
trigger times: 1
Loss after 492836040 batches: 0.0091
trigger times: 2
Loss after 492967140 batches: 0.0091
trigger times: 3
Loss after 493098240 batches: 0.0091
trigger times: 0
Loss after 493229340 batches: 0.0092
trigger times: 1
Loss after 493360440 batches: 0.0088
trigger times: 2
Loss after 493491540 batches: 0.0087
trigger times: 3
Loss after 493622640 batches: 0.0089
trigger times: 4
Loss after 493753740 batches: 0.0088
trigger times: 5
Loss after 493884840 batches: 0.0088
trigger times: 6
Loss after 494015940 batches: 0.0084
trigger times: 7
Loss after 494147040 batches: 0.0085
trigger times: 8
Loss after 494278140 batches: 0.0085
trigger times: 9
Loss after 494409240 batches: 0.0083
trigger times: 10
Loss after 494540340 batches: 0.0083
trigger times: 11
Loss after 494671440 batches: 0.0081
trigger times: 12
Loss after 494802540 batches: 0.0082
trigger times: 13
Loss after 494933640 batches: 0.0081
trigger times: 14
Loss after 495064740 batches: 0.0081
trigger times: 0
Loss after 495195840 batches: 0.0081
trigger times: 1
Loss after 495326940 batches: 0.0079
trigger times: 2
Loss after 495458040 batches: 0.0080
trigger times: 3
Loss after 495589140 batches: 0.0078
trigger times: 0
Loss after 495720240 batches: 0.0079
trigger times: 1
Loss after 495851340 batches: 0.0077
trigger times: 2
Loss after 495982440 batches: 0.0078
trigger times: 0
Loss after 496113540 batches: 0.0077
trigger times: 1
Loss after 496244640 batches: 0.0077
trigger times: 2
Loss after 496375740 batches: 0.0076
trigger times: 3
Loss after 496506840 batches: 0.0075
trigger times: 4
Loss after 496637940 batches: 0.0075
trigger times: 5
Loss after 496769040 batches: 0.0075
trigger times: 6
Loss after 496900140 batches: 0.0077
trigger times: 0
Loss after 497031240 batches: 0.0077
trigger times: 1
Loss after 497162340 batches: 0.0074
trigger times: 2
Loss after 497293440 batches: 0.0073
trigger times: 3
Loss after 497424540 batches: 0.0075
trigger times: 4
Loss after 497555640 batches: 0.0075
trigger times: 5
Loss after 497686740 batches: 0.0073
trigger times: 6
Loss after 497817840 batches: 0.0071
trigger times: 7
Loss after 497948940 batches: 0.0072
trigger times: 8
Loss after 498080040 batches: 0.0072
trigger times: 9
Loss after 498211140 batches: 0.0070
trigger times: 10
Loss after 498342240 batches: 0.0070
trigger times: 11
Loss after 498473340 batches: 0.0071
trigger times: 12
Loss after 498604440 batches: 0.0071
trigger times: 13
Loss after 498735540 batches: 0.0070
trigger times: 0
Loss after 498866640 batches: 0.0070
trigger times: 1
Loss after 498997740 batches: 0.0070
trigger times: 2
Loss after 499128840 batches: 0.0069
trigger times: 3
Loss after 499259940 batches: 0.0069
trigger times: 4
Loss after 499391040 batches: 0.0070
trigger times: 0
Loss after 499522140 batches: 0.0070
trigger times: 1
Loss after 499653240 batches: 0.0069
trigger times: 2
Loss after 499784340 batches: 0.0068
trigger times: 3
Loss after 499915440 batches: 0.0067
trigger times: 4
Loss after 500046540 batches: 0.0068
trigger times: 5
Loss after 500177640 batches: 0.0067
trigger times: 6
Loss after 500308740 batches: 0.0068
trigger times: 7
Loss after 500439840 batches: 0.0068
trigger times: 0
Loss after 500570940 batches: 0.0068
trigger times: 1
Loss after 500702040 batches: 0.0067
trigger times: 2
Loss after 500833140 batches: 0.0067
trigger times: 3
Loss after 500964240 batches: 0.0067
trigger times: 4
Loss after 501095340 batches: 0.0066
trigger times: 0
Loss after 501226440 batches: 0.0067
trigger times: 1
Loss after 501357540 batches: 0.0066
trigger times: 2
Loss after 501488640 batches: 0.0065
trigger times: 3
Loss after 501619740 batches: 0.0063
trigger times: 4
Loss after 501750840 batches: 0.0064
trigger times: 5
Loss after 501881940 batches: 0.0064
trigger times: 0
Loss after 502013040 batches: 0.0064
trigger times: 1
Loss after 502144140 batches: 0.0063
trigger times: 2
Loss after 502275240 batches: 0.0064
trigger times: 3
Loss after 502406340 batches: 0.0065
trigger times: 4
Loss after 502537440 batches: 0.0064
trigger times: 5
Loss after 502668540 batches: 0.0063
trigger times: 6
Loss after 502799640 batches: 0.0064
trigger times: 7
Loss after 502930740 batches: 0.0064
trigger times: 8
Loss after 503061840 batches: 0.0064
trigger times: 9
Loss after 503192940 batches: 0.0063
trigger times: 10
Loss after 503324040 batches: 0.0062
trigger times: 11
Loss after 503455140 batches: 0.0062
trigger times: 12
Loss after 503586240 batches: 0.0063
trigger times: 13
Loss after 503717340 batches: 0.0062
trigger times: 14
Loss after 503848440 batches: 0.0062
trigger times: 15
Loss after 503979540 batches: 0.0062
trigger times: 16
Loss after 504110640 batches: 0.0062
trigger times: 17
Loss after 504241740 batches: 0.0064
trigger times: 18
Loss after 504372840 batches: 0.0063
trigger times: 0
Loss after 504503940 batches: 0.0061
trigger times: 1
Loss after 504635040 batches: 0.0061
trigger times: 2
Loss after 504766140 batches: 0.0062
trigger times: 3
Loss after 504897240 batches: 0.0061
trigger times: 4
Loss after 505028340 batches: 0.0062
trigger times: 5
Loss after 505159440 batches: 0.0061
trigger times: 6
Loss after 505290540 batches: 0.0060
trigger times: 7
Loss after 505421640 batches: 0.0062
trigger times: 8
Loss after 505552740 batches: 0.0060
trigger times: 9
Loss after 505683840 batches: 0.0060
trigger times: 10
Loss after 505814940 batches: 0.0060
trigger times: 11
Loss after 505946040 batches: 0.0060
trigger times: 12
Loss after 506077140 batches: 0.0059
trigger times: 13
Loss after 506208240 batches: 0.0060
trigger times: 14
Loss after 506339340 batches: 0.0058
trigger times: 15
Loss after 506470440 batches: 0.0059
trigger times: 16
Loss after 506601540 batches: 0.0059
trigger times: 17
Loss after 506732640 batches: 0.0058
trigger times: 18
Loss after 506863740 batches: 0.0058
trigger times: 19
Loss after 506994840 batches: 0.0057
trigger times: 20
Early stopping!
Start to test process.
Loss after 507125940 batches: 0.0057
Time to train on one home:  959.3932564258575
trigger times: 0
Loss after 507204540 batches: 0.2673
trigger times: 0
Loss after 507283140 batches: 0.0735
trigger times: 0
Loss after 507361740 batches: 0.0452
trigger times: 1
Loss after 507440340 batches: 0.0367
trigger times: 2
Loss after 507518940 batches: 0.0321
trigger times: 0
Loss after 507597540 batches: 0.0305
trigger times: 1
Loss after 507676140 batches: 0.0285
trigger times: 2
Loss after 507754740 batches: 0.0263
trigger times: 3
Loss after 507833340 batches: 0.0251
trigger times: 4
Loss after 507911940 batches: 0.0245
trigger times: 5
Loss after 507990540 batches: 0.0235
trigger times: 6
Loss after 508069140 batches: 0.0231
trigger times: 7
Loss after 508147740 batches: 0.0230
trigger times: 8
Loss after 508226340 batches: 0.0220
trigger times: 9
Loss after 508304940 batches: 0.0218
trigger times: 10
Loss after 508383540 batches: 0.0212
trigger times: 11
Loss after 508462140 batches: 0.0204
trigger times: 12
Loss after 508540740 batches: 0.0202
trigger times: 13
Loss after 508619340 batches: 0.0199
trigger times: 14
Loss after 508697940 batches: 0.0193
trigger times: 15
Loss after 508776540 batches: 0.0193
trigger times: 16
Loss after 508855140 batches: 0.0191
trigger times: 17
Loss after 508933740 batches: 0.0191
trigger times: 18
Loss after 509012340 batches: 0.0187
trigger times: 19
Loss after 509090940 batches: 0.0185
trigger times: 20
Early stopping!
Start to test process.
Loss after 509169540 batches: 0.0183
Time to train on one home:  129.97591853141785
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097, 0.016851266692967333, 0.01539797175087967]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983], [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013], [0.5549709763791826, 0.4001270641513627, 0.48658498833859326, 1.0394903420759227, 0.4914126622529051, 24.558331909615507, 1517.0215]]
Round_9_results:  [0.5549709763791826, 0.4001270641513627, 0.48658498833859326, 1.0394903420759227, 0.4914126622529051, 24.558331909615507, 1517.0215]
trigger times: 0
Loss after 509300640 batches: 0.1252
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4132 < 4133; dropping {'Training_Loss': 0.12520987498310376, 'Validation_Loss': 0.21521219611167908, 'Training_R2': 0.873991431926624, 'Validation_R2': 0.7999812259032684, 'Training_F1': 0.8143626984311125, 'Validation_F1': 0.7109124271129361, 'Training_NEP': 0.37121262631753044, 'Validation_NEP': 0.5295895099714885, 'Training_NDE': 0.09459686870823275, 'Validation_NDE': 0.1592844078790675, 'Training_MAE': 12.293825989613362, 'Validation_MAE': 14.523541495706018, 'Training_MSE': 416.21143, 'Validation_MSE': 588.2329}.
trigger times: 0
Loss after 509431740 batches: 0.0312
trigger times: 1
Loss after 509562840 batches: 0.0229
trigger times: 2
Loss after 509693940 batches: 0.0196
trigger times: 3
Loss after 509825040 batches: 0.0180
trigger times: 4
Loss after 509956140 batches: 0.0166
trigger times: 5
Loss after 510087240 batches: 0.0155
trigger times: 0
Loss after 510218340 batches: 0.0149
trigger times: 1
Loss after 510349440 batches: 0.0142
trigger times: 2
Loss after 510480540 batches: 0.0138
trigger times: 3
Loss after 510611640 batches: 0.0134
trigger times: 0
Loss after 510742740 batches: 0.0134
trigger times: 1
Loss after 510873840 batches: 0.0129
trigger times: 0
Loss after 511004940 batches: 0.0127
trigger times: 1
Loss after 511136040 batches: 0.0126
trigger times: 2
Loss after 511267140 batches: 0.0120
trigger times: 3
Loss after 511398240 batches: 0.0120
trigger times: 4
Loss after 511529340 batches: 0.0115
trigger times: 5
Loss after 511660440 batches: 0.0114
trigger times: 6
Loss after 511791540 batches: 0.0113
trigger times: 7
Loss after 511922640 batches: 0.0113
trigger times: 8
Loss after 512053740 batches: 0.0108
trigger times: 9
Loss after 512184840 batches: 0.0110
trigger times: 10
Loss after 512315940 batches: 0.0107
trigger times: 11
Loss after 512447040 batches: 0.0107
trigger times: 12
Loss after 512578140 batches: 0.0108
trigger times: 13
Loss after 512709240 batches: 0.0105
trigger times: 14
Loss after 512840340 batches: 0.0104
trigger times: 15
Loss after 512971440 batches: 0.0102
trigger times: 16
Loss after 513102540 batches: 0.0103
trigger times: 17
Loss after 513233640 batches: 0.0101
trigger times: 18
Loss after 513364740 batches: 0.0101
trigger times: 19
Loss after 513495840 batches: 0.0099
trigger times: 20
Early stopping!
Start to test process.
Loss after 513626940 batches: 0.0099
Time to train on one home:  252.2163188457489
trigger times: 0
Loss after 513729540 batches: 0.3535
trigger times: 1
Loss after 513832140 batches: 0.1158
trigger times: 2
Loss after 513934740 batches: 0.0698
trigger times: 3
Loss after 514037340 batches: 0.0571
trigger times: 4
Loss after 514139940 batches: 0.0489
trigger times: 5
Loss after 514242540 batches: 0.0427
trigger times: 6
Loss after 514345140 batches: 0.0387
trigger times: 7
Loss after 514447740 batches: 0.0367
trigger times: 8
Loss after 514550340 batches: 0.0340
trigger times: 9
Loss after 514652940 batches: 0.0325
trigger times: 10
Loss after 514755540 batches: 0.0302
trigger times: 11
Loss after 514858140 batches: 0.0311
trigger times: 12
Loss after 514960740 batches: 0.0297
trigger times: 13
Loss after 515063340 batches: 0.0287
trigger times: 14
Loss after 515165940 batches: 0.0277
trigger times: 15
Loss after 515268540 batches: 0.0261
trigger times: 16
Loss after 515371140 batches: 0.0255
trigger times: 17
Loss after 515473740 batches: 0.0249
trigger times: 18
Loss after 515576340 batches: 0.0262
trigger times: 19
Loss after 515678940 batches: 0.0248
trigger times: 20
Early stopping!
Start to test process.
Loss after 515781540 batches: 0.0236
Time to train on one home:  130.75671935081482
trigger times: 0
Loss after 515912640 batches: 0.1846
trigger times: 1
Loss after 516043740 batches: 0.0551
trigger times: 0
Loss after 516174840 batches: 0.0394
trigger times: 0
Loss after 516305940 batches: 0.0340
trigger times: 1
Loss after 516437040 batches: 0.0300
trigger times: 0
Loss after 516568140 batches: 0.0277
trigger times: 1
Loss after 516699240 batches: 0.0265
trigger times: 2
Loss after 516830340 batches: 0.0249
trigger times: 3
Loss after 516961440 batches: 0.0238
trigger times: 4
Loss after 517092540 batches: 0.0228
trigger times: 5
Loss after 517223640 batches: 0.0221
trigger times: 6
Loss after 517354740 batches: 0.0214
trigger times: 7
Loss after 517485840 batches: 0.0211
trigger times: 8
Loss after 517616940 batches: 0.0206
trigger times: 9
Loss after 517748040 batches: 0.0206
trigger times: 10
Loss after 517879140 batches: 0.0197
trigger times: 11
Loss after 518010240 batches: 0.0191
trigger times: 12
Loss after 518141340 batches: 0.0188
trigger times: 13
Loss after 518272440 batches: 0.0188
trigger times: 0
Loss after 518403540 batches: 0.0185
trigger times: 1
Loss after 518534640 batches: 0.0184
trigger times: 2
Loss after 518665740 batches: 0.0179
trigger times: 3
Loss after 518796840 batches: 0.0177
trigger times: 4
Loss after 518927940 batches: 0.0178
trigger times: 5
Loss after 519059040 batches: 0.0173
trigger times: 6
Loss after 519190140 batches: 0.0172
trigger times: 7
Loss after 519321240 batches: 0.0171
trigger times: 8
Loss after 519452340 batches: 0.0165
trigger times: 9
Loss after 519583440 batches: 0.0166
trigger times: 10
Loss after 519714540 batches: 0.0165
trigger times: 11
Loss after 519845640 batches: 0.0165
trigger times: 12
Loss after 519976740 batches: 0.0162
trigger times: 13
Loss after 520107840 batches: 0.0163
trigger times: 14
Loss after 520238940 batches: 0.0158
trigger times: 15
Loss after 520370040 batches: 0.0159
trigger times: 16
Loss after 520501140 batches: 0.0156
trigger times: 17
Loss after 520632240 batches: 0.0156
trigger times: 18
Loss after 520763340 batches: 0.0159
trigger times: 19
Loss after 520894440 batches: 0.0152
trigger times: 20
Early stopping!
Start to test process.
Loss after 521025540 batches: 0.0152
Time to train on one home:  294.752858877182
trigger times: 0
Loss after 521156640 batches: 0.2196
trigger times: 0
Loss after 521287740 batches: 0.0677
trigger times: 0
Loss after 521418840 batches: 0.0481
trigger times: 1
Loss after 521549940 batches: 0.0407
trigger times: 2
Loss after 521681040 batches: 0.0362
trigger times: 3
Loss after 521812140 batches: 0.0345
trigger times: 4
Loss after 521943240 batches: 0.0323
trigger times: 5
Loss after 522074340 batches: 0.0309
trigger times: 6
Loss after 522205440 batches: 0.0297
trigger times: 7
Loss after 522336540 batches: 0.0284
trigger times: 8
Loss after 522467640 batches: 0.0279
trigger times: 9
Loss after 522598740 batches: 0.0265
trigger times: 0
Loss after 522729840 batches: 0.0261
trigger times: 1
Loss after 522860940 batches: 0.0259
trigger times: 2
Loss after 522992040 batches: 0.0255
trigger times: 3
Loss after 523123140 batches: 0.0253
trigger times: 0
Loss after 523254240 batches: 0.0244
trigger times: 1
Loss after 523385340 batches: 0.0237
trigger times: 2
Loss after 523516440 batches: 0.0235
trigger times: 3
Loss after 523647540 batches: 0.0235
trigger times: 4
Loss after 523778640 batches: 0.0229
trigger times: 5
Loss after 523909740 batches: 0.0226
trigger times: 6
Loss after 524040840 batches: 0.0223
trigger times: 7
Loss after 524171940 batches: 0.0226
trigger times: 8
Loss after 524303040 batches: 0.0220
trigger times: 0
Loss after 524434140 batches: 0.0220
trigger times: 1
Loss after 524565240 batches: 0.0219
trigger times: 2
Loss after 524696340 batches: 0.0215
trigger times: 3
Loss after 524827440 batches: 0.0214
trigger times: 4
Loss after 524958540 batches: 0.0209
trigger times: 5
Loss after 525089640 batches: 0.0211
trigger times: 6
Loss after 525220740 batches: 0.0210
trigger times: 7
Loss after 525351840 batches: 0.0209
trigger times: 8
Loss after 525482940 batches: 0.0204
trigger times: 9
Loss after 525614040 batches: 0.0203
trigger times: 10
Loss after 525745140 batches: 0.0202
trigger times: 11
Loss after 525876240 batches: 0.0201
trigger times: 12
Loss after 526007340 batches: 0.0200
trigger times: 13
Loss after 526138440 batches: 0.0198
trigger times: 14
Loss after 526269540 batches: 0.0195
trigger times: 15
Loss after 526400640 batches: 0.0197
trigger times: 16
Loss after 526531740 batches: 0.0192
trigger times: 17
Loss after 526662840 batches: 0.0192
trigger times: 18
Loss after 526793940 batches: 0.0193
trigger times: 19
Loss after 526925040 batches: 0.0193
trigger times: 20
Early stopping!
Start to test process.
Loss after 527056140 batches: 0.0192
Time to train on one home:  337.3632004261017
trigger times: 0
Loss after 527184780 batches: 0.1329
trigger times: 0
Loss after 527313420 batches: 0.0377
trigger times: 0
Loss after 527442060 batches: 0.0276
trigger times: 0
Loss after 527570700 batches: 0.0240
trigger times: 0
Loss after 527699340 batches: 0.0214
trigger times: 1
Loss after 527827980 batches: 0.0200
trigger times: 2
Loss after 527956620 batches: 0.0191
trigger times: 0
Loss after 528085260 batches: 0.0181
trigger times: 1
Loss after 528213900 batches: 0.0173
trigger times: 2
Loss after 528342540 batches: 0.0167
trigger times: 3
Loss after 528471180 batches: 0.0165
trigger times: 4
Loss after 528599820 batches: 0.0159
trigger times: 5
Loss after 528728460 batches: 0.0156
trigger times: 6
Loss after 528857100 batches: 0.0153
trigger times: 7
Loss after 528985740 batches: 0.0153
trigger times: 8
Loss after 529114380 batches: 0.0149
trigger times: 9
Loss after 529243020 batches: 0.0149
trigger times: 10
Loss after 529371660 batches: 0.0142
trigger times: 11
Loss after 529500300 batches: 0.0141
trigger times: 12
Loss after 529628940 batches: 0.0137
trigger times: 13
Loss after 529757580 batches: 0.0137
trigger times: 14
Loss after 529886220 batches: 0.0137
trigger times: 15
Loss after 530014860 batches: 0.0132
trigger times: 16
Loss after 530143500 batches: 0.0131
trigger times: 17
Loss after 530272140 batches: 0.0134
trigger times: 18
Loss after 530400780 batches: 0.0130
trigger times: 19
Loss after 530529420 batches: 0.0129
trigger times: 20
Early stopping!
Start to test process.
Loss after 530658060 batches: 0.0128
Time to train on one home:  206.857563495636
trigger times: 0
Loss after 530789160 batches: 0.4318
trigger times: 0
Loss after 530920260 batches: 0.1289
trigger times: 0
Loss after 531051360 batches: 0.0797
trigger times: 1
Loss after 531182460 batches: 0.0633
trigger times: 0
Loss after 531313560 batches: 0.0538
trigger times: 1
Loss after 531444660 batches: 0.0490
trigger times: 2
Loss after 531575760 batches: 0.0452
trigger times: 0
Loss after 531706860 batches: 0.0429
trigger times: 1
Loss after 531837960 batches: 0.0404
trigger times: 0
Loss after 531969060 batches: 0.0384
trigger times: 1
Loss after 532100160 batches: 0.0364
trigger times: 0
Loss after 532231260 batches: 0.0354
trigger times: 0
Loss after 532362360 batches: 0.0344
trigger times: 1
Loss after 532493460 batches: 0.0335
trigger times: 0
Loss after 532624560 batches: 0.0325
trigger times: 1
Loss after 532755660 batches: 0.0319
trigger times: 2
Loss after 532886760 batches: 0.0310
trigger times: 3
Loss after 533017860 batches: 0.0303
trigger times: 0
Loss after 533148960 batches: 0.0300
trigger times: 1
Loss after 533280060 batches: 0.0290
trigger times: 2
Loss after 533411160 batches: 0.0286
trigger times: 0
Loss after 533542260 batches: 0.0283
trigger times: 1
Loss after 533673360 batches: 0.0277
trigger times: 0
Loss after 533804460 batches: 0.0274
trigger times: 1
Loss after 533935560 batches: 0.0272
trigger times: 2
Loss after 534066660 batches: 0.0266
trigger times: 0
Loss after 534197760 batches: 0.0260
trigger times: 1
Loss after 534328860 batches: 0.0259
trigger times: 2
Loss after 534459960 batches: 0.0258
trigger times: 3
Loss after 534591060 batches: 0.0253
trigger times: 0
Loss after 534722160 batches: 0.0251
trigger times: 1
Loss after 534853260 batches: 0.0247
trigger times: 2
Loss after 534984360 batches: 0.0242
trigger times: 3
Loss after 535115460 batches: 0.0243
trigger times: 4
Loss after 535246560 batches: 0.0242
trigger times: 0
Loss after 535377660 batches: 0.0236
trigger times: 1
Loss after 535508760 batches: 0.0237
trigger times: 2
Loss after 535639860 batches: 0.0236
trigger times: 3
Loss after 535770960 batches: 0.0233
trigger times: 4
Loss after 535902060 batches: 0.0233
trigger times: 0
Loss after 536033160 batches: 0.0228
trigger times: 1
Loss after 536164260 batches: 0.0230
trigger times: 2
Loss after 536295360 batches: 0.0228
trigger times: 0
Loss after 536426460 batches: 0.0222
trigger times: 1
Loss after 536557560 batches: 0.0227
trigger times: 0
Loss after 536688660 batches: 0.0221
trigger times: 0
Loss after 536819760 batches: 0.0218
trigger times: 1
Loss after 536950860 batches: 0.0217
trigger times: 2
Loss after 537081960 batches: 0.0215
trigger times: 3
Loss after 537213060 batches: 0.0214
trigger times: 4
Loss after 537344160 batches: 0.0211
trigger times: 0
Loss after 537475260 batches: 0.0212
trigger times: 0
Loss after 537606360 batches: 0.0209
trigger times: 1
Loss after 537737460 batches: 0.0208
trigger times: 2
Loss after 537868560 batches: 0.0205
trigger times: 3
Loss after 537999660 batches: 0.0204
trigger times: 4
Loss after 538130760 batches: 0.0204
trigger times: 5
Loss after 538261860 batches: 0.0204
trigger times: 6
Loss after 538392960 batches: 0.0206
trigger times: 0
Loss after 538524060 batches: 0.0203
trigger times: 1
Loss after 538655160 batches: 0.0199
trigger times: 2
Loss after 538786260 batches: 0.0200
trigger times: 3
Loss after 538917360 batches: 0.0200
trigger times: 4
Loss after 539048460 batches: 0.0199
trigger times: 5
Loss after 539179560 batches: 0.0199
trigger times: 6
Loss after 539310660 batches: 0.0197
trigger times: 7
Loss after 539441760 batches: 0.0194
trigger times: 0
Loss after 539572860 batches: 0.0194
trigger times: 1
Loss after 539703960 batches: 0.0197
trigger times: 2
Loss after 539835060 batches: 0.0194
trigger times: 0
Loss after 539966160 batches: 0.0193
trigger times: 1
Loss after 540097260 batches: 0.0195
trigger times: 2
Loss after 540228360 batches: 0.0188
trigger times: 3
Loss after 540359460 batches: 0.0186
trigger times: 4
Loss after 540490560 batches: 0.0185
trigger times: 5
Loss after 540621660 batches: 0.0186
trigger times: 6
Loss after 540752760 batches: 0.0186
trigger times: 7
Loss after 540883860 batches: 0.0187
trigger times: 0
Loss after 541014960 batches: 0.0186
trigger times: 0
Loss after 541146060 batches: 0.0186
trigger times: 1
Loss after 541277160 batches: 0.0187
trigger times: 2
Loss after 541408260 batches: 0.0184
trigger times: 3
Loss after 541539360 batches: 0.0183
trigger times: 4
Loss after 541670460 batches: 0.0182
trigger times: 5
Loss after 541801560 batches: 0.0184
trigger times: 6
Loss after 541932660 batches: 0.0181
trigger times: 7
Loss after 542063760 batches: 0.0182
trigger times: 8
Loss after 542194860 batches: 0.0182
trigger times: 9
Loss after 542325960 batches: 0.0182
trigger times: 10
Loss after 542457060 batches: 0.0178
trigger times: 11
Loss after 542588160 batches: 0.0181
trigger times: 12
Loss after 542719260 batches: 0.0178
trigger times: 13
Loss after 542850360 batches: 0.0177
trigger times: 14
Loss after 542981460 batches: 0.0175
trigger times: 15
Loss after 543112560 batches: 0.0175
trigger times: 16
Loss after 543243660 batches: 0.0177
trigger times: 17
Loss after 543374760 batches: 0.0178
trigger times: 18
Loss after 543505860 batches: 0.0175
trigger times: 19
Loss after 543636960 batches: 0.0176
trigger times: 20
Early stopping!
Start to test process.
Loss after 543768060 batches: 0.0176
Time to train on one home:  718.2090539932251
trigger times: 0
Loss after 543899160 batches: 0.3296
trigger times: 0
Loss after 544030260 batches: 0.1083
trigger times: 1
Loss after 544161360 batches: 0.0662
trigger times: 0
Loss after 544292460 batches: 0.0526
trigger times: 1
Loss after 544423560 batches: 0.0431
trigger times: 2
Loss after 544554660 batches: 0.0390
trigger times: 0
Loss after 544685760 batches: 0.0368
trigger times: 1
Loss after 544816860 batches: 0.0368
trigger times: 0
Loss after 544947960 batches: 0.0339
trigger times: 0
Loss after 545079060 batches: 0.0330
trigger times: 1
Loss after 545210160 batches: 0.0321
trigger times: 2
Loss after 545341260 batches: 0.0310
trigger times: 0
Loss after 545472360 batches: 0.0304
trigger times: 1
Loss after 545603460 batches: 0.0304
trigger times: 2
Loss after 545734560 batches: 0.0308
trigger times: 0
Loss after 545865660 batches: 0.0293
trigger times: 1
Loss after 545996760 batches: 0.0285
trigger times: 2
Loss after 546127860 batches: 0.0274
trigger times: 0
Loss after 546258960 batches: 0.0275
trigger times: 1
Loss after 546390060 batches: 0.0266
trigger times: 0
Loss after 546521160 batches: 0.0276
trigger times: 0
Loss after 546652260 batches: 0.0273
trigger times: 1
Loss after 546783360 batches: 0.0270
trigger times: 0
Loss after 546914460 batches: 0.0263
trigger times: 1
Loss after 547045560 batches: 0.0269
trigger times: 2
Loss after 547176660 batches: 0.0269
trigger times: 3
Loss after 547307760 batches: 0.0252
trigger times: 4
Loss after 547438860 batches: 0.0251
trigger times: 5
Loss after 547569960 batches: 0.0248
trigger times: 6
Loss after 547701060 batches: 0.0247
trigger times: 7
Loss after 547832160 batches: 0.0254
trigger times: 8
Loss after 547963260 batches: 0.0253
trigger times: 9
Loss after 548094360 batches: 0.0258
trigger times: 10
Loss after 548225460 batches: 0.0233
trigger times: 11
Loss after 548356560 batches: 0.0254
trigger times: 12
Loss after 548487660 batches: 0.0242
trigger times: 0
Loss after 548618760 batches: 0.0229
trigger times: 1
Loss after 548749860 batches: 0.0226
trigger times: 2
Loss after 548880960 batches: 0.0230
trigger times: 0
Loss after 549012060 batches: 0.0233
trigger times: 1
Loss after 549143160 batches: 0.0222
trigger times: 0
Loss after 549274260 batches: 0.0230
trigger times: 1
Loss after 549405360 batches: 0.0224
trigger times: 2
Loss after 549536460 batches: 0.0225
trigger times: 0
Loss after 549667560 batches: 0.0225
trigger times: 1
Loss after 549798660 batches: 0.0217
trigger times: 2
Loss after 549929760 batches: 0.0216
trigger times: 3
Loss after 550060860 batches: 0.0215
trigger times: 4
Loss after 550191960 batches: 0.0214
trigger times: 0
Loss after 550323060 batches: 0.0218
trigger times: 1
Loss after 550454160 batches: 0.0221
trigger times: 2
Loss after 550585260 batches: 0.0226
trigger times: 3
Loss after 550716360 batches: 0.0214
trigger times: 4
Loss after 550847460 batches: 0.0213
trigger times: 5
Loss after 550978560 batches: 0.0220
trigger times: 6
Loss after 551109660 batches: 0.0218
trigger times: 7
Loss after 551240760 batches: 0.0215
trigger times: 0
Loss after 551371860 batches: 0.0213
trigger times: 1
Loss after 551502960 batches: 0.0204
trigger times: 2
Loss after 551634060 batches: 0.0211
trigger times: 3
Loss after 551765160 batches: 0.0201
trigger times: 4
Loss after 551896260 batches: 0.0208
trigger times: 5
Loss after 552027360 batches: 0.0210
trigger times: 6
Loss after 552158460 batches: 0.0204
trigger times: 7
Loss after 552289560 batches: 0.0202
trigger times: 8
Loss after 552420660 batches: 0.0202
trigger times: 9
Loss after 552551760 batches: 0.0208
trigger times: 10
Loss after 552682860 batches: 0.0208
trigger times: 11
Loss after 552813960 batches: 0.0196
trigger times: 12
Loss after 552945060 batches: 0.0204
trigger times: 13
Loss after 553076160 batches: 0.0209
trigger times: 14
Loss after 553207260 batches: 0.0203
trigger times: 0
Loss after 553338360 batches: 0.0199
trigger times: 1
Loss after 553469460 batches: 0.0204
trigger times: 2
Loss after 553600560 batches: 0.0213
trigger times: 3
Loss after 553731660 batches: 0.0199
trigger times: 4
Loss after 553862760 batches: 0.0200
trigger times: 5
Loss after 553993860 batches: 0.0194
trigger times: 6
Loss after 554124960 batches: 0.0207
trigger times: 7
Loss after 554256060 batches: 0.0193
trigger times: 8
Loss after 554387160 batches: 0.0184
trigger times: 9
Loss after 554518260 batches: 0.0200
trigger times: 10
Loss after 554649360 batches: 0.0196
trigger times: 11
Loss after 554780460 batches: 0.0200
trigger times: 12
Loss after 554911560 batches: 0.0192
trigger times: 13
Loss after 555042660 batches: 0.0189
trigger times: 14
Loss after 555173760 batches: 0.0184
trigger times: 15
Loss after 555304860 batches: 0.0199
trigger times: 16
Loss after 555435960 batches: 0.0196
trigger times: 17
Loss after 555567060 batches: 0.0188
trigger times: 18
Loss after 555698160 batches: 0.0194
trigger times: 19
Loss after 555829260 batches: 0.0195
trigger times: 20
Early stopping!
Start to test process.
Loss after 555960360 batches: 0.0193
Time to train on one home:  669.2782647609711
trigger times: 0
Loss after 556091460 batches: 0.0763
trigger times: 1
Loss after 556222560 batches: 0.0244
trigger times: 2
Loss after 556353660 batches: 0.0173
trigger times: 3
Loss after 556484760 batches: 0.0148
trigger times: 4
Loss after 556615860 batches: 0.0137
trigger times: 5
Loss after 556746960 batches: 0.0127
trigger times: 6
Loss after 556878060 batches: 0.0119
trigger times: 7
Loss after 557009160 batches: 0.0115
trigger times: 8
Loss after 557140260 batches: 0.0113
trigger times: 9
Loss after 557271360 batches: 0.0109
trigger times: 10
Loss after 557402460 batches: 0.0106
trigger times: 11
Loss after 557533560 batches: 0.0101
trigger times: 12
Loss after 557664660 batches: 0.0098
trigger times: 13
Loss after 557795760 batches: 0.0097
trigger times: 14
Loss after 557926860 batches: 0.0095
trigger times: 15
Loss after 558057960 batches: 0.0094
trigger times: 16
Loss after 558189060 batches: 0.0091
trigger times: 17
Loss after 558320160 batches: 0.0090
trigger times: 18
Loss after 558451260 batches: 0.0091
trigger times: 19
Loss after 558582360 batches: 0.0086
trigger times: 20
Early stopping!
Start to test process.
Loss after 558713460 batches: 0.0086
Time to train on one home:  159.90431380271912
trigger times: 0
Loss after 558792060 batches: 0.3055
trigger times: 0
Loss after 558870660 batches: 0.0738
trigger times: 1
Loss after 558949260 batches: 0.0447
trigger times: 2
Loss after 559027860 batches: 0.0368
trigger times: 3
Loss after 559106460 batches: 0.0323
trigger times: 4
Loss after 559185060 batches: 0.0300
trigger times: 0
Loss after 559263660 batches: 0.0280
trigger times: 1
Loss after 559342260 batches: 0.0276
trigger times: 2
Loss after 559420860 batches: 0.0256
trigger times: 3
Loss after 559499460 batches: 0.0244
trigger times: 4
Loss after 559578060 batches: 0.0237
trigger times: 0
Loss after 559656660 batches: 0.0237
trigger times: 1
Loss after 559735260 batches: 0.0227
trigger times: 2
Loss after 559813860 batches: 0.0219
trigger times: 3
Loss after 559892460 batches: 0.0218
trigger times: 0
Loss after 559971060 batches: 0.0215
trigger times: 1
Loss after 560049660 batches: 0.0203
trigger times: 2
Loss after 560128260 batches: 0.0200
trigger times: 0
Loss after 560206860 batches: 0.0194
trigger times: 1
Loss after 560285460 batches: 0.0192
trigger times: 2
Loss after 560364060 batches: 0.0193
trigger times: 3
Loss after 560442660 batches: 0.0191
trigger times: 0
Loss after 560521260 batches: 0.0187
trigger times: 1
Loss after 560599860 batches: 0.0182
trigger times: 2
Loss after 560678460 batches: 0.0181
trigger times: 3
Loss after 560757060 batches: 0.0186
trigger times: 4
Loss after 560835660 batches: 0.0183
trigger times: 5
Loss after 560914260 batches: 0.0178
trigger times: 6
Loss after 560992860 batches: 0.0172
trigger times: 7
Loss after 561071460 batches: 0.0171
trigger times: 8
Loss after 561150060 batches: 0.0173
trigger times: 9
Loss after 561228660 batches: 0.0170
trigger times: 10
Loss after 561307260 batches: 0.0165
trigger times: 11
Loss after 561385860 batches: 0.0172
trigger times: 12
Loss after 561464460 batches: 0.0170
trigger times: 0
Loss after 561543060 batches: 0.0170
trigger times: 1
Loss after 561621660 batches: 0.0164
trigger times: 2
Loss after 561700260 batches: 0.0166
trigger times: 3
Loss after 561778860 batches: 0.0161
trigger times: 4
Loss after 561857460 batches: 0.0155
trigger times: 5
Loss after 561936060 batches: 0.0161
trigger times: 6
Loss after 562014660 batches: 0.0158
trigger times: 7
Loss after 562093260 batches: 0.0156
trigger times: 8
Loss after 562171860 batches: 0.0154
trigger times: 9
Loss after 562250460 batches: 0.0157
trigger times: 10
Loss after 562329060 batches: 0.0158
trigger times: 11
Loss after 562407660 batches: 0.0154
trigger times: 12
Loss after 562486260 batches: 0.0153
trigger times: 13
Loss after 562564860 batches: 0.0152
trigger times: 14
Loss after 562643460 batches: 0.0154
trigger times: 15
Loss after 562722060 batches: 0.0150
trigger times: 16
Loss after 562800660 batches: 0.0150
trigger times: 17
Loss after 562879260 batches: 0.0147
trigger times: 18
Loss after 562957860 batches: 0.0147
trigger times: 19
Loss after 563036460 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 563115060 batches: 0.0146
Time to train on one home:  266.5003583431244
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097, 0.016851266692967333, 0.01539797175087967, 0.015622467135075917]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983], [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013], [0.5549709763791826, 0.4001270641513627, 0.48658498833859326, 1.0394903420759227, 0.4914126622529051, 24.558331909615507, 1517.0215], [0.5461304287115732, 0.4097276108845518, 0.4876006728779417, 1.0136453079994863, 0.48354794633174836, 23.94773371608454, 1492.7424]]
Round_10_results:  [0.5461304287115732, 0.4097276108845518, 0.4876006728779417, 1.0136453079994863, 0.48354794633174836, 23.94773371608454, 1492.7424]
trigger times: 0
Loss after 563246160 batches: 0.1243
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4571 < 4572; dropping {'Training_Loss': 0.12428251077544014, 'Validation_Loss': 0.20108584231800503, 'Training_R2': 0.8748132226422592, 'Validation_R2': 0.8133221625301009, 'Training_F1': 0.8150433612901631, 'Validation_F1': 0.7200532273440853, 'Training_NEP': 0.3694785153654066, 'Validation_NEP': 0.5088090193159783, 'Training_NDE': 0.0939799358311982, 'Validation_NDE': 0.14866038920505295, 'Training_MAE': 12.236395673991886, 'Validation_MAE': 13.953654228957323, 'Training_MSE': 413.49704, 'Validation_MSE': 548.9987}.
trigger times: 0
Loss after 563377260 batches: 0.0300
trigger times: 1
Loss after 563508360 batches: 0.0223
trigger times: 0
Loss after 563639460 batches: 0.0191
trigger times: 0
Loss after 563770560 batches: 0.0174
trigger times: 1
Loss after 563901660 batches: 0.0162
trigger times: 2
Loss after 564032760 batches: 0.0152
trigger times: 3
Loss after 564163860 batches: 0.0150
trigger times: 4
Loss after 564294960 batches: 0.0143
trigger times: 5
Loss after 564426060 batches: 0.0139
trigger times: 6
Loss after 564557160 batches: 0.0132
trigger times: 0
Loss after 564688260 batches: 0.0130
trigger times: 1
Loss after 564819360 batches: 0.0127
trigger times: 2
Loss after 564950460 batches: 0.0124
trigger times: 3
Loss after 565081560 batches: 0.0118
trigger times: 4
Loss after 565212660 batches: 0.0119
trigger times: 5
Loss after 565343760 batches: 0.0116
trigger times: 6
Loss after 565474860 batches: 0.0113
trigger times: 0
Loss after 565605960 batches: 0.0114
trigger times: 1
Loss after 565737060 batches: 0.0112
trigger times: 2
Loss after 565868160 batches: 0.0110
trigger times: 3
Loss after 565999260 batches: 0.0110
trigger times: 4
Loss after 566130360 batches: 0.0106
trigger times: 5
Loss after 566261460 batches: 0.0107
trigger times: 6
Loss after 566392560 batches: 0.0104
trigger times: 7
Loss after 566523660 batches: 0.0105
trigger times: 8
Loss after 566654760 batches: 0.0102
trigger times: 9
Loss after 566785860 batches: 0.0103
trigger times: 10
Loss after 566916960 batches: 0.0100
trigger times: 11
Loss after 567048060 batches: 0.0103
trigger times: 12
Loss after 567179160 batches: 0.0101
trigger times: 13
Loss after 567310260 batches: 0.0097
trigger times: 14
Loss after 567441360 batches: 0.0099
trigger times: 15
Loss after 567572460 batches: 0.0098
trigger times: 16
Loss after 567703560 batches: 0.0098
trigger times: 17
Loss after 567834660 batches: 0.0097
trigger times: 18
Loss after 567965760 batches: 0.0099
trigger times: 19
Loss after 568096860 batches: 0.0095
trigger times: 20
Early stopping!
Start to test process.
Loss after 568227960 batches: 0.0095
Time to train on one home:  287.4835591316223
trigger times: 0
Loss after 568330560 batches: 0.3287
trigger times: 0
Loss after 568433160 batches: 0.0980
trigger times: 1
Loss after 568535760 batches: 0.0610
trigger times: 0
Loss after 568638360 batches: 0.0500
trigger times: 0
Loss after 568740960 batches: 0.0487
trigger times: 0
Loss after 568843560 batches: 0.0423
trigger times: 1
Loss after 568946160 batches: 0.0378
trigger times: 2
Loss after 569048760 batches: 0.0341
trigger times: 3
Loss after 569151360 batches: 0.0323
trigger times: 4
Loss after 569253960 batches: 0.0317
trigger times: 5
Loss after 569356560 batches: 0.0292
trigger times: 0
Loss after 569459160 batches: 0.0292
trigger times: 1
Loss after 569561760 batches: 0.0321
trigger times: 2
Loss after 569664360 batches: 0.0293
trigger times: 3
Loss after 569766960 batches: 0.0287
trigger times: 4
Loss after 569869560 batches: 0.0266
trigger times: 5
Loss after 569972160 batches: 0.0258
trigger times: 6
Loss after 570074760 batches: 0.0245
trigger times: 0
Loss after 570177360 batches: 0.0236
trigger times: 1
Loss after 570279960 batches: 0.0239
trigger times: 2
Loss after 570382560 batches: 0.0252
trigger times: 3
Loss after 570485160 batches: 0.0241
trigger times: 4
Loss after 570587760 batches: 0.0233
trigger times: 5
Loss after 570690360 batches: 0.0243
trigger times: 6
Loss after 570792960 batches: 0.0223
trigger times: 7
Loss after 570895560 batches: 0.0216
trigger times: 8
Loss after 570998160 batches: 0.0223
trigger times: 0
Loss after 571100760 batches: 0.0253
trigger times: 1
Loss after 571203360 batches: 0.0216
trigger times: 2
Loss after 571305960 batches: 0.0214
trigger times: 3
Loss after 571408560 batches: 0.0210
trigger times: 4
Loss after 571511160 batches: 0.0210
trigger times: 5
Loss after 571613760 batches: 0.0204
trigger times: 6
Loss after 571716360 batches: 0.0205
trigger times: 7
Loss after 571818960 batches: 0.0199
trigger times: 8
Loss after 571921560 batches: 0.0204
trigger times: 9
Loss after 572024160 batches: 0.0202
trigger times: 10
Loss after 572126760 batches: 0.0196
trigger times: 11
Loss after 572229360 batches: 0.0199
trigger times: 0
Loss after 572331960 batches: 0.0207
trigger times: 0
Loss after 572434560 batches: 0.0223
trigger times: 1
Loss after 572537160 batches: 0.0193
trigger times: 2
Loss after 572639760 batches: 0.0191
trigger times: 3
Loss after 572742360 batches: 0.0197
trigger times: 4
Loss after 572844960 batches: 0.0225
trigger times: 5
Loss after 572947560 batches: 0.0228
trigger times: 6
Loss after 573050160 batches: 0.0194
trigger times: 7
Loss after 573152760 batches: 0.0192
trigger times: 8
Loss after 573255360 batches: 0.0186
trigger times: 9
Loss after 573357960 batches: 0.0187
trigger times: 10
Loss after 573460560 batches: 0.0181
trigger times: 11
Loss after 573563160 batches: 0.0191
trigger times: 12
Loss after 573665760 batches: 0.0201
trigger times: 13
Loss after 573768360 batches: 0.0181
trigger times: 14
Loss after 573870960 batches: 0.0180
trigger times: 15
Loss after 573973560 batches: 0.0177
trigger times: 16
Loss after 574076160 batches: 0.0171
trigger times: 17
Loss after 574178760 batches: 0.0173
trigger times: 18
Loss after 574281360 batches: 0.0179
trigger times: 19
Loss after 574383960 batches: 0.0171
trigger times: 20
Early stopping!
Start to test process.
Loss after 574486560 batches: 0.0169
Time to train on one home:  357.7891960144043
trigger times: 0
Loss after 574617660 batches: 0.1808
trigger times: 0
Loss after 574748760 batches: 0.0528
trigger times: 1
Loss after 574879860 batches: 0.0372
trigger times: 2
Loss after 575010960 batches: 0.0316
trigger times: 3
Loss after 575142060 batches: 0.0285
trigger times: 4
Loss after 575273160 batches: 0.0269
trigger times: 5
Loss after 575404260 batches: 0.0250
trigger times: 6
Loss after 575535360 batches: 0.0236
trigger times: 7
Loss after 575666460 batches: 0.0229
trigger times: 8
Loss after 575797560 batches: 0.0222
trigger times: 9
Loss after 575928660 batches: 0.0213
trigger times: 10
Loss after 576059760 batches: 0.0209
trigger times: 11
Loss after 576190860 batches: 0.0202
trigger times: 12
Loss after 576321960 batches: 0.0199
trigger times: 13
Loss after 576453060 batches: 0.0194
trigger times: 14
Loss after 576584160 batches: 0.0190
trigger times: 15
Loss after 576715260 batches: 0.0187
trigger times: 16
Loss after 576846360 batches: 0.0184
trigger times: 17
Loss after 576977460 batches: 0.0182
trigger times: 18
Loss after 577108560 batches: 0.0179
trigger times: 19
Loss after 577239660 batches: 0.0177
trigger times: 20
Early stopping!
Start to test process.
Loss after 577370760 batches: 0.0175
Time to train on one home:  167.50358629226685
trigger times: 0
Loss after 577501860 batches: 0.2321
trigger times: 0
Loss after 577632960 batches: 0.0660
trigger times: 1
Loss after 577764060 batches: 0.0474
trigger times: 0
Loss after 577895160 batches: 0.0397
trigger times: 1
Loss after 578026260 batches: 0.0369
trigger times: 2
Loss after 578157360 batches: 0.0339
trigger times: 3
Loss after 578288460 batches: 0.0322
trigger times: 0
Loss after 578419560 batches: 0.0306
trigger times: 1
Loss after 578550660 batches: 0.0299
trigger times: 2
Loss after 578681760 batches: 0.0287
trigger times: 3
Loss after 578812860 batches: 0.0278
trigger times: 4
Loss after 578943960 batches: 0.0270
trigger times: 5
Loss after 579075060 batches: 0.0262
trigger times: 6
Loss after 579206160 batches: 0.0259
trigger times: 7
Loss after 579337260 batches: 0.0252
trigger times: 8
Loss after 579468360 batches: 0.0247
trigger times: 9
Loss after 579599460 batches: 0.0243
trigger times: 10
Loss after 579730560 batches: 0.0241
trigger times: 11
Loss after 579861660 batches: 0.0239
trigger times: 12
Loss after 579992760 batches: 0.0233
trigger times: 0
Loss after 580123860 batches: 0.0229
trigger times: 1
Loss after 580254960 batches: 0.0226
trigger times: 2
Loss after 580386060 batches: 0.0223
trigger times: 0
Loss after 580517160 batches: 0.0223
trigger times: 1
Loss after 580648260 batches: 0.0221
trigger times: 0
Loss after 580779360 batches: 0.0220
trigger times: 1
Loss after 580910460 batches: 0.0218
trigger times: 2
Loss after 581041560 batches: 0.0215
trigger times: 0
Loss after 581172660 batches: 0.0215
trigger times: 1
Loss after 581303760 batches: 0.0213
trigger times: 2
Loss after 581434860 batches: 0.0208
trigger times: 3
Loss after 581565960 batches: 0.0209
trigger times: 4
Loss after 581697060 batches: 0.0206
trigger times: 5
Loss after 581828160 batches: 0.0205
trigger times: 6
Loss after 581959260 batches: 0.0202
trigger times: 7
Loss after 582090360 batches: 0.0202
trigger times: 8
Loss after 582221460 batches: 0.0201
trigger times: 9
Loss after 582352560 batches: 0.0198
trigger times: 10
Loss after 582483660 batches: 0.0198
trigger times: 11
Loss after 582614760 batches: 0.0199
trigger times: 0
Loss after 582745860 batches: 0.0197
trigger times: 1
Loss after 582876960 batches: 0.0201
trigger times: 2
Loss after 583008060 batches: 0.0196
trigger times: 3
Loss after 583139160 batches: 0.0195
trigger times: 4
Loss after 583270260 batches: 0.0191
trigger times: 0
Loss after 583401360 batches: 0.0192
trigger times: 0
Loss after 583532460 batches: 0.0190
trigger times: 1
Loss after 583663560 batches: 0.0192
trigger times: 2
Loss after 583794660 batches: 0.0190
trigger times: 3
Loss after 583925760 batches: 0.0189
trigger times: 4
Loss after 584056860 batches: 0.0187
trigger times: 5
Loss after 584187960 batches: 0.0183
trigger times: 6
Loss after 584319060 batches: 0.0186
trigger times: 7
Loss after 584450160 batches: 0.0185
trigger times: 8
Loss after 584581260 batches: 0.0182
trigger times: 9
Loss after 584712360 batches: 0.0183
trigger times: 10
Loss after 584843460 batches: 0.0183
trigger times: 11
Loss after 584974560 batches: 0.0178
trigger times: 12
Loss after 585105660 batches: 0.0180
trigger times: 13
Loss after 585236760 batches: 0.0180
trigger times: 14
Loss after 585367860 batches: 0.0178
trigger times: 15
Loss after 585498960 batches: 0.0181
trigger times: 16
Loss after 585630060 batches: 0.0178
trigger times: 17
Loss after 585761160 batches: 0.0181
trigger times: 0
Loss after 585892260 batches: 0.0179
trigger times: 1
Loss after 586023360 batches: 0.0176
trigger times: 2
Loss after 586154460 batches: 0.0173
trigger times: 3
Loss after 586285560 batches: 0.0174
trigger times: 4
Loss after 586416660 batches: 0.0172
trigger times: 5
Loss after 586547760 batches: 0.0171
trigger times: 6
Loss after 586678860 batches: 0.0173
trigger times: 7
Loss after 586809960 batches: 0.0171
trigger times: 8
Loss after 586941060 batches: 0.0171
trigger times: 9
Loss after 587072160 batches: 0.0172
trigger times: 10
Loss after 587203260 batches: 0.0172
trigger times: 11
Loss after 587334360 batches: 0.0172
trigger times: 12
Loss after 587465460 batches: 0.0169
trigger times: 13
Loss after 587596560 batches: 0.0169
trigger times: 14
Loss after 587727660 batches: 0.0166
trigger times: 15
Loss after 587858760 batches: 0.0167
trigger times: 16
Loss after 587989860 batches: 0.0167
trigger times: 17
Loss after 588120960 batches: 0.0165
trigger times: 18
Loss after 588252060 batches: 0.0166
trigger times: 19
Loss after 588383160 batches: 0.0166
trigger times: 20
Early stopping!
Start to test process.
Loss after 588514260 batches: 0.0165
Time to train on one home:  615.525041103363
trigger times: 0
Loss after 588642900 batches: 0.1362
trigger times: 0
Loss after 588771540 batches: 0.0364
trigger times: 0
Loss after 588900180 batches: 0.0270
trigger times: 1
Loss after 589028820 batches: 0.0236
trigger times: 0
Loss after 589157460 batches: 0.0208
trigger times: 0
Loss after 589286100 batches: 0.0195
trigger times: 1
Loss after 589414740 batches: 0.0190
trigger times: 2
Loss after 589543380 batches: 0.0180
trigger times: 3
Loss after 589672020 batches: 0.0172
trigger times: 4
Loss after 589800660 batches: 0.0165
trigger times: 5
Loss after 589929300 batches: 0.0163
trigger times: 6
Loss after 590057940 batches: 0.0159
trigger times: 7
Loss after 590186580 batches: 0.0155
trigger times: 8
Loss after 590315220 batches: 0.0153
trigger times: 9
Loss after 590443860 batches: 0.0149
trigger times: 10
Loss after 590572500 batches: 0.0149
trigger times: 11
Loss after 590701140 batches: 0.0147
trigger times: 12
Loss after 590829780 batches: 0.0142
trigger times: 13
Loss after 590958420 batches: 0.0142
trigger times: 14
Loss after 591087060 batches: 0.0140
trigger times: 15
Loss after 591215700 batches: 0.0139
trigger times: 16
Loss after 591344340 batches: 0.0136
trigger times: 17
Loss after 591472980 batches: 0.0132
trigger times: 18
Loss after 591601620 batches: 0.0132
trigger times: 19
Loss after 591730260 batches: 0.0130
trigger times: 20
Early stopping!
Start to test process.
Loss after 591858900 batches: 0.0127
Time to train on one home:  192.91703295707703
trigger times: 0
Loss after 591990000 batches: 0.2168
trigger times: 0
Loss after 592121100 batches: 0.0594
trigger times: 0
Loss after 592252200 batches: 0.0424
trigger times: 1
Loss after 592383300 batches: 0.0358
trigger times: 2
Loss after 592514400 batches: 0.0328
trigger times: 0
Loss after 592645500 batches: 0.0307
trigger times: 0
Loss after 592776600 batches: 0.0292
trigger times: 1
Loss after 592907700 batches: 0.0283
trigger times: 2
Loss after 593038800 batches: 0.0271
trigger times: 0
Loss after 593169900 batches: 0.0260
trigger times: 1
Loss after 593301000 batches: 0.0257
trigger times: 2
Loss after 593432100 batches: 0.0250
trigger times: 3
Loss after 593563200 batches: 0.0243
trigger times: 0
Loss after 593694300 batches: 0.0239
trigger times: 1
Loss after 593825400 batches: 0.0235
trigger times: 2
Loss after 593956500 batches: 0.0235
trigger times: 3
Loss after 594087600 batches: 0.0228
trigger times: 4
Loss after 594218700 batches: 0.0223
trigger times: 5
Loss after 594349800 batches: 0.0221
trigger times: 6
Loss after 594480900 batches: 0.0220
trigger times: 7
Loss after 594612000 batches: 0.0216
trigger times: 8
Loss after 594743100 batches: 0.0213
trigger times: 9
Loss after 594874200 batches: 0.0210
trigger times: 10
Loss after 595005300 batches: 0.0211
trigger times: 0
Loss after 595136400 batches: 0.0210
trigger times: 1
Loss after 595267500 batches: 0.0204
trigger times: 2
Loss after 595398600 batches: 0.0203
trigger times: 0
Loss after 595529700 batches: 0.0197
trigger times: 1
Loss after 595660800 batches: 0.0200
trigger times: 2
Loss after 595791900 batches: 0.0200
trigger times: 3
Loss after 595923000 batches: 0.0196
trigger times: 4
Loss after 596054100 batches: 0.0193
trigger times: 0
Loss after 596185200 batches: 0.0198
trigger times: 1
Loss after 596316300 batches: 0.0195
trigger times: 2
Loss after 596447400 batches: 0.0193
trigger times: 0
Loss after 596578500 batches: 0.0193
trigger times: 1
Loss after 596709600 batches: 0.0188
trigger times: 2
Loss after 596840700 batches: 0.0188
trigger times: 3
Loss after 596971800 batches: 0.0187
trigger times: 4
Loss after 597102900 batches: 0.0186
trigger times: 5
Loss after 597234000 batches: 0.0188
trigger times: 6
Loss after 597365100 batches: 0.0185
trigger times: 7
Loss after 597496200 batches: 0.0185
trigger times: 8
Loss after 597627300 batches: 0.0186
trigger times: 9
Loss after 597758400 batches: 0.0183
trigger times: 10
Loss after 597889500 batches: 0.0184
trigger times: 0
Loss after 598020600 batches: 0.0180
trigger times: 1
Loss after 598151700 batches: 0.0175
trigger times: 2
Loss after 598282800 batches: 0.0177
trigger times: 3
Loss after 598413900 batches: 0.0175
trigger times: 4
Loss after 598545000 batches: 0.0178
trigger times: 5
Loss after 598676100 batches: 0.0176
trigger times: 6
Loss after 598807200 batches: 0.0176
trigger times: 7
Loss after 598938300 batches: 0.0175
trigger times: 8
Loss after 599069400 batches: 0.0173
trigger times: 9
Loss after 599200500 batches: 0.0173
trigger times: 10
Loss after 599331600 batches: 0.0171
trigger times: 11
Loss after 599462700 batches: 0.0170
trigger times: 12
Loss after 599593800 batches: 0.0172
trigger times: 13
Loss after 599724900 batches: 0.0168
trigger times: 14
Loss after 599856000 batches: 0.0169
trigger times: 15
Loss after 599987100 batches: 0.0168
trigger times: 16
Loss after 600118200 batches: 0.0168
trigger times: 17
Loss after 600249300 batches: 0.0168
trigger times: 18
Loss after 600380400 batches: 0.0167
trigger times: 19
Loss after 600511500 batches: 0.0164
trigger times: 20
Early stopping!
Start to test process.
Loss after 600642600 batches: 0.0165
Time to train on one home:  486.11960768699646
trigger times: 0
Loss after 600773700 batches: 0.2584
trigger times: 1
Loss after 600904800 batches: 0.0921
trigger times: 0
Loss after 601035900 batches: 0.0616
trigger times: 0
Loss after 601167000 batches: 0.0477
trigger times: 1
Loss after 601298100 batches: 0.0409
trigger times: 2
Loss after 601429200 batches: 0.0370
trigger times: 3
Loss after 601560300 batches: 0.0340
trigger times: 0
Loss after 601691400 batches: 0.0333
trigger times: 0
Loss after 601822500 batches: 0.0320
trigger times: 1
Loss after 601953600 batches: 0.0302
trigger times: 2
Loss after 602084700 batches: 0.0302
trigger times: 0
Loss after 602215800 batches: 0.0295
trigger times: 1
Loss after 602346900 batches: 0.0283
trigger times: 2
Loss after 602478000 batches: 0.0282
trigger times: 3
Loss after 602609100 batches: 0.0269
trigger times: 4
Loss after 602740200 batches: 0.0263
trigger times: 5
Loss after 602871300 batches: 0.0259
trigger times: 6
Loss after 603002400 batches: 0.0260
trigger times: 7
Loss after 603133500 batches: 0.0260
trigger times: 0
Loss after 603264600 batches: 0.0256
trigger times: 1
Loss after 603395700 batches: 0.0256
trigger times: 2
Loss after 603526800 batches: 0.0253
trigger times: 0
Loss after 603657900 batches: 0.0253
trigger times: 1
Loss after 603789000 batches: 0.0259
trigger times: 2
Loss after 603920100 batches: 0.0250
trigger times: 0
Loss after 604051200 batches: 0.0243
trigger times: 1
Loss after 604182300 batches: 0.0234
trigger times: 2
Loss after 604313400 batches: 0.0237
trigger times: 3
Loss after 604444500 batches: 0.0224
trigger times: 4
Loss after 604575600 batches: 0.0235
trigger times: 5
Loss after 604706700 batches: 0.0230
trigger times: 6
Loss after 604837800 batches: 0.0234
trigger times: 7
Loss after 604968900 batches: 0.0221
trigger times: 8
Loss after 605100000 batches: 0.0226
trigger times: 9
Loss after 605231100 batches: 0.0224
trigger times: 10
Loss after 605362200 batches: 0.0227
trigger times: 11
Loss after 605493300 batches: 0.0230
trigger times: 0
Loss after 605624400 batches: 0.0221
trigger times: 1
Loss after 605755500 batches: 0.0219
trigger times: 2
Loss after 605886600 batches: 0.0219
trigger times: 3
Loss after 606017700 batches: 0.0213
trigger times: 0
Loss after 606148800 batches: 0.0222
trigger times: 1
Loss after 606279900 batches: 0.0217
trigger times: 2
Loss after 606411000 batches: 0.0211
trigger times: 3
Loss after 606542100 batches: 0.0212
trigger times: 4
Loss after 606673200 batches: 0.0212
trigger times: 0
Loss after 606804300 batches: 0.0214
trigger times: 1
Loss after 606935400 batches: 0.0210
trigger times: 2
Loss after 607066500 batches: 0.0212
trigger times: 3
Loss after 607197600 batches: 0.0208
trigger times: 4
Loss after 607328700 batches: 0.0215
trigger times: 0
Loss after 607459800 batches: 0.0209
trigger times: 1
Loss after 607590900 batches: 0.0210
trigger times: 2
Loss after 607722000 batches: 0.0228
trigger times: 3
Loss after 607853100 batches: 0.0215
trigger times: 4
Loss after 607984200 batches: 0.0207
trigger times: 5
Loss after 608115300 batches: 0.0204
trigger times: 6
Loss after 608246400 batches: 0.0211
trigger times: 7
Loss after 608377500 batches: 0.0212
trigger times: 8
Loss after 608508600 batches: 0.0205
trigger times: 9
Loss after 608639700 batches: 0.0197
trigger times: 10
Loss after 608770800 batches: 0.0193
trigger times: 11
Loss after 608901900 batches: 0.0206
trigger times: 12
Loss after 609033000 batches: 0.0205
trigger times: 13
Loss after 609164100 batches: 0.0200
trigger times: 14
Loss after 609295200 batches: 0.0191
trigger times: 15
Loss after 609426300 batches: 0.0190
trigger times: 16
Loss after 609557400 batches: 0.0190
trigger times: 17
Loss after 609688500 batches: 0.0189
trigger times: 18
Loss after 609819600 batches: 0.0193
trigger times: 19
Loss after 609950700 batches: 0.0200
trigger times: 20
Early stopping!
Start to test process.
Loss after 610081800 batches: 0.0196
Time to train on one home:  521.8264815807343
trigger times: 0
Loss after 610212900 batches: 0.0789
trigger times: 0
Loss after 610344000 batches: 0.0229
trigger times: 1
Loss after 610475100 batches: 0.0167
trigger times: 0
Loss after 610606200 batches: 0.0147
trigger times: 0
Loss after 610737300 batches: 0.0133
trigger times: 1
Loss after 610868400 batches: 0.0129
trigger times: 2
Loss after 610999500 batches: 0.0119
trigger times: 3
Loss after 611130600 batches: 0.0116
trigger times: 0
Loss after 611261700 batches: 0.0112
trigger times: 1
Loss after 611392800 batches: 0.0106
trigger times: 2
Loss after 611523900 batches: 0.0103
trigger times: 3
Loss after 611655000 batches: 0.0102
trigger times: 4
Loss after 611786100 batches: 0.0103
trigger times: 5
Loss after 611917200 batches: 0.0098
trigger times: 0
Loss after 612048300 batches: 0.0095
trigger times: 1
Loss after 612179400 batches: 0.0093
trigger times: 0
Loss after 612310500 batches: 0.0091
trigger times: 1
Loss after 612441600 batches: 0.0090
trigger times: 2
Loss after 612572700 batches: 0.0090
trigger times: 3
Loss after 612703800 batches: 0.0090
trigger times: 4
Loss after 612834900 batches: 0.0090
trigger times: 5
Loss after 612966000 batches: 0.0087
trigger times: 6
Loss after 613097100 batches: 0.0087
trigger times: 7
Loss after 613228200 batches: 0.0087
trigger times: 8
Loss after 613359300 batches: 0.0083
trigger times: 9
Loss after 613490400 batches: 0.0084
trigger times: 0
Loss after 613621500 batches: 0.0082
trigger times: 1
Loss after 613752600 batches: 0.0082
trigger times: 2
Loss after 613883700 batches: 0.0081
trigger times: 3
Loss after 614014800 batches: 0.0079
trigger times: 4
Loss after 614145900 batches: 0.0080
trigger times: 5
Loss after 614277000 batches: 0.0080
trigger times: 6
Loss after 614408100 batches: 0.0080
trigger times: 0
Loss after 614539200 batches: 0.0079
trigger times: 0
Loss after 614670300 batches: 0.0079
trigger times: 1
Loss after 614801400 batches: 0.0079
trigger times: 2
Loss after 614932500 batches: 0.0076
trigger times: 0
Loss after 615063600 batches: 0.0075
trigger times: 1
Loss after 615194700 batches: 0.0077
trigger times: 2
Loss after 615325800 batches: 0.0076
trigger times: 3
Loss after 615456900 batches: 0.0075
trigger times: 4
Loss after 615588000 batches: 0.0074
trigger times: 5
Loss after 615719100 batches: 0.0074
trigger times: 6
Loss after 615850200 batches: 0.0076
trigger times: 7
Loss after 615981300 batches: 0.0072
trigger times: 8
Loss after 616112400 batches: 0.0073
trigger times: 0
Loss after 616243500 batches: 0.0073
trigger times: 1
Loss after 616374600 batches: 0.0074
trigger times: 2
Loss after 616505700 batches: 0.0073
trigger times: 3
Loss after 616636800 batches: 0.0072
trigger times: 4
Loss after 616767900 batches: 0.0072
trigger times: 5
Loss after 616899000 batches: 0.0071
trigger times: 6
Loss after 617030100 batches: 0.0070
trigger times: 7
Loss after 617161200 batches: 0.0072
trigger times: 8
Loss after 617292300 batches: 0.0069
trigger times: 9
Loss after 617423400 batches: 0.0070
trigger times: 10
Loss after 617554500 batches: 0.0070
trigger times: 11
Loss after 617685600 batches: 0.0067
trigger times: 12
Loss after 617816700 batches: 0.0070
trigger times: 0
Loss after 617947800 batches: 0.0069
trigger times: 1
Loss after 618078900 batches: 0.0068
trigger times: 2
Loss after 618210000 batches: 0.0067
trigger times: 0
Loss after 618341100 batches: 0.0067
trigger times: 1
Loss after 618472200 batches: 0.0066
trigger times: 2
Loss after 618603300 batches: 0.0067
trigger times: 3
Loss after 618734400 batches: 0.0064
trigger times: 4
Loss after 618865500 batches: 0.0066
trigger times: 5
Loss after 618996600 batches: 0.0065
trigger times: 6
Loss after 619127700 batches: 0.0066
trigger times: 7
Loss after 619258800 batches: 0.0064
trigger times: 8
Loss after 619389900 batches: 0.0065
trigger times: 9
Loss after 619521000 batches: 0.0066
trigger times: 0
Loss after 619652100 batches: 0.0065
trigger times: 1
Loss after 619783200 batches: 0.0065
trigger times: 2
Loss after 619914300 batches: 0.0065
trigger times: 3
Loss after 620045400 batches: 0.0065
trigger times: 4
Loss after 620176500 batches: 0.0063
trigger times: 5
Loss after 620307600 batches: 0.0063
trigger times: 6
Loss after 620438700 batches: 0.0063
trigger times: 7
Loss after 620569800 batches: 0.0063
trigger times: 8
Loss after 620700900 batches: 0.0063
trigger times: 9
Loss after 620832000 batches: 0.0062
trigger times: 10
Loss after 620963100 batches: 0.0062
trigger times: 11
Loss after 621094200 batches: 0.0063
trigger times: 12
Loss after 621225300 batches: 0.0062
trigger times: 13
Loss after 621356400 batches: 0.0061
trigger times: 14
Loss after 621487500 batches: 0.0064
trigger times: 15
Loss after 621618600 batches: 0.0063
trigger times: 16
Loss after 621749700 batches: 0.0062
trigger times: 0
Loss after 621880800 batches: 0.0061
trigger times: 1
Loss after 622011900 batches: 0.0062
trigger times: 2
Loss after 622143000 batches: 0.0061
trigger times: 3
Loss after 622274100 batches: 0.0061
trigger times: 4
Loss after 622405200 batches: 0.0061
trigger times: 5
Loss after 622536300 batches: 0.0062
trigger times: 6
Loss after 622667400 batches: 0.0060
trigger times: 7
Loss after 622798500 batches: 0.0061
trigger times: 8
Loss after 622929600 batches: 0.0059
trigger times: 9
Loss after 623060700 batches: 0.0060
trigger times: 10
Loss after 623191800 batches: 0.0060
trigger times: 11
Loss after 623322900 batches: 0.0060
trigger times: 12
Loss after 623454000 batches: 0.0060
trigger times: 13
Loss after 623585100 batches: 0.0060
trigger times: 14
Loss after 623716200 batches: 0.0059
trigger times: 15
Loss after 623847300 batches: 0.0059
trigger times: 16
Loss after 623978400 batches: 0.0060
trigger times: 17
Loss after 624109500 batches: 0.0059
trigger times: 18
Loss after 624240600 batches: 0.0059
trigger times: 19
Loss after 624371700 batches: 0.0060
trigger times: 20
Early stopping!
Start to test process.
Loss after 624502800 batches: 0.0059
Time to train on one home:  789.6575818061829
trigger times: 0
Loss after 624581400 batches: 0.2375
trigger times: 0
Loss after 624660000 batches: 0.0631
trigger times: 1
Loss after 624738600 batches: 0.0414
trigger times: 2
Loss after 624817200 batches: 0.0338
trigger times: 3
Loss after 624895800 batches: 0.0301
trigger times: 4
Loss after 624974400 batches: 0.0281
trigger times: 5
Loss after 625053000 batches: 0.0264
trigger times: 6
Loss after 625131600 batches: 0.0251
trigger times: 7
Loss after 625210200 batches: 0.0245
trigger times: 8
Loss after 625288800 batches: 0.0226
trigger times: 9
Loss after 625367400 batches: 0.0215
trigger times: 0
Loss after 625446000 batches: 0.0210
trigger times: 1
Loss after 625524600 batches: 0.0212
trigger times: 2
Loss after 625603200 batches: 0.0208
trigger times: 3
Loss after 625681800 batches: 0.0200
trigger times: 4
Loss after 625760400 batches: 0.0195
trigger times: 5
Loss after 625839000 batches: 0.0195
trigger times: 6
Loss after 625917600 batches: 0.0191
trigger times: 7
Loss after 625996200 batches: 0.0188
trigger times: 8
Loss after 626074800 batches: 0.0179
trigger times: 9
Loss after 626153400 batches: 0.0188
trigger times: 10
Loss after 626232000 batches: 0.0179
trigger times: 11
Loss after 626310600 batches: 0.0181
trigger times: 12
Loss after 626389200 batches: 0.0179
trigger times: 13
Loss after 626467800 batches: 0.0174
trigger times: 14
Loss after 626546400 batches: 0.0167
trigger times: 15
Loss after 626625000 batches: 0.0164
trigger times: 16
Loss after 626703600 batches: 0.0166
trigger times: 17
Loss after 626782200 batches: 0.0165
trigger times: 18
Loss after 626860800 batches: 0.0162
trigger times: 19
Loss after 626939400 batches: 0.0164
trigger times: 0
Loss after 627018000 batches: 0.0163
trigger times: 1
Loss after 627096600 batches: 0.0162
trigger times: 2
Loss after 627175200 batches: 0.0158
trigger times: 3
Loss after 627253800 batches: 0.0157
trigger times: 4
Loss after 627332400 batches: 0.0157
trigger times: 5
Loss after 627411000 batches: 0.0160
trigger times: 6
Loss after 627489600 batches: 0.0157
trigger times: 7
Loss after 627568200 batches: 0.0159
trigger times: 8
Loss after 627646800 batches: 0.0153
trigger times: 9
Loss after 627725400 batches: 0.0151
trigger times: 10
Loss after 627804000 batches: 0.0149
trigger times: 11
Loss after 627882600 batches: 0.0153
trigger times: 12
Loss after 627961200 batches: 0.0150
trigger times: 13
Loss after 628039800 batches: 0.0149
trigger times: 14
Loss after 628118400 batches: 0.0150
trigger times: 15
Loss after 628197000 batches: 0.0147
trigger times: 16
Loss after 628275600 batches: 0.0152
trigger times: 17
Loss after 628354200 batches: 0.0155
trigger times: 18
Loss after 628432800 batches: 0.0142
trigger times: 19
Loss after 628511400 batches: 0.0140
trigger times: 20
Early stopping!
Start to test process.
Loss after 628590000 batches: 0.0141
Time to train on one home:  248.66698241233826
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097, 0.016851266692967333, 0.01539797175087967, 0.015622467135075917, 0.014339386766728896]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983], [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013], [0.5549709763791826, 0.4001270641513627, 0.48658498833859326, 1.0394903420759227, 0.4914126622529051, 24.558331909615507, 1517.0215], [0.5461304287115732, 0.4097276108845518, 0.4876006728779417, 1.0136453079994863, 0.48354794633174836, 23.94773371608454, 1492.7424], [0.5402040051089393, 0.41609081728992114, 0.4927051878338842, 1.0002093606660662, 0.47833524208513384, 23.630304644569275, 1476.6505]]
Round_11_results:  [0.5402040051089393, 0.41609081728992114, 0.4927051878338842, 1.0002093606660662, 0.47833524208513384, 23.630304644569275, 1476.6505]
trigger times: 0
Loss after 628721100 batches: 0.1180
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5105 < 5106; dropping {'Training_Loss': 0.11802049972257524, 'Validation_Loss': 0.20468250579304165, 'Training_R2': 0.8811514269460129, 'Validation_R2': 0.8100306090982564, 'Training_F1': 0.8226488645158616, 'Validation_F1': 0.7457467086840934, 'Training_NEP': 0.3542877446411771, 'Validation_NEP': 0.48670752859259225, 'Training_NDE': 0.08922173335706944, 'Validation_NDE': 0.1512816088468656, 'Training_MAE': 11.733307474152356, 'Validation_MAE': 13.347539659853904, 'Training_MSE': 392.56165, 'Validation_MSE': 558.6788}.
trigger times: 0
Loss after 628852200 batches: 0.0300
trigger times: 1
Loss after 628983300 batches: 0.0213
trigger times: 0
Loss after 629114400 batches: 0.0185
trigger times: 1
Loss after 629245500 batches: 0.0168
trigger times: 0
Loss after 629376600 batches: 0.0159
trigger times: 1
Loss after 629507700 batches: 0.0148
trigger times: 2
Loss after 629638800 batches: 0.0141
trigger times: 3
Loss after 629769900 batches: 0.0134
trigger times: 0
Loss after 629901000 batches: 0.0132
trigger times: 1
Loss after 630032100 batches: 0.0127
trigger times: 2
Loss after 630163200 batches: 0.0124
trigger times: 3
Loss after 630294300 batches: 0.0124
trigger times: 4
Loss after 630425400 batches: 0.0119
trigger times: 5
Loss after 630556500 batches: 0.0116
trigger times: 6
Loss after 630687600 batches: 0.0114
trigger times: 7
Loss after 630818700 batches: 0.0116
trigger times: 8
Loss after 630949800 batches: 0.0112
trigger times: 9
Loss after 631080900 batches: 0.0110
trigger times: 10
Loss after 631212000 batches: 0.0108
trigger times: 11
Loss after 631343100 batches: 0.0107
trigger times: 12
Loss after 631474200 batches: 0.0105
trigger times: 13
Loss after 631605300 batches: 0.0104
trigger times: 14
Loss after 631736400 batches: 0.0101
trigger times: 15
Loss after 631867500 batches: 0.0101
trigger times: 16
Loss after 631998600 batches: 0.0099
trigger times: 17
Loss after 632129700 batches: 0.0098
trigger times: 18
Loss after 632260800 batches: 0.0101
trigger times: 19
Loss after 632391900 batches: 0.0098
trigger times: 20
Early stopping!
Start to test process.
Loss after 632523000 batches: 0.0097
Time to train on one home:  224.18110489845276
trigger times: 0
Loss after 632625600 batches: 0.2755
trigger times: 0
Loss after 632728200 batches: 0.1014
trigger times: 0
Loss after 632830800 batches: 0.0606
trigger times: 1
Loss after 632933400 batches: 0.0458
trigger times: 2
Loss after 633036000 batches: 0.0402
trigger times: 3
Loss after 633138600 batches: 0.0361
trigger times: 4
Loss after 633241200 batches: 0.0339
trigger times: 5
Loss after 633343800 batches: 0.0315
trigger times: 6
Loss after 633446400 batches: 0.0299
trigger times: 7
Loss after 633549000 batches: 0.0293
trigger times: 8
Loss after 633651600 batches: 0.0289
trigger times: 9
Loss after 633754200 batches: 0.0263
trigger times: 10
Loss after 633856800 batches: 0.0275
trigger times: 11
Loss after 633959400 batches: 0.0263
trigger times: 12
Loss after 634062000 batches: 0.0244
trigger times: 13
Loss after 634164600 batches: 0.0249
trigger times: 14
Loss after 634267200 batches: 0.0253
trigger times: 15
Loss after 634369800 batches: 0.0258
trigger times: 16
Loss after 634472400 batches: 0.0238
trigger times: 17
Loss after 634575000 batches: 0.0232
trigger times: 18
Loss after 634677600 batches: 0.0241
trigger times: 19
Loss after 634780200 batches: 0.0248
trigger times: 20
Early stopping!
Start to test process.
Loss after 634882800 batches: 0.0238
Time to train on one home:  142.24947452545166
trigger times: 0
Loss after 635013900 batches: 0.1484
trigger times: 1
Loss after 635145000 batches: 0.0467
trigger times: 2
Loss after 635276100 batches: 0.0347
trigger times: 3
Loss after 635407200 batches: 0.0298
trigger times: 4
Loss after 635538300 batches: 0.0269
trigger times: 5
Loss after 635669400 batches: 0.0256
trigger times: 6
Loss after 635800500 batches: 0.0240
trigger times: 7
Loss after 635931600 batches: 0.0229
trigger times: 8
Loss after 636062700 batches: 0.0219
trigger times: 9
Loss after 636193800 batches: 0.0214
trigger times: 10
Loss after 636324900 batches: 0.0205
trigger times: 11
Loss after 636456000 batches: 0.0201
trigger times: 12
Loss after 636587100 batches: 0.0195
trigger times: 13
Loss after 636718200 batches: 0.0193
trigger times: 14
Loss after 636849300 batches: 0.0187
trigger times: 15
Loss after 636980400 batches: 0.0185
trigger times: 16
Loss after 637111500 batches: 0.0181
trigger times: 17
Loss after 637242600 batches: 0.0180
trigger times: 18
Loss after 637373700 batches: 0.0177
trigger times: 19
Loss after 637504800 batches: 0.0172
trigger times: 20
Early stopping!
Start to test process.
Loss after 637635900 batches: 0.0174
Time to train on one home:  160.49228692054749
trigger times: 0
Loss after 637767000 batches: 0.2765
trigger times: 0
Loss after 637898100 batches: 0.0649
trigger times: 1
Loss after 638029200 batches: 0.0462
trigger times: 0
Loss after 638160300 batches: 0.0393
trigger times: 1
Loss after 638291400 batches: 0.0350
trigger times: 0
Loss after 638422500 batches: 0.0330
trigger times: 0
Loss after 638553600 batches: 0.0306
trigger times: 1
Loss after 638684700 batches: 0.0295
trigger times: 0
Loss after 638815800 batches: 0.0286
trigger times: 0
Loss after 638946900 batches: 0.0276
trigger times: 1
Loss after 639078000 batches: 0.0269
trigger times: 2
Loss after 639209100 batches: 0.0259
trigger times: 3
Loss after 639340200 batches: 0.0254
trigger times: 4
Loss after 639471300 batches: 0.0247
trigger times: 5
Loss after 639602400 batches: 0.0242
trigger times: 0
Loss after 639733500 batches: 0.0238
trigger times: 1
Loss after 639864600 batches: 0.0233
trigger times: 2
Loss after 639995700 batches: 0.0232
trigger times: 0
Loss after 640126800 batches: 0.0230
trigger times: 1
Loss after 640257900 batches: 0.0224
trigger times: 2
Loss after 640389000 batches: 0.0222
trigger times: 3
Loss after 640520100 batches: 0.0222
trigger times: 4
Loss after 640651200 batches: 0.0219
trigger times: 5
Loss after 640782300 batches: 0.0216
trigger times: 6
Loss after 640913400 batches: 0.0213
trigger times: 7
Loss after 641044500 batches: 0.0211
trigger times: 8
Loss after 641175600 batches: 0.0207
trigger times: 9
Loss after 641306700 batches: 0.0210
trigger times: 10
Loss after 641437800 batches: 0.0208
trigger times: 11
Loss after 641568900 batches: 0.0207
trigger times: 0
Loss after 641700000 batches: 0.0204
trigger times: 1
Loss after 641831100 batches: 0.0203
trigger times: 0
Loss after 641962200 batches: 0.0201
trigger times: 1
Loss after 642093300 batches: 0.0197
trigger times: 2
Loss after 642224400 batches: 0.0195
trigger times: 0
Loss after 642355500 batches: 0.0194
trigger times: 1
Loss after 642486600 batches: 0.0195
trigger times: 2
Loss after 642617700 batches: 0.0192
trigger times: 3
Loss after 642748800 batches: 0.0194
trigger times: 4
Loss after 642879900 batches: 0.0193
trigger times: 5
Loss after 643011000 batches: 0.0192
trigger times: 6
Loss after 643142100 batches: 0.0190
trigger times: 7
Loss after 643273200 batches: 0.0188
trigger times: 8
Loss after 643404300 batches: 0.0187
trigger times: 9
Loss after 643535400 batches: 0.0186
trigger times: 10
Loss after 643666500 batches: 0.0185
trigger times: 11
Loss after 643797600 batches: 0.0184
trigger times: 12
Loss after 643928700 batches: 0.0183
trigger times: 13
Loss after 644059800 batches: 0.0185
trigger times: 14
Loss after 644190900 batches: 0.0183
trigger times: 15
Loss after 644322000 batches: 0.0179
trigger times: 16
Loss after 644453100 batches: 0.0181
trigger times: 0
Loss after 644584200 batches: 0.0180
trigger times: 0
Loss after 644715300 batches: 0.0178
trigger times: 1
Loss after 644846400 batches: 0.0180
trigger times: 2
Loss after 644977500 batches: 0.0178
trigger times: 3
Loss after 645108600 batches: 0.0176
trigger times: 0
Loss after 645239700 batches: 0.0178
trigger times: 1
Loss after 645370800 batches: 0.0177
trigger times: 2
Loss after 645501900 batches: 0.0174
trigger times: 3
Loss after 645633000 batches: 0.0174
trigger times: 4
Loss after 645764100 batches: 0.0172
trigger times: 5
Loss after 645895200 batches: 0.0172
trigger times: 6
Loss after 646026300 batches: 0.0172
trigger times: 0
Loss after 646157400 batches: 0.0172
trigger times: 1
Loss after 646288500 batches: 0.0172
trigger times: 2
Loss after 646419600 batches: 0.0172
trigger times: 3
Loss after 646550700 batches: 0.0170
trigger times: 4
Loss after 646681800 batches: 0.0170
trigger times: 5
Loss after 646812900 batches: 0.0169
trigger times: 6
Loss after 646944000 batches: 0.0168
trigger times: 7
Loss after 647075100 batches: 0.0167
trigger times: 8
Loss after 647206200 batches: 0.0165
trigger times: 9
Loss after 647337300 batches: 0.0166
trigger times: 10
Loss after 647468400 batches: 0.0165
trigger times: 11
Loss after 647599500 batches: 0.0166
trigger times: 12
Loss after 647730600 batches: 0.0166
trigger times: 13
Loss after 647861700 batches: 0.0166
trigger times: 14
Loss after 647992800 batches: 0.0165
trigger times: 15
Loss after 648123900 batches: 0.0164
trigger times: 0
Loss after 648255000 batches: 0.0162
trigger times: 1
Loss after 648386100 batches: 0.0161
trigger times: 2
Loss after 648517200 batches: 0.0163
trigger times: 3
Loss after 648648300 batches: 0.0163
trigger times: 0
Loss after 648779400 batches: 0.0163
trigger times: 0
Loss after 648910500 batches: 0.0162
trigger times: 0
Loss after 649041600 batches: 0.0162
trigger times: 1
Loss after 649172700 batches: 0.0163
trigger times: 2
Loss after 649303800 batches: 0.0159
trigger times: 3
Loss after 649434900 batches: 0.0159
trigger times: 4
Loss after 649566000 batches: 0.0161
trigger times: 5
Loss after 649697100 batches: 0.0160
trigger times: 6
Loss after 649828200 batches: 0.0158
trigger times: 0
Loss after 649959300 batches: 0.0159
trigger times: 1
Loss after 650090400 batches: 0.0158
trigger times: 2
Loss after 650221500 batches: 0.0158
trigger times: 3
Loss after 650352600 batches: 0.0158
trigger times: 4
Loss after 650483700 batches: 0.0157
trigger times: 5
Loss after 650614800 batches: 0.0157
trigger times: 6
Loss after 650745900 batches: 0.0155
trigger times: 7
Loss after 650877000 batches: 0.0157
trigger times: 8
Loss after 651008100 batches: 0.0157
trigger times: 9
Loss after 651139200 batches: 0.0156
trigger times: 10
Loss after 651270300 batches: 0.0155
trigger times: 11
Loss after 651401400 batches: 0.0155
trigger times: 12
Loss after 651532500 batches: 0.0155
trigger times: 13
Loss after 651663600 batches: 0.0155
trigger times: 14
Loss after 651794700 batches: 0.0154
trigger times: 15
Loss after 651925800 batches: 0.0154
trigger times: 16
Loss after 652056900 batches: 0.0155
trigger times: 0
Loss after 652188000 batches: 0.0154
trigger times: 1
Loss after 652319100 batches: 0.0153
trigger times: 2
Loss after 652450200 batches: 0.0151
trigger times: 3
Loss after 652581300 batches: 0.0151
trigger times: 4
Loss after 652712400 batches: 0.0152
trigger times: 5
Loss after 652843500 batches: 0.0153
trigger times: 6
Loss after 652974600 batches: 0.0152
trigger times: 7
Loss after 653105700 batches: 0.0150
trigger times: 8
Loss after 653236800 batches: 0.0150
trigger times: 0
Loss after 653367900 batches: 0.0151
trigger times: 1
Loss after 653499000 batches: 0.0150
trigger times: 2
Loss after 653630100 batches: 0.0150
trigger times: 0
Loss after 653761200 batches: 0.0147
trigger times: 1
Loss after 653892300 batches: 0.0149
trigger times: 2
Loss after 654023400 batches: 0.0150
trigger times: 3
Loss after 654154500 batches: 0.0148
trigger times: 4
Loss after 654285600 batches: 0.0146
trigger times: 5
Loss after 654416700 batches: 0.0147
trigger times: 6
Loss after 654547800 batches: 0.0148
trigger times: 7
Loss after 654678900 batches: 0.0145
trigger times: 8
Loss after 654810000 batches: 0.0147
trigger times: 9
Loss after 654941100 batches: 0.0148
trigger times: 10
Loss after 655072200 batches: 0.0145
trigger times: 11
Loss after 655203300 batches: 0.0146
trigger times: 12
Loss after 655334400 batches: 0.0145
trigger times: 13
Loss after 655465500 batches: 0.0143
trigger times: 14
Loss after 655596600 batches: 0.0143
trigger times: 15
Loss after 655727700 batches: 0.0143
trigger times: 16
Loss after 655858800 batches: 0.0143
trigger times: 17
Loss after 655989900 batches: 0.0143
trigger times: 18
Loss after 656121000 batches: 0.0144
trigger times: 19
Loss after 656252100 batches: 0.0144
trigger times: 20
Early stopping!
Start to test process.
Loss after 656383200 batches: 0.0143
Time to train on one home:  1023.3311223983765
trigger times: 0
Loss after 656511840 batches: 0.1187
trigger times: 0
Loss after 656640480 batches: 0.0337
trigger times: 0
Loss after 656769120 batches: 0.0257
trigger times: 1
Loss after 656897760 batches: 0.0226
trigger times: 2
Loss after 657026400 batches: 0.0205
trigger times: 3
Loss after 657155040 batches: 0.0189
trigger times: 4
Loss after 657283680 batches: 0.0183
trigger times: 5
Loss after 657412320 batches: 0.0175
trigger times: 0
Loss after 657540960 batches: 0.0168
trigger times: 1
Loss after 657669600 batches: 0.0165
trigger times: 2
Loss after 657798240 batches: 0.0155
trigger times: 3
Loss after 657926880 batches: 0.0155
trigger times: 4
Loss after 658055520 batches: 0.0150
trigger times: 0
Loss after 658184160 batches: 0.0149
trigger times: 1
Loss after 658312800 batches: 0.0146
trigger times: 2
Loss after 658441440 batches: 0.0141
trigger times: 3
Loss after 658570080 batches: 0.0139
trigger times: 4
Loss after 658698720 batches: 0.0139
trigger times: 5
Loss after 658827360 batches: 0.0133
trigger times: 6
Loss after 658956000 batches: 0.0134
trigger times: 7
Loss after 659084640 batches: 0.0134
trigger times: 8
Loss after 659213280 batches: 0.0130
trigger times: 9
Loss after 659341920 batches: 0.0129
trigger times: 10
Loss after 659470560 batches: 0.0128
trigger times: 11
Loss after 659599200 batches: 0.0128
trigger times: 12
Loss after 659727840 batches: 0.0125
trigger times: 13
Loss after 659856480 batches: 0.0124
trigger times: 14
Loss after 659985120 batches: 0.0122
trigger times: 15
Loss after 660113760 batches: 0.0124
trigger times: 16
Loss after 660242400 batches: 0.0122
trigger times: 17
Loss after 660371040 batches: 0.0121
trigger times: 18
Loss after 660499680 batches: 0.0119
trigger times: 19
Loss after 660628320 batches: 0.0118
trigger times: 20
Early stopping!
Start to test process.
Loss after 660756960 batches: 0.0117
Time to train on one home:  248.25470495224
trigger times: 0
Loss after 660888060 batches: 0.2803
trigger times: 0
Loss after 661019160 batches: 0.0630
trigger times: 0
Loss after 661150260 batches: 0.0442
trigger times: 0
Loss after 661281360 batches: 0.0379
trigger times: 1
Loss after 661412460 batches: 0.0346
trigger times: 2
Loss after 661543560 batches: 0.0323
trigger times: 0
Loss after 661674660 batches: 0.0303
trigger times: 0
Loss after 661805760 batches: 0.0290
trigger times: 0
Loss after 661936860 batches: 0.0282
trigger times: 1
Loss after 662067960 batches: 0.0274
trigger times: 0
Loss after 662199060 batches: 0.0260
trigger times: 1
Loss after 662330160 batches: 0.0256
trigger times: 0
Loss after 662461260 batches: 0.0250
trigger times: 0
Loss after 662592360 batches: 0.0244
trigger times: 1
Loss after 662723460 batches: 0.0243
trigger times: 2
Loss after 662854560 batches: 0.0237
trigger times: 3
Loss after 662985660 batches: 0.0232
trigger times: 4
Loss after 663116760 batches: 0.0232
trigger times: 5
Loss after 663247860 batches: 0.0227
trigger times: 6
Loss after 663378960 batches: 0.0224
trigger times: 7
Loss after 663510060 batches: 0.0219
trigger times: 0
Loss after 663641160 batches: 0.0216
trigger times: 1
Loss after 663772260 batches: 0.0214
trigger times: 2
Loss after 663903360 batches: 0.0214
trigger times: 3
Loss after 664034460 batches: 0.0209
trigger times: 4
Loss after 664165560 batches: 0.0208
trigger times: 5
Loss after 664296660 batches: 0.0208
trigger times: 6
Loss after 664427760 batches: 0.0205
trigger times: 7
Loss after 664558860 batches: 0.0201
trigger times: 8
Loss after 664689960 batches: 0.0200
trigger times: 9
Loss after 664821060 batches: 0.0201
trigger times: 10
Loss after 664952160 batches: 0.0199
trigger times: 11
Loss after 665083260 batches: 0.0198
trigger times: 12
Loss after 665214360 batches: 0.0195
trigger times: 13
Loss after 665345460 batches: 0.0195
trigger times: 14
Loss after 665476560 batches: 0.0196
trigger times: 15
Loss after 665607660 batches: 0.0190
trigger times: 16
Loss after 665738760 batches: 0.0191
trigger times: 17
Loss after 665869860 batches: 0.0189
trigger times: 18
Loss after 666000960 batches: 0.0185
trigger times: 19
Loss after 666132060 batches: 0.0184
trigger times: 20
Early stopping!
Start to test process.
Loss after 666263160 batches: 0.0186
Time to train on one home:  308.9181230068207
trigger times: 0
Loss after 666394260 batches: 0.2461
trigger times: 1
Loss after 666525360 batches: 0.0820
trigger times: 0
Loss after 666656460 batches: 0.0527
trigger times: 0
Loss after 666787560 batches: 0.0444
trigger times: 1
Loss after 666918660 batches: 0.0384
trigger times: 2
Loss after 667049760 batches: 0.0352
trigger times: 3
Loss after 667180860 batches: 0.0350
trigger times: 0
Loss after 667311960 batches: 0.0317
trigger times: 0
Loss after 667443060 batches: 0.0317
trigger times: 1
Loss after 667574160 batches: 0.0299
trigger times: 2
Loss after 667705260 batches: 0.0289
trigger times: 3
Loss after 667836360 batches: 0.0281
trigger times: 4
Loss after 667967460 batches: 0.0268
trigger times: 5
Loss after 668098560 batches: 0.0270
trigger times: 0
Loss after 668229660 batches: 0.0269
trigger times: 0
Loss after 668360760 batches: 0.0265
trigger times: 1
Loss after 668491860 batches: 0.0256
trigger times: 2
Loss after 668622960 batches: 0.0250
trigger times: 3
Loss after 668754060 batches: 0.0252
trigger times: 4
Loss after 668885160 batches: 0.0249
trigger times: 0
Loss after 669016260 batches: 0.0244
trigger times: 1
Loss after 669147360 batches: 0.0247
trigger times: 0
Loss after 669278460 batches: 0.0245
trigger times: 1
Loss after 669409560 batches: 0.0239
trigger times: 0
Loss after 669540660 batches: 0.0235
trigger times: 1
Loss after 669671760 batches: 0.0224
trigger times: 2
Loss after 669802860 batches: 0.0238
trigger times: 3
Loss after 669933960 batches: 0.0237
trigger times: 4
Loss after 670065060 batches: 0.0223
trigger times: 5
Loss after 670196160 batches: 0.0233
trigger times: 0
Loss after 670327260 batches: 0.0229
trigger times: 1
Loss after 670458360 batches: 0.0217
trigger times: 2
Loss after 670589460 batches: 0.0225
trigger times: 3
Loss after 670720560 batches: 0.0232
trigger times: 4
Loss after 670851660 batches: 0.0241
trigger times: 5
Loss after 670982760 batches: 0.0220
trigger times: 6
Loss after 671113860 batches: 0.0227
trigger times: 7
Loss after 671244960 batches: 0.0222
trigger times: 8
Loss after 671376060 batches: 0.0219
trigger times: 9
Loss after 671507160 batches: 0.0212
trigger times: 10
Loss after 671638260 batches: 0.0218
trigger times: 11
Loss after 671769360 batches: 0.0210
trigger times: 0
Loss after 671900460 batches: 0.0207
trigger times: 1
Loss after 672031560 batches: 0.0213
trigger times: 2
Loss after 672162660 batches: 0.0216
trigger times: 3
Loss after 672293760 batches: 0.0208
trigger times: 4
Loss after 672424860 batches: 0.0205
trigger times: 5
Loss after 672555960 batches: 0.0204
trigger times: 6
Loss after 672687060 batches: 0.0223
trigger times: 0
Loss after 672818160 batches: 0.0211
trigger times: 1
Loss after 672949260 batches: 0.0196
trigger times: 2
Loss after 673080360 batches: 0.0197
trigger times: 3
Loss after 673211460 batches: 0.0199
trigger times: 4
Loss after 673342560 batches: 0.0199
trigger times: 5
Loss after 673473660 batches: 0.0200
trigger times: 6
Loss after 673604760 batches: 0.0196
trigger times: 7
Loss after 673735860 batches: 0.0192
trigger times: 8
Loss after 673866960 batches: 0.0205
trigger times: 0
Loss after 673998060 batches: 0.0199
trigger times: 1
Loss after 674129160 batches: 0.0190
trigger times: 2
Loss after 674260260 batches: 0.0190
trigger times: 3
Loss after 674391360 batches: 0.0197
trigger times: 4
Loss after 674522460 batches: 0.0190
trigger times: 5
Loss after 674653560 batches: 0.0191
trigger times: 6
Loss after 674784660 batches: 0.0196
trigger times: 7
Loss after 674915760 batches: 0.0195
trigger times: 8
Loss after 675046860 batches: 0.0194
trigger times: 9
Loss after 675177960 batches: 0.0194
trigger times: 10
Loss after 675309060 batches: 0.0196
trigger times: 11
Loss after 675440160 batches: 0.0186
trigger times: 12
Loss after 675571260 batches: 0.0189
trigger times: 13
Loss after 675702360 batches: 0.0192
trigger times: 14
Loss after 675833460 batches: 0.0184
trigger times: 15
Loss after 675964560 batches: 0.0190
trigger times: 16
Loss after 676095660 batches: 0.0199
trigger times: 17
Loss after 676226760 batches: 0.0189
trigger times: 18
Loss after 676357860 batches: 0.0182
trigger times: 19
Loss after 676488960 batches: 0.0185
trigger times: 20
Early stopping!
Start to test process.
Loss after 676620060 batches: 0.0191
Time to train on one home:  570.0375163555145
trigger times: 0
Loss after 676751160 batches: 0.0752
trigger times: 1
Loss after 676882260 batches: 0.0218
trigger times: 0
Loss after 677013360 batches: 0.0159
trigger times: 1
Loss after 677144460 batches: 0.0135
trigger times: 2
Loss after 677275560 batches: 0.0124
trigger times: 3
Loss after 677406660 batches: 0.0113
trigger times: 4
Loss after 677537760 batches: 0.0113
trigger times: 5
Loss after 677668860 batches: 0.0107
trigger times: 6
Loss after 677799960 batches: 0.0104
trigger times: 7
Loss after 677931060 batches: 0.0103
trigger times: 8
Loss after 678062160 batches: 0.0097
trigger times: 9
Loss after 678193260 batches: 0.0095
trigger times: 10
Loss after 678324360 batches: 0.0091
trigger times: 11
Loss after 678455460 batches: 0.0091
trigger times: 12
Loss after 678586560 batches: 0.0093
trigger times: 13
Loss after 678717660 batches: 0.0089
trigger times: 14
Loss after 678848760 batches: 0.0085
trigger times: 15
Loss after 678979860 batches: 0.0086
trigger times: 16
Loss after 679110960 batches: 0.0084
trigger times: 17
Loss after 679242060 batches: 0.0084
trigger times: 18
Loss after 679373160 batches: 0.0083
trigger times: 19
Loss after 679504260 batches: 0.0081
trigger times: 20
Early stopping!
Start to test process.
Loss after 679635360 batches: 0.0079
Time to train on one home:  174.49904489517212
trigger times: 0
Loss after 679713960 batches: 0.2382
trigger times: 0
Loss after 679792560 batches: 0.0603
trigger times: 0
Loss after 679871160 batches: 0.0390
trigger times: 0
Loss after 679949760 batches: 0.0315
trigger times: 0
Loss after 680028360 batches: 0.0286
trigger times: 1
Loss after 680106960 batches: 0.0266
trigger times: 0
Loss after 680185560 batches: 0.0245
trigger times: 1
Loss after 680264160 batches: 0.0235
trigger times: 2
Loss after 680342760 batches: 0.0226
trigger times: 3
Loss after 680421360 batches: 0.0218
trigger times: 0
Loss after 680499960 batches: 0.0215
trigger times: 1
Loss after 680578560 batches: 0.0213
trigger times: 2
Loss after 680657160 batches: 0.0200
trigger times: 3
Loss after 680735760 batches: 0.0197
trigger times: 4
Loss after 680814360 batches: 0.0195
trigger times: 0
Loss after 680892960 batches: 0.0187
trigger times: 1
Loss after 680971560 batches: 0.0185
trigger times: 2
Loss after 681050160 batches: 0.0180
trigger times: 3
Loss after 681128760 batches: 0.0181
trigger times: 4
Loss after 681207360 batches: 0.0180
trigger times: 5
Loss after 681285960 batches: 0.0179
trigger times: 6
Loss after 681364560 batches: 0.0171
trigger times: 7
Loss after 681443160 batches: 0.0173
trigger times: 8
Loss after 681521760 batches: 0.0172
trigger times: 9
Loss after 681600360 batches: 0.0166
trigger times: 10
Loss after 681678960 batches: 0.0165
trigger times: 0
Loss after 681757560 batches: 0.0164
trigger times: 1
Loss after 681836160 batches: 0.0169
trigger times: 2
Loss after 681914760 batches: 0.0158
trigger times: 3
Loss after 681993360 batches: 0.0161
trigger times: 4
Loss after 682071960 batches: 0.0161
trigger times: 5
Loss after 682150560 batches: 0.0159
trigger times: 6
Loss after 682229160 batches: 0.0153
trigger times: 7
Loss after 682307760 batches: 0.0154
trigger times: 8
Loss after 682386360 batches: 0.0155
trigger times: 9
Loss after 682464960 batches: 0.0153
trigger times: 10
Loss after 682543560 batches: 0.0150
trigger times: 11
Loss after 682622160 batches: 0.0152
trigger times: 12
Loss after 682700760 batches: 0.0151
trigger times: 13
Loss after 682779360 batches: 0.0149
trigger times: 14
Loss after 682857960 batches: 0.0147
trigger times: 15
Loss after 682936560 batches: 0.0149
trigger times: 16
Loss after 683015160 batches: 0.0143
trigger times: 17
Loss after 683093760 batches: 0.0144
trigger times: 18
Loss after 683172360 batches: 0.0144
trigger times: 19
Loss after 683250960 batches: 0.0141
trigger times: 20
Early stopping!
Start to test process.
Loss after 683329560 batches: 0.0146
Time to train on one home:  226.42710900306702
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097, 0.016851266692967333, 0.01539797175087967, 0.015622467135075917, 0.014339386766728896, 0.015215330347522192]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983], [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013], [0.5549709763791826, 0.4001270641513627, 0.48658498833859326, 1.0394903420759227, 0.4914126622529051, 24.558331909615507, 1517.0215], [0.5461304287115732, 0.4097276108845518, 0.4876006728779417, 1.0136453079994863, 0.48354794633174836, 23.94773371608454, 1492.7424], [0.5402040051089393, 0.41609081728992114, 0.4927051878338842, 1.0002093606660662, 0.47833524208513384, 23.630304644569275, 1476.6505], [0.5334714584880405, 0.4234695591708837, 0.501786021212506, 1.0005226338786515, 0.47229061667346856, 23.637705836501173, 1457.9905]]
Round_12_results:  [0.5334714584880405, 0.4234695591708837, 0.501786021212506, 1.0005226338786515, 0.47229061667346856, 23.637705836501173, 1457.9905]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5547 < 5548; dropping {'Training_Loss': 0.11264651549874612, 'Validation_Loss': 0.1948369418581327, 'Training_R2': 0.8865392771117, 'Validation_R2': 0.8190164617825768, 'Training_F1': 0.8235116407095585, 'Validation_F1': 0.7400281676093018, 'Training_NEP': 0.352552105997412, 'Validation_NEP': 0.48705535843673725, 'Training_NDE': 0.08517697860319923, 'Validation_NDE': 0.14412574945029577, 'Training_MAE': 11.675826564413459, 'Validation_MAE': 13.357078597237987, 'Training_MSE': 374.76538, 'Validation_MSE': 532.25244}.
trigger times: 0
Loss after 683460660 batches: 0.1126
trigger times: 0
Loss after 683591760 batches: 0.0278
trigger times: 0
Loss after 683722860 batches: 0.0206
trigger times: 1
Loss after 683853960 batches: 0.0180
trigger times: 2
Loss after 683985060 batches: 0.0168
trigger times: 3
Loss after 684116160 batches: 0.0155
trigger times: 4
Loss after 684247260 batches: 0.0148
trigger times: 5
Loss after 684378360 batches: 0.0138
trigger times: 6
Loss after 684509460 batches: 0.0133
trigger times: 7
Loss after 684640560 batches: 0.0129
trigger times: 8
Loss after 684771660 batches: 0.0126
trigger times: 9
Loss after 684902760 batches: 0.0124
trigger times: 10
Loss after 685033860 batches: 0.0122
trigger times: 11
Loss after 685164960 batches: 0.0117
trigger times: 12
Loss after 685296060 batches: 0.0118
trigger times: 13
Loss after 685427160 batches: 0.0113
trigger times: 14
Loss after 685558260 batches: 0.0111
trigger times: 15
Loss after 685689360 batches: 0.0109
trigger times: 16
Loss after 685820460 batches: 0.0109
trigger times: 17
Loss after 685951560 batches: 0.0106
trigger times: 18
Loss after 686082660 batches: 0.0106
trigger times: 19
Loss after 686213760 batches: 0.0104
trigger times: 20
Early stopping!
Start to test process.
Loss after 686344860 batches: 0.0106
Time to train on one home:  175.10353326797485
trigger times: 0
Loss after 686447460 batches: 0.2654
trigger times: 0
Loss after 686550060 batches: 0.0795
trigger times: 1
Loss after 686652660 batches: 0.0546
trigger times: 2
Loss after 686755260 batches: 0.0429
trigger times: 3
Loss after 686857860 batches: 0.0387
trigger times: 4
Loss after 686960460 batches: 0.0346
trigger times: 5
Loss after 687063060 batches: 0.0336
trigger times: 6
Loss after 687165660 batches: 0.0322
trigger times: 7
Loss after 687268260 batches: 0.0301
trigger times: 8
Loss after 687370860 batches: 0.0290
trigger times: 9
Loss after 687473460 batches: 0.0303
trigger times: 10
Loss after 687576060 batches: 0.0280
trigger times: 0
Loss after 687678660 batches: 0.0261
trigger times: 1
Loss after 687781260 batches: 0.0253
trigger times: 2
Loss after 687883860 batches: 0.0248
trigger times: 3
Loss after 687986460 batches: 0.0240
trigger times: 0
Loss after 688089060 batches: 0.0251
trigger times: 1
Loss after 688191660 batches: 0.0278
trigger times: 2
Loss after 688294260 batches: 0.0235
trigger times: 3
Loss after 688396860 batches: 0.0224
trigger times: 4
Loss after 688499460 batches: 0.0232
trigger times: 5
Loss after 688602060 batches: 0.0222
trigger times: 6
Loss after 688704660 batches: 0.0242
trigger times: 7
Loss after 688807260 batches: 0.0229
trigger times: 8
Loss after 688909860 batches: 0.0220
trigger times: 9
Loss after 689012460 batches: 0.0220
trigger times: 10
Loss after 689115060 batches: 0.0205
trigger times: 11
Loss after 689217660 batches: 0.0198
trigger times: 12
Loss after 689320260 batches: 0.0196
trigger times: 13
Loss after 689422860 batches: 0.0197
trigger times: 14
Loss after 689525460 batches: 0.0213
trigger times: 15
Loss after 689628060 batches: 0.0202
trigger times: 16
Loss after 689730660 batches: 0.0194
trigger times: 17
Loss after 689833260 batches: 0.0193
trigger times: 18
Loss after 689935860 batches: 0.0206
trigger times: 19
Loss after 690038460 batches: 0.0191
trigger times: 20
Early stopping!
Start to test process.
Loss after 690141060 batches: 0.0206
Time to train on one home:  222.2034511566162
trigger times: 0
Loss after 690272160 batches: 0.1542
trigger times: 1
Loss after 690403260 batches: 0.0474
trigger times: 2
Loss after 690534360 batches: 0.0346
trigger times: 3
Loss after 690665460 batches: 0.0300
trigger times: 4
Loss after 690796560 batches: 0.0274
trigger times: 5
Loss after 690927660 batches: 0.0257
trigger times: 6
Loss after 691058760 batches: 0.0242
trigger times: 7
Loss after 691189860 batches: 0.0233
trigger times: 8
Loss after 691320960 batches: 0.0223
trigger times: 9
Loss after 691452060 batches: 0.0215
trigger times: 10
Loss after 691583160 batches: 0.0207
trigger times: 11
Loss after 691714260 batches: 0.0200
trigger times: 12
Loss after 691845360 batches: 0.0197
trigger times: 13
Loss after 691976460 batches: 0.0192
trigger times: 14
Loss after 692107560 batches: 0.0191
trigger times: 15
Loss after 692238660 batches: 0.0185
trigger times: 16
Loss after 692369760 batches: 0.0181
trigger times: 17
Loss after 692500860 batches: 0.0179
trigger times: 18
Loss after 692631960 batches: 0.0177
trigger times: 19
Loss after 692763060 batches: 0.0175
trigger times: 20
Early stopping!
Start to test process.
Loss after 692894160 batches: 0.0172
Time to train on one home:  160.3617627620697
trigger times: 0
Loss after 693025260 batches: 0.1902
trigger times: 0
Loss after 693156360 batches: 0.0550
trigger times: 0
Loss after 693287460 batches: 0.0396
trigger times: 0
Loss after 693418560 batches: 0.0343
trigger times: 0
Loss after 693549660 batches: 0.0312
trigger times: 1
Loss after 693680760 batches: 0.0296
trigger times: 2
Loss after 693811860 batches: 0.0275
trigger times: 3
Loss after 693942960 batches: 0.0266
trigger times: 4
Loss after 694074060 batches: 0.0256
trigger times: 0
Loss after 694205160 batches: 0.0246
trigger times: 1
Loss after 694336260 batches: 0.0242
trigger times: 2
Loss after 694467360 batches: 0.0234
trigger times: 3
Loss after 694598460 batches: 0.0233
trigger times: 4
Loss after 694729560 batches: 0.0227
trigger times: 5
Loss after 694860660 batches: 0.0222
trigger times: 6
Loss after 694991760 batches: 0.0221
trigger times: 7
Loss after 695122860 batches: 0.0215
trigger times: 8
Loss after 695253960 batches: 0.0213
trigger times: 9
Loss after 695385060 batches: 0.0215
trigger times: 10
Loss after 695516160 batches: 0.0210
trigger times: 11
Loss after 695647260 batches: 0.0206
trigger times: 12
Loss after 695778360 batches: 0.0207
trigger times: 13
Loss after 695909460 batches: 0.0202
trigger times: 14
Loss after 696040560 batches: 0.0198
trigger times: 15
Loss after 696171660 batches: 0.0198
trigger times: 16
Loss after 696302760 batches: 0.0198
trigger times: 17
Loss after 696433860 batches: 0.0196
trigger times: 18
Loss after 696564960 batches: 0.0192
trigger times: 19
Loss after 696696060 batches: 0.0192
trigger times: 20
Early stopping!
Start to test process.
Loss after 696827160 batches: 0.0194
Time to train on one home:  223.9641227722168
trigger times: 0
Loss after 696955800 batches: 0.1123
trigger times: 0
Loss after 697084440 batches: 0.0329
trigger times: 1
Loss after 697213080 batches: 0.0246
trigger times: 0
Loss after 697341720 batches: 0.0212
trigger times: 1
Loss after 697470360 batches: 0.0197
trigger times: 2
Loss after 697599000 batches: 0.0184
trigger times: 3
Loss after 697727640 batches: 0.0176
trigger times: 4
Loss after 697856280 batches: 0.0168
trigger times: 0
Loss after 697984920 batches: 0.0163
trigger times: 1
Loss after 698113560 batches: 0.0161
trigger times: 2
Loss after 698242200 batches: 0.0153
trigger times: 3
Loss after 698370840 batches: 0.0152
trigger times: 4
Loss after 698499480 batches: 0.0148
trigger times: 5
Loss after 698628120 batches: 0.0145
trigger times: 0
Loss after 698756760 batches: 0.0146
trigger times: 1
Loss after 698885400 batches: 0.0140
trigger times: 0
Loss after 699014040 batches: 0.0136
trigger times: 1
Loss after 699142680 batches: 0.0133
trigger times: 2
Loss after 699271320 batches: 0.0131
trigger times: 3
Loss after 699399960 batches: 0.0130
trigger times: 4
Loss after 699528600 batches: 0.0131
trigger times: 5
Loss after 699657240 batches: 0.0128
trigger times: 0
Loss after 699785880 batches: 0.0125
trigger times: 1
Loss after 699914520 batches: 0.0124
trigger times: 2
Loss after 700043160 batches: 0.0122
trigger times: 3
Loss after 700171800 batches: 0.0124
trigger times: 4
Loss after 700300440 batches: 0.0123
trigger times: 5
Loss after 700429080 batches: 0.0122
trigger times: 6
Loss after 700557720 batches: 0.0119
trigger times: 7
Loss after 700686360 batches: 0.0118
trigger times: 8
Loss after 700815000 batches: 0.0116
trigger times: 9
Loss after 700943640 batches: 0.0118
trigger times: 10
Loss after 701072280 batches: 0.0117
trigger times: 11
Loss after 701200920 batches: 0.0117
trigger times: 12
Loss after 701329560 batches: 0.0112
trigger times: 13
Loss after 701458200 batches: 0.0113
trigger times: 14
Loss after 701586840 batches: 0.0112
trigger times: 15
Loss after 701715480 batches: 0.0113
trigger times: 16
Loss after 701844120 batches: 0.0111
trigger times: 17
Loss after 701972760 batches: 0.0109
trigger times: 18
Loss after 702101400 batches: 0.0108
trigger times: 19
Loss after 702230040 batches: 0.0108
trigger times: 20
Early stopping!
Start to test process.
Loss after 702358680 batches: 0.0108
Time to train on one home:  311.6945173740387
trigger times: 0
Loss after 702489780 batches: 0.2179
trigger times: 0
Loss after 702620880 batches: 0.0543
trigger times: 0
Loss after 702751980 batches: 0.0399
trigger times: 1
Loss after 702883080 batches: 0.0351
trigger times: 2
Loss after 703014180 batches: 0.0316
trigger times: 3
Loss after 703145280 batches: 0.0301
trigger times: 4
Loss after 703276380 batches: 0.0284
trigger times: 5
Loss after 703407480 batches: 0.0277
trigger times: 6
Loss after 703538580 batches: 0.0267
trigger times: 7
Loss after 703669680 batches: 0.0257
trigger times: 8
Loss after 703800780 batches: 0.0250
trigger times: 0
Loss after 703931880 batches: 0.0241
trigger times: 1
Loss after 704062980 batches: 0.0238
trigger times: 0
Loss after 704194080 batches: 0.0235
trigger times: 0
Loss after 704325180 batches: 0.0229
trigger times: 1
Loss after 704456280 batches: 0.0226
trigger times: 2
Loss after 704587380 batches: 0.0222
trigger times: 3
Loss after 704718480 batches: 0.0218
trigger times: 0
Loss after 704849580 batches: 0.0217
trigger times: 1
Loss after 704980680 batches: 0.0217
trigger times: 2
Loss after 705111780 batches: 0.0211
trigger times: 3
Loss after 705242880 batches: 0.0206
trigger times: 4
Loss after 705373980 batches: 0.0208
trigger times: 5
Loss after 705505080 batches: 0.0206
trigger times: 6
Loss after 705636180 batches: 0.0201
trigger times: 7
Loss after 705767280 batches: 0.0201
trigger times: 8
Loss after 705898380 batches: 0.0201
trigger times: 9
Loss after 706029480 batches: 0.0197
trigger times: 0
Loss after 706160580 batches: 0.0194
trigger times: 1
Loss after 706291680 batches: 0.0193
trigger times: 2
Loss after 706422780 batches: 0.0193
trigger times: 0
Loss after 706553880 batches: 0.0192
trigger times: 1
Loss after 706684980 batches: 0.0192
trigger times: 2
Loss after 706816080 batches: 0.0191
trigger times: 0
Loss after 706947180 batches: 0.0190
trigger times: 1
Loss after 707078280 batches: 0.0190
trigger times: 2
Loss after 707209380 batches: 0.0186
trigger times: 3
Loss after 707340480 batches: 0.0185
trigger times: 4
Loss after 707471580 batches: 0.0186
trigger times: 5
Loss after 707602680 batches: 0.0181
trigger times: 6
Loss after 707733780 batches: 0.0180
trigger times: 7
Loss after 707864880 batches: 0.0181
trigger times: 8
Loss after 707995980 batches: 0.0181
trigger times: 9
Loss after 708127080 batches: 0.0181
trigger times: 10
Loss after 708258180 batches: 0.0176
trigger times: 11
Loss after 708389280 batches: 0.0178
trigger times: 12
Loss after 708520380 batches: 0.0178
trigger times: 13
Loss after 708651480 batches: 0.0176
trigger times: 14
Loss after 708782580 batches: 0.0173
trigger times: 15
Loss after 708913680 batches: 0.0175
trigger times: 16
Loss after 709044780 batches: 0.0172
trigger times: 0
Loss after 709175880 batches: 0.0172
trigger times: 1
Loss after 709306980 batches: 0.0171
trigger times: 0
Loss after 709438080 batches: 0.0170
trigger times: 1
Loss after 709569180 batches: 0.0171
trigger times: 2
Loss after 709700280 batches: 0.0170
trigger times: 3
Loss after 709831380 batches: 0.0167
trigger times: 0
Loss after 709962480 batches: 0.0169
trigger times: 1
Loss after 710093580 batches: 0.0167
trigger times: 2
Loss after 710224680 batches: 0.0165
trigger times: 3
Loss after 710355780 batches: 0.0169
trigger times: 4
Loss after 710486880 batches: 0.0166
trigger times: 5
Loss after 710617980 batches: 0.0166
trigger times: 6
Loss after 710749080 batches: 0.0164
trigger times: 7
Loss after 710880180 batches: 0.0163
trigger times: 0
Loss after 711011280 batches: 0.0162
trigger times: 1
Loss after 711142380 batches: 0.0162
trigger times: 2
Loss after 711273480 batches: 0.0162
trigger times: 3
Loss after 711404580 batches: 0.0161
trigger times: 4
Loss after 711535680 batches: 0.0160
trigger times: 5
Loss after 711666780 batches: 0.0158
trigger times: 6
Loss after 711797880 batches: 0.0160
trigger times: 7
Loss after 711928980 batches: 0.0161
trigger times: 8
Loss after 712060080 batches: 0.0162
trigger times: 9
Loss after 712191180 batches: 0.0158
trigger times: 10
Loss after 712322280 batches: 0.0156
trigger times: 11
Loss after 712453380 batches: 0.0156
trigger times: 12
Loss after 712584480 batches: 0.0154
trigger times: 13
Loss after 712715580 batches: 0.0157
trigger times: 14
Loss after 712846680 batches: 0.0156
trigger times: 15
Loss after 712977780 batches: 0.0158
trigger times: 16
Loss after 713108880 batches: 0.0156
trigger times: 17
Loss after 713239980 batches: 0.0157
trigger times: 18
Loss after 713371080 batches: 0.0155
trigger times: 19
Loss after 713502180 batches: 0.0154
trigger times: 20
Early stopping!
Start to test process.
Loss after 713633280 batches: 0.0156
Time to train on one home:  621.0440232753754
trigger times: 0
Loss after 713764380 batches: 0.2180
trigger times: 0
Loss after 713895480 batches: 0.0813
trigger times: 1
Loss after 714026580 batches: 0.0530
trigger times: 2
Loss after 714157680 batches: 0.0403
trigger times: 0
Loss after 714288780 batches: 0.0361
trigger times: 0
Loss after 714419880 batches: 0.0330
trigger times: 0
Loss after 714550980 batches: 0.0315
trigger times: 0
Loss after 714682080 batches: 0.0304
trigger times: 1
Loss after 714813180 batches: 0.0291
trigger times: 0
Loss after 714944280 batches: 0.0288
trigger times: 1
Loss after 715075380 batches: 0.0280
trigger times: 2
Loss after 715206480 batches: 0.0275
trigger times: 0
Loss after 715337580 batches: 0.0265
trigger times: 0
Loss after 715468680 batches: 0.0264
trigger times: 1
Loss after 715599780 batches: 0.0254
trigger times: 2
Loss after 715730880 batches: 0.0251
trigger times: 3
Loss after 715861980 batches: 0.0241
trigger times: 0
Loss after 715993080 batches: 0.0238
trigger times: 0
Loss after 716124180 batches: 0.0245
trigger times: 1
Loss after 716255280 batches: 0.0243
trigger times: 2
Loss after 716386380 batches: 0.0243
trigger times: 0
Loss after 716517480 batches: 0.0241
trigger times: 1
Loss after 716648580 batches: 0.0240
trigger times: 2
Loss after 716779680 batches: 0.0235
trigger times: 3
Loss after 716910780 batches: 0.0224
trigger times: 4
Loss after 717041880 batches: 0.0220
trigger times: 5
Loss after 717172980 batches: 0.0227
trigger times: 6
Loss after 717304080 batches: 0.0224
trigger times: 7
Loss after 717435180 batches: 0.0221
trigger times: 8
Loss after 717566280 batches: 0.0227
trigger times: 9
Loss after 717697380 batches: 0.0221
trigger times: 10
Loss after 717828480 batches: 0.0218
trigger times: 0
Loss after 717959580 batches: 0.0220
trigger times: 1
Loss after 718090680 batches: 0.0226
trigger times: 2
Loss after 718221780 batches: 0.0219
trigger times: 3
Loss after 718352880 batches: 0.0215
trigger times: 4
Loss after 718483980 batches: 0.0208
trigger times: 5
Loss after 718615080 batches: 0.0215
trigger times: 0
Loss after 718746180 batches: 0.0213
trigger times: 1
Loss after 718877280 batches: 0.0204
trigger times: 2
Loss after 719008380 batches: 0.0206
trigger times: 3
Loss after 719139480 batches: 0.0204
trigger times: 4
Loss after 719270580 batches: 0.0211
trigger times: 5
Loss after 719401680 batches: 0.0204
trigger times: 6
Loss after 719532780 batches: 0.0204
trigger times: 0
Loss after 719663880 batches: 0.0204
trigger times: 0
Loss after 719794980 batches: 0.0205
trigger times: 1
Loss after 719926080 batches: 0.0201
trigger times: 2
Loss after 720057180 batches: 0.0203
trigger times: 3
Loss after 720188280 batches: 0.0206
trigger times: 4
Loss after 720319380 batches: 0.0201
trigger times: 5
Loss after 720450480 batches: 0.0189
trigger times: 6
Loss after 720581580 batches: 0.0195
trigger times: 7
Loss after 720712680 batches: 0.0201
trigger times: 8
Loss after 720843780 batches: 0.0208
trigger times: 9
Loss after 720974880 batches: 0.0191
trigger times: 10
Loss after 721105980 batches: 0.0189
trigger times: 0
Loss after 721237080 batches: 0.0198
trigger times: 1
Loss after 721368180 batches: 0.0191
trigger times: 2
Loss after 721499280 batches: 0.0192
trigger times: 3
Loss after 721630380 batches: 0.0189
trigger times: 4
Loss after 721761480 batches: 0.0187
trigger times: 5
Loss after 721892580 batches: 0.0186
trigger times: 6
Loss after 722023680 batches: 0.0185
trigger times: 0
Loss after 722154780 batches: 0.0187
trigger times: 1
Loss after 722285880 batches: 0.0188
trigger times: 2
Loss after 722416980 batches: 0.0188
trigger times: 3
Loss after 722548080 batches: 0.0185
trigger times: 4
Loss after 722679180 batches: 0.0189
trigger times: 0
Loss after 722810280 batches: 0.0199
trigger times: 1
Loss after 722941380 batches: 0.0188
trigger times: 2
Loss after 723072480 batches: 0.0186
trigger times: 0
Loss after 723203580 batches: 0.0187
trigger times: 1
Loss after 723334680 batches: 0.0187
trigger times: 2
Loss after 723465780 batches: 0.0179
trigger times: 3
Loss after 723596880 batches: 0.0180
trigger times: 4
Loss after 723727980 batches: 0.0175
trigger times: 5
Loss after 723859080 batches: 0.0179
trigger times: 6
Loss after 723990180 batches: 0.0178
trigger times: 7
Loss after 724121280 batches: 0.0184
trigger times: 8
Loss after 724252380 batches: 0.0180
trigger times: 9
Loss after 724383480 batches: 0.0183
trigger times: 10
Loss after 724514580 batches: 0.0195
trigger times: 11
Loss after 724645680 batches: 0.0177
trigger times: 12
Loss after 724776780 batches: 0.0178
trigger times: 13
Loss after 724907880 batches: 0.0180
trigger times: 14
Loss after 725038980 batches: 0.0178
trigger times: 15
Loss after 725170080 batches: 0.0177
trigger times: 16
Loss after 725301180 batches: 0.0177
trigger times: 17
Loss after 725432280 batches: 0.0175
trigger times: 18
Loss after 725563380 batches: 0.0185
trigger times: 19
Loss after 725694480 batches: 0.0173
trigger times: 20
Early stopping!
Start to test process.
Loss after 725825580 batches: 0.0171
Time to train on one home:  670.5142221450806
trigger times: 0
Loss after 725956680 batches: 0.0691
trigger times: 0
Loss after 726087780 batches: 0.0203
trigger times: 1
Loss after 726218880 batches: 0.0151
trigger times: 2
Loss after 726349980 batches: 0.0136
trigger times: 0
Loss after 726481080 batches: 0.0123
trigger times: 1
Loss after 726612180 batches: 0.0116
trigger times: 2
Loss after 726743280 batches: 0.0112
trigger times: 3
Loss after 726874380 batches: 0.0107
trigger times: 4
Loss after 727005480 batches: 0.0104
trigger times: 0
Loss after 727136580 batches: 0.0100
trigger times: 1
Loss after 727267680 batches: 0.0096
trigger times: 2
Loss after 727398780 batches: 0.0095
trigger times: 3
Loss after 727529880 batches: 0.0093
trigger times: 0
Loss after 727660980 batches: 0.0091
trigger times: 1
Loss after 727792080 batches: 0.0090
trigger times: 2
Loss after 727923180 batches: 0.0087
trigger times: 3
Loss after 728054280 batches: 0.0087
trigger times: 4
Loss after 728185380 batches: 0.0084
trigger times: 5
Loss after 728316480 batches: 0.0085
trigger times: 6
Loss after 728447580 batches: 0.0085
trigger times: 7
Loss after 728578680 batches: 0.0083
trigger times: 8
Loss after 728709780 batches: 0.0082
trigger times: 9
Loss after 728840880 batches: 0.0079
trigger times: 10
Loss after 728971980 batches: 0.0078
trigger times: 11
Loss after 729103080 batches: 0.0080
trigger times: 12
Loss after 729234180 batches: 0.0078
trigger times: 13
Loss after 729365280 batches: 0.0078
trigger times: 14
Loss after 729496380 batches: 0.0076
trigger times: 15
Loss after 729627480 batches: 0.0075
trigger times: 16
Loss after 729758580 batches: 0.0076
trigger times: 17
Loss after 729889680 batches: 0.0077
trigger times: 18
Loss after 730020780 batches: 0.0076
trigger times: 19
Loss after 730151880 batches: 0.0075
trigger times: 20
Early stopping!
Start to test process.
Loss after 730282980 batches: 0.0074
Time to train on one home:  252.43969416618347
trigger times: 0
Loss after 730361580 batches: 0.2307
trigger times: 0
Loss after 730440180 batches: 0.0591
trigger times: 1
Loss after 730518780 batches: 0.0371
trigger times: 0
Loss after 730597380 batches: 0.0311
trigger times: 0
Loss after 730675980 batches: 0.0274
trigger times: 0
Loss after 730754580 batches: 0.0266
trigger times: 1
Loss after 730833180 batches: 0.0247
trigger times: 2
Loss after 730911780 batches: 0.0231
trigger times: 3
Loss after 730990380 batches: 0.0222
trigger times: 4
Loss after 731068980 batches: 0.0211
trigger times: 5
Loss after 731147580 batches: 0.0208
trigger times: 6
Loss after 731226180 batches: 0.0205
trigger times: 7
Loss after 731304780 batches: 0.0200
trigger times: 0
Loss after 731383380 batches: 0.0197
trigger times: 1
Loss after 731461980 batches: 0.0187
trigger times: 2
Loss after 731540580 batches: 0.0188
trigger times: 0
Loss after 731619180 batches: 0.0180
trigger times: 1
Loss after 731697780 batches: 0.0179
trigger times: 2
Loss after 731776380 batches: 0.0178
trigger times: 0
Loss after 731854980 batches: 0.0179
trigger times: 1
Loss after 731933580 batches: 0.0176
trigger times: 2
Loss after 732012180 batches: 0.0170
trigger times: 0
Loss after 732090780 batches: 0.0169
trigger times: 1
Loss after 732169380 batches: 0.0171
trigger times: 2
Loss after 732247980 batches: 0.0168
trigger times: 3
Loss after 732326580 batches: 0.0163
trigger times: 4
Loss after 732405180 batches: 0.0166
trigger times: 5
Loss after 732483780 batches: 0.0160
trigger times: 6
Loss after 732562380 batches: 0.0159
trigger times: 7
Loss after 732640980 batches: 0.0160
trigger times: 8
Loss after 732719580 batches: 0.0153
trigger times: 9
Loss after 732798180 batches: 0.0154
trigger times: 10
Loss after 732876780 batches: 0.0155
trigger times: 0
Loss after 732955380 batches: 0.0151
trigger times: 1
Loss after 733033980 batches: 0.0153
trigger times: 2
Loss after 733112580 batches: 0.0153
trigger times: 3
Loss after 733191180 batches: 0.0145
trigger times: 4
Loss after 733269780 batches: 0.0149
trigger times: 5
Loss after 733348380 batches: 0.0150
trigger times: 6
Loss after 733426980 batches: 0.0149
trigger times: 7
Loss after 733505580 batches: 0.0146
trigger times: 8
Loss after 733584180 batches: 0.0148
trigger times: 9
Loss after 733662780 batches: 0.0144
trigger times: 10
Loss after 733741380 batches: 0.0141
trigger times: 11
Loss after 733819980 batches: 0.0141
trigger times: 12
Loss after 733898580 batches: 0.0138
trigger times: 13
Loss after 733977180 batches: 0.0141
trigger times: 0
Loss after 734055780 batches: 0.0137
trigger times: 1
Loss after 734134380 batches: 0.0142
trigger times: 2
Loss after 734212980 batches: 0.0140
trigger times: 3
Loss after 734291580 batches: 0.0139
trigger times: 4
Loss after 734370180 batches: 0.0136
trigger times: 5
Loss after 734448780 batches: 0.0136
trigger times: 6
Loss after 734527380 batches: 0.0134
trigger times: 7
Loss after 734605980 batches: 0.0134
trigger times: 8
Loss after 734684580 batches: 0.0135
trigger times: 9
Loss after 734763180 batches: 0.0135
trigger times: 10
Loss after 734841780 batches: 0.0134
trigger times: 11
Loss after 734920380 batches: 0.0133
trigger times: 12
Loss after 734998980 batches: 0.0132
trigger times: 13
Loss after 735077580 batches: 0.0135
trigger times: 14
Loss after 735156180 batches: 0.0131
trigger times: 15
Loss after 735234780 batches: 0.0131
trigger times: 16
Loss after 735313380 batches: 0.0133
trigger times: 17
Loss after 735391980 batches: 0.0131
trigger times: 18
Loss after 735470580 batches: 0.0134
trigger times: 19
Loss after 735549180 batches: 0.0130
trigger times: 20
Early stopping!
Start to test process.
Loss after 735627780 batches: 0.0128
Time to train on one home:  322.2501401901245
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097, 0.016851266692967333, 0.01539797175087967, 0.015622467135075917, 0.014339386766728896, 0.015215330347522192, 0.014610499733001695]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983], [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013], [0.5549709763791826, 0.4001270641513627, 0.48658498833859326, 1.0394903420759227, 0.4914126622529051, 24.558331909615507, 1517.0215], [0.5461304287115732, 0.4097276108845518, 0.4876006728779417, 1.0136453079994863, 0.48354794633174836, 23.94773371608454, 1492.7424], [0.5402040051089393, 0.41609081728992114, 0.4927051878338842, 1.0002093606660662, 0.47833524208513384, 23.630304644569275, 1476.6505], [0.5334714584880405, 0.4234695591708837, 0.501786021212506, 1.0005226338786515, 0.47229061667346856, 23.637705836501173, 1457.9905], [0.5237383676899804, 0.43399432408412464, 0.5083585548121925, 0.9778479298034527, 0.46366878622151636, 23.10200782557115, 1431.3744]]
Round_13_results:  [0.5237383676899804, 0.43399432408412464, 0.5083585548121925, 0.9778479298034527, 0.46366878622151636, 23.10200782557115, 1431.3744]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5982 < 5983; dropping {'Training_Loss': 0.11313238402582565, 'Validation_Loss': 0.19587314873933792, 'Training_R2': 0.886119565906558, 'Validation_R2': 0.8181368900235655, 'Training_F1': 0.8258147277268635, 'Validation_F1': 0.7300862495672963, 'Training_NEP': 0.3482612367471459, 'Validation_NEP': 0.4922871395259789, 'Training_NDE': 0.0854920632547849, 'Validation_NDE': 0.1448261940333303, 'Training_MAE': 11.533721484555986, 'Validation_MAE': 13.500555740035136, 'Training_MSE': 376.15167, 'Validation_MSE': 534.8392}.
trigger times: 0
Loss after 735758880 batches: 0.1131
trigger times: 0
Loss after 735889980 batches: 0.0280
trigger times: 0
Loss after 736021080 batches: 0.0203
trigger times: 1
Loss after 736152180 batches: 0.0181
trigger times: 0
Loss after 736283280 batches: 0.0168
trigger times: 1
Loss after 736414380 batches: 0.0154
trigger times: 0
Loss after 736545480 batches: 0.0144
trigger times: 1
Loss after 736676580 batches: 0.0141
trigger times: 2
Loss after 736807680 batches: 0.0133
trigger times: 3
Loss after 736938780 batches: 0.0129
trigger times: 4
Loss after 737069880 batches: 0.0127
trigger times: 5
Loss after 737200980 batches: 0.0124
trigger times: 6
Loss after 737332080 batches: 0.0120
trigger times: 7
Loss after 737463180 batches: 0.0114
trigger times: 8
Loss after 737594280 batches: 0.0114
trigger times: 9
Loss after 737725380 batches: 0.0114
trigger times: 10
Loss after 737856480 batches: 0.0112
trigger times: 11
Loss after 737987580 batches: 0.0109
trigger times: 0
Loss after 738118680 batches: 0.0106
trigger times: 1
Loss after 738249780 batches: 0.0105
trigger times: 0
Loss after 738380880 batches: 0.0106
trigger times: 1
Loss after 738511980 batches: 0.0104
trigger times: 2
Loss after 738643080 batches: 0.0101
trigger times: 3
Loss after 738774180 batches: 0.0102
trigger times: 4
Loss after 738905280 batches: 0.0100
trigger times: 5
Loss after 739036380 batches: 0.0100
trigger times: 6
Loss after 739167480 batches: 0.0100
trigger times: 7
Loss after 739298580 batches: 0.0098
trigger times: 8
Loss after 739429680 batches: 0.0097
trigger times: 9
Loss after 739560780 batches: 0.0096
trigger times: 10
Loss after 739691880 batches: 0.0095
trigger times: 11
Loss after 739822980 batches: 0.0095
trigger times: 12
Loss after 739954080 batches: 0.0093
trigger times: 13
Loss after 740085180 batches: 0.0094
trigger times: 14
Loss after 740216280 batches: 0.0093
trigger times: 15
Loss after 740347380 batches: 0.0093
trigger times: 16
Loss after 740478480 batches: 0.0090
trigger times: 17
Loss after 740609580 batches: 0.0093
trigger times: 18
Loss after 740740680 batches: 0.0091
trigger times: 0
Loss after 740871780 batches: 0.0091
trigger times: 1
Loss after 741002880 batches: 0.0089
trigger times: 2
Loss after 741133980 batches: 0.0088
trigger times: 3
Loss after 741265080 batches: 0.0089
trigger times: 4
Loss after 741396180 batches: 0.0090
trigger times: 5
Loss after 741527280 batches: 0.0086
trigger times: 6
Loss after 741658380 batches: 0.0086
trigger times: 7
Loss after 741789480 batches: 0.0083
trigger times: 8
Loss after 741920580 batches: 0.0085
trigger times: 9
Loss after 742051680 batches: 0.0085
trigger times: 10
Loss after 742182780 batches: 0.0086
trigger times: 11
Loss after 742313880 batches: 0.0081
trigger times: 12
Loss after 742444980 batches: 0.0083
trigger times: 13
Loss after 742576080 batches: 0.0086
trigger times: 14
Loss after 742707180 batches: 0.0087
trigger times: 15
Loss after 742838280 batches: 0.0084
trigger times: 16
Loss after 742969380 batches: 0.0080
trigger times: 17
Loss after 743100480 batches: 0.0080
trigger times: 18
Loss after 743231580 batches: 0.0083
trigger times: 19
Loss after 743362680 batches: 0.0082
trigger times: 20
Early stopping!
Start to test process.
Loss after 743493780 batches: 0.0081
Time to train on one home:  437.0769386291504
trigger times: 0
Loss after 743596380 batches: 0.2467
trigger times: 0
Loss after 743698980 batches: 0.0754
trigger times: 1
Loss after 743801580 batches: 0.0514
trigger times: 2
Loss after 743904180 batches: 0.0425
trigger times: 3
Loss after 744006780 batches: 0.0369
trigger times: 4
Loss after 744109380 batches: 0.0363
trigger times: 5
Loss after 744211980 batches: 0.0321
trigger times: 6
Loss after 744314580 batches: 0.0318
trigger times: 7
Loss after 744417180 batches: 0.0299
trigger times: 8
Loss after 744519780 batches: 0.0334
trigger times: 9
Loss after 744622380 batches: 0.0272
trigger times: 10
Loss after 744724980 batches: 0.0254
trigger times: 11
Loss after 744827580 batches: 0.0262
trigger times: 12
Loss after 744930180 batches: 0.0263
trigger times: 13
Loss after 745032780 batches: 0.0233
trigger times: 14
Loss after 745135380 batches: 0.0232
trigger times: 15
Loss after 745237980 batches: 0.0234
trigger times: 16
Loss after 745340580 batches: 0.0232
trigger times: 17
Loss after 745443180 batches: 0.0253
trigger times: 18
Loss after 745545780 batches: 0.0224
trigger times: 19
Loss after 745648380 batches: 0.0216
trigger times: 20
Early stopping!
Start to test process.
Loss after 745750980 batches: 0.0233
Time to train on one home:  136.30319547653198
trigger times: 0
Loss after 745882080 batches: 0.1380
trigger times: 0
Loss after 746013180 batches: 0.0443
trigger times: 1
Loss after 746144280 batches: 0.0331
trigger times: 2
Loss after 746275380 batches: 0.0287
trigger times: 0
Loss after 746406480 batches: 0.0263
trigger times: 1
Loss after 746537580 batches: 0.0247
trigger times: 2
Loss after 746668680 batches: 0.0237
trigger times: 3
Loss after 746799780 batches: 0.0224
trigger times: 4
Loss after 746930880 batches: 0.0216
trigger times: 5
Loss after 747061980 batches: 0.0210
trigger times: 6
Loss after 747193080 batches: 0.0204
trigger times: 7
Loss after 747324180 batches: 0.0198
trigger times: 8
Loss after 747455280 batches: 0.0193
trigger times: 9
Loss after 747586380 batches: 0.0187
trigger times: 10
Loss after 747717480 batches: 0.0187
trigger times: 11
Loss after 747848580 batches: 0.0183
trigger times: 12
Loss after 747979680 batches: 0.0180
trigger times: 13
Loss after 748110780 batches: 0.0175
trigger times: 14
Loss after 748241880 batches: 0.0174
trigger times: 15
Loss after 748372980 batches: 0.0173
trigger times: 16
Loss after 748504080 batches: 0.0170
trigger times: 17
Loss after 748635180 batches: 0.0166
trigger times: 18
Loss after 748766280 batches: 0.0163
trigger times: 19
Loss after 748897380 batches: 0.0164
trigger times: 20
Early stopping!
Start to test process.
Loss after 749028480 batches: 0.0161
Time to train on one home:  188.6098177433014
trigger times: 0
Loss after 749159580 batches: 0.1823
trigger times: 0
Loss after 749290680 batches: 0.0510
trigger times: 1
Loss after 749421780 batches: 0.0382
trigger times: 2
Loss after 749552880 batches: 0.0334
trigger times: 0
Loss after 749683980 batches: 0.0309
trigger times: 1
Loss after 749815080 batches: 0.0293
trigger times: 2
Loss after 749946180 batches: 0.0280
trigger times: 3
Loss after 750077280 batches: 0.0267
trigger times: 4
Loss after 750208380 batches: 0.0256
trigger times: 5
Loss after 750339480 batches: 0.0248
trigger times: 0
Loss after 750470580 batches: 0.0244
trigger times: 1
Loss after 750601680 batches: 0.0240
trigger times: 2
Loss after 750732780 batches: 0.0234
trigger times: 3
Loss after 750863880 batches: 0.0229
trigger times: 0
Loss after 750994980 batches: 0.0227
trigger times: 1
Loss after 751126080 batches: 0.0223
trigger times: 0
Loss after 751257180 batches: 0.0215
trigger times: 1
Loss after 751388280 batches: 0.0216
trigger times: 2
Loss after 751519380 batches: 0.0214
trigger times: 3
Loss after 751650480 batches: 0.0213
trigger times: 4
Loss after 751781580 batches: 0.0208
trigger times: 5
Loss after 751912680 batches: 0.0205
trigger times: 6
Loss after 752043780 batches: 0.0201
trigger times: 7
Loss after 752174880 batches: 0.0203
trigger times: 8
Loss after 752305980 batches: 0.0200
trigger times: 0
Loss after 752437080 batches: 0.0200
trigger times: 0
Loss after 752568180 batches: 0.0198
trigger times: 0
Loss after 752699280 batches: 0.0195
trigger times: 1
Loss after 752830380 batches: 0.0195
trigger times: 2
Loss after 752961480 batches: 0.0194
trigger times: 0
Loss after 753092580 batches: 0.0191
trigger times: 1
Loss after 753223680 batches: 0.0190
trigger times: 0
Loss after 753354780 batches: 0.0187
trigger times: 1
Loss after 753485880 batches: 0.0187
trigger times: 2
Loss after 753616980 batches: 0.0186
trigger times: 3
Loss after 753748080 batches: 0.0184
trigger times: 4
Loss after 753879180 batches: 0.0183
trigger times: 5
Loss after 754010280 batches: 0.0181
trigger times: 6
Loss after 754141380 batches: 0.0180
trigger times: 7
Loss after 754272480 batches: 0.0179
trigger times: 0
Loss after 754403580 batches: 0.0178
trigger times: 1
Loss after 754534680 batches: 0.0180
trigger times: 2
Loss after 754665780 batches: 0.0179
trigger times: 3
Loss after 754796880 batches: 0.0175
trigger times: 4
Loss after 754927980 batches: 0.0173
trigger times: 5
Loss after 755059080 batches: 0.0174
trigger times: 6
Loss after 755190180 batches: 0.0175
trigger times: 7
Loss after 755321280 batches: 0.0173
trigger times: 0
Loss after 755452380 batches: 0.0172
trigger times: 1
Loss after 755583480 batches: 0.0173
trigger times: 2
Loss after 755714580 batches: 0.0172
trigger times: 3
Loss after 755845680 batches: 0.0170
trigger times: 4
Loss after 755976780 batches: 0.0172
trigger times: 5
Loss after 756107880 batches: 0.0171
trigger times: 0
Loss after 756238980 batches: 0.0171
trigger times: 1
Loss after 756370080 batches: 0.0167
trigger times: 2
Loss after 756501180 batches: 0.0169
trigger times: 3
Loss after 756632280 batches: 0.0167
trigger times: 4
Loss after 756763380 batches: 0.0164
trigger times: 0
Loss after 756894480 batches: 0.0167
trigger times: 1
Loss after 757025580 batches: 0.0166
trigger times: 2
Loss after 757156680 batches: 0.0167
trigger times: 3
Loss after 757287780 batches: 0.0162
trigger times: 0
Loss after 757418880 batches: 0.0164
trigger times: 1
Loss after 757549980 batches: 0.0165
trigger times: 2
Loss after 757681080 batches: 0.0163
trigger times: 3
Loss after 757812180 batches: 0.0162
trigger times: 0
Loss after 757943280 batches: 0.0161
trigger times: 1
Loss after 758074380 batches: 0.0162
trigger times: 2
Loss after 758205480 batches: 0.0161
trigger times: 3
Loss after 758336580 batches: 0.0159
trigger times: 0
Loss after 758467680 batches: 0.0159
trigger times: 1
Loss after 758598780 batches: 0.0159
trigger times: 2
Loss after 758729880 batches: 0.0157
trigger times: 3
Loss after 758860980 batches: 0.0159
trigger times: 4
Loss after 758992080 batches: 0.0158
trigger times: 5
Loss after 759123180 batches: 0.0157
trigger times: 6
Loss after 759254280 batches: 0.0157
trigger times: 7
Loss after 759385380 batches: 0.0156
trigger times: 8
Loss after 759516480 batches: 0.0156
trigger times: 9
Loss after 759647580 batches: 0.0154
trigger times: 10
Loss after 759778680 batches: 0.0156
trigger times: 11
Loss after 759909780 batches: 0.0156
trigger times: 12
Loss after 760040880 batches: 0.0155
trigger times: 13
Loss after 760171980 batches: 0.0153
trigger times: 14
Loss after 760303080 batches: 0.0152
trigger times: 15
Loss after 760434180 batches: 0.0152
trigger times: 16
Loss after 760565280 batches: 0.0156
trigger times: 17
Loss after 760696380 batches: 0.0156
trigger times: 18
Loss after 760827480 batches: 0.0153
trigger times: 19
Loss after 760958580 batches: 0.0151
trigger times: 20
Early stopping!
Start to test process.
Loss after 761089680 batches: 0.0152
Time to train on one home:  662.7677836418152
trigger times: 0
Loss after 761218320 batches: 0.1233
trigger times: 0
Loss after 761346960 batches: 0.0326
trigger times: 1
Loss after 761475600 batches: 0.0240
trigger times: 0
Loss after 761604240 batches: 0.0208
trigger times: 0
Loss after 761732880 batches: 0.0194
trigger times: 1
Loss after 761861520 batches: 0.0182
trigger times: 2
Loss after 761990160 batches: 0.0173
trigger times: 3
Loss after 762118800 batches: 0.0164
trigger times: 4
Loss after 762247440 batches: 0.0157
trigger times: 0
Loss after 762376080 batches: 0.0153
trigger times: 1
Loss after 762504720 batches: 0.0150
trigger times: 2
Loss after 762633360 batches: 0.0148
trigger times: 3
Loss after 762762000 batches: 0.0146
trigger times: 4
Loss after 762890640 batches: 0.0140
trigger times: 0
Loss after 763019280 batches: 0.0138
trigger times: 1
Loss after 763147920 batches: 0.0138
trigger times: 2
Loss after 763276560 batches: 0.0132
trigger times: 3
Loss after 763405200 batches: 0.0134
trigger times: 4
Loss after 763533840 batches: 0.0129
trigger times: 5
Loss after 763662480 batches: 0.0130
trigger times: 6
Loss after 763791120 batches: 0.0128
trigger times: 7
Loss after 763919760 batches: 0.0126
trigger times: 8
Loss after 764048400 batches: 0.0126
trigger times: 9
Loss after 764177040 batches: 0.0121
trigger times: 10
Loss after 764305680 batches: 0.0122
trigger times: 11
Loss after 764434320 batches: 0.0121
trigger times: 12
Loss after 764562960 batches: 0.0119
trigger times: 13
Loss after 764691600 batches: 0.0117
trigger times: 14
Loss after 764820240 batches: 0.0119
trigger times: 15
Loss after 764948880 batches: 0.0114
trigger times: 16
Loss after 765077520 batches: 0.0114
trigger times: 17
Loss after 765206160 batches: 0.0115
trigger times: 18
Loss after 765334800 batches: 0.0114
trigger times: 19
Loss after 765463440 batches: 0.0113
trigger times: 0
Loss after 765592080 batches: 0.0112
trigger times: 1
Loss after 765720720 batches: 0.0110
trigger times: 2
Loss after 765849360 batches: 0.0111
trigger times: 3
Loss after 765978000 batches: 0.0110
trigger times: 4
Loss after 766106640 batches: 0.0108
trigger times: 5
Loss after 766235280 batches: 0.0109
trigger times: 6
Loss after 766363920 batches: 0.0109
trigger times: 7
Loss after 766492560 batches: 0.0107
trigger times: 8
Loss after 766621200 batches: 0.0108
trigger times: 9
Loss after 766749840 batches: 0.0106
trigger times: 10
Loss after 766878480 batches: 0.0106
trigger times: 11
Loss after 767007120 batches: 0.0107
trigger times: 12
Loss after 767135760 batches: 0.0104
trigger times: 13
Loss after 767264400 batches: 0.0102
trigger times: 14
Loss after 767393040 batches: 0.0100
trigger times: 15
Loss after 767521680 batches: 0.0101
trigger times: 16
Loss after 767650320 batches: 0.0102
trigger times: 17
Loss after 767778960 batches: 0.0101
trigger times: 18
Loss after 767907600 batches: 0.0101
trigger times: 19
Loss after 768036240 batches: 0.0100
trigger times: 20
Early stopping!
Start to test process.
Loss after 768164880 batches: 0.0101
Time to train on one home:  396.96877813339233
trigger times: 0
Loss after 768295980 batches: 0.2398
trigger times: 0
Loss after 768427080 batches: 0.0552
trigger times: 0
Loss after 768558180 batches: 0.0396
trigger times: 0
Loss after 768689280 batches: 0.0345
trigger times: 0
Loss after 768820380 batches: 0.0321
trigger times: 1
Loss after 768951480 batches: 0.0296
trigger times: 0
Loss after 769082580 batches: 0.0283
trigger times: 0
Loss after 769213680 batches: 0.0271
trigger times: 0
Loss after 769344780 batches: 0.0258
trigger times: 1
Loss after 769475880 batches: 0.0253
trigger times: 2
Loss after 769606980 batches: 0.0248
trigger times: 3
Loss after 769738080 batches: 0.0241
trigger times: 4
Loss after 769869180 batches: 0.0236
trigger times: 0
Loss after 770000280 batches: 0.0228
trigger times: 0
Loss after 770131380 batches: 0.0226
trigger times: 1
Loss after 770262480 batches: 0.0221
trigger times: 2
Loss after 770393580 batches: 0.0218
trigger times: 3
Loss after 770524680 batches: 0.0215
trigger times: 4
Loss after 770655780 batches: 0.0214
trigger times: 0
Loss after 770786880 batches: 0.0210
trigger times: 0
Loss after 770917980 batches: 0.0207
trigger times: 0
Loss after 771049080 batches: 0.0207
trigger times: 0
Loss after 771180180 batches: 0.0201
trigger times: 1
Loss after 771311280 batches: 0.0200
trigger times: 0
Loss after 771442380 batches: 0.0202
trigger times: 1
Loss after 771573480 batches: 0.0199
trigger times: 2
Loss after 771704580 batches: 0.0197
trigger times: 3
Loss after 771835680 batches: 0.0196
trigger times: 0
Loss after 771966780 batches: 0.0192
trigger times: 0
Loss after 772097880 batches: 0.0190
trigger times: 1
Loss after 772228980 batches: 0.0191
trigger times: 2
Loss after 772360080 batches: 0.0187
trigger times: 0
Loss after 772491180 batches: 0.0189
trigger times: 1
Loss after 772622280 batches: 0.0188
trigger times: 2
Loss after 772753380 batches: 0.0182
trigger times: 3
Loss after 772884480 batches: 0.0182
trigger times: 4
Loss after 773015580 batches: 0.0181
trigger times: 5
Loss after 773146680 batches: 0.0181
trigger times: 6
Loss after 773277780 batches: 0.0180
trigger times: 7
Loss after 773408880 batches: 0.0178
trigger times: 8
Loss after 773539980 batches: 0.0178
trigger times: 9
Loss after 773671080 batches: 0.0178
trigger times: 0
Loss after 773802180 batches: 0.0176
trigger times: 1
Loss after 773933280 batches: 0.0177
trigger times: 2
Loss after 774064380 batches: 0.0172
trigger times: 3
Loss after 774195480 batches: 0.0171
trigger times: 4
Loss after 774326580 batches: 0.0172
trigger times: 5
Loss after 774457680 batches: 0.0173
trigger times: 6
Loss after 774588780 batches: 0.0174
trigger times: 7
Loss after 774719880 batches: 0.0173
trigger times: 0
Loss after 774850980 batches: 0.0171
trigger times: 0
Loss after 774982080 batches: 0.0169
trigger times: 1
Loss after 775113180 batches: 0.0168
trigger times: 2
Loss after 775244280 batches: 0.0169
trigger times: 3
Loss after 775375380 batches: 0.0169
trigger times: 4
Loss after 775506480 batches: 0.0168
trigger times: 5
Loss after 775637580 batches: 0.0164
trigger times: 0
Loss after 775768680 batches: 0.0165
trigger times: 1
Loss after 775899780 batches: 0.0166
trigger times: 2
Loss after 776030880 batches: 0.0166
trigger times: 3
Loss after 776161980 batches: 0.0165
trigger times: 4
Loss after 776293080 batches: 0.0164
trigger times: 5
Loss after 776424180 batches: 0.0162
trigger times: 6
Loss after 776555280 batches: 0.0162
trigger times: 0
Loss after 776686380 batches: 0.0161
trigger times: 1
Loss after 776817480 batches: 0.0161
trigger times: 2
Loss after 776948580 batches: 0.0158
trigger times: 3
Loss after 777079680 batches: 0.0159
trigger times: 4
Loss after 777210780 batches: 0.0158
trigger times: 5
Loss after 777341880 batches: 0.0159
trigger times: 6
Loss after 777472980 batches: 0.0158
trigger times: 0
Loss after 777604080 batches: 0.0160
trigger times: 1
Loss after 777735180 batches: 0.0157
trigger times: 2
Loss after 777866280 batches: 0.0157
trigger times: 3
Loss after 777997380 batches: 0.0156
trigger times: 4
Loss after 778128480 batches: 0.0155
trigger times: 5
Loss after 778259580 batches: 0.0154
trigger times: 6
Loss after 778390680 batches: 0.0155
trigger times: 7
Loss after 778521780 batches: 0.0154
trigger times: 8
Loss after 778652880 batches: 0.0155
trigger times: 9
Loss after 778783980 batches: 0.0150
trigger times: 10
Loss after 778915080 batches: 0.0151
trigger times: 0
Loss after 779046180 batches: 0.0152
trigger times: 1
Loss after 779177280 batches: 0.0151
trigger times: 2
Loss after 779308380 batches: 0.0153
trigger times: 3
Loss after 779439480 batches: 0.0153
trigger times: 4
Loss after 779570580 batches: 0.0150
trigger times: 5
Loss after 779701680 batches: 0.0150
trigger times: 6
Loss after 779832780 batches: 0.0153
trigger times: 7
Loss after 779963880 batches: 0.0154
trigger times: 8
Loss after 780094980 batches: 0.0151
trigger times: 9
Loss after 780226080 batches: 0.0150
trigger times: 10
Loss after 780357180 batches: 0.0149
trigger times: 11
Loss after 780488280 batches: 0.0149
trigger times: 12
Loss after 780619380 batches: 0.0148
trigger times: 13
Loss after 780750480 batches: 0.0146
trigger times: 14
Loss after 780881580 batches: 0.0146
trigger times: 15
Loss after 781012680 batches: 0.0147
trigger times: 16
Loss after 781143780 batches: 0.0147
trigger times: 17
Loss after 781274880 batches: 0.0147
trigger times: 18
Loss after 781405980 batches: 0.0147
trigger times: 19
Loss after 781537080 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 781668180 batches: 0.0146
Time to train on one home:  749.3032629489899
trigger times: 0
Loss after 781799280 batches: 0.1897
trigger times: 1
Loss after 781930380 batches: 0.0749
trigger times: 0
Loss after 782061480 batches: 0.0477
trigger times: 1
Loss after 782192580 batches: 0.0408
trigger times: 0
Loss after 782323680 batches: 0.0371
trigger times: 0
Loss after 782454780 batches: 0.0340
trigger times: 1
Loss after 782585880 batches: 0.0311
trigger times: 2
Loss after 782716980 batches: 0.0297
trigger times: 3
Loss after 782848080 batches: 0.0288
trigger times: 4
Loss after 782979180 batches: 0.0274
trigger times: 0
Loss after 783110280 batches: 0.0276
trigger times: 1
Loss after 783241380 batches: 0.0273
trigger times: 0
Loss after 783372480 batches: 0.0277
trigger times: 0
Loss after 783503580 batches: 0.0247
trigger times: 1
Loss after 783634680 batches: 0.0235
trigger times: 2
Loss after 783765780 batches: 0.0243
trigger times: 0
Loss after 783896880 batches: 0.0238
trigger times: 0
Loss after 784027980 batches: 0.0233
trigger times: 0
Loss after 784159080 batches: 0.0231
trigger times: 1
Loss after 784290180 batches: 0.0227
trigger times: 2
Loss after 784421280 batches: 0.0230
trigger times: 0
Loss after 784552380 batches: 0.0227
trigger times: 0
Loss after 784683480 batches: 0.0234
trigger times: 1
Loss after 784814580 batches: 0.0220
trigger times: 2
Loss after 784945680 batches: 0.0218
trigger times: 3
Loss after 785076780 batches: 0.0215
trigger times: 4
Loss after 785207880 batches: 0.0220
trigger times: 5
Loss after 785338980 batches: 0.0217
trigger times: 0
Loss after 785470080 batches: 0.0213
trigger times: 1
Loss after 785601180 batches: 0.0213
trigger times: 0
Loss after 785732280 batches: 0.0219
trigger times: 1
Loss after 785863380 batches: 0.0214
trigger times: 2
Loss after 785994480 batches: 0.0208
trigger times: 3
Loss after 786125580 batches: 0.0204
trigger times: 0
Loss after 786256680 batches: 0.0211
trigger times: 1
Loss after 786387780 batches: 0.0205
trigger times: 2
Loss after 786518880 batches: 0.0210
trigger times: 0
Loss after 786649980 batches: 0.0219
trigger times: 1
Loss after 786781080 batches: 0.0204
trigger times: 2
Loss after 786912180 batches: 0.0204
trigger times: 3
Loss after 787043280 batches: 0.0195
trigger times: 4
Loss after 787174380 batches: 0.0201
trigger times: 5
Loss after 787305480 batches: 0.0203
trigger times: 6
Loss after 787436580 batches: 0.0209
trigger times: 7
Loss after 787567680 batches: 0.0201
trigger times: 8
Loss after 787698780 batches: 0.0198
trigger times: 9
Loss after 787829880 batches: 0.0196
trigger times: 10
Loss after 787960980 batches: 0.0186
trigger times: 11
Loss after 788092080 batches: 0.0189
trigger times: 12
Loss after 788223180 batches: 0.0191
trigger times: 13
Loss after 788354280 batches: 0.0193
trigger times: 14
Loss after 788485380 batches: 0.0192
trigger times: 15
Loss after 788616480 batches: 0.0194
trigger times: 16
Loss after 788747580 batches: 0.0193
trigger times: 17
Loss after 788878680 batches: 0.0188
trigger times: 18
Loss after 789009780 batches: 0.0190
trigger times: 19
Loss after 789140880 batches: 0.0181
trigger times: 20
Early stopping!
Start to test process.
Loss after 789271980 batches: 0.0191
Time to train on one home:  428.47927808761597
trigger times: 0
Loss after 789403080 batches: 0.0651
trigger times: 0
Loss after 789534180 batches: 0.0199
trigger times: 0
Loss after 789665280 batches: 0.0153
trigger times: 1
Loss after 789796380 batches: 0.0132
trigger times: 2
Loss after 789927480 batches: 0.0123
trigger times: 0
Loss after 790058580 batches: 0.0116
trigger times: 1
Loss after 790189680 batches: 0.0110
trigger times: 2
Loss after 790320780 batches: 0.0106
trigger times: 3
Loss after 790451880 batches: 0.0100
trigger times: 4
Loss after 790582980 batches: 0.0098
trigger times: 5
Loss after 790714080 batches: 0.0097
trigger times: 6
Loss after 790845180 batches: 0.0094
trigger times: 7
Loss after 790976280 batches: 0.0093
trigger times: 8
Loss after 791107380 batches: 0.0092
trigger times: 9
Loss after 791238480 batches: 0.0089
trigger times: 10
Loss after 791369580 batches: 0.0087
trigger times: 11
Loss after 791500680 batches: 0.0085
trigger times: 12
Loss after 791631780 batches: 0.0085
trigger times: 13
Loss after 791762880 batches: 0.0084
trigger times: 14
Loss after 791893980 batches: 0.0082
trigger times: 15
Loss after 792025080 batches: 0.0083
trigger times: 16
Loss after 792156180 batches: 0.0081
trigger times: 17
Loss after 792287280 batches: 0.0081
trigger times: 18
Loss after 792418380 batches: 0.0079
trigger times: 19
Loss after 792549480 batches: 0.0079
trigger times: 20
Early stopping!
Start to test process.
Loss after 792680580 batches: 0.0077
Time to train on one home:  197.52998113632202
trigger times: 0
Loss after 792759180 batches: 0.1986
trigger times: 0
Loss after 792837780 batches: 0.0547
trigger times: 1
Loss after 792916380 batches: 0.0361
trigger times: 0
Loss after 792994980 batches: 0.0289
trigger times: 0
Loss after 793073580 batches: 0.0265
trigger times: 0
Loss after 793152180 batches: 0.0240
trigger times: 1
Loss after 793230780 batches: 0.0227
trigger times: 0
Loss after 793309380 batches: 0.0221
trigger times: 1
Loss after 793387980 batches: 0.0213
trigger times: 2
Loss after 793466580 batches: 0.0209
trigger times: 3
Loss after 793545180 batches: 0.0201
trigger times: 0
Loss after 793623780 batches: 0.0192
trigger times: 1
Loss after 793702380 batches: 0.0187
trigger times: 2
Loss after 793780980 batches: 0.0184
trigger times: 3
Loss after 793859580 batches: 0.0182
trigger times: 4
Loss after 793938180 batches: 0.0181
trigger times: 5
Loss after 794016780 batches: 0.0179
trigger times: 6
Loss after 794095380 batches: 0.0177
trigger times: 7
Loss after 794173980 batches: 0.0169
trigger times: 8
Loss after 794252580 batches: 0.0168
trigger times: 9
Loss after 794331180 batches: 0.0164
trigger times: 10
Loss after 794409780 batches: 0.0161
trigger times: 11
Loss after 794488380 batches: 0.0157
trigger times: 12
Loss after 794566980 batches: 0.0158
trigger times: 13
Loss after 794645580 batches: 0.0158
trigger times: 14
Loss after 794724180 batches: 0.0152
trigger times: 15
Loss after 794802780 batches: 0.0156
trigger times: 16
Loss after 794881380 batches: 0.0154
trigger times: 17
Loss after 794959980 batches: 0.0156
trigger times: 18
Loss after 795038580 batches: 0.0150
trigger times: 19
Loss after 795117180 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 795195780 batches: 0.0149
Time to train on one home:  159.21948623657227
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097, 0.016851266692967333, 0.01539797175087967, 0.015622467135075917, 0.014339386766728896, 0.015215330347522192, 0.014610499733001695, 0.014354899421970126]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983], [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013], [0.5549709763791826, 0.4001270641513627, 0.48658498833859326, 1.0394903420759227, 0.4914126622529051, 24.558331909615507, 1517.0215], [0.5461304287115732, 0.4097276108845518, 0.4876006728779417, 1.0136453079994863, 0.48354794633174836, 23.94773371608454, 1492.7424], [0.5402040051089393, 0.41609081728992114, 0.4927051878338842, 1.0002093606660662, 0.47833524208513384, 23.630304644569275, 1476.6505], [0.5334714584880405, 0.4234695591708837, 0.501786021212506, 1.0005226338786515, 0.47229061667346856, 23.637705836501173, 1457.9905], [0.5237383676899804, 0.43399432408412464, 0.5083585548121925, 0.9778479298034527, 0.46366878622151636, 23.10200782557115, 1431.3744], [0.5306309130456712, 0.42652222327915057, 0.5055172115402072, 1.0003700248842091, 0.46978989075842903, 23.63410039430878, 1450.2706]]
Round_14_results:  [0.5306309130456712, 0.42652222327915057, 0.5055172115402072, 1.0003700248842091, 0.46978989075842903, 23.63410039430878, 1450.2706]
trigger times: 0
Loss after 795326880 batches: 0.1189
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 6455 < 6456; dropping {'Training_Loss': 0.11889696177446617, 'Validation_Loss': 0.18472710831297767, 'Training_R2': 0.880227670424536, 'Validation_R2': 0.8284987135234417, 'Training_F1': 0.8184043346267025, 'Validation_F1': 0.7442170419223534, 'Training_NEP': 0.36303565324109505, 'Validation_NEP': 0.4853294987485581, 'Training_NDE': 0.08991521377444534, 'Validation_NDE': 0.13657458401232792, 'Training_MAE': 12.02302085800811, 'Validation_MAE': 13.309748364434874, 'Training_MSE': 395.61285, 'Validation_MSE': 504.36612}.
trigger times: 0
Loss after 795457980 batches: 0.0286
trigger times: 0
Loss after 795589080 batches: 0.0207
trigger times: 1
Loss after 795720180 batches: 0.0180
trigger times: 2
Loss after 795851280 batches: 0.0164
trigger times: 3
Loss after 795982380 batches: 0.0149
trigger times: 4
Loss after 796113480 batches: 0.0138
trigger times: 5
Loss after 796244580 batches: 0.0136
trigger times: 6
Loss after 796375680 batches: 0.0132
trigger times: 7
Loss after 796506780 batches: 0.0125
trigger times: 8
Loss after 796637880 batches: 0.0126
trigger times: 0
Loss after 796768980 batches: 0.0119
trigger times: 1
Loss after 796900080 batches: 0.0118
trigger times: 0
Loss after 797031180 batches: 0.0117
trigger times: 1
Loss after 797162280 batches: 0.0115
trigger times: 0
Loss after 797293380 batches: 0.0107
trigger times: 1
Loss after 797424480 batches: 0.0108
trigger times: 2
Loss after 797555580 batches: 0.0107
trigger times: 3
Loss after 797686680 batches: 0.0106
trigger times: 4
Loss after 797817780 batches: 0.0104
trigger times: 5
Loss after 797948880 batches: 0.0102
trigger times: 6
Loss after 798079980 batches: 0.0101
trigger times: 7
Loss after 798211080 batches: 0.0101
trigger times: 8
Loss after 798342180 batches: 0.0102
trigger times: 9
Loss after 798473280 batches: 0.0097
trigger times: 10
Loss after 798604380 batches: 0.0098
trigger times: 11
Loss after 798735480 batches: 0.0096
trigger times: 12
Loss after 798866580 batches: 0.0095
trigger times: 13
Loss after 798997680 batches: 0.0093
trigger times: 14
Loss after 799128780 batches: 0.0093
trigger times: 15
Loss after 799259880 batches: 0.0091
trigger times: 16
Loss after 799390980 batches: 0.0092
trigger times: 17
Loss after 799522080 batches: 0.0091
trigger times: 18
Loss after 799653180 batches: 0.0091
trigger times: 19
Loss after 799784280 batches: 0.0090
trigger times: 20
Early stopping!
Start to test process.
Loss after 799915380 batches: 0.0089
Time to train on one home:  269.4832561016083
trigger times: 0
Loss after 800017980 batches: 0.2587
trigger times: 0
Loss after 800120580 batches: 0.0809
trigger times: 1
Loss after 800223180 batches: 0.0597
trigger times: 0
Loss after 800325780 batches: 0.0530
trigger times: 1
Loss after 800428380 batches: 0.0393
trigger times: 0
Loss after 800530980 batches: 0.0359
trigger times: 1
Loss after 800633580 batches: 0.0328
trigger times: 2
Loss after 800736180 batches: 0.0300
trigger times: 3
Loss after 800838780 batches: 0.0282
trigger times: 4
Loss after 800941380 batches: 0.0280
trigger times: 5
Loss after 801043980 batches: 0.0272
trigger times: 6
Loss after 801146580 batches: 0.0269
trigger times: 0
Loss after 801249180 batches: 0.0256
trigger times: 1
Loss after 801351780 batches: 0.0267
trigger times: 2
Loss after 801454380 batches: 0.0248
trigger times: 3
Loss after 801556980 batches: 0.0236
trigger times: 4
Loss after 801659580 batches: 0.0230
trigger times: 5
Loss after 801762180 batches: 0.0227
trigger times: 6
Loss after 801864780 batches: 0.0224
trigger times: 7
Loss after 801967380 batches: 0.0276
trigger times: 8
Loss after 802069980 batches: 0.0230
trigger times: 9
Loss after 802172580 batches: 0.0225
trigger times: 10
Loss after 802275180 batches: 0.0221
trigger times: 11
Loss after 802377780 batches: 0.0210
trigger times: 0
Loss after 802480380 batches: 0.0218
trigger times: 1
Loss after 802582980 batches: 0.0207
trigger times: 2
Loss after 802685580 batches: 0.0197
trigger times: 3
Loss after 802788180 batches: 0.0195
trigger times: 4
Loss after 802890780 batches: 0.0193
trigger times: 0
Loss after 802993380 batches: 0.0191
trigger times: 0
Loss after 803095980 batches: 0.0194
trigger times: 1
Loss after 803198580 batches: 0.0190
trigger times: 2
Loss after 803301180 batches: 0.0190
trigger times: 3
Loss after 803403780 batches: 0.0199
trigger times: 4
Loss after 803506380 batches: 0.0201
trigger times: 5
Loss after 803608980 batches: 0.0201
trigger times: 6
Loss after 803711580 batches: 0.0190
trigger times: 7
Loss after 803814180 batches: 0.0200
trigger times: 0
Loss after 803916780 batches: 0.0226
trigger times: 1
Loss after 804019380 batches: 0.0189
trigger times: 0
Loss after 804121980 batches: 0.0195
trigger times: 1
Loss after 804224580 batches: 0.0190
trigger times: 0
Loss after 804327180 batches: 0.0180
trigger times: 1
Loss after 804429780 batches: 0.0177
trigger times: 2
Loss after 804532380 batches: 0.0173
trigger times: 3
Loss after 804634980 batches: 0.0176
trigger times: 4
Loss after 804737580 batches: 0.0171
trigger times: 5
Loss after 804840180 batches: 0.0183
trigger times: 6
Loss after 804942780 batches: 0.0180
trigger times: 7
Loss after 805045380 batches: 0.0183
trigger times: 8
Loss after 805147980 batches: 0.0169
trigger times: 9
Loss after 805250580 batches: 0.0166
trigger times: 10
Loss after 805353180 batches: 0.0166
trigger times: 11
Loss after 805455780 batches: 0.0166
trigger times: 12
Loss after 805558380 batches: 0.0165
trigger times: 13
Loss after 805660980 batches: 0.0164
trigger times: 14
Loss after 805763580 batches: 0.0171
trigger times: 15
Loss after 805866180 batches: 0.0168
trigger times: 16
Loss after 805968780 batches: 0.0160
trigger times: 17
Loss after 806071380 batches: 0.0166
trigger times: 18
Loss after 806173980 batches: 0.0193
trigger times: 19
Loss after 806276580 batches: 0.0173
trigger times: 20
Early stopping!
Start to test process.
Loss after 806379180 batches: 0.0169
Time to train on one home:  373.4974000453949
trigger times: 0
Loss after 806510280 batches: 0.1714
trigger times: 0
Loss after 806641380 batches: 0.0509
trigger times: 1
Loss after 806772480 batches: 0.0360
trigger times: 2
Loss after 806903580 batches: 0.0313
trigger times: 0
Loss after 807034680 batches: 0.0280
trigger times: 1
Loss after 807165780 batches: 0.0261
trigger times: 2
Loss after 807296880 batches: 0.0244
trigger times: 3
Loss after 807427980 batches: 0.0230
trigger times: 4
Loss after 807559080 batches: 0.0226
trigger times: 0
Loss after 807690180 batches: 0.0216
trigger times: 1
Loss after 807821280 batches: 0.0208
trigger times: 0
Loss after 807952380 batches: 0.0202
trigger times: 1
Loss after 808083480 batches: 0.0199
trigger times: 2
Loss after 808214580 batches: 0.0199
trigger times: 0
Loss after 808345680 batches: 0.0191
trigger times: 1
Loss after 808476780 batches: 0.0187
trigger times: 2
Loss after 808607880 batches: 0.0181
trigger times: 3
Loss after 808738980 batches: 0.0179
trigger times: 4
Loss after 808870080 batches: 0.0177
trigger times: 5
Loss after 809001180 batches: 0.0175
trigger times: 6
Loss after 809132280 batches: 0.0172
trigger times: 7
Loss after 809263380 batches: 0.0169
trigger times: 8
Loss after 809394480 batches: 0.0165
trigger times: 9
Loss after 809525580 batches: 0.0164
trigger times: 10
Loss after 809656680 batches: 0.0161
trigger times: 11
Loss after 809787780 batches: 0.0162
trigger times: 12
Loss after 809918880 batches: 0.0162
trigger times: 13
Loss after 810049980 batches: 0.0159
trigger times: 14
Loss after 810181080 batches: 0.0158
trigger times: 15
Loss after 810312180 batches: 0.0156
trigger times: 16
Loss after 810443280 batches: 0.0155
trigger times: 17
Loss after 810574380 batches: 0.0153
trigger times: 18
Loss after 810705480 batches: 0.0152
trigger times: 19
Loss after 810836580 batches: 0.0150
trigger times: 20
Early stopping!
Start to test process.
Loss after 810967680 batches: 0.0151
Time to train on one home:  262.4583864212036
trigger times: 0
Loss after 811098780 batches: 0.1670
trigger times: 0
Loss after 811229880 batches: 0.0492
trigger times: 1
Loss after 811360980 batches: 0.0368
trigger times: 2
Loss after 811492080 batches: 0.0323
trigger times: 3
Loss after 811623180 batches: 0.0293
trigger times: 4
Loss after 811754280 batches: 0.0275
trigger times: 5
Loss after 811885380 batches: 0.0266
trigger times: 6
Loss after 812016480 batches: 0.0254
trigger times: 0
Loss after 812147580 batches: 0.0244
trigger times: 1
Loss after 812278680 batches: 0.0239
trigger times: 2
Loss after 812409780 batches: 0.0231
trigger times: 3
Loss after 812540880 batches: 0.0227
trigger times: 0
Loss after 812671980 batches: 0.0225
trigger times: 1
Loss after 812803080 batches: 0.0222
trigger times: 0
Loss after 812934180 batches: 0.0214
trigger times: 1
Loss after 813065280 batches: 0.0212
trigger times: 0
Loss after 813196380 batches: 0.0211
trigger times: 1
Loss after 813327480 batches: 0.0207
trigger times: 2
Loss after 813458580 batches: 0.0203
trigger times: 3
Loss after 813589680 batches: 0.0201
trigger times: 0
Loss after 813720780 batches: 0.0200
trigger times: 1
Loss after 813851880 batches: 0.0197
trigger times: 0
Loss after 813982980 batches: 0.0193
trigger times: 0
Loss after 814114080 batches: 0.0194
trigger times: 1
Loss after 814245180 batches: 0.0193
trigger times: 2
Loss after 814376280 batches: 0.0189
trigger times: 3
Loss after 814507380 batches: 0.0192
trigger times: 4
Loss after 814638480 batches: 0.0190
trigger times: 5
Loss after 814769580 batches: 0.0188
trigger times: 6
Loss after 814900680 batches: 0.0187
trigger times: 7
Loss after 815031780 batches: 0.0184
trigger times: 8
Loss after 815162880 batches: 0.0183
trigger times: 9
Loss after 815293980 batches: 0.0183
trigger times: 10
Loss after 815425080 batches: 0.0181
trigger times: 11
Loss after 815556180 batches: 0.0183
trigger times: 12
Loss after 815687280 batches: 0.0179
trigger times: 13
Loss after 815818380 batches: 0.0178
trigger times: 14
Loss after 815949480 batches: 0.0178
trigger times: 0
Loss after 816080580 batches: 0.0179
trigger times: 1
Loss after 816211680 batches: 0.0178
trigger times: 2
Loss after 816342780 batches: 0.0173
trigger times: 3
Loss after 816473880 batches: 0.0173
trigger times: 4
Loss after 816604980 batches: 0.0175
trigger times: 5
Loss after 816736080 batches: 0.0173
trigger times: 6
Loss after 816867180 batches: 0.0169
trigger times: 7
Loss after 816998280 batches: 0.0172
trigger times: 8
Loss after 817129380 batches: 0.0170
trigger times: 9
Loss after 817260480 batches: 0.0169
trigger times: 10
Loss after 817391580 batches: 0.0168
trigger times: 11
Loss after 817522680 batches: 0.0166
trigger times: 12
Loss after 817653780 batches: 0.0166
trigger times: 13
Loss after 817784880 batches: 0.0166
trigger times: 14
Loss after 817915980 batches: 0.0163
trigger times: 15
Loss after 818047080 batches: 0.0166
trigger times: 16
Loss after 818178180 batches: 0.0163
trigger times: 17
Loss after 818309280 batches: 0.0164
trigger times: 0
Loss after 818440380 batches: 0.0164
trigger times: 1
Loss after 818571480 batches: 0.0159
trigger times: 2
Loss after 818702580 batches: 0.0163
trigger times: 3
Loss after 818833680 batches: 0.0160
trigger times: 4
Loss after 818964780 batches: 0.0160
trigger times: 5
Loss after 819095880 batches: 0.0160
trigger times: 6
Loss after 819226980 batches: 0.0160
trigger times: 7
Loss after 819358080 batches: 0.0159
trigger times: 8
Loss after 819489180 batches: 0.0155
trigger times: 9
Loss after 819620280 batches: 0.0155
trigger times: 10
Loss after 819751380 batches: 0.0156
trigger times: 11
Loss after 819882480 batches: 0.0156
trigger times: 12
Loss after 820013580 batches: 0.0156
trigger times: 13
Loss after 820144680 batches: 0.0158
trigger times: 14
Loss after 820275780 batches: 0.0156
trigger times: 15
Loss after 820406880 batches: 0.0154
trigger times: 16
Loss after 820537980 batches: 0.0155
trigger times: 17
Loss after 820669080 batches: 0.0152
trigger times: 18
Loss after 820800180 batches: 0.0154
trigger times: 0
Loss after 820931280 batches: 0.0154
trigger times: 1
Loss after 821062380 batches: 0.0155
trigger times: 2
Loss after 821193480 batches: 0.0151
trigger times: 3
Loss after 821324580 batches: 0.0153
trigger times: 4
Loss after 821455680 batches: 0.0153
trigger times: 5
Loss after 821586780 batches: 0.0152
trigger times: 6
Loss after 821717880 batches: 0.0150
trigger times: 7
Loss after 821848980 batches: 0.0148
trigger times: 8
Loss after 821980080 batches: 0.0149
trigger times: 9
Loss after 822111180 batches: 0.0148
trigger times: 10
Loss after 822242280 batches: 0.0149
trigger times: 11
Loss after 822373380 batches: 0.0150
trigger times: 12
Loss after 822504480 batches: 0.0151
trigger times: 13
Loss after 822635580 batches: 0.0148
trigger times: 14
Loss after 822766680 batches: 0.0146
trigger times: 15
Loss after 822897780 batches: 0.0148
trigger times: 16
Loss after 823028880 batches: 0.0145
trigger times: 17
Loss after 823159980 batches: 0.0149
trigger times: 18
Loss after 823291080 batches: 0.0149
trigger times: 19
Loss after 823422180 batches: 0.0148
trigger times: 20
Early stopping!
Start to test process.
Loss after 823553280 batches: 0.0148
Time to train on one home:  698.0021326541901
trigger times: 0
Loss after 823681920 batches: 0.1083
trigger times: 0
Loss after 823810560 batches: 0.0319
trigger times: 0
Loss after 823939200 batches: 0.0239
trigger times: 1
Loss after 824067840 batches: 0.0203
trigger times: 2
Loss after 824196480 batches: 0.0185
trigger times: 3
Loss after 824325120 batches: 0.0173
trigger times: 4
Loss after 824453760 batches: 0.0166
trigger times: 5
Loss after 824582400 batches: 0.0158
trigger times: 6
Loss after 824711040 batches: 0.0151
trigger times: 7
Loss after 824839680 batches: 0.0148
trigger times: 8
Loss after 824968320 batches: 0.0143
trigger times: 9
Loss after 825096960 batches: 0.0139
trigger times: 10
Loss after 825225600 batches: 0.0138
trigger times: 11
Loss after 825354240 batches: 0.0137
trigger times: 12
Loss after 825482880 batches: 0.0131
trigger times: 13
Loss after 825611520 batches: 0.0130
trigger times: 14
Loss after 825740160 batches: 0.0129
trigger times: 15
Loss after 825868800 batches: 0.0128
trigger times: 16
Loss after 825997440 batches: 0.0128
trigger times: 17
Loss after 826126080 batches: 0.0127
trigger times: 18
Loss after 826254720 batches: 0.0122
trigger times: 19
Loss after 826383360 batches: 0.0121
trigger times: 20
Early stopping!
Start to test process.
Loss after 826512000 batches: 0.0119
Time to train on one home:  172.92507648468018
trigger times: 0
Loss after 826643100 batches: 0.1948
trigger times: 0
Loss after 826774200 batches: 0.0497
trigger times: 0
Loss after 826905300 batches: 0.0358
trigger times: 1
Loss after 827036400 batches: 0.0313
trigger times: 0
Loss after 827167500 batches: 0.0295
trigger times: 0
Loss after 827298600 batches: 0.0277
trigger times: 1
Loss after 827429700 batches: 0.0263
trigger times: 0
Loss after 827560800 batches: 0.0250
trigger times: 1
Loss after 827691900 batches: 0.0241
trigger times: 2
Loss after 827823000 batches: 0.0233
trigger times: 3
Loss after 827954100 batches: 0.0229
trigger times: 4
Loss after 828085200 batches: 0.0226
trigger times: 0
Loss after 828216300 batches: 0.0220
trigger times: 1
Loss after 828347400 batches: 0.0219
trigger times: 0
Loss after 828478500 batches: 0.0214
trigger times: 1
Loss after 828609600 batches: 0.0210
trigger times: 0
Loss after 828740700 batches: 0.0206
trigger times: 1
Loss after 828871800 batches: 0.0203
trigger times: 2
Loss after 829002900 batches: 0.0203
trigger times: 0
Loss after 829134000 batches: 0.0198
trigger times: 1
Loss after 829265100 batches: 0.0197
trigger times: 2
Loss after 829396200 batches: 0.0194
trigger times: 3
Loss after 829527300 batches: 0.0191
trigger times: 4
Loss after 829658400 batches: 0.0190
trigger times: 5
Loss after 829789500 batches: 0.0189
trigger times: 6
Loss after 829920600 batches: 0.0188
trigger times: 7
Loss after 830051700 batches: 0.0186
trigger times: 8
Loss after 830182800 batches: 0.0184
trigger times: 0
Loss after 830313900 batches: 0.0185
trigger times: 1
Loss after 830445000 batches: 0.0182
trigger times: 2
Loss after 830576100 batches: 0.0180
trigger times: 3
Loss after 830707200 batches: 0.0178
trigger times: 4
Loss after 830838300 batches: 0.0177
trigger times: 5
Loss after 830969400 batches: 0.0177
trigger times: 6
Loss after 831100500 batches: 0.0177
trigger times: 7
Loss after 831231600 batches: 0.0174
trigger times: 8
Loss after 831362700 batches: 0.0174
trigger times: 0
Loss after 831493800 batches: 0.0174
trigger times: 1
Loss after 831624900 batches: 0.0175
trigger times: 2
Loss after 831756000 batches: 0.0170
trigger times: 0
Loss after 831887100 batches: 0.0171
trigger times: 1
Loss after 832018200 batches: 0.0172
trigger times: 2
Loss after 832149300 batches: 0.0169
trigger times: 3
Loss after 832280400 batches: 0.0167
trigger times: 4
Loss after 832411500 batches: 0.0168
trigger times: 5
Loss after 832542600 batches: 0.0169
trigger times: 6
Loss after 832673700 batches: 0.0166
trigger times: 7
Loss after 832804800 batches: 0.0165
trigger times: 8
Loss after 832935900 batches: 0.0165
trigger times: 9
Loss after 833067000 batches: 0.0163
trigger times: 10
Loss after 833198100 batches: 0.0164
trigger times: 11
Loss after 833329200 batches: 0.0163
trigger times: 12
Loss after 833460300 batches: 0.0162
trigger times: 13
Loss after 833591400 batches: 0.0162
trigger times: 14
Loss after 833722500 batches: 0.0162
trigger times: 15
Loss after 833853600 batches: 0.0159
trigger times: 16
Loss after 833984700 batches: 0.0160
trigger times: 17
Loss after 834115800 batches: 0.0158
trigger times: 18
Loss after 834246900 batches: 0.0161
trigger times: 0
Loss after 834378000 batches: 0.0157
trigger times: 1
Loss after 834509100 batches: 0.0158
trigger times: 2
Loss after 834640200 batches: 0.0157
trigger times: 3
Loss after 834771300 batches: 0.0156
trigger times: 0
Loss after 834902400 batches: 0.0157
trigger times: 1
Loss after 835033500 batches: 0.0158
trigger times: 0
Loss after 835164600 batches: 0.0155
trigger times: 1
Loss after 835295700 batches: 0.0155
trigger times: 2
Loss after 835426800 batches: 0.0153
trigger times: 3
Loss after 835557900 batches: 0.0153
trigger times: 4
Loss after 835689000 batches: 0.0153
trigger times: 5
Loss after 835820100 batches: 0.0153
trigger times: 6
Loss after 835951200 batches: 0.0154
trigger times: 7
Loss after 836082300 batches: 0.0151
trigger times: 0
Loss after 836213400 batches: 0.0150
trigger times: 1
Loss after 836344500 batches: 0.0151
trigger times: 0
Loss after 836475600 batches: 0.0149
trigger times: 1
Loss after 836606700 batches: 0.0150
trigger times: 2
Loss after 836737800 batches: 0.0148
trigger times: 3
Loss after 836868900 batches: 0.0150
trigger times: 4
Loss after 837000000 batches: 0.0150
trigger times: 5
Loss after 837131100 batches: 0.0147
trigger times: 6
Loss after 837262200 batches: 0.0149
trigger times: 0
Loss after 837393300 batches: 0.0148
trigger times: 1
Loss after 837524400 batches: 0.0145
trigger times: 2
Loss after 837655500 batches: 0.0147
trigger times: 3
Loss after 837786600 batches: 0.0148
trigger times: 4
Loss after 837917700 batches: 0.0147
trigger times: 5
Loss after 838048800 batches: 0.0146
trigger times: 6
Loss after 838179900 batches: 0.0146
trigger times: 7
Loss after 838311000 batches: 0.0145
trigger times: 0
Loss after 838442100 batches: 0.0147
trigger times: 0
Loss after 838573200 batches: 0.0145
trigger times: 1
Loss after 838704300 batches: 0.0146
trigger times: 2
Loss after 838835400 batches: 0.0145
trigger times: 3
Loss after 838966500 batches: 0.0144
trigger times: 4
Loss after 839097600 batches: 0.0144
trigger times: 5
Loss after 839228700 batches: 0.0143
trigger times: 6
Loss after 839359800 batches: 0.0147
trigger times: 7
Loss after 839490900 batches: 0.0142
trigger times: 8
Loss after 839622000 batches: 0.0142
trigger times: 9
Loss after 839753100 batches: 0.0141
trigger times: 10
Loss after 839884200 batches: 0.0143
trigger times: 11
Loss after 840015300 batches: 0.0140
trigger times: 12
Loss after 840146400 batches: 0.0140
trigger times: 13
Loss after 840277500 batches: 0.0141
trigger times: 14
Loss after 840408600 batches: 0.0139
trigger times: 15
Loss after 840539700 batches: 0.0141
trigger times: 16
Loss after 840670800 batches: 0.0140
trigger times: 17
Loss after 840801900 batches: 0.0142
trigger times: 18
Loss after 840933000 batches: 0.0139
trigger times: 19
Loss after 841064100 batches: 0.0140
trigger times: 20
Early stopping!
Start to test process.
Loss after 841195200 batches: 0.0140
Time to train on one home:  812.6826586723328
trigger times: 0
Loss after 841326300 batches: 0.1921
trigger times: 0
Loss after 841457400 batches: 0.0704
trigger times: 0
Loss after 841588500 batches: 0.0502
trigger times: 1
Loss after 841719600 batches: 0.0403
trigger times: 2
Loss after 841850700 batches: 0.0333
trigger times: 0
Loss after 841981800 batches: 0.0311
trigger times: 1
Loss after 842112900 batches: 0.0295
trigger times: 0
Loss after 842244000 batches: 0.0285
trigger times: 1
Loss after 842375100 batches: 0.0277
trigger times: 0
Loss after 842506200 batches: 0.0274
trigger times: 1
Loss after 842637300 batches: 0.0259
trigger times: 2
Loss after 842768400 batches: 0.0251
trigger times: 0
Loss after 842899500 batches: 0.0252
trigger times: 0
Loss after 843030600 batches: 0.0240
trigger times: 1
Loss after 843161700 batches: 0.0248
trigger times: 0
Loss after 843292800 batches: 0.0240
trigger times: 1
Loss after 843423900 batches: 0.0238
trigger times: 2
Loss after 843555000 batches: 0.0235
trigger times: 3
Loss after 843686100 batches: 0.0233
trigger times: 0
Loss after 843817200 batches: 0.0228
trigger times: 1
Loss after 843948300 batches: 0.0221
trigger times: 0
Loss after 844079400 batches: 0.0227
trigger times: 1
Loss after 844210500 batches: 0.0226
trigger times: 2
Loss after 844341600 batches: 0.0226
trigger times: 3
Loss after 844472700 batches: 0.0226
trigger times: 4
Loss after 844603800 batches: 0.0232
trigger times: 0
Loss after 844734900 batches: 0.0227
trigger times: 1
Loss after 844866000 batches: 0.0219
trigger times: 0
Loss after 844997100 batches: 0.0213
trigger times: 0
Loss after 845128200 batches: 0.0220
trigger times: 1
Loss after 845259300 batches: 0.0211
trigger times: 0
Loss after 845390400 batches: 0.0203
trigger times: 1
Loss after 845521500 batches: 0.0205
trigger times: 2
Loss after 845652600 batches: 0.0210
trigger times: 0
Loss after 845783700 batches: 0.0205
trigger times: 1
Loss after 845914800 batches: 0.0199
trigger times: 2
Loss after 846045900 batches: 0.0200
trigger times: 3
Loss after 846177000 batches: 0.0202
trigger times: 4
Loss after 846308100 batches: 0.0200
trigger times: 5
Loss after 846439200 batches: 0.0195
trigger times: 6
Loss after 846570300 batches: 0.0205
trigger times: 7
Loss after 846701400 batches: 0.0195
trigger times: 8
Loss after 846832500 batches: 0.0198
trigger times: 9
Loss after 846963600 batches: 0.0211
trigger times: 10
Loss after 847094700 batches: 0.0192
trigger times: 11
Loss after 847225800 batches: 0.0197
trigger times: 12
Loss after 847356900 batches: 0.0194
trigger times: 13
Loss after 847488000 batches: 0.0203
trigger times: 14
Loss after 847619100 batches: 0.0207
trigger times: 15
Loss after 847750200 batches: 0.0190
trigger times: 0
Loss after 847881300 batches: 0.0191
trigger times: 1
Loss after 848012400 batches: 0.0191
trigger times: 2
Loss after 848143500 batches: 0.0195
trigger times: 3
Loss after 848274600 batches: 0.0187
trigger times: 4
Loss after 848405700 batches: 0.0192
trigger times: 0
Loss after 848536800 batches: 0.0190
trigger times: 0
Loss after 848667900 batches: 0.0183
trigger times: 1
Loss after 848799000 batches: 0.0178
trigger times: 0
Loss after 848930100 batches: 0.0179
trigger times: 1
Loss after 849061200 batches: 0.0182
trigger times: 2
Loss after 849192300 batches: 0.0183
trigger times: 3
Loss after 849323400 batches: 0.0178
trigger times: 4
Loss after 849454500 batches: 0.0184
trigger times: 5
Loss after 849585600 batches: 0.0182
trigger times: 6
Loss after 849716700 batches: 0.0188
trigger times: 0
Loss after 849847800 batches: 0.0187
trigger times: 1
Loss after 849978900 batches: 0.0173
trigger times: 2
Loss after 850110000 batches: 0.0179
trigger times: 3
Loss after 850241100 batches: 0.0177
trigger times: 4
Loss after 850372200 batches: 0.0180
trigger times: 5
Loss after 850503300 batches: 0.0174
trigger times: 6
Loss after 850634400 batches: 0.0170
trigger times: 7
Loss after 850765500 batches: 0.0169
trigger times: 8
Loss after 850896600 batches: 0.0174
trigger times: 9
Loss after 851027700 batches: 0.0177
trigger times: 10
Loss after 851158800 batches: 0.0174
trigger times: 11
Loss after 851289900 batches: 0.0177
trigger times: 12
Loss after 851421000 batches: 0.0170
trigger times: 13
Loss after 851552100 batches: 0.0178
trigger times: 14
Loss after 851683200 batches: 0.0171
trigger times: 15
Loss after 851814300 batches: 0.0169
trigger times: 16
Loss after 851945400 batches: 0.0167
trigger times: 17
Loss after 852076500 batches: 0.0171
trigger times: 18
Loss after 852207600 batches: 0.0164
trigger times: 19
Loss after 852338700 batches: 0.0181
trigger times: 20
Early stopping!
Start to test process.
Loss after 852469800 batches: 0.0176
Time to train on one home:  627.5518927574158
trigger times: 0
Loss after 852600900 batches: 0.0648
trigger times: 0
Loss after 852732000 batches: 0.0197
trigger times: 1
Loss after 852863100 batches: 0.0148
trigger times: 2
Loss after 852994200 batches: 0.0134
trigger times: 3
Loss after 853125300 batches: 0.0121
trigger times: 4
Loss after 853256400 batches: 0.0115
trigger times: 5
Loss after 853387500 batches: 0.0109
trigger times: 6
Loss after 853518600 batches: 0.0105
trigger times: 7
Loss after 853649700 batches: 0.0101
trigger times: 8
Loss after 853780800 batches: 0.0098
trigger times: 9
Loss after 853911900 batches: 0.0094
trigger times: 10
Loss after 854043000 batches: 0.0095
trigger times: 11
Loss after 854174100 batches: 0.0093
trigger times: 12
Loss after 854305200 batches: 0.0089
trigger times: 13
Loss after 854436300 batches: 0.0088
trigger times: 14
Loss after 854567400 batches: 0.0087
trigger times: 15
Loss after 854698500 batches: 0.0088
trigger times: 16
Loss after 854829600 batches: 0.0084
trigger times: 17
Loss after 854960700 batches: 0.0083
trigger times: 18
Loss after 855091800 batches: 0.0083
trigger times: 19
Loss after 855222900 batches: 0.0081
trigger times: 20
Early stopping!
Start to test process.
Loss after 855354000 batches: 0.0080
Time to train on one home:  168.5312955379486
trigger times: 0
Loss after 855432600 batches: 0.2043
trigger times: 0
Loss after 855511200 batches: 0.0511
trigger times: 0
Loss after 855589800 batches: 0.0346
trigger times: 0
Loss after 855668400 batches: 0.0284
trigger times: 1
Loss after 855747000 batches: 0.0257
trigger times: 2
Loss after 855825600 batches: 0.0244
trigger times: 3
Loss after 855904200 batches: 0.0229
trigger times: 4
Loss after 855982800 batches: 0.0228
trigger times: 5
Loss after 856061400 batches: 0.0210
trigger times: 0
Loss after 856140000 batches: 0.0202
trigger times: 0
Loss after 856218600 batches: 0.0201
trigger times: 1
Loss after 856297200 batches: 0.0202
trigger times: 2
Loss after 856375800 batches: 0.0188
trigger times: 3
Loss after 856454400 batches: 0.0185
trigger times: 4
Loss after 856533000 batches: 0.0181
trigger times: 5
Loss after 856611600 batches: 0.0177
trigger times: 6
Loss after 856690200 batches: 0.0177
trigger times: 7
Loss after 856768800 batches: 0.0173
trigger times: 8
Loss after 856847400 batches: 0.0169
trigger times: 9
Loss after 856926000 batches: 0.0170
trigger times: 10
Loss after 857004600 batches: 0.0163
trigger times: 11
Loss after 857083200 batches: 0.0163
trigger times: 12
Loss after 857161800 batches: 0.0165
trigger times: 0
Loss after 857240400 batches: 0.0158
trigger times: 0
Loss after 857319000 batches: 0.0157
trigger times: 1
Loss after 857397600 batches: 0.0159
trigger times: 2
Loss after 857476200 batches: 0.0155
trigger times: 3
Loss after 857554800 batches: 0.0153
trigger times: 0
Loss after 857633400 batches: 0.0157
trigger times: 1
Loss after 857712000 batches: 0.0148
trigger times: 2
Loss after 857790600 batches: 0.0150
trigger times: 0
Loss after 857869200 batches: 0.0145
trigger times: 1
Loss after 857947800 batches: 0.0147
trigger times: 2
Loss after 858026400 batches: 0.0151
trigger times: 3
Loss after 858105000 batches: 0.0149
trigger times: 4
Loss after 858183600 batches: 0.0143
trigger times: 5
Loss after 858262200 batches: 0.0141
trigger times: 6
Loss after 858340800 batches: 0.0143
trigger times: 7
Loss after 858419400 batches: 0.0141
trigger times: 8
Loss after 858498000 batches: 0.0140
trigger times: 9
Loss after 858576600 batches: 0.0139
trigger times: 0
Loss after 858655200 batches: 0.0142
trigger times: 1
Loss after 858733800 batches: 0.0143
trigger times: 2
Loss after 858812400 batches: 0.0139
trigger times: 3
Loss after 858891000 batches: 0.0136
trigger times: 4
Loss after 858969600 batches: 0.0136
trigger times: 5
Loss after 859048200 batches: 0.0137
trigger times: 6
Loss after 859126800 batches: 0.0132
trigger times: 7
Loss after 859205400 batches: 0.0137
trigger times: 8
Loss after 859284000 batches: 0.0139
trigger times: 9
Loss after 859362600 batches: 0.0134
trigger times: 10
Loss after 859441200 batches: 0.0128
trigger times: 11
Loss after 859519800 batches: 0.0128
trigger times: 12
Loss after 859598400 batches: 0.0132
trigger times: 13
Loss after 859677000 batches: 0.0132
trigger times: 14
Loss after 859755600 batches: 0.0129
trigger times: 15
Loss after 859834200 batches: 0.0129
trigger times: 16
Loss after 859912800 batches: 0.0124
trigger times: 17
Loss after 859991400 batches: 0.0131
trigger times: 18
Loss after 860070000 batches: 0.0126
trigger times: 19
Loss after 860148600 batches: 0.0128
trigger times: 20
Early stopping!
Start to test process.
Loss after 860227200 batches: 0.0128
Time to train on one home:  297.0069830417633
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097, 0.016851266692967333, 0.01539797175087967, 0.015622467135075917, 0.014339386766728896, 0.015215330347522192, 0.014610499733001695, 0.014354899421970126, 0.01333385277873801]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983], [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013], [0.5549709763791826, 0.4001270641513627, 0.48658498833859326, 1.0394903420759227, 0.4914126622529051, 24.558331909615507, 1517.0215], [0.5461304287115732, 0.4097276108845518, 0.4876006728779417, 1.0136453079994863, 0.48354794633174836, 23.94773371608454, 1492.7424], [0.5402040051089393, 0.41609081728992114, 0.4927051878338842, 1.0002093606660662, 0.47833524208513384, 23.630304644569275, 1476.6505], [0.5334714584880405, 0.4234695591708837, 0.501786021212506, 1.0005226338786515, 0.47229061667346856, 23.637705836501173, 1457.9905], [0.5237383676899804, 0.43399432408412464, 0.5083585548121925, 0.9778479298034527, 0.46366878622151636, 23.10200782557115, 1431.3744], [0.5306309130456712, 0.42652222327915057, 0.5055172115402072, 1.0003700248842091, 0.46978989075842903, 23.63410039430878, 1450.2706], [0.5348766446113586, 0.42189465272218896, 0.49757962064175115, 0.9924071418125425, 0.47358077151210515, 23.445974427653898, 1461.9733]]
Round_15_results:  [0.5348766446113586, 0.42189465272218896, 0.49757962064175115, 0.9924071418125425, 0.47358077151210515, 23.445974427653898, 1461.9733]
trigger times: 0
Loss after 860358300 batches: 0.1092
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 6990 < 6991; dropping {'Training_Loss': 0.10922154278125402, 'Validation_Loss': 0.19143504401048025, 'Training_R2': 0.8900046641310465, 'Validation_R2': 0.8222150126266689, 'Training_F1': 0.829207894815155, 'Validation_F1': 0.7349301971603973, 'Training_NEP': 0.34124623831552753, 'Validation_NEP': 0.4934479061473699, 'Training_NDE': 0.08257545105706073, 'Validation_NDE': 0.14157859216682048, 'Training_MAE': 11.301398648742863, 'Validation_MAE': 13.532388776519392, 'Training_MSE': 363.31906, 'Validation_MSE': 522.8459}.
trigger times: 0
Loss after 860489400 batches: 0.0262
trigger times: 1
Loss after 860620500 batches: 0.0200
trigger times: 2
Loss after 860751600 batches: 0.0169
trigger times: 3
Loss after 860882700 batches: 0.0152
trigger times: 4
Loss after 861013800 batches: 0.0143
trigger times: 5
Loss after 861144900 batches: 0.0135
trigger times: 6
Loss after 861276000 batches: 0.0130
trigger times: 7
Loss after 861407100 batches: 0.0126
trigger times: 8
Loss after 861538200 batches: 0.0123
trigger times: 9
Loss after 861669300 batches: 0.0118
trigger times: 10
Loss after 861800400 batches: 0.0116
trigger times: 11
Loss after 861931500 batches: 0.0113
trigger times: 12
Loss after 862062600 batches: 0.0115
trigger times: 13
Loss after 862193700 batches: 0.0111
trigger times: 14
Loss after 862324800 batches: 0.0106
trigger times: 15
Loss after 862455900 batches: 0.0104
trigger times: 16
Loss after 862587000 batches: 0.0102
trigger times: 17
Loss after 862718100 batches: 0.0102
trigger times: 18
Loss after 862849200 batches: 0.0102
trigger times: 19
Loss after 862980300 batches: 0.0099
trigger times: 20
Early stopping!
Start to test process.
Loss after 863111400 batches: 0.0100
Time to train on one home:  169.60152745246887
trigger times: 0
Loss after 863214000 batches: 0.2286
trigger times: 0
Loss after 863316600 batches: 0.0734
trigger times: 0
Loss after 863419200 batches: 0.0484
trigger times: 1
Loss after 863521800 batches: 0.0391
trigger times: 2
Loss after 863624400 batches: 0.0344
trigger times: 3
Loss after 863727000 batches: 0.0313
trigger times: 4
Loss after 863829600 batches: 0.0300
trigger times: 5
Loss after 863932200 batches: 0.0306
trigger times: 0
Loss after 864034800 batches: 0.0280
trigger times: 1
Loss after 864137400 batches: 0.0274
trigger times: 2
Loss after 864240000 batches: 0.0305
trigger times: 3
Loss after 864342600 batches: 0.0261
trigger times: 4
Loss after 864445200 batches: 0.0250
trigger times: 0
Loss after 864547800 batches: 0.0241
trigger times: 1
Loss after 864650400 batches: 0.0243
trigger times: 2
Loss after 864753000 batches: 0.0232
trigger times: 3
Loss after 864855600 batches: 0.0225
trigger times: 4
Loss after 864958200 batches: 0.0218
trigger times: 5
Loss after 865060800 batches: 0.0219
trigger times: 6
Loss after 865163400 batches: 0.0208
trigger times: 7
Loss after 865266000 batches: 0.0200
trigger times: 8
Loss after 865368600 batches: 0.0200
trigger times: 9
Loss after 865471200 batches: 0.0198
trigger times: 0
Loss after 865573800 batches: 0.0195
trigger times: 1
Loss after 865676400 batches: 0.0229
trigger times: 2
Loss after 865779000 batches: 0.0206
trigger times: 3
Loss after 865881600 batches: 0.0195
trigger times: 4
Loss after 865984200 batches: 0.0229
trigger times: 5
Loss after 866086800 batches: 0.0269
trigger times: 0
Loss after 866189400 batches: 0.0220
trigger times: 1
Loss after 866292000 batches: 0.0191
trigger times: 2
Loss after 866394600 batches: 0.0180
trigger times: 0
Loss after 866497200 batches: 0.0177
trigger times: 1
Loss after 866599800 batches: 0.0176
trigger times: 2
Loss after 866702400 batches: 0.0185
trigger times: 3
Loss after 866805000 batches: 0.0192
trigger times: 4
Loss after 866907600 batches: 0.0180
trigger times: 5
Loss after 867010200 batches: 0.0185
trigger times: 6
Loss after 867112800 batches: 0.0170
trigger times: 7
Loss after 867215400 batches: 0.0175
trigger times: 8
Loss after 867318000 batches: 0.0183
trigger times: 9
Loss after 867420600 batches: 0.0167
trigger times: 10
Loss after 867523200 batches: 0.0172
trigger times: 11
Loss after 867625800 batches: 0.0164
trigger times: 12
Loss after 867728400 batches: 0.0163
trigger times: 13
Loss after 867831000 batches: 0.0167
trigger times: 14
Loss after 867933600 batches: 0.0178
trigger times: 15
Loss after 868036200 batches: 0.0168
trigger times: 16
Loss after 868138800 batches: 0.0161
trigger times: 17
Loss after 868241400 batches: 0.0160
trigger times: 18
Loss after 868344000 batches: 0.0161
trigger times: 19
Loss after 868446600 batches: 0.0156
trigger times: 20
Early stopping!
Start to test process.
Loss after 868549200 batches: 0.0160
Time to train on one home:  316.50825357437134
trigger times: 0
Loss after 868680300 batches: 0.1290
trigger times: 0
Loss after 868811400 batches: 0.0409
trigger times: 1
Loss after 868942500 batches: 0.0311
trigger times: 2
Loss after 869073600 batches: 0.0276
trigger times: 3
Loss after 869204700 batches: 0.0253
trigger times: 4
Loss after 869335800 batches: 0.0235
trigger times: 5
Loss after 869466900 batches: 0.0223
trigger times: 6
Loss after 869598000 batches: 0.0216
trigger times: 7
Loss after 869729100 batches: 0.0207
trigger times: 8
Loss after 869860200 batches: 0.0201
trigger times: 9
Loss after 869991300 batches: 0.0193
trigger times: 10
Loss after 870122400 batches: 0.0191
trigger times: 11
Loss after 870253500 batches: 0.0187
trigger times: 12
Loss after 870384600 batches: 0.0182
trigger times: 13
Loss after 870515700 batches: 0.0178
trigger times: 14
Loss after 870646800 batches: 0.0176
trigger times: 15
Loss after 870777900 batches: 0.0172
trigger times: 16
Loss after 870909000 batches: 0.0169
trigger times: 17
Loss after 871040100 batches: 0.0167
trigger times: 18
Loss after 871171200 batches: 0.0164
trigger times: 19
Loss after 871302300 batches: 0.0164
trigger times: 20
Early stopping!
Start to test process.
Loss after 871433400 batches: 0.0160
Time to train on one home:  169.3331356048584
trigger times: 0
Loss after 871564500 batches: 0.1622
trigger times: 0
Loss after 871695600 batches: 0.0470
trigger times: 0
Loss after 871826700 batches: 0.0352
trigger times: 1
Loss after 871957800 batches: 0.0306
trigger times: 2
Loss after 872088900 batches: 0.0286
trigger times: 0
Loss after 872220000 batches: 0.0270
trigger times: 1
Loss after 872351100 batches: 0.0258
trigger times: 2
Loss after 872482200 batches: 0.0245
trigger times: 3
Loss after 872613300 batches: 0.0238
trigger times: 4
Loss after 872744400 batches: 0.0231
trigger times: 5
Loss after 872875500 batches: 0.0226
trigger times: 6
Loss after 873006600 batches: 0.0220
trigger times: 0
Loss after 873137700 batches: 0.0217
trigger times: 1
Loss after 873268800 batches: 0.0211
trigger times: 2
Loss after 873399900 batches: 0.0209
trigger times: 3
Loss after 873531000 batches: 0.0206
trigger times: 4
Loss after 873662100 batches: 0.0203
trigger times: 0
Loss after 873793200 batches: 0.0200
trigger times: 0
Loss after 873924300 batches: 0.0199
trigger times: 1
Loss after 874055400 batches: 0.0196
trigger times: 2
Loss after 874186500 batches: 0.0198
trigger times: 3
Loss after 874317600 batches: 0.0193
trigger times: 4
Loss after 874448700 batches: 0.0189
trigger times: 0
Loss after 874579800 batches: 0.0189
trigger times: 0
Loss after 874710900 batches: 0.0187
trigger times: 1
Loss after 874842000 batches: 0.0188
trigger times: 2
Loss after 874973100 batches: 0.0185
trigger times: 0
Loss after 875104200 batches: 0.0183
trigger times: 1
Loss after 875235300 batches: 0.0182
trigger times: 2
Loss after 875366400 batches: 0.0181
trigger times: 3
Loss after 875497500 batches: 0.0177
trigger times: 4
Loss after 875628600 batches: 0.0177
trigger times: 5
Loss after 875759700 batches: 0.0177
trigger times: 0
Loss after 875890800 batches: 0.0178
trigger times: 1
Loss after 876021900 batches: 0.0176
trigger times: 2
Loss after 876153000 batches: 0.0177
trigger times: 3
Loss after 876284100 batches: 0.0174
trigger times: 4
Loss after 876415200 batches: 0.0172
trigger times: 0
Loss after 876546300 batches: 0.0173
trigger times: 1
Loss after 876677400 batches: 0.0172
trigger times: 2
Loss after 876808500 batches: 0.0170
trigger times: 0
Loss after 876939600 batches: 0.0169
trigger times: 1
Loss after 877070700 batches: 0.0171
trigger times: 2
Loss after 877201800 batches: 0.0167
trigger times: 3
Loss after 877332900 batches: 0.0167
trigger times: 4
Loss after 877464000 batches: 0.0166
trigger times: 5
Loss after 877595100 batches: 0.0167
trigger times: 6
Loss after 877726200 batches: 0.0166
trigger times: 7
Loss after 877857300 batches: 0.0162
trigger times: 8
Loss after 877988400 batches: 0.0163
trigger times: 0
Loss after 878119500 batches: 0.0161
trigger times: 1
Loss after 878250600 batches: 0.0161
trigger times: 2
Loss after 878381700 batches: 0.0161
trigger times: 3
Loss after 878512800 batches: 0.0160
trigger times: 4
Loss after 878643900 batches: 0.0160
trigger times: 5
Loss after 878775000 batches: 0.0156
trigger times: 6
Loss after 878906100 batches: 0.0158
trigger times: 7
Loss after 879037200 batches: 0.0157
trigger times: 8
Loss after 879168300 batches: 0.0158
trigger times: 9
Loss after 879299400 batches: 0.0157
trigger times: 10
Loss after 879430500 batches: 0.0156
trigger times: 11
Loss after 879561600 batches: 0.0155
trigger times: 12
Loss after 879692700 batches: 0.0154
trigger times: 13
Loss after 879823800 batches: 0.0154
trigger times: 14
Loss after 879954900 batches: 0.0156
trigger times: 15
Loss after 880086000 batches: 0.0154
trigger times: 16
Loss after 880217100 batches: 0.0154
trigger times: 17
Loss after 880348200 batches: 0.0153
trigger times: 18
Loss after 880479300 batches: 0.0151
trigger times: 19
Loss after 880610400 batches: 0.0153
trigger times: 20
Early stopping!
Start to test process.
Loss after 880741500 batches: 0.0151
Time to train on one home:  519.850822687149
trigger times: 0
Loss after 880870140 batches: 0.1054
trigger times: 0
Loss after 880998780 batches: 0.0298
trigger times: 0
Loss after 881127420 batches: 0.0219
trigger times: 0
Loss after 881256060 batches: 0.0201
trigger times: 1
Loss after 881384700 batches: 0.0182
trigger times: 2
Loss after 881513340 batches: 0.0169
trigger times: 0
Loss after 881641980 batches: 0.0160
trigger times: 0
Loss after 881770620 batches: 0.0159
trigger times: 0
Loss after 881899260 batches: 0.0152
trigger times: 1
Loss after 882027900 batches: 0.0148
trigger times: 2
Loss after 882156540 batches: 0.0144
trigger times: 3
Loss after 882285180 batches: 0.0141
trigger times: 4
Loss after 882413820 batches: 0.0139
trigger times: 5
Loss after 882542460 batches: 0.0134
trigger times: 6
Loss after 882671100 batches: 0.0132
trigger times: 7
Loss after 882799740 batches: 0.0132
trigger times: 8
Loss after 882928380 batches: 0.0129
trigger times: 9
Loss after 883057020 batches: 0.0127
trigger times: 10
Loss after 883185660 batches: 0.0125
trigger times: 11
Loss after 883314300 batches: 0.0123
trigger times: 12
Loss after 883442940 batches: 0.0122
trigger times: 13
Loss after 883571580 batches: 0.0119
trigger times: 14
Loss after 883700220 batches: 0.0120
trigger times: 15
Loss after 883828860 batches: 0.0119
trigger times: 16
Loss after 883957500 batches: 0.0117
trigger times: 17
Loss after 884086140 batches: 0.0117
trigger times: 18
Loss after 884214780 batches: 0.0116
trigger times: 19
Loss after 884343420 batches: 0.0114
trigger times: 20
Early stopping!
Start to test process.
Loss after 884472060 batches: 0.0112
Time to train on one home:  217.16195011138916
trigger times: 0
Loss after 884603160 batches: 0.1736
trigger times: 0
Loss after 884734260 batches: 0.0454
trigger times: 0
Loss after 884865360 batches: 0.0340
trigger times: 0
Loss after 884996460 batches: 0.0296
trigger times: 0
Loss after 885127560 batches: 0.0275
trigger times: 0
Loss after 885258660 batches: 0.0261
trigger times: 0
Loss after 885389760 batches: 0.0250
trigger times: 1
Loss after 885520860 batches: 0.0238
trigger times: 0
Loss after 885651960 batches: 0.0230
trigger times: 1
Loss after 885783060 batches: 0.0228
trigger times: 2
Loss after 885914160 batches: 0.0221
trigger times: 3
Loss after 886045260 batches: 0.0213
trigger times: 4
Loss after 886176360 batches: 0.0209
trigger times: 0
Loss after 886307460 batches: 0.0208
trigger times: 1
Loss after 886438560 batches: 0.0203
trigger times: 2
Loss after 886569660 batches: 0.0198
trigger times: 3
Loss after 886700760 batches: 0.0198
trigger times: 4
Loss after 886831860 batches: 0.0197
trigger times: 5
Loss after 886962960 batches: 0.0193
trigger times: 6
Loss after 887094060 batches: 0.0187
trigger times: 7
Loss after 887225160 batches: 0.0185
trigger times: 0
Loss after 887356260 batches: 0.0183
trigger times: 1
Loss after 887487360 batches: 0.0184
trigger times: 2
Loss after 887618460 batches: 0.0183
trigger times: 3
Loss after 887749560 batches: 0.0180
trigger times: 4
Loss after 887880660 batches: 0.0181
trigger times: 5
Loss after 888011760 batches: 0.0180
trigger times: 6
Loss after 888142860 batches: 0.0179
trigger times: 7
Loss after 888273960 batches: 0.0178
trigger times: 8
Loss after 888405060 batches: 0.0175
trigger times: 9
Loss after 888536160 batches: 0.0175
trigger times: 10
Loss after 888667260 batches: 0.0175
trigger times: 11
Loss after 888798360 batches: 0.0172
trigger times: 0
Loss after 888929460 batches: 0.0170
trigger times: 1
Loss after 889060560 batches: 0.0170
trigger times: 2
Loss after 889191660 batches: 0.0168
trigger times: 3
Loss after 889322760 batches: 0.0166
trigger times: 4
Loss after 889453860 batches: 0.0167
trigger times: 5
Loss after 889584960 batches: 0.0167
trigger times: 6
Loss after 889716060 batches: 0.0166
trigger times: 7
Loss after 889847160 batches: 0.0164
trigger times: 8
Loss after 889978260 batches: 0.0166
trigger times: 0
Loss after 890109360 batches: 0.0163
trigger times: 1
Loss after 890240460 batches: 0.0162
trigger times: 2
Loss after 890371560 batches: 0.0162
trigger times: 3
Loss after 890502660 batches: 0.0161
trigger times: 4
Loss after 890633760 batches: 0.0163
trigger times: 0
Loss after 890764860 batches: 0.0160
trigger times: 1
Loss after 890895960 batches: 0.0157
trigger times: 2
Loss after 891027060 batches: 0.0157
trigger times: 3
Loss after 891158160 batches: 0.0157
trigger times: 4
Loss after 891289260 batches: 0.0158
trigger times: 5
Loss after 891420360 batches: 0.0156
trigger times: 6
Loss after 891551460 batches: 0.0156
trigger times: 7
Loss after 891682560 batches: 0.0157
trigger times: 8
Loss after 891813660 batches: 0.0155
trigger times: 9
Loss after 891944760 batches: 0.0156
trigger times: 10
Loss after 892075860 batches: 0.0153
trigger times: 11
Loss after 892206960 batches: 0.0154
trigger times: 12
Loss after 892338060 batches: 0.0151
trigger times: 13
Loss after 892469160 batches: 0.0150
trigger times: 14
Loss after 892600260 batches: 0.0151
trigger times: 15
Loss after 892731360 batches: 0.0151
trigger times: 0
Loss after 892862460 batches: 0.0150
trigger times: 1
Loss after 892993560 batches: 0.0150
trigger times: 2
Loss after 893124660 batches: 0.0150
trigger times: 3
Loss after 893255760 batches: 0.0151
trigger times: 4
Loss after 893386860 batches: 0.0149
trigger times: 5
Loss after 893517960 batches: 0.0149
trigger times: 6
Loss after 893649060 batches: 0.0150
trigger times: 0
Loss after 893780160 batches: 0.0149
trigger times: 1
Loss after 893911260 batches: 0.0145
trigger times: 2
Loss after 894042360 batches: 0.0147
trigger times: 3
Loss after 894173460 batches: 0.0146
trigger times: 4
Loss after 894304560 batches: 0.0145
trigger times: 5
Loss after 894435660 batches: 0.0145
trigger times: 6
Loss after 894566760 batches: 0.0145
trigger times: 7
Loss after 894697860 batches: 0.0144
trigger times: 8
Loss after 894828960 batches: 0.0145
trigger times: 9
Loss after 894960060 batches: 0.0143
trigger times: 10
Loss after 895091160 batches: 0.0141
trigger times: 0
Loss after 895222260 batches: 0.0145
trigger times: 1
Loss after 895353360 batches: 0.0142
trigger times: 2
Loss after 895484460 batches: 0.0142
trigger times: 3
Loss after 895615560 batches: 0.0143
trigger times: 4
Loss after 895746660 batches: 0.0143
trigger times: 5
Loss after 895877760 batches: 0.0143
trigger times: 6
Loss after 896008860 batches: 0.0144
trigger times: 7
Loss after 896139960 batches: 0.0140
trigger times: 8
Loss after 896271060 batches: 0.0142
trigger times: 9
Loss after 896402160 batches: 0.0140
trigger times: 10
Loss after 896533260 batches: 0.0139
trigger times: 11
Loss after 896664360 batches: 0.0140
trigger times: 12
Loss after 896795460 batches: 0.0139
trigger times: 13
Loss after 896926560 batches: 0.0139
trigger times: 14
Loss after 897057660 batches: 0.0138
trigger times: 15
Loss after 897188760 batches: 0.0138
trigger times: 16
Loss after 897319860 batches: 0.0138
trigger times: 17
Loss after 897450960 batches: 0.0138
trigger times: 18
Loss after 897582060 batches: 0.0138
trigger times: 19
Loss after 897713160 batches: 0.0138
trigger times: 20
Early stopping!
Start to test process.
Loss after 897844260 batches: 0.0137
Time to train on one home:  740.6626079082489
trigger times: 0
Loss after 897975360 batches: 0.1924
trigger times: 0
Loss after 898106460 batches: 0.0700
trigger times: 1
Loss after 898237560 batches: 0.0458
trigger times: 2
Loss after 898368660 batches: 0.0370
trigger times: 3
Loss after 898499760 batches: 0.0345
trigger times: 4
Loss after 898630860 batches: 0.0307
trigger times: 5
Loss after 898761960 batches: 0.0282
trigger times: 6
Loss after 898893060 batches: 0.0264
trigger times: 7
Loss after 899024160 batches: 0.0261
trigger times: 8
Loss after 899155260 batches: 0.0255
trigger times: 9
Loss after 899286360 batches: 0.0256
trigger times: 0
Loss after 899417460 batches: 0.0250
trigger times: 0
Loss after 899548560 batches: 0.0247
trigger times: 1
Loss after 899679660 batches: 0.0247
trigger times: 2
Loss after 899810760 batches: 0.0259
trigger times: 3
Loss after 899941860 batches: 0.0244
trigger times: 4
Loss after 900072960 batches: 0.0234
trigger times: 5
Loss after 900204060 batches: 0.0231
trigger times: 0
Loss after 900335160 batches: 0.0230
trigger times: 1
Loss after 900466260 batches: 0.0225
trigger times: 2
Loss after 900597360 batches: 0.0225
trigger times: 0
Loss after 900728460 batches: 0.0217
trigger times: 1
Loss after 900859560 batches: 0.0215
trigger times: 2
Loss after 900990660 batches: 0.0222
trigger times: 3
Loss after 901121760 batches: 0.0211
trigger times: 4
Loss after 901252860 batches: 0.0211
trigger times: 5
Loss after 901383960 batches: 0.0211
trigger times: 6
Loss after 901515060 batches: 0.0219
trigger times: 7
Loss after 901646160 batches: 0.0205
trigger times: 8
Loss after 901777260 batches: 0.0198
trigger times: 9
Loss after 901908360 batches: 0.0197
trigger times: 10
Loss after 902039460 batches: 0.0202
trigger times: 11
Loss after 902170560 batches: 0.0217
trigger times: 12
Loss after 902301660 batches: 0.0211
trigger times: 0
Loss after 902432760 batches: 0.0208
trigger times: 1
Loss after 902563860 batches: 0.0205
trigger times: 2
Loss after 902694960 batches: 0.0211
trigger times: 3
Loss after 902826060 batches: 0.0215
trigger times: 4
Loss after 902957160 batches: 0.0206
trigger times: 5
Loss after 903088260 batches: 0.0201
trigger times: 6
Loss after 903219360 batches: 0.0192
trigger times: 7
Loss after 903350460 batches: 0.0193
trigger times: 0
Loss after 903481560 batches: 0.0196
trigger times: 1
Loss after 903612660 batches: 0.0190
trigger times: 2
Loss after 903743760 batches: 0.0189
trigger times: 3
Loss after 903874860 batches: 0.0189
trigger times: 4
Loss after 904005960 batches: 0.0194
trigger times: 5
Loss after 904137060 batches: 0.0194
trigger times: 6
Loss after 904268160 batches: 0.0196
trigger times: 7
Loss after 904399260 batches: 0.0187
trigger times: 8
Loss after 904530360 batches: 0.0180
trigger times: 9
Loss after 904661460 batches: 0.0181
trigger times: 10
Loss after 904792560 batches: 0.0184
trigger times: 11
Loss after 904923660 batches: 0.0189
trigger times: 12
Loss after 905054760 batches: 0.0188
trigger times: 13
Loss after 905185860 batches: 0.0183
trigger times: 14
Loss after 905316960 batches: 0.0183
trigger times: 15
Loss after 905448060 batches: 0.0181
trigger times: 16
Loss after 905579160 batches: 0.0178
trigger times: 17
Loss after 905710260 batches: 0.0180
trigger times: 0
Loss after 905841360 batches: 0.0180
trigger times: 1
Loss after 905972460 batches: 0.0182
trigger times: 2
Loss after 906103560 batches: 0.0177
trigger times: 3
Loss after 906234660 batches: 0.0181
trigger times: 4
Loss after 906365760 batches: 0.0176
trigger times: 0
Loss after 906496860 batches: 0.0185
trigger times: 0
Loss after 906627960 batches: 0.0175
trigger times: 0
Loss after 906759060 batches: 0.0164
trigger times: 1
Loss after 906890160 batches: 0.0167
trigger times: 2
Loss after 907021260 batches: 0.0166
trigger times: 3
Loss after 907152360 batches: 0.0167
trigger times: 0
Loss after 907283460 batches: 0.0172
trigger times: 1
Loss after 907414560 batches: 0.0171
trigger times: 2
Loss after 907545660 batches: 0.0172
trigger times: 3
Loss after 907676760 batches: 0.0170
trigger times: 4
Loss after 907807860 batches: 0.0172
trigger times: 5
Loss after 907938960 batches: 0.0168
trigger times: 6
Loss after 908070060 batches: 0.0176
trigger times: 7
Loss after 908201160 batches: 0.0170
trigger times: 8
Loss after 908332260 batches: 0.0178
trigger times: 9
Loss after 908463360 batches: 0.0178
trigger times: 10
Loss after 908594460 batches: 0.0165
trigger times: 11
Loss after 908725560 batches: 0.0168
trigger times: 12
Loss after 908856660 batches: 0.0171
trigger times: 0
Loss after 908987760 batches: 0.0166
trigger times: 1
Loss after 909118860 batches: 0.0166
trigger times: 2
Loss after 909249960 batches: 0.0162
trigger times: 3
Loss after 909381060 batches: 0.0171
trigger times: 4
Loss after 909512160 batches: 0.0174
trigger times: 5
Loss after 909643260 batches: 0.0168
trigger times: 6
Loss after 909774360 batches: 0.0171
trigger times: 7
Loss after 909905460 batches: 0.0163
trigger times: 8
Loss after 910036560 batches: 0.0161
trigger times: 9
Loss after 910167660 batches: 0.0167
trigger times: 10
Loss after 910298760 batches: 0.0163
trigger times: 11
Loss after 910429860 batches: 0.0163
trigger times: 12
Loss after 910560960 batches: 0.0168
trigger times: 13
Loss after 910692060 batches: 0.0169
trigger times: 14
Loss after 910823160 batches: 0.0167
trigger times: 15
Loss after 910954260 batches: 0.0169
trigger times: 16
Loss after 911085360 batches: 0.0163
trigger times: 17
Loss after 911216460 batches: 0.0159
trigger times: 18
Loss after 911347560 batches: 0.0160
trigger times: 19
Loss after 911478660 batches: 0.0163
trigger times: 20
Early stopping!
Start to test process.
Loss after 911609760 batches: 0.0160
Time to train on one home:  763.9449846744537
trigger times: 0
Loss after 911740860 batches: 0.0671
trigger times: 0
Loss after 911871960 batches: 0.0193
trigger times: 0
Loss after 912003060 batches: 0.0149
trigger times: 1
Loss after 912134160 batches: 0.0131
trigger times: 2
Loss after 912265260 batches: 0.0119
trigger times: 0
Loss after 912396360 batches: 0.0114
trigger times: 0
Loss after 912527460 batches: 0.0108
trigger times: 1
Loss after 912658560 batches: 0.0104
trigger times: 2
Loss after 912789660 batches: 0.0102
trigger times: 3
Loss after 912920760 batches: 0.0099
trigger times: 4
Loss after 913051860 batches: 0.0096
trigger times: 5
Loss after 913182960 batches: 0.0093
trigger times: 6
Loss after 913314060 batches: 0.0092
trigger times: 7
Loss after 913445160 batches: 0.0090
trigger times: 8
Loss after 913576260 batches: 0.0087
trigger times: 9
Loss after 913707360 batches: 0.0088
trigger times: 10
Loss after 913838460 batches: 0.0086
trigger times: 11
Loss after 913969560 batches: 0.0083
trigger times: 12
Loss after 914100660 batches: 0.0083
trigger times: 13
Loss after 914231760 batches: 0.0083
trigger times: 14
Loss after 914362860 batches: 0.0083
trigger times: 15
Loss after 914493960 batches: 0.0080
trigger times: 16
Loss after 914625060 batches: 0.0080
trigger times: 17
Loss after 914756160 batches: 0.0079
trigger times: 18
Loss after 914887260 batches: 0.0079
trigger times: 19
Loss after 915018360 batches: 0.0078
trigger times: 20
Early stopping!
Start to test process.
Loss after 915149460 batches: 0.0078
Time to train on one home:  204.5782232284546
trigger times: 0
Loss after 915228060 batches: 0.2012
trigger times: 0
Loss after 915306660 batches: 0.0503
trigger times: 1
Loss after 915385260 batches: 0.0339
trigger times: 0
Loss after 915463860 batches: 0.0275
trigger times: 1
Loss after 915542460 batches: 0.0244
trigger times: 0
Loss after 915621060 batches: 0.0232
trigger times: 0
Loss after 915699660 batches: 0.0224
trigger times: 1
Loss after 915778260 batches: 0.0213
trigger times: 2
Loss after 915856860 batches: 0.0204
trigger times: 3
Loss after 915935460 batches: 0.0196
trigger times: 4
Loss after 916014060 batches: 0.0190
trigger times: 0
Loss after 916092660 batches: 0.0188
trigger times: 1
Loss after 916171260 batches: 0.0183
trigger times: 0
Loss after 916249860 batches: 0.0177
trigger times: 1
Loss after 916328460 batches: 0.0179
trigger times: 2
Loss after 916407060 batches: 0.0174
trigger times: 3
Loss after 916485660 batches: 0.0166
trigger times: 0
Loss after 916564260 batches: 0.0169
trigger times: 1
Loss after 916642860 batches: 0.0168
trigger times: 2
Loss after 916721460 batches: 0.0163
trigger times: 3
Loss after 916800060 batches: 0.0161
trigger times: 4
Loss after 916878660 batches: 0.0155
trigger times: 5
Loss after 916957260 batches: 0.0155
trigger times: 6
Loss after 917035860 batches: 0.0157
trigger times: 7
Loss after 917114460 batches: 0.0153
trigger times: 8
Loss after 917193060 batches: 0.0154
trigger times: 9
Loss after 917271660 batches: 0.0148
trigger times: 10
Loss after 917350260 batches: 0.0150
trigger times: 11
Loss after 917428860 batches: 0.0153
trigger times: 12
Loss after 917507460 batches: 0.0150
trigger times: 13
Loss after 917586060 batches: 0.0147
trigger times: 14
Loss after 917664660 batches: 0.0145
trigger times: 15
Loss after 917743260 batches: 0.0143
trigger times: 16
Loss after 917821860 batches: 0.0143
trigger times: 17
Loss after 917900460 batches: 0.0139
trigger times: 18
Loss after 917979060 batches: 0.0144
trigger times: 19
Loss after 918057660 batches: 0.0141
trigger times: 20
Early stopping!
Start to test process.
Loss after 918136260 batches: 0.0136
Time to train on one home:  186.81745839118958
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097, 0.016851266692967333, 0.01539797175087967, 0.015622467135075917, 0.014339386766728896, 0.015215330347522192, 0.014610499733001695, 0.014354899421970126, 0.01333385277873801, 0.01326294574980891]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983], [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013], [0.5549709763791826, 0.4001270641513627, 0.48658498833859326, 1.0394903420759227, 0.4914126622529051, 24.558331909615507, 1517.0215], [0.5461304287115732, 0.4097276108845518, 0.4876006728779417, 1.0136453079994863, 0.48354794633174836, 23.94773371608454, 1492.7424], [0.5402040051089393, 0.41609081728992114, 0.4927051878338842, 1.0002093606660662, 0.47833524208513384, 23.630304644569275, 1476.6505], [0.5334714584880405, 0.4234695591708837, 0.501786021212506, 1.0005226338786515, 0.47229061667346856, 23.637705836501173, 1457.9905], [0.5237383676899804, 0.43399432408412464, 0.5083585548121925, 0.9778479298034527, 0.46366878622151636, 23.10200782557115, 1431.3744], [0.5306309130456712, 0.42652222327915057, 0.5055172115402072, 1.0003700248842091, 0.46978989075842903, 23.63410039430878, 1450.2706], [0.5348766446113586, 0.42189465272218896, 0.49757962064175115, 0.9924071418125425, 0.47358077151210515, 23.445974427653898, 1461.9733], [0.5336118737856547, 0.4232833712453734, 0.5012302308952711, 0.9933548830158522, 0.47244314081431027, 23.468365153275084, 1458.4613]]
Round_16_results:  [0.5336118737856547, 0.4232833712453734, 0.5012302308952711, 0.9933548830158522, 0.47244314081431027, 23.468365153275084, 1458.4613]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 7459 < 7460; dropping {'Training_Loss': 0.11853495060975822, 'Validation_Loss': 0.18990556481811735, 'Training_R2': 0.8805078425262676, 'Validation_R2': 0.8236959782398063, 'Training_F1': 0.8202817877351365, 'Validation_F1': 0.7379421701447526, 'Training_NEP': 0.3590478273672858, 'Validation_NEP': 0.4830693588022875, 'Training_NDE': 0.08970488360461311, 'Validation_NDE': 0.14039922922030135, 'Training_MAE': 11.89095197377907, 'Validation_MAE': 13.247765950361888, 'Training_MSE': 394.6874, 'Validation_MSE': 518.4905}.
trigger times: 0
Loss after 918267360 batches: 0.1185
trigger times: 0
Loss after 918398460 batches: 0.0262
trigger times: 0
Loss after 918529560 batches: 0.0195
trigger times: 0
Loss after 918660660 batches: 0.0168
trigger times: 0
Loss after 918791760 batches: 0.0156
trigger times: 1
Loss after 918922860 batches: 0.0146
trigger times: 0
Loss after 919053960 batches: 0.0140
trigger times: 1
Loss after 919185060 batches: 0.0132
trigger times: 2
Loss after 919316160 batches: 0.0127
trigger times: 3
Loss after 919447260 batches: 0.0123
trigger times: 4
Loss after 919578360 batches: 0.0120
trigger times: 0
Loss after 919709460 batches: 0.0120
trigger times: 1
Loss after 919840560 batches: 0.0116
trigger times: 2
Loss after 919971660 batches: 0.0113
trigger times: 3
Loss after 920102760 batches: 0.0113
trigger times: 4
Loss after 920233860 batches: 0.0110
trigger times: 5
Loss after 920364960 batches: 0.0108
trigger times: 6
Loss after 920496060 batches: 0.0107
trigger times: 7
Loss after 920627160 batches: 0.0101
trigger times: 8
Loss after 920758260 batches: 0.0100
trigger times: 9
Loss after 920889360 batches: 0.0101
trigger times: 10
Loss after 921020460 batches: 0.0102
trigger times: 11
Loss after 921151560 batches: 0.0101
trigger times: 12
Loss after 921282660 batches: 0.0098
trigger times: 13
Loss after 921413760 batches: 0.0097
trigger times: 14
Loss after 921544860 batches: 0.0097
trigger times: 15
Loss after 921675960 batches: 0.0095
trigger times: 16
Loss after 921807060 batches: 0.0094
trigger times: 17
Loss after 921938160 batches: 0.0095
trigger times: 18
Loss after 922069260 batches: 0.0094
trigger times: 19
Loss after 922200360 batches: 0.0091
trigger times: 20
Early stopping!
Start to test process.
Loss after 922331460 batches: 0.0090
Time to train on one home:  240.27249717712402
trigger times: 0
Loss after 922434060 batches: 0.2039
trigger times: 0
Loss after 922536660 batches: 0.0701
trigger times: 0
Loss after 922639260 batches: 0.0472
trigger times: 1
Loss after 922741860 batches: 0.0403
trigger times: 2
Loss after 922844460 batches: 0.0406
trigger times: 3
Loss after 922947060 batches: 0.0316
trigger times: 4
Loss after 923049660 batches: 0.0309
trigger times: 5
Loss after 923152260 batches: 0.0274
trigger times: 6
Loss after 923254860 batches: 0.0264
trigger times: 7
Loss after 923357460 batches: 0.0258
trigger times: 8
Loss after 923460060 batches: 0.0246
trigger times: 9
Loss after 923562660 batches: 0.0248
trigger times: 0
Loss after 923665260 batches: 0.0236
trigger times: 1
Loss after 923767860 batches: 0.0221
trigger times: 2
Loss after 923870460 batches: 0.0208
trigger times: 0
Loss after 923973060 batches: 0.0212
trigger times: 1
Loss after 924075660 batches: 0.0210
trigger times: 2
Loss after 924178260 batches: 0.0205
trigger times: 3
Loss after 924280860 batches: 0.0207
trigger times: 0
Loss after 924383460 batches: 0.0221
trigger times: 1
Loss after 924486060 batches: 0.0225
trigger times: 2
Loss after 924588660 batches: 0.0260
trigger times: 3
Loss after 924691260 batches: 0.0209
trigger times: 4
Loss after 924793860 batches: 0.0214
trigger times: 5
Loss after 924896460 batches: 0.0202
trigger times: 6
Loss after 924999060 batches: 0.0201
trigger times: 7
Loss after 925101660 batches: 0.0189
trigger times: 8
Loss after 925204260 batches: 0.0186
trigger times: 9
Loss after 925306860 batches: 0.0182
trigger times: 10
Loss after 925409460 batches: 0.0179
trigger times: 11
Loss after 925512060 batches: 0.0182
trigger times: 12
Loss after 925614660 batches: 0.0186
trigger times: 13
Loss after 925717260 batches: 0.0190
trigger times: 14
Loss after 925819860 batches: 0.0177
trigger times: 15
Loss after 925922460 batches: 0.0183
trigger times: 16
Loss after 926025060 batches: 0.0178
trigger times: 17
Loss after 926127660 batches: 0.0174
trigger times: 18
Loss after 926230260 batches: 0.0174
trigger times: 19
Loss after 926332860 batches: 0.0185
trigger times: 20
Early stopping!
Start to test process.
Loss after 926435460 batches: 0.0168
Time to train on one home:  240.3226556777954
trigger times: 0
Loss after 926566560 batches: 0.1441
trigger times: 1
Loss after 926697660 batches: 0.0428
trigger times: 2
Loss after 926828760 batches: 0.0317
trigger times: 0
Loss after 926959860 batches: 0.0278
trigger times: 1
Loss after 927090960 batches: 0.0254
trigger times: 2
Loss after 927222060 batches: 0.0238
trigger times: 3
Loss after 927353160 batches: 0.0230
trigger times: 4
Loss after 927484260 batches: 0.0218
trigger times: 5
Loss after 927615360 batches: 0.0208
trigger times: 6
Loss after 927746460 batches: 0.0203
trigger times: 7
Loss after 927877560 batches: 0.0197
trigger times: 8
Loss after 928008660 batches: 0.0194
trigger times: 9
Loss after 928139760 batches: 0.0188
trigger times: 10
Loss after 928270860 batches: 0.0185
trigger times: 11
Loss after 928401960 batches: 0.0182
trigger times: 12
Loss after 928533060 batches: 0.0176
trigger times: 0
Loss after 928664160 batches: 0.0173
trigger times: 1
Loss after 928795260 batches: 0.0173
trigger times: 2
Loss after 928926360 batches: 0.0168
trigger times: 3
Loss after 929057460 batches: 0.0167
trigger times: 4
Loss after 929188560 batches: 0.0163
trigger times: 5
Loss after 929319660 batches: 0.0164
trigger times: 6
Loss after 929450760 batches: 0.0158
trigger times: 7
Loss after 929581860 batches: 0.0157
trigger times: 8
Loss after 929712960 batches: 0.0157
trigger times: 9
Loss after 929844060 batches: 0.0156
trigger times: 10
Loss after 929975160 batches: 0.0153
trigger times: 11
Loss after 930106260 batches: 0.0152
trigger times: 12
Loss after 930237360 batches: 0.0148
trigger times: 13
Loss after 930368460 batches: 0.0150
trigger times: 14
Loss after 930499560 batches: 0.0147
trigger times: 15
Loss after 930630660 batches: 0.0148
trigger times: 16
Loss after 930761760 batches: 0.0147
trigger times: 17
Loss after 930892860 batches: 0.0144
trigger times: 18
Loss after 931023960 batches: 0.0145
trigger times: 19
Loss after 931155060 batches: 0.0142
trigger times: 20
Early stopping!
Start to test process.
Loss after 931286160 batches: 0.0142
Time to train on one home:  276.7429246902466
trigger times: 0
Loss after 931417260 batches: 0.1575
trigger times: 0
Loss after 931548360 batches: 0.0450
trigger times: 0
Loss after 931679460 batches: 0.0344
trigger times: 1
Loss after 931810560 batches: 0.0303
trigger times: 2
Loss after 931941660 batches: 0.0279
trigger times: 0
Loss after 932072760 batches: 0.0262
trigger times: 1
Loss after 932203860 batches: 0.0250
trigger times: 0
Loss after 932334960 batches: 0.0242
trigger times: 1
Loss after 932466060 batches: 0.0237
trigger times: 2
Loss after 932597160 batches: 0.0231
trigger times: 3
Loss after 932728260 batches: 0.0224
trigger times: 4
Loss after 932859360 batches: 0.0216
trigger times: 5
Loss after 932990460 batches: 0.0214
trigger times: 6
Loss after 933121560 batches: 0.0210
trigger times: 0
Loss after 933252660 batches: 0.0205
trigger times: 0
Loss after 933383760 batches: 0.0203
trigger times: 0
Loss after 933514860 batches: 0.0202
trigger times: 1
Loss after 933645960 batches: 0.0196
trigger times: 2
Loss after 933777060 batches: 0.0196
trigger times: 3
Loss after 933908160 batches: 0.0195
trigger times: 4
Loss after 934039260 batches: 0.0193
trigger times: 5
Loss after 934170360 batches: 0.0192
trigger times: 6
Loss after 934301460 batches: 0.0188
trigger times: 7
Loss after 934432560 batches: 0.0187
trigger times: 8
Loss after 934563660 batches: 0.0184
trigger times: 9
Loss after 934694760 batches: 0.0185
trigger times: 10
Loss after 934825860 batches: 0.0181
trigger times: 11
Loss after 934956960 batches: 0.0182
trigger times: 12
Loss after 935088060 batches: 0.0182
trigger times: 0
Loss after 935219160 batches: 0.0178
trigger times: 1
Loss after 935350260 batches: 0.0178
trigger times: 2
Loss after 935481360 batches: 0.0175
trigger times: 3
Loss after 935612460 batches: 0.0175
trigger times: 4
Loss after 935743560 batches: 0.0175
trigger times: 5
Loss after 935874660 batches: 0.0173
trigger times: 6
Loss after 936005760 batches: 0.0171
trigger times: 7
Loss after 936136860 batches: 0.0171
trigger times: 8
Loss after 936267960 batches: 0.0173
trigger times: 0
Loss after 936399060 batches: 0.0174
trigger times: 1
Loss after 936530160 batches: 0.0168
trigger times: 2
Loss after 936661260 batches: 0.0168
trigger times: 3
Loss after 936792360 batches: 0.0166
trigger times: 4
Loss after 936923460 batches: 0.0168
trigger times: 5
Loss after 937054560 batches: 0.0165
trigger times: 6
Loss after 937185660 batches: 0.0164
trigger times: 7
Loss after 937316760 batches: 0.0162
trigger times: 8
Loss after 937447860 batches: 0.0161
trigger times: 9
Loss after 937578960 batches: 0.0163
trigger times: 10
Loss after 937710060 batches: 0.0165
trigger times: 11
Loss after 937841160 batches: 0.0159
trigger times: 12
Loss after 937972260 batches: 0.0162
trigger times: 13
Loss after 938103360 batches: 0.0158
trigger times: 14
Loss after 938234460 batches: 0.0160
trigger times: 15
Loss after 938365560 batches: 0.0160
trigger times: 16
Loss after 938496660 batches: 0.0157
trigger times: 17
Loss after 938627760 batches: 0.0155
trigger times: 18
Loss after 938758860 batches: 0.0156
trigger times: 19
Loss after 938889960 batches: 0.0155
trigger times: 20
Early stopping!
Start to test process.
Loss after 939021060 batches: 0.0157
Time to train on one home:  432.85203742980957
trigger times: 0
Loss after 939149700 batches: 0.0984
trigger times: 0
Loss after 939278340 batches: 0.0284
trigger times: 0
Loss after 939406980 batches: 0.0223
trigger times: 0
Loss after 939535620 batches: 0.0198
trigger times: 0
Loss after 939664260 batches: 0.0182
trigger times: 0
Loss after 939792900 batches: 0.0168
trigger times: 0
Loss after 939921540 batches: 0.0162
trigger times: 1
Loss after 940050180 batches: 0.0157
trigger times: 2
Loss after 940178820 batches: 0.0153
trigger times: 3
Loss after 940307460 batches: 0.0145
trigger times: 4
Loss after 940436100 batches: 0.0144
trigger times: 5
Loss after 940564740 batches: 0.0142
trigger times: 6
Loss after 940693380 batches: 0.0135
trigger times: 7
Loss after 940822020 batches: 0.0133
trigger times: 8
Loss after 940950660 batches: 0.0131
trigger times: 9
Loss after 941079300 batches: 0.0127
trigger times: 10
Loss after 941207940 batches: 0.0129
trigger times: 11
Loss after 941336580 batches: 0.0124
trigger times: 12
Loss after 941465220 batches: 0.0124
trigger times: 13
Loss after 941593860 batches: 0.0119
trigger times: 14
Loss after 941722500 batches: 0.0119
trigger times: 15
Loss after 941851140 batches: 0.0118
trigger times: 16
Loss after 941979780 batches: 0.0118
trigger times: 17
Loss after 942108420 batches: 0.0116
trigger times: 18
Loss after 942237060 batches: 0.0116
trigger times: 19
Loss after 942365700 batches: 0.0115
trigger times: 20
Early stopping!
Start to test process.
Loss after 942494340 batches: 0.0117
Time to train on one home:  201.7919042110443
trigger times: 0
Loss after 942625440 batches: 0.1667
trigger times: 0
Loss after 942756540 batches: 0.0444
trigger times: 0
Loss after 942887640 batches: 0.0330
trigger times: 0
Loss after 943018740 batches: 0.0289
trigger times: 0
Loss after 943149840 batches: 0.0263
trigger times: 1
Loss after 943280940 batches: 0.0249
trigger times: 0
Loss after 943412040 batches: 0.0238
trigger times: 1
Loss after 943543140 batches: 0.0231
trigger times: 2
Loss after 943674240 batches: 0.0225
trigger times: 0
Loss after 943805340 batches: 0.0216
trigger times: 0
Loss after 943936440 batches: 0.0215
trigger times: 0
Loss after 944067540 batches: 0.0207
trigger times: 1
Loss after 944198640 batches: 0.0204
trigger times: 2
Loss after 944329740 batches: 0.0203
trigger times: 3
Loss after 944460840 batches: 0.0196
trigger times: 4
Loss after 944591940 batches: 0.0194
trigger times: 0
Loss after 944723040 batches: 0.0190
trigger times: 1
Loss after 944854140 batches: 0.0188
trigger times: 2
Loss after 944985240 batches: 0.0188
trigger times: 3
Loss after 945116340 batches: 0.0185
trigger times: 4
Loss after 945247440 batches: 0.0181
trigger times: 0
Loss after 945378540 batches: 0.0184
trigger times: 1
Loss after 945509640 batches: 0.0183
trigger times: 2
Loss after 945640740 batches: 0.0180
trigger times: 3
Loss after 945771840 batches: 0.0174
trigger times: 0
Loss after 945902940 batches: 0.0175
trigger times: 1
Loss after 946034040 batches: 0.0176
trigger times: 2
Loss after 946165140 batches: 0.0172
trigger times: 3
Loss after 946296240 batches: 0.0175
trigger times: 4
Loss after 946427340 batches: 0.0172
trigger times: 5
Loss after 946558440 batches: 0.0169
trigger times: 6
Loss after 946689540 batches: 0.0170
trigger times: 7
Loss after 946820640 batches: 0.0169
trigger times: 8
Loss after 946951740 batches: 0.0167
trigger times: 9
Loss after 947082840 batches: 0.0166
trigger times: 10
Loss after 947213940 batches: 0.0164
trigger times: 11
Loss after 947345040 batches: 0.0163
trigger times: 12
Loss after 947476140 batches: 0.0164
trigger times: 0
Loss after 947607240 batches: 0.0162
trigger times: 1
Loss after 947738340 batches: 0.0162
trigger times: 2
Loss after 947869440 batches: 0.0164
trigger times: 3
Loss after 948000540 batches: 0.0160
trigger times: 0
Loss after 948131640 batches: 0.0159
trigger times: 1
Loss after 948262740 batches: 0.0160
trigger times: 2
Loss after 948393840 batches: 0.0159
trigger times: 0
Loss after 948524940 batches: 0.0156
trigger times: 1
Loss after 948656040 batches: 0.0157
trigger times: 2
Loss after 948787140 batches: 0.0159
trigger times: 0
Loss after 948918240 batches: 0.0155
trigger times: 1
Loss after 949049340 batches: 0.0155
trigger times: 2
Loss after 949180440 batches: 0.0156
trigger times: 3
Loss after 949311540 batches: 0.0153
trigger times: 4
Loss after 949442640 batches: 0.0154
trigger times: 5
Loss after 949573740 batches: 0.0153
trigger times: 0
Loss after 949704840 batches: 0.0150
trigger times: 1
Loss after 949835940 batches: 0.0152
trigger times: 2
Loss after 949967040 batches: 0.0151
trigger times: 3
Loss after 950098140 batches: 0.0150
trigger times: 4
Loss after 950229240 batches: 0.0150
trigger times: 5
Loss after 950360340 batches: 0.0150
trigger times: 6
Loss after 950491440 batches: 0.0146
trigger times: 7
Loss after 950622540 batches: 0.0150
trigger times: 8
Loss after 950753640 batches: 0.0149
trigger times: 9
Loss after 950884740 batches: 0.0147
trigger times: 10
Loss after 951015840 batches: 0.0145
trigger times: 11
Loss after 951146940 batches: 0.0149
trigger times: 12
Loss after 951278040 batches: 0.0147
trigger times: 13
Loss after 951409140 batches: 0.0145
trigger times: 14
Loss after 951540240 batches: 0.0144
trigger times: 15
Loss after 951671340 batches: 0.0146
trigger times: 16
Loss after 951802440 batches: 0.0147
trigger times: 17
Loss after 951933540 batches: 0.0143
trigger times: 18
Loss after 952064640 batches: 0.0145
trigger times: 19
Loss after 952195740 batches: 0.0142
trigger times: 20
Early stopping!
Start to test process.
Loss after 952326840 batches: 0.0142
Time to train on one home:  548.8012368679047
trigger times: 0
Loss after 952457940 batches: 0.1696
trigger times: 0
Loss after 952589040 batches: 0.0620
trigger times: 0
Loss after 952720140 batches: 0.0454
trigger times: 1
Loss after 952851240 batches: 0.0358
trigger times: 2
Loss after 952982340 batches: 0.0310
trigger times: 3
Loss after 953113440 batches: 0.0282
trigger times: 0
Loss after 953244540 batches: 0.0286
trigger times: 1
Loss after 953375640 batches: 0.0280
trigger times: 2
Loss after 953506740 batches: 0.0261
trigger times: 3
Loss after 953637840 batches: 0.0249
trigger times: 4
Loss after 953768940 batches: 0.0244
trigger times: 0
Loss after 953900040 batches: 0.0236
trigger times: 0
Loss after 954031140 batches: 0.0236
trigger times: 1
Loss after 954162240 batches: 0.0250
trigger times: 2
Loss after 954293340 batches: 0.0243
trigger times: 3
Loss after 954424440 batches: 0.0239
trigger times: 4
Loss after 954555540 batches: 0.0220
trigger times: 0
Loss after 954686640 batches: 0.0220
trigger times: 1
Loss after 954817740 batches: 0.0224
trigger times: 2
Loss after 954948840 batches: 0.0211
trigger times: 3
Loss after 955079940 batches: 0.0210
trigger times: 4
Loss after 955211040 batches: 0.0213
trigger times: 5
Loss after 955342140 batches: 0.0211
trigger times: 0
Loss after 955473240 batches: 0.0211
trigger times: 1
Loss after 955604340 batches: 0.0203
trigger times: 2
Loss after 955735440 batches: 0.0206
trigger times: 3
Loss after 955866540 batches: 0.0208
trigger times: 4
Loss after 955997640 batches: 0.0206
trigger times: 5
Loss after 956128740 batches: 0.0197
trigger times: 6
Loss after 956259840 batches: 0.0211
trigger times: 0
Loss after 956390940 batches: 0.0205
trigger times: 1
Loss after 956522040 batches: 0.0201
trigger times: 2
Loss after 956653140 batches: 0.0187
trigger times: 3
Loss after 956784240 batches: 0.0196
trigger times: 4
Loss after 956915340 batches: 0.0206
trigger times: 5
Loss after 957046440 batches: 0.0201
trigger times: 6
Loss after 957177540 batches: 0.0190
trigger times: 7
Loss after 957308640 batches: 0.0197
trigger times: 0
Loss after 957439740 batches: 0.0203
trigger times: 1
Loss after 957570840 batches: 0.0193
trigger times: 2
Loss after 957701940 batches: 0.0188
trigger times: 3
Loss after 957833040 batches: 0.0194
trigger times: 4
Loss after 957964140 batches: 0.0189
trigger times: 0
Loss after 958095240 batches: 0.0190
trigger times: 1
Loss after 958226340 batches: 0.0186
trigger times: 2
Loss after 958357440 batches: 0.0188
trigger times: 3
Loss after 958488540 batches: 0.0185
trigger times: 0
Loss after 958619640 batches: 0.0188
trigger times: 1
Loss after 958750740 batches: 0.0187
trigger times: 0
Loss after 958881840 batches: 0.0176
trigger times: 1
Loss after 959012940 batches: 0.0178
trigger times: 0
Loss after 959144040 batches: 0.0190
trigger times: 1
Loss after 959275140 batches: 0.0182
trigger times: 0
Loss after 959406240 batches: 0.0182
trigger times: 1
Loss after 959537340 batches: 0.0174
trigger times: 2
Loss after 959668440 batches: 0.0173
trigger times: 3
Loss after 959799540 batches: 0.0176
trigger times: 4
Loss after 959930640 batches: 0.0171
trigger times: 5
Loss after 960061740 batches: 0.0178
trigger times: 6
Loss after 960192840 batches: 0.0174
trigger times: 7
Loss after 960323940 batches: 0.0180
trigger times: 8
Loss after 960455040 batches: 0.0186
trigger times: 9
Loss after 960586140 batches: 0.0178
trigger times: 10
Loss after 960717240 batches: 0.0176
trigger times: 11
Loss after 960848340 batches: 0.0167
trigger times: 12
Loss after 960979440 batches: 0.0169
trigger times: 13
Loss after 961110540 batches: 0.0171
trigger times: 14
Loss after 961241640 batches: 0.0174
trigger times: 15
Loss after 961372740 batches: 0.0168
trigger times: 16
Loss after 961503840 batches: 0.0175
trigger times: 17
Loss after 961634940 batches: 0.0177
trigger times: 18
Loss after 961766040 batches: 0.0169
trigger times: 19
Loss after 961897140 batches: 0.0161
trigger times: 20
Early stopping!
Start to test process.
Loss after 962028240 batches: 0.0169
Time to train on one home:  541.614307641983
trigger times: 0
Loss after 962159340 batches: 0.0657
trigger times: 0
Loss after 962290440 batches: 0.0198
trigger times: 0
Loss after 962421540 batches: 0.0148
trigger times: 0
Loss after 962552640 batches: 0.0130
trigger times: 1
Loss after 962683740 batches: 0.0123
trigger times: 2
Loss after 962814840 batches: 0.0113
trigger times: 3
Loss after 962945940 batches: 0.0110
trigger times: 0
Loss after 963077040 batches: 0.0102
trigger times: 1
Loss after 963208140 batches: 0.0102
trigger times: 2
Loss after 963339240 batches: 0.0098
trigger times: 3
Loss after 963470340 batches: 0.0095
trigger times: 4
Loss after 963601440 batches: 0.0092
trigger times: 5
Loss after 963732540 batches: 0.0089
trigger times: 6
Loss after 963863640 batches: 0.0091
trigger times: 7
Loss after 963994740 batches: 0.0088
trigger times: 8
Loss after 964125840 batches: 0.0086
trigger times: 9
Loss after 964256940 batches: 0.0087
trigger times: 10
Loss after 964388040 batches: 0.0085
trigger times: 11
Loss after 964519140 batches: 0.0083
trigger times: 12
Loss after 964650240 batches: 0.0083
trigger times: 13
Loss after 964781340 batches: 0.0080
trigger times: 14
Loss after 964912440 batches: 0.0081
trigger times: 15
Loss after 965043540 batches: 0.0080
trigger times: 16
Loss after 965174640 batches: 0.0078
trigger times: 17
Loss after 965305740 batches: 0.0080
trigger times: 18
Loss after 965436840 batches: 0.0078
trigger times: 19
Loss after 965567940 batches: 0.0076
trigger times: 20
Early stopping!
Start to test process.
Loss after 965699040 batches: 0.0075
Time to train on one home:  212.51884746551514
trigger times: 0
Loss after 965777640 batches: 0.1918
trigger times: 0
Loss after 965856240 batches: 0.0489
trigger times: 1
Loss after 965934840 batches: 0.0330
trigger times: 0
Loss after 966013440 batches: 0.0275
trigger times: 0
Loss after 966092040 batches: 0.0245
trigger times: 0
Loss after 966170640 batches: 0.0231
trigger times: 1
Loss after 966249240 batches: 0.0221
trigger times: 2
Loss after 966327840 batches: 0.0212
trigger times: 0
Loss after 966406440 batches: 0.0209
trigger times: 1
Loss after 966485040 batches: 0.0197
trigger times: 2
Loss after 966563640 batches: 0.0194
trigger times: 3
Loss after 966642240 batches: 0.0189
trigger times: 4
Loss after 966720840 batches: 0.0186
trigger times: 5
Loss after 966799440 batches: 0.0182
trigger times: 6
Loss after 966878040 batches: 0.0174
trigger times: 7
Loss after 966956640 batches: 0.0173
trigger times: 0
Loss after 967035240 batches: 0.0171
trigger times: 0
Loss after 967113840 batches: 0.0169
trigger times: 0
Loss after 967192440 batches: 0.0164
trigger times: 1
Loss after 967271040 batches: 0.0168
trigger times: 2
Loss after 967349640 batches: 0.0163
trigger times: 3
Loss after 967428240 batches: 0.0159
trigger times: 4
Loss after 967506840 batches: 0.0159
trigger times: 5
Loss after 967585440 batches: 0.0155
trigger times: 6
Loss after 967664040 batches: 0.0153
trigger times: 7
Loss after 967742640 batches: 0.0151
trigger times: 8
Loss after 967821240 batches: 0.0146
trigger times: 9
Loss after 967899840 batches: 0.0150
trigger times: 0
Loss after 967978440 batches: 0.0148
trigger times: 1
Loss after 968057040 batches: 0.0145
trigger times: 2
Loss after 968135640 batches: 0.0146
trigger times: 3
Loss after 968214240 batches: 0.0146
trigger times: 4
Loss after 968292840 batches: 0.0141
trigger times: 5
Loss after 968371440 batches: 0.0144
trigger times: 0
Loss after 968450040 batches: 0.0145
trigger times: 1
Loss after 968528640 batches: 0.0143
trigger times: 2
Loss after 968607240 batches: 0.0139
trigger times: 3
Loss after 968685840 batches: 0.0137
trigger times: 4
Loss after 968764440 batches: 0.0137
trigger times: 5
Loss after 968843040 batches: 0.0134
trigger times: 6
Loss after 968921640 batches: 0.0134
trigger times: 7
Loss after 969000240 batches: 0.0134
trigger times: 8
Loss after 969078840 batches: 0.0130
trigger times: 9
Loss after 969157440 batches: 0.0133
trigger times: 0
Loss after 969236040 batches: 0.0134
trigger times: 1
Loss after 969314640 batches: 0.0133
trigger times: 2
Loss after 969393240 batches: 0.0135
trigger times: 3
Loss after 969471840 batches: 0.0130
trigger times: 4
Loss after 969550440 batches: 0.0129
trigger times: 5
Loss after 969629040 batches: 0.0128
trigger times: 6
Loss after 969707640 batches: 0.0125
trigger times: 7
Loss after 969786240 batches: 0.0128
trigger times: 8
Loss after 969864840 batches: 0.0126
trigger times: 9
Loss after 969943440 batches: 0.0126
trigger times: 10
Loss after 970022040 batches: 0.0125
trigger times: 11
Loss after 970100640 batches: 0.0125
trigger times: 12
Loss after 970179240 batches: 0.0121
trigger times: 13
Loss after 970257840 batches: 0.0124
trigger times: 14
Loss after 970336440 batches: 0.0122
trigger times: 15
Loss after 970415040 batches: 0.0124
trigger times: 16
Loss after 970493640 batches: 0.0119
trigger times: 17
Loss after 970572240 batches: 0.0122
trigger times: 18
Loss after 970650840 batches: 0.0123
trigger times: 19
Loss after 970729440 batches: 0.0124
trigger times: 20
Early stopping!
Start to test process.
Loss after 970808040 batches: 0.0122
Time to train on one home:  311.9249289035797
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097, 0.016851266692967333, 0.01539797175087967, 0.015622467135075917, 0.014339386766728896, 0.015215330347522192, 0.014610499733001695, 0.014354899421970126, 0.01333385277873801, 0.01326294574980891, 0.013142881573656695]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983], [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013], [0.5549709763791826, 0.4001270641513627, 0.48658498833859326, 1.0394903420759227, 0.4914126622529051, 24.558331909615507, 1517.0215], [0.5461304287115732, 0.4097276108845518, 0.4876006728779417, 1.0136453079994863, 0.48354794633174836, 23.94773371608454, 1492.7424], [0.5402040051089393, 0.41609081728992114, 0.4927051878338842, 1.0002093606660662, 0.47833524208513384, 23.630304644569275, 1476.6505], [0.5334714584880405, 0.4234695591708837, 0.501786021212506, 1.0005226338786515, 0.47229061667346856, 23.637705836501173, 1457.9905], [0.5237383676899804, 0.43399432408412464, 0.5083585548121925, 0.9778479298034527, 0.46366878622151636, 23.10200782557115, 1431.3744], [0.5306309130456712, 0.42652222327915057, 0.5055172115402072, 1.0003700248842091, 0.46978989075842903, 23.63410039430878, 1450.2706], [0.5348766446113586, 0.42189465272218896, 0.49757962064175115, 0.9924071418125425, 0.47358077151210515, 23.445974427653898, 1461.9733], [0.5336118737856547, 0.4232833712453734, 0.5012302308952711, 0.9933548830158522, 0.47244314081431027, 23.468365153275084, 1458.4613], [0.5353295041455163, 0.4214272084397156, 0.5019043823357494, 0.9976955154496634, 0.4739636993382793, 23.570914150309818, 1463.1555]]
Round_17_results:  [0.5353295041455163, 0.4214272084397156, 0.5019043823357494, 0.9976955154496634, 0.4739636993382793, 23.570914150309818, 1463.1555]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 7896 < 7897; dropping {'Training_Loss': 0.10770148710119275, 'Validation_Loss': 0.18032289296388626, 'Training_R2': 0.8914556645287042, 'Validation_R2': 0.8325961804428399, 'Training_F1': 0.8292984725578916, 'Validation_F1': 0.7409186035025612, 'Training_NEP': 0.3411695680984958, 'Validation_NEP': 0.48161115467477905, 'Training_NDE': 0.0814861593032423, 'Validation_NDE': 0.13331157735203936, 'Training_MAE': 11.29885948320821, 'Validation_MAE': 13.207775943467267, 'Training_MSE': 358.52628, 'Validation_MSE': 492.31595}.
trigger times: 0
Loss after 970939140 batches: 0.1077
trigger times: 0
Loss after 971070240 batches: 0.0255
trigger times: 0
Loss after 971201340 batches: 0.0187
trigger times: 1
Loss after 971332440 batches: 0.0161
trigger times: 0
Loss after 971463540 batches: 0.0154
trigger times: 1
Loss after 971594640 batches: 0.0142
trigger times: 2
Loss after 971725740 batches: 0.0134
trigger times: 3
Loss after 971856840 batches: 0.0128
trigger times: 4
Loss after 971987940 batches: 0.0126
trigger times: 5
Loss after 972119040 batches: 0.0118
trigger times: 6
Loss after 972250140 batches: 0.0116
trigger times: 7
Loss after 972381240 batches: 0.0116
trigger times: 8
Loss after 972512340 batches: 0.0113
trigger times: 9
Loss after 972643440 batches: 0.0112
trigger times: 10
Loss after 972774540 batches: 0.0111
trigger times: 11
Loss after 972905640 batches: 0.0106
trigger times: 12
Loss after 973036740 batches: 0.0105
trigger times: 13
Loss after 973167840 batches: 0.0104
trigger times: 14
Loss after 973298940 batches: 0.0102
trigger times: 15
Loss after 973430040 batches: 0.0102
trigger times: 16
Loss after 973561140 batches: 0.0099
trigger times: 17
Loss after 973692240 batches: 0.0099
trigger times: 18
Loss after 973823340 batches: 0.0098
trigger times: 19
Loss after 973954440 batches: 0.0098
trigger times: 20
Early stopping!
Start to test process.
Loss after 974085540 batches: 0.0097
Time to train on one home:  190.89485549926758
trigger times: 0
Loss after 974188140 batches: 0.2109
trigger times: 0
Loss after 974290740 batches: 0.0674
trigger times: 1
Loss after 974393340 batches: 0.0456
trigger times: 2
Loss after 974495940 batches: 0.0358
trigger times: 3
Loss after 974598540 batches: 0.0307
trigger times: 4
Loss after 974701140 batches: 0.0293
trigger times: 5
Loss after 974803740 batches: 0.0322
trigger times: 6
Loss after 974906340 batches: 0.0287
trigger times: 7
Loss after 975008940 batches: 0.0257
trigger times: 8
Loss after 975111540 batches: 0.0245
trigger times: 9
Loss after 975214140 batches: 0.0241
trigger times: 10
Loss after 975316740 batches: 0.0236
trigger times: 11
Loss after 975419340 batches: 0.0260
trigger times: 12
Loss after 975521940 batches: 0.0226
trigger times: 0
Loss after 975624540 batches: 0.0209
trigger times: 1
Loss after 975727140 batches: 0.0219
trigger times: 2
Loss after 975829740 batches: 0.0238
trigger times: 3
Loss after 975932340 batches: 0.0215
trigger times: 4
Loss after 976034940 batches: 0.0244
trigger times: 0
Loss after 976137540 batches: 0.0256
trigger times: 1
Loss after 976240140 batches: 0.0213
trigger times: 2
Loss after 976342740 batches: 0.0197
trigger times: 3
Loss after 976445340 batches: 0.0191
trigger times: 4
Loss after 976547940 batches: 0.0190
trigger times: 5
Loss after 976650540 batches: 0.0185
trigger times: 6
Loss after 976753140 batches: 0.0185
trigger times: 0
Loss after 976855740 batches: 0.0185
trigger times: 1
Loss after 976958340 batches: 0.0181
trigger times: 2
Loss after 977060940 batches: 0.0198
trigger times: 3
Loss after 977163540 batches: 0.0175
trigger times: 4
Loss after 977266140 batches: 0.0172
trigger times: 5
Loss after 977368740 batches: 0.0174
trigger times: 6
Loss after 977471340 batches: 0.0171
trigger times: 7
Loss after 977573940 batches: 0.0170
trigger times: 8
Loss after 977676540 batches: 0.0177
trigger times: 9
Loss after 977779140 batches: 0.0181
trigger times: 10
Loss after 977881740 batches: 0.0187
trigger times: 11
Loss after 977984340 batches: 0.0181
trigger times: 12
Loss after 978086940 batches: 0.0202
trigger times: 13
Loss after 978189540 batches: 0.0182
trigger times: 14
Loss after 978292140 batches: 0.0179
trigger times: 15
Loss after 978394740 batches: 0.0173
trigger times: 16
Loss after 978497340 batches: 0.0164
trigger times: 17
Loss after 978599940 batches: 0.0167
trigger times: 18
Loss after 978702540 batches: 0.0164
trigger times: 19
Loss after 978805140 batches: 0.0171
trigger times: 20
Early stopping!
Start to test process.
Loss after 978907740 batches: 0.0167
Time to train on one home:  281.3414213657379
trigger times: 0
Loss after 979038840 batches: 0.1159
trigger times: 1
Loss after 979169940 batches: 0.0388
trigger times: 0
Loss after 979301040 batches: 0.0297
trigger times: 1
Loss after 979432140 batches: 0.0263
trigger times: 2
Loss after 979563240 batches: 0.0242
trigger times: 3
Loss after 979694340 batches: 0.0228
trigger times: 4
Loss after 979825440 batches: 0.0213
trigger times: 0
Loss after 979956540 batches: 0.0208
trigger times: 1
Loss after 980087640 batches: 0.0201
trigger times: 2
Loss after 980218740 batches: 0.0196
trigger times: 3
Loss after 980349840 batches: 0.0191
trigger times: 4
Loss after 980480940 batches: 0.0183
trigger times: 5
Loss after 980612040 batches: 0.0179
trigger times: 6
Loss after 980743140 batches: 0.0177
trigger times: 7
Loss after 980874240 batches: 0.0173
trigger times: 8
Loss after 981005340 batches: 0.0169
trigger times: 9
Loss after 981136440 batches: 0.0171
trigger times: 10
Loss after 981267540 batches: 0.0165
trigger times: 11
Loss after 981398640 batches: 0.0161
trigger times: 12
Loss after 981529740 batches: 0.0160
trigger times: 13
Loss after 981660840 batches: 0.0159
trigger times: 14
Loss after 981791940 batches: 0.0157
trigger times: 15
Loss after 981923040 batches: 0.0155
trigger times: 16
Loss after 982054140 batches: 0.0153
trigger times: 17
Loss after 982185240 batches: 0.0150
trigger times: 18
Loss after 982316340 batches: 0.0149
trigger times: 19
Loss after 982447440 batches: 0.0150
trigger times: 20
Early stopping!
Start to test process.
Loss after 982578540 batches: 0.0147
Time to train on one home:  213.76239919662476
trigger times: 0
Loss after 982709640 batches: 0.1556
trigger times: 0
Loss after 982840740 batches: 0.0439
trigger times: 0
Loss after 982971840 batches: 0.0336
trigger times: 1
Loss after 983102940 batches: 0.0300
trigger times: 0
Loss after 983234040 batches: 0.0279
trigger times: 1
Loss after 983365140 batches: 0.0261
trigger times: 2
Loss after 983496240 batches: 0.0253
trigger times: 3
Loss after 983627340 batches: 0.0240
trigger times: 0
Loss after 983758440 batches: 0.0234
trigger times: 1
Loss after 983889540 batches: 0.0228
trigger times: 2
Loss after 984020640 batches: 0.0221
trigger times: 0
Loss after 984151740 batches: 0.0217
trigger times: 1
Loss after 984282840 batches: 0.0213
trigger times: 2
Loss after 984413940 batches: 0.0208
trigger times: 0
Loss after 984545040 batches: 0.0204
trigger times: 1
Loss after 984676140 batches: 0.0202
trigger times: 2
Loss after 984807240 batches: 0.0199
trigger times: 0
Loss after 984938340 batches: 0.0195
trigger times: 1
Loss after 985069440 batches: 0.0194
trigger times: 2
Loss after 985200540 batches: 0.0194
trigger times: 3
Loss after 985331640 batches: 0.0192
trigger times: 4
Loss after 985462740 batches: 0.0188
trigger times: 5
Loss after 985593840 batches: 0.0187
trigger times: 6
Loss after 985724940 batches: 0.0184
trigger times: 7
Loss after 985856040 batches: 0.0184
trigger times: 8
Loss after 985987140 batches: 0.0180
trigger times: 9
Loss after 986118240 batches: 0.0179
trigger times: 10
Loss after 986249340 batches: 0.0181
trigger times: 11
Loss after 986380440 batches: 0.0178
trigger times: 12
Loss after 986511540 batches: 0.0179
trigger times: 13
Loss after 986642640 batches: 0.0177
trigger times: 14
Loss after 986773740 batches: 0.0176
trigger times: 15
Loss after 986904840 batches: 0.0176
trigger times: 16
Loss after 987035940 batches: 0.0172
trigger times: 17
Loss after 987167040 batches: 0.0171
trigger times: 0
Loss after 987298140 batches: 0.0171
trigger times: 1
Loss after 987429240 batches: 0.0171
trigger times: 2
Loss after 987560340 batches: 0.0170
trigger times: 3
Loss after 987691440 batches: 0.0166
trigger times: 4
Loss after 987822540 batches: 0.0167
trigger times: 5
Loss after 987953640 batches: 0.0167
trigger times: 6
Loss after 988084740 batches: 0.0167
trigger times: 0
Loss after 988215840 batches: 0.0168
trigger times: 1
Loss after 988346940 batches: 0.0164
trigger times: 2
Loss after 988478040 batches: 0.0164
trigger times: 3
Loss after 988609140 batches: 0.0166
trigger times: 4
Loss after 988740240 batches: 0.0164
trigger times: 5
Loss after 988871340 batches: 0.0165
trigger times: 6
Loss after 989002440 batches: 0.0160
trigger times: 7
Loss after 989133540 batches: 0.0161
trigger times: 8
Loss after 989264640 batches: 0.0159
trigger times: 0
Loss after 989395740 batches: 0.0158
trigger times: 1
Loss after 989526840 batches: 0.0159
trigger times: 2
Loss after 989657940 batches: 0.0158
trigger times: 3
Loss after 989789040 batches: 0.0156
trigger times: 4
Loss after 989920140 batches: 0.0155
trigger times: 0
Loss after 990051240 batches: 0.0157
trigger times: 1
Loss after 990182340 batches: 0.0155
trigger times: 2
Loss after 990313440 batches: 0.0155
trigger times: 3
Loss after 990444540 batches: 0.0154
trigger times: 4
Loss after 990575640 batches: 0.0154
trigger times: 5
Loss after 990706740 batches: 0.0155
trigger times: 6
Loss after 990837840 batches: 0.0153
trigger times: 0
Loss after 990968940 batches: 0.0153
trigger times: 1
Loss after 991100040 batches: 0.0152
trigger times: 2
Loss after 991231140 batches: 0.0151
trigger times: 3
Loss after 991362240 batches: 0.0148
trigger times: 4
Loss after 991493340 batches: 0.0150
trigger times: 5
Loss after 991624440 batches: 0.0150
trigger times: 6
Loss after 991755540 batches: 0.0150
trigger times: 7
Loss after 991886640 batches: 0.0150
trigger times: 8
Loss after 992017740 batches: 0.0150
trigger times: 9
Loss after 992148840 batches: 0.0148
trigger times: 10
Loss after 992279940 batches: 0.0147
trigger times: 11
Loss after 992411040 batches: 0.0148
trigger times: 12
Loss after 992542140 batches: 0.0147
trigger times: 13
Loss after 992673240 batches: 0.0147
trigger times: 14
Loss after 992804340 batches: 0.0147
trigger times: 15
Loss after 992935440 batches: 0.0145
trigger times: 16
Loss after 993066540 batches: 0.0145
trigger times: 17
Loss after 993197640 batches: 0.0144
trigger times: 18
Loss after 993328740 batches: 0.0146
trigger times: 19
Loss after 993459840 batches: 0.0147
trigger times: 20
Early stopping!
Start to test process.
Loss after 993590940 batches: 0.0146
Time to train on one home:  615.1434576511383
trigger times: 0
Loss after 993719580 batches: 0.1055
trigger times: 0
Loss after 993848220 batches: 0.0299
trigger times: 0
Loss after 993976860 batches: 0.0223
trigger times: 0
Loss after 994105500 batches: 0.0193
trigger times: 1
Loss after 994234140 batches: 0.0178
trigger times: 2
Loss after 994362780 batches: 0.0173
trigger times: 3
Loss after 994491420 batches: 0.0162
trigger times: 4
Loss after 994620060 batches: 0.0157
trigger times: 0
Loss after 994748700 batches: 0.0149
trigger times: 0
Loss after 994877340 batches: 0.0145
trigger times: 1
Loss after 995005980 batches: 0.0143
trigger times: 0
Loss after 995134620 batches: 0.0138
trigger times: 1
Loss after 995263260 batches: 0.0136
trigger times: 2
Loss after 995391900 batches: 0.0135
trigger times: 3
Loss after 995520540 batches: 0.0133
trigger times: 4
Loss after 995649180 batches: 0.0131
trigger times: 5
Loss after 995777820 batches: 0.0128
trigger times: 6
Loss after 995906460 batches: 0.0124
trigger times: 7
Loss after 996035100 batches: 0.0128
trigger times: 8
Loss after 996163740 batches: 0.0124
trigger times: 9
Loss after 996292380 batches: 0.0122
trigger times: 10
Loss after 996421020 batches: 0.0120
trigger times: 11
Loss after 996549660 batches: 0.0117
trigger times: 12
Loss after 996678300 batches: 0.0116
trigger times: 13
Loss after 996806940 batches: 0.0115
trigger times: 14
Loss after 996935580 batches: 0.0115
trigger times: 15
Loss after 997064220 batches: 0.0114
trigger times: 16
Loss after 997192860 batches: 0.0109
trigger times: 17
Loss after 997321500 batches: 0.0111
trigger times: 18
Loss after 997450140 batches: 0.0111
trigger times: 19
Loss after 997578780 batches: 0.0110
trigger times: 20
Early stopping!
Start to test process.
Loss after 997707420 batches: 0.0110
Time to train on one home:  236.63537406921387
trigger times: 0
Loss after 997838520 batches: 0.1666
trigger times: 0
Loss after 997969620 batches: 0.0415
trigger times: 0
Loss after 998100720 batches: 0.0317
trigger times: 0
Loss after 998231820 batches: 0.0279
trigger times: 0
Loss after 998362920 batches: 0.0261
trigger times: 1
Loss after 998494020 batches: 0.0248
trigger times: 2
Loss after 998625120 batches: 0.0235
trigger times: 0
Loss after 998756220 batches: 0.0224
trigger times: 1
Loss after 998887320 batches: 0.0219
trigger times: 0
Loss after 999018420 batches: 0.0214
trigger times: 1
Loss after 999149520 batches: 0.0213
trigger times: 2
Loss after 999280620 batches: 0.0205
trigger times: 3
Loss after 999411720 batches: 0.0199
trigger times: 4
Loss after 999542820 batches: 0.0195
trigger times: 5
Loss after 999673920 batches: 0.0194
trigger times: 6
Loss after 999805020 batches: 0.0191
trigger times: 7
Loss after 999936120 batches: 0.0188
trigger times: 8
Loss after 1000067220 batches: 0.0189
trigger times: 9
Loss after 1000198320 batches: 0.0184
trigger times: 10
Loss after 1000329420 batches: 0.0182
trigger times: 0
Loss after 1000460520 batches: 0.0181
trigger times: 0
Loss after 1000591620 batches: 0.0181
trigger times: 1
Loss after 1000722720 batches: 0.0179
trigger times: 0
Loss after 1000853820 batches: 0.0174
trigger times: 1
Loss after 1000984920 batches: 0.0181
trigger times: 2
Loss after 1001116020 batches: 0.0175
trigger times: 3
Loss after 1001247120 batches: 0.0173
trigger times: 4
Loss after 1001378220 batches: 0.0170
trigger times: 5
Loss after 1001509320 batches: 0.0172
trigger times: 6
Loss after 1001640420 batches: 0.0170
trigger times: 7
Loss after 1001771520 batches: 0.0168
trigger times: 8
Loss after 1001902620 batches: 0.0167
trigger times: 9
Loss after 1002033720 batches: 0.0165
trigger times: 10
Loss after 1002164820 batches: 0.0165
trigger times: 11
Loss after 1002295920 batches: 0.0165
trigger times: 12
Loss after 1002427020 batches: 0.0163
trigger times: 13
Loss after 1002558120 batches: 0.0164
trigger times: 0
Loss after 1002689220 batches: 0.0160
trigger times: 1
Loss after 1002820320 batches: 0.0159
trigger times: 2
Loss after 1002951420 batches: 0.0161
trigger times: 3
Loss after 1003082520 batches: 0.0158
trigger times: 4
Loss after 1003213620 batches: 0.0157
trigger times: 5
Loss after 1003344720 batches: 0.0158
trigger times: 6
Loss after 1003475820 batches: 0.0156
trigger times: 7
Loss after 1003606920 batches: 0.0155
trigger times: 8
Loss after 1003738020 batches: 0.0155
trigger times: 9
Loss after 1003869120 batches: 0.0156
trigger times: 10
Loss after 1004000220 batches: 0.0154
trigger times: 0
Loss after 1004131320 batches: 0.0155
trigger times: 1
Loss after 1004262420 batches: 0.0152
trigger times: 2
Loss after 1004393520 batches: 0.0154
trigger times: 3
Loss after 1004524620 batches: 0.0153
trigger times: 4
Loss after 1004655720 batches: 0.0151
trigger times: 5
Loss after 1004786820 batches: 0.0151
trigger times: 6
Loss after 1004917920 batches: 0.0152
trigger times: 7
Loss after 1005049020 batches: 0.0151
trigger times: 8
Loss after 1005180120 batches: 0.0149
trigger times: 9
Loss after 1005311220 batches: 0.0148
trigger times: 10
Loss after 1005442320 batches: 0.0150
trigger times: 11
Loss after 1005573420 batches: 0.0150
trigger times: 12
Loss after 1005704520 batches: 0.0146
trigger times: 13
Loss after 1005835620 batches: 0.0147
trigger times: 14
Loss after 1005966720 batches: 0.0148
trigger times: 15
Loss after 1006097820 batches: 0.0146
trigger times: 16
Loss after 1006228920 batches: 0.0144
trigger times: 17
Loss after 1006360020 batches: 0.0145
trigger times: 18
Loss after 1006491120 batches: 0.0145
trigger times: 19
Loss after 1006622220 batches: 0.0145
trigger times: 20
Early stopping!
Start to test process.
Loss after 1006753320 batches: 0.0146
Time to train on one home:  507.2952125072479
trigger times: 0
Loss after 1006884420 batches: 0.1597
trigger times: 0
Loss after 1007015520 batches: 0.0633
trigger times: 1
Loss after 1007146620 batches: 0.0434
trigger times: 2
Loss after 1007277720 batches: 0.0341
trigger times: 3
Loss after 1007408820 batches: 0.0327
trigger times: 0
Loss after 1007539920 batches: 0.0295
trigger times: 1
Loss after 1007671020 batches: 0.0279
trigger times: 2
Loss after 1007802120 batches: 0.0267
trigger times: 3
Loss after 1007933220 batches: 0.0253
trigger times: 4
Loss after 1008064320 batches: 0.0241
trigger times: 5
Loss after 1008195420 batches: 0.0245
trigger times: 0
Loss after 1008326520 batches: 0.0236
trigger times: 1
Loss after 1008457620 batches: 0.0238
trigger times: 2
Loss after 1008588720 batches: 0.0230
trigger times: 3
Loss after 1008719820 batches: 0.0228
trigger times: 4
Loss after 1008850920 batches: 0.0224
trigger times: 5
Loss after 1008982020 batches: 0.0224
trigger times: 6
Loss after 1009113120 batches: 0.0230
trigger times: 7
Loss after 1009244220 batches: 0.0223
trigger times: 0
Loss after 1009375320 batches: 0.0223
trigger times: 1
Loss after 1009506420 batches: 0.0205
trigger times: 2
Loss after 1009637520 batches: 0.0207
trigger times: 3
Loss after 1009768620 batches: 0.0209
trigger times: 4
Loss after 1009899720 batches: 0.0211
trigger times: 5
Loss after 1010030820 batches: 0.0212
trigger times: 6
Loss after 1010161920 batches: 0.0202
trigger times: 7
Loss after 1010293020 batches: 0.0197
trigger times: 8
Loss after 1010424120 batches: 0.0201
trigger times: 9
Loss after 1010555220 batches: 0.0201
trigger times: 10
Loss after 1010686320 batches: 0.0195
trigger times: 0
Loss after 1010817420 batches: 0.0194
trigger times: 0
Loss after 1010948520 batches: 0.0188
trigger times: 1
Loss after 1011079620 batches: 0.0186
trigger times: 2
Loss after 1011210720 batches: 0.0185
trigger times: 0
Loss after 1011341820 batches: 0.0190
trigger times: 1
Loss after 1011472920 batches: 0.0189
trigger times: 2
Loss after 1011604020 batches: 0.0194
trigger times: 3
Loss after 1011735120 batches: 0.0195
trigger times: 4
Loss after 1011866220 batches: 0.0190
trigger times: 5
Loss after 1011997320 batches: 0.0183
trigger times: 6
Loss after 1012128420 batches: 0.0184
trigger times: 7
Loss after 1012259520 batches: 0.0190
trigger times: 8
Loss after 1012390620 batches: 0.0190
trigger times: 9
Loss after 1012521720 batches: 0.0186
trigger times: 0
Loss after 1012652820 batches: 0.0183
trigger times: 1
Loss after 1012783920 batches: 0.0173
trigger times: 2
Loss after 1012915020 batches: 0.0181
trigger times: 3
Loss after 1013046120 batches: 0.0180
trigger times: 4
Loss after 1013177220 batches: 0.0178
trigger times: 0
Loss after 1013308320 batches: 0.0181
trigger times: 1
Loss after 1013439420 batches: 0.0183
trigger times: 2
Loss after 1013570520 batches: 0.0173
trigger times: 3
Loss after 1013701620 batches: 0.0177
trigger times: 4
Loss after 1013832720 batches: 0.0173
trigger times: 5
Loss after 1013963820 batches: 0.0174
trigger times: 6
Loss after 1014094920 batches: 0.0183
trigger times: 7
Loss after 1014226020 batches: 0.0174
trigger times: 8
Loss after 1014357120 batches: 0.0178
trigger times: 9
Loss after 1014488220 batches: 0.0179
trigger times: 10
Loss after 1014619320 batches: 0.0174
trigger times: 0
Loss after 1014750420 batches: 0.0176
trigger times: 0
Loss after 1014881520 batches: 0.0181
trigger times: 1
Loss after 1015012620 batches: 0.0177
trigger times: 2
Loss after 1015143720 batches: 0.0168
trigger times: 3
Loss after 1015274820 batches: 0.0170
trigger times: 4
Loss after 1015405920 batches: 0.0166
trigger times: 5
Loss after 1015537020 batches: 0.0170
trigger times: 6
Loss after 1015668120 batches: 0.0167
trigger times: 7
Loss after 1015799220 batches: 0.0177
trigger times: 8
Loss after 1015930320 batches: 0.0165
trigger times: 9
Loss after 1016061420 batches: 0.0165
trigger times: 10
Loss after 1016192520 batches: 0.0171
trigger times: 11
Loss after 1016323620 batches: 0.0171
trigger times: 12
Loss after 1016454720 batches: 0.0165
trigger times: 13
Loss after 1016585820 batches: 0.0164
trigger times: 14
Loss after 1016716920 batches: 0.0163
trigger times: 15
Loss after 1016848020 batches: 0.0166
trigger times: 16
Loss after 1016979120 batches: 0.0165
trigger times: 17
Loss after 1017110220 batches: 0.0171
trigger times: 18
Loss after 1017241320 batches: 0.0161
trigger times: 19
Loss after 1017372420 batches: 0.0165
trigger times: 20
Early stopping!
Start to test process.
Loss after 1017503520 batches: 0.0162
Time to train on one home:  600.5467965602875
trigger times: 0
Loss after 1017634620 batches: 0.0641
trigger times: 0
Loss after 1017765720 batches: 0.0187
trigger times: 1
Loss after 1017896820 batches: 0.0143
trigger times: 2
Loss after 1018027920 batches: 0.0129
trigger times: 3
Loss after 1018159020 batches: 0.0122
trigger times: 4
Loss after 1018290120 batches: 0.0114
trigger times: 5
Loss after 1018421220 batches: 0.0108
trigger times: 6
Loss after 1018552320 batches: 0.0104
trigger times: 7
Loss after 1018683420 batches: 0.0100
trigger times: 8
Loss after 1018814520 batches: 0.0097
trigger times: 9
Loss after 1018945620 batches: 0.0094
trigger times: 10
Loss after 1019076720 batches: 0.0094
trigger times: 11
Loss after 1019207820 batches: 0.0091
trigger times: 12
Loss after 1019338920 batches: 0.0089
trigger times: 13
Loss after 1019470020 batches: 0.0088
trigger times: 14
Loss after 1019601120 batches: 0.0085
trigger times: 15
Loss after 1019732220 batches: 0.0085
trigger times: 16
Loss after 1019863320 batches: 0.0084
trigger times: 17
Loss after 1019994420 batches: 0.0082
trigger times: 18
Loss after 1020125520 batches: 0.0082
trigger times: 19
Loss after 1020256620 batches: 0.0081
trigger times: 20
Early stopping!
Start to test process.
Loss after 1020387720 batches: 0.0080
Time to train on one home:  168.95807814598083
trigger times: 0
Loss after 1020466320 batches: 0.1849
trigger times: 0
Loss after 1020544920 batches: 0.0472
trigger times: 0
Loss after 1020623520 batches: 0.0317
trigger times: 1
Loss after 1020702120 batches: 0.0266
trigger times: 2
Loss after 1020780720 batches: 0.0250
trigger times: 3
Loss after 1020859320 batches: 0.0230
trigger times: 0
Loss after 1020937920 batches: 0.0217
trigger times: 1
Loss after 1021016520 batches: 0.0206
trigger times: 2
Loss after 1021095120 batches: 0.0195
trigger times: 3
Loss after 1021173720 batches: 0.0191
trigger times: 0
Loss after 1021252320 batches: 0.0182
trigger times: 1
Loss after 1021330920 batches: 0.0182
trigger times: 0
Loss after 1021409520 batches: 0.0178
trigger times: 1
Loss after 1021488120 batches: 0.0170
trigger times: 2
Loss after 1021566720 batches: 0.0174
trigger times: 3
Loss after 1021645320 batches: 0.0171
trigger times: 0
Loss after 1021723920 batches: 0.0166
trigger times: 1
Loss after 1021802520 batches: 0.0165
trigger times: 0
Loss after 1021881120 batches: 0.0155
trigger times: 1
Loss after 1021959720 batches: 0.0157
trigger times: 2
Loss after 1022038320 batches: 0.0158
trigger times: 3
Loss after 1022116920 batches: 0.0156
trigger times: 0
Loss after 1022195520 batches: 0.0156
trigger times: 1
Loss after 1022274120 batches: 0.0152
trigger times: 2
Loss after 1022352720 batches: 0.0152
trigger times: 3
Loss after 1022431320 batches: 0.0149
trigger times: 4
Loss after 1022509920 batches: 0.0147
trigger times: 5
Loss after 1022588520 batches: 0.0145
trigger times: 6
Loss after 1022667120 batches: 0.0142
trigger times: 0
Loss after 1022745720 batches: 0.0138
trigger times: 1
Loss after 1022824320 batches: 0.0138
trigger times: 0
Loss after 1022902920 batches: 0.0140
trigger times: 1
Loss after 1022981520 batches: 0.0143
trigger times: 2
Loss after 1023060120 batches: 0.0138
trigger times: 3
Loss after 1023138720 batches: 0.0139
trigger times: 4
Loss after 1023217320 batches: 0.0134
trigger times: 5
Loss after 1023295920 batches: 0.0136
trigger times: 6
Loss after 1023374520 batches: 0.0137
trigger times: 7
Loss after 1023453120 batches: 0.0137
trigger times: 8
Loss after 1023531720 batches: 0.0135
trigger times: 9
Loss after 1023610320 batches: 0.0133
trigger times: 10
Loss after 1023688920 batches: 0.0125
trigger times: 11
Loss after 1023767520 batches: 0.0127
trigger times: 12
Loss after 1023846120 batches: 0.0126
trigger times: 13
Loss after 1023924720 batches: 0.0128
trigger times: 14
Loss after 1024003320 batches: 0.0132
trigger times: 15
Loss after 1024081920 batches: 0.0129
trigger times: 16
Loss after 1024160520 batches: 0.0127
trigger times: 17
Loss after 1024239120 batches: 0.0125
trigger times: 18
Loss after 1024317720 batches: 0.0122
trigger times: 19
Loss after 1024396320 batches: 0.0125
trigger times: 20
Early stopping!
Start to test process.
Loss after 1024474920 batches: 0.0124
Time to train on one home:  251.0557963848114
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097, 0.016851266692967333, 0.01539797175087967, 0.015622467135075917, 0.014339386766728896, 0.015215330347522192, 0.014610499733001695, 0.014354899421970126, 0.01333385277873801, 0.01326294574980891, 0.013142881573656695, 0.013096885190459488]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983], [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013], [0.5549709763791826, 0.4001270641513627, 0.48658498833859326, 1.0394903420759227, 0.4914126622529051, 24.558331909615507, 1517.0215], [0.5461304287115732, 0.4097276108845518, 0.4876006728779417, 1.0136453079994863, 0.48354794633174836, 23.94773371608454, 1492.7424], [0.5402040051089393, 0.41609081728992114, 0.4927051878338842, 1.0002093606660662, 0.47833524208513384, 23.630304644569275, 1476.6505], [0.5334714584880405, 0.4234695591708837, 0.501786021212506, 1.0005226338786515, 0.47229061667346856, 23.637705836501173, 1457.9905], [0.5237383676899804, 0.43399432408412464, 0.5083585548121925, 0.9778479298034527, 0.46366878622151636, 23.10200782557115, 1431.3744], [0.5306309130456712, 0.42652222327915057, 0.5055172115402072, 1.0003700248842091, 0.46978989075842903, 23.63410039430878, 1450.2706], [0.5348766446113586, 0.42189465272218896, 0.49757962064175115, 0.9924071418125425, 0.47358077151210515, 23.445974427653898, 1461.9733], [0.5336118737856547, 0.4232833712453734, 0.5012302308952711, 0.9933548830158522, 0.47244314081431027, 23.468365153275084, 1458.4613], [0.5353295041455163, 0.4214272084397156, 0.5019043823357494, 0.9976955154496634, 0.4739636993382793, 23.570914150309818, 1463.1555], [0.5284832881556617, 0.42883928258247084, 0.5055071880025568, 0.9883304284579222, 0.4678917683181634, 23.349660613459925, 1444.411]]
Round_18_results:  [0.5284832881556617, 0.42883928258247084, 0.5055071880025568, 0.9883304284579222, 0.4678917683181634, 23.349660613459925, 1444.411]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 8337 < 8338; dropping {'Training_Loss': 0.1110912543141617, 'Validation_Loss': 0.18190721091296938, 'Training_R2': 0.8880423760647647, 'Validation_R2': 0.8309371103051171, 'Training_F1': 0.8256638354484632, 'Validation_F1': 0.7320046841615696, 'Training_NEP': 0.3486088796229931, 'Validation_NEP': 0.4919044892993553, 'Training_NDE': 0.08404857553908607, 'Validation_NDE': 0.13463277335331686, 'Training_MAE': 11.545234727153879, 'Validation_MAE': 13.490061883302573, 'Training_MSE': 369.80057, 'Validation_MSE': 497.19513}.
trigger times: 0
Loss after 1024606020 batches: 0.1111
trigger times: 0
Loss after 1024737120 batches: 0.0254
trigger times: 1
Loss after 1024868220 batches: 0.0189
trigger times: 0
Loss after 1024999320 batches: 0.0166
trigger times: 0
Loss after 1025130420 batches: 0.0155
trigger times: 1
Loss after 1025261520 batches: 0.0145
trigger times: 2
Loss after 1025392620 batches: 0.0135
trigger times: 3
Loss after 1025523720 batches: 0.0131
trigger times: 4
Loss after 1025654820 batches: 0.0124
trigger times: 5
Loss after 1025785920 batches: 0.0124
trigger times: 6
Loss after 1025917020 batches: 0.0118
trigger times: 7
Loss after 1026048120 batches: 0.0119
trigger times: 8
Loss after 1026179220 batches: 0.0114
trigger times: 9
Loss after 1026310320 batches: 0.0112
trigger times: 0
Loss after 1026441420 batches: 0.0108
trigger times: 1
Loss after 1026572520 batches: 0.0105
trigger times: 2
Loss after 1026703620 batches: 0.0105
trigger times: 3
Loss after 1026834720 batches: 0.0102
trigger times: 4
Loss after 1026965820 batches: 0.0101
trigger times: 5
Loss after 1027096920 batches: 0.0102
trigger times: 6
Loss after 1027228020 batches: 0.0101
trigger times: 7
Loss after 1027359120 batches: 0.0099
trigger times: 8
Loss after 1027490220 batches: 0.0100
trigger times: 9
Loss after 1027621320 batches: 0.0099
trigger times: 10
Loss after 1027752420 batches: 0.0094
trigger times: 11
Loss after 1027883520 batches: 0.0094
trigger times: 12
Loss after 1028014620 batches: 0.0094
trigger times: 13
Loss after 1028145720 batches: 0.0094
trigger times: 14
Loss after 1028276820 batches: 0.0093
trigger times: 15
Loss after 1028407920 batches: 0.0092
trigger times: 16
Loss after 1028539020 batches: 0.0090
trigger times: 17
Loss after 1028670120 batches: 0.0089
trigger times: 18
Loss after 1028801220 batches: 0.0088
trigger times: 19
Loss after 1028932320 batches: 0.0090
trigger times: 20
Early stopping!
Start to test process.
Loss after 1029063420 batches: 0.0089
Time to train on one home:  261.88995385169983
trigger times: 0
Loss after 1029166020 batches: 0.2004
trigger times: 0
Loss after 1029268620 batches: 0.0593
trigger times: 0
Loss after 1029371220 batches: 0.0399
trigger times: 1
Loss after 1029473820 batches: 0.0347
trigger times: 2
Loss after 1029576420 batches: 0.0327
trigger times: 3
Loss after 1029679020 batches: 0.0307
trigger times: 4
Loss after 1029781620 batches: 0.0277
trigger times: 5
Loss after 1029884220 batches: 0.0275
trigger times: 6
Loss after 1029986820 batches: 0.0266
trigger times: 7
Loss after 1030089420 batches: 0.0243
trigger times: 8
Loss after 1030192020 batches: 0.0241
trigger times: 9
Loss after 1030294620 batches: 0.0229
trigger times: 10
Loss after 1030397220 batches: 0.0232
trigger times: 11
Loss after 1030499820 batches: 0.0245
trigger times: 12
Loss after 1030602420 batches: 0.0230
trigger times: 0
Loss after 1030705020 batches: 0.0210
trigger times: 1
Loss after 1030807620 batches: 0.0207
trigger times: 2
Loss after 1030910220 batches: 0.0222
trigger times: 0
Loss after 1031012820 batches: 0.0214
trigger times: 1
Loss after 1031115420 batches: 0.0214
trigger times: 2
Loss after 1031218020 batches: 0.0213
trigger times: 3
Loss after 1031320620 batches: 0.0204
trigger times: 4
Loss after 1031423220 batches: 0.0195
trigger times: 5
Loss after 1031525820 batches: 0.0196
trigger times: 6
Loss after 1031628420 batches: 0.0202
trigger times: 7
Loss after 1031731020 batches: 0.0188
trigger times: 8
Loss after 1031833620 batches: 0.0185
trigger times: 9
Loss after 1031936220 batches: 0.0179
trigger times: 10
Loss after 1032038820 batches: 0.0181
trigger times: 0
Loss after 1032141420 batches: 0.0212
trigger times: 1
Loss after 1032244020 batches: 0.0182
trigger times: 2
Loss after 1032346620 batches: 0.0171
trigger times: 3
Loss after 1032449220 batches: 0.0181
trigger times: 4
Loss after 1032551820 batches: 0.0191
trigger times: 5
Loss after 1032654420 batches: 0.0177
trigger times: 6
Loss after 1032757020 batches: 0.0168
trigger times: 7
Loss after 1032859620 batches: 0.0167
trigger times: 8
Loss after 1032962220 batches: 0.0166
trigger times: 9
Loss after 1033064820 batches: 0.0166
trigger times: 10
Loss after 1033167420 batches: 0.0168
trigger times: 11
Loss after 1033270020 batches: 0.0163
trigger times: 12
Loss after 1033372620 batches: 0.0168
trigger times: 13
Loss after 1033475220 batches: 0.0158
trigger times: 14
Loss after 1033577820 batches: 0.0176
trigger times: 15
Loss after 1033680420 batches: 0.0181
trigger times: 16
Loss after 1033783020 batches: 0.0171
trigger times: 17
Loss after 1033885620 batches: 0.0167
trigger times: 18
Loss after 1033988220 batches: 0.0160
trigger times: 19
Loss after 1034090820 batches: 0.0155
trigger times: 20
Early stopping!
Start to test process.
Loss after 1034193420 batches: 0.0159
Time to train on one home:  298.12775588035583
trigger times: 0
Loss after 1034324520 batches: 0.1105
trigger times: 1
Loss after 1034455620 batches: 0.0381
trigger times: 2
Loss after 1034586720 batches: 0.0295
trigger times: 3
Loss after 1034717820 batches: 0.0258
trigger times: 4
Loss after 1034848920 batches: 0.0237
trigger times: 5
Loss after 1034980020 batches: 0.0224
trigger times: 6
Loss after 1035111120 batches: 0.0216
trigger times: 7
Loss after 1035242220 batches: 0.0205
trigger times: 8
Loss after 1035373320 batches: 0.0198
trigger times: 9
Loss after 1035504420 batches: 0.0192
trigger times: 10
Loss after 1035635520 batches: 0.0188
trigger times: 11
Loss after 1035766620 batches: 0.0180
trigger times: 12
Loss after 1035897720 batches: 0.0177
trigger times: 13
Loss after 1036028820 batches: 0.0175
trigger times: 14
Loss after 1036159920 batches: 0.0169
trigger times: 15
Loss after 1036291020 batches: 0.0169
trigger times: 16
Loss after 1036422120 batches: 0.0165
trigger times: 17
Loss after 1036553220 batches: 0.0165
trigger times: 18
Loss after 1036684320 batches: 0.0160
trigger times: 19
Loss after 1036815420 batches: 0.0161
trigger times: 20
Early stopping!
Start to test process.
Loss after 1036946520 batches: 0.0157
Time to train on one home:  161.6895878314972
trigger times: 0
Loss after 1037077620 batches: 0.1426
trigger times: 0
Loss after 1037208720 batches: 0.0424
trigger times: 0
Loss after 1037339820 batches: 0.0329
trigger times: 1
Loss after 1037470920 batches: 0.0292
trigger times: 2
Loss after 1037602020 batches: 0.0274
trigger times: 3
Loss after 1037733120 batches: 0.0255
trigger times: 0
Loss after 1037864220 batches: 0.0240
trigger times: 1
Loss after 1037995320 batches: 0.0236
trigger times: 2
Loss after 1038126420 batches: 0.0226
trigger times: 3
Loss after 1038257520 batches: 0.0221
trigger times: 4
Loss after 1038388620 batches: 0.0218
trigger times: 5
Loss after 1038519720 batches: 0.0212
trigger times: 6
Loss after 1038650820 batches: 0.0207
trigger times: 7
Loss after 1038781920 batches: 0.0205
trigger times: 8
Loss after 1038913020 batches: 0.0199
trigger times: 0
Loss after 1039044120 batches: 0.0197
trigger times: 1
Loss after 1039175220 batches: 0.0192
trigger times: 2
Loss after 1039306320 batches: 0.0193
trigger times: 3
Loss after 1039437420 batches: 0.0190
trigger times: 0
Loss after 1039568520 batches: 0.0188
trigger times: 1
Loss after 1039699620 batches: 0.0186
trigger times: 2
Loss after 1039830720 batches: 0.0189
trigger times: 3
Loss after 1039961820 batches: 0.0184
trigger times: 0
Loss after 1040092920 batches: 0.0182
trigger times: 1
Loss after 1040224020 batches: 0.0180
trigger times: 2
Loss after 1040355120 batches: 0.0180
trigger times: 3
Loss after 1040486220 batches: 0.0178
trigger times: 0
Loss after 1040617320 batches: 0.0178
trigger times: 1
Loss after 1040748420 batches: 0.0175
trigger times: 2
Loss after 1040879520 batches: 0.0175
trigger times: 3
Loss after 1041010620 batches: 0.0172
trigger times: 4
Loss after 1041141720 batches: 0.0173
trigger times: 5
Loss after 1041272820 batches: 0.0171
trigger times: 6
Loss after 1041403920 batches: 0.0171
trigger times: 7
Loss after 1041535020 batches: 0.0168
trigger times: 8
Loss after 1041666120 batches: 0.0169
trigger times: 9
Loss after 1041797220 batches: 0.0168
trigger times: 10
Loss after 1041928320 batches: 0.0165
trigger times: 11
Loss after 1042059420 batches: 0.0166
trigger times: 12
Loss after 1042190520 batches: 0.0167
trigger times: 13
Loss after 1042321620 batches: 0.0166
trigger times: 0
Loss after 1042452720 batches: 0.0164
trigger times: 1
Loss after 1042583820 batches: 0.0164
trigger times: 2
Loss after 1042714920 batches: 0.0163
trigger times: 3
Loss after 1042846020 batches: 0.0162
trigger times: 4
Loss after 1042977120 batches: 0.0162
trigger times: 5
Loss after 1043108220 batches: 0.0159
trigger times: 6
Loss after 1043239320 batches: 0.0157
trigger times: 7
Loss after 1043370420 batches: 0.0160
trigger times: 8
Loss after 1043501520 batches: 0.0159
trigger times: 9
Loss after 1043632620 batches: 0.0155
trigger times: 0
Loss after 1043763720 batches: 0.0155
trigger times: 1
Loss after 1043894820 batches: 0.0158
trigger times: 0
Loss after 1044025920 batches: 0.0157
trigger times: 1
Loss after 1044157020 batches: 0.0155
trigger times: 2
Loss after 1044288120 batches: 0.0152
trigger times: 3
Loss after 1044419220 batches: 0.0154
trigger times: 0
Loss after 1044550320 batches: 0.0153
trigger times: 1
Loss after 1044681420 batches: 0.0152
trigger times: 2
Loss after 1044812520 batches: 0.0151
trigger times: 0
Loss after 1044943620 batches: 0.0152
trigger times: 1
Loss after 1045074720 batches: 0.0151
trigger times: 2
Loss after 1045205820 batches: 0.0152
trigger times: 3
Loss after 1045336920 batches: 0.0152
trigger times: 4
Loss after 1045468020 batches: 0.0149
trigger times: 5
Loss after 1045599120 batches: 0.0150
trigger times: 6
Loss after 1045730220 batches: 0.0148
trigger times: 7
Loss after 1045861320 batches: 0.0146
trigger times: 8
Loss after 1045992420 batches: 0.0147
trigger times: 9
Loss after 1046123520 batches: 0.0149
trigger times: 10
Loss after 1046254620 batches: 0.0148
trigger times: 11
Loss after 1046385720 batches: 0.0147
trigger times: 12
Loss after 1046516820 batches: 0.0145
trigger times: 13
Loss after 1046647920 batches: 0.0146
trigger times: 14
Loss after 1046779020 batches: 0.0144
trigger times: 15
Loss after 1046910120 batches: 0.0145
trigger times: 16
Loss after 1047041220 batches: 0.0144
trigger times: 17
Loss after 1047172320 batches: 0.0144
trigger times: 18
Loss after 1047303420 batches: 0.0146
trigger times: 19
Loss after 1047434520 batches: 0.0145
trigger times: 20
Early stopping!
Start to test process.
Loss after 1047565620 batches: 0.0146
Time to train on one home:  591.8935856819153
trigger times: 0
Loss after 1047694260 batches: 0.0923
trigger times: 0
Loss after 1047822900 batches: 0.0280
trigger times: 0
Loss after 1047951540 batches: 0.0213
trigger times: 0
Loss after 1048080180 batches: 0.0192
trigger times: 0
Loss after 1048208820 batches: 0.0174
trigger times: 0
Loss after 1048337460 batches: 0.0167
trigger times: 1
Loss after 1048466100 batches: 0.0162
trigger times: 2
Loss after 1048594740 batches: 0.0157
trigger times: 3
Loss after 1048723380 batches: 0.0150
trigger times: 4
Loss after 1048852020 batches: 0.0141
trigger times: 5
Loss after 1048980660 batches: 0.0139
trigger times: 6
Loss after 1049109300 batches: 0.0137
trigger times: 7
Loss after 1049237940 batches: 0.0135
trigger times: 0
Loss after 1049366580 batches: 0.0131
trigger times: 1
Loss after 1049495220 batches: 0.0127
trigger times: 2
Loss after 1049623860 batches: 0.0129
trigger times: 3
Loss after 1049752500 batches: 0.0126
trigger times: 4
Loss after 1049881140 batches: 0.0123
trigger times: 5
Loss after 1050009780 batches: 0.0122
trigger times: 6
Loss after 1050138420 batches: 0.0121
trigger times: 7
Loss after 1050267060 batches: 0.0121
trigger times: 8
Loss after 1050395700 batches: 0.0117
trigger times: 9
Loss after 1050524340 batches: 0.0116
trigger times: 10
Loss after 1050652980 batches: 0.0117
trigger times: 11
Loss after 1050781620 batches: 0.0114
trigger times: 12
Loss after 1050910260 batches: 0.0112
trigger times: 13
Loss after 1051038900 batches: 0.0112
trigger times: 14
Loss after 1051167540 batches: 0.0109
trigger times: 15
Loss after 1051296180 batches: 0.0109
trigger times: 16
Loss after 1051424820 batches: 0.0108
trigger times: 17
Loss after 1051553460 batches: 0.0109
trigger times: 18
Loss after 1051682100 batches: 0.0107
trigger times: 19
Loss after 1051810740 batches: 0.0108
trigger times: 20
Early stopping!
Start to test process.
Loss after 1051939380 batches: 0.0105
Time to train on one home:  251.9045159816742
trigger times: 0
Loss after 1052070480 batches: 0.1621
trigger times: 0
Loss after 1052201580 batches: 0.0415
trigger times: 1
Loss after 1052332680 batches: 0.0314
trigger times: 0
Loss after 1052463780 batches: 0.0278
trigger times: 1
Loss after 1052594880 batches: 0.0255
trigger times: 2
Loss after 1052725980 batches: 0.0239
trigger times: 0
Loss after 1052857080 batches: 0.0230
trigger times: 1
Loss after 1052988180 batches: 0.0225
trigger times: 2
Loss after 1053119280 batches: 0.0215
trigger times: 3
Loss after 1053250380 batches: 0.0209
trigger times: 0
Loss after 1053381480 batches: 0.0209
trigger times: 1
Loss after 1053512580 batches: 0.0201
trigger times: 2
Loss after 1053643680 batches: 0.0202
trigger times: 3
Loss after 1053774780 batches: 0.0195
trigger times: 4
Loss after 1053905880 batches: 0.0192
trigger times: 5
Loss after 1054036980 batches: 0.0189
trigger times: 6
Loss after 1054168080 batches: 0.0186
trigger times: 0
Loss after 1054299180 batches: 0.0184
trigger times: 1
Loss after 1054430280 batches: 0.0183
trigger times: 2
Loss after 1054561380 batches: 0.0180
trigger times: 3
Loss after 1054692480 batches: 0.0179
trigger times: 4
Loss after 1054823580 batches: 0.0177
trigger times: 5
Loss after 1054954680 batches: 0.0176
trigger times: 6
Loss after 1055085780 batches: 0.0174
trigger times: 7
Loss after 1055216880 batches: 0.0175
trigger times: 8
Loss after 1055347980 batches: 0.0170
trigger times: 9
Loss after 1055479080 batches: 0.0171
trigger times: 0
Loss after 1055610180 batches: 0.0172
trigger times: 1
Loss after 1055741280 batches: 0.0170
trigger times: 2
Loss after 1055872380 batches: 0.0167
trigger times: 0
Loss after 1056003480 batches: 0.0167
trigger times: 1
Loss after 1056134580 batches: 0.0168
trigger times: 2
Loss after 1056265680 batches: 0.0163
trigger times: 3
Loss after 1056396780 batches: 0.0164
trigger times: 4
Loss after 1056527880 batches: 0.0165
trigger times: 5
Loss after 1056658980 batches: 0.0164
trigger times: 0
Loss after 1056790080 batches: 0.0161
trigger times: 1
Loss after 1056921180 batches: 0.0159
trigger times: 2
Loss after 1057052280 batches: 0.0158
trigger times: 3
Loss after 1057183380 batches: 0.0158
trigger times: 4
Loss after 1057314480 batches: 0.0159
trigger times: 5
Loss after 1057445580 batches: 0.0156
trigger times: 6
Loss after 1057576680 batches: 0.0158
trigger times: 7
Loss after 1057707780 batches: 0.0158
trigger times: 8
Loss after 1057838880 batches: 0.0156
trigger times: 9
Loss after 1057969980 batches: 0.0156
trigger times: 10
Loss after 1058101080 batches: 0.0153
trigger times: 0
Loss after 1058232180 batches: 0.0153
trigger times: 1
Loss after 1058363280 batches: 0.0154
trigger times: 2
Loss after 1058494380 batches: 0.0154
trigger times: 3
Loss after 1058625480 batches: 0.0154
trigger times: 4
Loss after 1058756580 batches: 0.0152
trigger times: 5
Loss after 1058887680 batches: 0.0151
trigger times: 6
Loss after 1059018780 batches: 0.0150
trigger times: 0
Loss after 1059149880 batches: 0.0150
trigger times: 1
Loss after 1059280980 batches: 0.0148
trigger times: 2
Loss after 1059412080 batches: 0.0150
trigger times: 0
Loss after 1059543180 batches: 0.0148
trigger times: 1
Loss after 1059674280 batches: 0.0147
trigger times: 2
Loss after 1059805380 batches: 0.0145
trigger times: 3
Loss after 1059936480 batches: 0.0147
trigger times: 4
Loss after 1060067580 batches: 0.0147
trigger times: 5
Loss after 1060198680 batches: 0.0146
trigger times: 6
Loss after 1060329780 batches: 0.0144
trigger times: 7
Loss after 1060460880 batches: 0.0147
trigger times: 8
Loss after 1060591980 batches: 0.0144
trigger times: 9
Loss after 1060723080 batches: 0.0145
trigger times: 10
Loss after 1060854180 batches: 0.0144
trigger times: 11
Loss after 1060985280 batches: 0.0141
trigger times: 12
Loss after 1061116380 batches: 0.0143
trigger times: 13
Loss after 1061247480 batches: 0.0140
trigger times: 14
Loss after 1061378580 batches: 0.0142
trigger times: 15
Loss after 1061509680 batches: 0.0140
trigger times: 16
Loss after 1061640780 batches: 0.0141
trigger times: 17
Loss after 1061771880 batches: 0.0142
trigger times: 18
Loss after 1061902980 batches: 0.0140
trigger times: 19
Loss after 1062034080 batches: 0.0141
trigger times: 20
Early stopping!
Start to test process.
Loss after 1062165180 batches: 0.0142
Time to train on one home:  570.0291969776154
trigger times: 0
Loss after 1062296280 batches: 0.1463
trigger times: 0
Loss after 1062427380 batches: 0.0606
trigger times: 0
Loss after 1062558480 batches: 0.0414
trigger times: 1
Loss after 1062689580 batches: 0.0342
trigger times: 2
Loss after 1062820680 batches: 0.0327
trigger times: 3
Loss after 1062951780 batches: 0.0301
trigger times: 0
Loss after 1063082880 batches: 0.0301
trigger times: 1
Loss after 1063213980 batches: 0.0263
trigger times: 2
Loss after 1063345080 batches: 0.0253
trigger times: 3
Loss after 1063476180 batches: 0.0241
trigger times: 4
Loss after 1063607280 batches: 0.0238
trigger times: 0
Loss after 1063738380 batches: 0.0238
trigger times: 1
Loss after 1063869480 batches: 0.0226
trigger times: 2
Loss after 1064000580 batches: 0.0237
trigger times: 3
Loss after 1064131680 batches: 0.0218
trigger times: 4
Loss after 1064262780 batches: 0.0216
trigger times: 0
Loss after 1064393880 batches: 0.0215
trigger times: 1
Loss after 1064524980 batches: 0.0212
trigger times: 2
Loss after 1064656080 batches: 0.0210
trigger times: 0
Loss after 1064787180 batches: 0.0205
trigger times: 1
Loss after 1064918280 batches: 0.0213
trigger times: 2
Loss after 1065049380 batches: 0.0206
trigger times: 3
Loss after 1065180480 batches: 0.0196
trigger times: 4
Loss after 1065311580 batches: 0.0201
trigger times: 5
Loss after 1065442680 batches: 0.0203
trigger times: 0
Loss after 1065573780 batches: 0.0198
trigger times: 0
Loss after 1065704880 batches: 0.0192
trigger times: 1
Loss after 1065835980 batches: 0.0195
trigger times: 0
Loss after 1065967080 batches: 0.0200
trigger times: 0
Loss after 1066098180 batches: 0.0201
trigger times: 1
Loss after 1066229280 batches: 0.0195
trigger times: 2
Loss after 1066360380 batches: 0.0193
trigger times: 3
Loss after 1066491480 batches: 0.0196
trigger times: 4
Loss after 1066622580 batches: 0.0195
trigger times: 5
Loss after 1066753680 batches: 0.0190
trigger times: 6
Loss after 1066884780 batches: 0.0192
trigger times: 7
Loss after 1067015880 batches: 0.0191
trigger times: 8
Loss after 1067146980 batches: 0.0184
trigger times: 9
Loss after 1067278080 batches: 0.0187
trigger times: 0
Loss after 1067409180 batches: 0.0188
trigger times: 1
Loss after 1067540280 batches: 0.0188
trigger times: 2
Loss after 1067671380 batches: 0.0177
trigger times: 3
Loss after 1067802480 batches: 0.0193
trigger times: 4
Loss after 1067933580 batches: 0.0181
trigger times: 5
Loss after 1068064680 batches: 0.0190
trigger times: 6
Loss after 1068195780 batches: 0.0192
trigger times: 7
Loss after 1068326880 batches: 0.0187
trigger times: 8
Loss after 1068457980 batches: 0.0178
trigger times: 0
Loss after 1068589080 batches: 0.0175
trigger times: 1
Loss after 1068720180 batches: 0.0174
trigger times: 0
Loss after 1068851280 batches: 0.0184
trigger times: 0
Loss after 1068982380 batches: 0.0179
trigger times: 1
Loss after 1069113480 batches: 0.0177
trigger times: 2
Loss after 1069244580 batches: 0.0181
trigger times: 3
Loss after 1069375680 batches: 0.0167
trigger times: 4
Loss after 1069506780 batches: 0.0172
trigger times: 5
Loss after 1069637880 batches: 0.0171
trigger times: 6
Loss after 1069768980 batches: 0.0176
trigger times: 7
Loss after 1069900080 batches: 0.0169
trigger times: 8
Loss after 1070031180 batches: 0.0166
trigger times: 9
Loss after 1070162280 batches: 0.0169
trigger times: 10
Loss after 1070293380 batches: 0.0163
trigger times: 11
Loss after 1070424480 batches: 0.0167
trigger times: 12
Loss after 1070555580 batches: 0.0165
trigger times: 13
Loss after 1070686680 batches: 0.0163
trigger times: 14
Loss after 1070817780 batches: 0.0161
trigger times: 15
Loss after 1070948880 batches: 0.0163
trigger times: 16
Loss after 1071079980 batches: 0.0162
trigger times: 17
Loss after 1071211080 batches: 0.0163
trigger times: 18
Loss after 1071342180 batches: 0.0162
trigger times: 19
Loss after 1071473280 batches: 0.0174
trigger times: 20
Early stopping!
Start to test process.
Loss after 1071604380 batches: 0.0172
Time to train on one home:  526.080240726471
trigger times: 0
Loss after 1071735480 batches: 0.0622
trigger times: 0
Loss after 1071866580 batches: 0.0185
trigger times: 1
Loss after 1071997680 batches: 0.0146
trigger times: 2
Loss after 1072128780 batches: 0.0129
trigger times: 3
Loss after 1072259880 batches: 0.0119
trigger times: 4
Loss after 1072390980 batches: 0.0114
trigger times: 5
Loss after 1072522080 batches: 0.0108
trigger times: 6
Loss after 1072653180 batches: 0.0103
trigger times: 7
Loss after 1072784280 batches: 0.0099
trigger times: 8
Loss after 1072915380 batches: 0.0097
trigger times: 9
Loss after 1073046480 batches: 0.0095
trigger times: 10
Loss after 1073177580 batches: 0.0091
trigger times: 11
Loss after 1073308680 batches: 0.0090
trigger times: 12
Loss after 1073439780 batches: 0.0088
trigger times: 0
Loss after 1073570880 batches: 0.0088
trigger times: 1
Loss after 1073701980 batches: 0.0087
trigger times: 2
Loss after 1073833080 batches: 0.0085
trigger times: 3
Loss after 1073964180 batches: 0.0083
trigger times: 4
Loss after 1074095280 batches: 0.0083
trigger times: 5
Loss after 1074226380 batches: 0.0083
trigger times: 6
Loss after 1074357480 batches: 0.0082
trigger times: 7
Loss after 1074488580 batches: 0.0080
trigger times: 8
Loss after 1074619680 batches: 0.0079
trigger times: 9
Loss after 1074750780 batches: 0.0079
trigger times: 10
Loss after 1074881880 batches: 0.0078
trigger times: 11
Loss after 1075012980 batches: 0.0077
trigger times: 12
Loss after 1075144080 batches: 0.0077
trigger times: 13
Loss after 1075275180 batches: 0.0075
trigger times: 14
Loss after 1075406280 batches: 0.0075
trigger times: 15
Loss after 1075537380 batches: 0.0075
trigger times: 16
Loss after 1075668480 batches: 0.0073
trigger times: 17
Loss after 1075799580 batches: 0.0072
trigger times: 18
Loss after 1075930680 batches: 0.0071
trigger times: 19
Loss after 1076061780 batches: 0.0072
trigger times: 20
Early stopping!
Start to test process.
Loss after 1076192880 batches: 0.0073
Time to train on one home:  261.89520049095154
trigger times: 0
Loss after 1076271480 batches: 0.1676
trigger times: 0
Loss after 1076350080 batches: 0.0449
trigger times: 0
Loss after 1076428680 batches: 0.0310
trigger times: 0
Loss after 1076507280 batches: 0.0262
trigger times: 0
Loss after 1076585880 batches: 0.0242
trigger times: 0
Loss after 1076664480 batches: 0.0222
trigger times: 1
Loss after 1076743080 batches: 0.0206
trigger times: 0
Loss after 1076821680 batches: 0.0203
trigger times: 1
Loss after 1076900280 batches: 0.0196
trigger times: 2
Loss after 1076978880 batches: 0.0191
trigger times: 3
Loss after 1077057480 batches: 0.0185
trigger times: 4
Loss after 1077136080 batches: 0.0175
trigger times: 5
Loss after 1077214680 batches: 0.0176
trigger times: 6
Loss after 1077293280 batches: 0.0169
trigger times: 7
Loss after 1077371880 batches: 0.0169
trigger times: 8
Loss after 1077450480 batches: 0.0162
trigger times: 9
Loss after 1077529080 batches: 0.0165
trigger times: 10
Loss after 1077607680 batches: 0.0170
trigger times: 11
Loss after 1077686280 batches: 0.0170
trigger times: 12
Loss after 1077764880 batches: 0.0160
trigger times: 13
Loss after 1077843480 batches: 0.0154
trigger times: 14
Loss after 1077922080 batches: 0.0153
trigger times: 15
Loss after 1078000680 batches: 0.0151
trigger times: 16
Loss after 1078079280 batches: 0.0147
trigger times: 17
Loss after 1078157880 batches: 0.0144
trigger times: 18
Loss after 1078236480 batches: 0.0143
trigger times: 19
Loss after 1078315080 batches: 0.0142
trigger times: 20
Early stopping!
Start to test process.
Loss after 1078393680 batches: 0.0141
Time to train on one home:  140.75488710403442
train_results:  [0.07004044145543929, 0.0834501805535458, 0.057980438835306757, 0.0356069942723444, 0.02728977541200127, 0.024900338277644905, 0.018450228963904372, 0.01986145720359097, 0.016851266692967333, 0.01539797175087967, 0.015622467135075917, 0.014339386766728896, 0.015215330347522192, 0.014610499733001695, 0.014354899421970126, 0.01333385277873801, 0.01326294574980891, 0.013142881573656695, 0.013096885190459488, 0.013137692339932006]
test_results:  [[0.8801295492384169, 0.04794828015902586, 0.22299869240455464, 1.4479286647617668, 0.779915615942307, 34.20783367708165, 2407.648], [0.7351381546921201, 0.2050382292669316, 0.32059472429947034, 1.1800512802079124, 0.651228379877755, 27.879134453368444, 2010.3824], [0.7316293782658048, 0.20896485994443148, 0.11353792582353804, 1.1059440463148817, 0.6480117052795185, 26.12832449084969, 2000.4525], [0.6228069298797183, 0.3266519244638607, 0.3243018183385818, 1.0640918504123331, 0.5516030990029136, 25.13955136182915, 1702.833], [0.5820489393340217, 0.3707648805084185, 0.4120436492888168, 1.0362687328545235, 0.5154660041712653, 24.482220237058552, 1591.2756], [0.5658141606383853, 0.38839351422341495, 0.4324962536581986, 1.030293598773447, 0.5010247228463923, 24.341055552762896, 1546.6945], [0.5792470342583127, 0.37382562730225755, 0.4495999178357937, 1.0289970825263561, 0.5129586569639528, 24.3104248917231, 1583.5353], [0.5851478510432773, 0.3674739039761532, 0.44258158233797046, 1.0430387770235228, 0.5181619543341852, 24.64216495709607, 1599.5983], [0.5514055755403307, 0.4039706904323844, 0.4797889268109016, 1.0076278608912723, 0.48826398440700414, 23.80556937135676, 1507.3013], [0.5549709763791826, 0.4001270641513627, 0.48658498833859326, 1.0394903420759227, 0.4914126622529051, 24.558331909615507, 1517.0215], [0.5461304287115732, 0.4097276108845518, 0.4876006728779417, 1.0136453079994863, 0.48354794633174836, 23.94773371608454, 1492.7424], [0.5402040051089393, 0.41609081728992114, 0.4927051878338842, 1.0002093606660662, 0.47833524208513384, 23.630304644569275, 1476.6505], [0.5334714584880405, 0.4234695591708837, 0.501786021212506, 1.0005226338786515, 0.47229061667346856, 23.637705836501173, 1457.9905], [0.5237383676899804, 0.43399432408412464, 0.5083585548121925, 0.9778479298034527, 0.46366878622151636, 23.10200782557115, 1431.3744], [0.5306309130456712, 0.42652222327915057, 0.5055172115402072, 1.0003700248842091, 0.46978989075842903, 23.63410039430878, 1450.2706], [0.5348766446113586, 0.42189465272218896, 0.49757962064175115, 0.9924071418125425, 0.47358077151210515, 23.445974427653898, 1461.9733], [0.5336118737856547, 0.4232833712453734, 0.5012302308952711, 0.9933548830158522, 0.47244314081431027, 23.468365153275084, 1458.4613], [0.5353295041455163, 0.4214272084397156, 0.5019043823357494, 0.9976955154496634, 0.4739636993382793, 23.570914150309818, 1463.1555], [0.5284832881556617, 0.42883928258247084, 0.5055071880025568, 0.9883304284579222, 0.4678917683181634, 23.349660613459925, 1444.411], [0.5294026566876305, 0.42783839944372715, 0.5059478804137693, 0.9873427420098317, 0.46871168636817223, 23.326326167113464, 1446.9421]]
Round_19_results:  [0.5294026566876305, 0.42783839944372715, 0.5059478804137693, 0.9873427420098317, 0.46871168636817223, 23.326326167113464, 1446.9421]