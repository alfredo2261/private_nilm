LSTM(
  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))
  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))
  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))
  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))
  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))
  (linear1): Linear(in_features=23500, out_features=1024, bias=True)
  (linear2): Linear(in_features=1024, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (dropout): Dropout(p=0.2, inplace=False)
)
Window Length:  499
trigger times: 0
Loss after 131100 batches: 0.9471
trigger times: 0
Loss after 262200 batches: 0.4641
trigger times: 0
Loss after 393300 batches: 0.3214
trigger times: 1
Loss after 524400 batches: 0.2518
trigger times: 0
Loss after 655500 batches: 0.2128
trigger times: 0
Loss after 786600 batches: 0.1862
trigger times: 1
Loss after 917700 batches: 0.1671
trigger times: 0
Loss after 1048800 batches: 0.1493
trigger times: 0
Loss after 1179900 batches: 0.1353
trigger times: 0
Loss after 1311000 batches: 0.1205
trigger times: 1
Loss after 1442100 batches: 0.1097
trigger times: 0
Loss after 1573200 batches: 0.0993
trigger times: 1
Loss after 1704300 batches: 0.0911
trigger times: 2
Loss after 1835400 batches: 0.0841
trigger times: 3
Loss after 1966500 batches: 0.0776
trigger times: 4
Loss after 2097600 batches: 0.0736
trigger times: 5
Loss after 2228700 batches: 0.0672
trigger times: 0
Loss after 2359800 batches: 0.0648
trigger times: 1
Loss after 2490900 batches: 0.0597
trigger times: 2
Loss after 2622000 batches: 0.0583
trigger times: 3
Loss after 2753100 batches: 0.0536
trigger times: 4
Loss after 2884200 batches: 0.0510
trigger times: 5
Loss after 3015300 batches: 0.0503
trigger times: 6
Loss after 3146400 batches: 0.0478
trigger times: 7
Loss after 3277500 batches: 0.0451
trigger times: 8
Loss after 3408600 batches: 0.0437
trigger times: 9
Loss after 3539700 batches: 0.0425
trigger times: 10
Loss after 3670800 batches: 0.0399
trigger times: 0
Loss after 3801900 batches: 0.0391
trigger times: 1
Loss after 3933000 batches: 0.0370
trigger times: 0
Loss after 4064100 batches: 0.0367
trigger times: 1
Loss after 4195200 batches: 0.0361
trigger times: 2
Loss after 4326300 batches: 0.0342
trigger times: 3
Loss after 4457400 batches: 0.0334
trigger times: 4
Loss after 4588500 batches: 0.0325
trigger times: 0
Loss after 4719600 batches: 0.0313
trigger times: 1
Loss after 4850700 batches: 0.0314
trigger times: 2
Loss after 4981800 batches: 0.0305
trigger times: 3
Loss after 5112900 batches: 0.0300
trigger times: 0
Loss after 5244000 batches: 0.0289
trigger times: 1
Loss after 5375100 batches: 0.0279
trigger times: 2
Loss after 5506200 batches: 0.0275
trigger times: 3
Loss after 5637300 batches: 0.0273
trigger times: 4
Loss after 5768400 batches: 0.0261
trigger times: 5
Loss after 5899500 batches: 0.0261
trigger times: 6
Loss after 6030600 batches: 0.0250
trigger times: 0
Loss after 6161700 batches: 0.0244
trigger times: 0
Loss after 6292800 batches: 0.0242
trigger times: 1
Loss after 6423900 batches: 0.0233
trigger times: 2
Loss after 6555000 batches: 0.0234
trigger times: 3
Loss after 6686100 batches: 0.0229
trigger times: 4
Loss after 6817200 batches: 0.0223
trigger times: 5
Loss after 6948300 batches: 0.0221
trigger times: 6
Loss after 7079400 batches: 0.0224
trigger times: 7
Loss after 7210500 batches: 0.0220
trigger times: 8
Loss after 7341600 batches: 0.0208
trigger times: 9
Loss after 7472700 batches: 0.0206
trigger times: 10
Loss after 7603800 batches: 0.0204
trigger times: 11
Loss after 7734900 batches: 0.0202
trigger times: 12
Loss after 7866000 batches: 0.0197
trigger times: 13
Loss after 7997100 batches: 0.0195
trigger times: 14
Loss after 8128200 batches: 0.0191
trigger times: 15
Loss after 8259300 batches: 0.0190
trigger times: 16
Loss after 8390400 batches: 0.0191
trigger times: 17
Loss after 8521500 batches: 0.0187
trigger times: 0
Loss after 8652600 batches: 0.0183
trigger times: 1
Loss after 8783700 batches: 0.0184
trigger times: 2
Loss after 8914800 batches: 0.0186
trigger times: 3
Loss after 9045900 batches: 0.0179
trigger times: 4
Loss after 9177000 batches: 0.0179
trigger times: 5
Loss after 9308100 batches: 0.0173
trigger times: 6
Loss after 9439200 batches: 0.0170
trigger times: 7
Loss after 9570300 batches: 0.0170
trigger times: 8
Loss after 9701400 batches: 0.0164
trigger times: 9
Loss after 9832500 batches: 0.0165
trigger times: 10
Loss after 9963600 batches: 0.0163
trigger times: 11
Loss after 10094700 batches: 0.0159
trigger times: 12
Loss after 10225800 batches: 0.0163
trigger times: 13
Loss after 10356900 batches: 0.0154
trigger times: 14
Loss after 10488000 batches: 0.0155
trigger times: 15
Loss after 10619100 batches: 0.0151
trigger times: 16
Loss after 10750200 batches: 0.0156
trigger times: 17
Loss after 10881300 batches: 0.0152
trigger times: 18
Loss after 11012400 batches: 0.0151
trigger times: 19
Loss after 11143500 batches: 0.0147
trigger times: 20
Early stopping!
Start to test process.
Loss after 11274600 batches: 0.0150
Time to train on one home:  619.7163996696472
trigger times: 0
Loss after 11377200 batches: 0.9651
trigger times: 1
Loss after 11479800 batches: 0.7778
trigger times: 0
Loss after 11582400 batches: 0.6472
trigger times: 0
Loss after 11685000 batches: 0.5544
trigger times: 1
Loss after 11787600 batches: 0.4845
trigger times: 0
Loss after 11890200 batches: 0.4463
trigger times: 0
Loss after 11992800 batches: 0.4283
trigger times: 1
Loss after 12095400 batches: 0.3455
trigger times: 2
Loss after 12198000 batches: 0.3144
trigger times: 3
Loss after 12300600 batches: 0.2796
trigger times: 4
Loss after 12403200 batches: 0.2558
trigger times: 5
Loss after 12505800 batches: 0.2432
trigger times: 6
Loss after 12608400 batches: 0.2514
trigger times: 7
Loss after 12711000 batches: 0.2152
trigger times: 8
Loss after 12813600 batches: 0.2005
trigger times: 9
Loss after 12916200 batches: 0.1851
trigger times: 10
Loss after 13018800 batches: 0.1763
trigger times: 11
Loss after 13121400 batches: 0.1839
trigger times: 12
Loss after 13224000 batches: 0.1706
trigger times: 13
Loss after 13326600 batches: 0.1605
trigger times: 14
Loss after 13429200 batches: 0.1472
trigger times: 15
Loss after 13531800 batches: 0.1378
trigger times: 16
Loss after 13634400 batches: 0.1333
trigger times: 17
Loss after 13737000 batches: 0.1210
trigger times: 18
Loss after 13839600 batches: 0.1235
trigger times: 19
Loss after 13942200 batches: 0.1214
trigger times: 20
Early stopping!
Start to test process.
Loss after 14044800 batches: 0.1229
Time to train on one home:  167.00913000106812
train_results:  [0.0689694630316821]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732]]
Round_0_results:  [0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 114 < 115; dropping {'Training_Loss': 0.2687372788786888, 'Validation_Loss': 0.26754182908270097, 'Training_R2': 0.7293177250354796, 'Validation_R2': 0.7511145867456166, 'Training_F1': 0.7118113606762928, 'Validation_F1': 0.6977744763599191, 'Training_NEP': 0.5777771125629527, 'Validation_NEP': 0.5803444426778788, 'Training_NDE': 0.20320598843369184, 'Validation_NDE': 0.19819922334285162, 'Training_MAE': 19.134832112510907, 'Validation_MAE': 15.915452319832259, 'Training_MSE': 894.0745, 'Validation_MSE': 731.9443}.
trigger times: 0
Loss after 14175900 batches: 0.2687
trigger times: 0
Loss after 14307000 batches: 0.0629
trigger times: 0
Loss after 14438100 batches: 0.0413
trigger times: 0
Loss after 14569200 batches: 0.0351
trigger times: 1
Loss after 14700300 batches: 0.0301
trigger times: 0
Loss after 14831400 batches: 0.0274
trigger times: 1
Loss after 14962500 batches: 0.0254
trigger times: 2
Loss after 15093600 batches: 0.0238
trigger times: 0
Loss after 15224700 batches: 0.0228
trigger times: 0
Loss after 15355800 batches: 0.0219
trigger times: 0
Loss after 15486900 batches: 0.0212
trigger times: 0
Loss after 15618000 batches: 0.0204
trigger times: 1
Loss after 15749100 batches: 0.0197
trigger times: 0
Loss after 15880200 batches: 0.0192
trigger times: 1
Loss after 16011300 batches: 0.0189
trigger times: 2
Loss after 16142400 batches: 0.0185
trigger times: 3
Loss after 16273500 batches: 0.0184
trigger times: 0
Loss after 16404600 batches: 0.0175
trigger times: 1
Loss after 16535700 batches: 0.0173
trigger times: 2
Loss after 16666800 batches: 0.0171
trigger times: 3
Loss after 16797900 batches: 0.0169
trigger times: 0
Loss after 16929000 batches: 0.0165
trigger times: 1
Loss after 17060100 batches: 0.0162
trigger times: 2
Loss after 17191200 batches: 0.0160
trigger times: 3
Loss after 17322300 batches: 0.0158
trigger times: 4
Loss after 17453400 batches: 0.0153
trigger times: 5
Loss after 17584500 batches: 0.0152
trigger times: 6
Loss after 17715600 batches: 0.0153
trigger times: 7
Loss after 17846700 batches: 0.0149
trigger times: 0
Loss after 17977800 batches: 0.0148
trigger times: 1
Loss after 18108900 batches: 0.0147
trigger times: 2
Loss after 18240000 batches: 0.0143
trigger times: 3
Loss after 18371100 batches: 0.0142
trigger times: 4
Loss after 18502200 batches: 0.0141
trigger times: 5
Loss after 18633300 batches: 0.0138
trigger times: 6
Loss after 18764400 batches: 0.0139
trigger times: 7
Loss after 18895500 batches: 0.0136
trigger times: 8
Loss after 19026600 batches: 0.0135
trigger times: 9
Loss after 19157700 batches: 0.0135
trigger times: 10
Loss after 19288800 batches: 0.0134
trigger times: 0
Loss after 19419900 batches: 0.0131
trigger times: 1
Loss after 19551000 batches: 0.0131
trigger times: 2
Loss after 19682100 batches: 0.0127
trigger times: 3
Loss after 19813200 batches: 0.0129
trigger times: 4
Loss after 19944300 batches: 0.0125
trigger times: 5
Loss after 20075400 batches: 0.0130
trigger times: 6
Loss after 20206500 batches: 0.0125
trigger times: 7
Loss after 20337600 batches: 0.0124
trigger times: 8
Loss after 20468700 batches: 0.0122
trigger times: 9
Loss after 20599800 batches: 0.0121
trigger times: 10
Loss after 20730900 batches: 0.0121
trigger times: 11
Loss after 20862000 batches: 0.0123
trigger times: 12
Loss after 20993100 batches: 0.0122
trigger times: 13
Loss after 21124200 batches: 0.0120
trigger times: 14
Loss after 21255300 batches: 0.0119
trigger times: 15
Loss after 21386400 batches: 0.0116
trigger times: 16
Loss after 21517500 batches: 0.0116
trigger times: 17
Loss after 21648600 batches: 0.0115
trigger times: 18
Loss after 21779700 batches: 0.0116
trigger times: 19
Loss after 21910800 batches: 0.0114
trigger times: 20
Early stopping!
Start to test process.
Loss after 22041900 batches: 0.0116
Time to train on one home:  444.6615250110626
trigger times: 0
Loss after 22144500 batches: 0.5797
trigger times: 1
Loss after 22247100 batches: 0.3165
trigger times: 2
Loss after 22349700 batches: 0.2150
trigger times: 3
Loss after 22452300 batches: 0.1664
trigger times: 4
Loss after 22554900 batches: 0.1456
trigger times: 5
Loss after 22657500 batches: 0.1273
trigger times: 6
Loss after 22760100 batches: 0.1167
trigger times: 7
Loss after 22862700 batches: 0.1043
trigger times: 8
Loss after 22965300 batches: 0.0987
trigger times: 9
Loss after 23067900 batches: 0.0965
trigger times: 10
Loss after 23170500 batches: 0.1094
trigger times: 11
Loss after 23273100 batches: 0.1036
trigger times: 12
Loss after 23375700 batches: 0.0807
trigger times: 13
Loss after 23478300 batches: 0.0757
trigger times: 14
Loss after 23580900 batches: 0.0769
trigger times: 15
Loss after 23683500 batches: 0.0771
trigger times: 16
Loss after 23786100 batches: 0.0684
trigger times: 17
Loss after 23888700 batches: 0.0679
trigger times: 18
Loss after 23991300 batches: 0.0617
trigger times: 19
Loss after 24093900 batches: 0.0632
trigger times: 20
Early stopping!
Start to test process.
Loss after 24196500 batches: 0.0628
Time to train on one home:  132.43622994422913
train_results:  [0.0689694630316821, 0.037200865878824804]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417]]
Round_1_results:  [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417]
trigger times: 0
Loss after 24327600 batches: 0.0623
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 196 < 197; dropping {'Training_Loss': 0.06225758994806488, 'Validation_Loss': 0.22711321794324452, 'Training_R2': 0.9373437383851668, 'Validation_R2': 0.7886279612246258, 'Training_F1': 0.8620780110088353, 'Validation_F1': 0.7337799622345402, 'Training_NEP': 0.2761834951353016, 'Validation_NEP': 0.5159135956189642, 'Training_NDE': 0.04703716774462255, 'Validation_NDE': 0.1683255493919005, 'Training_MAE': 9.146649627947426, 'Validation_MAE': 14.148491186266732, 'Training_MSE': 206.95616, 'Validation_MSE': 621.6216}.
trigger times: 1
Loss after 24458700 batches: 0.0234
trigger times: 0
Loss after 24589800 batches: 0.0192
trigger times: 0
Loss after 24720900 batches: 0.0172
trigger times: 1
Loss after 24852000 batches: 0.0166
trigger times: 2
Loss after 24983100 batches: 0.0158
trigger times: 3
Loss after 25114200 batches: 0.0152
trigger times: 4
Loss after 25245300 batches: 0.0146
trigger times: 5
Loss after 25376400 batches: 0.0143
trigger times: 0
Loss after 25507500 batches: 0.0138
trigger times: 1
Loss after 25638600 batches: 0.0136
trigger times: 2
Loss after 25769700 batches: 0.0135
trigger times: 3
Loss after 25900800 batches: 0.0129
trigger times: 4
Loss after 26031900 batches: 0.0126
trigger times: 5
Loss after 26163000 batches: 0.0130
trigger times: 6
Loss after 26294100 batches: 0.0125
trigger times: 7
Loss after 26425200 batches: 0.0123
trigger times: 8
Loss after 26556300 batches: 0.0122
trigger times: 9
Loss after 26687400 batches: 0.0123
trigger times: 10
Loss after 26818500 batches: 0.0119
trigger times: 11
Loss after 26949600 batches: 0.0116
trigger times: 12
Loss after 27080700 batches: 0.0118
trigger times: 13
Loss after 27211800 batches: 0.0116
trigger times: 14
Loss after 27342900 batches: 0.0116
trigger times: 15
Loss after 27474000 batches: 0.0120
trigger times: 16
Loss after 27605100 batches: 0.0114
trigger times: 17
Loss after 27736200 batches: 0.0112
trigger times: 18
Loss after 27867300 batches: 0.0111
trigger times: 19
Loss after 27998400 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 28129500 batches: 0.0111
Time to train on one home:  224.06791734695435
trigger times: 0
Loss after 28232100 batches: 0.3089
trigger times: 1
Loss after 28334700 batches: 0.1228
trigger times: 2
Loss after 28437300 batches: 0.0979
trigger times: 3
Loss after 28539900 batches: 0.0801
trigger times: 4
Loss after 28642500 batches: 0.0769
trigger times: 5
Loss after 28745100 batches: 0.0701
trigger times: 6
Loss after 28847700 batches: 0.0655
trigger times: 7
Loss after 28950300 batches: 0.0596
trigger times: 8
Loss after 29052900 batches: 0.0560
trigger times: 9
Loss after 29155500 batches: 0.0548
trigger times: 10
Loss after 29258100 batches: 0.0568
trigger times: 11
Loss after 29360700 batches: 0.0541
trigger times: 12
Loss after 29463300 batches: 0.0498
trigger times: 13
Loss after 29565900 batches: 0.0474
trigger times: 14
Loss after 29668500 batches: 0.0505
trigger times: 15
Loss after 29771100 batches: 0.0461
trigger times: 16
Loss after 29873700 batches: 0.0425
trigger times: 17
Loss after 29976300 batches: 0.0417
trigger times: 18
Loss after 30078900 batches: 0.0403
trigger times: 19
Loss after 30181500 batches: 0.0435
trigger times: 20
Early stopping!
Start to test process.
Loss after 30284100 batches: 0.0416
Time to train on one home:  132.2182650566101
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461]]
Round_2_results:  [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 247 < 248; dropping {'Training_Loss': 0.039870604486116824, 'Validation_Loss': 0.2304050773382187, 'Training_R2': 0.9598905906212873, 'Validation_R2': 0.7857220259009124, 'Training_F1': 0.8888984329769656, 'Validation_F1': 0.7579700114470203, 'Training_NEP': 0.2223406323404491, 'Validation_NEP': 0.4709717018877625, 'Training_NDE': 0.030110845563719395, 'Validation_NDE': 0.17063968309991268, 'Training_MAE': 7.363480794093338, 'Validation_MAE': 12.915998007661553, 'Training_MSE': 132.48302, 'Validation_MSE': 630.16766}.
trigger times: 0
Loss after 30415200 batches: 0.0399
trigger times: 1
Loss after 30546300 batches: 0.0179
trigger times: 2
Loss after 30677400 batches: 0.0154
trigger times: 3
Loss after 30808500 batches: 0.0142
trigger times: 4
Loss after 30939600 batches: 0.0134
trigger times: 5
Loss after 31070700 batches: 0.0132
trigger times: 6
Loss after 31201800 batches: 0.0126
trigger times: 7
Loss after 31332900 batches: 0.0127
trigger times: 8
Loss after 31464000 batches: 0.0124
trigger times: 9
Loss after 31595100 batches: 0.0122
trigger times: 10
Loss after 31726200 batches: 0.0119
trigger times: 11
Loss after 31857300 batches: 0.0116
trigger times: 12
Loss after 31988400 batches: 0.0115
trigger times: 13
Loss after 32119500 batches: 0.0114
trigger times: 14
Loss after 32250600 batches: 0.0114
trigger times: 15
Loss after 32381700 batches: 0.0111
trigger times: 16
Loss after 32512800 batches: 0.0110
trigger times: 17
Loss after 32643900 batches: 0.0109
trigger times: 18
Loss after 32775000 batches: 0.0110
trigger times: 19
Loss after 32906100 batches: 0.0107
trigger times: 20
Early stopping!
Start to test process.
Loss after 33037200 batches: 0.0107
Time to train on one home:  159.93159890174866
trigger times: 0
Loss after 33139800 batches: 0.2008
trigger times: 1
Loss after 33242400 batches: 0.0742
trigger times: 2
Loss after 33345000 batches: 0.0632
trigger times: 3
Loss after 33447600 batches: 0.0558
trigger times: 4
Loss after 33550200 batches: 0.0574
trigger times: 5
Loss after 33652800 batches: 0.0553
trigger times: 6
Loss after 33755400 batches: 0.0455
trigger times: 7
Loss after 33858000 batches: 0.0430
trigger times: 8
Loss after 33960600 batches: 0.0505
trigger times: 9
Loss after 34063200 batches: 0.0447
trigger times: 10
Loss after 34165800 batches: 0.0396
trigger times: 11
Loss after 34268400 batches: 0.0368
trigger times: 12
Loss after 34371000 batches: 0.0369
trigger times: 13
Loss after 34473600 batches: 0.0383
trigger times: 14
Loss after 34576200 batches: 0.0368
trigger times: 15
Loss after 34678800 batches: 0.0351
trigger times: 16
Loss after 34781400 batches: 0.0328
trigger times: 17
Loss after 34884000 batches: 0.0320
trigger times: 18
Loss after 34986600 batches: 0.0322
trigger times: 19
Loss after 35089200 batches: 0.0344
trigger times: 20
Early stopping!
Start to test process.
Loss after 35191800 batches: 0.0307
Time to train on one home:  132.35304880142212
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036]]
Round_3_results:  [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 289 < 290; dropping {'Training_Loss': 0.03132015083617759, 'Validation_Loss': 0.20961336625946891, 'Training_R2': 0.9685141265235445, 'Validation_R2': 0.8049798836806396, 'Training_F1': 0.9010791853499036, 'Validation_F1': 0.7318268502666955, 'Training_NEP': 0.19797559376330134, 'Validation_NEP': 0.49516261793873834, 'Training_NDE': 0.023637004093895447, 'Validation_NDE': 0.15530374032496283, 'Training_MAE': 6.556559037500261, 'Validation_MAE': 13.579413287742163, 'Training_MSE': 103.999115, 'Validation_MSE': 573.5324}.
trigger times: 0
Loss after 35322900 batches: 0.0313
trigger times: 0
Loss after 35454000 batches: 0.0155
trigger times: 0
Loss after 35585100 batches: 0.0139
trigger times: 0
Loss after 35716200 batches: 0.0129
trigger times: 0
Loss after 35847300 batches: 0.0125
trigger times: 1
Loss after 35978400 batches: 0.0122
trigger times: 2
Loss after 36109500 batches: 0.0117
trigger times: 3
Loss after 36240600 batches: 0.0117
trigger times: 4
Loss after 36371700 batches: 0.0115
trigger times: 5
Loss after 36502800 batches: 0.0111
trigger times: 6
Loss after 36633900 batches: 0.0109
trigger times: 7
Loss after 36765000 batches: 0.0111
trigger times: 8
Loss after 36896100 batches: 0.0108
trigger times: 9
Loss after 37027200 batches: 0.0103
trigger times: 10
Loss after 37158300 batches: 0.0106
trigger times: 11
Loss after 37289400 batches: 0.0103
trigger times: 12
Loss after 37420500 batches: 0.0103
trigger times: 13
Loss after 37551600 batches: 0.0104
trigger times: 14
Loss after 37682700 batches: 0.0103
trigger times: 15
Loss after 37813800 batches: 0.0102
trigger times: 16
Loss after 37944900 batches: 0.0104
trigger times: 17
Loss after 38076000 batches: 0.0102
trigger times: 18
Loss after 38207100 batches: 0.0100
trigger times: 19
Loss after 38338200 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 38469300 batches: 0.0099
Time to train on one home:  188.79679799079895
trigger times: 0
Loss after 38571900 batches: 0.1465
trigger times: 1
Loss after 38674500 batches: 0.0564
trigger times: 2
Loss after 38777100 batches: 0.0462
trigger times: 3
Loss after 38879700 batches: 0.0436
trigger times: 4
Loss after 38982300 batches: 0.0384
trigger times: 5
Loss after 39084900 batches: 0.0362
trigger times: 6
Loss after 39187500 batches: 0.0355
trigger times: 7
Loss after 39290100 batches: 0.0342
trigger times: 8
Loss after 39392700 batches: 0.0336
trigger times: 9
Loss after 39495300 batches: 0.0340
trigger times: 10
Loss after 39597900 batches: 0.0388
trigger times: 11
Loss after 39700500 batches: 0.0323
trigger times: 12
Loss after 39803100 batches: 0.0353
trigger times: 13
Loss after 39905700 batches: 0.0327
trigger times: 14
Loss after 40008300 batches: 0.0323
trigger times: 15
Loss after 40110900 batches: 0.0324
trigger times: 16
Loss after 40213500 batches: 0.0296
trigger times: 17
Loss after 40316100 batches: 0.0300
trigger times: 18
Loss after 40418700 batches: 0.0279
trigger times: 19
Loss after 40521300 batches: 0.0274
trigger times: 20
Early stopping!
Start to test process.
Loss after 40623900 batches: 0.0267
Time to train on one home:  132.36578035354614
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269]]
Round_4_results:  [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269]
trigger times: 0
Loss after 40755000 batches: 0.0337
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 335 < 336; dropping {'Training_Loss': 0.03369728807922242, 'Validation_Loss': 0.23302484220928615, 'Training_R2': 0.9660773947121284, 'Validation_R2': 0.7833133077769092, 'Training_F1': 0.897618481928482, 'Validation_F1': 0.7357605556182594, 'Training_NEP': 0.20495678536500694, 'Validation_NEP': 0.4965894534654435, 'Training_NDE': 0.025466301916782137, 'Validation_NDE': 0.17255785924043793, 'Training_MAE': 6.787762258152849, 'Validation_MAE': 13.618543037462405, 'Training_MSE': 112.04774, 'Validation_MSE': 637.2514}.
trigger times: 1
Loss after 40886100 batches: 0.0144
trigger times: 0
Loss after 41017200 batches: 0.0124
trigger times: 0
Loss after 41148300 batches: 0.0118
trigger times: 1
Loss after 41279400 batches: 0.0115
trigger times: 2
Loss after 41410500 batches: 0.0112
trigger times: 3
Loss after 41541600 batches: 0.0108
trigger times: 4
Loss after 41672700 batches: 0.0105
trigger times: 5
Loss after 41803800 batches: 0.0105
trigger times: 6
Loss after 41934900 batches: 0.0103
trigger times: 7
Loss after 42066000 batches: 0.0103
trigger times: 0
Loss after 42197100 batches: 0.0101
trigger times: 1
Loss after 42328200 batches: 0.0101
trigger times: 2
Loss after 42459300 batches: 0.0100
trigger times: 3
Loss after 42590400 batches: 0.0100
trigger times: 4
Loss after 42721500 batches: 0.0099
trigger times: 5
Loss after 42852600 batches: 0.0098
trigger times: 6
Loss after 42983700 batches: 0.0096
trigger times: 7
Loss after 43114800 batches: 0.0096
trigger times: 8
Loss after 43245900 batches: 0.0095
trigger times: 9
Loss after 43377000 batches: 0.0095
trigger times: 10
Loss after 43508100 batches: 0.0095
trigger times: 11
Loss after 43639200 batches: 0.0095
trigger times: 12
Loss after 43770300 batches: 0.0091
trigger times: 13
Loss after 43901400 batches: 0.0092
trigger times: 14
Loss after 44032500 batches: 0.0094
trigger times: 15
Loss after 44163600 batches: 0.0093
trigger times: 16
Loss after 44294700 batches: 0.0092
trigger times: 17
Loss after 44425800 batches: 0.0090
trigger times: 18
Loss after 44556900 batches: 0.0089
trigger times: 19
Loss after 44688000 batches: 0.0090
trigger times: 20
Early stopping!
Start to test process.
Loss after 44819100 batches: 0.0090
Time to train on one home:  238.18212032318115
trigger times: 0
Loss after 44921700 batches: 0.1243
trigger times: 0
Loss after 45024300 batches: 0.0444
trigger times: 1
Loss after 45126900 batches: 0.0410
trigger times: 2
Loss after 45229500 batches: 0.0407
trigger times: 3
Loss after 45332100 batches: 0.0407
trigger times: 4
Loss after 45434700 batches: 0.0337
trigger times: 5
Loss after 45537300 batches: 0.0321
trigger times: 6
Loss after 45639900 batches: 0.0307
trigger times: 7
Loss after 45742500 batches: 0.0301
trigger times: 8
Loss after 45845100 batches: 0.0303
trigger times: 9
Loss after 45947700 batches: 0.0286
trigger times: 10
Loss after 46050300 batches: 0.0300
trigger times: 11
Loss after 46152900 batches: 0.0286
trigger times: 12
Loss after 46255500 batches: 0.0271
trigger times: 13
Loss after 46358100 batches: 0.0265
trigger times: 14
Loss after 46460700 batches: 0.0280
trigger times: 15
Loss after 46563300 batches: 0.0299
trigger times: 16
Loss after 46665900 batches: 0.0260
trigger times: 17
Loss after 46768500 batches: 0.0259
trigger times: 18
Loss after 46871100 batches: 0.0248
trigger times: 19
Loss after 46973700 batches: 0.0243
trigger times: 20
Early stopping!
Start to test process.
Loss after 47076300 batches: 0.0230
Time to train on one home:  137.86091351509094
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113]]
Round_5_results:  [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113]
trigger times: 0
Loss after 47207400 batches: 0.0287
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 389 < 390; dropping {'Training_Loss': 0.028706613998368102, 'Validation_Loss': 0.2196604965461625, 'Training_R2': 0.9711188700903424, 'Validation_R2': 0.7958076773702352, 'Training_F1': 0.9054047381895816, 'Validation_F1': 0.7348928846780411, 'Training_NEP': 0.18940556557765797, 'Validation_NEP': 0.49309570125197333, 'Training_NDE': 0.021681576864030338, 'Validation_NDE': 0.1626080019258806, 'Training_MAE': 6.272736700190378, 'Validation_MAE': 13.522729857079018, 'Training_MSE': 95.395546, 'Validation_MSE': 600.50684}.
trigger times: 0
Loss after 47338500 batches: 0.0130
trigger times: 1
Loss after 47469600 batches: 0.0114
trigger times: 2
Loss after 47600700 batches: 0.0111
trigger times: 0
Loss after 47731800 batches: 0.0104
trigger times: 1
Loss after 47862900 batches: 0.0102
trigger times: 0
Loss after 47994000 batches: 0.0105
trigger times: 1
Loss after 48125100 batches: 0.0099
trigger times: 2
Loss after 48256200 batches: 0.0098
trigger times: 0
Loss after 48387300 batches: 0.0096
trigger times: 1
Loss after 48518400 batches: 0.0096
trigger times: 2
Loss after 48649500 batches: 0.0094
trigger times: 3
Loss after 48780600 batches: 0.0094
trigger times: 4
Loss after 48911700 batches: 0.0093
trigger times: 5
Loss after 49042800 batches: 0.0090
trigger times: 6
Loss after 49173900 batches: 0.0091
trigger times: 7
Loss after 49305000 batches: 0.0092
trigger times: 8
Loss after 49436100 batches: 0.0090
trigger times: 9
Loss after 49567200 batches: 0.0089
trigger times: 10
Loss after 49698300 batches: 0.0093
trigger times: 11
Loss after 49829400 batches: 0.0087
trigger times: 12
Loss after 49960500 batches: 0.0090
trigger times: 13
Loss after 50091600 batches: 0.0089
trigger times: 14
Loss after 50222700 batches: 0.0089
trigger times: 15
Loss after 50353800 batches: 0.0086
trigger times: 16
Loss after 50484900 batches: 0.0086
trigger times: 17
Loss after 50616000 batches: 0.0086
trigger times: 18
Loss after 50747100 batches: 0.0085
trigger times: 19
Loss after 50878200 batches: 0.0085
trigger times: 0
Loss after 51009300 batches: 0.0086
trigger times: 1
Loss after 51140400 batches: 0.0085
trigger times: 2
Loss after 51271500 batches: 0.0086
trigger times: 3
Loss after 51402600 batches: 0.0085
trigger times: 4
Loss after 51533700 batches: 0.0084
trigger times: 5
Loss after 51664800 batches: 0.0085
trigger times: 6
Loss after 51795900 batches: 0.0085
trigger times: 7
Loss after 51927000 batches: 0.0082
trigger times: 8
Loss after 52058100 batches: 0.0081
trigger times: 9
Loss after 52189200 batches: 0.0083
trigger times: 10
Loss after 52320300 batches: 0.0084
trigger times: 11
Loss after 52451400 batches: 0.0082
trigger times: 12
Loss after 52582500 batches: 0.0081
trigger times: 13
Loss after 52713600 batches: 0.0083
trigger times: 14
Loss after 52844700 batches: 0.0079
trigger times: 15
Loss after 52975800 batches: 0.0078
trigger times: 16
Loss after 53106900 batches: 0.0081
trigger times: 17
Loss after 53238000 batches: 0.0081
trigger times: 18
Loss after 53369100 batches: 0.0080
trigger times: 19
Loss after 53500200 batches: 0.0078
trigger times: 20
Early stopping!
Start to test process.
Loss after 53631300 batches: 0.0078
Time to train on one home:  366.2934060096741
trigger times: 0
Loss after 53733900 batches: 0.1145
trigger times: 1
Loss after 53836500 batches: 0.0402
trigger times: 2
Loss after 53939100 batches: 0.0341
trigger times: 3
Loss after 54041700 batches: 0.0320
trigger times: 4
Loss after 54144300 batches: 0.0309
trigger times: 5
Loss after 54246900 batches: 0.0293
trigger times: 6
Loss after 54349500 batches: 0.0302
trigger times: 7
Loss after 54452100 batches: 0.0337
trigger times: 8
Loss after 54554700 batches: 0.0322
trigger times: 9
Loss after 54657300 batches: 0.0275
trigger times: 10
Loss after 54759900 batches: 0.0261
trigger times: 11
Loss after 54862500 batches: 0.0271
trigger times: 12
Loss after 54965100 batches: 0.0296
trigger times: 13
Loss after 55067700 batches: 0.0251
trigger times: 14
Loss after 55170300 batches: 0.0237
trigger times: 15
Loss after 55272900 batches: 0.0241
trigger times: 16
Loss after 55375500 batches: 0.0244
trigger times: 17
Loss after 55478100 batches: 0.0235
trigger times: 18
Loss after 55580700 batches: 0.0238
trigger times: 19
Loss after 55683300 batches: 0.0241
trigger times: 20
Early stopping!
Start to test process.
Loss after 55785900 batches: 0.0232
Time to train on one home:  132.23861384391785
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846]]
Round_6_results:  [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846]
trigger times: 0
Loss after 55917000 batches: 0.0264
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 460 < 461; dropping {'Training_Loss': 0.026413867109507886, 'Validation_Loss': 0.22938311596711478, 'Training_R2': 0.9734374435594384, 'Validation_R2': 0.7867816978679338, 'Training_F1': 0.9085934333730044, 'Validation_F1': 0.7420061402123023, 'Training_NEP': 0.18300921852378266, 'Validation_NEP': 0.4903372709492382, 'Training_NDE': 0.01994098260603712, 'Validation_NDE': 0.1697958162050412, 'Training_MAE': 6.060902371090118, 'Validation_MAE': 13.447082254151717, 'Training_MSE': 87.73719, 'Validation_MSE': 627.0513}.
trigger times: 0
Loss after 56048100 batches: 0.0117
trigger times: 1
Loss after 56179200 batches: 0.0104
trigger times: 0
Loss after 56310300 batches: 0.0098
trigger times: 1
Loss after 56441400 batches: 0.0095
trigger times: 2
Loss after 56572500 batches: 0.0093
trigger times: 3
Loss after 56703600 batches: 0.0091
trigger times: 4
Loss after 56834700 batches: 0.0087
trigger times: 5
Loss after 56965800 batches: 0.0088
trigger times: 6
Loss after 57096900 batches: 0.0087
trigger times: 7
Loss after 57228000 batches: 0.0087
trigger times: 8
Loss after 57359100 batches: 0.0086
trigger times: 0
Loss after 57490200 batches: 0.0088
trigger times: 1
Loss after 57621300 batches: 0.0086
trigger times: 2
Loss after 57752400 batches: 0.0084
trigger times: 3
Loss after 57883500 batches: 0.0083
trigger times: 4
Loss after 58014600 batches: 0.0083
trigger times: 5
Loss after 58145700 batches: 0.0083
trigger times: 6
Loss after 58276800 batches: 0.0081
trigger times: 7
Loss after 58407900 batches: 0.0080
trigger times: 8
Loss after 58539000 batches: 0.0081
trigger times: 9
Loss after 58670100 batches: 0.0079
trigger times: 10
Loss after 58801200 batches: 0.0082
trigger times: 11
Loss after 58932300 batches: 0.0081
trigger times: 0
Loss after 59063400 batches: 0.0079
trigger times: 1
Loss after 59194500 batches: 0.0080
trigger times: 2
Loss after 59325600 batches: 0.0079
trigger times: 3
Loss after 59456700 batches: 0.0077
trigger times: 4
Loss after 59587800 batches: 0.0077
trigger times: 5
Loss after 59718900 batches: 0.0078
trigger times: 6
Loss after 59850000 batches: 0.0080
trigger times: 7
Loss after 59981100 batches: 0.0078
trigger times: 8
Loss after 60112200 batches: 0.0078
trigger times: 9
Loss after 60243300 batches: 0.0077
trigger times: 10
Loss after 60374400 batches: 0.0077
trigger times: 11
Loss after 60505500 batches: 0.0077
trigger times: 12
Loss after 60636600 batches: 0.0080
trigger times: 13
Loss after 60767700 batches: 0.0077
trigger times: 14
Loss after 60898800 batches: 0.0076
trigger times: 15
Loss after 61029900 batches: 0.0075
trigger times: 16
Loss after 61161000 batches: 0.0073
trigger times: 17
Loss after 61292100 batches: 0.0074
trigger times: 18
Loss after 61423200 batches: 0.0074
trigger times: 19
Loss after 61554300 batches: 0.0073
trigger times: 20
Early stopping!
Start to test process.
Loss after 61685400 batches: 0.0074
Time to train on one home:  330.2016763687134
trigger times: 0
Loss after 61788000 batches: 0.1164
trigger times: 0
Loss after 61890600 batches: 0.0385
trigger times: 0
Loss after 61993200 batches: 0.0317
trigger times: 1
Loss after 62095800 batches: 0.0308
trigger times: 2
Loss after 62198400 batches: 0.0283
trigger times: 3
Loss after 62301000 batches: 0.0285
trigger times: 4
Loss after 62403600 batches: 0.0268
trigger times: 5
Loss after 62506200 batches: 0.0261
trigger times: 6
Loss after 62608800 batches: 0.0251
trigger times: 7
Loss after 62711400 batches: 0.0261
trigger times: 8
Loss after 62814000 batches: 0.0250
trigger times: 9
Loss after 62916600 batches: 0.0242
trigger times: 10
Loss after 63019200 batches: 0.0261
trigger times: 11
Loss after 63121800 batches: 0.0243
trigger times: 12
Loss after 63224400 batches: 0.0247
trigger times: 13
Loss after 63327000 batches: 0.0236
trigger times: 14
Loss after 63429600 batches: 0.0252
trigger times: 15
Loss after 63532200 batches: 0.0229
trigger times: 16
Loss after 63634800 batches: 0.0239
trigger times: 17
Loss after 63737400 batches: 0.0254
trigger times: 18
Loss after 63840000 batches: 0.0255
trigger times: 19
Loss after 63942600 batches: 0.0233
trigger times: 20
Early stopping!
Start to test process.
Loss after 64045200 batches: 0.0235
Time to train on one home:  143.57787442207336
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756]]
Round_7_results:  [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 528 < 529; dropping {'Training_Loss': 0.02641325951817463, 'Validation_Loss': 0.2154605992966228, 'Training_R2': 0.9734173341050895, 'Validation_R2': 0.7997057158387065, 'Training_F1': 0.9084659072866313, 'Validation_F1': 0.7558498544571103, 'Training_NEP': 0.1833155530414694, 'Validation_NEP': 0.4680468668487807, 'Training_NDE': 0.019956079130360214, 'Validation_NDE': 0.1595038095712169, 'Training_MAE': 6.071047562789044, 'Validation_MAE': 12.835786896495408, 'Training_MSE': 87.80362, 'Validation_MSE': 589.04315}.
trigger times: 0
Loss after 64176300 batches: 0.0264
trigger times: 1
Loss after 64307400 batches: 0.0108
trigger times: 2
Loss after 64438500 batches: 0.0096
trigger times: 0
Loss after 64569600 batches: 0.0090
trigger times: 1
Loss after 64700700 batches: 0.0087
trigger times: 2
Loss after 64831800 batches: 0.0084
trigger times: 3
Loss after 64962900 batches: 0.0083
trigger times: 4
Loss after 65094000 batches: 0.0082
trigger times: 5
Loss after 65225100 batches: 0.0080
trigger times: 6
Loss after 65356200 batches: 0.0080
trigger times: 7
Loss after 65487300 batches: 0.0080
trigger times: 8
Loss after 65618400 batches: 0.0077
trigger times: 9
Loss after 65749500 batches: 0.0077
trigger times: 10
Loss after 65880600 batches: 0.0078
trigger times: 11
Loss after 66011700 batches: 0.0077
trigger times: 12
Loss after 66142800 batches: 0.0077
trigger times: 13
Loss after 66273900 batches: 0.0076
trigger times: 14
Loss after 66405000 batches: 0.0076
trigger times: 15
Loss after 66536100 batches: 0.0079
trigger times: 16
Loss after 66667200 batches: 0.0075
trigger times: 17
Loss after 66798300 batches: 0.0074
trigger times: 18
Loss after 66929400 batches: 0.0074
trigger times: 19
Loss after 67060500 batches: 0.0073
trigger times: 20
Early stopping!
Start to test process.
Loss after 67191600 batches: 0.0075
Time to train on one home:  181.68726563453674
trigger times: 0
Loss after 67294200 batches: 0.1082
trigger times: 1
Loss after 67396800 batches: 0.0357
trigger times: 2
Loss after 67499400 batches: 0.0304
trigger times: 3
Loss after 67602000 batches: 0.0279
trigger times: 4
Loss after 67704600 batches: 0.0270
trigger times: 5
Loss after 67807200 batches: 0.0262
trigger times: 6
Loss after 67909800 batches: 0.0260
trigger times: 7
Loss after 68012400 batches: 0.0276
trigger times: 8
Loss after 68115000 batches: 0.0257
trigger times: 9
Loss after 68217600 batches: 0.0252
trigger times: 10
Loss after 68320200 batches: 0.0236
trigger times: 11
Loss after 68422800 batches: 0.0234
trigger times: 12
Loss after 68525400 batches: 0.0240
trigger times: 13
Loss after 68628000 batches: 0.0225
trigger times: 14
Loss after 68730600 batches: 0.0216
trigger times: 15
Loss after 68833200 batches: 0.0220
trigger times: 16
Loss after 68935800 batches: 0.0216
trigger times: 17
Loss after 69038400 batches: 0.0212
trigger times: 18
Loss after 69141000 batches: 0.0210
trigger times: 19
Loss after 69243600 batches: 0.0240
trigger times: 20
Early stopping!
Start to test process.
Loss after 69346200 batches: 0.0213
Time to train on one home:  132.2257444858551
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122, 0.014372845554577064]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756], [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386]]
Round_8_results:  [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386]
trigger times: 0
Loss after 69477300 batches: 0.0234
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 573 < 574; dropping {'Training_Loss': 0.02344278529075519, 'Validation_Loss': 0.21161100433932412, 'Training_R2': 0.976414892747396, 'Validation_R2': 0.8032347873371123, 'Training_F1': 0.9153580589955358, 'Validation_F1': 0.7620091946018829, 'Training_NEP': 0.16933811642087299, 'Validation_NEP': 0.4609815818724764, 'Training_NDE': 0.017705758650832317, 'Validation_NDE': 0.15669344306174812, 'Training_MAE': 5.6081425821608395, 'Validation_MAE': 12.642027470373339, 'Training_MSE': 77.902565, 'Validation_MSE': 578.6646}.
trigger times: 1
Loss after 69608400 batches: 0.0104
trigger times: 2
Loss after 69739500 batches: 0.0090
trigger times: 3
Loss after 69870600 batches: 0.0086
trigger times: 4
Loss after 70001700 batches: 0.0084
trigger times: 5
Loss after 70132800 batches: 0.0084
trigger times: 6
Loss after 70263900 batches: 0.0080
trigger times: 7
Loss after 70395000 batches: 0.0079
trigger times: 8
Loss after 70526100 batches: 0.0079
trigger times: 9
Loss after 70657200 batches: 0.0080
trigger times: 10
Loss after 70788300 batches: 0.0078
trigger times: 11
Loss after 70919400 batches: 0.0077
trigger times: 12
Loss after 71050500 batches: 0.0076
trigger times: 13
Loss after 71181600 batches: 0.0078
trigger times: 14
Loss after 71312700 batches: 0.0074
trigger times: 15
Loss after 71443800 batches: 0.0075
trigger times: 16
Loss after 71574900 batches: 0.0076
trigger times: 17
Loss after 71706000 batches: 0.0075
trigger times: 18
Loss after 71837100 batches: 0.0074
trigger times: 19
Loss after 71968200 batches: 0.0073
trigger times: 20
Early stopping!
Start to test process.
Loss after 72099300 batches: 0.0073
Time to train on one home:  160.5376329421997
trigger times: 0
Loss after 72201900 batches: 0.0930
trigger times: 1
Loss after 72304500 batches: 0.0326
trigger times: 0
Loss after 72407100 batches: 0.0282
trigger times: 1
Loss after 72509700 batches: 0.0266
trigger times: 2
Loss after 72612300 batches: 0.0251
trigger times: 3
Loss after 72714900 batches: 0.0237
trigger times: 4
Loss after 72817500 batches: 0.0234
trigger times: 5
Loss after 72920100 batches: 0.0236
trigger times: 6
Loss after 73022700 batches: 0.0294
trigger times: 7
Loss after 73125300 batches: 0.0283
trigger times: 8
Loss after 73227900 batches: 0.0236
trigger times: 9
Loss after 73330500 batches: 0.0220
trigger times: 10
Loss after 73433100 batches: 0.0224
trigger times: 11
Loss after 73535700 batches: 0.0223
trigger times: 12
Loss after 73638300 batches: 0.0216
trigger times: 13
Loss after 73740900 batches: 0.0212
trigger times: 14
Loss after 73843500 batches: 0.0208
trigger times: 15
Loss after 73946100 batches: 0.0208
trigger times: 16
Loss after 74048700 batches: 0.0202
trigger times: 17
Loss after 74151300 batches: 0.0208
trigger times: 18
Loss after 74253900 batches: 0.0202
trigger times: 19
Loss after 74356500 batches: 0.0209
trigger times: 20
Early stopping!
Start to test process.
Loss after 74459100 batches: 0.0192
Time to train on one home:  144.08846521377563
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122, 0.014372845554577064, 0.013282680470221957]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756], [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386], [0.29729848437839085, 0.6796477634281426, 0.7054645356699167, 0.6421362552432476, 0.26243081830278, 15.170699187034348, 810.14026]]
Round_9_results:  [0.29729848437839085, 0.6796477634281426, 0.7054645356699167, 0.6421362552432476, 0.26243081830278, 15.170699187034348, 810.14026]
trigger times: 0
Loss after 74590200 batches: 0.0256
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 617 < 618; dropping {'Training_Loss': 0.025620196664811305, 'Validation_Loss': 0.20390765203369987, 'Training_R2': 0.974229753555296, 'Validation_R2': 0.8104912504806216, 'Training_F1': 0.9108717256295304, 'Validation_F1': 0.7516655775784032, 'Training_NEP': 0.17843507526173544, 'Validation_NEP': 0.4715759069283208, 'Training_NDE': 0.01934618142862267, 'Validation_NDE': 0.1509147783322503, 'Training_MAE': 5.909415817755401, 'Validation_MAE': 12.932567816566824, 'Training_MSE': 85.12016, 'Validation_MSE': 557.3241}.
trigger times: 1
Loss after 74721300 batches: 0.0105
trigger times: 2
Loss after 74852400 batches: 0.0091
trigger times: 3
Loss after 74983500 batches: 0.0085
trigger times: 4
Loss after 75114600 batches: 0.0082
trigger times: 5
Loss after 75245700 batches: 0.0081
trigger times: 6
Loss after 75376800 batches: 0.0076
trigger times: 7
Loss after 75507900 batches: 0.0078
trigger times: 8
Loss after 75639000 batches: 0.0077
trigger times: 9
Loss after 75770100 batches: 0.0074
trigger times: 0
Loss after 75901200 batches: 0.0076
trigger times: 1
Loss after 76032300 batches: 0.0075
trigger times: 2
Loss after 76163400 batches: 0.0075
trigger times: 3
Loss after 76294500 batches: 0.0075
trigger times: 0
Loss after 76425600 batches: 0.0076
trigger times: 1
Loss after 76556700 batches: 0.0074
trigger times: 2
Loss after 76687800 batches: 0.0073
trigger times: 3
Loss after 76818900 batches: 0.0073
trigger times: 4
Loss after 76950000 batches: 0.0072
trigger times: 5
Loss after 77081100 batches: 0.0072
trigger times: 6
Loss after 77212200 batches: 0.0071
trigger times: 7
Loss after 77343300 batches: 0.0074
trigger times: 8
Loss after 77474400 batches: 0.0071
trigger times: 9
Loss after 77605500 batches: 0.0071
trigger times: 10
Loss after 77736600 batches: 0.0071
trigger times: 11
Loss after 77867700 batches: 0.0070
trigger times: 12
Loss after 77998800 batches: 0.0072
trigger times: 13
Loss after 78129900 batches: 0.0071
trigger times: 14
Loss after 78261000 batches: 0.0070
trigger times: 15
Loss after 78392100 batches: 0.0071
trigger times: 16
Loss after 78523200 batches: 0.0070
trigger times: 17
Loss after 78654300 batches: 0.0069
trigger times: 18
Loss after 78785400 batches: 0.0072
trigger times: 19
Loss after 78916500 batches: 0.0070
trigger times: 20
Early stopping!
Start to test process.
Loss after 79047600 batches: 0.0068
Time to train on one home:  259.9730033874512
trigger times: 0
Loss after 79150200 batches: 0.0810
trigger times: 0
Loss after 79252800 batches: 0.0271
trigger times: 1
Loss after 79355400 batches: 0.0241
trigger times: 2
Loss after 79458000 batches: 0.0236
trigger times: 3
Loss after 79560600 batches: 0.0249
trigger times: 4
Loss after 79663200 batches: 0.0221
trigger times: 5
Loss after 79765800 batches: 0.0218
trigger times: 6
Loss after 79868400 batches: 0.0235
trigger times: 7
Loss after 79971000 batches: 0.0213
trigger times: 8
Loss after 80073600 batches: 0.0209
trigger times: 9
Loss after 80176200 batches: 0.0211
trigger times: 10
Loss after 80278800 batches: 0.0211
trigger times: 11
Loss after 80381400 batches: 0.0260
trigger times: 12
Loss after 80484000 batches: 0.0230
trigger times: 13
Loss after 80586600 batches: 0.0199
trigger times: 14
Loss after 80689200 batches: 0.0206
trigger times: 15
Loss after 80791800 batches: 0.0217
trigger times: 16
Loss after 80894400 batches: 0.0191
trigger times: 17
Loss after 80997000 batches: 0.0197
trigger times: 18
Loss after 81099600 batches: 0.0207
trigger times: 19
Loss after 81202200 batches: 0.0189
trigger times: 20
Early stopping!
Start to test process.
Loss after 81304800 batches: 0.0183
Time to train on one home:  137.94893503189087
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122, 0.014372845554577064, 0.013282680470221957, 0.012552672623291022]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756], [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386], [0.29729848437839085, 0.6796477634281426, 0.7054645356699167, 0.6421362552432476, 0.26243081830278, 15.170699187034348, 810.14026], [0.2910480449597041, 0.6864099454469708, 0.7043301290692494, 0.6284525681290563, 0.25689127539306345, 14.847417174402532, 793.0393]]
Round_10_results:  [0.2910480449597041, 0.6864099454469708, 0.7043301290692494, 0.6284525681290563, 0.25689127539306345, 14.847417174402532, 793.0393]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 674 < 675; dropping {'Training_Loss': 0.023274353968928446, 'Validation_Loss': 0.19782584657271704, 'Training_R2': 0.9765800713183661, 'Validation_R2': 0.816102289811961, 'Training_F1': 0.9149407355020479, 'Validation_F1': 0.7597843080413884, 'Training_NEP': 0.17025659842770413, 'Validation_NEP': 0.45814697523616554, 'Training_NDE': 0.01758175616568075, 'Validation_NDE': 0.14644644238971374, 'Training_MAE': 5.638560884680846, 'Validation_MAE': 12.564290796343144, 'Training_MSE': 77.35698, 'Validation_MSE': 540.8227}.
trigger times: 0
Loss after 81435900 batches: 0.0233
trigger times: 1
Loss after 81567000 batches: 0.0099
trigger times: 2
Loss after 81698100 batches: 0.0090
trigger times: 3
Loss after 81829200 batches: 0.0081
trigger times: 4
Loss after 81960300 batches: 0.0078
trigger times: 5
Loss after 82091400 batches: 0.0077
trigger times: 6
Loss after 82222500 batches: 0.0075
trigger times: 7
Loss after 82353600 batches: 0.0079
trigger times: 8
Loss after 82484700 batches: 0.0074
trigger times: 9
Loss after 82615800 batches: 0.0074
trigger times: 10
Loss after 82746900 batches: 0.0073
trigger times: 11
Loss after 82878000 batches: 0.0073
trigger times: 12
Loss after 83009100 batches: 0.0073
trigger times: 13
Loss after 83140200 batches: 0.0071
trigger times: 14
Loss after 83271300 batches: 0.0070
trigger times: 15
Loss after 83402400 batches: 0.0071
trigger times: 16
Loss after 83533500 batches: 0.0070
trigger times: 17
Loss after 83664600 batches: 0.0072
trigger times: 18
Loss after 83795700 batches: 0.0072
trigger times: 19
Loss after 83926800 batches: 0.0068
trigger times: 20
Early stopping!
Start to test process.
Loss after 84057900 batches: 0.0071
Time to train on one home:  160.33417987823486
trigger times: 0
Loss after 84160500 batches: 0.0773
trigger times: 0
Loss after 84263100 batches: 0.0281
trigger times: 1
Loss after 84365700 batches: 0.0239
trigger times: 2
Loss after 84468300 batches: 0.0241
trigger times: 3
Loss after 84570900 batches: 0.0244
trigger times: 4
Loss after 84673500 batches: 0.0217
trigger times: 5
Loss after 84776100 batches: 0.0217
trigger times: 6
Loss after 84878700 batches: 0.0208
trigger times: 7
Loss after 84981300 batches: 0.0206
trigger times: 8
Loss after 85083900 batches: 0.0204
trigger times: 9
Loss after 85186500 batches: 0.0200
trigger times: 10
Loss after 85289100 batches: 0.0204
trigger times: 11
Loss after 85391700 batches: 0.0209
trigger times: 12
Loss after 85494300 batches: 0.0191
trigger times: 13
Loss after 85596900 batches: 0.0190
trigger times: 14
Loss after 85699500 batches: 0.0187
trigger times: 15
Loss after 85802100 batches: 0.0191
trigger times: 16
Loss after 85904700 batches: 0.0205
trigger times: 17
Loss after 86007300 batches: 0.0184
trigger times: 18
Loss after 86109900 batches: 0.0178
trigger times: 19
Loss after 86212500 batches: 0.0180
trigger times: 20
Early stopping!
Start to test process.
Loss after 86315100 batches: 0.0193
Time to train on one home:  137.79998922348022
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122, 0.014372845554577064, 0.013282680470221957, 0.012552672623291022, 0.013186224890447759]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756], [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386], [0.29729848437839085, 0.6796477634281426, 0.7054645356699167, 0.6421362552432476, 0.26243081830278, 15.170699187034348, 810.14026], [0.2910480449597041, 0.6864099454469708, 0.7043301290692494, 0.6284525681290563, 0.25689127539306345, 14.847417174402532, 793.0393], [0.29057884050740135, 0.6869003152995016, 0.7028138162738262, 0.6202061906571094, 0.25648956706398235, 14.652593550929886, 791.79926]]
Round_11_results:  [0.29057884050740135, 0.6869003152995016, 0.7028138162738262, 0.6202061906571094, 0.25648956706398235, 14.652593550929886, 791.79926]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 717 < 718; dropping {'Training_Loss': 0.023881501852060266, 'Validation_Loss': 0.2169395329223739, 'Training_R2': 0.9759817910427253, 'Validation_R2': 0.7983062266021084, 'Training_F1': 0.9139365436591202, 'Validation_F1': 0.7506174134436816, 'Training_NEP': 0.1722150477249816, 'Validation_NEP': 0.47574657902155937, 'Training_NDE': 0.01803089578809572, 'Validation_NDE': 0.16061828902641473, 'Training_MAE': 5.703420841382897, 'Validation_MAE': 13.046944948422011, 'Training_MSE': 79.33312, 'Validation_MSE': 593.1589}.
trigger times: 0
Loss after 86446200 batches: 0.0239
trigger times: 0
Loss after 86577300 batches: 0.0098
trigger times: 1
Loss after 86708400 batches: 0.0084
trigger times: 2
Loss after 86839500 batches: 0.0080
trigger times: 0
Loss after 86970600 batches: 0.0078
trigger times: 1
Loss after 87101700 batches: 0.0075
trigger times: 2
Loss after 87232800 batches: 0.0073
trigger times: 3
Loss after 87363900 batches: 0.0075
trigger times: 0
Loss after 87495000 batches: 0.0072
trigger times: 1
Loss after 87626100 batches: 0.0071
trigger times: 2
Loss after 87757200 batches: 0.0072
trigger times: 3
Loss after 87888300 batches: 0.0071
trigger times: 4
Loss after 88019400 batches: 0.0071
trigger times: 5
Loss after 88150500 batches: 0.0070
trigger times: 6
Loss after 88281600 batches: 0.0070
trigger times: 7
Loss after 88412700 batches: 0.0071
trigger times: 0
Loss after 88543800 batches: 0.0070
trigger times: 1
Loss after 88674900 batches: 0.0068
trigger times: 2
Loss after 88806000 batches: 0.0068
trigger times: 3
Loss after 88937100 batches: 0.0068
trigger times: 4
Loss after 89068200 batches: 0.0070
trigger times: 5
Loss after 89199300 batches: 0.0068
trigger times: 6
Loss after 89330400 batches: 0.0068
trigger times: 0
Loss after 89461500 batches: 0.0069
trigger times: 1
Loss after 89592600 batches: 0.0067
trigger times: 2
Loss after 89723700 batches: 0.0067
trigger times: 3
Loss after 89854800 batches: 0.0066
trigger times: 4
Loss after 89985900 batches: 0.0066
trigger times: 5
Loss after 90117000 batches: 0.0067
trigger times: 6
Loss after 90248100 batches: 0.0066
trigger times: 7
Loss after 90379200 batches: 0.0068
trigger times: 8
Loss after 90510300 batches: 0.0067
trigger times: 9
Loss after 90641400 batches: 0.0066
trigger times: 10
Loss after 90772500 batches: 0.0065
trigger times: 11
Loss after 90903600 batches: 0.0065
trigger times: 12
Loss after 91034700 batches: 0.0066
trigger times: 13
Loss after 91165800 batches: 0.0065
trigger times: 14
Loss after 91296900 batches: 0.0064
trigger times: 15
Loss after 91428000 batches: 0.0065
trigger times: 16
Loss after 91559100 batches: 0.0068
trigger times: 17
Loss after 91690200 batches: 0.0066
trigger times: 18
Loss after 91821300 batches: 0.0066
trigger times: 19
Loss after 91952400 batches: 0.0066
trigger times: 20
Early stopping!
Start to test process.
Loss after 92083500 batches: 0.0064
Time to train on one home:  323.9605507850647
trigger times: 0
Loss after 92186100 batches: 0.0705
trigger times: 1
Loss after 92288700 batches: 0.0246
trigger times: 2
Loss after 92391300 batches: 0.0216
trigger times: 3
Loss after 92493900 batches: 0.0223
trigger times: 4
Loss after 92596500 batches: 0.0204
trigger times: 5
Loss after 92699100 batches: 0.0202
trigger times: 6
Loss after 92801700 batches: 0.0207
trigger times: 7
Loss after 92904300 batches: 0.0228
trigger times: 8
Loss after 93006900 batches: 0.0206
trigger times: 9
Loss after 93109500 batches: 0.0192
trigger times: 10
Loss after 93212100 batches: 0.0188
trigger times: 11
Loss after 93314700 batches: 0.0190
trigger times: 12
Loss after 93417300 batches: 0.0198
trigger times: 13
Loss after 93519900 batches: 0.0207
trigger times: 14
Loss after 93622500 batches: 0.0185
trigger times: 15
Loss after 93725100 batches: 0.0184
trigger times: 16
Loss after 93827700 batches: 0.0174
trigger times: 17
Loss after 93930300 batches: 0.0183
trigger times: 18
Loss after 94032900 batches: 0.0199
trigger times: 19
Loss after 94135500 batches: 0.0192
trigger times: 20
Early stopping!
Start to test process.
Loss after 94238100 batches: 0.0184
Time to train on one home:  132.3026580810547
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122, 0.014372845554577064, 0.013282680470221957, 0.012552672623291022, 0.013186224890447759, 0.012415743237972152]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756], [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386], [0.29729848437839085, 0.6796477634281426, 0.7054645356699167, 0.6421362552432476, 0.26243081830278, 15.170699187034348, 810.14026], [0.2910480449597041, 0.6864099454469708, 0.7043301290692494, 0.6284525681290563, 0.25689127539306345, 14.847417174402532, 793.0393], [0.29057884050740135, 0.6869003152995016, 0.7028138162738262, 0.6202061906571094, 0.25648956706398235, 14.652593550929886, 791.79926], [0.2833130541774962, 0.694813689422725, 0.7105434628633682, 0.6132649061332406, 0.25000697381313813, 14.488603216131956, 771.7871]]
Round_12_results:  [0.2833130541774962, 0.694813689422725, 0.7105434628633682, 0.6132649061332406, 0.25000697381313813, 14.488603216131956, 771.7871]
trigger times: 0
Loss after 94369200 batches: 0.0237
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 782 < 783; dropping {'Training_Loss': 0.023739226303010615, 'Validation_Loss': 0.19917474852667916, 'Training_R2': 0.9761344141150684, 'Validation_R2': 0.8148477580775249, 'Training_F1': 0.9139535623849423, 'Validation_F1': 0.7567907675866866, 'Training_NEP': 0.17220606490503645, 'Validation_NEP': 0.46782309876763084, 'Training_NDE': 0.01791631893862402, 'Validation_NDE': 0.14744548533149526, 'Training_MAE': 5.703123348201167, 'Validation_MAE': 12.82965024735338, 'Training_MSE': 78.828995, 'Validation_MSE': 544.512}.
trigger times: 1
Loss after 94500300 batches: 0.0096
trigger times: 2
Loss after 94631400 batches: 0.0084
trigger times: 3
Loss after 94762500 batches: 0.0077
trigger times: 4
Loss after 94893600 batches: 0.0074
trigger times: 5
Loss after 95024700 batches: 0.0073
trigger times: 6
Loss after 95155800 batches: 0.0070
trigger times: 7
Loss after 95286900 batches: 0.0069
trigger times: 8
Loss after 95418000 batches: 0.0070
trigger times: 9
Loss after 95549100 batches: 0.0070
trigger times: 10
Loss after 95680200 batches: 0.0069
trigger times: 11
Loss after 95811300 batches: 0.0068
trigger times: 12
Loss after 95942400 batches: 0.0067
trigger times: 13
Loss after 96073500 batches: 0.0067
trigger times: 14
Loss after 96204600 batches: 0.0067
trigger times: 15
Loss after 96335700 batches: 0.0067
trigger times: 16
Loss after 96466800 batches: 0.0068
trigger times: 17
Loss after 96597900 batches: 0.0068
trigger times: 18
Loss after 96729000 batches: 0.0066
trigger times: 19
Loss after 96860100 batches: 0.0065
trigger times: 20
Early stopping!
Start to test process.
Loss after 96991200 batches: 0.0065
Time to train on one home:  159.97989559173584
trigger times: 0
Loss after 97093800 batches: 0.0757
trigger times: 0
Loss after 97196400 batches: 0.0268
trigger times: 1
Loss after 97299000 batches: 0.0266
trigger times: 0
Loss after 97401600 batches: 0.0255
trigger times: 1
Loss after 97504200 batches: 0.0210
trigger times: 2
Loss after 97606800 batches: 0.0196
trigger times: 3
Loss after 97709400 batches: 0.0214
trigger times: 4
Loss after 97812000 batches: 0.0272
trigger times: 5
Loss after 97914600 batches: 0.0199
trigger times: 6
Loss after 98017200 batches: 0.0190
trigger times: 7
Loss after 98119800 batches: 0.0187
trigger times: 8
Loss after 98222400 batches: 0.0189
trigger times: 9
Loss after 98325000 batches: 0.0192
trigger times: 10
Loss after 98427600 batches: 0.0203
trigger times: 11
Loss after 98530200 batches: 0.0182
trigger times: 12
Loss after 98632800 batches: 0.0180
trigger times: 13
Loss after 98735400 batches: 0.0178
trigger times: 14
Loss after 98838000 batches: 0.0181
trigger times: 15
Loss after 98940600 batches: 0.0169
trigger times: 16
Loss after 99043200 batches: 0.0180
trigger times: 17
Loss after 99145800 batches: 0.0168
trigger times: 18
Loss after 99248400 batches: 0.0170
trigger times: 19
Loss after 99351000 batches: 0.0173
trigger times: 20
Early stopping!
Start to test process.
Loss after 99453600 batches: 0.0168
Time to train on one home:  149.9608118534088
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122, 0.014372845554577064, 0.013282680470221957, 0.012552672623291022, 0.013186224890447759, 0.012415743237972152, 0.011652671889948382]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756], [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386], [0.29729848437839085, 0.6796477634281426, 0.7054645356699167, 0.6421362552432476, 0.26243081830278, 15.170699187034348, 810.14026], [0.2910480449597041, 0.6864099454469708, 0.7043301290692494, 0.6284525681290563, 0.25689127539306345, 14.847417174402532, 793.0393], [0.29057884050740135, 0.6869003152995016, 0.7028138162738262, 0.6202061906571094, 0.25648956706398235, 14.652593550929886, 791.79926], [0.2833130541774962, 0.694813689422725, 0.7105434628633682, 0.6132649061332406, 0.25000697381313813, 14.488603216131956, 771.7871], [0.2839242004685932, 0.6941031795540529, 0.7070122510946861, 0.6132107287518146, 0.2505890196519411, 14.487323256077305, 773.58386]]
Round_13_results:  [0.2839242004685932, 0.6941031795540529, 0.7070122510946861, 0.6132107287518146, 0.2505890196519411, 14.487323256077305, 773.58386]
trigger times: 0
Loss after 99584700 batches: 0.0226
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 827 < 828; dropping {'Training_Loss': 0.022599655625252228, 'Validation_Loss': 0.21162221332391104, 'Training_R2': 0.9772723287011136, 'Validation_R2': 0.8033578227025266, 'Training_F1': 0.9163378781772299, 'Validation_F1': 0.760367920416057, 'Training_NEP': 0.1674848693146341, 'Validation_NEP': 0.46250336112912066, 'Training_NDE': 0.0170620662608648, 'Validation_NDE': 0.15659546418243192, 'Training_MAE': 5.546766713387543, 'Validation_MAE': 12.683760971065917, 'Training_MSE': 75.070404, 'Validation_MSE': 578.3028}.
trigger times: 1
Loss after 99715800 batches: 0.0090
trigger times: 2
Loss after 99846900 batches: 0.0080
trigger times: 3
Loss after 99978000 batches: 0.0077
trigger times: 4
Loss after 100109100 batches: 0.0073
trigger times: 5
Loss after 100240200 batches: 0.0071
trigger times: 6
Loss after 100371300 batches: 0.0070
trigger times: 7
Loss after 100502400 batches: 0.0069
trigger times: 8
Loss after 100633500 batches: 0.0070
trigger times: 9
Loss after 100764600 batches: 0.0068
trigger times: 10
Loss after 100895700 batches: 0.0068
trigger times: 11
Loss after 101026800 batches: 0.0067
trigger times: 12
Loss after 101157900 batches: 0.0067
trigger times: 13
Loss after 101289000 batches: 0.0066
trigger times: 14
Loss after 101420100 batches: 0.0067
trigger times: 15
Loss after 101551200 batches: 0.0069
trigger times: 16
Loss after 101682300 batches: 0.0067
trigger times: 17
Loss after 101813400 batches: 0.0065
trigger times: 18
Loss after 101944500 batches: 0.0064
trigger times: 19
Loss after 102075600 batches: 0.0063
trigger times: 20
Early stopping!
Start to test process.
Loss after 102206700 batches: 0.0065
Time to train on one home:  160.26126337051392
trigger times: 0
Loss after 102309300 batches: 0.0670
trigger times: 0
Loss after 102411900 batches: 0.0250
trigger times: 0
Loss after 102514500 batches: 0.0221
trigger times: 0
Loss after 102617100 batches: 0.0230
trigger times: 1
Loss after 102719700 batches: 0.0192
trigger times: 2
Loss after 102822300 batches: 0.0191
trigger times: 3
Loss after 102924900 batches: 0.0180
trigger times: 4
Loss after 103027500 batches: 0.0177
trigger times: 5
Loss after 103130100 batches: 0.0172
trigger times: 6
Loss after 103232700 batches: 0.0172
trigger times: 7
Loss after 103335300 batches: 0.0167
trigger times: 8
Loss after 103437900 batches: 0.0164
trigger times: 9
Loss after 103540500 batches: 0.0176
trigger times: 10
Loss after 103643100 batches: 0.0202
trigger times: 11
Loss after 103745700 batches: 0.0198
trigger times: 12
Loss after 103848300 batches: 0.0180
trigger times: 13
Loss after 103950900 batches: 0.0179
trigger times: 14
Loss after 104053500 batches: 0.0166
trigger times: 15
Loss after 104156100 batches: 0.0160
trigger times: 16
Loss after 104258700 batches: 0.0159
trigger times: 17
Loss after 104361300 batches: 0.0169
trigger times: 18
Loss after 104463900 batches: 0.0174
trigger times: 19
Loss after 104566500 batches: 0.0167
trigger times: 20
Early stopping!
Start to test process.
Loss after 104669100 batches: 0.0170
Time to train on one home:  150.01069164276123
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122, 0.014372845554577064, 0.013282680470221957, 0.012552672623291022, 0.013186224890447759, 0.012415743237972152, 0.011652671889948382, 0.011758733952044772]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756], [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386], [0.29729848437839085, 0.6796477634281426, 0.7054645356699167, 0.6421362552432476, 0.26243081830278, 15.170699187034348, 810.14026], [0.2910480449597041, 0.6864099454469708, 0.7043301290692494, 0.6284525681290563, 0.25689127539306345, 14.847417174402532, 793.0393], [0.29057884050740135, 0.6869003152995016, 0.7028138162738262, 0.6202061906571094, 0.25648956706398235, 14.652593550929886, 791.79926], [0.2833130541774962, 0.694813689422725, 0.7105434628633682, 0.6132649061332406, 0.25000697381313813, 14.488603216131956, 771.7871], [0.2839242004685932, 0.6941031795540529, 0.7070122510946861, 0.6132107287518146, 0.2505890196519411, 14.487323256077305, 773.58386], [0.27948591113090515, 0.698940550898816, 0.712652734724527, 0.6098184794309123, 0.24662627122843855, 14.407180150008116, 761.35065]]
Round_14_results:  [0.27948591113090515, 0.698940550898816, 0.712652734724527, 0.6098184794309123, 0.24662627122843855, 14.407180150008116, 761.35065]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 872 < 873; dropping {'Training_Loss': 0.022012164060659003, 'Validation_Loss': 0.20227593763007057, 'Training_R2': 0.9778517218603647, 'Validation_R2': 0.8119545377250936, 'Training_F1': 0.9176049554169668, 'Validation_F1': 0.7499069304448305, 'Training_NEP': 0.16494803258741028, 'Validation_NEP': 0.4740878151989948, 'Training_NDE': 0.016627105531970405, 'Validation_NDE': 0.14974949350663683, 'Training_MAE': 5.462751712071632, 'Validation_MAE': 13.001454762617747, 'Training_MSE': 73.156654, 'Validation_MSE': 553.02075}.
trigger times: 0
Loss after 104800200 batches: 0.0220
trigger times: 1
Loss after 104931300 batches: 0.0088
trigger times: 0
Loss after 105062400 batches: 0.0077
trigger times: 1
Loss after 105193500 batches: 0.0075
trigger times: 2
Loss after 105324600 batches: 0.0073
trigger times: 3
Loss after 105455700 batches: 0.0071
trigger times: 4
Loss after 105586800 batches: 0.0070
trigger times: 5
Loss after 105717900 batches: 0.0070
trigger times: 6
Loss after 105849000 batches: 0.0069
trigger times: 7
Loss after 105980100 batches: 0.0066
trigger times: 8
Loss after 106111200 batches: 0.0067
trigger times: 9
Loss after 106242300 batches: 0.0067
trigger times: 10
Loss after 106373400 batches: 0.0066
trigger times: 11
Loss after 106504500 batches: 0.0065
trigger times: 12
Loss after 106635600 batches: 0.0066
trigger times: 13
Loss after 106766700 batches: 0.0065
trigger times: 14
Loss after 106897800 batches: 0.0064
trigger times: 15
Loss after 107028900 batches: 0.0064
trigger times: 16
Loss after 107160000 batches: 0.0065
trigger times: 17
Loss after 107291100 batches: 0.0065
trigger times: 18
Loss after 107422200 batches: 0.0063
trigger times: 19
Loss after 107553300 batches: 0.0065
trigger times: 20
Early stopping!
Start to test process.
Loss after 107684400 batches: 0.0064
Time to train on one home:  174.92767214775085
trigger times: 0
Loss after 107787000 batches: 0.0561
trigger times: 0
Loss after 107889600 batches: 0.0212
trigger times: 0
Loss after 107992200 batches: 0.0186
trigger times: 1
Loss after 108094800 batches: 0.0177
trigger times: 2
Loss after 108197400 batches: 0.0180
trigger times: 3
Loss after 108300000 batches: 0.0175
trigger times: 4
Loss after 108402600 batches: 0.0179
trigger times: 5
Loss after 108505200 batches: 0.0188
trigger times: 6
Loss after 108607800 batches: 0.0174
trigger times: 7
Loss after 108710400 batches: 0.0171
trigger times: 8
Loss after 108813000 batches: 0.0171
trigger times: 9
Loss after 108915600 batches: 0.0164
trigger times: 10
Loss after 109018200 batches: 0.0167
trigger times: 11
Loss after 109120800 batches: 0.0169
trigger times: 12
Loss after 109223400 batches: 0.0160
trigger times: 13
Loss after 109326000 batches: 0.0168
trigger times: 14
Loss after 109428600 batches: 0.0167
trigger times: 15
Loss after 109531200 batches: 0.0163
trigger times: 16
Loss after 109633800 batches: 0.0158
trigger times: 17
Loss after 109736400 batches: 0.0162
trigger times: 18
Loss after 109839000 batches: 0.0165
trigger times: 19
Loss after 109941600 batches: 0.0160
trigger times: 20
Early stopping!
Start to test process.
Loss after 110044200 batches: 0.0157
Time to train on one home:  142.99325037002563
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122, 0.014372845554577064, 0.013282680470221957, 0.012552672623291022, 0.013186224890447759, 0.012415743237972152, 0.011652671889948382, 0.011758733952044772, 0.011035635479142672]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756], [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386], [0.29729848437839085, 0.6796477634281426, 0.7054645356699167, 0.6421362552432476, 0.26243081830278, 15.170699187034348, 810.14026], [0.2910480449597041, 0.6864099454469708, 0.7043301290692494, 0.6284525681290563, 0.25689127539306345, 14.847417174402532, 793.0393], [0.29057884050740135, 0.6869003152995016, 0.7028138162738262, 0.6202061906571094, 0.25648956706398235, 14.652593550929886, 791.79926], [0.2833130541774962, 0.694813689422725, 0.7105434628633682, 0.6132649061332406, 0.25000697381313813, 14.488603216131956, 771.7871], [0.2839242004685932, 0.6941031795540529, 0.7070122510946861, 0.6132107287518146, 0.2505890196519411, 14.487323256077305, 773.58386], [0.27948591113090515, 0.698940550898816, 0.712652734724527, 0.6098184794309123, 0.24662627122843855, 14.407180150008116, 761.35065], [0.2833963806430499, 0.6946747512717111, 0.7117215621396701, 0.6055824506888411, 0.2501207911944501, 14.307102452683193, 772.1384]]
Round_15_results:  [0.2833963806430499, 0.6946747512717111, 0.7117215621396701, 0.6055824506888411, 0.2501207911944501, 14.307102452683193, 772.1384]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 918 < 919; dropping {'Training_Loss': 0.02130033385078862, 'Validation_Loss': 0.2143225587076611, 'Training_R2': 0.9785652443440289, 'Validation_R2': 0.8007976557087002, 'Training_F1': 0.91802004410123, 'Validation_F1': 0.7540227193064425, 'Training_NEP': 0.16413512508655645, 'Validation_NEP': 0.4688185747874651, 'Training_NDE': 0.016091451538439976, 'Validation_NDE': 0.15863424621939184, 'Training_MAE': 5.435829827812771, 'Validation_MAE': 12.856950329794252, 'Training_MSE': 70.799866, 'Validation_MSE': 585.8319}.
trigger times: 0
Loss after 110175300 batches: 0.0213
trigger times: 0
Loss after 110306400 batches: 0.0086
trigger times: 1
Loss after 110437500 batches: 0.0075
trigger times: 2
Loss after 110568600 batches: 0.0072
trigger times: 3
Loss after 110699700 batches: 0.0069
trigger times: 4
Loss after 110830800 batches: 0.0067
trigger times: 5
Loss after 110961900 batches: 0.0068
trigger times: 6
Loss after 111093000 batches: 0.0068
trigger times: 7
Loss after 111224100 batches: 0.0068
trigger times: 0
Loss after 111355200 batches: 0.0066
trigger times: 1
Loss after 111486300 batches: 0.0065
trigger times: 2
Loss after 111617400 batches: 0.0065
trigger times: 3
Loss after 111748500 batches: 0.0064
trigger times: 4
Loss after 111879600 batches: 0.0064
trigger times: 5
Loss after 112010700 batches: 0.0063
trigger times: 6
Loss after 112141800 batches: 0.0064
trigger times: 7
Loss after 112272900 batches: 0.0063
trigger times: 8
Loss after 112404000 batches: 0.0064
trigger times: 9
Loss after 112535100 batches: 0.0064
trigger times: 10
Loss after 112666200 batches: 0.0064
trigger times: 11
Loss after 112797300 batches: 0.0063
trigger times: 12
Loss after 112928400 batches: 0.0064
trigger times: 13
Loss after 113059500 batches: 0.0064
trigger times: 14
Loss after 113190600 batches: 0.0063
trigger times: 15
Loss after 113321700 batches: 0.0064
trigger times: 16
Loss after 113452800 batches: 0.0062
trigger times: 17
Loss after 113583900 batches: 0.0061
trigger times: 18
Loss after 113715000 batches: 0.0062
trigger times: 19
Loss after 113846100 batches: 0.0063
trigger times: 20
Early stopping!
Start to test process.
Loss after 113977200 batches: 0.0065
Time to train on one home:  222.4040687084198
trigger times: 0
Loss after 114079800 batches: 0.0582
trigger times: 0
Loss after 114182400 batches: 0.0213
trigger times: 1
Loss after 114285000 batches: 0.0200
trigger times: 2
Loss after 114387600 batches: 0.0186
trigger times: 3
Loss after 114490200 batches: 0.0173
trigger times: 4
Loss after 114592800 batches: 0.0174
trigger times: 5
Loss after 114695400 batches: 0.0168
trigger times: 6
Loss after 114798000 batches: 0.0170
trigger times: 7
Loss after 114900600 batches: 0.0175
trigger times: 8
Loss after 115003200 batches: 0.0169
trigger times: 9
Loss after 115105800 batches: 0.0164
trigger times: 10
Loss after 115208400 batches: 0.0164
trigger times: 11
Loss after 115311000 batches: 0.0162
trigger times: 12
Loss after 115413600 batches: 0.0162
trigger times: 13
Loss after 115516200 batches: 0.0177
trigger times: 14
Loss after 115618800 batches: 0.0169
trigger times: 15
Loss after 115721400 batches: 0.0162
trigger times: 16
Loss after 115824000 batches: 0.0162
trigger times: 17
Loss after 115926600 batches: 0.0157
trigger times: 18
Loss after 116029200 batches: 0.0150
trigger times: 19
Loss after 116131800 batches: 0.0163
trigger times: 20
Early stopping!
Start to test process.
Loss after 116234400 batches: 0.0157
Time to train on one home:  137.68499946594238
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122, 0.014372845554577064, 0.013282680470221957, 0.012552672623291022, 0.013186224890447759, 0.012415743237972152, 0.011652671889948382, 0.011758733952044772, 0.011035635479142672, 0.011062181679622304]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756], [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386], [0.29729848437839085, 0.6796477634281426, 0.7054645356699167, 0.6421362552432476, 0.26243081830278, 15.170699187034348, 810.14026], [0.2910480449597041, 0.6864099454469708, 0.7043301290692494, 0.6284525681290563, 0.25689127539306345, 14.847417174402532, 793.0393], [0.29057884050740135, 0.6869003152995016, 0.7028138162738262, 0.6202061906571094, 0.25648956706398235, 14.652593550929886, 791.79926], [0.2833130541774962, 0.694813689422725, 0.7105434628633682, 0.6132649061332406, 0.25000697381313813, 14.488603216131956, 771.7871], [0.2839242004685932, 0.6941031795540529, 0.7070122510946861, 0.6132107287518146, 0.2505890196519411, 14.487323256077305, 773.58386], [0.27948591113090515, 0.698940550898816, 0.712652734724527, 0.6098184794309123, 0.24662627122843855, 14.407180150008116, 761.35065], [0.2833963806430499, 0.6946747512717111, 0.7117215621396701, 0.6055824506888411, 0.2501207911944501, 14.307102452683193, 772.1384], [0.2766948449942801, 0.7018993806846155, 0.7137988191772131, 0.5946304241288545, 0.24420241388248945, 14.048356899736604, 753.8681]]
Round_16_results:  [0.2766948449942801, 0.7018993806846155, 0.7137988191772131, 0.5946304241288545, 0.24420241388248945, 14.048356899736604, 753.8681]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 970 < 971; dropping {'Training_Loss': 0.02094283362604537, 'Validation_Loss': 0.20411839336156845, 'Training_R2': 0.9789269643579774, 'Validation_R2': 0.8102537189295183, 'Training_F1': 0.919497564403958, 'Validation_F1': 0.75873019904349, 'Training_NEP': 0.16110004433171396, 'Validation_NEP': 0.4610072149645394, 'Training_NDE': 0.01581990190342861, 'Validation_NDE': 0.15110393593828486, 'Training_MAE': 5.335313972426588, 'Validation_MAE': 12.642730436102891, 'Training_MSE': 69.60508, 'Validation_MSE': 558.02264}.
trigger times: 0
Loss after 116365500 batches: 0.0209
trigger times: 1
Loss after 116496600 batches: 0.0086
trigger times: 2
Loss after 116627700 batches: 0.0076
trigger times: 3
Loss after 116758800 batches: 0.0072
trigger times: 4
Loss after 116889900 batches: 0.0067
trigger times: 5
Loss after 117021000 batches: 0.0067
trigger times: 6
Loss after 117152100 batches: 0.0066
trigger times: 7
Loss after 117283200 batches: 0.0065
trigger times: 8
Loss after 117414300 batches: 0.0064
trigger times: 9
Loss after 117545400 batches: 0.0065
trigger times: 10
Loss after 117676500 batches: 0.0065
trigger times: 11
Loss after 117807600 batches: 0.0066
trigger times: 12
Loss after 117938700 batches: 0.0066
trigger times: 13
Loss after 118069800 batches: 0.0063
trigger times: 14
Loss after 118200900 batches: 0.0063
trigger times: 15
Loss after 118332000 batches: 0.0062
trigger times: 16
Loss after 118463100 batches: 0.0063
trigger times: 17
Loss after 118594200 batches: 0.0063
trigger times: 18
Loss after 118725300 batches: 0.0063
trigger times: 19
Loss after 118856400 batches: 0.0064
trigger times: 20
Early stopping!
Start to test process.
Loss after 118987500 batches: 0.0063
Time to train on one home:  159.79222750663757
trigger times: 0
Loss after 119090100 batches: 0.0599
trigger times: 1
Loss after 119192700 batches: 0.0218
trigger times: 0
Loss after 119295300 batches: 0.0188
trigger times: 1
Loss after 119397900 batches: 0.0170
trigger times: 2
Loss after 119500500 batches: 0.0170
trigger times: 3
Loss after 119603100 batches: 0.0169
trigger times: 4
Loss after 119705700 batches: 0.0161
trigger times: 5
Loss after 119808300 batches: 0.0168
trigger times: 6
Loss after 119910900 batches: 0.0171
trigger times: 7
Loss after 120013500 batches: 0.0163
trigger times: 8
Loss after 120116100 batches: 0.0170
trigger times: 9
Loss after 120218700 batches: 0.0162
trigger times: 10
Loss after 120321300 batches: 0.0156
trigger times: 11
Loss after 120423900 batches: 0.0159
trigger times: 12
Loss after 120526500 batches: 0.0171
trigger times: 13
Loss after 120629100 batches: 0.0154
trigger times: 14
Loss after 120731700 batches: 0.0155
trigger times: 15
Loss after 120834300 batches: 0.0171
trigger times: 16
Loss after 120936900 batches: 0.0155
trigger times: 17
Loss after 121039500 batches: 0.0162
trigger times: 18
Loss after 121142100 batches: 0.0161
trigger times: 19
Loss after 121244700 batches: 0.0151
trigger times: 20
Early stopping!
Start to test process.
Loss after 121347300 batches: 0.0152
Time to train on one home:  143.64688420295715
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122, 0.014372845554577064, 0.013282680470221957, 0.012552672623291022, 0.013186224890447759, 0.012415743237972152, 0.011652671889948382, 0.011758733952044772, 0.011035635479142672, 0.011062181679622304, 0.010736244664965106]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756], [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386], [0.29729848437839085, 0.6796477634281426, 0.7054645356699167, 0.6421362552432476, 0.26243081830278, 15.170699187034348, 810.14026], [0.2910480449597041, 0.6864099454469708, 0.7043301290692494, 0.6284525681290563, 0.25689127539306345, 14.847417174402532, 793.0393], [0.29057884050740135, 0.6869003152995016, 0.7028138162738262, 0.6202061906571094, 0.25648956706398235, 14.652593550929886, 791.79926], [0.2833130541774962, 0.694813689422725, 0.7105434628633682, 0.6132649061332406, 0.25000697381313813, 14.488603216131956, 771.7871], [0.2839242004685932, 0.6941031795540529, 0.7070122510946861, 0.6132107287518146, 0.2505890196519411, 14.487323256077305, 773.58386], [0.27948591113090515, 0.698940550898816, 0.712652734724527, 0.6098184794309123, 0.24662627122843855, 14.407180150008116, 761.35065], [0.2833963806430499, 0.6946747512717111, 0.7117215621396701, 0.6055824506888411, 0.2501207911944501, 14.307102452683193, 772.1384], [0.2766948449942801, 0.7018993806846155, 0.7137988191772131, 0.5946304241288545, 0.24420241388248945, 14.048356899736604, 753.8681], [0.2858842495414946, 0.6919759352179791, 0.7103743530475856, 0.6055350639167756, 0.2523316466984063, 14.3059829232086, 778.96344]]
Round_17_results:  [0.2858842495414946, 0.6919759352179791, 0.7103743530475856, 0.6055350639167756, 0.2523316466984063, 14.3059829232086, 778.96344]
trigger times: 0
Loss after 121478400 batches: 0.0217
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1014 < 1015; dropping {'Training_Loss': 0.021734427651917597, 'Validation_Loss': 0.19982492675383887, 'Training_R2': 0.9781262090851937, 'Validation_R2': 0.8141883502598588, 'Training_F1': 0.9176155330048552, 'Validation_F1': 0.768393402294885, 'Training_NEP': 0.16483842852381148, 'Validation_NEP': 0.44923170376918503, 'Training_NDE': 0.016421043100135528, 'Validation_NDE': 0.1479706029573886, 'Training_MAE': 5.459121842853538, 'Validation_MAE': 12.319797065522936, 'Training_MSE': 72.25002, 'Validation_MSE': 546.4514}.
trigger times: 1
Loss after 121609500 batches: 0.0081
trigger times: 2
Loss after 121740600 batches: 0.0073
trigger times: 3
Loss after 121871700 batches: 0.0069
trigger times: 4
Loss after 122002800 batches: 0.0066
trigger times: 5
Loss after 122133900 batches: 0.0066
trigger times: 6
Loss after 122265000 batches: 0.0065
trigger times: 7
Loss after 122396100 batches: 0.0064
trigger times: 8
Loss after 122527200 batches: 0.0064
trigger times: 9
Loss after 122658300 batches: 0.0064
trigger times: 10
Loss after 122789400 batches: 0.0064
trigger times: 11
Loss after 122920500 batches: 0.0063
trigger times: 12
Loss after 123051600 batches: 0.0062
trigger times: 13
Loss after 123182700 batches: 0.0062
trigger times: 14
Loss after 123313800 batches: 0.0061
trigger times: 15
Loss after 123444900 batches: 0.0061
trigger times: 16
Loss after 123576000 batches: 0.0060
trigger times: 17
Loss after 123707100 batches: 0.0062
trigger times: 18
Loss after 123838200 batches: 0.0062
trigger times: 19
Loss after 123969300 batches: 0.0061
trigger times: 20
Early stopping!
Start to test process.
Loss after 124100400 batches: 0.0061
Time to train on one home:  160.15190148353577
trigger times: 0
Loss after 124203000 batches: 0.0558
trigger times: 0
Loss after 124305600 batches: 0.0201
trigger times: 1
Loss after 124408200 batches: 0.0180
trigger times: 0
Loss after 124510800 batches: 0.0171
trigger times: 1
Loss after 124613400 batches: 0.0159
trigger times: 2
Loss after 124716000 batches: 0.0160
trigger times: 3
Loss after 124818600 batches: 0.0164
trigger times: 4
Loss after 124921200 batches: 0.0161
trigger times: 5
Loss after 125023800 batches: 0.0159
trigger times: 6
Loss after 125126400 batches: 0.0160
trigger times: 7
Loss after 125229000 batches: 0.0154
trigger times: 8
Loss after 125331600 batches: 0.0165
trigger times: 9
Loss after 125434200 batches: 0.0162
trigger times: 10
Loss after 125536800 batches: 0.0154
trigger times: 11
Loss after 125639400 batches: 0.0151
trigger times: 12
Loss after 125742000 batches: 0.0146
trigger times: 13
Loss after 125844600 batches: 0.0148
trigger times: 14
Loss after 125947200 batches: 0.0146
trigger times: 15
Loss after 126049800 batches: 0.0142
trigger times: 16
Loss after 126152400 batches: 0.0144
trigger times: 17
Loss after 126255000 batches: 0.0150
trigger times: 18
Loss after 126357600 batches: 0.0148
trigger times: 19
Loss after 126460200 batches: 0.0148
trigger times: 20
Early stopping!
Start to test process.
Loss after 126562800 batches: 0.0151
Time to train on one home:  150.21816158294678
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122, 0.014372845554577064, 0.013282680470221957, 0.012552672623291022, 0.013186224890447759, 0.012415743237972152, 0.011652671889948382, 0.011758733952044772, 0.011035635479142672, 0.011062181679622304, 0.010736244664965106, 0.010615158364844491]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756], [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386], [0.29729848437839085, 0.6796477634281426, 0.7054645356699167, 0.6421362552432476, 0.26243081830278, 15.170699187034348, 810.14026], [0.2910480449597041, 0.6864099454469708, 0.7043301290692494, 0.6284525681290563, 0.25689127539306345, 14.847417174402532, 793.0393], [0.29057884050740135, 0.6869003152995016, 0.7028138162738262, 0.6202061906571094, 0.25648956706398235, 14.652593550929886, 791.79926], [0.2833130541774962, 0.694813689422725, 0.7105434628633682, 0.6132649061332406, 0.25000697381313813, 14.488603216131956, 771.7871], [0.2839242004685932, 0.6941031795540529, 0.7070122510946861, 0.6132107287518146, 0.2505890196519411, 14.487323256077305, 773.58386], [0.27948591113090515, 0.698940550898816, 0.712652734724527, 0.6098184794309123, 0.24662627122843855, 14.407180150008116, 761.35065], [0.2833963806430499, 0.6946747512717111, 0.7117215621396701, 0.6055824506888411, 0.2501207911944501, 14.307102452683193, 772.1384], [0.2766948449942801, 0.7018993806846155, 0.7137988191772131, 0.5946304241288545, 0.24420241388248945, 14.048356899736604, 753.8681], [0.2858842495414946, 0.6919759352179791, 0.7103743530475856, 0.6055350639167756, 0.2523316466984063, 14.3059829232086, 778.96344], [0.28754592935244244, 0.6901916253625527, 0.7123256119317919, 0.6088832085974488, 0.25379334367444784, 14.385084041344221, 783.4759]]
Round_18_results:  [0.28754592935244244, 0.6901916253625527, 0.7123256119317919, 0.6088832085974488, 0.25379334367444784, 14.385084041344221, 783.4759]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1059 < 1060; dropping {'Training_Loss': 0.021549641927879937, 'Validation_Loss': 0.20802026987075806, 'Training_R2': 0.978297216389312, 'Validation_R2': 0.8066438501032247, 'Training_F1': 0.9178988911019058, 'Validation_F1': 0.7684104441039852, 'Training_NEP': 0.16428409470450653, 'Validation_NEP': 0.45302842409776445, 'Training_NDE': 0.016292664881549557, 'Validation_NDE': 0.15397864518052432, 'Training_MAE': 5.440763406120667, 'Validation_MAE': 12.423918888560333, 'Training_MSE': 71.68517, 'Validation_MSE': 568.6389}.
trigger times: 0
Loss after 126693900 batches: 0.0215
trigger times: 1
Loss after 126825000 batches: 0.0080
trigger times: 2
Loss after 126956100 batches: 0.0072
trigger times: 3
Loss after 127087200 batches: 0.0068
trigger times: 4
Loss after 127218300 batches: 0.0066
trigger times: 5
Loss after 127349400 batches: 0.0066
trigger times: 6
Loss after 127480500 batches: 0.0065
trigger times: 7
Loss after 127611600 batches: 0.0065
trigger times: 8
Loss after 127742700 batches: 0.0063
trigger times: 9
Loss after 127873800 batches: 0.0065
trigger times: 10
Loss after 128004900 batches: 0.0064
trigger times: 11
Loss after 128136000 batches: 0.0062
trigger times: 12
Loss after 128267100 batches: 0.0061
trigger times: 13
Loss after 128398200 batches: 0.0064
trigger times: 14
Loss after 128529300 batches: 0.0062
trigger times: 15
Loss after 128660400 batches: 0.0061
trigger times: 16
Loss after 128791500 batches: 0.0061
trigger times: 17
Loss after 128922600 batches: 0.0061
trigger times: 18
Loss after 129053700 batches: 0.0061
trigger times: 19
Loss after 129184800 batches: 0.0060
trigger times: 20
Early stopping!
Start to test process.
Loss after 129315900 batches: 0.0060
Time to train on one home:  161.3097996711731
trigger times: 0
Loss after 129418500 batches: 0.0502
trigger times: 0
Loss after 129521100 batches: 0.0204
trigger times: 0
Loss after 129623700 batches: 0.0171
trigger times: 1
Loss after 129726300 batches: 0.0167
trigger times: 2
Loss after 129828900 batches: 0.0158
trigger times: 3
Loss after 129931500 batches: 0.0163
trigger times: 4
Loss after 130034100 batches: 0.0173
trigger times: 5
Loss after 130136700 batches: 0.0173
trigger times: 6
Loss after 130239300 batches: 0.0158
trigger times: 7
Loss after 130341900 batches: 0.0156
trigger times: 8
Loss after 130444500 batches: 0.0155
trigger times: 9
Loss after 130547100 batches: 0.0165
trigger times: 10
Loss after 130649700 batches: 0.0153
trigger times: 11
Loss after 130752300 batches: 0.0145
trigger times: 12
Loss after 130854900 batches: 0.0153
trigger times: 13
Loss after 130957500 batches: 0.0194
trigger times: 14
Loss after 131060100 batches: 0.0168
trigger times: 15
Loss after 131162700 batches: 0.0152
trigger times: 16
Loss after 131265300 batches: 0.0145
trigger times: 17
Loss after 131367900 batches: 0.0143
trigger times: 18
Loss after 131470500 batches: 0.0150
trigger times: 19
Loss after 131573100 batches: 0.0147
trigger times: 20
Early stopping!
Start to test process.
Loss after 131675700 batches: 0.0143
Time to train on one home:  144.21072936058044
train_results:  [0.0689694630316821, 0.037200865878824804, 0.026351787942325275, 0.020699803051785613, 0.018301236893402652, 0.015998922322097722, 0.01549665208488388, 0.015456665517312122, 0.014372845554577064, 0.013282680470221957, 0.012552672623291022, 0.013186224890447759, 0.012415743237972152, 0.011652671889948382, 0.011758733952044772, 0.011035635479142672, 0.011062181679622304, 0.010736244664965106, 0.010615158364844491, 0.010179157931983525]
test_results:  [[0.6549717320336236, 0.2917592682456541, 0.36909718366488103, 1.3309615037838955, 0.5801869741214118, 31.444442575165876, 1791.0732], [0.34452857904964024, 0.6284788099668424, 0.6477357782881402, 0.7036084867901357, 0.3043481480278493, 16.62300269044001, 939.5417], [0.2994718882772658, 0.6773124169833715, 0.6752816723535908, 0.6643611251064075, 0.2643439214703436, 15.695769703473669, 816.0461], [0.2841635396083196, 0.6939147481391532, 0.6821420674917427, 0.6521426927449435, 0.25074338164714727, 15.407104859557203, 774.06036], [0.29484690560234916, 0.6823419556149029, 0.6899777704701947, 0.6523622511112886, 0.26022375064562053, 15.412292004657049, 803.3269], [0.29286809265613556, 0.6844438925879798, 0.6941099809357797, 0.6495371601268584, 0.2585018552539466, 15.345548217570055, 798.0113], [0.2968037658267551, 0.6802233595143503, 0.6975679326725281, 0.6431477612716838, 0.2619592931043552, 15.194596379503981, 808.6846], [0.3046152972512775, 0.6717647780765856, 0.6993798799780817, 0.6514798570550169, 0.26888851723635326, 15.391445128806604, 830.0756], [0.28695843120416004, 0.6908408960186834, 0.7017873758154218, 0.6191419720594417, 0.25326146466710364, 14.627451005118722, 781.83386], [0.29729848437839085, 0.6796477634281426, 0.7054645356699167, 0.6421362552432476, 0.26243081830278, 15.170699187034348, 810.14026], [0.2910480449597041, 0.6864099454469708, 0.7043301290692494, 0.6284525681290563, 0.25689127539306345, 14.847417174402532, 793.0393], [0.29057884050740135, 0.6869003152995016, 0.7028138162738262, 0.6202061906571094, 0.25648956706398235, 14.652593550929886, 791.79926], [0.2833130541774962, 0.694813689422725, 0.7105434628633682, 0.6132649061332406, 0.25000697381313813, 14.488603216131956, 771.7871], [0.2839242004685932, 0.6941031795540529, 0.7070122510946861, 0.6132107287518146, 0.2505890196519411, 14.487323256077305, 773.58386], [0.27948591113090515, 0.698940550898816, 0.712652734724527, 0.6098184794309123, 0.24662627122843855, 14.407180150008116, 761.35065], [0.2833963806430499, 0.6946747512717111, 0.7117215621396701, 0.6055824506888411, 0.2501207911944501, 14.307102452683193, 772.1384], [0.2766948449942801, 0.7018993806846155, 0.7137988191772131, 0.5946304241288545, 0.24420241388248945, 14.048356899736604, 753.8681], [0.2858842495414946, 0.6919759352179791, 0.7103743530475856, 0.6055350639167756, 0.2523316466984063, 14.3059829232086, 778.96344], [0.28754592935244244, 0.6901916253625527, 0.7123256119317919, 0.6088832085974488, 0.25379334367444784, 14.385084041344221, 783.4759], [0.28428057084480923, 0.6937331442260399, 0.7148098292531689, 0.6013202161894334, 0.2508921505898441, 14.20640563494843, 774.5197]]
Round_19_results:  [0.28428057084480923, 0.6937331442260399, 0.7148098292531689, 0.6013202161894334, 0.2508921505898441, 14.20640563494843, 774.5197]