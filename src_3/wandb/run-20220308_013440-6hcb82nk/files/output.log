LSTM(
  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))
  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))
  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))
  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))
  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))
  (linear1): Linear(in_features=23500, out_features=1024, bias=True)
  (linear2): Linear(in_features=1024, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (dropout): Dropout(p=0.2, inplace=False)
)
Window Length:  499
trigger times: 0
Loss after 131100 batches: 0.7982
trigger times: 0
Loss after 262200 batches: 0.4066
trigger times: 0
Loss after 393300 batches: 0.3019
trigger times: 0
Loss after 524400 batches: 0.2381
trigger times: 1
Loss after 655500 batches: 0.2014
trigger times: 0
Loss after 786600 batches: 0.1690
trigger times: 0
Loss after 917700 batches: 0.1449
trigger times: 0
Loss after 1048800 batches: 0.1287
trigger times: 1
Loss after 1179900 batches: 0.1094
trigger times: 0
Loss after 1311000 batches: 0.0994
trigger times: 0
Loss after 1442100 batches: 0.0893
trigger times: 1
Loss after 1573200 batches: 0.0798
trigger times: 0
Loss after 1704300 batches: 0.0721
trigger times: 0
Loss after 1835400 batches: 0.0668
trigger times: 0
Loss after 1966500 batches: 0.0630
trigger times: 1
Loss after 2097600 batches: 0.0567
trigger times: 2
Loss after 2228700 batches: 0.0544
trigger times: 3
Loss after 2359800 batches: 0.0506
trigger times: 0
Loss after 2490900 batches: 0.0482
trigger times: 1
Loss after 2622000 batches: 0.0463
trigger times: 2
Loss after 2753100 batches: 0.0441
trigger times: 3
Loss after 2884200 batches: 0.0427
trigger times: 4
Loss after 3015300 batches: 0.0398
trigger times: 0
Loss after 3146400 batches: 0.0388
trigger times: 1
Loss after 3277500 batches: 0.0360
trigger times: 0
Loss after 3408600 batches: 0.0355
trigger times: 0
Loss after 3539700 batches: 0.0338
trigger times: 1
Loss after 3670800 batches: 0.0321
trigger times: 2
Loss after 3801900 batches: 0.0310
trigger times: 3
Loss after 3933000 batches: 0.0300
trigger times: 0
Loss after 4064100 batches: 0.0289
trigger times: 0
Loss after 4195200 batches: 0.0285
trigger times: 1
Loss after 4326300 batches: 0.0270
trigger times: 2
Loss after 4457400 batches: 0.0265
trigger times: 3
Loss after 4588500 batches: 0.0256
trigger times: 4
Loss after 4719600 batches: 0.0253
trigger times: 5
Loss after 4850700 batches: 0.0252
trigger times: 6
Loss after 4981800 batches: 0.0241
trigger times: 7
Loss after 5112900 batches: 0.0233
trigger times: 0
Loss after 5244000 batches: 0.0232
trigger times: 0
Loss after 5375100 batches: 0.0230
trigger times: 1
Loss after 5506200 batches: 0.0223
trigger times: 2
Loss after 5637300 batches: 0.0223
trigger times: 3
Loss after 5768400 batches: 0.0209
trigger times: 4
Loss after 5899500 batches: 0.0205
trigger times: 5
Loss after 6030600 batches: 0.0206
trigger times: 6
Loss after 6161700 batches: 0.0200
trigger times: 7
Loss after 6292800 batches: 0.0199
trigger times: 8
Loss after 6423900 batches: 0.0189
trigger times: 9
Loss after 6555000 batches: 0.0192
trigger times: 10
Loss after 6686100 batches: 0.0188
trigger times: 11
Loss after 6817200 batches: 0.0180
trigger times: 12
Loss after 6948300 batches: 0.0183
trigger times: 13
Loss after 7079400 batches: 0.0175
trigger times: 14
Loss after 7210500 batches: 0.0170
trigger times: 15
Loss after 7341600 batches: 0.0173
trigger times: 16
Loss after 7472700 batches: 0.0169
trigger times: 17
Loss after 7603800 batches: 0.0166
trigger times: 18
Loss after 7734900 batches: 0.0163
trigger times: 19
Loss after 7866000 batches: 0.0161
trigger times: 20
Early stopping!
Start to test process.
Loss after 7997100 batches: 0.0163
Time to train on one home:  443.60323190689087
trigger times: 0
Loss after 8099700 batches: 1.0160
trigger times: 0
Loss after 8202300 batches: 0.8057
trigger times: 1
Loss after 8304900 batches: 0.6976
trigger times: 2
Loss after 8407500 batches: 0.6154
trigger times: 0
Loss after 8510100 batches: 0.5453
trigger times: 1
Loss after 8612700 batches: 0.4951
trigger times: 0
Loss after 8715300 batches: 0.4598
trigger times: 1
Loss after 8817900 batches: 0.4136
trigger times: 2
Loss after 8920500 batches: 0.3719
trigger times: 3
Loss after 9023100 batches: 0.3504
trigger times: 4
Loss after 9125700 batches: 0.3348
trigger times: 5
Loss after 9228300 batches: 0.2989
trigger times: 6
Loss after 9330900 batches: 0.2649
trigger times: 7
Loss after 9433500 batches: 0.2429
trigger times: 8
Loss after 9536100 batches: 0.2392
trigger times: 9
Loss after 9638700 batches: 0.2449
trigger times: 10
Loss after 9741300 batches: 0.2141
trigger times: 11
Loss after 9843900 batches: 0.1910
trigger times: 12
Loss after 9946500 batches: 0.1787
trigger times: 13
Loss after 10049100 batches: 0.1817
trigger times: 14
Loss after 10151700 batches: 0.1621
trigger times: 15
Loss after 10254300 batches: 0.1550
trigger times: 16
Loss after 10356900 batches: 0.1432
trigger times: 17
Loss after 10459500 batches: 0.1394
trigger times: 18
Loss after 10562100 batches: 0.1323
trigger times: 19
Loss after 10664700 batches: 0.1321
trigger times: 20
Early stopping!
Start to test process.
Loss after 10767300 batches: 0.1275
Time to train on one home:  165.07503986358643
trigger times: 0
Loss after 10898400 batches: 0.9873
trigger times: 0
Loss after 11029500 batches: 0.6419
trigger times: 0
Loss after 11160600 batches: 0.5155
trigger times: 1
Loss after 11291700 batches: 0.4462
trigger times: 2
Loss after 11422800 batches: 0.3773
trigger times: 3
Loss after 11553900 batches: 0.3262
trigger times: 4
Loss after 11685000 batches: 0.2845
trigger times: 5
Loss after 11816100 batches: 0.2556
trigger times: 6
Loss after 11947200 batches: 0.2266
trigger times: 7
Loss after 12078300 batches: 0.2009
trigger times: 8
Loss after 12209400 batches: 0.1825
trigger times: 9
Loss after 12340500 batches: 0.1633
trigger times: 10
Loss after 12471600 batches: 0.1502
trigger times: 11
Loss after 12602700 batches: 0.1370
trigger times: 12
Loss after 12733800 batches: 0.1254
trigger times: 13
Loss after 12864900 batches: 0.1155
trigger times: 14
Loss after 12996000 batches: 0.1088
trigger times: 15
Loss after 13127100 batches: 0.0995
trigger times: 16
Loss after 13258200 batches: 0.0933
trigger times: 17
Loss after 13389300 batches: 0.0897
trigger times: 18
Loss after 13520400 batches: 0.0828
trigger times: 19
Loss after 13651500 batches: 0.0795
trigger times: 20
Early stopping!
Start to test process.
Loss after 13782600 batches: 0.0757
Time to train on one home:  174.51779770851135
trigger times: 0
Loss after 13913700 batches: 0.9927
trigger times: 0
Loss after 14044800 batches: 0.8490
trigger times: 0
Loss after 14175900 batches: 0.7250
trigger times: 1
Loss after 14307000 batches: 0.6572
trigger times: 2
Loss after 14438100 batches: 0.5946
trigger times: 3
Loss after 14569200 batches: 0.5200
trigger times: 4
Loss after 14700300 batches: 0.4356
trigger times: 5
Loss after 14831400 batches: 0.3518
trigger times: 6
Loss after 14962500 batches: 0.2862
trigger times: 7
Loss after 15093600 batches: 0.2414
trigger times: 8
Loss after 15224700 batches: 0.2028
trigger times: 9
Loss after 15355800 batches: 0.1804
trigger times: 10
Loss after 15486900 batches: 0.1609
trigger times: 11
Loss after 15618000 batches: 0.1470
trigger times: 12
Loss after 15749100 batches: 0.1341
trigger times: 13
Loss after 15880200 batches: 0.1262
trigger times: 14
Loss after 16011300 batches: 0.1188
trigger times: 15
Loss after 16142400 batches: 0.1127
trigger times: 16
Loss after 16273500 batches: 0.1083
trigger times: 17
Loss after 16404600 batches: 0.1031
trigger times: 18
Loss after 16535700 batches: 0.0989
trigger times: 19
Loss after 16666800 batches: 0.0959
trigger times: 20
Early stopping!
Start to test process.
Loss after 16797900 batches: 0.0919
Time to train on one home:  174.55330204963684
train_results:  [0.07783265326043251]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505]]
Round_0_results:  [0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 135 < 136; dropping {'Training_Loss': 0.5190557033385871, 'Validation_Loss': 0.3509262800216675, 'Training_R2': 0.47794254054537166, 'Validation_R2': 0.6734115407426213, 'Training_F1': 0.5906658416226243, 'Validation_F1': 0.6856096645293834, 'Training_NEP': 0.8204449876127875, 'Validation_NEP': 0.641025286208114, 'Training_NDE': 0.39191780134685533, 'Validation_NDE': 0.260077833132758, 'Training_MAE': 27.17151087186975, 'Validation_MAE': 17.57957279882978, 'Training_MSE': 1724.377, 'Validation_MSE': 960.4602}.
trigger times: 0
Loss after 16929000 batches: 0.5191
trigger times: 0
Loss after 17060100 batches: 0.2307
trigger times: 0
Loss after 17191200 batches: 0.1739
trigger times: 0
Loss after 17322300 batches: 0.1342
trigger times: 0
Loss after 17453400 batches: 0.1088
trigger times: 0
Loss after 17584500 batches: 0.0881
trigger times: 1
Loss after 17715600 batches: 0.0772
trigger times: 2
Loss after 17846700 batches: 0.0678
trigger times: 3
Loss after 17977800 batches: 0.0619
trigger times: 0
Loss after 18108900 batches: 0.0563
trigger times: 1
Loss after 18240000 batches: 0.0517
trigger times: 0
Loss after 18371100 batches: 0.0485
trigger times: 0
Loss after 18502200 batches: 0.0456
trigger times: 0
Loss after 18633300 batches: 0.0435
trigger times: 1
Loss after 18764400 batches: 0.0405
trigger times: 2
Loss after 18895500 batches: 0.0397
trigger times: 0
Loss after 19026600 batches: 0.0377
trigger times: 1
Loss after 19157700 batches: 0.0361
trigger times: 0
Loss after 19288800 batches: 0.0353
trigger times: 1
Loss after 19419900 batches: 0.0337
trigger times: 0
Loss after 19551000 batches: 0.0333
trigger times: 1
Loss after 19682100 batches: 0.0316
trigger times: 2
Loss after 19813200 batches: 0.0307
trigger times: 3
Loss after 19944300 batches: 0.0300
trigger times: 4
Loss after 20075400 batches: 0.0290
trigger times: 0
Loss after 20206500 batches: 0.0282
trigger times: 1
Loss after 20337600 batches: 0.0275
trigger times: 2
Loss after 20468700 batches: 0.0270
trigger times: 3
Loss after 20599800 batches: 0.0264
trigger times: 4
Loss after 20730900 batches: 0.0257
trigger times: 5
Loss after 20862000 batches: 0.0250
trigger times: 0
Loss after 20993100 batches: 0.0243
trigger times: 1
Loss after 21124200 batches: 0.0250
trigger times: 2
Loss after 21255300 batches: 0.0235
trigger times: 3
Loss after 21386400 batches: 0.0232
trigger times: 4
Loss after 21517500 batches: 0.0227
trigger times: 5
Loss after 21648600 batches: 0.0224
trigger times: 0
Loss after 21779700 batches: 0.0219
trigger times: 1
Loss after 21910800 batches: 0.0215
trigger times: 2
Loss after 22041900 batches: 0.0218
trigger times: 3
Loss after 22173000 batches: 0.0213
trigger times: 4
Loss after 22304100 batches: 0.0210
trigger times: 5
Loss after 22435200 batches: 0.0205
trigger times: 6
Loss after 22566300 batches: 0.0205
trigger times: 7
Loss after 22697400 batches: 0.0199
trigger times: 8
Loss after 22828500 batches: 0.0197
trigger times: 0
Loss after 22959600 batches: 0.0197
trigger times: 1
Loss after 23090700 batches: 0.0191
trigger times: 2
Loss after 23221800 batches: 0.0190
trigger times: 0
Loss after 23352900 batches: 0.0186
trigger times: 1
Loss after 23484000 batches: 0.0181
trigger times: 0
Loss after 23615100 batches: 0.0183
trigger times: 1
Loss after 23746200 batches: 0.0180
trigger times: 2
Loss after 23877300 batches: 0.0174
trigger times: 3
Loss after 24008400 batches: 0.0174
trigger times: 4
Loss after 24139500 batches: 0.0175
trigger times: 5
Loss after 24270600 batches: 0.0171
trigger times: 6
Loss after 24401700 batches: 0.0168
trigger times: 0
Loss after 24532800 batches: 0.0170
trigger times: 1
Loss after 24663900 batches: 0.0164
trigger times: 2
Loss after 24795000 batches: 0.0163
trigger times: 3
Loss after 24926100 batches: 0.0159
trigger times: 4
Loss after 25057200 batches: 0.0158
trigger times: 5
Loss after 25188300 batches: 0.0159
trigger times: 6
Loss after 25319400 batches: 0.0156
trigger times: 7
Loss after 25450500 batches: 0.0153
trigger times: 8
Loss after 25581600 batches: 0.0158
trigger times: 9
Loss after 25712700 batches: 0.0154
trigger times: 10
Loss after 25843800 batches: 0.0152
trigger times: 11
Loss after 25974900 batches: 0.0150
trigger times: 0
Loss after 26106000 batches: 0.0151
trigger times: 1
Loss after 26237100 batches: 0.0149
trigger times: 2
Loss after 26368200 batches: 0.0146
trigger times: 3
Loss after 26499300 batches: 0.0142
trigger times: 4
Loss after 26630400 batches: 0.0145
trigger times: 5
Loss after 26761500 batches: 0.0142
trigger times: 6
Loss after 26892600 batches: 0.0143
trigger times: 7
Loss after 27023700 batches: 0.0137
trigger times: 8
Loss after 27154800 batches: 0.0139
trigger times: 9
Loss after 27285900 batches: 0.0136
trigger times: 10
Loss after 27417000 batches: 0.0135
trigger times: 11
Loss after 27548100 batches: 0.0139
trigger times: 12
Loss after 27679200 batches: 0.0133
trigger times: 13
Loss after 27810300 batches: 0.0135
trigger times: 14
Loss after 27941400 batches: 0.0133
trigger times: 15
Loss after 28072500 batches: 0.0131
trigger times: 16
Loss after 28203600 batches: 0.0133
trigger times: 17
Loss after 28334700 batches: 0.0130
trigger times: 18
Loss after 28465800 batches: 0.0131
trigger times: 19
Loss after 28596900 batches: 0.0133
trigger times: 20
Early stopping!
Start to test process.
Loss after 28728000 batches: 0.0128
Time to train on one home:  657.468859910965
trigger times: 0
Loss after 28830600 batches: 0.7568
trigger times: 1
Loss after 28933200 batches: 0.5103
trigger times: 2
Loss after 29035800 batches: 0.4209
trigger times: 3
Loss after 29138400 batches: 0.3374
trigger times: 4
Loss after 29241000 batches: 0.2814
trigger times: 5
Loss after 29343600 batches: 0.2375
trigger times: 6
Loss after 29446200 batches: 0.2174
trigger times: 7
Loss after 29548800 batches: 0.1953
trigger times: 8
Loss after 29651400 batches: 0.1727
trigger times: 9
Loss after 29754000 batches: 0.1649
trigger times: 10
Loss after 29856600 batches: 0.1652
trigger times: 11
Loss after 29959200 batches: 0.1402
trigger times: 12
Loss after 30061800 batches: 0.1305
trigger times: 13
Loss after 30164400 batches: 0.1222
trigger times: 14
Loss after 30267000 batches: 0.1193
trigger times: 15
Loss after 30369600 batches: 0.1213
trigger times: 16
Loss after 30472200 batches: 0.1152
trigger times: 17
Loss after 30574800 batches: 0.1070
trigger times: 18
Loss after 30677400 batches: 0.1063
trigger times: 19
Loss after 30780000 batches: 0.0988
trigger times: 20
Early stopping!
Start to test process.
Loss after 30882600 batches: 0.0931
Time to train on one home:  131.52371430397034
trigger times: 0
Loss after 31013700 batches: 0.5396
trigger times: 1
Loss after 31144800 batches: 0.3246
trigger times: 2
Loss after 31275900 batches: 0.2337
trigger times: 3
Loss after 31407000 batches: 0.1771
trigger times: 4
Loss after 31538100 batches: 0.1407
trigger times: 5
Loss after 31669200 batches: 0.1185
trigger times: 6
Loss after 31800300 batches: 0.1060
trigger times: 7
Loss after 31931400 batches: 0.0945
trigger times: 8
Loss after 32062500 batches: 0.0877
trigger times: 9
Loss after 32193600 batches: 0.0785
trigger times: 10
Loss after 32324700 batches: 0.0741
trigger times: 11
Loss after 32455800 batches: 0.0703
trigger times: 12
Loss after 32586900 batches: 0.0669
trigger times: 13
Loss after 32718000 batches: 0.0633
trigger times: 14
Loss after 32849100 batches: 0.0608
trigger times: 15
Loss after 32980200 batches: 0.0581
trigger times: 16
Loss after 33111300 batches: 0.0553
trigger times: 17
Loss after 33242400 batches: 0.0530
trigger times: 18
Loss after 33373500 batches: 0.0519
trigger times: 19
Loss after 33504600 batches: 0.0499
trigger times: 20
Early stopping!
Start to test process.
Loss after 33635700 batches: 0.0479
Time to train on one home:  161.02880835533142
trigger times: 0
Loss after 33766800 batches: 0.7483
trigger times: 1
Loss after 33897900 batches: 0.5374
trigger times: 2
Loss after 34029000 batches: 0.3676
trigger times: 3
Loss after 34160100 batches: 0.2627
trigger times: 4
Loss after 34291200 batches: 0.2112
trigger times: 5
Loss after 34422300 batches: 0.1737
trigger times: 6
Loss after 34553400 batches: 0.1529
trigger times: 7
Loss after 34684500 batches: 0.1384
trigger times: 8
Loss after 34815600 batches: 0.1285
trigger times: 9
Loss after 34946700 batches: 0.1194
trigger times: 10
Loss after 35077800 batches: 0.1129
trigger times: 11
Loss after 35208900 batches: 0.1053
trigger times: 12
Loss after 35340000 batches: 0.1021
trigger times: 13
Loss after 35471100 batches: 0.0968
trigger times: 14
Loss after 35602200 batches: 0.0942
trigger times: 15
Loss after 35733300 batches: 0.0913
trigger times: 16
Loss after 35864400 batches: 0.0866
trigger times: 17
Loss after 35995500 batches: 0.0841
trigger times: 18
Loss after 36126600 batches: 0.0817
trigger times: 19
Loss after 36257700 batches: 0.0786
trigger times: 20
Early stopping!
Start to test process.
Loss after 36388800 batches: 0.0772
Time to train on one home:  160.68582391738892
train_results:  [0.07783265326043251, 0.05773241920455259]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323]]
Round_1_results:  [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 289 < 290; dropping {'Training_Loss': 0.18703240465443088, 'Validation_Loss': 0.30506065156724715, 'Training_R2': 0.8118573752909682, 'Validation_R2': 0.7162972144097183, 'Training_F1': 0.7638953445593205, 'Validation_F1': 0.714025656897094, 'Training_NEP': 0.47227868077235685, 'Validation_NEP': 0.5656035030056247, 'Training_NDE': 0.14124200790583447, 'Validation_NDE': 0.22592594330438157, 'Training_MAE': 15.640933277557862, 'Validation_MAE': 15.51119459760659, 'Training_MSE': 621.4426, 'Validation_MSE': 834.33826}.
trigger times: 0
Loss after 36519900 batches: 0.1870
trigger times: 0
Loss after 36651000 batches: 0.0583
trigger times: 0
Loss after 36782100 batches: 0.0412
trigger times: 0
Loss after 36913200 batches: 0.0330
trigger times: 0
Loss after 37044300 batches: 0.0287
trigger times: 0
Loss after 37175400 batches: 0.0264
trigger times: 1
Loss after 37306500 batches: 0.0247
trigger times: 0
Loss after 37437600 batches: 0.0234
trigger times: 1
Loss after 37568700 batches: 0.0220
trigger times: 2
Loss after 37699800 batches: 0.0216
trigger times: 0
Loss after 37830900 batches: 0.0200
trigger times: 1
Loss after 37962000 batches: 0.0196
trigger times: 2
Loss after 38093100 batches: 0.0189
trigger times: 3
Loss after 38224200 batches: 0.0181
trigger times: 4
Loss after 38355300 batches: 0.0177
trigger times: 5
Loss after 38486400 batches: 0.0171
trigger times: 6
Loss after 38617500 batches: 0.0166
trigger times: 7
Loss after 38748600 batches: 0.0162
trigger times: 0
Loss after 38879700 batches: 0.0165
trigger times: 1
Loss after 39010800 batches: 0.0156
trigger times: 2
Loss after 39141900 batches: 0.0157
trigger times: 3
Loss after 39273000 batches: 0.0151
trigger times: 4
Loss after 39404100 batches: 0.0151
trigger times: 5
Loss after 39535200 batches: 0.0150
trigger times: 6
Loss after 39666300 batches: 0.0146
trigger times: 7
Loss after 39797400 batches: 0.0142
trigger times: 8
Loss after 39928500 batches: 0.0143
trigger times: 9
Loss after 40059600 batches: 0.0143
trigger times: 10
Loss after 40190700 batches: 0.0140
trigger times: 11
Loss after 40321800 batches: 0.0136
trigger times: 12
Loss after 40452900 batches: 0.0134
trigger times: 13
Loss after 40584000 batches: 0.0134
trigger times: 14
Loss after 40715100 batches: 0.0133
trigger times: 15
Loss after 40846200 batches: 0.0131
trigger times: 16
Loss after 40977300 batches: 0.0130
trigger times: 17
Loss after 41108400 batches: 0.0130
trigger times: 0
Loss after 41239500 batches: 0.0126
trigger times: 1
Loss after 41370600 batches: 0.0123
trigger times: 2
Loss after 41501700 batches: 0.0124
trigger times: 0
Loss after 41632800 batches: 0.0121
trigger times: 1
Loss after 41763900 batches: 0.0120
trigger times: 2
Loss after 41895000 batches: 0.0122
trigger times: 3
Loss after 42026100 batches: 0.0118
trigger times: 4
Loss after 42157200 batches: 0.0117
trigger times: 5
Loss after 42288300 batches: 0.0118
trigger times: 6
Loss after 42419400 batches: 0.0116
trigger times: 7
Loss after 42550500 batches: 0.0113
trigger times: 8
Loss after 42681600 batches: 0.0113
trigger times: 9
Loss after 42812700 batches: 0.0112
trigger times: 10
Loss after 42943800 batches: 0.0112
trigger times: 11
Loss after 43074900 batches: 0.0112
trigger times: 12
Loss after 43206000 batches: 0.0111
trigger times: 13
Loss after 43337100 batches: 0.0106
trigger times: 14
Loss after 43468200 batches: 0.0108
trigger times: 15
Loss after 43599300 batches: 0.0109
trigger times: 16
Loss after 43730400 batches: 0.0109
trigger times: 17
Loss after 43861500 batches: 0.0108
trigger times: 18
Loss after 43992600 batches: 0.0107
trigger times: 0
Loss after 44123700 batches: 0.0105
trigger times: 1
Loss after 44254800 batches: 0.0104
trigger times: 2
Loss after 44385900 batches: 0.0102
trigger times: 0
Loss after 44517000 batches: 0.0105
trigger times: 1
Loss after 44648100 batches: 0.0104
trigger times: 2
Loss after 44779200 batches: 0.0102
trigger times: 3
Loss after 44910300 batches: 0.0102
trigger times: 4
Loss after 45041400 batches: 0.0102
trigger times: 5
Loss after 45172500 batches: 0.0102
trigger times: 6
Loss after 45303600 batches: 0.0100
trigger times: 7
Loss after 45434700 batches: 0.0098
trigger times: 8
Loss after 45565800 batches: 0.0097
trigger times: 9
Loss after 45696900 batches: 0.0096
trigger times: 10
Loss after 45828000 batches: 0.0099
trigger times: 11
Loss after 45959100 batches: 0.0095
trigger times: 12
Loss after 46090200 batches: 0.0095
trigger times: 13
Loss after 46221300 batches: 0.0096
trigger times: 14
Loss after 46352400 batches: 0.0096
trigger times: 15
Loss after 46483500 batches: 0.0093
trigger times: 16
Loss after 46614600 batches: 0.0091
trigger times: 17
Loss after 46745700 batches: 0.0093
trigger times: 18
Loss after 46876800 batches: 0.0091
trigger times: 19
Loss after 47007900 batches: 0.0092
trigger times: 20
Early stopping!
Start to test process.
Loss after 47139000 batches: 0.0094
Time to train on one home:  594.0404374599457
trigger times: 0
Loss after 47241600 batches: 0.5006
trigger times: 1
Loss after 47344200 batches: 0.2628
trigger times: 2
Loss after 47446800 batches: 0.1933
trigger times: 3
Loss after 47549400 batches: 0.1538
trigger times: 4
Loss after 47652000 batches: 0.1293
trigger times: 5
Loss after 47754600 batches: 0.1139
trigger times: 6
Loss after 47857200 batches: 0.0999
trigger times: 7
Loss after 47959800 batches: 0.0922
trigger times: 8
Loss after 48062400 batches: 0.0900
trigger times: 9
Loss after 48165000 batches: 0.0818
trigger times: 10
Loss after 48267600 batches: 0.0783
trigger times: 11
Loss after 48370200 batches: 0.0738
trigger times: 12
Loss after 48472800 batches: 0.0700
trigger times: 13
Loss after 48575400 batches: 0.0690
trigger times: 14
Loss after 48678000 batches: 0.0725
trigger times: 15
Loss after 48780600 batches: 0.0670
trigger times: 16
Loss after 48883200 batches: 0.0643
trigger times: 17
Loss after 48985800 batches: 0.0607
trigger times: 18
Loss after 49088400 batches: 0.0574
trigger times: 19
Loss after 49191000 batches: 0.0518
trigger times: 20
Early stopping!
Start to test process.
Loss after 49293600 batches: 0.0529
Time to train on one home:  131.53861021995544
trigger times: 0
Loss after 49424700 batches: 0.3285
trigger times: 0
Loss after 49555800 batches: 0.1336
trigger times: 1
Loss after 49686900 batches: 0.0889
trigger times: 0
Loss after 49818000 batches: 0.0704
trigger times: 1
Loss after 49949100 batches: 0.0621
trigger times: 2
Loss after 50080200 batches: 0.0555
trigger times: 3
Loss after 50211300 batches: 0.0505
trigger times: 0
Loss after 50342400 batches: 0.0473
trigger times: 1
Loss after 50473500 batches: 0.0443
trigger times: 2
Loss after 50604600 batches: 0.0421
trigger times: 3
Loss after 50735700 batches: 0.0401
trigger times: 0
Loss after 50866800 batches: 0.0383
trigger times: 1
Loss after 50997900 batches: 0.0364
trigger times: 0
Loss after 51129000 batches: 0.0350
trigger times: 1
Loss after 51260100 batches: 0.0336
trigger times: 2
Loss after 51391200 batches: 0.0328
trigger times: 3
Loss after 51522300 batches: 0.0314
trigger times: 0
Loss after 51653400 batches: 0.0316
trigger times: 0
Loss after 51784500 batches: 0.0299
trigger times: 1
Loss after 51915600 batches: 0.0295
trigger times: 2
Loss after 52046700 batches: 0.0286
trigger times: 3
Loss after 52177800 batches: 0.0281
trigger times: 4
Loss after 52308900 batches: 0.0272
trigger times: 5
Loss after 52440000 batches: 0.0270
trigger times: 0
Loss after 52571100 batches: 0.0263
trigger times: 0
Loss after 52702200 batches: 0.0261
trigger times: 1
Loss after 52833300 batches: 0.0256
trigger times: 2
Loss after 52964400 batches: 0.0256
trigger times: 3
Loss after 53095500 batches: 0.0244
trigger times: 4
Loss after 53226600 batches: 0.0244
trigger times: 5
Loss after 53357700 batches: 0.0241
trigger times: 6
Loss after 53488800 batches: 0.0236
trigger times: 7
Loss after 53619900 batches: 0.0235
trigger times: 8
Loss after 53751000 batches: 0.0235
trigger times: 0
Loss after 53882100 batches: 0.0233
trigger times: 1
Loss after 54013200 batches: 0.0229
trigger times: 2
Loss after 54144300 batches: 0.0221
trigger times: 3
Loss after 54275400 batches: 0.0221
trigger times: 4
Loss after 54406500 batches: 0.0219
trigger times: 5
Loss after 54537600 batches: 0.0216
trigger times: 0
Loss after 54668700 batches: 0.0211
trigger times: 1
Loss after 54799800 batches: 0.0212
trigger times: 0
Loss after 54930900 batches: 0.0212
trigger times: 1
Loss after 55062000 batches: 0.0206
trigger times: 0
Loss after 55193100 batches: 0.0204
trigger times: 1
Loss after 55324200 batches: 0.0206
trigger times: 2
Loss after 55455300 batches: 0.0203
trigger times: 3
Loss after 55586400 batches: 0.0199
trigger times: 4
Loss after 55717500 batches: 0.0196
trigger times: 5
Loss after 55848600 batches: 0.0197
trigger times: 6
Loss after 55979700 batches: 0.0194
trigger times: 7
Loss after 56110800 batches: 0.0193
trigger times: 8
Loss after 56241900 batches: 0.0194
trigger times: 9
Loss after 56373000 batches: 0.0187
trigger times: 10
Loss after 56504100 batches: 0.0191
trigger times: 11
Loss after 56635200 batches: 0.0187
trigger times: 0
Loss after 56766300 batches: 0.0187
trigger times: 1
Loss after 56897400 batches: 0.0186
trigger times: 2
Loss after 57028500 batches: 0.0184
trigger times: 3
Loss after 57159600 batches: 0.0179
trigger times: 4
Loss after 57290700 batches: 0.0179
trigger times: 5
Loss after 57421800 batches: 0.0180
trigger times: 6
Loss after 57552900 batches: 0.0179
trigger times: 7
Loss after 57684000 batches: 0.0175
trigger times: 8
Loss after 57815100 batches: 0.0179
trigger times: 9
Loss after 57946200 batches: 0.0177
trigger times: 10
Loss after 58077300 batches: 0.0173
trigger times: 11
Loss after 58208400 batches: 0.0173
trigger times: 0
Loss after 58339500 batches: 0.0180
trigger times: 1
Loss after 58470600 batches: 0.0175
trigger times: 2
Loss after 58601700 batches: 0.0171
trigger times: 3
Loss after 58732800 batches: 0.0168
trigger times: 4
Loss after 58863900 batches: 0.0169
trigger times: 5
Loss after 58995000 batches: 0.0167
trigger times: 6
Loss after 59126100 batches: 0.0168
trigger times: 7
Loss after 59257200 batches: 0.0168
trigger times: 8
Loss after 59388300 batches: 0.0163
trigger times: 9
Loss after 59519400 batches: 0.0165
trigger times: 10
Loss after 59650500 batches: 0.0162
trigger times: 11
Loss after 59781600 batches: 0.0164
trigger times: 12
Loss after 59912700 batches: 0.0161
trigger times: 13
Loss after 60043800 batches: 0.0162
trigger times: 14
Loss after 60174900 batches: 0.0159
trigger times: 15
Loss after 60306000 batches: 0.0159
trigger times: 16
Loss after 60437100 batches: 0.0158
trigger times: 17
Loss after 60568200 batches: 0.0159
trigger times: 18
Loss after 60699300 batches: 0.0158
trigger times: 19
Loss after 60830400 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 60961500 batches: 0.0157
Time to train on one home:  642.8496725559235
trigger times: 0
Loss after 61092600 batches: 0.5358
trigger times: 1
Loss after 61223700 batches: 0.2159
trigger times: 2
Loss after 61354800 batches: 0.1361
trigger times: 3
Loss after 61485900 batches: 0.1069
trigger times: 4
Loss after 61617000 batches: 0.0945
trigger times: 5
Loss after 61748100 batches: 0.0875
trigger times: 6
Loss after 61879200 batches: 0.0784
trigger times: 7
Loss after 62010300 batches: 0.0743
trigger times: 8
Loss after 62141400 batches: 0.0704
trigger times: 9
Loss after 62272500 batches: 0.0662
trigger times: 10
Loss after 62403600 batches: 0.0630
trigger times: 11
Loss after 62534700 batches: 0.0609
trigger times: 12
Loss after 62665800 batches: 0.0595
trigger times: 13
Loss after 62796900 batches: 0.0566
trigger times: 14
Loss after 62928000 batches: 0.0549
trigger times: 15
Loss after 63059100 batches: 0.0537
trigger times: 16
Loss after 63190200 batches: 0.0521
trigger times: 17
Loss after 63321300 batches: 0.0503
trigger times: 18
Loss after 63452400 batches: 0.0490
trigger times: 19
Loss after 63583500 batches: 0.0478
trigger times: 20
Early stopping!
Start to test process.
Loss after 63714600 batches: 0.0468
Time to train on one home:  160.4188356399536
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103]]
Round_2_results:  [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 502 < 503; dropping {'Training_Loss': 0.11891597917057434, 'Validation_Loss': 0.24332077470090654, 'Training_R2': 0.8803399936827732, 'Validation_R2': 0.7736523771417705, 'Training_F1': 0.8125863884514759, 'Validation_F1': 0.7390508503223898, 'Training_NEP': 0.37643920607034465, 'Validation_NEP': 0.5279128384471727, 'Training_NDE': 0.08983089071074639, 'Validation_NDE': 0.18025131513090673, 'Training_MAE': 12.466919945601195, 'Validation_MAE': 14.477560206425233, 'Training_MSE': 395.24182, 'Validation_MSE': 665.66315}.
trigger times: 0
Loss after 63845700 batches: 0.1189
trigger times: 0
Loss after 63976800 batches: 0.0353
trigger times: 1
Loss after 64107900 batches: 0.0265
trigger times: 0
Loss after 64239000 batches: 0.0224
trigger times: 0
Loss after 64370100 batches: 0.0200
trigger times: 1
Loss after 64501200 batches: 0.0187
trigger times: 2
Loss after 64632300 batches: 0.0173
trigger times: 3
Loss after 64763400 batches: 0.0167
trigger times: 4
Loss after 64894500 batches: 0.0163
trigger times: 5
Loss after 65025600 batches: 0.0155
trigger times: 6
Loss after 65156700 batches: 0.0149
trigger times: 7
Loss after 65287800 batches: 0.0144
trigger times: 8
Loss after 65418900 batches: 0.0142
trigger times: 9
Loss after 65550000 batches: 0.0139
trigger times: 10
Loss after 65681100 batches: 0.0138
trigger times: 0
Loss after 65812200 batches: 0.0132
trigger times: 0
Loss after 65943300 batches: 0.0130
trigger times: 1
Loss after 66074400 batches: 0.0130
trigger times: 2
Loss after 66205500 batches: 0.0126
trigger times: 0
Loss after 66336600 batches: 0.0123
trigger times: 1
Loss after 66467700 batches: 0.0120
trigger times: 0
Loss after 66598800 batches: 0.0122
trigger times: 1
Loss after 66729900 batches: 0.0119
trigger times: 2
Loss after 66861000 batches: 0.0118
trigger times: 3
Loss after 66992100 batches: 0.0115
trigger times: 4
Loss after 67123200 batches: 0.0115
trigger times: 5
Loss after 67254300 batches: 0.0114
trigger times: 6
Loss after 67385400 batches: 0.0111
trigger times: 7
Loss after 67516500 batches: 0.0110
trigger times: 8
Loss after 67647600 batches: 0.0111
trigger times: 9
Loss after 67778700 batches: 0.0109
trigger times: 10
Loss after 67909800 batches: 0.0110
trigger times: 11
Loss after 68040900 batches: 0.0110
trigger times: 12
Loss after 68172000 batches: 0.0107
trigger times: 13
Loss after 68303100 batches: 0.0105
trigger times: 14
Loss after 68434200 batches: 0.0105
trigger times: 15
Loss after 68565300 batches: 0.0106
trigger times: 16
Loss after 68696400 batches: 0.0104
trigger times: 17
Loss after 68827500 batches: 0.0102
trigger times: 18
Loss after 68958600 batches: 0.0103
trigger times: 19
Loss after 69089700 batches: 0.0104
trigger times: 20
Early stopping!
Start to test process.
Loss after 69220800 batches: 0.0100
Time to train on one home:  309.6936776638031
trigger times: 0
Loss after 69323400 batches: 0.3858
trigger times: 1
Loss after 69426000 batches: 0.1632
trigger times: 2
Loss after 69528600 batches: 0.1153
trigger times: 3
Loss after 69631200 batches: 0.0972
trigger times: 4
Loss after 69733800 batches: 0.0957
trigger times: 5
Loss after 69836400 batches: 0.0737
trigger times: 6
Loss after 69939000 batches: 0.0706
trigger times: 7
Loss after 70041600 batches: 0.0642
trigger times: 8
Loss after 70144200 batches: 0.0632
trigger times: 9
Loss after 70246800 batches: 0.0570
trigger times: 10
Loss after 70349400 batches: 0.0509
trigger times: 11
Loss after 70452000 batches: 0.0499
trigger times: 12
Loss after 70554600 batches: 0.0479
trigger times: 13
Loss after 70657200 batches: 0.0451
trigger times: 14
Loss after 70759800 batches: 0.0425
trigger times: 15
Loss after 70862400 batches: 0.0408
trigger times: 16
Loss after 70965000 batches: 0.0403
trigger times: 17
Loss after 71067600 batches: 0.0384
trigger times: 18
Loss after 71170200 batches: 0.0359
trigger times: 19
Loss after 71272800 batches: 0.0366
trigger times: 20
Early stopping!
Start to test process.
Loss after 71375400 batches: 0.0359
Time to train on one home:  131.50544691085815
trigger times: 0
Loss after 71506500 batches: 0.1776
trigger times: 1
Loss after 71637600 batches: 0.0613
trigger times: 2
Loss after 71768700 batches: 0.0434
trigger times: 3
Loss after 71899800 batches: 0.0373
trigger times: 4
Loss after 72030900 batches: 0.0328
trigger times: 5
Loss after 72162000 batches: 0.0302
trigger times: 6
Loss after 72293100 batches: 0.0282
trigger times: 7
Loss after 72424200 batches: 0.0270
trigger times: 0
Loss after 72555300 batches: 0.0256
trigger times: 1
Loss after 72686400 batches: 0.0245
trigger times: 2
Loss after 72817500 batches: 0.0236
trigger times: 3
Loss after 72948600 batches: 0.0230
trigger times: 4
Loss after 73079700 batches: 0.0223
trigger times: 0
Loss after 73210800 batches: 0.0220
trigger times: 1
Loss after 73341900 batches: 0.0214
trigger times: 2
Loss after 73473000 batches: 0.0209
trigger times: 3
Loss after 73604100 batches: 0.0204
trigger times: 0
Loss after 73735200 batches: 0.0199
trigger times: 1
Loss after 73866300 batches: 0.0196
trigger times: 0
Loss after 73997400 batches: 0.0194
trigger times: 1
Loss after 74128500 batches: 0.0193
trigger times: 2
Loss after 74259600 batches: 0.0192
trigger times: 3
Loss after 74390700 batches: 0.0187
trigger times: 4
Loss after 74521800 batches: 0.0187
trigger times: 5
Loss after 74652900 batches: 0.0180
trigger times: 6
Loss after 74784000 batches: 0.0179
trigger times: 7
Loss after 74915100 batches: 0.0175
trigger times: 0
Loss after 75046200 batches: 0.0175
trigger times: 1
Loss after 75177300 batches: 0.0175
trigger times: 0
Loss after 75308400 batches: 0.0173
trigger times: 1
Loss after 75439500 batches: 0.0169
trigger times: 2
Loss after 75570600 batches: 0.0170
trigger times: 3
Loss after 75701700 batches: 0.0171
trigger times: 4
Loss after 75832800 batches: 0.0167
trigger times: 5
Loss after 75963900 batches: 0.0168
trigger times: 6
Loss after 76095000 batches: 0.0165
trigger times: 7
Loss after 76226100 batches: 0.0162
trigger times: 8
Loss after 76357200 batches: 0.0161
trigger times: 0
Loss after 76488300 batches: 0.0162
trigger times: 1
Loss after 76619400 batches: 0.0163
trigger times: 2
Loss after 76750500 batches: 0.0158
trigger times: 3
Loss after 76881600 batches: 0.0159
trigger times: 0
Loss after 77012700 batches: 0.0154
trigger times: 1
Loss after 77143800 batches: 0.0155
trigger times: 2
Loss after 77274900 batches: 0.0152
trigger times: 3
Loss after 77406000 batches: 0.0153
trigger times: 4
Loss after 77537100 batches: 0.0153
trigger times: 0
Loss after 77668200 batches: 0.0153
trigger times: 1
Loss after 77799300 batches: 0.0153
trigger times: 2
Loss after 77930400 batches: 0.0151
trigger times: 3
Loss after 78061500 batches: 0.0147
trigger times: 4
Loss after 78192600 batches: 0.0147
trigger times: 5
Loss after 78323700 batches: 0.0146
trigger times: 6
Loss after 78454800 batches: 0.0146
trigger times: 7
Loss after 78585900 batches: 0.0146
trigger times: 8
Loss after 78717000 batches: 0.0145
trigger times: 9
Loss after 78848100 batches: 0.0145
trigger times: 10
Loss after 78979200 batches: 0.0144
trigger times: 11
Loss after 79110300 batches: 0.0144
trigger times: 12
Loss after 79241400 batches: 0.0141
trigger times: 13
Loss after 79372500 batches: 0.0140
trigger times: 14
Loss after 79503600 batches: 0.0140
trigger times: 15
Loss after 79634700 batches: 0.0140
trigger times: 16
Loss after 79765800 batches: 0.0139
trigger times: 17
Loss after 79896900 batches: 0.0136
trigger times: 0
Loss after 80028000 batches: 0.0136
trigger times: 1
Loss after 80159100 batches: 0.0135
trigger times: 2
Loss after 80290200 batches: 0.0137
trigger times: 3
Loss after 80421300 batches: 0.0137
trigger times: 0
Loss after 80552400 batches: 0.0136
trigger times: 1
Loss after 80683500 batches: 0.0134
trigger times: 2
Loss after 80814600 batches: 0.0135
trigger times: 3
Loss after 80945700 batches: 0.0133
trigger times: 4
Loss after 81076800 batches: 0.0131
trigger times: 5
Loss after 81207900 batches: 0.0131
trigger times: 6
Loss after 81339000 batches: 0.0132
trigger times: 7
Loss after 81470100 batches: 0.0132
trigger times: 8
Loss after 81601200 batches: 0.0137
trigger times: 9
Loss after 81732300 batches: 0.0133
trigger times: 10
Loss after 81863400 batches: 0.0129
trigger times: 0
Loss after 81994500 batches: 0.0131
trigger times: 1
Loss after 82125600 batches: 0.0131
trigger times: 2
Loss after 82256700 batches: 0.0130
trigger times: 3
Loss after 82387800 batches: 0.0128
trigger times: 4
Loss after 82518900 batches: 0.0126
trigger times: 5
Loss after 82650000 batches: 0.0126
trigger times: 6
Loss after 82781100 batches: 0.0128
trigger times: 0
Loss after 82912200 batches: 0.0127
trigger times: 1
Loss after 83043300 batches: 0.0127
trigger times: 2
Loss after 83174400 batches: 0.0127
trigger times: 3
Loss after 83305500 batches: 0.0126
trigger times: 4
Loss after 83436600 batches: 0.0126
trigger times: 5
Loss after 83567700 batches: 0.0127
trigger times: 0
Loss after 83698800 batches: 0.0126
trigger times: 1
Loss after 83829900 batches: 0.0124
trigger times: 2
Loss after 83961000 batches: 0.0123
trigger times: 3
Loss after 84092100 batches: 0.0126
trigger times: 4
Loss after 84223200 batches: 0.0124
trigger times: 5
Loss after 84354300 batches: 0.0124
trigger times: 6
Loss after 84485400 batches: 0.0125
trigger times: 7
Loss after 84616500 batches: 0.0122
trigger times: 8
Loss after 84747600 batches: 0.0125
trigger times: 9
Loss after 84878700 batches: 0.0121
trigger times: 10
Loss after 85009800 batches: 0.0123
trigger times: 11
Loss after 85140900 batches: 0.0124
trigger times: 12
Loss after 85272000 batches: 0.0122
trigger times: 13
Loss after 85403100 batches: 0.0123
trigger times: 14
Loss after 85534200 batches: 0.0122
trigger times: 15
Loss after 85665300 batches: 0.0119
trigger times: 16
Loss after 85796400 batches: 0.0122
trigger times: 17
Loss after 85927500 batches: 0.0121
trigger times: 18
Loss after 86058600 batches: 0.0121
trigger times: 19
Loss after 86189700 batches: 0.0119
trigger times: 20
Early stopping!
Start to test process.
Loss after 86320800 batches: 0.0117
Time to train on one home:  821.9935703277588
trigger times: 0
Loss after 86451900 batches: 0.3895
trigger times: 0
Loss after 86583000 batches: 0.1285
trigger times: 1
Loss after 86714100 batches: 0.0923
trigger times: 2
Loss after 86845200 batches: 0.0777
trigger times: 3
Loss after 86976300 batches: 0.0696
trigger times: 4
Loss after 87107400 batches: 0.0633
trigger times: 5
Loss after 87238500 batches: 0.0594
trigger times: 6
Loss after 87369600 batches: 0.0558
trigger times: 7
Loss after 87500700 batches: 0.0526
trigger times: 8
Loss after 87631800 batches: 0.0499
trigger times: 9
Loss after 87762900 batches: 0.0479
trigger times: 10
Loss after 87894000 batches: 0.0462
trigger times: 11
Loss after 88025100 batches: 0.0451
trigger times: 12
Loss after 88156200 batches: 0.0440
trigger times: 13
Loss after 88287300 batches: 0.0428
trigger times: 0
Loss after 88418400 batches: 0.0409
trigger times: 1
Loss after 88549500 batches: 0.0399
trigger times: 2
Loss after 88680600 batches: 0.0390
trigger times: 0
Loss after 88811700 batches: 0.0376
trigger times: 1
Loss after 88942800 batches: 0.0375
trigger times: 2
Loss after 89073900 batches: 0.0358
trigger times: 0
Loss after 89205000 batches: 0.0356
trigger times: 1
Loss after 89336100 batches: 0.0352
trigger times: 2
Loss after 89467200 batches: 0.0353
trigger times: 3
Loss after 89598300 batches: 0.0342
trigger times: 0
Loss after 89729400 batches: 0.0335
trigger times: 1
Loss after 89860500 batches: 0.0327
trigger times: 2
Loss after 89991600 batches: 0.0330
trigger times: 3
Loss after 90122700 batches: 0.0323
trigger times: 4
Loss after 90253800 batches: 0.0318
trigger times: 5
Loss after 90384900 batches: 0.0317
trigger times: 6
Loss after 90516000 batches: 0.0315
trigger times: 7
Loss after 90647100 batches: 0.0309
trigger times: 8
Loss after 90778200 batches: 0.0302
trigger times: 9
Loss after 90909300 batches: 0.0301
trigger times: 10
Loss after 91040400 batches: 0.0299
trigger times: 11
Loss after 91171500 batches: 0.0297
trigger times: 0
Loss after 91302600 batches: 0.0294
trigger times: 1
Loss after 91433700 batches: 0.0285
trigger times: 2
Loss after 91564800 batches: 0.0280
trigger times: 3
Loss after 91695900 batches: 0.0280
trigger times: 4
Loss after 91827000 batches: 0.0280
trigger times: 0
Loss after 91958100 batches: 0.0277
trigger times: 1
Loss after 92089200 batches: 0.0275
trigger times: 2
Loss after 92220300 batches: 0.0275
trigger times: 3
Loss after 92351400 batches: 0.0271
trigger times: 4
Loss after 92482500 batches: 0.0269
trigger times: 5
Loss after 92613600 batches: 0.0269
trigger times: 6
Loss after 92744700 batches: 0.0263
trigger times: 0
Loss after 92875800 batches: 0.0266
trigger times: 1
Loss after 93006900 batches: 0.0257
trigger times: 0
Loss after 93138000 batches: 0.0257
trigger times: 1
Loss after 93269100 batches: 0.0255
trigger times: 2
Loss after 93400200 batches: 0.0254
trigger times: 0
Loss after 93531300 batches: 0.0252
trigger times: 1
Loss after 93662400 batches: 0.0253
trigger times: 2
Loss after 93793500 batches: 0.0251
trigger times: 3
Loss after 93924600 batches: 0.0247
trigger times: 4
Loss after 94055700 batches: 0.0246
trigger times: 5
Loss after 94186800 batches: 0.0243
trigger times: 6
Loss after 94317900 batches: 0.0243
trigger times: 7
Loss after 94449000 batches: 0.0240
trigger times: 8
Loss after 94580100 batches: 0.0243
trigger times: 9
Loss after 94711200 batches: 0.0239
trigger times: 10
Loss after 94842300 batches: 0.0238
trigger times: 11
Loss after 94973400 batches: 0.0235
trigger times: 12
Loss after 95104500 batches: 0.0236
trigger times: 13
Loss after 95235600 batches: 0.0233
trigger times: 14
Loss after 95366700 batches: 0.0236
trigger times: 15
Loss after 95497800 batches: 0.0229
trigger times: 0
Loss after 95628900 batches: 0.0230
trigger times: 0
Loss after 95760000 batches: 0.0229
trigger times: 1
Loss after 95891100 batches: 0.0226
trigger times: 0
Loss after 96022200 batches: 0.0227
trigger times: 1
Loss after 96153300 batches: 0.0223
trigger times: 2
Loss after 96284400 batches: 0.0220
trigger times: 3
Loss after 96415500 batches: 0.0222
trigger times: 4
Loss after 96546600 batches: 0.0221
trigger times: 5
Loss after 96677700 batches: 0.0218
trigger times: 6
Loss after 96808800 batches: 0.0217
trigger times: 7
Loss after 96939900 batches: 0.0218
trigger times: 8
Loss after 97071000 batches: 0.0218
trigger times: 9
Loss after 97202100 batches: 0.0216
trigger times: 10
Loss after 97333200 batches: 0.0220
trigger times: 11
Loss after 97464300 batches: 0.0213
trigger times: 12
Loss after 97595400 batches: 0.0214
trigger times: 13
Loss after 97726500 batches: 0.0215
trigger times: 14
Loss after 97857600 batches: 0.0215
trigger times: 15
Loss after 97988700 batches: 0.0212
trigger times: 16
Loss after 98119800 batches: 0.0213
trigger times: 17
Loss after 98250900 batches: 0.0208
trigger times: 18
Loss after 98382000 batches: 0.0210
trigger times: 19
Loss after 98513100 batches: 0.0207
trigger times: 20
Early stopping!
Start to test process.
Loss after 98644200 batches: 0.0208
Time to train on one home:  677.0118825435638
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761]]
Round_3_results:  [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 773 < 774; dropping {'Training_Loss': 0.11838079960841053, 'Validation_Loss': 0.27934570444954765, 'Training_R2': 0.8809105370857795, 'Validation_R2': 0.7405180613558253, 'Training_F1': 0.8112924138308005, 'Validation_F1': 0.7011474200236083, 'Training_NEP': 0.3778135882598094, 'Validation_NEP': 0.5861449916491153, 'Training_NDE': 0.08940257365094824, 'Validation_NDE': 0.20663773757688111, 'Training_MAE': 12.512436758022469, 'Validation_MAE': 16.0745274376271, 'Training_MSE': 393.3573, 'Validation_MSE': 763.1074}.
trigger times: 0
Loss after 98775300 batches: 0.1184
trigger times: 0
Loss after 98906400 batches: 0.0356
trigger times: 0
Loss after 99037500 batches: 0.0249
trigger times: 0
Loss after 99168600 batches: 0.0214
trigger times: 1
Loss after 99299700 batches: 0.0195
trigger times: 0
Loss after 99430800 batches: 0.0183
trigger times: 1
Loss after 99561900 batches: 0.0171
trigger times: 2
Loss after 99693000 batches: 0.0163
trigger times: 3
Loss after 99824100 batches: 0.0159
trigger times: 4
Loss after 99955200 batches: 0.0150
trigger times: 5
Loss after 100086300 batches: 0.0146
trigger times: 6
Loss after 100217400 batches: 0.0143
trigger times: 7
Loss after 100348500 batches: 0.0138
trigger times: 8
Loss after 100479600 batches: 0.0136
trigger times: 9
Loss after 100610700 batches: 0.0134
trigger times: 10
Loss after 100741800 batches: 0.0129
trigger times: 11
Loss after 100872900 batches: 0.0128
trigger times: 0
Loss after 101004000 batches: 0.0126
trigger times: 0
Loss after 101135100 batches: 0.0123
trigger times: 1
Loss after 101266200 batches: 0.0121
trigger times: 2
Loss after 101397300 batches: 0.0123
trigger times: 3
Loss after 101528400 batches: 0.0119
trigger times: 4
Loss after 101659500 batches: 0.0115
trigger times: 0
Loss after 101790600 batches: 0.0120
trigger times: 1
Loss after 101921700 batches: 0.0117
trigger times: 2
Loss after 102052800 batches: 0.0115
trigger times: 3
Loss after 102183900 batches: 0.0112
trigger times: 4
Loss after 102315000 batches: 0.0110
trigger times: 5
Loss after 102446100 batches: 0.0108
trigger times: 6
Loss after 102577200 batches: 0.0108
trigger times: 0
Loss after 102708300 batches: 0.0108
trigger times: 1
Loss after 102839400 batches: 0.0105
trigger times: 2
Loss after 102970500 batches: 0.0107
trigger times: 0
Loss after 103101600 batches: 0.0107
trigger times: 1
Loss after 103232700 batches: 0.0103
trigger times: 2
Loss after 103363800 batches: 0.0105
trigger times: 3
Loss after 103494900 batches: 0.0102
trigger times: 4
Loss after 103626000 batches: 0.0101
trigger times: 5
Loss after 103757100 batches: 0.0102
trigger times: 6
Loss after 103888200 batches: 0.0098
trigger times: 7
Loss after 104019300 batches: 0.0099
trigger times: 8
Loss after 104150400 batches: 0.0100
trigger times: 9
Loss after 104281500 batches: 0.0098
trigger times: 10
Loss after 104412600 batches: 0.0098
trigger times: 11
Loss after 104543700 batches: 0.0098
trigger times: 12
Loss after 104674800 batches: 0.0096
trigger times: 13
Loss after 104805900 batches: 0.0096
trigger times: 14
Loss after 104937000 batches: 0.0095
trigger times: 15
Loss after 105068100 batches: 0.0097
trigger times: 16
Loss after 105199200 batches: 0.0095
trigger times: 17
Loss after 105330300 batches: 0.0093
trigger times: 18
Loss after 105461400 batches: 0.0091
trigger times: 19
Loss after 105592500 batches: 0.0094
trigger times: 20
Early stopping!
Start to test process.
Loss after 105723600 batches: 0.0093
Time to train on one home:  394.1357090473175
trigger times: 0
Loss after 105826200 batches: 0.3663
trigger times: 1
Loss after 105928800 batches: 0.1389
trigger times: 2
Loss after 106031400 batches: 0.0878
trigger times: 3
Loss after 106134000 batches: 0.0720
trigger times: 4
Loss after 106236600 batches: 0.0618
trigger times: 5
Loss after 106339200 batches: 0.0533
trigger times: 6
Loss after 106441800 batches: 0.0503
trigger times: 7
Loss after 106544400 batches: 0.0493
trigger times: 8
Loss after 106647000 batches: 0.0432
trigger times: 9
Loss after 106749600 batches: 0.0422
trigger times: 10
Loss after 106852200 batches: 0.0398
trigger times: 11
Loss after 106954800 batches: 0.0377
trigger times: 12
Loss after 107057400 batches: 0.0421
trigger times: 13
Loss after 107160000 batches: 0.0352
trigger times: 14
Loss after 107262600 batches: 0.0342
trigger times: 15
Loss after 107365200 batches: 0.0338
trigger times: 16
Loss after 107467800 batches: 0.0315
trigger times: 17
Loss after 107570400 batches: 0.0330
trigger times: 18
Loss after 107673000 batches: 0.0326
trigger times: 19
Loss after 107775600 batches: 0.0375
trigger times: 20
Early stopping!
Start to test process.
Loss after 107878200 batches: 0.0319
Time to train on one home:  130.99860858917236
trigger times: 0
Loss after 108009300 batches: 0.2832
trigger times: 1
Loss after 108140400 batches: 0.1011
trigger times: 2
Loss after 108271500 batches: 0.0612
trigger times: 3
Loss after 108402600 batches: 0.0480
trigger times: 4
Loss after 108533700 batches: 0.0406
trigger times: 5
Loss after 108664800 batches: 0.0364
trigger times: 0
Loss after 108795900 batches: 0.0334
trigger times: 1
Loss after 108927000 batches: 0.0310
trigger times: 2
Loss after 109058100 batches: 0.0291
trigger times: 3
Loss after 109189200 batches: 0.0275
trigger times: 4
Loss after 109320300 batches: 0.0263
trigger times: 5
Loss after 109451400 batches: 0.0250
trigger times: 0
Loss after 109582500 batches: 0.0244
trigger times: 1
Loss after 109713600 batches: 0.0234
trigger times: 2
Loss after 109844700 batches: 0.0228
trigger times: 3
Loss after 109975800 batches: 0.0224
trigger times: 4
Loss after 110106900 batches: 0.0219
trigger times: 5
Loss after 110238000 batches: 0.0211
trigger times: 6
Loss after 110369100 batches: 0.0210
trigger times: 7
Loss after 110500200 batches: 0.0203
trigger times: 8
Loss after 110631300 batches: 0.0200
trigger times: 9
Loss after 110762400 batches: 0.0197
trigger times: 10
Loss after 110893500 batches: 0.0196
trigger times: 11
Loss after 111024600 batches: 0.0189
trigger times: 12
Loss after 111155700 batches: 0.0185
trigger times: 13
Loss after 111286800 batches: 0.0185
trigger times: 14
Loss after 111417900 batches: 0.0185
trigger times: 15
Loss after 111549000 batches: 0.0182
trigger times: 16
Loss after 111680100 batches: 0.0179
trigger times: 17
Loss after 111811200 batches: 0.0176
trigger times: 18
Loss after 111942300 batches: 0.0173
trigger times: 19
Loss after 112073400 batches: 0.0171
trigger times: 20
Early stopping!
Start to test process.
Loss after 112204500 batches: 0.0172
Time to train on one home:  245.27983117103577
trigger times: 0
Loss after 112335600 batches: 0.3218
trigger times: 0
Loss after 112466700 batches: 0.0925
trigger times: 0
Loss after 112597800 batches: 0.0641
trigger times: 0
Loss after 112728900 batches: 0.0538
trigger times: 0
Loss after 112860000 batches: 0.0475
trigger times: 0
Loss after 112991100 batches: 0.0431
trigger times: 0
Loss after 113122200 batches: 0.0407
trigger times: 1
Loss after 113253300 batches: 0.0384
trigger times: 0
Loss after 113384400 batches: 0.0368
trigger times: 1
Loss after 113515500 batches: 0.0353
trigger times: 2
Loss after 113646600 batches: 0.0341
trigger times: 3
Loss after 113777700 batches: 0.0321
trigger times: 4
Loss after 113908800 batches: 0.0316
trigger times: 5
Loss after 114039900 batches: 0.0312
trigger times: 6
Loss after 114171000 batches: 0.0303
trigger times: 7
Loss after 114302100 batches: 0.0298
trigger times: 8
Loss after 114433200 batches: 0.0290
trigger times: 0
Loss after 114564300 batches: 0.0283
trigger times: 1
Loss after 114695400 batches: 0.0282
trigger times: 2
Loss after 114826500 batches: 0.0280
trigger times: 3
Loss after 114957600 batches: 0.0270
trigger times: 0
Loss after 115088700 batches: 0.0265
trigger times: 1
Loss after 115219800 batches: 0.0264
trigger times: 2
Loss after 115350900 batches: 0.0263
trigger times: 3
Loss after 115482000 batches: 0.0255
trigger times: 4
Loss after 115613100 batches: 0.0254
trigger times: 5
Loss after 115744200 batches: 0.0255
trigger times: 6
Loss after 115875300 batches: 0.0252
trigger times: 7
Loss after 116006400 batches: 0.0249
trigger times: 8
Loss after 116137500 batches: 0.0245
trigger times: 9
Loss after 116268600 batches: 0.0240
trigger times: 10
Loss after 116399700 batches: 0.0239
trigger times: 11
Loss after 116530800 batches: 0.0238
trigger times: 12
Loss after 116661900 batches: 0.0236
trigger times: 0
Loss after 116793000 batches: 0.0237
trigger times: 1
Loss after 116924100 batches: 0.0232
trigger times: 2
Loss after 117055200 batches: 0.0230
trigger times: 3
Loss after 117186300 batches: 0.0229
trigger times: 4
Loss after 117317400 batches: 0.0228
trigger times: 5
Loss after 117448500 batches: 0.0226
trigger times: 6
Loss after 117579600 batches: 0.0223
trigger times: 7
Loss after 117710700 batches: 0.0223
trigger times: 8
Loss after 117841800 batches: 0.0218
trigger times: 9
Loss after 117972900 batches: 0.0217
trigger times: 10
Loss after 118104000 batches: 0.0217
trigger times: 11
Loss after 118235100 batches: 0.0216
trigger times: 12
Loss after 118366200 batches: 0.0216
trigger times: 0
Loss after 118497300 batches: 0.0215
trigger times: 1
Loss after 118628400 batches: 0.0216
trigger times: 0
Loss after 118759500 batches: 0.0217
trigger times: 0
Loss after 118890600 batches: 0.0214
trigger times: 1
Loss after 119021700 batches: 0.0211
trigger times: 2
Loss after 119152800 batches: 0.0208
trigger times: 3
Loss after 119283900 batches: 0.0206
trigger times: 4
Loss after 119415000 batches: 0.0211
trigger times: 5
Loss after 119546100 batches: 0.0204
trigger times: 6
Loss after 119677200 batches: 0.0205
trigger times: 7
Loss after 119808300 batches: 0.0204
trigger times: 8
Loss after 119939400 batches: 0.0204
trigger times: 9
Loss after 120070500 batches: 0.0200
trigger times: 10
Loss after 120201600 batches: 0.0200
trigger times: 11
Loss after 120332700 batches: 0.0196
trigger times: 12
Loss after 120463800 batches: 0.0198
trigger times: 13
Loss after 120594900 batches: 0.0200
trigger times: 14
Loss after 120726000 batches: 0.0199
trigger times: 15
Loss after 120857100 batches: 0.0194
trigger times: 0
Loss after 120988200 batches: 0.0196
trigger times: 1
Loss after 121119300 batches: 0.0193
trigger times: 2
Loss after 121250400 batches: 0.0193
trigger times: 0
Loss after 121381500 batches: 0.0195
trigger times: 0
Loss after 121512600 batches: 0.0191
trigger times: 0
Loss after 121643700 batches: 0.0190
trigger times: 1
Loss after 121774800 batches: 0.0194
trigger times: 2
Loss after 121905900 batches: 0.0190
trigger times: 3
Loss after 122037000 batches: 0.0188
trigger times: 4
Loss after 122168100 batches: 0.0187
trigger times: 5
Loss after 122299200 batches: 0.0188
trigger times: 6
Loss after 122430300 batches: 0.0187
trigger times: 7
Loss after 122561400 batches: 0.0187
trigger times: 8
Loss after 122692500 batches: 0.0183
trigger times: 9
Loss after 122823600 batches: 0.0184
trigger times: 10
Loss after 122954700 batches: 0.0185
trigger times: 11
Loss after 123085800 batches: 0.0186
trigger times: 12
Loss after 123216900 batches: 0.0181
trigger times: 13
Loss after 123348000 batches: 0.0182
trigger times: 14
Loss after 123479100 batches: 0.0181
trigger times: 15
Loss after 123610200 batches: 0.0181
trigger times: 16
Loss after 123741300 batches: 0.0180
trigger times: 17
Loss after 123872400 batches: 0.0180
trigger times: 18
Loss after 124003500 batches: 0.0179
trigger times: 19
Loss after 124134600 batches: 0.0179
trigger times: 20
Early stopping!
Start to test process.
Loss after 124265700 batches: 0.0180
Time to train on one home:  664.7525577545166
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268]]
Round_4_results:  [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 973 < 974; dropping {'Training_Loss': 0.07930449150362105, 'Validation_Loss': 0.25566550095876056, 'Training_R2': 0.9202100378751654, 'Validation_R2': 0.7622956904639777, 'Training_F1': 0.8450836793819002, 'Validation_F1': 0.7326262558166169, 'Training_NEP': 0.3101323969520489, 'Validation_NEP': 0.5138908360362506, 'Training_NDE': 0.059899740841136, 'Validation_NDE': 0.18929518174347515, 'Training_MAE': 10.270969928185691, 'Validation_MAE': 14.093018726593277, 'Training_MSE': 263.54947, 'Validation_MSE': 699.0619}.
trigger times: 0
Loss after 124396800 batches: 0.0793
trigger times: 0
Loss after 124527900 batches: 0.0262
trigger times: 0
Loss after 124659000 batches: 0.0201
trigger times: 0
Loss after 124790100 batches: 0.0171
trigger times: 1
Loss after 124921200 batches: 0.0160
trigger times: 0
Loss after 125052300 batches: 0.0152
trigger times: 1
Loss after 125183400 batches: 0.0143
trigger times: 2
Loss after 125314500 batches: 0.0135
trigger times: 3
Loss after 125445600 batches: 0.0132
trigger times: 4
Loss after 125576700 batches: 0.0129
trigger times: 5
Loss after 125707800 batches: 0.0125
trigger times: 0
Loss after 125838900 batches: 0.0123
trigger times: 1
Loss after 125970000 batches: 0.0118
trigger times: 2
Loss after 126101100 batches: 0.0116
trigger times: 3
Loss after 126232200 batches: 0.0116
trigger times: 4
Loss after 126363300 batches: 0.0114
trigger times: 5
Loss after 126494400 batches: 0.0110
trigger times: 6
Loss after 126625500 batches: 0.0110
trigger times: 7
Loss after 126756600 batches: 0.0109
trigger times: 0
Loss after 126887700 batches: 0.0110
trigger times: 1
Loss after 127018800 batches: 0.0111
trigger times: 2
Loss after 127149900 batches: 0.0106
trigger times: 3
Loss after 127281000 batches: 0.0105
trigger times: 4
Loss after 127412100 batches: 0.0100
trigger times: 5
Loss after 127543200 batches: 0.0101
trigger times: 6
Loss after 127674300 batches: 0.0100
trigger times: 7
Loss after 127805400 batches: 0.0099
trigger times: 8
Loss after 127936500 batches: 0.0098
trigger times: 9
Loss after 128067600 batches: 0.0098
trigger times: 10
Loss after 128198700 batches: 0.0096
trigger times: 11
Loss after 128329800 batches: 0.0096
trigger times: 12
Loss after 128460900 batches: 0.0095
trigger times: 13
Loss after 128592000 batches: 0.0094
trigger times: 14
Loss after 128723100 batches: 0.0091
trigger times: 15
Loss after 128854200 batches: 0.0091
trigger times: 16
Loss after 128985300 batches: 0.0092
trigger times: 17
Loss after 129116400 batches: 0.0093
trigger times: 18
Loss after 129247500 batches: 0.0093
trigger times: 19
Loss after 129378600 batches: 0.0091
trigger times: 20
Early stopping!
Start to test process.
Loss after 129509700 batches: 0.0089
Time to train on one home:  295.95618414878845
trigger times: 0
Loss after 129612300 batches: 0.3087
trigger times: 1
Loss after 129714900 batches: 0.1006
trigger times: 0
Loss after 129817500 batches: 0.0703
trigger times: 0
Loss after 129920100 batches: 0.0576
trigger times: 0
Loss after 130022700 batches: 0.0552
trigger times: 0
Loss after 130125300 batches: 0.0487
trigger times: 1
Loss after 130227900 batches: 0.0433
trigger times: 0
Loss after 130330500 batches: 0.0423
trigger times: 1
Loss after 130433100 batches: 0.0430
trigger times: 2
Loss after 130535700 batches: 0.0382
trigger times: 0
Loss after 130638300 batches: 0.0457
trigger times: 1
Loss after 130740900 batches: 0.0349
trigger times: 2
Loss after 130843500 batches: 0.0330
trigger times: 3
Loss after 130946100 batches: 0.0332
trigger times: 4
Loss after 131048700 batches: 0.0304
trigger times: 5
Loss after 131151300 batches: 0.0302
trigger times: 6
Loss after 131253900 batches: 0.0317
trigger times: 7
Loss after 131356500 batches: 0.0301
trigger times: 8
Loss after 131459100 batches: 0.0314
trigger times: 9
Loss after 131561700 batches: 0.0299
trigger times: 10
Loss after 131664300 batches: 0.0319
trigger times: 11
Loss after 131766900 batches: 0.0296
trigger times: 12
Loss after 131869500 batches: 0.0262
trigger times: 13
Loss after 131972100 batches: 0.0255
trigger times: 14
Loss after 132074700 batches: 0.0253
trigger times: 15
Loss after 132177300 batches: 0.0272
trigger times: 16
Loss after 132279900 batches: 0.0262
trigger times: 17
Loss after 132382500 batches: 0.0256
trigger times: 18
Loss after 132485100 batches: 0.0244
trigger times: 19
Loss after 132587700 batches: 0.0250
trigger times: 20
Early stopping!
Start to test process.
Loss after 132690300 batches: 0.0268
Time to train on one home:  188.28047370910645
trigger times: 0
Loss after 132821400 batches: 0.1092
trigger times: 0
Loss after 132952500 batches: 0.0370
trigger times: 1
Loss after 133083600 batches: 0.0288
trigger times: 2
Loss after 133214700 batches: 0.0253
trigger times: 3
Loss after 133345800 batches: 0.0237
trigger times: 4
Loss after 133476900 batches: 0.0218
trigger times: 5
Loss after 133608000 batches: 0.0209
trigger times: 6
Loss after 133739100 batches: 0.0198
trigger times: 7
Loss after 133870200 batches: 0.0194
trigger times: 8
Loss after 134001300 batches: 0.0189
trigger times: 9
Loss after 134132400 batches: 0.0184
trigger times: 10
Loss after 134263500 batches: 0.0182
trigger times: 11
Loss after 134394600 batches: 0.0178
trigger times: 12
Loss after 134525700 batches: 0.0171
trigger times: 13
Loss after 134656800 batches: 0.0170
trigger times: 14
Loss after 134787900 batches: 0.0168
trigger times: 15
Loss after 134919000 batches: 0.0167
trigger times: 16
Loss after 135050100 batches: 0.0164
trigger times: 17
Loss after 135181200 batches: 0.0160
trigger times: 18
Loss after 135312300 batches: 0.0158
trigger times: 19
Loss after 135443400 batches: 0.0160
trigger times: 20
Early stopping!
Start to test process.
Loss after 135574500 batches: 0.0158
Time to train on one home:  167.73111772537231
trigger times: 0
Loss after 135705600 batches: 0.1796
trigger times: 1
Loss after 135836700 batches: 0.0589
trigger times: 0
Loss after 135967800 batches: 0.0431
trigger times: 1
Loss after 136098900 batches: 0.0368
trigger times: 0
Loss after 136230000 batches: 0.0336
trigger times: 1
Loss after 136361100 batches: 0.0318
trigger times: 0
Loss after 136492200 batches: 0.0301
trigger times: 1
Loss after 136623300 batches: 0.0288
trigger times: 2
Loss after 136754400 batches: 0.0279
trigger times: 0
Loss after 136885500 batches: 0.0269
trigger times: 0
Loss after 137016600 batches: 0.0265
trigger times: 1
Loss after 137147700 batches: 0.0258
trigger times: 2
Loss after 137278800 batches: 0.0251
trigger times: 3
Loss after 137409900 batches: 0.0245
trigger times: 4
Loss after 137541000 batches: 0.0241
trigger times: 5
Loss after 137672100 batches: 0.0240
trigger times: 6
Loss after 137803200 batches: 0.0234
trigger times: 7
Loss after 137934300 batches: 0.0230
trigger times: 8
Loss after 138065400 batches: 0.0229
trigger times: 9
Loss after 138196500 batches: 0.0227
trigger times: 10
Loss after 138327600 batches: 0.0225
trigger times: 11
Loss after 138458700 batches: 0.0224
trigger times: 0
Loss after 138589800 batches: 0.0218
trigger times: 1
Loss after 138720900 batches: 0.0219
trigger times: 2
Loss after 138852000 batches: 0.0216
trigger times: 3
Loss after 138983100 batches: 0.0215
trigger times: 4
Loss after 139114200 batches: 0.0210
trigger times: 5
Loss after 139245300 batches: 0.0207
trigger times: 6
Loss after 139376400 batches: 0.0208
trigger times: 0
Loss after 139507500 batches: 0.0208
trigger times: 0
Loss after 139638600 batches: 0.0207
trigger times: 1
Loss after 139769700 batches: 0.0206
trigger times: 2
Loss after 139900800 batches: 0.0205
trigger times: 3
Loss after 140031900 batches: 0.0204
trigger times: 4
Loss after 140163000 batches: 0.0204
trigger times: 5
Loss after 140294100 batches: 0.0202
trigger times: 0
Loss after 140425200 batches: 0.0200
trigger times: 1
Loss after 140556300 batches: 0.0198
trigger times: 2
Loss after 140687400 batches: 0.0197
trigger times: 3
Loss after 140818500 batches: 0.0196
trigger times: 4
Loss after 140949600 batches: 0.0195
trigger times: 5
Loss after 141080700 batches: 0.0192
trigger times: 0
Loss after 141211800 batches: 0.0192
trigger times: 0
Loss after 141342900 batches: 0.0189
trigger times: 1
Loss after 141474000 batches: 0.0189
trigger times: 2
Loss after 141605100 batches: 0.0190
trigger times: 3
Loss after 141736200 batches: 0.0187
trigger times: 4
Loss after 141867300 batches: 0.0185
trigger times: 0
Loss after 141998400 batches: 0.0185
trigger times: 1
Loss after 142129500 batches: 0.0186
trigger times: 0
Loss after 142260600 batches: 0.0185
trigger times: 1
Loss after 142391700 batches: 0.0188
trigger times: 2
Loss after 142522800 batches: 0.0184
trigger times: 3
Loss after 142653900 batches: 0.0183
trigger times: 0
Loss after 142785000 batches: 0.0182
trigger times: 0
Loss after 142916100 batches: 0.0179
trigger times: 1
Loss after 143047200 batches: 0.0181
trigger times: 2
Loss after 143178300 batches: 0.0181
trigger times: 3
Loss after 143309400 batches: 0.0181
trigger times: 4
Loss after 143440500 batches: 0.0177
trigger times: 5
Loss after 143571600 batches: 0.0175
trigger times: 6
Loss after 143702700 batches: 0.0176
trigger times: 7
Loss after 143833800 batches: 0.0175
trigger times: 8
Loss after 143964900 batches: 0.0175
trigger times: 9
Loss after 144096000 batches: 0.0178
trigger times: 10
Loss after 144227100 batches: 0.0172
trigger times: 11
Loss after 144358200 batches: 0.0172
trigger times: 12
Loss after 144489300 batches: 0.0171
trigger times: 13
Loss after 144620400 batches: 0.0171
trigger times: 14
Loss after 144751500 batches: 0.0170
trigger times: 15
Loss after 144882600 batches: 0.0172
trigger times: 16
Loss after 145013700 batches: 0.0171
trigger times: 17
Loss after 145144800 batches: 0.0167
trigger times: 18
Loss after 145275900 batches: 0.0169
trigger times: 19
Loss after 145407000 batches: 0.0170
trigger times: 20
Early stopping!
Start to test process.
Loss after 145538100 batches: 0.0165
Time to train on one home:  550.0742175579071
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466]]
Round_5_results:  [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466]
trigger times: 0
Loss after 145669200 batches: 0.0696
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1142 < 1143; dropping {'Training_Loss': 0.06960579137897717, 'Validation_Loss': 0.24788773473766115, 'Training_R2': 0.9299955312033298, 'Validation_R2': 0.7694385923733842, 'Training_F1': 0.8547897636148107, 'Validation_F1': 0.7458252605518508, 'Training_NEP': 0.29062155226498343, 'Validation_NEP': 0.49534666964849394, 'Training_NDE': 0.05255359730690713, 'Validation_NDE': 0.18360695119453757, 'Training_MAE': 9.624809446327564, 'Validation_MAE': 13.584460749207432, 'Training_MSE': 231.2276, 'Validation_MSE': 678.0554}.
trigger times: 0
Loss after 145800300 batches: 0.0240
trigger times: 1
Loss after 145931400 batches: 0.0188
trigger times: 0
Loss after 146062500 batches: 0.0159
trigger times: 1
Loss after 146193600 batches: 0.0144
trigger times: 0
Loss after 146324700 batches: 0.0138
trigger times: 0
Loss after 146455800 batches: 0.0128
trigger times: 1
Loss after 146586900 batches: 0.0124
trigger times: 0
Loss after 146718000 batches: 0.0124
trigger times: 1
Loss after 146849100 batches: 0.0123
trigger times: 2
Loss after 146980200 batches: 0.0118
trigger times: 0
Loss after 147111300 batches: 0.0116
trigger times: 0
Loss after 147242400 batches: 0.0113
trigger times: 1
Loss after 147373500 batches: 0.0108
trigger times: 2
Loss after 147504600 batches: 0.0107
trigger times: 3
Loss after 147635700 batches: 0.0105
trigger times: 4
Loss after 147766800 batches: 0.0106
trigger times: 5
Loss after 147897900 batches: 0.0106
trigger times: 6
Loss after 148029000 batches: 0.0104
trigger times: 7
Loss after 148160100 batches: 0.0100
trigger times: 8
Loss after 148291200 batches: 0.0101
trigger times: 9
Loss after 148422300 batches: 0.0100
trigger times: 10
Loss after 148553400 batches: 0.0098
trigger times: 11
Loss after 148684500 batches: 0.0095
trigger times: 12
Loss after 148815600 batches: 0.0095
trigger times: 13
Loss after 148946700 batches: 0.0097
trigger times: 14
Loss after 149077800 batches: 0.0096
trigger times: 15
Loss after 149208900 batches: 0.0094
trigger times: 16
Loss after 149340000 batches: 0.0092
trigger times: 17
Loss after 149471100 batches: 0.0093
trigger times: 18
Loss after 149602200 batches: 0.0091
trigger times: 19
Loss after 149733300 batches: 0.0093
trigger times: 0
Loss after 149864400 batches: 0.0090
trigger times: 1
Loss after 149995500 batches: 0.0090
trigger times: 2
Loss after 150126600 batches: 0.0090
trigger times: 3
Loss after 150257700 batches: 0.0088
trigger times: 4
Loss after 150388800 batches: 0.0088
trigger times: 5
Loss after 150519900 batches: 0.0087
trigger times: 6
Loss after 150651000 batches: 0.0087
trigger times: 7
Loss after 150782100 batches: 0.0086
trigger times: 8
Loss after 150913200 batches: 0.0087
trigger times: 9
Loss after 151044300 batches: 0.0086
trigger times: 10
Loss after 151175400 batches: 0.0083
trigger times: 11
Loss after 151306500 batches: 0.0086
trigger times: 12
Loss after 151437600 batches: 0.0086
trigger times: 13
Loss after 151568700 batches: 0.0084
trigger times: 14
Loss after 151699800 batches: 0.0082
trigger times: 15
Loss after 151830900 batches: 0.0085
trigger times: 16
Loss after 151962000 batches: 0.0082
trigger times: 17
Loss after 152093100 batches: 0.0084
trigger times: 18
Loss after 152224200 batches: 0.0080
trigger times: 19
Loss after 152355300 batches: 0.0081
trigger times: 20
Early stopping!
Start to test process.
Loss after 152486400 batches: 0.0082
Time to train on one home:  387.42813301086426
trigger times: 0
Loss after 152589000 batches: 0.2315
trigger times: 0
Loss after 152691600 batches: 0.0793
trigger times: 0
Loss after 152794200 batches: 0.0571
trigger times: 1
Loss after 152896800 batches: 0.0580
trigger times: 2
Loss after 152999400 batches: 0.0438
trigger times: 3
Loss after 153102000 batches: 0.0406
trigger times: 4
Loss after 153204600 batches: 0.0388
trigger times: 5
Loss after 153307200 batches: 0.0364
trigger times: 6
Loss after 153409800 batches: 0.0349
trigger times: 7
Loss after 153512400 batches: 0.0323
trigger times: 8
Loss after 153615000 batches: 0.0317
trigger times: 9
Loss after 153717600 batches: 0.0308
trigger times: 10
Loss after 153820200 batches: 0.0292
trigger times: 11
Loss after 153922800 batches: 0.0284
trigger times: 12
Loss after 154025400 batches: 0.0287
trigger times: 13
Loss after 154128000 batches: 0.0270
trigger times: 14
Loss after 154230600 batches: 0.0266
trigger times: 15
Loss after 154333200 batches: 0.0266
trigger times: 16
Loss after 154435800 batches: 0.0259
trigger times: 17
Loss after 154538400 batches: 0.0250
trigger times: 18
Loss after 154641000 batches: 0.0257
trigger times: 19
Loss after 154743600 batches: 0.0247
trigger times: 20
Early stopping!
Start to test process.
Loss after 154846200 batches: 0.0259
Time to train on one home:  142.44699692726135
trigger times: 0
Loss after 154977300 batches: 0.0956
trigger times: 1
Loss after 155108400 batches: 0.0343
trigger times: 0
Loss after 155239500 batches: 0.0267
trigger times: 1
Loss after 155370600 batches: 0.0237
trigger times: 2
Loss after 155501700 batches: 0.0224
trigger times: 3
Loss after 155632800 batches: 0.0213
trigger times: 4
Loss after 155763900 batches: 0.0204
trigger times: 5
Loss after 155895000 batches: 0.0196
trigger times: 6
Loss after 156026100 batches: 0.0190
trigger times: 7
Loss after 156157200 batches: 0.0185
trigger times: 8
Loss after 156288300 batches: 0.0179
trigger times: 9
Loss after 156419400 batches: 0.0175
trigger times: 10
Loss after 156550500 batches: 0.0173
trigger times: 11
Loss after 156681600 batches: 0.0168
trigger times: 12
Loss after 156812700 batches: 0.0168
trigger times: 13
Loss after 156943800 batches: 0.0163
trigger times: 0
Loss after 157074900 batches: 0.0163
trigger times: 0
Loss after 157206000 batches: 0.0159
trigger times: 1
Loss after 157337100 batches: 0.0159
trigger times: 2
Loss after 157468200 batches: 0.0157
trigger times: 3
Loss after 157599300 batches: 0.0157
trigger times: 4
Loss after 157730400 batches: 0.0154
trigger times: 5
Loss after 157861500 batches: 0.0151
trigger times: 6
Loss after 157992600 batches: 0.0149
trigger times: 7
Loss after 158123700 batches: 0.0150
trigger times: 8
Loss after 158254800 batches: 0.0149
trigger times: 9
Loss after 158385900 batches: 0.0147
trigger times: 10
Loss after 158517000 batches: 0.0146
trigger times: 11
Loss after 158648100 batches: 0.0144
trigger times: 12
Loss after 158779200 batches: 0.0143
trigger times: 13
Loss after 158910300 batches: 0.0143
trigger times: 14
Loss after 159041400 batches: 0.0141
trigger times: 15
Loss after 159172500 batches: 0.0140
trigger times: 16
Loss after 159303600 batches: 0.0142
trigger times: 17
Loss after 159434700 batches: 0.0140
trigger times: 18
Loss after 159565800 batches: 0.0139
trigger times: 19
Loss after 159696900 batches: 0.0137
trigger times: 20
Early stopping!
Start to test process.
Loss after 159828000 batches: 0.0136
Time to train on one home:  281.44396781921387
trigger times: 0
Loss after 159959100 batches: 0.1308
trigger times: 1
Loss after 160090200 batches: 0.0459
trigger times: 0
Loss after 160221300 batches: 0.0358
trigger times: 0
Loss after 160352400 batches: 0.0310
trigger times: 1
Loss after 160483500 batches: 0.0283
trigger times: 2
Loss after 160614600 batches: 0.0268
trigger times: 3
Loss after 160745700 batches: 0.0256
trigger times: 4
Loss after 160876800 batches: 0.0252
trigger times: 0
Loss after 161007900 batches: 0.0243
trigger times: 1
Loss after 161139000 batches: 0.0237
trigger times: 2
Loss after 161270100 batches: 0.0233
trigger times: 0
Loss after 161401200 batches: 0.0227
trigger times: 1
Loss after 161532300 batches: 0.0223
trigger times: 2
Loss after 161663400 batches: 0.0218
trigger times: 3
Loss after 161794500 batches: 0.0213
trigger times: 4
Loss after 161925600 batches: 0.0216
trigger times: 5
Loss after 162056700 batches: 0.0213
trigger times: 6
Loss after 162187800 batches: 0.0212
trigger times: 7
Loss after 162318900 batches: 0.0211
trigger times: 8
Loss after 162450000 batches: 0.0201
trigger times: 9
Loss after 162581100 batches: 0.0205
trigger times: 10
Loss after 162712200 batches: 0.0202
trigger times: 11
Loss after 162843300 batches: 0.0201
trigger times: 12
Loss after 162974400 batches: 0.0197
trigger times: 13
Loss after 163105500 batches: 0.0195
trigger times: 14
Loss after 163236600 batches: 0.0198
trigger times: 15
Loss after 163367700 batches: 0.0196
trigger times: 16
Loss after 163498800 batches: 0.0195
trigger times: 0
Loss after 163629900 batches: 0.0193
trigger times: 1
Loss after 163761000 batches: 0.0187
trigger times: 2
Loss after 163892100 batches: 0.0189
trigger times: 3
Loss after 164023200 batches: 0.0191
trigger times: 4
Loss after 164154300 batches: 0.0189
trigger times: 5
Loss after 164285400 batches: 0.0186
trigger times: 6
Loss after 164416500 batches: 0.0187
trigger times: 7
Loss after 164547600 batches: 0.0182
trigger times: 8
Loss after 164678700 batches: 0.0184
trigger times: 9
Loss after 164809800 batches: 0.0184
trigger times: 10
Loss after 164940900 batches: 0.0182
trigger times: 11
Loss after 165072000 batches: 0.0180
trigger times: 12
Loss after 165203100 batches: 0.0178
trigger times: 13
Loss after 165334200 batches: 0.0177
trigger times: 14
Loss after 165465300 batches: 0.0177
trigger times: 15
Loss after 165596400 batches: 0.0177
trigger times: 16
Loss after 165727500 batches: 0.0176
trigger times: 17
Loss after 165858600 batches: 0.0176
trigger times: 18
Loss after 165989700 batches: 0.0178
trigger times: 19
Loss after 166120800 batches: 0.0178
trigger times: 20
Early stopping!
Start to test process.
Loss after 166251900 batches: 0.0177
Time to train on one home:  359.29379749298096
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473]]
Round_6_results:  [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1305 < 1306; dropping {'Training_Loss': 0.0923343134938546, 'Validation_Loss': 0.24880631102455986, 'Training_R2': 0.9070500546773631, 'Validation_R2': 0.7686927119641762, 'Training_F1': 0.828879672931303, 'Validation_F1': 0.7167908256231827, 'Training_NEP': 0.3426466957280049, 'Validation_NEP': 0.5247082111820158, 'Training_NDE': 0.06977917381778992, 'Validation_NDE': 0.18420093103400922, 'Training_MAE': 11.347779020837569, 'Validation_MAE': 14.389676031630525, 'Training_MSE': 307.01746, 'Validation_MSE': 680.2489}.
trigger times: 0
Loss after 166383000 batches: 0.0923
trigger times: 0
Loss after 166514100 batches: 0.0258
trigger times: 0
Loss after 166645200 batches: 0.0189
trigger times: 0
Loss after 166776300 batches: 0.0168
trigger times: 1
Loss after 166907400 batches: 0.0151
trigger times: 2
Loss after 167038500 batches: 0.0142
trigger times: 0
Loss after 167169600 batches: 0.0133
trigger times: 1
Loss after 167300700 batches: 0.0130
trigger times: 0
Loss after 167431800 batches: 0.0121
trigger times: 1
Loss after 167562900 batches: 0.0120
trigger times: 0
Loss after 167694000 batches: 0.0118
trigger times: 1
Loss after 167825100 batches: 0.0114
trigger times: 0
Loss after 167956200 batches: 0.0113
trigger times: 1
Loss after 168087300 batches: 0.0109
trigger times: 2
Loss after 168218400 batches: 0.0110
trigger times: 3
Loss after 168349500 batches: 0.0109
trigger times: 4
Loss after 168480600 batches: 0.0105
trigger times: 5
Loss after 168611700 batches: 0.0104
trigger times: 6
Loss after 168742800 batches: 0.0102
trigger times: 7
Loss after 168873900 batches: 0.0102
trigger times: 8
Loss after 169005000 batches: 0.0100
trigger times: 9
Loss after 169136100 batches: 0.0098
trigger times: 10
Loss after 169267200 batches: 0.0098
trigger times: 11
Loss after 169398300 batches: 0.0098
trigger times: 12
Loss after 169529400 batches: 0.0094
trigger times: 13
Loss after 169660500 batches: 0.0092
trigger times: 14
Loss after 169791600 batches: 0.0093
trigger times: 15
Loss after 169922700 batches: 0.0092
trigger times: 16
Loss after 170053800 batches: 0.0092
trigger times: 17
Loss after 170184900 batches: 0.0094
trigger times: 18
Loss after 170316000 batches: 0.0093
trigger times: 19
Loss after 170447100 batches: 0.0089
trigger times: 20
Early stopping!
Start to test process.
Loss after 170578200 batches: 0.0088
Time to train on one home:  245.32913756370544
trigger times: 0
Loss after 170680800 batches: 0.2256
trigger times: 0
Loss after 170783400 batches: 0.0673
trigger times: 0
Loss after 170886000 batches: 0.0528
trigger times: 0
Loss after 170988600 batches: 0.0453
trigger times: 1
Loss after 171091200 batches: 0.0401
trigger times: 2
Loss after 171193800 batches: 0.0366
trigger times: 3
Loss after 171296400 batches: 0.0349
trigger times: 4
Loss after 171399000 batches: 0.0314
trigger times: 5
Loss after 171501600 batches: 0.0316
trigger times: 6
Loss after 171604200 batches: 0.0320
trigger times: 0
Loss after 171706800 batches: 0.0310
trigger times: 1
Loss after 171809400 batches: 0.0294
trigger times: 2
Loss after 171912000 batches: 0.0285
trigger times: 0
Loss after 172014600 batches: 0.0309
trigger times: 1
Loss after 172117200 batches: 0.0275
trigger times: 2
Loss after 172219800 batches: 0.0258
trigger times: 3
Loss after 172322400 batches: 0.0254
trigger times: 4
Loss after 172425000 batches: 0.0247
trigger times: 5
Loss after 172527600 batches: 0.0245
trigger times: 6
Loss after 172630200 batches: 0.0254
trigger times: 7
Loss after 172732800 batches: 0.0255
trigger times: 8
Loss after 172835400 batches: 0.0251
trigger times: 9
Loss after 172938000 batches: 0.0258
trigger times: 10
Loss after 173040600 batches: 0.0266
trigger times: 11
Loss after 173143200 batches: 0.0248
trigger times: 12
Loss after 173245800 batches: 0.0225
trigger times: 13
Loss after 173348400 batches: 0.0228
trigger times: 14
Loss after 173451000 batches: 0.0232
trigger times: 15
Loss after 173553600 batches: 0.0222
trigger times: 16
Loss after 173656200 batches: 0.0211
trigger times: 17
Loss after 173758800 batches: 0.0217
trigger times: 18
Loss after 173861400 batches: 0.0213
trigger times: 19
Loss after 173964000 batches: 0.0235
trigger times: 20
Early stopping!
Start to test process.
Loss after 174066600 batches: 0.0207
Time to train on one home:  205.21841478347778
trigger times: 0
Loss after 174197700 batches: 0.0978
trigger times: 1
Loss after 174328800 batches: 0.0332
trigger times: 0
Loss after 174459900 batches: 0.0256
trigger times: 1
Loss after 174591000 batches: 0.0228
trigger times: 2
Loss after 174722100 batches: 0.0215
trigger times: 0
Loss after 174853200 batches: 0.0202
trigger times: 1
Loss after 174984300 batches: 0.0190
trigger times: 2
Loss after 175115400 batches: 0.0185
trigger times: 3
Loss after 175246500 batches: 0.0178
trigger times: 4
Loss after 175377600 batches: 0.0173
trigger times: 5
Loss after 175508700 batches: 0.0171
trigger times: 6
Loss after 175639800 batches: 0.0169
trigger times: 7
Loss after 175770900 batches: 0.0162
trigger times: 0
Loss after 175902000 batches: 0.0161
trigger times: 1
Loss after 176033100 batches: 0.0161
trigger times: 2
Loss after 176164200 batches: 0.0157
trigger times: 3
Loss after 176295300 batches: 0.0156
trigger times: 4
Loss after 176426400 batches: 0.0155
trigger times: 5
Loss after 176557500 batches: 0.0150
trigger times: 6
Loss after 176688600 batches: 0.0150
trigger times: 7
Loss after 176819700 batches: 0.0148
trigger times: 8
Loss after 176950800 batches: 0.0146
trigger times: 9
Loss after 177081900 batches: 0.0145
trigger times: 10
Loss after 177213000 batches: 0.0143
trigger times: 11
Loss after 177344100 batches: 0.0143
trigger times: 0
Loss after 177475200 batches: 0.0141
trigger times: 1
Loss after 177606300 batches: 0.0139
trigger times: 2
Loss after 177737400 batches: 0.0138
trigger times: 3
Loss after 177868500 batches: 0.0140
trigger times: 4
Loss after 177999600 batches: 0.0141
trigger times: 5
Loss after 178130700 batches: 0.0140
trigger times: 6
Loss after 178261800 batches: 0.0136
trigger times: 7
Loss after 178392900 batches: 0.0135
trigger times: 8
Loss after 178524000 batches: 0.0134
trigger times: 9
Loss after 178655100 batches: 0.0132
trigger times: 10
Loss after 178786200 batches: 0.0135
trigger times: 11
Loss after 178917300 batches: 0.0134
trigger times: 12
Loss after 179048400 batches: 0.0133
trigger times: 13
Loss after 179179500 batches: 0.0130
trigger times: 14
Loss after 179310600 batches: 0.0131
trigger times: 0
Loss after 179441700 batches: 0.0129
trigger times: 1
Loss after 179572800 batches: 0.0130
trigger times: 2
Loss after 179703900 batches: 0.0129
trigger times: 3
Loss after 179835000 batches: 0.0128
trigger times: 4
Loss after 179966100 batches: 0.0127
trigger times: 5
Loss after 180097200 batches: 0.0131
trigger times: 6
Loss after 180228300 batches: 0.0127
trigger times: 7
Loss after 180359400 batches: 0.0126
trigger times: 8
Loss after 180490500 batches: 0.0126
trigger times: 9
Loss after 180621600 batches: 0.0124
trigger times: 10
Loss after 180752700 batches: 0.0123
trigger times: 11
Loss after 180883800 batches: 0.0122
trigger times: 12
Loss after 181014900 batches: 0.0122
trigger times: 13
Loss after 181146000 batches: 0.0121
trigger times: 14
Loss after 181277100 batches: 0.0121
trigger times: 0
Loss after 181408200 batches: 0.0120
trigger times: 1
Loss after 181539300 batches: 0.0119
trigger times: 2
Loss after 181670400 batches: 0.0118
trigger times: 3
Loss after 181801500 batches: 0.0120
trigger times: 4
Loss after 181932600 batches: 0.0119
trigger times: 5
Loss after 182063700 batches: 0.0117
trigger times: 6
Loss after 182194800 batches: 0.0117
trigger times: 7
Loss after 182325900 batches: 0.0119
trigger times: 8
Loss after 182457000 batches: 0.0116
trigger times: 9
Loss after 182588100 batches: 0.0118
trigger times: 10
Loss after 182719200 batches: 0.0116
trigger times: 11
Loss after 182850300 batches: 0.0116
trigger times: 12
Loss after 182981400 batches: 0.0114
trigger times: 13
Loss after 183112500 batches: 0.0112
trigger times: 14
Loss after 183243600 batches: 0.0114
trigger times: 15
Loss after 183374700 batches: 0.0115
trigger times: 16
Loss after 183505800 batches: 0.0114
trigger times: 17
Loss after 183636900 batches: 0.0112
trigger times: 18
Loss after 183768000 batches: 0.0113
trigger times: 19
Loss after 183899100 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 184030200 batches: 0.0111
Time to train on one home:  549.6637048721313
trigger times: 0
Loss after 184161300 batches: 0.1174
trigger times: 0
Loss after 184292400 batches: 0.0391
trigger times: 0
Loss after 184423500 batches: 0.0312
trigger times: 1
Loss after 184554600 batches: 0.0283
trigger times: 2
Loss after 184685700 batches: 0.0261
trigger times: 3
Loss after 184816800 batches: 0.0249
trigger times: 4
Loss after 184947900 batches: 0.0242
trigger times: 0
Loss after 185079000 batches: 0.0237
trigger times: 0
Loss after 185210100 batches: 0.0224
trigger times: 1
Loss after 185341200 batches: 0.0222
trigger times: 2
Loss after 185472300 batches: 0.0220
trigger times: 3
Loss after 185603400 batches: 0.0217
trigger times: 0
Loss after 185734500 batches: 0.0214
trigger times: 1
Loss after 185865600 batches: 0.0209
trigger times: 2
Loss after 185996700 batches: 0.0204
trigger times: 3
Loss after 186127800 batches: 0.0203
trigger times: 4
Loss after 186258900 batches: 0.0200
trigger times: 0
Loss after 186390000 batches: 0.0201
trigger times: 1
Loss after 186521100 batches: 0.0198
trigger times: 0
Loss after 186652200 batches: 0.0197
trigger times: 1
Loss after 186783300 batches: 0.0191
trigger times: 2
Loss after 186914400 batches: 0.0190
trigger times: 0
Loss after 187045500 batches: 0.0189
trigger times: 1
Loss after 187176600 batches: 0.0191
trigger times: 0
Loss after 187307700 batches: 0.0190
trigger times: 0
Loss after 187438800 batches: 0.0189
trigger times: 1
Loss after 187569900 batches: 0.0186
trigger times: 2
Loss after 187701000 batches: 0.0183
trigger times: 3
Loss after 187832100 batches: 0.0183
trigger times: 4
Loss after 187963200 batches: 0.0187
trigger times: 5
Loss after 188094300 batches: 0.0183
trigger times: 6
Loss after 188225400 batches: 0.0181
trigger times: 7
Loss after 188356500 batches: 0.0183
trigger times: 8
Loss after 188487600 batches: 0.0178
trigger times: 9
Loss after 188618700 batches: 0.0176
trigger times: 10
Loss after 188749800 batches: 0.0179
trigger times: 11
Loss after 188880900 batches: 0.0177
trigger times: 12
Loss after 189012000 batches: 0.0177
trigger times: 13
Loss after 189143100 batches: 0.0177
trigger times: 14
Loss after 189274200 batches: 0.0174
trigger times: 0
Loss after 189405300 batches: 0.0171
trigger times: 1
Loss after 189536400 batches: 0.0171
trigger times: 2
Loss after 189667500 batches: 0.0174
trigger times: 3
Loss after 189798600 batches: 0.0171
trigger times: 4
Loss after 189929700 batches: 0.0171
trigger times: 5
Loss after 190060800 batches: 0.0170
trigger times: 6
Loss after 190191900 batches: 0.0166
trigger times: 7
Loss after 190323000 batches: 0.0167
trigger times: 8
Loss after 190454100 batches: 0.0170
trigger times: 9
Loss after 190585200 batches: 0.0167
trigger times: 0
Loss after 190716300 batches: 0.0169
trigger times: 0
Loss after 190847400 batches: 0.0167
trigger times: 0
Loss after 190978500 batches: 0.0166
trigger times: 1
Loss after 191109600 batches: 0.0165
trigger times: 2
Loss after 191240700 batches: 0.0165
trigger times: 3
Loss after 191371800 batches: 0.0161
trigger times: 4
Loss after 191502900 batches: 0.0163
trigger times: 5
Loss after 191634000 batches: 0.0163
trigger times: 6
Loss after 191765100 batches: 0.0161
trigger times: 0
Loss after 191896200 batches: 0.0163
trigger times: 1
Loss after 192027300 batches: 0.0162
trigger times: 2
Loss after 192158400 batches: 0.0161
trigger times: 3
Loss after 192289500 batches: 0.0161
trigger times: 4
Loss after 192420600 batches: 0.0160
trigger times: 5
Loss after 192551700 batches: 0.0159
trigger times: 6
Loss after 192682800 batches: 0.0159
trigger times: 7
Loss after 192813900 batches: 0.0159
trigger times: 8
Loss after 192945000 batches: 0.0155
trigger times: 9
Loss after 193076100 batches: 0.0158
trigger times: 10
Loss after 193207200 batches: 0.0156
trigger times: 11
Loss after 193338300 batches: 0.0155
trigger times: 12
Loss after 193469400 batches: 0.0158
trigger times: 13
Loss after 193600500 batches: 0.0156
trigger times: 14
Loss after 193731600 batches: 0.0155
trigger times: 15
Loss after 193862700 batches: 0.0153
trigger times: 16
Loss after 193993800 batches: 0.0153
trigger times: 17
Loss after 194124900 batches: 0.0154
trigger times: 18
Loss after 194256000 batches: 0.0156
trigger times: 19
Loss after 194387100 batches: 0.0154
trigger times: 20
Early stopping!
Start to test process.
Loss after 194518200 batches: 0.0152
Time to train on one home:  577.2936272621155
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175]]
Round_7_results:  [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175]
trigger times: 0
Loss after 194649300 batches: 0.0778
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1528 < 1529; dropping {'Training_Loss': 0.07776349821602399, 'Validation_Loss': 0.2420165157980389, 'Training_R2': 0.9216502250621943, 'Validation_R2': 0.7749754430210577, 'Training_F1': 0.8451426878401689, 'Validation_F1': 0.7373713397837097, 'Training_NEP': 0.3101504598915963, 'Validation_NEP': 0.5052891848242286, 'Training_NDE': 0.05881856675647133, 'Validation_NDE': 0.17919769520888193, 'Training_MAE': 10.271568136920827, 'Validation_MAE': 13.857125764294764, 'Training_MSE': 258.79245, 'Validation_MSE': 661.77216}.
trigger times: 0
Loss after 194780400 batches: 0.0211
trigger times: 0
Loss after 194911500 batches: 0.0163
trigger times: 1
Loss after 195042600 batches: 0.0148
trigger times: 0
Loss after 195173700 batches: 0.0136
trigger times: 0
Loss after 195304800 batches: 0.0129
trigger times: 1
Loss after 195435900 batches: 0.0121
trigger times: 2
Loss after 195567000 batches: 0.0117
trigger times: 3
Loss after 195698100 batches: 0.0113
trigger times: 4
Loss after 195829200 batches: 0.0115
trigger times: 5
Loss after 195960300 batches: 0.0108
trigger times: 6
Loss after 196091400 batches: 0.0106
trigger times: 7
Loss after 196222500 batches: 0.0106
trigger times: 8
Loss after 196353600 batches: 0.0106
trigger times: 9
Loss after 196484700 batches: 0.0102
trigger times: 10
Loss after 196615800 batches: 0.0100
trigger times: 0
Loss after 196746900 batches: 0.0099
trigger times: 1
Loss after 196878000 batches: 0.0099
trigger times: 2
Loss after 197009100 batches: 0.0096
trigger times: 3
Loss after 197140200 batches: 0.0096
trigger times: 0
Loss after 197271300 batches: 0.0095
trigger times: 1
Loss after 197402400 batches: 0.0095
trigger times: 2
Loss after 197533500 batches: 0.0091
trigger times: 3
Loss after 197664600 batches: 0.0094
trigger times: 4
Loss after 197795700 batches: 0.0092
trigger times: 5
Loss after 197926800 batches: 0.0093
trigger times: 6
Loss after 198057900 batches: 0.0092
trigger times: 7
Loss after 198189000 batches: 0.0088
trigger times: 8
Loss after 198320100 batches: 0.0088
trigger times: 9
Loss after 198451200 batches: 0.0089
trigger times: 10
Loss after 198582300 batches: 0.0088
trigger times: 11
Loss after 198713400 batches: 0.0087
trigger times: 12
Loss after 198844500 batches: 0.0087
trigger times: 13
Loss after 198975600 batches: 0.0088
trigger times: 14
Loss after 199106700 batches: 0.0086
trigger times: 15
Loss after 199237800 batches: 0.0088
trigger times: 16
Loss after 199368900 batches: 0.0086
trigger times: 17
Loss after 199500000 batches: 0.0083
trigger times: 18
Loss after 199631100 batches: 0.0083
trigger times: 19
Loss after 199762200 batches: 0.0084
trigger times: 20
Early stopping!
Start to test process.
Loss after 199893300 batches: 0.0082
Time to train on one home:  301.72370886802673
trigger times: 0
Loss after 199995900 batches: 0.1992
trigger times: 0
Loss after 200098500 batches: 0.0598
trigger times: 0
Loss after 200201100 batches: 0.0468
trigger times: 0
Loss after 200303700 batches: 0.0451
trigger times: 1
Loss after 200406300 batches: 0.0380
trigger times: 2
Loss after 200508900 batches: 0.0344
trigger times: 3
Loss after 200611500 batches: 0.0329
trigger times: 4
Loss after 200714100 batches: 0.0298
trigger times: 5
Loss after 200816700 batches: 0.0289
trigger times: 6
Loss after 200919300 batches: 0.0278
trigger times: 7
Loss after 201021900 batches: 0.0287
trigger times: 8
Loss after 201124500 batches: 0.0266
trigger times: 9
Loss after 201227100 batches: 0.0266
trigger times: 0
Loss after 201329700 batches: 0.0299
trigger times: 1
Loss after 201432300 batches: 0.0268
trigger times: 2
Loss after 201534900 batches: 0.0290
trigger times: 3
Loss after 201637500 batches: 0.0272
trigger times: 4
Loss after 201740100 batches: 0.0264
trigger times: 5
Loss after 201842700 batches: 0.0244
trigger times: 6
Loss after 201945300 batches: 0.0238
trigger times: 7
Loss after 202047900 batches: 0.0226
trigger times: 8
Loss after 202150500 batches: 0.0226
trigger times: 9
Loss after 202253100 batches: 0.0216
trigger times: 10
Loss after 202355700 batches: 0.0223
trigger times: 11
Loss after 202458300 batches: 0.0218
trigger times: 12
Loss after 202560900 batches: 0.0235
trigger times: 13
Loss after 202663500 batches: 0.0225
trigger times: 14
Loss after 202766100 batches: 0.0212
trigger times: 15
Loss after 202868700 batches: 0.0223
trigger times: 16
Loss after 202971300 batches: 0.0230
trigger times: 17
Loss after 203073900 batches: 0.0213
trigger times: 18
Loss after 203176500 batches: 0.0218
trigger times: 19
Loss after 203279100 batches: 0.0204
trigger times: 20
Early stopping!
Start to test process.
Loss after 203381700 batches: 0.0202
Time to train on one home:  204.88614296913147
trigger times: 0
Loss after 203512800 batches: 0.1024
trigger times: 0
Loss after 203643900 batches: 0.0312
trigger times: 1
Loss after 203775000 batches: 0.0236
trigger times: 2
Loss after 203906100 batches: 0.0205
trigger times: 3
Loss after 204037200 batches: 0.0191
trigger times: 4
Loss after 204168300 batches: 0.0183
trigger times: 5
Loss after 204299400 batches: 0.0176
trigger times: 6
Loss after 204430500 batches: 0.0169
trigger times: 7
Loss after 204561600 batches: 0.0165
trigger times: 8
Loss after 204692700 batches: 0.0160
trigger times: 9
Loss after 204823800 batches: 0.0156
trigger times: 10
Loss after 204954900 batches: 0.0154
trigger times: 11
Loss after 205086000 batches: 0.0151
trigger times: 12
Loss after 205217100 batches: 0.0147
trigger times: 13
Loss after 205348200 batches: 0.0146
trigger times: 14
Loss after 205479300 batches: 0.0144
trigger times: 15
Loss after 205610400 batches: 0.0143
trigger times: 16
Loss after 205741500 batches: 0.0143
trigger times: 17
Loss after 205872600 batches: 0.0140
trigger times: 18
Loss after 206003700 batches: 0.0137
trigger times: 19
Loss after 206134800 batches: 0.0139
trigger times: 20
Early stopping!
Start to test process.
Loss after 206265900 batches: 0.0136
Time to train on one home:  167.32546520233154
trigger times: 0
Loss after 206397000 batches: 0.1081
trigger times: 0
Loss after 206528100 batches: 0.0362
trigger times: 0
Loss after 206659200 batches: 0.0287
trigger times: 0
Loss after 206790300 batches: 0.0260
trigger times: 1
Loss after 206921400 batches: 0.0244
trigger times: 0
Loss after 207052500 batches: 0.0229
trigger times: 1
Loss after 207183600 batches: 0.0223
trigger times: 2
Loss after 207314700 batches: 0.0219
trigger times: 3
Loss after 207445800 batches: 0.0212
trigger times: 4
Loss after 207576900 batches: 0.0211
trigger times: 5
Loss after 207708000 batches: 0.0207
trigger times: 6
Loss after 207839100 batches: 0.0200
trigger times: 7
Loss after 207970200 batches: 0.0199
trigger times: 8
Loss after 208101300 batches: 0.0196
trigger times: 9
Loss after 208232400 batches: 0.0196
trigger times: 10
Loss after 208363500 batches: 0.0190
trigger times: 0
Loss after 208494600 batches: 0.0187
trigger times: 1
Loss after 208625700 batches: 0.0186
trigger times: 2
Loss after 208756800 batches: 0.0186
trigger times: 3
Loss after 208887900 batches: 0.0183
trigger times: 4
Loss after 209019000 batches: 0.0183
trigger times: 5
Loss after 209150100 batches: 0.0180
trigger times: 6
Loss after 209281200 batches: 0.0178
trigger times: 7
Loss after 209412300 batches: 0.0180
trigger times: 8
Loss after 209543400 batches: 0.0177
trigger times: 9
Loss after 209674500 batches: 0.0175
trigger times: 10
Loss after 209805600 batches: 0.0175
trigger times: 11
Loss after 209936700 batches: 0.0174
trigger times: 12
Loss after 210067800 batches: 0.0171
trigger times: 13
Loss after 210198900 batches: 0.0172
trigger times: 14
Loss after 210330000 batches: 0.0172
trigger times: 15
Loss after 210461100 batches: 0.0169
trigger times: 16
Loss after 210592200 batches: 0.0169
trigger times: 17
Loss after 210723300 batches: 0.0168
trigger times: 0
Loss after 210854400 batches: 0.0170
trigger times: 0
Loss after 210985500 batches: 0.0167
trigger times: 1
Loss after 211116600 batches: 0.0169
trigger times: 2
Loss after 211247700 batches: 0.0167
trigger times: 3
Loss after 211378800 batches: 0.0166
trigger times: 4
Loss after 211509900 batches: 0.0164
trigger times: 5
Loss after 211641000 batches: 0.0161
trigger times: 6
Loss after 211772100 batches: 0.0165
trigger times: 7
Loss after 211903200 batches: 0.0164
trigger times: 8
Loss after 212034300 batches: 0.0163
trigger times: 9
Loss after 212165400 batches: 0.0161
trigger times: 10
Loss after 212296500 batches: 0.0160
trigger times: 0
Loss after 212427600 batches: 0.0160
trigger times: 0
Loss after 212558700 batches: 0.0164
trigger times: 0
Loss after 212689800 batches: 0.0156
trigger times: 1
Loss after 212820900 batches: 0.0156
trigger times: 2
Loss after 212952000 batches: 0.0157
trigger times: 3
Loss after 213083100 batches: 0.0155
trigger times: 4
Loss after 213214200 batches: 0.0158
trigger times: 5
Loss after 213345300 batches: 0.0158
trigger times: 6
Loss after 213476400 batches: 0.0156
trigger times: 0
Loss after 213607500 batches: 0.0156
trigger times: 1
Loss after 213738600 batches: 0.0154
trigger times: 2
Loss after 213869700 batches: 0.0154
trigger times: 3
Loss after 214000800 batches: 0.0153
trigger times: 4
Loss after 214131900 batches: 0.0152
trigger times: 5
Loss after 214263000 batches: 0.0153
trigger times: 6
Loss after 214394100 batches: 0.0155
trigger times: 7
Loss after 214525200 batches: 0.0150
trigger times: 8
Loss after 214656300 batches: 0.0151
trigger times: 9
Loss after 214787400 batches: 0.0150
trigger times: 10
Loss after 214918500 batches: 0.0151
trigger times: 11
Loss after 215049600 batches: 0.0148
trigger times: 12
Loss after 215180700 batches: 0.0151
trigger times: 13
Loss after 215311800 batches: 0.0149
trigger times: 14
Loss after 215442900 batches: 0.0148
trigger times: 15
Loss after 215574000 batches: 0.0148
trigger times: 16
Loss after 215705100 batches: 0.0149
trigger times: 17
Loss after 215836200 batches: 0.0151
trigger times: 18
Loss after 215967300 batches: 0.0147
trigger times: 19
Loss after 216098400 batches: 0.0148
trigger times: 20
Early stopping!
Start to test process.
Loss after 216229500 batches: 0.0148
Time to train on one home:  552.693507194519
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358, 0.014216079169119518]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175], [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912]]
Round_8_results:  [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912]
trigger times: 0
Loss after 216360600 batches: 0.0659
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1701 < 1702; dropping {'Training_Loss': 0.06594951000978362, 'Validation_Loss': 0.2254282964600457, 'Training_R2': 0.93362265478805, 'Validation_R2': 0.7903850762968238, 'Training_F1': 0.8562281023561917, 'Validation_F1': 0.7594837539301463, 'Training_NEP': 0.28772234646357, 'Validation_NEP': 0.48407967491539866, 'Training_NDE': 0.04983065125031436, 'Validation_NDE': 0.16692627557315848, 'Training_MAE': 9.528793499929863, 'Validation_MAE': 13.27547300972819, 'Training_MSE': 219.24704, 'Validation_MSE': 616.4541}.
trigger times: 0
Loss after 216491700 batches: 0.0200
trigger times: 0
Loss after 216622800 batches: 0.0159
trigger times: 1
Loss after 216753900 batches: 0.0142
trigger times: 2
Loss after 216885000 batches: 0.0132
trigger times: 3
Loss after 217016100 batches: 0.0122
trigger times: 4
Loss after 217147200 batches: 0.0116
trigger times: 5
Loss after 217278300 batches: 0.0116
trigger times: 6
Loss after 217409400 batches: 0.0114
trigger times: 7
Loss after 217540500 batches: 0.0108
trigger times: 8
Loss after 217671600 batches: 0.0104
trigger times: 9
Loss after 217802700 batches: 0.0104
trigger times: 10
Loss after 217933800 batches: 0.0101
trigger times: 11
Loss after 218064900 batches: 0.0100
trigger times: 12
Loss after 218196000 batches: 0.0100
trigger times: 13
Loss after 218327100 batches: 0.0099
trigger times: 14
Loss after 218458200 batches: 0.0098
trigger times: 15
Loss after 218589300 batches: 0.0096
trigger times: 16
Loss after 218720400 batches: 0.0096
trigger times: 17
Loss after 218851500 batches: 0.0094
trigger times: 18
Loss after 218982600 batches: 0.0093
trigger times: 19
Loss after 219113700 batches: 0.0095
trigger times: 20
Early stopping!
Start to test process.
Loss after 219244800 batches: 0.0090
Time to train on one home:  174.75243616104126
trigger times: 0
Loss after 219347400 batches: 0.1712
trigger times: 0
Loss after 219450000 batches: 0.0546
trigger times: 0
Loss after 219552600 batches: 0.0447
trigger times: 0
Loss after 219655200 batches: 0.0367
trigger times: 1
Loss after 219757800 batches: 0.0344
trigger times: 0
Loss after 219860400 batches: 0.0332
trigger times: 0
Loss after 219963000 batches: 0.0310
trigger times: 1
Loss after 220065600 batches: 0.0310
trigger times: 0
Loss after 220168200 batches: 0.0276
trigger times: 1
Loss after 220270800 batches: 0.0256
trigger times: 2
Loss after 220373400 batches: 0.0258
trigger times: 3
Loss after 220476000 batches: 0.0246
trigger times: 4
Loss after 220578600 batches: 0.0246
trigger times: 5
Loss after 220681200 batches: 0.0318
trigger times: 0
Loss after 220783800 batches: 0.0295
trigger times: 1
Loss after 220886400 batches: 0.0242
trigger times: 2
Loss after 220989000 batches: 0.0234
trigger times: 0
Loss after 221091600 batches: 0.0264
trigger times: 1
Loss after 221194200 batches: 0.0233
trigger times: 2
Loss after 221296800 batches: 0.0223
trigger times: 3
Loss after 221399400 batches: 0.0216
trigger times: 4
Loss after 221502000 batches: 0.0216
trigger times: 5
Loss after 221604600 batches: 0.0220
trigger times: 6
Loss after 221707200 batches: 0.0212
trigger times: 7
Loss after 221809800 batches: 0.0212
trigger times: 8
Loss after 221912400 batches: 0.0217
trigger times: 9
Loss after 222015000 batches: 0.0205
trigger times: 10
Loss after 222117600 batches: 0.0215
trigger times: 11
Loss after 222220200 batches: 0.0225
trigger times: 12
Loss after 222322800 batches: 0.0214
trigger times: 13
Loss after 222425400 batches: 0.0204
trigger times: 14
Loss after 222528000 batches: 0.0196
trigger times: 15
Loss after 222630600 batches: 0.0186
trigger times: 16
Loss after 222733200 batches: 0.0184
trigger times: 17
Loss after 222835800 batches: 0.0195
trigger times: 18
Loss after 222938400 batches: 0.0184
trigger times: 19
Loss after 223041000 batches: 0.0183
trigger times: 20
Early stopping!
Start to test process.
Loss after 223143600 batches: 0.0186
Time to train on one home:  227.9944748878479
trigger times: 0
Loss after 223274700 batches: 0.0716
trigger times: 0
Loss after 223405800 batches: 0.0275
trigger times: 1
Loss after 223536900 batches: 0.0220
trigger times: 2
Loss after 223668000 batches: 0.0197
trigger times: 3
Loss after 223799100 batches: 0.0185
trigger times: 4
Loss after 223930200 batches: 0.0178
trigger times: 5
Loss after 224061300 batches: 0.0171
trigger times: 6
Loss after 224192400 batches: 0.0167
trigger times: 7
Loss after 224323500 batches: 0.0161
trigger times: 8
Loss after 224454600 batches: 0.0158
trigger times: 9
Loss after 224585700 batches: 0.0155
trigger times: 10
Loss after 224716800 batches: 0.0152
trigger times: 11
Loss after 224847900 batches: 0.0152
trigger times: 12
Loss after 224979000 batches: 0.0149
trigger times: 13
Loss after 225110100 batches: 0.0146
trigger times: 14
Loss after 225241200 batches: 0.0142
trigger times: 15
Loss after 225372300 batches: 0.0143
trigger times: 16
Loss after 225503400 batches: 0.0141
trigger times: 17
Loss after 225634500 batches: 0.0139
trigger times: 18
Loss after 225765600 batches: 0.0138
trigger times: 19
Loss after 225896700 batches: 0.0138
trigger times: 20
Early stopping!
Start to test process.
Loss after 226027800 batches: 0.0137
Time to train on one home:  167.79491806030273
trigger times: 0
Loss after 226158900 batches: 0.1005
trigger times: 0
Loss after 226290000 batches: 0.0324
trigger times: 0
Loss after 226421100 batches: 0.0261
trigger times: 1
Loss after 226552200 batches: 0.0238
trigger times: 0
Loss after 226683300 batches: 0.0222
trigger times: 1
Loss after 226814400 batches: 0.0214
trigger times: 2
Loss after 226945500 batches: 0.0207
trigger times: 3
Loss after 227076600 batches: 0.0202
trigger times: 4
Loss after 227207700 batches: 0.0200
trigger times: 5
Loss after 227338800 batches: 0.0194
trigger times: 6
Loss after 227469900 batches: 0.0189
trigger times: 0
Loss after 227601000 batches: 0.0188
trigger times: 1
Loss after 227732100 batches: 0.0183
trigger times: 2
Loss after 227863200 batches: 0.0182
trigger times: 0
Loss after 227994300 batches: 0.0180
trigger times: 1
Loss after 228125400 batches: 0.0180
trigger times: 0
Loss after 228256500 batches: 0.0179
trigger times: 1
Loss after 228387600 batches: 0.0177
trigger times: 2
Loss after 228518700 batches: 0.0175
trigger times: 3
Loss after 228649800 batches: 0.0172
trigger times: 4
Loss after 228780900 batches: 0.0174
trigger times: 5
Loss after 228912000 batches: 0.0171
trigger times: 6
Loss after 229043100 batches: 0.0172
trigger times: 7
Loss after 229174200 batches: 0.0168
trigger times: 8
Loss after 229305300 batches: 0.0167
trigger times: 9
Loss after 229436400 batches: 0.0164
trigger times: 10
Loss after 229567500 batches: 0.0165
trigger times: 11
Loss after 229698600 batches: 0.0164
trigger times: 12
Loss after 229829700 batches: 0.0165
trigger times: 13
Loss after 229960800 batches: 0.0164
trigger times: 14
Loss after 230091900 batches: 0.0164
trigger times: 15
Loss after 230223000 batches: 0.0164
trigger times: 16
Loss after 230354100 batches: 0.0163
trigger times: 17
Loss after 230485200 batches: 0.0160
trigger times: 0
Loss after 230616300 batches: 0.0163
trigger times: 1
Loss after 230747400 batches: 0.0157
trigger times: 0
Loss after 230878500 batches: 0.0159
trigger times: 1
Loss after 231009600 batches: 0.0158
trigger times: 2
Loss after 231140700 batches: 0.0158
trigger times: 3
Loss after 231271800 batches: 0.0154
trigger times: 4
Loss after 231402900 batches: 0.0156
trigger times: 5
Loss after 231534000 batches: 0.0154
trigger times: 6
Loss after 231665100 batches: 0.0155
trigger times: 7
Loss after 231796200 batches: 0.0155
trigger times: 8
Loss after 231927300 batches: 0.0154
trigger times: 9
Loss after 232058400 batches: 0.0154
trigger times: 10
Loss after 232189500 batches: 0.0153
trigger times: 11
Loss after 232320600 batches: 0.0152
trigger times: 12
Loss after 232451700 batches: 0.0151
trigger times: 13
Loss after 232582800 batches: 0.0153
trigger times: 14
Loss after 232713900 batches: 0.0155
trigger times: 15
Loss after 232845000 batches: 0.0153
trigger times: 16
Loss after 232976100 batches: 0.0151
trigger times: 17
Loss after 233107200 batches: 0.0153
trigger times: 18
Loss after 233238300 batches: 0.0149
trigger times: 0
Loss after 233369400 batches: 0.0151
trigger times: 1
Loss after 233500500 batches: 0.0151
trigger times: 2
Loss after 233631600 batches: 0.0148
trigger times: 3
Loss after 233762700 batches: 0.0149
trigger times: 4
Loss after 233893800 batches: 0.0149
trigger times: 5
Loss after 234024900 batches: 0.0147
trigger times: 6
Loss after 234156000 batches: 0.0147
trigger times: 7
Loss after 234287100 batches: 0.0146
trigger times: 8
Loss after 234418200 batches: 0.0145
trigger times: 9
Loss after 234549300 batches: 0.0145
trigger times: 10
Loss after 234680400 batches: 0.0145
trigger times: 11
Loss after 234811500 batches: 0.0145
trigger times: 12
Loss after 234942600 batches: 0.0144
trigger times: 13
Loss after 235073700 batches: 0.0143
trigger times: 14
Loss after 235204800 batches: 0.0144
trigger times: 15
Loss after 235335900 batches: 0.0143
trigger times: 16
Loss after 235467000 batches: 0.0143
trigger times: 17
Loss after 235598100 batches: 0.0141
trigger times: 18
Loss after 235729200 batches: 0.0141
trigger times: 19
Loss after 235860300 batches: 0.0141
trigger times: 20
Early stopping!
Start to test process.
Loss after 235991400 batches: 0.0141
Time to train on one home:  550.7489700317383
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358, 0.014216079169119518, 0.013823527540373118]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175], [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912], [0.4441254701879289, 0.5205093148975452, 0.5665822904653603, 0.943887952691261, 0.39279617400695876, 22.29969119423179, 1212.5862]]
Round_9_results:  [0.4441254701879289, 0.5205093148975452, 0.5665822904653603, 0.943887952691261, 0.39279617400695876, 22.29969119423179, 1212.5862]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1860 < 1861; dropping {'Training_Loss': 0.05884141713943122, 'Validation_Loss': 0.23007231619622973, 'Training_R2': 0.9407743495939311, 'Validation_R2': 0.7862838078420534, 'Training_F1': 0.8636177414948859, 'Validation_F1': 0.7440653389632232, 'Training_NEP': 0.2729278329876553, 'Validation_NEP': 0.49092578182168173, 'Training_NDE': 0.0444617470167598, 'Validation_NDE': 0.17019230957582296, 'Training_MAE': 9.038828554291657, 'Validation_MAE': 13.463221663856164, 'Training_MSE': 195.62471, 'Validation_MSE': 628.5155}.
trigger times: 0
Loss after 236122500 batches: 0.0588
trigger times: 0
Loss after 236253600 batches: 0.0196
trigger times: 0
Loss after 236384700 batches: 0.0153
trigger times: 1
Loss after 236515800 batches: 0.0137
trigger times: 2
Loss after 236646900 batches: 0.0128
trigger times: 3
Loss after 236778000 batches: 0.0119
trigger times: 0
Loss after 236909100 batches: 0.0119
trigger times: 1
Loss after 237040200 batches: 0.0115
trigger times: 2
Loss after 237171300 batches: 0.0109
trigger times: 3
Loss after 237302400 batches: 0.0108
trigger times: 0
Loss after 237433500 batches: 0.0106
trigger times: 0
Loss after 237564600 batches: 0.0103
trigger times: 1
Loss after 237695700 batches: 0.0102
trigger times: 2
Loss after 237826800 batches: 0.0101
trigger times: 3
Loss after 237957900 batches: 0.0099
trigger times: 4
Loss after 238089000 batches: 0.0097
trigger times: 5
Loss after 238220100 batches: 0.0094
trigger times: 6
Loss after 238351200 batches: 0.0095
trigger times: 7
Loss after 238482300 batches: 0.0094
trigger times: 8
Loss after 238613400 batches: 0.0093
trigger times: 9
Loss after 238744500 batches: 0.0093
trigger times: 10
Loss after 238875600 batches: 0.0092
trigger times: 11
Loss after 239006700 batches: 0.0092
trigger times: 12
Loss after 239137800 batches: 0.0091
trigger times: 13
Loss after 239268900 batches: 0.0088
trigger times: 0
Loss after 239400000 batches: 0.0089
trigger times: 1
Loss after 239531100 batches: 0.0086
trigger times: 2
Loss after 239662200 batches: 0.0087
trigger times: 3
Loss after 239793300 batches: 0.0087
trigger times: 4
Loss after 239924400 batches: 0.0085
trigger times: 5
Loss after 240055500 batches: 0.0085
trigger times: 6
Loss after 240186600 batches: 0.0083
trigger times: 7
Loss after 240317700 batches: 0.0083
trigger times: 8
Loss after 240448800 batches: 0.0084
trigger times: 9
Loss after 240579900 batches: 0.0081
trigger times: 10
Loss after 240711000 batches: 0.0082
trigger times: 11
Loss after 240842100 batches: 0.0083
trigger times: 12
Loss after 240973200 batches: 0.0083
trigger times: 13
Loss after 241104300 batches: 0.0083
trigger times: 14
Loss after 241235400 batches: 0.0081
trigger times: 15
Loss after 241366500 batches: 0.0080
trigger times: 16
Loss after 241497600 batches: 0.0080
trigger times: 17
Loss after 241628700 batches: 0.0080
trigger times: 18
Loss after 241759800 batches: 0.0079
trigger times: 19
Loss after 241890900 batches: 0.0078
trigger times: 20
Early stopping!
Start to test process.
Loss after 242022000 batches: 0.0078
Time to train on one home:  338.57673621177673
trigger times: 0
Loss after 242124600 batches: 0.1420
trigger times: 0
Loss after 242227200 batches: 0.0490
trigger times: 1
Loss after 242329800 batches: 0.0383
trigger times: 2
Loss after 242432400 batches: 0.0351
trigger times: 3
Loss after 242535000 batches: 0.0317
trigger times: 4
Loss after 242637600 batches: 0.0296
trigger times: 5
Loss after 242740200 batches: 0.0288
trigger times: 6
Loss after 242842800 batches: 0.0324
trigger times: 7
Loss after 242945400 batches: 0.0284
trigger times: 8
Loss after 243048000 batches: 0.0264
trigger times: 9
Loss after 243150600 batches: 0.0248
trigger times: 10
Loss after 243253200 batches: 0.0252
trigger times: 11
Loss after 243355800 batches: 0.0277
trigger times: 12
Loss after 243458400 batches: 0.0251
trigger times: 13
Loss after 243561000 batches: 0.0229
trigger times: 14
Loss after 243663600 batches: 0.0236
trigger times: 15
Loss after 243766200 batches: 0.0222
trigger times: 16
Loss after 243868800 batches: 0.0218
trigger times: 17
Loss after 243971400 batches: 0.0215
trigger times: 18
Loss after 244074000 batches: 0.0220
trigger times: 19
Loss after 244176600 batches: 0.0215
trigger times: 20
Early stopping!
Start to test process.
Loss after 244279200 batches: 0.0202
Time to train on one home:  137.23637342453003
trigger times: 0
Loss after 244410300 batches: 0.0702
trigger times: 0
Loss after 244541400 batches: 0.0266
trigger times: 0
Loss after 244672500 batches: 0.0216
trigger times: 1
Loss after 244803600 batches: 0.0197
trigger times: 2
Loss after 244934700 batches: 0.0188
trigger times: 3
Loss after 245065800 batches: 0.0177
trigger times: 4
Loss after 245196900 batches: 0.0172
trigger times: 5
Loss after 245328000 batches: 0.0165
trigger times: 6
Loss after 245459100 batches: 0.0162
trigger times: 7
Loss after 245590200 batches: 0.0158
trigger times: 8
Loss after 245721300 batches: 0.0155
trigger times: 9
Loss after 245852400 batches: 0.0152
trigger times: 10
Loss after 245983500 batches: 0.0150
trigger times: 11
Loss after 246114600 batches: 0.0150
trigger times: 12
Loss after 246245700 batches: 0.0147
trigger times: 13
Loss after 246376800 batches: 0.0143
trigger times: 14
Loss after 246507900 batches: 0.0142
trigger times: 15
Loss after 246639000 batches: 0.0141
trigger times: 16
Loss after 246770100 batches: 0.0139
trigger times: 17
Loss after 246901200 batches: 0.0138
trigger times: 18
Loss after 247032300 batches: 0.0140
trigger times: 19
Loss after 247163400 batches: 0.0135
trigger times: 20
Early stopping!
Start to test process.
Loss after 247294500 batches: 0.0134
Time to train on one home:  175.15921258926392
trigger times: 0
Loss after 247425600 batches: 0.0998
trigger times: 0
Loss after 247556700 batches: 0.0295
trigger times: 1
Loss after 247687800 batches: 0.0235
trigger times: 0
Loss after 247818900 batches: 0.0216
trigger times: 0
Loss after 247950000 batches: 0.0207
trigger times: 1
Loss after 248081100 batches: 0.0200
trigger times: 2
Loss after 248212200 batches: 0.0194
trigger times: 3
Loss after 248343300 batches: 0.0188
trigger times: 4
Loss after 248474400 batches: 0.0186
trigger times: 5
Loss after 248605500 batches: 0.0182
trigger times: 6
Loss after 248736600 batches: 0.0179
trigger times: 7
Loss after 248867700 batches: 0.0175
trigger times: 8
Loss after 248998800 batches: 0.0175
trigger times: 9
Loss after 249129900 batches: 0.0172
trigger times: 10
Loss after 249261000 batches: 0.0170
trigger times: 11
Loss after 249392100 batches: 0.0169
trigger times: 12
Loss after 249523200 batches: 0.0169
trigger times: 13
Loss after 249654300 batches: 0.0169
trigger times: 14
Loss after 249785400 batches: 0.0165
trigger times: 15
Loss after 249916500 batches: 0.0165
trigger times: 16
Loss after 250047600 batches: 0.0164
trigger times: 17
Loss after 250178700 batches: 0.0160
trigger times: 18
Loss after 250309800 batches: 0.0163
trigger times: 19
Loss after 250440900 batches: 0.0159
trigger times: 0
Loss after 250572000 batches: 0.0161
trigger times: 1
Loss after 250703100 batches: 0.0159
trigger times: 2
Loss after 250834200 batches: 0.0160
trigger times: 0
Loss after 250965300 batches: 0.0158
trigger times: 1
Loss after 251096400 batches: 0.0158
trigger times: 2
Loss after 251227500 batches: 0.0160
trigger times: 3
Loss after 251358600 batches: 0.0156
trigger times: 4
Loss after 251489700 batches: 0.0154
trigger times: 5
Loss after 251620800 batches: 0.0154
trigger times: 6
Loss after 251751900 batches: 0.0154
trigger times: 7
Loss after 251883000 batches: 0.0157
trigger times: 8
Loss after 252014100 batches: 0.0153
trigger times: 9
Loss after 252145200 batches: 0.0152
trigger times: 10
Loss after 252276300 batches: 0.0150
trigger times: 11
Loss after 252407400 batches: 0.0149
trigger times: 12
Loss after 252538500 batches: 0.0153
trigger times: 13
Loss after 252669600 batches: 0.0152
trigger times: 14
Loss after 252800700 batches: 0.0152
trigger times: 15
Loss after 252931800 batches: 0.0151
trigger times: 16
Loss after 253062900 batches: 0.0149
trigger times: 17
Loss after 253194000 batches: 0.0146
trigger times: 18
Loss after 253325100 batches: 0.0149
trigger times: 19
Loss after 253456200 batches: 0.0149
trigger times: 20
Early stopping!
Start to test process.
Loss after 253587300 batches: 0.0149
Time to train on one home:  352.47926092147827
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358, 0.014216079169119518, 0.013823527540373118, 0.014062286198334994]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175], [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912], [0.4441254701879289, 0.5205093148975452, 0.5665822904653603, 0.943887952691261, 0.39279617400695876, 22.29969119423179, 1212.5862], [0.43548785315619576, 0.5298403732534891, 0.5768369757093087, 0.9242649282254869, 0.38515222150584383, 21.836090207869006, 1188.9888]]
Round_10_results:  [0.43548785315619576, 0.5298403732534891, 0.5768369757093087, 0.9242649282254869, 0.38515222150584383, 21.836090207869006, 1188.9888]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1999 < 2000; dropping {'Training_Loss': 0.05980947768350817, 'Validation_Loss': 0.23749742905298868, 'Training_R2': 0.9397509022349286, 'Validation_R2': 0.7792263313512837, 'Training_F1': 0.863580679909691, 'Validation_F1': 0.7368471725864093, 'Training_NEP': 0.2729765303069087, 'Validation_NEP': 0.4982917723127675, 'Training_NDE': 0.04523006711538174, 'Validation_NDE': 0.1758125118244831, 'Training_MAE': 9.040441312928131, 'Validation_MAE': 13.665227682744304, 'Training_MSE': 199.00519, 'Validation_MSE': 649.2708}.
trigger times: 0
Loss after 253718400 batches: 0.0598
trigger times: 0
Loss after 253849500 batches: 0.0186
trigger times: 1
Loss after 253980600 batches: 0.0146
trigger times: 2
Loss after 254111700 batches: 0.0128
trigger times: 3
Loss after 254242800 batches: 0.0118
trigger times: 4
Loss after 254373900 batches: 0.0117
trigger times: 5
Loss after 254505000 batches: 0.0110
trigger times: 6
Loss after 254636100 batches: 0.0106
trigger times: 7
Loss after 254767200 batches: 0.0106
trigger times: 8
Loss after 254898300 batches: 0.0103
trigger times: 9
Loss after 255029400 batches: 0.0100
trigger times: 0
Loss after 255160500 batches: 0.0098
trigger times: 1
Loss after 255291600 batches: 0.0095
trigger times: 2
Loss after 255422700 batches: 0.0095
trigger times: 3
Loss after 255553800 batches: 0.0094
trigger times: 4
Loss after 255684900 batches: 0.0092
trigger times: 5
Loss after 255816000 batches: 0.0092
trigger times: 0
Loss after 255947100 batches: 0.0092
trigger times: 1
Loss after 256078200 batches: 0.0090
trigger times: 2
Loss after 256209300 batches: 0.0088
trigger times: 3
Loss after 256340400 batches: 0.0086
trigger times: 0
Loss after 256471500 batches: 0.0088
trigger times: 0
Loss after 256602600 batches: 0.0088
trigger times: 1
Loss after 256733700 batches: 0.0086
trigger times: 2
Loss after 256864800 batches: 0.0085
trigger times: 3
Loss after 256995900 batches: 0.0085
trigger times: 4
Loss after 257127000 batches: 0.0085
trigger times: 5
Loss after 257258100 batches: 0.0084
trigger times: 6
Loss after 257389200 batches: 0.0084
trigger times: 7
Loss after 257520300 batches: 0.0083
trigger times: 8
Loss after 257651400 batches: 0.0083
trigger times: 9
Loss after 257782500 batches: 0.0082
trigger times: 10
Loss after 257913600 batches: 0.0080
trigger times: 0
Loss after 258044700 batches: 0.0081
trigger times: 1
Loss after 258175800 batches: 0.0080
trigger times: 2
Loss after 258306900 batches: 0.0079
trigger times: 3
Loss after 258438000 batches: 0.0080
trigger times: 4
Loss after 258569100 batches: 0.0079
trigger times: 5
Loss after 258700200 batches: 0.0078
trigger times: 6
Loss after 258831300 batches: 0.0079
trigger times: 7
Loss after 258962400 batches: 0.0078
trigger times: 8
Loss after 259093500 batches: 0.0077
trigger times: 9
Loss after 259224600 batches: 0.0077
trigger times: 10
Loss after 259355700 batches: 0.0076
trigger times: 11
Loss after 259486800 batches: 0.0076
trigger times: 12
Loss after 259617900 batches: 0.0075
trigger times: 13
Loss after 259749000 batches: 0.0075
trigger times: 14
Loss after 259880100 batches: 0.0075
trigger times: 15
Loss after 260011200 batches: 0.0073
trigger times: 16
Loss after 260142300 batches: 0.0075
trigger times: 17
Loss after 260273400 batches: 0.0074
trigger times: 18
Loss after 260404500 batches: 0.0074
trigger times: 19
Loss after 260535600 batches: 0.0072
trigger times: 20
Early stopping!
Start to test process.
Loss after 260666700 batches: 0.0071
Time to train on one home:  394.90447473526
trigger times: 0
Loss after 260769300 batches: 0.1340
trigger times: 0
Loss after 260871900 batches: 0.0493
trigger times: 0
Loss after 260974500 batches: 0.0359
trigger times: 1
Loss after 261077100 batches: 0.0316
trigger times: 2
Loss after 261179700 batches: 0.0294
trigger times: 3
Loss after 261282300 batches: 0.0273
trigger times: 4
Loss after 261384900 batches: 0.0261
trigger times: 0
Loss after 261487500 batches: 0.0262
trigger times: 1
Loss after 261590100 batches: 0.0271
trigger times: 2
Loss after 261692700 batches: 0.0301
trigger times: 3
Loss after 261795300 batches: 0.0246
trigger times: 4
Loss after 261897900 batches: 0.0237
trigger times: 5
Loss after 262000500 batches: 0.0228
trigger times: 6
Loss after 262103100 batches: 0.0231
trigger times: 7
Loss after 262205700 batches: 0.0227
trigger times: 8
Loss after 262308300 batches: 0.0226
trigger times: 9
Loss after 262410900 batches: 0.0228
trigger times: 10
Loss after 262513500 batches: 0.0222
trigger times: 11
Loss after 262616100 batches: 0.0257
trigger times: 12
Loss after 262718700 batches: 0.0217
trigger times: 13
Loss after 262821300 batches: 0.0204
trigger times: 14
Loss after 262923900 batches: 0.0198
trigger times: 15
Loss after 263026500 batches: 0.0197
trigger times: 16
Loss after 263129100 batches: 0.0200
trigger times: 17
Loss after 263231700 batches: 0.0192
trigger times: 18
Loss after 263334300 batches: 0.0188
trigger times: 19
Loss after 263436900 batches: 0.0191
trigger times: 20
Early stopping!
Start to test process.
Loss after 263539500 batches: 0.0192
Time to train on one home:  171.27854824066162
trigger times: 0
Loss after 263670600 batches: 0.0815
trigger times: 0
Loss after 263801700 batches: 0.0265
trigger times: 1
Loss after 263932800 batches: 0.0215
trigger times: 2
Loss after 264063900 batches: 0.0195
trigger times: 3
Loss after 264195000 batches: 0.0183
trigger times: 4
Loss after 264326100 batches: 0.0174
trigger times: 5
Loss after 264457200 batches: 0.0168
trigger times: 6
Loss after 264588300 batches: 0.0165
trigger times: 7
Loss after 264719400 batches: 0.0161
trigger times: 8
Loss after 264850500 batches: 0.0159
trigger times: 9
Loss after 264981600 batches: 0.0154
trigger times: 10
Loss after 265112700 batches: 0.0153
trigger times: 11
Loss after 265243800 batches: 0.0149
trigger times: 12
Loss after 265374900 batches: 0.0148
trigger times: 13
Loss after 265506000 batches: 0.0147
trigger times: 14
Loss after 265637100 batches: 0.0144
trigger times: 15
Loss after 265768200 batches: 0.0141
trigger times: 16
Loss after 265899300 batches: 0.0141
trigger times: 17
Loss after 266030400 batches: 0.0139
trigger times: 18
Loss after 266161500 batches: 0.0135
trigger times: 19
Loss after 266292600 batches: 0.0137
trigger times: 20
Early stopping!
Start to test process.
Loss after 266423700 batches: 0.0136
Time to train on one home:  167.78028106689453
trigger times: 0
Loss after 266554800 batches: 0.0849
trigger times: 0
Loss after 266685900 batches: 0.0268
trigger times: 0
Loss after 266817000 batches: 0.0224
trigger times: 1
Loss after 266948100 batches: 0.0208
trigger times: 0
Loss after 267079200 batches: 0.0201
trigger times: 1
Loss after 267210300 batches: 0.0191
trigger times: 2
Loss after 267341400 batches: 0.0189
trigger times: 3
Loss after 267472500 batches: 0.0181
trigger times: 4
Loss after 267603600 batches: 0.0181
trigger times: 5
Loss after 267734700 batches: 0.0179
trigger times: 6
Loss after 267865800 batches: 0.0174
trigger times: 7
Loss after 267996900 batches: 0.0173
trigger times: 0
Loss after 268128000 batches: 0.0172
trigger times: 1
Loss after 268259100 batches: 0.0168
trigger times: 2
Loss after 268390200 batches: 0.0165
trigger times: 0
Loss after 268521300 batches: 0.0167
trigger times: 1
Loss after 268652400 batches: 0.0166
trigger times: 2
Loss after 268783500 batches: 0.0163
trigger times: 3
Loss after 268914600 batches: 0.0160
trigger times: 4
Loss after 269045700 batches: 0.0162
trigger times: 5
Loss after 269176800 batches: 0.0161
trigger times: 6
Loss after 269307900 batches: 0.0159
trigger times: 7
Loss after 269439000 batches: 0.0158
trigger times: 8
Loss after 269570100 batches: 0.0157
trigger times: 9
Loss after 269701200 batches: 0.0156
trigger times: 10
Loss after 269832300 batches: 0.0155
trigger times: 11
Loss after 269963400 batches: 0.0155
trigger times: 12
Loss after 270094500 batches: 0.0154
trigger times: 13
Loss after 270225600 batches: 0.0156
trigger times: 14
Loss after 270356700 batches: 0.0152
trigger times: 15
Loss after 270487800 batches: 0.0153
trigger times: 16
Loss after 270618900 batches: 0.0154
trigger times: 0
Loss after 270750000 batches: 0.0151
trigger times: 1
Loss after 270881100 batches: 0.0152
trigger times: 2
Loss after 271012200 batches: 0.0152
trigger times: 3
Loss after 271143300 batches: 0.0151
trigger times: 0
Loss after 271274400 batches: 0.0150
trigger times: 1
Loss after 271405500 batches: 0.0150
trigger times: 2
Loss after 271536600 batches: 0.0150
trigger times: 3
Loss after 271667700 batches: 0.0150
trigger times: 4
Loss after 271798800 batches: 0.0150
trigger times: 5
Loss after 271929900 batches: 0.0145
trigger times: 6
Loss after 272061000 batches: 0.0147
trigger times: 7
Loss after 272192100 batches: 0.0148
trigger times: 8
Loss after 272323200 batches: 0.0146
trigger times: 9
Loss after 272454300 batches: 0.0145
trigger times: 10
Loss after 272585400 batches: 0.0148
trigger times: 11
Loss after 272716500 batches: 0.0146
trigger times: 12
Loss after 272847600 batches: 0.0146
trigger times: 13
Loss after 272978700 batches: 0.0145
trigger times: 14
Loss after 273109800 batches: 0.0143
trigger times: 15
Loss after 273240900 batches: 0.0144
trigger times: 16
Loss after 273372000 batches: 0.0144
trigger times: 17
Loss after 273503100 batches: 0.0145
trigger times: 18
Loss after 273634200 batches: 0.0140
trigger times: 19
Loss after 273765300 batches: 0.0141
trigger times: 20
Early stopping!
Start to test process.
Loss after 273896400 batches: 0.0142
Time to train on one home:  415.78285932540894
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358, 0.014216079169119518, 0.013823527540373118, 0.014062286198334994, 0.013536151017310049]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175], [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912], [0.4441254701879289, 0.5205093148975452, 0.5665822904653603, 0.943887952691261, 0.39279617400695876, 22.29969119423179, 1212.5862], [0.43548785315619576, 0.5298403732534891, 0.5768369757093087, 0.9242649282254869, 0.38515222150584383, 21.836090207869006, 1188.9888], [0.4311514099438985, 0.5345118722814012, 0.5754949634807172, 0.9187582104432755, 0.3813253547865264, 21.705992026525585, 1177.175]]
Round_11_results:  [0.4311514099438985, 0.5345118722814012, 0.5754949634807172, 0.9187582104432755, 0.3813253547865264, 21.705992026525585, 1177.175]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2160 < 2161; dropping {'Training_Loss': 0.05919841852671695, 'Validation_Loss': 0.22946705172459284, 'Training_R2': 0.940392240235601, 'Validation_R2': 0.7867850871307924, 'Training_F1': 0.8646722263149409, 'Validation_F1': 0.7613111092704318, 'Training_NEP': 0.2706255509039582, 'Validation_NEP': 0.480004393127076, 'Training_NDE': 0.04474860329450986, 'Validation_NDE': 0.16979311717476242, 'Training_MAE': 8.96258153759733, 'Validation_MAE': 13.163711875783926, 'Training_MSE': 196.88687, 'Validation_MSE': 627.0413}.
trigger times: 0
Loss after 274027500 batches: 0.0592
trigger times: 1
Loss after 274158600 batches: 0.0174
trigger times: 0
Loss after 274289700 batches: 0.0136
trigger times: 1
Loss after 274420800 batches: 0.0125
trigger times: 2
Loss after 274551900 batches: 0.0115
trigger times: 0
Loss after 274683000 batches: 0.0110
trigger times: 1
Loss after 274814100 batches: 0.0103
trigger times: 2
Loss after 274945200 batches: 0.0102
trigger times: 3
Loss after 275076300 batches: 0.0099
trigger times: 4
Loss after 275207400 batches: 0.0096
trigger times: 5
Loss after 275338500 batches: 0.0094
trigger times: 6
Loss after 275469600 batches: 0.0095
trigger times: 0
Loss after 275600700 batches: 0.0092
trigger times: 1
Loss after 275731800 batches: 0.0091
trigger times: 2
Loss after 275862900 batches: 0.0092
trigger times: 3
Loss after 275994000 batches: 0.0089
trigger times: 4
Loss after 276125100 batches: 0.0089
trigger times: 5
Loss after 276256200 batches: 0.0086
trigger times: 6
Loss after 276387300 batches: 0.0086
trigger times: 7
Loss after 276518400 batches: 0.0085
trigger times: 8
Loss after 276649500 batches: 0.0084
trigger times: 9
Loss after 276780600 batches: 0.0084
trigger times: 10
Loss after 276911700 batches: 0.0083
trigger times: 11
Loss after 277042800 batches: 0.0082
trigger times: 0
Loss after 277173900 batches: 0.0082
trigger times: 1
Loss after 277305000 batches: 0.0082
trigger times: 2
Loss after 277436100 batches: 0.0082
trigger times: 3
Loss after 277567200 batches: 0.0079
trigger times: 4
Loss after 277698300 batches: 0.0081
trigger times: 5
Loss after 277829400 batches: 0.0080
trigger times: 6
Loss after 277960500 batches: 0.0080
trigger times: 0
Loss after 278091600 batches: 0.0076
trigger times: 1
Loss after 278222700 batches: 0.0077
trigger times: 2
Loss after 278353800 batches: 0.0079
trigger times: 3
Loss after 278484900 batches: 0.0079
trigger times: 4
Loss after 278616000 batches: 0.0078
trigger times: 5
Loss after 278747100 batches: 0.0078
trigger times: 6
Loss after 278878200 batches: 0.0078
trigger times: 7
Loss after 279009300 batches: 0.0075
trigger times: 8
Loss after 279140400 batches: 0.0075
trigger times: 9
Loss after 279271500 batches: 0.0075
trigger times: 10
Loss after 279402600 batches: 0.0074
trigger times: 11
Loss after 279533700 batches: 0.0075
trigger times: 12
Loss after 279664800 batches: 0.0074
trigger times: 13
Loss after 279795900 batches: 0.0072
trigger times: 14
Loss after 279927000 batches: 0.0074
trigger times: 15
Loss after 280058100 batches: 0.0073
trigger times: 16
Loss after 280189200 batches: 0.0072
trigger times: 17
Loss after 280320300 batches: 0.0071
trigger times: 18
Loss after 280451400 batches: 0.0072
trigger times: 19
Loss after 280582500 batches: 0.0072
trigger times: 20
Early stopping!
Start to test process.
Loss after 280713600 batches: 0.0072
Time to train on one home:  381.04627561569214
trigger times: 0
Loss after 280816200 batches: 0.1342
trigger times: 0
Loss after 280918800 batches: 0.0427
trigger times: 0
Loss after 281021400 batches: 0.0340
trigger times: 1
Loss after 281124000 batches: 0.0312
trigger times: 0
Loss after 281226600 batches: 0.0323
trigger times: 1
Loss after 281329200 batches: 0.0291
trigger times: 2
Loss after 281431800 batches: 0.0257
trigger times: 3
Loss after 281534400 batches: 0.0249
trigger times: 4
Loss after 281637000 batches: 0.0242
trigger times: 5
Loss after 281739600 batches: 0.0229
trigger times: 6
Loss after 281842200 batches: 0.0225
trigger times: 7
Loss after 281944800 batches: 0.0233
trigger times: 8
Loss after 282047400 batches: 0.0226
trigger times: 9
Loss after 282150000 batches: 0.0230
trigger times: 10
Loss after 282252600 batches: 0.0216
trigger times: 11
Loss after 282355200 batches: 0.0212
trigger times: 12
Loss after 282457800 batches: 0.0211
trigger times: 13
Loss after 282560400 batches: 0.0210
trigger times: 14
Loss after 282663000 batches: 0.0210
trigger times: 15
Loss after 282765600 batches: 0.0213
trigger times: 16
Loss after 282868200 batches: 0.0194
trigger times: 17
Loss after 282970800 batches: 0.0192
trigger times: 18
Loss after 283073400 batches: 0.0218
trigger times: 19
Loss after 283176000 batches: 0.0223
trigger times: 20
Early stopping!
Start to test process.
Loss after 283278600 batches: 0.0201
Time to train on one home:  154.46393537521362
trigger times: 0
Loss after 283409700 batches: 0.0656
trigger times: 0
Loss after 283540800 batches: 0.0250
trigger times: 1
Loss after 283671900 batches: 0.0209
trigger times: 2
Loss after 283803000 batches: 0.0189
trigger times: 3
Loss after 283934100 batches: 0.0181
trigger times: 4
Loss after 284065200 batches: 0.0175
trigger times: 5
Loss after 284196300 batches: 0.0171
trigger times: 6
Loss after 284327400 batches: 0.0162
trigger times: 7
Loss after 284458500 batches: 0.0160
trigger times: 8
Loss after 284589600 batches: 0.0160
trigger times: 9
Loss after 284720700 batches: 0.0155
trigger times: 10
Loss after 284851800 batches: 0.0149
trigger times: 11
Loss after 284982900 batches: 0.0147
trigger times: 12
Loss after 285114000 batches: 0.0147
trigger times: 13
Loss after 285245100 batches: 0.0144
trigger times: 14
Loss after 285376200 batches: 0.0140
trigger times: 15
Loss after 285507300 batches: 0.0142
trigger times: 16
Loss after 285638400 batches: 0.0139
trigger times: 17
Loss after 285769500 batches: 0.0137
trigger times: 18
Loss after 285900600 batches: 0.0138
trigger times: 19
Loss after 286031700 batches: 0.0135
trigger times: 20
Early stopping!
Start to test process.
Loss after 286162800 batches: 0.0135
Time to train on one home:  167.5539960861206
trigger times: 0
Loss after 286293900 batches: 0.0806
trigger times: 0
Loss after 286425000 batches: 0.0268
trigger times: 0
Loss after 286556100 batches: 0.0221
trigger times: 1
Loss after 286687200 batches: 0.0203
trigger times: 2
Loss after 286818300 batches: 0.0195
trigger times: 3
Loss after 286949400 batches: 0.0188
trigger times: 4
Loss after 287080500 batches: 0.0184
trigger times: 5
Loss after 287211600 batches: 0.0180
trigger times: 6
Loss after 287342700 batches: 0.0175
trigger times: 7
Loss after 287473800 batches: 0.0173
trigger times: 8
Loss after 287604900 batches: 0.0171
trigger times: 0
Loss after 287736000 batches: 0.0168
trigger times: 1
Loss after 287867100 batches: 0.0168
trigger times: 2
Loss after 287998200 batches: 0.0164
trigger times: 0
Loss after 288129300 batches: 0.0163
trigger times: 1
Loss after 288260400 batches: 0.0160
trigger times: 2
Loss after 288391500 batches: 0.0160
trigger times: 3
Loss after 288522600 batches: 0.0160
trigger times: 4
Loss after 288653700 batches: 0.0161
trigger times: 5
Loss after 288784800 batches: 0.0158
trigger times: 0
Loss after 288915900 batches: 0.0156
trigger times: 1
Loss after 289047000 batches: 0.0156
trigger times: 0
Loss after 289178100 batches: 0.0155
trigger times: 1
Loss after 289309200 batches: 0.0155
trigger times: 0
Loss after 289440300 batches: 0.0154
trigger times: 0
Loss after 289571400 batches: 0.0153
trigger times: 1
Loss after 289702500 batches: 0.0153
trigger times: 0
Loss after 289833600 batches: 0.0151
trigger times: 1
Loss after 289964700 batches: 0.0151
trigger times: 2
Loss after 290095800 batches: 0.0152
trigger times: 3
Loss after 290226900 batches: 0.0150
trigger times: 4
Loss after 290358000 batches: 0.0152
trigger times: 5
Loss after 290489100 batches: 0.0149
trigger times: 0
Loss after 290620200 batches: 0.0151
trigger times: 0
Loss after 290751300 batches: 0.0148
trigger times: 1
Loss after 290882400 batches: 0.0149
trigger times: 2
Loss after 291013500 batches: 0.0148
trigger times: 3
Loss after 291144600 batches: 0.0146
trigger times: 4
Loss after 291275700 batches: 0.0148
trigger times: 5
Loss after 291406800 batches: 0.0147
trigger times: 0
Loss after 291537900 batches: 0.0146
trigger times: 1
Loss after 291669000 batches: 0.0147
trigger times: 2
Loss after 291800100 batches: 0.0145
trigger times: 3
Loss after 291931200 batches: 0.0144
trigger times: 4
Loss after 292062300 batches: 0.0142
trigger times: 5
Loss after 292193400 batches: 0.0140
trigger times: 6
Loss after 292324500 batches: 0.0142
trigger times: 7
Loss after 292455600 batches: 0.0144
trigger times: 8
Loss after 292586700 batches: 0.0144
trigger times: 9
Loss after 292717800 batches: 0.0143
trigger times: 10
Loss after 292848900 batches: 0.0142
trigger times: 0
Loss after 292980000 batches: 0.0139
trigger times: 1
Loss after 293111100 batches: 0.0138
trigger times: 2
Loss after 293242200 batches: 0.0138
trigger times: 3
Loss after 293373300 batches: 0.0139
trigger times: 4
Loss after 293504400 batches: 0.0141
trigger times: 5
Loss after 293635500 batches: 0.0138
trigger times: 6
Loss after 293766600 batches: 0.0138
trigger times: 7
Loss after 293897700 batches: 0.0135
trigger times: 8
Loss after 294028800 batches: 0.0138
trigger times: 9
Loss after 294159900 batches: 0.0139
trigger times: 10
Loss after 294291000 batches: 0.0136
trigger times: 11
Loss after 294422100 batches: 0.0136
trigger times: 12
Loss after 294553200 batches: 0.0137
trigger times: 0
Loss after 294684300 batches: 0.0136
trigger times: 1
Loss after 294815400 batches: 0.0136
trigger times: 2
Loss after 294946500 batches: 0.0136
trigger times: 3
Loss after 295077600 batches: 0.0134
trigger times: 4
Loss after 295208700 batches: 0.0135
trigger times: 5
Loss after 295339800 batches: 0.0134
trigger times: 6
Loss after 295470900 batches: 0.0133
trigger times: 7
Loss after 295602000 batches: 0.0131
trigger times: 8
Loss after 295733100 batches: 0.0132
trigger times: 9
Loss after 295864200 batches: 0.0134
trigger times: 10
Loss after 295995300 batches: 0.0136
trigger times: 11
Loss after 296126400 batches: 0.0135
trigger times: 12
Loss after 296257500 batches: 0.0134
trigger times: 13
Loss after 296388600 batches: 0.0129
trigger times: 14
Loss after 296519700 batches: 0.0132
trigger times: 15
Loss after 296650800 batches: 0.0133
trigger times: 16
Loss after 296781900 batches: 0.0131
trigger times: 17
Loss after 296913000 batches: 0.0131
trigger times: 0
Loss after 297044100 batches: 0.0131
trigger times: 1
Loss after 297175200 batches: 0.0131
trigger times: 2
Loss after 297306300 batches: 0.0132
trigger times: 3
Loss after 297437400 batches: 0.0134
trigger times: 4
Loss after 297568500 batches: 0.0131
trigger times: 5
Loss after 297699600 batches: 0.0130
trigger times: 6
Loss after 297830700 batches: 0.0130
trigger times: 7
Loss after 297961800 batches: 0.0131
trigger times: 8
Loss after 298092900 batches: 0.0129
trigger times: 9
Loss after 298224000 batches: 0.0129
trigger times: 10
Loss after 298355100 batches: 0.0130
trigger times: 11
Loss after 298486200 batches: 0.0130
trigger times: 12
Loss after 298617300 batches: 0.0132
trigger times: 13
Loss after 298748400 batches: 0.0130
trigger times: 14
Loss after 298879500 batches: 0.0129
trigger times: 15
Loss after 299010600 batches: 0.0127
trigger times: 16
Loss after 299141700 batches: 0.0129
trigger times: 17
Loss after 299272800 batches: 0.0128
trigger times: 18
Loss after 299403900 batches: 0.0127
trigger times: 19
Loss after 299535000 batches: 0.0129
trigger times: 20
Early stopping!
Start to test process.
Loss after 299666100 batches: 0.0128
Time to train on one home:  741.6469337940216
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358, 0.014216079169119518, 0.013823527540373118, 0.014062286198334994, 0.013536151017310049, 0.01339236478051815]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175], [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912], [0.4441254701879289, 0.5205093148975452, 0.5665822904653603, 0.943887952691261, 0.39279617400695876, 22.29969119423179, 1212.5862], [0.43548785315619576, 0.5298403732534891, 0.5768369757093087, 0.9242649282254869, 0.38515222150584383, 21.836090207869006, 1188.9888], [0.4311514099438985, 0.5345118722814012, 0.5754949634807172, 0.9187582104432755, 0.3813253547865264, 21.705992026525585, 1177.175], [0.43552306294441223, 0.5297965797882871, 0.571442542482683, 0.9199573944962688, 0.3851880968755061, 21.734323179593652, 1189.0996]]
Round_12_results:  [0.43552306294441223, 0.5297965797882871, 0.571442542482683, 0.9199573944962688, 0.3851880968755061, 21.734323179593652, 1189.0996]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2362 < 2363; dropping {'Training_Loss': 0.07032329898397878, 'Validation_Loss': 0.248751699924469, 'Training_R2': 0.9291343496265881, 'Validation_R2': 0.7688051017016734, 'Training_F1': 0.8536886171304188, 'Validation_F1': 0.7384793504895499, 'Training_NEP': 0.29278177136119515, 'Validation_NEP': 0.4969925114315627, 'Training_NDE': 0.05320010160256386, 'Validation_NDE': 0.1841114297715912, 'Training_MAE': 9.69635161861766, 'Validation_MAE': 13.629596559078463, 'Training_MSE': 234.07208, 'Validation_MSE': 679.91846}.
trigger times: 0
Loss after 299797200 batches: 0.0703
trigger times: 0
Loss after 299928300 batches: 0.0182
trigger times: 1
Loss after 300059400 batches: 0.0137
trigger times: 0
Loss after 300190500 batches: 0.0122
trigger times: 0
Loss after 300321600 batches: 0.0116
trigger times: 0
Loss after 300452700 batches: 0.0109
trigger times: 1
Loss after 300583800 batches: 0.0104
trigger times: 2
Loss after 300714900 batches: 0.0101
trigger times: 0
Loss after 300846000 batches: 0.0098
trigger times: 1
Loss after 300977100 batches: 0.0096
trigger times: 0
Loss after 301108200 batches: 0.0095
trigger times: 1
Loss after 301239300 batches: 0.0093
trigger times: 2
Loss after 301370400 batches: 0.0091
trigger times: 0
Loss after 301501500 batches: 0.0088
trigger times: 1
Loss after 301632600 batches: 0.0088
trigger times: 2
Loss after 301763700 batches: 0.0088
trigger times: 3
Loss after 301894800 batches: 0.0084
trigger times: 4
Loss after 302025900 batches: 0.0086
trigger times: 5
Loss after 302157000 batches: 0.0085
trigger times: 6
Loss after 302288100 batches: 0.0084
trigger times: 7
Loss after 302419200 batches: 0.0084
trigger times: 8
Loss after 302550300 batches: 0.0082
trigger times: 9
Loss after 302681400 batches: 0.0080
trigger times: 10
Loss after 302812500 batches: 0.0079
trigger times: 11
Loss after 302943600 batches: 0.0082
trigger times: 12
Loss after 303074700 batches: 0.0080
trigger times: 13
Loss after 303205800 batches: 0.0079
trigger times: 14
Loss after 303336900 batches: 0.0080
trigger times: 15
Loss after 303468000 batches: 0.0078
trigger times: 16
Loss after 303599100 batches: 0.0079
trigger times: 17
Loss after 303730200 batches: 0.0080
trigger times: 18
Loss after 303861300 batches: 0.0079
trigger times: 19
Loss after 303992400 batches: 0.0077
trigger times: 0
Loss after 304123500 batches: 0.0077
trigger times: 1
Loss after 304254600 batches: 0.0076
trigger times: 2
Loss after 304385700 batches: 0.0075
trigger times: 3
Loss after 304516800 batches: 0.0076
trigger times: 4
Loss after 304647900 batches: 0.0073
trigger times: 5
Loss after 304779000 batches: 0.0075
trigger times: 6
Loss after 304910100 batches: 0.0074
trigger times: 7
Loss after 305041200 batches: 0.0074
trigger times: 8
Loss after 305172300 batches: 0.0073
trigger times: 9
Loss after 305303400 batches: 0.0074
trigger times: 10
Loss after 305434500 batches: 0.0074
trigger times: 11
Loss after 305565600 batches: 0.0071
trigger times: 12
Loss after 305696700 batches: 0.0072
trigger times: 13
Loss after 305827800 batches: 0.0072
trigger times: 14
Loss after 305958900 batches: 0.0070
trigger times: 15
Loss after 306090000 batches: 0.0070
trigger times: 16
Loss after 306221100 batches: 0.0069
trigger times: 17
Loss after 306352200 batches: 0.0071
trigger times: 18
Loss after 306483300 batches: 0.0071
trigger times: 19
Loss after 306614400 batches: 0.0070
trigger times: 20
Early stopping!
Start to test process.
Loss after 306745500 batches: 0.0070
Time to train on one home:  394.63306188583374
trigger times: 0
Loss after 306848100 batches: 0.1391
trigger times: 0
Loss after 306950700 batches: 0.0449
trigger times: 0
Loss after 307053300 batches: 0.0343
trigger times: 0
Loss after 307155900 batches: 0.0304
trigger times: 1
Loss after 307258500 batches: 0.0277
trigger times: 0
Loss after 307361100 batches: 0.0272
trigger times: 1
Loss after 307463700 batches: 0.0279
trigger times: 2
Loss after 307566300 batches: 0.0269
trigger times: 3
Loss after 307668900 batches: 0.0244
trigger times: 4
Loss after 307771500 batches: 0.0231
trigger times: 5
Loss after 307874100 batches: 0.0234
trigger times: 6
Loss after 307976700 batches: 0.0234
trigger times: 7
Loss after 308079300 batches: 0.0254
trigger times: 8
Loss after 308181900 batches: 0.0219
trigger times: 9
Loss after 308284500 batches: 0.0211
trigger times: 10
Loss after 308387100 batches: 0.0204
trigger times: 11
Loss after 308489700 batches: 0.0208
trigger times: 12
Loss after 308592300 batches: 0.0233
trigger times: 13
Loss after 308694900 batches: 0.0215
trigger times: 14
Loss after 308797500 batches: 0.0215
trigger times: 15
Loss after 308900100 batches: 0.0202
trigger times: 16
Loss after 309002700 batches: 0.0195
trigger times: 17
Loss after 309105300 batches: 0.0199
trigger times: 18
Loss after 309207900 batches: 0.0188
trigger times: 19
Loss after 309310500 batches: 0.0218
trigger times: 20
Early stopping!
Start to test process.
Loss after 309413100 batches: 0.0215
Time to train on one home:  160.2319700717926
trigger times: 0
Loss after 309544200 batches: 0.0809
trigger times: 1
Loss after 309675300 batches: 0.0261
trigger times: 2
Loss after 309806400 batches: 0.0210
trigger times: 3
Loss after 309937500 batches: 0.0193
trigger times: 4
Loss after 310068600 batches: 0.0183
trigger times: 5
Loss after 310199700 batches: 0.0175
trigger times: 6
Loss after 310330800 batches: 0.0170
trigger times: 7
Loss after 310461900 batches: 0.0165
trigger times: 8
Loss after 310593000 batches: 0.0161
trigger times: 9
Loss after 310724100 batches: 0.0157
trigger times: 10
Loss after 310855200 batches: 0.0153
trigger times: 11
Loss after 310986300 batches: 0.0154
trigger times: 12
Loss after 311117400 batches: 0.0150
trigger times: 13
Loss after 311248500 batches: 0.0147
trigger times: 14
Loss after 311379600 batches: 0.0146
trigger times: 15
Loss after 311510700 batches: 0.0143
trigger times: 16
Loss after 311641800 batches: 0.0140
trigger times: 17
Loss after 311772900 batches: 0.0138
trigger times: 18
Loss after 311904000 batches: 0.0136
trigger times: 19
Loss after 312035100 batches: 0.0138
trigger times: 20
Early stopping!
Start to test process.
Loss after 312166200 batches: 0.0136
Time to train on one home:  160.74679112434387
trigger times: 0
Loss after 312297300 batches: 0.0806
trigger times: 0
Loss after 312428400 batches: 0.0249
trigger times: 0
Loss after 312559500 batches: 0.0206
trigger times: 0
Loss after 312690600 batches: 0.0187
trigger times: 0
Loss after 312821700 batches: 0.0182
trigger times: 0
Loss after 312952800 batches: 0.0176
trigger times: 1
Loss after 313083900 batches: 0.0173
trigger times: 2
Loss after 313215000 batches: 0.0169
trigger times: 0
Loss after 313346100 batches: 0.0166
trigger times: 1
Loss after 313477200 batches: 0.0162
trigger times: 2
Loss after 313608300 batches: 0.0163
trigger times: 3
Loss after 313739400 batches: 0.0162
trigger times: 4
Loss after 313870500 batches: 0.0160
trigger times: 5
Loss after 314001600 batches: 0.0154
trigger times: 6
Loss after 314132700 batches: 0.0156
trigger times: 7
Loss after 314263800 batches: 0.0154
trigger times: 8
Loss after 314394900 batches: 0.0152
trigger times: 9
Loss after 314526000 batches: 0.0152
trigger times: 10
Loss after 314657100 batches: 0.0151
trigger times: 11
Loss after 314788200 batches: 0.0149
trigger times: 12
Loss after 314919300 batches: 0.0149
trigger times: 13
Loss after 315050400 batches: 0.0148
trigger times: 14
Loss after 315181500 batches: 0.0149
trigger times: 15
Loss after 315312600 batches: 0.0148
trigger times: 16
Loss after 315443700 batches: 0.0145
trigger times: 17
Loss after 315574800 batches: 0.0145
trigger times: 18
Loss after 315705900 batches: 0.0148
trigger times: 19
Loss after 315837000 batches: 0.0144
trigger times: 20
Early stopping!
Start to test process.
Loss after 315968100 batches: 0.0143
Time to train on one home:  217.46716833114624
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358, 0.014216079169119518, 0.013823527540373118, 0.014062286198334994, 0.013536151017310049, 0.01339236478051815, 0.01412220146439665]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175], [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912], [0.4441254701879289, 0.5205093148975452, 0.5665822904653603, 0.943887952691261, 0.39279617400695876, 22.29969119423179, 1212.5862], [0.43548785315619576, 0.5298403732534891, 0.5768369757093087, 0.9242649282254869, 0.38515222150584383, 21.836090207869006, 1188.9888], [0.4311514099438985, 0.5345118722814012, 0.5754949634807172, 0.9187582104432755, 0.3813253547865264, 21.705992026525585, 1177.175], [0.43552306294441223, 0.5297965797882871, 0.571442542482683, 0.9199573944962688, 0.3851880968755061, 21.734323179593652, 1189.0996], [0.4262552526262071, 0.5398158400379639, 0.5797626089679692, 0.9012375911472185, 0.3769803731079171, 21.292061115849403, 1163.762]]
Round_13_results:  [0.4262552526262071, 0.5398158400379639, 0.5797626089679692, 0.9012375911472185, 0.3769803731079171, 21.292061115849403, 1163.762]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2492 < 2493; dropping {'Training_Loss': 0.047480294902650814, 'Validation_Loss': 0.2321892256538073, 'Training_R2': 0.9521876993434677, 'Validation_R2': 0.784217465348396, 'Training_F1': 0.8785156108475406, 'Validation_F1': 0.7485832939516773, 'Training_NEP': 0.24297346245514906, 'Validation_NEP': 0.4842015426153205, 'Training_NDE': 0.03589354277251067, 'Validation_NDE': 0.17183783581236708, 'Training_MAE': 8.046799208177672, 'Validation_MAE': 13.278815127658161, 'Training_MSE': 157.92596, 'Validation_MSE': 634.5924}.
trigger times: 0
Loss after 316099200 batches: 0.0475
trigger times: 0
Loss after 316230300 batches: 0.0156
trigger times: 0
Loss after 316361400 batches: 0.0123
trigger times: 0
Loss after 316492500 batches: 0.0110
trigger times: 1
Loss after 316623600 batches: 0.0105
trigger times: 0
Loss after 316754700 batches: 0.0101
trigger times: 1
Loss after 316885800 batches: 0.0098
trigger times: 2
Loss after 317016900 batches: 0.0094
trigger times: 3
Loss after 317148000 batches: 0.0091
trigger times: 4
Loss after 317279100 batches: 0.0091
trigger times: 5
Loss after 317410200 batches: 0.0089
trigger times: 6
Loss after 317541300 batches: 0.0088
trigger times: 7
Loss after 317672400 batches: 0.0085
trigger times: 8
Loss after 317803500 batches: 0.0087
trigger times: 9
Loss after 317934600 batches: 0.0086
trigger times: 10
Loss after 318065700 batches: 0.0082
trigger times: 11
Loss after 318196800 batches: 0.0083
trigger times: 12
Loss after 318327900 batches: 0.0082
trigger times: 13
Loss after 318459000 batches: 0.0081
trigger times: 14
Loss after 318590100 batches: 0.0079
trigger times: 15
Loss after 318721200 batches: 0.0079
trigger times: 16
Loss after 318852300 batches: 0.0080
trigger times: 17
Loss after 318983400 batches: 0.0077
trigger times: 18
Loss after 319114500 batches: 0.0077
trigger times: 19
Loss after 319245600 batches: 0.0079
trigger times: 20
Early stopping!
Start to test process.
Loss after 319376700 batches: 0.0077
Time to train on one home:  196.4187490940094
trigger times: 0
Loss after 319479300 batches: 0.1207
trigger times: 0
Loss after 319581900 batches: 0.0390
trigger times: 0
Loss after 319684500 batches: 0.0317
trigger times: 1
Loss after 319787100 batches: 0.0296
trigger times: 2
Loss after 319889700 batches: 0.0298
trigger times: 3
Loss after 319992300 batches: 0.0265
trigger times: 4
Loss after 320094900 batches: 0.0247
trigger times: 5
Loss after 320197500 batches: 0.0241
trigger times: 6
Loss after 320300100 batches: 0.0235
trigger times: 0
Loss after 320402700 batches: 0.0239
trigger times: 1
Loss after 320505300 batches: 0.0236
trigger times: 2
Loss after 320607900 batches: 0.0226
trigger times: 3
Loss after 320710500 batches: 0.0209
trigger times: 4
Loss after 320813100 batches: 0.0208
trigger times: 5
Loss after 320915700 batches: 0.0205
trigger times: 6
Loss after 321018300 batches: 0.0202
trigger times: 7
Loss after 321120900 batches: 0.0196
trigger times: 0
Loss after 321223500 batches: 0.0198
trigger times: 1
Loss after 321326100 batches: 0.0195
trigger times: 2
Loss after 321428700 batches: 0.0196
trigger times: 3
Loss after 321531300 batches: 0.0218
trigger times: 0
Loss after 321633900 batches: 0.0238
trigger times: 1
Loss after 321736500 batches: 0.0206
trigger times: 2
Loss after 321839100 batches: 0.0226
trigger times: 3
Loss after 321941700 batches: 0.0199
trigger times: 4
Loss after 322044300 batches: 0.0204
trigger times: 5
Loss after 322146900 batches: 0.0201
trigger times: 6
Loss after 322249500 batches: 0.0208
trigger times: 7
Loss after 322352100 batches: 0.0185
trigger times: 8
Loss after 322454700 batches: 0.0177
trigger times: 9
Loss after 322557300 batches: 0.0177
trigger times: 10
Loss after 322659900 batches: 0.0182
trigger times: 11
Loss after 322762500 batches: 0.0169
trigger times: 12
Loss after 322865100 batches: 0.0176
trigger times: 13
Loss after 322967700 batches: 0.0189
trigger times: 14
Loss after 323070300 batches: 0.0173
trigger times: 15
Loss after 323172900 batches: 0.0166
trigger times: 16
Loss after 323275500 batches: 0.0166
trigger times: 17
Loss after 323378100 batches: 0.0186
trigger times: 18
Loss after 323480700 batches: 0.0168
trigger times: 19
Loss after 323583300 batches: 0.0173
trigger times: 20
Early stopping!
Start to test process.
Loss after 323685900 batches: 0.0175
Time to train on one home:  251.09289479255676
trigger times: 0
Loss after 323817000 batches: 0.0608
trigger times: 0
Loss after 323948100 batches: 0.0241
trigger times: 0
Loss after 324079200 batches: 0.0204
trigger times: 1
Loss after 324210300 batches: 0.0187
trigger times: 2
Loss after 324341400 batches: 0.0183
trigger times: 3
Loss after 324472500 batches: 0.0175
trigger times: 4
Loss after 324603600 batches: 0.0168
trigger times: 5
Loss after 324734700 batches: 0.0164
trigger times: 6
Loss after 324865800 batches: 0.0160
trigger times: 7
Loss after 324996900 batches: 0.0153
trigger times: 8
Loss after 325128000 batches: 0.0153
trigger times: 9
Loss after 325259100 batches: 0.0151
trigger times: 10
Loss after 325390200 batches: 0.0148
trigger times: 11
Loss after 325521300 batches: 0.0146
trigger times: 12
Loss after 325652400 batches: 0.0144
trigger times: 13
Loss after 325783500 batches: 0.0140
trigger times: 14
Loss after 325914600 batches: 0.0141
trigger times: 15
Loss after 326045700 batches: 0.0139
trigger times: 16
Loss after 326176800 batches: 0.0136
trigger times: 17
Loss after 326307900 batches: 0.0136
trigger times: 18
Loss after 326439000 batches: 0.0136
trigger times: 19
Loss after 326570100 batches: 0.0133
trigger times: 20
Early stopping!
Start to test process.
Loss after 326701200 batches: 0.0134
Time to train on one home:  174.5937840938568
trigger times: 0
Loss after 326832300 batches: 0.0769
trigger times: 0
Loss after 326963400 batches: 0.0241
trigger times: 0
Loss after 327094500 batches: 0.0205
trigger times: 0
Loss after 327225600 batches: 0.0191
trigger times: 1
Loss after 327356700 batches: 0.0181
trigger times: 0
Loss after 327487800 batches: 0.0175
trigger times: 1
Loss after 327618900 batches: 0.0172
trigger times: 2
Loss after 327750000 batches: 0.0168
trigger times: 0
Loss after 327881100 batches: 0.0166
trigger times: 1
Loss after 328012200 batches: 0.0164
trigger times: 0
Loss after 328143300 batches: 0.0160
trigger times: 1
Loss after 328274400 batches: 0.0159
trigger times: 2
Loss after 328405500 batches: 0.0158
trigger times: 3
Loss after 328536600 batches: 0.0158
trigger times: 4
Loss after 328667700 batches: 0.0155
trigger times: 5
Loss after 328798800 batches: 0.0154
trigger times: 6
Loss after 328929900 batches: 0.0151
trigger times: 7
Loss after 329061000 batches: 0.0152
trigger times: 8
Loss after 329192100 batches: 0.0153
trigger times: 9
Loss after 329323200 batches: 0.0151
trigger times: 10
Loss after 329454300 batches: 0.0150
trigger times: 11
Loss after 329585400 batches: 0.0147
trigger times: 12
Loss after 329716500 batches: 0.0147
trigger times: 13
Loss after 329847600 batches: 0.0151
trigger times: 14
Loss after 329978700 batches: 0.0148
trigger times: 15
Loss after 330109800 batches: 0.0149
trigger times: 0
Loss after 330240900 batches: 0.0144
trigger times: 0
Loss after 330372000 batches: 0.0145
trigger times: 1
Loss after 330503100 batches: 0.0145
trigger times: 2
Loss after 330634200 batches: 0.0142
trigger times: 3
Loss after 330765300 batches: 0.0146
trigger times: 4
Loss after 330896400 batches: 0.0143
trigger times: 5
Loss after 331027500 batches: 0.0142
trigger times: 6
Loss after 331158600 batches: 0.0142
trigger times: 7
Loss after 331289700 batches: 0.0143
trigger times: 8
Loss after 331420800 batches: 0.0143
trigger times: 9
Loss after 331551900 batches: 0.0142
trigger times: 10
Loss after 331683000 batches: 0.0142
trigger times: 11
Loss after 331814100 batches: 0.0141
trigger times: 12
Loss after 331945200 batches: 0.0142
trigger times: 13
Loss after 332076300 batches: 0.0140
trigger times: 14
Loss after 332207400 batches: 0.0139
trigger times: 15
Loss after 332338500 batches: 0.0138
trigger times: 16
Loss after 332469600 batches: 0.0140
trigger times: 17
Loss after 332600700 batches: 0.0139
trigger times: 18
Loss after 332731800 batches: 0.0137
trigger times: 19
Loss after 332862900 batches: 0.0136
trigger times: 20
Early stopping!
Start to test process.
Loss after 332994000 batches: 0.0138
Time to train on one home:  352.7843723297119
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358, 0.014216079169119518, 0.013823527540373118, 0.014062286198334994, 0.013536151017310049, 0.01339236478051815, 0.01412220146439665, 0.013110444862403894]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175], [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912], [0.4441254701879289, 0.5205093148975452, 0.5665822904653603, 0.943887952691261, 0.39279617400695876, 22.29969119423179, 1212.5862], [0.43548785315619576, 0.5298403732534891, 0.5768369757093087, 0.9242649282254869, 0.38515222150584383, 21.836090207869006, 1188.9888], [0.4311514099438985, 0.5345118722814012, 0.5754949634807172, 0.9187582104432755, 0.3813253547865264, 21.705992026525585, 1177.175], [0.43552306294441223, 0.5297965797882871, 0.571442542482683, 0.9199573944962688, 0.3851880968755061, 21.734323179593652, 1189.0996], [0.4262552526262071, 0.5398158400379639, 0.5797626089679692, 0.9012375911472185, 0.3769803731079171, 21.292061115849403, 1163.762], [0.4279141707552804, 0.5380279725473285, 0.5824082127012885, 0.9113846643500105, 0.3784449844794665, 21.531789357217836, 1168.2832]]
Round_14_results:  [0.4279141707552804, 0.5380279725473285, 0.5824082127012885, 0.9113846643500105, 0.3784449844794665, 21.531789357217836, 1168.2832]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2631 < 2632; dropping {'Training_Loss': 0.051878656313385604, 'Validation_Loss': 0.23876437462038463, 'Training_R2': 0.9477789893323596, 'Validation_R2': 0.7781350270354778, 'Training_F1': 0.873657218126074, 'Validation_F1': 0.7502241082585261, 'Training_NEP': 0.25261224615267214, 'Validation_NEP': 0.47993076466086904, 'Training_NDE': 0.03920323963257361, 'Validation_NDE': 0.17668156905445576, 'Training_MAE': 8.366016608470273, 'Validation_MAE': 13.161692677774736, 'Training_MSE': 172.4881, 'Validation_MSE': 652.48016}.
trigger times: 0
Loss after 333125100 batches: 0.0519
trigger times: 0
Loss after 333256200 batches: 0.0154
trigger times: 0
Loss after 333387300 batches: 0.0125
trigger times: 1
Loss after 333518400 batches: 0.0112
trigger times: 0
Loss after 333649500 batches: 0.0106
trigger times: 0
Loss after 333780600 batches: 0.0101
trigger times: 1
Loss after 333911700 batches: 0.0098
trigger times: 2
Loss after 334042800 batches: 0.0096
trigger times: 3
Loss after 334173900 batches: 0.0092
trigger times: 4
Loss after 334305000 batches: 0.0091
trigger times: 5
Loss after 334436100 batches: 0.0088
trigger times: 6
Loss after 334567200 batches: 0.0087
trigger times: 7
Loss after 334698300 batches: 0.0087
trigger times: 8
Loss after 334829400 batches: 0.0087
trigger times: 9
Loss after 334960500 batches: 0.0084
trigger times: 0
Loss after 335091600 batches: 0.0083
trigger times: 1
Loss after 335222700 batches: 0.0082
trigger times: 2
Loss after 335353800 batches: 0.0083
trigger times: 3
Loss after 335484900 batches: 0.0079
trigger times: 0
Loss after 335616000 batches: 0.0081
trigger times: 1
Loss after 335747100 batches: 0.0081
trigger times: 2
Loss after 335878200 batches: 0.0079
trigger times: 0
Loss after 336009300 batches: 0.0079
trigger times: 1
Loss after 336140400 batches: 0.0078
trigger times: 2
Loss after 336271500 batches: 0.0077
trigger times: 3
Loss after 336402600 batches: 0.0076
trigger times: 4
Loss after 336533700 batches: 0.0075
trigger times: 5
Loss after 336664800 batches: 0.0076
trigger times: 6
Loss after 336795900 batches: 0.0076
trigger times: 7
Loss after 336927000 batches: 0.0074
trigger times: 8
Loss after 337058100 batches: 0.0075
trigger times: 9
Loss after 337189200 batches: 0.0073
trigger times: 10
Loss after 337320300 batches: 0.0073
trigger times: 11
Loss after 337451400 batches: 0.0073
trigger times: 12
Loss after 337582500 batches: 0.0073
trigger times: 13
Loss after 337713600 batches: 0.0073
trigger times: 14
Loss after 337844700 batches: 0.0074
trigger times: 15
Loss after 337975800 batches: 0.0073
trigger times: 16
Loss after 338106900 batches: 0.0072
trigger times: 17
Loss after 338238000 batches: 0.0073
trigger times: 18
Loss after 338369100 batches: 0.0071
trigger times: 19
Loss after 338500200 batches: 0.0072
trigger times: 20
Early stopping!
Start to test process.
Loss after 338631300 batches: 0.0069
Time to train on one home:  317.0523045063019
trigger times: 0
Loss after 338733900 batches: 0.1116
trigger times: 0
Loss after 338836500 batches: 0.0364
trigger times: 1
Loss after 338939100 batches: 0.0296
trigger times: 2
Loss after 339041700 batches: 0.0263
trigger times: 3
Loss after 339144300 batches: 0.0247
trigger times: 4
Loss after 339246900 batches: 0.0230
trigger times: 5
Loss after 339349500 batches: 0.0231
trigger times: 6
Loss after 339452100 batches: 0.0222
trigger times: 7
Loss after 339554700 batches: 0.0220
trigger times: 8
Loss after 339657300 batches: 0.0225
trigger times: 9
Loss after 339759900 batches: 0.0215
trigger times: 10
Loss after 339862500 batches: 0.0213
trigger times: 11
Loss after 339965100 batches: 0.0240
trigger times: 0
Loss after 340067700 batches: 0.0248
trigger times: 1
Loss after 340170300 batches: 0.0221
trigger times: 2
Loss after 340272900 batches: 0.0206
trigger times: 3
Loss after 340375500 batches: 0.0196
trigger times: 4
Loss after 340478100 batches: 0.0189
trigger times: 5
Loss after 340580700 batches: 0.0188
trigger times: 6
Loss after 340683300 batches: 0.0195
trigger times: 7
Loss after 340785900 batches: 0.0183
trigger times: 8
Loss after 340888500 batches: 0.0185
trigger times: 9
Loss after 340991100 batches: 0.0177
trigger times: 10
Loss after 341093700 batches: 0.0187
trigger times: 11
Loss after 341196300 batches: 0.0198
trigger times: 12
Loss after 341298900 batches: 0.0202
trigger times: 13
Loss after 341401500 batches: 0.0180
trigger times: 14
Loss after 341504100 batches: 0.0176
trigger times: 15
Loss after 341606700 batches: 0.0186
trigger times: 16
Loss after 341709300 batches: 0.0173
trigger times: 17
Loss after 341811900 batches: 0.0169
trigger times: 18
Loss after 341914500 batches: 0.0162
trigger times: 19
Loss after 342017100 batches: 0.0174
trigger times: 20
Early stopping!
Start to test process.
Loss after 342119700 batches: 0.0183
Time to train on one home:  205.24379086494446
trigger times: 0
Loss after 342250800 batches: 0.0635
trigger times: 1
Loss after 342381900 batches: 0.0237
trigger times: 2
Loss after 342513000 batches: 0.0198
trigger times: 3
Loss after 342644100 batches: 0.0185
trigger times: 4
Loss after 342775200 batches: 0.0176
trigger times: 5
Loss after 342906300 batches: 0.0170
trigger times: 6
Loss after 343037400 batches: 0.0165
trigger times: 0
Loss after 343168500 batches: 0.0161
trigger times: 1
Loss after 343299600 batches: 0.0157
trigger times: 2
Loss after 343430700 batches: 0.0153
trigger times: 3
Loss after 343561800 batches: 0.0150
trigger times: 4
Loss after 343692900 batches: 0.0148
trigger times: 5
Loss after 343824000 batches: 0.0146
trigger times: 6
Loss after 343955100 batches: 0.0144
trigger times: 7
Loss after 344086200 batches: 0.0141
trigger times: 8
Loss after 344217300 batches: 0.0141
trigger times: 9
Loss after 344348400 batches: 0.0138
trigger times: 10
Loss after 344479500 batches: 0.0138
trigger times: 11
Loss after 344610600 batches: 0.0137
trigger times: 12
Loss after 344741700 batches: 0.0134
trigger times: 13
Loss after 344872800 batches: 0.0132
trigger times: 14
Loss after 345003900 batches: 0.0133
trigger times: 15
Loss after 345135000 batches: 0.0131
trigger times: 16
Loss after 345266100 batches: 0.0129
trigger times: 17
Loss after 345397200 batches: 0.0128
trigger times: 18
Loss after 345528300 batches: 0.0128
trigger times: 19
Loss after 345659400 batches: 0.0126
trigger times: 20
Early stopping!
Start to test process.
Loss after 345790500 batches: 0.0124
Time to train on one home:  210.09993600845337
trigger times: 0
Loss after 345921600 batches: 0.0774
trigger times: 0
Loss after 346052700 batches: 0.0233
trigger times: 0
Loss after 346183800 batches: 0.0197
trigger times: 0
Loss after 346314900 batches: 0.0186
trigger times: 1
Loss after 346446000 batches: 0.0179
trigger times: 2
Loss after 346577100 batches: 0.0173
trigger times: 0
Loss after 346708200 batches: 0.0170
trigger times: 1
Loss after 346839300 batches: 0.0167
trigger times: 2
Loss after 346970400 batches: 0.0165
trigger times: 3
Loss after 347101500 batches: 0.0160
trigger times: 4
Loss after 347232600 batches: 0.0159
trigger times: 5
Loss after 347363700 batches: 0.0157
trigger times: 6
Loss after 347494800 batches: 0.0158
trigger times: 0
Loss after 347625900 batches: 0.0157
trigger times: 1
Loss after 347757000 batches: 0.0153
trigger times: 2
Loss after 347888100 batches: 0.0153
trigger times: 3
Loss after 348019200 batches: 0.0153
trigger times: 4
Loss after 348150300 batches: 0.0153
trigger times: 5
Loss after 348281400 batches: 0.0151
trigger times: 6
Loss after 348412500 batches: 0.0150
trigger times: 7
Loss after 348543600 batches: 0.0149
trigger times: 8
Loss after 348674700 batches: 0.0147
trigger times: 9
Loss after 348805800 batches: 0.0147
trigger times: 10
Loss after 348936900 batches: 0.0145
trigger times: 0
Loss after 349068000 batches: 0.0144
trigger times: 0
Loss after 349199100 batches: 0.0144
trigger times: 1
Loss after 349330200 batches: 0.0145
trigger times: 2
Loss after 349461300 batches: 0.0143
trigger times: 3
Loss after 349592400 batches: 0.0144
trigger times: 4
Loss after 349723500 batches: 0.0143
trigger times: 5
Loss after 349854600 batches: 0.0143
trigger times: 6
Loss after 349985700 batches: 0.0143
trigger times: 7
Loss after 350116800 batches: 0.0142
trigger times: 8
Loss after 350247900 batches: 0.0141
trigger times: 9
Loss after 350379000 batches: 0.0139
trigger times: 10
Loss after 350510100 batches: 0.0139
trigger times: 11
Loss after 350641200 batches: 0.0141
trigger times: 12
Loss after 350772300 batches: 0.0140
trigger times: 13
Loss after 350903400 batches: 0.0141
trigger times: 0
Loss after 351034500 batches: 0.0137
trigger times: 1
Loss after 351165600 batches: 0.0136
trigger times: 2
Loss after 351296700 batches: 0.0137
trigger times: 3
Loss after 351427800 batches: 0.0137
trigger times: 4
Loss after 351558900 batches: 0.0138
trigger times: 5
Loss after 351690000 batches: 0.0134
trigger times: 6
Loss after 351821100 batches: 0.0136
trigger times: 7
Loss after 351952200 batches: 0.0137
trigger times: 8
Loss after 352083300 batches: 0.0135
trigger times: 0
Loss after 352214400 batches: 0.0135
trigger times: 1
Loss after 352345500 batches: 0.0135
trigger times: 2
Loss after 352476600 batches: 0.0134
trigger times: 3
Loss after 352607700 batches: 0.0133
trigger times: 4
Loss after 352738800 batches: 0.0134
trigger times: 5
Loss after 352869900 batches: 0.0132
trigger times: 6
Loss after 353001000 batches: 0.0133
trigger times: 7
Loss after 353132100 batches: 0.0134
trigger times: 8
Loss after 353263200 batches: 0.0132
trigger times: 9
Loss after 353394300 batches: 0.0131
trigger times: 10
Loss after 353525400 batches: 0.0133
trigger times: 11
Loss after 353656500 batches: 0.0133
trigger times: 12
Loss after 353787600 batches: 0.0130
trigger times: 13
Loss after 353918700 batches: 0.0131
trigger times: 14
Loss after 354049800 batches: 0.0128
trigger times: 15
Loss after 354180900 batches: 0.0130
trigger times: 16
Loss after 354312000 batches: 0.0129
trigger times: 17
Loss after 354443100 batches: 0.0129
trigger times: 0
Loss after 354574200 batches: 0.0130
trigger times: 1
Loss after 354705300 batches: 0.0128
trigger times: 2
Loss after 354836400 batches: 0.0128
trigger times: 3
Loss after 354967500 batches: 0.0129
trigger times: 4
Loss after 355098600 batches: 0.0130
trigger times: 5
Loss after 355229700 batches: 0.0129
trigger times: 6
Loss after 355360800 batches: 0.0129
trigger times: 7
Loss after 355491900 batches: 0.0127
trigger times: 8
Loss after 355623000 batches: 0.0131
trigger times: 9
Loss after 355754100 batches: 0.0129
trigger times: 10
Loss after 355885200 batches: 0.0126
trigger times: 0
Loss after 356016300 batches: 0.0127
trigger times: 1
Loss after 356147400 batches: 0.0127
trigger times: 2
Loss after 356278500 batches: 0.0126
trigger times: 3
Loss after 356409600 batches: 0.0127
trigger times: 4
Loss after 356540700 batches: 0.0125
trigger times: 5
Loss after 356671800 batches: 0.0127
trigger times: 6
Loss after 356802900 batches: 0.0128
trigger times: 7
Loss after 356934000 batches: 0.0124
trigger times: 8
Loss after 357065100 batches: 0.0126
trigger times: 9
Loss after 357196200 batches: 0.0127
trigger times: 10
Loss after 357327300 batches: 0.0125
trigger times: 11
Loss after 357458400 batches: 0.0126
trigger times: 12
Loss after 357589500 batches: 0.0125
trigger times: 13
Loss after 357720600 batches: 0.0127
trigger times: 14
Loss after 357851700 batches: 0.0124
trigger times: 15
Loss after 357982800 batches: 0.0126
trigger times: 16
Loss after 358113900 batches: 0.0123
trigger times: 17
Loss after 358245000 batches: 0.0124
trigger times: 18
Loss after 358376100 batches: 0.0126
trigger times: 19
Loss after 358507200 batches: 0.0124
trigger times: 20
Early stopping!
Start to test process.
Loss after 358638300 batches: 0.0125
Time to train on one home:  707.1631305217743
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358, 0.014216079169119518, 0.013823527540373118, 0.014062286198334994, 0.013536151017310049, 0.01339236478051815, 0.01412220146439665, 0.013110444862403894, 0.012553285140885578]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175], [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912], [0.4441254701879289, 0.5205093148975452, 0.5665822904653603, 0.943887952691261, 0.39279617400695876, 22.29969119423179, 1212.5862], [0.43548785315619576, 0.5298403732534891, 0.5768369757093087, 0.9242649282254869, 0.38515222150584383, 21.836090207869006, 1188.9888], [0.4311514099438985, 0.5345118722814012, 0.5754949634807172, 0.9187582104432755, 0.3813253547865264, 21.705992026525585, 1177.175], [0.43552306294441223, 0.5297965797882871, 0.571442542482683, 0.9199573944962688, 0.3851880968755061, 21.734323179593652, 1189.0996], [0.4262552526262071, 0.5398158400379639, 0.5797626089679692, 0.9012375911472185, 0.3769803731079171, 21.292061115849403, 1163.762], [0.4279141707552804, 0.5380279725473285, 0.5824082127012885, 0.9113846643500105, 0.3784449844794665, 21.531789357217836, 1168.2832], [0.4270105991098616, 0.5390387342494132, 0.5791757731347951, 0.9101790603926071, 0.37761697396383614, 21.503306531607038, 1165.727]]
Round_15_results:  [0.4270105991098616, 0.5390387342494132, 0.5791757731347951, 0.9101790603926071, 0.37761697396383614, 21.503306531607038, 1165.727]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2834 < 2835; dropping {'Training_Loss': 0.05530854001781851, 'Validation_Loss': 0.2446476055516137, 'Training_R2': 0.9443049016298933, 'Validation_R2': 0.7725902863748935, 'Training_F1': 0.8710258282967999, 'Validation_F1': 0.758161961861086, 'Training_NEP': 0.25804379597341937, 'Validation_NEP': 0.4784084010505146, 'Training_NDE': 0.04181129893596726, 'Validation_NDE': 0.18109710823047834, 'Training_MAE': 8.54589876660857, 'Validation_MAE': 13.11994315167909, 'Training_MSE': 183.96317, 'Validation_MSE': 668.7866}.
trigger times: 0
Loss after 358769400 batches: 0.0553
trigger times: 0
Loss after 358900500 batches: 0.0158
trigger times: 1
Loss after 359031600 batches: 0.0124
trigger times: 0
Loss after 359162700 batches: 0.0111
trigger times: 1
Loss after 359293800 batches: 0.0105
trigger times: 2
Loss after 359424900 batches: 0.0100
trigger times: 3
Loss after 359556000 batches: 0.0096
trigger times: 4
Loss after 359687100 batches: 0.0095
trigger times: 5
Loss after 359818200 batches: 0.0091
trigger times: 6
Loss after 359949300 batches: 0.0090
trigger times: 7
Loss after 360080400 batches: 0.0088
trigger times: 8
Loss after 360211500 batches: 0.0087
trigger times: 9
Loss after 360342600 batches: 0.0085
trigger times: 10
Loss after 360473700 batches: 0.0084
trigger times: 11
Loss after 360604800 batches: 0.0084
trigger times: 12
Loss after 360735900 batches: 0.0083
trigger times: 13
Loss after 360867000 batches: 0.0082
trigger times: 0
Loss after 360998100 batches: 0.0082
trigger times: 0
Loss after 361129200 batches: 0.0080
trigger times: 0
Loss after 361260300 batches: 0.0079
trigger times: 1
Loss after 361391400 batches: 0.0080
trigger times: 0
Loss after 361522500 batches: 0.0078
trigger times: 1
Loss after 361653600 batches: 0.0077
trigger times: 2
Loss after 361784700 batches: 0.0077
trigger times: 3
Loss after 361915800 batches: 0.0077
trigger times: 4
Loss after 362046900 batches: 0.0078
trigger times: 5
Loss after 362178000 batches: 0.0078
trigger times: 6
Loss after 362309100 batches: 0.0075
trigger times: 7
Loss after 362440200 batches: 0.0074
trigger times: 8
Loss after 362571300 batches: 0.0074
trigger times: 9
Loss after 362702400 batches: 0.0072
trigger times: 10
Loss after 362833500 batches: 0.0074
trigger times: 11
Loss after 362964600 batches: 0.0074
trigger times: 12
Loss after 363095700 batches: 0.0077
trigger times: 13
Loss after 363226800 batches: 0.0073
trigger times: 14
Loss after 363357900 batches: 0.0074
trigger times: 15
Loss after 363489000 batches: 0.0072
trigger times: 16
Loss after 363620100 batches: 0.0071
trigger times: 17
Loss after 363751200 batches: 0.0072
trigger times: 18
Loss after 363882300 batches: 0.0072
trigger times: 19
Loss after 364013400 batches: 0.0070
trigger times: 20
Early stopping!
Start to test process.
Loss after 364144500 batches: 0.0072
Time to train on one home:  309.75964069366455
trigger times: 0
Loss after 364247100 batches: 0.1119
trigger times: 0
Loss after 364349700 batches: 0.0359
trigger times: 1
Loss after 364452300 batches: 0.0295
trigger times: 2
Loss after 364554900 batches: 0.0267
trigger times: 3
Loss after 364657500 batches: 0.0245
trigger times: 4
Loss after 364760100 batches: 0.0237
trigger times: 5
Loss after 364862700 batches: 0.0236
trigger times: 6
Loss after 364965300 batches: 0.0240
trigger times: 7
Loss after 365067900 batches: 0.0212
trigger times: 8
Loss after 365170500 batches: 0.0212
trigger times: 9
Loss after 365273100 batches: 0.0210
trigger times: 10
Loss after 365375700 batches: 0.0201
trigger times: 11
Loss after 365478300 batches: 0.0205
trigger times: 12
Loss after 365580900 batches: 0.0227
trigger times: 13
Loss after 365683500 batches: 0.0209
trigger times: 14
Loss after 365786100 batches: 0.0192
trigger times: 15
Loss after 365888700 batches: 0.0199
trigger times: 16
Loss after 365991300 batches: 0.0188
trigger times: 17
Loss after 366093900 batches: 0.0189
trigger times: 18
Loss after 366196500 batches: 0.0202
trigger times: 19
Loss after 366299100 batches: 0.0189
trigger times: 20
Early stopping!
Start to test process.
Loss after 366401700 batches: 0.0188
Time to train on one home:  136.92870664596558
trigger times: 0
Loss after 366532800 batches: 0.0720
trigger times: 0
Loss after 366663900 batches: 0.0245
trigger times: 1
Loss after 366795000 batches: 0.0203
trigger times: 2
Loss after 366926100 batches: 0.0187
trigger times: 3
Loss after 367057200 batches: 0.0175
trigger times: 4
Loss after 367188300 batches: 0.0169
trigger times: 5
Loss after 367319400 batches: 0.0165
trigger times: 6
Loss after 367450500 batches: 0.0160
trigger times: 7
Loss after 367581600 batches: 0.0157
trigger times: 8
Loss after 367712700 batches: 0.0152
trigger times: 9
Loss after 367843800 batches: 0.0148
trigger times: 10
Loss after 367974900 batches: 0.0146
trigger times: 11
Loss after 368106000 batches: 0.0145
trigger times: 12
Loss after 368237100 batches: 0.0143
trigger times: 13
Loss after 368368200 batches: 0.0143
trigger times: 14
Loss after 368499300 batches: 0.0140
trigger times: 15
Loss after 368630400 batches: 0.0138
trigger times: 16
Loss after 368761500 batches: 0.0135
trigger times: 17
Loss after 368892600 batches: 0.0134
trigger times: 18
Loss after 369023700 batches: 0.0132
trigger times: 19
Loss after 369154800 batches: 0.0133
trigger times: 20
Early stopping!
Start to test process.
Loss after 369285900 batches: 0.0132
Time to train on one home:  167.82512879371643
trigger times: 0
Loss after 369417000 batches: 0.0743
trigger times: 0
Loss after 369548100 batches: 0.0227
trigger times: 0
Loss after 369679200 batches: 0.0191
trigger times: 1
Loss after 369810300 batches: 0.0177
trigger times: 2
Loss after 369941400 batches: 0.0171
trigger times: 0
Loss after 370072500 batches: 0.0165
trigger times: 1
Loss after 370203600 batches: 0.0161
trigger times: 2
Loss after 370334700 batches: 0.0159
trigger times: 3
Loss after 370465800 batches: 0.0155
trigger times: 4
Loss after 370596900 batches: 0.0155
trigger times: 5
Loss after 370728000 batches: 0.0152
trigger times: 6
Loss after 370859100 batches: 0.0151
trigger times: 7
Loss after 370990200 batches: 0.0149
trigger times: 8
Loss after 371121300 batches: 0.0149
trigger times: 9
Loss after 371252400 batches: 0.0146
trigger times: 10
Loss after 371383500 batches: 0.0146
trigger times: 11
Loss after 371514600 batches: 0.0144
trigger times: 12
Loss after 371645700 batches: 0.0146
trigger times: 0
Loss after 371776800 batches: 0.0145
trigger times: 1
Loss after 371907900 batches: 0.0140
trigger times: 2
Loss after 372039000 batches: 0.0141
trigger times: 3
Loss after 372170100 batches: 0.0141
trigger times: 4
Loss after 372301200 batches: 0.0140
trigger times: 5
Loss after 372432300 batches: 0.0138
trigger times: 6
Loss after 372563400 batches: 0.0140
trigger times: 7
Loss after 372694500 batches: 0.0139
trigger times: 8
Loss after 372825600 batches: 0.0139
trigger times: 9
Loss after 372956700 batches: 0.0136
trigger times: 10
Loss after 373087800 batches: 0.0138
trigger times: 11
Loss after 373218900 batches: 0.0136
trigger times: 12
Loss after 373350000 batches: 0.0134
trigger times: 13
Loss after 373481100 batches: 0.0137
trigger times: 14
Loss after 373612200 batches: 0.0135
trigger times: 15
Loss after 373743300 batches: 0.0134
trigger times: 16
Loss after 373874400 batches: 0.0136
trigger times: 17
Loss after 374005500 batches: 0.0135
trigger times: 18
Loss after 374136600 batches: 0.0135
trigger times: 19
Loss after 374267700 batches: 0.0132
trigger times: 20
Early stopping!
Start to test process.
Loss after 374398800 batches: 0.0133
Time to train on one home:  288.4537835121155
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358, 0.014216079169119518, 0.013823527540373118, 0.014062286198334994, 0.013536151017310049, 0.01339236478051815, 0.01412220146439665, 0.013110444862403894, 0.012553285140885578, 0.01315656214124487]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175], [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912], [0.4441254701879289, 0.5205093148975452, 0.5665822904653603, 0.943887952691261, 0.39279617400695876, 22.29969119423179, 1212.5862], [0.43548785315619576, 0.5298403732534891, 0.5768369757093087, 0.9242649282254869, 0.38515222150584383, 21.836090207869006, 1188.9888], [0.4311514099438985, 0.5345118722814012, 0.5754949634807172, 0.9187582104432755, 0.3813253547865264, 21.705992026525585, 1177.175], [0.43552306294441223, 0.5297965797882871, 0.571442542482683, 0.9199573944962688, 0.3851880968755061, 21.734323179593652, 1189.0996], [0.4262552526262071, 0.5398158400379639, 0.5797626089679692, 0.9012375911472185, 0.3769803731079171, 21.292061115849403, 1163.762], [0.4279141707552804, 0.5380279725473285, 0.5824082127012885, 0.9113846643500105, 0.3784449844794665, 21.531789357217836, 1168.2832], [0.4270105991098616, 0.5390387342494132, 0.5791757731347951, 0.9101790603926071, 0.37761697396383614, 21.503306531607038, 1165.727], [0.42016881373193526, 0.5464277852226262, 0.5891647805548108, 0.8852416475519027, 0.3715639033995971, 20.914151215083283, 1147.0409]]
Round_16_results:  [0.42016881373193526, 0.5464277852226262, 0.5891647805548108, 0.8852416475519027, 0.3715639033995971, 20.914151215083283, 1147.0409]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2959 < 2960; dropping {'Training_Loss': 0.04797176565131489, 'Validation_Loss': 0.24271506236659157, 'Training_R2': 0.9516942424193917, 'Validation_R2': 0.7744040024824861, 'Training_F1': 0.8787572656012282, 'Validation_F1': 0.7528162731229304, 'Training_NEP': 0.24245202163831825, 'Validation_NEP': 0.48544778764463414, 'Training_NDE': 0.036263989644288494, 'Validation_NDE': 0.17965276033082117, 'Training_MAE': 8.029530122452888, 'Validation_MAE': 13.31299233671585, 'Training_MSE': 159.55588, 'Validation_MSE': 663.45276}.
trigger times: 0
Loss after 374529900 batches: 0.0480
trigger times: 0
Loss after 374661000 batches: 0.0147
trigger times: 0
Loss after 374792100 batches: 0.0117
trigger times: 1
Loss after 374923200 batches: 0.0104
trigger times: 0
Loss after 375054300 batches: 0.0100
trigger times: 1
Loss after 375185400 batches: 0.0100
trigger times: 0
Loss after 375316500 batches: 0.0097
trigger times: 1
Loss after 375447600 batches: 0.0091
trigger times: 2
Loss after 375578700 batches: 0.0090
trigger times: 3
Loss after 375709800 batches: 0.0087
trigger times: 4
Loss after 375840900 batches: 0.0085
trigger times: 5
Loss after 375972000 batches: 0.0084
trigger times: 6
Loss after 376103100 batches: 0.0083
trigger times: 7
Loss after 376234200 batches: 0.0081
trigger times: 8
Loss after 376365300 batches: 0.0082
trigger times: 0
Loss after 376496400 batches: 0.0082
trigger times: 1
Loss after 376627500 batches: 0.0081
trigger times: 2
Loss after 376758600 batches: 0.0080
trigger times: 0
Loss after 376889700 batches: 0.0079
trigger times: 1
Loss after 377020800 batches: 0.0078
trigger times: 2
Loss after 377151900 batches: 0.0078
trigger times: 3
Loss after 377283000 batches: 0.0075
trigger times: 4
Loss after 377414100 batches: 0.0074
trigger times: 5
Loss after 377545200 batches: 0.0075
trigger times: 6
Loss after 377676300 batches: 0.0075
trigger times: 7
Loss after 377807400 batches: 0.0075
trigger times: 8
Loss after 377938500 batches: 0.0074
trigger times: 9
Loss after 378069600 batches: 0.0072
trigger times: 10
Loss after 378200700 batches: 0.0075
trigger times: 11
Loss after 378331800 batches: 0.0073
trigger times: 12
Loss after 378462900 batches: 0.0072
trigger times: 13
Loss after 378594000 batches: 0.0072
trigger times: 14
Loss after 378725100 batches: 0.0072
trigger times: 15
Loss after 378856200 batches: 0.0072
trigger times: 16
Loss after 378987300 batches: 0.0072
trigger times: 17
Loss after 379118400 batches: 0.0070
trigger times: 18
Loss after 379249500 batches: 0.0070
trigger times: 19
Loss after 379380600 batches: 0.0071
trigger times: 20
Early stopping!
Start to test process.
Loss after 379511700 batches: 0.0070
Time to train on one home:  288.59202432632446
trigger times: 0
Loss after 379614300 batches: 0.1032
trigger times: 1
Loss after 379716900 batches: 0.0356
trigger times: 2
Loss after 379819500 batches: 0.0300
trigger times: 3
Loss after 379922100 batches: 0.0267
trigger times: 4
Loss after 380024700 batches: 0.0246
trigger times: 5
Loss after 380127300 batches: 0.0240
trigger times: 6
Loss after 380229900 batches: 0.0297
trigger times: 7
Loss after 380332500 batches: 0.0239
trigger times: 8
Loss after 380435100 batches: 0.0211
trigger times: 9
Loss after 380537700 batches: 0.0209
trigger times: 10
Loss after 380640300 batches: 0.0204
trigger times: 11
Loss after 380742900 batches: 0.0203
trigger times: 12
Loss after 380845500 batches: 0.0208
trigger times: 13
Loss after 380948100 batches: 0.0201
trigger times: 14
Loss after 381050700 batches: 0.0204
trigger times: 15
Loss after 381153300 batches: 0.0193
trigger times: 16
Loss after 381255900 batches: 0.0183
trigger times: 17
Loss after 381358500 batches: 0.0177
trigger times: 18
Loss after 381461100 batches: 0.0192
trigger times: 19
Loss after 381563700 batches: 0.0185
trigger times: 20
Early stopping!
Start to test process.
Loss after 381666300 batches: 0.0198
Time to train on one home:  131.2419662475586
trigger times: 0
Loss after 381797400 batches: 0.0544
trigger times: 0
Loss after 381928500 batches: 0.0222
trigger times: 1
Loss after 382059600 batches: 0.0190
trigger times: 2
Loss after 382190700 batches: 0.0177
trigger times: 3
Loss after 382321800 batches: 0.0171
trigger times: 4
Loss after 382452900 batches: 0.0167
trigger times: 5
Loss after 382584000 batches: 0.0160
trigger times: 6
Loss after 382715100 batches: 0.0156
trigger times: 7
Loss after 382846200 batches: 0.0154
trigger times: 8
Loss after 382977300 batches: 0.0149
trigger times: 9
Loss after 383108400 batches: 0.0146
trigger times: 10
Loss after 383239500 batches: 0.0146
trigger times: 11
Loss after 383370600 batches: 0.0145
trigger times: 12
Loss after 383501700 batches: 0.0141
trigger times: 13
Loss after 383632800 batches: 0.0139
trigger times: 14
Loss after 383763900 batches: 0.0138
trigger times: 15
Loss after 383895000 batches: 0.0136
trigger times: 16
Loss after 384026100 batches: 0.0137
trigger times: 17
Loss after 384157200 batches: 0.0134
trigger times: 18
Loss after 384288300 batches: 0.0131
trigger times: 19
Loss after 384419400 batches: 0.0131
trigger times: 20
Early stopping!
Start to test process.
Loss after 384550500 batches: 0.0131
Time to train on one home:  167.72482419013977
trigger times: 0
Loss after 384681600 batches: 0.0721
trigger times: 0
Loss after 384812700 batches: 0.0222
trigger times: 0
Loss after 384943800 batches: 0.0186
trigger times: 1
Loss after 385074900 batches: 0.0175
trigger times: 2
Loss after 385206000 batches: 0.0167
trigger times: 3
Loss after 385337100 batches: 0.0167
trigger times: 4
Loss after 385468200 batches: 0.0158
trigger times: 5
Loss after 385599300 batches: 0.0157
trigger times: 6
Loss after 385730400 batches: 0.0153
trigger times: 7
Loss after 385861500 batches: 0.0152
trigger times: 8
Loss after 385992600 batches: 0.0149
trigger times: 9
Loss after 386123700 batches: 0.0149
trigger times: 10
Loss after 386254800 batches: 0.0150
trigger times: 11
Loss after 386385900 batches: 0.0146
trigger times: 12
Loss after 386517000 batches: 0.0145
trigger times: 13
Loss after 386648100 batches: 0.0144
trigger times: 14
Loss after 386779200 batches: 0.0143
trigger times: 15
Loss after 386910300 batches: 0.0144
trigger times: 16
Loss after 387041400 batches: 0.0142
trigger times: 17
Loss after 387172500 batches: 0.0142
trigger times: 18
Loss after 387303600 batches: 0.0140
trigger times: 19
Loss after 387434700 batches: 0.0141
trigger times: 20
Early stopping!
Start to test process.
Loss after 387565800 batches: 0.0140
Time to train on one home:  174.7628788948059
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358, 0.014216079169119518, 0.013823527540373118, 0.014062286198334994, 0.013536151017310049, 0.01339236478051815, 0.01412220146439665, 0.013110444862403894, 0.012553285140885578, 0.01315656214124487, 0.013457664819927977]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175], [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912], [0.4441254701879289, 0.5205093148975452, 0.5665822904653603, 0.943887952691261, 0.39279617400695876, 22.29969119423179, 1212.5862], [0.43548785315619576, 0.5298403732534891, 0.5768369757093087, 0.9242649282254869, 0.38515222150584383, 21.836090207869006, 1188.9888], [0.4311514099438985, 0.5345118722814012, 0.5754949634807172, 0.9187582104432755, 0.3813253547865264, 21.705992026525585, 1177.175], [0.43552306294441223, 0.5297965797882871, 0.571442542482683, 0.9199573944962688, 0.3851880968755061, 21.734323179593652, 1189.0996], [0.4262552526262071, 0.5398158400379639, 0.5797626089679692, 0.9012375911472185, 0.3769803731079171, 21.292061115849403, 1163.762], [0.4279141707552804, 0.5380279725473285, 0.5824082127012885, 0.9113846643500105, 0.3784449844794665, 21.531789357217836, 1168.2832], [0.4270105991098616, 0.5390387342494132, 0.5791757731347951, 0.9101790603926071, 0.37761697396383614, 21.503306531607038, 1165.727], [0.42016881373193526, 0.5464277852226262, 0.5891647805548108, 0.8852416475519027, 0.3715639033995971, 20.914151215083283, 1147.0409], [0.41514548493756187, 0.5518728682249614, 0.595137961113949, 0.8854988252284184, 0.3671033208754327, 20.920227129869563, 1133.2708]]
Round_17_results:  [0.41514548493756187, 0.5518728682249614, 0.595137961113949, 0.8854988252284184, 0.3671033208754327, 20.920227129869563, 1133.2708]
trigger times: 0
Loss after 387696900 batches: 0.0460
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3064 < 3065; dropping {'Training_Loss': 0.046047517889232004, 'Validation_Loss': 0.24914993594090143, 'Training_R2': 0.9536399580320682, 'Validation_R2': 0.7684758188734623, 'Training_F1': 0.8824517228305981, 'Validation_F1': 0.7519842960782332, 'Training_NEP': 0.23506245708562762, 'Validation_NEP': 0.4828331962315384, 'Training_NDE': 0.03480330639734658, 'Validation_NDE': 0.18437365325812766, 'Training_MAE': 7.784802399554569, 'Validation_MAE': 13.24128939289346, 'Training_MSE': 153.12907, 'Validation_MSE': 680.88684}.
trigger times: 0
Loss after 387828000 batches: 0.0142
trigger times: 0
Loss after 387959100 batches: 0.0111
trigger times: 1
Loss after 388090200 batches: 0.0102
trigger times: 2
Loss after 388221300 batches: 0.0097
trigger times: 3
Loss after 388352400 batches: 0.0092
trigger times: 4
Loss after 388483500 batches: 0.0091
trigger times: 5
Loss after 388614600 batches: 0.0087
trigger times: 6
Loss after 388745700 batches: 0.0086
trigger times: 7
Loss after 388876800 batches: 0.0084
trigger times: 8
Loss after 389007900 batches: 0.0085
trigger times: 0
Loss after 389139000 batches: 0.0083
trigger times: 0
Loss after 389270100 batches: 0.0082
trigger times: 1
Loss after 389401200 batches: 0.0080
trigger times: 2
Loss after 389532300 batches: 0.0079
trigger times: 3
Loss after 389663400 batches: 0.0079
trigger times: 4
Loss after 389794500 batches: 0.0079
trigger times: 5
Loss after 389925600 batches: 0.0077
trigger times: 6
Loss after 390056700 batches: 0.0076
trigger times: 7
Loss after 390187800 batches: 0.0075
trigger times: 8
Loss after 390318900 batches: 0.0074
trigger times: 9
Loss after 390450000 batches: 0.0076
trigger times: 10
Loss after 390581100 batches: 0.0075
trigger times: 11
Loss after 390712200 batches: 0.0075
trigger times: 12
Loss after 390843300 batches: 0.0072
trigger times: 13
Loss after 390974400 batches: 0.0071
trigger times: 14
Loss after 391105500 batches: 0.0072
trigger times: 15
Loss after 391236600 batches: 0.0073
trigger times: 16
Loss after 391367700 batches: 0.0072
trigger times: 17
Loss after 391498800 batches: 0.0070
trigger times: 18
Loss after 391629900 batches: 0.0071
trigger times: 19
Loss after 391761000 batches: 0.0071
trigger times: 20
Early stopping!
Start to test process.
Loss after 391892100 batches: 0.0071
Time to train on one home:  245.82486414909363
trigger times: 0
Loss after 391994700 batches: 0.0975
trigger times: 0
Loss after 392097300 batches: 0.0329
trigger times: 0
Loss after 392199900 batches: 0.0275
trigger times: 1
Loss after 392302500 batches: 0.0248
trigger times: 2
Loss after 392405100 batches: 0.0231
trigger times: 3
Loss after 392507700 batches: 0.0222
trigger times: 4
Loss after 392610300 batches: 0.0223
trigger times: 5
Loss after 392712900 batches: 0.0221
trigger times: 6
Loss after 392815500 batches: 0.0208
trigger times: 7
Loss after 392918100 batches: 0.0208
trigger times: 8
Loss after 393020700 batches: 0.0213
trigger times: 9
Loss after 393123300 batches: 0.0204
trigger times: 10
Loss after 393225900 batches: 0.0196
trigger times: 11
Loss after 393328500 batches: 0.0191
trigger times: 12
Loss after 393431100 batches: 0.0192
trigger times: 13
Loss after 393533700 batches: 0.0183
trigger times: 14
Loss after 393636300 batches: 0.0187
trigger times: 15
Loss after 393738900 batches: 0.0187
trigger times: 16
Loss after 393841500 batches: 0.0194
trigger times: 17
Loss after 393944100 batches: 0.0184
trigger times: 18
Loss after 394046700 batches: 0.0176
trigger times: 19
Loss after 394149300 batches: 0.0174
trigger times: 20
Early stopping!
Start to test process.
Loss after 394251900 batches: 0.0177
Time to train on one home:  142.6076455116272
trigger times: 0
Loss after 394383000 batches: 0.0528
trigger times: 0
Loss after 394514100 batches: 0.0212
trigger times: 1
Loss after 394645200 batches: 0.0186
trigger times: 2
Loss after 394776300 batches: 0.0175
trigger times: 3
Loss after 394907400 batches: 0.0165
trigger times: 4
Loss after 395038500 batches: 0.0159
trigger times: 5
Loss after 395169600 batches: 0.0157
trigger times: 6
Loss after 395300700 batches: 0.0153
trigger times: 7
Loss after 395431800 batches: 0.0149
trigger times: 8
Loss after 395562900 batches: 0.0147
trigger times: 9
Loss after 395694000 batches: 0.0144
trigger times: 10
Loss after 395825100 batches: 0.0142
trigger times: 11
Loss after 395956200 batches: 0.0139
trigger times: 12
Loss after 396087300 batches: 0.0139
trigger times: 13
Loss after 396218400 batches: 0.0135
trigger times: 14
Loss after 396349500 batches: 0.0136
trigger times: 15
Loss after 396480600 batches: 0.0135
trigger times: 16
Loss after 396611700 batches: 0.0132
trigger times: 17
Loss after 396742800 batches: 0.0132
trigger times: 18
Loss after 396873900 batches: 0.0130
trigger times: 19
Loss after 397005000 batches: 0.0130
trigger times: 20
Early stopping!
Start to test process.
Loss after 397136100 batches: 0.0128
Time to train on one home:  167.76689648628235
trigger times: 0
Loss after 397267200 batches: 0.0703
trigger times: 0
Loss after 397398300 batches: 0.0217
trigger times: 0
Loss after 397529400 batches: 0.0183
trigger times: 1
Loss after 397660500 batches: 0.0175
trigger times: 2
Loss after 397791600 batches: 0.0167
trigger times: 3
Loss after 397922700 batches: 0.0165
trigger times: 4
Loss after 398053800 batches: 0.0162
trigger times: 5
Loss after 398184900 batches: 0.0157
trigger times: 0
Loss after 398316000 batches: 0.0156
trigger times: 1
Loss after 398447100 batches: 0.0154
trigger times: 2
Loss after 398578200 batches: 0.0151
trigger times: 3
Loss after 398709300 batches: 0.0149
trigger times: 4
Loss after 398840400 batches: 0.0149
trigger times: 0
Loss after 398971500 batches: 0.0149
trigger times: 1
Loss after 399102600 batches: 0.0148
trigger times: 2
Loss after 399233700 batches: 0.0143
trigger times: 3
Loss after 399364800 batches: 0.0145
trigger times: 4
Loss after 399495900 batches: 0.0143
trigger times: 5
Loss after 399627000 batches: 0.0145
trigger times: 6
Loss after 399758100 batches: 0.0143
trigger times: 7
Loss after 399889200 batches: 0.0144
trigger times: 8
Loss after 400020300 batches: 0.0141
trigger times: 9
Loss after 400151400 batches: 0.0141
trigger times: 10
Loss after 400282500 batches: 0.0141
trigger times: 11
Loss after 400413600 batches: 0.0140
trigger times: 12
Loss after 400544700 batches: 0.0140
trigger times: 13
Loss after 400675800 batches: 0.0139
trigger times: 0
Loss after 400806900 batches: 0.0140
trigger times: 1
Loss after 400938000 batches: 0.0138
trigger times: 2
Loss after 401069100 batches: 0.0137
trigger times: 3
Loss after 401200200 batches: 0.0137
trigger times: 4
Loss after 401331300 batches: 0.0138
trigger times: 5
Loss after 401462400 batches: 0.0137
trigger times: 6
Loss after 401593500 batches: 0.0137
trigger times: 7
Loss after 401724600 batches: 0.0135
trigger times: 8
Loss after 401855700 batches: 0.0137
trigger times: 9
Loss after 401986800 batches: 0.0135
trigger times: 10
Loss after 402117900 batches: 0.0135
trigger times: 11
Loss after 402249000 batches: 0.0136
trigger times: 12
Loss after 402380100 batches: 0.0135
trigger times: 13
Loss after 402511200 batches: 0.0135
trigger times: 14
Loss after 402642300 batches: 0.0134
trigger times: 15
Loss after 402773400 batches: 0.0133
trigger times: 16
Loss after 402904500 batches: 0.0134
trigger times: 17
Loss after 403035600 batches: 0.0133
trigger times: 18
Loss after 403166700 batches: 0.0131
trigger times: 19
Loss after 403297800 batches: 0.0131
trigger times: 20
Early stopping!
Start to test process.
Loss after 403428900 batches: 0.0132
Time to train on one home:  352.6360704898834
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358, 0.014216079169119518, 0.013823527540373118, 0.014062286198334994, 0.013536151017310049, 0.01339236478051815, 0.01412220146439665, 0.013110444862403894, 0.012553285140885578, 0.01315656214124487, 0.013457664819927977, 0.0127064111774958]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175], [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912], [0.4441254701879289, 0.5205093148975452, 0.5665822904653603, 0.943887952691261, 0.39279617400695876, 22.29969119423179, 1212.5862], [0.43548785315619576, 0.5298403732534891, 0.5768369757093087, 0.9242649282254869, 0.38515222150584383, 21.836090207869006, 1188.9888], [0.4311514099438985, 0.5345118722814012, 0.5754949634807172, 0.9187582104432755, 0.3813253547865264, 21.705992026525585, 1177.175], [0.43552306294441223, 0.5297965797882871, 0.571442542482683, 0.9199573944962688, 0.3851880968755061, 21.734323179593652, 1189.0996], [0.4262552526262071, 0.5398158400379639, 0.5797626089679692, 0.9012375911472185, 0.3769803731079171, 21.292061115849403, 1163.762], [0.4279141707552804, 0.5380279725473285, 0.5824082127012885, 0.9113846643500105, 0.3784449844794665, 21.531789357217836, 1168.2832], [0.4270105991098616, 0.5390387342494132, 0.5791757731347951, 0.9101790603926071, 0.37761697396383614, 21.503306531607038, 1165.727], [0.42016881373193526, 0.5464277852226262, 0.5891647805548108, 0.8852416475519027, 0.3715639033995971, 20.914151215083283, 1147.0409], [0.41514548493756187, 0.5518728682249614, 0.595137961113949, 0.8854988252284184, 0.3671033208754327, 20.920227129869563, 1133.2708], [0.41333263946904075, 0.5538240382675925, 0.5963042365256811, 0.8716223650908109, 0.3655049329371587, 20.592390785465103, 1128.3364]]
Round_18_results:  [0.41333263946904075, 0.5538240382675925, 0.5963042365256811, 0.8716223650908109, 0.3655049329371587, 20.592390785465103, 1128.3364]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3190 < 3191; dropping {'Training_Loss': 0.04462055283827039, 'Validation_Loss': 0.24650255921814176, 'Training_R2': 0.9550600503373853, 'Validation_R2': 0.7709054206432703, 'Training_F1': 0.8833985337002007, 'Validation_F1': 0.7554989001726475, 'Training_NEP': 0.2329869451009224, 'Validation_NEP': 0.4794767501076889, 'Training_NDE': 0.033737217897067545, 'Validation_NDE': 0.1824388464829462, 'Training_MAE': 7.716065558805255, 'Validation_MAE': 13.14924171513554, 'Training_MSE': 148.43846, 'Validation_MSE': 673.74164}.
trigger times: 0
Loss after 403560000 batches: 0.0446
trigger times: 0
Loss after 403691100 batches: 0.0136
trigger times: 0
Loss after 403822200 batches: 0.0113
trigger times: 1
Loss after 403953300 batches: 0.0102
trigger times: 0
Loss after 404084400 batches: 0.0095
trigger times: 1
Loss after 404215500 batches: 0.0092
trigger times: 2
Loss after 404346600 batches: 0.0090
trigger times: 3
Loss after 404477700 batches: 0.0089
trigger times: 4
Loss after 404608800 batches: 0.0085
trigger times: 5
Loss after 404739900 batches: 0.0084
trigger times: 6
Loss after 404871000 batches: 0.0083
trigger times: 7
Loss after 405002100 batches: 0.0082
trigger times: 8
Loss after 405133200 batches: 0.0081
trigger times: 9
Loss after 405264300 batches: 0.0079
trigger times: 10
Loss after 405395400 batches: 0.0078
trigger times: 0
Loss after 405526500 batches: 0.0078
trigger times: 1
Loss after 405657600 batches: 0.0079
trigger times: 2
Loss after 405788700 batches: 0.0075
trigger times: 3
Loss after 405919800 batches: 0.0075
trigger times: 4
Loss after 406050900 batches: 0.0076
trigger times: 5
Loss after 406182000 batches: 0.0075
trigger times: 6
Loss after 406313100 batches: 0.0075
trigger times: 0
Loss after 406444200 batches: 0.0076
trigger times: 1
Loss after 406575300 batches: 0.0075
trigger times: 2
Loss after 406706400 batches: 0.0073
trigger times: 3
Loss after 406837500 batches: 0.0072
trigger times: 4
Loss after 406968600 batches: 0.0072
trigger times: 5
Loss after 407099700 batches: 0.0072
trigger times: 6
Loss after 407230800 batches: 0.0071
trigger times: 7
Loss after 407361900 batches: 0.0072
trigger times: 8
Loss after 407493000 batches: 0.0071
trigger times: 9
Loss after 407624100 batches: 0.0072
trigger times: 10
Loss after 407755200 batches: 0.0070
trigger times: 11
Loss after 407886300 batches: 0.0072
trigger times: 12
Loss after 408017400 batches: 0.0070
trigger times: 13
Loss after 408148500 batches: 0.0070
trigger times: 14
Loss after 408279600 batches: 0.0070
trigger times: 15
Loss after 408410700 batches: 0.0070
trigger times: 16
Loss after 408541800 batches: 0.0069
trigger times: 17
Loss after 408672900 batches: 0.0070
trigger times: 18
Loss after 408804000 batches: 0.0067
trigger times: 19
Loss after 408935100 batches: 0.0069
trigger times: 20
Early stopping!
Start to test process.
Loss after 409066200 batches: 0.0067
Time to train on one home:  316.1053252220154
trigger times: 0
Loss after 409168800 batches: 0.0984
trigger times: 0
Loss after 409271400 batches: 0.0330
trigger times: 1
Loss after 409374000 batches: 0.0292
trigger times: 2
Loss after 409476600 batches: 0.0248
trigger times: 3
Loss after 409579200 batches: 0.0229
trigger times: 4
Loss after 409681800 batches: 0.0232
trigger times: 5
Loss after 409784400 batches: 0.0214
trigger times: 6
Loss after 409887000 batches: 0.0213
trigger times: 0
Loss after 409989600 batches: 0.0205
trigger times: 1
Loss after 410092200 batches: 0.0204
trigger times: 2
Loss after 410194800 batches: 0.0206
trigger times: 3
Loss after 410297400 batches: 0.0229
trigger times: 4
Loss after 410400000 batches: 0.0220
trigger times: 5
Loss after 410502600 batches: 0.0200
trigger times: 6
Loss after 410605200 batches: 0.0197
trigger times: 7
Loss after 410707800 batches: 0.0192
trigger times: 8
Loss after 410810400 batches: 0.0183
trigger times: 9
Loss after 410913000 batches: 0.0179
trigger times: 10
Loss after 411015600 batches: 0.0175
trigger times: 11
Loss after 411118200 batches: 0.0180
trigger times: 12
Loss after 411220800 batches: 0.0175
trigger times: 13
Loss after 411323400 batches: 0.0177
trigger times: 0
Loss after 411426000 batches: 0.0170
trigger times: 1
Loss after 411528600 batches: 0.0168
trigger times: 2
Loss after 411631200 batches: 0.0170
trigger times: 3
Loss after 411733800 batches: 0.0174
trigger times: 4
Loss after 411836400 batches: 0.0172
trigger times: 5
Loss after 411939000 batches: 0.0177
trigger times: 6
Loss after 412041600 batches: 0.0173
trigger times: 7
Loss after 412144200 batches: 0.0183
trigger times: 8
Loss after 412246800 batches: 0.0163
trigger times: 9
Loss after 412349400 batches: 0.0168
trigger times: 10
Loss after 412452000 batches: 0.0153
trigger times: 11
Loss after 412554600 batches: 0.0159
trigger times: 12
Loss after 412657200 batches: 0.0169
trigger times: 13
Loss after 412759800 batches: 0.0166
trigger times: 14
Loss after 412862400 batches: 0.0173
trigger times: 15
Loss after 412965000 batches: 0.0175
trigger times: 16
Loss after 413067600 batches: 0.0166
trigger times: 17
Loss after 413170200 batches: 0.0158
trigger times: 18
Loss after 413272800 batches: 0.0157
trigger times: 19
Loss after 413375400 batches: 0.0155
trigger times: 20
Early stopping!
Start to test process.
Loss after 413478000 batches: 0.0156
Time to train on one home:  256.1355981826782
trigger times: 0
Loss after 413609100 batches: 0.0520
trigger times: 0
Loss after 413740200 batches: 0.0209
trigger times: 1
Loss after 413871300 batches: 0.0181
trigger times: 2
Loss after 414002400 batches: 0.0171
trigger times: 3
Loss after 414133500 batches: 0.0164
trigger times: 4
Loss after 414264600 batches: 0.0159
trigger times: 5
Loss after 414395700 batches: 0.0156
trigger times: 6
Loss after 414526800 batches: 0.0153
trigger times: 7
Loss after 414657900 batches: 0.0148
trigger times: 8
Loss after 414789000 batches: 0.0144
trigger times: 9
Loss after 414920100 batches: 0.0144
trigger times: 10
Loss after 415051200 batches: 0.0143
trigger times: 11
Loss after 415182300 batches: 0.0140
trigger times: 12
Loss after 415313400 batches: 0.0137
trigger times: 13
Loss after 415444500 batches: 0.0135
trigger times: 14
Loss after 415575600 batches: 0.0136
trigger times: 15
Loss after 415706700 batches: 0.0131
trigger times: 16
Loss after 415837800 batches: 0.0131
trigger times: 17
Loss after 415968900 batches: 0.0131
trigger times: 18
Loss after 416100000 batches: 0.0129
trigger times: 19
Loss after 416231100 batches: 0.0129
trigger times: 20
Early stopping!
Start to test process.
Loss after 416362200 batches: 0.0127
Time to train on one home:  167.84788942337036
trigger times: 0
Loss after 416493300 batches: 0.0686
trigger times: 0
Loss after 416624400 batches: 0.0216
trigger times: 0
Loss after 416755500 batches: 0.0183
trigger times: 1
Loss after 416886600 batches: 0.0173
trigger times: 2
Loss after 417017700 batches: 0.0168
trigger times: 3
Loss after 417148800 batches: 0.0161
trigger times: 0
Loss after 417279900 batches: 0.0159
trigger times: 1
Loss after 417411000 batches: 0.0156
trigger times: 2
Loss after 417542100 batches: 0.0155
trigger times: 3
Loss after 417673200 batches: 0.0154
trigger times: 4
Loss after 417804300 batches: 0.0151
trigger times: 5
Loss after 417935400 batches: 0.0151
trigger times: 6
Loss after 418066500 batches: 0.0147
trigger times: 7
Loss after 418197600 batches: 0.0145
trigger times: 8
Loss after 418328700 batches: 0.0145
trigger times: 9
Loss after 418459800 batches: 0.0144
trigger times: 10
Loss after 418590900 batches: 0.0143
trigger times: 11
Loss after 418722000 batches: 0.0142
trigger times: 12
Loss after 418853100 batches: 0.0145
trigger times: 13
Loss after 418984200 batches: 0.0143
trigger times: 14
Loss after 419115300 batches: 0.0140
trigger times: 0
Loss after 419246400 batches: 0.0141
trigger times: 1
Loss after 419377500 batches: 0.0142
trigger times: 2
Loss after 419508600 batches: 0.0140
trigger times: 3
Loss after 419639700 batches: 0.0137
trigger times: 4
Loss after 419770800 batches: 0.0139
trigger times: 5
Loss after 419901900 batches: 0.0136
trigger times: 6
Loss after 420033000 batches: 0.0135
trigger times: 7
Loss after 420164100 batches: 0.0135
trigger times: 8
Loss after 420295200 batches: 0.0136
trigger times: 9
Loss after 420426300 batches: 0.0135
trigger times: 0
Loss after 420557400 batches: 0.0134
trigger times: 1
Loss after 420688500 batches: 0.0136
trigger times: 2
Loss after 420819600 batches: 0.0136
trigger times: 3
Loss after 420950700 batches: 0.0134
trigger times: 4
Loss after 421081800 batches: 0.0135
trigger times: 5
Loss after 421212900 batches: 0.0133
trigger times: 6
Loss after 421344000 batches: 0.0132
trigger times: 7
Loss after 421475100 batches: 0.0133
trigger times: 8
Loss after 421606200 batches: 0.0131
trigger times: 9
Loss after 421737300 batches: 0.0132
trigger times: 10
Loss after 421868400 batches: 0.0132
trigger times: 11
Loss after 421999500 batches: 0.0132
trigger times: 12
Loss after 422130600 batches: 0.0133
trigger times: 13
Loss after 422261700 batches: 0.0130
trigger times: 14
Loss after 422392800 batches: 0.0130
trigger times: 15
Loss after 422523900 batches: 0.0130
trigger times: 16
Loss after 422655000 batches: 0.0130
trigger times: 17
Loss after 422786100 batches: 0.0131
trigger times: 18
Loss after 422917200 batches: 0.0128
trigger times: 19
Loss after 423048300 batches: 0.0129
trigger times: 20
Early stopping!
Start to test process.
Loss after 423179400 batches: 0.0129
Time to train on one home:  381.7949867248535
train_results:  [0.07783265326043251, 0.05773241920455259, 0.031186376489541576, 0.019605377905936, 0.019086784076302078, 0.017001707673425095, 0.01634626513115245, 0.01398637684322358, 0.014216079169119518, 0.013823527540373118, 0.014062286198334994, 0.013536151017310049, 0.01339236478051815, 0.01412220146439665, 0.013110444862403894, 0.012553285140885578, 0.01315656214124487, 0.013457664819927977, 0.0127064111774958, 0.011972610740077255]
test_results:  [[0.8999903599421183, 0.026435934115843218, 0.21522668808645584, 1.4996247514250285, 0.7975384133859535, 35.429172253608705, 2462.0505], [0.6397717595100403, 0.3084229593228297, 0.35510040680360083, 0.9688552072759457, 0.5665361685827184, 22.88955153265307, 1748.9323], [0.4712254951397578, 0.491080009540259, 0.5092405091368419, 0.9339386824739312, 0.4169045016704139, 22.0646361192922, 1287.0103], [0.48166144887606305, 0.47974466373122615, 0.5228458629191478, 1.0225047188904874, 0.42619035560495416, 24.1570404738081, 1315.6761], [0.47520481215582955, 0.4867632421790973, 0.5218224747504168, 0.9494390840968879, 0.4204407741283035, 22.430838663347306, 1297.9268], [0.4535050673617257, 0.5102757804849529, 0.5574103070020505, 0.9523168583251158, 0.4011794300090559, 22.498827110951947, 1238.466], [0.4436617328060998, 0.5209597142020532, 0.5649216976675754, 0.9339150704521869, 0.3924272093344788, 22.06407827681538, 1211.4473], [0.4680924067894618, 0.4945172797545223, 0.5377927920556665, 0.9717558594062314, 0.41408870851501134, 22.958080478893812, 1278.3175], [0.4565180821551217, 0.5070232166775006, 0.5533311554830512, 0.9486235282471517, 0.4038439126757171, 22.411570864082666, 1246.6912], [0.4441254701879289, 0.5205093148975452, 0.5665822904653603, 0.943887952691261, 0.39279617400695876, 22.29969119423179, 1212.5862], [0.43548785315619576, 0.5298403732534891, 0.5768369757093087, 0.9242649282254869, 0.38515222150584383, 21.836090207869006, 1188.9888], [0.4311514099438985, 0.5345118722814012, 0.5754949634807172, 0.9187582104432755, 0.3813253547865264, 21.705992026525585, 1177.175], [0.43552306294441223, 0.5297965797882871, 0.571442542482683, 0.9199573944962688, 0.3851880968755061, 21.734323179593652, 1189.0996], [0.4262552526262071, 0.5398158400379639, 0.5797626089679692, 0.9012375911472185, 0.3769803731079171, 21.292061115849403, 1163.762], [0.4279141707552804, 0.5380279725473285, 0.5824082127012885, 0.9113846643500105, 0.3784449844794665, 21.531789357217836, 1168.2832], [0.4270105991098616, 0.5390387342494132, 0.5791757731347951, 0.9101790603926071, 0.37761697396383614, 21.503306531607038, 1165.727], [0.42016881373193526, 0.5464277852226262, 0.5891647805548108, 0.8852416475519027, 0.3715639033995971, 20.914151215083283, 1147.0409], [0.41514548493756187, 0.5518728682249614, 0.595137961113949, 0.8854988252284184, 0.3671033208754327, 20.920227129869563, 1133.2708], [0.41333263946904075, 0.5538240382675925, 0.5963042365256811, 0.8716223650908109, 0.3655049329371587, 20.592390785465103, 1128.3364], [0.4220367603831821, 0.5443683431252735, 0.5892235756424559, 0.8935322720194819, 0.37325098721907973, 21.110020189686914, 1152.2489]]
Round_19_results:  [0.4220367603831821, 0.5443683431252735, 0.5892235756424559, 0.8935322720194819, 0.37325098721907973, 21.110020189686914, 1152.2489]