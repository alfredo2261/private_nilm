{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937e8860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from data_loaders import make_train_data, make_test_val_data, make_model\n",
    "from train import train, test, precision, recall, f1_score\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "import config_file\n",
    "from clean_data_seq2point import load_all_houses_with_device\n",
    "import random\n",
    "import optuna\n",
    "from data_loaders import PecanStreetDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lstm_seq2point import LSTM\n",
    "from clean_data_seq2point import normalize_y\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fee6b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnilm\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09f39938",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ = config_file.load_hyperparameters(\"refrigerator1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef2e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "homes = load_all_houses_with_device(config_file.path, config_['appliance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d44b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters, train_months, test_month, appliance, window_length, train_buildings,\n",
    "                   test_buildings, patience):\n",
    "    with wandb.init(project=\"global_models_feb10\", config=hyperparameters):\n",
    "        wandb.run.name = str(config_['appliance'])+\"_Test:\"+str(test_buildings)+\"_Train:\" + str(train_buildings)\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        # lengths = [85320, 132480, 132480, 132480, 132480, 132480, 132480]\n",
    "\n",
    "        model, criterion, optimizer = make_model(config)\n",
    "\n",
    "        print(model)\n",
    "        print(\"Window Length: \", window_length)\n",
    "\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "        example_ct = 0\n",
    "        batch_ct = 0\n",
    "        all_epochs = 0\n",
    "\n",
    "        #Scheduler for training on single building\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.95, verbose=True)\n",
    "        \n",
    "        #base_lr: 0.001*lr, max_lr: 4*lr, step_size_up:50, step_size_down:2000\n",
    "#         scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "#             optimizer,\n",
    "#             base_lr = config_['learning_rate'], #0.01\n",
    "#             max_lr = 1.1*config_['learning_rate'], #1\n",
    "#             step_size_up = 10000, #200\n",
    "#             step_size_down = 10000, #1000\n",
    "#             gamma = 0.95, #1\n",
    "#             cycle_momentum=False,\n",
    "#             verbose=False\n",
    "#         )\n",
    "\n",
    "        validation_loader, test_loader, test_val_seq_std, test_val_seq_mean = make_test_val_data(\n",
    "            config,\n",
    "            test_month,\n",
    "            appliance,\n",
    "            window_length,\n",
    "            test_buildings\n",
    "        )\n",
    "\n",
    "        time_log = time.time()\n",
    "        train_loader, train_seq_std, train_seq_mean = make_train_data(\n",
    "            config,\n",
    "            train_months,\n",
    "            appliance,\n",
    "            window_length,\n",
    "            train_buildings\n",
    "        )\n",
    "        \n",
    "        model, example_ct, batch_ct, all_epochs, best_model = train( \n",
    "            model,\n",
    "            train_loader,\n",
    "            validation_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            config,\n",
    "            example_ct,\n",
    "            batch_ct,\n",
    "            all_epochs,\n",
    "            scheduler,\n",
    "            test_val_seq_std,\n",
    "            test_val_seq_mean,\n",
    "            train_seq_std,\n",
    "            train_seq_mean,\n",
    "            patience\n",
    "        )\n",
    "\n",
    "        print(\"Time to train on one home: \", time.time() - time_log)\n",
    "\n",
    "        results = test(best_model, test_loader, criterion, test_val_seq_std, test_val_seq_mean)\n",
    "\n",
    "    return model, results, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3f25bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_ids = homes.dataid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df8df2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(home_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a7bfbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  145,   183,   335,   387,   526,  1417,  2358,  3383,  4628,\n",
       "        6240,  6526,  6672,  7021,  7069,  7365,  9004, 10554, 10811,\n",
       "       10983, 11878], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6269aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_ids_train = [x for x in home_ids if x!=3383]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96defcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[145,\n",
       " 183,\n",
       " 335,\n",
       " 387,\n",
       " 526,\n",
       " 1417,\n",
       " 2358,\n",
       " 4628,\n",
       " 6240,\n",
       " 6526,\n",
       " 6672,\n",
       " 7021,\n",
       " 7069,\n",
       " 7365,\n",
       " 9004,\n",
       " 10554,\n",
       " 10811,\n",
       " 10983,\n",
       " 11878]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_ids_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3671cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_select = [2,3,4,5,6,7,8,9,10,15,20]\n",
    "#random_select = [len(home_ids)]\n",
    "#random_select = [53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dff4a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"C:\\Users\\aar245\\Desktop\\privacy_preserving_nn\\models_power_ratio_filter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8370b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_homes_from_fl = [\n",
    "    [5058, 2358, 8825, 10811, 526],\n",
    "    [142, 10811, 2358, 6240, 2126, 1417, 7365, 5058, 10182, 9004],\n",
    "    [9053, 2561, 2126, 7021, 526, 9973, 6178, 3700, 8825, 142, 7069, 335, 690, 6526, 183],\n",
    "    [183, 6240, 5058, 6672, 8825, 3700, 7365, 9053, 3976, 9973, 6526, 3996, 526, 3488, 387, 11878, 7021, 10811, 9290, 6178],\n",
    "    [6672, 9290, 6178, 10983, 3700, 6526, 3488, 6240, 9973, 3976, 9004, 2126, 3383, 142, 2358, 690, 7021, 387, 10182, 10811, 5058, 8825, 2561, 526, 3996],\n",
    "    [526, 10811, 9973, 7021, 5058, 335, 3488, 9053, 10983, 3976, 11878, 142, 2561, 8825, 6672, 387, 183, 6178, 1417, 9290, 10182, 690, 2358, 7365, 3383, 6240, 7069, 9004, 3996, 3700],\n",
    "    [5058, 2358, 3488, 10182, 6672, 11878, 7021, 6526, 335, 10164, 142, 9973, 10983, 6240, 2126, 1417, 7069, 6178, 8825, 3383, 9004, 387, 3976, 3700, 9290, 3996, 690, 7365, 9053, 183, 2561, 526, 10811]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feba4843",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_homes_from_fl = [526, 5058, 526, 3488, 10983, 7021, 2358]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781f4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167dd33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789ba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac04ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f28b465c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patience:  20\n",
      "training_home:  [3383, 145]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1jdr9aru\" target=\"_blank\">happy-sunset-167</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 233700 batches: 0.6385\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 467400 batches: 0.4323\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 701100 batches: 0.3313\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 934800 batches: 0.2500\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 1168500 batches: 0.2006\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 1402200 batches: 0.1671\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 1635900 batches: 0.1411\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 1869600 batches: 0.1263\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 2103300 batches: 0.1138\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 2337000 batches: 0.0994\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 2570700 batches: 0.0905\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 2804400 batches: 0.0816\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 3038100 batches: 0.0743\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 3271800 batches: 0.0690\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 3505500 batches: 0.0652\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 3739200 batches: 0.0595\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 3972900 batches: 0.0560\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 4206600 batches: 0.0531\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 4440300 batches: 0.0504\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 0\n",
      "Loss after 4674000 batches: 0.0475\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 1\n",
      "Loss after 4907700 batches: 0.0451\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 2\n",
      "Loss after 5141400 batches: 0.0451\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 3\n",
      "Loss after 5375100 batches: 0.0418\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 4\n",
      "Loss after 5608800 batches: 0.0403\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 0\n",
      "Loss after 5842500 batches: 0.0387\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 1\n",
      "Loss after 6076200 batches: 0.0377\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 2\n",
      "Loss after 6309900 batches: 0.0362\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 3\n",
      "Loss after 6543600 batches: 0.0353\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 4\n",
      "Loss after 6777300 batches: 0.0346\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 5\n",
      "Loss after 7011000 batches: 0.0344\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 6\n",
      "Loss after 7244700 batches: 0.0322\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 7\n",
      "Loss after 7478400 batches: 0.0314\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 8\n",
      "Loss after 7712100 batches: 0.0306\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 9\n",
      "Loss after 7945800 batches: 0.0298\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 10\n",
      "Loss after 8179500 batches: 0.0295\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 11\n",
      "Loss after 8413200 batches: 0.0289\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 12\n",
      "Loss after 8646900 batches: 0.0281\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 13\n",
      "Loss after 8880600 batches: 0.0279\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 14\n",
      "Loss after 9114300 batches: 0.0270\n",
      "Adjusting learning rate of group 0 to 2.2013e-04.\n",
      "trigger times: 15\n",
      "Loss after 9348000 batches: 0.0266\n",
      "Adjusting learning rate of group 0 to 2.2013e-04.\n",
      "trigger times: 16\n",
      "Loss after 9581700 batches: 0.0268\n",
      "Adjusting learning rate of group 0 to 2.2013e-04.\n",
      "trigger times: 17\n",
      "Loss after 9815400 batches: 0.0255\n",
      "Adjusting learning rate of group 0 to 2.2013e-04.\n",
      "trigger times: 18\n",
      "Loss after 10049100 batches: 0.0256\n",
      "Adjusting learning rate of group 0 to 2.2013e-04.\n",
      "trigger times: 19\n",
      "Loss after 10282800 batches: 0.0250\n",
      "Adjusting learning rate of group 0 to 2.0912e-04.\n",
      "trigger times: 0\n",
      "Loss after 10516500 batches: 0.0246\n",
      "Adjusting learning rate of group 0 to 2.0912e-04.\n",
      "trigger times: 1\n",
      "Loss after 10750200 batches: 0.0245\n",
      "Adjusting learning rate of group 0 to 2.0912e-04.\n",
      "trigger times: 2\n",
      "Loss after 10983900 batches: 0.0243\n",
      "Adjusting learning rate of group 0 to 2.0912e-04.\n",
      "trigger times: 3\n",
      "Loss after 11217600 batches: 0.0231\n",
      "Adjusting learning rate of group 0 to 2.0912e-04.\n",
      "trigger times: 4\n",
      "Loss after 11451300 batches: 0.0231\n",
      "Adjusting learning rate of group 0 to 1.9867e-04.\n",
      "trigger times: 5\n",
      "Loss after 11685000 batches: 0.0233\n",
      "Adjusting learning rate of group 0 to 1.9867e-04.\n",
      "trigger times: 6\n",
      "Loss after 11918700 batches: 0.0227\n",
      "Adjusting learning rate of group 0 to 1.9867e-04.\n",
      "trigger times: 7\n",
      "Loss after 12152400 batches: 0.0219\n",
      "Adjusting learning rate of group 0 to 1.9867e-04.\n",
      "trigger times: 8\n",
      "Loss after 12386100 batches: 0.0219\n",
      "Adjusting learning rate of group 0 to 1.9867e-04.\n",
      "trigger times: 9\n",
      "Loss after 12619800 batches: 0.0221\n",
      "Adjusting learning rate of group 0 to 1.8873e-04.\n",
      "trigger times: 10\n",
      "Loss after 12853500 batches: 0.0209\n",
      "Adjusting learning rate of group 0 to 1.8873e-04.\n",
      "trigger times: 11\n",
      "Loss after 13087200 batches: 0.0211\n",
      "Adjusting learning rate of group 0 to 1.8873e-04.\n",
      "trigger times: 12\n",
      "Loss after 13320900 batches: 0.0208\n",
      "Adjusting learning rate of group 0 to 1.8873e-04.\n",
      "trigger times: 13\n",
      "Loss after 13554600 batches: 0.0212\n",
      "Adjusting learning rate of group 0 to 1.8873e-04.\n",
      "trigger times: 14\n",
      "Loss after 13788300 batches: 0.0202\n",
      "Adjusting learning rate of group 0 to 1.7930e-04.\n",
      "trigger times: 15\n",
      "Loss after 14022000 batches: 0.0201\n",
      "Adjusting learning rate of group 0 to 1.7930e-04.\n",
      "trigger times: 16\n",
      "Loss after 14255700 batches: 0.0198\n",
      "Adjusting learning rate of group 0 to 1.7930e-04.\n",
      "trigger times: 17\n",
      "Loss after 14489400 batches: 0.0194\n",
      "Adjusting learning rate of group 0 to 1.7930e-04.\n",
      "trigger times: 18\n",
      "Loss after 14723100 batches: 0.0197\n",
      "Adjusting learning rate of group 0 to 1.7930e-04.\n",
      "trigger times: 19\n",
      "Loss after 14956800 batches: 0.0194\n",
      "Adjusting learning rate of group 0 to 1.7033e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 15190500 batches: 0.0190\n",
      "Time to train on one home:  4861.634646654129\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7828... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>Training_Loss</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▃▅▆▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>Validation_F1</td><td>█▄▂▅▁▄▁▂▃▄▅▃▅▄▅▅▄▅▆▆▇▅▆▆▆▆▆▆▆▆▇▇▆▇▆▇▇▇▆▇</td></tr><tr><td>Validation_Loss</td><td>▂▄▇▄█▄▅▆▅▅▃▄▂▂▃▁▃▃▃▃▂▂▃▂▂▁▁▁▂▂▁▁▂▁▁▂▁▁▁▁</td></tr><tr><td>Validation_MAE</td><td>▂▃▆▃█▃▄▆▅▄▂▄▁▂▂▁▃▃▂▃▁▂▂▂▂▁▂▁▂▂▁▁▂▁▁▁▂▁▁▁</td></tr><tr><td>Validation_MSE</td><td>▂▄▇▄█▄▅▆▅▅▃▄▂▂▃▁▃▃▃▃▂▂▃▂▂▁▁▁▂▂▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>Validation_NDE</td><td>▂▄▇▄█▄▅▆▅▅▃▄▂▂▃▁▃▃▃▃▂▂▃▂▂▁▁▁▂▂▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>Validation_NEP</td><td>▂▃▆▃█▃▄▆▅▄▂▄▁▂▂▁▃▃▂▃▁▂▂▂▂▁▂▁▂▂▁▁▂▁▁▁▂▁▁▁</td></tr><tr><td>Validation_R2</td><td>▇▅▂▅▁▅▄▃▄▄▆▅▇▇▆█▆▆▆▆▇▇▆▇▇███▇▇██▇███████</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.50088</td></tr><tr><td>Test_Loss</td><td>0.48835</td></tr><tr><td>Test_MAE</td><td>23.11542</td></tr><tr><td>Test_MSE</td><td>1334.75317</td></tr><tr><td>Test_NDE</td><td>0.43237</td></tr><tr><td>Test_NEP</td><td>0.97842</td></tr><tr><td>Test_R2_Value</td><td>0.4722</td></tr><tr><td>Training_F1</td><td>0.9282</td></tr><tr><td>Training_Loss</td><td>0.019</td></tr><tr><td>Training_MAE</td><td>6.80647</td></tr><tr><td>Training_MSE</td><td>114.33601</td></tr><tr><td>Training_NDE</td><td>0.01384</td></tr><tr><td>Training_NEP</td><td>0.14362</td></tr><tr><td>Training_R2</td><td>0.98099</td></tr><tr><td>Validation_F1</td><td>0.52022</td></tr><tr><td>Validation_Loss</td><td>0.67591</td></tr><tr><td>Validation_MAE</td><td>27.15388</td></tr><tr><td>Validation_MSE</td><td>1859.00354</td></tr><tr><td>Validation_NDE</td><td>0.50339</td></tr><tr><td>Validation_NEP</td><td>0.99014</td></tr><tr><td>Validation_R2</td><td>0.36788</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">happy-sunset-167</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1jdr9aru\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/1jdr9aru</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220228_134540-1jdr9aru\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/dlnoui7n\" target=\"_blank\">smart-grass-168</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 364800 batches: 0.5370\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 729600 batches: 0.3581\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 1094400 batches: 0.2486\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 1459200 batches: 0.1831\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 1824000 batches: 0.1425\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 2188800 batches: 0.1181\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 2553600 batches: 0.1034\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 2918400 batches: 0.0915\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 3283200 batches: 0.0806\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 3648000 batches: 0.0736\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 4012800 batches: 0.0669\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 4377600 batches: 0.0628\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 4742400 batches: 0.0569\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 5107200 batches: 0.0539\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 5472000 batches: 0.0511\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 5836800 batches: 0.0483\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 6201600 batches: 0.0450\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 6566400 batches: 0.0430\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 6931200 batches: 0.0418\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 7296000 batches: 0.0403\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 7660800 batches: 0.0388\n",
      "Time to train on one home:  2380.5332367420197\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6132... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▄▅▆▆▆▇▇▇▇▇▇████████</td></tr><tr><td>Training_Loss</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▄▅▆▇▇▇▇▇████████████</td></tr><tr><td>Validation_F1</td><td>█▄▁▄▃▁▃▂▄▆▄▅▃▃▅▄▅▆▄▄▆</td></tr><tr><td>Validation_Loss</td><td>▁▅▇▅▇█▆▆▆▅▃▄▄▅▃▃▄▂▂▃▂</td></tr><tr><td>Validation_MAE</td><td>▁▅█▆▇█▆▇▆▅▄▄▅▆▄▄▄▃▄▄▄</td></tr><tr><td>Validation_MSE</td><td>▁▅▇▅▇█▆▆▆▅▃▄▄▅▃▃▄▂▂▃▂</td></tr><tr><td>Validation_NDE</td><td>▁▅▇▅▇█▆▆▆▅▃▄▄▅▃▃▄▂▂▃▂</td></tr><tr><td>Validation_NEP</td><td>▁▅█▆▇█▆▇▆▅▄▄▅▆▄▄▄▃▄▄▄</td></tr><tr><td>Validation_R2</td><td>█▄▂▄▂▁▃▃▃▄▆▅▅▄▆▆▅▇▇▆▇</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.4784</td></tr><tr><td>Test_Loss</td><td>0.58579</td></tr><tr><td>Test_MAE</td><td>25.0437</td></tr><tr><td>Test_MSE</td><td>1602.57117</td></tr><tr><td>Test_NDE</td><td>0.51912</td></tr><tr><td>Test_NEP</td><td>1.06003</td></tr><tr><td>Test_R2_Value</td><td>0.3663</td></tr><tr><td>Training_F1</td><td>0.91074</td></tr><tr><td>Training_Loss</td><td>0.03882</td></tr><tr><td>Training_MAE</td><td>9.99349</td></tr><tr><td>Training_MSE</td><td>231.90919</td></tr><tr><td>Training_NDE</td><td>0.02546</td></tr><tr><td>Training_NEP</td><td>0.17851</td></tr><tr><td>Training_R2</td><td>0.96118</td></tr><tr><td>Validation_F1</td><td>0.43527</td></tr><tr><td>Validation_Loss</td><td>0.75525</td></tr><tr><td>Validation_MAE</td><td>30.44778</td></tr><tr><td>Validation_MSE</td><td>2075.66943</td></tr><tr><td>Validation_NDE</td><td>0.56206</td></tr><tr><td>Validation_NEP</td><td>1.11025</td></tr><tr><td>Validation_R2</td><td>0.2942</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">smart-grass-168</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/dlnoui7n\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/dlnoui7n</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220228_150710-dlnoui7n\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1yfvrnqr\" target=\"_blank\">absurd-flower-169</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 495900 batches: 0.5861\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 991800 batches: 0.4138\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 1487700 batches: 0.3099\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 1983600 batches: 0.2268\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 3\n",
      "Loss after 2479500 batches: 0.1785\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 2975400 batches: 0.1433\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 3471300 batches: 0.1241\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 3967200 batches: 0.1063\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 4463100 batches: 0.0973\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 8\n",
      "Loss after 4959000 batches: 0.0876\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 5454900 batches: 0.0810\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 5950800 batches: 0.0735\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 6446700 batches: 0.0692\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 6942600 batches: 0.0639\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 13\n",
      "Loss after 7438500 batches: 0.0606\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 7934400 batches: 0.0562\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 8430300 batches: 0.0538\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 8926200 batches: 0.0527\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 9422100 batches: 0.0493\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 18\n",
      "Loss after 9918000 batches: 0.0474\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 10413900 batches: 0.0456\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 10909800 batches: 0.0437\n",
      "Time to train on one home:  3360.2631137371063\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9152... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▄▅▅▆▆▇▇▇▇▇▇▇████████</td></tr><tr><td>Training_Loss</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▃▅▆▆▇▇▇▇▇████████████</td></tr><tr><td>Validation_F1</td><td>█▇▄▂▂▁▁▃▂▂▂▃▃▄▃▃▃▃▃▄▄▄</td></tr><tr><td>Validation_Loss</td><td>▁▂▇█▇▇█▇▆▆▆▄▅▅▅▄▅▄▄▄▄▄</td></tr><tr><td>Validation_MAE</td><td>▁▁██▇▇▇▇▇▇▆▅▅▅▆▅▆▅▅▅▅▄</td></tr><tr><td>Validation_MSE</td><td>▁▂▇█▇▇█▇▆▆▆▄▅▅▅▄▅▄▄▄▄▄</td></tr><tr><td>Validation_NDE</td><td>▁▂▇█▇▇█▇▆▆▆▄▅▅▅▄▅▄▄▄▄▄</td></tr><tr><td>Validation_NEP</td><td>▁▁██▇▇▇▇▇▇▆▅▅▅▆▅▆▅▅▅▅▄</td></tr><tr><td>Validation_R2</td><td>█▇▂▁▂▂▁▂▃▃▃▅▄▄▄▅▄▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.37691</td></tr><tr><td>Test_Loss</td><td>0.64033</td></tr><tr><td>Test_MAE</td><td>27.0081</td></tr><tr><td>Test_MSE</td><td>1746.20776</td></tr><tr><td>Test_NDE</td><td>0.56565</td></tr><tr><td>Test_NEP</td><td>1.14318</td></tr><tr><td>Test_R2_Value</td><td>0.3095</td></tr><tr><td>Training_F1</td><td>0.9095</td></tr><tr><td>Training_Loss</td><td>0.04371</td></tr><tr><td>Training_MAE</td><td>10.78061</td></tr><tr><td>Training_MSE</td><td>249.15706</td></tr><tr><td>Training_NDE</td><td>0.02695</td></tr><tr><td>Training_NEP</td><td>0.18103</td></tr><tr><td>Training_R2</td><td>0.95628</td></tr><tr><td>Validation_F1</td><td>0.33761</td></tr><tr><td>Validation_Loss</td><td>0.86683</td></tr><tr><td>Validation_MAE</td><td>33.39671</td></tr><tr><td>Validation_MSE</td><td>2382.09448</td></tr><tr><td>Validation_NDE</td><td>0.64503</td></tr><tr><td>Validation_NEP</td><td>1.21778</td></tr><tr><td>Validation_R2</td><td>0.19001</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">absurd-flower-169</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1yfvrnqr\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/1yfvrnqr</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220228_154715-1yfvrnqr\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/2fo0w8xl\" target=\"_blank\">wise-shape-170</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 624540 batches: 0.5729\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 1249080 batches: 0.3764\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 1873620 batches: 0.2631\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 2498160 batches: 0.1931\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 3122700 batches: 0.1537\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 3747240 batches: 0.1284\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 4371780 batches: 0.1112\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 4996320 batches: 0.0988\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 5620860 batches: 0.0904\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 6245400 batches: 0.0838\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 6869940 batches: 0.0764\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 7494480 batches: 0.0699\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 8119020 batches: 0.0664\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 8743560 batches: 0.0626\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 9368100 batches: 0.0593\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 9992640 batches: 0.0561\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 10617180 batches: 0.0544\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 11241720 batches: 0.0515\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 11866260 batches: 0.0494\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 12490800 batches: 0.0483\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 13115340 batches: 0.0459\n",
      "Time to train on one home:  4000.7791361808777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7976... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▄▅▆▆▆▇▇▇▇▇▇████████</td></tr><tr><td>Training_Loss</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▄▅▆▇▇▇▇▇▇███████████</td></tr><tr><td>Validation_F1</td><td>█▆▄▁▂▂▂▄▃▃▄▄▃▅▃▅▅▅▅▅▅</td></tr><tr><td>Validation_Loss</td><td>▁▅██▇▇▆▆▅▄▅▄▄▄▄▅▃▃▃▄▃</td></tr><tr><td>Validation_MAE</td><td>▁▅██▇█▇▆▆▅▆▅▅▄▅▆▃▃▄▅▄</td></tr><tr><td>Validation_MSE</td><td>▁▅██▇▇▆▆▅▄▅▄▄▄▄▅▃▃▃▄▃</td></tr><tr><td>Validation_NDE</td><td>▁▅██▇▇▆▆▅▄▅▄▄▄▄▅▃▃▃▄▃</td></tr><tr><td>Validation_NEP</td><td>▁▅██▇█▇▆▆▅▆▅▅▄▅▆▃▃▄▅▄</td></tr><tr><td>Validation_R2</td><td>█▄▁▁▂▂▃▃▄▅▄▅▅▅▅▄▆▆▆▅▆</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.43358</td></tr><tr><td>Test_Loss</td><td>0.68045</td></tr><tr><td>Test_MAE</td><td>29.34642</td></tr><tr><td>Test_MSE</td><td>1866.20605</td></tr><tr><td>Test_NDE</td><td>0.60452</td></tr><tr><td>Test_NEP</td><td>1.24216</td></tr><tr><td>Test_R2_Value</td><td>0.26205</td></tr><tr><td>Training_F1</td><td>0.9054</td></tr><tr><td>Training_Loss</td><td>0.04592</td></tr><tr><td>Training_MAE</td><td>10.66902</td></tr><tr><td>Training_MSE</td><td>243.69234</td></tr><tr><td>Training_NDE</td><td>0.02872</td></tr><tr><td>Training_NEP</td><td>0.18923</td></tr><tr><td>Training_R2</td><td>0.95408</td></tr><tr><td>Validation_F1</td><td>0.37418</td></tr><tr><td>Validation_Loss</td><td>0.86924</td></tr><tr><td>Validation_MAE</td><td>34.05331</td></tr><tr><td>Validation_MSE</td><td>2389.35767</td></tr><tr><td>Validation_NDE</td><td>0.647</td></tr><tr><td>Validation_NEP</td><td>1.24173</td></tr><tr><td>Validation_R2</td><td>0.18754</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">wise-shape-170</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/2fo0w8xl\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/2fo0w8xl</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220228_164340-2fo0w8xl\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1jg1g8et\" target=\"_blank\">glad-salad-171</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 755640 batches: 0.5783\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 1511280 batches: 0.3954\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 2266920 batches: 0.2778\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 3022560 batches: 0.2031\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 3778200 batches: 0.1605\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 4533840 batches: 0.1363\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 5289480 batches: 0.1178\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 6045120 batches: 0.1068\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 6800760 batches: 0.0960\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 7556400 batches: 0.0877\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 8312040 batches: 0.0803\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 9067680 batches: 0.0756\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 9823320 batches: 0.0708\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 10578960 batches: 0.0664\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 11334600 batches: 0.0641\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 12090240 batches: 0.0594\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 12845880 batches: 0.0572\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 13601520 batches: 0.0548\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 14357160 batches: 0.0533\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 15112800 batches: 0.0508\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 15868440 batches: 0.0487\n",
      "Time to train on one home:  4787.226725578308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5588... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▄▅▆▆▆▇▇▇▇▇▇████████</td></tr><tr><td>Training_Loss</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▃▅▆▇▇▇▇▇▇███████████</td></tr><tr><td>Validation_F1</td><td>█▇▂▇▁▄▅▆▇▇▆▆▆▇▆▇▄▆▆▆▇</td></tr><tr><td>Validation_Loss</td><td>▁▅█████▆▇▇▆▆▆▅▆▅▇▆▅▆▅</td></tr><tr><td>Validation_MAE</td><td>▁▁▄▆▇▆▆▆▆▇▅▆▅▆▇▆██▆▇▆</td></tr><tr><td>Validation_MSE</td><td>▁▅█████▆▇▇▆▆▆▅▆▅▇▆▅▆▅</td></tr><tr><td>Validation_NDE</td><td>▁▅█████▆▇▇▆▆▆▅▆▅▇▆▅▆▅</td></tr><tr><td>Validation_NEP</td><td>▁▁▄▆▇▆▆▆▆▇▅▆▅▆▇▆██▆▇▆</td></tr><tr><td>Validation_R2</td><td>█▄▁▁▁▁▁▃▂▂▃▃▃▄▃▄▂▃▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.20592</td></tr><tr><td>Test_Loss</td><td>0.71443</td></tr><tr><td>Test_MAE</td><td>30.69693</td></tr><tr><td>Test_MSE</td><td>1947.58838</td></tr><tr><td>Test_NDE</td><td>0.63089</td></tr><tr><td>Test_NEP</td><td>1.29932</td></tr><tr><td>Test_R2_Value</td><td>0.22987</td></tr><tr><td>Training_F1</td><td>0.90829</td></tr><tr><td>Training_Loss</td><td>0.0487</td></tr><tr><td>Training_MAE</td><td>11.47495</td></tr><tr><td>Training_MSE</td><td>266.70609</td></tr><tr><td>Training_NDE</td><td>0.0284</td></tr><tr><td>Training_NEP</td><td>0.18343</td></tr><tr><td>Training_R2</td><td>0.95131</td></tr><tr><td>Validation_F1</td><td>0.35226</td></tr><tr><td>Validation_Loss</td><td>0.9158</td></tr><tr><td>Validation_MAE</td><td>35.38571</td></tr><tr><td>Validation_MSE</td><td>2520.25513</td></tr><tr><td>Validation_NDE</td><td>0.68245</td></tr><tr><td>Validation_NEP</td><td>1.29031</td></tr><tr><td>Validation_R2</td><td>0.14303</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glad-salad-171</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1jg1g8et\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/1jg1g8et</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220228_175046-1jg1g8et\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1g9q2hvd\" target=\"_blank\">brisk-moon-172</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 886740 batches: 0.6060\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 1773480 batches: 0.4160\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 2660220 batches: 0.2877\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 3546960 batches: 0.2128\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 4433700 batches: 0.1722\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 5320440 batches: 0.1460\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 6207180 batches: 0.1300\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 7093920 batches: 0.1164\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 7980660 batches: 0.1070\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 8867400 batches: 0.0976\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 9754140 batches: 0.0912\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 10640880 batches: 0.0834\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 11527620 batches: 0.0800\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 12414360 batches: 0.0763\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 13301100 batches: 0.0709\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 14187840 batches: 0.0671\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 15074580 batches: 0.0642\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 15961320 batches: 0.0623\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 16848060 batches: 0.0606\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 17734800 batches: 0.0590\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 18621540 batches: 0.0561\n",
      "Time to train on one home:  5640.650081157684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6796... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▄▅▆▆▇▇▇▇▇▇▇████████</td></tr><tr><td>Training_Loss</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▃▅▆▇▇▇▇▇▇███████████</td></tr><tr><td>Validation_F1</td><td>█▇▃▃▂▃▃▄▂▂▁▄▃▁▃▂▁▃▂▂▂</td></tr><tr><td>Validation_Loss</td><td>▁▄▇▆▆▇▆▆▆█▆▅▆▆▆▆▆▇▅▆▅</td></tr><tr><td>Validation_MAE</td><td>▁▄▆▆▆▇▆▆▇█▇▅▇▆▇▇▇█▇▇▇</td></tr><tr><td>Validation_MSE</td><td>▁▄▇▆▆▇▆▆▆█▆▅▆▆▆▆▆▇▅▆▆</td></tr><tr><td>Validation_NDE</td><td>▁▄▇▆▆▇▆▆▆█▆▅▆▆▆▆▆▇▅▆▆</td></tr><tr><td>Validation_NEP</td><td>▁▄▆▆▆▇▆▆▇█▇▅▇▆▇▇▇█▇▇▇</td></tr><tr><td>Validation_R2</td><td>█▅▂▃▃▂▃▃▃▁▃▄▃▃▃▃▃▂▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.35794</td></tr><tr><td>Test_Loss</td><td>0.68827</td></tr><tr><td>Test_MAE</td><td>28.26361</td></tr><tr><td>Test_MSE</td><td>1881.54224</td></tr><tr><td>Test_NDE</td><td>0.60949</td></tr><tr><td>Test_NEP</td><td>1.19633</td></tr><tr><td>Test_R2_Value</td><td>0.25599</td></tr><tr><td>Training_F1</td><td>0.89787</td></tr><tr><td>Training_Loss</td><td>0.05605</td></tr><tr><td>Training_MAE</td><td>12.25922</td></tr><tr><td>Training_MSE</td><td>309.11206</td></tr><tr><td>Training_NDE</td><td>0.03391</td></tr><tr><td>Training_NEP</td><td>0.20426</td></tr><tr><td>Training_R2</td><td>0.94394</td></tr><tr><td>Validation_F1</td><td>0.29938</td></tr><tr><td>Validation_Loss</td><td>0.95469</td></tr><tr><td>Validation_MAE</td><td>36.95933</td></tr><tr><td>Validation_MSE</td><td>2624.20776</td></tr><tr><td>Validation_NDE</td><td>0.7106</td></tr><tr><td>Validation_NEP</td><td>1.34769</td></tr><tr><td>Validation_R2</td><td>0.10768</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">brisk-moon-172</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1g9q2hvd\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/1g9q2hvd</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220301_090838-1g9q2hvd\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/36yrcjjj\" target=\"_blank\">smooth-cherry-173</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 1017840 batches: 0.5664\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 2035680 batches: 0.3898\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 3053520 batches: 0.2680\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 4071360 batches: 0.1945\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 5089200 batches: 0.1564\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 6107040 batches: 0.1316\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 7124880 batches: 0.1155\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 8142720 batches: 0.1044\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 9160560 batches: 0.0939\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 10178400 batches: 0.0881\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 11196240 batches: 0.0801\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 12214080 batches: 0.0750\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 13231920 batches: 0.0710\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 14249760 batches: 0.0671\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 15267600 batches: 0.0642\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 16285440 batches: 0.0602\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 17303280 batches: 0.0584\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 18321120 batches: 0.0564\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 19338960 batches: 0.0544\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 20356800 batches: 0.0529\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 21374640 batches: 0.0503\n",
      "Time to train on one home:  6466.372946739197\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8484... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▄▅▆▆▇▇▇▇▇▇▇████████</td></tr><tr><td>Training_Loss</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▃▅▆▇▇▇▇▇▇███████████</td></tr><tr><td>Validation_F1</td><td>█▁█▇▇▇▅██████▇█▇▇███▇</td></tr><tr><td>Validation_Loss</td><td>▁█▅▅▅▃▆▄▄▄▃▃▃▄▃▄▄▃▃▃▃</td></tr><tr><td>Validation_MAE</td><td>▁█▅▅▆▄▇▅▆▆▄▅▅▅▅▆▆▅▅▅▆</td></tr><tr><td>Validation_MSE</td><td>▁█▅▅▅▃▆▄▄▄▃▃▃▄▃▄▄▃▃▃▃</td></tr><tr><td>Validation_NDE</td><td>▁█▅▅▅▃▆▄▄▄▃▃▃▄▃▄▄▃▃▃▃</td></tr><tr><td>Validation_NEP</td><td>▁█▅▅▆▄▇▅▆▆▄▅▅▅▅▆▆▅▅▅▆</td></tr><tr><td>Validation_R2</td><td>█▁▄▄▄▆▃▅▅▅▆▆▆▅▆▅▅▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.09387</td></tr><tr><td>Test_Loss</td><td>0.80964</td></tr><tr><td>Test_MAE</td><td>32.47326</td></tr><tr><td>Test_MSE</td><td>2213.76367</td></tr><tr><td>Test_NDE</td><td>0.71711</td></tr><tr><td>Test_NEP</td><td>1.37451</td></tr><tr><td>Test_R2_Value</td><td>0.12462</td></tr><tr><td>Training_F1</td><td>0.90108</td></tr><tr><td>Training_Loss</td><td>0.05034</td></tr><tr><td>Training_MAE</td><td>11.4162</td></tr><tr><td>Training_MSE</td><td>272.43423</td></tr><tr><td>Training_NDE</td><td>0.03117</td></tr><tr><td>Training_NEP</td><td>0.19785</td></tr><tr><td>Training_R2</td><td>0.94966</td></tr><tr><td>Validation_F1</td><td>0.24439</td></tr><tr><td>Validation_Loss</td><td>0.97893</td></tr><tr><td>Validation_MAE</td><td>37.16924</td></tr><tr><td>Validation_MSE</td><td>2691.14795</td></tr><tr><td>Validation_NDE</td><td>0.72872</td></tr><tr><td>Validation_NEP</td><td>1.35535</td></tr><tr><td>Validation_R2</td><td>0.08492</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">smooth-cherry-173</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/36yrcjjj\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/36yrcjjj</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220301_104304-36yrcjjj\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/9x7vdix9\" target=\"_blank\">dandy-galaxy-174</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 1096440 batches: 0.5678\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 2192880 batches: 0.3903\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 3289320 batches: 0.2728\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 4385760 batches: 0.1992\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 5482200 batches: 0.1596\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 6578640 batches: 0.1329\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 7675080 batches: 0.1184\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 8771520 batches: 0.1065\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 9867960 batches: 0.0973\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 10964400 batches: 0.0900\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 12060840 batches: 0.0818\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 13157280 batches: 0.0764\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 14253720 batches: 0.0716\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 15350160 batches: 0.0686\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 16446600 batches: 0.0655\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 17543040 batches: 0.0610\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 18639480 batches: 0.0593\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 19735920 batches: 0.0569\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 20832360 batches: 0.0551\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 21928800 batches: 0.0539\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 23025240 batches: 0.0518\n",
      "Time to train on one home:  6951.265882492065\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2564... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▄▅▆▆▇▇▇▇▇▇▇████████</td></tr><tr><td>Training_Loss</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▃▅▆▇▇▇▇▇▇███████████</td></tr><tr><td>Validation_F1</td><td>▄▁▄▁▂▅▇▇▆▁▇▅▇▆▆▆▅▆▆▆█</td></tr><tr><td>Validation_Loss</td><td>▂▇▆█▆▄▄▅▃▅▃▄▂▃▃▂▃▃▂▃▁</td></tr><tr><td>Validation_MAE</td><td>▁█▅▇▆▅▃▅▄▅▄▅▄▃▄▃▅▄▄▅▃</td></tr><tr><td>Validation_MSE</td><td>▂▇▆█▆▅▄▅▃▅▃▄▂▃▃▂▃▃▂▃▁</td></tr><tr><td>Validation_NDE</td><td>▂▇▆█▆▅▄▅▃▅▃▄▂▃▃▂▃▃▂▃▁</td></tr><tr><td>Validation_NEP</td><td>▁█▅▇▆▅▃▅▄▅▄▅▄▃▄▃▅▄▄▅▃</td></tr><tr><td>Validation_R2</td><td>▇▂▃▁▃▄▅▄▆▄▆▅▇▆▆▇▆▆▇▆█</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.04648</td></tr><tr><td>Test_Loss</td><td>0.79556</td></tr><tr><td>Test_MAE</td><td>31.25917</td></tr><tr><td>Test_MSE</td><td>2175.47876</td></tr><tr><td>Test_NDE</td><td>0.70471</td></tr><tr><td>Test_NEP</td><td>1.32312</td></tr><tr><td>Test_R2_Value</td><td>0.13975</td></tr><tr><td>Training_F1</td><td>0.89744</td></tr><tr><td>Training_Loss</td><td>0.05185</td></tr><tr><td>Training_MAE</td><td>11.40619</td></tr><tr><td>Training_MSE</td><td>272.83249</td></tr><tr><td>Training_NDE</td><td>0.03266</td></tr><tr><td>Training_NEP</td><td>0.20514</td></tr><tr><td>Training_R2</td><td>0.94815</td></tr><tr><td>Validation_F1</td><td>0.33791</td></tr><tr><td>Validation_Loss</td><td>0.90176</td></tr><tr><td>Validation_MAE</td><td>34.85632</td></tr><tr><td>Validation_MSE</td><td>2481.10327</td></tr><tr><td>Validation_NDE</td><td>0.67184</td></tr><tr><td>Validation_NEP</td><td>1.27101</td></tr><tr><td>Validation_R2</td><td>0.15634</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dandy-galaxy-174</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/9x7vdix9\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/9x7vdix9</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220301_123115-9x7vdix9\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]_Test_home_[3383]_total_homes_1': [0.7955590923627217, 0.1397545033989599, 0.04647726666181743, 1.323119405328264, 0.7047084547625869, 31.259170188356897, 2175.4788]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1uzembru\" target=\"_blank\">easy-terrain-175</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 1227540 batches: 0.5629\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 2455080 batches: 0.3972\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 3682620 batches: 0.2861\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 4910160 batches: 0.2100\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 6137700 batches: 0.1685\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 7365240 batches: 0.1403\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 8592780 batches: 0.1230\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 9820320 batches: 0.1112\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 11047860 batches: 0.1024\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 12275400 batches: 0.0936\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 13502940 batches: 0.0859\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 14730480 batches: 0.0811\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 15958020 batches: 0.0760\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 17185560 batches: 0.0729\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 18413100 batches: 0.0700\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 19640640 batches: 0.0653\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 20868180 batches: 0.0634\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 22095720 batches: 0.0613\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 23323260 batches: 0.0593\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 24550800 batches: 0.0574\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 25778340 batches: 0.0549\n",
      "Time to train on one home:  7854.06521320343\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 7340... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▄▅▆▆▇▇▇▇▇▇▇████████</td></tr><tr><td>Training_Loss</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▃▅▆▆▇▇▇▇▇███████████</td></tr><tr><td>Validation_F1</td><td>▇▁▄█▅▇▆▆▅▃▆▆▄█▇▅▆▆▆▆▆</td></tr><tr><td>Validation_Loss</td><td>▁▃▅▆█▃█▅▆▆▄▃▃▂▂▄▃▃▁▂▁</td></tr><tr><td>Validation_MAE</td><td>▁▅▅▅█▃▇▆██▅▅▄▄▃▅▄▄▂▄▂</td></tr><tr><td>Validation_MSE</td><td>▁▃▅▆█▃█▅▆▆▄▃▃▂▂▄▃▃▁▂▁</td></tr><tr><td>Validation_NDE</td><td>▁▃▅▆█▃█▅▆▆▄▃▃▂▂▄▃▃▁▂▁</td></tr><tr><td>Validation_NEP</td><td>▁▅▅▅█▃▇▆██▅▅▄▄▃▅▄▄▂▄▂</td></tr><tr><td>Validation_R2</td><td>█▆▄▃▁▆▁▄▃▃▅▆▆▇▇▅▆▆█▇█</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.30343</td></tr><tr><td>Test_Loss</td><td>0.73638</td></tr><tr><td>Test_MAE</td><td>30.56782</td></tr><tr><td>Test_MSE</td><td>2016.07666</td></tr><tr><td>Test_NDE</td><td>0.65307</td></tr><tr><td>Test_NEP</td><td>1.29386</td></tr><tr><td>Test_R2_Value</td><td>0.20279</td></tr><tr><td>Training_F1</td><td>0.89323</td></tr><tr><td>Training_Loss</td><td>0.0549</td></tr><tr><td>Training_MAE</td><td>11.40195</td></tr><tr><td>Training_MSE</td><td>279.96802</td></tr><tr><td>Training_NDE</td><td>0.03522</td></tr><tr><td>Training_NEP</td><td>0.21356</td></tr><tr><td>Training_R2</td><td>0.94509</td></tr><tr><td>Validation_F1</td><td>0.31041</td></tr><tr><td>Validation_Loss</td><td>0.89523</td></tr><tr><td>Validation_MAE</td><td>34.44019</td></tr><tr><td>Validation_MSE</td><td>2462.8313</td></tr><tr><td>Validation_NDE</td><td>0.6669</td></tr><tr><td>Validation_NEP</td><td>1.25583</td></tr><tr><td>Validation_R2</td><td>0.16256</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">easy-terrain-175</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1uzembru\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/1uzembru</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220301_142731-1uzembru\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]_Test_home_[3383]_total_homes_1': [0.7955590923627217, 0.1397545033989599, 0.04647726666181743, 1.323119405328264, 0.7047084547625869, 31.259170188356897, 2175.4788], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240]_Test_home_[3383]_total_homes_1': [0.7363784319824642, 0.2027867142757257, 0.3034292298291827, 1.29385622862555, 0.6530728087722938, 30.567817150136477, 2016.0767]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/3f9rcyse\" target=\"_blank\">ruby-wind-176</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 1358640 batches: 0.5193\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 2717280 batches: 0.3408\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 4075920 batches: 0.2282\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 5434560 batches: 0.1694\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 6793200 batches: 0.1398\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 8151840 batches: 0.1187\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 9510480 batches: 0.1057\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 10869120 batches: 0.0960\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 12227760 batches: 0.0880\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 13586400 batches: 0.0817\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 14945040 batches: 0.0746\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 16303680 batches: 0.0708\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 17662320 batches: 0.0666\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 19020960 batches: 0.0641\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 20379600 batches: 0.0612\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 21738240 batches: 0.0582\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 23096880 batches: 0.0562\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 24455520 batches: 0.0542\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 25814160 batches: 0.0527\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 27172800 batches: 0.0514\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 28531440 batches: 0.0491\n",
      "Time to train on one home:  8584.959830284119\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 6260... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▅▅▆▆▇▇▇▇▇▇█████████</td></tr><tr><td>Training_Loss</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▄▅▆▇▇▇▇▇████████████</td></tr><tr><td>Validation_F1</td><td>▅█▃▄▅▅▆▄▁▇▂▃██▅▁▃▄▂▆▅</td></tr><tr><td>Validation_Loss</td><td>▁▇█▆▇█▇▇▇▅▇▆▄▆▆▆▆▆▅▅▅</td></tr><tr><td>Validation_MAE</td><td>▁▆█▆▇▇▇▇▆▄█▇▄▆▆█▇▆▇▆▅</td></tr><tr><td>Validation_MSE</td><td>▁▇█▆▇█▇▇▇▅▇▆▄▆▆▆▆▆▅▅▅</td></tr><tr><td>Validation_NDE</td><td>▁▇█▆▇█▇▇▇▅▇▆▄▆▆▆▆▆▅▅▅</td></tr><tr><td>Validation_NEP</td><td>▁▆█▆▇▇▇▇▆▄█▇▄▆▆█▇▆▇▆▅</td></tr><tr><td>Validation_R2</td><td>█▂▁▃▂▁▂▂▂▄▂▃▅▃▃▃▃▃▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.22412</td></tr><tr><td>Test_Loss</td><td>0.73595</td></tr><tr><td>Test_MAE</td><td>31.41698</td></tr><tr><td>Test_MSE</td><td>2010.47595</td></tr><tr><td>Test_NDE</td><td>0.65126</td></tr><tr><td>Test_NEP</td><td>1.3298</td></tr><tr><td>Test_R2_Value</td><td>0.205</td></tr><tr><td>Training_F1</td><td>0.90196</td></tr><tr><td>Training_Loss</td><td>0.04913</td></tr><tr><td>Training_MAE</td><td>10.84107</td></tr><tr><td>Training_MSE</td><td>249.19864</td></tr><tr><td>Training_NDE</td><td>0.03066</td></tr><tr><td>Training_NEP</td><td>0.19608</td></tr><tr><td>Training_R2</td><td>0.95087</td></tr><tr><td>Validation_F1</td><td>0.326</td></tr><tr><td>Validation_Loss</td><td>0.96485</td></tr><tr><td>Validation_MAE</td><td>36.17854</td></tr><tr><td>Validation_MSE</td><td>2651.12524</td></tr><tr><td>Validation_NDE</td><td>0.71788</td></tr><tr><td>Validation_NEP</td><td>1.31922</td></tr><tr><td>Validation_R2</td><td>0.09853</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">ruby-wind-176</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/3f9rcyse\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/3f9rcyse</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220301_163850-3f9rcyse\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]_Test_home_[3383]_total_homes_1': [0.7955590923627217, 0.1397545033989599, 0.04647726666181743, 1.323119405328264, 0.7047084547625869, 31.259170188356897, 2175.4788], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240]_Test_home_[3383]_total_homes_1': [0.7363784319824642, 0.2027867142757257, 0.3034292298291827, 1.29385622862555, 0.6530728087722938, 30.567817150136477, 2016.0767], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526]_Test_home_[3383]_total_homes_1': [0.7359541899628109, 0.20500130184728738, 0.2241232947225627, 1.3297989125279133, 0.6512586306200592, 31.416975940042928, 2010.476]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/3sofq96s\" target=\"_blank\">absurd-plant-177</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 1489740 batches: 0.5188\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 2979480 batches: 0.3277\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 4469220 batches: 0.2202\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 5958960 batches: 0.1698\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 3\n",
      "Loss after 7448700 batches: 0.1430\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 0\n",
      "Loss after 8938440 batches: 0.1226\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 1\n",
      "Loss after 10428180 batches: 0.1091\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 0\n",
      "Loss after 11917920 batches: 0.1000\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 0\n",
      "Loss after 13407660 batches: 0.0918\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 0\n",
      "Loss after 14897400 batches: 0.0854\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 1\n",
      "Loss after 16387140 batches: 0.0797\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 2\n",
      "Loss after 17876880 batches: 0.0747\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 3\n",
      "Loss after 19366620 batches: 0.0710\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 4\n",
      "Loss after 20856360 batches: 0.0681\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 5\n",
      "Loss after 22346100 batches: 0.0648\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 0\n",
      "Loss after 23835840 batches: 0.0610\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 1\n",
      "Loss after 25325580 batches: 0.0589\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 2\n",
      "Loss after 26815320 batches: 0.0576\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 3\n",
      "Loss after 28305060 batches: 0.0553\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 4\n",
      "Loss after 29794800 batches: 0.0542\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 5\n",
      "Loss after 31284540 batches: 0.0519\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 6\n",
      "Loss after 32774280 batches: 0.0505\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 7\n",
      "Loss after 34264020 batches: 0.0497\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 8\n",
      "Loss after 35753760 batches: 0.0488\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 9\n",
      "Loss after 37243500 batches: 0.0482\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 10\n",
      "Loss after 38733240 batches: 0.0463\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 11\n",
      "Loss after 40222980 batches: 0.0454\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 12\n",
      "Loss after 41712720 batches: 0.0444\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 13\n",
      "Loss after 43202460 batches: 0.0439\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 14\n",
      "Loss after 44692200 batches: 0.0437\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 15\n",
      "Loss after 46181940 batches: 0.0423\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 16\n",
      "Loss after 47671680 batches: 0.0415\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 17\n",
      "Loss after 49161420 batches: 0.0412\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 18\n",
      "Loss after 50651160 batches: 0.0407\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 19\n",
      "Loss after 52140900 batches: 0.0404\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 53630640 batches: 0.0393\n",
      "Time to train on one home:  16256.107741832733\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 748... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>Training_Loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▄▅▆▆▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>Validation_F1</td><td>█▁▅▁▂▃▄▅▄▃▄▄▇▄▅▅▅▄▄▄▅▅▅▆▄▅▄▅▄▄▄▄▄▄▅▆</td></tr><tr><td>Validation_Loss</td><td>▃▇▇██▅▅▄▄▃▅▄▂▃▃▂▂▃▃▂▂▁▂▂▂▁▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>Validation_MAE</td><td>▅▅▇██▄▄▃▃▁▄▄▂▂▃▁▂▄▃▃▂▁▄▂▂▃▃▃▃▂▂▃▃▃▂▂</td></tr><tr><td>Validation_MSE</td><td>▃▇███▅▅▄▄▃▅▄▂▃▃▂▂▃▃▂▂▁▂▂▂▁▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>Validation_NDE</td><td>▃▇███▅▅▄▄▃▅▄▂▃▃▂▂▃▃▂▂▁▂▂▂▁▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>Validation_NEP</td><td>▅▅▇██▄▄▃▃▁▄▄▂▂▃▁▂▄▃▃▂▁▄▂▂▃▃▃▃▂▂▃▃▃▂▂</td></tr><tr><td>Validation_R2</td><td>▆▂▁▁▁▄▄▅▅▆▄▅▇▆▆▇▇▆▆▇▇█▇▇▇█▇▇▇▇▇▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.06158</td></tr><tr><td>Test_Loss</td><td>0.89386</td></tr><tr><td>Test_MAE</td><td>33.88913</td></tr><tr><td>Test_MSE</td><td>2436.69702</td></tr><tr><td>Test_NDE</td><td>0.78933</td></tr><tr><td>Test_NEP</td><td>1.43444</td></tr><tr><td>Test_R2_Value</td><td>0.03646</td></tr><tr><td>Training_F1</td><td>0.91135</td></tr><tr><td>Training_Loss</td><td>0.03926</td></tr><tr><td>Training_MAE</td><td>9.66874</td></tr><tr><td>Training_MSE</td><td>195.60643</td></tr><tr><td>Training_NDE</td><td>0.02458</td></tr><tr><td>Training_NEP</td><td>0.17731</td></tr><tr><td>Training_R2</td><td>0.96074</td></tr><tr><td>Validation_F1</td><td>0.26308</td></tr><tr><td>Validation_Loss</td><td>0.99993</td></tr><tr><td>Validation_MAE</td><td>37.03171</td></tr><tr><td>Validation_MSE</td><td>2747.8252</td></tr><tr><td>Validation_NDE</td><td>0.74407</td></tr><tr><td>Validation_NEP</td><td>1.35033</td></tr><tr><td>Validation_R2</td><td>0.06565</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">absurd-plant-177</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/3sofq96s\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/3sofq96s</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220301_190220-3sofq96s\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]_Test_home_[3383]_total_homes_1': [0.7955590923627217, 0.1397545033989599, 0.04647726666181743, 1.323119405328264, 0.7047084547625869, 31.259170188356897, 2175.4788], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240]_Test_home_[3383]_total_homes_1': [0.7363784319824642, 0.2027867142757257, 0.3034292298291827, 1.29385622862555, 0.6530728087722938, 30.567817150136477, 2016.0767], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526]_Test_home_[3383]_total_homes_1': [0.7359541899628109, 0.20500130184728738, 0.2241232947225627, 1.3297989125279133, 0.6512586306200592, 31.416975940042928, 2010.476], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672]_Test_home_[3383]_total_homes_1': [0.8938575671778785, 0.03646150824903194, 0.06157500691977571, 1.4344386458988054, 0.7893255173191654, 33.889127146300446, 2436.697]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/24syw4hb\" target=\"_blank\">devoted-morning-178</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 1620840 batches: 0.5636\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 3241680 batches: 0.3835\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 4862520 batches: 0.2642\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 6483360 batches: 0.1971\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 8104200 batches: 0.1606\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 9725040 batches: 0.1383\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 11345880 batches: 0.1238\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 12966720 batches: 0.1132\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 14587560 batches: 0.1043\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 16208400 batches: 0.0987\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 17829240 batches: 0.0907\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 19450080 batches: 0.0859\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 21070920 batches: 0.0815\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 22691760 batches: 0.0782\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 24312600 batches: 0.0749\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 25933440 batches: 0.0717\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 27554280 batches: 0.0693\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 29175120 batches: 0.0667\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 30795960 batches: 0.0653\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 32416800 batches: 0.0633\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 34037640 batches: 0.0612\n",
      "Time to train on one home:  10671.765211105347\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9512... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▄▅▆▆▇▇▇▇▇▇█████████</td></tr><tr><td>Training_Loss</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▄▅▆▇▇▇▇▇▇███████████</td></tr><tr><td>Validation_F1</td><td>▇▆▁▂▄▄▅▃▄▆▆▇▄▅▆▇▆██▇▇</td></tr><tr><td>Validation_Loss</td><td>▁▁▇▆▅▆█▆▆▄▄▃▅▃▄▃▄▁▂▄▁</td></tr><tr><td>Validation_MAE</td><td>▁▁▇▅▅▅█▅▆▄▄▃▅▃▄▃▄▁▂▄▂</td></tr><tr><td>Validation_MSE</td><td>▁▁▇▆▅▆█▆▆▄▄▃▅▃▄▃▄▁▂▄▁</td></tr><tr><td>Validation_NDE</td><td>▁▁▇▆▅▆█▆▆▄▄▃▅▃▄▃▄▁▂▄▁</td></tr><tr><td>Validation_NEP</td><td>▁▁▇▅▅▅█▅▆▄▄▃▅▃▄▃▄▁▂▄▂</td></tr><tr><td>Validation_R2</td><td>██▂▃▄▃▁▃▃▅▅▆▄▆▅▆▅█▇▅█</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.30181</td></tr><tr><td>Test_Loss</td><td>0.69667</td></tr><tr><td>Test_MAE</td><td>28.94458</td></tr><tr><td>Test_MSE</td><td>1901.297</td></tr><tr><td>Test_NDE</td><td>0.61589</td></tr><tr><td>Test_NEP</td><td>1.22515</td></tr><tr><td>Test_R2_Value</td><td>0.24817</td></tr><tr><td>Training_F1</td><td>0.88921</td></tr><tr><td>Training_Loss</td><td>0.06124</td></tr><tr><td>Training_MAE</td><td>12.01418</td></tr><tr><td>Training_MSE</td><td>303.80576</td></tr><tr><td>Training_NDE</td><td>0.03846</td></tr><tr><td>Training_NEP</td><td>0.22159</td></tr><tr><td>Training_R2</td><td>0.93876</td></tr><tr><td>Validation_F1</td><td>0.36269</td></tr><tr><td>Validation_Loss</td><td>0.88609</td></tr><tr><td>Validation_MAE</td><td>34.28404</td></tr><tr><td>Validation_MSE</td><td>2436.92822</td></tr><tr><td>Validation_NDE</td><td>0.65988</td></tr><tr><td>Validation_NEP</td><td>1.25014</td></tr><tr><td>Validation_R2</td><td>0.17136</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">devoted-morning-178</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/24syw4hb\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/24syw4hb</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220301_233341-24syw4hb\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]_Test_home_[3383]_total_homes_1': [0.7955590923627217, 0.1397545033989599, 0.04647726666181743, 1.323119405328264, 0.7047084547625869, 31.259170188356897, 2175.4788], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240]_Test_home_[3383]_total_homes_1': [0.7363784319824642, 0.2027867142757257, 0.3034292298291827, 1.29385622862555, 0.6530728087722938, 30.567817150136477, 2016.0767], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526]_Test_home_[3383]_total_homes_1': [0.7359541899628109, 0.20500130184728738, 0.2241232947225627, 1.3297989125279133, 0.6512586306200592, 31.416975940042928, 2010.476], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672]_Test_home_[3383]_total_homes_1': [0.8938575671778785, 0.03646150824903194, 0.06157500691977571, 1.4344386458988054, 0.7893255173191654, 33.889127146300446, 2436.697], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021]_Test_home_[3383]_total_homes_1': [0.6966728157467312, 0.2481737472120581, 0.30181122567035945, 1.2251486728388805, 0.6158919970468059, 28.944576518253612, 1901.297]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1slci277\" target=\"_blank\">laced-bee-179</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 1751940 batches: 0.5350\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 3503880 batches: 0.3417\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 5255820 batches: 0.2300\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 7007760 batches: 0.1754\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 8759700 batches: 0.1464\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 10511640 batches: 0.1253\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 12263580 batches: 0.1121\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 14015520 batches: 0.1020\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 15767460 batches: 0.0948\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 17519400 batches: 0.0885\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 19271340 batches: 0.0817\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 21023280 batches: 0.0774\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 22775220 batches: 0.0732\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 24527160 batches: 0.0699\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 26279100 batches: 0.0673\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 28031040 batches: 0.0639\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 29782980 batches: 0.0622\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 31534920 batches: 0.0596\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 33286860 batches: 0.0581\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 35038800 batches: 0.0568\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 36790740 batches: 0.0546\n",
      "Time to train on one home:  11355.985830307007\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10580... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▅▅▆▆▇▇▇▇▇▇█████████</td></tr><tr><td>Training_Loss</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▄▅▆▇▇▇▇▇████████████</td></tr><tr><td>Validation_F1</td><td>█▁▂▄▃▁▃▅▂▄▄▄▅▄▅▃▃▃▃▄▄</td></tr><tr><td>Validation_Loss</td><td>▁██▆▅▅▅▄▅▄▄▃▃▄▃▃▃▃▃▃▄</td></tr><tr><td>Validation_MAE</td><td>▁██▇▅▅▅▅▇▄▅▄▅▆▅▄▃▄▄▄▅</td></tr><tr><td>Validation_MSE</td><td>▁██▆▅▅▅▄▅▄▄▃▃▄▃▃▃▃▃▃▄</td></tr><tr><td>Validation_NDE</td><td>▁██▆▅▅▅▄▅▄▄▃▃▄▃▃▃▃▃▃▄</td></tr><tr><td>Validation_NEP</td><td>▁██▇▅▅▅▅▇▄▅▄▅▆▅▄▃▄▄▄▅</td></tr><tr><td>Validation_R2</td><td>█▁▁▃▄▄▄▅▄▅▅▆▆▅▆▆▆▆▆▆▅</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.29494</td></tr><tr><td>Test_Loss</td><td>0.78116</td></tr><tr><td>Test_MAE</td><td>30.61021</td></tr><tr><td>Test_MSE</td><td>2131.1731</td></tr><tr><td>Test_NDE</td><td>0.69036</td></tr><tr><td>Test_NEP</td><td>1.29565</td></tr><tr><td>Test_R2_Value</td><td>0.15727</td></tr><tr><td>Training_F1</td><td>0.89273</td></tr><tr><td>Training_Loss</td><td>0.05462</td></tr><tr><td>Training_MAE</td><td>11.62745</td></tr><tr><td>Training_MSE</td><td>277.34113</td></tr><tr><td>Training_NDE</td><td>0.0346</td></tr><tr><td>Training_NEP</td><td>0.21455</td></tr><tr><td>Training_R2</td><td>0.94538</td></tr><tr><td>Validation_F1</td><td>0.32168</td></tr><tr><td>Validation_Loss</td><td>0.96249</td></tr><tr><td>Validation_MAE</td><td>35.75402</td></tr><tr><td>Validation_MSE</td><td>2646.89697</td></tr><tr><td>Validation_NDE</td><td>0.71674</td></tr><tr><td>Validation_NEP</td><td>1.30374</td></tr><tr><td>Validation_R2</td><td>0.09997</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">laced-bee-179</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1slci277\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/1slci277</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220302_023159-1slci277\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]_Test_home_[3383]_total_homes_1': [0.7955590923627217, 0.1397545033989599, 0.04647726666181743, 1.323119405328264, 0.7047084547625869, 31.259170188356897, 2175.4788], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240]_Test_home_[3383]_total_homes_1': [0.7363784319824642, 0.2027867142757257, 0.3034292298291827, 1.29385622862555, 0.6530728087722938, 30.567817150136477, 2016.0767], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526]_Test_home_[3383]_total_homes_1': [0.7359541899628109, 0.20500130184728738, 0.2241232947225627, 1.3297989125279133, 0.6512586306200592, 31.416975940042928, 2010.476], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672]_Test_home_[3383]_total_homes_1': [0.8938575671778785, 0.03646150824903194, 0.06157500691977571, 1.4344386458988054, 0.7893255173191654, 33.889127146300446, 2436.697], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021]_Test_home_[3383]_total_homes_1': [0.6966728157467312, 0.2481737472120581, 0.30181122567035945, 1.2251486728388805, 0.6158919970468059, 28.944576518253612, 1901.297], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069]_Test_home_[3383]_total_homes_1': [0.7811614433924358, 0.1572742344566146, 0.29493593617659514, 1.2956507687551084, 0.6903563859051755, 30.610213803903175, 2131.173]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/f3gr7v8r\" target=\"_blank\">astral-bird-180</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 1883040 batches: 0.5432\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 3766080 batches: 0.3381\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 5649120 batches: 0.2274\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 7532160 batches: 0.1775\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 9415200 batches: 0.1489\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 11298240 batches: 0.1295\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 13181280 batches: 0.1174\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 15064320 batches: 0.1078\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 16947360 batches: 0.1005\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 18830400 batches: 0.0945\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 20713440 batches: 0.0878\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 22596480 batches: 0.0835\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 24479520 batches: 0.0798\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 26362560 batches: 0.0765\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 28245600 batches: 0.0742\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 30128640 batches: 0.0705\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 32011680 batches: 0.0682\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 33894720 batches: 0.0659\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 35777760 batches: 0.0646\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 37660800 batches: 0.0630\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 39543840 batches: 0.0606\n",
      "Time to train on one home:  13052.089648246765\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13340... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▅▆▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>Training_Loss</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▄▆▆▇▇▇▇▇████████████</td></tr><tr><td>Validation_F1</td><td>▆▁▅▇▇▇▆█▅▄▆▇▇▅█▇▆▆▇▆▅</td></tr><tr><td>Validation_Loss</td><td>▁█▆▆▇▆▅▅▅▆█▅▅▆▅▅▅▅▅▅▅</td></tr><tr><td>Validation_MAE</td><td>▁▆▆▇▇█▆▆▆▇█▇▇▇▇▇▇▇▆█▇</td></tr><tr><td>Validation_MSE</td><td>▁█▆▆▇▆▅▅▅▆█▅▅▆▅▅▅▅▅▅▄</td></tr><tr><td>Validation_NDE</td><td>▁█▆▆▇▆▅▅▅▆█▅▅▆▅▅▅▅▅▅▄</td></tr><tr><td>Validation_NEP</td><td>▁▆▆▇▇█▆▆▆▇█▇▇▇▇▇▇▇▆█▇</td></tr><tr><td>Validation_R2</td><td>█▁▃▃▂▃▄▄▄▃▁▄▄▃▄▄▄▄▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.07742</td></tr><tr><td>Test_Loss</td><td>0.88957</td></tr><tr><td>Test_MAE</td><td>33.60423</td></tr><tr><td>Test_MSE</td><td>2423.43433</td></tr><tr><td>Test_NDE</td><td>0.78503</td></tr><tr><td>Test_NEP</td><td>1.42238</td></tr><tr><td>Test_R2_Value</td><td>0.04171</td></tr><tr><td>Training_F1</td><td>0.8897</td></tr><tr><td>Training_Loss</td><td>0.06063</td></tr><tr><td>Training_MAE</td><td>12.07523</td></tr><tr><td>Training_MSE</td><td>298.59003</td></tr><tr><td>Training_NDE</td><td>0.0377</td></tr><tr><td>Training_NEP</td><td>0.22061</td></tr><tr><td>Training_R2</td><td>0.93937</td></tr><tr><td>Validation_F1</td><td>0.28071</td></tr><tr><td>Validation_Loss</td><td>1.00499</td></tr><tr><td>Validation_MAE</td><td>37.14875</td></tr><tr><td>Validation_MSE</td><td>2760.39697</td></tr><tr><td>Validation_NDE</td><td>0.74747</td></tr><tr><td>Validation_NEP</td><td>1.3546</td></tr><tr><td>Validation_R2</td><td>0.06137</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">astral-bird-180</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/f3gr7v8r\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/f3gr7v8r</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220302_090821-f3gr7v8r\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]_Test_home_[3383]_total_homes_1': [0.7955590923627217, 0.1397545033989599, 0.04647726666181743, 1.323119405328264, 0.7047084547625869, 31.259170188356897, 2175.4788], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240]_Test_home_[3383]_total_homes_1': [0.7363784319824642, 0.2027867142757257, 0.3034292298291827, 1.29385622862555, 0.6530728087722938, 30.567817150136477, 2016.0767], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526]_Test_home_[3383]_total_homes_1': [0.7359541899628109, 0.20500130184728738, 0.2241232947225627, 1.3297989125279133, 0.6512586306200592, 31.416975940042928, 2010.476], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672]_Test_home_[3383]_total_homes_1': [0.8938575671778785, 0.03646150824903194, 0.06157500691977571, 1.4344386458988054, 0.7893255173191654, 33.889127146300446, 2436.697], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021]_Test_home_[3383]_total_homes_1': [0.6966728157467312, 0.2481737472120581, 0.30181122567035945, 1.2251486728388805, 0.6158919970468059, 28.944576518253612, 1901.297], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069]_Test_home_[3383]_total_homes_1': [0.7811614433924358, 0.1572742344566146, 0.29493593617659514, 1.2956507687551084, 0.6903563859051755, 30.610213803903175, 2131.173], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365]_Test_home_[3383]_total_homes_1': [0.8895747575494978, 0.041705902267637596, 0.07742435786762397, 1.4223797007069072, 0.7850293381242492, 33.604230243928974, 2423.4343]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/4cv4ag35\" target=\"_blank\">eager-lake-181</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 2014140 batches: 0.5471\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 4028280 batches: 0.3393\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 6042420 batches: 0.2328\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 8056560 batches: 0.1828\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 1\n",
      "Loss after 10070700 batches: 0.1542\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 2\n",
      "Loss after 12084840 batches: 0.1342\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 3\n",
      "Loss after 14098980 batches: 0.1212\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 16113120 batches: 0.1112\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 18127260 batches: 0.1031\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 6\n",
      "Loss after 20141400 batches: 0.0969\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 7\n",
      "Loss after 22155540 batches: 0.0899\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 8\n",
      "Loss after 24169680 batches: 0.0856\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 26183820 batches: 0.0817\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 0\n",
      "Loss after 28197960 batches: 0.0787\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 0\n",
      "Loss after 30212100 batches: 0.0760\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 0\n",
      "Loss after 32226240 batches: 0.0726\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 1\n",
      "Loss after 34240380 batches: 0.0703\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 2\n",
      "Loss after 36254520 batches: 0.0685\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 3\n",
      "Loss after 38268660 batches: 0.0663\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 4\n",
      "Loss after 40282800 batches: 0.0649\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 0\n",
      "Loss after 42296940 batches: 0.0631\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 1\n",
      "Loss after 44311080 batches: 0.0617\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 2\n",
      "Loss after 46325220 batches: 0.0604\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 3\n",
      "Loss after 48339360 batches: 0.0594\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 4\n",
      "Loss after 50353500 batches: 0.0586\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 5\n",
      "Loss after 52367640 batches: 0.0568\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 6\n",
      "Loss after 54381780 batches: 0.0562\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 7\n",
      "Loss after 56395920 batches: 0.0554\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 8\n",
      "Loss after 58410060 batches: 0.0544\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 9\n",
      "Loss after 60424200 batches: 0.0538\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 10\n",
      "Loss after 62438340 batches: 0.0526\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 11\n",
      "Loss after 64452480 batches: 0.0516\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 12\n",
      "Loss after 66466620 batches: 0.0515\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 13\n",
      "Loss after 68480760 batches: 0.0508\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 14\n",
      "Loss after 70494900 batches: 0.0504\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 15\n",
      "Loss after 72509040 batches: 0.0495\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 16\n",
      "Loss after 74523180 batches: 0.0487\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 17\n",
      "Loss after 76537320 batches: 0.0483\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 18\n",
      "Loss after 78551460 batches: 0.0481\n",
      "Adjusting learning rate of group 0 to 2.2013e-04.\n",
      "trigger times: 19\n",
      "Loss after 80565600 batches: 0.0475\n",
      "Adjusting learning rate of group 0 to 2.2013e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 82579740 batches: 0.0468\n",
      "Time to train on one home:  28113.670392990112\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 13556... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>Training_Loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▄▅▆▆▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>Validation_F1</td><td>█▄▃▄▄▁▄▃▃▄▃▃▃▄▄▄▅▅▃▅▆▄▅▄▅▆▆▅▅▄▅▆▄▄▅▄▅▄▅▅</td></tr><tr><td>Validation_Loss</td><td>▄▆▇▄▆█▄▅▅▅▄▄▄▄▄▃▃▃▄▃▁▃▂▂▂▂▁▂▁▂▁▁▂▂▂▂▁▂▂▁</td></tr><tr><td>Validation_MAE</td><td>▄▆▅▄▇█▅▅▅▄▄▅▅▃▃▂▃▃▅▃▁▂▃▂▂▂▁▂▂▂▂▂▂▃▃▂▂▂▂▂</td></tr><tr><td>Validation_MSE</td><td>▄▆▇▄▆█▄▅▅▅▄▅▄▄▄▃▃▃▄▃▁▃▂▂▂▂▁▂▁▂▁▁▂▂▂▂▁▂▂▁</td></tr><tr><td>Validation_NDE</td><td>▄▆▇▄▆█▄▅▅▅▄▅▄▄▄▃▃▃▄▃▁▃▂▂▂▂▁▂▁▂▁▁▂▂▂▂▁▂▂▁</td></tr><tr><td>Validation_NEP</td><td>▄▆▅▄▇█▅▅▅▄▄▅▅▃▃▂▃▃▅▃▁▂▃▂▂▂▁▂▂▂▂▂▂▃▃▂▂▂▂▂</td></tr><tr><td>Validation_R2</td><td>▅▃▂▅▃▁▅▄▄▄▅▄▅▅▅▆▆▆▅▆█▆▇▇▇▇█▇█▇██▇▇▇▇█▇▇█</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.0877</td></tr><tr><td>Test_Loss</td><td>0.85473</td></tr><tr><td>Test_MAE</td><td>33.06394</td></tr><tr><td>Test_MSE</td><td>2335.69092</td></tr><tr><td>Test_NDE</td><td>0.75661</td></tr><tr><td>Test_NEP</td><td>1.39951</td></tr><tr><td>Test_R2_Value</td><td>0.0764</td></tr><tr><td>Training_F1</td><td>0.90261</td></tr><tr><td>Training_Loss</td><td>0.0468</td></tr><tr><td>Training_MAE</td><td>10.62551</td></tr><tr><td>Training_MSE</td><td>221.26585</td></tr><tr><td>Training_NDE</td><td>0.02872</td></tr><tr><td>Training_NEP</td><td>0.19479</td></tr><tr><td>Training_R2</td><td>0.9532</td></tr><tr><td>Validation_F1</td><td>0.24055</td></tr><tr><td>Validation_Loss</td><td>0.98848</td></tr><tr><td>Validation_MAE</td><td>36.8523</td></tr><tr><td>Validation_MSE</td><td>2711.74219</td></tr><tr><td>Validation_NDE</td><td>0.7343</td></tr><tr><td>Validation_NEP</td><td>1.34379</td></tr><tr><td>Validation_R2</td><td>0.07792</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">eager-lake-181</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/4cv4ag35\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/4cv4ag35</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220302_124621-4cv4ag35\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]_Test_home_[3383]_total_homes_1': [0.7955590923627217, 0.1397545033989599, 0.04647726666181743, 1.323119405328264, 0.7047084547625869, 31.259170188356897, 2175.4788], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240]_Test_home_[3383]_total_homes_1': [0.7363784319824642, 0.2027867142757257, 0.3034292298291827, 1.29385622862555, 0.6530728087722938, 30.567817150136477, 2016.0767], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526]_Test_home_[3383]_total_homes_1': [0.7359541899628109, 0.20500130184728738, 0.2241232947225627, 1.3297989125279133, 0.6512586306200592, 31.416975940042928, 2010.476], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672]_Test_home_[3383]_total_homes_1': [0.8938575671778785, 0.03646150824903194, 0.06157500691977571, 1.4344386458988054, 0.7893255173191654, 33.889127146300446, 2436.697], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021]_Test_home_[3383]_total_homes_1': [0.6966728157467312, 0.2481737472120581, 0.30181122567035945, 1.2251486728388805, 0.6158919970468059, 28.944576518253612, 1901.297], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069]_Test_home_[3383]_total_homes_1': [0.7811614433924358, 0.1572742344566146, 0.29493593617659514, 1.2956507687551084, 0.6903563859051755, 30.610213803903175, 2131.173], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365]_Test_home_[3383]_total_homes_1': [0.8895747575494978, 0.041705902267637596, 0.07742435786762397, 1.4223797007069072, 0.7850293381242492, 33.604230243928974, 2423.4343], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004]_Test_home_[3383]_total_homes_1': [0.8547291841771868, 0.07640216477376749, 0.08769811471936954, 1.399510507129078, 0.7566063476717088, 33.063937348789636, 2335.691]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/2a3gnyfm\" target=\"_blank\">dashing-plasma-182</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 2108100 batches: 0.5725\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 4216200 batches: 0.3738\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 6324300 batches: 0.2621\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 8432400 batches: 0.2056\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 10540500 batches: 0.1736\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 12648600 batches: 0.1514\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 14756700 batches: 0.1373\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 16864800 batches: 0.1267\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 18972900 batches: 0.1186\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 21081000 batches: 0.1122\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 23189100 batches: 0.1048\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 25297200 batches: 0.1000\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 27405300 batches: 0.0960\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 29513400 batches: 0.0921\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 31621500 batches: 0.0892\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 33729600 batches: 0.0852\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 35837700 batches: 0.0827\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 37945800 batches: 0.0808\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 40053900 batches: 0.0786\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 42162000 batches: 0.0770\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 44270100 batches: 0.0742\n",
      "Time to train on one home:  14826.746459960938\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9316... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▅▆▆▆▇▇▇▇▇▇█████████</td></tr><tr><td>Training_Loss</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▄▅▆▇▇▇▇▇▇███████████</td></tr><tr><td>Validation_F1</td><td>█▄▁▂▃▁▁▃▄▃▃▄▃▃▃▃▃▄▃▄▄</td></tr><tr><td>Validation_Loss</td><td>▁▅█▇▆▇▆▆▆▆▅▆▆▆▆▆▅▆▇▆▅</td></tr><tr><td>Validation_MAE</td><td>▁▆▇█▇▇▆▆▆▅▆▆▆▆▇▆▆▅▇▇▅</td></tr><tr><td>Validation_MSE</td><td>▁▅█▇▆▇▆▆▆▆▅▆▆▆▆▆▅▆▇▆▅</td></tr><tr><td>Validation_NDE</td><td>▁▅█▇▆▇▆▆▆▆▅▆▆▆▆▆▅▆▇▆▅</td></tr><tr><td>Validation_NEP</td><td>▁▆▇█▇▇▆▆▆▅▆▆▆▆▇▆▆▅▇▇▅</td></tr><tr><td>Validation_R2</td><td>█▄▁▂▃▂▃▃▃▃▄▃▃▃▃▃▄▃▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.2691</td></tr><tr><td>Test_Loss</td><td>0.756</td></tr><tr><td>Test_MAE</td><td>30.8642</td></tr><tr><td>Test_MSE</td><td>2061.55347</td></tr><tr><td>Test_NDE</td><td>0.6678</td></tr><tr><td>Test_NEP</td><td>1.3064</td></tr><tr><td>Test_R2_Value</td><td>0.1848</td></tr><tr><td>Training_F1</td><td>0.87854</td></tr><tr><td>Training_Loss</td><td>0.07417</td></tr><tr><td>Training_MAE</td><td>13.14273</td></tr><tr><td>Training_MSE</td><td>351.52707</td></tr><tr><td>Training_NDE</td><td>0.04586</td></tr><tr><td>Training_NEP</td><td>0.24292</td></tr><tr><td>Training_R2</td><td>0.92582</td></tr><tr><td>Validation_F1</td><td>0.25291</td></tr><tr><td>Validation_Loss</td><td>1.03428</td></tr><tr><td>Validation_MAE</td><td>37.21092</td></tr><tr><td>Validation_MSE</td><td>2840.74292</td></tr><tr><td>Validation_NDE</td><td>0.76923</td></tr><tr><td>Validation_NEP</td><td>1.35687</td></tr><tr><td>Validation_R2</td><td>0.03405</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dashing-plasma-182</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/2a3gnyfm\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/2a3gnyfm</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220302_203522-2a3gnyfm\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]_Test_home_[3383]_total_homes_1': [0.7955590923627217, 0.1397545033989599, 0.04647726666181743, 1.323119405328264, 0.7047084547625869, 31.259170188356897, 2175.4788], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240]_Test_home_[3383]_total_homes_1': [0.7363784319824642, 0.2027867142757257, 0.3034292298291827, 1.29385622862555, 0.6530728087722938, 30.567817150136477, 2016.0767], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526]_Test_home_[3383]_total_homes_1': [0.7359541899628109, 0.20500130184728738, 0.2241232947225627, 1.3297989125279133, 0.6512586306200592, 31.416975940042928, 2010.476], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672]_Test_home_[3383]_total_homes_1': [0.8938575671778785, 0.03646150824903194, 0.06157500691977571, 1.4344386458988054, 0.7893255173191654, 33.889127146300446, 2436.697], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021]_Test_home_[3383]_total_homes_1': [0.6966728157467312, 0.2481737472120581, 0.30181122567035945, 1.2251486728388805, 0.6158919970468059, 28.944576518253612, 1901.297], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069]_Test_home_[3383]_total_homes_1': [0.7811614433924358, 0.1572742344566146, 0.29493593617659514, 1.2956507687551084, 0.6903563859051755, 30.610213803903175, 2131.173], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365]_Test_home_[3383]_total_homes_1': [0.8895747575494978, 0.041705902267637596, 0.07742435786762397, 1.4223797007069072, 0.7850293381242492, 33.604230243928974, 2423.4343], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004]_Test_home_[3383]_total_homes_1': [0.8547291841771868, 0.07640216477376749, 0.08769811471936954, 1.399510507129078, 0.7566063476717088, 33.063937348789636, 2335.691], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554]_Test_home_[3383]_total_homes_1': [0.7559956769148509, 0.18480381951444813, 0.26909949133531874, 1.3064014712538372, 0.6678042988288531, 30.864203003743217, 2061.5535]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554, 10811]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1xj3ps58\" target=\"_blank\">zesty-dragon-183</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 2239200 batches: 0.5262\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 4478400 batches: 0.3239\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 6717600 batches: 0.2200\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 8956800 batches: 0.1710\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 11196000 batches: 0.1445\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 0\n",
      "Loss after 13435200 batches: 0.1265\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 1\n",
      "Loss after 15674400 batches: 0.1146\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 2\n",
      "Loss after 17913600 batches: 0.1062\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 3\n",
      "Loss after 20152800 batches: 0.0991\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 0\n",
      "Loss after 22392000 batches: 0.0930\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 1\n",
      "Loss after 24631200 batches: 0.0877\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 2\n",
      "Loss after 26870400 batches: 0.0831\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 3\n",
      "Loss after 29109600 batches: 0.0797\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 4\n",
      "Loss after 31348800 batches: 0.0771\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 5\n",
      "Loss after 33588000 batches: 0.0737\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 6\n",
      "Loss after 35827200 batches: 0.0707\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 0\n",
      "Loss after 38066400 batches: 0.0682\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 1\n",
      "Loss after 40305600 batches: 0.0668\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 2\n",
      "Loss after 42544800 batches: 0.0653\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 3\n",
      "Loss after 44784000 batches: 0.0633\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 0\n",
      "Loss after 47023200 batches: 0.0615\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 1\n",
      "Loss after 49262400 batches: 0.0602\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 2\n",
      "Loss after 51501600 batches: 0.0593\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 3\n",
      "Loss after 53740800 batches: 0.0582\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 4\n",
      "Loss after 55980000 batches: 0.0574\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 5\n",
      "Loss after 58219200 batches: 0.0558\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 6\n",
      "Loss after 60458400 batches: 0.0549\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 0\n",
      "Loss after 62697600 batches: 0.0542\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 0\n",
      "Loss after 64936800 batches: 0.0533\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 1\n",
      "Loss after 67176000 batches: 0.0526\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 2\n",
      "Loss after 69415200 batches: 0.0514\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 3\n",
      "Loss after 71654400 batches: 0.0509\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 4\n",
      "Loss after 73893600 batches: 0.0506\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 0\n",
      "Loss after 76132800 batches: 0.0499\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 1\n",
      "Loss after 78372000 batches: 0.0493\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 2\n",
      "Loss after 80611200 batches: 0.0487\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 3\n",
      "Loss after 82850400 batches: 0.0480\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 0\n",
      "Loss after 85089600 batches: 0.0475\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 1\n",
      "Loss after 87328800 batches: 0.0473\n",
      "Adjusting learning rate of group 0 to 2.2013e-04.\n",
      "trigger times: 2\n",
      "Loss after 89568000 batches: 0.0469\n",
      "Adjusting learning rate of group 0 to 2.2013e-04.\n",
      "trigger times: 3\n",
      "Loss after 91807200 batches: 0.0462\n",
      "Adjusting learning rate of group 0 to 2.2013e-04.\n",
      "trigger times: 4\n",
      "Loss after 94046400 batches: 0.0459\n",
      "Adjusting learning rate of group 0 to 2.2013e-04.\n",
      "trigger times: 5\n",
      "Loss after 96285600 batches: 0.0455\n",
      "Adjusting learning rate of group 0 to 2.2013e-04.\n",
      "trigger times: 6\n",
      "Loss after 98524800 batches: 0.0453\n",
      "Adjusting learning rate of group 0 to 2.0912e-04.\n",
      "trigger times: 7\n",
      "Loss after 100764000 batches: 0.0448\n",
      "Adjusting learning rate of group 0 to 2.0912e-04.\n",
      "trigger times: 8\n",
      "Loss after 103003200 batches: 0.0443\n",
      "Adjusting learning rate of group 0 to 2.0912e-04.\n",
      "trigger times: 9\n",
      "Loss after 105242400 batches: 0.0438\n",
      "Adjusting learning rate of group 0 to 2.0912e-04.\n",
      "trigger times: 10\n",
      "Loss after 107481600 batches: 0.0436\n",
      "Adjusting learning rate of group 0 to 2.0912e-04.\n",
      "trigger times: 11\n",
      "Loss after 109720800 batches: 0.0434\n",
      "Adjusting learning rate of group 0 to 1.9867e-04.\n",
      "trigger times: 12\n",
      "Loss after 111960000 batches: 0.0430\n",
      "Adjusting learning rate of group 0 to 1.9867e-04.\n",
      "trigger times: 13\n",
      "Loss after 114199200 batches: 0.0427\n",
      "Adjusting learning rate of group 0 to 1.9867e-04.\n",
      "trigger times: 14\n",
      "Loss after 116438400 batches: 0.0422\n",
      "Adjusting learning rate of group 0 to 1.9867e-04.\n",
      "trigger times: 15\n",
      "Loss after 118677600 batches: 0.0422\n",
      "Adjusting learning rate of group 0 to 1.9867e-04.\n",
      "trigger times: 16\n",
      "Loss after 120916800 batches: 0.0418\n",
      "Adjusting learning rate of group 0 to 1.8873e-04.\n",
      "trigger times: 0\n",
      "Loss after 123156000 batches: 0.0416\n",
      "Adjusting learning rate of group 0 to 1.8873e-04.\n",
      "trigger times: 1\n",
      "Loss after 125395200 batches: 0.0412\n",
      "Adjusting learning rate of group 0 to 1.8873e-04.\n",
      "trigger times: 2\n",
      "Loss after 127634400 batches: 0.0409\n",
      "Adjusting learning rate of group 0 to 1.8873e-04.\n",
      "trigger times: 3\n",
      "Loss after 129873600 batches: 0.0407\n",
      "Adjusting learning rate of group 0 to 1.8873e-04.\n",
      "trigger times: 4\n",
      "Loss after 132112800 batches: 0.0405\n",
      "Adjusting learning rate of group 0 to 1.7930e-04.\n",
      "trigger times: 0\n",
      "Loss after 134352000 batches: 0.0404\n",
      "Adjusting learning rate of group 0 to 1.7930e-04.\n",
      "trigger times: 1\n",
      "Loss after 136591200 batches: 0.0399\n",
      "Adjusting learning rate of group 0 to 1.7930e-04.\n",
      "trigger times: 2\n",
      "Loss after 138830400 batches: 0.0398\n",
      "Adjusting learning rate of group 0 to 1.7930e-04.\n",
      "trigger times: 3\n",
      "Loss after 141069600 batches: 0.0397\n",
      "Adjusting learning rate of group 0 to 1.7930e-04.\n",
      "trigger times: 4\n",
      "Loss after 143308800 batches: 0.0395\n",
      "Adjusting learning rate of group 0 to 1.7033e-04.\n",
      "trigger times: 0\n",
      "Loss after 145548000 batches: 0.0393\n",
      "Adjusting learning rate of group 0 to 1.7033e-04.\n",
      "trigger times: 1\n",
      "Loss after 147787200 batches: 0.0390\n",
      "Adjusting learning rate of group 0 to 1.7033e-04.\n",
      "trigger times: 2\n",
      "Loss after 150026400 batches: 0.0388\n",
      "Adjusting learning rate of group 0 to 1.7033e-04.\n",
      "trigger times: 3\n",
      "Loss after 152265600 batches: 0.0386\n",
      "Adjusting learning rate of group 0 to 1.7033e-04.\n",
      "trigger times: 4\n",
      "Loss after 154504800 batches: 0.0387\n",
      "Adjusting learning rate of group 0 to 1.6181e-04.\n",
      "trigger times: 5\n",
      "Loss after 156744000 batches: 0.0384\n",
      "Adjusting learning rate of group 0 to 1.6181e-04.\n",
      "trigger times: 6\n",
      "Loss after 158983200 batches: 0.0380\n",
      "Adjusting learning rate of group 0 to 1.6181e-04.\n",
      "trigger times: 0\n",
      "Loss after 161222400 batches: 0.0380\n",
      "Adjusting learning rate of group 0 to 1.6181e-04.\n",
      "trigger times: 1\n",
      "Loss after 163461600 batches: 0.0379\n",
      "Adjusting learning rate of group 0 to 1.6181e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigger times: 2\n",
      "Loss after 165700800 batches: 0.0377\n",
      "Adjusting learning rate of group 0 to 1.5372e-04.\n",
      "trigger times: 3\n",
      "Loss after 167940000 batches: 0.0375\n",
      "Adjusting learning rate of group 0 to 1.5372e-04.\n",
      "trigger times: 4\n",
      "Loss after 170179200 batches: 0.0373\n",
      "Adjusting learning rate of group 0 to 1.5372e-04.\n",
      "trigger times: 5\n",
      "Loss after 172418400 batches: 0.0371\n",
      "Adjusting learning rate of group 0 to 1.5372e-04.\n",
      "trigger times: 6\n",
      "Loss after 174657600 batches: 0.0369\n",
      "Adjusting learning rate of group 0 to 1.5372e-04.\n",
      "trigger times: 7\n",
      "Loss after 176896800 batches: 0.0369\n",
      "Adjusting learning rate of group 0 to 1.4604e-04.\n",
      "trigger times: 8\n",
      "Loss after 179136000 batches: 0.0368\n",
      "Adjusting learning rate of group 0 to 1.4604e-04.\n",
      "trigger times: 9\n",
      "Loss after 181375200 batches: 0.0366\n",
      "Adjusting learning rate of group 0 to 1.4604e-04.\n",
      "trigger times: 10\n",
      "Loss after 183614400 batches: 0.0364\n",
      "Adjusting learning rate of group 0 to 1.4604e-04.\n",
      "trigger times: 11\n",
      "Loss after 185853600 batches: 0.0362\n",
      "Adjusting learning rate of group 0 to 1.4604e-04.\n",
      "trigger times: 12\n",
      "Loss after 188092800 batches: 0.0362\n",
      "Adjusting learning rate of group 0 to 1.3874e-04.\n",
      "trigger times: 13\n",
      "Loss after 190332000 batches: 0.0360\n",
      "Adjusting learning rate of group 0 to 1.3874e-04.\n",
      "trigger times: 14\n",
      "Loss after 192571200 batches: 0.0359\n",
      "Adjusting learning rate of group 0 to 1.3874e-04.\n",
      "trigger times: 15\n",
      "Loss after 194810400 batches: 0.0357\n",
      "Adjusting learning rate of group 0 to 1.3874e-04.\n",
      "trigger times: 16\n",
      "Loss after 197049600 batches: 0.0357\n",
      "Adjusting learning rate of group 0 to 1.3874e-04.\n",
      "trigger times: 17\n",
      "Loss after 199288800 batches: 0.0357\n",
      "Adjusting learning rate of group 0 to 1.3180e-04.\n",
      "trigger times: 18\n",
      "Loss after 201528000 batches: 0.0354\n",
      "Adjusting learning rate of group 0 to 1.3180e-04.\n",
      "trigger times: 19\n",
      "Loss after 203767200 batches: 0.0352\n",
      "Adjusting learning rate of group 0 to 1.3180e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 206006400 batches: 0.0351\n",
      "Time to train on one home:  68802.0111849308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12848... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▄▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>Training_Loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▅▆▇▇▇▇█████████████████████████████████</td></tr><tr><td>Validation_F1</td><td>▇▇▁▃▆▇▅█▂▂▄▆▆▄▅▅▆▆▅▆▅▃▆▄▆▄▅▄▆▆▄▅▄▅▄▂▅▃▃▅</td></tr><tr><td>Validation_Loss</td><td>▅██▇▂▄▃▂▃▃▃▃▁▂▂▁▁▂▂▁▁▂▂▂▂▂▁▂▁▁▁▂▂▁▁▂▂▁▁▁</td></tr><tr><td>Validation_MAE</td><td>▃▇█▇▃▅▄▃▄▅▃▃▂▃▃▂▁▂▂▂▂▂▂▂▂▃▁▂▁▂▁▂▃▂▂▂▂▁▂▁</td></tr><tr><td>Validation_MSE</td><td>▄██▇▂▄▃▂▃▃▃▃▁▂▂▁▁▂▂▁▁▂▂▂▂▂▁▂▁▁▁▂▂▁▁▂▂▁▁▁</td></tr><tr><td>Validation_NDE</td><td>▄██▇▂▄▃▂▃▃▃▃▁▂▂▁▁▂▂▁▁▂▂▂▂▂▁▂▁▁▁▂▂▁▁▂▂▁▁▁</td></tr><tr><td>Validation_NEP</td><td>▃▇█▇▃▅▄▃▄▅▃▃▂▃▃▂▁▂▂▂▂▂▂▂▂▃▁▂▁▂▁▂▃▂▂▂▂▁▂▁</td></tr><tr><td>Validation_R2</td><td>▅▁▁▂▇▅▆▇▆▆▆▆█▇▇██▇▇██▇▇▇▇▇█▇███▇▇██▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.18462</td></tr><tr><td>Test_Loss</td><td>0.80209</td></tr><tr><td>Test_MAE</td><td>32.56857</td></tr><tr><td>Test_MSE</td><td>2192.75146</td></tr><tr><td>Test_NDE</td><td>0.7103</td></tr><tr><td>Test_NEP</td><td>1.37854</td></tr><tr><td>Test_R2_Value</td><td>0.13292</td></tr><tr><td>Training_F1</td><td>0.91595</td></tr><tr><td>Training_Loss</td><td>0.03506</td></tr><tr><td>Training_MAE</td><td>9.42096</td></tr><tr><td>Training_MSE</td><td>171.96848</td></tr><tr><td>Training_NDE</td><td>0.02137</td></tr><tr><td>Training_NEP</td><td>0.1681</td></tr><tr><td>Training_R2</td><td>0.96494</td></tr><tr><td>Validation_F1</td><td>0.28029</td></tr><tr><td>Validation_Loss</td><td>0.97662</td></tr><tr><td>Validation_MAE</td><td>36.58772</td></tr><tr><td>Validation_MSE</td><td>2684.76807</td></tr><tr><td>Validation_NDE</td><td>0.72699</td></tr><tr><td>Validation_NEP</td><td>1.33414</td></tr><tr><td>Validation_R2</td><td>0.08709</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">zesty-dragon-183</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1xj3ps58\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/1xj3ps58</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220303_004257-1xj3ps58\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]_Test_home_[3383]_total_homes_1': [0.7955590923627217, 0.1397545033989599, 0.04647726666181743, 1.323119405328264, 0.7047084547625869, 31.259170188356897, 2175.4788], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240]_Test_home_[3383]_total_homes_1': [0.7363784319824642, 0.2027867142757257, 0.3034292298291827, 1.29385622862555, 0.6530728087722938, 30.567817150136477, 2016.0767], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526]_Test_home_[3383]_total_homes_1': [0.7359541899628109, 0.20500130184728738, 0.2241232947225627, 1.3297989125279133, 0.6512586306200592, 31.416975940042928, 2010.476], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672]_Test_home_[3383]_total_homes_1': [0.8938575671778785, 0.03646150824903194, 0.06157500691977571, 1.4344386458988054, 0.7893255173191654, 33.889127146300446, 2436.697], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021]_Test_home_[3383]_total_homes_1': [0.6966728157467312, 0.2481737472120581, 0.30181122567035945, 1.2251486728388805, 0.6158919970468059, 28.944576518253612, 1901.297], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069]_Test_home_[3383]_total_homes_1': [0.7811614433924358, 0.1572742344566146, 0.29493593617659514, 1.2956507687551084, 0.6903563859051755, 30.610213803903175, 2131.173], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365]_Test_home_[3383]_total_homes_1': [0.8895747575494978, 0.041705902267637596, 0.07742435786762397, 1.4223797007069072, 0.7850293381242492, 33.604230243928974, 2423.4343], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004]_Test_home_[3383]_total_homes_1': [0.8547291841771868, 0.07640216477376749, 0.08769811471936954, 1.399510507129078, 0.7566063476717088, 33.063937348789636, 2335.691], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554]_Test_home_[3383]_total_homes_1': [0.7559956769148509, 0.18480381951444813, 0.26909949133531874, 1.3064014712538372, 0.6678042988288531, 30.864203003743217, 2061.5535], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554, 10811]_Test_home_[3383]_total_homes_1': [0.8020904103914896, 0.13292440082959356, 0.1846156000498968, 1.3785428694857729, 0.7103036378196861, 32.568569394166325, 2192.7515]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554, 10811, 10983]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1g6dl4ys\" target=\"_blank\">cool-terrain-184</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 2370300 batches: 0.5032\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 4740600 batches: 0.3006\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 7110900 batches: 0.2045\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 9481200 batches: 0.1625\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 0\n",
      "Loss after 11851500 batches: 0.1391\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 0\n",
      "Loss after 14221800 batches: 0.1224\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 1\n",
      "Loss after 16592100 batches: 0.1108\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 2\n",
      "Loss after 18962400 batches: 0.1029\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 3\n",
      "Loss after 21332700 batches: 0.0964\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 4\n",
      "Loss after 23703000 batches: 0.0912\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 5\n",
      "Loss after 26073300 batches: 0.0852\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 6\n",
      "Loss after 28443600 batches: 0.0816\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 7\n",
      "Loss after 30813900 batches: 0.0782\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 8\n",
      "Loss after 33184200 batches: 0.0752\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 9\n",
      "Loss after 35554500 batches: 0.0726\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 10\n",
      "Loss after 37924800 batches: 0.0700\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 11\n",
      "Loss after 40295100 batches: 0.0676\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 12\n",
      "Loss after 42665400 batches: 0.0659\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 0\n",
      "Loss after 45035700 batches: 0.0642\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 1\n",
      "Loss after 47406000 batches: 0.0628\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 2\n",
      "Loss after 49776300 batches: 0.0607\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 3\n",
      "Loss after 52146600 batches: 0.0596\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 4\n",
      "Loss after 54516900 batches: 0.0583\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 5\n",
      "Loss after 56887200 batches: 0.0577\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 6\n",
      "Loss after 59257500 batches: 0.0568\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 7\n",
      "Loss after 61627800 batches: 0.0552\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 8\n",
      "Loss after 63998100 batches: 0.0543\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 9\n",
      "Loss after 66368400 batches: 0.0538\n",
      "Adjusting learning rate of group 0 to 2.5675e-04.\n",
      "trigger times: 10\n",
      "Loss after 68738700 batches: 0.0531\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 11\n",
      "Loss after 71109000 batches: 0.0522\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 12\n",
      "Loss after 73479300 batches: 0.0511\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 13\n",
      "Loss after 75849600 batches: 0.0506\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 14\n",
      "Loss after 78219900 batches: 0.0501\n",
      "Adjusting learning rate of group 0 to 2.4391e-04.\n",
      "trigger times: 15\n",
      "Loss after 80590200 batches: 0.0497\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 16\n",
      "Loss after 82960500 batches: 0.0490\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 17\n",
      "Loss after 85330800 batches: 0.0482\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 18\n",
      "Loss after 87701100 batches: 0.0477\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 19\n",
      "Loss after 90071400 batches: 0.0475\n",
      "Adjusting learning rate of group 0 to 2.3171e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 92441700 batches: 0.0470\n",
      "Time to train on one home:  32214.086183786392\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 690516... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▃▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>Training_Loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▄▆▆▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>Validation_F1</td><td>▆▄▁▅▆▆▄▄▂▃▇▅▆▄▅█▅▅▆▅▄▄▅▅▆▂▄▃▇▃▄▆▅▄▄▄▅▆▄</td></tr><tr><td>Validation_Loss</td><td>█▆█▆▅▂▄▄▄▅▄▄▃▄▄▂▃▃▁▄▃▂▂▂▃▃▃▁▁▂▃▃▂▃▃▁▁▁▃</td></tr><tr><td>Validation_MAE</td><td>█▅▆▅▄▂▄▂▃▅▄▆▃▄▄▂▃▂▁▄▂▃▂▂▄▄▃▂▁▃▃▃▃▃▃▁▂▂▃</td></tr><tr><td>Validation_MSE</td><td>▇▆█▆▅▂▄▄▄▅▄▄▃▄▄▂▃▃▁▃▃▂▂▂▃▃▃▁▁▂▃▃▂▃▃▁▁▁▃</td></tr><tr><td>Validation_NDE</td><td>▇▆█▆▅▂▄▄▄▅▄▄▃▄▄▂▃▃▁▃▃▂▂▂▃▃▃▁▁▂▃▃▂▃▃▁▁▁▃</td></tr><tr><td>Validation_NEP</td><td>█▅▆▅▄▂▄▂▃▅▄▆▃▄▄▂▃▂▁▄▂▃▂▂▄▄▃▂▁▃▃▃▃▃▃▁▂▂▃</td></tr><tr><td>Validation_R2</td><td>▂▃▁▃▄▇▅▅▅▄▅▅▆▅▅▇▆▆█▆▆▇▇▇▆▆▆██▇▆▆▇▆▆███▆</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.17368</td></tr><tr><td>Test_Loss</td><td>0.8672</td></tr><tr><td>Test_MAE</td><td>34.23658</td></tr><tr><td>Test_MSE</td><td>2371.89795</td></tr><tr><td>Test_NDE</td><td>0.76834</td></tr><tr><td>Test_NEP</td><td>1.44915</td></tr><tr><td>Test_R2_Value</td><td>0.06208</td></tr><tr><td>Training_F1</td><td>0.9024</td></tr><tr><td>Training_Loss</td><td>0.04701</td></tr><tr><td>Training_MAE</td><td>10.94381</td></tr><tr><td>Training_MSE</td><td>235.75211</td></tr><tr><td>Training_NDE</td><td>0.0289</td></tr><tr><td>Training_NEP</td><td>0.19521</td></tr><tr><td>Training_R2</td><td>0.95299</td></tr><tr><td>Validation_F1</td><td>0.29307</td></tr><tr><td>Validation_Loss</td><td>1.06715</td></tr><tr><td>Validation_MAE</td><td>39.54972</td></tr><tr><td>Validation_MSE</td><td>2932.56958</td></tr><tr><td>Validation_NDE</td><td>0.79409</td></tr><tr><td>Validation_NEP</td><td>1.44215</td></tr><tr><td>Validation_R2</td><td>0.00283</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">cool-terrain-184</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/1g6dl4ys\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/1g6dl4ys</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220303_195008-1g6dl4ys\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]_Test_home_[3383]_total_homes_1': [0.7955590923627217, 0.1397545033989599, 0.04647726666181743, 1.323119405328264, 0.7047084547625869, 31.259170188356897, 2175.4788], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240]_Test_home_[3383]_total_homes_1': [0.7363784319824642, 0.2027867142757257, 0.3034292298291827, 1.29385622862555, 0.6530728087722938, 30.567817150136477, 2016.0767], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526]_Test_home_[3383]_total_homes_1': [0.7359541899628109, 0.20500130184728738, 0.2241232947225627, 1.3297989125279133, 0.6512586306200592, 31.416975940042928, 2010.476], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672]_Test_home_[3383]_total_homes_1': [0.8938575671778785, 0.03646150824903194, 0.06157500691977571, 1.4344386458988054, 0.7893255173191654, 33.889127146300446, 2436.697], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021]_Test_home_[3383]_total_homes_1': [0.6966728157467312, 0.2481737472120581, 0.30181122567035945, 1.2251486728388805, 0.6158919970468059, 28.944576518253612, 1901.297], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069]_Test_home_[3383]_total_homes_1': [0.7811614433924358, 0.1572742344566146, 0.29493593617659514, 1.2956507687551084, 0.6903563859051755, 30.610213803903175, 2131.173], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365]_Test_home_[3383]_total_homes_1': [0.8895747575494978, 0.041705902267637596, 0.07742435786762397, 1.4223797007069072, 0.7850293381242492, 33.604230243928974, 2423.4343], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004]_Test_home_[3383]_total_homes_1': [0.8547291841771868, 0.07640216477376749, 0.08769811471936954, 1.399510507129078, 0.7566063476717088, 33.063937348789636, 2335.691], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554]_Test_home_[3383]_total_homes_1': [0.7559956769148509, 0.18480381951444813, 0.26909949133531874, 1.3064014712538372, 0.6678042988288531, 30.864203003743217, 2061.5535], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554, 10811]_Test_home_[3383]_total_homes_1': [0.8020904103914896, 0.13292440082959356, 0.1846156000498968, 1.3785428694857729, 0.7103036378196861, 32.568569394166325, 2192.7515], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554, 10811, 10983]_Test_home_[3383]_total_homes_1': [0.8671973956955804, 0.06208483003327736, 0.17367767682735796, 1.449145343632422, 0.7683350307989728, 34.236578151487535, 2371.898]}\n",
      "patience:  20\n",
      "training_home:  [3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554, 10811, 10983, 11878]\n",
      "test_home:  [3383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/global_models_feb10/runs/ikfsz3pt\" target=\"_blank\">cerulean-sky-185</a></strong> to <a href=\"https://wandb.ai/nilm/global_models_feb10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))\n",
      "  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))\n",
      "  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))\n",
      "  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))\n",
      "  (linear1): Linear(in_features=23500, out_features=1024, bias=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (leaky): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Window Length:  499\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 0\n",
      "Loss after 2501400 batches: 0.5210\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 1\n",
      "Loss after 5002800 batches: 0.3190\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 2\n",
      "Loss after 7504200 batches: 0.2182\n",
      "Adjusting learning rate of group 0 to 3.3181e-04.\n",
      "trigger times: 3\n",
      "Loss after 10005600 batches: 0.1716\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 4\n",
      "Loss after 12507000 batches: 0.1458\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 5\n",
      "Loss after 15008400 batches: 0.1281\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 6\n",
      "Loss after 17509800 batches: 0.1170\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 7\n",
      "Loss after 20011200 batches: 0.1082\n",
      "Adjusting learning rate of group 0 to 3.1522e-04.\n",
      "trigger times: 8\n",
      "Loss after 22512600 batches: 0.1018\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 9\n",
      "Loss after 25014000 batches: 0.0961\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 10\n",
      "Loss after 27515400 batches: 0.0904\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 11\n",
      "Loss after 30016800 batches: 0.0860\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 12\n",
      "Loss after 32518200 batches: 0.0832\n",
      "Adjusting learning rate of group 0 to 2.9946e-04.\n",
      "trigger times: 13\n",
      "Loss after 35019600 batches: 0.0802\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 14\n",
      "Loss after 37521000 batches: 0.0775\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 15\n",
      "Loss after 40022400 batches: 0.0744\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 16\n",
      "Loss after 42523800 batches: 0.0724\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 17\n",
      "Loss after 45025200 batches: 0.0701\n",
      "Adjusting learning rate of group 0 to 2.8448e-04.\n",
      "trigger times: 18\n",
      "Loss after 47526600 batches: 0.0690\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 19\n",
      "Loss after 50028000 batches: 0.0675\n",
      "Adjusting learning rate of group 0 to 2.7026e-04.\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Loss after 52529400 batches: 0.0655\n",
      "Time to train on one home:  18236.842062950134\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1162096... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>▁</td></tr><tr><td>Test_Loss</td><td>▁</td></tr><tr><td>Test_MAE</td><td>▁</td></tr><tr><td>Test_MSE</td><td>▁</td></tr><tr><td>Test_NDE</td><td>▁</td></tr><tr><td>Test_NEP</td><td>▁</td></tr><tr><td>Test_R2_Value</td><td>▁</td></tr><tr><td>Training_F1</td><td>▁▄▅▆▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>Training_Loss</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MAE</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_MSE</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NDE</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_NEP</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training_R2</td><td>▁▄▆▆▇▇▇▇▇████████████</td></tr><tr><td>Validation_F1</td><td>█▄▂▄▄▃▂▂▁▄▃▂▃▃▃▃▃▂▃▂▃</td></tr><tr><td>Validation_Loss</td><td>▁▆███▅▆▅▆▄▆▅▄▅▅▃▅▄▃▃▃</td></tr><tr><td>Validation_MAE</td><td>▁▆▇██▆▅▄▆▄▇▅▃▆▆▃▅▅▄▄▃</td></tr><tr><td>Validation_MSE</td><td>▁▆███▅▆▅▆▄▆▅▄▅▅▃▅▄▃▃▃</td></tr><tr><td>Validation_NDE</td><td>▁▆███▅▆▅▆▄▆▅▄▅▅▃▅▄▃▃▃</td></tr><tr><td>Validation_NEP</td><td>▁▆▇██▆▅▄▆▄▇▅▃▆▆▃▅▅▄▄▃</td></tr><tr><td>Validation_R2</td><td>█▃▁▁▁▄▃▄▃▅▃▄▅▄▄▆▄▅▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_F1_Score</td><td>0.28219</td></tr><tr><td>Test_Loss</td><td>0.85674</td></tr><tr><td>Test_MAE</td><td>33.44186</td></tr><tr><td>Test_MSE</td><td>2344.00146</td></tr><tr><td>Test_NDE</td><td>0.7593</td></tr><tr><td>Test_NEP</td><td>1.41551</td></tr><tr><td>Test_R2_Value</td><td>0.07312</td></tr><tr><td>Training_F1</td><td>0.88956</td></tr><tr><td>Training_Loss</td><td>0.06553</td></tr><tr><td>Training_MAE</td><td>12.60684</td></tr><tr><td>Training_MSE</td><td>319.86234</td></tr><tr><td>Training_NDE</td><td>0.0393</td></tr><tr><td>Training_NEP</td><td>0.22088</td></tr><tr><td>Training_R2</td><td>0.93447</td></tr><tr><td>Validation_F1</td><td>0.28727</td></tr><tr><td>Validation_Loss</td><td>1.0835</td></tr><tr><td>Validation_MAE</td><td>38.92638</td></tr><tr><td>Validation_MSE</td><td>2976.81763</td></tr><tr><td>Validation_NDE</td><td>0.80608</td></tr><tr><td>Validation_NEP</td><td>1.41942</td></tr><tr><td>Validation_R2</td><td>-0.01222</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">cerulean-sky-185</strong>: <a href=\"https://wandb.ai/nilm/global_models_feb10/runs/ikfsz3pt\" target=\"_blank\">https://wandb.ai/nilm/global_models_feb10/runs/ikfsz3pt</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220304_044734-ikfsz3pt\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refrigerator1_Train_home_[3383, 145]_Test_home_[3383]_total_homes_1': [0.48835255901018776, 0.4722010240016288, 0.500879405682105, 0.9784157072511019, 0.4323700644417161, 23.115421771276807, 1334.7532], 'refrigerator1_Train_home_[3383, 145, 183]_Test_home_[3383]_total_homes_1': [0.5857868274052938, 0.3662983092079293, 0.47840184004624153, 1.0600346433053338, 0.5191249952054428, 25.04369838972673, 1602.5712], 'refrigerator1_Train_home_[3383, 145, 183, 335]_Test_home_[3383]_total_homes_1': [0.6403339756859674, 0.3095003367487643, 0.376913692156315, 1.1431826138668957, 0.5656535868266659, 27.00809899645467, 1746.2078], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387]_Test_home_[3383]_total_homes_1': [0.6804485519727071, 0.2620496297170043, 0.43357659090918393, 1.2421575648091756, 0.6045249492015528, 29.346417687443424, 1866.206], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526]_Test_home_[3383]_total_homes_1': [0.7144321905242073, 0.22986878917519105, 0.2059222970458523, 1.2993213817063711, 0.6308873195955709, 30.696933350511777, 1947.5884], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417]_Test_home_[3383]_total_homes_1': [0.6882650991280873, 0.255985250253119, 0.3579358579191711, 1.1963252207906132, 0.6094928560350957, 28.26361213260227, 1881.5422], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358]_Test_home_[3383]_total_homes_1': [0.8096433805094825, 0.12461565168754574, 0.09386658873430402, 1.3745085685048657, 0.7171101201459953, 32.473257587502935, 2213.7637], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628]_Test_home_[3383]_total_homes_1': [0.7955590923627217, 0.1397545033989599, 0.04647726666181743, 1.323119405328264, 0.7047084547625869, 31.259170188356897, 2175.4788], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240]_Test_home_[3383]_total_homes_1': [0.7363784319824642, 0.2027867142757257, 0.3034292298291827, 1.29385622862555, 0.6530728087722938, 30.567817150136477, 2016.0767], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526]_Test_home_[3383]_total_homes_1': [0.7359541899628109, 0.20500130184728738, 0.2241232947225627, 1.3297989125279133, 0.6512586306200592, 31.416975940042928, 2010.476], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672]_Test_home_[3383]_total_homes_1': [0.8938575671778785, 0.03646150824903194, 0.06157500691977571, 1.4344386458988054, 0.7893255173191654, 33.889127146300446, 2436.697], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021]_Test_home_[3383]_total_homes_1': [0.6966728157467312, 0.2481737472120581, 0.30181122567035945, 1.2251486728388805, 0.6158919970468059, 28.944576518253612, 1901.297], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069]_Test_home_[3383]_total_homes_1': [0.7811614433924358, 0.1572742344566146, 0.29493593617659514, 1.2956507687551084, 0.6903563859051755, 30.610213803903175, 2131.173], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365]_Test_home_[3383]_total_homes_1': [0.8895747575494978, 0.041705902267637596, 0.07742435786762397, 1.4223797007069072, 0.7850293381242492, 33.604230243928974, 2423.4343], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004]_Test_home_[3383]_total_homes_1': [0.8547291841771868, 0.07640216477376749, 0.08769811471936954, 1.399510507129078, 0.7566063476717088, 33.063937348789636, 2335.691], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554]_Test_home_[3383]_total_homes_1': [0.7559956769148509, 0.18480381951444813, 0.26909949133531874, 1.3064014712538372, 0.6678042988288531, 30.864203003743217, 2061.5535], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554, 10811]_Test_home_[3383]_total_homes_1': [0.8020904103914896, 0.13292440082959356, 0.1846156000498968, 1.3785428694857729, 0.7103036378196861, 32.568569394166325, 2192.7515], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554, 10811, 10983]_Test_home_[3383]_total_homes_1': [0.8671973956955804, 0.06208483003327736, 0.17367767682735796, 1.449145343632422, 0.7683350307989728, 34.236578151487535, 2371.898], 'refrigerator1_Train_home_[3383, 145, 183, 335, 387, 526, 1417, 2358, 4628, 6240, 6526, 6672, 7021, 7069, 7365, 9004, 10554, 10811, 10983, 11878]_Test_home_[3383]_total_homes_1': [0.8567357632848952, 0.07311601487612074, 0.28219435839617046, 1.4155070866694368, 0.7592983438816735, 33.44186227398534, 2344.0015]}\n"
     ]
    }
   ],
   "source": [
    "final_results = {}\n",
    "random.seed(3)\n",
    "train_homes = []\n",
    "best_models = []\n",
    "max_patience = 200\n",
    "min_patience = 50\n",
    "training_homes = [3383]\n",
    "\n",
    "for i in home_ids_train:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    #training_homes=[3383]\n",
    "    training_homes.append(i)\n",
    "    #training_homes = train_homes_from_fl[-1]\n",
    "    testing_homes = [3383]\n",
    "    #testing_homes = [test_homes_from_fl[-1]]\n",
    "    #patience = int((max_patience-min_patience)/(1-random_select[-1])*len(training_homes)+max_patience+(max_patience-min_patience)/(1-random_select[-1]))\n",
    "    patience = 20\n",
    "    print(\"patience: \", patience)\n",
    "    print(\"training_home: \", training_homes)\n",
    "    print(\"test_home: \", testing_homes)\n",
    "    model, per_house_result, best_model = model_pipeline(\n",
    "    config_,\n",
    "    'sept_oct_nov',\n",
    "    'dec',\n",
    "    config_['appliance'],\n",
    "    config_['window_size'],\n",
    "    training_homes,\n",
    "    testing_homes,\n",
    "    patience)\n",
    "    result = {str(config_[\"appliance\"])+\"_Train_home_\"+str(training_homes)+\"_Test_home_\"+str(testing_homes)+\"_total_homes_\"+str(1): per_house_result}\n",
    "    final_results.update(result)\n",
    "    print(final_results)\n",
    "    #model.cpu()\n",
    "    #torch.save(model.state_dict(), PATH+\"\\\\refrigerator_model_total_houses_\"+str(random_select[i])+\"_trial_3.pth\")\n",
    "    best_model.cpu()\n",
    "    model.cpu()\n",
    "    best_models.append(best_model)\n",
    "    torch.save(best_model.state_dict(), PATH+\"\\\\seq2point_refrigerator_model_3383test_train\"+str(len(training_homes))+\"_homes_trial1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4615832",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\2/ipykernel_8940/263677992.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'best_models' is not defined"
     ]
    }
   ],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df5b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "['loss', 'r2', 'f1', 'nep', 'nde', 'mae', 'mse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcd29cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "r2s = []\n",
    "f1s = []\n",
    "neps = []\n",
    "ndes = []\n",
    "maes = []\n",
    "mses = []\n",
    "for i in list(final_results.values()):\n",
    "    losses.append(i[0])\n",
    "    r2s.append(i[1])\n",
    "    f1s.append(i[2])\n",
    "    neps.append(i[3])\n",
    "    ndes.append(i[4])\n",
    "    maes.append(i[5])\n",
    "    mses.append(i[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d9ad343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'total number of participating homes')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(2, 21), ndes)\n",
    "plt.ylabel('nde')\n",
    "plt.xlabel(\"total number of participating homes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d1d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteratively adding homes to global model, trial 1\n",
    "{'refrigerator1_Train_home_[2126]_Test_home_[2126]_total_homes_1': [0.09093445539474487, -0.014985910154828641, 0.45236979144305295, 0.9847796914909418, 0.701257628591099, 26.599626913521533], 'refrigerator1_Train_home_[3700, 6672, 7021, 335, 142]_Test_home_[335]_total_homes_1': [0.0656757652759552, -0.09652668914534113, 0.4245840864192342, 0.7664727537163236, 0.5843863017219274, 48.183727707719186], 'refrigerator1_Train_home_[2358, 2126, 11878, 5058, 9973, 5058, 7069, 526, 7021, 10164]_Test_home_[7021]_total_homes_1': [0.13397249579429626, -0.40018523219510693, 0.2910230798751214, 0.8850934777759952, 0.7701192994727751, 61.13399976295241], 'refrigerator1_Train_home_[3700, 142, 9053, 690, 10983, 183, 9053, 9973, 2358, 6672, 10811, 3700, 9290, 3996, 9004]_Test_home_[6672]_total_homes_1': [0.042496636509895325, 0.037969506482425586, 0.413924206725173, 0.8057882388974359, 0.6416308549043926, 33.92659721210321], 'refrigerator1_Train_home_[3996, 10811, 10182, 387, 526, 2126, 10983, 3996, 7021, 2561, 6178, 3700, 3488, 6672, 6672, 10182, 7365, 10811, 10164, 11878]_Test_home_[2126]_total_homes_1': [0.0644366666674614, 0.0145451466579819, 0.412771484028735, 0.8989641910315869, 0.680854499185984, 24.28168685511104], 'refrigerator1_Train_home_[8825, 3383, 6240, 6526, 387, 7069, 7021, 11878, 2358, 526, 5058, 7069, 5058, 3488, 335, 10182, 142, 3996, 3976, 387, 6672, 9053, 3700, 6672, 6526]_Test_home_[2358]_total_homes_1': [0.10874124616384506, 0.03966814702721477, 0.25522453925462363, 1.0269965594230064, 0.74486410858908, 33.9225955917141], 'refrigerator1_Train_home_[6178, 11878, 3383, 335, 6672, 183, 1417, 3976, 7021, 690, 183, 10164, 3383, 10983, 10182, 3700, 5058, 6240, 7069, 6672, 6526, 7021, 10983, 6178, 3996, 8825, 2126, 2561, 11878, 6240]_Test_home_[6240]_total_homes_1': [0.12406780570745468, -0.11294304342360695, 0.12954695709894726, 1.1009816272218376, 0.8701382041537851, 31.04498042525673], 'refrigerator1_Train_home_[3488, 9290, 11878, 3383, 3700, 6672, 526, 7021, 3383, 3488, 10811, 7021, 2561, 6178, 6672, 10983, 142, 3700, 7021, 2561, 6672, 690, 1417, 9053, 9973, 2358, 9053, 387, 9290, 11878, 7365, 526, 6178]_Test_home_[3700]_total_homes_1': [0.06764775514602661, -0.271149884470667, 0.43842910749882513, 0.7538627637057354, 0.8185075529036647, 30.15869901375244]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e2790f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 best models: drye1, train 3000, test 3000 aug\n",
    "drye1_trial1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c80e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_homes = [\n",
    "    4628,\n",
    "    5192\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8794c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_select = [33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d33020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(best_models)):\n",
    "    best_models[i].cpu()\n",
    "    torch.save(best_models[i].state_dict(), PATH+\"\\\\seq2point_refrigerator_model_power_ratio_filter_\"+str(random_select[i])+\"_homes_trial1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = LSTM(\n",
    "        config_['in_channels'],\n",
    "        config_['out_channels'],\n",
    "        config_['kernel_size'],\n",
    "        config_['hidden_size_1'],\n",
    "        config_['hidden_size_2'],\n",
    "        config_['fc1'],\n",
    "        config_['batch_size'],\n",
    "        config_['window_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5271022",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.load_state_dict(torch.load(PATH+\"\\\\seq2point_refrigerator_model_squ_foot_filter_2_homes_trial1.pth\"))\n",
    "model_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22234f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae56465",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d53161",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_models[0].state_dict(), PATH+\"\\\\global_refrigerator_model_test_home_142_single_home_jan27.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474ab93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "torch.save(model.state_dict(), PATH+\"\\\\dryer_model_total_houses_\"+str(1)+\"_trial_3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a827d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results for training refrigerator1 using MSE 0.09 hyperparameters\n",
    "{'Train_home_[  142   145   387   690   950  1240  1417  2358  2561  2786  3383  3488\\n  3700  3976  3996  4628  5058  5367  5982  6069  6178  6526  6594  6672\\n  6703  6907  7021  7069  8162  8627  8825  9002  9004  9053  9290 10164\\n   984  5192  7365  8849 11878   183   335   526  2126  6240  6564  7935\\n  9973 10182 10811 10983 11785]_Test_home_2358_trial_0': [0.06386639177799225, 0.5221559965325657, 0.756976470829962, 0.539304368116004, 0.18637192749997455, 23.918407925437478]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599aa7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ea2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c517fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f77f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(appliance, num_homes, model_type, trial):\n",
    "    model_test = LSTM(\n",
    "        config_['in_channels'],\n",
    "        config_['out_channels'],\n",
    "        config_['kernel_size'],\n",
    "        config_['hidden_size_1'],\n",
    "        config_['hidden_size_2'],\n",
    "        config_['fc1'],\n",
    "        config_['batch_size'],\n",
    "        config_['window_size'])\n",
    "    \n",
    "    model_type = 'global'\n",
    "    \n",
    "    root_path = r\"C:\\Users\\aar245\\Desktop\\privacy_preserving_nn\\models_squ_foot_filter\\\\\"\n",
    "    \n",
    "    model_test.load_state_dict(\n",
    "        torch.load(root_path+str(model_type)+\"_\"+str(appliance)+\"_model_squ_foot_filter_\"+str(num_homes)+\"_homes_trial\"+str(trial)+\".pth\")\n",
    "    )\n",
    "    model_test.eval()\n",
    "    \n",
    "    return model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data_loader(appliance, test_home, test_month):    \n",
    "    test_path = r\"C:\\Users\\aar245\\Desktop\\privacy_preserving_nn\\input\\1min_real_\"+str(test_month)+\"2019.csv\"\n",
    "    test_dataset = PecanStreetDataset(test_path, str(appliance)+\"1\", config_['window_size'], [test_home])\n",
    "    y_min = test_dataset.y_min\n",
    "    y_max = test_dataset.y_max\n",
    "    test_dataset = test_dataset[round(0.5*len(test_dataset)):]\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=config_['batch_size'], shuffle = False, num_workers=0)\n",
    "\n",
    "    return test_loader, y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e95135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_true_and_pred_lists(appliance, num_homes, model_type, trial, test_loader, y_min, y_max):\n",
    "    model_test = load_model(appliance, num_homes, model_type, trial)\n",
    "    predictions = []\n",
    "    true_vals = []\n",
    "    model_type = 'global'\n",
    "    for i, (inputs, outputs) in enumerate(test_loader):\n",
    "        inputs, outputs = inputs.cpu(), outputs.cpu()\n",
    "        prediction = model_test(inputs)\n",
    "        predictions.append(prediction.detach().numpy())\n",
    "        true_vals.append(outputs.detach().numpy())\n",
    "    \n",
    "    true_vals = [i for subitems in true_vals for i in subitems]\n",
    "    true_vals = [i for subitems in true_vals for i in subitems]\n",
    "    true_vals = denormalize(true_vals, y_min, y_max)\n",
    "    \n",
    "    predictions = [i for subitems in predictions for i in subitems]\n",
    "    predictions = [i for subitems in predictions for i in subitems]\n",
    "    predictions = denormalize(predictions, y_min, y_max)\n",
    "    \n",
    "    return true_vals, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a3d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(sequence, y_min, y_max):\n",
    "    sequence = np.add(np.multiply(sequence, y_max - y_min), y_min)\n",
    "    sequence = np.exp(sequence) - 1\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_true_and_pred(true, predictions, view):\n",
    "    if view == 0:\n",
    "        plt.figure(figsize=(20,4))\n",
    "        plt.plot(true)\n",
    "        plt.plot(predictions, alpha = 0.7)\n",
    "        plt.plot([np.mean(true)]*len(true))\n",
    "        plt.legend(['true', 'pred', 'mean'])\n",
    "        \n",
    "    elif view > 0:\n",
    "        plt.figure(figsize=(20,4))\n",
    "        plt.plot(true[0:view])\n",
    "        plt.plot(predictions[0:view], alpha = 0.7)\n",
    "        plt.plot([np.mean(true)]*view)\n",
    "        plt.legend(['true', 'pred', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_metrics(true, predictions, y_min, y_max, model_type, num_homes, test_home):\n",
    "    local_precision = precision(predictions, true)\n",
    "    local_recall = recall(predictions, true)\n",
    "\n",
    "    mse = np.mean(np.square(predictions - true))\n",
    "    r2 = metrics.r2_score(true, predictions)\n",
    "    f1 = f1_score(local_precision, local_recall)\n",
    "    nep = np.sum(abs(predictions - true))/np.sum(true)\n",
    "    nde = np.sum(np.square(predictions - true))/np.sum(np.square(true))\n",
    "    mae = np.sum(abs(predictions - true))/len(true)\n",
    "    \n",
    "    #results = {\"MSE\":mse, \"R2\":r2, \"F1\":f1, \"NEP\":nep, \"NDE\":nde, \"MAE\":mae}\n",
    "    results = {str(model_type)+\"_#TrainHomes:\"+str(num_homes)+\"_TestHomeID:\"+str(test_home):[mse, r2, f1, nep, nde, mae]}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ec7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#box-cox normalization\n",
    "for id in home_ids[0:1]:\n",
    "    test_loader, y_min, y_max = load_test_data_loader(\"refrigerator\", 142, \"dec\")\n",
    "    true, pred = create_true_and_pred_lists(best_models[0], test_loader, y_min, y_max)\n",
    "    plot_true_and_pred(true, pred, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard normalization\n",
    "for id in home_ids[0:1]:\n",
    "    test_loader, y_min, y_max = load_test_data_loader(\"refrigerator\", 5058, \"dec\")\n",
    "    true, pred = create_true_and_pred_lists(model_test, test_loader, y_min, y_max)\n",
    "    plot_true_and_pred(true, pred, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f11550",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.08261062204837799, 0.0006756758968593513, 0.47869045869881166, 0.7716401972127334, 0.6100338455022384, 40.55996281584161, 4326.6523]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8263b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_model_metrics(true, pred, y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_model_metrics(true, pred, y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc3e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1227d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"attention_boxcox_global_refrigerator_model_test_home_5058_all_homes_trial0.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1293e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1595c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5136c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cebc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = []\n",
    "predictions = []\n",
    "true_vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76119ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a09e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = r\"C:\\Users\\aar245\\Desktop\\privacy_preserving_nn\\input\\1min_real_dec2019.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PecanStreetDataset(test_path, 'refrigerator1', 499, [142])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b84e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_dataset, batch_size=config_['batch_size'], shuffle = False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inputs, outputs) in enumerate(test_loader):\n",
    "    inputs, outputs = inputs.cpu(), outputs.cpu()\n",
    "    prediction = best_models[0](inputs)\n",
    "    predictions.append(prediction.detach().numpy())\n",
    "    true_vals.append(outputs.detach().numpy())\n",
    "    test_inputs.append(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_flat = [i for sublist in predictions for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d895f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_flat = [i for sublist in predictions_flat for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_flat = [i for sublist in true_vals for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b7699",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_flat = [i for sublist in true_flat for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33009041",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true_flat)\n",
    "plt.plot(predictions_flat)\n",
    "\n",
    "\n",
    "plt.legend(['prediction', 'true'])\n",
    "#plt.plot(test_inputs[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = [i for subitems in predictions for i in subitems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = [i for subitems in predictions_list for i in subitems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vals_list = [i for subitems in true_vals for i in subitems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a867b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vals_list = [i for subitems in true_vals_list for i in subitems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d88aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(true_vals_list)\n",
    "plt.plot(predictions_list)\n",
    "\n",
    "plt.legend(['true', 'pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ec7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7f094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc32087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd8f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f20d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = LSTM(\n",
    "        config_['in_channels'],\n",
    "        config_['out_channels'],\n",
    "        config_['kernel_size'],\n",
    "        config_['hidden_size_1'],\n",
    "        config_['hidden_size_2'],\n",
    "        config_['fc1'],\n",
    "        config_['fc2'],\n",
    "        config_['kernel_size_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.load_state_dict(torch.load(r\"C:\\Users\\aar245.CORNELL\\Desktop\\privacy_preserving_nn\\models\\refrigerator_model_total_houses_53_trial_0.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85547abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af3bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cce84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_homes = [\n",
    "    [6069, 11878],\n",
    "    [7069, 9053, 11878],\n",
    "    [142, 9053, 6178, 11421],\n",
    "    [3996, 9053, 10983, 11421, 7935],\n",
    "    [5192, 3383, 10811, 7365, 142, 526],\n",
    "    [11878, 335, 6564, 183, 6240, 9053, 7365],\n",
    "    [8825, 7935, 9002, 2358, 7069, 1417, 335, 526],\n",
    "    [5058, 6178, 8825, 6564, 8627, 10182, 7365, 6526, 3976],\n",
    "    [11878, 8627, 5192, 6672, 183, 6240, 3488, 6703, 8825, 9290],\n",
    "    [11785, 11878, 1417, 5058, 6240, 6526, 2126, 335, 6069, 8825, 6594, 9004, 9053, 3976, 7935],\n",
    "    [3383, 183, 6526, 8825, 8627, 2126, 335, 6564, 9053, 7365, 11785, 3996, 6703, 11421, 3488, 6240, 2358, 6178, 10182, 10983],\n",
    "    [142, 526, 1417, 10983, 335, 3996, 8627, 3383, 6564, 2358, 11785, 6703, 183, 3488, 7365, 3976, 8825, 10182, 6594, 5192, 6178, 9004, 387, 9053, 6240],\n",
    "    [6240, 8825, 6069, 6564, 11785, 6178, 10983, 11878, 3488, 142, 7935, 5058, 6526, 9004, 9002, 3996, 9290, 10811, 526, 183, 7365, 5192, 3976, 9053, 1240, 3383, 11421, 6672, 2126, 7069],\n",
    "    [387, 183, 7069, 6178, 9004, 6564, 6594, 1240, 3976, 9053, 3488, 7365, 10811, 10182, 2358, 3383, 3996, 11878, 142, 6526, 526, 6703, 2126, 8825, 6672, 6240, 9290, 9002, 335, 1417, 11785, 8627, 5058, 10983, 5192],\n",
    "    [9002, 3488, 1240, 6672, 5058, 11878, 6240, 2126, 7935, 387, 183, 6178, 1417, 11785, 6526, 11421, 2358, 7365, 335, 3976, 526, 10811, 3383, 10983, 3996, 9290, 9053, 6703, 9004, 8627, 6069, 5192, 10182, 142, 6594, 7069, 6564, 8825]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e65474",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "for i in range(len(random_select)):\n",
    "    for j in train_homes[i]:\n",
    "        test_loader, y_min, y_max = load_test_data_loader(\"refrigerator\", j, \"dec\")\n",
    "\n",
    "        true, pred = create_true_and_pred_lists(\n",
    "        appliance='refrigerator',\n",
    "        num_homes=random_select[i],\n",
    "        model_type='global',\n",
    "        trial=1,\n",
    "        test_loader=test_loader,\n",
    "        y_min=y_min,\n",
    "        y_max=y_max\n",
    "    )\n",
    "\n",
    "        all_results.update(calculate_model_metrics(true, pred, y_min, y_max, 'global', random_select[i], j))\n",
    "    print(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac81484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_results = pd.DataFrame.from_dict(all_results, orient='index', columns=['mse', 'r2', 'f1', 'nep', 'nde', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9498d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_results.to_csv(r\"C:\\Users\\aar245.CORNELL\\Desktop\\privacy_preserving_nn\\global_squ_foot_filter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2cb824",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = pd.DataFrame(columns = ['mse', 'r2', 'f1', 'nep', 'nde', 'mae'])\n",
    "count = 0\n",
    "for i in range(len(random_select)):\n",
    "    df = pd.DataFrame(global_results[count:count+random_select[i]].mean()).T\n",
    "    df.index = ['global'+\"_\"+str(random_select[i])+\"_Homes\"]\n",
    "    count += int(random_select[i])\n",
    "    avg_df = avg_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64750d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4885cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_box_plots(data, metric):\n",
    "    home_iter = [2,3,4,5,6,7,8,9,10,15,20,25,30,35,38]\n",
    "    count = 0\n",
    "    all_bldgs = []\n",
    "    for j in range(len(home_iter)):\n",
    "        bldgs = data[count:count+home_iter[j]][str(metric)].values\n",
    "        all_bldgs.append(bldgs)\n",
    "        count += home_iter[j]\n",
    "    return plt.boxplot(all_bldgs, showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df34ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_box_plots(global_results, 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_global_results = pd.read_csv(r\"C:\\Users\\aar245.CORNELL\\Desktop\\privacy_preserving_nn\\global_vs_fl_corrected.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_global_results = old_global_results[0:int(len(old_global_results)/2)-33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_results.iloc[9:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "\n",
    "global_results.iloc[44:-73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111dc968",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.append(global_results.iloc[44:-73])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e27ac7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a91b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25011a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1138809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adjacent_box_plots(global_results, fl_results, metric):\n",
    "    \n",
    "    iters = []\n",
    "    for result in global_results.index:\n",
    "        iters.append(int(result.split(':')[1].split('_')[0]))\n",
    "        \n",
    "    home_iter = np.unique(iters)\n",
    "    \n",
    "    count = 0\n",
    "    all_global_results = []\n",
    "    all_fl_results = []\n",
    "    \n",
    "    for j in range(len(home_iter)):\n",
    "        bldgs = global_results[count:count+home_iter[j]][str(metric)].values\n",
    "        all_global_results.append(bldgs)\n",
    "        count += home_iter[j]\n",
    "        \n",
    "    count = 0\n",
    "    for j in range(len(home_iter)):\n",
    "        bldgs = fl_results[count:count+home_iter[j]][str(metric)].values\n",
    "        all_fl_results.append(bldgs)\n",
    "        count += home_iter[j]\n",
    "        \n",
    "    all_data = [list(a) for a in zip(all_global_results, all_fl_results)]\n",
    "    all_data = [item for sublist in all_data for item in sublist]\n",
    "    \n",
    "    positions = [1]\n",
    "    labels = ['global']\n",
    "    for i in range(len(all_data)-1):\n",
    "        if i%2==0:\n",
    "            positions.append(positions[i]+0.5)\n",
    "        else:\n",
    "            positions.append(positions[i]+1)\n",
    "\n",
    "    labels = []\n",
    "    for home in home_iter:\n",
    "        labels.append(home)\n",
    "        labels.append('')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 4))\n",
    "    bplot = ax.boxplot(\n",
    "        all_data,\n",
    "        notch=False,\n",
    "        patch_artist=True,\n",
    "        positions=positions,\n",
    "        labels=labels,\n",
    "        showmeans=True\n",
    "    )\n",
    "    \n",
    "    #ax.set_xticklabels(labels, ha='center')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels())\n",
    "    dx = 27/72.; dy = 0/72. \n",
    "    offset = matplotlib.transforms.ScaledTranslation(dx, dy, fig.dpi_scale_trans)\n",
    "    \n",
    "    for label in ax.xaxis.get_majorticklabels():\n",
    "        label.set_transform(label.get_transform() + offset)\n",
    "    \n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    \n",
    "    ax.set_title(\"Global and FL Metrics for increasing quantity of homes\")\n",
    "    \n",
    "    colors = []\n",
    "    for i in range(int(len(labels)/2)):\n",
    "        colors.append('blue')\n",
    "        colors.append('red')\n",
    "        \n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "    ax.set_xlabel('Number of homes')\n",
    "    ax.set_ylabel(str(metric))\n",
    "    \n",
    "    custom_lines = [Line2D([0], [0], color='blue', lw=4),\n",
    "                Line2D([0], [0], color='red', lw=4)]\n",
    "    \n",
    "    ax.legend(custom_lines, ['squ_foot', 'k_means'])\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e5d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_adjacent_box_plots(new_df, old_global_results, 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2f9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privacy_ml_env",
   "language": "python",
   "name": "privacy_ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
