LSTM(
  (conv1): Conv1d(1, 16, kernel_size=(7,), stride=(1,), padding=(3,))
  (lstm1): LSTM(16, 64, batch_first=True, bidirectional=True)
  (lstm2): LSTM(128, 85, batch_first=True, bidirectional=True)
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=170, out_features=170, bias=True)
  )
  (linear1): Linear(in_features=170, out_features=211, bias=True)
  (linear2): Linear(in_features=211, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (maxpool): MaxPool1d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)
  (dropout): Dropout(p=0.5, inplace=False)
)
Window Length:  136
trigger times: 0
Loss after 00963 batches: 0.1937
trigger times: 1
Loss after 01926 batches: 0.1688
trigger times: 2
Loss after 02889 batches: 0.1155
trigger times: 3
Loss after 03852 batches: 0.1025
trigger times: 4
Loss after 04815 batches: 0.0992
trigger times: 5
Loss after 05778 batches: 0.0972
trigger times: 6
Loss after 06741 batches: 0.0964
trigger times: 7
Loss after 07704 batches: 0.0960
trigger times: 8
Loss after 08667 batches: 0.0953
trigger times: 9
Loss after 09630 batches: 0.0955
trigger times: 10
Loss after 10593 batches: 0.0953
trigger times: 11
Loss after 11556 batches: 0.0951
trigger times: 12
Loss after 12519 batches: 0.0953
trigger times: 13
Loss after 13482 batches: 0.0951
trigger times: 14
Loss after 14445 batches: 0.0955
trigger times: 15
Loss after 15408 batches: 0.0955
trigger times: 16
Loss after 16371 batches: 0.0956
trigger times: 17
Loss after 17334 batches: 0.0956
trigger times: 18
Loss after 18297 batches: 0.0954
trigger times: 19
Loss after 19260 batches: 0.0955
trigger times: 20
Loss after 20223 batches: 0.0957
trigger times: 21
Loss after 21186 batches: 0.0950
trigger times: 22
Loss after 22149 batches: 0.0955
trigger times: 23
Loss after 23112 batches: 0.0954
trigger times: 24
Loss after 24075 batches: 0.0955
trigger times: 25
Early stopping!
Start to test process.
Loss after 25038 batches: 0.0954
Time to train on one home:  54.50317192077637
trigger times: 0
Loss after 25996 batches: 0.2194
trigger times: 1
Loss after 26954 batches: 0.1967
trigger times: 2
Loss after 27912 batches: 0.1543
trigger times: 3
Loss after 28870 batches: 0.1440
trigger times: 4
Loss after 29828 batches: 0.1423
trigger times: 5
Loss after 30786 batches: 0.1406
trigger times: 6
Loss after 31744 batches: 0.1398
trigger times: 7
Loss after 32702 batches: 0.1397
trigger times: 8
Loss after 33660 batches: 0.1390
trigger times: 9
Loss after 34618 batches: 0.1392
trigger times: 10
Loss after 35576 batches: 0.1396
trigger times: 11
Loss after 36534 batches: 0.1391
trigger times: 12
Loss after 37492 batches: 0.1395
trigger times: 13
Loss after 38450 batches: 0.1392
trigger times: 14
Loss after 39408 batches: 0.1403
trigger times: 15
Loss after 40366 batches: 0.1384
trigger times: 16
Loss after 41324 batches: 0.1405
trigger times: 17
Loss after 42282 batches: 0.1387
trigger times: 18
Loss after 43240 batches: 0.1392
trigger times: 19
Loss after 44198 batches: 0.1385
trigger times: 20
Loss after 45156 batches: 0.1374
trigger times: 21
Loss after 46114 batches: 0.1200
trigger times: 0
Loss after 47072 batches: 0.1108
trigger times: 1
Loss after 48030 batches: 0.0969
trigger times: 2
Loss after 48988 batches: 0.0819
trigger times: 3
Loss after 49946 batches: 0.0759
trigger times: 4
Loss after 50904 batches: 0.0725
trigger times: 5
Loss after 51862 batches: 0.0705
trigger times: 6
Loss after 52820 batches: 0.0686
trigger times: 0
Loss after 53778 batches: 0.0649
trigger times: 0
Loss after 54736 batches: 0.0640
trigger times: 0
Loss after 55694 batches: 0.0621
trigger times: 0
Loss after 56652 batches: 0.0595
trigger times: 1
Loss after 57610 batches: 0.0604
trigger times: 0
Loss after 58568 batches: 0.0576
trigger times: 1
Loss after 59526 batches: 0.0527
trigger times: 2
Loss after 60484 batches: 0.0481
trigger times: 3
Loss after 61442 batches: 0.0469
trigger times: 4
Loss after 62400 batches: 0.0434
trigger times: 5
Loss after 63358 batches: 0.0432
trigger times: 6
Loss after 64316 batches: 0.0432
trigger times: 7
Loss after 65274 batches: 0.0418
trigger times: 8
Loss after 66232 batches: 0.0430
trigger times: 9
Loss after 67190 batches: 0.0439
trigger times: 10
Loss after 68148 batches: 0.0408
trigger times: 11
Loss after 69106 batches: 0.0425
trigger times: 12
Loss after 70064 batches: 0.0381
trigger times: 13
Loss after 71022 batches: 0.0370
trigger times: 14
Loss after 71980 batches: 0.0367
trigger times: 15
Loss after 72938 batches: 0.0370
trigger times: 16
Loss after 73896 batches: 0.0379
trigger times: 17
Loss after 74854 batches: 0.0366
trigger times: 18
Loss after 75812 batches: 0.0394
trigger times: 19
Loss after 76770 batches: 0.0386
trigger times: 20
Loss after 77728 batches: 0.0372
trigger times: 21
Loss after 78686 batches: 0.0368
trigger times: 22
Loss after 79644 batches: 0.0359
trigger times: 23
Loss after 80602 batches: 0.0352
trigger times: 24
Loss after 81560 batches: 0.0347
trigger times: 25
Early stopping!
Start to test process.
Loss after 82518 batches: 0.0329
Time to train on one home:  84.49648404121399
trigger times: 0
Loss after 83481 batches: 0.3794
trigger times: 1
Loss after 84444 batches: 0.3328
trigger times: 2
Loss after 85407 batches: 0.1782
trigger times: 3
Loss after 86370 batches: 0.0994
trigger times: 4
Loss after 87333 batches: 0.0927
trigger times: 5
Loss after 88296 batches: 0.0793
trigger times: 6
Loss after 89259 batches: 0.0768
trigger times: 7
Loss after 90222 batches: 0.0744
trigger times: 8
Loss after 91185 batches: 0.0732
trigger times: 9
Loss after 92148 batches: 0.0727
trigger times: 10
Loss after 93111 batches: 0.0726
trigger times: 11
Loss after 94074 batches: 0.0723
trigger times: 12
Loss after 95037 batches: 0.0720
trigger times: 13
Loss after 96000 batches: 0.0720
trigger times: 14
Loss after 96963 batches: 0.0720
trigger times: 15
Loss after 97926 batches: 0.0717
trigger times: 16
Loss after 98889 batches: 0.0715
trigger times: 17
Loss after 99852 batches: 0.0716
trigger times: 18
Loss after 100815 batches: 0.0712
trigger times: 19
Loss after 101778 batches: 0.0711
trigger times: 20
Loss after 102741 batches: 0.0712
trigger times: 21
Loss after 103704 batches: 0.0712
trigger times: 22
Loss after 104667 batches: 0.0710
trigger times: 23
Loss after 105630 batches: 0.0709
trigger times: 24
Loss after 106593 batches: 0.0709
trigger times: 25
Early stopping!
Start to test process.
Loss after 107556 batches: 0.0709
Time to train on one home:  55.7741641998291
trigger times: 0
Loss after 108519 batches: 0.3509
trigger times: 1
Loss after 109482 batches: 0.3091
trigger times: 2
Loss after 110445 batches: 0.1719
trigger times: 3
Loss after 111408 batches: 0.1124
trigger times: 4
Loss after 112371 batches: 0.1036
trigger times: 5
Loss after 113334 batches: 0.0927
trigger times: 6
Loss after 114297 batches: 0.0904
trigger times: 7
Loss after 115260 batches: 0.0892
trigger times: 8
Loss after 116223 batches: 0.0882
trigger times: 9
Loss after 117186 batches: 0.0876
trigger times: 10
Loss after 118149 batches: 0.0876
trigger times: 11
Loss after 119112 batches: 0.0874
trigger times: 12
Loss after 120075 batches: 0.0872
trigger times: 13
Loss after 121038 batches: 0.0869
trigger times: 14
Loss after 122001 batches: 0.0870
trigger times: 15
Loss after 122964 batches: 0.0869
trigger times: 16
Loss after 123927 batches: 0.0867
trigger times: 17
Loss after 124890 batches: 0.0869
trigger times: 18
Loss after 125853 batches: 0.0867
trigger times: 19
Loss after 126816 batches: 0.0865
trigger times: 20
Loss after 127779 batches: 0.0865
trigger times: 21
Loss after 128742 batches: 0.0866
trigger times: 22
Loss after 129705 batches: 0.0866
trigger times: 23
Loss after 130668 batches: 0.0866
trigger times: 24
Loss after 131631 batches: 0.0864
trigger times: 25
Early stopping!
Start to test process.
Loss after 132594 batches: 0.0863
Time to train on one home:  60.99970626831055
trigger times: 0
Loss after 133557 batches: 0.2191
trigger times: 1
Loss after 134520 batches: 0.1858
trigger times: 2
Loss after 135483 batches: 0.0956
trigger times: 3
Loss after 136446 batches: 0.0714
trigger times: 4
Loss after 137409 batches: 0.0646
trigger times: 5
Loss after 138372 batches: 0.0583
trigger times: 6
Loss after 139335 batches: 0.0570
trigger times: 7
Loss after 140298 batches: 0.0556
trigger times: 8
Loss after 141261 batches: 0.0554
trigger times: 9
Loss after 142224 batches: 0.0548
trigger times: 10
Loss after 143187 batches: 0.0548
trigger times: 11
Loss after 144150 batches: 0.0547
trigger times: 12
Loss after 145113 batches: 0.0545
trigger times: 13
Loss after 146076 batches: 0.0545
trigger times: 14
Loss after 147039 batches: 0.0544
trigger times: 15
Loss after 148002 batches: 0.0543
trigger times: 16
Loss after 148965 batches: 0.0542
trigger times: 17
Loss after 149928 batches: 0.0542
trigger times: 18
Loss after 150891 batches: 0.0542
trigger times: 19
Loss after 151854 batches: 0.0541
trigger times: 20
Loss after 152817 batches: 0.0543
trigger times: 21
Loss after 153780 batches: 0.0538
trigger times: 22
Loss after 154743 batches: 0.0540
trigger times: 23
Loss after 155706 batches: 0.0540
trigger times: 24
Loss after 156669 batches: 0.0541
trigger times: 25
Early stopping!
Start to test process.
Loss after 157632 batches: 0.0539
Time to train on one home:  56.838218450546265
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 158595 batches: 0.5050
trigger times: 1
Loss after 159558 batches: 0.4495
trigger times: 2
Loss after 160521 batches: 0.2429
trigger times: 3
Loss after 161484 batches: 0.0706
trigger times: 4
Loss after 162447 batches: 0.0568
trigger times: 5
Loss after 163410 batches: 0.0379
trigger times: 6
Loss after 164373 batches: 0.0333
trigger times: 7
Loss after 165336 batches: 0.0299
trigger times: 8
Loss after 166299 batches: 0.0290
trigger times: 9
Loss after 167262 batches: 0.0280
trigger times: 10
Loss after 168225 batches: 0.0273
trigger times: 11
Loss after 169188 batches: 0.0272
trigger times: 12
Loss after 170151 batches: 0.0267
trigger times: 13
Loss after 171114 batches: 0.0267
trigger times: 14
Loss after 172077 batches: 0.0264
trigger times: 15
Loss after 173040 batches: 0.0264
trigger times: 16
Loss after 174003 batches: 0.0261
trigger times: 17
Loss after 174966 batches: 0.0262
trigger times: 18
Loss after 175929 batches: 0.0259
trigger times: 19
Loss after 176892 batches: 0.0258
trigger times: 20
Loss after 177855 batches: 0.0260
trigger times: 21
Loss after 178818 batches: 0.0258
trigger times: 22
Loss after 179781 batches: 0.0259
trigger times: 23
Loss after 180744 batches: 0.0251
trigger times: 24
Loss after 181707 batches: 0.0252
trigger times: 25
Early stopping!
Start to test process.
Loss after 182670 batches: 0.0249
Time to train on one home:  57.427090883255005
trigger times: 0
Loss after 183633 batches: 0.2735
trigger times: 1
Loss after 184596 batches: 0.2431
trigger times: 2
Loss after 185559 batches: 0.1656
trigger times: 3
Loss after 186522 batches: 0.1421
trigger times: 4
Loss after 187485 batches: 0.1382
trigger times: 5
Loss after 188448 batches: 0.1341
trigger times: 6
Loss after 189411 batches: 0.1329
trigger times: 7
Loss after 190374 batches: 0.1323
trigger times: 8
Loss after 191337 batches: 0.1322
trigger times: 9
Loss after 192300 batches: 0.1319
trigger times: 10
Loss after 193263 batches: 0.1315
trigger times: 11
Loss after 194226 batches: 0.1314
trigger times: 12
Loss after 195189 batches: 0.1314
trigger times: 13
Loss after 196152 batches: 0.1312
trigger times: 14
Loss after 197115 batches: 0.1313
trigger times: 15
Loss after 198078 batches: 0.1311
trigger times: 16
Loss after 199041 batches: 0.1311
trigger times: 17
Loss after 200004 batches: 0.1308
trigger times: 18
Loss after 200967 batches: 0.1309
trigger times: 19
Loss after 201930 batches: 0.1311
trigger times: 20
Loss after 202893 batches: 0.1308
trigger times: 21
Loss after 203856 batches: 0.1311
trigger times: 22
Loss after 204819 batches: 0.1310
trigger times: 23
Loss after 205782 batches: 0.1310
trigger times: 24
Loss after 206745 batches: 0.1309
trigger times: 25
Early stopping!
Start to test process.
Loss after 207708 batches: 0.1306
Time to train on one home:  57.31226110458374
trigger times: 0
Loss after 208671 batches: 0.3890
trigger times: 1
Loss after 209634 batches: 0.3455
trigger times: 2
Loss after 210597 batches: 0.2009
trigger times: 3
Loss after 211560 batches: 0.1298
trigger times: 4
Loss after 212523 batches: 0.1212
trigger times: 5
Loss after 213486 batches: 0.1094
trigger times: 6
Loss after 214449 batches: 0.1074
trigger times: 7
Loss after 215412 batches: 0.1059
trigger times: 8
Loss after 216375 batches: 0.1048
trigger times: 9
Loss after 217338 batches: 0.1043
trigger times: 10
Loss after 218301 batches: 0.1039
trigger times: 11
Loss after 219264 batches: 0.1042
trigger times: 12
Loss after 220227 batches: 0.1039
trigger times: 13
Loss after 221190 batches: 0.1038
trigger times: 14
Loss after 222153 batches: 0.1036
trigger times: 15
Loss after 223116 batches: 0.1034
trigger times: 16
Loss after 224079 batches: 0.1034
trigger times: 17
Loss after 225042 batches: 0.1032
trigger times: 18
Loss after 226005 batches: 0.1031
trigger times: 19
Loss after 226968 batches: 0.1030
trigger times: 20
Loss after 227931 batches: 0.1028
trigger times: 21
Loss after 228894 batches: 0.1030
trigger times: 22
Loss after 229857 batches: 0.1026
trigger times: 23
Loss after 230820 batches: 0.1027
trigger times: 24
Loss after 231783 batches: 0.1027
trigger times: 25
Early stopping!
Start to test process.
Loss after 232746 batches: 0.1022
Time to train on one home:  53.738470792770386
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 233709 batches: 0.3779
trigger times: 1
Loss after 234672 batches: 0.3363
trigger times: 2
Loss after 235635 batches: 0.1995
trigger times: 3
Loss after 236598 batches: 0.1399
trigger times: 4
Loss after 237561 batches: 0.1275
trigger times: 5
Loss after 238524 batches: 0.1176
trigger times: 6
Loss after 239487 batches: 0.1158
trigger times: 7
Loss after 240450 batches: 0.1141
trigger times: 8
Loss after 241413 batches: 0.1137
trigger times: 9
Loss after 242376 batches: 0.1138
trigger times: 10
Loss after 243339 batches: 0.1130
trigger times: 11
Loss after 244302 batches: 0.1128
trigger times: 12
Loss after 245265 batches: 0.1125
trigger times: 13
Loss after 246228 batches: 0.1124
trigger times: 14
Loss after 247191 batches: 0.1124
trigger times: 15
Loss after 248154 batches: 0.1122
trigger times: 16
Loss after 249117 batches: 0.1126
trigger times: 17
Loss after 250080 batches: 0.1126
trigger times: 18
Loss after 251043 batches: 0.1120
trigger times: 19
Loss after 252006 batches: 0.1122
trigger times: 20
Loss after 252969 batches: 0.1119
trigger times: 21
Loss after 253932 batches: 0.1119
trigger times: 22
Loss after 254895 batches: 0.1122
trigger times: 23
Loss after 255858 batches: 0.1116
trigger times: 24
Loss after 256821 batches: 0.1117
trigger times: 25
Early stopping!
Start to test process.
Loss after 257784 batches: 0.1117
Time to train on one home:  55.611892223358154
trigger times: 0
Loss after 258747 batches: 0.2056
trigger times: 1
Loss after 259710 batches: 0.1819
trigger times: 2
Loss after 260673 batches: 0.1346
trigger times: 3
Loss after 261636 batches: 0.1262
trigger times: 4
Loss after 262599 batches: 0.1239
trigger times: 5
Loss after 263562 batches: 0.1211
trigger times: 6
Loss after 264525 batches: 0.1201
trigger times: 7
Loss after 265488 batches: 0.1198
trigger times: 8
Loss after 266451 batches: 0.1194
trigger times: 9
Loss after 267414 batches: 0.1199
trigger times: 10
Loss after 268377 batches: 0.1197
trigger times: 11
Loss after 269340 batches: 0.1192
trigger times: 12
Loss after 270303 batches: 0.1192
trigger times: 13
Loss after 271266 batches: 0.1195
trigger times: 14
Loss after 272229 batches: 0.1195
trigger times: 15
Loss after 273192 batches: 0.1196
trigger times: 16
Loss after 274155 batches: 0.1193
trigger times: 17
Loss after 275118 batches: 0.1195
trigger times: 18
Loss after 276081 batches: 0.1195
trigger times: 19
Loss after 277044 batches: 0.1196
trigger times: 20
Loss after 278007 batches: 0.1192
trigger times: 21
Loss after 278970 batches: 0.1195
trigger times: 22
Loss after 279933 batches: 0.1195
trigger times: 23
Loss after 280896 batches: 0.1191
trigger times: 24
Loss after 281859 batches: 0.1190
trigger times: 25
Early stopping!
Start to test process.
Loss after 282822 batches: 0.1198
Time to train on one home:  56.7936487197876
trigger times: 0
Loss after 283785 batches: 0.2517
trigger times: 1
Loss after 284748 batches: 0.2191
trigger times: 2
Loss after 285711 batches: 0.1365
trigger times: 3
Loss after 286674 batches: 0.1178
trigger times: 4
Loss after 287637 batches: 0.1102
trigger times: 5
Loss after 288600 batches: 0.1047
trigger times: 6
Loss after 289563 batches: 0.1040
trigger times: 7
Loss after 290526 batches: 0.1038
trigger times: 8
Loss after 291489 batches: 0.1033
trigger times: 9
Loss after 292452 batches: 0.1029
trigger times: 10
Loss after 293415 batches: 0.1028
trigger times: 11
Loss after 294378 batches: 0.1030
trigger times: 12
Loss after 295341 batches: 0.1026
trigger times: 13
Loss after 296304 batches: 0.1030
trigger times: 14
Loss after 297267 batches: 0.1030
trigger times: 15
Loss after 298230 batches: 0.1029
trigger times: 16
Loss after 299193 batches: 0.1024
trigger times: 17
Loss after 300156 batches: 0.1025
trigger times: 18
Loss after 301119 batches: 0.1022
trigger times: 19
Loss after 302082 batches: 0.1024
trigger times: 20
Loss after 303045 batches: 0.1021
trigger times: 21
Loss after 304008 batches: 0.1023
trigger times: 22
Loss after 304971 batches: 0.1026
trigger times: 23
Loss after 305934 batches: 0.1023
trigger times: 24
Loss after 306897 batches: 0.1027
trigger times: 25
Early stopping!
Start to test process.
Loss after 307860 batches: 0.1021
Time to train on one home:  56.245781660079956
trigger times: 0
Loss after 308823 batches: 0.2276
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 309786 batches: 0.1952
trigger times: 2
Loss after 310749 batches: 0.1088
trigger times: 3
Loss after 311712 batches: 0.0857
trigger times: 4
Loss after 312675 batches: 0.0791
trigger times: 5
Loss after 313638 batches: 0.0733
trigger times: 6
Loss after 314601 batches: 0.0717
trigger times: 7
Loss after 315564 batches: 0.0708
trigger times: 8
Loss after 316527 batches: 0.0705
trigger times: 9
Loss after 317490 batches: 0.0705
trigger times: 10
Loss after 318453 batches: 0.0700
trigger times: 11
Loss after 319416 batches: 0.0699
trigger times: 12
Loss after 320379 batches: 0.0698
trigger times: 13
Loss after 321342 batches: 0.0700
trigger times: 14
Loss after 322305 batches: 0.0699
trigger times: 15
Loss after 323268 batches: 0.0698
trigger times: 16
Loss after 324231 batches: 0.0696
trigger times: 17
Loss after 325194 batches: 0.0699
trigger times: 18
Loss after 326157 batches: 0.0698
trigger times: 19
Loss after 327120 batches: 0.0695
trigger times: 20
Loss after 328083 batches: 0.0696
trigger times: 21
Loss after 329046 batches: 0.0695
trigger times: 22
Loss after 330009 batches: 0.0693
trigger times: 23
Loss after 330972 batches: 0.0696
trigger times: 24
Loss after 331935 batches: 0.0694
trigger times: 25
Early stopping!
Start to test process.
Loss after 332898 batches: 0.0694
Time to train on one home:  58.40022826194763
trigger times: 0
Loss after 333861 batches: 0.2291
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 334824 batches: 0.2055
trigger times: 2
Loss after 335787 batches: 0.1562
trigger times: 3
Loss after 336750 batches: 0.1441
trigger times: 4
Loss after 337713 batches: 0.1415
trigger times: 5
Loss after 338676 batches: 0.1396
trigger times: 6
Loss after 339639 batches: 0.1388
trigger times: 7
Loss after 340602 batches: 0.1384
trigger times: 8
Loss after 341565 batches: 0.1389
trigger times: 9
Loss after 342528 batches: 0.1382
trigger times: 10
Loss after 343491 batches: 0.1384
trigger times: 11
Loss after 344454 batches: 0.1381
trigger times: 12
Loss after 345417 batches: 0.1382
trigger times: 13
Loss after 346380 batches: 0.1384
trigger times: 14
Loss after 347343 batches: 0.1381
trigger times: 15
Loss after 348306 batches: 0.1380
trigger times: 16
Loss after 349269 batches: 0.1381
trigger times: 17
Loss after 350232 batches: 0.1379
trigger times: 18
Loss after 351195 batches: 0.1382
trigger times: 19
Loss after 352158 batches: 0.1376
trigger times: 20
Loss after 353121 batches: 0.1379
trigger times: 21
Loss after 354084 batches: 0.1378
trigger times: 22
Loss after 355047 batches: 0.1378
trigger times: 23
Loss after 356010 batches: 0.1380
trigger times: 24
Loss after 356973 batches: 0.1377
trigger times: 25
Early stopping!
Start to test process.
Loss after 357936 batches: 0.1381
Time to train on one home:  56.478790283203125
trigger times: 0
Loss after 358865 batches: 0.2156
trigger times: 1
Loss after 359794 batches: 0.1897
trigger times: 2
Loss after 360723 batches: 0.1492
trigger times: 3
Loss after 361652 batches: 0.1391
trigger times: 4
Loss after 362581 batches: 0.1370
trigger times: 5
Loss after 363510 batches: 0.1355
trigger times: 6
Loss after 364439 batches: 0.1351
trigger times: 7
Loss after 365368 batches: 0.1345
trigger times: 8
Loss after 366297 batches: 0.1354
trigger times: 9
Loss after 367226 batches: 0.1364
trigger times: 10
Loss after 368155 batches: 0.1353
trigger times: 11
Loss after 369084 batches: 0.1368
trigger times: 12
Loss after 370013 batches: 0.1339
trigger times: 13
Loss after 370942 batches: 0.1356
trigger times: 14
Loss after 371871 batches: 0.1237
trigger times: 0
Loss after 372800 batches: 0.0918
trigger times: 1
Loss after 373729 batches: 0.0836
trigger times: 2
Loss after 374658 batches: 0.0723
trigger times: 3
Loss after 375587 batches: 0.0698
trigger times: 4
Loss after 376516 batches: 0.0727
trigger times: 5
Loss after 377445 batches: 0.0678
trigger times: 6
Loss after 378374 batches: 0.0651
trigger times: 7
Loss after 379303 batches: 0.0612
trigger times: 8
Loss after 380232 batches: 0.0633
trigger times: 9
Loss after 381161 batches: 0.0617
trigger times: 10
Loss after 382090 batches: 0.0613
trigger times: 11
Loss after 383019 batches: 0.0576
trigger times: 12
Loss after 383948 batches: 0.0551
trigger times: 13
Loss after 384877 batches: 0.0570
trigger times: 14
Loss after 385806 batches: 0.0545
trigger times: 0
Loss after 386735 batches: 0.0493
trigger times: 0
Loss after 387664 batches: 0.0464
trigger times: 0
Loss after 388593 batches: 0.0442
trigger times: 1
Loss after 389522 batches: 0.0429
trigger times: 2
Loss after 390451 batches: 0.0432
trigger times: 3
Loss after 391380 batches: 0.0438
trigger times: 4
Loss after 392309 batches: 0.0424
trigger times: 5
Loss after 393238 batches: 0.0409
trigger times: 0
Loss after 394167 batches: 0.0421
trigger times: 1
Loss after 395096 batches: 0.0425
trigger times: 2
Loss after 396025 batches: 0.0405
trigger times: 3
Loss after 396954 batches: 0.0385
trigger times: 4
Loss after 397883 batches: 0.0462
trigger times: 5
Loss after 398812 batches: 0.0392
trigger times: 6
Loss after 399741 batches: 0.0384
trigger times: 7
Loss after 400670 batches: 0.0368
trigger times: 8
Loss after 401599 batches: 0.0383
trigger times: 9
Loss after 402528 batches: 0.0404
trigger times: 10
Loss after 403457 batches: 0.0436
trigger times: 11
Loss after 404386 batches: 0.0439
trigger times: 12
Loss after 405315 batches: 0.0430
trigger times: 13
Loss after 406244 batches: 0.0405
trigger times: 14
Loss after 407173 batches: 0.0415
trigger times: 15
Loss after 408102 batches: 0.0401
trigger times: 16
Loss after 409031 batches: 0.0362
trigger times: 17
Loss after 409960 batches: 0.0379
trigger times: 18
Loss after 410889 batches: 0.0358
trigger times: 19
Loss after 411818 batches: 0.0348
trigger times: 20
Loss after 412747 batches: 0.0375
trigger times: 21
Loss after 413676 batches: 0.0343
trigger times: 22
Loss after 414605 batches: 0.0367
trigger times: 23
Loss after 415534 batches: 0.0355
trigger times: 24
Loss after 416463 batches: 0.0360
trigger times: 25
Early stopping!
Start to test process.
Loss after 417392 batches: 0.0354
Time to train on one home:  87.99925565719604
trigger times: 0
Loss after 418354 batches: 0.2835
trigger times: 1
Loss after 419316 batches: 0.2449
trigger times: 2
Loss after 420278 batches: 0.1345
trigger times: 3
Loss after 421240 batches: 0.0970
trigger times: 4
Loss after 422202 batches: 0.0893
trigger times: 5
Loss after 423164 batches: 0.0822
trigger times: 6
Loss after 424126 batches: 0.0804
trigger times: 7
Loss after 425088 batches: 0.0784
trigger times: 8
Loss after 426050 batches: 0.0787
trigger times: 9
Loss after 427012 batches: 0.0779
trigger times: 10
Loss after 427974 batches: 0.0778
trigger times: 11
Loss after 428936 batches: 0.0773
trigger times: 12
Loss after 429898 batches: 0.0780
trigger times: 13
Loss after 430860 batches: 0.0772
trigger times: 14
Loss after 431822 batches: 0.0774
trigger times: 15
Loss after 432784 batches: 0.0776
trigger times: 16
Loss after 433746 batches: 0.0773
trigger times: 17
Loss after 434708 batches: 0.0776
trigger times: 18
Loss after 435670 batches: 0.0773
trigger times: 19
Loss after 436632 batches: 0.0770
trigger times: 20
Loss after 437594 batches: 0.0773
trigger times: 21
Loss after 438556 batches: 0.0773
trigger times: 22
Loss after 439518 batches: 0.0769
trigger times: 23
Loss after 440480 batches: 0.0771
trigger times: 24
Loss after 441442 batches: 0.0771
trigger times: 25
Early stopping!
Start to test process.
Loss after 442404 batches: 0.0770
Time to train on one home:  54.92232084274292
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 443367 batches: 0.1824
trigger times: 1
Loss after 444330 batches: 0.1549
trigger times: 2
Loss after 445293 batches: 0.0894
trigger times: 3
Loss after 446256 batches: 0.0730
trigger times: 4
Loss after 447219 batches: 0.0701
trigger times: 5
Loss after 448182 batches: 0.0659
trigger times: 6
Loss after 449145 batches: 0.0649
trigger times: 7
Loss after 450108 batches: 0.0643
trigger times: 8
Loss after 451071 batches: 0.0639
trigger times: 9
Loss after 452034 batches: 0.0637
trigger times: 10
Loss after 452997 batches: 0.0637
trigger times: 11
Loss after 453960 batches: 0.0635
trigger times: 12
Loss after 454923 batches: 0.0637
trigger times: 13
Loss after 455886 batches: 0.0637
trigger times: 14
Loss after 456849 batches: 0.0634
trigger times: 15
Loss after 457812 batches: 0.0632
trigger times: 16
Loss after 458775 batches: 0.0632
trigger times: 17
Loss after 459738 batches: 0.0631
trigger times: 18
Loss after 460701 batches: 0.0633
trigger times: 19
Loss after 461664 batches: 0.0631
trigger times: 20
Loss after 462627 batches: 0.0634
trigger times: 21
Loss after 463590 batches: 0.0631
trigger times: 22
Loss after 464553 batches: 0.0631
trigger times: 23
Loss after 465516 batches: 0.0631
trigger times: 24
Loss after 466479 batches: 0.0630
trigger times: 25
Early stopping!
Start to test process.
Loss after 467442 batches: 0.0631
Time to train on one home:  54.413456201553345
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 468405 batches: 0.2147
trigger times: 1
Loss after 469368 batches: 0.1863
trigger times: 2
Loss after 470331 batches: 0.1144
trigger times: 3
Loss after 471294 batches: 0.0974
trigger times: 4
Loss after 472257 batches: 0.0932
trigger times: 5
Loss after 473220 batches: 0.0894
trigger times: 6
Loss after 474183 batches: 0.0879
trigger times: 7
Loss after 475146 batches: 0.0874
trigger times: 8
Loss after 476109 batches: 0.0858
trigger times: 9
Loss after 477072 batches: 0.0862
trigger times: 10
Loss after 478035 batches: 0.0862
trigger times: 11
Loss after 478998 batches: 0.0861
trigger times: 12
Loss after 479961 batches: 0.0863
trigger times: 13
Loss after 480924 batches: 0.0855
trigger times: 14
Loss after 481887 batches: 0.0858
trigger times: 15
Loss after 482850 batches: 0.0857
trigger times: 16
Loss after 483813 batches: 0.0856
trigger times: 17
Loss after 484776 batches: 0.0857
trigger times: 18
Loss after 485739 batches: 0.0856
trigger times: 19
Loss after 486702 batches: 0.0855
trigger times: 20
Loss after 487665 batches: 0.0855
trigger times: 21
Loss after 488628 batches: 0.0858
trigger times: 22
Loss after 489591 batches: 0.0857
trigger times: 23
Loss after 490554 batches: 0.0861
trigger times: 24
Loss after 491517 batches: 0.0857
trigger times: 25
Early stopping!
Start to test process.
Loss after 492480 batches: 0.0861
Time to train on one home:  57.21568512916565
trigger times: 0
Loss after 493443 batches: 0.2935
trigger times: 1
Loss after 494406 batches: 0.2616
trigger times: 2
Loss after 495369 batches: 0.1830
trigger times: 3
Loss after 496332 batches: 0.1627
trigger times: 4
Loss after 497295 batches: 0.1574
trigger times: 5
Loss after 498258 batches: 0.1526
trigger times: 6
Loss after 499221 batches: 0.1510
trigger times: 7
Loss after 500184 batches: 0.1505
trigger times: 8
Loss after 501147 batches: 0.1502
trigger times: 9
Loss after 502110 batches: 0.1500
trigger times: 10
Loss after 503073 batches: 0.1498
trigger times: 11
Loss after 504036 batches: 0.1499
trigger times: 12
Loss after 504999 batches: 0.1500
trigger times: 13
Loss after 505962 batches: 0.1496
trigger times: 14
Loss after 506925 batches: 0.1497
trigger times: 15
Loss after 507888 batches: 0.1497
trigger times: 16
Loss after 508851 batches: 0.1497
trigger times: 17
Loss after 509814 batches: 0.1493
trigger times: 18
Loss after 510777 batches: 0.1496
trigger times: 19
Loss after 511740 batches: 0.1496
trigger times: 20
Loss after 512703 batches: 0.1494
trigger times: 21
Loss after 513666 batches: 0.1493
trigger times: 22
Loss after 514629 batches: 0.1491
trigger times: 23
Loss after 515592 batches: 0.1492
trigger times: 24
Loss after 516555 batches: 0.1492
trigger times: 25
Early stopping!
Start to test process.
Loss after 517518 batches: 0.1494
Time to train on one home:  55.42722821235657
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 518481 batches: 0.1400
trigger times: 1
Loss after 519444 batches: 0.1238
trigger times: 2
Loss after 520407 batches: 0.0946
trigger times: 3
Loss after 521370 batches: 0.0886
trigger times: 4
Loss after 522333 batches: 0.0875
trigger times: 5
Loss after 523296 batches: 0.0869
trigger times: 6
Loss after 524259 batches: 0.0865
trigger times: 7
Loss after 525222 batches: 0.0863
trigger times: 8
Loss after 526185 batches: 0.0862
trigger times: 9
Loss after 527148 batches: 0.0863
trigger times: 10
Loss after 528111 batches: 0.0854
trigger times: 11
Loss after 529074 batches: 0.0853
trigger times: 12
Loss after 530037 batches: 0.0859
trigger times: 13
Loss after 531000 batches: 0.0860
trigger times: 14
Loss after 531963 batches: 0.0858
trigger times: 15
Loss after 532926 batches: 0.0860
trigger times: 16
Loss after 533889 batches: 0.0862
trigger times: 17
Loss after 534852 batches: 0.0854
trigger times: 18
Loss after 535815 batches: 0.0855
trigger times: 19
Loss after 536778 batches: 0.0854
trigger times: 20
Loss after 537741 batches: 0.0858
trigger times: 21
Loss after 538704 batches: 0.0857
trigger times: 22
Loss after 539667 batches: 0.0860
trigger times: 23
Loss after 540630 batches: 0.0853
trigger times: 24
Loss after 541593 batches: 0.0852
trigger times: 25
Early stopping!
Start to test process.
Loss after 542556 batches: 0.0855
Time to train on one home:  58.75437569618225
trigger times: 0
Loss after 543515 batches: 0.1841
trigger times: 1
Loss after 544474 batches: 0.1631
trigger times: 2
Loss after 545433 batches: 0.1294
trigger times: 3
Loss after 546392 batches: 0.1236
trigger times: 4
Loss after 547351 batches: 0.1211
trigger times: 5
Loss after 548310 batches: 0.1195
trigger times: 6
Loss after 549269 batches: 0.1205
trigger times: 7
Loss after 550228 batches: 0.1191
trigger times: 8
Loss after 551187 batches: 0.1198
trigger times: 9
Loss after 552146 batches: 0.1190
trigger times: 10
Loss after 553105 batches: 0.1189
trigger times: 11
Loss after 554064 batches: 0.1192
trigger times: 12
Loss after 555023 batches: 0.1189
trigger times: 13
Loss after 555982 batches: 0.1185
trigger times: 14
Loss after 556941 batches: 0.1189
trigger times: 15
Loss after 557900 batches: 0.1184
trigger times: 16
Loss after 558859 batches: 0.1190
trigger times: 17
Loss after 559818 batches: 0.1187
trigger times: 18
Loss after 560777 batches: 0.1187
trigger times: 19
Loss after 561736 batches: 0.1186
trigger times: 20
Loss after 562695 batches: 0.1128
trigger times: 21
Loss after 563654 batches: 0.1013
trigger times: 22
Loss after 564613 batches: 0.0853
trigger times: 23
Loss after 565572 batches: 0.0668
trigger times: 0
Loss after 566531 batches: 0.0607
trigger times: 1
Loss after 567490 batches: 0.0546
trigger times: 2
Loss after 568449 batches: 0.0501
trigger times: 3
Loss after 569408 batches: 0.0465
trigger times: 0
Loss after 570367 batches: 0.0451
trigger times: 1
Loss after 571326 batches: 0.0431
trigger times: 0
Loss after 572285 batches: 0.0426
trigger times: 0
Loss after 573244 batches: 0.0390
trigger times: 0
Loss after 574203 batches: 0.0378
trigger times: 1
Loss after 575162 batches: 0.0367
trigger times: 2
Loss after 576121 batches: 0.0366
trigger times: 3
Loss after 577080 batches: 0.0349
trigger times: 4
Loss after 578039 batches: 0.0355
trigger times: 0
Loss after 578998 batches: 0.0362
trigger times: 0
Loss after 579957 batches: 0.0341
trigger times: 1
Loss after 580916 batches: 0.0334
trigger times: 2
Loss after 581875 batches: 0.0329
trigger times: 3
Loss after 582834 batches: 0.0316
trigger times: 4
Loss after 583793 batches: 0.0308
trigger times: 5
Loss after 584752 batches: 0.0303
trigger times: 6
Loss after 585711 batches: 0.0303
trigger times: 7
Loss after 586670 batches: 0.0297
trigger times: 8
Loss after 587629 batches: 0.0284
trigger times: 9
Loss after 588588 batches: 0.0282
trigger times: 10
Loss after 589547 batches: 0.0273
trigger times: 11
Loss after 590506 batches: 0.0285
trigger times: 12
Loss after 591465 batches: 0.0280
trigger times: 13
Loss after 592424 batches: 0.0277
trigger times: 14
Loss after 593383 batches: 0.0293
trigger times: 15
Loss after 594342 batches: 0.0274
trigger times: 16
Loss after 595301 batches: 0.0270
trigger times: 17
Loss after 596260 batches: 0.0266
trigger times: 18
Loss after 597219 batches: 0.0261
trigger times: 19
Loss after 598178 batches: 0.0249
trigger times: 20
Loss after 599137 batches: 0.0254
trigger times: 21
Loss after 600096 batches: 0.0251
trigger times: 22
Loss after 601055 batches: 0.0252
trigger times: 23
Loss after 602014 batches: 0.0264
trigger times: 24
Loss after 602973 batches: 0.0253
trigger times: 25
Early stopping!
Start to test process.
Loss after 603932 batches: 0.0259
Time to train on one home:  87.2467360496521
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 604895 batches: 0.3603
trigger times: 1
Loss after 605858 batches: 0.3135
trigger times: 2
Loss after 606821 batches: 0.1492
trigger times: 3
Loss after 607784 batches: 0.0622
trigger times: 4
Loss after 608747 batches: 0.0531
trigger times: 5
Loss after 609710 batches: 0.0381
trigger times: 6
Loss after 610673 batches: 0.0347
trigger times: 7
Loss after 611636 batches: 0.0334
trigger times: 8
Loss after 612599 batches: 0.0315
trigger times: 9
Loss after 613562 batches: 0.0313
trigger times: 10
Loss after 614525 batches: 0.0310
trigger times: 11
Loss after 615488 batches: 0.0304
trigger times: 12
Loss after 616451 batches: 0.0302
trigger times: 13
Loss after 617414 batches: 0.0302
trigger times: 14
Loss after 618377 batches: 0.0300
trigger times: 15
Loss after 619340 batches: 0.0296
trigger times: 16
Loss after 620303 batches: 0.0296
trigger times: 17
Loss after 621266 batches: 0.0295
trigger times: 18
Loss after 622229 batches: 0.0295
trigger times: 19
Loss after 623192 batches: 0.0292
trigger times: 20
Loss after 624155 batches: 0.0290
trigger times: 21
Loss after 625118 batches: 0.0290
trigger times: 22
Loss after 626081 batches: 0.0288
trigger times: 23
Loss after 627044 batches: 0.0288
trigger times: 24
Loss after 628007 batches: 0.0285
trigger times: 25
Early stopping!
Start to test process.
Loss after 628970 batches: 0.0286
Time to train on one home:  56.85087847709656
trigger times: 0
Loss after 629915 batches: 0.2258
trigger times: 1
Loss after 630860 batches: 0.1950
trigger times: 2
Loss after 631805 batches: 0.1291
trigger times: 3
Loss after 632750 batches: 0.1135
trigger times: 4
Loss after 633695 batches: 0.1098
trigger times: 5
Loss after 634640 batches: 0.1051
trigger times: 6
Loss after 635585 batches: 0.1033
trigger times: 7
Loss after 636530 batches: 0.1031
trigger times: 8
Loss after 637475 batches: 0.1029
trigger times: 9
Loss after 638420 batches: 0.1025
trigger times: 10
Loss after 639365 batches: 0.1031
trigger times: 11
Loss after 640310 batches: 0.1025
trigger times: 12
Loss after 641255 batches: 0.1027
trigger times: 13
Loss after 642200 batches: 0.1026
trigger times: 14
Loss after 643145 batches: 0.1023
trigger times: 15
Loss after 644090 batches: 0.1024
trigger times: 16
Loss after 645035 batches: 0.1023
trigger times: 17
Loss after 645980 batches: 0.1025
trigger times: 18
Loss after 646925 batches: 0.1023
trigger times: 19
Loss after 647870 batches: 0.1024
trigger times: 20
Loss after 648815 batches: 0.1025
trigger times: 21
Loss after 649760 batches: 0.1022
trigger times: 22
Loss after 650705 batches: 0.1021
trigger times: 23
Loss after 651650 batches: 0.1023
trigger times: 24
Loss after 652595 batches: 0.1027
trigger times: 25
Early stopping!
Start to test process.
Loss after 653540 batches: 0.1024
Time to train on one home:  55.89514923095703
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 654477 batches: 0.2225
trigger times: 1
Loss after 655414 batches: 0.1920
trigger times: 2
Loss after 656351 batches: 0.1227
trigger times: 3
Loss after 657288 batches: 0.1049
trigger times: 4
Loss after 658225 batches: 0.1005
trigger times: 5
Loss after 659162 batches: 0.0970
trigger times: 6
Loss after 660099 batches: 0.0950
trigger times: 7
Loss after 661036 batches: 0.0952
trigger times: 8
Loss after 661973 batches: 0.0950
trigger times: 9
Loss after 662910 batches: 0.0945
trigger times: 10
Loss after 663847 batches: 0.0947
trigger times: 11
Loss after 664784 batches: 0.0946
trigger times: 12
Loss after 665721 batches: 0.0944
trigger times: 13
Loss after 666658 batches: 0.0942
trigger times: 14
Loss after 667595 batches: 0.0945
trigger times: 15
Loss after 668532 batches: 0.0942
trigger times: 16
Loss after 669469 batches: 0.0949
trigger times: 17
Loss after 670406 batches: 0.0941
trigger times: 18
Loss after 671343 batches: 0.0942
trigger times: 19
Loss after 672280 batches: 0.0939
trigger times: 20
Loss after 673217 batches: 0.0944
trigger times: 21
Loss after 674154 batches: 0.0943
trigger times: 22
Loss after 675091 batches: 0.0942
trigger times: 23
Loss after 676028 batches: 0.0941
trigger times: 24
Loss after 676965 batches: 0.0938
trigger times: 25
Early stopping!
Start to test process.
Loss after 677902 batches: 0.0941
Time to train on one home:  53.29595398902893
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 678865 batches: 0.3570
trigger times: 1
Loss after 679828 batches: 0.3097
trigger times: 2
Loss after 680791 batches: 0.1465
trigger times: 3
Loss after 681754 batches: 0.0457
trigger times: 4
Loss after 682717 batches: 0.0356
trigger times: 5
Loss after 683680 batches: 0.0235
trigger times: 6
Loss after 684643 batches: 0.0206
trigger times: 7
Loss after 685606 batches: 0.0188
trigger times: 8
Loss after 686569 batches: 0.0177
trigger times: 9
Loss after 687532 batches: 0.0173
trigger times: 10
Loss after 688495 batches: 0.0171
trigger times: 11
Loss after 689458 batches: 0.0168
trigger times: 12
Loss after 690421 batches: 0.0166
trigger times: 13
Loss after 691384 batches: 0.0164
trigger times: 14
Loss after 692347 batches: 0.0162
trigger times: 15
Loss after 693310 batches: 0.0161
trigger times: 16
Loss after 694273 batches: 0.0159
trigger times: 17
Loss after 695236 batches: 0.0159
trigger times: 18
Loss after 696199 batches: 0.0157
trigger times: 19
Loss after 697162 batches: 0.0155
trigger times: 20
Loss after 698125 batches: 0.0156
trigger times: 21
Loss after 699088 batches: 0.0154
trigger times: 22
Loss after 700051 batches: 0.0151
trigger times: 23
Loss after 701014 batches: 0.0150
trigger times: 24
Loss after 701977 batches: 0.0150
trigger times: 25
Early stopping!
Start to test process.
Loss after 702940 batches: 0.0150
Time to train on one home:  57.62356877326965
trigger times: 0
Loss after 703903 batches: 0.2410
trigger times: 1
Loss after 704866 batches: 0.2111
trigger times: 2
Loss after 705829 batches: 0.1317
trigger times: 3
Loss after 706792 batches: 0.1052
trigger times: 4
Loss after 707755 batches: 0.1011
trigger times: 5
Loss after 708718 batches: 0.0971
trigger times: 6
Loss after 709681 batches: 0.0957
trigger times: 7
Loss after 710644 batches: 0.0951
trigger times: 8
Loss after 711607 batches: 0.0949
trigger times: 9
Loss after 712570 batches: 0.0946
trigger times: 10
Loss after 713533 batches: 0.0945
trigger times: 11
Loss after 714496 batches: 0.0944
trigger times: 12
Loss after 715459 batches: 0.0944
trigger times: 13
Loss after 716422 batches: 0.0941
trigger times: 14
Loss after 717385 batches: 0.0942
trigger times: 15
Loss after 718348 batches: 0.0947
trigger times: 16
Loss after 719311 batches: 0.0944
trigger times: 17
Loss after 720274 batches: 0.0941
trigger times: 18
Loss after 721237 batches: 0.0942
trigger times: 19
Loss after 722200 batches: 0.0941
trigger times: 20
Loss after 723163 batches: 0.0938
trigger times: 21
Loss after 724126 batches: 0.0938
trigger times: 22
Loss after 725089 batches: 0.0940
trigger times: 23
Loss after 726052 batches: 0.0946
trigger times: 24
Loss after 727015 batches: 0.0944
trigger times: 25
Early stopping!
Start to test process.
Loss after 727978 batches: 0.0944
Time to train on one home:  55.617199659347534
trigger times: 0
Loss after 728941 batches: 0.3899
trigger times: 1
Loss after 729904 batches: 0.3464
trigger times: 2
Loss after 730867 batches: 0.1961
trigger times: 3
Loss after 731830 batches: 0.1237
trigger times: 4
Loss after 732793 batches: 0.1133
trigger times: 5
Loss after 733756 batches: 0.1013
trigger times: 6
Loss after 734719 batches: 0.0992
trigger times: 7
Loss after 735682 batches: 0.0977
trigger times: 8
Loss after 736645 batches: 0.0967
trigger times: 9
Loss after 737608 batches: 0.0962
trigger times: 10
Loss after 738571 batches: 0.0958
trigger times: 11
Loss after 739534 batches: 0.0959
trigger times: 12
Loss after 740497 batches: 0.0954
trigger times: 13
Loss after 741460 batches: 0.0951
trigger times: 14
Loss after 742423 batches: 0.0948
trigger times: 15
Loss after 743386 batches: 0.0950
trigger times: 16
Loss after 744349 batches: 0.0947
trigger times: 17
Loss after 745312 batches: 0.0948
trigger times: 18
Loss after 746275 batches: 0.0947
trigger times: 19
Loss after 747238 batches: 0.0946
trigger times: 20
Loss after 748201 batches: 0.0949
trigger times: 21
Loss after 749164 batches: 0.0949
trigger times: 22
Loss after 750127 batches: 0.0944
trigger times: 23
Loss after 751090 batches: 0.0944
trigger times: 24
Loss after 752053 batches: 0.0945
trigger times: 25
Early stopping!
Start to test process.
Loss after 753016 batches: 0.0943
Time to train on one home:  61.493449449539185
trigger times: 0
Loss after 753912 batches: 0.2868
trigger times: 0
Loss after 754808 batches: 0.2629
trigger times: 1
Loss after 755704 batches: 0.1972
trigger times: 2
Loss after 756600 batches: 0.1365
trigger times: 3
Loss after 757496 batches: 0.1187
trigger times: 4
Loss after 758392 batches: 0.1191
trigger times: 5
Loss after 759288 batches: 0.1149
trigger times: 6
Loss after 760184 batches: 0.1117
trigger times: 7
Loss after 761080 batches: 0.1112
trigger times: 8
Loss after 761976 batches: 0.1119
trigger times: 9
Loss after 762872 batches: 0.1112
trigger times: 10
Loss after 763768 batches: 0.1102
trigger times: 11
Loss after 764664 batches: 0.1103
trigger times: 12
Loss after 765560 batches: 0.1104
trigger times: 13
Loss after 766456 batches: 0.1100
trigger times: 14
Loss after 767352 batches: 0.1099
trigger times: 15
Loss after 768248 batches: 0.1097
trigger times: 16
Loss after 769144 batches: 0.1094
trigger times: 17
Loss after 770040 batches: 0.1097
trigger times: 18
Loss after 770936 batches: 0.1095
trigger times: 19
Loss after 771832 batches: 0.1095
trigger times: 20
Loss after 772728 batches: 0.1095
trigger times: 21
Loss after 773624 batches: 0.1094
trigger times: 22
Loss after 774520 batches: 0.1095
trigger times: 23
Loss after 775416 batches: 0.1096
trigger times: 24
Loss after 776312 batches: 0.1096
trigger times: 25
Early stopping!
Start to test process.
Loss after 777208 batches: 0.1099
Time to train on one home:  55.6991605758667
trigger times: 0
Loss after 778171 batches: 0.5495
trigger times: 1
Loss after 779134 batches: 0.4958
trigger times: 2
Loss after 780097 batches: 0.2987
trigger times: 3
Loss after 781060 batches: 0.1596
trigger times: 4
Loss after 782023 batches: 0.1474
trigger times: 5
Loss after 782986 batches: 0.1320
trigger times: 6
Loss after 783949 batches: 0.1283
trigger times: 7
Loss after 784912 batches: 0.1249
trigger times: 8
Loss after 785875 batches: 0.1238
trigger times: 9
Loss after 786838 batches: 0.1219
trigger times: 10
Loss after 787801 batches: 0.1222
trigger times: 11
Loss after 788764 batches: 0.1207
trigger times: 12
Loss after 789727 batches: 0.1203
trigger times: 13
Loss after 790690 batches: 0.1204
trigger times: 14
Loss after 791653 batches: 0.1200
trigger times: 15
Loss after 792616 batches: 0.1196
trigger times: 16
Loss after 793579 batches: 0.1201
trigger times: 17
Loss after 794542 batches: 0.1195
trigger times: 18
Loss after 795505 batches: 0.1191
trigger times: 19
Loss after 796468 batches: 0.1189
trigger times: 20
Loss after 797431 batches: 0.1195
trigger times: 21
Loss after 798394 batches: 0.1189
trigger times: 22
Loss after 799357 batches: 0.1190
trigger times: 23
Loss after 800320 batches: 0.1190
trigger times: 24
Loss after 801283 batches: 0.1184
trigger times: 25
Early stopping!
Start to test process.
Loss after 802246 batches: 0.1185
Time to train on one home:  56.70681667327881
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 803209 batches: 0.2719
trigger times: 1
Loss after 804172 batches: 0.2380
trigger times: 2
Loss after 805135 batches: 0.1357
trigger times: 3
Loss after 806098 batches: 0.1037
trigger times: 4
Loss after 807061 batches: 0.0984
trigger times: 5
Loss after 808024 batches: 0.0890
trigger times: 6
Loss after 808987 batches: 0.0869
trigger times: 7
Loss after 809950 batches: 0.0860
trigger times: 8
Loss after 810913 batches: 0.0858
trigger times: 9
Loss after 811876 batches: 0.0853
trigger times: 10
Loss after 812839 batches: 0.0855
trigger times: 11
Loss after 813802 batches: 0.0852
trigger times: 12
Loss after 814765 batches: 0.0851
trigger times: 13
Loss after 815728 batches: 0.0849
trigger times: 14
Loss after 816691 batches: 0.0847
trigger times: 15
Loss after 817654 batches: 0.0846
trigger times: 16
Loss after 818617 batches: 0.0845
trigger times: 17
Loss after 819580 batches: 0.0846
trigger times: 18
Loss after 820543 batches: 0.0845
trigger times: 19
Loss after 821506 batches: 0.0844
trigger times: 20
Loss after 822469 batches: 0.0844
trigger times: 21
Loss after 823432 batches: 0.0843
trigger times: 22
Loss after 824395 batches: 0.0842
trigger times: 23
Loss after 825358 batches: 0.0842
trigger times: 24
Loss after 826321 batches: 0.0842
trigger times: 25
Early stopping!
Start to test process.
Loss after 827284 batches: 0.0842
Time to train on one home:  57.01075577735901
trigger times: 0
Loss after 828247 batches: 0.3751
trigger times: 1
Loss after 829210 batches: 0.3349
trigger times: 2
Loss after 830173 batches: 0.2187
trigger times: 3
Loss after 831136 batches: 0.1766
trigger times: 4
Loss after 832099 batches: 0.1680
trigger times: 5
Loss after 833062 batches: 0.1595
trigger times: 6
Loss after 834025 batches: 0.1579
trigger times: 7
Loss after 834988 batches: 0.1561
trigger times: 8
Loss after 835951 batches: 0.1562
trigger times: 9
Loss after 836914 batches: 0.1556
trigger times: 10
Loss after 837877 batches: 0.1552
trigger times: 11
Loss after 838840 batches: 0.1549
trigger times: 12
Loss after 839803 batches: 0.1553
trigger times: 13
Loss after 840766 batches: 0.1547
trigger times: 14
Loss after 841729 batches: 0.1549
trigger times: 15
Loss after 842692 batches: 0.1549
trigger times: 16
Loss after 843655 batches: 0.1548
trigger times: 17
Loss after 844618 batches: 0.1546
trigger times: 18
Loss after 845581 batches: 0.1546
trigger times: 19
Loss after 846544 batches: 0.1546
trigger times: 20
Loss after 847507 batches: 0.1545
trigger times: 21
Loss after 848470 batches: 0.1545
trigger times: 22
Loss after 849433 batches: 0.1544
trigger times: 23
Loss after 850396 batches: 0.1543
trigger times: 24
Loss after 851359 batches: 0.1544
trigger times: 25
Early stopping!
Start to test process.
Loss after 852322 batches: 0.1547
Time to train on one home:  54.91442036628723
trigger times: 0
Loss after 853285 batches: 0.2010
trigger times: 1
Loss after 854248 batches: 0.1725
trigger times: 2
Loss after 855211 batches: 0.1055
trigger times: 3
Loss after 856174 batches: 0.0879
trigger times: 4
Loss after 857137 batches: 0.0848
trigger times: 5
Loss after 858100 batches: 0.0805
trigger times: 6
Loss after 859063 batches: 0.0788
trigger times: 7
Loss after 860026 batches: 0.0786
trigger times: 8
Loss after 860989 batches: 0.0782
trigger times: 9
Loss after 861952 batches: 0.0785
trigger times: 10
Loss after 862915 batches: 0.0779
trigger times: 11
Loss after 863878 batches: 0.0779
trigger times: 12
Loss after 864841 batches: 0.0781
trigger times: 13
Loss after 865804 batches: 0.0780
trigger times: 14
Loss after 866767 batches: 0.0781
trigger times: 15
Loss after 867730 batches: 0.0778
trigger times: 16
Loss after 868693 batches: 0.0776
trigger times: 17
Loss after 869656 batches: 0.0778
trigger times: 18
Loss after 870619 batches: 0.0777
trigger times: 19
Loss after 871582 batches: 0.0776
trigger times: 20
Loss after 872545 batches: 0.0780
trigger times: 21
Loss after 873508 batches: 0.0778
trigger times: 22
Loss after 874471 batches: 0.0779
trigger times: 23
Loss after 875434 batches: 0.0778
trigger times: 24
Loss after 876397 batches: 0.0776
trigger times: 25
Early stopping!
Start to test process.
Loss after 877360 batches: 0.0778
Time to train on one home:  53.968674182891846
trigger times: 0
Loss after 878323 batches: 0.3976
trigger times: 1
Loss after 879286 batches: 0.3485
trigger times: 2
Loss after 880249 batches: 0.1849
trigger times: 3
Loss after 881212 batches: 0.0854
trigger times: 4
Loss after 882175 batches: 0.0755
trigger times: 5
Loss after 883138 batches: 0.0615
trigger times: 6
Loss after 884101 batches: 0.0600
trigger times: 7
Loss after 885064 batches: 0.0579
trigger times: 8
Loss after 886027 batches: 0.0568
trigger times: 9
Loss after 886990 batches: 0.0559
trigger times: 10
Loss after 887953 batches: 0.0555
trigger times: 11
Loss after 888916 batches: 0.0557
trigger times: 12
Loss after 889879 batches: 0.0552
trigger times: 13
Loss after 890842 batches: 0.0550
trigger times: 14
Loss after 891805 batches: 0.0550
trigger times: 15
Loss after 892768 batches: 0.0546
trigger times: 16
Loss after 893731 batches: 0.0549
trigger times: 17
Loss after 894694 batches: 0.0545
trigger times: 18
Loss after 895657 batches: 0.0545
trigger times: 19
Loss after 896620 batches: 0.0544
trigger times: 20
Loss after 897583 batches: 0.0543
trigger times: 21
Loss after 898546 batches: 0.0541
trigger times: 22
Loss after 899509 batches: 0.0542
trigger times: 23
Loss after 900472 batches: 0.0541
trigger times: 24
Loss after 901435 batches: 0.0541
trigger times: 25
Early stopping!
Start to test process.
Loss after 902398 batches: 0.0538
Time to train on one home:  57.25266194343567
trigger times: 0
Loss after 903293 batches: 0.1978
trigger times: 0
Loss after 904188 batches: 0.1792
trigger times: 1
Loss after 905083 batches: 0.1316
trigger times: 2
Loss after 905978 batches: 0.0954
trigger times: 3
Loss after 906873 batches: 0.0898
trigger times: 4
Loss after 907768 batches: 0.0882
trigger times: 5
Loss after 908663 batches: 0.0869
trigger times: 6
Loss after 909558 batches: 0.0858
trigger times: 7
Loss after 910453 batches: 0.0854
trigger times: 8
Loss after 911348 batches: 0.0847
trigger times: 9
Loss after 912243 batches: 0.0848
trigger times: 10
Loss after 913138 batches: 0.0845
trigger times: 11
Loss after 914033 batches: 0.0845
trigger times: 12
Loss after 914928 batches: 0.0845
trigger times: 13
Loss after 915823 batches: 0.0845
trigger times: 14
Loss after 916718 batches: 0.0843
trigger times: 15
Loss after 917613 batches: 0.0846
trigger times: 16
Loss after 918508 batches: 0.0843
trigger times: 17
Loss after 919403 batches: 0.0845
trigger times: 18
Loss after 920298 batches: 0.0843
trigger times: 19
Loss after 921193 batches: 0.0842
trigger times: 20
Loss after 922088 batches: 0.0840
trigger times: 21
Loss after 922983 batches: 0.0835
trigger times: 22
Loss after 923878 batches: 0.0727
trigger times: 0
Loss after 924773 batches: 0.0438
trigger times: 1
Loss after 925668 batches: 0.0338
trigger times: 2
Loss after 926563 batches: 0.0294
trigger times: 3
Loss after 927458 batches: 0.0237
trigger times: 4
Loss after 928353 batches: 0.0210
trigger times: 5
Loss after 929248 batches: 0.0201
trigger times: 6
Loss after 930143 batches: 0.0188
trigger times: 7
Loss after 931038 batches: 0.0182
trigger times: 8
Loss after 931933 batches: 0.0166
trigger times: 9
Loss after 932828 batches: 0.0147
trigger times: 10
Loss after 933723 batches: 0.0130
trigger times: 11
Loss after 934618 batches: 0.0112
trigger times: 12
Loss after 935513 batches: 0.0100
trigger times: 13
Loss after 936408 batches: 0.0089
trigger times: 14
Loss after 937303 batches: 0.0085
trigger times: 15
Loss after 938198 batches: 0.0086
trigger times: 16
Loss after 939093 batches: 0.0078
trigger times: 17
Loss after 939988 batches: 0.0076
trigger times: 18
Loss after 940883 batches: 0.0081
trigger times: 19
Loss after 941778 batches: 0.0071
trigger times: 20
Loss after 942673 batches: 0.0080
trigger times: 21
Loss after 943568 batches: 0.0081
trigger times: 22
Loss after 944463 batches: 0.0106
trigger times: 23
Loss after 945358 batches: 0.0082
trigger times: 24
Loss after 946253 batches: 0.0077
trigger times: 25
Early stopping!
Start to test process.
Loss after 947148 batches: 0.0070
Time to train on one home:  73.31781196594238
train_results:  [0.08167134079891343]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628]]
Round_0_results:  [0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 994 < 995; dropping {'Training_Loss': 0.09898427660976138, 'Validation_Loss': 0.13163089752197266, 'Training_R2': -0.20536737876006117, 'Validation_R2': -0.20346356541045196, 'Training_F1': 0.1545023566706077, 'Validation_F1': 0.06797668829704885, 'Training_NEP': 1.0509531619556596, 'Validation_NEP': 1.0814647735820855, 'Training_NDE': 0.8876981393833728, 'Validation_NDE': 0.9338852126408794, 'Training_MAE': 30.262374232942697, 'Validation_MAE': 36.0238445118535, 'Training_MSE': 2792.8584, 'Validation_MSE': 4625.9067}.
trigger times: 0
Loss after 948111 batches: 0.0990
trigger times: 0
Loss after 949074 batches: 0.0955
trigger times: 1
Loss after 950037 batches: 0.0963
trigger times: 2
Loss after 951000 batches: 0.0953
trigger times: 3
Loss after 951963 batches: 0.0947
trigger times: 4
Loss after 952926 batches: 0.0953
trigger times: 5
Loss after 953889 batches: 0.0956
trigger times: 6
Loss after 954852 batches: 0.0952
trigger times: 7
Loss after 955815 batches: 0.0950
trigger times: 8
Loss after 956778 batches: 0.0947
trigger times: 9
Loss after 957741 batches: 0.0952
trigger times: 0
Loss after 958704 batches: 0.0921
trigger times: 0
Loss after 959667 batches: 0.0829
trigger times: 1
Loss after 960630 batches: 0.0791
trigger times: 2
Loss after 961593 batches: 0.0777
trigger times: 3
Loss after 962556 batches: 0.0767
trigger times: 0
Loss after 963519 batches: 0.0748
trigger times: 0
Loss after 964482 batches: 0.0740
trigger times: 0
Loss after 965445 batches: 0.0726
trigger times: 0
Loss after 966408 batches: 0.0677
trigger times: 0
Loss after 967371 batches: 0.0628
trigger times: 0
Loss after 968334 batches: 0.0601
trigger times: 1
Loss after 969297 batches: 0.0586
trigger times: 0
Loss after 970260 batches: 0.0572
trigger times: 1
Loss after 971223 batches: 0.0540
trigger times: 0
Loss after 972186 batches: 0.0545
trigger times: 1
Loss after 973149 batches: 0.0532
trigger times: 2
Loss after 974112 batches: 0.0530
trigger times: 3
Loss after 975075 batches: 0.0524
trigger times: 4
Loss after 976038 batches: 0.0502
trigger times: 0
Loss after 977001 batches: 0.0500
trigger times: 1
Loss after 977964 batches: 0.0490
trigger times: 0
Loss after 978927 batches: 0.0486
trigger times: 1
Loss after 979890 batches: 0.0483
trigger times: 2
Loss after 980853 batches: 0.0486
trigger times: 3
Loss after 981816 batches: 0.0487
trigger times: 4
Loss after 982779 batches: 0.0478
trigger times: 5
Loss after 983742 batches: 0.0475
trigger times: 6
Loss after 984705 batches: 0.0471
trigger times: 7
Loss after 985668 batches: 0.0465
trigger times: 8
Loss after 986631 batches: 0.0468
trigger times: 9
Loss after 987594 batches: 0.0455
trigger times: 10
Loss after 988557 batches: 0.0461
trigger times: 11
Loss after 989520 batches: 0.0462
trigger times: 12
Loss after 990483 batches: 0.0439
trigger times: 13
Loss after 991446 batches: 0.0436
trigger times: 14
Loss after 992409 batches: 0.0443
trigger times: 15
Loss after 993372 batches: 0.0441
trigger times: 16
Loss after 994335 batches: 0.0445
trigger times: 17
Loss after 995298 batches: 0.0435
trigger times: 18
Loss after 996261 batches: 0.0427
trigger times: 19
Loss after 997224 batches: 0.0437
trigger times: 20
Loss after 998187 batches: 0.0423
trigger times: 21
Loss after 999150 batches: 0.0422
trigger times: 22
Loss after 1000113 batches: 0.0427
trigger times: 23
Loss after 1001076 batches: 0.0416
trigger times: 24
Loss after 1002039 batches: 0.0420
trigger times: 25
Early stopping!
Start to test process.
Loss after 1003002 batches: 0.0416
Time to train on one home:  83.02085065841675
trigger times: 0
Loss after 1003960 batches: 0.1464
trigger times: 0
Loss after 1004918 batches: 0.1397
trigger times: 0
Loss after 1005876 batches: 0.1414
trigger times: 1
Loss after 1006834 batches: 0.1412
trigger times: 2
Loss after 1007792 batches: 0.1393
trigger times: 3
Loss after 1008750 batches: 0.1395
trigger times: 4
Loss after 1009708 batches: 0.1389
trigger times: 5
Loss after 1010666 batches: 0.1384
trigger times: 6
Loss after 1011624 batches: 0.1389
trigger times: 7
Loss after 1012582 batches: 0.1391
trigger times: 8
Loss after 1013540 batches: 0.1392
trigger times: 9
Loss after 1014498 batches: 0.1390
trigger times: 10
Loss after 1015456 batches: 0.1388
trigger times: 11
Loss after 1016414 batches: 0.1390
trigger times: 12
Loss after 1017372 batches: 0.1394
trigger times: 13
Loss after 1018330 batches: 0.1389
trigger times: 14
Loss after 1019288 batches: 0.1390
trigger times: 15
Loss after 1020246 batches: 0.1391
trigger times: 16
Loss after 1021204 batches: 0.1385
trigger times: 17
Loss after 1022162 batches: 0.1391
trigger times: 18
Loss after 1023120 batches: 0.1391
trigger times: 19
Loss after 1024078 batches: 0.1388
trigger times: 20
Loss after 1025036 batches: 0.1381
trigger times: 21
Loss after 1025994 batches: 0.1389
trigger times: 22
Loss after 1026952 batches: 0.1385
trigger times: 23
Loss after 1027910 batches: 0.1382
trigger times: 0
Loss after 1028868 batches: 0.1206
trigger times: 1
Loss after 1029826 batches: 0.1054
trigger times: 2
Loss after 1030784 batches: 0.0830
trigger times: 3
Loss after 1031742 batches: 0.0751
trigger times: 4
Loss after 1032700 batches: 0.0750
trigger times: 5
Loss after 1033658 batches: 0.0715
trigger times: 6
Loss after 1034616 batches: 0.0700
trigger times: 7
Loss after 1035574 batches: 0.0682
trigger times: 8
Loss after 1036532 batches: 0.0670
trigger times: 9
Loss after 1037490 batches: 0.0673
trigger times: 0
Loss after 1038448 batches: 0.0632
trigger times: 1
Loss after 1039406 batches: 0.0626
trigger times: 0
Loss after 1040364 batches: 0.0608
trigger times: 0
Loss after 1041322 batches: 0.0533
trigger times: 1
Loss after 1042280 batches: 0.0532
trigger times: 2
Loss after 1043238 batches: 0.0514
trigger times: 3
Loss after 1044196 batches: 0.0475
trigger times: 4
Loss after 1045154 batches: 0.0459
trigger times: 5
Loss after 1046112 batches: 0.0462
trigger times: 6
Loss after 1047070 batches: 0.0443
trigger times: 0
Loss after 1048028 batches: 0.0425
trigger times: 1
Loss after 1048986 batches: 0.0438
trigger times: 2
Loss after 1049944 batches: 0.0418
trigger times: 3
Loss after 1050902 batches: 0.0398
trigger times: 4
Loss after 1051860 batches: 0.0379
trigger times: 5
Loss after 1052818 batches: 0.0386
trigger times: 6
Loss after 1053776 batches: 0.0366
trigger times: 7
Loss after 1054734 batches: 0.0378
trigger times: 8
Loss after 1055692 batches: 0.0423
trigger times: 9
Loss after 1056650 batches: 0.0390
trigger times: 10
Loss after 1057608 batches: 0.0386
trigger times: 11
Loss after 1058566 batches: 0.0379
trigger times: 12
Loss after 1059524 batches: 0.0347
trigger times: 13
Loss after 1060482 batches: 0.0337
trigger times: 14
Loss after 1061440 batches: 0.0340
trigger times: 15
Loss after 1062398 batches: 0.0335
trigger times: 16
Loss after 1063356 batches: 0.0328
trigger times: 17
Loss after 1064314 batches: 0.0315
trigger times: 18
Loss after 1065272 batches: 0.0328
trigger times: 19
Loss after 1066230 batches: 0.0325
trigger times: 20
Loss after 1067188 batches: 0.0315
trigger times: 21
Loss after 1068146 batches: 0.0311
trigger times: 22
Loss after 1069104 batches: 0.0327
trigger times: 0
Loss after 1070062 batches: 0.0327
trigger times: 1
Loss after 1071020 batches: 0.0320
trigger times: 2
Loss after 1071978 batches: 0.0321
trigger times: 3
Loss after 1072936 batches: 0.0313
trigger times: 4
Loss after 1073894 batches: 0.0311
trigger times: 5
Loss after 1074852 batches: 0.0346
trigger times: 6
Loss after 1075810 batches: 0.0345
trigger times: 7
Loss after 1076768 batches: 0.0312
trigger times: 8
Loss after 1077726 batches: 0.0304
trigger times: 0
Loss after 1078684 batches: 0.0310
trigger times: 1
Loss after 1079642 batches: 0.0360
trigger times: 2
Loss after 1080600 batches: 0.0343
trigger times: 3
Loss after 1081558 batches: 0.0344
trigger times: 4
Loss after 1082516 batches: 0.0315
trigger times: 5
Loss after 1083474 batches: 0.0332
trigger times: 6
Loss after 1084432 batches: 0.0306
trigger times: 7
Loss after 1085390 batches: 0.0316
trigger times: 8
Loss after 1086348 batches: 0.0307
trigger times: 9
Loss after 1087306 batches: 0.0299
trigger times: 10
Loss after 1088264 batches: 0.0303
trigger times: 0
Loss after 1089222 batches: 0.0292
trigger times: 1
Loss after 1090180 batches: 0.0295
trigger times: 2
Loss after 1091138 batches: 0.0287
trigger times: 3
Loss after 1092096 batches: 0.0294
trigger times: 4
Loss after 1093054 batches: 0.0272
trigger times: 5
Loss after 1094012 batches: 0.0277
trigger times: 6
Loss after 1094970 batches: 0.0279
trigger times: 7
Loss after 1095928 batches: 0.0278
trigger times: 8
Loss after 1096886 batches: 0.0267
trigger times: 0
Loss after 1097844 batches: 0.0253
trigger times: 1
Loss after 1098802 batches: 0.0274
trigger times: 2
Loss after 1099760 batches: 0.0256
trigger times: 0
Loss after 1100718 batches: 0.0271
trigger times: 1
Loss after 1101676 batches: 0.0261
trigger times: 2
Loss after 1102634 batches: 0.0265
trigger times: 3
Loss after 1103592 batches: 0.0279
trigger times: 4
Loss after 1104550 batches: 0.0268
trigger times: 5
Loss after 1105508 batches: 0.0263
trigger times: 6
Loss after 1106466 batches: 0.0273
trigger times: 7
Loss after 1107424 batches: 0.0261
trigger times: 8
Loss after 1108382 batches: 0.0259
trigger times: 9
Loss after 1109340 batches: 0.0260
trigger times: 10
Loss after 1110298 batches: 0.0252
trigger times: 11
Loss after 1111256 batches: 0.0238
trigger times: 12
Loss after 1112214 batches: 0.0240
trigger times: 13
Loss after 1113172 batches: 0.0249
trigger times: 14
Loss after 1114130 batches: 0.0244
trigger times: 15
Loss after 1115088 batches: 0.0245
trigger times: 16
Loss after 1116046 batches: 0.0251
trigger times: 17
Loss after 1117004 batches: 0.0245
trigger times: 0
Loss after 1117962 batches: 0.0227
trigger times: 1
Loss after 1118920 batches: 0.0241
trigger times: 2
Loss after 1119878 batches: 0.0235
trigger times: 0
Loss after 1120836 batches: 0.0251
trigger times: 1
Loss after 1121794 batches: 0.0238
trigger times: 2
Loss after 1122752 batches: 0.0236
trigger times: 3
Loss after 1123710 batches: 0.0236
trigger times: 4
Loss after 1124668 batches: 0.0228
trigger times: 5
Loss after 1125626 batches: 0.0228
trigger times: 6
Loss after 1126584 batches: 0.0231
trigger times: 7
Loss after 1127542 batches: 0.0234
trigger times: 8
Loss after 1128500 batches: 0.0232
trigger times: 9
Loss after 1129458 batches: 0.0228
trigger times: 10
Loss after 1130416 batches: 0.0224
trigger times: 11
Loss after 1131374 batches: 0.0228
trigger times: 12
Loss after 1132332 batches: 0.0223
trigger times: 13
Loss after 1133290 batches: 0.0229
trigger times: 0
Loss after 1134248 batches: 0.0234
trigger times: 1
Loss after 1135206 batches: 0.0221
trigger times: 2
Loss after 1136164 batches: 0.0223
trigger times: 3
Loss after 1137122 batches: 0.0232
trigger times: 4
Loss after 1138080 batches: 0.0227
trigger times: 5
Loss after 1139038 batches: 0.0220
trigger times: 6
Loss after 1139996 batches: 0.0228
trigger times: 7
Loss after 1140954 batches: 0.0221
trigger times: 8
Loss after 1141912 batches: 0.0223
trigger times: 9
Loss after 1142870 batches: 0.0241
trigger times: 10
Loss after 1143828 batches: 0.0224
trigger times: 11
Loss after 1144786 batches: 0.0215
trigger times: 12
Loss after 1145744 batches: 0.0218
trigger times: 13
Loss after 1146702 batches: 0.0224
trigger times: 14
Loss after 1147660 batches: 0.0236
trigger times: 15
Loss after 1148618 batches: 0.0241
trigger times: 16
Loss after 1149576 batches: 0.0230
trigger times: 17
Loss after 1150534 batches: 0.0209
trigger times: 18
Loss after 1151492 batches: 0.0198
trigger times: 19
Loss after 1152450 batches: 0.0210
trigger times: 20
Loss after 1153408 batches: 0.0216
trigger times: 21
Loss after 1154366 batches: 0.0216
trigger times: 22
Loss after 1155324 batches: 0.0217
trigger times: 23
Loss after 1156282 batches: 0.0222
trigger times: 24
Loss after 1157240 batches: 0.0224
trigger times: 25
Early stopping!
Start to test process.
Loss after 1158198 batches: 0.0212
Time to train on one home:  168.4559326171875
trigger times: 0
Loss after 1159161 batches: 0.0963
trigger times: 1
Loss after 1160124 batches: 0.0764
trigger times: 2
Loss after 1161087 batches: 0.0786
trigger times: 3
Loss after 1162050 batches: 0.0742
trigger times: 4
Loss after 1163013 batches: 0.0722
trigger times: 5
Loss after 1163976 batches: 0.0716
trigger times: 6
Loss after 1164939 batches: 0.0715
trigger times: 7
Loss after 1165902 batches: 0.0709
trigger times: 8
Loss after 1166865 batches: 0.0710
trigger times: 9
Loss after 1167828 batches: 0.0709
trigger times: 10
Loss after 1168791 batches: 0.0705
trigger times: 11
Loss after 1169754 batches: 0.0707
trigger times: 12
Loss after 1170717 batches: 0.0706
trigger times: 13
Loss after 1171680 batches: 0.0707
trigger times: 14
Loss after 1172643 batches: 0.0702
trigger times: 15
Loss after 1173606 batches: 0.0704
trigger times: 16
Loss after 1174569 batches: 0.0704
trigger times: 17
Loss after 1175532 batches: 0.0702
trigger times: 18
Loss after 1176495 batches: 0.0703
trigger times: 19
Loss after 1177458 batches: 0.0703
trigger times: 20
Loss after 1178421 batches: 0.0700
trigger times: 21
Loss after 1179384 batches: 0.0701
trigger times: 22
Loss after 1180347 batches: 0.0700
trigger times: 23
Loss after 1181310 batches: 0.0701
trigger times: 24
Loss after 1182273 batches: 0.0700
trigger times: 25
Early stopping!
Start to test process.
Loss after 1183236 batches: 0.0701
Time to train on one home:  53.959638595581055
trigger times: 0
Loss after 1184199 batches: 0.1013
trigger times: 1
Loss after 1185162 batches: 0.0895
trigger times: 2
Loss after 1186125 batches: 0.0906
trigger times: 3
Loss after 1187088 batches: 0.0880
trigger times: 4
Loss after 1188051 batches: 0.0867
trigger times: 5
Loss after 1189014 batches: 0.0867
trigger times: 6
Loss after 1189977 batches: 0.0863
trigger times: 7
Loss after 1190940 batches: 0.0864
trigger times: 8
Loss after 1191903 batches: 0.0862
trigger times: 9
Loss after 1192866 batches: 0.0861
trigger times: 10
Loss after 1193829 batches: 0.0858
trigger times: 11
Loss after 1194792 batches: 0.0859
trigger times: 12
Loss after 1195755 batches: 0.0858
trigger times: 13
Loss after 1196718 batches: 0.0858
trigger times: 14
Loss after 1197681 batches: 0.0858
trigger times: 15
Loss after 1198644 batches: 0.0857
trigger times: 16
Loss after 1199607 batches: 0.0845
trigger times: 17
Loss after 1200570 batches: 0.0824
trigger times: 18
Loss after 1201533 batches: 0.0815
trigger times: 19
Loss after 1202496 batches: 0.0814
trigger times: 20
Loss after 1203459 batches: 0.0809
trigger times: 21
Loss after 1204422 batches: 0.0806
trigger times: 22
Loss after 1205385 batches: 0.0810
trigger times: 23
Loss after 1206348 batches: 0.0805
trigger times: 24
Loss after 1207311 batches: 0.0801
trigger times: 25
Early stopping!
Start to test process.
Loss after 1208274 batches: 0.0803
Time to train on one home:  56.961745262145996
trigger times: 0
Loss after 1209237 batches: 0.0542
trigger times: 1
Loss after 1210200 batches: 0.0539
trigger times: 2
Loss after 1211163 batches: 0.0539
trigger times: 3
Loss after 1212126 batches: 0.0539
trigger times: 4
Loss after 1213089 batches: 0.0538
trigger times: 5
Loss after 1214052 batches: 0.0537
trigger times: 6
Loss after 1215015 batches: 0.0537
trigger times: 7
Loss after 1215978 batches: 0.0537
trigger times: 8
Loss after 1216941 batches: 0.0536
trigger times: 9
Loss after 1217904 batches: 0.0532
trigger times: 0
Loss after 1218867 batches: 0.0520
trigger times: 1
Loss after 1219830 batches: 0.0494
trigger times: 2
Loss after 1220793 batches: 0.0485
trigger times: 3
Loss after 1221756 batches: 0.0474
trigger times: 4
Loss after 1222719 batches: 0.0473
trigger times: 0
Loss after 1223682 batches: 0.0462
trigger times: 1
Loss after 1224645 batches: 0.0457
trigger times: 0
Loss after 1225608 batches: 0.0454
trigger times: 1
Loss after 1226571 batches: 0.0456
trigger times: 2
Loss after 1227534 batches: 0.0447
trigger times: 3
Loss after 1228497 batches: 0.0431
trigger times: 0
Loss after 1229460 batches: 0.0405
trigger times: 0
Loss after 1230423 batches: 0.0384
trigger times: 0
Loss after 1231386 batches: 0.0369
trigger times: 0
Loss after 1232349 batches: 0.0354
trigger times: 1
Loss after 1233312 batches: 0.0346
trigger times: 2
Loss after 1234275 batches: 0.0335
trigger times: 3
Loss after 1235238 batches: 0.0329
trigger times: 4
Loss after 1236201 batches: 0.0326
trigger times: 5
Loss after 1237164 batches: 0.0316
trigger times: 6
Loss after 1238127 batches: 0.0309
trigger times: 7
Loss after 1239090 batches: 0.0309
trigger times: 8
Loss after 1240053 batches: 0.0304
trigger times: 9
Loss after 1241016 batches: 0.0295
trigger times: 10
Loss after 1241979 batches: 0.0286
trigger times: 11
Loss after 1242942 batches: 0.0283
trigger times: 12
Loss after 1243905 batches: 0.0280
trigger times: 13
Loss after 1244868 batches: 0.0278
trigger times: 14
Loss after 1245831 batches: 0.0275
trigger times: 15
Loss after 1246794 batches: 0.0276
trigger times: 16
Loss after 1247757 batches: 0.0266
trigger times: 17
Loss after 1248720 batches: 0.0260
trigger times: 18
Loss after 1249683 batches: 0.0250
trigger times: 19
Loss after 1250646 batches: 0.0253
trigger times: 20
Loss after 1251609 batches: 0.0248
trigger times: 21
Loss after 1252572 batches: 0.0246
trigger times: 22
Loss after 1253535 batches: 0.0245
trigger times: 23
Loss after 1254498 batches: 0.0243
trigger times: 24
Loss after 1255461 batches: 0.0244
trigger times: 25
Early stopping!
Start to test process.
Loss after 1256424 batches: 0.0242
Time to train on one home:  77.3525185585022
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 1257387 batches: 0.1145
trigger times: 1
Loss after 1258350 batches: 0.0548
trigger times: 2
Loss after 1259313 batches: 0.0483
trigger times: 3
Loss after 1260276 batches: 0.0348
trigger times: 4
Loss after 1261239 batches: 0.0307
trigger times: 5
Loss after 1262202 batches: 0.0286
trigger times: 6
Loss after 1263165 batches: 0.0271
trigger times: 7
Loss after 1264128 batches: 0.0260
trigger times: 8
Loss after 1265091 batches: 0.0253
trigger times: 9
Loss after 1266054 batches: 0.0250
trigger times: 10
Loss after 1267017 batches: 0.0246
trigger times: 11
Loss after 1267980 batches: 0.0244
trigger times: 12
Loss after 1268943 batches: 0.0242
trigger times: 13
Loss after 1269906 batches: 0.0240
trigger times: 14
Loss after 1270869 batches: 0.0240
trigger times: 15
Loss after 1271832 batches: 0.0239
trigger times: 16
Loss after 1272795 batches: 0.0241
trigger times: 17
Loss after 1273758 batches: 0.0236
trigger times: 18
Loss after 1274721 batches: 0.0238
trigger times: 19
Loss after 1275684 batches: 0.0237
trigger times: 20
Loss after 1276647 batches: 0.0236
trigger times: 21
Loss after 1277610 batches: 0.0234
trigger times: 22
Loss after 1278573 batches: 0.0236
trigger times: 23
Loss after 1279536 batches: 0.0236
trigger times: 24
Loss after 1280499 batches: 0.0234
trigger times: 25
Early stopping!
Start to test process.
Loss after 1281462 batches: 0.0233
Time to train on one home:  58.03342270851135
trigger times: 0
Loss after 1282425 batches: 0.1310
trigger times: 0
Loss after 1283388 batches: 0.1308
trigger times: 1
Loss after 1284351 batches: 0.1308
trigger times: 2
Loss after 1285314 batches: 0.1310
trigger times: 3
Loss after 1286277 batches: 0.1308
trigger times: 0
Loss after 1287240 batches: 0.1307
trigger times: 1
Loss after 1288203 batches: 0.1309
trigger times: 0
Loss after 1289166 batches: 0.1261
trigger times: 1
Loss after 1290129 batches: 0.1251
trigger times: 2
Loss after 1291092 batches: 0.1233
trigger times: 0
Loss after 1292055 batches: 0.1216
trigger times: 0
Loss after 1293018 batches: 0.1173
trigger times: 1
Loss after 1293981 batches: 0.1124
trigger times: 0
Loss after 1294944 batches: 0.1102
trigger times: 1
Loss after 1295907 batches: 0.1069
trigger times: 2
Loss after 1296870 batches: 0.1063
trigger times: 3
Loss after 1297833 batches: 0.1046
trigger times: 0
Loss after 1298796 batches: 0.1025
trigger times: 1
Loss after 1299759 batches: 0.1024
trigger times: 2
Loss after 1300722 batches: 0.1022
trigger times: 3
Loss after 1301685 batches: 0.1019
trigger times: 4
Loss after 1302648 batches: 0.1018
trigger times: 5
Loss after 1303611 batches: 0.0993
trigger times: 0
Loss after 1304574 batches: 0.0980
trigger times: 1
Loss after 1305537 batches: 0.0977
trigger times: 2
Loss after 1306500 batches: 0.0964
trigger times: 3
Loss after 1307463 batches: 0.0965
trigger times: 0
Loss after 1308426 batches: 0.0947
trigger times: 0
Loss after 1309389 batches: 0.0935
trigger times: 0
Loss after 1310352 batches: 0.0928
trigger times: 1
Loss after 1311315 batches: 0.0938
trigger times: 2
Loss after 1312278 batches: 0.0921
trigger times: 3
Loss after 1313241 batches: 0.0907
trigger times: 4
Loss after 1314204 batches: 0.0904
trigger times: 5
Loss after 1315167 batches: 0.0907
trigger times: 6
Loss after 1316130 batches: 0.0899
trigger times: 7
Loss after 1317093 batches: 0.0875
trigger times: 8
Loss after 1318056 batches: 0.0879
trigger times: 9
Loss after 1319019 batches: 0.0878
trigger times: 10
Loss after 1319982 batches: 0.0879
trigger times: 11
Loss after 1320945 batches: 0.0869
trigger times: 12
Loss after 1321908 batches: 0.0868
trigger times: 13
Loss after 1322871 batches: 0.0857
trigger times: 14
Loss after 1323834 batches: 0.0864
trigger times: 15
Loss after 1324797 batches: 0.0847
trigger times: 16
Loss after 1325760 batches: 0.0835
trigger times: 17
Loss after 1326723 batches: 0.0822
trigger times: 18
Loss after 1327686 batches: 0.0842
trigger times: 19
Loss after 1328649 batches: 0.0836
trigger times: 20
Loss after 1329612 batches: 0.0819
trigger times: 21
Loss after 1330575 batches: 0.0824
trigger times: 22
Loss after 1331538 batches: 0.0814
trigger times: 23
Loss after 1332501 batches: 0.0818
trigger times: 24
Loss after 1333464 batches: 0.0827
trigger times: 25
Early stopping!
Start to test process.
Loss after 1334427 batches: 0.0796
Time to train on one home:  80.00977444648743
trigger times: 0
Loss after 1335390 batches: 0.1230
trigger times: 1
Loss after 1336353 batches: 0.1064
trigger times: 2
Loss after 1337316 batches: 0.1093
trigger times: 3
Loss after 1338279 batches: 0.1057
trigger times: 4
Loss after 1339242 batches: 0.1048
trigger times: 5
Loss after 1340205 batches: 0.1032
trigger times: 6
Loss after 1341168 batches: 0.1028
trigger times: 7
Loss after 1342131 batches: 0.1025
trigger times: 8
Loss after 1343094 batches: 0.1024
trigger times: 9
Loss after 1344057 batches: 0.1016
trigger times: 0
Loss after 1345020 batches: 0.0957
trigger times: 1
Loss after 1345983 batches: 0.0846
trigger times: 2
Loss after 1346946 batches: 0.0740
trigger times: 3
Loss after 1347909 batches: 0.0716
trigger times: 4
Loss after 1348872 batches: 0.0692
trigger times: 5
Loss after 1349835 batches: 0.0691
trigger times: 6
Loss after 1350798 batches: 0.0672
trigger times: 7
Loss after 1351761 batches: 0.0657
trigger times: 8
Loss after 1352724 batches: 0.0633
trigger times: 9
Loss after 1353687 batches: 0.0627
trigger times: 10
Loss after 1354650 batches: 0.0586
trigger times: 11
Loss after 1355613 batches: 0.0583
trigger times: 0
Loss after 1356576 batches: 0.0590
trigger times: 0
Loss after 1357539 batches: 0.0580
trigger times: 1
Loss after 1358502 batches: 0.0565
trigger times: 2
Loss after 1359465 batches: 0.0530
trigger times: 3
Loss after 1360428 batches: 0.0531
trigger times: 4
Loss after 1361391 batches: 0.0509
trigger times: 5
Loss after 1362354 batches: 0.0500
trigger times: 6
Loss after 1363317 batches: 0.0483
trigger times: 7
Loss after 1364280 batches: 0.0473
trigger times: 0
Loss after 1365243 batches: 0.0484
trigger times: 1
Loss after 1366206 batches: 0.0485
trigger times: 2
Loss after 1367169 batches: 0.0460
trigger times: 3
Loss after 1368132 batches: 0.0452
trigger times: 4
Loss after 1369095 batches: 0.0450
trigger times: 5
Loss after 1370058 batches: 0.0433
trigger times: 0
Loss after 1371021 batches: 0.0429
trigger times: 1
Loss after 1371984 batches: 0.0422
trigger times: 2
Loss after 1372947 batches: 0.0428
trigger times: 3
Loss after 1373910 batches: 0.0428
trigger times: 4
Loss after 1374873 batches: 0.0415
trigger times: 5
Loss after 1375836 batches: 0.0404
trigger times: 0
Loss after 1376799 batches: 0.0398
trigger times: 1
Loss after 1377762 batches: 0.0400
trigger times: 2
Loss after 1378725 batches: 0.0395
trigger times: 3
Loss after 1379688 batches: 0.0385
trigger times: 4
Loss after 1380651 batches: 0.0387
trigger times: 5
Loss after 1381614 batches: 0.0376
trigger times: 6
Loss after 1382577 batches: 0.0374
trigger times: 0
Loss after 1383540 batches: 0.0379
trigger times: 1
Loss after 1384503 batches: 0.0363
trigger times: 2
Loss after 1385466 batches: 0.0368
trigger times: 3
Loss after 1386429 batches: 0.0361
trigger times: 4
Loss after 1387392 batches: 0.0359
trigger times: 5
Loss after 1388355 batches: 0.0354
trigger times: 6
Loss after 1389318 batches: 0.0355
trigger times: 7
Loss after 1390281 batches: 0.0349
trigger times: 8
Loss after 1391244 batches: 0.0345
trigger times: 9
Loss after 1392207 batches: 0.0355
trigger times: 10
Loss after 1393170 batches: 0.0345
trigger times: 11
Loss after 1394133 batches: 0.0343
trigger times: 12
Loss after 1395096 batches: 0.0335
trigger times: 13
Loss after 1396059 batches: 0.0337
trigger times: 14
Loss after 1397022 batches: 0.0337
trigger times: 15
Loss after 1397985 batches: 0.0324
trigger times: 16
Loss after 1398948 batches: 0.0344
trigger times: 17
Loss after 1399911 batches: 0.0351
trigger times: 18
Loss after 1400874 batches: 0.0365
trigger times: 19
Loss after 1401837 batches: 0.0357
trigger times: 20
Loss after 1402800 batches: 0.0351
trigger times: 21
Loss after 1403763 batches: 0.0337
trigger times: 22
Loss after 1404726 batches: 0.0328
trigger times: 23
Loss after 1405689 batches: 0.0339
trigger times: 24
Loss after 1406652 batches: 0.0326
trigger times: 25
Early stopping!
Start to test process.
Loss after 1407615 batches: 0.0323
Time to train on one home:  97.63047885894775
trigger times: 0
Loss after 1408578 batches: 0.1275
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 1409541 batches: 0.1149
trigger times: 2
Loss after 1410504 batches: 0.1163
trigger times: 3
Loss after 1411467 batches: 0.1142
trigger times: 4
Loss after 1412430 batches: 0.1124
trigger times: 5
Loss after 1413393 batches: 0.1125
trigger times: 6
Loss after 1414356 batches: 0.1122
trigger times: 7
Loss after 1415319 batches: 0.1120
trigger times: 8
Loss after 1416282 batches: 0.1115
trigger times: 9
Loss after 1417245 batches: 0.1113
trigger times: 10
Loss after 1418208 batches: 0.1114
trigger times: 11
Loss after 1419171 batches: 0.1112
trigger times: 12
Loss after 1420134 batches: 0.1111
trigger times: 13
Loss after 1421097 batches: 0.1101
trigger times: 14
Loss after 1422060 batches: 0.1077
trigger times: 15
Loss after 1423023 batches: 0.1060
trigger times: 16
Loss after 1423986 batches: 0.1046
trigger times: 0
Loss after 1424949 batches: 0.1029
trigger times: 1
Loss after 1425912 batches: 0.1015
trigger times: 2
Loss after 1426875 batches: 0.1002
trigger times: 3
Loss after 1427838 batches: 0.0981
trigger times: 4
Loss after 1428801 batches: 0.0958
trigger times: 5
Loss after 1429764 batches: 0.0944
trigger times: 6
Loss after 1430727 batches: 0.0924
trigger times: 7
Loss after 1431690 batches: 0.0903
trigger times: 8
Loss after 1432653 batches: 0.0897
trigger times: 9
Loss after 1433616 batches: 0.0890
trigger times: 10
Loss after 1434579 batches: 0.0868
trigger times: 11
Loss after 1435542 batches: 0.0846
trigger times: 12
Loss after 1436505 batches: 0.0862
trigger times: 13
Loss after 1437468 batches: 0.0848
trigger times: 14
Loss after 1438431 batches: 0.0824
trigger times: 15
Loss after 1439394 batches: 0.0832
trigger times: 16
Loss after 1440357 batches: 0.0826
trigger times: 17
Loss after 1441320 batches: 0.0807
trigger times: 18
Loss after 1442283 batches: 0.0806
trigger times: 19
Loss after 1443246 batches: 0.0805
trigger times: 20
Loss after 1444209 batches: 0.0798
trigger times: 21
Loss after 1445172 batches: 0.0794
trigger times: 22
Loss after 1446135 batches: 0.0787
trigger times: 23
Loss after 1447098 batches: 0.0794
trigger times: 24
Loss after 1448061 batches: 0.0783
trigger times: 25
Early stopping!
Start to test process.
Loss after 1449024 batches: 0.0782
Time to train on one home:  66.81674218177795
trigger times: 0
Loss after 1449987 batches: 0.1257
trigger times: 0
Loss after 1450950 batches: 0.1201
trigger times: 1
Loss after 1451913 batches: 0.1225
trigger times: 2
Loss after 1452876 batches: 0.1201
trigger times: 3
Loss after 1453839 batches: 0.1196
trigger times: 4
Loss after 1454802 batches: 0.1201
trigger times: 5
Loss after 1455765 batches: 0.1192
trigger times: 6
Loss after 1456728 batches: 0.1190
trigger times: 7
Loss after 1457691 batches: 0.1194
trigger times: 8
Loss after 1458654 batches: 0.1194
trigger times: 9
Loss after 1459617 batches: 0.1192
trigger times: 10
Loss after 1460580 batches: 0.1191
trigger times: 11
Loss after 1461543 batches: 0.1191
trigger times: 12
Loss after 1462506 batches: 0.1196
trigger times: 13
Loss after 1463469 batches: 0.1193
trigger times: 14
Loss after 1464432 batches: 0.1190
trigger times: 15
Loss after 1465395 batches: 0.1193
trigger times: 16
Loss after 1466358 batches: 0.1191
trigger times: 17
Loss after 1467321 batches: 0.1187
trigger times: 18
Loss after 1468284 batches: 0.1193
trigger times: 19
Loss after 1469247 batches: 0.1170
trigger times: 0
Loss after 1470210 batches: 0.1115
trigger times: 1
Loss after 1471173 batches: 0.1058
trigger times: 2
Loss after 1472136 batches: 0.0981
trigger times: 0
Loss after 1473099 batches: 0.0888
trigger times: 0
Loss after 1474062 batches: 0.0815
trigger times: 1
Loss after 1475025 batches: 0.0791
trigger times: 0
Loss after 1475988 batches: 0.0753
trigger times: 1
Loss after 1476951 batches: 0.0727
trigger times: 2
Loss after 1477914 batches: 0.0718
trigger times: 0
Loss after 1478877 batches: 0.0698
trigger times: 1
Loss after 1479840 batches: 0.0683
trigger times: 2
Loss after 1480803 batches: 0.0677
trigger times: 3
Loss after 1481766 batches: 0.0654
trigger times: 4
Loss after 1482729 batches: 0.0649
trigger times: 0
Loss after 1483692 batches: 0.0632
trigger times: 1
Loss after 1484655 batches: 0.0623
trigger times: 2
Loss after 1485618 batches: 0.0624
trigger times: 0
Loss after 1486581 batches: 0.0606
trigger times: 0
Loss after 1487544 batches: 0.0597
trigger times: 0
Loss after 1488507 batches: 0.0581
trigger times: 1
Loss after 1489470 batches: 0.0577
trigger times: 2
Loss after 1490433 batches: 0.0577
trigger times: 3
Loss after 1491396 batches: 0.0566
trigger times: 4
Loss after 1492359 batches: 0.0561
trigger times: 5
Loss after 1493322 batches: 0.0548
trigger times: 6
Loss after 1494285 batches: 0.0547
trigger times: 7
Loss after 1495248 batches: 0.0544
trigger times: 8
Loss after 1496211 batches: 0.0532
trigger times: 9
Loss after 1497174 batches: 0.0539
trigger times: 10
Loss after 1498137 batches: 0.0532
trigger times: 11
Loss after 1499100 batches: 0.0526
trigger times: 12
Loss after 1500063 batches: 0.0513
trigger times: 0
Loss after 1501026 batches: 0.0510
trigger times: 1
Loss after 1501989 batches: 0.0523
trigger times: 2
Loss after 1502952 batches: 0.0520
trigger times: 3
Loss after 1503915 batches: 0.0525
trigger times: 4
Loss after 1504878 batches: 0.0513
trigger times: 5
Loss after 1505841 batches: 0.0521
trigger times: 6
Loss after 1506804 batches: 0.0506
trigger times: 7
Loss after 1507767 batches: 0.0502
trigger times: 8
Loss after 1508730 batches: 0.0495
trigger times: 9
Loss after 1509693 batches: 0.0502
trigger times: 10
Loss after 1510656 batches: 0.0492
trigger times: 11
Loss after 1511619 batches: 0.0484
trigger times: 12
Loss after 1512582 batches: 0.0477
trigger times: 13
Loss after 1513545 batches: 0.0483
trigger times: 14
Loss after 1514508 batches: 0.0487
trigger times: 15
Loss after 1515471 batches: 0.0476
trigger times: 16
Loss after 1516434 batches: 0.0475
trigger times: 17
Loss after 1517397 batches: 0.0468
trigger times: 18
Loss after 1518360 batches: 0.0469
trigger times: 19
Loss after 1519323 batches: 0.0462
trigger times: 20
Loss after 1520286 batches: 0.0473
trigger times: 21
Loss after 1521249 batches: 0.0459
trigger times: 22
Loss after 1522212 batches: 0.0467
trigger times: 23
Loss after 1523175 batches: 0.0444
trigger times: 24
Loss after 1524138 batches: 0.0453
trigger times: 25
Early stopping!
Start to test process.
Loss after 1525101 batches: 0.0451
Time to train on one home:  99.12500238418579
trigger times: 0
Loss after 1526064 batches: 0.1019
trigger times: 0
Loss after 1527027 batches: 0.1020
trigger times: 1
Loss after 1527990 batches: 0.1024
trigger times: 0
Loss after 1528953 batches: 0.1020
trigger times: 1
Loss after 1529916 batches: 0.1023
trigger times: 2
Loss after 1530879 batches: 0.1020
trigger times: 3
Loss after 1531842 batches: 0.1018
trigger times: 4
Loss after 1532805 batches: 0.1024
trigger times: 0
Loss after 1533768 batches: 0.1019
trigger times: 0
Loss after 1534731 batches: 0.1011
trigger times: 0
Loss after 1535694 batches: 0.0997
trigger times: 1
Loss after 1536657 batches: 0.0977
trigger times: 2
Loss after 1537620 batches: 0.0983
trigger times: 3
Loss after 1538583 batches: 0.0961
trigger times: 4
Loss after 1539546 batches: 0.0962
trigger times: 5
Loss after 1540509 batches: 0.0956
trigger times: 6
Loss after 1541472 batches: 0.0936
trigger times: 7
Loss after 1542435 batches: 0.0919
trigger times: 0
Loss after 1543398 batches: 0.0891
trigger times: 1
Loss after 1544361 batches: 0.0869
trigger times: 2
Loss after 1545324 batches: 0.0854
trigger times: 0
Loss after 1546287 batches: 0.0839
trigger times: 1
Loss after 1547250 batches: 0.0826
trigger times: 2
Loss after 1548213 batches: 0.0812
trigger times: 3
Loss after 1549176 batches: 0.0794
trigger times: 0
Loss after 1550139 batches: 0.0782
trigger times: 0
Loss after 1551102 batches: 0.0773
trigger times: 1
Loss after 1552065 batches: 0.0787
trigger times: 2
Loss after 1553028 batches: 0.0769
trigger times: 3
Loss after 1553991 batches: 0.0754
trigger times: 4
Loss after 1554954 batches: 0.0751
trigger times: 0
Loss after 1555917 batches: 0.0751
trigger times: 1
Loss after 1556880 batches: 0.0745
trigger times: 2
Loss after 1557843 batches: 0.0732
trigger times: 3
Loss after 1558806 batches: 0.0730
trigger times: 4
Loss after 1559769 batches: 0.0737
trigger times: 5
Loss after 1560732 batches: 0.0723
trigger times: 6
Loss after 1561695 batches: 0.0732
trigger times: 7
Loss after 1562658 batches: 0.0710
trigger times: 8
Loss after 1563621 batches: 0.0709
trigger times: 9
Loss after 1564584 batches: 0.0704
trigger times: 10
Loss after 1565547 batches: 0.0700
trigger times: 11
Loss after 1566510 batches: 0.0691
trigger times: 12
Loss after 1567473 batches: 0.0691
trigger times: 13
Loss after 1568436 batches: 0.0695
trigger times: 14
Loss after 1569399 batches: 0.0684
trigger times: 15
Loss after 1570362 batches: 0.0675
trigger times: 16
Loss after 1571325 batches: 0.0679
trigger times: 17
Loss after 1572288 batches: 0.0684
trigger times: 18
Loss after 1573251 batches: 0.0680
trigger times: 19
Loss after 1574214 batches: 0.0675
trigger times: 20
Loss after 1575177 batches: 0.0667
trigger times: 21
Loss after 1576140 batches: 0.0690
trigger times: 22
Loss after 1577103 batches: 0.0679
trigger times: 23
Loss after 1578066 batches: 0.0654
trigger times: 24
Loss after 1579029 batches: 0.0661
trigger times: 25
Early stopping!
Start to test process.
Loss after 1579992 batches: 0.0660
Time to train on one home:  86.70975947380066
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 1580955 batches: 0.0693
trigger times: 1
Loss after 1581918 batches: 0.0695
trigger times: 2
Loss after 1582881 batches: 0.0696
trigger times: 3
Loss after 1583844 batches: 0.0694
trigger times: 4
Loss after 1584807 batches: 0.0695
trigger times: 5
Loss after 1585770 batches: 0.0694
trigger times: 6
Loss after 1586733 batches: 0.0691
trigger times: 7
Loss after 1587696 batches: 0.0694
trigger times: 8
Loss after 1588659 batches: 0.0693
trigger times: 9
Loss after 1589622 batches: 0.0692
trigger times: 0
Loss after 1590585 batches: 0.0692
trigger times: 1
Loss after 1591548 batches: 0.0692
trigger times: 2
Loss after 1592511 batches: 0.0693
trigger times: 3
Loss after 1593474 batches: 0.0692
trigger times: 0
Loss after 1594437 batches: 0.0692
trigger times: 0
Loss after 1595400 batches: 0.0682
trigger times: 1
Loss after 1596363 batches: 0.0665
trigger times: 0
Loss after 1597326 batches: 0.0654
trigger times: 1
Loss after 1598289 batches: 0.0642
trigger times: 2
Loss after 1599252 batches: 0.0631
trigger times: 3
Loss after 1600215 batches: 0.0616
trigger times: 4
Loss after 1601178 batches: 0.0595
trigger times: 5
Loss after 1602141 batches: 0.0572
trigger times: 6
Loss after 1603104 batches: 0.0552
trigger times: 7
Loss after 1604067 batches: 0.0533
trigger times: 8
Loss after 1605030 batches: 0.0518
trigger times: 9
Loss after 1605993 batches: 0.0510
trigger times: 10
Loss after 1606956 batches: 0.0501
trigger times: 11
Loss after 1607919 batches: 0.0491
trigger times: 0
Loss after 1608882 batches: 0.0486
trigger times: 1
Loss after 1609845 batches: 0.0477
trigger times: 2
Loss after 1610808 batches: 0.0468
trigger times: 0
Loss after 1611771 batches: 0.0456
trigger times: 1
Loss after 1612734 batches: 0.0454
trigger times: 0
Loss after 1613697 batches: 0.0449
trigger times: 0
Loss after 1614660 batches: 0.0448
trigger times: 1
Loss after 1615623 batches: 0.0440
trigger times: 2
Loss after 1616586 batches: 0.0431
trigger times: 3
Loss after 1617549 batches: 0.0426
trigger times: 4
Loss after 1618512 batches: 0.0423
trigger times: 5
Loss after 1619475 batches: 0.0411
trigger times: 6
Loss after 1620438 batches: 0.0406
trigger times: 7
Loss after 1621401 batches: 0.0411
trigger times: 8
Loss after 1622364 batches: 0.0398
trigger times: 9
Loss after 1623327 batches: 0.0393
trigger times: 10
Loss after 1624290 batches: 0.0393
trigger times: 11
Loss after 1625253 batches: 0.0387
trigger times: 12
Loss after 1626216 batches: 0.0383
trigger times: 13
Loss after 1627179 batches: 0.0384
trigger times: 14
Loss after 1628142 batches: 0.0388
trigger times: 15
Loss after 1629105 batches: 0.0376
trigger times: 16
Loss after 1630068 batches: 0.0382
trigger times: 17
Loss after 1631031 batches: 0.0372
trigger times: 18
Loss after 1631994 batches: 0.0369
trigger times: 19
Loss after 1632957 batches: 0.0374
trigger times: 20
Loss after 1633920 batches: 0.0361
trigger times: 21
Loss after 1634883 batches: 0.0350
trigger times: 22
Loss after 1635846 batches: 0.0359
trigger times: 23
Loss after 1636809 batches: 0.0365
trigger times: 24
Loss after 1637772 batches: 0.0364
trigger times: 25
Early stopping!
Start to test process.
Loss after 1638735 batches: 0.0362
Time to train on one home:  84.99627947807312
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 1639698 batches: 0.1424
trigger times: 0
Loss after 1640661 batches: 0.1384
trigger times: 1
Loss after 1641624 batches: 0.1402
trigger times: 2
Loss after 1642587 batches: 0.1382
trigger times: 3
Loss after 1643550 batches: 0.1384
trigger times: 4
Loss after 1644513 batches: 0.1381
trigger times: 5
Loss after 1645476 batches: 0.1378
trigger times: 6
Loss after 1646439 batches: 0.1378
trigger times: 7
Loss after 1647402 batches: 0.1377
trigger times: 8
Loss after 1648365 batches: 0.1375
trigger times: 0
Loss after 1649328 batches: 0.1326
trigger times: 1
Loss after 1650291 batches: 0.1179
trigger times: 0
Loss after 1651254 batches: 0.1095
trigger times: 0
Loss after 1652217 batches: 0.1039
trigger times: 1
Loss after 1653180 batches: 0.0943
trigger times: 0
Loss after 1654143 batches: 0.0864
trigger times: 0
Loss after 1655106 batches: 0.0814
trigger times: 1
Loss after 1656069 batches: 0.0772
trigger times: 0
Loss after 1657032 batches: 0.0730
trigger times: 1
Loss after 1657995 batches: 0.0703
trigger times: 2
Loss after 1658958 batches: 0.0671
trigger times: 3
Loss after 1659921 batches: 0.0664
trigger times: 0
Loss after 1660884 batches: 0.0645
trigger times: 1
Loss after 1661847 batches: 0.0604
trigger times: 2
Loss after 1662810 batches: 0.0578
trigger times: 3
Loss after 1663773 batches: 0.0584
trigger times: 4
Loss after 1664736 batches: 0.0550
trigger times: 0
Loss after 1665699 batches: 0.0512
trigger times: 1
Loss after 1666662 batches: 0.0524
trigger times: 2
Loss after 1667625 batches: 0.0534
trigger times: 0
Loss after 1668588 batches: 0.0658
trigger times: 0
Loss after 1669551 batches: 0.0605
trigger times: 1
Loss after 1670514 batches: 0.0574
trigger times: 2
Loss after 1671477 batches: 0.0539
trigger times: 3
Loss after 1672440 batches: 0.0520
trigger times: 4
Loss after 1673403 batches: 0.0498
trigger times: 5
Loss after 1674366 batches: 0.0502
trigger times: 6
Loss after 1675329 batches: 0.0500
trigger times: 7
Loss after 1676292 batches: 0.0467
trigger times: 8
Loss after 1677255 batches: 0.0449
trigger times: 9
Loss after 1678218 batches: 0.0459
trigger times: 10
Loss after 1679181 batches: 0.0460
trigger times: 11
Loss after 1680144 batches: 0.0470
trigger times: 12
Loss after 1681107 batches: 0.0464
trigger times: 13
Loss after 1682070 batches: 0.0420
trigger times: 14
Loss after 1683033 batches: 0.0439
trigger times: 15
Loss after 1683996 batches: 0.0408
trigger times: 16
Loss after 1684959 batches: 0.0362
trigger times: 17
Loss after 1685922 batches: 0.0402
trigger times: 18
Loss after 1686885 batches: 0.0454
trigger times: 19
Loss after 1687848 batches: 0.0456
trigger times: 20
Loss after 1688811 batches: 0.0442
trigger times: 21
Loss after 1689774 batches: 0.0408
trigger times: 22
Loss after 1690737 batches: 0.0395
trigger times: 23
Loss after 1691700 batches: 0.0370
trigger times: 24
Loss after 1692663 batches: 0.0372
trigger times: 25
Early stopping!
Start to test process.
Loss after 1693626 batches: 0.0371
Time to train on one home:  82.88691520690918
trigger times: 0
Loss after 1694555 batches: 0.1452
trigger times: 0
Loss after 1695484 batches: 0.1370
trigger times: 0
Loss after 1696413 batches: 0.1362
trigger times: 1
Loss after 1697342 batches: 0.1384
trigger times: 2
Loss after 1698271 batches: 0.1367
trigger times: 3
Loss after 1699200 batches: 0.1358
trigger times: 4
Loss after 1700129 batches: 0.1343
trigger times: 5
Loss after 1701058 batches: 0.1339
trigger times: 6
Loss after 1701987 batches: 0.1352
trigger times: 7
Loss after 1702916 batches: 0.1350
trigger times: 8
Loss after 1703845 batches: 0.1355
trigger times: 9
Loss after 1704774 batches: 0.1352
trigger times: 10
Loss after 1705703 batches: 0.1346
trigger times: 11
Loss after 1706632 batches: 0.1347
trigger times: 12
Loss after 1707561 batches: 0.1341
trigger times: 13
Loss after 1708490 batches: 0.1338
trigger times: 14
Loss after 1709419 batches: 0.1339
trigger times: 15
Loss after 1710348 batches: 0.1356
trigger times: 16
Loss after 1711277 batches: 0.1328
trigger times: 0
Loss after 1712206 batches: 0.1096
trigger times: 1
Loss after 1713135 batches: 0.0974
trigger times: 2
Loss after 1714064 batches: 0.0804
trigger times: 3
Loss after 1714993 batches: 0.0674
trigger times: 4
Loss after 1715922 batches: 0.0655
trigger times: 5
Loss after 1716851 batches: 0.0638
trigger times: 6
Loss after 1717780 batches: 0.0617
trigger times: 7
Loss after 1718709 batches: 0.0627
trigger times: 8
Loss after 1719638 batches: 0.0616
trigger times: 9
Loss after 1720567 batches: 0.0621
trigger times: 10
Loss after 1721496 batches: 0.0617
trigger times: 11
Loss after 1722425 batches: 0.0601
trigger times: 12
Loss after 1723354 batches: 0.0571
trigger times: 13
Loss after 1724283 batches: 0.0561
trigger times: 14
Loss after 1725212 batches: 0.0583
trigger times: 0
Loss after 1726141 batches: 0.0557
trigger times: 1
Loss after 1727070 batches: 0.0535
trigger times: 2
Loss after 1727999 batches: 0.0540
trigger times: 0
Loss after 1728928 batches: 0.0523
trigger times: 0
Loss after 1729857 batches: 0.0486
trigger times: 0
Loss after 1730786 batches: 0.0436
trigger times: 1
Loss after 1731715 batches: 0.0464
trigger times: 2
Loss after 1732644 batches: 0.0445
trigger times: 3
Loss after 1733573 batches: 0.0443
trigger times: 0
Loss after 1734502 batches: 0.0443
trigger times: 1
Loss after 1735431 batches: 0.0436
trigger times: 2
Loss after 1736360 batches: 0.0437
trigger times: 3
Loss after 1737289 batches: 0.0453
trigger times: 0
Loss after 1738218 batches: 0.0456
trigger times: 0
Loss after 1739147 batches: 0.0411
trigger times: 1
Loss after 1740076 batches: 0.0407
trigger times: 2
Loss after 1741005 batches: 0.0399
trigger times: 3
Loss after 1741934 batches: 0.0423
trigger times: 0
Loss after 1742863 batches: 0.0380
trigger times: 1
Loss after 1743792 batches: 0.0403
trigger times: 0
Loss after 1744721 batches: 0.0381
trigger times: 1
Loss after 1745650 batches: 0.0385
trigger times: 2
Loss after 1746579 batches: 0.0385
trigger times: 3
Loss after 1747508 batches: 0.0374
trigger times: 4
Loss after 1748437 batches: 0.0399
trigger times: 5
Loss after 1749366 batches: 0.0380
trigger times: 6
Loss after 1750295 batches: 0.0354
trigger times: 7
Loss after 1751224 batches: 0.0363
trigger times: 8
Loss after 1752153 batches: 0.0373
trigger times: 9
Loss after 1753082 batches: 0.0374
trigger times: 10
Loss after 1754011 batches: 0.0379
trigger times: 11
Loss after 1754940 batches: 0.0364
trigger times: 12
Loss after 1755869 batches: 0.0350
trigger times: 0
Loss after 1756798 batches: 0.0338
trigger times: 1
Loss after 1757727 batches: 0.0347
trigger times: 2
Loss after 1758656 batches: 0.0359
trigger times: 3
Loss after 1759585 batches: 0.0347
trigger times: 4
Loss after 1760514 batches: 0.0342
trigger times: 5
Loss after 1761443 batches: 0.0343
trigger times: 6
Loss after 1762372 batches: 0.0327
trigger times: 7
Loss after 1763301 batches: 0.0325
trigger times: 8
Loss after 1764230 batches: 0.0336
trigger times: 9
Loss after 1765159 batches: 0.0352
trigger times: 10
Loss after 1766088 batches: 0.0332
trigger times: 11
Loss after 1767017 batches: 0.0337
trigger times: 12
Loss after 1767946 batches: 0.0334
trigger times: 13
Loss after 1768875 batches: 0.0342
trigger times: 14
Loss after 1769804 batches: 0.0340
trigger times: 15
Loss after 1770733 batches: 0.0337
trigger times: 16
Loss after 1771662 batches: 0.0336
trigger times: 17
Loss after 1772591 batches: 0.0329
trigger times: 18
Loss after 1773520 batches: 0.0322
trigger times: 19
Loss after 1774449 batches: 0.0327
trigger times: 20
Loss after 1775378 batches: 0.0337
trigger times: 21
Loss after 1776307 batches: 0.0340
trigger times: 22
Loss after 1777236 batches: 0.0341
trigger times: 23
Loss after 1778165 batches: 0.0340
trigger times: 24
Loss after 1779094 batches: 0.0306
trigger times: 25
Early stopping!
Start to test process.
Loss after 1780023 batches: 0.0342
Time to train on one home:  106.85895776748657
trigger times: 0
Loss after 1780985 batches: 0.0812
trigger times: 1
Loss after 1781947 batches: 0.0779
trigger times: 2
Loss after 1782909 batches: 0.0783
trigger times: 3
Loss after 1783871 batches: 0.0776
trigger times: 4
Loss after 1784833 batches: 0.0769
trigger times: 5
Loss after 1785795 batches: 0.0768
trigger times: 6
Loss after 1786757 batches: 0.0770
trigger times: 7
Loss after 1787719 batches: 0.0769
trigger times: 8
Loss after 1788681 batches: 0.0769
trigger times: 9
Loss after 1789643 batches: 0.0762
trigger times: 10
Loss after 1790605 batches: 0.0765
trigger times: 11
Loss after 1791567 batches: 0.0762
trigger times: 12
Loss after 1792529 batches: 0.0764
trigger times: 0
Loss after 1793491 batches: 0.0747
trigger times: 1
Loss after 1794453 batches: 0.0720
trigger times: 2
Loss after 1795415 batches: 0.0718
trigger times: 3
Loss after 1796377 batches: 0.0709
trigger times: 4
Loss after 1797339 batches: 0.0705
trigger times: 5
Loss after 1798301 batches: 0.0703
trigger times: 6
Loss after 1799263 batches: 0.0704
trigger times: 7
Loss after 1800225 batches: 0.0699
trigger times: 8
Loss after 1801187 batches: 0.0695
trigger times: 9
Loss after 1802149 batches: 0.0696
trigger times: 10
Loss after 1803111 batches: 0.0697
trigger times: 11
Loss after 1804073 batches: 0.0698
trigger times: 12
Loss after 1805035 batches: 0.0701
trigger times: 13
Loss after 1805997 batches: 0.0691
trigger times: 14
Loss after 1806959 batches: 0.0701
trigger times: 15
Loss after 1807921 batches: 0.0697
trigger times: 16
Loss after 1808883 batches: 0.0690
trigger times: 17
Loss after 1809845 batches: 0.0695
trigger times: 18
Loss after 1810807 batches: 0.0689
trigger times: 19
Loss after 1811769 batches: 0.0692
trigger times: 20
Loss after 1812731 batches: 0.0690
trigger times: 21
Loss after 1813693 batches: 0.0688
trigger times: 22
Loss after 1814655 batches: 0.0681
trigger times: 23
Loss after 1815617 batches: 0.0677
trigger times: 24
Loss after 1816579 batches: 0.0674
trigger times: 25
Early stopping!
Start to test process.
Loss after 1817541 batches: 0.0667
Time to train on one home:  67.21691370010376
trigger times: 0
Loss after 1818504 batches: 0.0642
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 1819467 batches: 0.0632
trigger times: 1
Loss after 1820430 batches: 0.0637
trigger times: 2
Loss after 1821393 batches: 0.0632
trigger times: 3
Loss after 1822356 batches: 0.0630
trigger times: 4
Loss after 1823319 batches: 0.0631
trigger times: 5
Loss after 1824282 batches: 0.0630
trigger times: 6
Loss after 1825245 batches: 0.0631
trigger times: 7
Loss after 1826208 batches: 0.0629
trigger times: 0
Loss after 1827171 batches: 0.0611
trigger times: 0
Loss after 1828134 batches: 0.0585
trigger times: 0
Loss after 1829097 batches: 0.0578
trigger times: 1
Loss after 1830060 batches: 0.0569
trigger times: 2
Loss after 1831023 batches: 0.0565
trigger times: 3
Loss after 1831986 batches: 0.0547
trigger times: 4
Loss after 1832949 batches: 0.0537
trigger times: 5
Loss after 1833912 batches: 0.0527
trigger times: 0
Loss after 1834875 batches: 0.0521
trigger times: 1
Loss after 1835838 batches: 0.0515
trigger times: 2
Loss after 1836801 batches: 0.0511
trigger times: 0
Loss after 1837764 batches: 0.0510
trigger times: 0
Loss after 1838727 batches: 0.0503
trigger times: 0
Loss after 1839690 batches: 0.0502
trigger times: 0
Loss after 1840653 batches: 0.0496
trigger times: 1
Loss after 1841616 batches: 0.0497
trigger times: 2
Loss after 1842579 batches: 0.0492
trigger times: 0
Loss after 1843542 batches: 0.0487
trigger times: 0
Loss after 1844505 batches: 0.0477
trigger times: 0
Loss after 1845468 batches: 0.0479
trigger times: 1
Loss after 1846431 batches: 0.0470
trigger times: 2
Loss after 1847394 batches: 0.0468
trigger times: 3
Loss after 1848357 batches: 0.0464
trigger times: 4
Loss after 1849320 batches: 0.0458
trigger times: 5
Loss after 1850283 batches: 0.0459
trigger times: 6
Loss after 1851246 batches: 0.0453
trigger times: 7
Loss after 1852209 batches: 0.0442
trigger times: 8
Loss after 1853172 batches: 0.0441
trigger times: 9
Loss after 1854135 batches: 0.0431
trigger times: 10
Loss after 1855098 batches: 0.0429
trigger times: 11
Loss after 1856061 batches: 0.0430
trigger times: 12
Loss after 1857024 batches: 0.0425
trigger times: 13
Loss after 1857987 batches: 0.0421
trigger times: 14
Loss after 1858950 batches: 0.0416
trigger times: 15
Loss after 1859913 batches: 0.0413
trigger times: 16
Loss after 1860876 batches: 0.0413
trigger times: 17
Loss after 1861839 batches: 0.0409
trigger times: 18
Loss after 1862802 batches: 0.0404
trigger times: 19
Loss after 1863765 batches: 0.0413
trigger times: 20
Loss after 1864728 batches: 0.0415
trigger times: 21
Loss after 1865691 batches: 0.0409
trigger times: 22
Loss after 1866654 batches: 0.0399
trigger times: 23
Loss after 1867617 batches: 0.0398
trigger times: 24
Loss after 1868580 batches: 0.0393
trigger times: 25
Early stopping!
Start to test process.
Loss after 1869543 batches: 0.0394
Time to train on one home:  80.74779081344604
trigger times: 0
Loss after 1870506 batches: 0.0861
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 1871469 batches: 0.0851
trigger times: 1
Loss after 1872432 batches: 0.0857
trigger times: 2
Loss after 1873395 batches: 0.0858
trigger times: 3
Loss after 1874358 batches: 0.0857
trigger times: 4
Loss after 1875321 batches: 0.0854
trigger times: 5
Loss after 1876284 batches: 0.0855
trigger times: 6
Loss after 1877247 batches: 0.0853
trigger times: 7
Loss after 1878210 batches: 0.0853
trigger times: 8
Loss after 1879173 batches: 0.0853
trigger times: 0
Loss after 1880136 batches: 0.0840
trigger times: 1
Loss after 1881099 batches: 0.0817
trigger times: 2
Loss after 1882062 batches: 0.0796
trigger times: 3
Loss after 1883025 batches: 0.0785
trigger times: 4
Loss after 1883988 batches: 0.0777
trigger times: 5
Loss after 1884951 batches: 0.0767
trigger times: 6
Loss after 1885914 batches: 0.0755
trigger times: 7
Loss after 1886877 batches: 0.0742
trigger times: 8
Loss after 1887840 batches: 0.0711
trigger times: 9
Loss after 1888803 batches: 0.0690
trigger times: 10
Loss after 1889766 batches: 0.0665
trigger times: 11
Loss after 1890729 batches: 0.0637
trigger times: 0
Loss after 1891692 batches: 0.0623
trigger times: 1
Loss after 1892655 batches: 0.0628
trigger times: 2
Loss after 1893618 batches: 0.0604
trigger times: 0
Loss after 1894581 batches: 0.0590
trigger times: 0
Loss after 1895544 batches: 0.0586
trigger times: 1
Loss after 1896507 batches: 0.0582
trigger times: 2
Loss after 1897470 batches: 0.0573
trigger times: 0
Loss after 1898433 batches: 0.0559
trigger times: 1
Loss after 1899396 batches: 0.0559
trigger times: 0
Loss after 1900359 batches: 0.0550
trigger times: 1
Loss after 1901322 batches: 0.0536
trigger times: 2
Loss after 1902285 batches: 0.0539
trigger times: 3
Loss after 1903248 batches: 0.0533
trigger times: 4
Loss after 1904211 batches: 0.0536
trigger times: 0
Loss after 1905174 batches: 0.0525
trigger times: 1
Loss after 1906137 batches: 0.0517
trigger times: 0
Loss after 1907100 batches: 0.0519
trigger times: 0
Loss after 1908063 batches: 0.0515
trigger times: 1
Loss after 1909026 batches: 0.0500
trigger times: 2
Loss after 1909989 batches: 0.0493
trigger times: 3
Loss after 1910952 batches: 0.0500
trigger times: 4
Loss after 1911915 batches: 0.0493
trigger times: 5
Loss after 1912878 batches: 0.0483
trigger times: 0
Loss after 1913841 batches: 0.0489
trigger times: 1
Loss after 1914804 batches: 0.0481
trigger times: 0
Loss after 1915767 batches: 0.0482
trigger times: 1
Loss after 1916730 batches: 0.0462
trigger times: 0
Loss after 1917693 batches: 0.0473
trigger times: 1
Loss after 1918656 batches: 0.0473
trigger times: 2
Loss after 1919619 batches: 0.0463
trigger times: 3
Loss after 1920582 batches: 0.0467
trigger times: 4
Loss after 1921545 batches: 0.0456
trigger times: 5
Loss after 1922508 batches: 0.0464
trigger times: 6
Loss after 1923471 batches: 0.0457
trigger times: 0
Loss after 1924434 batches: 0.0446
trigger times: 0
Loss after 1925397 batches: 0.0450
trigger times: 1
Loss after 1926360 batches: 0.0455
trigger times: 2
Loss after 1927323 batches: 0.0448
trigger times: 3
Loss after 1928286 batches: 0.0435
trigger times: 4
Loss after 1929249 batches: 0.0438
trigger times: 5
Loss after 1930212 batches: 0.0423
trigger times: 6
Loss after 1931175 batches: 0.0432
trigger times: 7
Loss after 1932138 batches: 0.0429
trigger times: 8
Loss after 1933101 batches: 0.0426
trigger times: 0
Loss after 1934064 batches: 0.0424
trigger times: 0
Loss after 1935027 batches: 0.0411
trigger times: 1
Loss after 1935990 batches: 0.0407
trigger times: 2
Loss after 1936953 batches: 0.0420
trigger times: 3
Loss after 1937916 batches: 0.0412
trigger times: 4
Loss after 1938879 batches: 0.0417
trigger times: 5
Loss after 1939842 batches: 0.0404
trigger times: 6
Loss after 1940805 batches: 0.0402
trigger times: 7
Loss after 1941768 batches: 0.0401
trigger times: 8
Loss after 1942731 batches: 0.0398
trigger times: 9
Loss after 1943694 batches: 0.0390
trigger times: 10
Loss after 1944657 batches: 0.0391
trigger times: 11
Loss after 1945620 batches: 0.0388
trigger times: 12
Loss after 1946583 batches: 0.0384
trigger times: 13
Loss after 1947546 batches: 0.0381
trigger times: 14
Loss after 1948509 batches: 0.0375
trigger times: 15
Loss after 1949472 batches: 0.0374
trigger times: 16
Loss after 1950435 batches: 0.0386
trigger times: 17
Loss after 1951398 batches: 0.0388
trigger times: 18
Loss after 1952361 batches: 0.0377
trigger times: 19
Loss after 1953324 batches: 0.0376
trigger times: 20
Loss after 1954287 batches: 0.0377
trigger times: 21
Loss after 1955250 batches: 0.0376
trigger times: 22
Loss after 1956213 batches: 0.0379
trigger times: 23
Loss after 1957176 batches: 0.0371
trigger times: 24
Loss after 1958139 batches: 0.0362
trigger times: 25
Early stopping!
Start to test process.
Loss after 1959102 batches: 0.0357
Time to train on one home:  110.38083505630493
trigger times: 0
Loss after 1960065 batches: 0.1492
trigger times: 0
Loss after 1961028 batches: 0.1493
trigger times: 1
Loss after 1961991 batches: 0.1492
trigger times: 0
Loss after 1962954 batches: 0.1490
trigger times: 1
Loss after 1963917 batches: 0.1493
trigger times: 2
Loss after 1964880 batches: 0.1489
trigger times: 0
Loss after 1965843 batches: 0.1490
trigger times: 0
Loss after 1966806 batches: 0.1480
trigger times: 0
Loss after 1967769 batches: 0.1425
trigger times: 0
Loss after 1968732 batches: 0.1368
trigger times: 0
Loss after 1969695 batches: 0.1338
trigger times: 1
Loss after 1970658 batches: 0.1332
trigger times: 2
Loss after 1971621 batches: 0.1333
trigger times: 3
Loss after 1972584 batches: 0.1334
trigger times: 4
Loss after 1973547 batches: 0.1322
trigger times: 5
Loss after 1974510 batches: 0.1309
trigger times: 6
Loss after 1975473 batches: 0.1300
trigger times: 7
Loss after 1976436 batches: 0.1273
trigger times: 8
Loss after 1977399 batches: 0.1243
trigger times: 9
Loss after 1978362 batches: 0.1216
trigger times: 10
Loss after 1979325 batches: 0.1189
trigger times: 0
Loss after 1980288 batches: 0.1183
trigger times: 1
Loss after 1981251 batches: 0.1170
trigger times: 2
Loss after 1982214 batches: 0.1143
trigger times: 0
Loss after 1983177 batches: 0.1123
trigger times: 1
Loss after 1984140 batches: 0.1133
trigger times: 2
Loss after 1985103 batches: 0.1126
trigger times: 3
Loss after 1986066 batches: 0.1098
trigger times: 4
Loss after 1987029 batches: 0.1074
trigger times: 0
Loss after 1987992 batches: 0.1081
trigger times: 1
Loss after 1988955 batches: 0.1081
trigger times: 2
Loss after 1989918 batches: 0.1066
trigger times: 3
Loss after 1990881 batches: 0.1049
trigger times: 4
Loss after 1991844 batches: 0.1032
trigger times: 5
Loss after 1992807 batches: 0.1012
trigger times: 6
Loss after 1993770 batches: 0.1028
trigger times: 7
Loss after 1994733 batches: 0.0998
trigger times: 8
Loss after 1995696 batches: 0.0991
trigger times: 9
Loss after 1996659 batches: 0.0999
trigger times: 10
Loss after 1997622 batches: 0.0998
trigger times: 11
Loss after 1998585 batches: 0.0987
trigger times: 12
Loss after 1999548 batches: 0.0977
trigger times: 13
Loss after 2000511 batches: 0.0971
trigger times: 14
Loss after 2001474 batches: 0.0957
trigger times: 15
Loss after 2002437 batches: 0.0955
trigger times: 16
Loss after 2003400 batches: 0.0966
trigger times: 17
Loss after 2004363 batches: 0.0987
trigger times: 18
Loss after 2005326 batches: 0.0958
trigger times: 19
Loss after 2006289 batches: 0.0938
trigger times: 20
Loss after 2007252 batches: 0.0928
trigger times: 21
Loss after 2008215 batches: 0.0921
trigger times: 22
Loss after 2009178 batches: 0.0931
trigger times: 23
Loss after 2010141 batches: 0.0928
trigger times: 24
Loss after 2011104 batches: 0.0914
trigger times: 25
Early stopping!
Start to test process.
Loss after 2012067 batches: 0.0915
Time to train on one home:  79.91188597679138
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 2013030 batches: 0.1022
trigger times: 0
Loss after 2013993 batches: 0.0869
trigger times: 0
Loss after 2014956 batches: 0.0879
trigger times: 1
Loss after 2015919 batches: 0.0889
trigger times: 2
Loss after 2016882 batches: 0.0863
trigger times: 3
Loss after 2017845 batches: 0.0862
trigger times: 4
Loss after 2018808 batches: 0.0859
trigger times: 5
Loss after 2019771 batches: 0.0858
trigger times: 6
Loss after 2020734 batches: 0.0852
trigger times: 7
Loss after 2021697 batches: 0.0856
trigger times: 8
Loss after 2022660 batches: 0.0854
trigger times: 9
Loss after 2023623 batches: 0.0851
trigger times: 10
Loss after 2024586 batches: 0.0859
trigger times: 11
Loss after 2025549 batches: 0.0858
trigger times: 12
Loss after 2026512 batches: 0.0854
trigger times: 13
Loss after 2027475 batches: 0.0848
trigger times: 14
Loss after 2028438 batches: 0.0854
trigger times: 15
Loss after 2029401 batches: 0.0857
trigger times: 16
Loss after 2030364 batches: 0.0857
trigger times: 17
Loss after 2031327 batches: 0.0854
trigger times: 18
Loss after 2032290 batches: 0.0850
trigger times: 19
Loss after 2033253 batches: 0.0849
trigger times: 20
Loss after 2034216 batches: 0.0862
trigger times: 21
Loss after 2035179 batches: 0.0857
trigger times: 22
Loss after 2036142 batches: 0.0852
trigger times: 23
Loss after 2037105 batches: 0.0852
trigger times: 24
Loss after 2038068 batches: 0.0855
trigger times: 25
Early stopping!
Start to test process.
Loss after 2039031 batches: 0.0854
Time to train on one home:  54.523581743240356
trigger times: 0
Loss after 2039990 batches: 0.1315
trigger times: 0
Loss after 2040949 batches: 0.1201
trigger times: 0
Loss after 2041908 batches: 0.1218
trigger times: 1
Loss after 2042867 batches: 0.1213
trigger times: 2
Loss after 2043826 batches: 0.1191
trigger times: 3
Loss after 2044785 batches: 0.1195
trigger times: 4
Loss after 2045744 batches: 0.1187
trigger times: 5
Loss after 2046703 batches: 0.1190
trigger times: 6
Loss after 2047662 batches: 0.1185
trigger times: 7
Loss after 2048621 batches: 0.1186
trigger times: 8
Loss after 2049580 batches: 0.1188
trigger times: 9
Loss after 2050539 batches: 0.1182
trigger times: 10
Loss after 2051498 batches: 0.1184
trigger times: 11
Loss after 2052457 batches: 0.1188
trigger times: 12
Loss after 2053416 batches: 0.1188
trigger times: 13
Loss after 2054375 batches: 0.1188
trigger times: 14
Loss after 2055334 batches: 0.1186
trigger times: 15
Loss after 2056293 batches: 0.1185
trigger times: 16
Loss after 2057252 batches: 0.1184
trigger times: 17
Loss after 2058211 batches: 0.1191
trigger times: 18
Loss after 2059170 batches: 0.1194
trigger times: 19
Loss after 2060129 batches: 0.1182
trigger times: 20
Loss after 2061088 batches: 0.1187
trigger times: 21
Loss after 2062047 batches: 0.1195
trigger times: 22
Loss after 2063006 batches: 0.1188
trigger times: 23
Loss after 2063965 batches: 0.1188
trigger times: 24
Loss after 2064924 batches: 0.1191
trigger times: 25
Early stopping!
Start to test process.
Loss after 2065883 batches: 0.1190
Time to train on one home:  57.19373965263367
trigger times: 0
Loss after 2066846 batches: 0.0620
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 2067809 batches: 0.0364
trigger times: 2
Loss after 2068772 batches: 0.0394
trigger times: 3
Loss after 2069735 batches: 0.0335
trigger times: 4
Loss after 2070698 batches: 0.0310
trigger times: 5
Loss after 2071661 batches: 0.0302
trigger times: 6
Loss after 2072624 batches: 0.0298
trigger times: 7
Loss after 2073587 batches: 0.0291
trigger times: 8
Loss after 2074550 batches: 0.0288
trigger times: 9
Loss after 2075513 batches: 0.0287
trigger times: 10
Loss after 2076476 batches: 0.0284
trigger times: 11
Loss after 2077439 batches: 0.0285
trigger times: 12
Loss after 2078402 batches: 0.0284
trigger times: 13
Loss after 2079365 batches: 0.0281
trigger times: 14
Loss after 2080328 batches: 0.0282
trigger times: 15
Loss after 2081291 batches: 0.0283
trigger times: 16
Loss after 2082254 batches: 0.0283
trigger times: 17
Loss after 2083217 batches: 0.0281
trigger times: 18
Loss after 2084180 batches: 0.0283
trigger times: 19
Loss after 2085143 batches: 0.0281
trigger times: 20
Loss after 2086106 batches: 0.0281
trigger times: 21
Loss after 2087069 batches: 0.0279
trigger times: 22
Loss after 2088032 batches: 0.0281
trigger times: 23
Loss after 2088995 batches: 0.0279
trigger times: 24
Loss after 2089958 batches: 0.0280
trigger times: 25
Early stopping!
Start to test process.
Loss after 2090921 batches: 0.0278
Time to train on one home:  56.39993095397949
trigger times: 0
Loss after 2091866 batches: 0.1029
trigger times: 0
Loss after 2092811 batches: 0.1024
trigger times: 1
Loss after 2093756 batches: 0.1027
trigger times: 2
Loss after 2094701 batches: 0.1021
trigger times: 3
Loss after 2095646 batches: 0.1030
trigger times: 4
Loss after 2096591 batches: 0.1025
trigger times: 5
Loss after 2097536 batches: 0.1018
trigger times: 6
Loss after 2098481 batches: 0.1011
trigger times: 0
Loss after 2099426 batches: 0.0882
trigger times: 0
Loss after 2100371 batches: 0.0852
trigger times: 0
Loss after 2101316 batches: 0.0800
trigger times: 0
Loss after 2102261 batches: 0.0771
trigger times: 0
Loss after 2103206 batches: 0.0749
trigger times: 0
Loss after 2104151 batches: 0.0721
trigger times: 0
Loss after 2105096 batches: 0.0689
trigger times: 0
Loss after 2106041 batches: 0.0667
trigger times: 0
Loss after 2106986 batches: 0.0601
trigger times: 0
Loss after 2107931 batches: 0.0564
trigger times: 1
Loss after 2108876 batches: 0.0548
trigger times: 2
Loss after 2109821 batches: 0.0507
trigger times: 0
Loss after 2110766 batches: 0.0544
trigger times: 1
Loss after 2111711 batches: 0.0487
trigger times: 2
Loss after 2112656 batches: 0.0479
trigger times: 0
Loss after 2113601 batches: 0.0454
trigger times: 1
Loss after 2114546 batches: 0.0455
trigger times: 2
Loss after 2115491 batches: 0.0441
trigger times: 3
Loss after 2116436 batches: 0.0439
trigger times: 4
Loss after 2117381 batches: 0.0433
trigger times: 5
Loss after 2118326 batches: 0.0427
trigger times: 6
Loss after 2119271 batches: 0.0420
trigger times: 7
Loss after 2120216 batches: 0.0437
trigger times: 8
Loss after 2121161 batches: 0.0421
trigger times: 9
Loss after 2122106 batches: 0.0410
trigger times: 0
Loss after 2123051 batches: 0.0396
trigger times: 1
Loss after 2123996 batches: 0.0408
trigger times: 2
Loss after 2124941 batches: 0.0411
trigger times: 3
Loss after 2125886 batches: 0.0423
trigger times: 4
Loss after 2126831 batches: 0.0406
trigger times: 0
Loss after 2127776 batches: 0.0400
trigger times: 1
Loss after 2128721 batches: 0.0384
trigger times: 2
Loss after 2129666 batches: 0.0361
trigger times: 3
Loss after 2130611 batches: 0.0363
trigger times: 4
Loss after 2131556 batches: 0.0356
trigger times: 5
Loss after 2132501 batches: 0.0378
trigger times: 6
Loss after 2133446 batches: 0.0367
trigger times: 0
Loss after 2134391 batches: 0.0357
trigger times: 0
Loss after 2135336 batches: 0.0355
trigger times: 0
Loss after 2136281 batches: 0.0339
trigger times: 1
Loss after 2137226 batches: 0.0356
trigger times: 2
Loss after 2138171 batches: 0.0339
trigger times: 3
Loss after 2139116 batches: 0.0327
trigger times: 4
Loss after 2140061 batches: 0.0325
trigger times: 5
Loss after 2141006 batches: 0.0328
trigger times: 6
Loss after 2141951 batches: 0.0328
trigger times: 7
Loss after 2142896 batches: 0.0355
trigger times: 8
Loss after 2143841 batches: 0.0329
trigger times: 9
Loss after 2144786 batches: 0.0334
trigger times: 10
Loss after 2145731 batches: 0.0311
trigger times: 11
Loss after 2146676 batches: 0.0303
trigger times: 12
Loss after 2147621 batches: 0.0296
trigger times: 13
Loss after 2148566 batches: 0.0286
trigger times: 14
Loss after 2149511 batches: 0.0291
trigger times: 15
Loss after 2150456 batches: 0.0299
trigger times: 16
Loss after 2151401 batches: 0.0288
trigger times: 17
Loss after 2152346 batches: 0.0281
trigger times: 18
Loss after 2153291 batches: 0.0282
trigger times: 19
Loss after 2154236 batches: 0.0283
trigger times: 20
Loss after 2155181 batches: 0.0292
trigger times: 21
Loss after 2156126 batches: 0.0279
trigger times: 22
Loss after 2157071 batches: 0.0274
trigger times: 0
Loss after 2158016 batches: 0.0275
trigger times: 1
Loss after 2158961 batches: 0.0278
trigger times: 2
Loss after 2159906 batches: 0.0287
trigger times: 0
Loss after 2160851 batches: 0.0268
trigger times: 1
Loss after 2161796 batches: 0.0274
trigger times: 0
Loss after 2162741 batches: 0.0273
trigger times: 1
Loss after 2163686 batches: 0.0273
trigger times: 2
Loss after 2164631 batches: 0.0275
trigger times: 3
Loss after 2165576 batches: 0.0270
trigger times: 4
Loss after 2166521 batches: 0.0258
trigger times: 5
Loss after 2167466 batches: 0.0268
trigger times: 6
Loss after 2168411 batches: 0.0263
trigger times: 7
Loss after 2169356 batches: 0.0256
trigger times: 8
Loss after 2170301 batches: 0.0240
trigger times: 9
Loss after 2171246 batches: 0.0246
trigger times: 10
Loss after 2172191 batches: 0.0241
trigger times: 11
Loss after 2173136 batches: 0.0244
trigger times: 12
Loss after 2174081 batches: 0.0244
trigger times: 13
Loss after 2175026 batches: 0.0243
trigger times: 14
Loss after 2175971 batches: 0.0253
trigger times: 15
Loss after 2176916 batches: 0.0238
trigger times: 16
Loss after 2177861 batches: 0.0243
trigger times: 17
Loss after 2178806 batches: 0.0232
trigger times: 18
Loss after 2179751 batches: 0.0234
trigger times: 19
Loss after 2180696 batches: 0.0253
trigger times: 20
Loss after 2181641 batches: 0.0243
trigger times: 21
Loss after 2182586 batches: 0.0270
trigger times: 22
Loss after 2183531 batches: 0.0264
trigger times: 23
Loss after 2184476 batches: 0.0267
trigger times: 24
Loss after 2185421 batches: 0.0245
trigger times: 25
Early stopping!
Start to test process.
Loss after 2186366 batches: 0.0241
Time to train on one home:  117.36056208610535
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 2187303 batches: 0.0952
trigger times: 0
Loss after 2188240 batches: 0.0944
trigger times: 1
Loss after 2189177 batches: 0.0941
trigger times: 2
Loss after 2190114 batches: 0.0943
trigger times: 3
Loss after 2191051 batches: 0.0940
trigger times: 4
Loss after 2191988 batches: 0.0940
trigger times: 5
Loss after 2192925 batches: 0.0943
trigger times: 6
Loss after 2193862 batches: 0.0941
trigger times: 7
Loss after 2194799 batches: 0.0936
trigger times: 8
Loss after 2195736 batches: 0.0933
trigger times: 0
Loss after 2196673 batches: 0.0919
trigger times: 1
Loss after 2197610 batches: 0.0914
trigger times: 2
Loss after 2198547 batches: 0.0897
trigger times: 3
Loss after 2199484 batches: 0.0897
trigger times: 4
Loss after 2200421 batches: 0.0896
trigger times: 5
Loss after 2201358 batches: 0.0900
trigger times: 6
Loss after 2202295 batches: 0.0891
trigger times: 7
Loss after 2203232 batches: 0.0887
trigger times: 8
Loss after 2204169 batches: 0.0874
trigger times: 9
Loss after 2205106 batches: 0.0868
trigger times: 10
Loss after 2206043 batches: 0.0870
trigger times: 11
Loss after 2206980 batches: 0.0859
trigger times: 12
Loss after 2207917 batches: 0.0849
trigger times: 13
Loss after 2208854 batches: 0.0836
trigger times: 14
Loss after 2209791 batches: 0.0819
trigger times: 15
Loss after 2210728 batches: 0.0804
trigger times: 16
Loss after 2211665 batches: 0.0797
trigger times: 17
Loss after 2212602 batches: 0.0781
trigger times: 18
Loss after 2213539 batches: 0.0755
trigger times: 19
Loss after 2214476 batches: 0.0755
trigger times: 20
Loss after 2215413 batches: 0.0756
trigger times: 21
Loss after 2216350 batches: 0.0738
trigger times: 22
Loss after 2217287 batches: 0.0766
trigger times: 23
Loss after 2218224 batches: 0.0738
trigger times: 24
Loss after 2219161 batches: 0.0730
trigger times: 25
Early stopping!
Start to test process.
Loss after 2220098 batches: 0.0727
Time to train on one home:  63.646721601486206
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 2221061 batches: 0.0523
trigger times: 1
Loss after 2222024 batches: 0.0239
trigger times: 2
Loss after 2222987 batches: 0.0253
trigger times: 3
Loss after 2223950 batches: 0.0198
trigger times: 4
Loss after 2224913 batches: 0.0178
trigger times: 5
Loss after 2225876 batches: 0.0164
trigger times: 6
Loss after 2226839 batches: 0.0159
trigger times: 7
Loss after 2227802 batches: 0.0154
trigger times: 8
Loss after 2228765 batches: 0.0151
trigger times: 9
Loss after 2229728 batches: 0.0149
trigger times: 10
Loss after 2230691 batches: 0.0148
trigger times: 11
Loss after 2231654 batches: 0.0147
trigger times: 12
Loss after 2232617 batches: 0.0149
trigger times: 13
Loss after 2233580 batches: 0.0146
trigger times: 14
Loss after 2234543 batches: 0.0147
trigger times: 15
Loss after 2235506 batches: 0.0147
trigger times: 16
Loss after 2236469 batches: 0.0147
trigger times: 17
Loss after 2237432 batches: 0.0146
trigger times: 18
Loss after 2238395 batches: 0.0144
trigger times: 19
Loss after 2239358 batches: 0.0145
trigger times: 20
Loss after 2240321 batches: 0.0144
trigger times: 21
Loss after 2241284 batches: 0.0144
trigger times: 22
Loss after 2242247 batches: 0.0143
trigger times: 23
Loss after 2243210 batches: 0.0142
trigger times: 24
Loss after 2244173 batches: 0.0141
trigger times: 25
Early stopping!
Start to test process.
Loss after 2245136 batches: 0.0142
Time to train on one home:  57.273677825927734
trigger times: 0
Loss after 2246099 batches: 0.0938
trigger times: 0
Loss after 2247062 batches: 0.0940
trigger times: 0
Loss after 2248025 batches: 0.0942
trigger times: 1
Loss after 2248988 batches: 0.0940
trigger times: 2
Loss after 2249951 batches: 0.0944
trigger times: 3
Loss after 2250914 batches: 0.0942
trigger times: 4
Loss after 2251877 batches: 0.0936
trigger times: 5
Loss after 2252840 batches: 0.0936
trigger times: 6
Loss after 2253803 batches: 0.0939
trigger times: 7
Loss after 2254766 batches: 0.0940
trigger times: 8
Loss after 2255729 batches: 0.0939
trigger times: 9
Loss after 2256692 batches: 0.0935
trigger times: 10
Loss after 2257655 batches: 0.0935
trigger times: 0
Loss after 2258618 batches: 0.0931
trigger times: 1
Loss after 2259581 batches: 0.0939
trigger times: 2
Loss after 2260544 batches: 0.0931
trigger times: 3
Loss after 2261507 batches: 0.0921
trigger times: 4
Loss after 2262470 batches: 0.0907
trigger times: 5
Loss after 2263433 batches: 0.0899
trigger times: 6
Loss after 2264396 batches: 0.0903
trigger times: 7
Loss after 2265359 batches: 0.0891
trigger times: 8
Loss after 2266322 batches: 0.0892
trigger times: 9
Loss after 2267285 batches: 0.0884
trigger times: 10
Loss after 2268248 batches: 0.0881
trigger times: 11
Loss after 2269211 batches: 0.0876
trigger times: 12
Loss after 2270174 batches: 0.0859
trigger times: 13
Loss after 2271137 batches: 0.0845
trigger times: 14
Loss after 2272100 batches: 0.0861
trigger times: 15
Loss after 2273063 batches: 0.0848
trigger times: 16
Loss after 2274026 batches: 0.0834
trigger times: 17
Loss after 2274989 batches: 0.0822
trigger times: 18
Loss after 2275952 batches: 0.0809
trigger times: 19
Loss after 2276915 batches: 0.0791
trigger times: 20
Loss after 2277878 batches: 0.0785
trigger times: 21
Loss after 2278841 batches: 0.0781
trigger times: 22
Loss after 2279804 batches: 0.0770
trigger times: 23
Loss after 2280767 batches: 0.0767
trigger times: 24
Loss after 2281730 batches: 0.0763
trigger times: 25
Early stopping!
Start to test process.
Loss after 2282693 batches: 0.0745
Time to train on one home:  65.8361189365387
trigger times: 0
Loss after 2283656 batches: 0.1168
trigger times: 1
Loss after 2284619 batches: 0.0992
trigger times: 2
Loss after 2285582 batches: 0.1012
trigger times: 3
Loss after 2286545 batches: 0.0968
trigger times: 4
Loss after 2287508 batches: 0.0957
trigger times: 5
Loss after 2288471 batches: 0.0950
trigger times: 6
Loss after 2289434 batches: 0.0943
trigger times: 7
Loss after 2290397 batches: 0.0940
trigger times: 8
Loss after 2291360 batches: 0.0937
trigger times: 9
Loss after 2292323 batches: 0.0940
trigger times: 10
Loss after 2293286 batches: 0.0937
trigger times: 11
Loss after 2294249 batches: 0.0936
trigger times: 12
Loss after 2295212 batches: 0.0936
trigger times: 13
Loss after 2296175 batches: 0.0936
trigger times: 14
Loss after 2297138 batches: 0.0930
trigger times: 0
Loss after 2298101 batches: 0.0908
trigger times: 1
Loss after 2299064 batches: 0.0863
trigger times: 2
Loss after 2300027 batches: 0.0830
trigger times: 3
Loss after 2300990 batches: 0.0807
trigger times: 4
Loss after 2301953 batches: 0.0774
trigger times: 5
Loss after 2302916 batches: 0.0769
trigger times: 6
Loss after 2303879 batches: 0.0755
trigger times: 7
Loss after 2304842 batches: 0.0749
trigger times: 8
Loss after 2305805 batches: 0.0733
trigger times: 9
Loss after 2306768 batches: 0.0725
trigger times: 10
Loss after 2307731 batches: 0.0724
trigger times: 11
Loss after 2308694 batches: 0.0708
trigger times: 12
Loss after 2309657 batches: 0.0694
trigger times: 13
Loss after 2310620 batches: 0.0681
trigger times: 14
Loss after 2311583 batches: 0.0650
trigger times: 15
Loss after 2312546 batches: 0.0643
trigger times: 16
Loss after 2313509 batches: 0.0633
trigger times: 17
Loss after 2314472 batches: 0.0619
trigger times: 18
Loss after 2315435 batches: 0.0606
trigger times: 19
Loss after 2316398 batches: 0.0587
trigger times: 20
Loss after 2317361 batches: 0.0574
trigger times: 21
Loss after 2318324 batches: 0.0548
trigger times: 22
Loss after 2319287 batches: 0.0548
trigger times: 23
Loss after 2320250 batches: 0.0532
trigger times: 24
Loss after 2321213 batches: 0.0517
trigger times: 25
Early stopping!
Start to test process.
Loss after 2322176 batches: 0.0518
Time to train on one home:  65.07328677177429
trigger times: 0
Loss after 2323072 batches: 0.1102
trigger times: 1
Loss after 2323968 batches: 0.1094
trigger times: 2
Loss after 2324864 batches: 0.1097
trigger times: 3
Loss after 2325760 batches: 0.1093
trigger times: 4
Loss after 2326656 batches: 0.1096
trigger times: 5
Loss after 2327552 batches: 0.1096
trigger times: 6
Loss after 2328448 batches: 0.1091
trigger times: 7
Loss after 2329344 batches: 0.1097
trigger times: 8
Loss after 2330240 batches: 0.1098
trigger times: 9
Loss after 2331136 batches: 0.1094
trigger times: 10
Loss after 2332032 batches: 0.1090
trigger times: 11
Loss after 2332928 batches: 0.1091
trigger times: 12
Loss after 2333824 batches: 0.1092
trigger times: 13
Loss after 2334720 batches: 0.1091
trigger times: 14
Loss after 2335616 batches: 0.1089
trigger times: 0
Loss after 2336512 batches: 0.1072
trigger times: 1
Loss after 2337408 batches: 0.1032
trigger times: 2
Loss after 2338304 batches: 0.1005
trigger times: 3
Loss after 2339200 batches: 0.0994
trigger times: 4
Loss after 2340096 batches: 0.0977
trigger times: 5
Loss after 2340992 batches: 0.0967
trigger times: 6
Loss after 2341888 batches: 0.0956
trigger times: 7
Loss after 2342784 batches: 0.0930
trigger times: 8
Loss after 2343680 batches: 0.0924
trigger times: 9
Loss after 2344576 batches: 0.0900
trigger times: 10
Loss after 2345472 batches: 0.0881
trigger times: 11
Loss after 2346368 batches: 0.0888
trigger times: 12
Loss after 2347264 batches: 0.0900
trigger times: 13
Loss after 2348160 batches: 0.0903
trigger times: 14
Loss after 2349056 batches: 0.0901
trigger times: 15
Loss after 2349952 batches: 0.0879
trigger times: 16
Loss after 2350848 batches: 0.0879
trigger times: 17
Loss after 2351744 batches: 0.0857
trigger times: 18
Loss after 2352640 batches: 0.0849
trigger times: 19
Loss after 2353536 batches: 0.0841
trigger times: 20
Loss after 2354432 batches: 0.0847
trigger times: 21
Loss after 2355328 batches: 0.0862
trigger times: 22
Loss after 2356224 batches: 0.0845
trigger times: 23
Loss after 2357120 batches: 0.0845
trigger times: 24
Loss after 2358016 batches: 0.0827
trigger times: 25
Early stopping!
Start to test process.
Loss after 2358912 batches: 0.0822
Time to train on one home:  67.08972454071045
trigger times: 0
Loss after 2359875 batches: 0.1879
trigger times: 1
Loss after 2360838 batches: 0.1396
trigger times: 2
Loss after 2361801 batches: 0.1371
trigger times: 3
Loss after 2362764 batches: 0.1246
trigger times: 4
Loss after 2363727 batches: 0.1228
trigger times: 5
Loss after 2364690 batches: 0.1210
trigger times: 6
Loss after 2365653 batches: 0.1199
trigger times: 7
Loss after 2366616 batches: 0.1185
trigger times: 8
Loss after 2367579 batches: 0.1180
trigger times: 9
Loss after 2368542 batches: 0.1177
trigger times: 10
Loss after 2369505 batches: 0.1177
trigger times: 11
Loss after 2370468 batches: 0.1173
trigger times: 12
Loss after 2371431 batches: 0.1175
trigger times: 13
Loss after 2372394 batches: 0.1172
trigger times: 14
Loss after 2373357 batches: 0.1174
trigger times: 15
Loss after 2374320 batches: 0.1173
trigger times: 16
Loss after 2375283 batches: 0.1170
trigger times: 17
Loss after 2376246 batches: 0.1173
trigger times: 18
Loss after 2377209 batches: 0.1155
trigger times: 19
Loss after 2378172 batches: 0.1058
trigger times: 20
Loss after 2379135 batches: 0.1012
trigger times: 21
Loss after 2380098 batches: 0.0977
trigger times: 22
Loss after 2381061 batches: 0.0930
trigger times: 23
Loss after 2382024 batches: 0.0930
trigger times: 24
Loss after 2382987 batches: 0.0916
trigger times: 25
Early stopping!
Start to test process.
Loss after 2383950 batches: 0.0889
Time to train on one home:  55.41349744796753
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 2384913 batches: 0.0864
trigger times: 1
Loss after 2385876 batches: 0.0846
trigger times: 2
Loss after 2386839 batches: 0.0849
trigger times: 3
Loss after 2387802 batches: 0.0846
trigger times: 4
Loss after 2388765 batches: 0.0842
trigger times: 5
Loss after 2389728 batches: 0.0840
trigger times: 6
Loss after 2390691 batches: 0.0842
trigger times: 7
Loss after 2391654 batches: 0.0839
trigger times: 8
Loss after 2392617 batches: 0.0842
trigger times: 9
Loss after 2393580 batches: 0.0838
trigger times: 0
Loss after 2394543 batches: 0.0824
trigger times: 1
Loss after 2395506 batches: 0.0815
trigger times: 2
Loss after 2396469 batches: 0.0782
trigger times: 3
Loss after 2397432 batches: 0.0778
trigger times: 4
Loss after 2398395 batches: 0.0765
trigger times: 5
Loss after 2399358 batches: 0.0760
trigger times: 6
Loss after 2400321 batches: 0.0760
trigger times: 7
Loss after 2401284 batches: 0.0759
trigger times: 8
Loss after 2402247 batches: 0.0757
trigger times: 9
Loss after 2403210 batches: 0.0762
trigger times: 10
Loss after 2404173 batches: 0.0752
trigger times: 11
Loss after 2405136 batches: 0.0757
trigger times: 12
Loss after 2406099 batches: 0.0755
trigger times: 13
Loss after 2407062 batches: 0.0754
trigger times: 14
Loss after 2408025 batches: 0.0751
trigger times: 15
Loss after 2408988 batches: 0.0754
trigger times: 16
Loss after 2409951 batches: 0.0748
trigger times: 17
Loss after 2410914 batches: 0.0747
trigger times: 18
Loss after 2411877 batches: 0.0740
trigger times: 19
Loss after 2412840 batches: 0.0740
trigger times: 20
Loss after 2413803 batches: 0.0730
trigger times: 21
Loss after 2414766 batches: 0.0744
trigger times: 22
Loss after 2415729 batches: 0.0741
trigger times: 23
Loss after 2416692 batches: 0.0733
trigger times: 24
Loss after 2417655 batches: 0.0739
trigger times: 25
Early stopping!
Start to test process.
Loss after 2418618 batches: 0.0745
Time to train on one home:  66.99811339378357
trigger times: 0
Loss after 2419581 batches: 0.1605
trigger times: 1
Loss after 2420544 batches: 0.1550
trigger times: 2
Loss after 2421507 batches: 0.1565
trigger times: 3
Loss after 2422470 batches: 0.1552
trigger times: 4
Loss after 2423433 batches: 0.1546
trigger times: 5
Loss after 2424396 batches: 0.1547
trigger times: 6
Loss after 2425359 batches: 0.1544
trigger times: 7
Loss after 2426322 batches: 0.1543
trigger times: 8
Loss after 2427285 batches: 0.1528
trigger times: 0
Loss after 2428248 batches: 0.1462
trigger times: 1
Loss after 2429211 batches: 0.1368
trigger times: 0
Loss after 2430174 batches: 0.1271
trigger times: 1
Loss after 2431137 batches: 0.1119
trigger times: 2
Loss after 2432100 batches: 0.0985
trigger times: 3
Loss after 2433063 batches: 0.0877
trigger times: 4
Loss after 2434026 batches: 0.0805
trigger times: 5
Loss after 2434989 batches: 0.0754
trigger times: 6
Loss after 2435952 batches: 0.0725
trigger times: 7
Loss after 2436915 batches: 0.0709
trigger times: 8
Loss after 2437878 batches: 0.0699
trigger times: 9
Loss after 2438841 batches: 0.0684
trigger times: 10
Loss after 2439804 batches: 0.0670
trigger times: 11
Loss after 2440767 batches: 0.0673
trigger times: 12
Loss after 2441730 batches: 0.0663
trigger times: 13
Loss after 2442693 batches: 0.0665
trigger times: 14
Loss after 2443656 batches: 0.0645
trigger times: 15
Loss after 2444619 batches: 0.0616
trigger times: 16
Loss after 2445582 batches: 0.0613
trigger times: 17
Loss after 2446545 batches: 0.0594
trigger times: 18
Loss after 2447508 batches: 0.0605
trigger times: 19
Loss after 2448471 batches: 0.0591
trigger times: 20
Loss after 2449434 batches: 0.0593
trigger times: 21
Loss after 2450397 batches: 0.0577
trigger times: 22
Loss after 2451360 batches: 0.0582
trigger times: 23
Loss after 2452323 batches: 0.0574
trigger times: 24
Loss after 2453286 batches: 0.0559
trigger times: 25
Early stopping!
Start to test process.
Loss after 2454249 batches: 0.0552
Time to train on one home:  64.70277142524719
trigger times: 0
Loss after 2455212 batches: 0.0784
trigger times: 0
Loss after 2456175 batches: 0.0776
trigger times: 1
Loss after 2457138 batches: 0.0776
trigger times: 2
Loss after 2458101 batches: 0.0777
trigger times: 3
Loss after 2459064 batches: 0.0779
trigger times: 4
Loss after 2460027 batches: 0.0776
trigger times: 5
Loss after 2460990 batches: 0.0778
trigger times: 6
Loss after 2461953 batches: 0.0777
trigger times: 7
Loss after 2462916 batches: 0.0780
trigger times: 8
Loss after 2463879 batches: 0.0778
trigger times: 9
Loss after 2464842 batches: 0.0778
trigger times: 10
Loss after 2465805 batches: 0.0776
trigger times: 11
Loss after 2466768 batches: 0.0773
trigger times: 12
Loss after 2467731 batches: 0.0761
trigger times: 13
Loss after 2468694 batches: 0.0740
trigger times: 0
Loss after 2469657 batches: 0.0724
trigger times: 1
Loss after 2470620 batches: 0.0708
trigger times: 2
Loss after 2471583 batches: 0.0676
trigger times: 0
Loss after 2472546 batches: 0.0638
trigger times: 1
Loss after 2473509 batches: 0.0604
trigger times: 2
Loss after 2474472 batches: 0.0574
trigger times: 0
Loss after 2475435 batches: 0.0557
trigger times: 1
Loss after 2476398 batches: 0.0555
trigger times: 2
Loss after 2477361 batches: 0.0541
trigger times: 3
Loss after 2478324 batches: 0.0519
trigger times: 4
Loss after 2479287 batches: 0.0510
trigger times: 0
Loss after 2480250 batches: 0.0498
trigger times: 1
Loss after 2481213 batches: 0.0490
trigger times: 0
Loss after 2482176 batches: 0.0492
trigger times: 1
Loss after 2483139 batches: 0.0492
trigger times: 2
Loss after 2484102 batches: 0.0486
trigger times: 3
Loss after 2485065 batches: 0.0466
trigger times: 0
Loss after 2486028 batches: 0.0466
trigger times: 1
Loss after 2486991 batches: 0.0462
trigger times: 2
Loss after 2487954 batches: 0.0453
trigger times: 3
Loss after 2488917 batches: 0.0447
trigger times: 4
Loss after 2489880 batches: 0.0438
trigger times: 0
Loss after 2490843 batches: 0.0431
trigger times: 1
Loss after 2491806 batches: 0.0430
trigger times: 0
Loss after 2492769 batches: 0.0423
trigger times: 1
Loss after 2493732 batches: 0.0415
trigger times: 2
Loss after 2494695 batches: 0.0411
trigger times: 3
Loss after 2495658 batches: 0.0409
trigger times: 4
Loss after 2496621 batches: 0.0407
trigger times: 5
Loss after 2497584 batches: 0.0403
trigger times: 6
Loss after 2498547 batches: 0.0392
trigger times: 7
Loss after 2499510 batches: 0.0395
trigger times: 8
Loss after 2500473 batches: 0.0383
trigger times: 9
Loss after 2501436 batches: 0.0381
trigger times: 10
Loss after 2502399 batches: 0.0375
trigger times: 11
Loss after 2503362 batches: 0.0373
trigger times: 12
Loss after 2504325 batches: 0.0368
trigger times: 13
Loss after 2505288 batches: 0.0356
trigger times: 14
Loss after 2506251 batches: 0.0368
trigger times: 15
Loss after 2507214 batches: 0.0360
trigger times: 16
Loss after 2508177 batches: 0.0355
trigger times: 0
Loss after 2509140 batches: 0.0341
trigger times: 1
Loss after 2510103 batches: 0.0344
trigger times: 2
Loss after 2511066 batches: 0.0341
trigger times: 3
Loss after 2512029 batches: 0.0337
trigger times: 4
Loss after 2512992 batches: 0.0333
trigger times: 5
Loss after 2513955 batches: 0.0331
trigger times: 6
Loss after 2514918 batches: 0.0324
trigger times: 7
Loss after 2515881 batches: 0.0314
trigger times: 8
Loss after 2516844 batches: 0.0316
trigger times: 9
Loss after 2517807 batches: 0.0310
trigger times: 10
Loss after 2518770 batches: 0.0305
trigger times: 11
Loss after 2519733 batches: 0.0310
trigger times: 12
Loss after 2520696 batches: 0.0299
trigger times: 13
Loss after 2521659 batches: 0.0296
trigger times: 14
Loss after 2522622 batches: 0.0302
trigger times: 15
Loss after 2523585 batches: 0.0297
trigger times: 16
Loss after 2524548 batches: 0.0295
trigger times: 17
Loss after 2525511 batches: 0.0297
trigger times: 18
Loss after 2526474 batches: 0.0298
trigger times: 19
Loss after 2527437 batches: 0.0282
trigger times: 20
Loss after 2528400 batches: 0.0287
trigger times: 21
Loss after 2529363 batches: 0.0278
trigger times: 22
Loss after 2530326 batches: 0.0285
trigger times: 23
Loss after 2531289 batches: 0.0286
trigger times: 24
Loss after 2532252 batches: 0.0277
trigger times: 25
Early stopping!
Start to test process.
Loss after 2533215 batches: 0.0272
Time to train on one home:  102.34835267066956
trigger times: 0
Loss after 2534178 batches: 0.0910
trigger times: 1
Loss after 2535141 batches: 0.0627
trigger times: 2
Loss after 2536104 batches: 0.0647
trigger times: 3
Loss after 2537067 batches: 0.0581
trigger times: 4
Loss after 2538030 batches: 0.0562
trigger times: 5
Loss after 2538993 batches: 0.0550
trigger times: 6
Loss after 2539956 batches: 0.0545
trigger times: 7
Loss after 2540919 batches: 0.0540
trigger times: 8
Loss after 2541882 batches: 0.0538
trigger times: 9
Loss after 2542845 batches: 0.0536
trigger times: 10
Loss after 2543808 batches: 0.0536
trigger times: 11
Loss after 2544771 batches: 0.0536
trigger times: 12
Loss after 2545734 batches: 0.0536
trigger times: 13
Loss after 2546697 batches: 0.0533
trigger times: 14
Loss after 2547660 batches: 0.0533
trigger times: 15
Loss after 2548623 batches: 0.0532
trigger times: 16
Loss after 2549586 batches: 0.0532
trigger times: 17
Loss after 2550549 batches: 0.0532
trigger times: 18
Loss after 2551512 batches: 0.0531
trigger times: 19
Loss after 2552475 batches: 0.0529
trigger times: 20
Loss after 2553438 batches: 0.0531
trigger times: 21
Loss after 2554401 batches: 0.0529
trigger times: 22
Loss after 2555364 batches: 0.0530
trigger times: 23
Loss after 2556327 batches: 0.0529
trigger times: 24
Loss after 2557290 batches: 0.0530
trigger times: 25
Early stopping!
Start to test process.
Loss after 2558253 batches: 0.0527
Time to train on one home:  52.53626561164856
trigger times: 0
Loss after 2559148 batches: 0.0861
trigger times: 0
Loss after 2560043 batches: 0.0846
trigger times: 0
Loss after 2560938 batches: 0.0844
trigger times: 1
Loss after 2561833 batches: 0.0847
trigger times: 2
Loss after 2562728 batches: 0.0841
trigger times: 3
Loss after 2563623 batches: 0.0843
trigger times: 4
Loss after 2564518 batches: 0.0840
trigger times: 5
Loss after 2565413 batches: 0.0841
trigger times: 6
Loss after 2566308 batches: 0.0840
trigger times: 7
Loss after 2567203 batches: 0.0840
trigger times: 8
Loss after 2568098 batches: 0.0840
trigger times: 9
Loss after 2568993 batches: 0.0840
trigger times: 10
Loss after 2569888 batches: 0.0839
trigger times: 11
Loss after 2570783 batches: 0.0839
trigger times: 12
Loss after 2571678 batches: 0.0838
trigger times: 13
Loss after 2572573 batches: 0.0836
trigger times: 14
Loss after 2573468 batches: 0.0829
trigger times: 0
Loss after 2574363 batches: 0.0687
trigger times: 1
Loss after 2575258 batches: 0.0304
trigger times: 2
Loss after 2576153 batches: 0.0240
trigger times: 3
Loss after 2577048 batches: 0.0205
trigger times: 0
Loss after 2577943 batches: 0.0185
trigger times: 1
Loss after 2578838 batches: 0.0171
trigger times: 2
Loss after 2579733 batches: 0.0164
trigger times: 0
Loss after 2580628 batches: 0.0152
trigger times: 1
Loss after 2581523 batches: 0.0139
trigger times: 2
Loss after 2582418 batches: 0.0121
trigger times: 0
Loss after 2583313 batches: 0.0106
trigger times: 0
Loss after 2584208 batches: 0.0098
trigger times: 1
Loss after 2585103 batches: 0.0093
trigger times: 2
Loss after 2585998 batches: 0.0087
trigger times: 0
Loss after 2586893 batches: 0.0080
trigger times: 1
Loss after 2587788 batches: 0.0090
trigger times: 2
Loss after 2588683 batches: 0.0085
trigger times: 3
Loss after 2589578 batches: 0.0080
trigger times: 4
Loss after 2590473 batches: 0.0072
trigger times: 5
Loss after 2591368 batches: 0.0073
trigger times: 6
Loss after 2592263 batches: 0.0069
trigger times: 7
Loss after 2593158 batches: 0.0069
trigger times: 8
Loss after 2594053 batches: 0.0069
trigger times: 9
Loss after 2594948 batches: 0.0072
trigger times: 10
Loss after 2595843 batches: 0.0068
trigger times: 11
Loss after 2596738 batches: 0.0064
trigger times: 12
Loss after 2597633 batches: 0.0059
trigger times: 13
Loss after 2598528 batches: 0.0059
trigger times: 14
Loss after 2599423 batches: 0.0056
trigger times: 0
Loss after 2600318 batches: 0.0058
trigger times: 1
Loss after 2601213 batches: 0.0062
trigger times: 2
Loss after 2602108 batches: 0.0066
trigger times: 3
Loss after 2603003 batches: 0.0058
trigger times: 4
Loss after 2603898 batches: 0.0054
trigger times: 5
Loss after 2604793 batches: 0.0053
trigger times: 0
Loss after 2605688 batches: 0.0055
trigger times: 0
Loss after 2606583 batches: 0.0052
trigger times: 1
Loss after 2607478 batches: 0.0057
trigger times: 0
Loss after 2608373 batches: 0.0051
trigger times: 0
Loss after 2609268 batches: 0.0046
trigger times: 0
Loss after 2610163 batches: 0.0050
trigger times: 1
Loss after 2611058 batches: 0.0045
trigger times: 2
Loss after 2611953 batches: 0.0049
trigger times: 3
Loss after 2612848 batches: 0.0052
trigger times: 4
Loss after 2613743 batches: 0.0051
trigger times: 5
Loss after 2614638 batches: 0.0052
trigger times: 6
Loss after 2615533 batches: 0.0048
trigger times: 7
Loss after 2616428 batches: 0.0043
trigger times: 8
Loss after 2617323 batches: 0.0043
trigger times: 9
Loss after 2618218 batches: 0.0046
trigger times: 10
Loss after 2619113 batches: 0.0051
trigger times: 11
Loss after 2620008 batches: 0.0048
trigger times: 12
Loss after 2620903 batches: 0.0043
trigger times: 13
Loss after 2621798 batches: 0.0046
trigger times: 14
Loss after 2622693 batches: 0.0055
trigger times: 15
Loss after 2623588 batches: 0.0052
trigger times: 16
Loss after 2624483 batches: 0.0091
trigger times: 17
Loss after 2625378 batches: 0.0077
trigger times: 18
Loss after 2626273 batches: 0.0066
trigger times: 19
Loss after 2627168 batches: 0.0058
trigger times: 20
Loss after 2628063 batches: 0.0057
trigger times: 0
Loss after 2628958 batches: 0.0062
trigger times: 1
Loss after 2629853 batches: 0.0051
trigger times: 2
Loss after 2630748 batches: 0.0045
trigger times: 3
Loss after 2631643 batches: 0.0049
trigger times: 4
Loss after 2632538 batches: 0.0049
trigger times: 5
Loss after 2633433 batches: 0.0048
trigger times: 0
Loss after 2634328 batches: 0.0050
trigger times: 1
Loss after 2635223 batches: 0.0054
trigger times: 2
Loss after 2636118 batches: 0.0047
trigger times: 3
Loss after 2637013 batches: 0.0046
trigger times: 4
Loss after 2637908 batches: 0.0043
trigger times: 5
Loss after 2638803 batches: 0.0045
trigger times: 6
Loss after 2639698 batches: 0.0045
trigger times: 0
Loss after 2640593 batches: 0.0043
trigger times: 0
Loss after 2641488 batches: 0.0042
trigger times: 1
Loss after 2642383 batches: 0.0042
trigger times: 2
Loss after 2643278 batches: 0.0045
trigger times: 3
Loss after 2644173 batches: 0.0044
trigger times: 4
Loss after 2645068 batches: 0.0047
trigger times: 5
Loss after 2645963 batches: 0.0057
trigger times: 0
Loss after 2646858 batches: 0.0056
trigger times: 1
Loss after 2647753 batches: 0.0049
trigger times: 2
Loss after 2648648 batches: 0.0050
trigger times: 0
Loss after 2649543 batches: 0.0046
trigger times: 1
Loss after 2650438 batches: 0.0045
trigger times: 2
Loss after 2651333 batches: 0.0041
trigger times: 0
Loss after 2652228 batches: 0.0047
trigger times: 1
Loss after 2653123 batches: 0.0048
trigger times: 2
Loss after 2654018 batches: 0.0042
trigger times: 3
Loss after 2654913 batches: 0.0040
trigger times: 4
Loss after 2655808 batches: 0.0039
trigger times: 5
Loss after 2656703 batches: 0.0039
trigger times: 6
Loss after 2657598 batches: 0.0040
trigger times: 7
Loss after 2658493 batches: 0.0037
trigger times: 8
Loss after 2659388 batches: 0.0037
trigger times: 9
Loss after 2660283 batches: 0.0046
trigger times: 10
Loss after 2661178 batches: 0.0044
trigger times: 11
Loss after 2662073 batches: 0.0039
trigger times: 12
Loss after 2662968 batches: 0.0036
trigger times: 13
Loss after 2663863 batches: 0.0033
trigger times: 14
Loss after 2664758 batches: 0.0030
trigger times: 15
Loss after 2665653 batches: 0.0031
trigger times: 16
Loss after 2666548 batches: 0.0029
trigger times: 0
Loss after 2667443 batches: 0.0033
trigger times: 1
Loss after 2668338 batches: 0.0029
trigger times: 2
Loss after 2669233 batches: 0.0035
trigger times: 3
Loss after 2670128 batches: 0.0033
trigger times: 4
Loss after 2671023 batches: 0.0030
trigger times: 5
Loss after 2671918 batches: 0.0042
trigger times: 6
Loss after 2672813 batches: 0.0047
trigger times: 7
Loss after 2673708 batches: 0.0040
trigger times: 8
Loss after 2674603 batches: 0.0049
trigger times: 9
Loss after 2675498 batches: 0.0051
trigger times: 10
Loss after 2676393 batches: 0.0041
trigger times: 11
Loss after 2677288 batches: 0.0039
trigger times: 12
Loss after 2678183 batches: 0.0036
trigger times: 13
Loss after 2679078 batches: 0.0033
trigger times: 14
Loss after 2679973 batches: 0.0034
trigger times: 15
Loss after 2680868 batches: 0.0034
trigger times: 16
Loss after 2681763 batches: 0.0035
trigger times: 0
Loss after 2682658 batches: 0.0034
trigger times: 1
Loss after 2683553 batches: 0.0032
trigger times: 2
Loss after 2684448 batches: 0.0028
trigger times: 3
Loss after 2685343 batches: 0.0031
trigger times: 0
Loss after 2686238 batches: 0.0030
trigger times: 0
Loss after 2687133 batches: 0.0028
trigger times: 1
Loss after 2688028 batches: 0.0031
trigger times: 2
Loss after 2688923 batches: 0.0027
trigger times: 3
Loss after 2689818 batches: 0.0026
trigger times: 4
Loss after 2690713 batches: 0.0026
trigger times: 0
Loss after 2691608 batches: 0.0027
trigger times: 0
Loss after 2692503 batches: 0.0027
trigger times: 1
Loss after 2693398 batches: 0.0028
trigger times: 2
Loss after 2694293 batches: 0.0027
trigger times: 3
Loss after 2695188 batches: 0.0042
trigger times: 4
Loss after 2696083 batches: 0.0076
trigger times: 5
Loss after 2696978 batches: 0.0230
trigger times: 6
Loss after 2697873 batches: 0.0101
trigger times: 7
Loss after 2698768 batches: 0.0083
trigger times: 8
Loss after 2699663 batches: 0.0070
trigger times: 9
Loss after 2700558 batches: 0.0062
trigger times: 10
Loss after 2701453 batches: 0.0059
trigger times: 11
Loss after 2702348 batches: 0.0054
trigger times: 12
Loss after 2703243 batches: 0.0050
trigger times: 13
Loss after 2704138 batches: 0.0052
trigger times: 14
Loss after 2705033 batches: 0.0047
trigger times: 15
Loss after 2705928 batches: 0.0045
trigger times: 16
Loss after 2706823 batches: 0.0068
trigger times: 17
Loss after 2707718 batches: 0.0067
trigger times: 18
Loss after 2708613 batches: 0.0056
trigger times: 19
Loss after 2709508 batches: 0.0056
trigger times: 20
Loss after 2710403 batches: 0.0053
trigger times: 21
Loss after 2711298 batches: 0.0047
trigger times: 22
Loss after 2712193 batches: 0.0043
trigger times: 23
Loss after 2713088 batches: 0.0047
trigger times: 24
Loss after 2713983 batches: 0.0043
trigger times: 25
Early stopping!
Start to test process.
Loss after 2714878 batches: 0.0048
Time to train on one home:  169.4469771385193
train_results:  [0.08167134079891343, 0.05326533742725424]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487]]
Round_1_results:  [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487]
trigger times: 0
Loss after 2715841 batches: 0.0920
trigger times: 0
Loss after 2716804 batches: 0.0849
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2852 < 2853; dropping {'Training_Loss': 0.0920363941362926, 'Validation_Loss': 0.09590362012386322, 'Training_R2': -0.08503367628989, 'Validation_R2': -0.12559713767665337, 'Training_F1': 0.22905680447635943, 'Validation_F1': 0.12607194171326927, 'Training_NEP': 1.0633445455817159, 'Validation_NEP': 1.026310720106914, 'Training_NDE': 0.7990778526505132, 'Validation_NDE': 0.8734610218345569, 'Training_MAE': 30.61918622241126, 'Validation_MAE': 34.18665009265199, 'Training_MSE': 2514.0427, 'Validation_MSE': 4326.601}.
trigger times: 1
Loss after 2717767 batches: 0.0789
trigger times: 2
Loss after 2718730 batches: 0.0753
trigger times: 3
Loss after 2719693 batches: 0.0739
trigger times: 4
Loss after 2720656 batches: 0.0710
trigger times: 0
Loss after 2721619 batches: 0.0670
trigger times: 0
Loss after 2722582 batches: 0.0608
trigger times: 0
Loss after 2723545 batches: 0.0576
trigger times: 0
Loss after 2724508 batches: 0.0550
trigger times: 1
Loss after 2725471 batches: 0.0537
trigger times: 0
Loss after 2726434 batches: 0.0516
trigger times: 1
Loss after 2727397 batches: 0.0511
trigger times: 2
Loss after 2728360 batches: 0.0491
trigger times: 3
Loss after 2729323 batches: 0.0494
trigger times: 4
Loss after 2730286 batches: 0.0484
trigger times: 5
Loss after 2731249 batches: 0.0476
trigger times: 6
Loss after 2732212 batches: 0.0468
trigger times: 7
Loss after 2733175 batches: 0.0480
trigger times: 8
Loss after 2734138 batches: 0.0470
trigger times: 9
Loss after 2735101 batches: 0.0469
trigger times: 10
Loss after 2736064 batches: 0.0453
trigger times: 0
Loss after 2737027 batches: 0.0459
trigger times: 1
Loss after 2737990 batches: 0.0452
trigger times: 2
Loss after 2738953 batches: 0.0445
trigger times: 3
Loss after 2739916 batches: 0.0444
trigger times: 4
Loss after 2740879 batches: 0.0441
trigger times: 5
Loss after 2741842 batches: 0.0439
trigger times: 6
Loss after 2742805 batches: 0.0436
trigger times: 7
Loss after 2743768 batches: 0.0443
trigger times: 8
Loss after 2744731 batches: 0.0448
trigger times: 9
Loss after 2745694 batches: 0.0425
trigger times: 10
Loss after 2746657 batches: 0.0421
trigger times: 11
Loss after 2747620 batches: 0.0440
trigger times: 12
Loss after 2748583 batches: 0.0422
trigger times: 13
Loss after 2749546 batches: 0.0429
trigger times: 14
Loss after 2750509 batches: 0.0417
trigger times: 15
Loss after 2751472 batches: 0.0421
trigger times: 16
Loss after 2752435 batches: 0.0410
trigger times: 17
Loss after 2753398 batches: 0.0414
trigger times: 18
Loss after 2754361 batches: 0.0409
trigger times: 19
Loss after 2755324 batches: 0.0403
trigger times: 20
Loss after 2756287 batches: 0.0402
trigger times: 21
Loss after 2757250 batches: 0.0407
trigger times: 22
Loss after 2758213 batches: 0.0393
trigger times: 23
Loss after 2759176 batches: 0.0390
trigger times: 24
Loss after 2760139 batches: 0.0405
trigger times: 25
Early stopping!
Start to test process.
Loss after 2761102 batches: 0.0419
Time to train on one home:  76.12903642654419
trigger times: 0
Loss after 2762060 batches: 0.1282
trigger times: 0
Loss after 2763018 batches: 0.0972
trigger times: 0
Loss after 2763976 batches: 0.0804
trigger times: 0
Loss after 2764934 batches: 0.0681
trigger times: 0
Loss after 2765892 batches: 0.0680
trigger times: 0
Loss after 2766850 batches: 0.0592
trigger times: 0
Loss after 2767808 batches: 0.0535
trigger times: 1
Loss after 2768766 batches: 0.0472
trigger times: 2
Loss after 2769724 batches: 0.0470
trigger times: 0
Loss after 2770682 batches: 0.0451
trigger times: 0
Loss after 2771640 batches: 0.0422
trigger times: 0
Loss after 2772598 batches: 0.0390
trigger times: 1
Loss after 2773556 batches: 0.0382
trigger times: 2
Loss after 2774514 batches: 0.0374
trigger times: 3
Loss after 2775472 batches: 0.0376
trigger times: 0
Loss after 2776430 batches: 0.0361
trigger times: 1
Loss after 2777388 batches: 0.0355
trigger times: 2
Loss after 2778346 batches: 0.0356
trigger times: 3
Loss after 2779304 batches: 0.0340
trigger times: 4
Loss after 2780262 batches: 0.0342
trigger times: 5
Loss after 2781220 batches: 0.0347
trigger times: 6
Loss after 2782178 batches: 0.0333
trigger times: 7
Loss after 2783136 batches: 0.0321
trigger times: 8
Loss after 2784094 batches: 0.0316
trigger times: 0
Loss after 2785052 batches: 0.0330
trigger times: 1
Loss after 2786010 batches: 0.0324
trigger times: 2
Loss after 2786968 batches: 0.0317
trigger times: 3
Loss after 2787926 batches: 0.0332
trigger times: 0
Loss after 2788884 batches: 0.0300
trigger times: 1
Loss after 2789842 batches: 0.0297
trigger times: 2
Loss after 2790800 batches: 0.0288
trigger times: 3
Loss after 2791758 batches: 0.0296
trigger times: 0
Loss after 2792716 batches: 0.0299
trigger times: 1
Loss after 2793674 batches: 0.0297
trigger times: 2
Loss after 2794632 batches: 0.0281
trigger times: 3
Loss after 2795590 batches: 0.0286
trigger times: 4
Loss after 2796548 batches: 0.0283
trigger times: 5
Loss after 2797506 batches: 0.0278
trigger times: 6
Loss after 2798464 batches: 0.0273
trigger times: 7
Loss after 2799422 batches: 0.0271
trigger times: 0
Loss after 2800380 batches: 0.0271
trigger times: 1
Loss after 2801338 batches: 0.0270
trigger times: 2
Loss after 2802296 batches: 0.0276
trigger times: 3
Loss after 2803254 batches: 0.0266
trigger times: 4
Loss after 2804212 batches: 0.0264
trigger times: 5
Loss after 2805170 batches: 0.0256
trigger times: 6
Loss after 2806128 batches: 0.0253
trigger times: 7
Loss after 2807086 batches: 0.0258
trigger times: 8
Loss after 2808044 batches: 0.0250
trigger times: 9
Loss after 2809002 batches: 0.0245
trigger times: 10
Loss after 2809960 batches: 0.0259
trigger times: 11
Loss after 2810918 batches: 0.0263
trigger times: 12
Loss after 2811876 batches: 0.0271
trigger times: 13
Loss after 2812834 batches: 0.0255
trigger times: 14
Loss after 2813792 batches: 0.0268
trigger times: 15
Loss after 2814750 batches: 0.0265
trigger times: 16
Loss after 2815708 batches: 0.0258
trigger times: 17
Loss after 2816666 batches: 0.0248
trigger times: 18
Loss after 2817624 batches: 0.0238
trigger times: 19
Loss after 2818582 batches: 0.0231
trigger times: 20
Loss after 2819540 batches: 0.0246
trigger times: 21
Loss after 2820498 batches: 0.0259
trigger times: 22
Loss after 2821456 batches: 0.0258
trigger times: 23
Loss after 2822414 batches: 0.0248
trigger times: 24
Loss after 2823372 batches: 0.0254
trigger times: 25
Early stopping!
Start to test process.
Loss after 2824330 batches: 0.0269
Time to train on one home:  89.15708088874817
trigger times: 0
Loss after 2825293 batches: 0.0795
trigger times: 1
Loss after 2826256 batches: 0.0698
trigger times: 2
Loss after 2827219 batches: 0.0704
trigger times: 3
Loss after 2828182 batches: 0.0702
trigger times: 4
Loss after 2829145 batches: 0.0699
trigger times: 5
Loss after 2830108 batches: 0.0697
trigger times: 6
Loss after 2831071 batches: 0.0694
trigger times: 7
Loss after 2832034 batches: 0.0689
trigger times: 8
Loss after 2832997 batches: 0.0689
trigger times: 9
Loss after 2833960 batches: 0.0683
trigger times: 10
Loss after 2834923 batches: 0.0676
trigger times: 11
Loss after 2835886 batches: 0.0674
trigger times: 12
Loss after 2836849 batches: 0.0669
trigger times: 13
Loss after 2837812 batches: 0.0663
trigger times: 14
Loss after 2838775 batches: 0.0669
trigger times: 15
Loss after 2839738 batches: 0.0666
trigger times: 16
Loss after 2840701 batches: 0.0658
trigger times: 17
Loss after 2841664 batches: 0.0644
trigger times: 18
Loss after 2842627 batches: 0.0632
trigger times: 19
Loss after 2843590 batches: 0.0627
trigger times: 20
Loss after 2844553 batches: 0.0635
trigger times: 21
Loss after 2845516 batches: 0.0625
trigger times: 22
Loss after 2846479 batches: 0.0617
trigger times: 23
Loss after 2847442 batches: 0.0619
trigger times: 24
Loss after 2848405 batches: 0.0618
trigger times: 25
Early stopping!
Start to test process.
Loss after 2849368 batches: 0.0610
Time to train on one home:  56.55094790458679
trigger times: 0
Loss after 2850331 batches: 0.0837
trigger times: 1
Loss after 2851294 batches: 0.0818
trigger times: 0
Loss after 2852257 batches: 0.0810
trigger times: 1
Loss after 2853220 batches: 0.0809
trigger times: 2
Loss after 2854183 batches: 0.0796
trigger times: 0
Loss after 2855146 batches: 0.0795
trigger times: 1
Loss after 2856109 batches: 0.0776
trigger times: 2
Loss after 2857072 batches: 0.0755
trigger times: 3
Loss after 2858035 batches: 0.0745
trigger times: 4
Loss after 2858998 batches: 0.0744
trigger times: 5
Loss after 2859961 batches: 0.0728
trigger times: 6
Loss after 2860924 batches: 0.0729
trigger times: 7
Loss after 2861887 batches: 0.0730
trigger times: 8
Loss after 2862850 batches: 0.0713
trigger times: 9
Loss after 2863813 batches: 0.0722
trigger times: 10
Loss after 2864776 batches: 0.0714
trigger times: 11
Loss after 2865739 batches: 0.0713
trigger times: 12
Loss after 2866702 batches: 0.0718
trigger times: 0
Loss after 2867665 batches: 0.0704
trigger times: 1
Loss after 2868628 batches: 0.0702
trigger times: 2
Loss after 2869591 batches: 0.0694
trigger times: 3
Loss after 2870554 batches: 0.0687
trigger times: 4
Loss after 2871517 batches: 0.0696
trigger times: 0
Loss after 2872480 batches: 0.0691
trigger times: 1
Loss after 2873443 batches: 0.0684
trigger times: 2
Loss after 2874406 batches: 0.0673
trigger times: 3
Loss after 2875369 batches: 0.0672
trigger times: 4
Loss after 2876332 batches: 0.0674
trigger times: 0
Loss after 2877295 batches: 0.0679
trigger times: 1
Loss after 2878258 batches: 0.0666
trigger times: 0
Loss after 2879221 batches: 0.0669
trigger times: 1
Loss after 2880184 batches: 0.0684
trigger times: 2
Loss after 2881147 batches: 0.0673
trigger times: 3
Loss after 2882110 batches: 0.0656
trigger times: 4
Loss after 2883073 batches: 0.0658
trigger times: 5
Loss after 2884036 batches: 0.0654
trigger times: 6
Loss after 2884999 batches: 0.0660
trigger times: 7
Loss after 2885962 batches: 0.0646
trigger times: 8
Loss after 2886925 batches: 0.0637
trigger times: 9
Loss after 2887888 batches: 0.0646
trigger times: 10
Loss after 2888851 batches: 0.0648
trigger times: 11
Loss after 2889814 batches: 0.0647
trigger times: 12
Loss after 2890777 batches: 0.0642
trigger times: 13
Loss after 2891740 batches: 0.0649
trigger times: 14
Loss after 2892703 batches: 0.0640
trigger times: 15
Loss after 2893666 batches: 0.0643
trigger times: 16
Loss after 2894629 batches: 0.0640
trigger times: 17
Loss after 2895592 batches: 0.0661
trigger times: 18
Loss after 2896555 batches: 0.0649
trigger times: 19
Loss after 2897518 batches: 0.0654
trigger times: 20
Loss after 2898481 batches: 0.0644
trigger times: 21
Loss after 2899444 batches: 0.0636
trigger times: 22
Loss after 2900407 batches: 0.0637
trigger times: 23
Loss after 2901370 batches: 0.0641
trigger times: 24
Loss after 2902333 batches: 0.0633
trigger times: 25
Early stopping!
Start to test process.
Loss after 2903296 batches: 0.0627
Time to train on one home:  76.98170137405396
trigger times: 0
Loss after 2904259 batches: 0.0466
trigger times: 0
Loss after 2905222 batches: 0.0458
trigger times: 1
Loss after 2906185 batches: 0.0448
trigger times: 2
Loss after 2907148 batches: 0.0441
trigger times: 0
Loss after 2908111 batches: 0.0433
trigger times: 0
Loss after 2909074 batches: 0.0410
trigger times: 0
Loss after 2910037 batches: 0.0387
trigger times: 0
Loss after 2911000 batches: 0.0363
trigger times: 1
Loss after 2911963 batches: 0.0345
trigger times: 0
Loss after 2912926 batches: 0.0338
trigger times: 1
Loss after 2913889 batches: 0.0325
trigger times: 2
Loss after 2914852 batches: 0.0315
trigger times: 3
Loss after 2915815 batches: 0.0313
trigger times: 4
Loss after 2916778 batches: 0.0310
trigger times: 5
Loss after 2917741 batches: 0.0299
trigger times: 6
Loss after 2918704 batches: 0.0291
trigger times: 7
Loss after 2919667 batches: 0.0283
trigger times: 8
Loss after 2920630 batches: 0.0282
trigger times: 9
Loss after 2921593 batches: 0.0282
trigger times: 10
Loss after 2922556 batches: 0.0268
trigger times: 11
Loss after 2923519 batches: 0.0257
trigger times: 12
Loss after 2924482 batches: 0.0258
trigger times: 13
Loss after 2925445 batches: 0.0251
trigger times: 14
Loss after 2926408 batches: 0.0248
trigger times: 15
Loss after 2927371 batches: 0.0249
trigger times: 16
Loss after 2928334 batches: 0.0241
trigger times: 17
Loss after 2929297 batches: 0.0247
trigger times: 18
Loss after 2930260 batches: 0.0239
trigger times: 19
Loss after 2931223 batches: 0.0239
trigger times: 20
Loss after 2932186 batches: 0.0237
trigger times: 21
Loss after 2933149 batches: 0.0234
trigger times: 22
Loss after 2934112 batches: 0.0235
trigger times: 23
Loss after 2935075 batches: 0.0228
trigger times: 24
Loss after 2936038 batches: 0.0230
trigger times: 25
Early stopping!
Start to test process.
Loss after 2937001 batches: 0.0223
Time to train on one home:  65.24230790138245
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 2937964 batches: 0.0644
trigger times: 1
Loss after 2938927 batches: 0.0331
trigger times: 2
Loss after 2939890 batches: 0.0294
trigger times: 3
Loss after 2940853 batches: 0.0263
trigger times: 4
Loss after 2941816 batches: 0.0236
trigger times: 5
Loss after 2942779 batches: 0.0229
trigger times: 6
Loss after 2943742 batches: 0.0228
trigger times: 7
Loss after 2944705 batches: 0.0228
trigger times: 8
Loss after 2945668 batches: 0.0227
trigger times: 9
Loss after 2946631 batches: 0.0228
trigger times: 10
Loss after 2947594 batches: 0.0227
trigger times: 11
Loss after 2948557 batches: 0.0225
trigger times: 12
Loss after 2949520 batches: 0.0225
trigger times: 13
Loss after 2950483 batches: 0.0225
trigger times: 14
Loss after 2951446 batches: 0.0227
trigger times: 15
Loss after 2952409 batches: 0.0225
trigger times: 16
Loss after 2953372 batches: 0.0226
trigger times: 17
Loss after 2954335 batches: 0.0227
trigger times: 18
Loss after 2955298 batches: 0.0225
trigger times: 19
Loss after 2956261 batches: 0.0225
trigger times: 20
Loss after 2957224 batches: 0.0226
trigger times: 21
Loss after 2958187 batches: 0.0226
trigger times: 22
Loss after 2959150 batches: 0.0225
trigger times: 23
Loss after 2960113 batches: 0.0224
trigger times: 24
Loss after 2961076 batches: 0.0223
trigger times: 25
Early stopping!
Start to test process.
Loss after 2962039 batches: 0.0222
Time to train on one home:  54.345510721206665
trigger times: 0
Loss after 2963002 batches: 0.1227
trigger times: 0
Loss after 2963965 batches: 0.1194
trigger times: 1
Loss after 2964928 batches: 0.1187
trigger times: 0
Loss after 2965891 batches: 0.1152
trigger times: 1
Loss after 2966854 batches: 0.1096
trigger times: 0
Loss after 2967817 batches: 0.1067
trigger times: 0
Loss after 2968780 batches: 0.1041
trigger times: 0
Loss after 2969743 batches: 0.1014
trigger times: 0
Loss after 2970706 batches: 0.0994
trigger times: 0
Loss after 2971669 batches: 0.0980
trigger times: 1
Loss after 2972632 batches: 0.0957
trigger times: 2
Loss after 2973595 batches: 0.0951
trigger times: 3
Loss after 2974558 batches: 0.0956
trigger times: 4
Loss after 2975521 batches: 0.0930
trigger times: 5
Loss after 2976484 batches: 0.0919
trigger times: 6
Loss after 2977447 batches: 0.0908
trigger times: 7
Loss after 2978410 batches: 0.0914
trigger times: 8
Loss after 2979373 batches: 0.0891
trigger times: 9
Loss after 2980336 batches: 0.0883
trigger times: 10
Loss after 2981299 batches: 0.0886
trigger times: 11
Loss after 2982262 batches: 0.0894
trigger times: 12
Loss after 2983225 batches: 0.0899
trigger times: 13
Loss after 2984188 batches: 0.0886
trigger times: 14
Loss after 2985151 batches: 0.0874
trigger times: 15
Loss after 2986114 batches: 0.0863
trigger times: 16
Loss after 2987077 batches: 0.0866
trigger times: 17
Loss after 2988040 batches: 0.0860
trigger times: 18
Loss after 2989003 batches: 0.0853
trigger times: 19
Loss after 2989966 batches: 0.0866
trigger times: 20
Loss after 2990929 batches: 0.0858
trigger times: 21
Loss after 2991892 batches: 0.0848
trigger times: 22
Loss after 2992855 batches: 0.0820
trigger times: 23
Loss after 2993818 batches: 0.0835
trigger times: 24
Loss after 2994781 batches: 0.0840
trigger times: 25
Early stopping!
Start to test process.
Loss after 2995744 batches: 0.0830
Time to train on one home:  68.69725894927979
trigger times: 0
Loss after 2996707 batches: 0.0832
trigger times: 0
Loss after 2997670 batches: 0.0713
trigger times: 1
Loss after 2998633 batches: 0.0691
trigger times: 2
Loss after 2999596 batches: 0.0662
trigger times: 3
Loss after 3000559 batches: 0.0619
trigger times: 4
Loss after 3001522 batches: 0.0612
trigger times: 0
Loss after 3002485 batches: 0.0603
trigger times: 0
Loss after 3003448 batches: 0.0567
trigger times: 1
Loss after 3004411 batches: 0.0550
trigger times: 2
Loss after 3005374 batches: 0.0523
trigger times: 0
Loss after 3006337 batches: 0.0512
trigger times: 0
Loss after 3007300 batches: 0.0486
trigger times: 1
Loss after 3008263 batches: 0.0490
trigger times: 0
Loss after 3009226 batches: 0.0455
trigger times: 0
Loss after 3010189 batches: 0.0427
trigger times: 1
Loss after 3011152 batches: 0.0458
trigger times: 2
Loss after 3012115 batches: 0.0436
trigger times: 3
Loss after 3013078 batches: 0.0424
trigger times: 0
Loss after 3014041 batches: 0.0414
trigger times: 1
Loss after 3015004 batches: 0.0394
trigger times: 0
Loss after 3015967 batches: 0.0397
trigger times: 1
Loss after 3016930 batches: 0.0392
trigger times: 2
Loss after 3017893 batches: 0.0383
trigger times: 3
Loss after 3018856 batches: 0.0370
trigger times: 4
Loss after 3019819 batches: 0.0357
trigger times: 5
Loss after 3020782 batches: 0.0371
trigger times: 6
Loss after 3021745 batches: 0.0348
trigger times: 7
Loss after 3022708 batches: 0.0358
trigger times: 8
Loss after 3023671 batches: 0.0355
trigger times: 9
Loss after 3024634 batches: 0.0359
trigger times: 10
Loss after 3025597 batches: 0.0351
trigger times: 11
Loss after 3026560 batches: 0.0344
trigger times: 12
Loss after 3027523 batches: 0.0342
trigger times: 0
Loss after 3028486 batches: 0.0335
trigger times: 1
Loss after 3029449 batches: 0.0337
trigger times: 2
Loss after 3030412 batches: 0.0344
trigger times: 3
Loss after 3031375 batches: 0.0344
trigger times: 4
Loss after 3032338 batches: 0.0334
trigger times: 0
Loss after 3033301 batches: 0.0330
trigger times: 1
Loss after 3034264 batches: 0.0314
trigger times: 2
Loss after 3035227 batches: 0.0321
trigger times: 3
Loss after 3036190 batches: 0.0325
trigger times: 4
Loss after 3037153 batches: 0.0315
trigger times: 5
Loss after 3038116 batches: 0.0306
trigger times: 6
Loss after 3039079 batches: 0.0310
trigger times: 0
Loss after 3040042 batches: 0.0319
trigger times: 1
Loss after 3041005 batches: 0.0335
trigger times: 2
Loss after 3041968 batches: 0.0320
trigger times: 3
Loss after 3042931 batches: 0.0308
trigger times: 4
Loss after 3043894 batches: 0.0303
trigger times: 5
Loss after 3044857 batches: 0.0298
trigger times: 6
Loss after 3045820 batches: 0.0303
trigger times: 7
Loss after 3046783 batches: 0.0298
trigger times: 8
Loss after 3047746 batches: 0.0297
trigger times: 9
Loss after 3048709 batches: 0.0288
trigger times: 10
Loss after 3049672 batches: 0.0293
trigger times: 11
Loss after 3050635 batches: 0.0304
trigger times: 12
Loss after 3051598 batches: 0.0292
trigger times: 13
Loss after 3052561 batches: 0.0302
trigger times: 14
Loss after 3053524 batches: 0.0296
trigger times: 15
Loss after 3054487 batches: 0.0285
trigger times: 16
Loss after 3055450 batches: 0.0284
trigger times: 17
Loss after 3056413 batches: 0.0273
trigger times: 18
Loss after 3057376 batches: 0.0279
trigger times: 19
Loss after 3058339 batches: 0.0292
trigger times: 20
Loss after 3059302 batches: 0.0287
trigger times: 21
Loss after 3060265 batches: 0.0282
trigger times: 22
Loss after 3061228 batches: 0.0276
trigger times: 23
Loss after 3062191 batches: 0.0284
trigger times: 24
Loss after 3063154 batches: 0.0285
trigger times: 25
Early stopping!
Start to test process.
Loss after 3064117 batches: 0.0278
Time to train on one home:  92.4183509349823
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3065080 batches: 0.1049
trigger times: 1
Loss after 3066043 batches: 0.1016
trigger times: 0
Loss after 3067006 batches: 0.1002
trigger times: 0
Loss after 3067969 batches: 0.0980
trigger times: 1
Loss after 3068932 batches: 0.0938
trigger times: 0
Loss after 3069895 batches: 0.0904
trigger times: 1
Loss after 3070858 batches: 0.0872
trigger times: 2
Loss after 3071821 batches: 0.0846
trigger times: 3
Loss after 3072784 batches: 0.0836
trigger times: 4
Loss after 3073747 batches: 0.0824
trigger times: 5
Loss after 3074710 batches: 0.0810
trigger times: 6
Loss after 3075673 batches: 0.0797
trigger times: 7
Loss after 3076636 batches: 0.0796
trigger times: 8
Loss after 3077599 batches: 0.0789
trigger times: 9
Loss after 3078562 batches: 0.0775
trigger times: 10
Loss after 3079525 batches: 0.0777
trigger times: 11
Loss after 3080488 batches: 0.0778
trigger times: 12
Loss after 3081451 batches: 0.0771
trigger times: 13
Loss after 3082414 batches: 0.0762
trigger times: 14
Loss after 3083377 batches: 0.0751
trigger times: 15
Loss after 3084340 batches: 0.0746
trigger times: 16
Loss after 3085303 batches: 0.0742
trigger times: 17
Loss after 3086266 batches: 0.0729
trigger times: 18
Loss after 3087229 batches: 0.0739
trigger times: 19
Loss after 3088192 batches: 0.0728
trigger times: 20
Loss after 3089155 batches: 0.0740
trigger times: 21
Loss after 3090118 batches: 0.0711
trigger times: 22
Loss after 3091081 batches: 0.0713
trigger times: 23
Loss after 3092044 batches: 0.0715
trigger times: 24
Loss after 3093007 batches: 0.0709
trigger times: 25
Early stopping!
Start to test process.
Loss after 3093970 batches: 0.0707
Time to train on one home:  60.95885252952576
trigger times: 0
Loss after 3094933 batches: 0.1240
trigger times: 0
Loss after 3095896 batches: 0.1109
trigger times: 1
Loss after 3096859 batches: 0.1067
trigger times: 0
Loss after 3097822 batches: 0.1000
trigger times: 0
Loss after 3098785 batches: 0.0913
trigger times: 0
Loss after 3099748 batches: 0.0822
trigger times: 0
Loss after 3100711 batches: 0.0774
trigger times: 0
Loss after 3101674 batches: 0.0751
trigger times: 0
Loss after 3102637 batches: 0.0735
trigger times: 0
Loss after 3103600 batches: 0.0703
trigger times: 1
Loss after 3104563 batches: 0.0676
trigger times: 2
Loss after 3105526 batches: 0.0667
trigger times: 3
Loss after 3106489 batches: 0.0640
trigger times: 4
Loss after 3107452 batches: 0.0616
trigger times: 5
Loss after 3108415 batches: 0.0614
trigger times: 6
Loss after 3109378 batches: 0.0609
trigger times: 7
Loss after 3110341 batches: 0.0600
trigger times: 8
Loss after 3111304 batches: 0.0588
trigger times: 9
Loss after 3112267 batches: 0.0580
trigger times: 10
Loss after 3113230 batches: 0.0586
trigger times: 11
Loss after 3114193 batches: 0.0583
trigger times: 12
Loss after 3115156 batches: 0.0573
trigger times: 13
Loss after 3116119 batches: 0.0562
trigger times: 14
Loss after 3117082 batches: 0.0583
trigger times: 15
Loss after 3118045 batches: 0.0565
trigger times: 16
Loss after 3119008 batches: 0.0560
trigger times: 17
Loss after 3119971 batches: 0.0556
trigger times: 18
Loss after 3120934 batches: 0.0546
trigger times: 19
Loss after 3121897 batches: 0.0551
trigger times: 20
Loss after 3122860 batches: 0.0542
trigger times: 21
Loss after 3123823 batches: 0.0536
trigger times: 22
Loss after 3124786 batches: 0.0535
trigger times: 23
Loss after 3125749 batches: 0.0529
trigger times: 0
Loss after 3126712 batches: 0.0538
trigger times: 1
Loss after 3127675 batches: 0.0537
trigger times: 2
Loss after 3128638 batches: 0.0526
trigger times: 3
Loss after 3129601 batches: 0.0533
trigger times: 4
Loss after 3130564 batches: 0.0517
trigger times: 5
Loss after 3131527 batches: 0.0530
trigger times: 0
Loss after 3132490 batches: 0.0512
trigger times: 0
Loss after 3133453 batches: 0.0515
trigger times: 1
Loss after 3134416 batches: 0.0509
trigger times: 2
Loss after 3135379 batches: 0.0497
trigger times: 3
Loss after 3136342 batches: 0.0508
trigger times: 4
Loss after 3137305 batches: 0.0504
trigger times: 5
Loss after 3138268 batches: 0.0509
trigger times: 6
Loss after 3139231 batches: 0.0504
trigger times: 7
Loss after 3140194 batches: 0.0500
trigger times: 8
Loss after 3141157 batches: 0.0505
trigger times: 9
Loss after 3142120 batches: 0.0501
trigger times: 10
Loss after 3143083 batches: 0.0494
trigger times: 11
Loss after 3144046 batches: 0.0493
trigger times: 12
Loss after 3145009 batches: 0.0489
trigger times: 13
Loss after 3145972 batches: 0.0482
trigger times: 14
Loss after 3146935 batches: 0.0483
trigger times: 15
Loss after 3147898 batches: 0.0473
trigger times: 16
Loss after 3148861 batches: 0.0476
trigger times: 17
Loss after 3149824 batches: 0.0472
trigger times: 18
Loss after 3150787 batches: 0.0470
trigger times: 19
Loss after 3151750 batches: 0.0465
trigger times: 20
Loss after 3152713 batches: 0.0475
trigger times: 21
Loss after 3153676 batches: 0.0473
trigger times: 22
Loss after 3154639 batches: 0.0474
trigger times: 23
Loss after 3155602 batches: 0.0462
trigger times: 24
Loss after 3156565 batches: 0.0456
trigger times: 25
Early stopping!
Start to test process.
Loss after 3157528 batches: 0.0460
Time to train on one home:  86.71403479576111
trigger times: 0
Loss after 3158491 batches: 0.0979
trigger times: 0
Loss after 3159454 batches: 0.0961
trigger times: 1
Loss after 3160417 batches: 0.0949
trigger times: 2
Loss after 3161380 batches: 0.0949
trigger times: 0
Loss after 3162343 batches: 0.0933
trigger times: 1
Loss after 3163306 batches: 0.0923
trigger times: 0
Loss after 3164269 batches: 0.0900
trigger times: 1
Loss after 3165232 batches: 0.0847
trigger times: 2
Loss after 3166195 batches: 0.0815
trigger times: 3
Loss after 3167158 batches: 0.0801
trigger times: 0
Loss after 3168121 batches: 0.0783
trigger times: 1
Loss after 3169084 batches: 0.0775
trigger times: 2
Loss after 3170047 batches: 0.0755
trigger times: 3
Loss after 3171010 batches: 0.0769
trigger times: 4
Loss after 3171973 batches: 0.0757
trigger times: 0
Loss after 3172936 batches: 0.0745
trigger times: 1
Loss after 3173899 batches: 0.0736
trigger times: 2
Loss after 3174862 batches: 0.0730
trigger times: 3
Loss after 3175825 batches: 0.0734
trigger times: 4
Loss after 3176788 batches: 0.0714
trigger times: 5
Loss after 3177751 batches: 0.0722
trigger times: 6
Loss after 3178714 batches: 0.0709
trigger times: 7
Loss after 3179677 batches: 0.0711
trigger times: 8
Loss after 3180640 batches: 0.0703
trigger times: 9
Loss after 3181603 batches: 0.0707
trigger times: 10
Loss after 3182566 batches: 0.0699
trigger times: 11
Loss after 3183529 batches: 0.0682
trigger times: 12
Loss after 3184492 batches: 0.0690
trigger times: 13
Loss after 3185455 batches: 0.0691
trigger times: 14
Loss after 3186418 batches: 0.0688
trigger times: 15
Loss after 3187381 batches: 0.0687
trigger times: 16
Loss after 3188344 batches: 0.0674
trigger times: 17
Loss after 3189307 batches: 0.0668
trigger times: 18
Loss after 3190270 batches: 0.0669
trigger times: 19
Loss after 3191233 batches: 0.0662
trigger times: 20
Loss after 3192196 batches: 0.0670
trigger times: 21
Loss after 3193159 batches: 0.0659
trigger times: 22
Loss after 3194122 batches: 0.0671
trigger times: 23
Loss after 3195085 batches: 0.0663
trigger times: 24
Loss after 3196048 batches: 0.0665
trigger times: 25
Early stopping!
Start to test process.
Loss after 3197011 batches: 0.0652
Time to train on one home:  68.89536952972412
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3197974 batches: 0.0678
trigger times: 1
Loss after 3198937 batches: 0.0667
trigger times: 2
Loss after 3199900 batches: 0.0654
trigger times: 0
Loss after 3200863 batches: 0.0642
trigger times: 1
Loss after 3201826 batches: 0.0626
trigger times: 2
Loss after 3202789 batches: 0.0611
trigger times: 3
Loss after 3203752 batches: 0.0589
trigger times: 4
Loss after 3204715 batches: 0.0559
trigger times: 5
Loss after 3205678 batches: 0.0539
trigger times: 6
Loss after 3206641 batches: 0.0521
trigger times: 7
Loss after 3207604 batches: 0.0514
trigger times: 8
Loss after 3208567 batches: 0.0500
trigger times: 9
Loss after 3209530 batches: 0.0491
trigger times: 10
Loss after 3210493 batches: 0.0480
trigger times: 11
Loss after 3211456 batches: 0.0474
trigger times: 12
Loss after 3212419 batches: 0.0468
trigger times: 13
Loss after 3213382 batches: 0.0463
trigger times: 14
Loss after 3214345 batches: 0.0454
trigger times: 15
Loss after 3215308 batches: 0.0442
trigger times: 16
Loss after 3216271 batches: 0.0432
trigger times: 0
Loss after 3217234 batches: 0.0430
trigger times: 1
Loss after 3218197 batches: 0.0420
trigger times: 2
Loss after 3219160 batches: 0.0424
trigger times: 3
Loss after 3220123 batches: 0.0409
trigger times: 4
Loss after 3221086 batches: 0.0408
trigger times: 5
Loss after 3222049 batches: 0.0410
trigger times: 6
Loss after 3223012 batches: 0.0404
trigger times: 7
Loss after 3223975 batches: 0.0398
trigger times: 8
Loss after 3224938 batches: 0.0387
trigger times: 9
Loss after 3225901 batches: 0.0380
trigger times: 10
Loss after 3226864 batches: 0.0382
trigger times: 11
Loss after 3227827 batches: 0.0378
trigger times: 12
Loss after 3228790 batches: 0.0376
trigger times: 13
Loss after 3229753 batches: 0.0378
trigger times: 14
Loss after 3230716 batches: 0.0369
trigger times: 15
Loss after 3231679 batches: 0.0365
trigger times: 16
Loss after 3232642 batches: 0.0369
trigger times: 17
Loss after 3233605 batches: 0.0358
trigger times: 18
Loss after 3234568 batches: 0.0359
trigger times: 19
Loss after 3235531 batches: 0.0364
trigger times: 20
Loss after 3236494 batches: 0.0357
trigger times: 21
Loss after 3237457 batches: 0.0356
trigger times: 22
Loss after 3238420 batches: 0.0349
trigger times: 23
Loss after 3239383 batches: 0.0352
trigger times: 24
Loss after 3240346 batches: 0.0348
trigger times: 25
Early stopping!
Start to test process.
Loss after 3241309 batches: 0.0342
Time to train on one home:  70.96862936019897
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3242272 batches: 0.1373
trigger times: 0
Loss after 3243235 batches: 0.1192
trigger times: 1
Loss after 3244198 batches: 0.1119
trigger times: 2
Loss after 3245161 batches: 0.1055
trigger times: 0
Loss after 3246124 batches: 0.0963
trigger times: 0
Loss after 3247087 batches: 0.0854
trigger times: 0
Loss after 3248050 batches: 0.0794
trigger times: 0
Loss after 3249013 batches: 0.0743
trigger times: 1
Loss after 3249976 batches: 0.0698
trigger times: 0
Loss after 3250939 batches: 0.0651
trigger times: 1
Loss after 3251902 batches: 0.0625
trigger times: 2
Loss after 3252865 batches: 0.0591
trigger times: 3
Loss after 3253828 batches: 0.0578
trigger times: 4
Loss after 3254791 batches: 0.0593
trigger times: 5
Loss after 3255754 batches: 0.0556
trigger times: 6
Loss after 3256717 batches: 0.0531
trigger times: 7
Loss after 3257680 batches: 0.0512
trigger times: 8
Loss after 3258643 batches: 0.0497
trigger times: 9
Loss after 3259606 batches: 0.0484
trigger times: 10
Loss after 3260569 batches: 0.0479
trigger times: 11
Loss after 3261532 batches: 0.0465
trigger times: 12
Loss after 3262495 batches: 0.0464
trigger times: 13
Loss after 3263458 batches: 0.0452
trigger times: 14
Loss after 3264421 batches: 0.0431
trigger times: 15
Loss after 3265384 batches: 0.0449
trigger times: 16
Loss after 3266347 batches: 0.0422
trigger times: 17
Loss after 3267310 batches: 0.0410
trigger times: 18
Loss after 3268273 batches: 0.0417
trigger times: 19
Loss after 3269236 batches: 0.0398
trigger times: 20
Loss after 3270199 batches: 0.0405
trigger times: 21
Loss after 3271162 batches: 0.0376
trigger times: 22
Loss after 3272125 batches: 0.0368
trigger times: 23
Loss after 3273088 batches: 0.0374
trigger times: 24
Loss after 3274051 batches: 0.0372
trigger times: 25
Early stopping!
Start to test process.
Loss after 3275014 batches: 0.0336
Time to train on one home:  68.47441482543945
trigger times: 0
Loss after 3275943 batches: 0.1272
trigger times: 0
Loss after 3276872 batches: 0.0947
trigger times: 0
Loss after 3277801 batches: 0.0796
trigger times: 1
Loss after 3278730 batches: 0.0631
trigger times: 2
Loss after 3279659 batches: 0.0657
trigger times: 0
Loss after 3280588 batches: 0.0667
trigger times: 0
Loss after 3281517 batches: 0.0600
trigger times: 0
Loss after 3282446 batches: 0.0539
trigger times: 0
Loss after 3283375 batches: 0.0514
trigger times: 1
Loss after 3284304 batches: 0.0463
trigger times: 0
Loss after 3285233 batches: 0.0432
trigger times: 0
Loss after 3286162 batches: 0.0438
trigger times: 1
Loss after 3287091 batches: 0.0434
trigger times: 2
Loss after 3288020 batches: 0.0425
trigger times: 3
Loss after 3288949 batches: 0.0414
trigger times: 4
Loss after 3289878 batches: 0.0400
trigger times: 5
Loss after 3290807 batches: 0.0404
trigger times: 6
Loss after 3291736 batches: 0.0396
trigger times: 7
Loss after 3292665 batches: 0.0392
trigger times: 8
Loss after 3293594 batches: 0.0387
trigger times: 9
Loss after 3294523 batches: 0.0387
trigger times: 10
Loss after 3295452 batches: 0.0400
trigger times: 11
Loss after 3296381 batches: 0.0382
trigger times: 12
Loss after 3297310 batches: 0.0362
trigger times: 13
Loss after 3298239 batches: 0.0391
trigger times: 14
Loss after 3299168 batches: 0.0360
trigger times: 15
Loss after 3300097 batches: 0.0345
trigger times: 16
Loss after 3301026 batches: 0.0348
trigger times: 17
Loss after 3301955 batches: 0.0350
trigger times: 18
Loss after 3302884 batches: 0.0333
trigger times: 19
Loss after 3303813 batches: 0.0407
trigger times: 0
Loss after 3304742 batches: 0.0390
trigger times: 1
Loss after 3305671 batches: 0.0381
trigger times: 2
Loss after 3306600 batches: 0.0352
trigger times: 3
Loss after 3307529 batches: 0.0358
trigger times: 4
Loss after 3308458 batches: 0.0344
trigger times: 5
Loss after 3309387 batches: 0.0357
trigger times: 6
Loss after 3310316 batches: 0.0351
trigger times: 7
Loss after 3311245 batches: 0.0335
trigger times: 8
Loss after 3312174 batches: 0.0367
trigger times: 9
Loss after 3313103 batches: 0.0469
trigger times: 10
Loss after 3314032 batches: 0.0406
trigger times: 11
Loss after 3314961 batches: 0.0415
trigger times: 12
Loss after 3315890 batches: 0.0382
trigger times: 13
Loss after 3316819 batches: 0.0383
trigger times: 14
Loss after 3317748 batches: 0.0365
trigger times: 15
Loss after 3318677 batches: 0.0357
trigger times: 16
Loss after 3319606 batches: 0.0369
trigger times: 17
Loss after 3320535 batches: 0.0343
trigger times: 18
Loss after 3321464 batches: 0.0358
trigger times: 19
Loss after 3322393 batches: 0.0346
trigger times: 20
Loss after 3323322 batches: 0.0347
trigger times: 21
Loss after 3324251 batches: 0.0363
trigger times: 22
Loss after 3325180 batches: 0.0356
trigger times: 23
Loss after 3326109 batches: 0.0369
trigger times: 24
Loss after 3327038 batches: 0.0353
trigger times: 25
Early stopping!
Start to test process.
Loss after 3327967 batches: 0.0328
Time to train on one home:  80.06086349487305
trigger times: 0
Loss after 3328929 batches: 0.0717
trigger times: 1
Loss after 3329891 batches: 0.0713
trigger times: 2
Loss after 3330853 batches: 0.0703
trigger times: 3
Loss after 3331815 batches: 0.0696
trigger times: 4
Loss after 3332777 batches: 0.0694
trigger times: 5
Loss after 3333739 batches: 0.0691
trigger times: 6
Loss after 3334701 batches: 0.0690
trigger times: 7
Loss after 3335663 batches: 0.0689
trigger times: 8
Loss after 3336625 batches: 0.0682
trigger times: 9
Loss after 3337587 batches: 0.0675
trigger times: 10
Loss after 3338549 batches: 0.0676
trigger times: 11
Loss after 3339511 batches: 0.0664
trigger times: 12
Loss after 3340473 batches: 0.0648
trigger times: 13
Loss after 3341435 batches: 0.0653
trigger times: 14
Loss after 3342397 batches: 0.0640
trigger times: 0
Loss after 3343359 batches: 0.0640
trigger times: 0
Loss after 3344321 batches: 0.0643
trigger times: 1
Loss after 3345283 batches: 0.0643
trigger times: 2
Loss after 3346245 batches: 0.0641
trigger times: 3
Loss after 3347207 batches: 0.0639
trigger times: 4
Loss after 3348169 batches: 0.0625
trigger times: 5
Loss after 3349131 batches: 0.0629
trigger times: 6
Loss after 3350093 batches: 0.0625
trigger times: 7
Loss after 3351055 batches: 0.0623
trigger times: 8
Loss after 3352017 batches: 0.0619
trigger times: 9
Loss after 3352979 batches: 0.0618
trigger times: 10
Loss after 3353941 batches: 0.0618
trigger times: 11
Loss after 3354903 batches: 0.0610
trigger times: 12
Loss after 3355865 batches: 0.0613
trigger times: 13
Loss after 3356827 batches: 0.0619
trigger times: 14
Loss after 3357789 batches: 0.0618
trigger times: 15
Loss after 3358751 batches: 0.0611
trigger times: 16
Loss after 3359713 batches: 0.0608
trigger times: 17
Loss after 3360675 batches: 0.0616
trigger times: 18
Loss after 3361637 batches: 0.0613
trigger times: 19
Loss after 3362599 batches: 0.0607
trigger times: 20
Loss after 3363561 batches: 0.0610
trigger times: 21
Loss after 3364523 batches: 0.0606
trigger times: 22
Loss after 3365485 batches: 0.0615
trigger times: 23
Loss after 3366447 batches: 0.0607
trigger times: 24
Loss after 3367409 batches: 0.0605
trigger times: 25
Early stopping!
Start to test process.
Loss after 3368371 batches: 0.0600
Time to train on one home:  69.83592176437378
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3369334 batches: 0.0625
trigger times: 1
Loss after 3370297 batches: 0.0586
trigger times: 2
Loss after 3371260 batches: 0.0571
trigger times: 0
Loss after 3372223 batches: 0.0551
trigger times: 1
Loss after 3373186 batches: 0.0536
trigger times: 2
Loss after 3374149 batches: 0.0519
trigger times: 3
Loss after 3375112 batches: 0.0515
trigger times: 0
Loss after 3376075 batches: 0.0510
trigger times: 0
Loss after 3377038 batches: 0.0501
trigger times: 0
Loss after 3378001 batches: 0.0495
trigger times: 0
Loss after 3378964 batches: 0.0490
trigger times: 0
Loss after 3379927 batches: 0.0476
trigger times: 1
Loss after 3380890 batches: 0.0472
trigger times: 0
Loss after 3381853 batches: 0.0462
trigger times: 0
Loss after 3382816 batches: 0.0458
trigger times: 0
Loss after 3383779 batches: 0.0454
trigger times: 1
Loss after 3384742 batches: 0.0452
trigger times: 2
Loss after 3385705 batches: 0.0437
trigger times: 3
Loss after 3386668 batches: 0.0440
trigger times: 4
Loss after 3387631 batches: 0.0437
trigger times: 5
Loss after 3388594 batches: 0.0430
trigger times: 6
Loss after 3389557 batches: 0.0430
trigger times: 7
Loss after 3390520 batches: 0.0421
trigger times: 8
Loss after 3391483 batches: 0.0422
trigger times: 9
Loss after 3392446 batches: 0.0421
trigger times: 10
Loss after 3393409 batches: 0.0418
trigger times: 11
Loss after 3394372 batches: 0.0419
trigger times: 12
Loss after 3395335 batches: 0.0410
trigger times: 13
Loss after 3396298 batches: 0.0412
trigger times: 14
Loss after 3397261 batches: 0.0403
trigger times: 15
Loss after 3398224 batches: 0.0404
trigger times: 16
Loss after 3399187 batches: 0.0401
trigger times: 17
Loss after 3400150 batches: 0.0402
trigger times: 18
Loss after 3401113 batches: 0.0395
trigger times: 19
Loss after 3402076 batches: 0.0392
trigger times: 20
Loss after 3403039 batches: 0.0390
trigger times: 21
Loss after 3404002 batches: 0.0394
trigger times: 22
Loss after 3404965 batches: 0.0389
trigger times: 23
Loss after 3405928 batches: 0.0385
trigger times: 24
Loss after 3406891 batches: 0.0393
trigger times: 25
Early stopping!
Start to test process.
Loss after 3407854 batches: 0.0389
Time to train on one home:  67.75150680541992
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3408817 batches: 0.0814
trigger times: 0
Loss after 3409780 batches: 0.0781
trigger times: 1
Loss after 3410743 batches: 0.0771
trigger times: 0
Loss after 3411706 batches: 0.0757
trigger times: 1
Loss after 3412669 batches: 0.0738
trigger times: 0
Loss after 3413632 batches: 0.0715
trigger times: 0
Loss after 3414595 batches: 0.0691
trigger times: 1
Loss after 3415558 batches: 0.0669
trigger times: 0
Loss after 3416521 batches: 0.0615
trigger times: 0
Loss after 3417484 batches: 0.0592
trigger times: 0
Loss after 3418447 batches: 0.0581
trigger times: 0
Loss after 3419410 batches: 0.0568
trigger times: 1
Loss after 3420373 batches: 0.0565
trigger times: 2
Loss after 3421336 batches: 0.0556
trigger times: 3
Loss after 3422299 batches: 0.0543
trigger times: 4
Loss after 3423262 batches: 0.0541
trigger times: 5
Loss after 3424225 batches: 0.0545
trigger times: 0
Loss after 3425188 batches: 0.0534
trigger times: 1
Loss after 3426151 batches: 0.0517
trigger times: 2
Loss after 3427114 batches: 0.0513
trigger times: 3
Loss after 3428077 batches: 0.0510
trigger times: 4
Loss after 3429040 batches: 0.0503
trigger times: 0
Loss after 3430003 batches: 0.0504
trigger times: 1
Loss after 3430966 batches: 0.0495
trigger times: 2
Loss after 3431929 batches: 0.0497
trigger times: 3
Loss after 3432892 batches: 0.0493
trigger times: 4
Loss after 3433855 batches: 0.0487
trigger times: 5
Loss after 3434818 batches: 0.0484
trigger times: 6
Loss after 3435781 batches: 0.0486
trigger times: 7
Loss after 3436744 batches: 0.0475
trigger times: 0
Loss after 3437707 batches: 0.0477
trigger times: 1
Loss after 3438670 batches: 0.0464
trigger times: 2
Loss after 3439633 batches: 0.0470
trigger times: 3
Loss after 3440596 batches: 0.0462
trigger times: 4
Loss after 3441559 batches: 0.0467
trigger times: 5
Loss after 3442522 batches: 0.0463
trigger times: 6
Loss after 3443485 batches: 0.0453
trigger times: 7
Loss after 3444448 batches: 0.0449
trigger times: 8
Loss after 3445411 batches: 0.0446
trigger times: 9
Loss after 3446374 batches: 0.0451
trigger times: 10
Loss after 3447337 batches: 0.0446
trigger times: 11
Loss after 3448300 batches: 0.0433
trigger times: 12
Loss after 3449263 batches: 0.0438
trigger times: 13
Loss after 3450226 batches: 0.0435
trigger times: 14
Loss after 3451189 batches: 0.0444
trigger times: 15
Loss after 3452152 batches: 0.0435
trigger times: 16
Loss after 3453115 batches: 0.0425
trigger times: 17
Loss after 3454078 batches: 0.0424
trigger times: 18
Loss after 3455041 batches: 0.0430
trigger times: 19
Loss after 3456004 batches: 0.0431
trigger times: 20
Loss after 3456967 batches: 0.0429
trigger times: 21
Loss after 3457930 batches: 0.0439
trigger times: 22
Loss after 3458893 batches: 0.0421
trigger times: 23
Loss after 3459856 batches: 0.0415
trigger times: 24
Loss after 3460819 batches: 0.0417
trigger times: 25
Early stopping!
Start to test process.
Loss after 3461782 batches: 0.0411
Time to train on one home:  77.43660092353821
trigger times: 0
Loss after 3462745 batches: 0.1374
trigger times: 0
Loss after 3463708 batches: 0.1327
trigger times: 1
Loss after 3464671 batches: 0.1321
trigger times: 2
Loss after 3465634 batches: 0.1306
trigger times: 3
Loss after 3466597 batches: 0.1292
trigger times: 4
Loss after 3467560 batches: 0.1277
trigger times: 5
Loss after 3468523 batches: 0.1251
trigger times: 0
Loss after 3469486 batches: 0.1209
trigger times: 1
Loss after 3470449 batches: 0.1192
trigger times: 2
Loss after 3471412 batches: 0.1169
trigger times: 3
Loss after 3472375 batches: 0.1141
trigger times: 4
Loss after 3473338 batches: 0.1129
trigger times: 0
Loss after 3474301 batches: 0.1114
trigger times: 1
Loss after 3475264 batches: 0.1144
trigger times: 2
Loss after 3476227 batches: 0.1110
trigger times: 3
Loss after 3477190 batches: 0.1090
trigger times: 4
Loss after 3478153 batches: 0.1072
trigger times: 5
Loss after 3479116 batches: 0.1051
trigger times: 6
Loss after 3480079 batches: 0.1048
trigger times: 7
Loss after 3481042 batches: 0.1021
trigger times: 8
Loss after 3482005 batches: 0.1028
trigger times: 9
Loss after 3482968 batches: 0.1000
trigger times: 10
Loss after 3483931 batches: 0.0980
trigger times: 0
Loss after 3484894 batches: 0.0998
trigger times: 1
Loss after 3485857 batches: 0.0993
trigger times: 2
Loss after 3486820 batches: 0.0977
trigger times: 3
Loss after 3487783 batches: 0.0983
trigger times: 4
Loss after 3488746 batches: 0.0977
trigger times: 5
Loss after 3489709 batches: 0.0961
trigger times: 6
Loss after 3490672 batches: 0.0977
trigger times: 7
Loss after 3491635 batches: 0.0959
trigger times: 8
Loss after 3492598 batches: 0.0947
trigger times: 9
Loss after 3493561 batches: 0.0944
trigger times: 10
Loss after 3494524 batches: 0.0954
trigger times: 11
Loss after 3495487 batches: 0.0959
trigger times: 12
Loss after 3496450 batches: 0.0932
trigger times: 13
Loss after 3497413 batches: 0.0935
trigger times: 14
Loss after 3498376 batches: 0.0902
trigger times: 15
Loss after 3499339 batches: 0.0895
trigger times: 16
Loss after 3500302 batches: 0.0910
trigger times: 17
Loss after 3501265 batches: 0.0910
trigger times: 18
Loss after 3502228 batches: 0.0883
trigger times: 19
Loss after 3503191 batches: 0.0888
trigger times: 20
Loss after 3504154 batches: 0.0884
trigger times: 21
Loss after 3505117 batches: 0.0866
trigger times: 22
Loss after 3506080 batches: 0.0854
trigger times: 23
Loss after 3507043 batches: 0.0847
trigger times: 24
Loss after 3508006 batches: 0.0867
trigger times: 25
Early stopping!
Start to test process.
Loss after 3508969 batches: 0.0858
Time to train on one home:  75.33022880554199
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3509932 batches: 0.1093
trigger times: 1
Loss after 3510895 batches: 0.0857
trigger times: 2
Loss after 3511858 batches: 0.0841
trigger times: 3
Loss after 3512821 batches: 0.0823
trigger times: 0
Loss after 3513784 batches: 0.0813
trigger times: 0
Loss after 3514747 batches: 0.0800
trigger times: 1
Loss after 3515710 batches: 0.0799
trigger times: 2
Loss after 3516673 batches: 0.0796
trigger times: 3
Loss after 3517636 batches: 0.0792
trigger times: 4
Loss after 3518599 batches: 0.0785
trigger times: 5
Loss after 3519562 batches: 0.0775
trigger times: 6
Loss after 3520525 batches: 0.0774
trigger times: 7
Loss after 3521488 batches: 0.0775
trigger times: 8
Loss after 3522451 batches: 0.0761
trigger times: 9
Loss after 3523414 batches: 0.0754
trigger times: 0
Loss after 3524377 batches: 0.0724
trigger times: 1
Loss after 3525340 batches: 0.0694
trigger times: 2
Loss after 3526303 batches: 0.0668
trigger times: 0
Loss after 3527266 batches: 0.0649
trigger times: 1
Loss after 3528229 batches: 0.0618
trigger times: 2
Loss after 3529192 batches: 0.0587
trigger times: 3
Loss after 3530155 batches: 0.0576
trigger times: 4
Loss after 3531118 batches: 0.0562
trigger times: 5
Loss after 3532081 batches: 0.0542
trigger times: 6
Loss after 3533044 batches: 0.0533
trigger times: 7
Loss after 3534007 batches: 0.0524
trigger times: 8
Loss after 3534970 batches: 0.0505
trigger times: 9
Loss after 3535933 batches: 0.0511
trigger times: 10
Loss after 3536896 batches: 0.0502
trigger times: 11
Loss after 3537859 batches: 0.0492
trigger times: 0
Loss after 3538822 batches: 0.0486
trigger times: 1
Loss after 3539785 batches: 0.0479
trigger times: 2
Loss after 3540748 batches: 0.0478
trigger times: 3
Loss after 3541711 batches: 0.0471
trigger times: 4
Loss after 3542674 batches: 0.0459
trigger times: 5
Loss after 3543637 batches: 0.0453
trigger times: 6
Loss after 3544600 batches: 0.0454
trigger times: 7
Loss after 3545563 batches: 0.0434
trigger times: 8
Loss after 3546526 batches: 0.0429
trigger times: 9
Loss after 3547489 batches: 0.0425
trigger times: 10
Loss after 3548452 batches: 0.0444
trigger times: 11
Loss after 3549415 batches: 0.0432
trigger times: 12
Loss after 3550378 batches: 0.0407
trigger times: 13
Loss after 3551341 batches: 0.0403
trigger times: 14
Loss after 3552304 batches: 0.0401
trigger times: 15
Loss after 3553267 batches: 0.0398
trigger times: 0
Loss after 3554230 batches: 0.0407
trigger times: 1
Loss after 3555193 batches: 0.0392
trigger times: 2
Loss after 3556156 batches: 0.0377
trigger times: 3
Loss after 3557119 batches: 0.0370
trigger times: 4
Loss after 3558082 batches: 0.0365
trigger times: 5
Loss after 3559045 batches: 0.0369
trigger times: 6
Loss after 3560008 batches: 0.0383
trigger times: 7
Loss after 3560971 batches: 0.0365
trigger times: 8
Loss after 3561934 batches: 0.0368
trigger times: 9
Loss after 3562897 batches: 0.0364
trigger times: 10
Loss after 3563860 batches: 0.0366
trigger times: 0
Loss after 3564823 batches: 0.0362
trigger times: 1
Loss after 3565786 batches: 0.0350
trigger times: 2
Loss after 3566749 batches: 0.0339
trigger times: 3
Loss after 3567712 batches: 0.0351
trigger times: 4
Loss after 3568675 batches: 0.0345
trigger times: 5
Loss after 3569638 batches: 0.0342
trigger times: 6
Loss after 3570601 batches: 0.0356
trigger times: 7
Loss after 3571564 batches: 0.0337
trigger times: 8
Loss after 3572527 batches: 0.0336
trigger times: 9
Loss after 3573490 batches: 0.0333
trigger times: 10
Loss after 3574453 batches: 0.0313
trigger times: 11
Loss after 3575416 batches: 0.0327
trigger times: 12
Loss after 3576379 batches: 0.0333
trigger times: 0
Loss after 3577342 batches: 0.0320
trigger times: 1
Loss after 3578305 batches: 0.0322
trigger times: 2
Loss after 3579268 batches: 0.0329
trigger times: 3
Loss after 3580231 batches: 0.0319
trigger times: 4
Loss after 3581194 batches: 0.0323
trigger times: 5
Loss after 3582157 batches: 0.0311
trigger times: 6
Loss after 3583120 batches: 0.0304
trigger times: 7
Loss after 3584083 batches: 0.0303
trigger times: 8
Loss after 3585046 batches: 0.0307
trigger times: 9
Loss after 3586009 batches: 0.0306
trigger times: 10
Loss after 3586972 batches: 0.0315
trigger times: 11
Loss after 3587935 batches: 0.0303
trigger times: 12
Loss after 3588898 batches: 0.0306
trigger times: 13
Loss after 3589861 batches: 0.0304
trigger times: 14
Loss after 3590824 batches: 0.0285
trigger times: 15
Loss after 3591787 batches: 0.0301
trigger times: 16
Loss after 3592750 batches: 0.0307
trigger times: 17
Loss after 3593713 batches: 0.0306
trigger times: 18
Loss after 3594676 batches: 0.0282
trigger times: 19
Loss after 3595639 batches: 0.0296
trigger times: 20
Loss after 3596602 batches: 0.0284
trigger times: 21
Loss after 3597565 batches: 0.0279
trigger times: 22
Loss after 3598528 batches: 0.0275
trigger times: 23
Loss after 3599491 batches: 0.0282
trigger times: 24
Loss after 3600454 batches: 0.0268
trigger times: 25
Early stopping!
Start to test process.
Loss after 3601417 batches: 0.0275
Time to train on one home:  116.45282435417175
trigger times: 0
Loss after 3602376 batches: 0.1252
trigger times: 0
Loss after 3603335 batches: 0.1023
trigger times: 0
Loss after 3604294 batches: 0.0862
trigger times: 1
Loss after 3605253 batches: 0.0618
trigger times: 0
Loss after 3606212 batches: 0.0533
trigger times: 0
Loss after 3607171 batches: 0.0466
trigger times: 0
Loss after 3608130 batches: 0.0428
trigger times: 1
Loss after 3609089 batches: 0.0398
trigger times: 2
Loss after 3610048 batches: 0.0370
trigger times: 3
Loss after 3611007 batches: 0.0350
trigger times: 4
Loss after 3611966 batches: 0.0336
trigger times: 0
Loss after 3612925 batches: 0.0335
trigger times: 1
Loss after 3613884 batches: 0.0319
trigger times: 2
Loss after 3614843 batches: 0.0313
trigger times: 3
Loss after 3615802 batches: 0.0313
trigger times: 4
Loss after 3616761 batches: 0.0304
trigger times: 5
Loss after 3617720 batches: 0.0315
trigger times: 6
Loss after 3618679 batches: 0.0307
trigger times: 7
Loss after 3619638 batches: 0.0295
trigger times: 8
Loss after 3620597 batches: 0.0292
trigger times: 9
Loss after 3621556 batches: 0.0285
trigger times: 10
Loss after 3622515 batches: 0.0279
trigger times: 11
Loss after 3623474 batches: 0.0282
trigger times: 12
Loss after 3624433 batches: 0.0278
trigger times: 0
Loss after 3625392 batches: 0.0272
trigger times: 1
Loss after 3626351 batches: 0.0260
trigger times: 2
Loss after 3627310 batches: 0.0262
trigger times: 3
Loss after 3628269 batches: 0.0265
trigger times: 4
Loss after 3629228 batches: 0.0262
trigger times: 5
Loss after 3630187 batches: 0.0247
trigger times: 6
Loss after 3631146 batches: 0.0257
trigger times: 7
Loss after 3632105 batches: 0.0247
trigger times: 8
Loss after 3633064 batches: 0.0246
trigger times: 9
Loss after 3634023 batches: 0.0237
trigger times: 10
Loss after 3634982 batches: 0.0277
trigger times: 11
Loss after 3635941 batches: 0.0270
trigger times: 12
Loss after 3636900 batches: 0.0260
trigger times: 13
Loss after 3637859 batches: 0.0245
trigger times: 14
Loss after 3638818 batches: 0.0245
trigger times: 15
Loss after 3639777 batches: 0.0237
trigger times: 16
Loss after 3640736 batches: 0.0237
trigger times: 17
Loss after 3641695 batches: 0.0235
trigger times: 18
Loss after 3642654 batches: 0.0227
trigger times: 19
Loss after 3643613 batches: 0.0226
trigger times: 20
Loss after 3644572 batches: 0.0222
trigger times: 21
Loss after 3645531 batches: 0.0224
trigger times: 22
Loss after 3646490 batches: 0.0220
trigger times: 23
Loss after 3647449 batches: 0.0215
trigger times: 24
Loss after 3648408 batches: 0.0220
trigger times: 25
Early stopping!
Start to test process.
Loss after 3649367 batches: 0.0219
Time to train on one home:  75.83601880073547
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3650330 batches: 0.0393
trigger times: 1
Loss after 3651293 batches: 0.0280
trigger times: 2
Loss after 3652256 batches: 0.0286
trigger times: 3
Loss after 3653219 batches: 0.0283
trigger times: 4
Loss after 3654182 batches: 0.0277
trigger times: 5
Loss after 3655145 batches: 0.0274
trigger times: 6
Loss after 3656108 batches: 0.0270
trigger times: 7
Loss after 3657071 batches: 0.0268
trigger times: 8
Loss after 3658034 batches: 0.0263
trigger times: 9
Loss after 3658997 batches: 0.0263
trigger times: 10
Loss after 3659960 batches: 0.0260
trigger times: 11
Loss after 3660923 batches: 0.0258
trigger times: 12
Loss after 3661886 batches: 0.0255
trigger times: 13
Loss after 3662849 batches: 0.0255
trigger times: 14
Loss after 3663812 batches: 0.0254
trigger times: 15
Loss after 3664775 batches: 0.0252
trigger times: 16
Loss after 3665738 batches: 0.0252
trigger times: 17
Loss after 3666701 batches: 0.0250
trigger times: 18
Loss after 3667664 batches: 0.0247
trigger times: 19
Loss after 3668627 batches: 0.0245
trigger times: 20
Loss after 3669590 batches: 0.0244
trigger times: 21
Loss after 3670553 batches: 0.0243
trigger times: 22
Loss after 3671516 batches: 0.0240
trigger times: 23
Loss after 3672479 batches: 0.0238
trigger times: 24
Loss after 3673442 batches: 0.0240
trigger times: 25
Early stopping!
Start to test process.
Loss after 3674405 batches: 0.0236
Time to train on one home:  56.64090180397034
trigger times: 0
Loss after 3675350 batches: 0.0922
trigger times: 0
Loss after 3676295 batches: 0.0817
trigger times: 1
Loss after 3677240 batches: 0.0787
trigger times: 2
Loss after 3678185 batches: 0.0748
trigger times: 0
Loss after 3679130 batches: 0.0696
trigger times: 0
Loss after 3680075 batches: 0.0696
trigger times: 0
Loss after 3681020 batches: 0.0652
trigger times: 0
Loss after 3681965 batches: 0.0572
trigger times: 0
Loss after 3682910 batches: 0.0511
trigger times: 1
Loss after 3683855 batches: 0.0503
trigger times: 2
Loss after 3684800 batches: 0.0457
trigger times: 3
Loss after 3685745 batches: 0.0441
trigger times: 4
Loss after 3686690 batches: 0.0440
trigger times: 5
Loss after 3687635 batches: 0.0425
trigger times: 6
Loss after 3688580 batches: 0.0436
trigger times: 7
Loss after 3689525 batches: 0.0434
trigger times: 8
Loss after 3690470 batches: 0.0442
trigger times: 9
Loss after 3691415 batches: 0.0415
trigger times: 10
Loss after 3692360 batches: 0.0395
trigger times: 11
Loss after 3693305 batches: 0.0379
trigger times: 12
Loss after 3694250 batches: 0.0373
trigger times: 13
Loss after 3695195 batches: 0.0376
trigger times: 0
Loss after 3696140 batches: 0.0379
trigger times: 1
Loss after 3697085 batches: 0.0396
trigger times: 2
Loss after 3698030 batches: 0.0381
trigger times: 3
Loss after 3698975 batches: 0.0354
trigger times: 4
Loss after 3699920 batches: 0.0347
trigger times: 5
Loss after 3700865 batches: 0.0352
trigger times: 6
Loss after 3701810 batches: 0.0346
trigger times: 7
Loss after 3702755 batches: 0.0343
trigger times: 8
Loss after 3703700 batches: 0.0335
trigger times: 9
Loss after 3704645 batches: 0.0326
trigger times: 10
Loss after 3705590 batches: 0.0324
trigger times: 11
Loss after 3706535 batches: 0.0317
trigger times: 12
Loss after 3707480 batches: 0.0320
trigger times: 13
Loss after 3708425 batches: 0.0315
trigger times: 14
Loss after 3709370 batches: 0.0309
trigger times: 15
Loss after 3710315 batches: 0.0301
trigger times: 16
Loss after 3711260 batches: 0.0303
trigger times: 17
Loss after 3712205 batches: 0.0286
trigger times: 18
Loss after 3713150 batches: 0.0309
trigger times: 19
Loss after 3714095 batches: 0.0292
trigger times: 20
Loss after 3715040 batches: 0.0287
trigger times: 21
Loss after 3715985 batches: 0.0284
trigger times: 22
Loss after 3716930 batches: 0.0294
trigger times: 23
Loss after 3717875 batches: 0.0301
trigger times: 24
Loss after 3718820 batches: 0.0286
trigger times: 25
Early stopping!
Start to test process.
Loss after 3719765 batches: 0.0286
Time to train on one home:  72.12052202224731
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3720702 batches: 0.0961
trigger times: 0
Loss after 3721639 batches: 0.0914
trigger times: 1
Loss after 3722576 batches: 0.0919
trigger times: 2
Loss after 3723513 batches: 0.0897
trigger times: 3
Loss after 3724450 batches: 0.0875
trigger times: 4
Loss after 3725387 batches: 0.0854
trigger times: 0
Loss after 3726324 batches: 0.0829
trigger times: 0
Loss after 3727261 batches: 0.0814
trigger times: 1
Loss after 3728198 batches: 0.0806
trigger times: 0
Loss after 3729135 batches: 0.0789
trigger times: 0
Loss after 3730072 batches: 0.0767
trigger times: 1
Loss after 3731009 batches: 0.0740
trigger times: 2
Loss after 3731946 batches: 0.0733
trigger times: 3
Loss after 3732883 batches: 0.0708
trigger times: 4
Loss after 3733820 batches: 0.0698
trigger times: 5
Loss after 3734757 batches: 0.0690
trigger times: 6
Loss after 3735694 batches: 0.0705
trigger times: 7
Loss after 3736631 batches: 0.0707
trigger times: 8
Loss after 3737568 batches: 0.0694
trigger times: 9
Loss after 3738505 batches: 0.0672
trigger times: 10
Loss after 3739442 batches: 0.0675
trigger times: 11
Loss after 3740379 batches: 0.0663
trigger times: 12
Loss after 3741316 batches: 0.0662
trigger times: 13
Loss after 3742253 batches: 0.0654
trigger times: 14
Loss after 3743190 batches: 0.0648
trigger times: 15
Loss after 3744127 batches: 0.0660
trigger times: 16
Loss after 3745064 batches: 0.0629
trigger times: 17
Loss after 3746001 batches: 0.0634
trigger times: 18
Loss after 3746938 batches: 0.0636
trigger times: 19
Loss after 3747875 batches: 0.0626
trigger times: 20
Loss after 3748812 batches: 0.0623
trigger times: 21
Loss after 3749749 batches: 0.0610
trigger times: 22
Loss after 3750686 batches: 0.0613
trigger times: 23
Loss after 3751623 batches: 0.0606
trigger times: 24
Loss after 3752560 batches: 0.0611
trigger times: 25
Early stopping!
Start to test process.
Loss after 3753497 batches: 0.0624
Time to train on one home:  62.671281814575195
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 3754460 batches: 0.0247
trigger times: 1
Loss after 3755423 batches: 0.0164
trigger times: 2
Loss after 3756386 batches: 0.0154
trigger times: 3
Loss after 3757349 batches: 0.0151
trigger times: 4
Loss after 3758312 batches: 0.0146
trigger times: 5
Loss after 3759275 batches: 0.0142
trigger times: 6
Loss after 3760238 batches: 0.0141
trigger times: 7
Loss after 3761201 batches: 0.0139
trigger times: 8
Loss after 3762164 batches: 0.0140
trigger times: 9
Loss after 3763127 batches: 0.0141
trigger times: 10
Loss after 3764090 batches: 0.0139
trigger times: 11
Loss after 3765053 batches: 0.0139
trigger times: 12
Loss after 3766016 batches: 0.0141
trigger times: 13
Loss after 3766979 batches: 0.0138
trigger times: 14
Loss after 3767942 batches: 0.0137
trigger times: 15
Loss after 3768905 batches: 0.0139
trigger times: 16
Loss after 3769868 batches: 0.0138
trigger times: 17
Loss after 3770831 batches: 0.0140
trigger times: 18
Loss after 3771794 batches: 0.0137
trigger times: 19
Loss after 3772757 batches: 0.0138
trigger times: 20
Loss after 3773720 batches: 0.0138
trigger times: 21
Loss after 3774683 batches: 0.0139
trigger times: 22
Loss after 3775646 batches: 0.0139
trigger times: 23
Loss after 3776609 batches: 0.0140
trigger times: 24
Loss after 3777572 batches: 0.0142
trigger times: 25
Early stopping!
Start to test process.
Loss after 3778535 batches: 0.0139
Time to train on one home:  56.57604622840881
trigger times: 0
Loss after 3779498 batches: 0.0943
trigger times: 1
Loss after 3780461 batches: 0.0901
trigger times: 2
Loss after 3781424 batches: 0.0902
trigger times: 3
Loss after 3782387 batches: 0.0883
trigger times: 4
Loss after 3783350 batches: 0.0872
trigger times: 5
Loss after 3784313 batches: 0.0860
trigger times: 6
Loss after 3785276 batches: 0.0859
trigger times: 7
Loss after 3786239 batches: 0.0851
trigger times: 8
Loss after 3787202 batches: 0.0829
trigger times: 9
Loss after 3788165 batches: 0.0813
trigger times: 10
Loss after 3789128 batches: 0.0804
trigger times: 11
Loss after 3790091 batches: 0.0788
trigger times: 12
Loss after 3791054 batches: 0.0762
trigger times: 13
Loss after 3792017 batches: 0.0760
trigger times: 14
Loss after 3792980 batches: 0.0751
trigger times: 15
Loss after 3793943 batches: 0.0734
trigger times: 16
Loss after 3794906 batches: 0.0730
trigger times: 17
Loss after 3795869 batches: 0.0723
trigger times: 18
Loss after 3796832 batches: 0.0720
trigger times: 19
Loss after 3797795 batches: 0.0712
trigger times: 20
Loss after 3798758 batches: 0.0707
trigger times: 21
Loss after 3799721 batches: 0.0702
trigger times: 22
Loss after 3800684 batches: 0.0703
trigger times: 23
Loss after 3801647 batches: 0.0692
trigger times: 24
Loss after 3802610 batches: 0.0694
trigger times: 25
Early stopping!
Start to test process.
Loss after 3803573 batches: 0.0680
Time to train on one home:  55.83823609352112
trigger times: 0
Loss after 3804536 batches: 0.0935
trigger times: 1
Loss after 3805499 batches: 0.0860
trigger times: 0
Loss after 3806462 batches: 0.0854
trigger times: 1
Loss after 3807425 batches: 0.0812
trigger times: 2
Loss after 3808388 batches: 0.0763
trigger times: 3
Loss after 3809351 batches: 0.0733
trigger times: 4
Loss after 3810314 batches: 0.0703
trigger times: 5
Loss after 3811277 batches: 0.0673
trigger times: 6
Loss after 3812240 batches: 0.0648
trigger times: 7
Loss after 3813203 batches: 0.0639
trigger times: 8
Loss after 3814166 batches: 0.0624
trigger times: 9
Loss after 3815129 batches: 0.0624
trigger times: 10
Loss after 3816092 batches: 0.0609
trigger times: 11
Loss after 3817055 batches: 0.0595
trigger times: 12
Loss after 3818018 batches: 0.0573
trigger times: 13
Loss after 3818981 batches: 0.0558
trigger times: 14
Loss after 3819944 batches: 0.0551
trigger times: 15
Loss after 3820907 batches: 0.0535
trigger times: 16
Loss after 3821870 batches: 0.0517
trigger times: 17
Loss after 3822833 batches: 0.0514
trigger times: 18
Loss after 3823796 batches: 0.0514
trigger times: 19
Loss after 3824759 batches: 0.0508
trigger times: 20
Loss after 3825722 batches: 0.0493
trigger times: 21
Loss after 3826685 batches: 0.0491
trigger times: 22
Loss after 3827648 batches: 0.0491
trigger times: 23
Loss after 3828611 batches: 0.0480
trigger times: 24
Loss after 3829574 batches: 0.0475
trigger times: 25
Early stopping!
Start to test process.
Loss after 3830537 batches: 0.0475
Time to train on one home:  59.615065574645996
trigger times: 0
Loss after 3831433 batches: 0.1026
trigger times: 0
Loss after 3832329 batches: 0.1008
trigger times: 1
Loss after 3833225 batches: 0.0985
trigger times: 0
Loss after 3834121 batches: 0.0970
trigger times: 1
Loss after 3835017 batches: 0.0951
trigger times: 2
Loss after 3835913 batches: 0.0943
trigger times: 3
Loss after 3836809 batches: 0.0930
trigger times: 4
Loss after 3837705 batches: 0.0930
trigger times: 5
Loss after 3838601 batches: 0.0927
trigger times: 6
Loss after 3839497 batches: 0.0901
trigger times: 7
Loss after 3840393 batches: 0.0893
trigger times: 8
Loss after 3841289 batches: 0.0882
trigger times: 9
Loss after 3842185 batches: 0.0856
trigger times: 10
Loss after 3843081 batches: 0.0845
trigger times: 11
Loss after 3843977 batches: 0.0837
trigger times: 12
Loss after 3844873 batches: 0.0818
trigger times: 13
Loss after 3845769 batches: 0.0819
trigger times: 14
Loss after 3846665 batches: 0.0828
trigger times: 15
Loss after 3847561 batches: 0.0845
trigger times: 16
Loss after 3848457 batches: 0.0815
trigger times: 17
Loss after 3849353 batches: 0.0798
trigger times: 18
Loss after 3850249 batches: 0.0793
trigger times: 19
Loss after 3851145 batches: 0.0794
trigger times: 20
Loss after 3852041 batches: 0.0800
trigger times: 21
Loss after 3852937 batches: 0.0789
trigger times: 22
Loss after 3853833 batches: 0.0792
trigger times: 23
Loss after 3854729 batches: 0.0778
trigger times: 24
Loss after 3855625 batches: 0.0770
trigger times: 25
Early stopping!
Start to test process.
Loss after 3856521 batches: 0.0783
Time to train on one home:  57.07681751251221
trigger times: 0
Loss after 3857484 batches: 0.1421
trigger times: 1
Loss after 3858447 batches: 0.1119
trigger times: 2
Loss after 3859410 batches: 0.1139
trigger times: 3
Loss after 3860373 batches: 0.1069
trigger times: 4
Loss after 3861336 batches: 0.0985
trigger times: 5
Loss after 3862299 batches: 0.0927
trigger times: 6
Loss after 3863262 batches: 0.0909
trigger times: 7
Loss after 3864225 batches: 0.0879
trigger times: 8
Loss after 3865188 batches: 0.0859
trigger times: 9
Loss after 3866151 batches: 0.0846
trigger times: 10
Loss after 3867114 batches: 0.0847
trigger times: 11
Loss after 3868077 batches: 0.0840
trigger times: 12
Loss after 3869040 batches: 0.0827
trigger times: 13
Loss after 3870003 batches: 0.0812
trigger times: 14
Loss after 3870966 batches: 0.0797
trigger times: 15
Loss after 3871929 batches: 0.0777
trigger times: 16
Loss after 3872892 batches: 0.0773
trigger times: 17
Loss after 3873855 batches: 0.0759
trigger times: 18
Loss after 3874818 batches: 0.0753
trigger times: 19
Loss after 3875781 batches: 0.0760
trigger times: 20
Loss after 3876744 batches: 0.0744
trigger times: 21
Loss after 3877707 batches: 0.0726
trigger times: 22
Loss after 3878670 batches: 0.0719
trigger times: 23
Loss after 3879633 batches: 0.0705
trigger times: 24
Loss after 3880596 batches: 0.0675
trigger times: 25
Early stopping!
Start to test process.
Loss after 3881559 batches: 0.0699
Time to train on one home:  56.84483218193054
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3882522 batches: 0.0791
trigger times: 1
Loss after 3883485 batches: 0.0783
trigger times: 2
Loss after 3884448 batches: 0.0770
trigger times: 3
Loss after 3885411 batches: 0.0761
trigger times: 4
Loss after 3886374 batches: 0.0756
trigger times: 5
Loss after 3887337 batches: 0.0756
trigger times: 6
Loss after 3888300 batches: 0.0757
trigger times: 7
Loss after 3889263 batches: 0.0749
trigger times: 8
Loss after 3890226 batches: 0.0751
trigger times: 9
Loss after 3891189 batches: 0.0747
trigger times: 10
Loss after 3892152 batches: 0.0741
trigger times: 11
Loss after 3893115 batches: 0.0749
trigger times: 12
Loss after 3894078 batches: 0.0750
trigger times: 13
Loss after 3895041 batches: 0.0742
trigger times: 0
Loss after 3896004 batches: 0.0741
trigger times: 1
Loss after 3896967 batches: 0.0738
trigger times: 2
Loss after 3897930 batches: 0.0740
trigger times: 3
Loss after 3898893 batches: 0.0744
trigger times: 4
Loss after 3899856 batches: 0.0731
trigger times: 5
Loss after 3900819 batches: 0.0727
trigger times: 6
Loss after 3901782 batches: 0.0717
trigger times: 7
Loss after 3902745 batches: 0.0716
trigger times: 8
Loss after 3903708 batches: 0.0705
trigger times: 9
Loss after 3904671 batches: 0.0714
trigger times: 10
Loss after 3905634 batches: 0.0714
trigger times: 11
Loss after 3906597 batches: 0.0708
trigger times: 12
Loss after 3907560 batches: 0.0709
trigger times: 13
Loss after 3908523 batches: 0.0711
trigger times: 14
Loss after 3909486 batches: 0.0722
trigger times: 15
Loss after 3910449 batches: 0.0714
trigger times: 16
Loss after 3911412 batches: 0.0704
trigger times: 17
Loss after 3912375 batches: 0.0707
trigger times: 18
Loss after 3913338 batches: 0.0703
trigger times: 19
Loss after 3914301 batches: 0.0698
trigger times: 20
Loss after 3915264 batches: 0.0699
trigger times: 21
Loss after 3916227 batches: 0.0701
trigger times: 22
Loss after 3917190 batches: 0.0692
trigger times: 23
Loss after 3918153 batches: 0.0700
trigger times: 24
Loss after 3919116 batches: 0.0695
trigger times: 25
Early stopping!
Start to test process.
Loss after 3920079 batches: 0.0711
Time to train on one home:  67.6436288356781
trigger times: 0
Loss after 3921042 batches: 0.1326
trigger times: 0
Loss after 3922005 batches: 0.1248
trigger times: 0
Loss after 3922968 batches: 0.1133
trigger times: 1
Loss after 3923931 batches: 0.0959
trigger times: 2
Loss after 3924894 batches: 0.0862
trigger times: 3
Loss after 3925857 batches: 0.0790
trigger times: 4
Loss after 3926820 batches: 0.0738
trigger times: 5
Loss after 3927783 batches: 0.0713
trigger times: 6
Loss after 3928746 batches: 0.0684
trigger times: 7
Loss after 3929709 batches: 0.0649
trigger times: 8
Loss after 3930672 batches: 0.0628
trigger times: 9
Loss after 3931635 batches: 0.0607
trigger times: 10
Loss after 3932598 batches: 0.0609
trigger times: 11
Loss after 3933561 batches: 0.0604
trigger times: 12
Loss after 3934524 batches: 0.0592
trigger times: 13
Loss after 3935487 batches: 0.0587
trigger times: 14
Loss after 3936450 batches: 0.0574
trigger times: 15
Loss after 3937413 batches: 0.0574
trigger times: 16
Loss after 3938376 batches: 0.0574
trigger times: 17
Loss after 3939339 batches: 0.0549
trigger times: 18
Loss after 3940302 batches: 0.0542
trigger times: 19
Loss after 3941265 batches: 0.0538
trigger times: 20
Loss after 3942228 batches: 0.0540
trigger times: 21
Loss after 3943191 batches: 0.0531
trigger times: 22
Loss after 3944154 batches: 0.0536
trigger times: 23
Loss after 3945117 batches: 0.0517
trigger times: 24
Loss after 3946080 batches: 0.0517
trigger times: 25
Early stopping!
Start to test process.
Loss after 3947043 batches: 0.0520
Time to train on one home:  54.05767846107483
trigger times: 0
Loss after 3948006 batches: 0.0769
trigger times: 0
Loss after 3948969 batches: 0.0733
trigger times: 1
Loss after 3949932 batches: 0.0722
trigger times: 0
Loss after 3950895 batches: 0.0683
trigger times: 1
Loss after 3951858 batches: 0.0644
trigger times: 2
Loss after 3952821 batches: 0.0612
trigger times: 3
Loss after 3953784 batches: 0.0585
trigger times: 4
Loss after 3954747 batches: 0.0539
trigger times: 5
Loss after 3955710 batches: 0.0523
trigger times: 0
Loss after 3956673 batches: 0.0499
trigger times: 1
Loss after 3957636 batches: 0.0500
trigger times: 2
Loss after 3958599 batches: 0.0491
trigger times: 0
Loss after 3959562 batches: 0.0481
trigger times: 1
Loss after 3960525 batches: 0.0480
trigger times: 0
Loss after 3961488 batches: 0.0472
trigger times: 1
Loss after 3962451 batches: 0.0464
trigger times: 2
Loss after 3963414 batches: 0.0449
trigger times: 3
Loss after 3964377 batches: 0.0449
trigger times: 4
Loss after 3965340 batches: 0.0442
trigger times: 5
Loss after 3966303 batches: 0.0446
trigger times: 6
Loss after 3967266 batches: 0.0437
trigger times: 7
Loss after 3968229 batches: 0.0431
trigger times: 8
Loss after 3969192 batches: 0.0427
trigger times: 9
Loss after 3970155 batches: 0.0423
trigger times: 10
Loss after 3971118 batches: 0.0419
trigger times: 11
Loss after 3972081 batches: 0.0422
trigger times: 12
Loss after 3973044 batches: 0.0420
trigger times: 13
Loss after 3974007 batches: 0.0405
trigger times: 14
Loss after 3974970 batches: 0.0404
trigger times: 15
Loss after 3975933 batches: 0.0404
trigger times: 16
Loss after 3976896 batches: 0.0398
trigger times: 17
Loss after 3977859 batches: 0.0396
trigger times: 18
Loss after 3978822 batches: 0.0390
trigger times: 19
Loss after 3979785 batches: 0.0396
trigger times: 20
Loss after 3980748 batches: 0.0392
trigger times: 21
Loss after 3981711 batches: 0.0388
trigger times: 22
Loss after 3982674 batches: 0.0387
trigger times: 23
Loss after 3983637 batches: 0.0371
trigger times: 24
Loss after 3984600 batches: 0.0383
trigger times: 25
Early stopping!
Start to test process.
Loss after 3985563 batches: 0.0385
Time to train on one home:  69.36705493927002
trigger times: 0
Loss after 3986526 batches: 0.0652
trigger times: 1
Loss after 3987489 batches: 0.0522
trigger times: 2
Loss after 3988452 batches: 0.0533
trigger times: 3
Loss after 3989415 batches: 0.0535
trigger times: 4
Loss after 3990378 batches: 0.0528
trigger times: 5
Loss after 3991341 batches: 0.0520
trigger times: 6
Loss after 3992304 batches: 0.0510
trigger times: 7
Loss after 3993267 batches: 0.0498
trigger times: 8
Loss after 3994230 batches: 0.0481
trigger times: 9
Loss after 3995193 batches: 0.0483
trigger times: 10
Loss after 3996156 batches: 0.0472
trigger times: 11
Loss after 3997119 batches: 0.0460
trigger times: 12
Loss after 3998082 batches: 0.0461
trigger times: 13
Loss after 3999045 batches: 0.0456
trigger times: 14
Loss after 4000008 batches: 0.0449
trigger times: 0
Loss after 4000971 batches: 0.0442
trigger times: 1
Loss after 4001934 batches: 0.0433
trigger times: 2
Loss after 4002897 batches: 0.0436
trigger times: 3
Loss after 4003860 batches: 0.0427
trigger times: 4
Loss after 4004823 batches: 0.0425
trigger times: 5
Loss after 4005786 batches: 0.0422
trigger times: 6
Loss after 4006749 batches: 0.0422
trigger times: 7
Loss after 4007712 batches: 0.0417
trigger times: 8
Loss after 4008675 batches: 0.0414
trigger times: 9
Loss after 4009638 batches: 0.0418
trigger times: 10
Loss after 4010601 batches: 0.0422
trigger times: 11
Loss after 4011564 batches: 0.0425
trigger times: 12
Loss after 4012527 batches: 0.0433
trigger times: 13
Loss after 4013490 batches: 0.0416
trigger times: 14
Loss after 4014453 batches: 0.0416
trigger times: 15
Loss after 4015416 batches: 0.0411
trigger times: 16
Loss after 4016379 batches: 0.0405
trigger times: 17
Loss after 4017342 batches: 0.0409
trigger times: 18
Loss after 4018305 batches: 0.0403
trigger times: 19
Loss after 4019268 batches: 0.0403
trigger times: 20
Loss after 4020231 batches: 0.0400
trigger times: 21
Loss after 4021194 batches: 0.0401
trigger times: 22
Loss after 4022157 batches: 0.0393
trigger times: 23
Loss after 4023120 batches: 0.0397
trigger times: 24
Loss after 4024083 batches: 0.0397
trigger times: 25
Early stopping!
Start to test process.
Loss after 4025046 batches: 0.0393
Time to train on one home:  67.25264096260071
trigger times: 0
Loss after 4025941 batches: 0.0663
trigger times: 0
Loss after 4026836 batches: 0.0350
trigger times: 1
Loss after 4027731 batches: 0.0242
trigger times: 0
Loss after 4028626 batches: 0.0217
trigger times: 1
Loss after 4029521 batches: 0.0175
trigger times: 0
Loss after 4030416 batches: 0.0152
trigger times: 1
Loss after 4031311 batches: 0.0129
trigger times: 2
Loss after 4032206 batches: 0.0105
trigger times: 0
Loss after 4033101 batches: 0.0097
trigger times: 1
Loss after 4033996 batches: 0.0087
trigger times: 2
Loss after 4034891 batches: 0.0076
trigger times: 3
Loss after 4035786 batches: 0.0074
trigger times: 4
Loss after 4036681 batches: 0.0072
trigger times: 5
Loss after 4037576 batches: 0.0066
trigger times: 6
Loss after 4038471 batches: 0.0065
trigger times: 7
Loss after 4039366 batches: 0.0073
trigger times: 8
Loss after 4040261 batches: 0.0067
trigger times: 9
Loss after 4041156 batches: 0.0063
trigger times: 10
Loss after 4042051 batches: 0.0054
trigger times: 0
Loss after 4042946 batches: 0.0058
trigger times: 1
Loss after 4043841 batches: 0.0057
trigger times: 2
Loss after 4044736 batches: 0.0059
trigger times: 3
Loss after 4045631 batches: 0.0055
trigger times: 4
Loss after 4046526 batches: 0.0049
trigger times: 5
Loss after 4047421 batches: 0.0048
trigger times: 6
Loss after 4048316 batches: 0.0047
trigger times: 7
Loss after 4049211 batches: 0.0049
trigger times: 0
Loss after 4050106 batches: 0.0049
trigger times: 1
Loss after 4051001 batches: 0.0045
trigger times: 2
Loss after 4051896 batches: 0.0046
trigger times: 0
Loss after 4052791 batches: 0.0046
trigger times: 1
Loss after 4053686 batches: 0.0045
trigger times: 2
Loss after 4054581 batches: 0.0047
trigger times: 0
Loss after 4055476 batches: 0.0044
trigger times: 1
Loss after 4056371 batches: 0.0042
trigger times: 2
Loss after 4057266 batches: 0.0056
trigger times: 0
Loss after 4058161 batches: 0.0053
trigger times: 1
Loss after 4059056 batches: 0.0051
trigger times: 2
Loss after 4059951 batches: 0.0044
trigger times: 3
Loss after 4060846 batches: 0.0040
trigger times: 4
Loss after 4061741 batches: 0.0041
trigger times: 5
Loss after 4062636 batches: 0.0038
trigger times: 6
Loss after 4063531 batches: 0.0035
trigger times: 7
Loss after 4064426 batches: 0.0039
trigger times: 8
Loss after 4065321 batches: 0.0042
trigger times: 9
Loss after 4066216 batches: 0.0042
trigger times: 10
Loss after 4067111 batches: 0.0040
trigger times: 11
Loss after 4068006 batches: 0.0034
trigger times: 12
Loss after 4068901 batches: 0.0040
trigger times: 13
Loss after 4069796 batches: 0.0043
trigger times: 14
Loss after 4070691 batches: 0.0041
trigger times: 15
Loss after 4071586 batches: 0.0037
trigger times: 16
Loss after 4072481 batches: 0.0038
trigger times: 17
Loss after 4073376 batches: 0.0036
trigger times: 18
Loss after 4074271 batches: 0.0037
trigger times: 19
Loss after 4075166 batches: 0.0036
trigger times: 20
Loss after 4076061 batches: 0.0036
trigger times: 21
Loss after 4076956 batches: 0.0038
trigger times: 22
Loss after 4077851 batches: 0.0037
trigger times: 23
Loss after 4078746 batches: 0.0039
trigger times: 24
Loss after 4079641 batches: 0.0033
trigger times: 25
Early stopping!
Start to test process.
Loss after 4080536 batches: 0.0031
Time to train on one home:  87.25162410736084
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902]]
Round_2_results:  [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4281 < 4282; dropping {'Training_Loss': 0.0693065987101623, 'Validation_Loss': 0.06564334034919739, 'Training_R2': 0.13285691932896082, 'Validation_R2': -0.010545146189749666, 'Training_F1': 0.37362952655832876, 'Validation_F1': 0.21501486274140214, 'Training_NEP': 0.9470287495779366, 'Validation_NEP': 0.9469342002922567, 'Training_NDE': 0.6386113620453139, 'Validation_NDE': 0.7841809247620983, 'Training_MAE': 27.269853183326212, 'Validation_MAE': 31.54259965518461, 'Training_MSE': 2009.1865, 'Validation_MSE': 3884.3616}.
trigger times: 0
Loss after 4081499 batches: 0.0693
trigger times: 0
Loss after 4082462 batches: 0.0608
trigger times: 0
Loss after 4083425 batches: 0.0563
trigger times: 1
Loss after 4084388 batches: 0.0528
trigger times: 2
Loss after 4085351 batches: 0.0496
trigger times: 0
Loss after 4086314 batches: 0.0480
trigger times: 0
Loss after 4087277 batches: 0.0467
trigger times: 1
Loss after 4088240 batches: 0.0453
trigger times: 2
Loss after 4089203 batches: 0.0444
trigger times: 3
Loss after 4090166 batches: 0.0443
trigger times: 4
Loss after 4091129 batches: 0.0447
trigger times: 5
Loss after 4092092 batches: 0.0435
trigger times: 6
Loss after 4093055 batches: 0.0424
trigger times: 7
Loss after 4094018 batches: 0.0433
trigger times: 8
Loss after 4094981 batches: 0.0428
trigger times: 9
Loss after 4095944 batches: 0.0417
trigger times: 10
Loss after 4096907 batches: 0.0419
trigger times: 11
Loss after 4097870 batches: 0.0422
trigger times: 12
Loss after 4098833 batches: 0.0409
trigger times: 13
Loss after 4099796 batches: 0.0420
trigger times: 14
Loss after 4100759 batches: 0.0427
trigger times: 15
Loss after 4101722 batches: 0.0423
trigger times: 16
Loss after 4102685 batches: 0.0407
trigger times: 17
Loss after 4103648 batches: 0.0401
trigger times: 18
Loss after 4104611 batches: 0.0409
trigger times: 19
Loss after 4105574 batches: 0.0391
trigger times: 20
Loss after 4106537 batches: 0.0395
trigger times: 21
Loss after 4107500 batches: 0.0396
trigger times: 22
Loss after 4108463 batches: 0.0387
trigger times: 23
Loss after 4109426 batches: 0.0397
trigger times: 24
Loss after 4110389 batches: 0.0385
trigger times: 25
Early stopping!
Start to test process.
Loss after 4111352 batches: 0.0391
Time to train on one home:  60.791614294052124
trigger times: 0
Loss after 4112310 batches: 0.0822
trigger times: 0
Loss after 4113268 batches: 0.0541
trigger times: 1
Loss after 4114226 batches: 0.0515
trigger times: 2
Loss after 4115184 batches: 0.0454
trigger times: 3
Loss after 4116142 batches: 0.0433
trigger times: 0
Loss after 4117100 batches: 0.0403
trigger times: 0
Loss after 4118058 batches: 0.0377
trigger times: 1
Loss after 4119016 batches: 0.0351
trigger times: 2
Loss after 4119974 batches: 0.0341
trigger times: 0
Loss after 4120932 batches: 0.0329
trigger times: 0
Loss after 4121890 batches: 0.0336
trigger times: 1
Loss after 4122848 batches: 0.0318
trigger times: 2
Loss after 4123806 batches: 0.0309
trigger times: 3
Loss after 4124764 batches: 0.0303
trigger times: 4
Loss after 4125722 batches: 0.0304
trigger times: 5
Loss after 4126680 batches: 0.0296
trigger times: 6
Loss after 4127638 batches: 0.0284
trigger times: 7
Loss after 4128596 batches: 0.0288
trigger times: 8
Loss after 4129554 batches: 0.0289
trigger times: 9
Loss after 4130512 batches: 0.0290
trigger times: 10
Loss after 4131470 batches: 0.0291
trigger times: 11
Loss after 4132428 batches: 0.0278
trigger times: 12
Loss after 4133386 batches: 0.0281
trigger times: 13
Loss after 4134344 batches: 0.0274
trigger times: 14
Loss after 4135302 batches: 0.0277
trigger times: 15
Loss after 4136260 batches: 0.0283
trigger times: 16
Loss after 4137218 batches: 0.0275
trigger times: 17
Loss after 4138176 batches: 0.0264
trigger times: 18
Loss after 4139134 batches: 0.0260
trigger times: 19
Loss after 4140092 batches: 0.0264
trigger times: 20
Loss after 4141050 batches: 0.0250
trigger times: 21
Loss after 4142008 batches: 0.0256
trigger times: 22
Loss after 4142966 batches: 0.0254
trigger times: 23
Loss after 4143924 batches: 0.0257
trigger times: 24
Loss after 4144882 batches: 0.0248
trigger times: 25
Early stopping!
Start to test process.
Loss after 4145840 batches: 0.0242
Time to train on one home:  65.27325963973999
trigger times: 0
Loss after 4146803 batches: 0.0820
trigger times: 1
Loss after 4147766 batches: 0.0683
trigger times: 2
Loss after 4148729 batches: 0.0678
trigger times: 3
Loss after 4149692 batches: 0.0663
trigger times: 4
Loss after 4150655 batches: 0.0643
trigger times: 5
Loss after 4151618 batches: 0.0638
trigger times: 6
Loss after 4152581 batches: 0.0637
trigger times: 7
Loss after 4153544 batches: 0.0628
trigger times: 8
Loss after 4154507 batches: 0.0616
trigger times: 9
Loss after 4155470 batches: 0.0619
trigger times: 10
Loss after 4156433 batches: 0.0608
trigger times: 11
Loss after 4157396 batches: 0.0605
trigger times: 12
Loss after 4158359 batches: 0.0607
trigger times: 13
Loss after 4159322 batches: 0.0599
trigger times: 14
Loss after 4160285 batches: 0.0591
trigger times: 15
Loss after 4161248 batches: 0.0590
trigger times: 16
Loss after 4162211 batches: 0.0594
trigger times: 17
Loss after 4163174 batches: 0.0584
trigger times: 18
Loss after 4164137 batches: 0.0583
trigger times: 19
Loss after 4165100 batches: 0.0577
trigger times: 20
Loss after 4166063 batches: 0.0575
trigger times: 21
Loss after 4167026 batches: 0.0567
trigger times: 22
Loss after 4167989 batches: 0.0576
trigger times: 23
Loss after 4168952 batches: 0.0566
trigger times: 24
Loss after 4169915 batches: 0.0565
trigger times: 25
Early stopping!
Start to test process.
Loss after 4170878 batches: 0.0570
Time to train on one home:  55.78912377357483
trigger times: 0
Loss after 4171841 batches: 0.0859
trigger times: 1
Loss after 4172804 batches: 0.0795
trigger times: 2
Loss after 4173767 batches: 0.0793
trigger times: 3
Loss after 4174730 batches: 0.0777
trigger times: 0
Loss after 4175693 batches: 0.0767
trigger times: 1
Loss after 4176656 batches: 0.0750
trigger times: 2
Loss after 4177619 batches: 0.0742
trigger times: 3
Loss after 4178582 batches: 0.0737
trigger times: 4
Loss after 4179545 batches: 0.0720
trigger times: 5
Loss after 4180508 batches: 0.0704
trigger times: 6
Loss after 4181471 batches: 0.0715
trigger times: 7
Loss after 4182434 batches: 0.0715
trigger times: 8
Loss after 4183397 batches: 0.0714
trigger times: 9
Loss after 4184360 batches: 0.0685
trigger times: 10
Loss after 4185323 batches: 0.0678
trigger times: 11
Loss after 4186286 batches: 0.0674
trigger times: 12
Loss after 4187249 batches: 0.0678
trigger times: 13
Loss after 4188212 batches: 0.0671
trigger times: 14
Loss after 4189175 batches: 0.0670
trigger times: 15
Loss after 4190138 batches: 0.0675
trigger times: 16
Loss after 4191101 batches: 0.0680
trigger times: 17
Loss after 4192064 batches: 0.0662
trigger times: 18
Loss after 4193027 batches: 0.0655
trigger times: 19
Loss after 4193990 batches: 0.0661
trigger times: 0
Loss after 4194953 batches: 0.0665
trigger times: 1
Loss after 4195916 batches: 0.0665
trigger times: 2
Loss after 4196879 batches: 0.0660
trigger times: 3
Loss after 4197842 batches: 0.0651
trigger times: 4
Loss after 4198805 batches: 0.0663
trigger times: 5
Loss after 4199768 batches: 0.0661
trigger times: 6
Loss after 4200731 batches: 0.0664
trigger times: 0
Loss after 4201694 batches: 0.0654
trigger times: 1
Loss after 4202657 batches: 0.0643
trigger times: 2
Loss after 4203620 batches: 0.0653
trigger times: 3
Loss after 4204583 batches: 0.0640
trigger times: 4
Loss after 4205546 batches: 0.0657
trigger times: 5
Loss after 4206509 batches: 0.0651
trigger times: 6
Loss after 4207472 batches: 0.0628
trigger times: 7
Loss after 4208435 batches: 0.0629
trigger times: 8
Loss after 4209398 batches: 0.0632
trigger times: 9
Loss after 4210361 batches: 0.0627
trigger times: 10
Loss after 4211324 batches: 0.0635
trigger times: 11
Loss after 4212287 batches: 0.0630
trigger times: 12
Loss after 4213250 batches: 0.0630
trigger times: 13
Loss after 4214213 batches: 0.0636
trigger times: 14
Loss after 4215176 batches: 0.0631
trigger times: 15
Loss after 4216139 batches: 0.0630
trigger times: 16
Loss after 4217102 batches: 0.0624
trigger times: 17
Loss after 4218065 batches: 0.0621
trigger times: 18
Loss after 4219028 batches: 0.0621
trigger times: 19
Loss after 4219991 batches: 0.0609
trigger times: 20
Loss after 4220954 batches: 0.0625
trigger times: 21
Loss after 4221917 batches: 0.0616
trigger times: 22
Loss after 4222880 batches: 0.0618
trigger times: 23
Loss after 4223843 batches: 0.0608
trigger times: 24
Loss after 4224806 batches: 0.0611
trigger times: 25
Early stopping!
Start to test process.
Loss after 4225769 batches: 0.0621
Time to train on one home:  78.05245471000671
trigger times: 0
Loss after 4226732 batches: 0.0379
trigger times: 1
Loss after 4227695 batches: 0.0351
trigger times: 2
Loss after 4228658 batches: 0.0339
trigger times: 3
Loss after 4229621 batches: 0.0317
trigger times: 4
Loss after 4230584 batches: 0.0303
trigger times: 5
Loss after 4231547 batches: 0.0287
trigger times: 6
Loss after 4232510 batches: 0.0280
trigger times: 7
Loss after 4233473 batches: 0.0268
trigger times: 8
Loss after 4234436 batches: 0.0258
trigger times: 9
Loss after 4235399 batches: 0.0252
trigger times: 10
Loss after 4236362 batches: 0.0255
trigger times: 11
Loss after 4237325 batches: 0.0245
trigger times: 12
Loss after 4238288 batches: 0.0241
trigger times: 13
Loss after 4239251 batches: 0.0235
trigger times: 14
Loss after 4240214 batches: 0.0230
trigger times: 15
Loss after 4241177 batches: 0.0226
trigger times: 16
Loss after 4242140 batches: 0.0232
trigger times: 17
Loss after 4243103 batches: 0.0231
trigger times: 18
Loss after 4244066 batches: 0.0225
trigger times: 19
Loss after 4245029 batches: 0.0219
trigger times: 20
Loss after 4245992 batches: 0.0216
trigger times: 21
Loss after 4246955 batches: 0.0213
trigger times: 22
Loss after 4247918 batches: 0.0212
trigger times: 23
Loss after 4248881 batches: 0.0221
trigger times: 24
Loss after 4249844 batches: 0.0214
trigger times: 25
Early stopping!
Start to test process.
Loss after 4250807 batches: 0.0210
Time to train on one home:  57.662224769592285
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4251770 batches: 0.0569
trigger times: 1
Loss after 4252733 batches: 0.0252
trigger times: 2
Loss after 4253696 batches: 0.0249
trigger times: 3
Loss after 4254659 batches: 0.0236
trigger times: 4
Loss after 4255622 batches: 0.0226
trigger times: 5
Loss after 4256585 batches: 0.0221
trigger times: 6
Loss after 4257548 batches: 0.0211
trigger times: 7
Loss after 4258511 batches: 0.0198
trigger times: 8
Loss after 4259474 batches: 0.0189
trigger times: 9
Loss after 4260437 batches: 0.0180
trigger times: 10
Loss after 4261400 batches: 0.0174
trigger times: 11
Loss after 4262363 batches: 0.0173
trigger times: 12
Loss after 4263326 batches: 0.0165
trigger times: 13
Loss after 4264289 batches: 0.0163
trigger times: 14
Loss after 4265252 batches: 0.0162
trigger times: 15
Loss after 4266215 batches: 0.0159
trigger times: 16
Loss after 4267178 batches: 0.0160
trigger times: 17
Loss after 4268141 batches: 0.0158
trigger times: 18
Loss after 4269104 batches: 0.0160
trigger times: 19
Loss after 4270067 batches: 0.0155
trigger times: 20
Loss after 4271030 batches: 0.0157
trigger times: 21
Loss after 4271993 batches: 0.0154
trigger times: 22
Loss after 4272956 batches: 0.0151
trigger times: 23
Loss after 4273919 batches: 0.0153
trigger times: 24
Loss after 4274882 batches: 0.0150
trigger times: 25
Early stopping!
Start to test process.
Loss after 4275845 batches: 0.0147
Time to train on one home:  54.75297021865845
trigger times: 0
Loss after 4276808 batches: 0.1064
trigger times: 0
Loss after 4277771 batches: 0.1014
trigger times: 0
Loss after 4278734 batches: 0.0984
trigger times: 1
Loss after 4279697 batches: 0.0952
trigger times: 0
Loss after 4280660 batches: 0.0935
trigger times: 1
Loss after 4281623 batches: 0.0905
trigger times: 2
Loss after 4282586 batches: 0.0901
trigger times: 3
Loss after 4283549 batches: 0.0889
trigger times: 4
Loss after 4284512 batches: 0.0878
trigger times: 5
Loss after 4285475 batches: 0.0867
trigger times: 6
Loss after 4286438 batches: 0.0872
trigger times: 7
Loss after 4287401 batches: 0.0864
trigger times: 8
Loss after 4288364 batches: 0.0850
trigger times: 9
Loss after 4289327 batches: 0.0840
trigger times: 10
Loss after 4290290 batches: 0.0828
trigger times: 11
Loss after 4291253 batches: 0.0828
trigger times: 12
Loss after 4292216 batches: 0.0829
trigger times: 13
Loss after 4293179 batches: 0.0811
trigger times: 14
Loss after 4294142 batches: 0.0823
trigger times: 15
Loss after 4295105 batches: 0.0819
trigger times: 16
Loss after 4296068 batches: 0.0806
trigger times: 17
Loss after 4297031 batches: 0.0823
trigger times: 18
Loss after 4297994 batches: 0.0816
trigger times: 19
Loss after 4298957 batches: 0.0806
trigger times: 20
Loss after 4299920 batches: 0.0799
trigger times: 21
Loss after 4300883 batches: 0.0810
trigger times: 22
Loss after 4301846 batches: 0.0785
trigger times: 23
Loss after 4302809 batches: 0.0796
trigger times: 24
Loss after 4303772 batches: 0.0790
trigger times: 25
Early stopping!
Start to test process.
Loss after 4304735 batches: 0.0801
Time to train on one home:  63.826231479644775
trigger times: 0
Loss after 4305698 batches: 0.0630
trigger times: 1
Loss after 4306661 batches: 0.0563
trigger times: 0
Loss after 4307624 batches: 0.0514
trigger times: 1
Loss after 4308587 batches: 0.0449
trigger times: 2
Loss after 4309550 batches: 0.0419
trigger times: 3
Loss after 4310513 batches: 0.0402
trigger times: 4
Loss after 4311476 batches: 0.0406
trigger times: 0
Loss after 4312439 batches: 0.0391
trigger times: 1
Loss after 4313402 batches: 0.0372
trigger times: 2
Loss after 4314365 batches: 0.0366
trigger times: 3
Loss after 4315328 batches: 0.0352
trigger times: 0
Loss after 4316291 batches: 0.0339
trigger times: 1
Loss after 4317254 batches: 0.0346
trigger times: 2
Loss after 4318217 batches: 0.0343
trigger times: 3
Loss after 4319180 batches: 0.0337
trigger times: 4
Loss after 4320143 batches: 0.0334
trigger times: 5
Loss after 4321106 batches: 0.0333
trigger times: 6
Loss after 4322069 batches: 0.0320
trigger times: 7
Loss after 4323032 batches: 0.0321
trigger times: 8
Loss after 4323995 batches: 0.0319
trigger times: 9
Loss after 4324958 batches: 0.0331
trigger times: 10
Loss after 4325921 batches: 0.0348
trigger times: 11
Loss after 4326884 batches: 0.0345
trigger times: 12
Loss after 4327847 batches: 0.0329
trigger times: 13
Loss after 4328810 batches: 0.0320
trigger times: 14
Loss after 4329773 batches: 0.0324
trigger times: 15
Loss after 4330736 batches: 0.0316
trigger times: 16
Loss after 4331699 batches: 0.0313
trigger times: 17
Loss after 4332662 batches: 0.0313
trigger times: 18
Loss after 4333625 batches: 0.0311
trigger times: 19
Loss after 4334588 batches: 0.0311
trigger times: 20
Loss after 4335551 batches: 0.0295
trigger times: 21
Loss after 4336514 batches: 0.0294
trigger times: 22
Loss after 4337477 batches: 0.0296
trigger times: 23
Loss after 4338440 batches: 0.0314
trigger times: 24
Loss after 4339403 batches: 0.0307
trigger times: 25
Early stopping!
Start to test process.
Loss after 4340366 batches: 0.0296
Time to train on one home:  64.80777072906494
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4341329 batches: 0.0887
trigger times: 1
Loss after 4342292 batches: 0.0848
trigger times: 2
Loss after 4343255 batches: 0.0833
trigger times: 3
Loss after 4344218 batches: 0.0796
trigger times: 4
Loss after 4345181 batches: 0.0780
trigger times: 5
Loss after 4346144 batches: 0.0784
trigger times: 6
Loss after 4347107 batches: 0.0775
trigger times: 7
Loss after 4348070 batches: 0.0762
trigger times: 8
Loss after 4349033 batches: 0.0736
trigger times: 9
Loss after 4349996 batches: 0.0747
trigger times: 10
Loss after 4350959 batches: 0.0730
trigger times: 11
Loss after 4351922 batches: 0.0730
trigger times: 12
Loss after 4352885 batches: 0.0713
trigger times: 13
Loss after 4353848 batches: 0.0708
trigger times: 14
Loss after 4354811 batches: 0.0709
trigger times: 15
Loss after 4355774 batches: 0.0712
trigger times: 16
Loss after 4356737 batches: 0.0708
trigger times: 17
Loss after 4357700 batches: 0.0712
trigger times: 18
Loss after 4358663 batches: 0.0705
trigger times: 19
Loss after 4359626 batches: 0.0693
trigger times: 20
Loss after 4360589 batches: 0.0695
trigger times: 21
Loss after 4361552 batches: 0.0691
trigger times: 22
Loss after 4362515 batches: 0.0686
trigger times: 23
Loss after 4363478 batches: 0.0684
trigger times: 24
Loss after 4364441 batches: 0.0680
trigger times: 25
Early stopping!
Start to test process.
Loss after 4365404 batches: 0.0686
Time to train on one home:  56.86003303527832
trigger times: 0
Loss after 4366367 batches: 0.0957
trigger times: 0
Loss after 4367330 batches: 0.0822
trigger times: 0
Loss after 4368293 batches: 0.0732
trigger times: 0
Loss after 4369256 batches: 0.0709
trigger times: 1
Loss after 4370219 batches: 0.0670
trigger times: 2
Loss after 4371182 batches: 0.0647
trigger times: 0
Loss after 4372145 batches: 0.0625
trigger times: 0
Loss after 4373108 batches: 0.0608
trigger times: 0
Loss after 4374071 batches: 0.0595
trigger times: 1
Loss after 4375034 batches: 0.0583
trigger times: 2
Loss after 4375997 batches: 0.0571
trigger times: 3
Loss after 4376960 batches: 0.0575
trigger times: 4
Loss after 4377923 batches: 0.0560
trigger times: 5
Loss after 4378886 batches: 0.0555
trigger times: 6
Loss after 4379849 batches: 0.0550
trigger times: 0
Loss after 4380812 batches: 0.0536
trigger times: 1
Loss after 4381775 batches: 0.0540
trigger times: 2
Loss after 4382738 batches: 0.0541
trigger times: 3
Loss after 4383701 batches: 0.0535
trigger times: 4
Loss after 4384664 batches: 0.0528
trigger times: 0
Loss after 4385627 batches: 0.0527
trigger times: 1
Loss after 4386590 batches: 0.0520
trigger times: 2
Loss after 4387553 batches: 0.0520
trigger times: 3
Loss after 4388516 batches: 0.0512
trigger times: 4
Loss after 4389479 batches: 0.0522
trigger times: 5
Loss after 4390442 batches: 0.0512
trigger times: 0
Loss after 4391405 batches: 0.0511
trigger times: 1
Loss after 4392368 batches: 0.0508
trigger times: 2
Loss after 4393331 batches: 0.0491
trigger times: 0
Loss after 4394294 batches: 0.0498
trigger times: 0
Loss after 4395257 batches: 0.0497
trigger times: 1
Loss after 4396220 batches: 0.0505
trigger times: 2
Loss after 4397183 batches: 0.0500
trigger times: 0
Loss after 4398146 batches: 0.0499
trigger times: 1
Loss after 4399109 batches: 0.0487
trigger times: 0
Loss after 4400072 batches: 0.0496
trigger times: 0
Loss after 4401035 batches: 0.0486
trigger times: 1
Loss after 4401998 batches: 0.0490
trigger times: 2
Loss after 4402961 batches: 0.0493
trigger times: 3
Loss after 4403924 batches: 0.0488
trigger times: 4
Loss after 4404887 batches: 0.0476
trigger times: 5
Loss after 4405850 batches: 0.0467
trigger times: 6
Loss after 4406813 batches: 0.0466
trigger times: 7
Loss after 4407776 batches: 0.0467
trigger times: 8
Loss after 4408739 batches: 0.0461
trigger times: 9
Loss after 4409702 batches: 0.0460
trigger times: 10
Loss after 4410665 batches: 0.0458
trigger times: 11
Loss after 4411628 batches: 0.0459
trigger times: 12
Loss after 4412591 batches: 0.0469
trigger times: 13
Loss after 4413554 batches: 0.0451
trigger times: 14
Loss after 4414517 batches: 0.0452
trigger times: 15
Loss after 4415480 batches: 0.0450
trigger times: 16
Loss after 4416443 batches: 0.0444
trigger times: 17
Loss after 4417406 batches: 0.0452
trigger times: 18
Loss after 4418369 batches: 0.0433
trigger times: 19
Loss after 4419332 batches: 0.0449
trigger times: 20
Loss after 4420295 batches: 0.0445
trigger times: 21
Loss after 4421258 batches: 0.0430
trigger times: 0
Loss after 4422221 batches: 0.0434
trigger times: 0
Loss after 4423184 batches: 0.0445
trigger times: 1
Loss after 4424147 batches: 0.0423
trigger times: 2
Loss after 4425110 batches: 0.0417
trigger times: 3
Loss after 4426073 batches: 0.0412
trigger times: 4
Loss after 4427036 batches: 0.0416
trigger times: 5
Loss after 4427999 batches: 0.0419
trigger times: 6
Loss after 4428962 batches: 0.0420
trigger times: 7
Loss after 4429925 batches: 0.0405
trigger times: 8
Loss after 4430888 batches: 0.0416
trigger times: 9
Loss after 4431851 batches: 0.0419
trigger times: 10
Loss after 4432814 batches: 0.0424
trigger times: 11
Loss after 4433777 batches: 0.0426
trigger times: 12
Loss after 4434740 batches: 0.0416
trigger times: 13
Loss after 4435703 batches: 0.0422
trigger times: 0
Loss after 4436666 batches: 0.0418
trigger times: 1
Loss after 4437629 batches: 0.0408
trigger times: 2
Loss after 4438592 batches: 0.0402
trigger times: 3
Loss after 4439555 batches: 0.0403
trigger times: 0
Loss after 4440518 batches: 0.0394
trigger times: 1
Loss after 4441481 batches: 0.0401
trigger times: 2
Loss after 4442444 batches: 0.0402
trigger times: 3
Loss after 4443407 batches: 0.0402
trigger times: 4
Loss after 4444370 batches: 0.0400
trigger times: 5
Loss after 4445333 batches: 0.0389
trigger times: 6
Loss after 4446296 batches: 0.0396
trigger times: 7
Loss after 4447259 batches: 0.0391
trigger times: 8
Loss after 4448222 batches: 0.0390
trigger times: 9
Loss after 4449185 batches: 0.0402
trigger times: 10
Loss after 4450148 batches: 0.0394
trigger times: 11
Loss after 4451111 batches: 0.0393
trigger times: 12
Loss after 4452074 batches: 0.0390
trigger times: 13
Loss after 4453037 batches: 0.0382
trigger times: 14
Loss after 4454000 batches: 0.0384
trigger times: 15
Loss after 4454963 batches: 0.0387
trigger times: 16
Loss after 4455926 batches: 0.0396
trigger times: 17
Loss after 4456889 batches: 0.0396
trigger times: 18
Loss after 4457852 batches: 0.0387
trigger times: 19
Loss after 4458815 batches: 0.0390
trigger times: 20
Loss after 4459778 batches: 0.0380
trigger times: 21
Loss after 4460741 batches: 0.0370
trigger times: 22
Loss after 4461704 batches: 0.0371
trigger times: 23
Loss after 4462667 batches: 0.0380
trigger times: 24
Loss after 4463630 batches: 0.0369
trigger times: 25
Early stopping!
Start to test process.
Loss after 4464593 batches: 0.0373
Time to train on one home:  117.69082593917847
trigger times: 0
Loss after 4465556 batches: 0.0885
trigger times: 1
Loss after 4466519 batches: 0.0822
trigger times: 2
Loss after 4467482 batches: 0.0800
trigger times: 3
Loss after 4468445 batches: 0.0760
trigger times: 4
Loss after 4469408 batches: 0.0745
trigger times: 5
Loss after 4470371 batches: 0.0727
trigger times: 6
Loss after 4471334 batches: 0.0718
trigger times: 7
Loss after 4472297 batches: 0.0710
trigger times: 8
Loss after 4473260 batches: 0.0701
trigger times: 9
Loss after 4474223 batches: 0.0690
trigger times: 10
Loss after 4475186 batches: 0.0702
trigger times: 11
Loss after 4476149 batches: 0.0686
trigger times: 12
Loss after 4477112 batches: 0.0677
trigger times: 13
Loss after 4478075 batches: 0.0673
trigger times: 14
Loss after 4479038 batches: 0.0680
trigger times: 15
Loss after 4480001 batches: 0.0684
trigger times: 16
Loss after 4480964 batches: 0.0673
trigger times: 17
Loss after 4481927 batches: 0.0664
trigger times: 18
Loss after 4482890 batches: 0.0668
trigger times: 19
Loss after 4483853 batches: 0.0652
trigger times: 20
Loss after 4484816 batches: 0.0659
trigger times: 21
Loss after 4485779 batches: 0.0639
trigger times: 22
Loss after 4486742 batches: 0.0649
trigger times: 23
Loss after 4487705 batches: 0.0645
trigger times: 24
Loss after 4488668 batches: 0.0638
trigger times: 25
Early stopping!
Start to test process.
Loss after 4489631 batches: 0.0628
Time to train on one home:  56.01718831062317
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4490594 batches: 0.0644
trigger times: 1
Loss after 4491557 batches: 0.0580
trigger times: 2
Loss after 4492520 batches: 0.0575
trigger times: 0
Loss after 4493483 batches: 0.0538
trigger times: 1
Loss after 4494446 batches: 0.0514
trigger times: 2
Loss after 4495409 batches: 0.0487
trigger times: 3
Loss after 4496372 batches: 0.0474
trigger times: 4
Loss after 4497335 batches: 0.0457
trigger times: 5
Loss after 4498298 batches: 0.0439
trigger times: 6
Loss after 4499261 batches: 0.0428
trigger times: 7
Loss after 4500224 batches: 0.0421
trigger times: 8
Loss after 4501187 batches: 0.0405
trigger times: 9
Loss after 4502150 batches: 0.0406
trigger times: 10
Loss after 4503113 batches: 0.0397
trigger times: 11
Loss after 4504076 batches: 0.0392
trigger times: 12
Loss after 4505039 batches: 0.0392
trigger times: 13
Loss after 4506002 batches: 0.0382
trigger times: 14
Loss after 4506965 batches: 0.0375
trigger times: 15
Loss after 4507928 batches: 0.0370
trigger times: 16
Loss after 4508891 batches: 0.0366
trigger times: 17
Loss after 4509854 batches: 0.0362
trigger times: 18
Loss after 4510817 batches: 0.0364
trigger times: 19
Loss after 4511780 batches: 0.0367
trigger times: 20
Loss after 4512743 batches: 0.0363
trigger times: 21
Loss after 4513706 batches: 0.0359
trigger times: 22
Loss after 4514669 batches: 0.0356
trigger times: 23
Loss after 4515632 batches: 0.0351
trigger times: 24
Loss after 4516595 batches: 0.0341
trigger times: 25
Early stopping!
Start to test process.
Loss after 4517558 batches: 0.0344
Time to train on one home:  59.24718165397644
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4518521 batches: 0.0954
trigger times: 0
Loss after 4519484 batches: 0.0712
trigger times: 1
Loss after 4520447 batches: 0.0681
trigger times: 2
Loss after 4521410 batches: 0.0618
trigger times: 3
Loss after 4522373 batches: 0.0577
trigger times: 4
Loss after 4523336 batches: 0.0537
trigger times: 5
Loss after 4524299 batches: 0.0511
trigger times: 6
Loss after 4525262 batches: 0.0504
trigger times: 7
Loss after 4526225 batches: 0.0514
trigger times: 8
Loss after 4527188 batches: 0.0485
trigger times: 9
Loss after 4528151 batches: 0.0449
trigger times: 10
Loss after 4529114 batches: 0.0429
trigger times: 11
Loss after 4530077 batches: 0.0421
trigger times: 12
Loss after 4531040 batches: 0.0429
trigger times: 13
Loss after 4532003 batches: 0.0411
trigger times: 14
Loss after 4532966 batches: 0.0394
trigger times: 15
Loss after 4533929 batches: 0.0391
trigger times: 16
Loss after 4534892 batches: 0.0357
trigger times: 17
Loss after 4535855 batches: 0.0369
trigger times: 18
Loss after 4536818 batches: 0.0349
trigger times: 19
Loss after 4537781 batches: 0.0349
trigger times: 20
Loss after 4538744 batches: 0.0344
trigger times: 21
Loss after 4539707 batches: 0.0343
trigger times: 22
Loss after 4540670 batches: 0.0328
trigger times: 23
Loss after 4541633 batches: 0.0342
trigger times: 24
Loss after 4542596 batches: 0.0340
trigger times: 25
Early stopping!
Start to test process.
Loss after 4543559 batches: 0.0332
Time to train on one home:  57.17978620529175
trigger times: 0
Loss after 4544488 batches: 0.0937
trigger times: 0
Loss after 4545417 batches: 0.0661
trigger times: 1
Loss after 4546346 batches: 0.0562
trigger times: 0
Loss after 4547275 batches: 0.0501
trigger times: 0
Loss after 4548204 batches: 0.0470
trigger times: 1
Loss after 4549133 batches: 0.0433
trigger times: 2
Loss after 4550062 batches: 0.0428
trigger times: 3
Loss after 4550991 batches: 0.0409
trigger times: 0
Loss after 4551920 batches: 0.0405
trigger times: 1
Loss after 4552849 batches: 0.0419
trigger times: 2
Loss after 4553778 batches: 0.0432
trigger times: 3
Loss after 4554707 batches: 0.0414
trigger times: 4
Loss after 4555636 batches: 0.0399
trigger times: 5
Loss after 4556565 batches: 0.0392
trigger times: 6
Loss after 4557494 batches: 0.0393
trigger times: 7
Loss after 4558423 batches: 0.0381
trigger times: 8
Loss after 4559352 batches: 0.0344
trigger times: 9
Loss after 4560281 batches: 0.0361
trigger times: 10
Loss after 4561210 batches: 0.0338
trigger times: 11
Loss after 4562139 batches: 0.0363
trigger times: 12
Loss after 4563068 batches: 0.0356
trigger times: 13
Loss after 4563997 batches: 0.0350
trigger times: 14
Loss after 4564926 batches: 0.0325
trigger times: 15
Loss after 4565855 batches: 0.0362
trigger times: 16
Loss after 4566784 batches: 0.0323
trigger times: 17
Loss after 4567713 batches: 0.0384
trigger times: 18
Loss after 4568642 batches: 0.0374
trigger times: 19
Loss after 4569571 batches: 0.0360
trigger times: 20
Loss after 4570500 batches: 0.0354
trigger times: 21
Loss after 4571429 batches: 0.0363
trigger times: 22
Loss after 4572358 batches: 0.0367
trigger times: 23
Loss after 4573287 batches: 0.0349
trigger times: 24
Loss after 4574216 batches: 0.0330
trigger times: 25
Early stopping!
Start to test process.
Loss after 4575145 batches: 0.0320
Time to train on one home:  64.6105306148529
trigger times: 0
Loss after 4576107 batches: 0.0747
trigger times: 1
Loss after 4577069 batches: 0.0708
trigger times: 2
Loss after 4578031 batches: 0.0708
trigger times: 3
Loss after 4578993 batches: 0.0669
trigger times: 4
Loss after 4579955 batches: 0.0661
trigger times: 5
Loss after 4580917 batches: 0.0652
trigger times: 0
Loss after 4581879 batches: 0.0640
trigger times: 1
Loss after 4582841 batches: 0.0651
trigger times: 2
Loss after 4583803 batches: 0.0633
trigger times: 3
Loss after 4584765 batches: 0.0636
trigger times: 4
Loss after 4585727 batches: 0.0633
trigger times: 5
Loss after 4586689 batches: 0.0632
trigger times: 6
Loss after 4587651 batches: 0.0626
trigger times: 7
Loss after 4588613 batches: 0.0624
trigger times: 8
Loss after 4589575 batches: 0.0618
trigger times: 9
Loss after 4590537 batches: 0.0618
trigger times: 10
Loss after 4591499 batches: 0.0608
trigger times: 11
Loss after 4592461 batches: 0.0612
trigger times: 12
Loss after 4593423 batches: 0.0615
trigger times: 13
Loss after 4594385 batches: 0.0598
trigger times: 14
Loss after 4595347 batches: 0.0606
trigger times: 15
Loss after 4596309 batches: 0.0605
trigger times: 16
Loss after 4597271 batches: 0.0600
trigger times: 17
Loss after 4598233 batches: 0.0605
trigger times: 18
Loss after 4599195 batches: 0.0599
trigger times: 19
Loss after 4600157 batches: 0.0598
trigger times: 20
Loss after 4601119 batches: 0.0603
trigger times: 21
Loss after 4602081 batches: 0.0606
trigger times: 22
Loss after 4603043 batches: 0.0604
trigger times: 23
Loss after 4604005 batches: 0.0592
trigger times: 24
Loss after 4604967 batches: 0.0601
trigger times: 25
Early stopping!
Start to test process.
Loss after 4605929 batches: 0.0593
Time to train on one home:  60.811622858047485
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4606892 batches: 0.0578
trigger times: 1
Loss after 4607855 batches: 0.0510
trigger times: 2
Loss after 4608818 batches: 0.0511
trigger times: 3
Loss after 4609781 batches: 0.0489
trigger times: 4
Loss after 4610744 batches: 0.0468
trigger times: 5
Loss after 4611707 batches: 0.0454
trigger times: 6
Loss after 4612670 batches: 0.0445
trigger times: 7
Loss after 4613633 batches: 0.0435
trigger times: 8
Loss after 4614596 batches: 0.0429
trigger times: 0
Loss after 4615559 batches: 0.0425
trigger times: 0
Loss after 4616522 batches: 0.0418
trigger times: 1
Loss after 4617485 batches: 0.0422
trigger times: 0
Loss after 4618448 batches: 0.0409
trigger times: 1
Loss after 4619411 batches: 0.0406
trigger times: 2
Loss after 4620374 batches: 0.0407
trigger times: 3
Loss after 4621337 batches: 0.0404
trigger times: 4
Loss after 4622300 batches: 0.0404
trigger times: 5
Loss after 4623263 batches: 0.0400
trigger times: 6
Loss after 4624226 batches: 0.0396
trigger times: 7
Loss after 4625189 batches: 0.0392
trigger times: 8
Loss after 4626152 batches: 0.0392
trigger times: 9
Loss after 4627115 batches: 0.0388
trigger times: 10
Loss after 4628078 batches: 0.0386
trigger times: 11
Loss after 4629041 batches: 0.0385
trigger times: 12
Loss after 4630004 batches: 0.0385
trigger times: 13
Loss after 4630967 batches: 0.0383
trigger times: 14
Loss after 4631930 batches: 0.0381
trigger times: 15
Loss after 4632893 batches: 0.0380
trigger times: 16
Loss after 4633856 batches: 0.0379
trigger times: 17
Loss after 4634819 batches: 0.0376
trigger times: 18
Loss after 4635782 batches: 0.0375
trigger times: 19
Loss after 4636745 batches: 0.0367
trigger times: 20
Loss after 4637708 batches: 0.0365
trigger times: 21
Loss after 4638671 batches: 0.0367
trigger times: 22
Loss after 4639634 batches: 0.0362
trigger times: 23
Loss after 4640597 batches: 0.0362
trigger times: 24
Loss after 4641560 batches: 0.0365
trigger times: 25
Early stopping!
Start to test process.
Loss after 4642523 batches: 0.0364
Time to train on one home:  67.20774626731873
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4643486 batches: 0.0667
trigger times: 1
Loss after 4644449 batches: 0.0615
trigger times: 0
Loss after 4645412 batches: 0.0591
trigger times: 0
Loss after 4646375 batches: 0.0568
trigger times: 1
Loss after 4647338 batches: 0.0538
trigger times: 0
Loss after 4648301 batches: 0.0529
trigger times: 1
Loss after 4649264 batches: 0.0517
trigger times: 0
Loss after 4650227 batches: 0.0507
trigger times: 0
Loss after 4651190 batches: 0.0492
trigger times: 1
Loss after 4652153 batches: 0.0490
trigger times: 2
Loss after 4653116 batches: 0.0482
trigger times: 0
Loss after 4654079 batches: 0.0480
trigger times: 1
Loss after 4655042 batches: 0.0474
trigger times: 2
Loss after 4656005 batches: 0.0469
trigger times: 3
Loss after 4656968 batches: 0.0471
trigger times: 0
Loss after 4657931 batches: 0.0468
trigger times: 1
Loss after 4658894 batches: 0.0465
trigger times: 2
Loss after 4659857 batches: 0.0453
trigger times: 0
Loss after 4660820 batches: 0.0455
trigger times: 1
Loss after 4661783 batches: 0.0440
trigger times: 2
Loss after 4662746 batches: 0.0438
trigger times: 3
Loss after 4663709 batches: 0.0440
trigger times: 4
Loss after 4664672 batches: 0.0439
trigger times: 5
Loss after 4665635 batches: 0.0435
trigger times: 6
Loss after 4666598 batches: 0.0436
trigger times: 7
Loss after 4667561 batches: 0.0423
trigger times: 8
Loss after 4668524 batches: 0.0423
trigger times: 9
Loss after 4669487 batches: 0.0426
trigger times: 10
Loss after 4670450 batches: 0.0423
trigger times: 11
Loss after 4671413 batches: 0.0419
trigger times: 12
Loss after 4672376 batches: 0.0415
trigger times: 13
Loss after 4673339 batches: 0.0412
trigger times: 14
Loss after 4674302 batches: 0.0400
trigger times: 15
Loss after 4675265 batches: 0.0403
trigger times: 16
Loss after 4676228 batches: 0.0412
trigger times: 17
Loss after 4677191 batches: 0.0416
trigger times: 18
Loss after 4678154 batches: 0.0411
trigger times: 19
Loss after 4679117 batches: 0.0397
trigger times: 20
Loss after 4680080 batches: 0.0393
trigger times: 21
Loss after 4681043 batches: 0.0396
trigger times: 22
Loss after 4682006 batches: 0.0393
trigger times: 23
Loss after 4682969 batches: 0.0398
trigger times: 24
Loss after 4683932 batches: 0.0393
trigger times: 25
Early stopping!
Start to test process.
Loss after 4684895 batches: 0.0379
Time to train on one home:  69.8389093875885
trigger times: 0
Loss after 4685858 batches: 0.1206
trigger times: 1
Loss after 4686821 batches: 0.1171
trigger times: 2
Loss after 4687784 batches: 0.1131
trigger times: 0
Loss after 4688747 batches: 0.1092
trigger times: 1
Loss after 4689710 batches: 0.1056
trigger times: 2
Loss after 4690673 batches: 0.1051
trigger times: 3
Loss after 4691636 batches: 0.1039
trigger times: 4
Loss after 4692599 batches: 0.1006
trigger times: 5
Loss after 4693562 batches: 0.0998
trigger times: 6
Loss after 4694525 batches: 0.0986
trigger times: 7
Loss after 4695488 batches: 0.0984
trigger times: 8
Loss after 4696451 batches: 0.0967
trigger times: 9
Loss after 4697414 batches: 0.0952
trigger times: 10
Loss after 4698377 batches: 0.0952
trigger times: 11
Loss after 4699340 batches: 0.0948
trigger times: 12
Loss after 4700303 batches: 0.0931
trigger times: 13
Loss after 4701266 batches: 0.0929
trigger times: 14
Loss after 4702229 batches: 0.0909
trigger times: 15
Loss after 4703192 batches: 0.0904
trigger times: 16
Loss after 4704155 batches: 0.0898
trigger times: 17
Loss after 4705118 batches: 0.0906
trigger times: 18
Loss after 4706081 batches: 0.0905
trigger times: 19
Loss after 4707044 batches: 0.0925
trigger times: 20
Loss after 4708007 batches: 0.0947
trigger times: 21
Loss after 4708970 batches: 0.0905
trigger times: 22
Loss after 4709933 batches: 0.0879
trigger times: 23
Loss after 4710896 batches: 0.0892
trigger times: 24
Loss after 4711859 batches: 0.0866
trigger times: 25
Early stopping!
Start to test process.
Loss after 4712822 batches: 0.0870
Time to train on one home:  55.96505570411682
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4713785 batches: 0.1192
trigger times: 1
Loss after 4714748 batches: 0.0848
trigger times: 2
Loss after 4715711 batches: 0.0821
trigger times: 3
Loss after 4716674 batches: 0.0817
trigger times: 4
Loss after 4717637 batches: 0.0783
trigger times: 5
Loss after 4718600 batches: 0.0753
trigger times: 6
Loss after 4719563 batches: 0.0716
trigger times: 7
Loss after 4720526 batches: 0.0680
trigger times: 8
Loss after 4721489 batches: 0.0652
trigger times: 9
Loss after 4722452 batches: 0.0629
trigger times: 0
Loss after 4723415 batches: 0.0604
trigger times: 0
Loss after 4724378 batches: 0.0583
trigger times: 1
Loss after 4725341 batches: 0.0577
trigger times: 0
Loss after 4726304 batches: 0.0556
trigger times: 0
Loss after 4727267 batches: 0.0547
trigger times: 1
Loss after 4728230 batches: 0.0547
trigger times: 2
Loss after 4729193 batches: 0.0532
trigger times: 3
Loss after 4730156 batches: 0.0518
trigger times: 4
Loss after 4731119 batches: 0.0511
trigger times: 5
Loss after 4732082 batches: 0.0506
trigger times: 6
Loss after 4733045 batches: 0.0508
trigger times: 7
Loss after 4734008 batches: 0.0491
trigger times: 8
Loss after 4734971 batches: 0.0492
trigger times: 0
Loss after 4735934 batches: 0.0487
trigger times: 1
Loss after 4736897 batches: 0.0469
trigger times: 2
Loss after 4737860 batches: 0.0469
trigger times: 3
Loss after 4738823 batches: 0.0464
trigger times: 4
Loss after 4739786 batches: 0.0454
trigger times: 0
Loss after 4740749 batches: 0.0461
trigger times: 1
Loss after 4741712 batches: 0.0448
trigger times: 2
Loss after 4742675 batches: 0.0438
trigger times: 3
Loss after 4743638 batches: 0.0428
trigger times: 4
Loss after 4744601 batches: 0.0433
trigger times: 5
Loss after 4745564 batches: 0.0440
trigger times: 6
Loss after 4746527 batches: 0.0427
trigger times: 7
Loss after 4747490 batches: 0.0434
trigger times: 8
Loss after 4748453 batches: 0.0430
trigger times: 0
Loss after 4749416 batches: 0.0413
trigger times: 1
Loss after 4750379 batches: 0.0409
trigger times: 0
Loss after 4751342 batches: 0.0406
trigger times: 1
Loss after 4752305 batches: 0.0403
trigger times: 2
Loss after 4753268 batches: 0.0396
trigger times: 3
Loss after 4754231 batches: 0.0394
trigger times: 4
Loss after 4755194 batches: 0.0394
trigger times: 5
Loss after 4756157 batches: 0.0394
trigger times: 6
Loss after 4757120 batches: 0.0385
trigger times: 7
Loss after 4758083 batches: 0.0384
trigger times: 8
Loss after 4759046 batches: 0.0381
trigger times: 9
Loss after 4760009 batches: 0.0375
trigger times: 10
Loss after 4760972 batches: 0.0360
trigger times: 11
Loss after 4761935 batches: 0.0372
trigger times: 12
Loss after 4762898 batches: 0.0374
trigger times: 13
Loss after 4763861 batches: 0.0363
trigger times: 14
Loss after 4764824 batches: 0.0366
trigger times: 15
Loss after 4765787 batches: 0.0356
trigger times: 16
Loss after 4766750 batches: 0.0357
trigger times: 17
Loss after 4767713 batches: 0.0347
trigger times: 18
Loss after 4768676 batches: 0.0343
trigger times: 19
Loss after 4769639 batches: 0.0348
trigger times: 20
Loss after 4770602 batches: 0.0339
trigger times: 21
Loss after 4771565 batches: 0.0342
trigger times: 22
Loss after 4772528 batches: 0.0338
trigger times: 23
Loss after 4773491 batches: 0.0340
trigger times: 24
Loss after 4774454 batches: 0.0326
trigger times: 25
Early stopping!
Start to test process.
Loss after 4775417 batches: 0.0336
Time to train on one home:  89.09207606315613
trigger times: 0
Loss after 4776376 batches: 0.1019
trigger times: 1
Loss after 4777335 batches: 0.0668
trigger times: 0
Loss after 4778294 batches: 0.0487
trigger times: 1
Loss after 4779253 batches: 0.0413
trigger times: 2
Loss after 4780212 batches: 0.0394
trigger times: 3
Loss after 4781171 batches: 0.0355
trigger times: 0
Loss after 4782130 batches: 0.0333
trigger times: 0
Loss after 4783089 batches: 0.0316
trigger times: 1
Loss after 4784048 batches: 0.0301
trigger times: 2
Loss after 4785007 batches: 0.0284
trigger times: 0
Loss after 4785966 batches: 0.0282
trigger times: 0
Loss after 4786925 batches: 0.0274
trigger times: 1
Loss after 4787884 batches: 0.0270
trigger times: 2
Loss after 4788843 batches: 0.0263
trigger times: 3
Loss after 4789802 batches: 0.0258
trigger times: 4
Loss after 4790761 batches: 0.0254
trigger times: 5
Loss after 4791720 batches: 0.0254
trigger times: 6
Loss after 4792679 batches: 0.0253
trigger times: 7
Loss after 4793638 batches: 0.0245
trigger times: 8
Loss after 4794597 batches: 0.0249
trigger times: 9
Loss after 4795556 batches: 0.0246
trigger times: 10
Loss after 4796515 batches: 0.0245
trigger times: 0
Loss after 4797474 batches: 0.0243
trigger times: 1
Loss after 4798433 batches: 0.0239
trigger times: 2
Loss after 4799392 batches: 0.0228
trigger times: 3
Loss after 4800351 batches: 0.0229
trigger times: 4
Loss after 4801310 batches: 0.0230
trigger times: 5
Loss after 4802269 batches: 0.0220
trigger times: 6
Loss after 4803228 batches: 0.0236
trigger times: 7
Loss after 4804187 batches: 0.0220
trigger times: 8
Loss after 4805146 batches: 0.0218
trigger times: 9
Loss after 4806105 batches: 0.0223
trigger times: 10
Loss after 4807064 batches: 0.0223
trigger times: 11
Loss after 4808023 batches: 0.0214
trigger times: 12
Loss after 4808982 batches: 0.0218
trigger times: 13
Loss after 4809941 batches: 0.0214
trigger times: 14
Loss after 4810900 batches: 0.0207
trigger times: 15
Loss after 4811859 batches: 0.0200
trigger times: 16
Loss after 4812818 batches: 0.0211
trigger times: 17
Loss after 4813777 batches: 0.0214
trigger times: 18
Loss after 4814736 batches: 0.0217
trigger times: 19
Loss after 4815695 batches: 0.0210
trigger times: 20
Loss after 4816654 batches: 0.0203
trigger times: 0
Loss after 4817613 batches: 0.0200
trigger times: 1
Loss after 4818572 batches: 0.0193
trigger times: 2
Loss after 4819531 batches: 0.0196
trigger times: 3
Loss after 4820490 batches: 0.0203
trigger times: 4
Loss after 4821449 batches: 0.0195
trigger times: 5
Loss after 4822408 batches: 0.0196
trigger times: 6
Loss after 4823367 batches: 0.0199
trigger times: 7
Loss after 4824326 batches: 0.0214
trigger times: 8
Loss after 4825285 batches: 0.0197
trigger times: 9
Loss after 4826244 batches: 0.0199
trigger times: 10
Loss after 4827203 batches: 0.0188
trigger times: 11
Loss after 4828162 batches: 0.0197
trigger times: 0
Loss after 4829121 batches: 0.0195
trigger times: 1
Loss after 4830080 batches: 0.0194
trigger times: 2
Loss after 4831039 batches: 0.0185
trigger times: 3
Loss after 4831998 batches: 0.0183
trigger times: 0
Loss after 4832957 batches: 0.0191
trigger times: 1
Loss after 4833916 batches: 0.0191
trigger times: 2
Loss after 4834875 batches: 0.0180
trigger times: 3
Loss after 4835834 batches: 0.0176
trigger times: 0
Loss after 4836793 batches: 0.0173
trigger times: 1
Loss after 4837752 batches: 0.0179
trigger times: 2
Loss after 4838711 batches: 0.0182
trigger times: 3
Loss after 4839670 batches: 0.0190
trigger times: 4
Loss after 4840629 batches: 0.0189
trigger times: 5
Loss after 4841588 batches: 0.0178
trigger times: 6
Loss after 4842547 batches: 0.0174
trigger times: 7
Loss after 4843506 batches: 0.0168
trigger times: 8
Loss after 4844465 batches: 0.0171
trigger times: 9
Loss after 4845424 batches: 0.0174
trigger times: 10
Loss after 4846383 batches: 0.0180
trigger times: 11
Loss after 4847342 batches: 0.0175
trigger times: 12
Loss after 4848301 batches: 0.0168
trigger times: 13
Loss after 4849260 batches: 0.0178
trigger times: 14
Loss after 4850219 batches: 0.0179
trigger times: 15
Loss after 4851178 batches: 0.0172
trigger times: 16
Loss after 4852137 batches: 0.0170
trigger times: 17
Loss after 4853096 batches: 0.0169
trigger times: 18
Loss after 4854055 batches: 0.0157
trigger times: 19
Loss after 4855014 batches: 0.0172
trigger times: 20
Loss after 4855973 batches: 0.0195
trigger times: 21
Loss after 4856932 batches: 0.0174
trigger times: 22
Loss after 4857891 batches: 0.0173
trigger times: 23
Loss after 4858850 batches: 0.0168
trigger times: 24
Loss after 4859809 batches: 0.0163
trigger times: 25
Early stopping!
Start to test process.
Loss after 4860768 batches: 0.0162
Time to train on one home:  110.04929304122925
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4861731 batches: 0.0488
trigger times: 1
Loss after 4862694 batches: 0.0274
trigger times: 2
Loss after 4863657 batches: 0.0273
trigger times: 3
Loss after 4864620 batches: 0.0274
trigger times: 4
Loss after 4865583 batches: 0.0267
trigger times: 5
Loss after 4866546 batches: 0.0260
trigger times: 6
Loss after 4867509 batches: 0.0255
trigger times: 7
Loss after 4868472 batches: 0.0251
trigger times: 8
Loss after 4869435 batches: 0.0249
trigger times: 9
Loss after 4870398 batches: 0.0245
trigger times: 10
Loss after 4871361 batches: 0.0243
trigger times: 11
Loss after 4872324 batches: 0.0241
trigger times: 12
Loss after 4873287 batches: 0.0240
trigger times: 13
Loss after 4874250 batches: 0.0240
trigger times: 14
Loss after 4875213 batches: 0.0238
trigger times: 15
Loss after 4876176 batches: 0.0236
trigger times: 16
Loss after 4877139 batches: 0.0235
trigger times: 17
Loss after 4878102 batches: 0.0232
trigger times: 18
Loss after 4879065 batches: 0.0231
trigger times: 19
Loss after 4880028 batches: 0.0227
trigger times: 20
Loss after 4880991 batches: 0.0227
trigger times: 21
Loss after 4881954 batches: 0.0224
trigger times: 22
Loss after 4882917 batches: 0.0222
trigger times: 23
Loss after 4883880 batches: 0.0226
trigger times: 24
Loss after 4884843 batches: 0.0226
trigger times: 25
Early stopping!
Start to test process.
Loss after 4885806 batches: 0.0222
Time to train on one home:  56.0059027671814
trigger times: 0
Loss after 4886751 batches: 0.0719
trigger times: 0
Loss after 4887696 batches: 0.0604
trigger times: 0
Loss after 4888641 batches: 0.0526
trigger times: 0
Loss after 4889586 batches: 0.0451
trigger times: 0
Loss after 4890531 batches: 0.0414
trigger times: 1
Loss after 4891476 batches: 0.0404
trigger times: 2
Loss after 4892421 batches: 0.0391
trigger times: 0
Loss after 4893366 batches: 0.0379
trigger times: 1
Loss after 4894311 batches: 0.0366
trigger times: 2
Loss after 4895256 batches: 0.0364
trigger times: 3
Loss after 4896201 batches: 0.0357
trigger times: 4
Loss after 4897146 batches: 0.0333
trigger times: 5
Loss after 4898091 batches: 0.0327
trigger times: 6
Loss after 4899036 batches: 0.0326
trigger times: 7
Loss after 4899981 batches: 0.0335
trigger times: 8
Loss after 4900926 batches: 0.0327
trigger times: 9
Loss after 4901871 batches: 0.0309
trigger times: 10
Loss after 4902816 batches: 0.0320
trigger times: 11
Loss after 4903761 batches: 0.0300
trigger times: 12
Loss after 4904706 batches: 0.0302
trigger times: 13
Loss after 4905651 batches: 0.0299
trigger times: 14
Loss after 4906596 batches: 0.0286
trigger times: 15
Loss after 4907541 batches: 0.0306
trigger times: 16
Loss after 4908486 batches: 0.0292
trigger times: 17
Loss after 4909431 batches: 0.0290
trigger times: 18
Loss after 4910376 batches: 0.0302
trigger times: 19
Loss after 4911321 batches: 0.0290
trigger times: 20
Loss after 4912266 batches: 0.0291
trigger times: 21
Loss after 4913211 batches: 0.0290
trigger times: 22
Loss after 4914156 batches: 0.0287
trigger times: 23
Loss after 4915101 batches: 0.0290
trigger times: 24
Loss after 4916046 batches: 0.0270
trigger times: 25
Early stopping!
Start to test process.
Loss after 4916991 batches: 0.0273
Time to train on one home:  62.738662242889404
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4917928 batches: 0.0895
trigger times: 1
Loss after 4918865 batches: 0.0804
trigger times: 2
Loss after 4919802 batches: 0.0807
trigger times: 0
Loss after 4920739 batches: 0.0762
trigger times: 1
Loss after 4921676 batches: 0.0740
trigger times: 2
Loss after 4922613 batches: 0.0728
trigger times: 3
Loss after 4923550 batches: 0.0704
trigger times: 4
Loss after 4924487 batches: 0.0706
trigger times: 5
Loss after 4925424 batches: 0.0698
trigger times: 6
Loss after 4926361 batches: 0.0675
trigger times: 7
Loss after 4927298 batches: 0.0672
trigger times: 8
Loss after 4928235 batches: 0.0659
trigger times: 9
Loss after 4929172 batches: 0.0657
trigger times: 10
Loss after 4930109 batches: 0.0644
trigger times: 11
Loss after 4931046 batches: 0.0661
trigger times: 12
Loss after 4931983 batches: 0.0643
trigger times: 13
Loss after 4932920 batches: 0.0633
trigger times: 14
Loss after 4933857 batches: 0.0626
trigger times: 15
Loss after 4934794 batches: 0.0615
trigger times: 16
Loss after 4935731 batches: 0.0609
trigger times: 17
Loss after 4936668 batches: 0.0603
trigger times: 18
Loss after 4937605 batches: 0.0628
trigger times: 19
Loss after 4938542 batches: 0.0617
trigger times: 20
Loss after 4939479 batches: 0.0635
trigger times: 21
Loss after 4940416 batches: 0.0595
trigger times: 22
Loss after 4941353 batches: 0.0606
trigger times: 23
Loss after 4942290 batches: 0.0595
trigger times: 24
Loss after 4943227 batches: 0.0607
trigger times: 25
Early stopping!
Start to test process.
Loss after 4944164 batches: 0.0596
Time to train on one home:  59.14129638671875
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 4945127 batches: 0.0292
trigger times: 1
Loss after 4946090 batches: 0.0152
trigger times: 2
Loss after 4947053 batches: 0.0144
trigger times: 3
Loss after 4948016 batches: 0.0141
trigger times: 4
Loss after 4948979 batches: 0.0141
trigger times: 5
Loss after 4949942 batches: 0.0142
trigger times: 6
Loss after 4950905 batches: 0.0140
trigger times: 7
Loss after 4951868 batches: 0.0138
trigger times: 8
Loss after 4952831 batches: 0.0138
trigger times: 9
Loss after 4953794 batches: 0.0139
trigger times: 10
Loss after 4954757 batches: 0.0140
trigger times: 11
Loss after 4955720 batches: 0.0137
trigger times: 12
Loss after 4956683 batches: 0.0137
trigger times: 13
Loss after 4957646 batches: 0.0136
trigger times: 14
Loss after 4958609 batches: 0.0140
trigger times: 15
Loss after 4959572 batches: 0.0137
trigger times: 16
Loss after 4960535 batches: 0.0138
trigger times: 17
Loss after 4961498 batches: 0.0138
trigger times: 18
Loss after 4962461 batches: 0.0140
trigger times: 19
Loss after 4963424 batches: 0.0137
trigger times: 20
Loss after 4964387 batches: 0.0136
trigger times: 21
Loss after 4965350 batches: 0.0139
trigger times: 22
Loss after 4966313 batches: 0.0139
trigger times: 23
Loss after 4967276 batches: 0.0135
trigger times: 24
Loss after 4968239 batches: 0.0137
trigger times: 25
Early stopping!
Start to test process.
Loss after 4969202 batches: 0.0135
Time to train on one home:  52.962886333465576
trigger times: 0
Loss after 4970165 batches: 0.0974
trigger times: 1
Loss after 4971128 batches: 0.0851
trigger times: 2
Loss after 4972091 batches: 0.0865
trigger times: 3
Loss after 4973054 batches: 0.0813
trigger times: 4
Loss after 4974017 batches: 0.0778
trigger times: 5
Loss after 4974980 batches: 0.0755
trigger times: 6
Loss after 4975943 batches: 0.0740
trigger times: 7
Loss after 4976906 batches: 0.0717
trigger times: 8
Loss after 4977869 batches: 0.0706
trigger times: 9
Loss after 4978832 batches: 0.0700
trigger times: 10
Loss after 4979795 batches: 0.0704
trigger times: 11
Loss after 4980758 batches: 0.0685
trigger times: 12
Loss after 4981721 batches: 0.0678
trigger times: 13
Loss after 4982684 batches: 0.0680
trigger times: 14
Loss after 4983647 batches: 0.0669
trigger times: 15
Loss after 4984610 batches: 0.0664
trigger times: 16
Loss after 4985573 batches: 0.0662
trigger times: 17
Loss after 4986536 batches: 0.0661
trigger times: 18
Loss after 4987499 batches: 0.0661
trigger times: 19
Loss after 4988462 batches: 0.0663
trigger times: 20
Loss after 4989425 batches: 0.0661
trigger times: 21
Loss after 4990388 batches: 0.0646
trigger times: 22
Loss after 4991351 batches: 0.0648
trigger times: 23
Loss after 4992314 batches: 0.0642
trigger times: 24
Loss after 4993277 batches: 0.0655
trigger times: 25
Early stopping!
Start to test process.
Loss after 4994240 batches: 0.0645
Time to train on one home:  56.45868158340454
trigger times: 0
Loss after 4995203 batches: 0.0881
trigger times: 1
Loss after 4996166 batches: 0.0752
trigger times: 2
Loss after 4997129 batches: 0.0695
trigger times: 3
Loss after 4998092 batches: 0.0649
trigger times: 4
Loss after 4999055 batches: 0.0631
trigger times: 5
Loss after 5000018 batches: 0.0596
trigger times: 6
Loss after 5000981 batches: 0.0584
trigger times: 7
Loss after 5001944 batches: 0.0566
trigger times: 8
Loss after 5002907 batches: 0.0544
trigger times: 9
Loss after 5003870 batches: 0.0531
trigger times: 10
Loss after 5004833 batches: 0.0516
trigger times: 11
Loss after 5005796 batches: 0.0514
trigger times: 12
Loss after 5006759 batches: 0.0499
trigger times: 13
Loss after 5007722 batches: 0.0499
trigger times: 14
Loss after 5008685 batches: 0.0490
trigger times: 15
Loss after 5009648 batches: 0.0487
trigger times: 16
Loss after 5010611 batches: 0.0472
trigger times: 17
Loss after 5011574 batches: 0.0471
trigger times: 18
Loss after 5012537 batches: 0.0464
trigger times: 19
Loss after 5013500 batches: 0.0456
trigger times: 20
Loss after 5014463 batches: 0.0448
trigger times: 21
Loss after 5015426 batches: 0.0439
trigger times: 22
Loss after 5016389 batches: 0.0443
trigger times: 23
Loss after 5017352 batches: 0.0439
trigger times: 24
Loss after 5018315 batches: 0.0435
trigger times: 25
Early stopping!
Start to test process.
Loss after 5019278 batches: 0.0428
Time to train on one home:  56.425527572631836
trigger times: 0
Loss after 5020174 batches: 0.1075
trigger times: 1
Loss after 5021070 batches: 0.0996
trigger times: 2
Loss after 5021966 batches: 0.0960
trigger times: 3
Loss after 5022862 batches: 0.0916
trigger times: 4
Loss after 5023758 batches: 0.0889
trigger times: 5
Loss after 5024654 batches: 0.0881
trigger times: 6
Loss after 5025550 batches: 0.0865
trigger times: 7
Loss after 5026446 batches: 0.0859
trigger times: 8
Loss after 5027342 batches: 0.0834
trigger times: 9
Loss after 5028238 batches: 0.0809
trigger times: 10
Loss after 5029134 batches: 0.0802
trigger times: 11
Loss after 5030030 batches: 0.0794
trigger times: 12
Loss after 5030926 batches: 0.0790
trigger times: 13
Loss after 5031822 batches: 0.0781
trigger times: 14
Loss after 5032718 batches: 0.0770
trigger times: 15
Loss after 5033614 batches: 0.0759
trigger times: 16
Loss after 5034510 batches: 0.0770
trigger times: 17
Loss after 5035406 batches: 0.0783
trigger times: 18
Loss after 5036302 batches: 0.0767
trigger times: 19
Loss after 5037198 batches: 0.0779
trigger times: 20
Loss after 5038094 batches: 0.0753
trigger times: 21
Loss after 5038990 batches: 0.0733
trigger times: 22
Loss after 5039886 batches: 0.0741
trigger times: 23
Loss after 5040782 batches: 0.0747
trigger times: 24
Loss after 5041678 batches: 0.0724
trigger times: 25
Early stopping!
Start to test process.
Loss after 5042574 batches: 0.0722
Time to train on one home:  54.64094281196594
trigger times: 0
Loss after 5043537 batches: 0.1547
trigger times: 1
Loss after 5044500 batches: 0.1134
trigger times: 2
Loss after 5045463 batches: 0.1117
trigger times: 3
Loss after 5046426 batches: 0.1025
trigger times: 4
Loss after 5047389 batches: 0.0939
trigger times: 5
Loss after 5048352 batches: 0.0879
trigger times: 6
Loss after 5049315 batches: 0.0874
trigger times: 7
Loss after 5050278 batches: 0.0857
trigger times: 8
Loss after 5051241 batches: 0.0834
trigger times: 9
Loss after 5052204 batches: 0.0831
trigger times: 10
Loss after 5053167 batches: 0.0819
trigger times: 11
Loss after 5054130 batches: 0.0810
trigger times: 12
Loss after 5055093 batches: 0.0800
trigger times: 13
Loss after 5056056 batches: 0.0787
trigger times: 14
Loss after 5057019 batches: 0.0778
trigger times: 15
Loss after 5057982 batches: 0.0765
trigger times: 16
Loss after 5058945 batches: 0.0769
trigger times: 17
Loss after 5059908 batches: 0.0747
trigger times: 18
Loss after 5060871 batches: 0.0719
trigger times: 19
Loss after 5061834 batches: 0.0713
trigger times: 20
Loss after 5062797 batches: 0.0690
trigger times: 21
Loss after 5063760 batches: 0.0695
trigger times: 22
Loss after 5064723 batches: 0.0678
trigger times: 23
Loss after 5065686 batches: 0.0663
trigger times: 24
Loss after 5066649 batches: 0.0655
trigger times: 25
Early stopping!
Start to test process.
Loss after 5067612 batches: 0.0657
Time to train on one home:  58.67662858963013
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5068575 batches: 0.0848
trigger times: 1
Loss after 5069538 batches: 0.0796
trigger times: 2
Loss after 5070501 batches: 0.0804
trigger times: 3
Loss after 5071464 batches: 0.0773
trigger times: 4
Loss after 5072427 batches: 0.0758
trigger times: 5
Loss after 5073390 batches: 0.0749
trigger times: 6
Loss after 5074353 batches: 0.0736
trigger times: 7
Loss after 5075316 batches: 0.0746
trigger times: 8
Loss after 5076279 batches: 0.0732
trigger times: 9
Loss after 5077242 batches: 0.0719
trigger times: 10
Loss after 5078205 batches: 0.0708
trigger times: 11
Loss after 5079168 batches: 0.0701
trigger times: 12
Loss after 5080131 batches: 0.0725
trigger times: 13
Loss after 5081094 batches: 0.0711
trigger times: 14
Loss after 5082057 batches: 0.0701
trigger times: 15
Loss after 5083020 batches: 0.0705
trigger times: 16
Loss after 5083983 batches: 0.0698
trigger times: 17
Loss after 5084946 batches: 0.0696
trigger times: 18
Loss after 5085909 batches: 0.0692
trigger times: 19
Loss after 5086872 batches: 0.0687
trigger times: 20
Loss after 5087835 batches: 0.0687
trigger times: 21
Loss after 5088798 batches: 0.0682
trigger times: 22
Loss after 5089761 batches: 0.0701
trigger times: 23
Loss after 5090724 batches: 0.0701
trigger times: 24
Loss after 5091687 batches: 0.0685
trigger times: 25
Early stopping!
Start to test process.
Loss after 5092650 batches: 0.0695
Time to train on one home:  56.30231809616089
trigger times: 0
Loss after 5093613 batches: 0.0944
trigger times: 0
Loss after 5094576 batches: 0.0789
trigger times: 1
Loss after 5095539 batches: 0.0742
trigger times: 2
Loss after 5096502 batches: 0.0701
trigger times: 3
Loss after 5097465 batches: 0.0642
trigger times: 4
Loss after 5098428 batches: 0.0628
trigger times: 5
Loss after 5099391 batches: 0.0599
trigger times: 6
Loss after 5100354 batches: 0.0588
trigger times: 7
Loss after 5101317 batches: 0.0575
trigger times: 8
Loss after 5102280 batches: 0.0558
trigger times: 9
Loss after 5103243 batches: 0.0542
trigger times: 10
Loss after 5104206 batches: 0.0534
trigger times: 11
Loss after 5105169 batches: 0.0525
trigger times: 12
Loss after 5106132 batches: 0.0536
trigger times: 13
Loss after 5107095 batches: 0.0526
trigger times: 14
Loss after 5108058 batches: 0.0522
trigger times: 15
Loss after 5109021 batches: 0.0511
trigger times: 16
Loss after 5109984 batches: 0.0525
trigger times: 17
Loss after 5110947 batches: 0.0516
trigger times: 18
Loss after 5111910 batches: 0.0505
trigger times: 19
Loss after 5112873 batches: 0.0496
trigger times: 20
Loss after 5113836 batches: 0.0511
trigger times: 21
Loss after 5114799 batches: 0.0493
trigger times: 22
Loss after 5115762 batches: 0.0493
trigger times: 23
Loss after 5116725 batches: 0.0509
trigger times: 24
Loss after 5117688 batches: 0.0490
trigger times: 25
Early stopping!
Start to test process.
Loss after 5118651 batches: 0.0484
Time to train on one home:  58.02267503738403
trigger times: 0
Loss after 5119614 batches: 0.0605
trigger times: 1
Loss after 5120577 batches: 0.0541
trigger times: 2
Loss after 5121540 batches: 0.0520
trigger times: 3
Loss after 5122503 batches: 0.0491
trigger times: 4
Loss after 5123466 batches: 0.0466
trigger times: 5
Loss after 5124429 batches: 0.0450
trigger times: 6
Loss after 5125392 batches: 0.0435
trigger times: 7
Loss after 5126355 batches: 0.0426
trigger times: 8
Loss after 5127318 batches: 0.0418
trigger times: 9
Loss after 5128281 batches: 0.0405
trigger times: 10
Loss after 5129244 batches: 0.0419
trigger times: 11
Loss after 5130207 batches: 0.0419
trigger times: 12
Loss after 5131170 batches: 0.0406
trigger times: 13
Loss after 5132133 batches: 0.0393
trigger times: 14
Loss after 5133096 batches: 0.0387
trigger times: 15
Loss after 5134059 batches: 0.0394
trigger times: 16
Loss after 5135022 batches: 0.0384
trigger times: 17
Loss after 5135985 batches: 0.0377
trigger times: 18
Loss after 5136948 batches: 0.0367
trigger times: 19
Loss after 5137911 batches: 0.0366
trigger times: 20
Loss after 5138874 batches: 0.0356
trigger times: 21
Loss after 5139837 batches: 0.0351
trigger times: 22
Loss after 5140800 batches: 0.0363
trigger times: 23
Loss after 5141763 batches: 0.0353
trigger times: 24
Loss after 5142726 batches: 0.0344
trigger times: 25
Early stopping!
Start to test process.
Loss after 5143689 batches: 0.0349
Time to train on one home:  56.931984186172485
trigger times: 0
Loss after 5144652 batches: 0.0662
trigger times: 1
Loss after 5145615 batches: 0.0494
trigger times: 2
Loss after 5146578 batches: 0.0498
trigger times: 3
Loss after 5147541 batches: 0.0476
trigger times: 4
Loss after 5148504 batches: 0.0473
trigger times: 5
Loss after 5149467 batches: 0.0462
trigger times: 6
Loss after 5150430 batches: 0.0456
trigger times: 7
Loss after 5151393 batches: 0.0435
trigger times: 8
Loss after 5152356 batches: 0.0437
trigger times: 9
Loss after 5153319 batches: 0.0431
trigger times: 10
Loss after 5154282 batches: 0.0422
trigger times: 11
Loss after 5155245 batches: 0.0420
trigger times: 12
Loss after 5156208 batches: 0.0415
trigger times: 13
Loss after 5157171 batches: 0.0412
trigger times: 14
Loss after 5158134 batches: 0.0405
trigger times: 15
Loss after 5159097 batches: 0.0404
trigger times: 16
Loss after 5160060 batches: 0.0400
trigger times: 17
Loss after 5161023 batches: 0.0401
trigger times: 18
Loss after 5161986 batches: 0.0403
trigger times: 19
Loss after 5162949 batches: 0.0395
trigger times: 20
Loss after 5163912 batches: 0.0398
trigger times: 21
Loss after 5164875 batches: 0.0403
trigger times: 22
Loss after 5165838 batches: 0.0398
trigger times: 23
Loss after 5166801 batches: 0.0396
trigger times: 24
Loss after 5167764 batches: 0.0394
trigger times: 25
Early stopping!
Start to test process.
Loss after 5168727 batches: 0.0392
Time to train on one home:  53.678919553756714
trigger times: 0
Loss after 5169622 batches: 0.0483
trigger times: 1
Loss after 5170517 batches: 0.0195
trigger times: 2
Loss after 5171412 batches: 0.0133
trigger times: 3
Loss after 5172307 batches: 0.0112
trigger times: 4
Loss after 5173202 batches: 0.0093
trigger times: 5
Loss after 5174097 batches: 0.0086
trigger times: 6
Loss after 5174992 batches: 0.0076
trigger times: 7
Loss after 5175887 batches: 0.0072
trigger times: 8
Loss after 5176782 batches: 0.0071
trigger times: 9
Loss after 5177677 batches: 0.0065
trigger times: 0
Loss after 5178572 batches: 0.0056
trigger times: 1
Loss after 5179467 batches: 0.0061
trigger times: 2
Loss after 5180362 batches: 0.0051
trigger times: 0
Loss after 5181257 batches: 0.0055
trigger times: 1
Loss after 5182152 batches: 0.0053
trigger times: 0
Loss after 5183047 batches: 0.0049
trigger times: 0
Loss after 5183942 batches: 0.0050
trigger times: 1
Loss after 5184837 batches: 0.0049
trigger times: 2
Loss after 5185732 batches: 0.0048
trigger times: 3
Loss after 5186627 batches: 0.0052
trigger times: 0
Loss after 5187522 batches: 0.0052
trigger times: 1
Loss after 5188417 batches: 0.0047
trigger times: 2
Loss after 5189312 batches: 0.0043
trigger times: 3
Loss after 5190207 batches: 0.0041
trigger times: 4
Loss after 5191102 batches: 0.0041
trigger times: 5
Loss after 5191997 batches: 0.0044
trigger times: 6
Loss after 5192892 batches: 0.0042
trigger times: 7
Loss after 5193787 batches: 0.0040
trigger times: 8
Loss after 5194682 batches: 0.0040
trigger times: 9
Loss after 5195577 batches: 0.0042
trigger times: 0
Loss after 5196472 batches: 0.0036
trigger times: 1
Loss after 5197367 batches: 0.0042
trigger times: 2
Loss after 5198262 batches: 0.0037
trigger times: 3
Loss after 5199157 batches: 0.0033
trigger times: 4
Loss after 5200052 batches: 0.0036
trigger times: 5
Loss after 5200947 batches: 0.0034
trigger times: 6
Loss after 5201842 batches: 0.0046
trigger times: 7
Loss after 5202737 batches: 0.0043
trigger times: 8
Loss after 5203632 batches: 0.0037
trigger times: 9
Loss after 5204527 batches: 0.0044
trigger times: 10
Loss after 5205422 batches: 0.0033
trigger times: 11
Loss after 5206317 batches: 0.0036
trigger times: 12
Loss after 5207212 batches: 0.0029
trigger times: 13
Loss after 5208107 batches: 0.0025
trigger times: 0
Loss after 5209002 batches: 0.0026
trigger times: 1
Loss after 5209897 batches: 0.0028
trigger times: 2
Loss after 5210792 batches: 0.0028
trigger times: 3
Loss after 5211687 batches: 0.0040
trigger times: 4
Loss after 5212582 batches: 0.0038
trigger times: 5
Loss after 5213477 batches: 0.0043
trigger times: 6
Loss after 5214372 batches: 0.0038
trigger times: 7
Loss after 5215267 batches: 0.0032
trigger times: 8
Loss after 5216162 batches: 0.0035
trigger times: 9
Loss after 5217057 batches: 0.0043
trigger times: 10
Loss after 5217952 batches: 0.0036
trigger times: 11
Loss after 5218847 batches: 0.0034
trigger times: 12
Loss after 5219742 batches: 0.0033
trigger times: 13
Loss after 5220637 batches: 0.0029
trigger times: 14
Loss after 5221532 batches: 0.0034
trigger times: 15
Loss after 5222427 batches: 0.0031
trigger times: 16
Loss after 5223322 batches: 0.0034
trigger times: 17
Loss after 5224217 batches: 0.0027
trigger times: 18
Loss after 5225112 batches: 0.0029
trigger times: 19
Loss after 5226007 batches: 0.0032
trigger times: 0
Loss after 5226902 batches: 0.0028
trigger times: 1
Loss after 5227797 batches: 0.0030
trigger times: 2
Loss after 5228692 batches: 0.0031
trigger times: 3
Loss after 5229587 batches: 0.0029
trigger times: 4
Loss after 5230482 batches: 0.0028
trigger times: 5
Loss after 5231377 batches: 0.0024
trigger times: 6
Loss after 5232272 batches: 0.0025
trigger times: 7
Loss after 5233167 batches: 0.0025
trigger times: 8
Loss after 5234062 batches: 0.0025
trigger times: 9
Loss after 5234957 batches: 0.0024
trigger times: 10
Loss after 5235852 batches: 0.0024
trigger times: 11
Loss after 5236747 batches: 0.0022
trigger times: 12
Loss after 5237642 batches: 0.0024
trigger times: 13
Loss after 5238537 batches: 0.0025
trigger times: 14
Loss after 5239432 batches: 0.0021
trigger times: 15
Loss after 5240327 batches: 0.0024
trigger times: 16
Loss after 5241222 batches: 0.0022
trigger times: 17
Loss after 5242117 batches: 0.0021
trigger times: 18
Loss after 5243012 batches: 0.0023
trigger times: 19
Loss after 5243907 batches: 0.0022
trigger times: 20
Loss after 5244802 batches: 0.0022
trigger times: 21
Loss after 5245697 batches: 0.0019
trigger times: 22
Loss after 5246592 batches: 0.0021
trigger times: 23
Loss after 5247487 batches: 0.0023
trigger times: 24
Loss after 5248382 batches: 0.0023
trigger times: 25
Early stopping!
Start to test process.
Loss after 5249277 batches: 0.0023
Time to train on one home:  103.03105998039246
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882]]
Round_3_results:  [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882]
trigger times: 0
Loss after 5250240 batches: 0.0690
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5506 < 5507; dropping {'Training_Loss': 0.06904151609965734, 'Validation_Loss': 0.06922151148319244, 'Training_R2': 0.17305075763450772, 'Validation_R2': 0.04237773053410587, 'Training_F1': 0.40271883587662216, 'Validation_F1': 0.254775105634391, 'Training_NEP': 0.907481283587532, 'Validation_NEP': 0.9331226498220987, 'Training_NDE': 0.6090104328476523, 'Validation_NDE': 0.743112873158743, 'Training_MAE': 26.131077204443248, 'Validation_MAE': 31.082533679150462, 'Training_MSE': 1916.0564, 'Validation_MSE': 3680.935}.
trigger times: 0
Loss after 5251203 batches: 0.0537
trigger times: 0
Loss after 5252166 batches: 0.0512
trigger times: 1
Loss after 5253129 batches: 0.0466
trigger times: 2
Loss after 5254092 batches: 0.0444
trigger times: 3
Loss after 5255055 batches: 0.0428
trigger times: 4
Loss after 5256018 batches: 0.0437
trigger times: 0
Loss after 5256981 batches: 0.0424
trigger times: 1
Loss after 5257944 batches: 0.0410
trigger times: 2
Loss after 5258907 batches: 0.0408
trigger times: 3
Loss after 5259870 batches: 0.0414
trigger times: 4
Loss after 5260833 batches: 0.0412
trigger times: 5
Loss after 5261796 batches: 0.0404
trigger times: 6
Loss after 5262759 batches: 0.0398
trigger times: 7
Loss after 5263722 batches: 0.0395
trigger times: 8
Loss after 5264685 batches: 0.0391
trigger times: 9
Loss after 5265648 batches: 0.0391
trigger times: 10
Loss after 5266611 batches: 0.0386
trigger times: 11
Loss after 5267574 batches: 0.0388
trigger times: 12
Loss after 5268537 batches: 0.0381
trigger times: 13
Loss after 5269500 batches: 0.0380
trigger times: 14
Loss after 5270463 batches: 0.0376
trigger times: 15
Loss after 5271426 batches: 0.0374
trigger times: 16
Loss after 5272389 batches: 0.0379
trigger times: 17
Loss after 5273352 batches: 0.0371
trigger times: 18
Loss after 5274315 batches: 0.0372
trigger times: 19
Loss after 5275278 batches: 0.0377
trigger times: 20
Loss after 5276241 batches: 0.0355
trigger times: 21
Loss after 5277204 batches: 0.0369
trigger times: 22
Loss after 5278167 batches: 0.0380
trigger times: 23
Loss after 5279130 batches: 0.0363
trigger times: 24
Loss after 5280093 batches: 0.0360
trigger times: 25
Early stopping!
Start to test process.
Loss after 5281056 batches: 0.0357
Time to train on one home:  60.80870246887207
trigger times: 0
Loss after 5282014 batches: 0.0942
trigger times: 0
Loss after 5282972 batches: 0.0505
trigger times: 1
Loss after 5283930 batches: 0.0479
trigger times: 2
Loss after 5284888 batches: 0.0412
trigger times: 3
Loss after 5285846 batches: 0.0367
trigger times: 4
Loss after 5286804 batches: 0.0350
trigger times: 5
Loss after 5287762 batches: 0.0335
trigger times: 6
Loss after 5288720 batches: 0.0319
trigger times: 7
Loss after 5289678 batches: 0.0312
trigger times: 8
Loss after 5290636 batches: 0.0296
trigger times: 0
Loss after 5291594 batches: 0.0279
trigger times: 1
Loss after 5292552 batches: 0.0292
trigger times: 2
Loss after 5293510 batches: 0.0272
trigger times: 3
Loss after 5294468 batches: 0.0276
trigger times: 4
Loss after 5295426 batches: 0.0261
trigger times: 0
Loss after 5296384 batches: 0.0255
trigger times: 1
Loss after 5297342 batches: 0.0263
trigger times: 2
Loss after 5298300 batches: 0.0254
trigger times: 3
Loss after 5299258 batches: 0.0259
trigger times: 4
Loss after 5300216 batches: 0.0248
trigger times: 5
Loss after 5301174 batches: 0.0247
trigger times: 6
Loss after 5302132 batches: 0.0259
trigger times: 7
Loss after 5303090 batches: 0.0255
trigger times: 8
Loss after 5304048 batches: 0.0246
trigger times: 9
Loss after 5305006 batches: 0.0249
trigger times: 10
Loss after 5305964 batches: 0.0245
trigger times: 11
Loss after 5306922 batches: 0.0267
trigger times: 12
Loss after 5307880 batches: 0.0250
trigger times: 13
Loss after 5308838 batches: 0.0234
trigger times: 14
Loss after 5309796 batches: 0.0240
trigger times: 15
Loss after 5310754 batches: 0.0254
trigger times: 16
Loss after 5311712 batches: 0.0239
trigger times: 17
Loss after 5312670 batches: 0.0237
trigger times: 18
Loss after 5313628 batches: 0.0272
trigger times: 19
Loss after 5314586 batches: 0.0273
trigger times: 20
Loss after 5315544 batches: 0.0275
trigger times: 0
Loss after 5316502 batches: 0.0262
trigger times: 1
Loss after 5317460 batches: 0.0251
trigger times: 2
Loss after 5318418 batches: 0.0245
trigger times: 3
Loss after 5319376 batches: 0.0245
trigger times: 4
Loss after 5320334 batches: 0.0234
trigger times: 5
Loss after 5321292 batches: 0.0230
trigger times: 6
Loss after 5322250 batches: 0.0228
trigger times: 7
Loss after 5323208 batches: 0.0225
trigger times: 8
Loss after 5324166 batches: 0.0308
trigger times: 9
Loss after 5325124 batches: 0.0283
trigger times: 10
Loss after 5326082 batches: 0.0256
trigger times: 11
Loss after 5327040 batches: 0.0235
trigger times: 12
Loss after 5327998 batches: 0.0231
trigger times: 13
Loss after 5328956 batches: 0.0233
trigger times: 14
Loss after 5329914 batches: 0.0232
trigger times: 0
Loss after 5330872 batches: 0.0238
trigger times: 1
Loss after 5331830 batches: 0.0227
trigger times: 2
Loss after 5332788 batches: 0.0236
trigger times: 3
Loss after 5333746 batches: 0.0220
trigger times: 4
Loss after 5334704 batches: 0.0218
trigger times: 5
Loss after 5335662 batches: 0.0215
trigger times: 6
Loss after 5336620 batches: 0.0215
trigger times: 7
Loss after 5337578 batches: 0.0210
trigger times: 8
Loss after 5338536 batches: 0.0200
trigger times: 9
Loss after 5339494 batches: 0.0195
trigger times: 10
Loss after 5340452 batches: 0.0211
trigger times: 0
Loss after 5341410 batches: 0.0207
trigger times: 1
Loss after 5342368 batches: 0.0198
trigger times: 2
Loss after 5343326 batches: 0.0217
trigger times: 3
Loss after 5344284 batches: 0.0204
trigger times: 4
Loss after 5345242 batches: 0.0204
trigger times: 5
Loss after 5346200 batches: 0.0211
trigger times: 6
Loss after 5347158 batches: 0.0237
trigger times: 7
Loss after 5348116 batches: 0.0225
trigger times: 8
Loss after 5349074 batches: 0.0206
trigger times: 9
Loss after 5350032 batches: 0.0218
trigger times: 10
Loss after 5350990 batches: 0.0217
trigger times: 11
Loss after 5351948 batches: 0.0205
trigger times: 12
Loss after 5352906 batches: 0.0196
trigger times: 0
Loss after 5353864 batches: 0.0202
trigger times: 1
Loss after 5354822 batches: 0.0195
trigger times: 2
Loss after 5355780 batches: 0.0197
trigger times: 3
Loss after 5356738 batches: 0.0194
trigger times: 4
Loss after 5357696 batches: 0.0185
trigger times: 0
Loss after 5358654 batches: 0.0188
trigger times: 1
Loss after 5359612 batches: 0.0194
trigger times: 2
Loss after 5360570 batches: 0.0181
trigger times: 3
Loss after 5361528 batches: 0.0185
trigger times: 4
Loss after 5362486 batches: 0.0174
trigger times: 5
Loss after 5363444 batches: 0.0184
trigger times: 6
Loss after 5364402 batches: 0.0191
trigger times: 0
Loss after 5365360 batches: 0.0194
trigger times: 1
Loss after 5366318 batches: 0.0179
trigger times: 2
Loss after 5367276 batches: 0.0189
trigger times: 3
Loss after 5368234 batches: 0.0181
trigger times: 4
Loss after 5369192 batches: 0.0182
trigger times: 5
Loss after 5370150 batches: 0.0176
trigger times: 6
Loss after 5371108 batches: 0.0174
trigger times: 7
Loss after 5372066 batches: 0.0168
trigger times: 8
Loss after 5373024 batches: 0.0163
trigger times: 9
Loss after 5373982 batches: 0.0170
trigger times: 10
Loss after 5374940 batches: 0.0163
trigger times: 11
Loss after 5375898 batches: 0.0169
trigger times: 12
Loss after 5376856 batches: 0.0166
trigger times: 13
Loss after 5377814 batches: 0.0172
trigger times: 14
Loss after 5378772 batches: 0.0181
trigger times: 15
Loss after 5379730 batches: 0.0186
trigger times: 16
Loss after 5380688 batches: 0.0174
trigger times: 17
Loss after 5381646 batches: 0.0166
trigger times: 18
Loss after 5382604 batches: 0.0159
trigger times: 19
Loss after 5383562 batches: 0.0160
trigger times: 20
Loss after 5384520 batches: 0.0159
trigger times: 21
Loss after 5385478 batches: 0.0171
trigger times: 22
Loss after 5386436 batches: 0.0172
trigger times: 23
Loss after 5387394 batches: 0.0160
trigger times: 24
Loss after 5388352 batches: 0.0165
trigger times: 25
Early stopping!
Start to test process.
Loss after 5389310 batches: 0.0150
Time to train on one home:  130.83376240730286
trigger times: 0
Loss after 5390273 batches: 0.0769
trigger times: 1
Loss after 5391236 batches: 0.0676
trigger times: 2
Loss after 5392199 batches: 0.0666
trigger times: 3
Loss after 5393162 batches: 0.0648
trigger times: 4
Loss after 5394125 batches: 0.0640
trigger times: 5
Loss after 5395088 batches: 0.0620
trigger times: 6
Loss after 5396051 batches: 0.0619
trigger times: 7
Loss after 5397014 batches: 0.0603
trigger times: 8
Loss after 5397977 batches: 0.0597
trigger times: 9
Loss after 5398940 batches: 0.0590
trigger times: 10
Loss after 5399903 batches: 0.0581
trigger times: 11
Loss after 5400866 batches: 0.0580
trigger times: 12
Loss after 5401829 batches: 0.0585
trigger times: 13
Loss after 5402792 batches: 0.0568
trigger times: 14
Loss after 5403755 batches: 0.0581
trigger times: 15
Loss after 5404718 batches: 0.0580
trigger times: 16
Loss after 5405681 batches: 0.0567
trigger times: 17
Loss after 5406644 batches: 0.0563
trigger times: 18
Loss after 5407607 batches: 0.0564
trigger times: 19
Loss after 5408570 batches: 0.0558
trigger times: 20
Loss after 5409533 batches: 0.0551
trigger times: 21
Loss after 5410496 batches: 0.0566
trigger times: 22
Loss after 5411459 batches: 0.0542
trigger times: 23
Loss after 5412422 batches: 0.0553
trigger times: 24
Loss after 5413385 batches: 0.0556
trigger times: 25
Early stopping!
Start to test process.
Loss after 5414348 batches: 0.0548
Time to train on one home:  56.907678842544556
trigger times: 0
Loss after 5415311 batches: 0.0854
trigger times: 1
Loss after 5416274 batches: 0.0790
trigger times: 2
Loss after 5417237 batches: 0.0780
trigger times: 0
Loss after 5418200 batches: 0.0758
trigger times: 0
Loss after 5419163 batches: 0.0752
trigger times: 1
Loss after 5420126 batches: 0.0727
trigger times: 2
Loss after 5421089 batches: 0.0713
trigger times: 3
Loss after 5422052 batches: 0.0694
trigger times: 4
Loss after 5423015 batches: 0.0689
trigger times: 5
Loss after 5423978 batches: 0.0687
trigger times: 6
Loss after 5424941 batches: 0.0673
trigger times: 7
Loss after 5425904 batches: 0.0671
trigger times: 8
Loss after 5426867 batches: 0.0680
trigger times: 9
Loss after 5427830 batches: 0.0671
trigger times: 10
Loss after 5428793 batches: 0.0668
trigger times: 0
Loss after 5429756 batches: 0.0657
trigger times: 0
Loss after 5430719 batches: 0.0655
trigger times: 1
Loss after 5431682 batches: 0.0658
trigger times: 2
Loss after 5432645 batches: 0.0672
trigger times: 3
Loss after 5433608 batches: 0.0664
trigger times: 4
Loss after 5434571 batches: 0.0667
trigger times: 5
Loss after 5435534 batches: 0.0645
trigger times: 6
Loss after 5436497 batches: 0.0643
trigger times: 7
Loss after 5437460 batches: 0.0647
trigger times: 8
Loss after 5438423 batches: 0.0642
trigger times: 9
Loss after 5439386 batches: 0.0629
trigger times: 10
Loss after 5440349 batches: 0.0613
trigger times: 11
Loss after 5441312 batches: 0.0639
trigger times: 12
Loss after 5442275 batches: 0.0636
trigger times: 13
Loss after 5443238 batches: 0.0632
trigger times: 14
Loss after 5444201 batches: 0.0634
trigger times: 15
Loss after 5445164 batches: 0.0629
trigger times: 16
Loss after 5446127 batches: 0.0622
trigger times: 17
Loss after 5447090 batches: 0.0624
trigger times: 18
Loss after 5448053 batches: 0.0614
trigger times: 19
Loss after 5449016 batches: 0.0614
trigger times: 20
Loss after 5449979 batches: 0.0610
trigger times: 21
Loss after 5450942 batches: 0.0609
trigger times: 22
Loss after 5451905 batches: 0.0612
trigger times: 23
Loss after 5452868 batches: 0.0610
trigger times: 24
Loss after 5453831 batches: 0.0604
trigger times: 25
Early stopping!
Start to test process.
Loss after 5454794 batches: 0.0603
Time to train on one home:  70.24568772315979
trigger times: 0
Loss after 5455757 batches: 0.0334
trigger times: 0
Loss after 5456720 batches: 0.0308
trigger times: 1
Loss after 5457683 batches: 0.0281
trigger times: 2
Loss after 5458646 batches: 0.0270
trigger times: 3
Loss after 5459609 batches: 0.0252
trigger times: 4
Loss after 5460572 batches: 0.0240
trigger times: 5
Loss after 5461535 batches: 0.0228
trigger times: 6
Loss after 5462498 batches: 0.0228
trigger times: 7
Loss after 5463461 batches: 0.0222
trigger times: 8
Loss after 5464424 batches: 0.0223
trigger times: 9
Loss after 5465387 batches: 0.0215
trigger times: 10
Loss after 5466350 batches: 0.0216
trigger times: 11
Loss after 5467313 batches: 0.0212
trigger times: 12
Loss after 5468276 batches: 0.0212
trigger times: 13
Loss after 5469239 batches: 0.0208
trigger times: 14
Loss after 5470202 batches: 0.0204
trigger times: 15
Loss after 5471165 batches: 0.0200
trigger times: 16
Loss after 5472128 batches: 0.0196
trigger times: 17
Loss after 5473091 batches: 0.0197
trigger times: 18
Loss after 5474054 batches: 0.0197
trigger times: 19
Loss after 5475017 batches: 0.0191
trigger times: 20
Loss after 5475980 batches: 0.0188
trigger times: 21
Loss after 5476943 batches: 0.0193
trigger times: 22
Loss after 5477906 batches: 0.0186
trigger times: 23
Loss after 5478869 batches: 0.0191
trigger times: 24
Loss after 5479832 batches: 0.0191
trigger times: 25
Early stopping!
Start to test process.
Loss after 5480795 batches: 0.0184
Time to train on one home:  53.75310516357422
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5481758 batches: 0.0543
trigger times: 1
Loss after 5482721 batches: 0.0244
trigger times: 2
Loss after 5483684 batches: 0.0242
trigger times: 3
Loss after 5484647 batches: 0.0217
trigger times: 4
Loss after 5485610 batches: 0.0203
trigger times: 5
Loss after 5486573 batches: 0.0189
trigger times: 6
Loss after 5487536 batches: 0.0174
trigger times: 7
Loss after 5488499 batches: 0.0166
trigger times: 8
Loss after 5489462 batches: 0.0160
trigger times: 9
Loss after 5490425 batches: 0.0158
trigger times: 10
Loss after 5491388 batches: 0.0157
trigger times: 11
Loss after 5492351 batches: 0.0156
trigger times: 12
Loss after 5493314 batches: 0.0149
trigger times: 13
Loss after 5494277 batches: 0.0149
trigger times: 14
Loss after 5495240 batches: 0.0150
trigger times: 15
Loss after 5496203 batches: 0.0148
trigger times: 16
Loss after 5497166 batches: 0.0153
trigger times: 17
Loss after 5498129 batches: 0.0147
trigger times: 18
Loss after 5499092 batches: 0.0145
trigger times: 19
Loss after 5500055 batches: 0.0144
trigger times: 20
Loss after 5501018 batches: 0.0142
trigger times: 21
Loss after 5501981 batches: 0.0146
trigger times: 22
Loss after 5502944 batches: 0.0143
trigger times: 23
Loss after 5503907 batches: 0.0142
trigger times: 24
Loss after 5504870 batches: 0.0145
trigger times: 25
Early stopping!
Start to test process.
Loss after 5505833 batches: 0.0141
Time to train on one home:  56.70331072807312
trigger times: 0
Loss after 5506796 batches: 0.1038
trigger times: 0
Loss after 5507759 batches: 0.0973
trigger times: 0
Loss after 5508722 batches: 0.0938
trigger times: 1
Loss after 5509685 batches: 0.0910
trigger times: 2
Loss after 5510648 batches: 0.0884
trigger times: 3
Loss after 5511611 batches: 0.0861
trigger times: 4
Loss after 5512574 batches: 0.0849
trigger times: 5
Loss after 5513537 batches: 0.0846
trigger times: 6
Loss after 5514500 batches: 0.0847
trigger times: 7
Loss after 5515463 batches: 0.0830
trigger times: 8
Loss after 5516426 batches: 0.0827
trigger times: 9
Loss after 5517389 batches: 0.0825
trigger times: 10
Loss after 5518352 batches: 0.0812
trigger times: 11
Loss after 5519315 batches: 0.0806
trigger times: 12
Loss after 5520278 batches: 0.0789
trigger times: 13
Loss after 5521241 batches: 0.0799
trigger times: 14
Loss after 5522204 batches: 0.0802
trigger times: 15
Loss after 5523167 batches: 0.0786
trigger times: 16
Loss after 5524130 batches: 0.0783
trigger times: 17
Loss after 5525093 batches: 0.0788
trigger times: 18
Loss after 5526056 batches: 0.0772
trigger times: 19
Loss after 5527019 batches: 0.0780
trigger times: 20
Loss after 5527982 batches: 0.0771
trigger times: 21
Loss after 5528945 batches: 0.0766
trigger times: 22
Loss after 5529908 batches: 0.0763
trigger times: 23
Loss after 5530871 batches: 0.0762
trigger times: 24
Loss after 5531834 batches: 0.0751
trigger times: 25
Early stopping!
Start to test process.
Loss after 5532797 batches: 0.0766
Time to train on one home:  58.12395095825195
trigger times: 0
Loss after 5533760 batches: 0.0611
trigger times: 0
Loss after 5534723 batches: 0.0477
trigger times: 1
Loss after 5535686 batches: 0.0424
trigger times: 2
Loss after 5536649 batches: 0.0400
trigger times: 0
Loss after 5537612 batches: 0.0384
trigger times: 1
Loss after 5538575 batches: 0.0355
trigger times: 2
Loss after 5539538 batches: 0.0350
trigger times: 3
Loss after 5540501 batches: 0.0337
trigger times: 4
Loss after 5541464 batches: 0.0335
trigger times: 0
Loss after 5542427 batches: 0.0330
trigger times: 1
Loss after 5543390 batches: 0.0319
trigger times: 2
Loss after 5544353 batches: 0.0315
trigger times: 3
Loss after 5545316 batches: 0.0312
trigger times: 4
Loss after 5546279 batches: 0.0307
trigger times: 5
Loss after 5547242 batches: 0.0307
trigger times: 6
Loss after 5548205 batches: 0.0297
trigger times: 7
Loss after 5549168 batches: 0.0315
trigger times: 8
Loss after 5550131 batches: 0.0305
trigger times: 9
Loss after 5551094 batches: 0.0314
trigger times: 10
Loss after 5552057 batches: 0.0307
trigger times: 11
Loss after 5553020 batches: 0.0302
trigger times: 12
Loss after 5553983 batches: 0.0293
trigger times: 13
Loss after 5554946 batches: 0.0285
trigger times: 14
Loss after 5555909 batches: 0.0295
trigger times: 15
Loss after 5556872 batches: 0.0292
trigger times: 16
Loss after 5557835 batches: 0.0284
trigger times: 17
Loss after 5558798 batches: 0.0295
trigger times: 18
Loss after 5559761 batches: 0.0283
trigger times: 19
Loss after 5560724 batches: 0.0278
trigger times: 20
Loss after 5561687 batches: 0.0278
trigger times: 21
Loss after 5562650 batches: 0.0271
trigger times: 22
Loss after 5563613 batches: 0.0254
trigger times: 23
Loss after 5564576 batches: 0.0269
trigger times: 24
Loss after 5565539 batches: 0.0269
trigger times: 25
Early stopping!
Start to test process.
Loss after 5566502 batches: 0.0272
Time to train on one home:  63.52540302276611
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5567465 batches: 0.0847
trigger times: 1
Loss after 5568428 batches: 0.0804
trigger times: 2
Loss after 5569391 batches: 0.0784
trigger times: 3
Loss after 5570354 batches: 0.0775
trigger times: 4
Loss after 5571317 batches: 0.0751
trigger times: 5
Loss after 5572280 batches: 0.0738
trigger times: 6
Loss after 5573243 batches: 0.0719
trigger times: 7
Loss after 5574206 batches: 0.0710
trigger times: 8
Loss after 5575169 batches: 0.0702
trigger times: 9
Loss after 5576132 batches: 0.0703
trigger times: 10
Loss after 5577095 batches: 0.0699
trigger times: 11
Loss after 5578058 batches: 0.0690
trigger times: 12
Loss after 5579021 batches: 0.0679
trigger times: 13
Loss after 5579984 batches: 0.0673
trigger times: 14
Loss after 5580947 batches: 0.0674
trigger times: 15
Loss after 5581910 batches: 0.0673
trigger times: 16
Loss after 5582873 batches: 0.0658
trigger times: 17
Loss after 5583836 batches: 0.0664
trigger times: 18
Loss after 5584799 batches: 0.0661
trigger times: 19
Loss after 5585762 batches: 0.0674
trigger times: 20
Loss after 5586725 batches: 0.0668
trigger times: 21
Loss after 5587688 batches: 0.0656
trigger times: 22
Loss after 5588651 batches: 0.0648
trigger times: 23
Loss after 5589614 batches: 0.0657
trigger times: 24
Loss after 5590577 batches: 0.0653
trigger times: 25
Early stopping!
Start to test process.
Loss after 5591540 batches: 0.0657
Time to train on one home:  57.015528202056885
trigger times: 0
Loss after 5592503 batches: 0.1016
trigger times: 0
Loss after 5593466 batches: 0.0759
trigger times: 1
Loss after 5594429 batches: 0.0704
trigger times: 0
Loss after 5595392 batches: 0.0665
trigger times: 1
Loss after 5596355 batches: 0.0628
trigger times: 2
Loss after 5597318 batches: 0.0607
trigger times: 3
Loss after 5598281 batches: 0.0576
trigger times: 4
Loss after 5599244 batches: 0.0559
trigger times: 5
Loss after 5600207 batches: 0.0550
trigger times: 0
Loss after 5601170 batches: 0.0545
trigger times: 1
Loss after 5602133 batches: 0.0530
trigger times: 2
Loss after 5603096 batches: 0.0533
trigger times: 3
Loss after 5604059 batches: 0.0536
trigger times: 4
Loss after 5605022 batches: 0.0527
trigger times: 5
Loss after 5605985 batches: 0.0530
trigger times: 6
Loss after 5606948 batches: 0.0520
trigger times: 7
Loss after 5607911 batches: 0.0515
trigger times: 8
Loss after 5608874 batches: 0.0512
trigger times: 9
Loss after 5609837 batches: 0.0507
trigger times: 10
Loss after 5610800 batches: 0.0497
trigger times: 11
Loss after 5611763 batches: 0.0502
trigger times: 0
Loss after 5612726 batches: 0.0495
trigger times: 1
Loss after 5613689 batches: 0.0498
trigger times: 2
Loss after 5614652 batches: 0.0488
trigger times: 3
Loss after 5615615 batches: 0.0481
trigger times: 4
Loss after 5616578 batches: 0.0479
trigger times: 5
Loss after 5617541 batches: 0.0477
trigger times: 6
Loss after 5618504 batches: 0.0483
trigger times: 7
Loss after 5619467 batches: 0.0484
trigger times: 8
Loss after 5620430 batches: 0.0473
trigger times: 9
Loss after 5621393 batches: 0.0481
trigger times: 10
Loss after 5622356 batches: 0.0465
trigger times: 11
Loss after 5623319 batches: 0.0465
trigger times: 12
Loss after 5624282 batches: 0.0470
trigger times: 13
Loss after 5625245 batches: 0.0460
trigger times: 14
Loss after 5626208 batches: 0.0458
trigger times: 15
Loss after 5627171 batches: 0.0458
trigger times: 0
Loss after 5628134 batches: 0.0454
trigger times: 1
Loss after 5629097 batches: 0.0457
trigger times: 2
Loss after 5630060 batches: 0.0447
trigger times: 3
Loss after 5631023 batches: 0.0460
trigger times: 0
Loss after 5631986 batches: 0.0460
trigger times: 0
Loss after 5632949 batches: 0.0455
trigger times: 1
Loss after 5633912 batches: 0.0436
trigger times: 2
Loss after 5634875 batches: 0.0435
trigger times: 3
Loss after 5635838 batches: 0.0442
trigger times: 4
Loss after 5636801 batches: 0.0436
trigger times: 5
Loss after 5637764 batches: 0.0440
trigger times: 6
Loss after 5638727 batches: 0.0440
trigger times: 7
Loss after 5639690 batches: 0.0428
trigger times: 8
Loss after 5640653 batches: 0.0434
trigger times: 9
Loss after 5641616 batches: 0.0426
trigger times: 10
Loss after 5642579 batches: 0.0427
trigger times: 11
Loss after 5643542 batches: 0.0436
trigger times: 12
Loss after 5644505 batches: 0.0421
trigger times: 13
Loss after 5645468 batches: 0.0425
trigger times: 14
Loss after 5646431 batches: 0.0422
trigger times: 15
Loss after 5647394 batches: 0.0421
trigger times: 16
Loss after 5648357 batches: 0.0403
trigger times: 17
Loss after 5649320 batches: 0.0419
trigger times: 18
Loss after 5650283 batches: 0.0419
trigger times: 19
Loss after 5651246 batches: 0.0416
trigger times: 20
Loss after 5652209 batches: 0.0405
trigger times: 21
Loss after 5653172 batches: 0.0403
trigger times: 22
Loss after 5654135 batches: 0.0408
trigger times: 23
Loss after 5655098 batches: 0.0412
trigger times: 24
Loss after 5656061 batches: 0.0404
trigger times: 25
Early stopping!
Start to test process.
Loss after 5657024 batches: 0.0397
Time to train on one home:  90.68221855163574
trigger times: 0
Loss after 5657987 batches: 0.0811
trigger times: 1
Loss after 5658950 batches: 0.0765
trigger times: 2
Loss after 5659913 batches: 0.0733
trigger times: 3
Loss after 5660876 batches: 0.0725
trigger times: 4
Loss after 5661839 batches: 0.0701
trigger times: 5
Loss after 5662802 batches: 0.0696
trigger times: 6
Loss after 5663765 batches: 0.0680
trigger times: 7
Loss after 5664728 batches: 0.0670
trigger times: 8
Loss after 5665691 batches: 0.0660
trigger times: 9
Loss after 5666654 batches: 0.0654
trigger times: 10
Loss after 5667617 batches: 0.0661
trigger times: 11
Loss after 5668580 batches: 0.0653
trigger times: 12
Loss after 5669543 batches: 0.0639
trigger times: 13
Loss after 5670506 batches: 0.0635
trigger times: 14
Loss after 5671469 batches: 0.0646
trigger times: 15
Loss after 5672432 batches: 0.0663
trigger times: 16
Loss after 5673395 batches: 0.0649
trigger times: 17
Loss after 5674358 batches: 0.0629
trigger times: 18
Loss after 5675321 batches: 0.0634
trigger times: 19
Loss after 5676284 batches: 0.0613
trigger times: 20
Loss after 5677247 batches: 0.0622
trigger times: 21
Loss after 5678210 batches: 0.0625
trigger times: 22
Loss after 5679173 batches: 0.0614
trigger times: 23
Loss after 5680136 batches: 0.0615
trigger times: 24
Loss after 5681099 batches: 0.0614
trigger times: 25
Early stopping!
Start to test process.
Loss after 5682062 batches: 0.0604
Time to train on one home:  57.21258521080017
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5683025 batches: 0.0611
trigger times: 0
Loss after 5683988 batches: 0.0549
trigger times: 1
Loss after 5684951 batches: 0.0531
trigger times: 2
Loss after 5685914 batches: 0.0496
trigger times: 3
Loss after 5686877 batches: 0.0464
trigger times: 4
Loss after 5687840 batches: 0.0441
trigger times: 5
Loss after 5688803 batches: 0.0414
trigger times: 6
Loss after 5689766 batches: 0.0406
trigger times: 7
Loss after 5690729 batches: 0.0388
trigger times: 8
Loss after 5691692 batches: 0.0382
trigger times: 9
Loss after 5692655 batches: 0.0377
trigger times: 10
Loss after 5693618 batches: 0.0366
trigger times: 11
Loss after 5694581 batches: 0.0362
trigger times: 12
Loss after 5695544 batches: 0.0360
trigger times: 13
Loss after 5696507 batches: 0.0357
trigger times: 14
Loss after 5697470 batches: 0.0360
trigger times: 15
Loss after 5698433 batches: 0.0353
trigger times: 16
Loss after 5699396 batches: 0.0351
trigger times: 17
Loss after 5700359 batches: 0.0341
trigger times: 18
Loss after 5701322 batches: 0.0339
trigger times: 19
Loss after 5702285 batches: 0.0340
trigger times: 0
Loss after 5703248 batches: 0.0335
trigger times: 1
Loss after 5704211 batches: 0.0341
trigger times: 2
Loss after 5705174 batches: 0.0329
trigger times: 3
Loss after 5706137 batches: 0.0329
trigger times: 4
Loss after 5707100 batches: 0.0331
trigger times: 5
Loss after 5708063 batches: 0.0328
trigger times: 6
Loss after 5709026 batches: 0.0323
trigger times: 7
Loss after 5709989 batches: 0.0318
trigger times: 8
Loss after 5710952 batches: 0.0317
trigger times: 9
Loss after 5711915 batches: 0.0316
trigger times: 10
Loss after 5712878 batches: 0.0318
trigger times: 11
Loss after 5713841 batches: 0.0311
trigger times: 12
Loss after 5714804 batches: 0.0303
trigger times: 13
Loss after 5715767 batches: 0.0303
trigger times: 14
Loss after 5716730 batches: 0.0308
trigger times: 15
Loss after 5717693 batches: 0.0299
trigger times: 16
Loss after 5718656 batches: 0.0290
trigger times: 17
Loss after 5719619 batches: 0.0298
trigger times: 18
Loss after 5720582 batches: 0.0306
trigger times: 19
Loss after 5721545 batches: 0.0303
trigger times: 20
Loss after 5722508 batches: 0.0298
trigger times: 21
Loss after 5723471 batches: 0.0293
trigger times: 22
Loss after 5724434 batches: 0.0282
trigger times: 23
Loss after 5725397 batches: 0.0280
trigger times: 24
Loss after 5726360 batches: 0.0282
trigger times: 25
Early stopping!
Start to test process.
Loss after 5727323 batches: 0.0278
Time to train on one home:  71.39326333999634
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5728286 batches: 0.0908
trigger times: 0
Loss after 5729249 batches: 0.0629
trigger times: 1
Loss after 5730212 batches: 0.0602
trigger times: 2
Loss after 5731175 batches: 0.0561
trigger times: 3
Loss after 5732138 batches: 0.0514
trigger times: 4
Loss after 5733101 batches: 0.0461
trigger times: 5
Loss after 5734064 batches: 0.0444
trigger times: 6
Loss after 5735027 batches: 0.0415
trigger times: 7
Loss after 5735990 batches: 0.0396
trigger times: 8
Loss after 5736953 batches: 0.0381
trigger times: 9
Loss after 5737916 batches: 0.0388
trigger times: 10
Loss after 5738879 batches: 0.0380
trigger times: 11
Loss after 5739842 batches: 0.0373
trigger times: 12
Loss after 5740805 batches: 0.0349
trigger times: 13
Loss after 5741768 batches: 0.0365
trigger times: 14
Loss after 5742731 batches: 0.0362
trigger times: 15
Loss after 5743694 batches: 0.0341
trigger times: 16
Loss after 5744657 batches: 0.0337
trigger times: 17
Loss after 5745620 batches: 0.0355
trigger times: 18
Loss after 5746583 batches: 0.0346
trigger times: 19
Loss after 5747546 batches: 0.0324
trigger times: 20
Loss after 5748509 batches: 0.0311
trigger times: 21
Loss after 5749472 batches: 0.0332
trigger times: 22
Loss after 5750435 batches: 0.0315
trigger times: 23
Loss after 5751398 batches: 0.0286
trigger times: 24
Loss after 5752361 batches: 0.0300
trigger times: 25
Early stopping!
Start to test process.
Loss after 5753324 batches: 0.0299
Time to train on one home:  56.45881152153015
trigger times: 0
Loss after 5754253 batches: 0.1159
trigger times: 0
Loss after 5755182 batches: 0.0668
trigger times: 0
Loss after 5756111 batches: 0.0583
trigger times: 0
Loss after 5757040 batches: 0.0461
trigger times: 1
Loss after 5757969 batches: 0.0434
trigger times: 0
Loss after 5758898 batches: 0.0437
trigger times: 1
Loss after 5759827 batches: 0.0431
trigger times: 2
Loss after 5760756 batches: 0.0413
trigger times: 3
Loss after 5761685 batches: 0.0386
trigger times: 4
Loss after 5762614 batches: 0.0384
trigger times: 5
Loss after 5763543 batches: 0.0385
trigger times: 6
Loss after 5764472 batches: 0.0369
trigger times: 7
Loss after 5765401 batches: 0.0365
trigger times: 8
Loss after 5766330 batches: 0.0348
trigger times: 9
Loss after 5767259 batches: 0.0336
trigger times: 10
Loss after 5768188 batches: 0.0338
trigger times: 11
Loss after 5769117 batches: 0.0327
trigger times: 12
Loss after 5770046 batches: 0.0347
trigger times: 13
Loss after 5770975 batches: 0.0325
trigger times: 14
Loss after 5771904 batches: 0.0320
trigger times: 15
Loss after 5772833 batches: 0.0327
trigger times: 16
Loss after 5773762 batches: 0.0328
trigger times: 17
Loss after 5774691 batches: 0.0321
trigger times: 18
Loss after 5775620 batches: 0.0307
trigger times: 19
Loss after 5776549 batches: 0.0322
trigger times: 20
Loss after 5777478 batches: 0.0289
trigger times: 21
Loss after 5778407 batches: 0.0311
trigger times: 22
Loss after 5779336 batches: 0.0311
trigger times: 23
Loss after 5780265 batches: 0.0302
trigger times: 24
Loss after 5781194 batches: 0.0282
trigger times: 25
Early stopping!
Start to test process.
Loss after 5782123 batches: 0.0293
Time to train on one home:  60.64362359046936
trigger times: 0
Loss after 5783085 batches: 0.0690
trigger times: 1
Loss after 5784047 batches: 0.0669
trigger times: 2
Loss after 5785009 batches: 0.0655
trigger times: 3
Loss after 5785971 batches: 0.0642
trigger times: 4
Loss after 5786933 batches: 0.0632
trigger times: 5
Loss after 5787895 batches: 0.0624
trigger times: 6
Loss after 5788857 batches: 0.0622
trigger times: 7
Loss after 5789819 batches: 0.0616
trigger times: 8
Loss after 5790781 batches: 0.0605
trigger times: 9
Loss after 5791743 batches: 0.0611
trigger times: 10
Loss after 5792705 batches: 0.0610
trigger times: 11
Loss after 5793667 batches: 0.0606
trigger times: 12
Loss after 5794629 batches: 0.0605
trigger times: 13
Loss after 5795591 batches: 0.0601
trigger times: 14
Loss after 5796553 batches: 0.0596
trigger times: 15
Loss after 5797515 batches: 0.0598
trigger times: 16
Loss after 5798477 batches: 0.0587
trigger times: 17
Loss after 5799439 batches: 0.0597
trigger times: 18
Loss after 5800401 batches: 0.0587
trigger times: 19
Loss after 5801363 batches: 0.0589
trigger times: 20
Loss after 5802325 batches: 0.0581
trigger times: 21
Loss after 5803287 batches: 0.0582
trigger times: 22
Loss after 5804249 batches: 0.0580
trigger times: 23
Loss after 5805211 batches: 0.0582
trigger times: 24
Loss after 5806173 batches: 0.0589
trigger times: 25
Early stopping!
Start to test process.
Loss after 5807135 batches: 0.0582
Time to train on one home:  56.07195854187012
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5808098 batches: 0.0564
trigger times: 1
Loss after 5809061 batches: 0.0480
trigger times: 2
Loss after 5810024 batches: 0.0488
trigger times: 0
Loss after 5810987 batches: 0.0463
trigger times: 1
Loss after 5811950 batches: 0.0446
trigger times: 2
Loss after 5812913 batches: 0.0434
trigger times: 3
Loss after 5813876 batches: 0.0417
trigger times: 4
Loss after 5814839 batches: 0.0411
trigger times: 5
Loss after 5815802 batches: 0.0408
trigger times: 6
Loss after 5816765 batches: 0.0401
trigger times: 7
Loss after 5817728 batches: 0.0395
trigger times: 8
Loss after 5818691 batches: 0.0397
trigger times: 9
Loss after 5819654 batches: 0.0391
trigger times: 10
Loss after 5820617 batches: 0.0384
trigger times: 11
Loss after 5821580 batches: 0.0388
trigger times: 12
Loss after 5822543 batches: 0.0383
trigger times: 13
Loss after 5823506 batches: 0.0383
trigger times: 14
Loss after 5824469 batches: 0.0381
trigger times: 15
Loss after 5825432 batches: 0.0375
trigger times: 16
Loss after 5826395 batches: 0.0375
trigger times: 17
Loss after 5827358 batches: 0.0376
trigger times: 18
Loss after 5828321 batches: 0.0371
trigger times: 19
Loss after 5829284 batches: 0.0359
trigger times: 20
Loss after 5830247 batches: 0.0364
trigger times: 21
Loss after 5831210 batches: 0.0358
trigger times: 22
Loss after 5832173 batches: 0.0360
trigger times: 23
Loss after 5833136 batches: 0.0356
trigger times: 24
Loss after 5834099 batches: 0.0360
trigger times: 25
Early stopping!
Start to test process.
Loss after 5835062 batches: 0.0362
Time to train on one home:  61.073487281799316
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5836025 batches: 0.0619
trigger times: 0
Loss after 5836988 batches: 0.0559
trigger times: 1
Loss after 5837951 batches: 0.0539
trigger times: 2
Loss after 5838914 batches: 0.0518
trigger times: 3
Loss after 5839877 batches: 0.0489
trigger times: 0
Loss after 5840840 batches: 0.0474
trigger times: 1
Loss after 5841803 batches: 0.0465
trigger times: 0
Loss after 5842766 batches: 0.0461
trigger times: 1
Loss after 5843729 batches: 0.0460
trigger times: 2
Loss after 5844692 batches: 0.0458
trigger times: 3
Loss after 5845655 batches: 0.0450
trigger times: 4
Loss after 5846618 batches: 0.0438
trigger times: 5
Loss after 5847581 batches: 0.0436
trigger times: 0
Loss after 5848544 batches: 0.0432
trigger times: 1
Loss after 5849507 batches: 0.0431
trigger times: 2
Loss after 5850470 batches: 0.0425
trigger times: 3
Loss after 5851433 batches: 0.0415
trigger times: 4
Loss after 5852396 batches: 0.0425
trigger times: 0
Loss after 5853359 batches: 0.0409
trigger times: 1
Loss after 5854322 batches: 0.0417
trigger times: 2
Loss after 5855285 batches: 0.0406
trigger times: 3
Loss after 5856248 batches: 0.0400
trigger times: 4
Loss after 5857211 batches: 0.0409
trigger times: 5
Loss after 5858174 batches: 0.0404
trigger times: 6
Loss after 5859137 batches: 0.0405
trigger times: 7
Loss after 5860100 batches: 0.0392
trigger times: 8
Loss after 5861063 batches: 0.0388
trigger times: 9
Loss after 5862026 batches: 0.0387
trigger times: 10
Loss after 5862989 batches: 0.0385
trigger times: 11
Loss after 5863952 batches: 0.0389
trigger times: 12
Loss after 5864915 batches: 0.0389
trigger times: 13
Loss after 5865878 batches: 0.0402
trigger times: 14
Loss after 5866841 batches: 0.0400
trigger times: 15
Loss after 5867804 batches: 0.0381
trigger times: 16
Loss after 5868767 batches: 0.0384
trigger times: 17
Loss after 5869730 batches: 0.0377
trigger times: 18
Loss after 5870693 batches: 0.0374
trigger times: 19
Loss after 5871656 batches: 0.0372
trigger times: 20
Loss after 5872619 batches: 0.0361
trigger times: 21
Loss after 5873582 batches: 0.0368
trigger times: 22
Loss after 5874545 batches: 0.0357
trigger times: 23
Loss after 5875508 batches: 0.0371
trigger times: 24
Loss after 5876471 batches: 0.0381
trigger times: 25
Early stopping!
Start to test process.
Loss after 5877434 batches: 0.0360
Time to train on one home:  70.45868396759033
trigger times: 0
Loss after 5878397 batches: 0.1158
trigger times: 0
Loss after 5879360 batches: 0.1087
trigger times: 1
Loss after 5880323 batches: 0.1036
trigger times: 2
Loss after 5881286 batches: 0.0996
trigger times: 3
Loss after 5882249 batches: 0.0973
trigger times: 4
Loss after 5883212 batches: 0.0957
trigger times: 5
Loss after 5884175 batches: 0.0931
trigger times: 6
Loss after 5885138 batches: 0.0904
trigger times: 7
Loss after 5886101 batches: 0.0907
trigger times: 8
Loss after 5887064 batches: 0.0915
trigger times: 9
Loss after 5888027 batches: 0.0918
trigger times: 10
Loss after 5888990 batches: 0.0908
trigger times: 11
Loss after 5889953 batches: 0.0888
trigger times: 12
Loss after 5890916 batches: 0.0906
trigger times: 13
Loss after 5891879 batches: 0.0916
trigger times: 14
Loss after 5892842 batches: 0.0884
trigger times: 15
Loss after 5893805 batches: 0.0874
trigger times: 16
Loss after 5894768 batches: 0.0865
trigger times: 17
Loss after 5895731 batches: 0.0859
trigger times: 18
Loss after 5896694 batches: 0.0868
trigger times: 19
Loss after 5897657 batches: 0.0858
trigger times: 20
Loss after 5898620 batches: 0.0877
trigger times: 21
Loss after 5899583 batches: 0.0872
trigger times: 22
Loss after 5900546 batches: 0.0856
trigger times: 23
Loss after 5901509 batches: 0.0839
trigger times: 24
Loss after 5902472 batches: 0.0841
trigger times: 25
Early stopping!
Start to test process.
Loss after 5903435 batches: 0.0831
Time to train on one home:  58.204365968704224
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5904398 batches: 0.1156
trigger times: 0
Loss after 5905361 batches: 0.0819
trigger times: 1
Loss after 5906324 batches: 0.0780
trigger times: 2
Loss after 5907287 batches: 0.0718
trigger times: 3
Loss after 5908250 batches: 0.0660
trigger times: 0
Loss after 5909213 batches: 0.0610
trigger times: 1
Loss after 5910176 batches: 0.0577
trigger times: 2
Loss after 5911139 batches: 0.0552
trigger times: 3
Loss after 5912102 batches: 0.0527
trigger times: 0
Loss after 5913065 batches: 0.0516
trigger times: 0
Loss after 5914028 batches: 0.0499
trigger times: 1
Loss after 5914991 batches: 0.0499
trigger times: 0
Loss after 5915954 batches: 0.0483
trigger times: 1
Loss after 5916917 batches: 0.0488
trigger times: 2
Loss after 5917880 batches: 0.0470
trigger times: 3
Loss after 5918843 batches: 0.0460
trigger times: 0
Loss after 5919806 batches: 0.0440
trigger times: 1
Loss after 5920769 batches: 0.0445
trigger times: 0
Loss after 5921732 batches: 0.0432
trigger times: 1
Loss after 5922695 batches: 0.0439
trigger times: 2
Loss after 5923658 batches: 0.0436
trigger times: 3
Loss after 5924621 batches: 0.0415
trigger times: 4
Loss after 5925584 batches: 0.0420
trigger times: 5
Loss after 5926547 batches: 0.0422
trigger times: 6
Loss after 5927510 batches: 0.0414
trigger times: 7
Loss after 5928473 batches: 0.0409
trigger times: 8
Loss after 5929436 batches: 0.0404
trigger times: 9
Loss after 5930399 batches: 0.0410
trigger times: 0
Loss after 5931362 batches: 0.0393
trigger times: 1
Loss after 5932325 batches: 0.0389
trigger times: 2
Loss after 5933288 batches: 0.0391
trigger times: 3
Loss after 5934251 batches: 0.0387
trigger times: 4
Loss after 5935214 batches: 0.0382
trigger times: 5
Loss after 5936177 batches: 0.0374
trigger times: 6
Loss after 5937140 batches: 0.0372
trigger times: 7
Loss after 5938103 batches: 0.0365
trigger times: 8
Loss after 5939066 batches: 0.0362
trigger times: 9
Loss after 5940029 batches: 0.0364
trigger times: 10
Loss after 5940992 batches: 0.0381
trigger times: 11
Loss after 5941955 batches: 0.0357
trigger times: 12
Loss after 5942918 batches: 0.0352
trigger times: 13
Loss after 5943881 batches: 0.0344
trigger times: 14
Loss after 5944844 batches: 0.0351
trigger times: 15
Loss after 5945807 batches: 0.0354
trigger times: 16
Loss after 5946770 batches: 0.0355
trigger times: 17
Loss after 5947733 batches: 0.0346
trigger times: 18
Loss after 5948696 batches: 0.0332
trigger times: 19
Loss after 5949659 batches: 0.0335
trigger times: 20
Loss after 5950622 batches: 0.0335
trigger times: 21
Loss after 5951585 batches: 0.0335
trigger times: 22
Loss after 5952548 batches: 0.0339
trigger times: 23
Loss after 5953511 batches: 0.0352
trigger times: 24
Loss after 5954474 batches: 0.0335
trigger times: 25
Early stopping!
Start to test process.
Loss after 5955437 batches: 0.0329
Time to train on one home:  78.47745752334595
trigger times: 0
Loss after 5956396 batches: 0.1074
trigger times: 1
Loss after 5957355 batches: 0.0595
trigger times: 0
Loss after 5958314 batches: 0.0454
trigger times: 0
Loss after 5959273 batches: 0.0379
trigger times: 0
Loss after 5960232 batches: 0.0340
trigger times: 1
Loss after 5961191 batches: 0.0307
trigger times: 0
Loss after 5962150 batches: 0.0291
trigger times: 1
Loss after 5963109 batches: 0.0281
trigger times: 2
Loss after 5964068 batches: 0.0262
trigger times: 3
Loss after 5965027 batches: 0.0253
trigger times: 4
Loss after 5965986 batches: 0.0246
trigger times: 5
Loss after 5966945 batches: 0.0252
trigger times: 6
Loss after 5967904 batches: 0.0245
trigger times: 7
Loss after 5968863 batches: 0.0245
trigger times: 0
Loss after 5969822 batches: 0.0238
trigger times: 1
Loss after 5970781 batches: 0.0240
trigger times: 0
Loss after 5971740 batches: 0.0227
trigger times: 1
Loss after 5972699 batches: 0.0230
trigger times: 2
Loss after 5973658 batches: 0.0236
trigger times: 3
Loss after 5974617 batches: 0.0226
trigger times: 4
Loss after 5975576 batches: 0.0230
trigger times: 5
Loss after 5976535 batches: 0.0233
trigger times: 6
Loss after 5977494 batches: 0.0243
trigger times: 7
Loss after 5978453 batches: 0.0234
trigger times: 8
Loss after 5979412 batches: 0.0227
trigger times: 9
Loss after 5980371 batches: 0.0221
trigger times: 0
Loss after 5981330 batches: 0.0217
trigger times: 0
Loss after 5982289 batches: 0.0218
trigger times: 1
Loss after 5983248 batches: 0.0202
trigger times: 2
Loss after 5984207 batches: 0.0208
trigger times: 3
Loss after 5985166 batches: 0.0216
trigger times: 4
Loss after 5986125 batches: 0.0213
trigger times: 5
Loss after 5987084 batches: 0.0208
trigger times: 6
Loss after 5988043 batches: 0.0207
trigger times: 7
Loss after 5989002 batches: 0.0203
trigger times: 8
Loss after 5989961 batches: 0.0196
trigger times: 9
Loss after 5990920 batches: 0.0209
trigger times: 10
Loss after 5991879 batches: 0.0202
trigger times: 11
Loss after 5992838 batches: 0.0201
trigger times: 12
Loss after 5993797 batches: 0.0198
trigger times: 13
Loss after 5994756 batches: 0.0198
trigger times: 14
Loss after 5995715 batches: 0.0193
trigger times: 15
Loss after 5996674 batches: 0.0201
trigger times: 16
Loss after 5997633 batches: 0.0199
trigger times: 17
Loss after 5998592 batches: 0.0197
trigger times: 18
Loss after 5999551 batches: 0.0197
trigger times: 19
Loss after 6000510 batches: 0.0193
trigger times: 20
Loss after 6001469 batches: 0.0182
trigger times: 21
Loss after 6002428 batches: 0.0194
trigger times: 22
Loss after 6003387 batches: 0.0204
trigger times: 23
Loss after 6004346 batches: 0.0187
trigger times: 24
Loss after 6005305 batches: 0.0183
trigger times: 0
Loss after 6006264 batches: 0.0180
trigger times: 1
Loss after 6007223 batches: 0.0174
trigger times: 2
Loss after 6008182 batches: 0.0187
trigger times: 3
Loss after 6009141 batches: 0.0173
trigger times: 4
Loss after 6010100 batches: 0.0176
trigger times: 5
Loss after 6011059 batches: 0.0175
trigger times: 6
Loss after 6012018 batches: 0.0177
trigger times: 7
Loss after 6012977 batches: 0.0168
trigger times: 8
Loss after 6013936 batches: 0.0174
trigger times: 9
Loss after 6014895 batches: 0.0174
trigger times: 10
Loss after 6015854 batches: 0.0169
trigger times: 11
Loss after 6016813 batches: 0.0166
trigger times: 12
Loss after 6017772 batches: 0.0163
trigger times: 13
Loss after 6018731 batches: 0.0169
trigger times: 14
Loss after 6019690 batches: 0.0168
trigger times: 15
Loss after 6020649 batches: 0.0165
trigger times: 16
Loss after 6021608 batches: 0.0184
trigger times: 17
Loss after 6022567 batches: 0.0174
trigger times: 0
Loss after 6023526 batches: 0.0168
trigger times: 1
Loss after 6024485 batches: 0.0169
trigger times: 2
Loss after 6025444 batches: 0.0165
trigger times: 3
Loss after 6026403 batches: 0.0155
trigger times: 4
Loss after 6027362 batches: 0.0163
trigger times: 5
Loss after 6028321 batches: 0.0166
trigger times: 6
Loss after 6029280 batches: 0.0172
trigger times: 7
Loss after 6030239 batches: 0.0162
trigger times: 8
Loss after 6031198 batches: 0.0156
trigger times: 9
Loss after 6032157 batches: 0.0157
trigger times: 10
Loss after 6033116 batches: 0.0155
trigger times: 11
Loss after 6034075 batches: 0.0156
trigger times: 12
Loss after 6035034 batches: 0.0156
trigger times: 13
Loss after 6035993 batches: 0.0147
trigger times: 14
Loss after 6036952 batches: 0.0169
trigger times: 15
Loss after 6037911 batches: 0.0164
trigger times: 0
Loss after 6038870 batches: 0.0163
trigger times: 1
Loss after 6039829 batches: 0.0148
trigger times: 2
Loss after 6040788 batches: 0.0149
trigger times: 3
Loss after 6041747 batches: 0.0150
trigger times: 4
Loss after 6042706 batches: 0.0138
trigger times: 5
Loss after 6043665 batches: 0.0142
trigger times: 6
Loss after 6044624 batches: 0.0142
trigger times: 7
Loss after 6045583 batches: 0.0139
trigger times: 8
Loss after 6046542 batches: 0.0149
trigger times: 9
Loss after 6047501 batches: 0.0154
trigger times: 10
Loss after 6048460 batches: 0.0147
trigger times: 11
Loss after 6049419 batches: 0.0151
trigger times: 12
Loss after 6050378 batches: 0.0140
trigger times: 13
Loss after 6051337 batches: 0.0135
trigger times: 14
Loss after 6052296 batches: 0.0137
trigger times: 15
Loss after 6053255 batches: 0.0140
trigger times: 16
Loss after 6054214 batches: 0.0131
trigger times: 17
Loss after 6055173 batches: 0.0134
trigger times: 18
Loss after 6056132 batches: 0.0134
trigger times: 19
Loss after 6057091 batches: 0.0132
trigger times: 20
Loss after 6058050 batches: 0.0136
trigger times: 21
Loss after 6059009 batches: 0.0138
trigger times: 22
Loss after 6059968 batches: 0.0139
trigger times: 23
Loss after 6060927 batches: 0.0137
trigger times: 24
Loss after 6061886 batches: 0.0137
trigger times: 25
Early stopping!
Start to test process.
Loss after 6062845 batches: 0.0131
Time to train on one home:  124.97633004188538
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6063808 batches: 0.0451
trigger times: 1
Loss after 6064771 batches: 0.0273
trigger times: 2
Loss after 6065734 batches: 0.0269
trigger times: 3
Loss after 6066697 batches: 0.0265
trigger times: 4
Loss after 6067660 batches: 0.0260
trigger times: 5
Loss after 6068623 batches: 0.0254
trigger times: 6
Loss after 6069586 batches: 0.0252
trigger times: 7
Loss after 6070549 batches: 0.0249
trigger times: 8
Loss after 6071512 batches: 0.0247
trigger times: 9
Loss after 6072475 batches: 0.0243
trigger times: 10
Loss after 6073438 batches: 0.0241
trigger times: 11
Loss after 6074401 batches: 0.0236
trigger times: 12
Loss after 6075364 batches: 0.0236
trigger times: 13
Loss after 6076327 batches: 0.0235
trigger times: 14
Loss after 6077290 batches: 0.0231
trigger times: 15
Loss after 6078253 batches: 0.0230
trigger times: 16
Loss after 6079216 batches: 0.0229
trigger times: 17
Loss after 6080179 batches: 0.0229
trigger times: 18
Loss after 6081142 batches: 0.0227
trigger times: 19
Loss after 6082105 batches: 0.0221
trigger times: 20
Loss after 6083068 batches: 0.0220
trigger times: 21
Loss after 6084031 batches: 0.0216
trigger times: 22
Loss after 6084994 batches: 0.0216
trigger times: 23
Loss after 6085957 batches: 0.0213
trigger times: 24
Loss after 6086920 batches: 0.0215
trigger times: 25
Early stopping!
Start to test process.
Loss after 6087883 batches: 0.0215
Time to train on one home:  54.67251443862915
trigger times: 0
Loss after 6088828 batches: 0.0758
trigger times: 0
Loss after 6089773 batches: 0.0542
trigger times: 1
Loss after 6090718 batches: 0.0467
trigger times: 0
Loss after 6091663 batches: 0.0426
trigger times: 0
Loss after 6092608 batches: 0.0384
trigger times: 1
Loss after 6093553 batches: 0.0365
trigger times: 2
Loss after 6094498 batches: 0.0346
trigger times: 0
Loss after 6095443 batches: 0.0331
trigger times: 1
Loss after 6096388 batches: 0.0325
trigger times: 2
Loss after 6097333 batches: 0.0323
trigger times: 3
Loss after 6098278 batches: 0.0315
trigger times: 0
Loss after 6099223 batches: 0.0302
trigger times: 1
Loss after 6100168 batches: 0.0304
trigger times: 0
Loss after 6101113 batches: 0.0292
trigger times: 1
Loss after 6102058 batches: 0.0287
trigger times: 2
Loss after 6103003 batches: 0.0294
trigger times: 3
Loss after 6103948 batches: 0.0286
trigger times: 4
Loss after 6104893 batches: 0.0286
trigger times: 5
Loss after 6105838 batches: 0.0283
trigger times: 6
Loss after 6106783 batches: 0.0285
trigger times: 7
Loss after 6107728 batches: 0.0275
trigger times: 8
Loss after 6108673 batches: 0.0267
trigger times: 0
Loss after 6109618 batches: 0.0259
trigger times: 0
Loss after 6110563 batches: 0.0267
trigger times: 1
Loss after 6111508 batches: 0.0269
trigger times: 2
Loss after 6112453 batches: 0.0277
trigger times: 3
Loss after 6113398 batches: 0.0262
trigger times: 4
Loss after 6114343 batches: 0.0273
trigger times: 5
Loss after 6115288 batches: 0.0271
trigger times: 6
Loss after 6116233 batches: 0.0269
trigger times: 7
Loss after 6117178 batches: 0.0265
trigger times: 8
Loss after 6118123 batches: 0.0270
trigger times: 9
Loss after 6119068 batches: 0.0256
trigger times: 10
Loss after 6120013 batches: 0.0249
trigger times: 11
Loss after 6120958 batches: 0.0242
trigger times: 12
Loss after 6121903 batches: 0.0254
trigger times: 13
Loss after 6122848 batches: 0.0257
trigger times: 14
Loss after 6123793 batches: 0.0241
trigger times: 15
Loss after 6124738 batches: 0.0245
trigger times: 16
Loss after 6125683 batches: 0.0242
trigger times: 17
Loss after 6126628 batches: 0.0238
trigger times: 18
Loss after 6127573 batches: 0.0223
trigger times: 0
Loss after 6128518 batches: 0.0239
trigger times: 1
Loss after 6129463 batches: 0.0235
trigger times: 2
Loss after 6130408 batches: 0.0225
trigger times: 3
Loss after 6131353 batches: 0.0241
trigger times: 4
Loss after 6132298 batches: 0.0246
trigger times: 5
Loss after 6133243 batches: 0.0238
trigger times: 6
Loss after 6134188 batches: 0.0231
trigger times: 7
Loss after 6135133 batches: 0.0219
trigger times: 8
Loss after 6136078 batches: 0.0207
trigger times: 9
Loss after 6137023 batches: 0.0212
trigger times: 10
Loss after 6137968 batches: 0.0208
trigger times: 11
Loss after 6138913 batches: 0.0209
trigger times: 12
Loss after 6139858 batches: 0.0218
trigger times: 13
Loss after 6140803 batches: 0.0216
trigger times: 14
Loss after 6141748 batches: 0.0222
trigger times: 15
Loss after 6142693 batches: 0.0211
trigger times: 16
Loss after 6143638 batches: 0.0229
trigger times: 17
Loss after 6144583 batches: 0.0219
trigger times: 18
Loss after 6145528 batches: 0.0216
trigger times: 19
Loss after 6146473 batches: 0.0220
trigger times: 20
Loss after 6147418 batches: 0.0223
trigger times: 21
Loss after 6148363 batches: 0.0220
trigger times: 22
Loss after 6149308 batches: 0.0217
trigger times: 23
Loss after 6150253 batches: 0.0227
trigger times: 24
Loss after 6151198 batches: 0.0200
trigger times: 25
Early stopping!
Start to test process.
Loss after 6152143 batches: 0.0197
Time to train on one home:  94.388516664505
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6153080 batches: 0.0870
trigger times: 1
Loss after 6154017 batches: 0.0780
trigger times: 2
Loss after 6154954 batches: 0.0764
trigger times: 0
Loss after 6155891 batches: 0.0734
trigger times: 1
Loss after 6156828 batches: 0.0710
trigger times: 2
Loss after 6157765 batches: 0.0695
trigger times: 3
Loss after 6158702 batches: 0.0670
trigger times: 4
Loss after 6159639 batches: 0.0650
trigger times: 5
Loss after 6160576 batches: 0.0647
trigger times: 6
Loss after 6161513 batches: 0.0645
trigger times: 7
Loss after 6162450 batches: 0.0625
trigger times: 8
Loss after 6163387 batches: 0.0638
trigger times: 9
Loss after 6164324 batches: 0.0628
trigger times: 10
Loss after 6165261 batches: 0.0614
trigger times: 11
Loss after 6166198 batches: 0.0616
trigger times: 12
Loss after 6167135 batches: 0.0619
trigger times: 13
Loss after 6168072 batches: 0.0614
trigger times: 14
Loss after 6169009 batches: 0.0601
trigger times: 15
Loss after 6169946 batches: 0.0603
trigger times: 16
Loss after 6170883 batches: 0.0602
trigger times: 17
Loss after 6171820 batches: 0.0592
trigger times: 18
Loss after 6172757 batches: 0.0591
trigger times: 19
Loss after 6173694 batches: 0.0587
trigger times: 20
Loss after 6174631 batches: 0.0586
trigger times: 21
Loss after 6175568 batches: 0.0588
trigger times: 22
Loss after 6176505 batches: 0.0601
trigger times: 23
Loss after 6177442 batches: 0.0580
trigger times: 24
Loss after 6178379 batches: 0.0577
trigger times: 25
Early stopping!
Start to test process.
Loss after 6179316 batches: 0.0570
Time to train on one home:  58.55535340309143
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 6180279 batches: 0.0217
trigger times: 1
Loss after 6181242 batches: 0.0148
trigger times: 2
Loss after 6182205 batches: 0.0140
trigger times: 3
Loss after 6183168 batches: 0.0139
trigger times: 4
Loss after 6184131 batches: 0.0139
trigger times: 5
Loss after 6185094 batches: 0.0136
trigger times: 6
Loss after 6186057 batches: 0.0134
trigger times: 7
Loss after 6187020 batches: 0.0132
trigger times: 8
Loss after 6187983 batches: 0.0128
trigger times: 9
Loss after 6188946 batches: 0.0122
trigger times: 10
Loss after 6189909 batches: 0.0119
trigger times: 11
Loss after 6190872 batches: 0.0115
trigger times: 12
Loss after 6191835 batches: 0.0113
trigger times: 13
Loss after 6192798 batches: 0.0110
trigger times: 14
Loss after 6193761 batches: 0.0108
trigger times: 15
Loss after 6194724 batches: 0.0106
trigger times: 16
Loss after 6195687 batches: 0.0105
trigger times: 17
Loss after 6196650 batches: 0.0104
trigger times: 18
Loss after 6197613 batches: 0.0103
trigger times: 19
Loss after 6198576 batches: 0.0101
trigger times: 20
Loss after 6199539 batches: 0.0101
trigger times: 21
Loss after 6200502 batches: 0.0101
trigger times: 22
Loss after 6201465 batches: 0.0100
trigger times: 23
Loss after 6202428 batches: 0.0097
trigger times: 24
Loss after 6203391 batches: 0.0096
trigger times: 25
Early stopping!
Start to test process.
Loss after 6204354 batches: 0.0096
Time to train on one home:  57.44461369514465
trigger times: 0
Loss after 6205317 batches: 0.0895
trigger times: 1
Loss after 6206280 batches: 0.0797
trigger times: 2
Loss after 6207243 batches: 0.0778
trigger times: 3
Loss after 6208206 batches: 0.0736
trigger times: 4
Loss after 6209169 batches: 0.0714
trigger times: 5
Loss after 6210132 batches: 0.0707
trigger times: 6
Loss after 6211095 batches: 0.0689
trigger times: 7
Loss after 6212058 batches: 0.0679
trigger times: 8
Loss after 6213021 batches: 0.0674
trigger times: 9
Loss after 6213984 batches: 0.0668
trigger times: 10
Loss after 6214947 batches: 0.0666
trigger times: 11
Loss after 6215910 batches: 0.0656
trigger times: 12
Loss after 6216873 batches: 0.0651
trigger times: 13
Loss after 6217836 batches: 0.0648
trigger times: 14
Loss after 6218799 batches: 0.0657
trigger times: 15
Loss after 6219762 batches: 0.0648
trigger times: 16
Loss after 6220725 batches: 0.0651
trigger times: 17
Loss after 6221688 batches: 0.0644
trigger times: 18
Loss after 6222651 batches: 0.0637
trigger times: 19
Loss after 6223614 batches: 0.0634
trigger times: 20
Loss after 6224577 batches: 0.0636
trigger times: 21
Loss after 6225540 batches: 0.0639
trigger times: 22
Loss after 6226503 batches: 0.0622
trigger times: 23
Loss after 6227466 batches: 0.0629
trigger times: 24
Loss after 6228429 batches: 0.0628
trigger times: 25
Early stopping!
Start to test process.
Loss after 6229392 batches: 0.0616
Time to train on one home:  56.36200523376465
trigger times: 0
Loss after 6230355 batches: 0.0755
trigger times: 1
Loss after 6231318 batches: 0.0662
trigger times: 2
Loss after 6232281 batches: 0.0597
trigger times: 3
Loss after 6233244 batches: 0.0567
trigger times: 4
Loss after 6234207 batches: 0.0536
trigger times: 5
Loss after 6235170 batches: 0.0528
trigger times: 6
Loss after 6236133 batches: 0.0518
trigger times: 7
Loss after 6237096 batches: 0.0493
trigger times: 8
Loss after 6238059 batches: 0.0482
trigger times: 9
Loss after 6239022 batches: 0.0484
trigger times: 10
Loss after 6239985 batches: 0.0457
trigger times: 11
Loss after 6240948 batches: 0.0453
trigger times: 12
Loss after 6241911 batches: 0.0460
trigger times: 13
Loss after 6242874 batches: 0.0446
trigger times: 14
Loss after 6243837 batches: 0.0449
trigger times: 15
Loss after 6244800 batches: 0.0445
trigger times: 16
Loss after 6245763 batches: 0.0437
trigger times: 17
Loss after 6246726 batches: 0.0442
trigger times: 18
Loss after 6247689 batches: 0.0427
trigger times: 19
Loss after 6248652 batches: 0.0422
trigger times: 20
Loss after 6249615 batches: 0.0414
trigger times: 21
Loss after 6250578 batches: 0.0411
trigger times: 22
Loss after 6251541 batches: 0.0409
trigger times: 23
Loss after 6252504 batches: 0.0399
trigger times: 24
Loss after 6253467 batches: 0.0409
trigger times: 25
Early stopping!
Start to test process.
Loss after 6254430 batches: 0.0398
Time to train on one home:  52.74506187438965
trigger times: 0
Loss after 6255326 batches: 0.1050
trigger times: 1
Loss after 6256222 batches: 0.0962
trigger times: 2
Loss after 6257118 batches: 0.0916
trigger times: 3
Loss after 6258014 batches: 0.0885
trigger times: 4
Loss after 6258910 batches: 0.0867
trigger times: 5
Loss after 6259806 batches: 0.0835
trigger times: 6
Loss after 6260702 batches: 0.0812
trigger times: 7
Loss after 6261598 batches: 0.0779
trigger times: 8
Loss after 6262494 batches: 0.0760
trigger times: 9
Loss after 6263390 batches: 0.0783
trigger times: 10
Loss after 6264286 batches: 0.0747
trigger times: 11
Loss after 6265182 batches: 0.0737
trigger times: 12
Loss after 6266078 batches: 0.0721
trigger times: 13
Loss after 6266974 batches: 0.0726
trigger times: 14
Loss after 6267870 batches: 0.0716
trigger times: 15
Loss after 6268766 batches: 0.0714
trigger times: 16
Loss after 6269662 batches: 0.0692
trigger times: 17
Loss after 6270558 batches: 0.0681
trigger times: 18
Loss after 6271454 batches: 0.0686
trigger times: 19
Loss after 6272350 batches: 0.0687
trigger times: 20
Loss after 6273246 batches: 0.0687
trigger times: 21
Loss after 6274142 batches: 0.0679
trigger times: 22
Loss after 6275038 batches: 0.0670
trigger times: 23
Loss after 6275934 batches: 0.0670
trigger times: 24
Loss after 6276830 batches: 0.0663
trigger times: 25
Early stopping!
Start to test process.
Loss after 6277726 batches: 0.0662
Time to train on one home:  55.20031809806824
trigger times: 0
Loss after 6278689 batches: 0.1568
trigger times: 1
Loss after 6279652 batches: 0.1133
trigger times: 2
Loss after 6280615 batches: 0.1075
trigger times: 3
Loss after 6281578 batches: 0.0964
trigger times: 4
Loss after 6282541 batches: 0.0910
trigger times: 5
Loss after 6283504 batches: 0.0873
trigger times: 6
Loss after 6284467 batches: 0.0864
trigger times: 7
Loss after 6285430 batches: 0.0845
trigger times: 8
Loss after 6286393 batches: 0.0828
trigger times: 9
Loss after 6287356 batches: 0.0838
trigger times: 10
Loss after 6288319 batches: 0.0814
trigger times: 11
Loss after 6289282 batches: 0.0806
trigger times: 12
Loss after 6290245 batches: 0.0788
trigger times: 13
Loss after 6291208 batches: 0.0767
trigger times: 14
Loss after 6292171 batches: 0.0751
trigger times: 15
Loss after 6293134 batches: 0.0713
trigger times: 16
Loss after 6294097 batches: 0.0703
trigger times: 17
Loss after 6295060 batches: 0.0685
trigger times: 18
Loss after 6296023 batches: 0.0665
trigger times: 19
Loss after 6296986 batches: 0.0655
trigger times: 20
Loss after 6297949 batches: 0.0648
trigger times: 21
Loss after 6298912 batches: 0.0665
trigger times: 22
Loss after 6299875 batches: 0.0645
trigger times: 23
Loss after 6300838 batches: 0.0639
trigger times: 24
Loss after 6301801 batches: 0.0609
trigger times: 25
Early stopping!
Start to test process.
Loss after 6302764 batches: 0.0617
Time to train on one home:  56.67686581611633
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6303727 batches: 0.0817
trigger times: 1
Loss after 6304690 batches: 0.0778
trigger times: 2
Loss after 6305653 batches: 0.0757
trigger times: 3
Loss after 6306616 batches: 0.0749
trigger times: 4
Loss after 6307579 batches: 0.0734
trigger times: 5
Loss after 6308542 batches: 0.0724
trigger times: 6
Loss after 6309505 batches: 0.0718
trigger times: 7
Loss after 6310468 batches: 0.0689
trigger times: 8
Loss after 6311431 batches: 0.0733
trigger times: 9
Loss after 6312394 batches: 0.0717
trigger times: 10
Loss after 6313357 batches: 0.0711
trigger times: 11
Loss after 6314320 batches: 0.0695
trigger times: 12
Loss after 6315283 batches: 0.0688
trigger times: 13
Loss after 6316246 batches: 0.0731
trigger times: 14
Loss after 6317209 batches: 0.0788
trigger times: 15
Loss after 6318172 batches: 0.0742
trigger times: 16
Loss after 6319135 batches: 0.0726
trigger times: 17
Loss after 6320098 batches: 0.0715
trigger times: 18
Loss after 6321061 batches: 0.0720
trigger times: 19
Loss after 6322024 batches: 0.0699
trigger times: 20
Loss after 6322987 batches: 0.0711
trigger times: 21
Loss after 6323950 batches: 0.0698
trigger times: 22
Loss after 6324913 batches: 0.0702
trigger times: 23
Loss after 6325876 batches: 0.0700
trigger times: 24
Loss after 6326839 batches: 0.0679
trigger times: 25
Early stopping!
Start to test process.
Loss after 6327802 batches: 0.0681
Time to train on one home:  56.01906180381775
trigger times: 0
Loss after 6328765 batches: 0.0972
trigger times: 0
Loss after 6329728 batches: 0.0738
trigger times: 1
Loss after 6330691 batches: 0.0699
trigger times: 2
Loss after 6331654 batches: 0.0647
trigger times: 3
Loss after 6332617 batches: 0.0607
trigger times: 4
Loss after 6333580 batches: 0.0575
trigger times: 5
Loss after 6334543 batches: 0.0554
trigger times: 6
Loss after 6335506 batches: 0.0544
trigger times: 7
Loss after 6336469 batches: 0.0538
trigger times: 8
Loss after 6337432 batches: 0.0539
trigger times: 9
Loss after 6338395 batches: 0.0523
trigger times: 10
Loss after 6339358 batches: 0.0517
trigger times: 11
Loss after 6340321 batches: 0.0522
trigger times: 12
Loss after 6341284 batches: 0.0501
trigger times: 13
Loss after 6342247 batches: 0.0493
trigger times: 14
Loss after 6343210 batches: 0.0496
trigger times: 15
Loss after 6344173 batches: 0.0484
trigger times: 16
Loss after 6345136 batches: 0.0491
trigger times: 17
Loss after 6346099 batches: 0.0488
trigger times: 18
Loss after 6347062 batches: 0.0490
trigger times: 19
Loss after 6348025 batches: 0.0479
trigger times: 20
Loss after 6348988 batches: 0.0468
trigger times: 21
Loss after 6349951 batches: 0.0475
trigger times: 22
Loss after 6350914 batches: 0.0471
trigger times: 23
Loss after 6351877 batches: 0.0456
trigger times: 24
Loss after 6352840 batches: 0.0463
trigger times: 25
Early stopping!
Start to test process.
Loss after 6353803 batches: 0.0458
Time to train on one home:  59.462018966674805
trigger times: 0
Loss after 6354766 batches: 0.0566
trigger times: 0
Loss after 6355729 batches: 0.0478
trigger times: 1
Loss after 6356692 batches: 0.0461
trigger times: 2
Loss after 6357655 batches: 0.0447
trigger times: 3
Loss after 6358618 batches: 0.0417
trigger times: 4
Loss after 6359581 batches: 0.0401
trigger times: 5
Loss after 6360544 batches: 0.0396
trigger times: 6
Loss after 6361507 batches: 0.0377
trigger times: 7
Loss after 6362470 batches: 0.0369
trigger times: 8
Loss after 6363433 batches: 0.0358
trigger times: 9
Loss after 6364396 batches: 0.0364
trigger times: 10
Loss after 6365359 batches: 0.0361
trigger times: 11
Loss after 6366322 batches: 0.0348
trigger times: 12
Loss after 6367285 batches: 0.0336
trigger times: 13
Loss after 6368248 batches: 0.0334
trigger times: 14
Loss after 6369211 batches: 0.0347
trigger times: 15
Loss after 6370174 batches: 0.0327
trigger times: 16
Loss after 6371137 batches: 0.0322
trigger times: 17
Loss after 6372100 batches: 0.0314
trigger times: 18
Loss after 6373063 batches: 0.0325
trigger times: 19
Loss after 6374026 batches: 0.0313
trigger times: 20
Loss after 6374989 batches: 0.0309
trigger times: 21
Loss after 6375952 batches: 0.0301
trigger times: 22
Loss after 6376915 batches: 0.0307
trigger times: 23
Loss after 6377878 batches: 0.0291
trigger times: 24
Loss after 6378841 batches: 0.0292
trigger times: 25
Early stopping!
Start to test process.
Loss after 6379804 batches: 0.0290
Time to train on one home:  56.441895961761475
trigger times: 0
Loss after 6380767 batches: 0.0611
trigger times: 1
Loss after 6381730 batches: 0.0470
trigger times: 2
Loss after 6382693 batches: 0.0472
trigger times: 3
Loss after 6383656 batches: 0.0461
trigger times: 4
Loss after 6384619 batches: 0.0448
trigger times: 5
Loss after 6385582 batches: 0.0429
trigger times: 6
Loss after 6386545 batches: 0.0428
trigger times: 7
Loss after 6387508 batches: 0.0413
trigger times: 8
Loss after 6388471 batches: 0.0418
trigger times: 9
Loss after 6389434 batches: 0.0425
trigger times: 10
Loss after 6390397 batches: 0.0418
trigger times: 11
Loss after 6391360 batches: 0.0412
trigger times: 12
Loss after 6392323 batches: 0.0409
trigger times: 13
Loss after 6393286 batches: 0.0402
trigger times: 14
Loss after 6394249 batches: 0.0399
trigger times: 15
Loss after 6395212 batches: 0.0390
trigger times: 16
Loss after 6396175 batches: 0.0387
trigger times: 17
Loss after 6397138 batches: 0.0393
trigger times: 18
Loss after 6398101 batches: 0.0391
trigger times: 19
Loss after 6399064 batches: 0.0389
trigger times: 20
Loss after 6400027 batches: 0.0384
trigger times: 21
Loss after 6400990 batches: 0.0382
trigger times: 22
Loss after 6401953 batches: 0.0375
trigger times: 23
Loss after 6402916 batches: 0.0377
trigger times: 24
Loss after 6403879 batches: 0.0377
trigger times: 25
Early stopping!
Start to test process.
Loss after 6404842 batches: 0.0376
Time to train on one home:  57.13971948623657
trigger times: 0
Loss after 6405737 batches: 0.0747
trigger times: 1
Loss after 6406632 batches: 0.0335
trigger times: 2
Loss after 6407527 batches: 0.0161
trigger times: 3
Loss after 6408422 batches: 0.0121
trigger times: 4
Loss after 6409317 batches: 0.0087
trigger times: 5
Loss after 6410212 batches: 0.0073
trigger times: 0
Loss after 6411107 batches: 0.0066
trigger times: 0
Loss after 6412002 batches: 0.0062
trigger times: 1
Loss after 6412897 batches: 0.0056
trigger times: 2
Loss after 6413792 batches: 0.0052
trigger times: 0
Loss after 6414687 batches: 0.0049
trigger times: 0
Loss after 6415582 batches: 0.0047
trigger times: 1
Loss after 6416477 batches: 0.0046
trigger times: 0
Loss after 6417372 batches: 0.0049
trigger times: 0
Loss after 6418267 batches: 0.0047
trigger times: 1
Loss after 6419162 batches: 0.0042
trigger times: 2
Loss after 6420057 batches: 0.0043
trigger times: 0
Loss after 6420952 batches: 0.0040
trigger times: 0
Loss after 6421847 batches: 0.0042
trigger times: 1
Loss after 6422742 batches: 0.0058
trigger times: 2
Loss after 6423637 batches: 0.0049
trigger times: 3
Loss after 6424532 batches: 0.0041
trigger times: 4
Loss after 6425427 batches: 0.0040
trigger times: 5
Loss after 6426322 batches: 0.0038
trigger times: 6
Loss after 6427217 batches: 0.0037
trigger times: 7
Loss after 6428112 batches: 0.0032
trigger times: 8
Loss after 6429007 batches: 0.0033
trigger times: 0
Loss after 6429902 batches: 0.0035
trigger times: 1
Loss after 6430797 batches: 0.0035
trigger times: 2
Loss after 6431692 batches: 0.0031
trigger times: 3
Loss after 6432587 batches: 0.0032
trigger times: 4
Loss after 6433482 batches: 0.0029
trigger times: 5
Loss after 6434377 batches: 0.0033
trigger times: 6
Loss after 6435272 batches: 0.0030
trigger times: 7
Loss after 6436167 batches: 0.0030
trigger times: 0
Loss after 6437062 batches: 0.0030
trigger times: 1
Loss after 6437957 batches: 0.0031
trigger times: 2
Loss after 6438852 batches: 0.0031
trigger times: 3
Loss after 6439747 batches: 0.0031
trigger times: 4
Loss after 6440642 batches: 0.0035
trigger times: 5
Loss after 6441537 batches: 0.0030
trigger times: 0
Loss after 6442432 batches: 0.0029
trigger times: 1
Loss after 6443327 batches: 0.0025
trigger times: 2
Loss after 6444222 batches: 0.0032
trigger times: 3
Loss after 6445117 batches: 0.0033
trigger times: 4
Loss after 6446012 batches: 0.0039
trigger times: 5
Loss after 6446907 batches: 0.0030
trigger times: 6
Loss after 6447802 batches: 0.0025
trigger times: 7
Loss after 6448697 batches: 0.0024
trigger times: 0
Loss after 6449592 batches: 0.0027
trigger times: 0
Loss after 6450487 batches: 0.0026
trigger times: 1
Loss after 6451382 batches: 0.0021
trigger times: 2
Loss after 6452277 batches: 0.0022
trigger times: 3
Loss after 6453172 batches: 0.0019
trigger times: 4
Loss after 6454067 batches: 0.0024
trigger times: 5
Loss after 6454962 batches: 0.0024
trigger times: 6
Loss after 6455857 batches: 0.0027
trigger times: 7
Loss after 6456752 batches: 0.0027
trigger times: 8
Loss after 6457647 batches: 0.0025
trigger times: 9
Loss after 6458542 batches: 0.0023
trigger times: 10
Loss after 6459437 batches: 0.0025
trigger times: 11
Loss after 6460332 batches: 0.0023
trigger times: 12
Loss after 6461227 batches: 0.0022
trigger times: 13
Loss after 6462122 batches: 0.0023
trigger times: 14
Loss after 6463017 batches: 0.0021
trigger times: 15
Loss after 6463912 batches: 0.0019
trigger times: 16
Loss after 6464807 batches: 0.0022
trigger times: 17
Loss after 6465702 batches: 0.0022
trigger times: 18
Loss after 6466597 batches: 0.0021
trigger times: 19
Loss after 6467492 batches: 0.0023
trigger times: 20
Loss after 6468387 batches: 0.0024
trigger times: 21
Loss after 6469282 batches: 0.0021
trigger times: 22
Loss after 6470177 batches: 0.0030
trigger times: 23
Loss after 6471072 batches: 0.0030
trigger times: 24
Loss after 6471967 batches: 0.0029
trigger times: 25
Early stopping!
Start to test process.
Loss after 6472862 batches: 0.0027
Time to train on one home:  93.17286610603333
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307]]
Round_4_results:  [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307]
trigger times: 0
Loss after 6473825 batches: 0.0602
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 6788 < 6789; dropping {'Training_Loss': 0.06021162016051156, 'Validation_Loss': 0.067413330078125, 'Training_R2': 0.230336410582622, 'Validation_R2': 0.06610405413229803, 'Training_F1': 0.4508923595035463, 'Validation_F1': 0.2737144040115599, 'Training_NEP': 0.8454707374523317, 'Validation_NEP': 0.9194294974353513, 'Training_NDE': 0.5668221561181173, 'Validation_NDE': 0.7247013006624757, 'Training_MAE': 24.345473029619175, 'Validation_MAE': 30.62641157096248, 'Training_MSE': 1783.325, 'Validation_MSE': 3589.7349}.
trigger times: 0
Loss after 6474788 batches: 0.0484
trigger times: 0
Loss after 6475751 batches: 0.0462
trigger times: 1
Loss after 6476714 batches: 0.0431
trigger times: 0
Loss after 6477677 batches: 0.0421
trigger times: 1
Loss after 6478640 batches: 0.0414
trigger times: 2
Loss after 6479603 batches: 0.0410
trigger times: 3
Loss after 6480566 batches: 0.0402
trigger times: 4
Loss after 6481529 batches: 0.0398
trigger times: 5
Loss after 6482492 batches: 0.0390
trigger times: 6
Loss after 6483455 batches: 0.0382
trigger times: 7
Loss after 6484418 batches: 0.0387
trigger times: 8
Loss after 6485381 batches: 0.0370
trigger times: 9
Loss after 6486344 batches: 0.0375
trigger times: 10
Loss after 6487307 batches: 0.0382
trigger times: 11
Loss after 6488270 batches: 0.0376
trigger times: 12
Loss after 6489233 batches: 0.0371
trigger times: 13
Loss after 6490196 batches: 0.0368
trigger times: 14
Loss after 6491159 batches: 0.0353
trigger times: 15
Loss after 6492122 batches: 0.0366
trigger times: 16
Loss after 6493085 batches: 0.0362
trigger times: 17
Loss after 6494048 batches: 0.0364
trigger times: 18
Loss after 6495011 batches: 0.0375
trigger times: 19
Loss after 6495974 batches: 0.0391
trigger times: 20
Loss after 6496937 batches: 0.0386
trigger times: 21
Loss after 6497900 batches: 0.0388
trigger times: 22
Loss after 6498863 batches: 0.0363
trigger times: 23
Loss after 6499826 batches: 0.0353
trigger times: 24
Loss after 6500789 batches: 0.0349
trigger times: 25
Early stopping!
Start to test process.
Loss after 6501752 batches: 0.0345
Time to train on one home:  55.872862100601196
trigger times: 0
Loss after 6502710 batches: 0.0732
trigger times: 0
Loss after 6503668 batches: 0.0459
trigger times: 1
Loss after 6504626 batches: 0.0419
trigger times: 2
Loss after 6505584 batches: 0.0362
trigger times: 3
Loss after 6506542 batches: 0.0331
trigger times: 0
Loss after 6507500 batches: 0.0323
trigger times: 1
Loss after 6508458 batches: 0.0295
trigger times: 2
Loss after 6509416 batches: 0.0286
trigger times: 0
Loss after 6510374 batches: 0.0286
trigger times: 1
Loss after 6511332 batches: 0.0276
trigger times: 2
Loss after 6512290 batches: 0.0286
trigger times: 3
Loss after 6513248 batches: 0.0283
trigger times: 4
Loss after 6514206 batches: 0.0271
trigger times: 5
Loss after 6515164 batches: 0.0258
trigger times: 6
Loss after 6516122 batches: 0.0260
trigger times: 7
Loss after 6517080 batches: 0.0245
trigger times: 8
Loss after 6518038 batches: 0.0253
trigger times: 0
Loss after 6518996 batches: 0.0246
trigger times: 1
Loss after 6519954 batches: 0.0242
trigger times: 2
Loss after 6520912 batches: 0.0238
trigger times: 3
Loss after 6521870 batches: 0.0240
trigger times: 4
Loss after 6522828 batches: 0.0229
trigger times: 0
Loss after 6523786 batches: 0.0229
trigger times: 1
Loss after 6524744 batches: 0.0234
trigger times: 2
Loss after 6525702 batches: 0.0225
trigger times: 3
Loss after 6526660 batches: 0.0219
trigger times: 4
Loss after 6527618 batches: 0.0223
trigger times: 5
Loss after 6528576 batches: 0.0218
trigger times: 6
Loss after 6529534 batches: 0.0215
trigger times: 7
Loss after 6530492 batches: 0.0201
trigger times: 8
Loss after 6531450 batches: 0.0203
trigger times: 9
Loss after 6532408 batches: 0.0221
trigger times: 10
Loss after 6533366 batches: 0.0222
trigger times: 11
Loss after 6534324 batches: 0.0220
trigger times: 12
Loss after 6535282 batches: 0.0226
trigger times: 13
Loss after 6536240 batches: 0.0215
trigger times: 14
Loss after 6537198 batches: 0.0200
trigger times: 15
Loss after 6538156 batches: 0.0198
trigger times: 16
Loss after 6539114 batches: 0.0192
trigger times: 17
Loss after 6540072 batches: 0.0195
trigger times: 18
Loss after 6541030 batches: 0.0195
trigger times: 19
Loss after 6541988 batches: 0.0199
trigger times: 20
Loss after 6542946 batches: 0.0211
trigger times: 21
Loss after 6543904 batches: 0.0199
trigger times: 22
Loss after 6544862 batches: 0.0184
trigger times: 23
Loss after 6545820 batches: 0.0190
trigger times: 24
Loss after 6546778 batches: 0.0192
trigger times: 25
Early stopping!
Start to test process.
Loss after 6547736 batches: 0.0179
Time to train on one home:  76.42525553703308
trigger times: 0
Loss after 6548699 batches: 0.0868
trigger times: 1
Loss after 6549662 batches: 0.0691
trigger times: 2
Loss after 6550625 batches: 0.0670
trigger times: 3
Loss after 6551588 batches: 0.0658
trigger times: 4
Loss after 6552551 batches: 0.0635
trigger times: 5
Loss after 6553514 batches: 0.0622
trigger times: 6
Loss after 6554477 batches: 0.0608
trigger times: 7
Loss after 6555440 batches: 0.0596
trigger times: 8
Loss after 6556403 batches: 0.0579
trigger times: 9
Loss after 6557366 batches: 0.0577
trigger times: 10
Loss after 6558329 batches: 0.0565
trigger times: 11
Loss after 6559292 batches: 0.0556
trigger times: 12
Loss after 6560255 batches: 0.0564
trigger times: 13
Loss after 6561218 batches: 0.0547
trigger times: 14
Loss after 6562181 batches: 0.0548
trigger times: 15
Loss after 6563144 batches: 0.0545
trigger times: 16
Loss after 6564107 batches: 0.0530
trigger times: 17
Loss after 6565070 batches: 0.0544
trigger times: 18
Loss after 6566033 batches: 0.0531
trigger times: 19
Loss after 6566996 batches: 0.0526
trigger times: 20
Loss after 6567959 batches: 0.0525
trigger times: 21
Loss after 6568922 batches: 0.0537
trigger times: 22
Loss after 6569885 batches: 0.0512
trigger times: 23
Loss after 6570848 batches: 0.0507
trigger times: 24
Loss after 6571811 batches: 0.0506
trigger times: 25
Early stopping!
Start to test process.
Loss after 6572774 batches: 0.0518
Time to train on one home:  54.357526779174805
trigger times: 0
Loss after 6573737 batches: 0.0916
trigger times: 1
Loss after 6574700 batches: 0.0779
trigger times: 2
Loss after 6575663 batches: 0.0772
trigger times: 3
Loss after 6576626 batches: 0.0759
trigger times: 4
Loss after 6577589 batches: 0.0739
trigger times: 5
Loss after 6578552 batches: 0.0717
trigger times: 6
Loss after 6579515 batches: 0.0700
trigger times: 7
Loss after 6580478 batches: 0.0680
trigger times: 8
Loss after 6581441 batches: 0.0683
trigger times: 9
Loss after 6582404 batches: 0.0668
trigger times: 10
Loss after 6583367 batches: 0.0664
trigger times: 11
Loss after 6584330 batches: 0.0663
trigger times: 12
Loss after 6585293 batches: 0.0659
trigger times: 13
Loss after 6586256 batches: 0.0663
trigger times: 14
Loss after 6587219 batches: 0.0652
trigger times: 15
Loss after 6588182 batches: 0.0645
trigger times: 16
Loss after 6589145 batches: 0.0633
trigger times: 17
Loss after 6590108 batches: 0.0624
trigger times: 18
Loss after 6591071 batches: 0.0649
trigger times: 19
Loss after 6592034 batches: 0.0629
trigger times: 20
Loss after 6592997 batches: 0.0635
trigger times: 21
Loss after 6593960 batches: 0.0633
trigger times: 22
Loss after 6594923 batches: 0.0634
trigger times: 23
Loss after 6595886 batches: 0.0620
trigger times: 24
Loss after 6596849 batches: 0.0614
trigger times: 25
Early stopping!
Start to test process.
Loss after 6597812 batches: 0.0618
Time to train on one home:  61.03378391265869
trigger times: 0
Loss after 6598775 batches: 0.0314
trigger times: 1
Loss after 6599738 batches: 0.0278
trigger times: 2
Loss after 6600701 batches: 0.0254
trigger times: 3
Loss after 6601664 batches: 0.0235
trigger times: 4
Loss after 6602627 batches: 0.0222
trigger times: 5
Loss after 6603590 batches: 0.0214
trigger times: 6
Loss after 6604553 batches: 0.0210
trigger times: 7
Loss after 6605516 batches: 0.0204
trigger times: 8
Loss after 6606479 batches: 0.0205
trigger times: 9
Loss after 6607442 batches: 0.0198
trigger times: 10
Loss after 6608405 batches: 0.0204
trigger times: 11
Loss after 6609368 batches: 0.0189
trigger times: 12
Loss after 6610331 batches: 0.0190
trigger times: 13
Loss after 6611294 batches: 0.0188
trigger times: 14
Loss after 6612257 batches: 0.0187
trigger times: 15
Loss after 6613220 batches: 0.0183
trigger times: 16
Loss after 6614183 batches: 0.0177
trigger times: 17
Loss after 6615146 batches: 0.0176
trigger times: 18
Loss after 6616109 batches: 0.0174
trigger times: 19
Loss after 6617072 batches: 0.0173
trigger times: 20
Loss after 6618035 batches: 0.0173
trigger times: 21
Loss after 6618998 batches: 0.0173
trigger times: 22
Loss after 6619961 batches: 0.0175
trigger times: 23
Loss after 6620924 batches: 0.0169
trigger times: 24
Loss after 6621887 batches: 0.0165
trigger times: 25
Early stopping!
Start to test process.
Loss after 6622850 batches: 0.0162
Time to train on one home:  56.32064867019653
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6623813 batches: 0.0657
trigger times: 1
Loss after 6624776 batches: 0.0245
trigger times: 2
Loss after 6625739 batches: 0.0232
trigger times: 3
Loss after 6626702 batches: 0.0203
trigger times: 4
Loss after 6627665 batches: 0.0187
trigger times: 5
Loss after 6628628 batches: 0.0171
trigger times: 6
Loss after 6629591 batches: 0.0164
trigger times: 7
Loss after 6630554 batches: 0.0161
trigger times: 8
Loss after 6631517 batches: 0.0154
trigger times: 9
Loss after 6632480 batches: 0.0152
trigger times: 10
Loss after 6633443 batches: 0.0152
trigger times: 11
Loss after 6634406 batches: 0.0146
trigger times: 12
Loss after 6635369 batches: 0.0145
trigger times: 13
Loss after 6636332 batches: 0.0146
trigger times: 14
Loss after 6637295 batches: 0.0144
trigger times: 15
Loss after 6638258 batches: 0.0144
trigger times: 16
Loss after 6639221 batches: 0.0142
trigger times: 17
Loss after 6640184 batches: 0.0142
trigger times: 18
Loss after 6641147 batches: 0.0142
trigger times: 19
Loss after 6642110 batches: 0.0141
trigger times: 20
Loss after 6643073 batches: 0.0141
trigger times: 21
Loss after 6644036 batches: 0.0138
trigger times: 22
Loss after 6644999 batches: 0.0136
trigger times: 23
Loss after 6645962 batches: 0.0138
trigger times: 24
Loss after 6646925 batches: 0.0137
trigger times: 25
Early stopping!
Start to test process.
Loss after 6647888 batches: 0.0137
Time to train on one home:  56.86373782157898
trigger times: 0
Loss after 6648851 batches: 0.1002
trigger times: 1
Loss after 6649814 batches: 0.0946
trigger times: 0
Loss after 6650777 batches: 0.0895
trigger times: 1
Loss after 6651740 batches: 0.0886
trigger times: 2
Loss after 6652703 batches: 0.0849
trigger times: 3
Loss after 6653666 batches: 0.0838
trigger times: 4
Loss after 6654629 batches: 0.0811
trigger times: 5
Loss after 6655592 batches: 0.0807
trigger times: 6
Loss after 6656555 batches: 0.0804
trigger times: 7
Loss after 6657518 batches: 0.0803
trigger times: 8
Loss after 6658481 batches: 0.0804
trigger times: 9
Loss after 6659444 batches: 0.0777
trigger times: 10
Loss after 6660407 batches: 0.0776
trigger times: 11
Loss after 6661370 batches: 0.0777
trigger times: 12
Loss after 6662333 batches: 0.0766
trigger times: 13
Loss after 6663296 batches: 0.0757
trigger times: 14
Loss after 6664259 batches: 0.0762
trigger times: 15
Loss after 6665222 batches: 0.0759
trigger times: 16
Loss after 6666185 batches: 0.0753
trigger times: 17
Loss after 6667148 batches: 0.0756
trigger times: 18
Loss after 6668111 batches: 0.0746
trigger times: 19
Loss after 6669074 batches: 0.0739
trigger times: 20
Loss after 6670037 batches: 0.0751
trigger times: 21
Loss after 6671000 batches: 0.0715
trigger times: 22
Loss after 6671963 batches: 0.0748
trigger times: 23
Loss after 6672926 batches: 0.0723
trigger times: 24
Loss after 6673889 batches: 0.0726
trigger times: 25
Early stopping!
Start to test process.
Loss after 6674852 batches: 0.0717
Time to train on one home:  58.91229844093323
trigger times: 0
Loss after 6675815 batches: 0.0562
trigger times: 1
Loss after 6676778 batches: 0.0448
trigger times: 0
Loss after 6677741 batches: 0.0400
trigger times: 1
Loss after 6678704 batches: 0.0360
trigger times: 0
Loss after 6679667 batches: 0.0356
trigger times: 1
Loss after 6680630 batches: 0.0339
trigger times: 2
Loss after 6681593 batches: 0.0321
trigger times: 3
Loss after 6682556 batches: 0.0314
trigger times: 4
Loss after 6683519 batches: 0.0313
trigger times: 0
Loss after 6684482 batches: 0.0305
trigger times: 1
Loss after 6685445 batches: 0.0298
trigger times: 2
Loss after 6686408 batches: 0.0307
trigger times: 3
Loss after 6687371 batches: 0.0302
trigger times: 4
Loss after 6688334 batches: 0.0289
trigger times: 5
Loss after 6689297 batches: 0.0281
trigger times: 6
Loss after 6690260 batches: 0.0282
trigger times: 7
Loss after 6691223 batches: 0.0274
trigger times: 8
Loss after 6692186 batches: 0.0298
trigger times: 9
Loss after 6693149 batches: 0.0277
trigger times: 10
Loss after 6694112 batches: 0.0274
trigger times: 0
Loss after 6695075 batches: 0.0273
trigger times: 1
Loss after 6696038 batches: 0.0254
trigger times: 2
Loss after 6697001 batches: 0.0259
trigger times: 3
Loss after 6697964 batches: 0.0257
trigger times: 0
Loss after 6698927 batches: 0.0264
trigger times: 1
Loss after 6699890 batches: 0.0263
trigger times: 2
Loss after 6700853 batches: 0.0258
trigger times: 3
Loss after 6701816 batches: 0.0252
trigger times: 4
Loss after 6702779 batches: 0.0262
trigger times: 5
Loss after 6703742 batches: 0.0258
trigger times: 6
Loss after 6704705 batches: 0.0237
trigger times: 7
Loss after 6705668 batches: 0.0251
trigger times: 8
Loss after 6706631 batches: 0.0242
trigger times: 9
Loss after 6707594 batches: 0.0241
trigger times: 10
Loss after 6708557 batches: 0.0249
trigger times: 11
Loss after 6709520 batches: 0.0257
trigger times: 12
Loss after 6710483 batches: 0.0255
trigger times: 13
Loss after 6711446 batches: 0.0254
trigger times: 14
Loss after 6712409 batches: 0.0246
trigger times: 15
Loss after 6713372 batches: 0.0239
trigger times: 16
Loss after 6714335 batches: 0.0237
trigger times: 17
Loss after 6715298 batches: 0.0228
trigger times: 18
Loss after 6716261 batches: 0.0225
trigger times: 19
Loss after 6717224 batches: 0.0232
trigger times: 20
Loss after 6718187 batches: 0.0246
trigger times: 21
Loss after 6719150 batches: 0.0246
trigger times: 22
Loss after 6720113 batches: 0.0233
trigger times: 23
Loss after 6721076 batches: 0.0245
trigger times: 24
Loss after 6722039 batches: 0.0231
trigger times: 25
Early stopping!
Start to test process.
Loss after 6723002 batches: 0.0235
Time to train on one home:  74.63056683540344
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6723965 batches: 0.0865
trigger times: 1
Loss after 6724928 batches: 0.0781
trigger times: 2
Loss after 6725891 batches: 0.0761
trigger times: 3
Loss after 6726854 batches: 0.0747
trigger times: 4
Loss after 6727817 batches: 0.0719
trigger times: 5
Loss after 6728780 batches: 0.0711
trigger times: 6
Loss after 6729743 batches: 0.0709
trigger times: 7
Loss after 6730706 batches: 0.0698
trigger times: 8
Loss after 6731669 batches: 0.0683
trigger times: 9
Loss after 6732632 batches: 0.0682
trigger times: 10
Loss after 6733595 batches: 0.0675
trigger times: 11
Loss after 6734558 batches: 0.0672
trigger times: 12
Loss after 6735521 batches: 0.0672
trigger times: 13
Loss after 6736484 batches: 0.0660
trigger times: 14
Loss after 6737447 batches: 0.0647
trigger times: 15
Loss after 6738410 batches: 0.0641
trigger times: 16
Loss after 6739373 batches: 0.0646
trigger times: 17
Loss after 6740336 batches: 0.0640
trigger times: 18
Loss after 6741299 batches: 0.0638
trigger times: 19
Loss after 6742262 batches: 0.0638
trigger times: 20
Loss after 6743225 batches: 0.0638
trigger times: 21
Loss after 6744188 batches: 0.0637
trigger times: 22
Loss after 6745151 batches: 0.0628
trigger times: 23
Loss after 6746114 batches: 0.0635
trigger times: 24
Loss after 6747077 batches: 0.0625
trigger times: 25
Early stopping!
Start to test process.
Loss after 6748040 batches: 0.0621
Time to train on one home:  54.01317811012268
trigger times: 0
Loss after 6749003 batches: 0.0918
trigger times: 0
Loss after 6749966 batches: 0.0706
trigger times: 0
Loss after 6750929 batches: 0.0655
trigger times: 1
Loss after 6751892 batches: 0.0627
trigger times: 0
Loss after 6752855 batches: 0.0592
trigger times: 0
Loss after 6753818 batches: 0.0562
trigger times: 1
Loss after 6754781 batches: 0.0561
trigger times: 2
Loss after 6755744 batches: 0.0538
trigger times: 0
Loss after 6756707 batches: 0.0520
trigger times: 1
Loss after 6757670 batches: 0.0514
trigger times: 0
Loss after 6758633 batches: 0.0511
trigger times: 0
Loss after 6759596 batches: 0.0486
trigger times: 1
Loss after 6760559 batches: 0.0506
trigger times: 2
Loss after 6761522 batches: 0.0502
trigger times: 3
Loss after 6762485 batches: 0.0479
trigger times: 4
Loss after 6763448 batches: 0.0483
trigger times: 5
Loss after 6764411 batches: 0.0474
trigger times: 6
Loss after 6765374 batches: 0.0481
trigger times: 7
Loss after 6766337 batches: 0.0469
trigger times: 8
Loss after 6767300 batches: 0.0462
trigger times: 9
Loss after 6768263 batches: 0.0457
trigger times: 10
Loss after 6769226 batches: 0.0456
trigger times: 11
Loss after 6770189 batches: 0.0466
trigger times: 12
Loss after 6771152 batches: 0.0441
trigger times: 13
Loss after 6772115 batches: 0.0451
trigger times: 14
Loss after 6773078 batches: 0.0451
trigger times: 15
Loss after 6774041 batches: 0.0451
trigger times: 16
Loss after 6775004 batches: 0.0445
trigger times: 17
Loss after 6775967 batches: 0.0437
trigger times: 18
Loss after 6776930 batches: 0.0447
trigger times: 19
Loss after 6777893 batches: 0.0449
trigger times: 20
Loss after 6778856 batches: 0.0441
trigger times: 21
Loss after 6779819 batches: 0.0433
trigger times: 22
Loss after 6780782 batches: 0.0431
trigger times: 23
Loss after 6781745 batches: 0.0428
trigger times: 24
Loss after 6782708 batches: 0.0426
trigger times: 25
Early stopping!
Start to test process.
Loss after 6783671 batches: 0.0421
Time to train on one home:  65.95761227607727
trigger times: 0
Loss after 6784634 batches: 0.0793
trigger times: 1
Loss after 6785597 batches: 0.0723
trigger times: 2
Loss after 6786560 batches: 0.0717
trigger times: 3
Loss after 6787523 batches: 0.0706
trigger times: 4
Loss after 6788486 batches: 0.0681
trigger times: 5
Loss after 6789449 batches: 0.0661
trigger times: 6
Loss after 6790412 batches: 0.0645
trigger times: 7
Loss after 6791375 batches: 0.0639
trigger times: 8
Loss after 6792338 batches: 0.0631
trigger times: 9
Loss after 6793301 batches: 0.0644
trigger times: 10
Loss after 6794264 batches: 0.0629
trigger times: 11
Loss after 6795227 batches: 0.0624
trigger times: 12
Loss after 6796190 batches: 0.0625
trigger times: 13
Loss after 6797153 batches: 0.0619
trigger times: 14
Loss after 6798116 batches: 0.0625
trigger times: 15
Loss after 6799079 batches: 0.0598
trigger times: 16
Loss after 6800042 batches: 0.0618
trigger times: 17
Loss after 6801005 batches: 0.0616
trigger times: 18
Loss after 6801968 batches: 0.0591
trigger times: 19
Loss after 6802931 batches: 0.0601
trigger times: 20
Loss after 6803894 batches: 0.0606
trigger times: 21
Loss after 6804857 batches: 0.0601
trigger times: 22
Loss after 6805820 batches: 0.0590
trigger times: 23
Loss after 6806783 batches: 0.0577
trigger times: 24
Loss after 6807746 batches: 0.0582
trigger times: 25
Early stopping!
Start to test process.
Loss after 6808709 batches: 0.0583
Time to train on one home:  55.02701139450073
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6809672 batches: 0.0627
trigger times: 1
Loss after 6810635 batches: 0.0544
trigger times: 2
Loss after 6811598 batches: 0.0513
trigger times: 3
Loss after 6812561 batches: 0.0483
trigger times: 4
Loss after 6813524 batches: 0.0455
trigger times: 5
Loss after 6814487 batches: 0.0426
trigger times: 6
Loss after 6815450 batches: 0.0402
trigger times: 7
Loss after 6816413 batches: 0.0386
trigger times: 8
Loss after 6817376 batches: 0.0378
trigger times: 9
Loss after 6818339 batches: 0.0367
trigger times: 10
Loss after 6819302 batches: 0.0356
trigger times: 11
Loss after 6820265 batches: 0.0352
trigger times: 12
Loss after 6821228 batches: 0.0343
trigger times: 13
Loss after 6822191 batches: 0.0340
trigger times: 14
Loss after 6823154 batches: 0.0338
trigger times: 15
Loss after 6824117 batches: 0.0332
trigger times: 16
Loss after 6825080 batches: 0.0333
trigger times: 17
Loss after 6826043 batches: 0.0324
trigger times: 18
Loss after 6827006 batches: 0.0321
trigger times: 19
Loss after 6827969 batches: 0.0323
trigger times: 20
Loss after 6828932 batches: 0.0314
trigger times: 21
Loss after 6829895 batches: 0.0309
trigger times: 22
Loss after 6830858 batches: 0.0314
trigger times: 23
Loss after 6831821 batches: 0.0305
trigger times: 24
Loss after 6832784 batches: 0.0307
trigger times: 25
Early stopping!
Start to test process.
Loss after 6833747 batches: 0.0306
Time to train on one home:  59.58770751953125
trigger times: 0
Loss after 6834710 batches: 0.0820
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6835673 batches: 0.0572
trigger times: 1
Loss after 6836636 batches: 0.0533
trigger times: 2
Loss after 6837599 batches: 0.0498
trigger times: 3
Loss after 6838562 batches: 0.0435
trigger times: 4
Loss after 6839525 batches: 0.0396
trigger times: 5
Loss after 6840488 batches: 0.0387
trigger times: 6
Loss after 6841451 batches: 0.0358
trigger times: 7
Loss after 6842414 batches: 0.0351
trigger times: 8
Loss after 6843377 batches: 0.0332
trigger times: 9
Loss after 6844340 batches: 0.0318
trigger times: 10
Loss after 6845303 batches: 0.0329
trigger times: 11
Loss after 6846266 batches: 0.0311
trigger times: 12
Loss after 6847229 batches: 0.0310
trigger times: 13
Loss after 6848192 batches: 0.0303
trigger times: 14
Loss after 6849155 batches: 0.0299
trigger times: 15
Loss after 6850118 batches: 0.0307
trigger times: 16
Loss after 6851081 batches: 0.0289
trigger times: 17
Loss after 6852044 batches: 0.0287
trigger times: 18
Loss after 6853007 batches: 0.0307
trigger times: 19
Loss after 6853970 batches: 0.0278
trigger times: 20
Loss after 6854933 batches: 0.0285
trigger times: 21
Loss after 6855896 batches: 0.0275
trigger times: 22
Loss after 6856859 batches: 0.0272
trigger times: 23
Loss after 6857822 batches: 0.0288
trigger times: 24
Loss after 6858785 batches: 0.0268
trigger times: 25
Early stopping!
Start to test process.
Loss after 6859748 batches: 0.0261
Time to train on one home:  57.19152784347534
trigger times: 0
Loss after 6860677 batches: 0.0940
trigger times: 0
Loss after 6861606 batches: 0.0620
trigger times: 0
Loss after 6862535 batches: 0.0524
trigger times: 0
Loss after 6863464 batches: 0.0460
trigger times: 0
Loss after 6864393 batches: 0.0410
trigger times: 0
Loss after 6865322 batches: 0.0382
trigger times: 1
Loss after 6866251 batches: 0.0372
trigger times: 2
Loss after 6867180 batches: 0.0373
trigger times: 0
Loss after 6868109 batches: 0.0354
trigger times: 0
Loss after 6869038 batches: 0.0366
trigger times: 1
Loss after 6869967 batches: 0.0332
trigger times: 2
Loss after 6870896 batches: 0.0311
trigger times: 3
Loss after 6871825 batches: 0.0335
trigger times: 4
Loss after 6872754 batches: 0.0314
trigger times: 5
Loss after 6873683 batches: 0.0295
trigger times: 6
Loss after 6874612 batches: 0.0302
trigger times: 7
Loss after 6875541 batches: 0.0327
trigger times: 8
Loss after 6876470 batches: 0.0300
trigger times: 9
Loss after 6877399 batches: 0.0314
trigger times: 0
Loss after 6878328 batches: 0.0310
trigger times: 1
Loss after 6879257 batches: 0.0308
trigger times: 2
Loss after 6880186 batches: 0.0301
trigger times: 3
Loss after 6881115 batches: 0.0295
trigger times: 4
Loss after 6882044 batches: 0.0289
trigger times: 5
Loss after 6882973 batches: 0.0295
trigger times: 6
Loss after 6883902 batches: 0.0273
trigger times: 7
Loss after 6884831 batches: 0.0282
trigger times: 8
Loss after 6885760 batches: 0.0289
trigger times: 9
Loss after 6886689 batches: 0.0283
trigger times: 0
Loss after 6887618 batches: 0.0266
trigger times: 1
Loss after 6888547 batches: 0.0295
trigger times: 2
Loss after 6889476 batches: 0.0285
trigger times: 3
Loss after 6890405 batches: 0.0295
trigger times: 4
Loss after 6891334 batches: 0.0258
trigger times: 5
Loss after 6892263 batches: 0.0301
trigger times: 6
Loss after 6893192 batches: 0.0289
trigger times: 7
Loss after 6894121 batches: 0.0284
trigger times: 8
Loss after 6895050 batches: 0.0268
trigger times: 9
Loss after 6895979 batches: 0.0267
trigger times: 10
Loss after 6896908 batches: 0.0271
trigger times: 11
Loss after 6897837 batches: 0.0267
trigger times: 12
Loss after 6898766 batches: 0.0292
trigger times: 13
Loss after 6899695 batches: 0.0293
trigger times: 14
Loss after 6900624 batches: 0.0271
trigger times: 15
Loss after 6901553 batches: 0.0263
trigger times: 16
Loss after 6902482 batches: 0.0282
trigger times: 17
Loss after 6903411 batches: 0.0286
trigger times: 18
Loss after 6904340 batches: 0.0273
trigger times: 19
Loss after 6905269 batches: 0.0291
trigger times: 20
Loss after 6906198 batches: 0.0274
trigger times: 21
Loss after 6907127 batches: 0.0278
trigger times: 22
Loss after 6908056 batches: 0.0249
trigger times: 23
Loss after 6908985 batches: 0.0254
trigger times: 24
Loss after 6909914 batches: 0.0270
trigger times: 25
Early stopping!
Start to test process.
Loss after 6910843 batches: 0.0245
Time to train on one home:  80.20848298072815
trigger times: 0
Loss after 6911805 batches: 0.0723
trigger times: 1
Loss after 6912767 batches: 0.0671
trigger times: 2
Loss after 6913729 batches: 0.0662
trigger times: 3
Loss after 6914691 batches: 0.0637
trigger times: 4
Loss after 6915653 batches: 0.0619
trigger times: 5
Loss after 6916615 batches: 0.0617
trigger times: 6
Loss after 6917577 batches: 0.0609
trigger times: 7
Loss after 6918539 batches: 0.0605
trigger times: 8
Loss after 6919501 batches: 0.0595
trigger times: 9
Loss after 6920463 batches: 0.0591
trigger times: 10
Loss after 6921425 batches: 0.0581
trigger times: 11
Loss after 6922387 batches: 0.0589
trigger times: 12
Loss after 6923349 batches: 0.0585
trigger times: 13
Loss after 6924311 batches: 0.0579
trigger times: 14
Loss after 6925273 batches: 0.0587
trigger times: 15
Loss after 6926235 batches: 0.0577
trigger times: 16
Loss after 6927197 batches: 0.0567
trigger times: 17
Loss after 6928159 batches: 0.0567
trigger times: 18
Loss after 6929121 batches: 0.0568
trigger times: 19
Loss after 6930083 batches: 0.0595
trigger times: 20
Loss after 6931045 batches: 0.0575
trigger times: 21
Loss after 6932007 batches: 0.0561
trigger times: 22
Loss after 6932969 batches: 0.0562
trigger times: 23
Loss after 6933931 batches: 0.0565
trigger times: 24
Loss after 6934893 batches: 0.0565
trigger times: 25
Early stopping!
Start to test process.
Loss after 6935855 batches: 0.0565
Time to train on one home:  56.58376383781433
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6936818 batches: 0.0543
trigger times: 1
Loss after 6937781 batches: 0.0462
trigger times: 2
Loss after 6938744 batches: 0.0477
trigger times: 0
Loss after 6939707 batches: 0.0443
trigger times: 1
Loss after 6940670 batches: 0.0424
trigger times: 2
Loss after 6941633 batches: 0.0416
trigger times: 3
Loss after 6942596 batches: 0.0401
trigger times: 4
Loss after 6943559 batches: 0.0397
trigger times: 5
Loss after 6944522 batches: 0.0391
trigger times: 6
Loss after 6945485 batches: 0.0386
trigger times: 7
Loss after 6946448 batches: 0.0384
trigger times: 8
Loss after 6947411 batches: 0.0378
trigger times: 9
Loss after 6948374 batches: 0.0374
trigger times: 10
Loss after 6949337 batches: 0.0371
trigger times: 11
Loss after 6950300 batches: 0.0368
trigger times: 12
Loss after 6951263 batches: 0.0366
trigger times: 13
Loss after 6952226 batches: 0.0364
trigger times: 14
Loss after 6953189 batches: 0.0364
trigger times: 15
Loss after 6954152 batches: 0.0359
trigger times: 16
Loss after 6955115 batches: 0.0358
trigger times: 17
Loss after 6956078 batches: 0.0347
trigger times: 18
Loss after 6957041 batches: 0.0356
trigger times: 19
Loss after 6958004 batches: 0.0354
trigger times: 20
Loss after 6958967 batches: 0.0350
trigger times: 21
Loss after 6959930 batches: 0.0347
trigger times: 22
Loss after 6960893 batches: 0.0341
trigger times: 23
Loss after 6961856 batches: 0.0343
trigger times: 24
Loss after 6962819 batches: 0.0339
trigger times: 25
Early stopping!
Start to test process.
Loss after 6963782 batches: 0.0347
Time to train on one home:  55.330190896987915
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6964745 batches: 0.0555
trigger times: 1
Loss after 6965708 batches: 0.0520
trigger times: 0
Loss after 6966671 batches: 0.0496
trigger times: 1
Loss after 6967634 batches: 0.0470
trigger times: 2
Loss after 6968597 batches: 0.0454
trigger times: 3
Loss after 6969560 batches: 0.0437
trigger times: 4
Loss after 6970523 batches: 0.0440
trigger times: 5
Loss after 6971486 batches: 0.0430
trigger times: 6
Loss after 6972449 batches: 0.0417
trigger times: 7
Loss after 6973412 batches: 0.0412
trigger times: 8
Loss after 6974375 batches: 0.0422
trigger times: 9
Loss after 6975338 batches: 0.0411
trigger times: 10
Loss after 6976301 batches: 0.0407
trigger times: 11
Loss after 6977264 batches: 0.0400
trigger times: 12
Loss after 6978227 batches: 0.0396
trigger times: 13
Loss after 6979190 batches: 0.0398
trigger times: 14
Loss after 6980153 batches: 0.0391
trigger times: 0
Loss after 6981116 batches: 0.0389
trigger times: 1
Loss after 6982079 batches: 0.0392
trigger times: 2
Loss after 6983042 batches: 0.0396
trigger times: 3
Loss after 6984005 batches: 0.0377
trigger times: 4
Loss after 6984968 batches: 0.0373
trigger times: 0
Loss after 6985931 batches: 0.0372
trigger times: 1
Loss after 6986894 batches: 0.0373
trigger times: 2
Loss after 6987857 batches: 0.0368
trigger times: 3
Loss after 6988820 batches: 0.0365
trigger times: 4
Loss after 6989783 batches: 0.0368
trigger times: 5
Loss after 6990746 batches: 0.0356
trigger times: 6
Loss after 6991709 batches: 0.0367
trigger times: 7
Loss after 6992672 batches: 0.0374
trigger times: 8
Loss after 6993635 batches: 0.0360
trigger times: 9
Loss after 6994598 batches: 0.0364
trigger times: 10
Loss after 6995561 batches: 0.0351
trigger times: 11
Loss after 6996524 batches: 0.0353
trigger times: 12
Loss after 6997487 batches: 0.0349
trigger times: 13
Loss after 6998450 batches: 0.0351
trigger times: 14
Loss after 6999413 batches: 0.0343
trigger times: 15
Loss after 7000376 batches: 0.0347
trigger times: 16
Loss after 7001339 batches: 0.0354
trigger times: 17
Loss after 7002302 batches: 0.0340
trigger times: 18
Loss after 7003265 batches: 0.0331
trigger times: 19
Loss after 7004228 batches: 0.0331
trigger times: 20
Loss after 7005191 batches: 0.0338
trigger times: 21
Loss after 7006154 batches: 0.0329
trigger times: 22
Loss after 7007117 batches: 0.0337
trigger times: 23
Loss after 7008080 batches: 0.0361
trigger times: 24
Loss after 7009043 batches: 0.0348
trigger times: 25
Early stopping!
Start to test process.
Loss after 7010006 batches: 0.0342
Time to train on one home:  73.95304560661316
trigger times: 0
Loss after 7010969 batches: 0.1074
trigger times: 0
Loss after 7011932 batches: 0.1009
trigger times: 1
Loss after 7012895 batches: 0.0976
trigger times: 2
Loss after 7013858 batches: 0.0931
trigger times: 3
Loss after 7014821 batches: 0.0920
trigger times: 4
Loss after 7015784 batches: 0.0909
trigger times: 5
Loss after 7016747 batches: 0.0901
trigger times: 6
Loss after 7017710 batches: 0.0887
trigger times: 7
Loss after 7018673 batches: 0.0884
trigger times: 8
Loss after 7019636 batches: 0.0864
trigger times: 9
Loss after 7020599 batches: 0.0865
trigger times: 10
Loss after 7021562 batches: 0.0874
trigger times: 11
Loss after 7022525 batches: 0.0849
trigger times: 12
Loss after 7023488 batches: 0.0841
trigger times: 13
Loss after 7024451 batches: 0.0836
trigger times: 14
Loss after 7025414 batches: 0.0828
trigger times: 15
Loss after 7026377 batches: 0.0838
trigger times: 16
Loss after 7027340 batches: 0.0850
trigger times: 17
Loss after 7028303 batches: 0.0831
trigger times: 18
Loss after 7029266 batches: 0.0804
trigger times: 19
Loss after 7030229 batches: 0.0836
trigger times: 20
Loss after 7031192 batches: 0.0830
trigger times: 21
Loss after 7032155 batches: 0.0833
trigger times: 22
Loss after 7033118 batches: 0.0825
trigger times: 23
Loss after 7034081 batches: 0.0814
trigger times: 24
Loss after 7035044 batches: 0.0798
trigger times: 25
Early stopping!
Start to test process.
Loss after 7036007 batches: 0.0807
Time to train on one home:  56.22067880630493
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7036970 batches: 0.1063
trigger times: 1
Loss after 7037933 batches: 0.0737
trigger times: 2
Loss after 7038896 batches: 0.0722
trigger times: 3
Loss after 7039859 batches: 0.0663
trigger times: 4
Loss after 7040822 batches: 0.0589
trigger times: 5
Loss after 7041785 batches: 0.0549
trigger times: 6
Loss after 7042748 batches: 0.0517
trigger times: 7
Loss after 7043711 batches: 0.0491
trigger times: 8
Loss after 7044674 batches: 0.0469
trigger times: 9
Loss after 7045637 batches: 0.0452
trigger times: 0
Loss after 7046600 batches: 0.0452
trigger times: 1
Loss after 7047563 batches: 0.0429
trigger times: 2
Loss after 7048526 batches: 0.0431
trigger times: 0
Loss after 7049489 batches: 0.0425
trigger times: 1
Loss after 7050452 batches: 0.0425
trigger times: 2
Loss after 7051415 batches: 0.0405
trigger times: 0
Loss after 7052378 batches: 0.0402
trigger times: 1
Loss after 7053341 batches: 0.0408
trigger times: 2
Loss after 7054304 batches: 0.0384
trigger times: 3
Loss after 7055267 batches: 0.0390
trigger times: 4
Loss after 7056230 batches: 0.0384
trigger times: 5
Loss after 7057193 batches: 0.0387
trigger times: 6
Loss after 7058156 batches: 0.0380
trigger times: 7
Loss after 7059119 batches: 0.0377
trigger times: 8
Loss after 7060082 batches: 0.0377
trigger times: 9
Loss after 7061045 batches: 0.0356
trigger times: 10
Loss after 7062008 batches: 0.0361
trigger times: 11
Loss after 7062971 batches: 0.0351
trigger times: 12
Loss after 7063934 batches: 0.0352
trigger times: 13
Loss after 7064897 batches: 0.0344
trigger times: 14
Loss after 7065860 batches: 0.0344
trigger times: 15
Loss after 7066823 batches: 0.0350
trigger times: 16
Loss after 7067786 batches: 0.0342
trigger times: 17
Loss after 7068749 batches: 0.0344
trigger times: 18
Loss after 7069712 batches: 0.0333
trigger times: 19
Loss after 7070675 batches: 0.0337
trigger times: 20
Loss after 7071638 batches: 0.0345
trigger times: 21
Loss after 7072601 batches: 0.0328
trigger times: 22
Loss after 7073564 batches: 0.0333
trigger times: 23
Loss after 7074527 batches: 0.0327
trigger times: 24
Loss after 7075490 batches: 0.0334
trigger times: 25
Early stopping!
Start to test process.
Loss after 7076453 batches: 0.0333
Time to train on one home:  72.32584643363953
trigger times: 0
Loss after 7077412 batches: 0.0971
trigger times: 1
Loss after 7078371 batches: 0.0555
trigger times: 0
Loss after 7079330 batches: 0.0435
trigger times: 0
Loss after 7080289 batches: 0.0362
trigger times: 0
Loss after 7081248 batches: 0.0327
trigger times: 0
Loss after 7082207 batches: 0.0289
trigger times: 1
Loss after 7083166 batches: 0.0275
trigger times: 0
Loss after 7084125 batches: 0.0264
trigger times: 0
Loss after 7085084 batches: 0.0251
trigger times: 1
Loss after 7086043 batches: 0.0239
trigger times: 2
Loss after 7087002 batches: 0.0239
trigger times: 3
Loss after 7087961 batches: 0.0233
trigger times: 4
Loss after 7088920 batches: 0.0227
trigger times: 5
Loss after 7089879 batches: 0.0228
trigger times: 6
Loss after 7090838 batches: 0.0226
trigger times: 7
Loss after 7091797 batches: 0.0225
trigger times: 8
Loss after 7092756 batches: 0.0221
trigger times: 9
Loss after 7093715 batches: 0.0214
trigger times: 10
Loss after 7094674 batches: 0.0208
trigger times: 11
Loss after 7095633 batches: 0.0213
trigger times: 12
Loss after 7096592 batches: 0.0216
trigger times: 0
Loss after 7097551 batches: 0.0220
trigger times: 1
Loss after 7098510 batches: 0.0218
trigger times: 2
Loss after 7099469 batches: 0.0205
trigger times: 3
Loss after 7100428 batches: 0.0198
trigger times: 4
Loss after 7101387 batches: 0.0200
trigger times: 5
Loss after 7102346 batches: 0.0199
trigger times: 6
Loss after 7103305 batches: 0.0200
trigger times: 7
Loss after 7104264 batches: 0.0190
trigger times: 8
Loss after 7105223 batches: 0.0186
trigger times: 9
Loss after 7106182 batches: 0.0187
trigger times: 10
Loss after 7107141 batches: 0.0196
trigger times: 11
Loss after 7108100 batches: 0.0185
trigger times: 12
Loss after 7109059 batches: 0.0187
trigger times: 13
Loss after 7110018 batches: 0.0187
trigger times: 14
Loss after 7110977 batches: 0.0187
trigger times: 0
Loss after 7111936 batches: 0.0195
trigger times: 1
Loss after 7112895 batches: 0.0183
trigger times: 2
Loss after 7113854 batches: 0.0176
trigger times: 3
Loss after 7114813 batches: 0.0183
trigger times: 4
Loss after 7115772 batches: 0.0181
trigger times: 5
Loss after 7116731 batches: 0.0176
trigger times: 6
Loss after 7117690 batches: 0.0174
trigger times: 7
Loss after 7118649 batches: 0.0174
trigger times: 8
Loss after 7119608 batches: 0.0194
trigger times: 9
Loss after 7120567 batches: 0.0193
trigger times: 10
Loss after 7121526 batches: 0.0181
trigger times: 11
Loss after 7122485 batches: 0.0181
trigger times: 12
Loss after 7123444 batches: 0.0176
trigger times: 13
Loss after 7124403 batches: 0.0180
trigger times: 14
Loss after 7125362 batches: 0.0173
trigger times: 15
Loss after 7126321 batches: 0.0169
trigger times: 16
Loss after 7127280 batches: 0.0164
trigger times: 17
Loss after 7128239 batches: 0.0158
trigger times: 18
Loss after 7129198 batches: 0.0161
trigger times: 19
Loss after 7130157 batches: 0.0158
trigger times: 20
Loss after 7131116 batches: 0.0160
trigger times: 21
Loss after 7132075 batches: 0.0152
trigger times: 22
Loss after 7133034 batches: 0.0159
trigger times: 23
Loss after 7133993 batches: 0.0164
trigger times: 24
Loss after 7134952 batches: 0.0177
trigger times: 25
Early stopping!
Start to test process.
Loss after 7135911 batches: 0.0172
Time to train on one home:  85.53549671173096
trigger times: 0
Loss after 7136874 batches: 0.0565
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 7137837 batches: 0.0279
trigger times: 2
Loss after 7138800 batches: 0.0270
trigger times: 3
Loss after 7139763 batches: 0.0267
trigger times: 4
Loss after 7140726 batches: 0.0262
trigger times: 5
Loss after 7141689 batches: 0.0253
trigger times: 6
Loss after 7142652 batches: 0.0251
trigger times: 7
Loss after 7143615 batches: 0.0247
trigger times: 8
Loss after 7144578 batches: 0.0246
trigger times: 9
Loss after 7145541 batches: 0.0239
trigger times: 10
Loss after 7146504 batches: 0.0236
trigger times: 11
Loss after 7147467 batches: 0.0234
trigger times: 12
Loss after 7148430 batches: 0.0230
trigger times: 13
Loss after 7149393 batches: 0.0228
trigger times: 14
Loss after 7150356 batches: 0.0227
trigger times: 15
Loss after 7151319 batches: 0.0226
trigger times: 16
Loss after 7152282 batches: 0.0219
trigger times: 17
Loss after 7153245 batches: 0.0217
trigger times: 18
Loss after 7154208 batches: 0.0218
trigger times: 19
Loss after 7155171 batches: 0.0215
trigger times: 20
Loss after 7156134 batches: 0.0214
trigger times: 21
Loss after 7157097 batches: 0.0210
trigger times: 22
Loss after 7158060 batches: 0.0207
trigger times: 23
Loss after 7159023 batches: 0.0206
trigger times: 24
Loss after 7159986 batches: 0.0204
trigger times: 25
Early stopping!
Start to test process.
Loss after 7160949 batches: 0.0204
Time to train on one home:  57.04797148704529
trigger times: 0
Loss after 7161894 batches: 0.0677
trigger times: 0
Loss after 7162839 batches: 0.0502
trigger times: 1
Loss after 7163784 batches: 0.0426
trigger times: 0
Loss after 7164729 batches: 0.0384
trigger times: 0
Loss after 7165674 batches: 0.0358
trigger times: 1
Loss after 7166619 batches: 0.0345
trigger times: 2
Loss after 7167564 batches: 0.0323
trigger times: 3
Loss after 7168509 batches: 0.0301
trigger times: 4
Loss after 7169454 batches: 0.0292
trigger times: 5
Loss after 7170399 batches: 0.0288
trigger times: 0
Loss after 7171344 batches: 0.0275
trigger times: 1
Loss after 7172289 batches: 0.0273
trigger times: 2
Loss after 7173234 batches: 0.0273
trigger times: 3
Loss after 7174179 batches: 0.0270
trigger times: 4
Loss after 7175124 batches: 0.0265
trigger times: 5
Loss after 7176069 batches: 0.0277
trigger times: 6
Loss after 7177014 batches: 0.0264
trigger times: 0
Loss after 7177959 batches: 0.0268
trigger times: 1
Loss after 7178904 batches: 0.0259
trigger times: 2
Loss after 7179849 batches: 0.0256
trigger times: 3
Loss after 7180794 batches: 0.0258
trigger times: 4
Loss after 7181739 batches: 0.0257
trigger times: 5
Loss after 7182684 batches: 0.0254
trigger times: 6
Loss after 7183629 batches: 0.0253
trigger times: 7
Loss after 7184574 batches: 0.0254
trigger times: 8
Loss after 7185519 batches: 0.0242
trigger times: 9
Loss after 7186464 batches: 0.0231
trigger times: 10
Loss after 7187409 batches: 0.0243
trigger times: 11
Loss after 7188354 batches: 0.0242
trigger times: 12
Loss after 7189299 batches: 0.0241
trigger times: 13
Loss after 7190244 batches: 0.0232
trigger times: 14
Loss after 7191189 batches: 0.0257
trigger times: 15
Loss after 7192134 batches: 0.0249
trigger times: 16
Loss after 7193079 batches: 0.0234
trigger times: 17
Loss after 7194024 batches: 0.0218
trigger times: 18
Loss after 7194969 batches: 0.0220
trigger times: 19
Loss after 7195914 batches: 0.0224
trigger times: 20
Loss after 7196859 batches: 0.0223
trigger times: 21
Loss after 7197804 batches: 0.0222
trigger times: 22
Loss after 7198749 batches: 0.0227
trigger times: 23
Loss after 7199694 batches: 0.0224
trigger times: 24
Loss after 7200639 batches: 0.0222
trigger times: 25
Early stopping!
Start to test process.
Loss after 7201584 batches: 0.0224
Time to train on one home:  70.76375079154968
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7202521 batches: 0.0853
trigger times: 1
Loss after 7203458 batches: 0.0764
trigger times: 2
Loss after 7204395 batches: 0.0741
trigger times: 3
Loss after 7205332 batches: 0.0707
trigger times: 4
Loss after 7206269 batches: 0.0681
trigger times: 5
Loss after 7207206 batches: 0.0661
trigger times: 6
Loss after 7208143 batches: 0.0637
trigger times: 7
Loss after 7209080 batches: 0.0623
trigger times: 8
Loss after 7210017 batches: 0.0619
trigger times: 9
Loss after 7210954 batches: 0.0619
trigger times: 10
Loss after 7211891 batches: 0.0610
trigger times: 11
Loss after 7212828 batches: 0.0590
trigger times: 12
Loss after 7213765 batches: 0.0600
trigger times: 13
Loss after 7214702 batches: 0.0594
trigger times: 14
Loss after 7215639 batches: 0.0594
trigger times: 15
Loss after 7216576 batches: 0.0590
trigger times: 16
Loss after 7217513 batches: 0.0585
trigger times: 17
Loss after 7218450 batches: 0.0572
trigger times: 18
Loss after 7219387 batches: 0.0568
trigger times: 19
Loss after 7220324 batches: 0.0572
trigger times: 20
Loss after 7221261 batches: 0.0567
trigger times: 21
Loss after 7222198 batches: 0.0595
trigger times: 22
Loss after 7223135 batches: 0.0592
trigger times: 23
Loss after 7224072 batches: 0.0593
trigger times: 24
Loss after 7225009 batches: 0.0571
trigger times: 25
Early stopping!
Start to test process.
Loss after 7225946 batches: 0.0561
Time to train on one home:  53.155596017837524
trigger times: 0
Loss after 7226909 batches: 0.0329
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 1
Loss after 7227872 batches: 0.0149
trigger times: 2
Loss after 7228835 batches: 0.0140
trigger times: 3
Loss after 7229798 batches: 0.0140
trigger times: 4
Loss after 7230761 batches: 0.0139
trigger times: 5
Loss after 7231724 batches: 0.0138
trigger times: 6
Loss after 7232687 batches: 0.0136
trigger times: 7
Loss after 7233650 batches: 0.0134
trigger times: 8
Loss after 7234613 batches: 0.0132
trigger times: 9
Loss after 7235576 batches: 0.0128
trigger times: 10
Loss after 7236539 batches: 0.0125
trigger times: 11
Loss after 7237502 batches: 0.0119
trigger times: 12
Loss after 7238465 batches: 0.0116
trigger times: 13
Loss after 7239428 batches: 0.0113
trigger times: 14
Loss after 7240391 batches: 0.0109
trigger times: 15
Loss after 7241354 batches: 0.0107
trigger times: 16
Loss after 7242317 batches: 0.0106
trigger times: 17
Loss after 7243280 batches: 0.0105
trigger times: 18
Loss after 7244243 batches: 0.0101
trigger times: 19
Loss after 7245206 batches: 0.0100
trigger times: 20
Loss after 7246169 batches: 0.0100
trigger times: 21
Loss after 7247132 batches: 0.0100
trigger times: 22
Loss after 7248095 batches: 0.0099
trigger times: 23
Loss after 7249058 batches: 0.0100
trigger times: 24
Loss after 7250021 batches: 0.0097
trigger times: 25
Early stopping!
Start to test process.
Loss after 7250984 batches: 0.0094
Time to train on one home:  55.69572639465332
trigger times: 0
Loss after 7251947 batches: 0.0910
trigger times: 1
Loss after 7252910 batches: 0.0794
trigger times: 2
Loss after 7253873 batches: 0.0757
trigger times: 3
Loss after 7254836 batches: 0.0718
trigger times: 4
Loss after 7255799 batches: 0.0688
trigger times: 5
Loss after 7256762 batches: 0.0675
trigger times: 6
Loss after 7257725 batches: 0.0670
trigger times: 7
Loss after 7258688 batches: 0.0666
trigger times: 8
Loss after 7259651 batches: 0.0652
trigger times: 9
Loss after 7260614 batches: 0.0644
trigger times: 10
Loss after 7261577 batches: 0.0645
trigger times: 11
Loss after 7262540 batches: 0.0634
trigger times: 12
Loss after 7263503 batches: 0.0631
trigger times: 13
Loss after 7264466 batches: 0.0637
trigger times: 14
Loss after 7265429 batches: 0.0635
trigger times: 15
Loss after 7266392 batches: 0.0631
trigger times: 16
Loss after 7267355 batches: 0.0619
trigger times: 17
Loss after 7268318 batches: 0.0616
trigger times: 18
Loss after 7269281 batches: 0.0615
trigger times: 19
Loss after 7270244 batches: 0.0612
trigger times: 20
Loss after 7271207 batches: 0.0613
trigger times: 21
Loss after 7272170 batches: 0.0611
trigger times: 22
Loss after 7273133 batches: 0.0615
trigger times: 23
Loss after 7274096 batches: 0.0610
trigger times: 24
Loss after 7275059 batches: 0.0605
trigger times: 25
Early stopping!
Start to test process.
Loss after 7276022 batches: 0.0605
Time to train on one home:  56.78129696846008
trigger times: 0
Loss after 7276985 batches: 0.0754
trigger times: 1
Loss after 7277948 batches: 0.0614
trigger times: 2
Loss after 7278911 batches: 0.0585
trigger times: 3
Loss after 7279874 batches: 0.0535
trigger times: 4
Loss after 7280837 batches: 0.0509
trigger times: 5
Loss after 7281800 batches: 0.0487
trigger times: 6
Loss after 7282763 batches: 0.0466
trigger times: 7
Loss after 7283726 batches: 0.0460
trigger times: 8
Loss after 7284689 batches: 0.0448
trigger times: 9
Loss after 7285652 batches: 0.0442
trigger times: 10
Loss after 7286615 batches: 0.0437
trigger times: 11
Loss after 7287578 batches: 0.0438
trigger times: 12
Loss after 7288541 batches: 0.0434
trigger times: 13
Loss after 7289504 batches: 0.0431
trigger times: 14
Loss after 7290467 batches: 0.0421
trigger times: 15
Loss after 7291430 batches: 0.0418
trigger times: 16
Loss after 7292393 batches: 0.0406
trigger times: 17
Loss after 7293356 batches: 0.0407
trigger times: 18
Loss after 7294319 batches: 0.0401
trigger times: 19
Loss after 7295282 batches: 0.0398
trigger times: 20
Loss after 7296245 batches: 0.0394
trigger times: 21
Loss after 7297208 batches: 0.0384
trigger times: 22
Loss after 7298171 batches: 0.0381
trigger times: 23
Loss after 7299134 batches: 0.0386
trigger times: 24
Loss after 7300097 batches: 0.0370
trigger times: 25
Early stopping!
Start to test process.
Loss after 7301060 batches: 0.0372
Time to train on one home:  55.53353452682495
trigger times: 0
Loss after 7301956 batches: 0.1049
trigger times: 1
Loss after 7302852 batches: 0.0945
trigger times: 2
Loss after 7303748 batches: 0.0896
trigger times: 3
Loss after 7304644 batches: 0.0860
trigger times: 4
Loss after 7305540 batches: 0.0808
trigger times: 5
Loss after 7306436 batches: 0.0795
trigger times: 6
Loss after 7307332 batches: 0.0761
trigger times: 7
Loss after 7308228 batches: 0.0757
trigger times: 8
Loss after 7309124 batches: 0.0731
trigger times: 9
Loss after 7310020 batches: 0.0712
trigger times: 10
Loss after 7310916 batches: 0.0729
trigger times: 11
Loss after 7311812 batches: 0.0699
trigger times: 12
Loss after 7312708 batches: 0.0696
trigger times: 13
Loss after 7313604 batches: 0.0695
trigger times: 14
Loss after 7314500 batches: 0.0705
trigger times: 15
Loss after 7315396 batches: 0.0698
trigger times: 16
Loss after 7316292 batches: 0.0692
trigger times: 17
Loss after 7317188 batches: 0.0673
trigger times: 18
Loss after 7318084 batches: 0.0685
trigger times: 19
Loss after 7318980 batches: 0.0667
trigger times: 20
Loss after 7319876 batches: 0.0688
trigger times: 21
Loss after 7320772 batches: 0.0674
trigger times: 22
Loss after 7321668 batches: 0.0682
trigger times: 23
Loss after 7322564 batches: 0.0684
trigger times: 24
Loss after 7323460 batches: 0.0659
trigger times: 25
Early stopping!
Start to test process.
Loss after 7324356 batches: 0.0648
Time to train on one home:  58.28970956802368
trigger times: 0
Loss after 7325319 batches: 0.1664
trigger times: 1
Loss after 7326282 batches: 0.1125
trigger times: 2
Loss after 7327245 batches: 0.1017
trigger times: 3
Loss after 7328208 batches: 0.0939
trigger times: 4
Loss after 7329171 batches: 0.0873
trigger times: 5
Loss after 7330134 batches: 0.0868
trigger times: 6
Loss after 7331097 batches: 0.0830
trigger times: 7
Loss after 7332060 batches: 0.0835
trigger times: 8
Loss after 7333023 batches: 0.0825
trigger times: 9
Loss after 7333986 batches: 0.0795
trigger times: 10
Loss after 7334949 batches: 0.0795
trigger times: 11
Loss after 7335912 batches: 0.0770
trigger times: 12
Loss after 7336875 batches: 0.0744
trigger times: 13
Loss after 7337838 batches: 0.0724
trigger times: 14
Loss after 7338801 batches: 0.0686
trigger times: 15
Loss after 7339764 batches: 0.0679
trigger times: 16
Loss after 7340727 batches: 0.0646
trigger times: 17
Loss after 7341690 batches: 0.0652
trigger times: 18
Loss after 7342653 batches: 0.0636
trigger times: 19
Loss after 7343616 batches: 0.0633
trigger times: 20
Loss after 7344579 batches: 0.0625
trigger times: 21
Loss after 7345542 batches: 0.0611
trigger times: 22
Loss after 7346505 batches: 0.0592
trigger times: 23
Loss after 7347468 batches: 0.0563
trigger times: 24
Loss after 7348431 batches: 0.0568
trigger times: 25
Early stopping!
Start to test process.
Loss after 7349394 batches: 0.0550
Time to train on one home:  56.317185163497925
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7350357 batches: 0.0850
trigger times: 1
Loss after 7351320 batches: 0.0780
trigger times: 2
Loss after 7352283 batches: 0.0784
trigger times: 3
Loss after 7353246 batches: 0.0761
trigger times: 4
Loss after 7354209 batches: 0.0738
trigger times: 5
Loss after 7355172 batches: 0.0715
trigger times: 6
Loss after 7356135 batches: 0.0698
trigger times: 7
Loss after 7357098 batches: 0.0686
trigger times: 8
Loss after 7358061 batches: 0.0643
trigger times: 9
Loss after 7359024 batches: 0.0654
trigger times: 10
Loss after 7359987 batches: 0.0654
trigger times: 11
Loss after 7360950 batches: 0.0639
trigger times: 12
Loss after 7361913 batches: 0.0649
trigger times: 13
Loss after 7362876 batches: 0.0666
trigger times: 14
Loss after 7363839 batches: 0.0647
trigger times: 15
Loss after 7364802 batches: 0.0630
trigger times: 16
Loss after 7365765 batches: 0.0634
trigger times: 17
Loss after 7366728 batches: 0.0627
trigger times: 18
Loss after 7367691 batches: 0.0608
trigger times: 19
Loss after 7368654 batches: 0.0616
trigger times: 20
Loss after 7369617 batches: 0.0614
trigger times: 21
Loss after 7370580 batches: 0.0618
trigger times: 22
Loss after 7371543 batches: 0.0614
trigger times: 23
Loss after 7372506 batches: 0.0590
trigger times: 24
Loss after 7373469 batches: 0.0613
trigger times: 25
Early stopping!
Start to test process.
Loss after 7374432 batches: 0.0595
Time to train on one home:  56.79197812080383
trigger times: 0
Loss after 7375395 batches: 0.0930
trigger times: 1
Loss after 7376358 batches: 0.0695
trigger times: 2
Loss after 7377321 batches: 0.0661
trigger times: 3
Loss after 7378284 batches: 0.0609
trigger times: 4
Loss after 7379247 batches: 0.0568
trigger times: 5
Loss after 7380210 batches: 0.0543
trigger times: 6
Loss after 7381173 batches: 0.0527
trigger times: 7
Loss after 7382136 batches: 0.0510
trigger times: 8
Loss after 7383099 batches: 0.0500
trigger times: 9
Loss after 7384062 batches: 0.0512
trigger times: 10
Loss after 7385025 batches: 0.0487
trigger times: 11
Loss after 7385988 batches: 0.0492
trigger times: 12
Loss after 7386951 batches: 0.0481
trigger times: 13
Loss after 7387914 batches: 0.0471
trigger times: 14
Loss after 7388877 batches: 0.0481
trigger times: 15
Loss after 7389840 batches: 0.0473
trigger times: 16
Loss after 7390803 batches: 0.0453
trigger times: 17
Loss after 7391766 batches: 0.0461
trigger times: 18
Loss after 7392729 batches: 0.0472
trigger times: 19
Loss after 7393692 batches: 0.0456
trigger times: 20
Loss after 7394655 batches: 0.0455
trigger times: 21
Loss after 7395618 batches: 0.0462
trigger times: 22
Loss after 7396581 batches: 0.0447
trigger times: 23
Loss after 7397544 batches: 0.0445
trigger times: 24
Loss after 7398507 batches: 0.0435
trigger times: 25
Early stopping!
Start to test process.
Loss after 7399470 batches: 0.0437
Time to train on one home:  57.0259051322937
trigger times: 0
Loss after 7400433 batches: 0.0515
trigger times: 1
Loss after 7401396 batches: 0.0449
trigger times: 2
Loss after 7402359 batches: 0.0422
trigger times: 3
Loss after 7403322 batches: 0.0403
trigger times: 4
Loss after 7404285 batches: 0.0382
trigger times: 5
Loss after 7405248 batches: 0.0367
trigger times: 6
Loss after 7406211 batches: 0.0350
trigger times: 7
Loss after 7407174 batches: 0.0338
trigger times: 8
Loss after 7408137 batches: 0.0336
trigger times: 9
Loss after 7409100 batches: 0.0316
trigger times: 10
Loss after 7410063 batches: 0.0317
trigger times: 11
Loss after 7411026 batches: 0.0313
trigger times: 12
Loss after 7411989 batches: 0.0304
trigger times: 13
Loss after 7412952 batches: 0.0303
trigger times: 14
Loss after 7413915 batches: 0.0293
trigger times: 15
Loss after 7414878 batches: 0.0292
trigger times: 16
Loss after 7415841 batches: 0.0294
trigger times: 17
Loss after 7416804 batches: 0.0292
trigger times: 18
Loss after 7417767 batches: 0.0279
trigger times: 19
Loss after 7418730 batches: 0.0278
trigger times: 20
Loss after 7419693 batches: 0.0276
trigger times: 21
Loss after 7420656 batches: 0.0278
trigger times: 22
Loss after 7421619 batches: 0.0271
trigger times: 23
Loss after 7422582 batches: 0.0276
trigger times: 24
Loss after 7423545 batches: 0.0269
trigger times: 25
Early stopping!
Start to test process.
Loss after 7424508 batches: 0.0255
Time to train on one home:  55.384401082992554
trigger times: 0
Loss after 7425471 batches: 0.0740
trigger times: 1
Loss after 7426434 batches: 0.0465
trigger times: 2
Loss after 7427397 batches: 0.0481
trigger times: 3
Loss after 7428360 batches: 0.0466
trigger times: 4
Loss after 7429323 batches: 0.0439
trigger times: 5
Loss after 7430286 batches: 0.0422
trigger times: 6
Loss after 7431249 batches: 0.0416
trigger times: 7
Loss after 7432212 batches: 0.0412
trigger times: 8
Loss after 7433175 batches: 0.0407
trigger times: 9
Loss after 7434138 batches: 0.0399
trigger times: 10
Loss after 7435101 batches: 0.0397
trigger times: 11
Loss after 7436064 batches: 0.0392
trigger times: 12
Loss after 7437027 batches: 0.0383
trigger times: 13
Loss after 7437990 batches: 0.0388
trigger times: 14
Loss after 7438953 batches: 0.0379
trigger times: 15
Loss after 7439916 batches: 0.0381
trigger times: 16
Loss after 7440879 batches: 0.0376
trigger times: 17
Loss after 7441842 batches: 0.0373
trigger times: 18
Loss after 7442805 batches: 0.0371
trigger times: 19
Loss after 7443768 batches: 0.0375
trigger times: 20
Loss after 7444731 batches: 0.0377
trigger times: 21
Loss after 7445694 batches: 0.0372
trigger times: 22
Loss after 7446657 batches: 0.0368
trigger times: 23
Loss after 7447620 batches: 0.0365
trigger times: 24
Loss after 7448583 batches: 0.0368
trigger times: 25
Early stopping!
Start to test process.
Loss after 7449546 batches: 0.0372
Time to train on one home:  53.52790331840515
trigger times: 0
Loss after 7450441 batches: 0.0591
trigger times: 1
Loss after 7451336 batches: 0.0314
trigger times: 2
Loss after 7452231 batches: 0.0151
trigger times: 3
Loss after 7453126 batches: 0.0092
trigger times: 4
Loss after 7454021 batches: 0.0074
trigger times: 5
Loss after 7454916 batches: 0.0068
trigger times: 6
Loss after 7455811 batches: 0.0060
trigger times: 0
Loss after 7456706 batches: 0.0054
trigger times: 0
Loss after 7457601 batches: 0.0047
trigger times: 0
Loss after 7458496 batches: 0.0042
trigger times: 0
Loss after 7459391 batches: 0.0041
trigger times: 0
Loss after 7460286 batches: 0.0043
trigger times: 1
Loss after 7461181 batches: 0.0042
trigger times: 2
Loss after 7462076 batches: 0.0041
trigger times: 3
Loss after 7462971 batches: 0.0045
trigger times: 4
Loss after 7463866 batches: 0.0045
trigger times: 5
Loss after 7464761 batches: 0.0052
trigger times: 6
Loss after 7465656 batches: 0.0043
trigger times: 7
Loss after 7466551 batches: 0.0037
trigger times: 8
Loss after 7467446 batches: 0.0040
trigger times: 9
Loss after 7468341 batches: 0.0044
trigger times: 10
Loss after 7469236 batches: 0.0040
trigger times: 11
Loss after 7470131 batches: 0.0040
trigger times: 12
Loss after 7471026 batches: 0.0041
trigger times: 13
Loss after 7471921 batches: 0.0036
trigger times: 14
Loss after 7472816 batches: 0.0039
trigger times: 15
Loss after 7473711 batches: 0.0035
trigger times: 16
Loss after 7474606 batches: 0.0034
trigger times: 17
Loss after 7475501 batches: 0.0035
trigger times: 18
Loss after 7476396 batches: 0.0032
trigger times: 19
Loss after 7477291 batches: 0.0030
trigger times: 20
Loss after 7478186 batches: 0.0029
trigger times: 0
Loss after 7479081 batches: 0.0029
trigger times: 1
Loss after 7479976 batches: 0.0031
trigger times: 2
Loss after 7480871 batches: 0.0032
trigger times: 3
Loss after 7481766 batches: 0.0059
trigger times: 4
Loss after 7482661 batches: 0.0039
trigger times: 5
Loss after 7483556 batches: 0.0029
trigger times: 0
Loss after 7484451 batches: 0.0028
trigger times: 1
Loss after 7485346 batches: 0.0029
trigger times: 0
Loss after 7486241 batches: 0.0024
trigger times: 1
Loss after 7487136 batches: 0.0027
trigger times: 2
Loss after 7488031 batches: 0.0024
trigger times: 3
Loss after 7488926 batches: 0.0030
trigger times: 4
Loss after 7489821 batches: 0.0026
trigger times: 5
Loss after 7490716 batches: 0.0026
trigger times: 6
Loss after 7491611 batches: 0.0028
trigger times: 7
Loss after 7492506 batches: 0.0022
trigger times: 8
Loss after 7493401 batches: 0.0023
trigger times: 9
Loss after 7494296 batches: 0.0024
trigger times: 10
Loss after 7495191 batches: 0.0022
trigger times: 11
Loss after 7496086 batches: 0.0024
trigger times: 12
Loss after 7496981 batches: 0.0022
trigger times: 13
Loss after 7497876 batches: 0.0024
trigger times: 14
Loss after 7498771 batches: 0.0023
trigger times: 15
Loss after 7499666 batches: 0.0023
trigger times: 16
Loss after 7500561 batches: 0.0029
trigger times: 17
Loss after 7501456 batches: 0.0046
trigger times: 18
Loss after 7502351 batches: 0.0034
trigger times: 19
Loss after 7503246 batches: 0.0034
trigger times: 20
Loss after 7504141 batches: 0.0026
trigger times: 21
Loss after 7505036 batches: 0.0028
trigger times: 22
Loss after 7505931 batches: 0.0031
trigger times: 23
Loss after 7506826 batches: 0.0031
trigger times: 24
Loss after 7507721 batches: 0.0029
trigger times: 25
Early stopping!
Start to test process.
Loss after 7508616 batches: 0.0024
Time to train on one home:  86.02909803390503
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755]]
Round_5_results:  [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 7874 < 7875; dropping {'Training_Loss': 0.06503248055066381, 'Validation_Loss': 0.07453606277704239, 'Training_R2': 0.2491109335259425, 'Validation_R2': 0.0576286104952376, 'Training_F1': 0.4644223937069595, 'Validation_F1': 0.2658760298846118, 'Training_NEP': 0.8513025679781416, 'Validation_NEP': 0.9349110316782818, 'Training_NDE': 0.5529955765786768, 'Validation_NDE': 0.7312782299155014, 'Training_MAE': 24.5134015769835, 'Validation_MAE': 31.14210509699847, 'Training_MSE': 1739.8237, 'Validation_MSE': 3622.3137}.
trigger times: 0
Loss after 7509579 batches: 0.0650
trigger times: 0
Loss after 7510542 batches: 0.0497
trigger times: 0
Loss after 7511505 batches: 0.0475
trigger times: 1
Loss after 7512468 batches: 0.0432
trigger times: 2
Loss after 7513431 batches: 0.0412
trigger times: 3
Loss after 7514394 batches: 0.0395
trigger times: 4
Loss after 7515357 batches: 0.0383
trigger times: 5
Loss after 7516320 batches: 0.0387
trigger times: 6
Loss after 7517283 batches: 0.0373
trigger times: 7
Loss after 7518246 batches: 0.0375
trigger times: 8
Loss after 7519209 batches: 0.0360
trigger times: 9
Loss after 7520172 batches: 0.0369
trigger times: 10
Loss after 7521135 batches: 0.0353
trigger times: 11
Loss after 7522098 batches: 0.0350
trigger times: 12
Loss after 7523061 batches: 0.0355
trigger times: 13
Loss after 7524024 batches: 0.0358
trigger times: 14
Loss after 7524987 batches: 0.0356
trigger times: 15
Loss after 7525950 batches: 0.0353
trigger times: 16
Loss after 7526913 batches: 0.0361
trigger times: 17
Loss after 7527876 batches: 0.0369
trigger times: 18
Loss after 7528839 batches: 0.0360
trigger times: 19
Loss after 7529802 batches: 0.0353
trigger times: 20
Loss after 7530765 batches: 0.0338
trigger times: 21
Loss after 7531728 batches: 0.0338
trigger times: 22
Loss after 7532691 batches: 0.0341
trigger times: 23
Loss after 7533654 batches: 0.0339
trigger times: 24
Loss after 7534617 batches: 0.0336
trigger times: 25
Early stopping!
Start to test process.
Loss after 7535580 batches: 0.0332
Time to train on one home:  57.69147491455078
trigger times: 0
Loss after 7536538 batches: 0.0929
trigger times: 0
Loss after 7537496 batches: 0.0494
trigger times: 1
Loss after 7538454 batches: 0.0458
trigger times: 2
Loss after 7539412 batches: 0.0365
trigger times: 3
Loss after 7540370 batches: 0.0323
trigger times: 4
Loss after 7541328 batches: 0.0303
trigger times: 5
Loss after 7542286 batches: 0.0286
trigger times: 6
Loss after 7543244 batches: 0.0276
trigger times: 7
Loss after 7544202 batches: 0.0254
trigger times: 8
Loss after 7545160 batches: 0.0266
trigger times: 9
Loss after 7546118 batches: 0.0249
trigger times: 10
Loss after 7547076 batches: 0.0251
trigger times: 11
Loss after 7548034 batches: 0.0242
trigger times: 12
Loss after 7548992 batches: 0.0233
trigger times: 13
Loss after 7549950 batches: 0.0238
trigger times: 14
Loss after 7550908 batches: 0.0240
trigger times: 15
Loss after 7551866 batches: 0.0240
trigger times: 16
Loss after 7552824 batches: 0.0221
trigger times: 17
Loss after 7553782 batches: 0.0221
trigger times: 18
Loss after 7554740 batches: 0.0231
trigger times: 19
Loss after 7555698 batches: 0.0217
trigger times: 20
Loss after 7556656 batches: 0.0215
trigger times: 21
Loss after 7557614 batches: 0.0206
trigger times: 22
Loss after 7558572 batches: 0.0211
trigger times: 0
Loss after 7559530 batches: 0.0203
trigger times: 1
Loss after 7560488 batches: 0.0203
trigger times: 2
Loss after 7561446 batches: 0.0201
trigger times: 0
Loss after 7562404 batches: 0.0199
trigger times: 0
Loss after 7563362 batches: 0.0198
trigger times: 1
Loss after 7564320 batches: 0.0200
trigger times: 2
Loss after 7565278 batches: 0.0199
trigger times: 3
Loss after 7566236 batches: 0.0199
trigger times: 4
Loss after 7567194 batches: 0.0186
trigger times: 5
Loss after 7568152 batches: 0.0212
trigger times: 6
Loss after 7569110 batches: 0.0205
trigger times: 0
Loss after 7570068 batches: 0.0199
trigger times: 1
Loss after 7571026 batches: 0.0206
trigger times: 2
Loss after 7571984 batches: 0.0195
trigger times: 3
Loss after 7572942 batches: 0.0198
trigger times: 4
Loss after 7573900 batches: 0.0191
trigger times: 5
Loss after 7574858 batches: 0.0187
trigger times: 6
Loss after 7575816 batches: 0.0183
trigger times: 7
Loss after 7576774 batches: 0.0194
trigger times: 8
Loss after 7577732 batches: 0.0206
trigger times: 9
Loss after 7578690 batches: 0.0203
trigger times: 10
Loss after 7579648 batches: 0.0193
trigger times: 11
Loss after 7580606 batches: 0.0182
trigger times: 12
Loss after 7581564 batches: 0.0178
trigger times: 13
Loss after 7582522 batches: 0.0184
trigger times: 14
Loss after 7583480 batches: 0.0177
trigger times: 15
Loss after 7584438 batches: 0.0181
trigger times: 0
Loss after 7585396 batches: 0.0180
trigger times: 1
Loss after 7586354 batches: 0.0174
trigger times: 2
Loss after 7587312 batches: 0.0175
trigger times: 3
Loss after 7588270 batches: 0.0169
trigger times: 4
Loss after 7589228 batches: 0.0166
trigger times: 5
Loss after 7590186 batches: 0.0198
trigger times: 6
Loss after 7591144 batches: 0.0177
trigger times: 7
Loss after 7592102 batches: 0.0168
trigger times: 8
Loss after 7593060 batches: 0.0180
trigger times: 9
Loss after 7594018 batches: 0.0172
trigger times: 10
Loss after 7594976 batches: 0.0173
trigger times: 11
Loss after 7595934 batches: 0.0201
trigger times: 12
Loss after 7596892 batches: 0.0191
trigger times: 13
Loss after 7597850 batches: 0.0194
trigger times: 14
Loss after 7598808 batches: 0.0193
trigger times: 15
Loss after 7599766 batches: 0.0177
trigger times: 16
Loss after 7600724 batches: 0.0185
trigger times: 17
Loss after 7601682 batches: 0.0183
trigger times: 18
Loss after 7602640 batches: 0.0165
trigger times: 19
Loss after 7603598 batches: 0.0168
trigger times: 20
Loss after 7604556 batches: 0.0176
trigger times: 21
Loss after 7605514 batches: 0.0171
trigger times: 22
Loss after 7606472 batches: 0.0163
trigger times: 23
Loss after 7607430 batches: 0.0166
trigger times: 24
Loss after 7608388 batches: 0.0157
trigger times: 25
Early stopping!
Start to test process.
Loss after 7609346 batches: 0.0152
Time to train on one home:  100.15231037139893
trigger times: 0
Loss after 7610309 batches: 0.0805
trigger times: 1
Loss after 7611272 batches: 0.0690
trigger times: 2
Loss after 7612235 batches: 0.0660
trigger times: 3
Loss after 7613198 batches: 0.0647
trigger times: 4
Loss after 7614161 batches: 0.0625
trigger times: 5
Loss after 7615124 batches: 0.0607
trigger times: 6
Loss after 7616087 batches: 0.0603
trigger times: 7
Loss after 7617050 batches: 0.0586
trigger times: 8
Loss after 7618013 batches: 0.0576
trigger times: 9
Loss after 7618976 batches: 0.0565
trigger times: 10
Loss after 7619939 batches: 0.0557
trigger times: 11
Loss after 7620902 batches: 0.0551
trigger times: 12
Loss after 7621865 batches: 0.0546
trigger times: 13
Loss after 7622828 batches: 0.0538
trigger times: 14
Loss after 7623791 batches: 0.0545
trigger times: 15
Loss after 7624754 batches: 0.0532
trigger times: 16
Loss after 7625717 batches: 0.0518
trigger times: 17
Loss after 7626680 batches: 0.0532
trigger times: 18
Loss after 7627643 batches: 0.0526
trigger times: 19
Loss after 7628606 batches: 0.0526
trigger times: 20
Loss after 7629569 batches: 0.0521
trigger times: 21
Loss after 7630532 batches: 0.0508
trigger times: 22
Loss after 7631495 batches: 0.0506
trigger times: 23
Loss after 7632458 batches: 0.0512
trigger times: 24
Loss after 7633421 batches: 0.0510
trigger times: 25
Early stopping!
Start to test process.
Loss after 7634384 batches: 0.0505
Time to train on one home:  57.003307819366455
trigger times: 0
Loss after 7635347 batches: 0.0868
trigger times: 1
Loss after 7636310 batches: 0.0776
trigger times: 2
Loss after 7637273 batches: 0.0763
trigger times: 3
Loss after 7638236 batches: 0.0751
trigger times: 0
Loss after 7639199 batches: 0.0729
trigger times: 1
Loss after 7640162 batches: 0.0704
trigger times: 2
Loss after 7641125 batches: 0.0686
trigger times: 3
Loss after 7642088 batches: 0.0676
trigger times: 4
Loss after 7643051 batches: 0.0664
trigger times: 5
Loss after 7644014 batches: 0.0653
trigger times: 6
Loss after 7644977 batches: 0.0658
trigger times: 7
Loss after 7645940 batches: 0.0640
trigger times: 8
Loss after 7646903 batches: 0.0639
trigger times: 9
Loss after 7647866 batches: 0.0641
trigger times: 10
Loss after 7648829 batches: 0.0638
trigger times: 11
Loss after 7649792 batches: 0.0642
trigger times: 12
Loss after 7650755 batches: 0.0633
trigger times: 13
Loss after 7651718 batches: 0.0613
trigger times: 14
Loss after 7652681 batches: 0.0626
trigger times: 15
Loss after 7653644 batches: 0.0613
trigger times: 0
Loss after 7654607 batches: 0.0615
trigger times: 1
Loss after 7655570 batches: 0.0615
trigger times: 2
Loss after 7656533 batches: 0.0621
trigger times: 3
Loss after 7657496 batches: 0.0614
trigger times: 4
Loss after 7658459 batches: 0.0612
trigger times: 5
Loss after 7659422 batches: 0.0611
trigger times: 6
Loss after 7660385 batches: 0.0618
trigger times: 7
Loss after 7661348 batches: 0.0596
trigger times: 8
Loss after 7662311 batches: 0.0610
trigger times: 9
Loss after 7663274 batches: 0.0599
trigger times: 10
Loss after 7664237 batches: 0.0590
trigger times: 11
Loss after 7665200 batches: 0.0599
trigger times: 12
Loss after 7666163 batches: 0.0613
trigger times: 13
Loss after 7667126 batches: 0.0598
trigger times: 14
Loss after 7668089 batches: 0.0591
trigger times: 15
Loss after 7669052 batches: 0.0605
trigger times: 16
Loss after 7670015 batches: 0.0603
trigger times: 17
Loss after 7670978 batches: 0.0601
trigger times: 18
Loss after 7671941 batches: 0.0594
trigger times: 19
Loss after 7672904 batches: 0.0590
trigger times: 20
Loss after 7673867 batches: 0.0598
trigger times: 21
Loss after 7674830 batches: 0.0594
trigger times: 22
Loss after 7675793 batches: 0.0575
trigger times: 23
Loss after 7676756 batches: 0.0574
trigger times: 24
Loss after 7677719 batches: 0.0570
trigger times: 25
Early stopping!
Start to test process.
Loss after 7678682 batches: 0.0556
Time to train on one home:  73.69332194328308
trigger times: 0
Loss after 7679645 batches: 0.0293
trigger times: 1
Loss after 7680608 batches: 0.0260
trigger times: 2
Loss after 7681571 batches: 0.0236
trigger times: 3
Loss after 7682534 batches: 0.0217
trigger times: 4
Loss after 7683497 batches: 0.0212
trigger times: 5
Loss after 7684460 batches: 0.0197
trigger times: 6
Loss after 7685423 batches: 0.0192
trigger times: 7
Loss after 7686386 batches: 0.0195
trigger times: 8
Loss after 7687349 batches: 0.0187
trigger times: 9
Loss after 7688312 batches: 0.0177
trigger times: 10
Loss after 7689275 batches: 0.0177
trigger times: 11
Loss after 7690238 batches: 0.0175
trigger times: 12
Loss after 7691201 batches: 0.0176
trigger times: 13
Loss after 7692164 batches: 0.0170
trigger times: 14
Loss after 7693127 batches: 0.0174
trigger times: 15
Loss after 7694090 batches: 0.0170
trigger times: 16
Loss after 7695053 batches: 0.0167
trigger times: 17
Loss after 7696016 batches: 0.0168
trigger times: 18
Loss after 7696979 batches: 0.0166
trigger times: 19
Loss after 7697942 batches: 0.0160
trigger times: 20
Loss after 7698905 batches: 0.0164
trigger times: 21
Loss after 7699868 batches: 0.0161
trigger times: 22
Loss after 7700831 batches: 0.0153
trigger times: 23
Loss after 7701794 batches: 0.0159
trigger times: 24
Loss after 7702757 batches: 0.0155
trigger times: 25
Early stopping!
Start to test process.
Loss after 7703720 batches: 0.0154
Time to train on one home:  54.87958312034607
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7704683 batches: 0.0533
trigger times: 1
Loss after 7705646 batches: 0.0239
trigger times: 2
Loss after 7706609 batches: 0.0203
trigger times: 3
Loss after 7707572 batches: 0.0185
trigger times: 4
Loss after 7708535 batches: 0.0173
trigger times: 5
Loss after 7709498 batches: 0.0163
trigger times: 6
Loss after 7710461 batches: 0.0154
trigger times: 7
Loss after 7711424 batches: 0.0154
trigger times: 8
Loss after 7712387 batches: 0.0145
trigger times: 9
Loss after 7713350 batches: 0.0145
trigger times: 10
Loss after 7714313 batches: 0.0142
trigger times: 11
Loss after 7715276 batches: 0.0140
trigger times: 12
Loss after 7716239 batches: 0.0139
trigger times: 13
Loss after 7717202 batches: 0.0137
trigger times: 14
Loss after 7718165 batches: 0.0137
trigger times: 15
Loss after 7719128 batches: 0.0141
trigger times: 16
Loss after 7720091 batches: 0.0138
trigger times: 17
Loss after 7721054 batches: 0.0136
trigger times: 18
Loss after 7722017 batches: 0.0135
trigger times: 19
Loss after 7722980 batches: 0.0135
trigger times: 20
Loss after 7723943 batches: 0.0132
trigger times: 21
Loss after 7724906 batches: 0.0131
trigger times: 22
Loss after 7725869 batches: 0.0132
trigger times: 23
Loss after 7726832 batches: 0.0131
trigger times: 24
Loss after 7727795 batches: 0.0138
trigger times: 25
Early stopping!
Start to test process.
Loss after 7728758 batches: 0.0131
Time to train on one home:  53.616100549697876
trigger times: 0
Loss after 7729721 batches: 0.1038
trigger times: 0
Loss after 7730684 batches: 0.0942
trigger times: 0
Loss after 7731647 batches: 0.0900
trigger times: 1
Loss after 7732610 batches: 0.0861
trigger times: 2
Loss after 7733573 batches: 0.0839
trigger times: 3
Loss after 7734536 batches: 0.0826
trigger times: 4
Loss after 7735499 batches: 0.0794
trigger times: 5
Loss after 7736462 batches: 0.0803
trigger times: 6
Loss after 7737425 batches: 0.0787
trigger times: 7
Loss after 7738388 batches: 0.0777
trigger times: 8
Loss after 7739351 batches: 0.0785
trigger times: 9
Loss after 7740314 batches: 0.0771
trigger times: 10
Loss after 7741277 batches: 0.0760
trigger times: 11
Loss after 7742240 batches: 0.0766
trigger times: 12
Loss after 7743203 batches: 0.0748
trigger times: 13
Loss after 7744166 batches: 0.0757
trigger times: 14
Loss after 7745129 batches: 0.0753
trigger times: 15
Loss after 7746092 batches: 0.0729
trigger times: 16
Loss after 7747055 batches: 0.0732
trigger times: 17
Loss after 7748018 batches: 0.0724
trigger times: 18
Loss after 7748981 batches: 0.0723
trigger times: 19
Loss after 7749944 batches: 0.0701
trigger times: 20
Loss after 7750907 batches: 0.0709
trigger times: 21
Loss after 7751870 batches: 0.0721
trigger times: 22
Loss after 7752833 batches: 0.0707
trigger times: 23
Loss after 7753796 batches: 0.0712
trigger times: 24
Loss after 7754759 batches: 0.0722
trigger times: 25
Early stopping!
Start to test process.
Loss after 7755722 batches: 0.0702
Time to train on one home:  58.60173964500427
trigger times: 0
Loss after 7756685 batches: 0.0540
trigger times: 0
Loss after 7757648 batches: 0.0435
trigger times: 0
Loss after 7758611 batches: 0.0372
trigger times: 1
Loss after 7759574 batches: 0.0357
trigger times: 2
Loss after 7760537 batches: 0.0331
trigger times: 3
Loss after 7761500 batches: 0.0328
trigger times: 4
Loss after 7762463 batches: 0.0308
trigger times: 5
Loss after 7763426 batches: 0.0301
trigger times: 6
Loss after 7764389 batches: 0.0294
trigger times: 7
Loss after 7765352 batches: 0.0291
trigger times: 8
Loss after 7766315 batches: 0.0284
trigger times: 9
Loss after 7767278 batches: 0.0286
trigger times: 10
Loss after 7768241 batches: 0.0273
trigger times: 11
Loss after 7769204 batches: 0.0264
trigger times: 12
Loss after 7770167 batches: 0.0271
trigger times: 13
Loss after 7771130 batches: 0.0271
trigger times: 14
Loss after 7772093 batches: 0.0273
trigger times: 15
Loss after 7773056 batches: 0.0270
trigger times: 0
Loss after 7774019 batches: 0.0274
trigger times: 1
Loss after 7774982 batches: 0.0260
trigger times: 2
Loss after 7775945 batches: 0.0274
trigger times: 3
Loss after 7776908 batches: 0.0263
trigger times: 4
Loss after 7777871 batches: 0.0257
trigger times: 5
Loss after 7778834 batches: 0.0251
trigger times: 6
Loss after 7779797 batches: 0.0240
trigger times: 7
Loss after 7780760 batches: 0.0238
trigger times: 8
Loss after 7781723 batches: 0.0239
trigger times: 9
Loss after 7782686 batches: 0.0241
trigger times: 10
Loss after 7783649 batches: 0.0244
trigger times: 11
Loss after 7784612 batches: 0.0224
trigger times: 12
Loss after 7785575 batches: 0.0231
trigger times: 13
Loss after 7786538 batches: 0.0236
trigger times: 14
Loss after 7787501 batches: 0.0247
trigger times: 15
Loss after 7788464 batches: 0.0236
trigger times: 16
Loss after 7789427 batches: 0.0238
trigger times: 17
Loss after 7790390 batches: 0.0225
trigger times: 18
Loss after 7791353 batches: 0.0231
trigger times: 19
Loss after 7792316 batches: 0.0228
trigger times: 20
Loss after 7793279 batches: 0.0236
trigger times: 21
Loss after 7794242 batches: 0.0223
trigger times: 22
Loss after 7795205 batches: 0.0233
trigger times: 23
Loss after 7796168 batches: 0.0231
trigger times: 24
Loss after 7797131 batches: 0.0233
trigger times: 25
Early stopping!
Start to test process.
Loss after 7798094 batches: 0.0240
Time to train on one home:  69.64336824417114
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7799057 batches: 0.0821
trigger times: 1
Loss after 7800020 batches: 0.0763
trigger times: 2
Loss after 7800983 batches: 0.0744
trigger times: 3
Loss after 7801946 batches: 0.0712
trigger times: 4
Loss after 7802909 batches: 0.0700
trigger times: 5
Loss after 7803872 batches: 0.0692
trigger times: 6
Loss after 7804835 batches: 0.0672
trigger times: 7
Loss after 7805798 batches: 0.0670
trigger times: 8
Loss after 7806761 batches: 0.0667
trigger times: 9
Loss after 7807724 batches: 0.0646
trigger times: 10
Loss after 7808687 batches: 0.0654
trigger times: 11
Loss after 7809650 batches: 0.0653
trigger times: 12
Loss after 7810613 batches: 0.0634
trigger times: 13
Loss after 7811576 batches: 0.0632
trigger times: 14
Loss after 7812539 batches: 0.0633
trigger times: 15
Loss after 7813502 batches: 0.0625
trigger times: 16
Loss after 7814465 batches: 0.0629
trigger times: 17
Loss after 7815428 batches: 0.0629
trigger times: 18
Loss after 7816391 batches: 0.0621
trigger times: 19
Loss after 7817354 batches: 0.0622
trigger times: 20
Loss after 7818317 batches: 0.0613
trigger times: 21
Loss after 7819280 batches: 0.0616
trigger times: 22
Loss after 7820243 batches: 0.0621
trigger times: 23
Loss after 7821206 batches: 0.0608
trigger times: 24
Loss after 7822169 batches: 0.0601
trigger times: 25
Early stopping!
Start to test process.
Loss after 7823132 batches: 0.0604
Time to train on one home:  60.302438259124756
trigger times: 0
Loss after 7824095 batches: 0.1046
trigger times: 0
Loss after 7825058 batches: 0.0731
trigger times: 0
Loss after 7826021 batches: 0.0673
trigger times: 1
Loss after 7826984 batches: 0.0600
trigger times: 2
Loss after 7827947 batches: 0.0583
trigger times: 3
Loss after 7828910 batches: 0.0544
trigger times: 4
Loss after 7829873 batches: 0.0530
trigger times: 5
Loss after 7830836 batches: 0.0514
trigger times: 6
Loss after 7831799 batches: 0.0502
trigger times: 7
Loss after 7832762 batches: 0.0495
trigger times: 8
Loss after 7833725 batches: 0.0484
trigger times: 9
Loss after 7834688 batches: 0.0487
trigger times: 10
Loss after 7835651 batches: 0.0469
trigger times: 11
Loss after 7836614 batches: 0.0474
trigger times: 12
Loss after 7837577 batches: 0.0461
trigger times: 13
Loss after 7838540 batches: 0.0464
trigger times: 14
Loss after 7839503 batches: 0.0458
trigger times: 15
Loss after 7840466 batches: 0.0454
trigger times: 16
Loss after 7841429 batches: 0.0450
trigger times: 17
Loss after 7842392 batches: 0.0444
trigger times: 18
Loss after 7843355 batches: 0.0455
trigger times: 19
Loss after 7844318 batches: 0.0437
trigger times: 20
Loss after 7845281 batches: 0.0449
trigger times: 21
Loss after 7846244 batches: 0.0438
trigger times: 22
Loss after 7847207 batches: 0.0433
trigger times: 23
Loss after 7848170 batches: 0.0427
trigger times: 24
Loss after 7849133 batches: 0.0420
trigger times: 25
Early stopping!
Start to test process.
Loss after 7850096 batches: 0.0415
Time to train on one home:  58.02618908882141
trigger times: 0
Loss after 7851059 batches: 0.0784
trigger times: 1
Loss after 7852022 batches: 0.0732
trigger times: 2
Loss after 7852985 batches: 0.0711
trigger times: 3
Loss after 7853948 batches: 0.0689
trigger times: 4
Loss after 7854911 batches: 0.0654
trigger times: 5
Loss after 7855874 batches: 0.0646
trigger times: 6
Loss after 7856837 batches: 0.0636
trigger times: 7
Loss after 7857800 batches: 0.0642
trigger times: 8
Loss after 7858763 batches: 0.0633
trigger times: 9
Loss after 7859726 batches: 0.0621
trigger times: 10
Loss after 7860689 batches: 0.0612
trigger times: 11
Loss after 7861652 batches: 0.0621
trigger times: 12
Loss after 7862615 batches: 0.0609
trigger times: 13
Loss after 7863578 batches: 0.0609
trigger times: 14
Loss after 7864541 batches: 0.0588
trigger times: 15
Loss after 7865504 batches: 0.0596
trigger times: 16
Loss after 7866467 batches: 0.0588
trigger times: 17
Loss after 7867430 batches: 0.0576
trigger times: 18
Loss after 7868393 batches: 0.0581
trigger times: 19
Loss after 7869356 batches: 0.0587
trigger times: 20
Loss after 7870319 batches: 0.0588
trigger times: 21
Loss after 7871282 batches: 0.0566
trigger times: 22
Loss after 7872245 batches: 0.0567
trigger times: 23
Loss after 7873208 batches: 0.0570
trigger times: 24
Loss after 7874171 batches: 0.0560
trigger times: 25
Early stopping!
Start to test process.
Loss after 7875134 batches: 0.0571
Time to train on one home:  56.81858944892883
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7876097 batches: 0.0621
trigger times: 1
Loss after 7877060 batches: 0.0525
trigger times: 2
Loss after 7878023 batches: 0.0507
trigger times: 3
Loss after 7878986 batches: 0.0466
trigger times: 4
Loss after 7879949 batches: 0.0420
trigger times: 5
Loss after 7880912 batches: 0.0397
trigger times: 6
Loss after 7881875 batches: 0.0378
trigger times: 7
Loss after 7882838 batches: 0.0366
trigger times: 0
Loss after 7883801 batches: 0.0353
trigger times: 1
Loss after 7884764 batches: 0.0351
trigger times: 2
Loss after 7885727 batches: 0.0337
trigger times: 3
Loss after 7886690 batches: 0.0329
trigger times: 4
Loss after 7887653 batches: 0.0326
trigger times: 5
Loss after 7888616 batches: 0.0317
trigger times: 0
Loss after 7889579 batches: 0.0313
trigger times: 1
Loss after 7890542 batches: 0.0306
trigger times: 2
Loss after 7891505 batches: 0.0309
trigger times: 3
Loss after 7892468 batches: 0.0312
trigger times: 4
Loss after 7893431 batches: 0.0308
trigger times: 5
Loss after 7894394 batches: 0.0302
trigger times: 6
Loss after 7895357 batches: 0.0296
trigger times: 7
Loss after 7896320 batches: 0.0296
trigger times: 8
Loss after 7897283 batches: 0.0297
trigger times: 9
Loss after 7898246 batches: 0.0293
trigger times: 10
Loss after 7899209 batches: 0.0289
trigger times: 11
Loss after 7900172 batches: 0.0288
trigger times: 12
Loss after 7901135 batches: 0.0283
trigger times: 13
Loss after 7902098 batches: 0.0284
trigger times: 14
Loss after 7903061 batches: 0.0275
trigger times: 15
Loss after 7904024 batches: 0.0274
trigger times: 16
Loss after 7904987 batches: 0.0275
trigger times: 17
Loss after 7905950 batches: 0.0273
trigger times: 18
Loss after 7906913 batches: 0.0273
trigger times: 19
Loss after 7907876 batches: 0.0268
trigger times: 20
Loss after 7908839 batches: 0.0261
trigger times: 21
Loss after 7909802 batches: 0.0267
trigger times: 22
Loss after 7910765 batches: 0.0259
trigger times: 23
Loss after 7911728 batches: 0.0263
trigger times: 24
Loss after 7912691 batches: 0.0259
trigger times: 25
Early stopping!
Start to test process.
Loss after 7913654 batches: 0.0267
Time to train on one home:  68.50807976722717
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7914617 batches: 0.0924
trigger times: 0
Loss after 7915580 batches: 0.0580
trigger times: 1
Loss after 7916543 batches: 0.0526
trigger times: 2
Loss after 7917506 batches: 0.0482
trigger times: 0
Loss after 7918469 batches: 0.0435
trigger times: 1
Loss after 7919432 batches: 0.0382
trigger times: 2
Loss after 7920395 batches: 0.0355
trigger times: 3
Loss after 7921358 batches: 0.0349
trigger times: 4
Loss after 7922321 batches: 0.0339
trigger times: 5
Loss after 7923284 batches: 0.0322
trigger times: 6
Loss after 7924247 batches: 0.0307
trigger times: 7
Loss after 7925210 batches: 0.0311
trigger times: 8
Loss after 7926173 batches: 0.0297
trigger times: 9
Loss after 7927136 batches: 0.0295
trigger times: 10
Loss after 7928099 batches: 0.0285
trigger times: 11
Loss after 7929062 batches: 0.0295
trigger times: 12
Loss after 7930025 batches: 0.0270
trigger times: 13
Loss after 7930988 batches: 0.0268
trigger times: 14
Loss after 7931951 batches: 0.0277
trigger times: 15
Loss after 7932914 batches: 0.0283
trigger times: 16
Loss after 7933877 batches: 0.0268
trigger times: 17
Loss after 7934840 batches: 0.0278
trigger times: 18
Loss after 7935803 batches: 0.0257
trigger times: 19
Loss after 7936766 batches: 0.0285
trigger times: 20
Loss after 7937729 batches: 0.0268
trigger times: 21
Loss after 7938692 batches: 0.0257
trigger times: 22
Loss after 7939655 batches: 0.0276
trigger times: 23
Loss after 7940618 batches: 0.0251
trigger times: 24
Loss after 7941581 batches: 0.0253
trigger times: 25
Early stopping!
Start to test process.
Loss after 7942544 batches: 0.0246
Time to train on one home:  57.96529841423035
trigger times: 0
Loss after 7943473 batches: 0.1194
trigger times: 0
Loss after 7944402 batches: 0.0686
trigger times: 0
Loss after 7945331 batches: 0.0528
trigger times: 1
Loss after 7946260 batches: 0.0481
trigger times: 2
Loss after 7947189 batches: 0.0422
trigger times: 0
Loss after 7948118 batches: 0.0399
trigger times: 0
Loss after 7949047 batches: 0.0361
trigger times: 1
Loss after 7949976 batches: 0.0352
trigger times: 2
Loss after 7950905 batches: 0.0358
trigger times: 3
Loss after 7951834 batches: 0.0372
trigger times: 4
Loss after 7952763 batches: 0.0330
trigger times: 0
Loss after 7953692 batches: 0.0335
trigger times: 0
Loss after 7954621 batches: 0.0310
trigger times: 1
Loss after 7955550 batches: 0.0314
trigger times: 2
Loss after 7956479 batches: 0.0320
trigger times: 3
Loss after 7957408 batches: 0.0317
trigger times: 4
Loss after 7958337 batches: 0.0288
trigger times: 5
Loss after 7959266 batches: 0.0314
trigger times: 6
Loss after 7960195 batches: 0.0301
trigger times: 7
Loss after 7961124 batches: 0.0287
trigger times: 0
Loss after 7962053 batches: 0.0286
trigger times: 1
Loss after 7962982 batches: 0.0324
trigger times: 2
Loss after 7963911 batches: 0.0343
trigger times: 3
Loss after 7964840 batches: 0.0367
trigger times: 4
Loss after 7965769 batches: 0.0337
trigger times: 5
Loss after 7966698 batches: 0.0309
trigger times: 6
Loss after 7967627 batches: 0.0339
trigger times: 7
Loss after 7968556 batches: 0.0318
trigger times: 8
Loss after 7969485 batches: 0.0330
trigger times: 0
Loss after 7970414 batches: 0.0334
trigger times: 1
Loss after 7971343 batches: 0.0303
trigger times: 2
Loss after 7972272 batches: 0.0320
trigger times: 3
Loss after 7973201 batches: 0.0270
trigger times: 4
Loss after 7974130 batches: 0.0289
trigger times: 5
Loss after 7975059 batches: 0.0258
trigger times: 6
Loss after 7975988 batches: 0.0271
trigger times: 7
Loss after 7976917 batches: 0.0276
trigger times: 8
Loss after 7977846 batches: 0.0283
trigger times: 9
Loss after 7978775 batches: 0.0271
trigger times: 0
Loss after 7979704 batches: 0.0263
trigger times: 1
Loss after 7980633 batches: 0.0263
trigger times: 2
Loss after 7981562 batches: 0.0277
trigger times: 3
Loss after 7982491 batches: 0.0276
trigger times: 4
Loss after 7983420 batches: 0.0260
trigger times: 5
Loss after 7984349 batches: 0.0252
trigger times: 6
Loss after 7985278 batches: 0.0261
trigger times: 7
Loss after 7986207 batches: 0.0277
trigger times: 8
Loss after 7987136 batches: 0.0258
trigger times: 9
Loss after 7988065 batches: 0.0268
trigger times: 10
Loss after 7988994 batches: 0.0259
trigger times: 11
Loss after 7989923 batches: 0.0266
trigger times: 12
Loss after 7990852 batches: 0.0283
trigger times: 13
Loss after 7991781 batches: 0.0269
trigger times: 14
Loss after 7992710 batches: 0.0262
trigger times: 15
Loss after 7993639 batches: 0.0277
trigger times: 16
Loss after 7994568 batches: 0.0260
trigger times: 17
Loss after 7995497 batches: 0.0250
trigger times: 18
Loss after 7996426 batches: 0.0254
trigger times: 19
Loss after 7997355 batches: 0.0246
trigger times: 20
Loss after 7998284 batches: 0.0255
trigger times: 21
Loss after 7999213 batches: 0.0237
trigger times: 22
Loss after 8000142 batches: 0.0268
trigger times: 23
Loss after 8001071 batches: 0.0263
trigger times: 24
Loss after 8002000 batches: 0.0243
trigger times: 25
Early stopping!
Start to test process.
Loss after 8002929 batches: 0.0238
Time to train on one home:  84.94819235801697
trigger times: 0
Loss after 8003891 batches: 0.0701
trigger times: 1
Loss after 8004853 batches: 0.0658
trigger times: 2
Loss after 8005815 batches: 0.0639
trigger times: 3
Loss after 8006777 batches: 0.0629
trigger times: 4
Loss after 8007739 batches: 0.0612
trigger times: 5
Loss after 8008701 batches: 0.0596
trigger times: 6
Loss after 8009663 batches: 0.0588
trigger times: 7
Loss after 8010625 batches: 0.0583
trigger times: 8
Loss after 8011587 batches: 0.0572
trigger times: 9
Loss after 8012549 batches: 0.0583
trigger times: 10
Loss after 8013511 batches: 0.0568
trigger times: 11
Loss after 8014473 batches: 0.0570
trigger times: 12
Loss after 8015435 batches: 0.0562
trigger times: 13
Loss after 8016397 batches: 0.0553
trigger times: 14
Loss after 8017359 batches: 0.0556
trigger times: 15
Loss after 8018321 batches: 0.0567
trigger times: 16
Loss after 8019283 batches: 0.0547
trigger times: 17
Loss after 8020245 batches: 0.0561
trigger times: 18
Loss after 8021207 batches: 0.0556
trigger times: 19
Loss after 8022169 batches: 0.0558
trigger times: 20
Loss after 8023131 batches: 0.0559
trigger times: 21
Loss after 8024093 batches: 0.0557
trigger times: 22
Loss after 8025055 batches: 0.0558
trigger times: 23
Loss after 8026017 batches: 0.0556
trigger times: 24
Loss after 8026979 batches: 0.0556
trigger times: 25
Early stopping!
Start to test process.
Loss after 8027941 batches: 0.0541
Time to train on one home:  56.21888542175293
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8028904 batches: 0.0570
trigger times: 0
Loss after 8029867 batches: 0.0460
trigger times: 1
Loss after 8030830 batches: 0.0472
trigger times: 0
Loss after 8031793 batches: 0.0443
trigger times: 1
Loss after 8032756 batches: 0.0408
trigger times: 2
Loss after 8033719 batches: 0.0405
trigger times: 3
Loss after 8034682 batches: 0.0395
trigger times: 4
Loss after 8035645 batches: 0.0386
trigger times: 5
Loss after 8036608 batches: 0.0378
trigger times: 6
Loss after 8037571 batches: 0.0376
trigger times: 7
Loss after 8038534 batches: 0.0375
trigger times: 8
Loss after 8039497 batches: 0.0368
trigger times: 9
Loss after 8040460 batches: 0.0362
trigger times: 10
Loss after 8041423 batches: 0.0361
trigger times: 11
Loss after 8042386 batches: 0.0355
trigger times: 12
Loss after 8043349 batches: 0.0355
trigger times: 13
Loss after 8044312 batches: 0.0355
trigger times: 14
Loss after 8045275 batches: 0.0351
trigger times: 15
Loss after 8046238 batches: 0.0359
trigger times: 16
Loss after 8047201 batches: 0.0350
trigger times: 17
Loss after 8048164 batches: 0.0343
trigger times: 18
Loss after 8049127 batches: 0.0342
trigger times: 19
Loss after 8050090 batches: 0.0340
trigger times: 20
Loss after 8051053 batches: 0.0335
trigger times: 21
Loss after 8052016 batches: 0.0337
trigger times: 22
Loss after 8052979 batches: 0.0331
trigger times: 23
Loss after 8053942 batches: 0.0326
trigger times: 24
Loss after 8054905 batches: 0.0322
trigger times: 25
Early stopping!
Start to test process.
Loss after 8055868 batches: 0.0331
Time to train on one home:  58.54623222351074
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8056831 batches: 0.0578
trigger times: 0
Loss after 8057794 batches: 0.0512
trigger times: 0
Loss after 8058757 batches: 0.0490
trigger times: 1
Loss after 8059720 batches: 0.0458
trigger times: 2
Loss after 8060683 batches: 0.0443
trigger times: 3
Loss after 8061646 batches: 0.0432
trigger times: 4
Loss after 8062609 batches: 0.0414
trigger times: 5
Loss after 8063572 batches: 0.0415
trigger times: 6
Loss after 8064535 batches: 0.0408
trigger times: 7
Loss after 8065498 batches: 0.0398
trigger times: 8
Loss after 8066461 batches: 0.0395
trigger times: 9
Loss after 8067424 batches: 0.0394
trigger times: 10
Loss after 8068387 batches: 0.0382
trigger times: 11
Loss after 8069350 batches: 0.0387
trigger times: 12
Loss after 8070313 batches: 0.0382
trigger times: 0
Loss after 8071276 batches: 0.0378
trigger times: 1
Loss after 8072239 batches: 0.0386
trigger times: 2
Loss after 8073202 batches: 0.0386
trigger times: 3
Loss after 8074165 batches: 0.0378
trigger times: 4
Loss after 8075128 batches: 0.0374
trigger times: 5
Loss after 8076091 batches: 0.0359
trigger times: 6
Loss after 8077054 batches: 0.0365
trigger times: 7
Loss after 8078017 batches: 0.0360
trigger times: 8
Loss after 8078980 batches: 0.0353
trigger times: 0
Loss after 8079943 batches: 0.0357
trigger times: 1
Loss after 8080906 batches: 0.0358
trigger times: 2
Loss after 8081869 batches: 0.0345
trigger times: 3
Loss after 8082832 batches: 0.0341
trigger times: 4
Loss after 8083795 batches: 0.0336
trigger times: 5
Loss after 8084758 batches: 0.0343
trigger times: 6
Loss after 8085721 batches: 0.0344
trigger times: 7
Loss after 8086684 batches: 0.0345
trigger times: 8
Loss after 8087647 batches: 0.0334
trigger times: 9
Loss after 8088610 batches: 0.0344
trigger times: 10
Loss after 8089573 batches: 0.0337
trigger times: 11
Loss after 8090536 batches: 0.0330
trigger times: 12
Loss after 8091499 batches: 0.0326
trigger times: 13
Loss after 8092462 batches: 0.0320
trigger times: 14
Loss after 8093425 batches: 0.0310
trigger times: 15
Loss after 8094388 batches: 0.0320
trigger times: 16
Loss after 8095351 batches: 0.0335
trigger times: 17
Loss after 8096314 batches: 0.0321
trigger times: 18
Loss after 8097277 batches: 0.0315
trigger times: 19
Loss after 8098240 batches: 0.0317
trigger times: 20
Loss after 8099203 batches: 0.0313
trigger times: 21
Loss after 8100166 batches: 0.0302
trigger times: 22
Loss after 8101129 batches: 0.0317
trigger times: 23
Loss after 8102092 batches: 0.0313
trigger times: 24
Loss after 8103055 batches: 0.0333
trigger times: 25
Early stopping!
Start to test process.
Loss after 8104018 batches: 0.0347
Time to train on one home:  77.72384643554688
trigger times: 0
Loss after 8104981 batches: 0.1094
trigger times: 0
Loss after 8105944 batches: 0.1001
trigger times: 1
Loss after 8106907 batches: 0.0950
trigger times: 2
Loss after 8107870 batches: 0.0932
trigger times: 3
Loss after 8108833 batches: 0.0903
trigger times: 4
Loss after 8109796 batches: 0.0867
trigger times: 5
Loss after 8110759 batches: 0.0872
trigger times: 6
Loss after 8111722 batches: 0.0889
trigger times: 7
Loss after 8112685 batches: 0.0888
trigger times: 8
Loss after 8113648 batches: 0.0850
trigger times: 9
Loss after 8114611 batches: 0.0834
trigger times: 10
Loss after 8115574 batches: 0.0836
trigger times: 11
Loss after 8116537 batches: 0.0824
trigger times: 12
Loss after 8117500 batches: 0.0827
trigger times: 13
Loss after 8118463 batches: 0.0797
trigger times: 14
Loss after 8119426 batches: 0.0803
trigger times: 15
Loss after 8120389 batches: 0.0821
trigger times: 16
Loss after 8121352 batches: 0.0821
trigger times: 17
Loss after 8122315 batches: 0.0804
trigger times: 18
Loss after 8123278 batches: 0.0807
trigger times: 19
Loss after 8124241 batches: 0.0787
trigger times: 20
Loss after 8125204 batches: 0.0786
trigger times: 21
Loss after 8126167 batches: 0.0800
trigger times: 22
Loss after 8127130 batches: 0.0786
trigger times: 23
Loss after 8128093 batches: 0.0796
trigger times: 24
Loss after 8129056 batches: 0.0770
trigger times: 25
Early stopping!
Start to test process.
Loss after 8130019 batches: 0.0757
Time to train on one home:  57.31023097038269
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8130982 batches: 0.1074
trigger times: 0
Loss after 8131945 batches: 0.0711
trigger times: 1
Loss after 8132908 batches: 0.0691
trigger times: 2
Loss after 8133871 batches: 0.0621
trigger times: 0
Loss after 8134834 batches: 0.0547
trigger times: 1
Loss after 8135797 batches: 0.0498
trigger times: 0
Loss after 8136760 batches: 0.0473
trigger times: 1
Loss after 8137723 batches: 0.0444
trigger times: 2
Loss after 8138686 batches: 0.0443
trigger times: 3
Loss after 8139649 batches: 0.0424
trigger times: 0
Loss after 8140612 batches: 0.0412
trigger times: 1
Loss after 8141575 batches: 0.0407
trigger times: 2
Loss after 8142538 batches: 0.0397
trigger times: 0
Loss after 8143501 batches: 0.0395
trigger times: 1
Loss after 8144464 batches: 0.0389
trigger times: 2
Loss after 8145427 batches: 0.0373
trigger times: 3
Loss after 8146390 batches: 0.0382
trigger times: 4
Loss after 8147353 batches: 0.0383
trigger times: 5
Loss after 8148316 batches: 0.0374
trigger times: 6
Loss after 8149279 batches: 0.0367
trigger times: 7
Loss after 8150242 batches: 0.0361
trigger times: 8
Loss after 8151205 batches: 0.0362
trigger times: 9
Loss after 8152168 batches: 0.0354
trigger times: 10
Loss after 8153131 batches: 0.0361
trigger times: 0
Loss after 8154094 batches: 0.0350
trigger times: 1
Loss after 8155057 batches: 0.0346
trigger times: 2
Loss after 8156020 batches: 0.0341
trigger times: 3
Loss after 8156983 batches: 0.0340
trigger times: 4
Loss after 8157946 batches: 0.0340
trigger times: 5
Loss after 8158909 batches: 0.0339
trigger times: 6
Loss after 8159872 batches: 0.0345
trigger times: 7
Loss after 8160835 batches: 0.0332
trigger times: 8
Loss after 8161798 batches: 0.0327
trigger times: 9
Loss after 8162761 batches: 0.0333
trigger times: 10
Loss after 8163724 batches: 0.0326
trigger times: 11
Loss after 8164687 batches: 0.0315
trigger times: 12
Loss after 8165650 batches: 0.0325
trigger times: 13
Loss after 8166613 batches: 0.0318
trigger times: 14
Loss after 8167576 batches: 0.0309
trigger times: 15
Loss after 8168539 batches: 0.0310
trigger times: 16
Loss after 8169502 batches: 0.0302
trigger times: 17
Loss after 8170465 batches: 0.0318
trigger times: 18
Loss after 8171428 batches: 0.0305
trigger times: 19
Loss after 8172391 batches: 0.0303
trigger times: 20
Loss after 8173354 batches: 0.0299
trigger times: 21
Loss after 8174317 batches: 0.0292
trigger times: 22
Loss after 8175280 batches: 0.0293
trigger times: 23
Loss after 8176243 batches: 0.0294
trigger times: 24
Loss after 8177206 batches: 0.0300
trigger times: 25
Early stopping!
Start to test process.
Loss after 8178169 batches: 0.0290
Time to train on one home:  77.09119701385498
trigger times: 0
Loss after 8179128 batches: 0.1118
trigger times: 1
Loss after 8180087 batches: 0.0579
trigger times: 0
Loss after 8181046 batches: 0.0448
trigger times: 1
Loss after 8182005 batches: 0.0366
trigger times: 0
Loss after 8182964 batches: 0.0323
trigger times: 1
Loss after 8183923 batches: 0.0286
trigger times: 2
Loss after 8184882 batches: 0.0273
trigger times: 3
Loss after 8185841 batches: 0.0255
trigger times: 0
Loss after 8186800 batches: 0.0243
trigger times: 0
Loss after 8187759 batches: 0.0236
trigger times: 0
Loss after 8188718 batches: 0.0227
trigger times: 1
Loss after 8189677 batches: 0.0215
trigger times: 2
Loss after 8190636 batches: 0.0214
trigger times: 3
Loss after 8191595 batches: 0.0210
trigger times: 4
Loss after 8192554 batches: 0.0199
trigger times: 5
Loss after 8193513 batches: 0.0213
trigger times: 6
Loss after 8194472 batches: 0.0213
trigger times: 7
Loss after 8195431 batches: 0.0199
trigger times: 0
Loss after 8196390 batches: 0.0204
trigger times: 1
Loss after 8197349 batches: 0.0202
trigger times: 2
Loss after 8198308 batches: 0.0194
trigger times: 3
Loss after 8199267 batches: 0.0189
trigger times: 4
Loss after 8200226 batches: 0.0195
trigger times: 5
Loss after 8201185 batches: 0.0194
trigger times: 0
Loss after 8202144 batches: 0.0189
trigger times: 1
Loss after 8203103 batches: 0.0190
trigger times: 2
Loss after 8204062 batches: 0.0197
trigger times: 3
Loss after 8205021 batches: 0.0194
trigger times: 0
Loss after 8205980 batches: 0.0193
trigger times: 1
Loss after 8206939 batches: 0.0182
trigger times: 2
Loss after 8207898 batches: 0.0181
trigger times: 3
Loss after 8208857 batches: 0.0178
trigger times: 4
Loss after 8209816 batches: 0.0183
trigger times: 5
Loss after 8210775 batches: 0.0180
trigger times: 6
Loss after 8211734 batches: 0.0169
trigger times: 7
Loss after 8212693 batches: 0.0171
trigger times: 8
Loss after 8213652 batches: 0.0170
trigger times: 9
Loss after 8214611 batches: 0.0176
trigger times: 10
Loss after 8215570 batches: 0.0174
trigger times: 11
Loss after 8216529 batches: 0.0170
trigger times: 12
Loss after 8217488 batches: 0.0168
trigger times: 13
Loss after 8218447 batches: 0.0180
trigger times: 14
Loss after 8219406 batches: 0.0183
trigger times: 15
Loss after 8220365 batches: 0.0168
trigger times: 16
Loss after 8221324 batches: 0.0164
trigger times: 17
Loss after 8222283 batches: 0.0163
trigger times: 18
Loss after 8223242 batches: 0.0159
trigger times: 19
Loss after 8224201 batches: 0.0162
trigger times: 20
Loss after 8225160 batches: 0.0155
trigger times: 21
Loss after 8226119 batches: 0.0158
trigger times: 22
Loss after 8227078 batches: 0.0151
trigger times: 23
Loss after 8228037 batches: 0.0160
trigger times: 24
Loss after 8228996 batches: 0.0150
trigger times: 25
Early stopping!
Start to test process.
Loss after 8229955 batches: 0.0154
Time to train on one home:  77.52324318885803
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8230918 batches: 0.0493
trigger times: 1
Loss after 8231881 batches: 0.0277
trigger times: 2
Loss after 8232844 batches: 0.0268
trigger times: 3
Loss after 8233807 batches: 0.0264
trigger times: 4
Loss after 8234770 batches: 0.0256
trigger times: 5
Loss after 8235733 batches: 0.0247
trigger times: 6
Loss after 8236696 batches: 0.0244
trigger times: 7
Loss after 8237659 batches: 0.0239
trigger times: 8
Loss after 8238622 batches: 0.0234
trigger times: 9
Loss after 8239585 batches: 0.0228
trigger times: 10
Loss after 8240548 batches: 0.0229
trigger times: 11
Loss after 8241511 batches: 0.0225
trigger times: 12
Loss after 8242474 batches: 0.0220
trigger times: 13
Loss after 8243437 batches: 0.0217
trigger times: 14
Loss after 8244400 batches: 0.0216
trigger times: 15
Loss after 8245363 batches: 0.0210
trigger times: 16
Loss after 8246326 batches: 0.0206
trigger times: 17
Loss after 8247289 batches: 0.0207
trigger times: 18
Loss after 8248252 batches: 0.0205
trigger times: 19
Loss after 8249215 batches: 0.0202
trigger times: 20
Loss after 8250178 batches: 0.0200
trigger times: 21
Loss after 8251141 batches: 0.0198
trigger times: 22
Loss after 8252104 batches: 0.0197
trigger times: 23
Loss after 8253067 batches: 0.0195
trigger times: 24
Loss after 8254030 batches: 0.0192
trigger times: 25
Early stopping!
Start to test process.
Loss after 8254993 batches: 0.0191
Time to train on one home:  54.85522818565369
trigger times: 0
Loss after 8255938 batches: 0.0764
trigger times: 0
Loss after 8256883 batches: 0.0538
trigger times: 0
Loss after 8257828 batches: 0.0442
trigger times: 1
Loss after 8258773 batches: 0.0375
trigger times: 2
Loss after 8259718 batches: 0.0343
trigger times: 0
Loss after 8260663 batches: 0.0325
trigger times: 0
Loss after 8261608 batches: 0.0324
trigger times: 1
Loss after 8262553 batches: 0.0301
trigger times: 0
Loss after 8263498 batches: 0.0283
trigger times: 1
Loss after 8264443 batches: 0.0272
trigger times: 2
Loss after 8265388 batches: 0.0285
trigger times: 3
Loss after 8266333 batches: 0.0270
trigger times: 4
Loss after 8267278 batches: 0.0264
trigger times: 5
Loss after 8268223 batches: 0.0263
trigger times: 6
Loss after 8269168 batches: 0.0253
trigger times: 0
Loss after 8270113 batches: 0.0241
trigger times: 1
Loss after 8271058 batches: 0.0253
trigger times: 2
Loss after 8272003 batches: 0.0247
trigger times: 3
Loss after 8272948 batches: 0.0258
trigger times: 4
Loss after 8273893 batches: 0.0266
trigger times: 5
Loss after 8274838 batches: 0.0246
trigger times: 6
Loss after 8275783 batches: 0.0252
trigger times: 7
Loss after 8276728 batches: 0.0241
trigger times: 8
Loss after 8277673 batches: 0.0243
trigger times: 9
Loss after 8278618 batches: 0.0237
trigger times: 10
Loss after 8279563 batches: 0.0245
trigger times: 11
Loss after 8280508 batches: 0.0233
trigger times: 12
Loss after 8281453 batches: 0.0226
trigger times: 13
Loss after 8282398 batches: 0.0220
trigger times: 14
Loss after 8283343 batches: 0.0224
trigger times: 15
Loss after 8284288 batches: 0.0231
trigger times: 16
Loss after 8285233 batches: 0.0214
trigger times: 17
Loss after 8286178 batches: 0.0222
trigger times: 18
Loss after 8287123 batches: 0.0215
trigger times: 19
Loss after 8288068 batches: 0.0218
trigger times: 20
Loss after 8289013 batches: 0.0209
trigger times: 21
Loss after 8289958 batches: 0.0213
trigger times: 22
Loss after 8290903 batches: 0.0219
trigger times: 0
Loss after 8291848 batches: 0.0240
trigger times: 1
Loss after 8292793 batches: 0.0216
trigger times: 2
Loss after 8293738 batches: 0.0209
trigger times: 3
Loss after 8294683 batches: 0.0211
trigger times: 4
Loss after 8295628 batches: 0.0213
trigger times: 5
Loss after 8296573 batches: 0.0210
trigger times: 6
Loss after 8297518 batches: 0.0200
trigger times: 7
Loss after 8298463 batches: 0.0209
trigger times: 8
Loss after 8299408 batches: 0.0206
trigger times: 9
Loss after 8300353 batches: 0.0208
trigger times: 10
Loss after 8301298 batches: 0.0191
trigger times: 11
Loss after 8302243 batches: 0.0197
trigger times: 12
Loss after 8303188 batches: 0.0198
trigger times: 13
Loss after 8304133 batches: 0.0198
trigger times: 14
Loss after 8305078 batches: 0.0220
trigger times: 15
Loss after 8306023 batches: 0.0209
trigger times: 16
Loss after 8306968 batches: 0.0203
trigger times: 17
Loss after 8307913 batches: 0.0199
trigger times: 18
Loss after 8308858 batches: 0.0193
trigger times: 19
Loss after 8309803 batches: 0.0200
trigger times: 20
Loss after 8310748 batches: 0.0201
trigger times: 21
Loss after 8311693 batches: 0.0203
trigger times: 22
Loss after 8312638 batches: 0.0192
trigger times: 23
Loss after 8313583 batches: 0.0205
trigger times: 0
Loss after 8314528 batches: 0.0218
trigger times: 1
Loss after 8315473 batches: 0.0203
trigger times: 2
Loss after 8316418 batches: 0.0198
trigger times: 3
Loss after 8317363 batches: 0.0191
trigger times: 4
Loss after 8318308 batches: 0.0185
trigger times: 5
Loss after 8319253 batches: 0.0172
trigger times: 6
Loss after 8320198 batches: 0.0177
trigger times: 7
Loss after 8321143 batches: 0.0166
trigger times: 8
Loss after 8322088 batches: 0.0177
trigger times: 9
Loss after 8323033 batches: 0.0172
trigger times: 10
Loss after 8323978 batches: 0.0180
trigger times: 11
Loss after 8324923 batches: 0.0182
trigger times: 12
Loss after 8325868 batches: 0.0189
trigger times: 13
Loss after 8326813 batches: 0.0194
trigger times: 14
Loss after 8327758 batches: 0.0180
trigger times: 15
Loss after 8328703 batches: 0.0184
trigger times: 16
Loss after 8329648 batches: 0.0170
trigger times: 17
Loss after 8330593 batches: 0.0186
trigger times: 18
Loss after 8331538 batches: 0.0171
trigger times: 19
Loss after 8332483 batches: 0.0178
trigger times: 20
Loss after 8333428 batches: 0.0178
trigger times: 0
Loss after 8334373 batches: 0.0173
trigger times: 1
Loss after 8335318 batches: 0.0182
trigger times: 2
Loss after 8336263 batches: 0.0171
trigger times: 3
Loss after 8337208 batches: 0.0179
trigger times: 4
Loss after 8338153 batches: 0.0174
trigger times: 5
Loss after 8339098 batches: 0.0165
trigger times: 6
Loss after 8340043 batches: 0.0165
trigger times: 7
Loss after 8340988 batches: 0.0153
trigger times: 8
Loss after 8341933 batches: 0.0155
trigger times: 9
Loss after 8342878 batches: 0.0158
trigger times: 10
Loss after 8343823 batches: 0.0159
trigger times: 11
Loss after 8344768 batches: 0.0162
trigger times: 12
Loss after 8345713 batches: 0.0157
trigger times: 13
Loss after 8346658 batches: 0.0152
trigger times: 14
Loss after 8347603 batches: 0.0153
trigger times: 15
Loss after 8348548 batches: 0.0153
trigger times: 16
Loss after 8349493 batches: 0.0161
trigger times: 17
Loss after 8350438 batches: 0.0152
trigger times: 18
Loss after 8351383 batches: 0.0142
trigger times: 19
Loss after 8352328 batches: 0.0145
trigger times: 20
Loss after 8353273 batches: 0.0147
trigger times: 21
Loss after 8354218 batches: 0.0151
trigger times: 22
Loss after 8355163 batches: 0.0156
trigger times: 23
Loss after 8356108 batches: 0.0147
trigger times: 24
Loss after 8357053 batches: 0.0154
trigger times: 25
Early stopping!
Start to test process.
Loss after 8357998 batches: 0.0145
Time to train on one home:  124.40503478050232
trigger times: 0
Loss after 8358935 batches: 0.0869
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 8359872 batches: 0.0751
trigger times: 2
Loss after 8360809 batches: 0.0726
trigger times: 3
Loss after 8361746 batches: 0.0699
trigger times: 4
Loss after 8362683 batches: 0.0656
trigger times: 5
Loss after 8363620 batches: 0.0634
trigger times: 6
Loss after 8364557 batches: 0.0628
trigger times: 7
Loss after 8365494 batches: 0.0621
trigger times: 8
Loss after 8366431 batches: 0.0613
trigger times: 9
Loss after 8367368 batches: 0.0599
trigger times: 10
Loss after 8368305 batches: 0.0588
trigger times: 11
Loss after 8369242 batches: 0.0587
trigger times: 12
Loss after 8370179 batches: 0.0578
trigger times: 13
Loss after 8371116 batches: 0.0576
trigger times: 14
Loss after 8372053 batches: 0.0567
trigger times: 15
Loss after 8372990 batches: 0.0568
trigger times: 16
Loss after 8373927 batches: 0.0564
trigger times: 17
Loss after 8374864 batches: 0.0568
trigger times: 18
Loss after 8375801 batches: 0.0559
trigger times: 19
Loss after 8376738 batches: 0.0561
trigger times: 20
Loss after 8377675 batches: 0.0559
trigger times: 21
Loss after 8378612 batches: 0.0557
trigger times: 22
Loss after 8379549 batches: 0.0551
trigger times: 23
Loss after 8380486 batches: 0.0537
trigger times: 24
Loss after 8381423 batches: 0.0539
trigger times: 25
Early stopping!
Start to test process.
Loss after 8382360 batches: 0.0540
Time to train on one home:  59.41592741012573
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 8383323 batches: 0.0243
trigger times: 1
Loss after 8384286 batches: 0.0146
trigger times: 2
Loss after 8385249 batches: 0.0136
trigger times: 3
Loss after 8386212 batches: 0.0137
trigger times: 4
Loss after 8387175 batches: 0.0134
trigger times: 5
Loss after 8388138 batches: 0.0132
trigger times: 6
Loss after 8389101 batches: 0.0125
trigger times: 7
Loss after 8390064 batches: 0.0120
trigger times: 8
Loss after 8391027 batches: 0.0114
trigger times: 9
Loss after 8391990 batches: 0.0109
trigger times: 10
Loss after 8392953 batches: 0.0104
trigger times: 11
Loss after 8393916 batches: 0.0102
trigger times: 12
Loss after 8394879 batches: 0.0103
trigger times: 13
Loss after 8395842 batches: 0.0099
trigger times: 14
Loss after 8396805 batches: 0.0098
trigger times: 15
Loss after 8397768 batches: 0.0097
trigger times: 16
Loss after 8398731 batches: 0.0095
trigger times: 17
Loss after 8399694 batches: 0.0093
trigger times: 18
Loss after 8400657 batches: 0.0091
trigger times: 19
Loss after 8401620 batches: 0.0091
trigger times: 20
Loss after 8402583 batches: 0.0092
trigger times: 21
Loss after 8403546 batches: 0.0091
trigger times: 22
Loss after 8404509 batches: 0.0095
trigger times: 23
Loss after 8405472 batches: 0.0095
trigger times: 24
Loss after 8406435 batches: 0.0091
trigger times: 25
Early stopping!
Start to test process.
Loss after 8407398 batches: 0.0090
Time to train on one home:  56.493870973587036
trigger times: 0
Loss after 8408361 batches: 0.0894
trigger times: 1
Loss after 8409324 batches: 0.0771
trigger times: 2
Loss after 8410287 batches: 0.0740
trigger times: 3
Loss after 8411250 batches: 0.0703
trigger times: 4
Loss after 8412213 batches: 0.0675
trigger times: 5
Loss after 8413176 batches: 0.0659
trigger times: 6
Loss after 8414139 batches: 0.0641
trigger times: 7
Loss after 8415102 batches: 0.0646
trigger times: 8
Loss after 8416065 batches: 0.0633
trigger times: 9
Loss after 8417028 batches: 0.0633
trigger times: 10
Loss after 8417991 batches: 0.0623
trigger times: 11
Loss after 8418954 batches: 0.0626
trigger times: 12
Loss after 8419917 batches: 0.0619
trigger times: 13
Loss after 8420880 batches: 0.0614
trigger times: 14
Loss after 8421843 batches: 0.0617
trigger times: 15
Loss after 8422806 batches: 0.0618
trigger times: 16
Loss after 8423769 batches: 0.0614
trigger times: 17
Loss after 8424732 batches: 0.0605
trigger times: 18
Loss after 8425695 batches: 0.0606
trigger times: 19
Loss after 8426658 batches: 0.0600
trigger times: 20
Loss after 8427621 batches: 0.0605
trigger times: 21
Loss after 8428584 batches: 0.0601
trigger times: 22
Loss after 8429547 batches: 0.0602
trigger times: 23
Loss after 8430510 batches: 0.0598
trigger times: 24
Loss after 8431473 batches: 0.0592
trigger times: 25
Early stopping!
Start to test process.
Loss after 8432436 batches: 0.0589
Time to train on one home:  57.100664377212524
trigger times: 0
Loss after 8433399 batches: 0.0683
trigger times: 1
Loss after 8434362 batches: 0.0564
trigger times: 2
Loss after 8435325 batches: 0.0526
trigger times: 3
Loss after 8436288 batches: 0.0495
trigger times: 4
Loss after 8437251 batches: 0.0475
trigger times: 5
Loss after 8438214 batches: 0.0458
trigger times: 6
Loss after 8439177 batches: 0.0449
trigger times: 7
Loss after 8440140 batches: 0.0440
trigger times: 8
Loss after 8441103 batches: 0.0426
trigger times: 9
Loss after 8442066 batches: 0.0418
trigger times: 10
Loss after 8443029 batches: 0.0417
trigger times: 11
Loss after 8443992 batches: 0.0409
trigger times: 12
Loss after 8444955 batches: 0.0402
trigger times: 13
Loss after 8445918 batches: 0.0396
trigger times: 14
Loss after 8446881 batches: 0.0389
trigger times: 15
Loss after 8447844 batches: 0.0396
trigger times: 16
Loss after 8448807 batches: 0.0390
trigger times: 17
Loss after 8449770 batches: 0.0380
trigger times: 18
Loss after 8450733 batches: 0.0379
trigger times: 19
Loss after 8451696 batches: 0.0376
trigger times: 20
Loss after 8452659 batches: 0.0374
trigger times: 21
Loss after 8453622 batches: 0.0368
trigger times: 22
Loss after 8454585 batches: 0.0364
trigger times: 23
Loss after 8455548 batches: 0.0380
trigger times: 24
Loss after 8456511 batches: 0.0375
trigger times: 25
Early stopping!
Start to test process.
Loss after 8457474 batches: 0.0361
Time to train on one home:  57.21260857582092
trigger times: 0
Loss after 8458370 batches: 0.1032
trigger times: 1
Loss after 8459266 batches: 0.0934
trigger times: 2
Loss after 8460162 batches: 0.0889
trigger times: 3
Loss after 8461058 batches: 0.0851
trigger times: 4
Loss after 8461954 batches: 0.0812
trigger times: 5
Loss after 8462850 batches: 0.0772
trigger times: 6
Loss after 8463746 batches: 0.0752
trigger times: 7
Loss after 8464642 batches: 0.0725
trigger times: 8
Loss after 8465538 batches: 0.0715
trigger times: 9
Loss after 8466434 batches: 0.0705
trigger times: 10
Loss after 8467330 batches: 0.0701
trigger times: 11
Loss after 8468226 batches: 0.0674
trigger times: 12
Loss after 8469122 batches: 0.0683
trigger times: 13
Loss after 8470018 batches: 0.0655
trigger times: 14
Loss after 8470914 batches: 0.0671
trigger times: 15
Loss after 8471810 batches: 0.0649
trigger times: 16
Loss after 8472706 batches: 0.0660
trigger times: 17
Loss after 8473602 batches: 0.0650
trigger times: 18
Loss after 8474498 batches: 0.0653
trigger times: 19
Loss after 8475394 batches: 0.0651
trigger times: 20
Loss after 8476290 batches: 0.0623
trigger times: 21
Loss after 8477186 batches: 0.0610
trigger times: 22
Loss after 8478082 batches: 0.0607
trigger times: 23
Loss after 8478978 batches: 0.0629
trigger times: 24
Loss after 8479874 batches: 0.0632
trigger times: 25
Early stopping!
Start to test process.
Loss after 8480770 batches: 0.0653
Time to train on one home:  54.19156861305237
trigger times: 0
Loss after 8481733 batches: 0.1565
trigger times: 1
Loss after 8482696 batches: 0.1146
trigger times: 2
Loss after 8483659 batches: 0.1025
trigger times: 3
Loss after 8484622 batches: 0.0944
trigger times: 4
Loss after 8485585 batches: 0.0873
trigger times: 5
Loss after 8486548 batches: 0.0842
trigger times: 6
Loss after 8487511 batches: 0.0815
trigger times: 7
Loss after 8488474 batches: 0.0796
trigger times: 8
Loss after 8489437 batches: 0.0757
trigger times: 9
Loss after 8490400 batches: 0.0753
trigger times: 10
Loss after 8491363 batches: 0.0730
trigger times: 11
Loss after 8492326 batches: 0.0698
trigger times: 12
Loss after 8493289 batches: 0.0693
trigger times: 13
Loss after 8494252 batches: 0.0666
trigger times: 14
Loss after 8495215 batches: 0.0649
trigger times: 15
Loss after 8496178 batches: 0.0636
trigger times: 16
Loss after 8497141 batches: 0.0608
trigger times: 17
Loss after 8498104 batches: 0.0606
trigger times: 18
Loss after 8499067 batches: 0.0587
trigger times: 19
Loss after 8500030 batches: 0.0555
trigger times: 20
Loss after 8500993 batches: 0.0543
trigger times: 21
Loss after 8501956 batches: 0.0553
trigger times: 22
Loss after 8502919 batches: 0.0529
trigger times: 23
Loss after 8503882 batches: 0.0533
trigger times: 24
Loss after 8504845 batches: 0.0529
trigger times: 25
Early stopping!
Start to test process.
Loss after 8505808 batches: 0.0518
Time to train on one home:  53.29188275337219
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8506771 batches: 0.0833
trigger times: 1
Loss after 8507734 batches: 0.0773
trigger times: 2
Loss after 8508697 batches: 0.0756
trigger times: 3
Loss after 8509660 batches: 0.0740
trigger times: 4
Loss after 8510623 batches: 0.0721
trigger times: 5
Loss after 8511586 batches: 0.0702
trigger times: 6
Loss after 8512549 batches: 0.0695
trigger times: 7
Loss after 8513512 batches: 0.0665
trigger times: 8
Loss after 8514475 batches: 0.0651
trigger times: 9
Loss after 8515438 batches: 0.0639
trigger times: 10
Loss after 8516401 batches: 0.0641
trigger times: 11
Loss after 8517364 batches: 0.0610
trigger times: 12
Loss after 8518327 batches: 0.0612
trigger times: 13
Loss after 8519290 batches: 0.0599
trigger times: 14
Loss after 8520253 batches: 0.0596
trigger times: 15
Loss after 8521216 batches: 0.0594
trigger times: 16
Loss after 8522179 batches: 0.0585
trigger times: 17
Loss after 8523142 batches: 0.0585
trigger times: 18
Loss after 8524105 batches: 0.0583
trigger times: 19
Loss after 8525068 batches: 0.0572
trigger times: 20
Loss after 8526031 batches: 0.0586
trigger times: 21
Loss after 8526994 batches: 0.0563
trigger times: 22
Loss after 8527957 batches: 0.0587
trigger times: 23
Loss after 8528920 batches: 0.0605
trigger times: 24
Loss after 8529883 batches: 0.0612
trigger times: 25
Early stopping!
Start to test process.
Loss after 8530846 batches: 0.0581
Time to train on one home:  56.84674286842346
trigger times: 0
Loss after 8531809 batches: 0.0957
trigger times: 1
Loss after 8532772 batches: 0.0696
trigger times: 2
Loss after 8533735 batches: 0.0650
trigger times: 3
Loss after 8534698 batches: 0.0606
trigger times: 4
Loss after 8535661 batches: 0.0559
trigger times: 5
Loss after 8536624 batches: 0.0535
trigger times: 6
Loss after 8537587 batches: 0.0520
trigger times: 7
Loss after 8538550 batches: 0.0491
trigger times: 8
Loss after 8539513 batches: 0.0478
trigger times: 9
Loss after 8540476 batches: 0.0479
trigger times: 10
Loss after 8541439 batches: 0.0473
trigger times: 11
Loss after 8542402 batches: 0.0465
trigger times: 12
Loss after 8543365 batches: 0.0464
trigger times: 13
Loss after 8544328 batches: 0.0464
trigger times: 14
Loss after 8545291 batches: 0.0460
trigger times: 15
Loss after 8546254 batches: 0.0457
trigger times: 16
Loss after 8547217 batches: 0.0460
trigger times: 17
Loss after 8548180 batches: 0.0464
trigger times: 18
Loss after 8549143 batches: 0.0452
trigger times: 19
Loss after 8550106 batches: 0.0443
trigger times: 20
Loss after 8551069 batches: 0.0429
trigger times: 21
Loss after 8552032 batches: 0.0442
trigger times: 22
Loss after 8552995 batches: 0.0433
trigger times: 23
Loss after 8553958 batches: 0.0422
trigger times: 24
Loss after 8554921 batches: 0.0427
trigger times: 25
Early stopping!
Start to test process.
Loss after 8555884 batches: 0.0419
Time to train on one home:  55.36622953414917
trigger times: 0
Loss after 8556847 batches: 0.0530
trigger times: 1
Loss after 8557810 batches: 0.0428
trigger times: 2
Loss after 8558773 batches: 0.0417
trigger times: 3
Loss after 8559736 batches: 0.0384
trigger times: 4
Loss after 8560699 batches: 0.0356
trigger times: 5
Loss after 8561662 batches: 0.0339
trigger times: 6
Loss after 8562625 batches: 0.0327
trigger times: 7
Loss after 8563588 batches: 0.0316
trigger times: 8
Loss after 8564551 batches: 0.0308
trigger times: 9
Loss after 8565514 batches: 0.0298
trigger times: 10
Loss after 8566477 batches: 0.0294
trigger times: 11
Loss after 8567440 batches: 0.0291
trigger times: 12
Loss after 8568403 batches: 0.0289
trigger times: 13
Loss after 8569366 batches: 0.0284
trigger times: 14
Loss after 8570329 batches: 0.0269
trigger times: 15
Loss after 8571292 batches: 0.0267
trigger times: 16
Loss after 8572255 batches: 0.0275
trigger times: 17
Loss after 8573218 batches: 0.0269
trigger times: 18
Loss after 8574181 batches: 0.0271
trigger times: 19
Loss after 8575144 batches: 0.0268
trigger times: 20
Loss after 8576107 batches: 0.0257
trigger times: 21
Loss after 8577070 batches: 0.0257
trigger times: 22
Loss after 8578033 batches: 0.0259
trigger times: 23
Loss after 8578996 batches: 0.0249
trigger times: 24
Loss after 8579959 batches: 0.0256
trigger times: 25
Early stopping!
Start to test process.
Loss after 8580922 batches: 0.0272
Time to train on one home:  58.99417185783386
trigger times: 0
Loss after 8581885 batches: 0.0629
trigger times: 1
Loss after 8582848 batches: 0.0458
trigger times: 2
Loss after 8583811 batches: 0.0465
trigger times: 3
Loss after 8584774 batches: 0.0446
trigger times: 4
Loss after 8585737 batches: 0.0419
trigger times: 5
Loss after 8586700 batches: 0.0413
trigger times: 6
Loss after 8587663 batches: 0.0403
trigger times: 7
Loss after 8588626 batches: 0.0390
trigger times: 8
Loss after 8589589 batches: 0.0394
trigger times: 9
Loss after 8590552 batches: 0.0394
trigger times: 10
Loss after 8591515 batches: 0.0383
trigger times: 11
Loss after 8592478 batches: 0.0380
trigger times: 12
Loss after 8593441 batches: 0.0377
trigger times: 13
Loss after 8594404 batches: 0.0376
trigger times: 14
Loss after 8595367 batches: 0.0374
trigger times: 15
Loss after 8596330 batches: 0.0379
trigger times: 16
Loss after 8597293 batches: 0.0371
trigger times: 17
Loss after 8598256 batches: 0.0369
trigger times: 18
Loss after 8599219 batches: 0.0363
trigger times: 19
Loss after 8600182 batches: 0.0365
trigger times: 20
Loss after 8601145 batches: 0.0364
trigger times: 21
Loss after 8602108 batches: 0.0360
trigger times: 22
Loss after 8603071 batches: 0.0364
trigger times: 23
Loss after 8604034 batches: 0.0371
trigger times: 24
Loss after 8604997 batches: 0.0367
trigger times: 25
Early stopping!
Start to test process.
Loss after 8605960 batches: 0.0365
Time to train on one home:  56.513951539993286
trigger times: 0
Loss after 8606855 batches: 0.0764
trigger times: 1
Loss after 8607750 batches: 0.0414
trigger times: 0
Loss after 8608645 batches: 0.0189
trigger times: 1
Loss after 8609540 batches: 0.0122
trigger times: 0
Loss after 8610435 batches: 0.0079
trigger times: 1
Loss after 8611330 batches: 0.0069
trigger times: 2
Loss after 8612225 batches: 0.0057
trigger times: 3
Loss after 8613120 batches: 0.0054
trigger times: 4
Loss after 8614015 batches: 0.0051
trigger times: 5
Loss after 8614910 batches: 0.0062
trigger times: 6
Loss after 8615805 batches: 0.0072
trigger times: 7
Loss after 8616700 batches: 0.0064
trigger times: 8
Loss after 8617595 batches: 0.0056
trigger times: 9
Loss after 8618490 batches: 0.0052
trigger times: 10
Loss after 8619385 batches: 0.0059
trigger times: 11
Loss after 8620280 batches: 0.0051
trigger times: 0
Loss after 8621175 batches: 0.0042
trigger times: 0
Loss after 8622070 batches: 0.0039
trigger times: 1
Loss after 8622965 batches: 0.0038
trigger times: 0
Loss after 8623860 batches: 0.0035
trigger times: 0
Loss after 8624755 batches: 0.0034
trigger times: 1
Loss after 8625650 batches: 0.0035
trigger times: 2
Loss after 8626545 batches: 0.0036
trigger times: 3
Loss after 8627440 batches: 0.0031
trigger times: 4
Loss after 8628335 batches: 0.0031
trigger times: 5
Loss after 8629230 batches: 0.0027
trigger times: 6
Loss after 8630125 batches: 0.0030
trigger times: 7
Loss after 8631020 batches: 0.0029
trigger times: 8
Loss after 8631915 batches: 0.0030
trigger times: 9
Loss after 8632810 batches: 0.0026
trigger times: 10
Loss after 8633705 batches: 0.0030
trigger times: 0
Loss after 8634600 batches: 0.0028
trigger times: 1
Loss after 8635495 batches: 0.0027
trigger times: 2
Loss after 8636390 batches: 0.0027
trigger times: 3
Loss after 8637285 batches: 0.0028
trigger times: 4
Loss after 8638180 batches: 0.0030
trigger times: 5
Loss after 8639075 batches: 0.0025
trigger times: 0
Loss after 8639970 batches: 0.0027
trigger times: 1
Loss after 8640865 batches: 0.0025
trigger times: 2
Loss after 8641760 batches: 0.0026
trigger times: 3
Loss after 8642655 batches: 0.0025
trigger times: 4
Loss after 8643550 batches: 0.0041
trigger times: 5
Loss after 8644445 batches: 0.0041
trigger times: 0
Loss after 8645340 batches: 0.0038
trigger times: 1
Loss after 8646235 batches: 0.0031
trigger times: 2
Loss after 8647130 batches: 0.0031
trigger times: 3
Loss after 8648025 batches: 0.0034
trigger times: 4
Loss after 8648920 batches: 0.0032
trigger times: 5
Loss after 8649815 batches: 0.0035
trigger times: 6
Loss after 8650710 batches: 0.0033
trigger times: 7
Loss after 8651605 batches: 0.0034
trigger times: 8
Loss after 8652500 batches: 0.0030
trigger times: 9
Loss after 8653395 batches: 0.0049
trigger times: 10
Loss after 8654290 batches: 0.0045
trigger times: 11
Loss after 8655185 batches: 0.0035
trigger times: 12
Loss after 8656080 batches: 0.0035
trigger times: 13
Loss after 8656975 batches: 0.0031
trigger times: 14
Loss after 8657870 batches: 0.0032
trigger times: 0
Loss after 8658765 batches: 0.0044
trigger times: 1
Loss after 8659660 batches: 0.0037
trigger times: 2
Loss after 8660555 batches: 0.0030
trigger times: 3
Loss after 8661450 batches: 0.0028
trigger times: 4
Loss after 8662345 batches: 0.0025
trigger times: 5
Loss after 8663240 batches: 0.0036
trigger times: 6
Loss after 8664135 batches: 0.0029
trigger times: 7
Loss after 8665030 batches: 0.0034
trigger times: 8
Loss after 8665925 batches: 0.0025
trigger times: 9
Loss after 8666820 batches: 0.0027
trigger times: 10
Loss after 8667715 batches: 0.0024
trigger times: 11
Loss after 8668610 batches: 0.0025
trigger times: 12
Loss after 8669505 batches: 0.0024
trigger times: 13
Loss after 8670400 batches: 0.0021
trigger times: 14
Loss after 8671295 batches: 0.0021
trigger times: 15
Loss after 8672190 batches: 0.0021
trigger times: 16
Loss after 8673085 batches: 0.0023
trigger times: 17
Loss after 8673980 batches: 0.0024
trigger times: 18
Loss after 8674875 batches: 0.0023
trigger times: 19
Loss after 8675770 batches: 0.0024
trigger times: 20
Loss after 8676665 batches: 0.0023
trigger times: 21
Loss after 8677560 batches: 0.0024
trigger times: 22
Loss after 8678455 batches: 0.0026
trigger times: 23
Loss after 8679350 batches: 0.0024
trigger times: 24
Loss after 8680245 batches: 0.0021
trigger times: 25
Early stopping!
Start to test process.
Loss after 8681140 batches: 0.0022
Time to train on one home:  99.80080389976501
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005]]
Round_6_results:  [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005]
trigger times: 0
Loss after 8682103 batches: 0.0577
trigger times: 1
Loss after 8683066 batches: 0.0463
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 9105 < 9106; dropping {'Training_Loss': 0.05771133037550109, 'Validation_Loss': 0.06681390106678009, 'Training_R2': 0.2967192882249514, 'Validation_R2': 0.10255318669904168, 'Training_F1': 0.5027848178825469, 'Validation_F1': 0.3005545939616234, 'Training_NEP': 0.7984165963370072, 'Validation_NEP': 0.9050540806328011, 'Training_NDE': 0.517934195577981, 'Validation_NDE': 0.6964168477608621, 'Training_MAE': 22.990541069576484, 'Validation_MAE': 30.147563075534485, 'Training_MSE': 1629.5143, 'Validation_MSE': 3449.631}.
trigger times: 0
Loss after 8684029 batches: 0.0445
trigger times: 1
Loss after 8684992 batches: 0.0417
trigger times: 2
Loss after 8685955 batches: 0.0389
trigger times: 3
Loss after 8686918 batches: 0.0375
trigger times: 4
Loss after 8687881 batches: 0.0374
trigger times: 5
Loss after 8688844 batches: 0.0355
trigger times: 6
Loss after 8689807 batches: 0.0357
trigger times: 7
Loss after 8690770 batches: 0.0353
trigger times: 8
Loss after 8691733 batches: 0.0356
trigger times: 9
Loss after 8692696 batches: 0.0360
trigger times: 10
Loss after 8693659 batches: 0.0336
trigger times: 11
Loss after 8694622 batches: 0.0335
trigger times: 12
Loss after 8695585 batches: 0.0353
trigger times: 13
Loss after 8696548 batches: 0.0343
trigger times: 14
Loss after 8697511 batches: 0.0336
trigger times: 15
Loss after 8698474 batches: 0.0336
trigger times: 16
Loss after 8699437 batches: 0.0331
trigger times: 17
Loss after 8700400 batches: 0.0322
trigger times: 18
Loss after 8701363 batches: 0.0328
trigger times: 19
Loss after 8702326 batches: 0.0317
trigger times: 20
Loss after 8703289 batches: 0.0320
trigger times: 21
Loss after 8704252 batches: 0.0327
trigger times: 22
Loss after 8705215 batches: 0.0314
trigger times: 23
Loss after 8706178 batches: 0.0324
trigger times: 24
Loss after 8707141 batches: 0.0335
trigger times: 25
Early stopping!
Start to test process.
Loss after 8708104 batches: 0.0320
Time to train on one home:  58.87092638015747
trigger times: 0
Loss after 8709062 batches: 0.0691
trigger times: 0
Loss after 8710020 batches: 0.0434
trigger times: 1
Loss after 8710978 batches: 0.0381
trigger times: 2
Loss after 8711936 batches: 0.0323
trigger times: 3
Loss after 8712894 batches: 0.0286
trigger times: 4
Loss after 8713852 batches: 0.0287
trigger times: 5
Loss after 8714810 batches: 0.0272
trigger times: 0
Loss after 8715768 batches: 0.0262
trigger times: 0
Loss after 8716726 batches: 0.0244
trigger times: 1
Loss after 8717684 batches: 0.0232
trigger times: 2
Loss after 8718642 batches: 0.0234
trigger times: 3
Loss after 8719600 batches: 0.0229
trigger times: 0
Loss after 8720558 batches: 0.0229
trigger times: 1
Loss after 8721516 batches: 0.0229
trigger times: 2
Loss after 8722474 batches: 0.0210
trigger times: 3
Loss after 8723432 batches: 0.0206
trigger times: 0
Loss after 8724390 batches: 0.0201
trigger times: 1
Loss after 8725348 batches: 0.0206
trigger times: 2
Loss after 8726306 batches: 0.0196
trigger times: 3
Loss after 8727264 batches: 0.0192
trigger times: 4
Loss after 8728222 batches: 0.0192
trigger times: 5
Loss after 8729180 batches: 0.0189
trigger times: 6
Loss after 8730138 batches: 0.0220
trigger times: 7
Loss after 8731096 batches: 0.0245
trigger times: 8
Loss after 8732054 batches: 0.0233
trigger times: 9
Loss after 8733012 batches: 0.0214
trigger times: 0
Loss after 8733970 batches: 0.0205
trigger times: 1
Loss after 8734928 batches: 0.0195
trigger times: 2
Loss after 8735886 batches: 0.0186
trigger times: 3
Loss after 8736844 batches: 0.0179
trigger times: 0
Loss after 8737802 batches: 0.0186
trigger times: 1
Loss after 8738760 batches: 0.0173
trigger times: 2
Loss after 8739718 batches: 0.0173
trigger times: 3
Loss after 8740676 batches: 0.0172
trigger times: 4
Loss after 8741634 batches: 0.0172
trigger times: 5
Loss after 8742592 batches: 0.0175
trigger times: 6
Loss after 8743550 batches: 0.0168
trigger times: 7
Loss after 8744508 batches: 0.0170
trigger times: 8
Loss after 8745466 batches: 0.0167
trigger times: 9
Loss after 8746424 batches: 0.0171
trigger times: 10
Loss after 8747382 batches: 0.0165
trigger times: 11
Loss after 8748340 batches: 0.0177
trigger times: 12
Loss after 8749298 batches: 0.0162
trigger times: 13
Loss after 8750256 batches: 0.0156
trigger times: 14
Loss after 8751214 batches: 0.0172
trigger times: 15
Loss after 8752172 batches: 0.0162
trigger times: 16
Loss after 8753130 batches: 0.0163
trigger times: 17
Loss after 8754088 batches: 0.0158
trigger times: 18
Loss after 8755046 batches: 0.0158
trigger times: 19
Loss after 8756004 batches: 0.0161
trigger times: 20
Loss after 8756962 batches: 0.0158
trigger times: 21
Loss after 8757920 batches: 0.0154
trigger times: 22
Loss after 8758878 batches: 0.0172
trigger times: 23
Loss after 8759836 batches: 0.0166
trigger times: 24
Loss after 8760794 batches: 0.0172
trigger times: 25
Early stopping!
Start to test process.
Loss after 8761752 batches: 0.0161
Time to train on one home:  78.70394730567932
trigger times: 0
Loss after 8762715 batches: 0.0928
trigger times: 1
Loss after 8763678 batches: 0.0702
trigger times: 2
Loss after 8764641 batches: 0.0669
trigger times: 3
Loss after 8765604 batches: 0.0662
trigger times: 4
Loss after 8766567 batches: 0.0636
trigger times: 5
Loss after 8767530 batches: 0.0616
trigger times: 6
Loss after 8768493 batches: 0.0602
trigger times: 7
Loss after 8769456 batches: 0.0592
trigger times: 8
Loss after 8770419 batches: 0.0572
trigger times: 9
Loss after 8771382 batches: 0.0565
trigger times: 10
Loss after 8772345 batches: 0.0551
trigger times: 11
Loss after 8773308 batches: 0.0540
trigger times: 12
Loss after 8774271 batches: 0.0538
trigger times: 13
Loss after 8775234 batches: 0.0529
trigger times: 14
Loss after 8776197 batches: 0.0532
trigger times: 15
Loss after 8777160 batches: 0.0522
trigger times: 16
Loss after 8778123 batches: 0.0521
trigger times: 17
Loss after 8779086 batches: 0.0512
trigger times: 18
Loss after 8780049 batches: 0.0516
trigger times: 19
Loss after 8781012 batches: 0.0508
trigger times: 20
Loss after 8781975 batches: 0.0504
trigger times: 21
Loss after 8782938 batches: 0.0498
trigger times: 22
Loss after 8783901 batches: 0.0499
trigger times: 23
Loss after 8784864 batches: 0.0497
trigger times: 24
Loss after 8785827 batches: 0.0480
trigger times: 25
Early stopping!
Start to test process.
Loss after 8786790 batches: 0.0495
Time to train on one home:  55.585142374038696
trigger times: 0
Loss after 8787753 batches: 0.0943
trigger times: 1
Loss after 8788716 batches: 0.0772
trigger times: 2
Loss after 8789679 batches: 0.0767
trigger times: 3
Loss after 8790642 batches: 0.0751
trigger times: 4
Loss after 8791605 batches: 0.0731
trigger times: 5
Loss after 8792568 batches: 0.0702
trigger times: 6
Loss after 8793531 batches: 0.0680
trigger times: 7
Loss after 8794494 batches: 0.0671
trigger times: 8
Loss after 8795457 batches: 0.0659
trigger times: 9
Loss after 8796420 batches: 0.0652
trigger times: 10
Loss after 8797383 batches: 0.0644
trigger times: 11
Loss after 8798346 batches: 0.0627
trigger times: 12
Loss after 8799309 batches: 0.0653
trigger times: 13
Loss after 8800272 batches: 0.0643
trigger times: 14
Loss after 8801235 batches: 0.0632
trigger times: 15
Loss after 8802198 batches: 0.0629
trigger times: 16
Loss after 8803161 batches: 0.0620
trigger times: 17
Loss after 8804124 batches: 0.0611
trigger times: 18
Loss after 8805087 batches: 0.0624
trigger times: 19
Loss after 8806050 batches: 0.0621
trigger times: 20
Loss after 8807013 batches: 0.0608
trigger times: 21
Loss after 8807976 batches: 0.0593
trigger times: 22
Loss after 8808939 batches: 0.0602
trigger times: 23
Loss after 8809902 batches: 0.0599
trigger times: 24
Loss after 8810865 batches: 0.0591
trigger times: 25
Early stopping!
Start to test process.
Loss after 8811828 batches: 0.0599
Time to train on one home:  56.79459619522095
trigger times: 0
Loss after 8812791 batches: 0.0283
trigger times: 1
Loss after 8813754 batches: 0.0242
trigger times: 2
Loss after 8814717 batches: 0.0221
trigger times: 3
Loss after 8815680 batches: 0.0204
trigger times: 4
Loss after 8816643 batches: 0.0195
trigger times: 5
Loss after 8817606 batches: 0.0187
trigger times: 6
Loss after 8818569 batches: 0.0184
trigger times: 7
Loss after 8819532 batches: 0.0181
trigger times: 8
Loss after 8820495 batches: 0.0176
trigger times: 9
Loss after 8821458 batches: 0.0167
trigger times: 10
Loss after 8822421 batches: 0.0168
trigger times: 11
Loss after 8823384 batches: 0.0168
trigger times: 12
Loss after 8824347 batches: 0.0164
trigger times: 13
Loss after 8825310 batches: 0.0163
trigger times: 14
Loss after 8826273 batches: 0.0161
trigger times: 15
Loss after 8827236 batches: 0.0164
trigger times: 16
Loss after 8828199 batches: 0.0163
trigger times: 17
Loss after 8829162 batches: 0.0161
trigger times: 18
Loss after 8830125 batches: 0.0156
trigger times: 19
Loss after 8831088 batches: 0.0155
trigger times: 20
Loss after 8832051 batches: 0.0146
trigger times: 21
Loss after 8833014 batches: 0.0154
trigger times: 22
Loss after 8833977 batches: 0.0150
trigger times: 23
Loss after 8834940 batches: 0.0150
trigger times: 24
Loss after 8835903 batches: 0.0149
trigger times: 25
Early stopping!
Start to test process.
Loss after 8836866 batches: 0.0152
Time to train on one home:  55.032926082611084
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8837829 batches: 0.0726
trigger times: 1
Loss after 8838792 batches: 0.0245
trigger times: 2
Loss after 8839755 batches: 0.0212
trigger times: 3
Loss after 8840718 batches: 0.0204
trigger times: 4
Loss after 8841681 batches: 0.0179
trigger times: 5
Loss after 8842644 batches: 0.0164
trigger times: 6
Loss after 8843607 batches: 0.0159
trigger times: 7
Loss after 8844570 batches: 0.0152
trigger times: 8
Loss after 8845533 batches: 0.0147
trigger times: 9
Loss after 8846496 batches: 0.0151
trigger times: 10
Loss after 8847459 batches: 0.0146
trigger times: 11
Loss after 8848422 batches: 0.0142
trigger times: 12
Loss after 8849385 batches: 0.0140
trigger times: 13
Loss after 8850348 batches: 0.0140
trigger times: 14
Loss after 8851311 batches: 0.0138
trigger times: 15
Loss after 8852274 batches: 0.0136
trigger times: 16
Loss after 8853237 batches: 0.0136
trigger times: 17
Loss after 8854200 batches: 0.0135
trigger times: 18
Loss after 8855163 batches: 0.0136
trigger times: 19
Loss after 8856126 batches: 0.0132
trigger times: 20
Loss after 8857089 batches: 0.0132
trigger times: 21
Loss after 8858052 batches: 0.0130
trigger times: 22
Loss after 8859015 batches: 0.0131
trigger times: 23
Loss after 8859978 batches: 0.0132
trigger times: 24
Loss after 8860941 batches: 0.0129
trigger times: 25
Early stopping!
Start to test process.
Loss after 8861904 batches: 0.0129
Time to train on one home:  60.84816861152649
trigger times: 0
Loss after 8862867 batches: 0.0991
trigger times: 1
Loss after 8863830 batches: 0.0921
trigger times: 0
Loss after 8864793 batches: 0.0871
trigger times: 1
Loss after 8865756 batches: 0.0842
trigger times: 2
Loss after 8866719 batches: 0.0819
trigger times: 3
Loss after 8867682 batches: 0.0804
trigger times: 4
Loss after 8868645 batches: 0.0792
trigger times: 5
Loss after 8869608 batches: 0.0772
trigger times: 6
Loss after 8870571 batches: 0.0766
trigger times: 7
Loss after 8871534 batches: 0.0752
trigger times: 8
Loss after 8872497 batches: 0.0756
trigger times: 9
Loss after 8873460 batches: 0.0749
trigger times: 10
Loss after 8874423 batches: 0.0744
trigger times: 11
Loss after 8875386 batches: 0.0726
trigger times: 12
Loss after 8876349 batches: 0.0714
trigger times: 13
Loss after 8877312 batches: 0.0712
trigger times: 14
Loss after 8878275 batches: 0.0709
trigger times: 15
Loss after 8879238 batches: 0.0701
trigger times: 16
Loss after 8880201 batches: 0.0691
trigger times: 17
Loss after 8881164 batches: 0.0703
trigger times: 18
Loss after 8882127 batches: 0.0691
trigger times: 19
Loss after 8883090 batches: 0.0678
trigger times: 20
Loss after 8884053 batches: 0.0682
trigger times: 21
Loss after 8885016 batches: 0.0690
trigger times: 22
Loss after 8885979 batches: 0.0685
trigger times: 23
Loss after 8886942 batches: 0.0673
trigger times: 24
Loss after 8887905 batches: 0.0687
trigger times: 25
Early stopping!
Start to test process.
Loss after 8888868 batches: 0.0653
Time to train on one home:  58.13580894470215
trigger times: 0
Loss after 8889831 batches: 0.0557
trigger times: 1
Loss after 8890794 batches: 0.0444
trigger times: 0
Loss after 8891757 batches: 0.0359
trigger times: 1
Loss after 8892720 batches: 0.0340
trigger times: 2
Loss after 8893683 batches: 0.0322
trigger times: 3
Loss after 8894646 batches: 0.0305
trigger times: 4
Loss after 8895609 batches: 0.0278
trigger times: 5
Loss after 8896572 batches: 0.0272
trigger times: 6
Loss after 8897535 batches: 0.0277
trigger times: 7
Loss after 8898498 batches: 0.0267
trigger times: 8
Loss after 8899461 batches: 0.0279
trigger times: 9
Loss after 8900424 batches: 0.0267
trigger times: 10
Loss after 8901387 batches: 0.0265
trigger times: 11
Loss after 8902350 batches: 0.0255
trigger times: 12
Loss after 8903313 batches: 0.0260
trigger times: 13
Loss after 8904276 batches: 0.0251
trigger times: 14
Loss after 8905239 batches: 0.0243
trigger times: 15
Loss after 8906202 batches: 0.0240
trigger times: 16
Loss after 8907165 batches: 0.0236
trigger times: 17
Loss after 8908128 batches: 0.0234
trigger times: 18
Loss after 8909091 batches: 0.0232
trigger times: 19
Loss after 8910054 batches: 0.0243
trigger times: 20
Loss after 8911017 batches: 0.0223
trigger times: 21
Loss after 8911980 batches: 0.0224
trigger times: 22
Loss after 8912943 batches: 0.0242
trigger times: 23
Loss after 8913906 batches: 0.0252
trigger times: 24
Loss after 8914869 batches: 0.0223
trigger times: 25
Early stopping!
Start to test process.
Loss after 8915832 batches: 0.0229
Time to train on one home:  58.98248481750488
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8916795 batches: 0.0850
trigger times: 1
Loss after 8917758 batches: 0.0764
trigger times: 2
Loss after 8918721 batches: 0.0735
trigger times: 3
Loss after 8919684 batches: 0.0701
trigger times: 4
Loss after 8920647 batches: 0.0695
trigger times: 5
Loss after 8921610 batches: 0.0680
trigger times: 6
Loss after 8922573 batches: 0.0669
trigger times: 7
Loss after 8923536 batches: 0.0659
trigger times: 8
Loss after 8924499 batches: 0.0640
trigger times: 9
Loss after 8925462 batches: 0.0639
trigger times: 10
Loss after 8926425 batches: 0.0633
trigger times: 11
Loss after 8927388 batches: 0.0630
trigger times: 12
Loss after 8928351 batches: 0.0615
trigger times: 13
Loss after 8929314 batches: 0.0617
trigger times: 14
Loss after 8930277 batches: 0.0610
trigger times: 15
Loss after 8931240 batches: 0.0608
trigger times: 16
Loss after 8932203 batches: 0.0623
trigger times: 17
Loss after 8933166 batches: 0.0628
trigger times: 18
Loss after 8934129 batches: 0.0615
trigger times: 19
Loss after 8935092 batches: 0.0608
trigger times: 20
Loss after 8936055 batches: 0.0601
trigger times: 21
Loss after 8937018 batches: 0.0592
trigger times: 22
Loss after 8937981 batches: 0.0593
trigger times: 23
Loss after 8938944 batches: 0.0588
trigger times: 24
Loss after 8939907 batches: 0.0586
trigger times: 25
Early stopping!
Start to test process.
Loss after 8940870 batches: 0.0578
Time to train on one home:  57.4169180393219
trigger times: 0
Loss after 8941833 batches: 0.0928
trigger times: 0
Loss after 8942796 batches: 0.0669
trigger times: 0
Loss after 8943759 batches: 0.0623
trigger times: 1
Loss after 8944722 batches: 0.0591
trigger times: 2
Loss after 8945685 batches: 0.0545
trigger times: 3
Loss after 8946648 batches: 0.0523
trigger times: 4
Loss after 8947611 batches: 0.0507
trigger times: 5
Loss after 8948574 batches: 0.0488
trigger times: 6
Loss after 8949537 batches: 0.0481
trigger times: 7
Loss after 8950500 batches: 0.0467
trigger times: 8
Loss after 8951463 batches: 0.0463
trigger times: 9
Loss after 8952426 batches: 0.0461
trigger times: 10
Loss after 8953389 batches: 0.0453
trigger times: 11
Loss after 8954352 batches: 0.0452
trigger times: 12
Loss after 8955315 batches: 0.0441
trigger times: 13
Loss after 8956278 batches: 0.0441
trigger times: 14
Loss after 8957241 batches: 0.0438
trigger times: 15
Loss after 8958204 batches: 0.0434
trigger times: 16
Loss after 8959167 batches: 0.0424
trigger times: 17
Loss after 8960130 batches: 0.0428
trigger times: 18
Loss after 8961093 batches: 0.0423
trigger times: 19
Loss after 8962056 batches: 0.0418
trigger times: 20
Loss after 8963019 batches: 0.0420
trigger times: 21
Loss after 8963982 batches: 0.0428
trigger times: 22
Loss after 8964945 batches: 0.0413
trigger times: 23
Loss after 8965908 batches: 0.0409
trigger times: 24
Loss after 8966871 batches: 0.0422
trigger times: 25
Early stopping!
Start to test process.
Loss after 8967834 batches: 0.0410
Time to train on one home:  57.421794414520264
trigger times: 0
Loss after 8968797 batches: 0.0778
trigger times: 1
Loss after 8969760 batches: 0.0714
trigger times: 2
Loss after 8970723 batches: 0.0685
trigger times: 3
Loss after 8971686 batches: 0.0664
trigger times: 4
Loss after 8972649 batches: 0.0650
trigger times: 5
Loss after 8973612 batches: 0.0633
trigger times: 6
Loss after 8974575 batches: 0.0630
trigger times: 7
Loss after 8975538 batches: 0.0614
trigger times: 8
Loss after 8976501 batches: 0.0607
trigger times: 9
Loss after 8977464 batches: 0.0607
trigger times: 10
Loss after 8978427 batches: 0.0604
trigger times: 11
Loss after 8979390 batches: 0.0602
trigger times: 12
Loss after 8980353 batches: 0.0587
trigger times: 13
Loss after 8981316 batches: 0.0587
trigger times: 14
Loss after 8982279 batches: 0.0584
trigger times: 15
Loss after 8983242 batches: 0.0587
trigger times: 16
Loss after 8984205 batches: 0.0576
trigger times: 17
Loss after 8985168 batches: 0.0577
trigger times: 18
Loss after 8986131 batches: 0.0567
trigger times: 19
Loss after 8987094 batches: 0.0563
trigger times: 20
Loss after 8988057 batches: 0.0557
trigger times: 21
Loss after 8989020 batches: 0.0555
trigger times: 22
Loss after 8989983 batches: 0.0555
trigger times: 23
Loss after 8990946 batches: 0.0562
trigger times: 24
Loss after 8991909 batches: 0.0552
trigger times: 25
Early stopping!
Start to test process.
Loss after 8992872 batches: 0.0569
Time to train on one home:  53.08612585067749
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8993835 batches: 0.0653
trigger times: 1
Loss after 8994798 batches: 0.0531
trigger times: 2
Loss after 8995761 batches: 0.0515
trigger times: 3
Loss after 8996724 batches: 0.0473
trigger times: 4
Loss after 8997687 batches: 0.0432
trigger times: 5
Loss after 8998650 batches: 0.0409
trigger times: 6
Loss after 8999613 batches: 0.0381
trigger times: 7
Loss after 9000576 batches: 0.0367
trigger times: 8
Loss after 9001539 batches: 0.0351
trigger times: 9
Loss after 9002502 batches: 0.0339
trigger times: 10
Loss after 9003465 batches: 0.0330
trigger times: 11
Loss after 9004428 batches: 0.0326
trigger times: 12
Loss after 9005391 batches: 0.0315
trigger times: 13
Loss after 9006354 batches: 0.0308
trigger times: 14
Loss after 9007317 batches: 0.0305
trigger times: 15
Loss after 9008280 batches: 0.0299
trigger times: 16
Loss after 9009243 batches: 0.0302
trigger times: 17
Loss after 9010206 batches: 0.0298
trigger times: 18
Loss after 9011169 batches: 0.0290
trigger times: 19
Loss after 9012132 batches: 0.0288
trigger times: 20
Loss after 9013095 batches: 0.0283
trigger times: 21
Loss after 9014058 batches: 0.0284
trigger times: 22
Loss after 9015021 batches: 0.0276
trigger times: 23
Loss after 9015984 batches: 0.0285
trigger times: 24
Loss after 9016947 batches: 0.0285
trigger times: 25
Early stopping!
Start to test process.
Loss after 9017910 batches: 0.0279
Time to train on one home:  57.1027889251709
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9018873 batches: 0.0833
trigger times: 0
Loss after 9019836 batches: 0.0550
trigger times: 1
Loss after 9020799 batches: 0.0491
trigger times: 2
Loss after 9021762 batches: 0.0449
trigger times: 0
Loss after 9022725 batches: 0.0409
trigger times: 1
Loss after 9023688 batches: 0.0369
trigger times: 2
Loss after 9024651 batches: 0.0329
trigger times: 3
Loss after 9025614 batches: 0.0322
trigger times: 4
Loss after 9026577 batches: 0.0303
trigger times: 5
Loss after 9027540 batches: 0.0301
trigger times: 6
Loss after 9028503 batches: 0.0287
trigger times: 7
Loss after 9029466 batches: 0.0296
trigger times: 8
Loss after 9030429 batches: 0.0285
trigger times: 9
Loss after 9031392 batches: 0.0275
trigger times: 10
Loss after 9032355 batches: 0.0268
trigger times: 11
Loss after 9033318 batches: 0.0262
trigger times: 12
Loss after 9034281 batches: 0.0264
trigger times: 13
Loss after 9035244 batches: 0.0261
trigger times: 14
Loss after 9036207 batches: 0.0252
trigger times: 15
Loss after 9037170 batches: 0.0261
trigger times: 16
Loss after 9038133 batches: 0.0257
trigger times: 17
Loss after 9039096 batches: 0.0255
trigger times: 18
Loss after 9040059 batches: 0.0259
trigger times: 19
Loss after 9041022 batches: 0.0257
trigger times: 20
Loss after 9041985 batches: 0.0249
trigger times: 21
Loss after 9042948 batches: 0.0259
trigger times: 0
Loss after 9043911 batches: 0.0261
trigger times: 1
Loss after 9044874 batches: 0.0247
trigger times: 2
Loss after 9045837 batches: 0.0241
trigger times: 3
Loss after 9046800 batches: 0.0230
trigger times: 4
Loss after 9047763 batches: 0.0230
trigger times: 5
Loss after 9048726 batches: 0.0221
trigger times: 6
Loss after 9049689 batches: 0.0232
trigger times: 7
Loss after 9050652 batches: 0.0222
trigger times: 8
Loss after 9051615 batches: 0.0224
trigger times: 9
Loss after 9052578 batches: 0.0207
trigger times: 10
Loss after 9053541 batches: 0.0204
trigger times: 11
Loss after 9054504 batches: 0.0219
trigger times: 12
Loss after 9055467 batches: 0.0218
trigger times: 13
Loss after 9056430 batches: 0.0217
trigger times: 14
Loss after 9057393 batches: 0.0210
trigger times: 15
Loss after 9058356 batches: 0.0207
trigger times: 16
Loss after 9059319 batches: 0.0218
trigger times: 17
Loss after 9060282 batches: 0.0205
trigger times: 18
Loss after 9061245 batches: 0.0223
trigger times: 0
Loss after 9062208 batches: 0.0210
trigger times: 1
Loss after 9063171 batches: 0.0207
trigger times: 2
Loss after 9064134 batches: 0.0207
trigger times: 3
Loss after 9065097 batches: 0.0217
trigger times: 4
Loss after 9066060 batches: 0.0211
trigger times: 5
Loss after 9067023 batches: 0.0213
trigger times: 6
Loss after 9067986 batches: 0.0204
trigger times: 7
Loss after 9068949 batches: 0.0197
trigger times: 8
Loss after 9069912 batches: 0.0202
trigger times: 0
Loss after 9070875 batches: 0.0183
trigger times: 1
Loss after 9071838 batches: 0.0189
trigger times: 2
Loss after 9072801 batches: 0.0176
trigger times: 3
Loss after 9073764 batches: 0.0173
trigger times: 4
Loss after 9074727 batches: 0.0174
trigger times: 5
Loss after 9075690 batches: 0.0173
trigger times: 6
Loss after 9076653 batches: 0.0187
trigger times: 7
Loss after 9077616 batches: 0.0196
trigger times: 8
Loss after 9078579 batches: 0.0183
trigger times: 9
Loss after 9079542 batches: 0.0199
trigger times: 10
Loss after 9080505 batches: 0.0196
trigger times: 11
Loss after 9081468 batches: 0.0188
trigger times: 12
Loss after 9082431 batches: 0.0174
trigger times: 13
Loss after 9083394 batches: 0.0164
trigger times: 14
Loss after 9084357 batches: 0.0170
trigger times: 15
Loss after 9085320 batches: 0.0164
trigger times: 0
Loss after 9086283 batches: 0.0172
trigger times: 1
Loss after 9087246 batches: 0.0171
trigger times: 2
Loss after 9088209 batches: 0.0180
trigger times: 3
Loss after 9089172 batches: 0.0168
trigger times: 4
Loss after 9090135 batches: 0.0162
trigger times: 5
Loss after 9091098 batches: 0.0172
trigger times: 6
Loss after 9092061 batches: 0.0170
trigger times: 7
Loss after 9093024 batches: 0.0177
trigger times: 8
Loss after 9093987 batches: 0.0174
trigger times: 9
Loss after 9094950 batches: 0.0176
trigger times: 0
Loss after 9095913 batches: 0.0181
trigger times: 1
Loss after 9096876 batches: 0.0169
trigger times: 2
Loss after 9097839 batches: 0.0178
trigger times: 3
Loss after 9098802 batches: 0.0161
trigger times: 4
Loss after 9099765 batches: 0.0153
trigger times: 5
Loss after 9100728 batches: 0.0159
trigger times: 6
Loss after 9101691 batches: 0.0155
trigger times: 7
Loss after 9102654 batches: 0.0156
trigger times: 8
Loss after 9103617 batches: 0.0162
trigger times: 9
Loss after 9104580 batches: 0.0159
trigger times: 10
Loss after 9105543 batches: 0.0176
trigger times: 11
Loss after 9106506 batches: 0.0163
trigger times: 12
Loss after 9107469 batches: 0.0156
trigger times: 13
Loss after 9108432 batches: 0.0155
trigger times: 14
Loss after 9109395 batches: 0.0159
trigger times: 15
Loss after 9110358 batches: 0.0161
trigger times: 16
Loss after 9111321 batches: 0.0162
trigger times: 17
Loss after 9112284 batches: 0.0153
trigger times: 18
Loss after 9113247 batches: 0.0153
trigger times: 19
Loss after 9114210 batches: 0.0149
trigger times: 20
Loss after 9115173 batches: 0.0151
trigger times: 21
Loss after 9116136 batches: 0.0140
trigger times: 22
Loss after 9117099 batches: 0.0146
trigger times: 23
Loss after 9118062 batches: 0.0165
trigger times: 24
Loss after 9119025 batches: 0.0156
trigger times: 0
Loss after 9119988 batches: 0.0172
trigger times: 1
Loss after 9120951 batches: 0.0173
trigger times: 2
Loss after 9121914 batches: 0.0169
trigger times: 3
Loss after 9122877 batches: 0.0160
trigger times: 4
Loss after 9123840 batches: 0.0156
trigger times: 5
Loss after 9124803 batches: 0.0151
trigger times: 6
Loss after 9125766 batches: 0.0146
trigger times: 7
Loss after 9126729 batches: 0.0142
trigger times: 8
Loss after 9127692 batches: 0.0140
trigger times: 9
Loss after 9128655 batches: 0.0135
trigger times: 10
Loss after 9129618 batches: 0.0147
trigger times: 11
Loss after 9130581 batches: 0.0164
trigger times: 12
Loss after 9131544 batches: 0.0160
trigger times: 13
Loss after 9132507 batches: 0.0159
trigger times: 14
Loss after 9133470 batches: 0.0154
trigger times: 15
Loss after 9134433 batches: 0.0151
trigger times: 16
Loss after 9135396 batches: 0.0144
trigger times: 17
Loss after 9136359 batches: 0.0156
trigger times: 18
Loss after 9137322 batches: 0.0154
trigger times: 19
Loss after 9138285 batches: 0.0146
trigger times: 20
Loss after 9139248 batches: 0.0141
trigger times: 21
Loss after 9140211 batches: 0.0138
trigger times: 22
Loss after 9141174 batches: 0.0144
trigger times: 23
Loss after 9142137 batches: 0.0139
trigger times: 24
Loss after 9143100 batches: 0.0133
trigger times: 25
Early stopping!
Start to test process.
Loss after 9144063 batches: 0.0128
Time to train on one home:  144.43056178092957
trigger times: 0
Loss after 9144992 batches: 0.0931
trigger times: 0
Loss after 9145921 batches: 0.0647
trigger times: 0
Loss after 9146850 batches: 0.0491
trigger times: 1
Loss after 9147779 batches: 0.0443
trigger times: 2
Loss after 9148708 batches: 0.0401
trigger times: 3
Loss after 9149637 batches: 0.0362
trigger times: 4
Loss after 9150566 batches: 0.0336
trigger times: 5
Loss after 9151495 batches: 0.0345
trigger times: 6
Loss after 9152424 batches: 0.0331
trigger times: 0
Loss after 9153353 batches: 0.0356
trigger times: 1
Loss after 9154282 batches: 0.0327
trigger times: 2
Loss after 9155211 batches: 0.0334
trigger times: 3
Loss after 9156140 batches: 0.0315
trigger times: 4
Loss after 9157069 batches: 0.0292
trigger times: 5
Loss after 9157998 batches: 0.0279
trigger times: 6
Loss after 9158927 batches: 0.0282
trigger times: 7
Loss after 9159856 batches: 0.0289
trigger times: 8
Loss after 9160785 batches: 0.0298
trigger times: 9
Loss after 9161714 batches: 0.0293
trigger times: 10
Loss after 9162643 batches: 0.0277
trigger times: 11
Loss after 9163572 batches: 0.0273
trigger times: 0
Loss after 9164501 batches: 0.0276
trigger times: 1
Loss after 9165430 batches: 0.0269
trigger times: 2
Loss after 9166359 batches: 0.0266
trigger times: 3
Loss after 9167288 batches: 0.0263
trigger times: 4
Loss after 9168217 batches: 0.0261
trigger times: 5
Loss after 9169146 batches: 0.0272
trigger times: 6
Loss after 9170075 batches: 0.0275
trigger times: 7
Loss after 9171004 batches: 0.0279
trigger times: 8
Loss after 9171933 batches: 0.0272
trigger times: 9
Loss after 9172862 batches: 0.0289
trigger times: 10
Loss after 9173791 batches: 0.0267
trigger times: 11
Loss after 9174720 batches: 0.0255
trigger times: 12
Loss after 9175649 batches: 0.0266
trigger times: 13
Loss after 9176578 batches: 0.0249
trigger times: 14
Loss after 9177507 batches: 0.0259
trigger times: 15
Loss after 9178436 batches: 0.0276
trigger times: 0
Loss after 9179365 batches: 0.0248
trigger times: 1
Loss after 9180294 batches: 0.0239
trigger times: 2
Loss after 9181223 batches: 0.0236
trigger times: 3
Loss after 9182152 batches: 0.0224
trigger times: 4
Loss after 9183081 batches: 0.0240
trigger times: 5
Loss after 9184010 batches: 0.0266
trigger times: 6
Loss after 9184939 batches: 0.0265
trigger times: 7
Loss after 9185868 batches: 0.0276
trigger times: 8
Loss after 9186797 batches: 0.0238
trigger times: 9
Loss after 9187726 batches: 0.0230
trigger times: 10
Loss after 9188655 batches: 0.0235
trigger times: 11
Loss after 9189584 batches: 0.0242
trigger times: 12
Loss after 9190513 batches: 0.0235
trigger times: 13
Loss after 9191442 batches: 0.0232
trigger times: 14
Loss after 9192371 batches: 0.0229
trigger times: 15
Loss after 9193300 batches: 0.0240
trigger times: 16
Loss after 9194229 batches: 0.0224
trigger times: 17
Loss after 9195158 batches: 0.0224
trigger times: 18
Loss after 9196087 batches: 0.0218
trigger times: 19
Loss after 9197016 batches: 0.0222
trigger times: 20
Loss after 9197945 batches: 0.0219
trigger times: 21
Loss after 9198874 batches: 0.0223
trigger times: 22
Loss after 9199803 batches: 0.0222
trigger times: 23
Loss after 9200732 batches: 0.0224
trigger times: 24
Loss after 9201661 batches: 0.0240
trigger times: 25
Early stopping!
Start to test process.
Loss after 9202590 batches: 0.0223
Time to train on one home:  85.86462831497192
trigger times: 0
Loss after 9203552 batches: 0.0761
trigger times: 1
Loss after 9204514 batches: 0.0669
trigger times: 2
Loss after 9205476 batches: 0.0661
trigger times: 3
Loss after 9206438 batches: 0.0630
trigger times: 4
Loss after 9207400 batches: 0.0616
trigger times: 5
Loss after 9208362 batches: 0.0608
trigger times: 6
Loss after 9209324 batches: 0.0592
trigger times: 7
Loss after 9210286 batches: 0.0579
trigger times: 8
Loss after 9211248 batches: 0.0575
trigger times: 9
Loss after 9212210 batches: 0.0561
trigger times: 10
Loss after 9213172 batches: 0.0563
trigger times: 11
Loss after 9214134 batches: 0.0560
trigger times: 12
Loss after 9215096 batches: 0.0564
trigger times: 13
Loss after 9216058 batches: 0.0562
trigger times: 14
Loss after 9217020 batches: 0.0551
trigger times: 15
Loss after 9217982 batches: 0.0557
trigger times: 16
Loss after 9218944 batches: 0.0543
trigger times: 17
Loss after 9219906 batches: 0.0543
trigger times: 18
Loss after 9220868 batches: 0.0546
trigger times: 19
Loss after 9221830 batches: 0.0541
trigger times: 20
Loss after 9222792 batches: 0.0547
trigger times: 21
Loss after 9223754 batches: 0.0545
trigger times: 22
Loss after 9224716 batches: 0.0542
trigger times: 23
Loss after 9225678 batches: 0.0540
trigger times: 24
Loss after 9226640 batches: 0.0529
trigger times: 25
Early stopping!
Start to test process.
Loss after 9227602 batches: 0.0535
Time to train on one home:  57.3531928062439
trigger times: 0
Loss after 9228565 batches: 0.0557
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 9229528 batches: 0.0453
trigger times: 2
Loss after 9230491 batches: 0.0466
trigger times: 3
Loss after 9231454 batches: 0.0444
trigger times: 4
Loss after 9232417 batches: 0.0409
trigger times: 5
Loss after 9233380 batches: 0.0398
trigger times: 6
Loss after 9234343 batches: 0.0386
trigger times: 7
Loss after 9235306 batches: 0.0380
trigger times: 8
Loss after 9236269 batches: 0.0373
trigger times: 9
Loss after 9237232 batches: 0.0361
trigger times: 10
Loss after 9238195 batches: 0.0364
trigger times: 11
Loss after 9239158 batches: 0.0363
trigger times: 12
Loss after 9240121 batches: 0.0349
trigger times: 13
Loss after 9241084 batches: 0.0346
trigger times: 14
Loss after 9242047 batches: 0.0353
trigger times: 15
Loss after 9243010 batches: 0.0343
trigger times: 16
Loss after 9243973 batches: 0.0342
trigger times: 17
Loss after 9244936 batches: 0.0335
trigger times: 18
Loss after 9245899 batches: 0.0337
trigger times: 19
Loss after 9246862 batches: 0.0342
trigger times: 20
Loss after 9247825 batches: 0.0328
trigger times: 21
Loss after 9248788 batches: 0.0330
trigger times: 22
Loss after 9249751 batches: 0.0331
trigger times: 23
Loss after 9250714 batches: 0.0327
trigger times: 24
Loss after 9251677 batches: 0.0325
trigger times: 25
Early stopping!
Start to test process.
Loss after 9252640 batches: 0.0320
Time to train on one home:  57.66544723510742
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9253603 batches: 0.0522
trigger times: 1
Loss after 9254566 batches: 0.0478
trigger times: 0
Loss after 9255529 batches: 0.0459
trigger times: 1
Loss after 9256492 batches: 0.0443
trigger times: 2
Loss after 9257455 batches: 0.0427
trigger times: 3
Loss after 9258418 batches: 0.0415
trigger times: 4
Loss after 9259381 batches: 0.0409
trigger times: 5
Loss after 9260344 batches: 0.0395
trigger times: 6
Loss after 9261307 batches: 0.0389
trigger times: 7
Loss after 9262270 batches: 0.0375
trigger times: 8
Loss after 9263233 batches: 0.0377
trigger times: 9
Loss after 9264196 batches: 0.0367
trigger times: 10
Loss after 9265159 batches: 0.0359
trigger times: 11
Loss after 9266122 batches: 0.0354
trigger times: 12
Loss after 9267085 batches: 0.0357
trigger times: 13
Loss after 9268048 batches: 0.0356
trigger times: 14
Loss after 9269011 batches: 0.0355
trigger times: 15
Loss after 9269974 batches: 0.0359
trigger times: 16
Loss after 9270937 batches: 0.0344
trigger times: 17
Loss after 9271900 batches: 0.0338
trigger times: 18
Loss after 9272863 batches: 0.0343
trigger times: 19
Loss after 9273826 batches: 0.0350
trigger times: 20
Loss after 9274789 batches: 0.0332
trigger times: 21
Loss after 9275752 batches: 0.0334
trigger times: 22
Loss after 9276715 batches: 0.0335
trigger times: 23
Loss after 9277678 batches: 0.0326
trigger times: 24
Loss after 9278641 batches: 0.0337
trigger times: 25
Early stopping!
Start to test process.
Loss after 9279604 batches: 0.0322
Time to train on one home:  56.12513184547424
trigger times: 0
Loss after 9280567 batches: 0.1032
trigger times: 0
Loss after 9281530 batches: 0.0963
trigger times: 1
Loss after 9282493 batches: 0.0930
trigger times: 2
Loss after 9283456 batches: 0.0900
trigger times: 3
Loss after 9284419 batches: 0.0872
trigger times: 4
Loss after 9285382 batches: 0.0878
trigger times: 5
Loss after 9286345 batches: 0.0856
trigger times: 6
Loss after 9287308 batches: 0.0844
trigger times: 7
Loss after 9288271 batches: 0.0850
trigger times: 8
Loss after 9289234 batches: 0.0843
trigger times: 9
Loss after 9290197 batches: 0.0825
trigger times: 10
Loss after 9291160 batches: 0.0840
trigger times: 11
Loss after 9292123 batches: 0.0841
trigger times: 12
Loss after 9293086 batches: 0.0828
trigger times: 13
Loss after 9294049 batches: 0.0820
trigger times: 14
Loss after 9295012 batches: 0.0801
trigger times: 15
Loss after 9295975 batches: 0.0774
trigger times: 16
Loss after 9296938 batches: 0.0780
trigger times: 17
Loss after 9297901 batches: 0.0787
trigger times: 18
Loss after 9298864 batches: 0.0773
trigger times: 19
Loss after 9299827 batches: 0.0757
trigger times: 20
Loss after 9300790 batches: 0.0753
trigger times: 21
Loss after 9301753 batches: 0.0741
trigger times: 22
Loss after 9302716 batches: 0.0757
trigger times: 23
Loss after 9303679 batches: 0.0758
trigger times: 24
Loss after 9304642 batches: 0.0766
trigger times: 25
Early stopping!
Start to test process.
Loss after 9305605 batches: 0.0742
Time to train on one home:  55.28757619857788
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9306568 batches: 0.1015
trigger times: 1
Loss after 9307531 batches: 0.0674
trigger times: 2
Loss after 9308494 batches: 0.0649
trigger times: 3
Loss after 9309457 batches: 0.0582
trigger times: 4
Loss after 9310420 batches: 0.0520
trigger times: 5
Loss after 9311383 batches: 0.0486
trigger times: 6
Loss after 9312346 batches: 0.0454
trigger times: 7
Loss after 9313309 batches: 0.0422
trigger times: 8
Loss after 9314272 batches: 0.0415
trigger times: 9
Loss after 9315235 batches: 0.0404
trigger times: 10
Loss after 9316198 batches: 0.0395
trigger times: 11
Loss after 9317161 batches: 0.0390
trigger times: 12
Loss after 9318124 batches: 0.0364
trigger times: 13
Loss after 9319087 batches: 0.0358
trigger times: 14
Loss after 9320050 batches: 0.0363
trigger times: 15
Loss after 9321013 batches: 0.0350
trigger times: 16
Loss after 9321976 batches: 0.0346
trigger times: 0
Loss after 9322939 batches: 0.0336
trigger times: 1
Loss after 9323902 batches: 0.0348
trigger times: 2
Loss after 9324865 batches: 0.0350
trigger times: 3
Loss after 9325828 batches: 0.0342
trigger times: 4
Loss after 9326791 batches: 0.0336
trigger times: 5
Loss after 9327754 batches: 0.0347
trigger times: 6
Loss after 9328717 batches: 0.0333
trigger times: 7
Loss after 9329680 batches: 0.0333
trigger times: 8
Loss after 9330643 batches: 0.0314
trigger times: 9
Loss after 9331606 batches: 0.0324
trigger times: 0
Loss after 9332569 batches: 0.0321
trigger times: 1
Loss after 9333532 batches: 0.0309
trigger times: 2
Loss after 9334495 batches: 0.0306
trigger times: 3
Loss after 9335458 batches: 0.0319
trigger times: 4
Loss after 9336421 batches: 0.0306
trigger times: 5
Loss after 9337384 batches: 0.0308
trigger times: 6
Loss after 9338347 batches: 0.0305
trigger times: 7
Loss after 9339310 batches: 0.0300
trigger times: 8
Loss after 9340273 batches: 0.0295
trigger times: 9
Loss after 9341236 batches: 0.0290
trigger times: 10
Loss after 9342199 batches: 0.0290
trigger times: 11
Loss after 9343162 batches: 0.0287
trigger times: 12
Loss after 9344125 batches: 0.0291
trigger times: 13
Loss after 9345088 batches: 0.0292
trigger times: 14
Loss after 9346051 batches: 0.0289
trigger times: 15
Loss after 9347014 batches: 0.0288
trigger times: 16
Loss after 9347977 batches: 0.0276
trigger times: 0
Loss after 9348940 batches: 0.0296
trigger times: 1
Loss after 9349903 batches: 0.0299
trigger times: 2
Loss after 9350866 batches: 0.0307
trigger times: 3
Loss after 9351829 batches: 0.0289
trigger times: 4
Loss after 9352792 batches: 0.0279
trigger times: 5
Loss after 9353755 batches: 0.0283
trigger times: 6
Loss after 9354718 batches: 0.0276
trigger times: 7
Loss after 9355681 batches: 0.0270
trigger times: 8
Loss after 9356644 batches: 0.0265
trigger times: 9
Loss after 9357607 batches: 0.0274
trigger times: 10
Loss after 9358570 batches: 0.0294
trigger times: 11
Loss after 9359533 batches: 0.0288
trigger times: 12
Loss after 9360496 batches: 0.0274
trigger times: 13
Loss after 9361459 batches: 0.0278
trigger times: 14
Loss after 9362422 batches: 0.0265
trigger times: 15
Loss after 9363385 batches: 0.0267
trigger times: 16
Loss after 9364348 batches: 0.0260
trigger times: 17
Loss after 9365311 batches: 0.0248
trigger times: 18
Loss after 9366274 batches: 0.0251
trigger times: 19
Loss after 9367237 batches: 0.0246
trigger times: 20
Loss after 9368200 batches: 0.0245
trigger times: 21
Loss after 9369163 batches: 0.0239
trigger times: 22
Loss after 9370126 batches: 0.0233
trigger times: 23
Loss after 9371089 batches: 0.0236
trigger times: 24
Loss after 9372052 batches: 0.0245
trigger times: 25
Early stopping!
Start to test process.
Loss after 9373015 batches: 0.0232
Time to train on one home:  92.2511773109436
trigger times: 0
Loss after 9373974 batches: 0.0965
trigger times: 1
Loss after 9374933 batches: 0.0525
trigger times: 0
Loss after 9375892 batches: 0.0412
trigger times: 1
Loss after 9376851 batches: 0.0325
trigger times: 2
Loss after 9377810 batches: 0.0284
trigger times: 0
Loss after 9378769 batches: 0.0263
trigger times: 0
Loss after 9379728 batches: 0.0242
trigger times: 0
Loss after 9380687 batches: 0.0235
trigger times: 0
Loss after 9381646 batches: 0.0232
trigger times: 0
Loss after 9382605 batches: 0.0220
trigger times: 1
Loss after 9383564 batches: 0.0211
trigger times: 0
Loss after 9384523 batches: 0.0204
trigger times: 0
Loss after 9385482 batches: 0.0203
trigger times: 1
Loss after 9386441 batches: 0.0207
trigger times: 0
Loss after 9387400 batches: 0.0190
trigger times: 1
Loss after 9388359 batches: 0.0187
trigger times: 2
Loss after 9389318 batches: 0.0179
trigger times: 3
Loss after 9390277 batches: 0.0188
trigger times: 4
Loss after 9391236 batches: 0.0191
trigger times: 5
Loss after 9392195 batches: 0.0184
trigger times: 6
Loss after 9393154 batches: 0.0178
trigger times: 7
Loss after 9394113 batches: 0.0183
trigger times: 8
Loss after 9395072 batches: 0.0186
trigger times: 9
Loss after 9396031 batches: 0.0186
trigger times: 10
Loss after 9396990 batches: 0.0182
trigger times: 11
Loss after 9397949 batches: 0.0176
trigger times: 12
Loss after 9398908 batches: 0.0177
trigger times: 13
Loss after 9399867 batches: 0.0176
trigger times: 14
Loss after 9400826 batches: 0.0202
trigger times: 15
Loss after 9401785 batches: 0.0205
trigger times: 16
Loss after 9402744 batches: 0.0197
trigger times: 17
Loss after 9403703 batches: 0.0180
trigger times: 18
Loss after 9404662 batches: 0.0176
trigger times: 19
Loss after 9405621 batches: 0.0175
trigger times: 20
Loss after 9406580 batches: 0.0166
trigger times: 21
Loss after 9407539 batches: 0.0159
trigger times: 22
Loss after 9408498 batches: 0.0159
trigger times: 23
Loss after 9409457 batches: 0.0166
trigger times: 24
Loss after 9410416 batches: 0.0171
trigger times: 25
Early stopping!
Start to test process.
Loss after 9411375 batches: 0.0171
Time to train on one home:  71.04301619529724
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9412338 batches: 0.0635
trigger times: 1
Loss after 9413301 batches: 0.0291
trigger times: 2
Loss after 9414264 batches: 0.0270
trigger times: 3
Loss after 9415227 batches: 0.0267
trigger times: 4
Loss after 9416190 batches: 0.0259
trigger times: 5
Loss after 9417153 batches: 0.0253
trigger times: 6
Loss after 9418116 batches: 0.0247
trigger times: 7
Loss after 9419079 batches: 0.0240
trigger times: 8
Loss after 9420042 batches: 0.0235
trigger times: 9
Loss after 9421005 batches: 0.0232
trigger times: 10
Loss after 9421968 batches: 0.0228
trigger times: 11
Loss after 9422931 batches: 0.0223
trigger times: 12
Loss after 9423894 batches: 0.0220
trigger times: 13
Loss after 9424857 batches: 0.0214
trigger times: 14
Loss after 9425820 batches: 0.0209
trigger times: 15
Loss after 9426783 batches: 0.0206
trigger times: 16
Loss after 9427746 batches: 0.0206
trigger times: 17
Loss after 9428709 batches: 0.0203
trigger times: 18
Loss after 9429672 batches: 0.0200
trigger times: 19
Loss after 9430635 batches: 0.0198
trigger times: 20
Loss after 9431598 batches: 0.0197
trigger times: 21
Loss after 9432561 batches: 0.0195
trigger times: 22
Loss after 9433524 batches: 0.0195
trigger times: 23
Loss after 9434487 batches: 0.0193
trigger times: 24
Loss after 9435450 batches: 0.0192
trigger times: 25
Early stopping!
Start to test process.
Loss after 9436413 batches: 0.0192
Time to train on one home:  56.30869746208191
trigger times: 0
Loss after 9437358 batches: 0.0675
trigger times: 0
Loss after 9438303 batches: 0.0479
trigger times: 0
Loss after 9439248 batches: 0.0389
trigger times: 0
Loss after 9440193 batches: 0.0335
trigger times: 1
Loss after 9441138 batches: 0.0330
trigger times: 2
Loss after 9442083 batches: 0.0299
trigger times: 0
Loss after 9443028 batches: 0.0287
trigger times: 0
Loss after 9443973 batches: 0.0281
trigger times: 1
Loss after 9444918 batches: 0.0277
trigger times: 2
Loss after 9445863 batches: 0.0266
trigger times: 3
Loss after 9446808 batches: 0.0261
trigger times: 4
Loss after 9447753 batches: 0.0249
trigger times: 5
Loss after 9448698 batches: 0.0242
trigger times: 6
Loss after 9449643 batches: 0.0243
trigger times: 7
Loss after 9450588 batches: 0.0231
trigger times: 8
Loss after 9451533 batches: 0.0240
trigger times: 9
Loss after 9452478 batches: 0.0239
trigger times: 10
Loss after 9453423 batches: 0.0235
trigger times: 11
Loss after 9454368 batches: 0.0245
trigger times: 12
Loss after 9455313 batches: 0.0227
trigger times: 13
Loss after 9456258 batches: 0.0217
trigger times: 14
Loss after 9457203 batches: 0.0233
trigger times: 15
Loss after 9458148 batches: 0.0228
trigger times: 16
Loss after 9459093 batches: 0.0229
trigger times: 17
Loss after 9460038 batches: 0.0228
trigger times: 18
Loss after 9460983 batches: 0.0214
trigger times: 19
Loss after 9461928 batches: 0.0221
trigger times: 20
Loss after 9462873 batches: 0.0211
trigger times: 21
Loss after 9463818 batches: 0.0214
trigger times: 22
Loss after 9464763 batches: 0.0209
trigger times: 23
Loss after 9465708 batches: 0.0214
trigger times: 24
Loss after 9466653 batches: 0.0203
trigger times: 25
Early stopping!
Start to test process.
Loss after 9467598 batches: 0.0203
Time to train on one home:  61.86002063751221
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9468535 batches: 0.0861
trigger times: 1
Loss after 9469472 batches: 0.0728
trigger times: 2
Loss after 9470409 batches: 0.0705
trigger times: 3
Loss after 9471346 batches: 0.0669
trigger times: 4
Loss after 9472283 batches: 0.0641
trigger times: 5
Loss after 9473220 batches: 0.0628
trigger times: 6
Loss after 9474157 batches: 0.0613
trigger times: 7
Loss after 9475094 batches: 0.0595
trigger times: 8
Loss after 9476031 batches: 0.0583
trigger times: 9
Loss after 9476968 batches: 0.0587
trigger times: 10
Loss after 9477905 batches: 0.0579
trigger times: 11
Loss after 9478842 batches: 0.0575
trigger times: 12
Loss after 9479779 batches: 0.0576
trigger times: 13
Loss after 9480716 batches: 0.0549
trigger times: 14
Loss after 9481653 batches: 0.0552
trigger times: 15
Loss after 9482590 batches: 0.0559
trigger times: 16
Loss after 9483527 batches: 0.0550
trigger times: 17
Loss after 9484464 batches: 0.0550
trigger times: 18
Loss after 9485401 batches: 0.0543
trigger times: 19
Loss after 9486338 batches: 0.0550
trigger times: 20
Loss after 9487275 batches: 0.0537
trigger times: 21
Loss after 9488212 batches: 0.0532
trigger times: 22
Loss after 9489149 batches: 0.0520
trigger times: 23
Loss after 9490086 batches: 0.0516
trigger times: 24
Loss after 9491023 batches: 0.0517
trigger times: 25
Early stopping!
Start to test process.
Loss after 9491960 batches: 0.0514
Time to train on one home:  57.2674503326416
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 9492923 batches: 0.0396
trigger times: 1
Loss after 9493886 batches: 0.0156
trigger times: 2
Loss after 9494849 batches: 0.0137
trigger times: 3
Loss after 9495812 batches: 0.0138
trigger times: 4
Loss after 9496775 batches: 0.0138
trigger times: 5
Loss after 9497738 batches: 0.0138
trigger times: 6
Loss after 9498701 batches: 0.0134
trigger times: 7
Loss after 9499664 batches: 0.0134
trigger times: 8
Loss after 9500627 batches: 0.0131
trigger times: 9
Loss after 9501590 batches: 0.0127
trigger times: 10
Loss after 9502553 batches: 0.0124
trigger times: 11
Loss after 9503516 batches: 0.0121
trigger times: 12
Loss after 9504479 batches: 0.0116
trigger times: 13
Loss after 9505442 batches: 0.0111
trigger times: 14
Loss after 9506405 batches: 0.0109
trigger times: 15
Loss after 9507368 batches: 0.0108
trigger times: 16
Loss after 9508331 batches: 0.0104
trigger times: 17
Loss after 9509294 batches: 0.0100
trigger times: 18
Loss after 9510257 batches: 0.0100
trigger times: 19
Loss after 9511220 batches: 0.0096
trigger times: 20
Loss after 9512183 batches: 0.0097
trigger times: 21
Loss after 9513146 batches: 0.0097
trigger times: 22
Loss after 9514109 batches: 0.0097
trigger times: 23
Loss after 9515072 batches: 0.0096
trigger times: 24
Loss after 9516035 batches: 0.0094
trigger times: 25
Early stopping!
Start to test process.
Loss after 9516998 batches: 0.0094
Time to train on one home:  56.78067064285278
trigger times: 0
Loss after 9517961 batches: 0.0946
trigger times: 1
Loss after 9518924 batches: 0.0785
trigger times: 2
Loss after 9519887 batches: 0.0755
trigger times: 3
Loss after 9520850 batches: 0.0700
trigger times: 4
Loss after 9521813 batches: 0.0677
trigger times: 5
Loss after 9522776 batches: 0.0658
trigger times: 6
Loss after 9523739 batches: 0.0643
trigger times: 7
Loss after 9524702 batches: 0.0638
trigger times: 8
Loss after 9525665 batches: 0.0614
trigger times: 9
Loss after 9526628 batches: 0.0627
trigger times: 10
Loss after 9527591 batches: 0.0617
trigger times: 11
Loss after 9528554 batches: 0.0612
trigger times: 12
Loss after 9529517 batches: 0.0618
trigger times: 13
Loss after 9530480 batches: 0.0615
trigger times: 14
Loss after 9531443 batches: 0.0599
trigger times: 15
Loss after 9532406 batches: 0.0604
trigger times: 16
Loss after 9533369 batches: 0.0590
trigger times: 17
Loss after 9534332 batches: 0.0593
trigger times: 18
Loss after 9535295 batches: 0.0593
trigger times: 19
Loss after 9536258 batches: 0.0598
trigger times: 20
Loss after 9537221 batches: 0.0595
trigger times: 21
Loss after 9538184 batches: 0.0587
trigger times: 22
Loss after 9539147 batches: 0.0582
trigger times: 23
Loss after 9540110 batches: 0.0587
trigger times: 24
Loss after 9541073 batches: 0.0579
trigger times: 25
Early stopping!
Start to test process.
Loss after 9542036 batches: 0.0580
Time to train on one home:  53.161834478378296
trigger times: 0
Loss after 9542999 batches: 0.0723
trigger times: 1
Loss after 9543962 batches: 0.0571
trigger times: 2
Loss after 9544925 batches: 0.0527
trigger times: 3
Loss after 9545888 batches: 0.0482
trigger times: 4
Loss after 9546851 batches: 0.0459
trigger times: 5
Loss after 9547814 batches: 0.0444
trigger times: 6
Loss after 9548777 batches: 0.0431
trigger times: 7
Loss after 9549740 batches: 0.0423
trigger times: 8
Loss after 9550703 batches: 0.0407
trigger times: 9
Loss after 9551666 batches: 0.0401
trigger times: 10
Loss after 9552629 batches: 0.0403
trigger times: 11
Loss after 9553592 batches: 0.0397
trigger times: 12
Loss after 9554555 batches: 0.0386
trigger times: 13
Loss after 9555518 batches: 0.0383
trigger times: 14
Loss after 9556481 batches: 0.0369
trigger times: 15
Loss after 9557444 batches: 0.0365
trigger times: 16
Loss after 9558407 batches: 0.0370
trigger times: 17
Loss after 9559370 batches: 0.0366
trigger times: 18
Loss after 9560333 batches: 0.0363
trigger times: 19
Loss after 9561296 batches: 0.0367
trigger times: 20
Loss after 9562259 batches: 0.0356
trigger times: 21
Loss after 9563222 batches: 0.0349
trigger times: 22
Loss after 9564185 batches: 0.0351
trigger times: 23
Loss after 9565148 batches: 0.0344
trigger times: 24
Loss after 9566111 batches: 0.0347
trigger times: 25
Early stopping!
Start to test process.
Loss after 9567074 batches: 0.0348
Time to train on one home:  56.292842388153076
trigger times: 0
Loss after 9567970 batches: 0.1036
trigger times: 1
Loss after 9568866 batches: 0.0928
trigger times: 2
Loss after 9569762 batches: 0.0880
trigger times: 3
Loss after 9570658 batches: 0.0849
trigger times: 4
Loss after 9571554 batches: 0.0785
trigger times: 5
Loss after 9572450 batches: 0.0754
trigger times: 6
Loss after 9573346 batches: 0.0753
trigger times: 7
Loss after 9574242 batches: 0.0734
trigger times: 8
Loss after 9575138 batches: 0.0701
trigger times: 9
Loss after 9576034 batches: 0.0684
trigger times: 10
Loss after 9576930 batches: 0.0682
trigger times: 11
Loss after 9577826 batches: 0.0676
trigger times: 12
Loss after 9578722 batches: 0.0657
trigger times: 13
Loss after 9579618 batches: 0.0634
trigger times: 14
Loss after 9580514 batches: 0.0630
trigger times: 15
Loss after 9581410 batches: 0.0628
trigger times: 16
Loss after 9582306 batches: 0.0616
trigger times: 17
Loss after 9583202 batches: 0.0616
trigger times: 18
Loss after 9584098 batches: 0.0636
trigger times: 19
Loss after 9584994 batches: 0.0616
trigger times: 20
Loss after 9585890 batches: 0.0595
trigger times: 21
Loss after 9586786 batches: 0.0589
trigger times: 22
Loss after 9587682 batches: 0.0607
trigger times: 23
Loss after 9588578 batches: 0.0590
trigger times: 24
Loss after 9589474 batches: 0.0602
trigger times: 25
Early stopping!
Start to test process.
Loss after 9590370 batches: 0.0620
Time to train on one home:  55.20426034927368
trigger times: 0
Loss after 9591333 batches: 0.1692
trigger times: 1
Loss after 9592296 batches: 0.1172
trigger times: 2
Loss after 9593259 batches: 0.1002
trigger times: 3
Loss after 9594222 batches: 0.0942
trigger times: 4
Loss after 9595185 batches: 0.0866
trigger times: 5
Loss after 9596148 batches: 0.0848
trigger times: 6
Loss after 9597111 batches: 0.0809
trigger times: 7
Loss after 9598074 batches: 0.0776
trigger times: 8
Loss after 9599037 batches: 0.0738
trigger times: 9
Loss after 9600000 batches: 0.0718
trigger times: 10
Loss after 9600963 batches: 0.0702
trigger times: 11
Loss after 9601926 batches: 0.0673
trigger times: 12
Loss after 9602889 batches: 0.0640
trigger times: 13
Loss after 9603852 batches: 0.0587
trigger times: 14
Loss after 9604815 batches: 0.0613
trigger times: 15
Loss after 9605778 batches: 0.0576
trigger times: 16
Loss after 9606741 batches: 0.0549
trigger times: 17
Loss after 9607704 batches: 0.0550
trigger times: 18
Loss after 9608667 batches: 0.0534
trigger times: 19
Loss after 9609630 batches: 0.0526
trigger times: 20
Loss after 9610593 batches: 0.0540
trigger times: 21
Loss after 9611556 batches: 0.0514
trigger times: 22
Loss after 9612519 batches: 0.0481
trigger times: 23
Loss after 9613482 batches: 0.0493
trigger times: 24
Loss after 9614445 batches: 0.0482
trigger times: 25
Early stopping!
Start to test process.
Loss after 9615408 batches: 0.0468
Time to train on one home:  55.389145612716675
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9616371 batches: 0.0875
trigger times: 1
Loss after 9617334 batches: 0.0775
trigger times: 2
Loss after 9618297 batches: 0.0778
trigger times: 3
Loss after 9619260 batches: 0.0756
trigger times: 4
Loss after 9620223 batches: 0.0725
trigger times: 5
Loss after 9621186 batches: 0.0691
trigger times: 6
Loss after 9622149 batches: 0.0669
trigger times: 7
Loss after 9623112 batches: 0.0674
trigger times: 8
Loss after 9624075 batches: 0.0654
trigger times: 9
Loss after 9625038 batches: 0.0638
trigger times: 10
Loss after 9626001 batches: 0.0630
trigger times: 11
Loss after 9626964 batches: 0.0603
trigger times: 12
Loss after 9627927 batches: 0.0596
trigger times: 13
Loss after 9628890 batches: 0.0626
trigger times: 14
Loss after 9629853 batches: 0.0599
trigger times: 15
Loss after 9630816 batches: 0.0575
trigger times: 16
Loss after 9631779 batches: 0.0583
trigger times: 17
Loss after 9632742 batches: 0.0582
trigger times: 18
Loss after 9633705 batches: 0.0578
trigger times: 19
Loss after 9634668 batches: 0.0608
trigger times: 20
Loss after 9635631 batches: 0.0583
trigger times: 21
Loss after 9636594 batches: 0.0579
trigger times: 22
Loss after 9637557 batches: 0.0570
trigger times: 23
Loss after 9638520 batches: 0.0563
trigger times: 24
Loss after 9639483 batches: 0.0560
trigger times: 25
Early stopping!
Start to test process.
Loss after 9640446 batches: 0.0560
Time to train on one home:  60.109763860702515
trigger times: 0
Loss after 9641409 batches: 0.0893
trigger times: 1
Loss after 9642372 batches: 0.0682
trigger times: 2
Loss after 9643335 batches: 0.0629
trigger times: 3
Loss after 9644298 batches: 0.0572
trigger times: 4
Loss after 9645261 batches: 0.0532
trigger times: 5
Loss after 9646224 batches: 0.0507
trigger times: 6
Loss after 9647187 batches: 0.0489
trigger times: 7
Loss after 9648150 batches: 0.0477
trigger times: 8
Loss after 9649113 batches: 0.0464
trigger times: 9
Loss after 9650076 batches: 0.0464
trigger times: 10
Loss after 9651039 batches: 0.0457
trigger times: 11
Loss after 9652002 batches: 0.0450
trigger times: 12
Loss after 9652965 batches: 0.0451
trigger times: 13
Loss after 9653928 batches: 0.0436
trigger times: 14
Loss after 9654891 batches: 0.0437
trigger times: 15
Loss after 9655854 batches: 0.0460
trigger times: 16
Loss after 9656817 batches: 0.0448
trigger times: 17
Loss after 9657780 batches: 0.0435
trigger times: 18
Loss after 9658743 batches: 0.0434
trigger times: 19
Loss after 9659706 batches: 0.0431
trigger times: 20
Loss after 9660669 batches: 0.0420
trigger times: 21
Loss after 9661632 batches: 0.0416
trigger times: 22
Loss after 9662595 batches: 0.0405
trigger times: 23
Loss after 9663558 batches: 0.0414
trigger times: 24
Loss after 9664521 batches: 0.0398
trigger times: 25
Early stopping!
Start to test process.
Loss after 9665484 batches: 0.0408
Time to train on one home:  56.52586078643799
trigger times: 0
Loss after 9666447 batches: 0.0499
trigger times: 1
Loss after 9667410 batches: 0.0415
trigger times: 2
Loss after 9668373 batches: 0.0390
trigger times: 3
Loss after 9669336 batches: 0.0362
trigger times: 4
Loss after 9670299 batches: 0.0333
trigger times: 5
Loss after 9671262 batches: 0.0315
trigger times: 6
Loss after 9672225 batches: 0.0293
trigger times: 7
Loss after 9673188 batches: 0.0294
trigger times: 8
Loss after 9674151 batches: 0.0300
trigger times: 9
Loss after 9675114 batches: 0.0288
trigger times: 10
Loss after 9676077 batches: 0.0287
trigger times: 11
Loss after 9677040 batches: 0.0277
trigger times: 12
Loss after 9678003 batches: 0.0263
trigger times: 13
Loss after 9678966 batches: 0.0258
trigger times: 14
Loss after 9679929 batches: 0.0264
trigger times: 15
Loss after 9680892 batches: 0.0259
trigger times: 16
Loss after 9681855 batches: 0.0253
trigger times: 17
Loss after 9682818 batches: 0.0255
trigger times: 18
Loss after 9683781 batches: 0.0247
trigger times: 19
Loss after 9684744 batches: 0.0245
trigger times: 20
Loss after 9685707 batches: 0.0244
trigger times: 21
Loss after 9686670 batches: 0.0242
trigger times: 22
Loss after 9687633 batches: 0.0242
trigger times: 23
Loss after 9688596 batches: 0.0235
trigger times: 24
Loss after 9689559 batches: 0.0233
trigger times: 25
Early stopping!
Start to test process.
Loss after 9690522 batches: 0.0236
Time to train on one home:  57.11326360702515
trigger times: 0
Loss after 9691485 batches: 0.0780
trigger times: 1
Loss after 9692448 batches: 0.0474
trigger times: 2
Loss after 9693411 batches: 0.0478
trigger times: 3
Loss after 9694374 batches: 0.0460
trigger times: 4
Loss after 9695337 batches: 0.0438
trigger times: 5
Loss after 9696300 batches: 0.0419
trigger times: 6
Loss after 9697263 batches: 0.0408
trigger times: 7
Loss after 9698226 batches: 0.0404
trigger times: 8
Loss after 9699189 batches: 0.0393
trigger times: 9
Loss after 9700152 batches: 0.0388
trigger times: 10
Loss after 9701115 batches: 0.0381
trigger times: 11
Loss after 9702078 batches: 0.0382
trigger times: 12
Loss after 9703041 batches: 0.0376
trigger times: 13
Loss after 9704004 batches: 0.0372
trigger times: 14
Loss after 9704967 batches: 0.0373
trigger times: 15
Loss after 9705930 batches: 0.0368
trigger times: 16
Loss after 9706893 batches: 0.0372
trigger times: 17
Loss after 9707856 batches: 0.0365
trigger times: 18
Loss after 9708819 batches: 0.0361
trigger times: 19
Loss after 9709782 batches: 0.0362
trigger times: 20
Loss after 9710745 batches: 0.0369
trigger times: 21
Loss after 9711708 batches: 0.0356
trigger times: 22
Loss after 9712671 batches: 0.0362
trigger times: 23
Loss after 9713634 batches: 0.0360
trigger times: 24
Loss after 9714597 batches: 0.0362
trigger times: 25
Early stopping!
Start to test process.
Loss after 9715560 batches: 0.0358
Time to train on one home:  57.8040292263031
trigger times: 0
Loss after 9716455 batches: 0.0626
trigger times: 1
Loss after 9717350 batches: 0.0337
trigger times: 2
Loss after 9718245 batches: 0.0162
trigger times: 0
Loss after 9719140 batches: 0.0097
trigger times: 0
Loss after 9720035 batches: 0.0070
trigger times: 0
Loss after 9720930 batches: 0.0059
trigger times: 1
Loss after 9721825 batches: 0.0052
trigger times: 2
Loss after 9722720 batches: 0.0045
trigger times: 3
Loss after 9723615 batches: 0.0044
trigger times: 0
Loss after 9724510 batches: 0.0052
trigger times: 1
Loss after 9725405 batches: 0.0049
trigger times: 2
Loss after 9726300 batches: 0.0043
trigger times: 3
Loss after 9727195 batches: 0.0041
trigger times: 4
Loss after 9728090 batches: 0.0036
trigger times: 0
Loss after 9728985 batches: 0.0034
trigger times: 0
Loss after 9729880 batches: 0.0033
trigger times: 1
Loss after 9730775 batches: 0.0033
trigger times: 2
Loss after 9731670 batches: 0.0030
trigger times: 3
Loss after 9732565 batches: 0.0032
trigger times: 4
Loss after 9733460 batches: 0.0026
trigger times: 5
Loss after 9734355 batches: 0.0028
trigger times: 0
Loss after 9735250 batches: 0.0025
trigger times: 1
Loss after 9736145 batches: 0.0043
trigger times: 2
Loss after 9737040 batches: 0.0051
trigger times: 3
Loss after 9737935 batches: 0.0048
trigger times: 4
Loss after 9738830 batches: 0.0046
trigger times: 5
Loss after 9739725 batches: 0.0043
trigger times: 6
Loss after 9740620 batches: 0.0037
trigger times: 7
Loss after 9741515 batches: 0.0047
trigger times: 8
Loss after 9742410 batches: 0.0036
trigger times: 9
Loss after 9743305 batches: 0.0035
trigger times: 10
Loss after 9744200 batches: 0.0031
trigger times: 11
Loss after 9745095 batches: 0.0030
trigger times: 12
Loss after 9745990 batches: 0.0029
trigger times: 13
Loss after 9746885 batches: 0.0031
trigger times: 14
Loss after 9747780 batches: 0.0028
trigger times: 15
Loss after 9748675 batches: 0.0027
trigger times: 16
Loss after 9749570 batches: 0.0027
trigger times: 17
Loss after 9750465 batches: 0.0025
trigger times: 18
Loss after 9751360 batches: 0.0025
trigger times: 19
Loss after 9752255 batches: 0.0032
trigger times: 20
Loss after 9753150 batches: 0.0029
trigger times: 21
Loss after 9754045 batches: 0.0023
trigger times: 22
Loss after 9754940 batches: 0.0024
trigger times: 23
Loss after 9755835 batches: 0.0021
trigger times: 24
Loss after 9756730 batches: 0.0023
trigger times: 25
Early stopping!
Start to test process.
Loss after 9757625 batches: 0.0021
Time to train on one home:  70.40003776550293
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171]]
Round_7_results:  [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 10232 < 10233; dropping {'Training_Loss': 0.06434862847839083, 'Validation_Loss': 0.07809089124202728, 'Training_R2': 0.28854809474056575, 'Validation_R2': 0.08316404188199222, 'Training_F1': 0.4920347343874064, 'Validation_F1': 0.28360044057450906, 'Training_NEP': 0.827296577656975, 'Validation_NEP': 0.930920026743934, 'Training_NDE': 0.5239519040322271, 'Validation_NDE': 0.7114627878737791, 'Training_MAE': 23.822145021287255, 'Validation_MAE': 31.009163789326674, 'Training_MSE': 1648.447, 'Validation_MSE': 3524.1597}.
trigger times: 0
Loss after 9758588 batches: 0.0643
trigger times: 0
Loss after 9759551 batches: 0.0465
trigger times: 0
Loss after 9760514 batches: 0.0447
trigger times: 1
Loss after 9761477 batches: 0.0419
trigger times: 2
Loss after 9762440 batches: 0.0397
trigger times: 3
Loss after 9763403 batches: 0.0370
trigger times: 4
Loss after 9764366 batches: 0.0361
trigger times: 5
Loss after 9765329 batches: 0.0354
trigger times: 6
Loss after 9766292 batches: 0.0354
trigger times: 7
Loss after 9767255 batches: 0.0351
trigger times: 8
Loss after 9768218 batches: 0.0346
trigger times: 9
Loss after 9769181 batches: 0.0351
trigger times: 10
Loss after 9770144 batches: 0.0342
trigger times: 11
Loss after 9771107 batches: 0.0334
trigger times: 12
Loss after 9772070 batches: 0.0329
trigger times: 13
Loss after 9773033 batches: 0.0343
trigger times: 14
Loss after 9773996 batches: 0.0331
trigger times: 15
Loss after 9774959 batches: 0.0336
trigger times: 16
Loss after 9775922 batches: 0.0331
trigger times: 17
Loss after 9776885 batches: 0.0321
trigger times: 18
Loss after 9777848 batches: 0.0338
trigger times: 19
Loss after 9778811 batches: 0.0340
trigger times: 20
Loss after 9779774 batches: 0.0324
trigger times: 21
Loss after 9780737 batches: 0.0321
trigger times: 22
Loss after 9781700 batches: 0.0304
trigger times: 23
Loss after 9782663 batches: 0.0306
trigger times: 24
Loss after 9783626 batches: 0.0295
trigger times: 25
Early stopping!
Start to test process.
Loss after 9784589 batches: 0.0309
Time to train on one home:  55.41729187965393
trigger times: 0
Loss after 9785547 batches: 0.0933
trigger times: 0
Loss after 9786505 batches: 0.0487
trigger times: 1
Loss after 9787463 batches: 0.0455
trigger times: 2
Loss after 9788421 batches: 0.0392
trigger times: 3
Loss after 9789379 batches: 0.0318
trigger times: 4
Loss after 9790337 batches: 0.0287
trigger times: 5
Loss after 9791295 batches: 0.0272
trigger times: 0
Loss after 9792253 batches: 0.0259
trigger times: 1
Loss after 9793211 batches: 0.0252
trigger times: 2
Loss after 9794169 batches: 0.0247
trigger times: 3
Loss after 9795127 batches: 0.0240
trigger times: 4
Loss after 9796085 batches: 0.0226
trigger times: 5
Loss after 9797043 batches: 0.0216
trigger times: 6
Loss after 9798001 batches: 0.0228
trigger times: 7
Loss after 9798959 batches: 0.0213
trigger times: 8
Loss after 9799917 batches: 0.0211
trigger times: 9
Loss after 9800875 batches: 0.0227
trigger times: 10
Loss after 9801833 batches: 0.0216
trigger times: 11
Loss after 9802791 batches: 0.0210
trigger times: 12
Loss after 9803749 batches: 0.0210
trigger times: 13
Loss after 9804707 batches: 0.0201
trigger times: 14
Loss after 9805665 batches: 0.0192
trigger times: 15
Loss after 9806623 batches: 0.0187
trigger times: 16
Loss after 9807581 batches: 0.0183
trigger times: 17
Loss after 9808539 batches: 0.0175
trigger times: 18
Loss after 9809497 batches: 0.0184
trigger times: 19
Loss after 9810455 batches: 0.0174
trigger times: 20
Loss after 9811413 batches: 0.0195
trigger times: 21
Loss after 9812371 batches: 0.0188
trigger times: 22
Loss after 9813329 batches: 0.0191
trigger times: 23
Loss after 9814287 batches: 0.0178
trigger times: 24
Loss after 9815245 batches: 0.0172
trigger times: 25
Early stopping!
Start to test process.
Loss after 9816203 batches: 0.0165
Time to train on one home:  63.059770584106445
trigger times: 0
Loss after 9817166 batches: 0.0804
trigger times: 1
Loss after 9818129 batches: 0.0695
trigger times: 2
Loss after 9819092 batches: 0.0665
trigger times: 3
Loss after 9820055 batches: 0.0652
trigger times: 4
Loss after 9821018 batches: 0.0629
trigger times: 5
Loss after 9821981 batches: 0.0612
trigger times: 6
Loss after 9822944 batches: 0.0589
trigger times: 7
Loss after 9823907 batches: 0.0580
trigger times: 8
Loss after 9824870 batches: 0.0563
trigger times: 9
Loss after 9825833 batches: 0.0549
trigger times: 10
Loss after 9826796 batches: 0.0548
trigger times: 11
Loss after 9827759 batches: 0.0541
trigger times: 12
Loss after 9828722 batches: 0.0527
trigger times: 13
Loss after 9829685 batches: 0.0511
trigger times: 14
Loss after 9830648 batches: 0.0509
trigger times: 15
Loss after 9831611 batches: 0.0527
trigger times: 16
Loss after 9832574 batches: 0.0520
trigger times: 17
Loss after 9833537 batches: 0.0499
trigger times: 18
Loss after 9834500 batches: 0.0497
trigger times: 19
Loss after 9835463 batches: 0.0487
trigger times: 20
Loss after 9836426 batches: 0.0488
trigger times: 21
Loss after 9837389 batches: 0.0486
trigger times: 22
Loss after 9838352 batches: 0.0476
trigger times: 23
Loss after 9839315 batches: 0.0481
trigger times: 24
Loss after 9840278 batches: 0.0480
trigger times: 25
Early stopping!
Start to test process.
Loss after 9841241 batches: 0.0475
Time to train on one home:  54.162845611572266
trigger times: 0
Loss after 9842204 batches: 0.0879
trigger times: 1
Loss after 9843167 batches: 0.0775
trigger times: 2
Loss after 9844130 batches: 0.0755
trigger times: 3
Loss after 9845093 batches: 0.0742
trigger times: 4
Loss after 9846056 batches: 0.0704
trigger times: 5
Loss after 9847019 batches: 0.0688
trigger times: 6
Loss after 9847982 batches: 0.0664
trigger times: 7
Loss after 9848945 batches: 0.0660
trigger times: 8
Loss after 9849908 batches: 0.0642
trigger times: 9
Loss after 9850871 batches: 0.0647
trigger times: 10
Loss after 9851834 batches: 0.0633
trigger times: 11
Loss after 9852797 batches: 0.0626
trigger times: 12
Loss after 9853760 batches: 0.0615
trigger times: 13
Loss after 9854723 batches: 0.0611
trigger times: 14
Loss after 9855686 batches: 0.0611
trigger times: 15
Loss after 9856649 batches: 0.0602
trigger times: 16
Loss after 9857612 batches: 0.0607
trigger times: 17
Loss after 9858575 batches: 0.0593
trigger times: 18
Loss after 9859538 batches: 0.0604
trigger times: 19
Loss after 9860501 batches: 0.0596
trigger times: 20
Loss after 9861464 batches: 0.0606
trigger times: 21
Loss after 9862427 batches: 0.0602
trigger times: 0
Loss after 9863390 batches: 0.0604
trigger times: 1
Loss after 9864353 batches: 0.0586
trigger times: 2
Loss after 9865316 batches: 0.0578
trigger times: 3
Loss after 9866279 batches: 0.0584
trigger times: 4
Loss after 9867242 batches: 0.0576
trigger times: 5
Loss after 9868205 batches: 0.0584
trigger times: 6
Loss after 9869168 batches: 0.0573
trigger times: 7
Loss after 9870131 batches: 0.0583
trigger times: 8
Loss after 9871094 batches: 0.0569
trigger times: 9
Loss after 9872057 batches: 0.0587
trigger times: 10
Loss after 9873020 batches: 0.0582
trigger times: 11
Loss after 9873983 batches: 0.0593
trigger times: 12
Loss after 9874946 batches: 0.0584
trigger times: 13
Loss after 9875909 batches: 0.0580
trigger times: 14
Loss after 9876872 batches: 0.0573
trigger times: 15
Loss after 9877835 batches: 0.0577
trigger times: 16
Loss after 9878798 batches: 0.0577
trigger times: 17
Loss after 9879761 batches: 0.0562
trigger times: 18
Loss after 9880724 batches: 0.0564
trigger times: 19
Loss after 9881687 batches: 0.0558
trigger times: 20
Loss after 9882650 batches: 0.0556
trigger times: 21
Loss after 9883613 batches: 0.0540
trigger times: 22
Loss after 9884576 batches: 0.0541
trigger times: 23
Loss after 9885539 batches: 0.0554
trigger times: 24
Loss after 9886502 batches: 0.0560
trigger times: 25
Early stopping!
Start to test process.
Loss after 9887465 batches: 0.0556
Time to train on one home:  79.39644980430603
trigger times: 0
Loss after 9888428 batches: 0.0276
trigger times: 1
Loss after 9889391 batches: 0.0230
trigger times: 2
Loss after 9890354 batches: 0.0215
trigger times: 3
Loss after 9891317 batches: 0.0191
trigger times: 4
Loss after 9892280 batches: 0.0184
trigger times: 5
Loss after 9893243 batches: 0.0178
trigger times: 6
Loss after 9894206 batches: 0.0172
trigger times: 7
Loss after 9895169 batches: 0.0172
trigger times: 8
Loss after 9896132 batches: 0.0166
trigger times: 9
Loss after 9897095 batches: 0.0169
trigger times: 10
Loss after 9898058 batches: 0.0158
trigger times: 11
Loss after 9899021 batches: 0.0156
trigger times: 12
Loss after 9899984 batches: 0.0155
trigger times: 13
Loss after 9900947 batches: 0.0152
trigger times: 14
Loss after 9901910 batches: 0.0150
trigger times: 15
Loss after 9902873 batches: 0.0155
trigger times: 16
Loss after 9903836 batches: 0.0144
trigger times: 17
Loss after 9904799 batches: 0.0148
trigger times: 18
Loss after 9905762 batches: 0.0152
trigger times: 19
Loss after 9906725 batches: 0.0154
trigger times: 20
Loss after 9907688 batches: 0.0145
trigger times: 21
Loss after 9908651 batches: 0.0142
trigger times: 22
Loss after 9909614 batches: 0.0141
trigger times: 23
Loss after 9910577 batches: 0.0136
trigger times: 24
Loss after 9911540 batches: 0.0138
trigger times: 25
Early stopping!
Start to test process.
Loss after 9912503 batches: 0.0140
Time to train on one home:  56.63664937019348
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9913466 batches: 0.0505
trigger times: 1
Loss after 9914429 batches: 0.0237
trigger times: 2
Loss after 9915392 batches: 0.0196
trigger times: 3
Loss after 9916355 batches: 0.0175
trigger times: 4
Loss after 9917318 batches: 0.0165
trigger times: 5
Loss after 9918281 batches: 0.0155
trigger times: 6
Loss after 9919244 batches: 0.0147
trigger times: 7
Loss after 9920207 batches: 0.0143
trigger times: 8
Loss after 9921170 batches: 0.0145
trigger times: 9
Loss after 9922133 batches: 0.0137
trigger times: 10
Loss after 9923096 batches: 0.0136
trigger times: 11
Loss after 9924059 batches: 0.0136
trigger times: 12
Loss after 9925022 batches: 0.0136
trigger times: 13
Loss after 9925985 batches: 0.0132
trigger times: 14
Loss after 9926948 batches: 0.0134
trigger times: 15
Loss after 9927911 batches: 0.0132
trigger times: 16
Loss after 9928874 batches: 0.0131
trigger times: 17
Loss after 9929837 batches: 0.0131
trigger times: 18
Loss after 9930800 batches: 0.0130
trigger times: 19
Loss after 9931763 batches: 0.0127
trigger times: 20
Loss after 9932726 batches: 0.0126
trigger times: 21
Loss after 9933689 batches: 0.0126
trigger times: 22
Loss after 9934652 batches: 0.0126
trigger times: 23
Loss after 9935615 batches: 0.0123
trigger times: 24
Loss after 9936578 batches: 0.0124
trigger times: 25
Early stopping!
Start to test process.
Loss after 9937541 batches: 0.0123
Time to train on one home:  56.937408685684204
trigger times: 0
Loss after 9938504 batches: 0.1042
trigger times: 0
Loss after 9939467 batches: 0.0928
trigger times: 0
Loss after 9940430 batches: 0.0889
trigger times: 1
Loss after 9941393 batches: 0.0848
trigger times: 2
Loss after 9942356 batches: 0.0807
trigger times: 3
Loss after 9943319 batches: 0.0787
trigger times: 4
Loss after 9944282 batches: 0.0777
trigger times: 5
Loss after 9945245 batches: 0.0771
trigger times: 6
Loss after 9946208 batches: 0.0749
trigger times: 7
Loss after 9947171 batches: 0.0734
trigger times: 8
Loss after 9948134 batches: 0.0733
trigger times: 9
Loss after 9949097 batches: 0.0731
trigger times: 10
Loss after 9950060 batches: 0.0724
trigger times: 11
Loss after 9951023 batches: 0.0722
trigger times: 12
Loss after 9951986 batches: 0.0709
trigger times: 13
Loss after 9952949 batches: 0.0710
trigger times: 14
Loss after 9953912 batches: 0.0680
trigger times: 15
Loss after 9954875 batches: 0.0690
trigger times: 16
Loss after 9955838 batches: 0.0707
trigger times: 17
Loss after 9956801 batches: 0.0683
trigger times: 18
Loss after 9957764 batches: 0.0690
trigger times: 19
Loss after 9958727 batches: 0.0666
trigger times: 20
Loss after 9959690 batches: 0.0672
trigger times: 21
Loss after 9960653 batches: 0.0665
trigger times: 22
Loss after 9961616 batches: 0.0659
trigger times: 23
Loss after 9962579 batches: 0.0657
trigger times: 0
Loss after 9963542 batches: 0.0656
trigger times: 1
Loss after 9964505 batches: 0.0653
trigger times: 0
Loss after 9965468 batches: 0.0670
trigger times: 1
Loss after 9966431 batches: 0.0680
trigger times: 2
Loss after 9967394 batches: 0.0652
trigger times: 3
Loss after 9968357 batches: 0.0632
trigger times: 4
Loss after 9969320 batches: 0.0652
trigger times: 5
Loss after 9970283 batches: 0.0631
trigger times: 6
Loss after 9971246 batches: 0.0637
trigger times: 7
Loss after 9972209 batches: 0.0613
trigger times: 8
Loss after 9973172 batches: 0.0619
trigger times: 9
Loss after 9974135 batches: 0.0601
trigger times: 10
Loss after 9975098 batches: 0.0603
trigger times: 11
Loss after 9976061 batches: 0.0593
trigger times: 12
Loss after 9977024 batches: 0.0593
trigger times: 13
Loss after 9977987 batches: 0.0596
trigger times: 14
Loss after 9978950 batches: 0.0587
trigger times: 15
Loss after 9979913 batches: 0.0579
trigger times: 16
Loss after 9980876 batches: 0.0575
trigger times: 17
Loss after 9981839 batches: 0.0573
trigger times: 18
Loss after 9982802 batches: 0.0582
trigger times: 19
Loss after 9983765 batches: 0.0604
trigger times: 20
Loss after 9984728 batches: 0.0592
trigger times: 21
Loss after 9985691 batches: 0.0589
trigger times: 22
Loss after 9986654 batches: 0.0591
trigger times: 23
Loss after 9987617 batches: 0.0570
trigger times: 24
Loss after 9988580 batches: 0.0561
trigger times: 25
Early stopping!
Start to test process.
Loss after 9989543 batches: 0.0557
Time to train on one home:  80.03743982315063
trigger times: 0
Loss after 9990506 batches: 0.0527
trigger times: 0
Loss after 9991469 batches: 0.0417
trigger times: 0
Loss after 9992432 batches: 0.0355
trigger times: 0
Loss after 9993395 batches: 0.0328
trigger times: 0
Loss after 9994358 batches: 0.0297
trigger times: 1
Loss after 9995321 batches: 0.0289
trigger times: 0
Loss after 9996284 batches: 0.0283
trigger times: 1
Loss after 9997247 batches: 0.0281
trigger times: 2
Loss after 9998210 batches: 0.0275
trigger times: 3
Loss after 9999173 batches: 0.0266
trigger times: 4
Loss after 10000136 batches: 0.0262
trigger times: 5
Loss after 10001099 batches: 0.0244
trigger times: 6
Loss after 10002062 batches: 0.0249
trigger times: 7
Loss after 10003025 batches: 0.0231
trigger times: 8
Loss after 10003988 batches: 0.0236
trigger times: 9
Loss after 10004951 batches: 0.0233
trigger times: 10
Loss after 10005914 batches: 0.0237
trigger times: 11
Loss after 10006877 batches: 0.0221
trigger times: 12
Loss after 10007840 batches: 0.0232
trigger times: 13
Loss after 10008803 batches: 0.0225
trigger times: 14
Loss after 10009766 batches: 0.0222
trigger times: 15
Loss after 10010729 batches: 0.0221
trigger times: 16
Loss after 10011692 batches: 0.0220
trigger times: 17
Loss after 10012655 batches: 0.0223
trigger times: 18
Loss after 10013618 batches: 0.0224
trigger times: 19
Loss after 10014581 batches: 0.0226
trigger times: 20
Loss after 10015544 batches: 0.0211
trigger times: 21
Loss after 10016507 batches: 0.0213
trigger times: 22
Loss after 10017470 batches: 0.0207
trigger times: 23
Loss after 10018433 batches: 0.0217
trigger times: 24
Loss after 10019396 batches: 0.0208
trigger times: 25
Early stopping!
Start to test process.
Loss after 10020359 batches: 0.0208
Time to train on one home:  58.83661699295044
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10021322 batches: 0.0793
trigger times: 1
Loss after 10022285 batches: 0.0739
trigger times: 2
Loss after 10023248 batches: 0.0712
trigger times: 3
Loss after 10024211 batches: 0.0687
trigger times: 4
Loss after 10025174 batches: 0.0673
trigger times: 5
Loss after 10026137 batches: 0.0664
trigger times: 6
Loss after 10027100 batches: 0.0637
trigger times: 7
Loss after 10028063 batches: 0.0636
trigger times: 8
Loss after 10029026 batches: 0.0624
trigger times: 9
Loss after 10029989 batches: 0.0628
trigger times: 10
Loss after 10030952 batches: 0.0615
trigger times: 11
Loss after 10031915 batches: 0.0630
trigger times: 12
Loss after 10032878 batches: 0.0626
trigger times: 13
Loss after 10033841 batches: 0.0618
trigger times: 14
Loss after 10034804 batches: 0.0611
trigger times: 15
Loss after 10035767 batches: 0.0605
trigger times: 16
Loss after 10036730 batches: 0.0600
trigger times: 17
Loss after 10037693 batches: 0.0606
trigger times: 18
Loss after 10038656 batches: 0.0599
trigger times: 19
Loss after 10039619 batches: 0.0577
trigger times: 20
Loss after 10040582 batches: 0.0582
trigger times: 21
Loss after 10041545 batches: 0.0572
trigger times: 22
Loss after 10042508 batches: 0.0571
trigger times: 23
Loss after 10043471 batches: 0.0568
trigger times: 24
Loss after 10044434 batches: 0.0569
trigger times: 25
Early stopping!
Start to test process.
Loss after 10045397 batches: 0.0565
Time to train on one home:  54.259872913360596
trigger times: 0
Loss after 10046360 batches: 0.1083
trigger times: 0
Loss after 10047323 batches: 0.0695
trigger times: 0
Loss after 10048286 batches: 0.0644
trigger times: 1
Loss after 10049249 batches: 0.0576
trigger times: 2
Loss after 10050212 batches: 0.0559
trigger times: 3
Loss after 10051175 batches: 0.0521
trigger times: 4
Loss after 10052138 batches: 0.0501
trigger times: 5
Loss after 10053101 batches: 0.0486
trigger times: 6
Loss after 10054064 batches: 0.0466
trigger times: 7
Loss after 10055027 batches: 0.0466
trigger times: 8
Loss after 10055990 batches: 0.0453
trigger times: 9
Loss after 10056953 batches: 0.0453
trigger times: 10
Loss after 10057916 batches: 0.0435
trigger times: 11
Loss after 10058879 batches: 0.0436
trigger times: 12
Loss after 10059842 batches: 0.0437
trigger times: 13
Loss after 10060805 batches: 0.0424
trigger times: 14
Loss after 10061768 batches: 0.0425
trigger times: 15
Loss after 10062731 batches: 0.0425
trigger times: 16
Loss after 10063694 batches: 0.0422
trigger times: 17
Loss after 10064657 batches: 0.0424
trigger times: 18
Loss after 10065620 batches: 0.0418
trigger times: 19
Loss after 10066583 batches: 0.0412
trigger times: 20
Loss after 10067546 batches: 0.0396
trigger times: 21
Loss after 10068509 batches: 0.0406
trigger times: 22
Loss after 10069472 batches: 0.0403
trigger times: 23
Loss after 10070435 batches: 0.0401
trigger times: 24
Loss after 10071398 batches: 0.0406
trigger times: 25
Early stopping!
Start to test process.
Loss after 10072361 batches: 0.0414
Time to train on one home:  58.49555563926697
trigger times: 0
Loss after 10073324 batches: 0.0783
trigger times: 1
Loss after 10074287 batches: 0.0711
trigger times: 2
Loss after 10075250 batches: 0.0676
trigger times: 3
Loss after 10076213 batches: 0.0668
trigger times: 4
Loss after 10077176 batches: 0.0633
trigger times: 5
Loss after 10078139 batches: 0.0634
trigger times: 6
Loss after 10079102 batches: 0.0618
trigger times: 7
Loss after 10080065 batches: 0.0610
trigger times: 8
Loss after 10081028 batches: 0.0596
trigger times: 9
Loss after 10081991 batches: 0.0594
trigger times: 10
Loss after 10082954 batches: 0.0590
trigger times: 11
Loss after 10083917 batches: 0.0584
trigger times: 12
Loss after 10084880 batches: 0.0588
trigger times: 13
Loss after 10085843 batches: 0.0577
trigger times: 14
Loss after 10086806 batches: 0.0571
trigger times: 15
Loss after 10087769 batches: 0.0575
trigger times: 16
Loss after 10088732 batches: 0.0567
trigger times: 17
Loss after 10089695 batches: 0.0568
trigger times: 18
Loss after 10090658 batches: 0.0559
trigger times: 19
Loss after 10091621 batches: 0.0540
trigger times: 20
Loss after 10092584 batches: 0.0549
trigger times: 21
Loss after 10093547 batches: 0.0551
trigger times: 22
Loss after 10094510 batches: 0.0555
trigger times: 23
Loss after 10095473 batches: 0.0535
trigger times: 24
Loss after 10096436 batches: 0.0557
trigger times: 25
Early stopping!
Start to test process.
Loss after 10097399 batches: 0.0542
Time to train on one home:  54.231743574142456
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10098362 batches: 0.0640
trigger times: 1
Loss after 10099325 batches: 0.0529
trigger times: 2
Loss after 10100288 batches: 0.0508
trigger times: 3
Loss after 10101251 batches: 0.0456
trigger times: 4
Loss after 10102214 batches: 0.0402
trigger times: 5
Loss after 10103177 batches: 0.0369
trigger times: 6
Loss after 10104140 batches: 0.0342
trigger times: 7
Loss after 10105103 batches: 0.0340
trigger times: 0
Loss after 10106066 batches: 0.0318
trigger times: 1
Loss after 10107029 batches: 0.0307
trigger times: 2
Loss after 10107992 batches: 0.0302
trigger times: 0
Loss after 10108955 batches: 0.0309
trigger times: 1
Loss after 10109918 batches: 0.0301
trigger times: 2
Loss after 10110881 batches: 0.0292
trigger times: 3
Loss after 10111844 batches: 0.0286
trigger times: 4
Loss after 10112807 batches: 0.0280
trigger times: 5
Loss after 10113770 batches: 0.0280
trigger times: 6
Loss after 10114733 batches: 0.0278
trigger times: 7
Loss after 10115696 batches: 0.0277
trigger times: 8
Loss after 10116659 batches: 0.0280
trigger times: 9
Loss after 10117622 batches: 0.0286
trigger times: 10
Loss after 10118585 batches: 0.0277
trigger times: 11
Loss after 10119548 batches: 0.0292
trigger times: 12
Loss after 10120511 batches: 0.0273
trigger times: 13
Loss after 10121474 batches: 0.0267
trigger times: 14
Loss after 10122437 batches: 0.0265
trigger times: 15
Loss after 10123400 batches: 0.0256
trigger times: 16
Loss after 10124363 batches: 0.0254
trigger times: 17
Loss after 10125326 batches: 0.0252
trigger times: 18
Loss after 10126289 batches: 0.0253
trigger times: 19
Loss after 10127252 batches: 0.0255
trigger times: 20
Loss after 10128215 batches: 0.0255
trigger times: 21
Loss after 10129178 batches: 0.0254
trigger times: 22
Loss after 10130141 batches: 0.0261
trigger times: 23
Loss after 10131104 batches: 0.0256
trigger times: 24
Loss after 10132067 batches: 0.0253
trigger times: 25
Early stopping!
Start to test process.
Loss after 10133030 batches: 0.0249
Time to train on one home:  70.05299615859985
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10133993 batches: 0.0944
trigger times: 0
Loss after 10134956 batches: 0.0556
trigger times: 1
Loss after 10135919 batches: 0.0509
trigger times: 2
Loss after 10136882 batches: 0.0468
trigger times: 3
Loss after 10137845 batches: 0.0408
trigger times: 4
Loss after 10138808 batches: 0.0365
trigger times: 5
Loss after 10139771 batches: 0.0339
trigger times: 6
Loss after 10140734 batches: 0.0322
trigger times: 7
Loss after 10141697 batches: 0.0319
trigger times: 8
Loss after 10142660 batches: 0.0317
trigger times: 9
Loss after 10143623 batches: 0.0314
trigger times: 10
Loss after 10144586 batches: 0.0307
trigger times: 11
Loss after 10145549 batches: 0.0275
trigger times: 12
Loss after 10146512 batches: 0.0284
trigger times: 13
Loss after 10147475 batches: 0.0270
trigger times: 14
Loss after 10148438 batches: 0.0259
trigger times: 15
Loss after 10149401 batches: 0.0252
trigger times: 16
Loss after 10150364 batches: 0.0244
trigger times: 17
Loss after 10151327 batches: 0.0236
trigger times: 18
Loss after 10152290 batches: 0.0244
trigger times: 19
Loss after 10153253 batches: 0.0249
trigger times: 20
Loss after 10154216 batches: 0.0235
trigger times: 21
Loss after 10155179 batches: 0.0226
trigger times: 22
Loss after 10156142 batches: 0.0237
trigger times: 23
Loss after 10157105 batches: 0.0244
trigger times: 24
Loss after 10158068 batches: 0.0234
trigger times: 25
Early stopping!
Start to test process.
Loss after 10159031 batches: 0.0231
Time to train on one home:  57.208816051483154
trigger times: 0
Loss after 10159960 batches: 0.1288
trigger times: 0
Loss after 10160889 batches: 0.0745
trigger times: 0
Loss after 10161818 batches: 0.0570
trigger times: 1
Loss after 10162747 batches: 0.0463
trigger times: 2
Loss after 10163676 batches: 0.0402
trigger times: 0
Loss after 10164605 batches: 0.0376
trigger times: 1
Loss after 10165534 batches: 0.0360
trigger times: 0
Loss after 10166463 batches: 0.0328
trigger times: 1
Loss after 10167392 batches: 0.0347
trigger times: 2
Loss after 10168321 batches: 0.0318
trigger times: 3
Loss after 10169250 batches: 0.0306
trigger times: 4
Loss after 10170179 batches: 0.0295
trigger times: 5
Loss after 10171108 batches: 0.0316
trigger times: 6
Loss after 10172037 batches: 0.0300
trigger times: 7
Loss after 10172966 batches: 0.0276
trigger times: 8
Loss after 10173895 batches: 0.0292
trigger times: 9
Loss after 10174824 batches: 0.0278
trigger times: 10
Loss after 10175753 batches: 0.0286
trigger times: 11
Loss after 10176682 batches: 0.0285
trigger times: 12
Loss after 10177611 batches: 0.0262
trigger times: 13
Loss after 10178540 batches: 0.0284
trigger times: 14
Loss after 10179469 batches: 0.0271
trigger times: 15
Loss after 10180398 batches: 0.0266
trigger times: 16
Loss after 10181327 batches: 0.0266
trigger times: 17
Loss after 10182256 batches: 0.0270
trigger times: 18
Loss after 10183185 batches: 0.0259
trigger times: 19
Loss after 10184114 batches: 0.0267
trigger times: 20
Loss after 10185043 batches: 0.0374
trigger times: 21
Loss after 10185972 batches: 0.0326
trigger times: 22
Loss after 10186901 batches: 0.0306
trigger times: 23
Loss after 10187830 batches: 0.0286
trigger times: 0
Loss after 10188759 batches: 0.0281
trigger times: 1
Loss after 10189688 batches: 0.0314
trigger times: 2
Loss after 10190617 batches: 0.0286
trigger times: 3
Loss after 10191546 batches: 0.0265
trigger times: 0
Loss after 10192475 batches: 0.0264
trigger times: 1
Loss after 10193404 batches: 0.0276
trigger times: 2
Loss after 10194333 batches: 0.0277
trigger times: 3
Loss after 10195262 batches: 0.0245
trigger times: 4
Loss after 10196191 batches: 0.0252
trigger times: 0
Loss after 10197120 batches: 0.0233
trigger times: 1
Loss after 10198049 batches: 0.0254
trigger times: 2
Loss after 10198978 batches: 0.0236
trigger times: 3
Loss after 10199907 batches: 0.0236
trigger times: 4
Loss after 10200836 batches: 0.0243
trigger times: 5
Loss after 10201765 batches: 0.0252
trigger times: 6
Loss after 10202694 batches: 0.0239
trigger times: 7
Loss after 10203623 batches: 0.0228
trigger times: 8
Loss after 10204552 batches: 0.0241
trigger times: 9
Loss after 10205481 batches: 0.0244
trigger times: 10
Loss after 10206410 batches: 0.0242
trigger times: 11
Loss after 10207339 batches: 0.0226
trigger times: 12
Loss after 10208268 batches: 0.0252
trigger times: 13
Loss after 10209197 batches: 0.0276
trigger times: 14
Loss after 10210126 batches: 0.0274
trigger times: 15
Loss after 10211055 batches: 0.0272
trigger times: 16
Loss after 10211984 batches: 0.0264
trigger times: 17
Loss after 10212913 batches: 0.0257
trigger times: 18
Loss after 10213842 batches: 0.0248
trigger times: 19
Loss after 10214771 batches: 0.0247
trigger times: 20
Loss after 10215700 batches: 0.0239
trigger times: 21
Loss after 10216629 batches: 0.0225
trigger times: 22
Loss after 10217558 batches: 0.0224
trigger times: 23
Loss after 10218487 batches: 0.0217
trigger times: 24
Loss after 10219416 batches: 0.0207
trigger times: 25
Early stopping!
Start to test process.
Loss after 10220345 batches: 0.0219
Time to train on one home:  88.86630153656006
trigger times: 0
Loss after 10221307 batches: 0.0709
trigger times: 1
Loss after 10222269 batches: 0.0656
trigger times: 2
Loss after 10223231 batches: 0.0642
trigger times: 3
Loss after 10224193 batches: 0.0618
trigger times: 4
Loss after 10225155 batches: 0.0593
trigger times: 5
Loss after 10226117 batches: 0.0591
trigger times: 6
Loss after 10227079 batches: 0.0571
trigger times: 7
Loss after 10228041 batches: 0.0566
trigger times: 8
Loss after 10229003 batches: 0.0577
trigger times: 9
Loss after 10229965 batches: 0.0570
trigger times: 10
Loss after 10230927 batches: 0.0565
trigger times: 11
Loss after 10231889 batches: 0.0554
trigger times: 12
Loss after 10232851 batches: 0.0562
trigger times: 13
Loss after 10233813 batches: 0.0552
trigger times: 14
Loss after 10234775 batches: 0.0561
trigger times: 15
Loss after 10235737 batches: 0.0549
trigger times: 16
Loss after 10236699 batches: 0.0543
trigger times: 17
Loss after 10237661 batches: 0.0548
trigger times: 18
Loss after 10238623 batches: 0.0544
trigger times: 19
Loss after 10239585 batches: 0.0533
trigger times: 20
Loss after 10240547 batches: 0.0535
trigger times: 21
Loss after 10241509 batches: 0.0533
trigger times: 22
Loss after 10242471 batches: 0.0542
trigger times: 23
Loss after 10243433 batches: 0.0536
trigger times: 24
Loss after 10244395 batches: 0.0520
trigger times: 25
Early stopping!
Start to test process.
Loss after 10245357 batches: 0.0518
Time to train on one home:  57.169475078582764
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10246320 batches: 0.0588
trigger times: 1
Loss after 10247283 batches: 0.0456
trigger times: 2
Loss after 10248246 batches: 0.0466
trigger times: 0
Loss after 10249209 batches: 0.0440
trigger times: 1
Loss after 10250172 batches: 0.0399
trigger times: 2
Loss after 10251135 batches: 0.0391
trigger times: 3
Loss after 10252098 batches: 0.0381
trigger times: 4
Loss after 10253061 batches: 0.0376
trigger times: 5
Loss after 10254024 batches: 0.0363
trigger times: 6
Loss after 10254987 batches: 0.0362
trigger times: 7
Loss after 10255950 batches: 0.0360
trigger times: 8
Loss after 10256913 batches: 0.0358
trigger times: 9
Loss after 10257876 batches: 0.0357
trigger times: 10
Loss after 10258839 batches: 0.0344
trigger times: 11
Loss after 10259802 batches: 0.0346
trigger times: 12
Loss after 10260765 batches: 0.0339
trigger times: 13
Loss after 10261728 batches: 0.0335
trigger times: 14
Loss after 10262691 batches: 0.0331
trigger times: 15
Loss after 10263654 batches: 0.0327
trigger times: 16
Loss after 10264617 batches: 0.0327
trigger times: 17
Loss after 10265580 batches: 0.0327
trigger times: 18
Loss after 10266543 batches: 0.0326
trigger times: 19
Loss after 10267506 batches: 0.0317
trigger times: 20
Loss after 10268469 batches: 0.0320
trigger times: 21
Loss after 10269432 batches: 0.0319
trigger times: 22
Loss after 10270395 batches: 0.0312
trigger times: 23
Loss after 10271358 batches: 0.0313
trigger times: 24
Loss after 10272321 batches: 0.0308
trigger times: 25
Early stopping!
Start to test process.
Loss after 10273284 batches: 0.0303
Time to train on one home:  56.36460542678833
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10274247 batches: 0.0572
trigger times: 0
Loss after 10275210 batches: 0.0478
trigger times: 0
Loss after 10276173 batches: 0.0468
trigger times: 1
Loss after 10277136 batches: 0.0440
trigger times: 2
Loss after 10278099 batches: 0.0425
trigger times: 3
Loss after 10279062 batches: 0.0405
trigger times: 4
Loss after 10280025 batches: 0.0397
trigger times: 5
Loss after 10280988 batches: 0.0391
trigger times: 6
Loss after 10281951 batches: 0.0392
trigger times: 7
Loss after 10282914 batches: 0.0375
trigger times: 8
Loss after 10283877 batches: 0.0374
trigger times: 9
Loss after 10284840 batches: 0.0361
trigger times: 10
Loss after 10285803 batches: 0.0366
trigger times: 11
Loss after 10286766 batches: 0.0347
trigger times: 12
Loss after 10287729 batches: 0.0348
trigger times: 13
Loss after 10288692 batches: 0.0343
trigger times: 14
Loss after 10289655 batches: 0.0339
trigger times: 15
Loss after 10290618 batches: 0.0333
trigger times: 16
Loss after 10291581 batches: 0.0342
trigger times: 17
Loss after 10292544 batches: 0.0342
trigger times: 18
Loss after 10293507 batches: 0.0327
trigger times: 19
Loss after 10294470 batches: 0.0320
trigger times: 20
Loss after 10295433 batches: 0.0323
trigger times: 21
Loss after 10296396 batches: 0.0329
trigger times: 22
Loss after 10297359 batches: 0.0322
trigger times: 23
Loss after 10298322 batches: 0.0330
trigger times: 24
Loss after 10299285 batches: 0.0368
trigger times: 25
Early stopping!
Start to test process.
Loss after 10300248 batches: 0.0370
Time to train on one home:  56.15334725379944
trigger times: 0
Loss after 10301211 batches: 0.1093
trigger times: 0
Loss after 10302174 batches: 0.0968
trigger times: 1
Loss after 10303137 batches: 0.0938
trigger times: 2
Loss after 10304100 batches: 0.0883
trigger times: 3
Loss after 10305063 batches: 0.0871
trigger times: 4
Loss after 10306026 batches: 0.0837
trigger times: 5
Loss after 10306989 batches: 0.0852
trigger times: 6
Loss after 10307952 batches: 0.0818
trigger times: 7
Loss after 10308915 batches: 0.0817
trigger times: 8
Loss after 10309878 batches: 0.0826
trigger times: 9
Loss after 10310841 batches: 0.0804
trigger times: 10
Loss after 10311804 batches: 0.0772
trigger times: 11
Loss after 10312767 batches: 0.0768
trigger times: 12
Loss after 10313730 batches: 0.0771
trigger times: 13
Loss after 10314693 batches: 0.0762
trigger times: 14
Loss after 10315656 batches: 0.0761
trigger times: 15
Loss after 10316619 batches: 0.0761
trigger times: 16
Loss after 10317582 batches: 0.0737
trigger times: 17
Loss after 10318545 batches: 0.0736
trigger times: 18
Loss after 10319508 batches: 0.0750
trigger times: 19
Loss after 10320471 batches: 0.0746
trigger times: 20
Loss after 10321434 batches: 0.0749
trigger times: 21
Loss after 10322397 batches: 0.0753
trigger times: 22
Loss after 10323360 batches: 0.0734
trigger times: 23
Loss after 10324323 batches: 0.0733
trigger times: 24
Loss after 10325286 batches: 0.0747
trigger times: 25
Early stopping!
Start to test process.
Loss after 10326249 batches: 0.0733
Time to train on one home:  57.949267625808716
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10327212 batches: 0.1080
trigger times: 1
Loss after 10328175 batches: 0.0645
trigger times: 2
Loss after 10329138 batches: 0.0637
trigger times: 0
Loss after 10330101 batches: 0.0535
trigger times: 0
Loss after 10331064 batches: 0.0492
trigger times: 1
Loss after 10332027 batches: 0.0455
trigger times: 2
Loss after 10332990 batches: 0.0431
trigger times: 3
Loss after 10333953 batches: 0.0410
trigger times: 0
Loss after 10334916 batches: 0.0386
trigger times: 1
Loss after 10335879 batches: 0.0390
trigger times: 2
Loss after 10336842 batches: 0.0369
trigger times: 3
Loss after 10337805 batches: 0.0361
trigger times: 0
Loss after 10338768 batches: 0.0362
trigger times: 1
Loss after 10339731 batches: 0.0368
trigger times: 2
Loss after 10340694 batches: 0.0357
trigger times: 3
Loss after 10341657 batches: 0.0345
trigger times: 4
Loss after 10342620 batches: 0.0344
trigger times: 5
Loss after 10343583 batches: 0.0338
trigger times: 6
Loss after 10344546 batches: 0.0341
trigger times: 7
Loss after 10345509 batches: 0.0335
trigger times: 8
Loss after 10346472 batches: 0.0319
trigger times: 0
Loss after 10347435 batches: 0.0315
trigger times: 1
Loss after 10348398 batches: 0.0321
trigger times: 2
Loss after 10349361 batches: 0.0317
trigger times: 3
Loss after 10350324 batches: 0.0314
trigger times: 4
Loss after 10351287 batches: 0.0311
trigger times: 5
Loss after 10352250 batches: 0.0300
trigger times: 6
Loss after 10353213 batches: 0.0305
trigger times: 7
Loss after 10354176 batches: 0.0301
trigger times: 8
Loss after 10355139 batches: 0.0308
trigger times: 9
Loss after 10356102 batches: 0.0305
trigger times: 10
Loss after 10357065 batches: 0.0302
trigger times: 11
Loss after 10358028 batches: 0.0300
trigger times: 12
Loss after 10358991 batches: 0.0295
trigger times: 13
Loss after 10359954 batches: 0.0290
trigger times: 14
Loss after 10360917 batches: 0.0293
trigger times: 15
Loss after 10361880 batches: 0.0279
trigger times: 16
Loss after 10362843 batches: 0.0289
trigger times: 17
Loss after 10363806 batches: 0.0273
trigger times: 18
Loss after 10364769 batches: 0.0275
trigger times: 19
Loss after 10365732 batches: 0.0273
trigger times: 20
Loss after 10366695 batches: 0.0270
trigger times: 21
Loss after 10367658 batches: 0.0279
trigger times: 22
Loss after 10368621 batches: 0.0281
trigger times: 23
Loss after 10369584 batches: 0.0273
trigger times: 24
Loss after 10370547 batches: 0.0270
trigger times: 25
Early stopping!
Start to test process.
Loss after 10371510 batches: 0.0273
Time to train on one home:  73.30266118049622
trigger times: 0
Loss after 10372469 batches: 0.1173
trigger times: 1
Loss after 10373428 batches: 0.0598
trigger times: 0
Loss after 10374387 batches: 0.0450
trigger times: 1
Loss after 10375346 batches: 0.0349
trigger times: 2
Loss after 10376305 batches: 0.0283
trigger times: 3
Loss after 10377264 batches: 0.0268
trigger times: 0
Loss after 10378223 batches: 0.0258
trigger times: 0
Loss after 10379182 batches: 0.0239
trigger times: 1
Loss after 10380141 batches: 0.0228
trigger times: 2
Loss after 10381100 batches: 0.0218
trigger times: 3
Loss after 10382059 batches: 0.0212
trigger times: 4
Loss after 10383018 batches: 0.0206
trigger times: 5
Loss after 10383977 batches: 0.0204
trigger times: 6
Loss after 10384936 batches: 0.0196
trigger times: 7
Loss after 10385895 batches: 0.0193
trigger times: 8
Loss after 10386854 batches: 0.0194
trigger times: 9
Loss after 10387813 batches: 0.0183
trigger times: 10
Loss after 10388772 batches: 0.0187
trigger times: 11
Loss after 10389731 batches: 0.0181
trigger times: 12
Loss after 10390690 batches: 0.0180
trigger times: 13
Loss after 10391649 batches: 0.0179
trigger times: 14
Loss after 10392608 batches: 0.0178
trigger times: 15
Loss after 10393567 batches: 0.0180
trigger times: 16
Loss after 10394526 batches: 0.0182
trigger times: 17
Loss after 10395485 batches: 0.0180
trigger times: 18
Loss after 10396444 batches: 0.0177
trigger times: 19
Loss after 10397403 batches: 0.0177
trigger times: 20
Loss after 10398362 batches: 0.0169
trigger times: 21
Loss after 10399321 batches: 0.0162
trigger times: 22
Loss after 10400280 batches: 0.0170
trigger times: 23
Loss after 10401239 batches: 0.0162
trigger times: 24
Loss after 10402198 batches: 0.0169
trigger times: 25
Early stopping!
Start to test process.
Loss after 10403157 batches: 0.0158
Time to train on one home:  64.34308743476868
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10404120 batches: 0.0499
trigger times: 1
Loss after 10405083 batches: 0.0281
trigger times: 2
Loss after 10406046 batches: 0.0263
trigger times: 3
Loss after 10407009 batches: 0.0261
trigger times: 4
Loss after 10407972 batches: 0.0250
trigger times: 5
Loss after 10408935 batches: 0.0244
trigger times: 6
Loss after 10409898 batches: 0.0237
trigger times: 7
Loss after 10410861 batches: 0.0229
trigger times: 8
Loss after 10411824 batches: 0.0224
trigger times: 9
Loss after 10412787 batches: 0.0219
trigger times: 10
Loss after 10413750 batches: 0.0212
trigger times: 11
Loss after 10414713 batches: 0.0209
trigger times: 12
Loss after 10415676 batches: 0.0201
trigger times: 13
Loss after 10416639 batches: 0.0199
trigger times: 14
Loss after 10417602 batches: 0.0197
trigger times: 15
Loss after 10418565 batches: 0.0193
trigger times: 16
Loss after 10419528 batches: 0.0191
trigger times: 17
Loss after 10420491 batches: 0.0192
trigger times: 18
Loss after 10421454 batches: 0.0191
trigger times: 19
Loss after 10422417 batches: 0.0191
trigger times: 20
Loss after 10423380 batches: 0.0190
trigger times: 21
Loss after 10424343 batches: 0.0186
trigger times: 22
Loss after 10425306 batches: 0.0187
trigger times: 23
Loss after 10426269 batches: 0.0186
trigger times: 24
Loss after 10427232 batches: 0.0183
trigger times: 25
Early stopping!
Start to test process.
Loss after 10428195 batches: 0.0183
Time to train on one home:  56.43389391899109
trigger times: 0
Loss after 10429140 batches: 0.0778
trigger times: 0
Loss after 10430085 batches: 0.0529
trigger times: 0
Loss after 10431030 batches: 0.0441
trigger times: 0
Loss after 10431975 batches: 0.0354
trigger times: 1
Loss after 10432920 batches: 0.0327
trigger times: 2
Loss after 10433865 batches: 0.0300
trigger times: 3
Loss after 10434810 batches: 0.0282
trigger times: 0
Loss after 10435755 batches: 0.0273
trigger times: 1
Loss after 10436700 batches: 0.0259
trigger times: 2
Loss after 10437645 batches: 0.0254
trigger times: 3
Loss after 10438590 batches: 0.0253
trigger times: 4
Loss after 10439535 batches: 0.0266
trigger times: 5
Loss after 10440480 batches: 0.0254
trigger times: 6
Loss after 10441425 batches: 0.0252
trigger times: 7
Loss after 10442370 batches: 0.0232
trigger times: 8
Loss after 10443315 batches: 0.0241
trigger times: 9
Loss after 10444260 batches: 0.0240
trigger times: 10
Loss after 10445205 batches: 0.0223
trigger times: 0
Loss after 10446150 batches: 0.0223
trigger times: 1
Loss after 10447095 batches: 0.0218
trigger times: 2
Loss after 10448040 batches: 0.0208
trigger times: 3
Loss after 10448985 batches: 0.0216
trigger times: 4
Loss after 10449930 batches: 0.0221
trigger times: 5
Loss after 10450875 batches: 0.0220
trigger times: 6
Loss after 10451820 batches: 0.0218
trigger times: 7
Loss after 10452765 batches: 0.0221
trigger times: 8
Loss after 10453710 batches: 0.0213
trigger times: 0
Loss after 10454655 batches: 0.0224
trigger times: 1
Loss after 10455600 batches: 0.0225
trigger times: 2
Loss after 10456545 batches: 0.0214
trigger times: 3
Loss after 10457490 batches: 0.0206
trigger times: 4
Loss after 10458435 batches: 0.0204
trigger times: 5
Loss after 10459380 batches: 0.0205
trigger times: 6
Loss after 10460325 batches: 0.0206
trigger times: 7
Loss after 10461270 batches: 0.0206
trigger times: 8
Loss after 10462215 batches: 0.0193
trigger times: 9
Loss after 10463160 batches: 0.0215
trigger times: 10
Loss after 10464105 batches: 0.0198
trigger times: 11
Loss after 10465050 batches: 0.0196
trigger times: 12
Loss after 10465995 batches: 0.0196
trigger times: 13
Loss after 10466940 batches: 0.0196
trigger times: 14
Loss after 10467885 batches: 0.0195
trigger times: 15
Loss after 10468830 batches: 0.0200
trigger times: 16
Loss after 10469775 batches: 0.0195
trigger times: 17
Loss after 10470720 batches: 0.0203
trigger times: 18
Loss after 10471665 batches: 0.0193
trigger times: 19
Loss after 10472610 batches: 0.0190
trigger times: 20
Loss after 10473555 batches: 0.0195
trigger times: 21
Loss after 10474500 batches: 0.0204
trigger times: 22
Loss after 10475445 batches: 0.0190
trigger times: 23
Loss after 10476390 batches: 0.0195
trigger times: 24
Loss after 10477335 batches: 0.0179
trigger times: 25
Early stopping!
Start to test process.
Loss after 10478280 batches: 0.0180
Time to train on one home:  79.639821767807
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10479217 batches: 0.0891
trigger times: 1
Loss after 10480154 batches: 0.0738
trigger times: 2
Loss after 10481091 batches: 0.0720
trigger times: 3
Loss after 10482028 batches: 0.0668
trigger times: 4
Loss after 10482965 batches: 0.0646
trigger times: 5
Loss after 10483902 batches: 0.0618
trigger times: 6
Loss after 10484839 batches: 0.0606
trigger times: 7
Loss after 10485776 batches: 0.0580
trigger times: 8
Loss after 10486713 batches: 0.0573
trigger times: 9
Loss after 10487650 batches: 0.0571
trigger times: 10
Loss after 10488587 batches: 0.0572
trigger times: 11
Loss after 10489524 batches: 0.0552
trigger times: 12
Loss after 10490461 batches: 0.0546
trigger times: 13
Loss after 10491398 batches: 0.0550
trigger times: 14
Loss after 10492335 batches: 0.0541
trigger times: 15
Loss after 10493272 batches: 0.0544
trigger times: 16
Loss after 10494209 batches: 0.0544
trigger times: 17
Loss after 10495146 batches: 0.0531
trigger times: 18
Loss after 10496083 batches: 0.0529
trigger times: 19
Loss after 10497020 batches: 0.0523
trigger times: 20
Loss after 10497957 batches: 0.0509
trigger times: 21
Loss after 10498894 batches: 0.0514
trigger times: 22
Loss after 10499831 batches: 0.0531
trigger times: 23
Loss after 10500768 batches: 0.0531
trigger times: 24
Loss after 10501705 batches: 0.0527
trigger times: 25
Early stopping!
Start to test process.
Loss after 10502642 batches: 0.0511
Time to train on one home:  56.75424027442932
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 10503605 batches: 0.0244
trigger times: 1
Loss after 10504568 batches: 0.0143
trigger times: 2
Loss after 10505531 batches: 0.0135
trigger times: 3
Loss after 10506494 batches: 0.0135
trigger times: 4
Loss after 10507457 batches: 0.0133
trigger times: 5
Loss after 10508420 batches: 0.0126
trigger times: 6
Loss after 10509383 batches: 0.0118
trigger times: 7
Loss after 10510346 batches: 0.0117
trigger times: 8
Loss after 10511309 batches: 0.0108
trigger times: 9
Loss after 10512272 batches: 0.0106
trigger times: 10
Loss after 10513235 batches: 0.0100
trigger times: 11
Loss after 10514198 batches: 0.0099
trigger times: 12
Loss after 10515161 batches: 0.0094
trigger times: 13
Loss after 10516124 batches: 0.0090
trigger times: 14
Loss after 10517087 batches: 0.0091
trigger times: 15
Loss after 10518050 batches: 0.0087
trigger times: 16
Loss after 10519013 batches: 0.0086
trigger times: 17
Loss after 10519976 batches: 0.0086
trigger times: 18
Loss after 10520939 batches: 0.0083
trigger times: 19
Loss after 10521902 batches: 0.0082
trigger times: 20
Loss after 10522865 batches: 0.0080
trigger times: 21
Loss after 10523828 batches: 0.0079
trigger times: 22
Loss after 10524791 batches: 0.0078
trigger times: 23
Loss after 10525754 batches: 0.0076
trigger times: 24
Loss after 10526717 batches: 0.0075
trigger times: 25
Early stopping!
Start to test process.
Loss after 10527680 batches: 0.0076
Time to train on one home:  53.877277851104736
trigger times: 0
Loss after 10528643 batches: 0.0917
trigger times: 1
Loss after 10529606 batches: 0.0785
trigger times: 2
Loss after 10530569 batches: 0.0733
trigger times: 3
Loss after 10531532 batches: 0.0696
trigger times: 4
Loss after 10532495 batches: 0.0662
trigger times: 5
Loss after 10533458 batches: 0.0645
trigger times: 6
Loss after 10534421 batches: 0.0629
trigger times: 7
Loss after 10535384 batches: 0.0618
trigger times: 8
Loss after 10536347 batches: 0.0609
trigger times: 9
Loss after 10537310 batches: 0.0601
trigger times: 10
Loss after 10538273 batches: 0.0611
trigger times: 11
Loss after 10539236 batches: 0.0613
trigger times: 12
Loss after 10540199 batches: 0.0605
trigger times: 13
Loss after 10541162 batches: 0.0590
trigger times: 14
Loss after 10542125 batches: 0.0594
trigger times: 15
Loss after 10543088 batches: 0.0592
trigger times: 16
Loss after 10544051 batches: 0.0592
trigger times: 17
Loss after 10545014 batches: 0.0583
trigger times: 18
Loss after 10545977 batches: 0.0585
trigger times: 19
Loss after 10546940 batches: 0.0580
trigger times: 20
Loss after 10547903 batches: 0.0580
trigger times: 21
Loss after 10548866 batches: 0.0563
trigger times: 22
Loss after 10549829 batches: 0.0572
trigger times: 23
Loss after 10550792 batches: 0.0567
trigger times: 24
Loss after 10551755 batches: 0.0568
trigger times: 25
Early stopping!
Start to test process.
Loss after 10552718 batches: 0.0572
Time to train on one home:  54.44712734222412
trigger times: 0
Loss after 10553681 batches: 0.0660
trigger times: 1
Loss after 10554644 batches: 0.0537
trigger times: 2
Loss after 10555607 batches: 0.0491
trigger times: 3
Loss after 10556570 batches: 0.0470
trigger times: 4
Loss after 10557533 batches: 0.0432
trigger times: 5
Loss after 10558496 batches: 0.0431
trigger times: 6
Loss after 10559459 batches: 0.0407
trigger times: 7
Loss after 10560422 batches: 0.0405
trigger times: 8
Loss after 10561385 batches: 0.0403
trigger times: 9
Loss after 10562348 batches: 0.0392
trigger times: 10
Loss after 10563311 batches: 0.0389
trigger times: 11
Loss after 10564274 batches: 0.0395
trigger times: 12
Loss after 10565237 batches: 0.0376
trigger times: 13
Loss after 10566200 batches: 0.0375
trigger times: 14
Loss after 10567163 batches: 0.0368
trigger times: 15
Loss after 10568126 batches: 0.0361
trigger times: 16
Loss after 10569089 batches: 0.0364
trigger times: 17
Loss after 10570052 batches: 0.0353
trigger times: 18
Loss after 10571015 batches: 0.0348
trigger times: 19
Loss after 10571978 batches: 0.0349
trigger times: 20
Loss after 10572941 batches: 0.0333
trigger times: 21
Loss after 10573904 batches: 0.0343
trigger times: 22
Loss after 10574867 batches: 0.0331
trigger times: 23
Loss after 10575830 batches: 0.0326
trigger times: 24
Loss after 10576793 batches: 0.0328
trigger times: 25
Early stopping!
Start to test process.
Loss after 10577756 batches: 0.0328
Time to train on one home:  56.84549260139465
trigger times: 0
Loss after 10578652 batches: 0.1019
trigger times: 1
Loss after 10579548 batches: 0.0919
trigger times: 2
Loss after 10580444 batches: 0.0886
trigger times: 3
Loss after 10581340 batches: 0.0837
trigger times: 4
Loss after 10582236 batches: 0.0773
trigger times: 5
Loss after 10583132 batches: 0.0745
trigger times: 6
Loss after 10584028 batches: 0.0717
trigger times: 7
Loss after 10584924 batches: 0.0695
trigger times: 8
Loss after 10585820 batches: 0.0682
trigger times: 9
Loss after 10586716 batches: 0.0670
trigger times: 10
Loss after 10587612 batches: 0.0659
trigger times: 11
Loss after 10588508 batches: 0.0642
trigger times: 12
Loss after 10589404 batches: 0.0642
trigger times: 13
Loss after 10590300 batches: 0.0642
trigger times: 14
Loss after 10591196 batches: 0.0652
trigger times: 15
Loss after 10592092 batches: 0.0640
trigger times: 16
Loss after 10592988 batches: 0.0622
trigger times: 17
Loss after 10593884 batches: 0.0620
trigger times: 18
Loss after 10594780 batches: 0.0616
trigger times: 19
Loss after 10595676 batches: 0.0616
trigger times: 20
Loss after 10596572 batches: 0.0615
trigger times: 21
Loss after 10597468 batches: 0.0587
trigger times: 22
Loss after 10598364 batches: 0.0610
trigger times: 23
Loss after 10599260 batches: 0.0596
trigger times: 24
Loss after 10600156 batches: 0.0595
trigger times: 25
Early stopping!
Start to test process.
Loss after 10601052 batches: 0.0591
Time to train on one home:  52.61085081100464
trigger times: 0
Loss after 10602015 batches: 0.1543
trigger times: 1
Loss after 10602978 batches: 0.1151
trigger times: 2
Loss after 10603941 batches: 0.1005
trigger times: 3
Loss after 10604904 batches: 0.0921
trigger times: 4
Loss after 10605867 batches: 0.0853
trigger times: 5
Loss after 10606830 batches: 0.0821
trigger times: 6
Loss after 10607793 batches: 0.0770
trigger times: 7
Loss after 10608756 batches: 0.0747
trigger times: 8
Loss after 10609719 batches: 0.0711
trigger times: 9
Loss after 10610682 batches: 0.0688
trigger times: 10
Loss after 10611645 batches: 0.0643
trigger times: 11
Loss after 10612608 batches: 0.0614
trigger times: 12
Loss after 10613571 batches: 0.0587
trigger times: 13
Loss after 10614534 batches: 0.0564
trigger times: 14
Loss after 10615497 batches: 0.0551
trigger times: 15
Loss after 10616460 batches: 0.0529
trigger times: 16
Loss after 10617423 batches: 0.0520
trigger times: 17
Loss after 10618386 batches: 0.0508
trigger times: 18
Loss after 10619349 batches: 0.0493
trigger times: 19
Loss after 10620312 batches: 0.0505
trigger times: 20
Loss after 10621275 batches: 0.0508
trigger times: 21
Loss after 10622238 batches: 0.0484
trigger times: 22
Loss after 10623201 batches: 0.0486
trigger times: 23
Loss after 10624164 batches: 0.0465
trigger times: 24
Loss after 10625127 batches: 0.0458
trigger times: 25
Early stopping!
Start to test process.
Loss after 10626090 batches: 0.0461
Time to train on one home:  60.885401487350464
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10627053 batches: 0.0850
trigger times: 1
Loss after 10628016 batches: 0.0771
trigger times: 2
Loss after 10628979 batches: 0.0747
trigger times: 3
Loss after 10629942 batches: 0.0732
trigger times: 4
Loss after 10630905 batches: 0.0710
trigger times: 5
Loss after 10631868 batches: 0.0682
trigger times: 6
Loss after 10632831 batches: 0.0659
trigger times: 7
Loss after 10633794 batches: 0.0626
trigger times: 8
Loss after 10634757 batches: 0.0613
trigger times: 9
Loss after 10635720 batches: 0.0615
trigger times: 10
Loss after 10636683 batches: 0.0573
trigger times: 11
Loss after 10637646 batches: 0.0568
trigger times: 12
Loss after 10638609 batches: 0.0574
trigger times: 13
Loss after 10639572 batches: 0.0575
trigger times: 14
Loss after 10640535 batches: 0.0561
trigger times: 15
Loss after 10641498 batches: 0.0540
trigger times: 16
Loss after 10642461 batches: 0.0544
trigger times: 17
Loss after 10643424 batches: 0.0552
trigger times: 18
Loss after 10644387 batches: 0.0589
trigger times: 19
Loss after 10645350 batches: 0.0566
trigger times: 20
Loss after 10646313 batches: 0.0544
trigger times: 21
Loss after 10647276 batches: 0.0550
trigger times: 22
Loss after 10648239 batches: 0.0522
trigger times: 23
Loss after 10649202 batches: 0.0507
trigger times: 24
Loss after 10650165 batches: 0.0516
trigger times: 25
Early stopping!
Start to test process.
Loss after 10651128 batches: 0.0518
Time to train on one home:  56.729676961898804
trigger times: 0
Loss after 10652091 batches: 0.0953
trigger times: 0
Loss after 10653054 batches: 0.0670
trigger times: 1
Loss after 10654017 batches: 0.0619
trigger times: 2
Loss after 10654980 batches: 0.0556
trigger times: 3
Loss after 10655943 batches: 0.0520
trigger times: 4
Loss after 10656906 batches: 0.0501
trigger times: 5
Loss after 10657869 batches: 0.0470
trigger times: 6
Loss after 10658832 batches: 0.0474
trigger times: 7
Loss after 10659795 batches: 0.0466
trigger times: 8
Loss after 10660758 batches: 0.0464
trigger times: 9
Loss after 10661721 batches: 0.0446
trigger times: 10
Loss after 10662684 batches: 0.0436
trigger times: 11
Loss after 10663647 batches: 0.0436
trigger times: 12
Loss after 10664610 batches: 0.0446
trigger times: 13
Loss after 10665573 batches: 0.0438
trigger times: 14
Loss after 10666536 batches: 0.0424
trigger times: 15
Loss after 10667499 batches: 0.0421
trigger times: 16
Loss after 10668462 batches: 0.0415
trigger times: 17
Loss after 10669425 batches: 0.0411
trigger times: 18
Loss after 10670388 batches: 0.0416
trigger times: 19
Loss after 10671351 batches: 0.0414
trigger times: 20
Loss after 10672314 batches: 0.0411
trigger times: 21
Loss after 10673277 batches: 0.0410
trigger times: 22
Loss after 10674240 batches: 0.0399
trigger times: 23
Loss after 10675203 batches: 0.0405
trigger times: 24
Loss after 10676166 batches: 0.0403
trigger times: 25
Early stopping!
Start to test process.
Loss after 10677129 batches: 0.0395
Time to train on one home:  57.33652663230896
trigger times: 0
Loss after 10678092 batches: 0.0535
trigger times: 1
Loss after 10679055 batches: 0.0415
trigger times: 2
Loss after 10680018 batches: 0.0398
trigger times: 3
Loss after 10680981 batches: 0.0359
trigger times: 4
Loss after 10681944 batches: 0.0319
trigger times: 5
Loss after 10682907 batches: 0.0301
trigger times: 6
Loss after 10683870 batches: 0.0285
trigger times: 7
Loss after 10684833 batches: 0.0277
trigger times: 8
Loss after 10685796 batches: 0.0283
trigger times: 9
Loss after 10686759 batches: 0.0275
trigger times: 10
Loss after 10687722 batches: 0.0265
trigger times: 11
Loss after 10688685 batches: 0.0258
trigger times: 12
Loss after 10689648 batches: 0.0248
trigger times: 13
Loss after 10690611 batches: 0.0248
trigger times: 14
Loss after 10691574 batches: 0.0247
trigger times: 15
Loss after 10692537 batches: 0.0242
trigger times: 16
Loss after 10693500 batches: 0.0239
trigger times: 17
Loss after 10694463 batches: 0.0237
trigger times: 18
Loss after 10695426 batches: 0.0241
trigger times: 19
Loss after 10696389 batches: 0.0236
trigger times: 20
Loss after 10697352 batches: 0.0237
trigger times: 21
Loss after 10698315 batches: 0.0232
trigger times: 22
Loss after 10699278 batches: 0.0235
trigger times: 23
Loss after 10700241 batches: 0.0228
trigger times: 24
Loss after 10701204 batches: 0.0233
trigger times: 25
Early stopping!
Start to test process.
Loss after 10702167 batches: 0.0226
Time to train on one home:  57.36150860786438
trigger times: 0
Loss after 10703130 batches: 0.0605
trigger times: 1
Loss after 10704093 batches: 0.0464
trigger times: 2
Loss after 10705056 batches: 0.0464
trigger times: 3
Loss after 10706019 batches: 0.0442
trigger times: 4
Loss after 10706982 batches: 0.0417
trigger times: 5
Loss after 10707945 batches: 0.0400
trigger times: 6
Loss after 10708908 batches: 0.0394
trigger times: 7
Loss after 10709871 batches: 0.0391
trigger times: 8
Loss after 10710834 batches: 0.0381
trigger times: 9
Loss after 10711797 batches: 0.0384
trigger times: 10
Loss after 10712760 batches: 0.0372
trigger times: 11
Loss after 10713723 batches: 0.0365
trigger times: 12
Loss after 10714686 batches: 0.0366
trigger times: 13
Loss after 10715649 batches: 0.0357
trigger times: 14
Loss after 10716612 batches: 0.0359
trigger times: 15
Loss after 10717575 batches: 0.0355
trigger times: 16
Loss after 10718538 batches: 0.0352
trigger times: 17
Loss after 10719501 batches: 0.0361
trigger times: 18
Loss after 10720464 batches: 0.0360
trigger times: 19
Loss after 10721427 batches: 0.0352
trigger times: 20
Loss after 10722390 batches: 0.0352
trigger times: 21
Loss after 10723353 batches: 0.0350
trigger times: 22
Loss after 10724316 batches: 0.0349
trigger times: 23
Loss after 10725279 batches: 0.0350
trigger times: 24
Loss after 10726242 batches: 0.0347
trigger times: 25
Early stopping!
Start to test process.
Loss after 10727205 batches: 0.0344
Time to train on one home:  56.82610583305359
trigger times: 0
Loss after 10728100 batches: 0.0818
trigger times: 1
Loss after 10728995 batches: 0.0465
trigger times: 0
Loss after 10729890 batches: 0.0230
trigger times: 0
Loss after 10730785 batches: 0.0137
trigger times: 0
Loss after 10731680 batches: 0.0114
trigger times: 0
Loss after 10732575 batches: 0.0082
trigger times: 0
Loss after 10733470 batches: 0.0064
trigger times: 1
Loss after 10734365 batches: 0.0056
trigger times: 0
Loss after 10735260 batches: 0.0051
trigger times: 1
Loss after 10736155 batches: 0.0046
trigger times: 0
Loss after 10737050 batches: 0.0043
trigger times: 1
Loss after 10737945 batches: 0.0042
trigger times: 0
Loss after 10738840 batches: 0.0033
trigger times: 1
Loss after 10739735 batches: 0.0036
trigger times: 2
Loss after 10740630 batches: 0.0030
trigger times: 0
Loss after 10741525 batches: 0.0032
trigger times: 0
Loss after 10742420 batches: 0.0034
trigger times: 1
Loss after 10743315 batches: 0.0033
trigger times: 0
Loss after 10744210 batches: 0.0033
trigger times: 1
Loss after 10745105 batches: 0.0033
trigger times: 2
Loss after 10746000 batches: 0.0030
trigger times: 3
Loss after 10746895 batches: 0.0028
trigger times: 4
Loss after 10747790 batches: 0.0029
trigger times: 0
Loss after 10748685 batches: 0.0029
trigger times: 1
Loss after 10749580 batches: 0.0029
trigger times: 2
Loss after 10750475 batches: 0.0031
trigger times: 3
Loss after 10751370 batches: 0.0026
trigger times: 4
Loss after 10752265 batches: 0.0029
trigger times: 5
Loss after 10753160 batches: 0.0043
trigger times: 6
Loss after 10754055 batches: 0.0034
trigger times: 7
Loss after 10754950 batches: 0.0029
trigger times: 8
Loss after 10755845 batches: 0.0028
trigger times: 9
Loss after 10756740 batches: 0.0027
trigger times: 10
Loss after 10757635 batches: 0.0026
trigger times: 11
Loss after 10758530 batches: 0.0032
trigger times: 12
Loss after 10759425 batches: 0.0056
trigger times: 13
Loss after 10760320 batches: 0.0044
trigger times: 14
Loss after 10761215 batches: 0.0042
trigger times: 15
Loss after 10762110 batches: 0.0044
trigger times: 16
Loss after 10763005 batches: 0.0036
trigger times: 17
Loss after 10763900 batches: 0.0034
trigger times: 18
Loss after 10764795 batches: 0.0033
trigger times: 19
Loss after 10765690 batches: 0.0028
trigger times: 20
Loss after 10766585 batches: 0.0027
trigger times: 21
Loss after 10767480 batches: 0.0024
trigger times: 22
Loss after 10768375 batches: 0.0026
trigger times: 23
Loss after 10769270 batches: 0.0023
trigger times: 24
Loss after 10770165 batches: 0.0022
trigger times: 25
Early stopping!
Start to test process.
Loss after 10771060 batches: 0.0020
Time to train on one home:  68.82362222671509
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336, 0.034888222413376094]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171], [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71]]
Round_8_results:  [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 11294 < 11295; dropping {'Training_Loss': 0.053629672420876365, 'Validation_Loss': 0.06310395896434784, 'Training_R2': 0.28585280039772276, 'Validation_R2': 0.11214227558261647, 'Training_F1': 0.49593278490796966, 'Validation_F1': 0.30944729322266795, 'Training_NEP': 0.7915177863837861, 'Validation_NEP': 0.8928244511677274, 'Training_NDE': 0.5259368672142608, 'Validation_NDE': 0.6889757396021733, 'Training_MAE': 22.791888668952055, 'Validation_MAE': 29.740191258116745, 'Training_MSE': 1654.6921, 'Validation_MSE': 3412.772}.
trigger times: 0
Loss after 10772023 batches: 0.0536
trigger times: 1
Loss after 10772986 batches: 0.0448
trigger times: 0
Loss after 10773949 batches: 0.0414
trigger times: 1
Loss after 10774912 batches: 0.0392
trigger times: 2
Loss after 10775875 batches: 0.0378
trigger times: 3
Loss after 10776838 batches: 0.0360
trigger times: 4
Loss after 10777801 batches: 0.0357
trigger times: 5
Loss after 10778764 batches: 0.0357
trigger times: 6
Loss after 10779727 batches: 0.0358
trigger times: 7
Loss after 10780690 batches: 0.0338
trigger times: 8
Loss after 10781653 batches: 0.0324
trigger times: 9
Loss after 10782616 batches: 0.0336
trigger times: 10
Loss after 10783579 batches: 0.0334
trigger times: 11
Loss after 10784542 batches: 0.0322
trigger times: 12
Loss after 10785505 batches: 0.0325
trigger times: 13
Loss after 10786468 batches: 0.0333
trigger times: 14
Loss after 10787431 batches: 0.0312
trigger times: 15
Loss after 10788394 batches: 0.0305
trigger times: 16
Loss after 10789357 batches: 0.0312
trigger times: 17
Loss after 10790320 batches: 0.0315
trigger times: 18
Loss after 10791283 batches: 0.0313
trigger times: 19
Loss after 10792246 batches: 0.0308
trigger times: 20
Loss after 10793209 batches: 0.0299
trigger times: 21
Loss after 10794172 batches: 0.0301
trigger times: 22
Loss after 10795135 batches: 0.0297
trigger times: 23
Loss after 10796098 batches: 0.0279
trigger times: 24
Loss after 10797061 batches: 0.0303
trigger times: 25
Early stopping!
Start to test process.
Loss after 10798024 batches: 0.0307
Time to train on one home:  59.031139612197876
trigger times: 0
Loss after 10798982 batches: 0.0643
trigger times: 0
Loss after 10799940 batches: 0.0412
trigger times: 1
Loss after 10800898 batches: 0.0372
trigger times: 0
Loss after 10801856 batches: 0.0306
trigger times: 1
Loss after 10802814 batches: 0.0277
trigger times: 2
Loss after 10803772 batches: 0.0258
trigger times: 3
Loss after 10804730 batches: 0.0247
trigger times: 4
Loss after 10805688 batches: 0.0239
trigger times: 5
Loss after 10806646 batches: 0.0228
trigger times: 6
Loss after 10807604 batches: 0.0246
trigger times: 7
Loss after 10808562 batches: 0.0231
trigger times: 8
Loss after 10809520 batches: 0.0214
trigger times: 9
Loss after 10810478 batches: 0.0214
trigger times: 10
Loss after 10811436 batches: 0.0209
trigger times: 11
Loss after 10812394 batches: 0.0194
trigger times: 12
Loss after 10813352 batches: 0.0194
trigger times: 13
Loss after 10814310 batches: 0.0187
trigger times: 14
Loss after 10815268 batches: 0.0186
trigger times: 15
Loss after 10816226 batches: 0.0177
trigger times: 16
Loss after 10817184 batches: 0.0187
trigger times: 17
Loss after 10818142 batches: 0.0187
trigger times: 18
Loss after 10819100 batches: 0.0186
trigger times: 19
Loss after 10820058 batches: 0.0176
trigger times: 20
Loss after 10821016 batches: 0.0174
trigger times: 21
Loss after 10821974 batches: 0.0172
trigger times: 22
Loss after 10822932 batches: 0.0167
trigger times: 23
Loss after 10823890 batches: 0.0188
trigger times: 24
Loss after 10824848 batches: 0.0203
trigger times: 25
Early stopping!
Start to test process.
Loss after 10825806 batches: 0.0199
Time to train on one home:  58.82352042198181
trigger times: 0
Loss after 10826769 batches: 0.0994
trigger times: 1
Loss after 10827732 batches: 0.0715
trigger times: 2
Loss after 10828695 batches: 0.0671
trigger times: 3
Loss after 10829658 batches: 0.0668
trigger times: 4
Loss after 10830621 batches: 0.0649
trigger times: 5
Loss after 10831584 batches: 0.0622
trigger times: 6
Loss after 10832547 batches: 0.0594
trigger times: 7
Loss after 10833510 batches: 0.0578
trigger times: 8
Loss after 10834473 batches: 0.0553
trigger times: 9
Loss after 10835436 batches: 0.0563
trigger times: 10
Loss after 10836399 batches: 0.0541
trigger times: 11
Loss after 10837362 batches: 0.0532
trigger times: 12
Loss after 10838325 batches: 0.0525
trigger times: 13
Loss after 10839288 batches: 0.0510
trigger times: 14
Loss after 10840251 batches: 0.0511
trigger times: 15
Loss after 10841214 batches: 0.0507
trigger times: 16
Loss after 10842177 batches: 0.0493
trigger times: 17
Loss after 10843140 batches: 0.0486
trigger times: 18
Loss after 10844103 batches: 0.0488
trigger times: 19
Loss after 10845066 batches: 0.0477
trigger times: 20
Loss after 10846029 batches: 0.0481
trigger times: 21
Loss after 10846992 batches: 0.0466
trigger times: 22
Loss after 10847955 batches: 0.0464
trigger times: 23
Loss after 10848918 batches: 0.0464
trigger times: 24
Loss after 10849881 batches: 0.0459
trigger times: 25
Early stopping!
Start to test process.
Loss after 10850844 batches: 0.0464
Time to train on one home:  54.97512698173523
trigger times: 0
Loss after 10851807 batches: 0.0981
trigger times: 1
Loss after 10852770 batches: 0.0774
trigger times: 2
Loss after 10853733 batches: 0.0768
trigger times: 3
Loss after 10854696 batches: 0.0755
trigger times: 4
Loss after 10855659 batches: 0.0727
trigger times: 5
Loss after 10856622 batches: 0.0694
trigger times: 6
Loss after 10857585 batches: 0.0683
trigger times: 7
Loss after 10858548 batches: 0.0670
trigger times: 8
Loss after 10859511 batches: 0.0651
trigger times: 9
Loss after 10860474 batches: 0.0629
trigger times: 10
Loss after 10861437 batches: 0.0619
trigger times: 11
Loss after 10862400 batches: 0.0622
trigger times: 12
Loss after 10863363 batches: 0.0619
trigger times: 13
Loss after 10864326 batches: 0.0614
trigger times: 14
Loss after 10865289 batches: 0.0611
trigger times: 15
Loss after 10866252 batches: 0.0600
trigger times: 16
Loss after 10867215 batches: 0.0596
trigger times: 17
Loss after 10868178 batches: 0.0591
trigger times: 18
Loss after 10869141 batches: 0.0598
trigger times: 19
Loss after 10870104 batches: 0.0592
trigger times: 20
Loss after 10871067 batches: 0.0598
trigger times: 21
Loss after 10872030 batches: 0.0579
trigger times: 22
Loss after 10872993 batches: 0.0597
trigger times: 23
Loss after 10873956 batches: 0.0588
trigger times: 24
Loss after 10874919 batches: 0.0603
trigger times: 25
Early stopping!
Start to test process.
Loss after 10875882 batches: 0.0580
Time to train on one home:  58.52979588508606
trigger times: 0
Loss after 10876845 batches: 0.0262
trigger times: 1
Loss after 10877808 batches: 0.0218
trigger times: 2
Loss after 10878771 batches: 0.0205
trigger times: 3
Loss after 10879734 batches: 0.0185
trigger times: 4
Loss after 10880697 batches: 0.0178
trigger times: 5
Loss after 10881660 batches: 0.0169
trigger times: 6
Loss after 10882623 batches: 0.0160
trigger times: 7
Loss after 10883586 batches: 0.0159
trigger times: 8
Loss after 10884549 batches: 0.0155
trigger times: 9
Loss after 10885512 batches: 0.0159
trigger times: 10
Loss after 10886475 batches: 0.0152
trigger times: 11
Loss after 10887438 batches: 0.0151
trigger times: 12
Loss after 10888401 batches: 0.0144
trigger times: 13
Loss after 10889364 batches: 0.0144
trigger times: 14
Loss after 10890327 batches: 0.0141
trigger times: 15
Loss after 10891290 batches: 0.0145
trigger times: 16
Loss after 10892253 batches: 0.0145
trigger times: 17
Loss after 10893216 batches: 0.0147
trigger times: 18
Loss after 10894179 batches: 0.0141
trigger times: 19
Loss after 10895142 batches: 0.0144
trigger times: 20
Loss after 10896105 batches: 0.0131
trigger times: 21
Loss after 10897068 batches: 0.0131
trigger times: 22
Loss after 10898031 batches: 0.0136
trigger times: 23
Loss after 10898994 batches: 0.0140
trigger times: 24
Loss after 10899957 batches: 0.0138
trigger times: 25
Early stopping!
Start to test process.
Loss after 10900920 batches: 0.0136
Time to train on one home:  56.743995904922485
trigger times: 0
Loss after 10901883 batches: 0.0876
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 10902846 batches: 0.0268
trigger times: 2
Loss after 10903809 batches: 0.0199
trigger times: 3
Loss after 10904772 batches: 0.0185
trigger times: 4
Loss after 10905735 batches: 0.0179
trigger times: 5
Loss after 10906698 batches: 0.0165
trigger times: 6
Loss after 10907661 batches: 0.0152
trigger times: 7
Loss after 10908624 batches: 0.0150
trigger times: 8
Loss after 10909587 batches: 0.0144
trigger times: 9
Loss after 10910550 batches: 0.0141
trigger times: 10
Loss after 10911513 batches: 0.0139
trigger times: 11
Loss after 10912476 batches: 0.0136
trigger times: 12
Loss after 10913439 batches: 0.0137
trigger times: 13
Loss after 10914402 batches: 0.0135
trigger times: 14
Loss after 10915365 batches: 0.0132
trigger times: 15
Loss after 10916328 batches: 0.0133
trigger times: 16
Loss after 10917291 batches: 0.0131
trigger times: 17
Loss after 10918254 batches: 0.0131
trigger times: 18
Loss after 10919217 batches: 0.0129
trigger times: 19
Loss after 10920180 batches: 0.0129
trigger times: 20
Loss after 10921143 batches: 0.0127
trigger times: 21
Loss after 10922106 batches: 0.0126
trigger times: 22
Loss after 10923069 batches: 0.0131
trigger times: 23
Loss after 10924032 batches: 0.0127
trigger times: 24
Loss after 10924995 batches: 0.0127
trigger times: 25
Early stopping!
Start to test process.
Loss after 10925958 batches: 0.0126
Time to train on one home:  57.09570240974426
trigger times: 0
Loss after 10926921 batches: 0.0978
trigger times: 1
Loss after 10927884 batches: 0.0912
trigger times: 0
Loss after 10928847 batches: 0.0857
trigger times: 1
Loss after 10929810 batches: 0.0829
trigger times: 2
Loss after 10930773 batches: 0.0803
trigger times: 3
Loss after 10931736 batches: 0.0784
trigger times: 4
Loss after 10932699 batches: 0.0766
trigger times: 5
Loss after 10933662 batches: 0.0757
trigger times: 6
Loss after 10934625 batches: 0.0731
trigger times: 7
Loss after 10935588 batches: 0.0723
trigger times: 8
Loss after 10936551 batches: 0.0710
trigger times: 9
Loss after 10937514 batches: 0.0724
trigger times: 10
Loss after 10938477 batches: 0.0700
trigger times: 11
Loss after 10939440 batches: 0.0690
trigger times: 12
Loss after 10940403 batches: 0.0692
trigger times: 13
Loss after 10941366 batches: 0.0675
trigger times: 14
Loss after 10942329 batches: 0.0683
trigger times: 15
Loss after 10943292 batches: 0.0671
trigger times: 16
Loss after 10944255 batches: 0.0668
trigger times: 17
Loss after 10945218 batches: 0.0660
trigger times: 18
Loss after 10946181 batches: 0.0686
trigger times: 19
Loss after 10947144 batches: 0.0654
trigger times: 20
Loss after 10948107 batches: 0.0636
trigger times: 21
Loss after 10949070 batches: 0.0659
trigger times: 22
Loss after 10950033 batches: 0.0647
trigger times: 23
Loss after 10950996 batches: 0.0647
trigger times: 24
Loss after 10951959 batches: 0.0630
trigger times: 25
Early stopping!
Start to test process.
Loss after 10952922 batches: 0.0635
Time to train on one home:  59.28983759880066
trigger times: 0
Loss after 10953885 batches: 0.0585
trigger times: 1
Loss after 10954848 batches: 0.0448
trigger times: 0
Loss after 10955811 batches: 0.0373
trigger times: 1
Loss after 10956774 batches: 0.0325
trigger times: 2
Loss after 10957737 batches: 0.0300
trigger times: 3
Loss after 10958700 batches: 0.0288
trigger times: 4
Loss after 10959663 batches: 0.0272
trigger times: 5
Loss after 10960626 batches: 0.0261
trigger times: 6
Loss after 10961589 batches: 0.0261
trigger times: 7
Loss after 10962552 batches: 0.0263
trigger times: 8
Loss after 10963515 batches: 0.0244
trigger times: 9
Loss after 10964478 batches: 0.0234
trigger times: 10
Loss after 10965441 batches: 0.0232
trigger times: 11
Loss after 10966404 batches: 0.0242
trigger times: 12
Loss after 10967367 batches: 0.0235
trigger times: 13
Loss after 10968330 batches: 0.0240
trigger times: 14
Loss after 10969293 batches: 0.0228
trigger times: 15
Loss after 10970256 batches: 0.0224
trigger times: 16
Loss after 10971219 batches: 0.0215
trigger times: 17
Loss after 10972182 batches: 0.0223
trigger times: 18
Loss after 10973145 batches: 0.0216
trigger times: 19
Loss after 10974108 batches: 0.0208
trigger times: 20
Loss after 10975071 batches: 0.0222
trigger times: 21
Loss after 10976034 batches: 0.0217
trigger times: 22
Loss after 10976997 batches: 0.0217
trigger times: 23
Loss after 10977960 batches: 0.0222
trigger times: 24
Loss after 10978923 batches: 0.0223
trigger times: 25
Early stopping!
Start to test process.
Loss after 10979886 batches: 0.0219
Time to train on one home:  57.01532006263733
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10980849 batches: 0.0870
trigger times: 1
Loss after 10981812 batches: 0.0751
trigger times: 2
Loss after 10982775 batches: 0.0732
trigger times: 3
Loss after 10983738 batches: 0.0686
trigger times: 4
Loss after 10984701 batches: 0.0665
trigger times: 5
Loss after 10985664 batches: 0.0648
trigger times: 6
Loss after 10986627 batches: 0.0646
trigger times: 7
Loss after 10987590 batches: 0.0636
trigger times: 8
Loss after 10988553 batches: 0.0643
trigger times: 9
Loss after 10989516 batches: 0.0638
trigger times: 10
Loss after 10990479 batches: 0.0615
trigger times: 11
Loss after 10991442 batches: 0.0599
trigger times: 12
Loss after 10992405 batches: 0.0595
trigger times: 13
Loss after 10993368 batches: 0.0592
trigger times: 14
Loss after 10994331 batches: 0.0603
trigger times: 15
Loss after 10995294 batches: 0.0593
trigger times: 16
Loss after 10996257 batches: 0.0595
trigger times: 17
Loss after 10997220 batches: 0.0582
trigger times: 18
Loss after 10998183 batches: 0.0568
trigger times: 19
Loss after 10999146 batches: 0.0567
trigger times: 20
Loss after 11000109 batches: 0.0575
trigger times: 21
Loss after 11001072 batches: 0.0576
trigger times: 22
Loss after 11002035 batches: 0.0567
trigger times: 23
Loss after 11002998 batches: 0.0580
trigger times: 24
Loss after 11003961 batches: 0.0557
trigger times: 25
Early stopping!
Start to test process.
Loss after 11004924 batches: 0.0571
Time to train on one home:  53.24838423728943
trigger times: 0
Loss after 11005887 batches: 0.0919
trigger times: 0
Loss after 11006850 batches: 0.0643
trigger times: 0
Loss after 11007813 batches: 0.0584
trigger times: 1
Loss after 11008776 batches: 0.0547
trigger times: 2
Loss after 11009739 batches: 0.0511
trigger times: 3
Loss after 11010702 batches: 0.0478
trigger times: 4
Loss after 11011665 batches: 0.0480
trigger times: 5
Loss after 11012628 batches: 0.0460
trigger times: 6
Loss after 11013591 batches: 0.0457
trigger times: 7
Loss after 11014554 batches: 0.0450
trigger times: 8
Loss after 11015517 batches: 0.0446
trigger times: 9
Loss after 11016480 batches: 0.0428
trigger times: 10
Loss after 11017443 batches: 0.0421
trigger times: 11
Loss after 11018406 batches: 0.0415
trigger times: 12
Loss after 11019369 batches: 0.0420
trigger times: 13
Loss after 11020332 batches: 0.0413
trigger times: 14
Loss after 11021295 batches: 0.0399
trigger times: 15
Loss after 11022258 batches: 0.0415
trigger times: 16
Loss after 11023221 batches: 0.0412
trigger times: 17
Loss after 11024184 batches: 0.0403
trigger times: 18
Loss after 11025147 batches: 0.0410
trigger times: 19
Loss after 11026110 batches: 0.0405
trigger times: 20
Loss after 11027073 batches: 0.0406
trigger times: 21
Loss after 11028036 batches: 0.0397
trigger times: 22
Loss after 11028999 batches: 0.0383
trigger times: 23
Loss after 11029962 batches: 0.0389
trigger times: 24
Loss after 11030925 batches: 0.0386
trigger times: 25
Early stopping!
Start to test process.
Loss after 11031888 batches: 0.0391
Time to train on one home:  58.725627183914185
trigger times: 0
Loss after 11032851 batches: 0.0753
trigger times: 1
Loss after 11033814 batches: 0.0698
trigger times: 2
Loss after 11034777 batches: 0.0679
trigger times: 3
Loss after 11035740 batches: 0.0649
trigger times: 4
Loss after 11036703 batches: 0.0629
trigger times: 5
Loss after 11037666 batches: 0.0632
trigger times: 6
Loss after 11038629 batches: 0.0608
trigger times: 7
Loss after 11039592 batches: 0.0604
trigger times: 8
Loss after 11040555 batches: 0.0603
trigger times: 9
Loss after 11041518 batches: 0.0591
trigger times: 10
Loss after 11042481 batches: 0.0585
trigger times: 11
Loss after 11043444 batches: 0.0569
trigger times: 12
Loss after 11044407 batches: 0.0570
trigger times: 13
Loss after 11045370 batches: 0.0562
trigger times: 14
Loss after 11046333 batches: 0.0560
trigger times: 15
Loss after 11047296 batches: 0.0550
trigger times: 16
Loss after 11048259 batches: 0.0548
trigger times: 17
Loss after 11049222 batches: 0.0538
trigger times: 18
Loss after 11050185 batches: 0.0529
trigger times: 19
Loss after 11051148 batches: 0.0550
trigger times: 20
Loss after 11052111 batches: 0.0540
trigger times: 21
Loss after 11053074 batches: 0.0532
trigger times: 22
Loss after 11054037 batches: 0.0521
trigger times: 23
Loss after 11055000 batches: 0.0531
trigger times: 24
Loss after 11055963 batches: 0.0527
trigger times: 25
Early stopping!
Start to test process.
Loss after 11056926 batches: 0.0539
Time to train on one home:  55.84538245201111
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11057889 batches: 0.0660
trigger times: 1
Loss after 11058852 batches: 0.0519
trigger times: 2
Loss after 11059815 batches: 0.0492
trigger times: 3
Loss after 11060778 batches: 0.0449
trigger times: 4
Loss after 11061741 batches: 0.0408
trigger times: 5
Loss after 11062704 batches: 0.0380
trigger times: 6
Loss after 11063667 batches: 0.0361
trigger times: 7
Loss after 11064630 batches: 0.0345
trigger times: 8
Loss after 11065593 batches: 0.0332
trigger times: 9
Loss after 11066556 batches: 0.0319
trigger times: 10
Loss after 11067519 batches: 0.0304
trigger times: 11
Loss after 11068482 batches: 0.0298
trigger times: 12
Loss after 11069445 batches: 0.0288
trigger times: 13
Loss after 11070408 batches: 0.0288
trigger times: 14
Loss after 11071371 batches: 0.0290
trigger times: 15
Loss after 11072334 batches: 0.0285
trigger times: 16
Loss after 11073297 batches: 0.0290
trigger times: 17
Loss after 11074260 batches: 0.0280
trigger times: 18
Loss after 11075223 batches: 0.0279
trigger times: 19
Loss after 11076186 batches: 0.0280
trigger times: 20
Loss after 11077149 batches: 0.0271
trigger times: 21
Loss after 11078112 batches: 0.0265
trigger times: 22
Loss after 11079075 batches: 0.0260
trigger times: 23
Loss after 11080038 batches: 0.0261
trigger times: 24
Loss after 11081001 batches: 0.0254
trigger times: 25
Early stopping!
Start to test process.
Loss after 11081964 batches: 0.0253
Time to train on one home:  57.77277731895447
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11082927 batches: 0.0806
trigger times: 0
Loss after 11083890 batches: 0.0543
trigger times: 1
Loss after 11084853 batches: 0.0463
trigger times: 2
Loss after 11085816 batches: 0.0436
trigger times: 0
Loss after 11086779 batches: 0.0389
trigger times: 1
Loss after 11087742 batches: 0.0358
trigger times: 2
Loss after 11088705 batches: 0.0313
trigger times: 3
Loss after 11089668 batches: 0.0310
trigger times: 4
Loss after 11090631 batches: 0.0289
trigger times: 5
Loss after 11091594 batches: 0.0275
trigger times: 6
Loss after 11092557 batches: 0.0279
trigger times: 7
Loss after 11093520 batches: 0.0262
trigger times: 8
Loss after 11094483 batches: 0.0263
trigger times: 9
Loss after 11095446 batches: 0.0250
trigger times: 10
Loss after 11096409 batches: 0.0236
trigger times: 11
Loss after 11097372 batches: 0.0239
trigger times: 12
Loss after 11098335 batches: 0.0235
trigger times: 13
Loss after 11099298 batches: 0.0237
trigger times: 14
Loss after 11100261 batches: 0.0223
trigger times: 15
Loss after 11101224 batches: 0.0230
trigger times: 16
Loss after 11102187 batches: 0.0231
trigger times: 17
Loss after 11103150 batches: 0.0224
trigger times: 18
Loss after 11104113 batches: 0.0250
trigger times: 19
Loss after 11105076 batches: 0.0251
trigger times: 20
Loss after 11106039 batches: 0.0237
trigger times: 21
Loss after 11107002 batches: 0.0225
trigger times: 22
Loss after 11107965 batches: 0.0199
trigger times: 23
Loss after 11108928 batches: 0.0215
trigger times: 24
Loss after 11109891 batches: 0.0208
trigger times: 25
Early stopping!
Start to test process.
Loss after 11110854 batches: 0.0197
Time to train on one home:  61.68855619430542
trigger times: 0
Loss after 11111783 batches: 0.0854
trigger times: 0
Loss after 11112712 batches: 0.0653
trigger times: 0
Loss after 11113641 batches: 0.0503
trigger times: 0
Loss after 11114570 batches: 0.0392
trigger times: 1
Loss after 11115499 batches: 0.0363
trigger times: 2
Loss after 11116428 batches: 0.0344
trigger times: 0
Loss after 11117357 batches: 0.0352
trigger times: 1
Loss after 11118286 batches: 0.0333
trigger times: 2
Loss after 11119215 batches: 0.0314
trigger times: 3
Loss after 11120144 batches: 0.0309
trigger times: 0
Loss after 11121073 batches: 0.0294
trigger times: 1
Loss after 11122002 batches: 0.0276
trigger times: 2
Loss after 11122931 batches: 0.0282
trigger times: 0
Loss after 11123860 batches: 0.0270
trigger times: 1
Loss after 11124789 batches: 0.0244
trigger times: 2
Loss after 11125718 batches: 0.0272
trigger times: 3
Loss after 11126647 batches: 0.0312
trigger times: 0
Loss after 11127576 batches: 0.0274
trigger times: 1
Loss after 11128505 batches: 0.0338
trigger times: 2
Loss after 11129434 batches: 0.0351
trigger times: 3
Loss after 11130363 batches: 0.0339
trigger times: 4
Loss after 11131292 batches: 0.0319
trigger times: 5
Loss after 11132221 batches: 0.0317
trigger times: 6
Loss after 11133150 batches: 0.0313
trigger times: 7
Loss after 11134079 batches: 0.0289
trigger times: 8
Loss after 11135008 batches: 0.0290
trigger times: 9
Loss after 11135937 batches: 0.0290
trigger times: 10
Loss after 11136866 batches: 0.0275
trigger times: 11
Loss after 11137795 batches: 0.0260
trigger times: 12
Loss after 11138724 batches: 0.0279
trigger times: 13
Loss after 11139653 batches: 0.0291
trigger times: 14
Loss after 11140582 batches: 0.0288
trigger times: 15
Loss after 11141511 batches: 0.0288
trigger times: 16
Loss after 11142440 batches: 0.0269
trigger times: 17
Loss after 11143369 batches: 0.0264
trigger times: 18
Loss after 11144298 batches: 0.0274
trigger times: 19
Loss after 11145227 batches: 0.0280
trigger times: 20
Loss after 11146156 batches: 0.0260
trigger times: 21
Loss after 11147085 batches: 0.0265
trigger times: 22
Loss after 11148014 batches: 0.0251
trigger times: 23
Loss after 11148943 batches: 0.0242
trigger times: 24
Loss after 11149872 batches: 0.0265
trigger times: 25
Early stopping!
Start to test process.
Loss after 11150801 batches: 0.0248
Time to train on one home:  70.02704215049744
trigger times: 0
Loss after 11151763 batches: 0.0789
trigger times: 1
Loss after 11152725 batches: 0.0667
trigger times: 2
Loss after 11153687 batches: 0.0658
trigger times: 3
Loss after 11154649 batches: 0.0621
trigger times: 4
Loss after 11155611 batches: 0.0604
trigger times: 5
Loss after 11156573 batches: 0.0578
trigger times: 6
Loss after 11157535 batches: 0.0564
trigger times: 7
Loss after 11158497 batches: 0.0555
trigger times: 8
Loss after 11159459 batches: 0.0560
trigger times: 9
Loss after 11160421 batches: 0.0555
trigger times: 10
Loss after 11161383 batches: 0.0558
trigger times: 11
Loss after 11162345 batches: 0.0552
trigger times: 12
Loss after 11163307 batches: 0.0544
trigger times: 13
Loss after 11164269 batches: 0.0548
trigger times: 14
Loss after 11165231 batches: 0.0537
trigger times: 15
Loss after 11166193 batches: 0.0541
trigger times: 16
Loss after 11167155 batches: 0.0529
trigger times: 17
Loss after 11168117 batches: 0.0527
trigger times: 18
Loss after 11169079 batches: 0.0530
trigger times: 19
Loss after 11170041 batches: 0.0539
trigger times: 20
Loss after 11171003 batches: 0.0528
trigger times: 21
Loss after 11171965 batches: 0.0524
trigger times: 22
Loss after 11172927 batches: 0.0522
trigger times: 23
Loss after 11173889 batches: 0.0511
trigger times: 24
Loss after 11174851 batches: 0.0508
trigger times: 25
Early stopping!
Start to test process.
Loss after 11175813 batches: 0.0515
Time to train on one home:  57.19379234313965
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11176776 batches: 0.0540
trigger times: 1
Loss after 11177739 batches: 0.0451
trigger times: 2
Loss after 11178702 batches: 0.0449
trigger times: 3
Loss after 11179665 batches: 0.0428
trigger times: 4
Loss after 11180628 batches: 0.0395
trigger times: 5
Loss after 11181591 batches: 0.0388
trigger times: 6
Loss after 11182554 batches: 0.0372
trigger times: 7
Loss after 11183517 batches: 0.0364
trigger times: 8
Loss after 11184480 batches: 0.0353
trigger times: 9
Loss after 11185443 batches: 0.0353
trigger times: 10
Loss after 11186406 batches: 0.0349
trigger times: 11
Loss after 11187369 batches: 0.0347
trigger times: 12
Loss after 11188332 batches: 0.0337
trigger times: 13
Loss after 11189295 batches: 0.0334
trigger times: 14
Loss after 11190258 batches: 0.0333
trigger times: 15
Loss after 11191221 batches: 0.0333
trigger times: 16
Loss after 11192184 batches: 0.0325
trigger times: 17
Loss after 11193147 batches: 0.0330
trigger times: 18
Loss after 11194110 batches: 0.0320
trigger times: 19
Loss after 11195073 batches: 0.0325
trigger times: 20
Loss after 11196036 batches: 0.0319
trigger times: 21
Loss after 11196999 batches: 0.0320
trigger times: 22
Loss after 11197962 batches: 0.0314
trigger times: 23
Loss after 11198925 batches: 0.0313
trigger times: 24
Loss after 11199888 batches: 0.0315
trigger times: 25
Early stopping!
Start to test process.
Loss after 11200851 batches: 0.0310
Time to train on one home:  56.93099880218506
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11201814 batches: 0.0498
trigger times: 1
Loss after 11202777 batches: 0.0455
trigger times: 0
Loss after 11203740 batches: 0.0428
trigger times: 1
Loss after 11204703 batches: 0.0420
trigger times: 2
Loss after 11205666 batches: 0.0398
trigger times: 3
Loss after 11206629 batches: 0.0386
trigger times: 4
Loss after 11207592 batches: 0.0379
trigger times: 5
Loss after 11208555 batches: 0.0368
trigger times: 6
Loss after 11209518 batches: 0.0359
trigger times: 7
Loss after 11210481 batches: 0.0352
trigger times: 8
Loss after 11211444 batches: 0.0355
trigger times: 9
Loss after 11212407 batches: 0.0354
trigger times: 10
Loss after 11213370 batches: 0.0364
trigger times: 11
Loss after 11214333 batches: 0.0366
trigger times: 12
Loss after 11215296 batches: 0.0349
trigger times: 13
Loss after 11216259 batches: 0.0339
trigger times: 14
Loss after 11217222 batches: 0.0333
trigger times: 15
Loss after 11218185 batches: 0.0322
trigger times: 16
Loss after 11219148 batches: 0.0321
trigger times: 17
Loss after 11220111 batches: 0.0322
trigger times: 18
Loss after 11221074 batches: 0.0325
trigger times: 19
Loss after 11222037 batches: 0.0322
trigger times: 20
Loss after 11223000 batches: 0.0328
trigger times: 21
Loss after 11223963 batches: 0.0325
trigger times: 22
Loss after 11224926 batches: 0.0313
trigger times: 23
Loss after 11225889 batches: 0.0304
trigger times: 24
Loss after 11226852 batches: 0.0305
trigger times: 25
Early stopping!
Start to test process.
Loss after 11227815 batches: 0.0298
Time to train on one home:  55.37202286720276
trigger times: 0
Loss after 11228778 batches: 0.1010
trigger times: 1
Loss after 11229741 batches: 0.0964
trigger times: 2
Loss after 11230704 batches: 0.0910
trigger times: 3
Loss after 11231667 batches: 0.0873
trigger times: 4
Loss after 11232630 batches: 0.0841
trigger times: 5
Loss after 11233593 batches: 0.0834
trigger times: 6
Loss after 11234556 batches: 0.0821
trigger times: 7
Loss after 11235519 batches: 0.0820
trigger times: 8
Loss after 11236482 batches: 0.0806
trigger times: 9
Loss after 11237445 batches: 0.0778
trigger times: 10
Loss after 11238408 batches: 0.0796
trigger times: 11
Loss after 11239371 batches: 0.0775
trigger times: 12
Loss after 11240334 batches: 0.0743
trigger times: 13
Loss after 11241297 batches: 0.0759
trigger times: 14
Loss after 11242260 batches: 0.0768
trigger times: 15
Loss after 11243223 batches: 0.0763
trigger times: 16
Loss after 11244186 batches: 0.0738
trigger times: 17
Loss after 11245149 batches: 0.0767
trigger times: 18
Loss after 11246112 batches: 0.0769
trigger times: 19
Loss after 11247075 batches: 0.0746
trigger times: 20
Loss after 11248038 batches: 0.0733
trigger times: 21
Loss after 11249001 batches: 0.0727
trigger times: 22
Loss after 11249964 batches: 0.0734
trigger times: 23
Loss after 11250927 batches: 0.0723
trigger times: 24
Loss after 11251890 batches: 0.0696
trigger times: 25
Early stopping!
Start to test process.
Loss after 11252853 batches: 0.0716
Time to train on one home:  54.778658866882324
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11253816 batches: 0.0932
trigger times: 1
Loss after 11254779 batches: 0.0595
trigger times: 2
Loss after 11255742 batches: 0.0564
trigger times: 3
Loss after 11256705 batches: 0.0508
trigger times: 4
Loss after 11257668 batches: 0.0476
trigger times: 5
Loss after 11258631 batches: 0.0423
trigger times: 6
Loss after 11259594 batches: 0.0405
trigger times: 7
Loss after 11260557 batches: 0.0380
trigger times: 8
Loss after 11261520 batches: 0.0361
trigger times: 9
Loss after 11262483 batches: 0.0363
trigger times: 10
Loss after 11263446 batches: 0.0351
trigger times: 11
Loss after 11264409 batches: 0.0355
trigger times: 12
Loss after 11265372 batches: 0.0342
trigger times: 13
Loss after 11266335 batches: 0.0352
trigger times: 14
Loss after 11267298 batches: 0.0347
trigger times: 15
Loss after 11268261 batches: 0.0337
trigger times: 16
Loss after 11269224 batches: 0.0324
trigger times: 17
Loss after 11270187 batches: 0.0329
trigger times: 18
Loss after 11271150 batches: 0.0326
trigger times: 19
Loss after 11272113 batches: 0.0307
trigger times: 20
Loss after 11273076 batches: 0.0320
trigger times: 21
Loss after 11274039 batches: 0.0321
trigger times: 22
Loss after 11275002 batches: 0.0310
trigger times: 23
Loss after 11275965 batches: 0.0309
trigger times: 24
Loss after 11276928 batches: 0.0300
trigger times: 25
Early stopping!
Start to test process.
Loss after 11277891 batches: 0.0297
Time to train on one home:  56.904303312301636
trigger times: 0
Loss after 11278850 batches: 0.0920
trigger times: 1
Loss after 11279809 batches: 0.0506
trigger times: 0
Loss after 11280768 batches: 0.0372
trigger times: 1
Loss after 11281727 batches: 0.0312
trigger times: 2
Loss after 11282686 batches: 0.0267
trigger times: 3
Loss after 11283645 batches: 0.0248
trigger times: 4
Loss after 11284604 batches: 0.0238
trigger times: 0
Loss after 11285563 batches: 0.0222
trigger times: 1
Loss after 11286522 batches: 0.0214
trigger times: 2
Loss after 11287481 batches: 0.0209
trigger times: 3
Loss after 11288440 batches: 0.0207
trigger times: 4
Loss after 11289399 batches: 0.0203
trigger times: 5
Loss after 11290358 batches: 0.0195
trigger times: 0
Loss after 11291317 batches: 0.0189
trigger times: 0
Loss after 11292276 batches: 0.0184
trigger times: 1
Loss after 11293235 batches: 0.0194
trigger times: 2
Loss after 11294194 batches: 0.0190
trigger times: 3
Loss after 11295153 batches: 0.0177
trigger times: 4
Loss after 11296112 batches: 0.0176
trigger times: 5
Loss after 11297071 batches: 0.0174
trigger times: 6
Loss after 11298030 batches: 0.0181
trigger times: 7
Loss after 11298989 batches: 0.0168
trigger times: 8
Loss after 11299948 batches: 0.0177
trigger times: 9
Loss after 11300907 batches: 0.0170
trigger times: 10
Loss after 11301866 batches: 0.0170
trigger times: 11
Loss after 11302825 batches: 0.0167
trigger times: 12
Loss after 11303784 batches: 0.0167
trigger times: 13
Loss after 11304743 batches: 0.0167
trigger times: 0
Loss after 11305702 batches: 0.0208
trigger times: 1
Loss after 11306661 batches: 0.0196
trigger times: 2
Loss after 11307620 batches: 0.0178
trigger times: 3
Loss after 11308579 batches: 0.0184
trigger times: 4
Loss after 11309538 batches: 0.0172
trigger times: 5
Loss after 11310497 batches: 0.0170
trigger times: 6
Loss after 11311456 batches: 0.0169
trigger times: 7
Loss after 11312415 batches: 0.0166
trigger times: 8
Loss after 11313374 batches: 0.0157
trigger times: 9
Loss after 11314333 batches: 0.0157
trigger times: 10
Loss after 11315292 batches: 0.0155
trigger times: 11
Loss after 11316251 batches: 0.0159
trigger times: 12
Loss after 11317210 batches: 0.0153
trigger times: 13
Loss after 11318169 batches: 0.0154
trigger times: 14
Loss after 11319128 batches: 0.0149
trigger times: 0
Loss after 11320087 batches: 0.0144
trigger times: 0
Loss after 11321046 batches: 0.0158
trigger times: 1
Loss after 11322005 batches: 0.0155
trigger times: 2
Loss after 11322964 batches: 0.0144
trigger times: 3
Loss after 11323923 batches: 0.0149
trigger times: 4
Loss after 11324882 batches: 0.0157
trigger times: 5
Loss after 11325841 batches: 0.0159
trigger times: 6
Loss after 11326800 batches: 0.0154
trigger times: 7
Loss after 11327759 batches: 0.0151
trigger times: 8
Loss after 11328718 batches: 0.0150
trigger times: 9
Loss after 11329677 batches: 0.0150
trigger times: 10
Loss after 11330636 batches: 0.0147
trigger times: 11
Loss after 11331595 batches: 0.0137
trigger times: 12
Loss after 11332554 batches: 0.0127
trigger times: 13
Loss after 11333513 batches: 0.0136
trigger times: 0
Loss after 11334472 batches: 0.0134
trigger times: 1
Loss after 11335431 batches: 0.0134
trigger times: 0
Loss after 11336390 batches: 0.0131
trigger times: 1
Loss after 11337349 batches: 0.0131
trigger times: 2
Loss after 11338308 batches: 0.0130
trigger times: 3
Loss after 11339267 batches: 0.0125
trigger times: 4
Loss after 11340226 batches: 0.0130
trigger times: 5
Loss after 11341185 batches: 0.0130
trigger times: 6
Loss after 11342144 batches: 0.0132
trigger times: 7
Loss after 11343103 batches: 0.0128
trigger times: 8
Loss after 11344062 batches: 0.0125
trigger times: 9
Loss after 11345021 batches: 0.0121
trigger times: 10
Loss after 11345980 batches: 0.0122
trigger times: 0
Loss after 11346939 batches: 0.0128
trigger times: 1
Loss after 11347898 batches: 0.0155
trigger times: 2
Loss after 11348857 batches: 0.0144
trigger times: 3
Loss after 11349816 batches: 0.0149
trigger times: 4
Loss after 11350775 batches: 0.0137
trigger times: 5
Loss after 11351734 batches: 0.0135
trigger times: 6
Loss after 11352693 batches: 0.0130
trigger times: 7
Loss after 11353652 batches: 0.0127
trigger times: 8
Loss after 11354611 batches: 0.0122
trigger times: 9
Loss after 11355570 batches: 0.0123
trigger times: 10
Loss after 11356529 batches: 0.0116
trigger times: 11
Loss after 11357488 batches: 0.0114
trigger times: 12
Loss after 11358447 batches: 0.0115
trigger times: 13
Loss after 11359406 batches: 0.0111
trigger times: 14
Loss after 11360365 batches: 0.0115
trigger times: 15
Loss after 11361324 batches: 0.0118
trigger times: 16
Loss after 11362283 batches: 0.0126
trigger times: 17
Loss after 11363242 batches: 0.0112
trigger times: 18
Loss after 11364201 batches: 0.0116
trigger times: 0
Loss after 11365160 batches: 0.0114
trigger times: 1
Loss after 11366119 batches: 0.0117
trigger times: 2
Loss after 11367078 batches: 0.0118
trigger times: 3
Loss after 11368037 batches: 0.0111
trigger times: 4
Loss after 11368996 batches: 0.0108
trigger times: 5
Loss after 11369955 batches: 0.0099
trigger times: 6
Loss after 11370914 batches: 0.0100
trigger times: 7
Loss after 11371873 batches: 0.0101
trigger times: 8
Loss after 11372832 batches: 0.0103
trigger times: 9
Loss after 11373791 batches: 0.0103
trigger times: 10
Loss after 11374750 batches: 0.0108
trigger times: 11
Loss after 11375709 batches: 0.0120
trigger times: 12
Loss after 11376668 batches: 0.0122
trigger times: 13
Loss after 11377627 batches: 0.0121
trigger times: 14
Loss after 11378586 batches: 0.0111
trigger times: 15
Loss after 11379545 batches: 0.0107
trigger times: 16
Loss after 11380504 batches: 0.0103
trigger times: 17
Loss after 11381463 batches: 0.0099
trigger times: 18
Loss after 11382422 batches: 0.0096
trigger times: 19
Loss after 11383381 batches: 0.0094
trigger times: 20
Loss after 11384340 batches: 0.0105
trigger times: 21
Loss after 11385299 batches: 0.0103
trigger times: 22
Loss after 11386258 batches: 0.0097
trigger times: 23
Loss after 11387217 batches: 0.0095
trigger times: 24
Loss after 11388176 batches: 0.0098
trigger times: 25
Early stopping!
Start to test process.
Loss after 11389135 batches: 0.0092
Time to train on one home:  130.20824575424194
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11390098 batches: 0.0681
trigger times: 1
Loss after 11391061 batches: 0.0307
trigger times: 2
Loss after 11392024 batches: 0.0266
trigger times: 3
Loss after 11392987 batches: 0.0267
trigger times: 4
Loss after 11393950 batches: 0.0257
trigger times: 5
Loss after 11394913 batches: 0.0250
trigger times: 6
Loss after 11395876 batches: 0.0244
trigger times: 7
Loss after 11396839 batches: 0.0238
trigger times: 8
Loss after 11397802 batches: 0.0230
trigger times: 9
Loss after 11398765 batches: 0.0226
trigger times: 10
Loss after 11399728 batches: 0.0222
trigger times: 11
Loss after 11400691 batches: 0.0216
trigger times: 12
Loss after 11401654 batches: 0.0213
trigger times: 13
Loss after 11402617 batches: 0.0209
trigger times: 14
Loss after 11403580 batches: 0.0204
trigger times: 15
Loss after 11404543 batches: 0.0199
trigger times: 16
Loss after 11405506 batches: 0.0198
trigger times: 17
Loss after 11406469 batches: 0.0195
trigger times: 18
Loss after 11407432 batches: 0.0191
trigger times: 19
Loss after 11408395 batches: 0.0191
trigger times: 20
Loss after 11409358 batches: 0.0191
trigger times: 21
Loss after 11410321 batches: 0.0189
trigger times: 22
Loss after 11411284 batches: 0.0188
trigger times: 23
Loss after 11412247 batches: 0.0183
trigger times: 24
Loss after 11413210 batches: 0.0185
trigger times: 25
Early stopping!
Start to test process.
Loss after 11414173 batches: 0.0182
Time to train on one home:  56.268242835998535
trigger times: 0
Loss after 11415118 batches: 0.0654
trigger times: 0
Loss after 11416063 batches: 0.0468
trigger times: 0
Loss after 11417008 batches: 0.0378
trigger times: 1
Loss after 11417953 batches: 0.0327
trigger times: 2
Loss after 11418898 batches: 0.0318
trigger times: 0
Loss after 11419843 batches: 0.0283
trigger times: 1
Loss after 11420788 batches: 0.0289
trigger times: 2
Loss after 11421733 batches: 0.0272
trigger times: 3
Loss after 11422678 batches: 0.0255
trigger times: 4
Loss after 11423623 batches: 0.0251
trigger times: 5
Loss after 11424568 batches: 0.0238
trigger times: 6
Loss after 11425513 batches: 0.0237
trigger times: 7
Loss after 11426458 batches: 0.0238
trigger times: 8
Loss after 11427403 batches: 0.0239
trigger times: 9
Loss after 11428348 batches: 0.0226
trigger times: 10
Loss after 11429293 batches: 0.0231
trigger times: 11
Loss after 11430238 batches: 0.0221
trigger times: 12
Loss after 11431183 batches: 0.0207
trigger times: 13
Loss after 11432128 batches: 0.0219
trigger times: 14
Loss after 11433073 batches: 0.0217
trigger times: 15
Loss after 11434018 batches: 0.0205
trigger times: 16
Loss after 11434963 batches: 0.0194
trigger times: 17
Loss after 11435908 batches: 0.0213
trigger times: 18
Loss after 11436853 batches: 0.0204
trigger times: 19
Loss after 11437798 batches: 0.0206
trigger times: 20
Loss after 11438743 batches: 0.0192
trigger times: 21
Loss after 11439688 batches: 0.0186
trigger times: 22
Loss after 11440633 batches: 0.0189
trigger times: 23
Loss after 11441578 batches: 0.0200
trigger times: 24
Loss after 11442523 batches: 0.0189
trigger times: 25
Early stopping!
Start to test process.
Loss after 11443468 batches: 0.0185
Time to train on one home:  60.494101762771606
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11444405 batches: 0.0835
trigger times: 1
Loss after 11445342 batches: 0.0713
trigger times: 2
Loss after 11446279 batches: 0.0679
trigger times: 3
Loss after 11447216 batches: 0.0645
trigger times: 4
Loss after 11448153 batches: 0.0624
trigger times: 5
Loss after 11449090 batches: 0.0594
trigger times: 6
Loss after 11450027 batches: 0.0589
trigger times: 7
Loss after 11450964 batches: 0.0575
trigger times: 8
Loss after 11451901 batches: 0.0563
trigger times: 9
Loss after 11452838 batches: 0.0554
trigger times: 10
Loss after 11453775 batches: 0.0561
trigger times: 11
Loss after 11454712 batches: 0.0550
trigger times: 12
Loss after 11455649 batches: 0.0539
trigger times: 13
Loss after 11456586 batches: 0.0527
trigger times: 14
Loss after 11457523 batches: 0.0529
trigger times: 15
Loss after 11458460 batches: 0.0530
trigger times: 16
Loss after 11459397 batches: 0.0519
trigger times: 17
Loss after 11460334 batches: 0.0522
trigger times: 18
Loss after 11461271 batches: 0.0517
trigger times: 19
Loss after 11462208 batches: 0.0518
trigger times: 20
Loss after 11463145 batches: 0.0504
trigger times: 21
Loss after 11464082 batches: 0.0522
trigger times: 22
Loss after 11465019 batches: 0.0499
trigger times: 23
Loss after 11465956 batches: 0.0494
trigger times: 24
Loss after 11466893 batches: 0.0502
trigger times: 25
Early stopping!
Start to test process.
Loss after 11467830 batches: 0.0491
Time to train on one home:  57.351237058639526
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 11468793 batches: 0.0515
trigger times: 1
Loss after 11469756 batches: 0.0171
trigger times: 2
Loss after 11470719 batches: 0.0139
trigger times: 3
Loss after 11471682 batches: 0.0138
trigger times: 4
Loss after 11472645 batches: 0.0136
trigger times: 5
Loss after 11473608 batches: 0.0137
trigger times: 6
Loss after 11474571 batches: 0.0139
trigger times: 7
Loss after 11475534 batches: 0.0138
trigger times: 8
Loss after 11476497 batches: 0.0138
trigger times: 9
Loss after 11477460 batches: 0.0138
trigger times: 10
Loss after 11478423 batches: 0.0137
trigger times: 11
Loss after 11479386 batches: 0.0137
trigger times: 12
Loss after 11480349 batches: 0.0136
trigger times: 13
Loss after 11481312 batches: 0.0136
trigger times: 14
Loss after 11482275 batches: 0.0135
trigger times: 15
Loss after 11483238 batches: 0.0132
trigger times: 16
Loss after 11484201 batches: 0.0129
trigger times: 17
Loss after 11485164 batches: 0.0123
trigger times: 18
Loss after 11486127 batches: 0.0120
trigger times: 19
Loss after 11487090 batches: 0.0116
trigger times: 20
Loss after 11488053 batches: 0.0112
trigger times: 21
Loss after 11489016 batches: 0.0108
trigger times: 22
Loss after 11489979 batches: 0.0105
trigger times: 23
Loss after 11490942 batches: 0.0101
trigger times: 24
Loss after 11491905 batches: 0.0095
trigger times: 25
Early stopping!
Start to test process.
Loss after 11492868 batches: 0.0092
Time to train on one home:  56.70449709892273
trigger times: 0
Loss after 11493831 batches: 0.0950
trigger times: 1
Loss after 11494794 batches: 0.0773
trigger times: 2
Loss after 11495757 batches: 0.0726
trigger times: 3
Loss after 11496720 batches: 0.0684
trigger times: 4
Loss after 11497683 batches: 0.0660
trigger times: 5
Loss after 11498646 batches: 0.0640
trigger times: 6
Loss after 11499609 batches: 0.0625
trigger times: 7
Loss after 11500572 batches: 0.0618
trigger times: 8
Loss after 11501535 batches: 0.0607
trigger times: 9
Loss after 11502498 batches: 0.0600
trigger times: 10
Loss after 11503461 batches: 0.0606
trigger times: 11
Loss after 11504424 batches: 0.0601
trigger times: 12
Loss after 11505387 batches: 0.0591
trigger times: 13
Loss after 11506350 batches: 0.0589
trigger times: 14
Loss after 11507313 batches: 0.0582
trigger times: 15
Loss after 11508276 batches: 0.0576
trigger times: 16
Loss after 11509239 batches: 0.0575
trigger times: 17
Loss after 11510202 batches: 0.0576
trigger times: 18
Loss after 11511165 batches: 0.0577
trigger times: 19
Loss after 11512128 batches: 0.0561
trigger times: 20
Loss after 11513091 batches: 0.0574
trigger times: 21
Loss after 11514054 batches: 0.0585
trigger times: 22
Loss after 11515017 batches: 0.0577
trigger times: 23
Loss after 11515980 batches: 0.0568
trigger times: 24
Loss after 11516943 batches: 0.0557
trigger times: 25
Early stopping!
Start to test process.
Loss after 11517906 batches: 0.0552
Time to train on one home:  53.04568409919739
trigger times: 0
Loss after 11518869 batches: 0.0750
trigger times: 1
Loss after 11519832 batches: 0.0544
trigger times: 2
Loss after 11520795 batches: 0.0513
trigger times: 3
Loss after 11521758 batches: 0.0469
trigger times: 4
Loss after 11522721 batches: 0.0441
trigger times: 5
Loss after 11523684 batches: 0.0419
trigger times: 6
Loss after 11524647 batches: 0.0405
trigger times: 7
Loss after 11525610 batches: 0.0391
trigger times: 8
Loss after 11526573 batches: 0.0390
trigger times: 9
Loss after 11527536 batches: 0.0375
trigger times: 10
Loss after 11528499 batches: 0.0377
trigger times: 11
Loss after 11529462 batches: 0.0375
trigger times: 12
Loss after 11530425 batches: 0.0374
trigger times: 13
Loss after 11531388 batches: 0.0374
trigger times: 14
Loss after 11532351 batches: 0.0356
trigger times: 15
Loss after 11533314 batches: 0.0346
trigger times: 16
Loss after 11534277 batches: 0.0341
trigger times: 17
Loss after 11535240 batches: 0.0352
trigger times: 18
Loss after 11536203 batches: 0.0340
trigger times: 19
Loss after 11537166 batches: 0.0334
trigger times: 20
Loss after 11538129 batches: 0.0336
trigger times: 21
Loss after 11539092 batches: 0.0341
trigger times: 22
Loss after 11540055 batches: 0.0332
trigger times: 23
Loss after 11541018 batches: 0.0333
trigger times: 24
Loss after 11541981 batches: 0.0332
trigger times: 25
Early stopping!
Start to test process.
Loss after 11542944 batches: 0.0321
Time to train on one home:  56.78859496116638
trigger times: 0
Loss after 11543840 batches: 0.1034
trigger times: 1
Loss after 11544736 batches: 0.0924
trigger times: 2
Loss after 11545632 batches: 0.0875
trigger times: 3
Loss after 11546528 batches: 0.0829
trigger times: 4
Loss after 11547424 batches: 0.0769
trigger times: 5
Loss after 11548320 batches: 0.0748
trigger times: 6
Loss after 11549216 batches: 0.0700
trigger times: 7
Loss after 11550112 batches: 0.0675
trigger times: 8
Loss after 11551008 batches: 0.0698
trigger times: 9
Loss after 11551904 batches: 0.0664
trigger times: 10
Loss after 11552800 batches: 0.0668
trigger times: 11
Loss after 11553696 batches: 0.0633
trigger times: 12
Loss after 11554592 batches: 0.0631
trigger times: 13
Loss after 11555488 batches: 0.0618
trigger times: 14
Loss after 11556384 batches: 0.0597
trigger times: 15
Loss after 11557280 batches: 0.0589
trigger times: 16
Loss after 11558176 batches: 0.0591
trigger times: 17
Loss after 11559072 batches: 0.0610
trigger times: 18
Loss after 11559968 batches: 0.0596
trigger times: 19
Loss after 11560864 batches: 0.0584
trigger times: 20
Loss after 11561760 batches: 0.0595
trigger times: 21
Loss after 11562656 batches: 0.0595
trigger times: 22
Loss after 11563552 batches: 0.0591
trigger times: 23
Loss after 11564448 batches: 0.0586
trigger times: 24
Loss after 11565344 batches: 0.0565
trigger times: 25
Early stopping!
Start to test process.
Loss after 11566240 batches: 0.0576
Time to train on one home:  55.293092012405396
trigger times: 0
Loss after 11567203 batches: 0.1751
trigger times: 1
Loss after 11568166 batches: 0.1189
trigger times: 2
Loss after 11569129 batches: 0.0991
trigger times: 3
Loss after 11570092 batches: 0.0938
trigger times: 4
Loss after 11571055 batches: 0.0853
trigger times: 5
Loss after 11572018 batches: 0.0818
trigger times: 6
Loss after 11572981 batches: 0.0760
trigger times: 7
Loss after 11573944 batches: 0.0715
trigger times: 8
Loss after 11574907 batches: 0.0671
trigger times: 9
Loss after 11575870 batches: 0.0646
trigger times: 10
Loss after 11576833 batches: 0.0617
trigger times: 11
Loss after 11577796 batches: 0.0606
trigger times: 12
Loss after 11578759 batches: 0.0596
trigger times: 13
Loss after 11579722 batches: 0.0552
trigger times: 14
Loss after 11580685 batches: 0.0535
trigger times: 15
Loss after 11581648 batches: 0.0524
trigger times: 16
Loss after 11582611 batches: 0.0515
trigger times: 17
Loss after 11583574 batches: 0.0493
trigger times: 18
Loss after 11584537 batches: 0.0492
trigger times: 19
Loss after 11585500 batches: 0.0481
trigger times: 20
Loss after 11586463 batches: 0.0461
trigger times: 21
Loss after 11587426 batches: 0.0474
trigger times: 22
Loss after 11588389 batches: 0.0487
trigger times: 23
Loss after 11589352 batches: 0.0456
trigger times: 24
Loss after 11590315 batches: 0.0453
trigger times: 25
Early stopping!
Start to test process.
Loss after 11591278 batches: 0.0460
Time to train on one home:  54.430359840393066
trigger times: 0
Loss after 11592241 batches: 0.0883
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 11593204 batches: 0.0768
trigger times: 2
Loss after 11594167 batches: 0.0764
trigger times: 3
Loss after 11595130 batches: 0.0738
trigger times: 4
Loss after 11596093 batches: 0.0697
trigger times: 5
Loss after 11597056 batches: 0.0657
trigger times: 6
Loss after 11598019 batches: 0.0625
trigger times: 7
Loss after 11598982 batches: 0.0626
trigger times: 8
Loss after 11599945 batches: 0.0604
trigger times: 9
Loss after 11600908 batches: 0.0595
trigger times: 10
Loss after 11601871 batches: 0.0597
trigger times: 11
Loss after 11602834 batches: 0.0581
trigger times: 12
Loss after 11603797 batches: 0.0561
trigger times: 13
Loss after 11604760 batches: 0.0550
trigger times: 14
Loss after 11605723 batches: 0.0551
trigger times: 15
Loss after 11606686 batches: 0.0544
trigger times: 16
Loss after 11607649 batches: 0.0540
trigger times: 17
Loss after 11608612 batches: 0.0525
trigger times: 18
Loss after 11609575 batches: 0.0527
trigger times: 19
Loss after 11610538 batches: 0.0533
trigger times: 20
Loss after 11611501 batches: 0.0506
trigger times: 21
Loss after 11612464 batches: 0.0501
trigger times: 22
Loss after 11613427 batches: 0.0497
trigger times: 23
Loss after 11614390 batches: 0.0505
trigger times: 24
Loss after 11615353 batches: 0.0511
trigger times: 25
Early stopping!
Start to test process.
Loss after 11616316 batches: 0.0502
Time to train on one home:  60.84650754928589
trigger times: 0
Loss after 11617279 batches: 0.0900
trigger times: 1
Loss after 11618242 batches: 0.0659
trigger times: 2
Loss after 11619205 batches: 0.0607
trigger times: 3
Loss after 11620168 batches: 0.0550
trigger times: 4
Loss after 11621131 batches: 0.0522
trigger times: 5
Loss after 11622094 batches: 0.0497
trigger times: 6
Loss after 11623057 batches: 0.0472
trigger times: 7
Loss after 11624020 batches: 0.0457
trigger times: 8
Loss after 11624983 batches: 0.0440
trigger times: 9
Loss after 11625946 batches: 0.0436
trigger times: 10
Loss after 11626909 batches: 0.0424
trigger times: 11
Loss after 11627872 batches: 0.0421
trigger times: 12
Loss after 11628835 batches: 0.0422
trigger times: 13
Loss after 11629798 batches: 0.0425
trigger times: 14
Loss after 11630761 batches: 0.0415
trigger times: 15
Loss after 11631724 batches: 0.0408
trigger times: 16
Loss after 11632687 batches: 0.0400
trigger times: 17
Loss after 11633650 batches: 0.0394
trigger times: 18
Loss after 11634613 batches: 0.0391
trigger times: 19
Loss after 11635576 batches: 0.0388
trigger times: 20
Loss after 11636539 batches: 0.0402
trigger times: 21
Loss after 11637502 batches: 0.0391
trigger times: 22
Loss after 11638465 batches: 0.0386
trigger times: 23
Loss after 11639428 batches: 0.0385
trigger times: 24
Loss after 11640391 batches: 0.0384
trigger times: 25
Early stopping!
Start to test process.
Loss after 11641354 batches: 0.0376
Time to train on one home:  56.114933490753174
trigger times: 0
Loss after 11642317 batches: 0.0473
trigger times: 1
Loss after 11643280 batches: 0.0390
trigger times: 2
Loss after 11644243 batches: 0.0357
trigger times: 3
Loss after 11645206 batches: 0.0326
trigger times: 4
Loss after 11646169 batches: 0.0302
trigger times: 5
Loss after 11647132 batches: 0.0292
trigger times: 6
Loss after 11648095 batches: 0.0272
trigger times: 7
Loss after 11649058 batches: 0.0262
trigger times: 8
Loss after 11650021 batches: 0.0258
trigger times: 9
Loss after 11650984 batches: 0.0248
trigger times: 10
Loss after 11651947 batches: 0.0250
trigger times: 11
Loss after 11652910 batches: 0.0236
trigger times: 12
Loss after 11653873 batches: 0.0238
trigger times: 13
Loss after 11654836 batches: 0.0236
trigger times: 14
Loss after 11655799 batches: 0.0235
trigger times: 15
Loss after 11656762 batches: 0.0240
trigger times: 16
Loss after 11657725 batches: 0.0236
trigger times: 17
Loss after 11658688 batches: 0.0235
trigger times: 18
Loss after 11659651 batches: 0.0227
trigger times: 19
Loss after 11660614 batches: 0.0224
trigger times: 20
Loss after 11661577 batches: 0.0222
trigger times: 21
Loss after 11662540 batches: 0.0222
trigger times: 22
Loss after 11663503 batches: 0.0227
trigger times: 23
Loss after 11664466 batches: 0.0226
trigger times: 24
Loss after 11665429 batches: 0.0229
trigger times: 25
Early stopping!
Start to test process.
Loss after 11666392 batches: 0.0228
Time to train on one home:  56.680785179138184
trigger times: 0
Loss after 11667355 batches: 0.0846
trigger times: 1
Loss after 11668318 batches: 0.0474
trigger times: 2
Loss after 11669281 batches: 0.0486
trigger times: 3
Loss after 11670244 batches: 0.0473
trigger times: 4
Loss after 11671207 batches: 0.0447
trigger times: 5
Loss after 11672170 batches: 0.0427
trigger times: 6
Loss after 11673133 batches: 0.0419
trigger times: 7
Loss after 11674096 batches: 0.0404
trigger times: 8
Loss after 11675059 batches: 0.0392
trigger times: 9
Loss after 11676022 batches: 0.0384
trigger times: 10
Loss after 11676985 batches: 0.0370
trigger times: 11
Loss after 11677948 batches: 0.0374
trigger times: 12
Loss after 11678911 batches: 0.0366
trigger times: 13
Loss after 11679874 batches: 0.0364
trigger times: 14
Loss after 11680837 batches: 0.0359
trigger times: 15
Loss after 11681800 batches: 0.0358
trigger times: 16
Loss after 11682763 batches: 0.0354
trigger times: 17
Loss after 11683726 batches: 0.0357
trigger times: 18
Loss after 11684689 batches: 0.0352
trigger times: 19
Loss after 11685652 batches: 0.0352
trigger times: 20
Loss after 11686615 batches: 0.0356
trigger times: 21
Loss after 11687578 batches: 0.0347
trigger times: 22
Loss after 11688541 batches: 0.0347
trigger times: 23
Loss after 11689504 batches: 0.0341
trigger times: 24
Loss after 11690467 batches: 0.0343
trigger times: 25
Early stopping!
Start to test process.
Loss after 11691430 batches: 0.0344
Time to train on one home:  57.30756878852844
trigger times: 0
Loss after 11692325 batches: 0.0613
trigger times: 1
Loss after 11693220 batches: 0.0330
trigger times: 2
Loss after 11694115 batches: 0.0140
trigger times: 0
Loss after 11695010 batches: 0.0092
trigger times: 1
Loss after 11695905 batches: 0.0067
trigger times: 2
Loss after 11696800 batches: 0.0055
trigger times: 0
Loss after 11697695 batches: 0.0048
trigger times: 1
Loss after 11698590 batches: 0.0047
trigger times: 2
Loss after 11699485 batches: 0.0041
trigger times: 3
Loss after 11700380 batches: 0.0043
trigger times: 0
Loss after 11701275 batches: 0.0042
trigger times: 1
Loss after 11702170 batches: 0.0035
trigger times: 2
Loss after 11703065 batches: 0.0031
trigger times: 3
Loss after 11703960 batches: 0.0034
trigger times: 4
Loss after 11704855 batches: 0.0034
trigger times: 5
Loss after 11705750 batches: 0.0034
trigger times: 6
Loss after 11706645 batches: 0.0030
trigger times: 7
Loss after 11707540 batches: 0.0030
trigger times: 8
Loss after 11708435 batches: 0.0037
trigger times: 9
Loss after 11709330 batches: 0.0034
trigger times: 10
Loss after 11710225 batches: 0.0030
trigger times: 11
Loss after 11711120 batches: 0.0028
trigger times: 12
Loss after 11712015 batches: 0.0027
trigger times: 13
Loss after 11712910 batches: 0.0022
trigger times: 14
Loss after 11713805 batches: 0.0026
trigger times: 15
Loss after 11714700 batches: 0.0022
trigger times: 16
Loss after 11715595 batches: 0.0023
trigger times: 17
Loss after 11716490 batches: 0.0032
trigger times: 18
Loss after 11717385 batches: 0.0026
trigger times: 19
Loss after 11718280 batches: 0.0033
trigger times: 20
Loss after 11719175 batches: 0.0038
trigger times: 21
Loss after 11720070 batches: 0.0026
trigger times: 22
Loss after 11720965 batches: 0.0026
trigger times: 23
Loss after 11721860 batches: 0.0022
trigger times: 24
Loss after 11722755 batches: 0.0020
trigger times: 25
Early stopping!
Start to test process.
Loss after 11723650 batches: 0.0019
Time to train on one home:  62.49457788467407
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336, 0.034888222413376094, 0.034613204319843105]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171], [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71], [0.13675299286842346, 0.15565351472302724, 0.32920502623266784, 1.059933101560365, 0.6549021202780799, 35.010519125969424, 3184.605]]
Round_9_results:  [0.13675299286842346, 0.15565351472302724, 0.32920502623266784, 1.059933101560365, 0.6549021202780799, 35.010519125969424, 3184.605]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 12291 < 12292; dropping {'Training_Loss': 0.06446156491126333, 'Validation_Loss': 0.07796590030193329, 'Training_R2': 0.3015472737699626, 'Validation_R2': 0.10183648645892274, 'Training_F1': 0.5048599771508453, 'Validation_F1': 0.296868321407154, 'Training_NEP': 0.8148652436295412, 'Validation_NEP': 0.924060823837373, 'Training_NDE': 0.5143785996688442, 'Validation_NDE': 0.6969730080451432, 'Training_MAE': 23.464182653247118, 'Validation_MAE': 30.780682136463618, 'Training_MSE': 1618.3275, 'Validation_MSE': 3452.386}.
trigger times: 0
Loss after 11724613 batches: 0.0645
trigger times: 0
Loss after 11725576 batches: 0.0458
trigger times: 0
Loss after 11726539 batches: 0.0448
trigger times: 1
Loss after 11727502 batches: 0.0416
trigger times: 2
Loss after 11728465 batches: 0.0380
trigger times: 3
Loss after 11729428 batches: 0.0368
trigger times: 4
Loss after 11730391 batches: 0.0362
trigger times: 5
Loss after 11731354 batches: 0.0336
trigger times: 6
Loss after 11732317 batches: 0.0321
trigger times: 7
Loss after 11733280 batches: 0.0318
trigger times: 8
Loss after 11734243 batches: 0.0327
trigger times: 9
Loss after 11735206 batches: 0.0322
trigger times: 10
Loss after 11736169 batches: 0.0352
trigger times: 11
Loss after 11737132 batches: 0.0337
trigger times: 12
Loss after 11738095 batches: 0.0312
trigger times: 13
Loss after 11739058 batches: 0.0321
trigger times: 14
Loss after 11740021 batches: 0.0316
trigger times: 15
Loss after 11740984 batches: 0.0312
trigger times: 16
Loss after 11741947 batches: 0.0302
trigger times: 17
Loss after 11742910 batches: 0.0312
trigger times: 18
Loss after 11743873 batches: 0.0312
trigger times: 19
Loss after 11744836 batches: 0.0298
trigger times: 20
Loss after 11745799 batches: 0.0302
trigger times: 21
Loss after 11746762 batches: 0.0301
trigger times: 22
Loss after 11747725 batches: 0.0301
trigger times: 23
Loss after 11748688 batches: 0.0285
trigger times: 24
Loss after 11749651 batches: 0.0298
trigger times: 25
Early stopping!
Start to test process.
Loss after 11750614 batches: 0.0294
Time to train on one home:  54.585115909576416
trigger times: 0
Loss after 11751572 batches: 0.0915
trigger times: 0
Loss after 11752530 batches: 0.0488
trigger times: 1
Loss after 11753488 batches: 0.0423
trigger times: 0
Loss after 11754446 batches: 0.0361
trigger times: 1
Loss after 11755404 batches: 0.0318
trigger times: 2
Loss after 11756362 batches: 0.0263
trigger times: 3
Loss after 11757320 batches: 0.0246
trigger times: 4
Loss after 11758278 batches: 0.0235
trigger times: 5
Loss after 11759236 batches: 0.0222
trigger times: 6
Loss after 11760194 batches: 0.0226
trigger times: 7
Loss after 11761152 batches: 0.0234
trigger times: 8
Loss after 11762110 batches: 0.0208
trigger times: 0
Loss after 11763068 batches: 0.0210
trigger times: 0
Loss after 11764026 batches: 0.0203
trigger times: 1
Loss after 11764984 batches: 0.0195
trigger times: 2
Loss after 11765942 batches: 0.0193
trigger times: 3
Loss after 11766900 batches: 0.0195
trigger times: 4
Loss after 11767858 batches: 0.0174
trigger times: 5
Loss after 11768816 batches: 0.0171
trigger times: 6
Loss after 11769774 batches: 0.0187
trigger times: 7
Loss after 11770732 batches: 0.0172
trigger times: 8
Loss after 11771690 batches: 0.0176
trigger times: 0
Loss after 11772648 batches: 0.0176
trigger times: 1
Loss after 11773606 batches: 0.0171
trigger times: 2
Loss after 11774564 batches: 0.0163
trigger times: 3
Loss after 11775522 batches: 0.0170
trigger times: 4
Loss after 11776480 batches: 0.0163
trigger times: 5
Loss after 11777438 batches: 0.0156
trigger times: 6
Loss after 11778396 batches: 0.0169
trigger times: 7
Loss after 11779354 batches: 0.0162
trigger times: 8
Loss after 11780312 batches: 0.0173
trigger times: 9
Loss after 11781270 batches: 0.0158
trigger times: 10
Loss after 11782228 batches: 0.0161
trigger times: 11
Loss after 11783186 batches: 0.0173
trigger times: 12
Loss after 11784144 batches: 0.0167
trigger times: 13
Loss after 11785102 batches: 0.0164
trigger times: 14
Loss after 11786060 batches: 0.0172
trigger times: 15
Loss after 11787018 batches: 0.0157
trigger times: 16
Loss after 11787976 batches: 0.0161
trigger times: 17
Loss after 11788934 batches: 0.0162
trigger times: 18
Loss after 11789892 batches: 0.0177
trigger times: 19
Loss after 11790850 batches: 0.0168
trigger times: 20
Loss after 11791808 batches: 0.0177
trigger times: 21
Loss after 11792766 batches: 0.0162
trigger times: 22
Loss after 11793724 batches: 0.0151
trigger times: 23
Loss after 11794682 batches: 0.0141
trigger times: 24
Loss after 11795640 batches: 0.0142
trigger times: 25
Early stopping!
Start to test process.
Loss after 11796598 batches: 0.0146
Time to train on one home:  75.21387553215027
trigger times: 0
Loss after 11797561 batches: 0.0825
trigger times: 1
Loss after 11798524 batches: 0.0694
trigger times: 2
Loss after 11799487 batches: 0.0672
trigger times: 3
Loss after 11800450 batches: 0.0661
trigger times: 4
Loss after 11801413 batches: 0.0639
trigger times: 5
Loss after 11802376 batches: 0.0618
trigger times: 6
Loss after 11803339 batches: 0.0595
trigger times: 7
Loss after 11804302 batches: 0.0580
trigger times: 8
Loss after 11805265 batches: 0.0565
trigger times: 9
Loss after 11806228 batches: 0.0547
trigger times: 10
Loss after 11807191 batches: 0.0532
trigger times: 11
Loss after 11808154 batches: 0.0509
trigger times: 12
Loss after 11809117 batches: 0.0505
trigger times: 13
Loss after 11810080 batches: 0.0496
trigger times: 14
Loss after 11811043 batches: 0.0501
trigger times: 15
Loss after 11812006 batches: 0.0501
trigger times: 16
Loss after 11812969 batches: 0.0495
trigger times: 17
Loss after 11813932 batches: 0.0478
trigger times: 18
Loss after 11814895 batches: 0.0478
trigger times: 19
Loss after 11815858 batches: 0.0477
trigger times: 20
Loss after 11816821 batches: 0.0472
trigger times: 21
Loss after 11817784 batches: 0.0473
trigger times: 22
Loss after 11818747 batches: 0.0468
trigger times: 23
Loss after 11819710 batches: 0.0460
trigger times: 24
Loss after 11820673 batches: 0.0470
trigger times: 25
Early stopping!
Start to test process.
Loss after 11821636 batches: 0.0446
Time to train on one home:  55.10766077041626
trigger times: 0
Loss after 11822599 batches: 0.0901
trigger times: 1
Loss after 11823562 batches: 0.0780
trigger times: 2
Loss after 11824525 batches: 0.0762
trigger times: 3
Loss after 11825488 batches: 0.0746
trigger times: 0
Loss after 11826451 batches: 0.0719
trigger times: 1
Loss after 11827414 batches: 0.0685
trigger times: 2
Loss after 11828377 batches: 0.0661
trigger times: 3
Loss after 11829340 batches: 0.0646
trigger times: 4
Loss after 11830303 batches: 0.0649
trigger times: 5
Loss after 11831266 batches: 0.0624
trigger times: 6
Loss after 11832229 batches: 0.0619
trigger times: 7
Loss after 11833192 batches: 0.0615
trigger times: 8
Loss after 11834155 batches: 0.0606
trigger times: 0
Loss after 11835118 batches: 0.0603
trigger times: 1
Loss after 11836081 batches: 0.0609
trigger times: 2
Loss after 11837044 batches: 0.0605
trigger times: 3
Loss after 11838007 batches: 0.0595
trigger times: 4
Loss after 11838970 batches: 0.0602
trigger times: 5
Loss after 11839933 batches: 0.0599
trigger times: 6
Loss after 11840896 batches: 0.0592
trigger times: 7
Loss after 11841859 batches: 0.0596
trigger times: 8
Loss after 11842822 batches: 0.0579
trigger times: 9
Loss after 11843785 batches: 0.0578
trigger times: 10
Loss after 11844748 batches: 0.0577
trigger times: 11
Loss after 11845711 batches: 0.0575
trigger times: 12
Loss after 11846674 batches: 0.0572
trigger times: 13
Loss after 11847637 batches: 0.0563
trigger times: 14
Loss after 11848600 batches: 0.0561
trigger times: 15
Loss after 11849563 batches: 0.0553
trigger times: 16
Loss after 11850526 batches: 0.0552
trigger times: 17
Loss after 11851489 batches: 0.0556
trigger times: 18
Loss after 11852452 batches: 0.0558
trigger times: 19
Loss after 11853415 batches: 0.0552
trigger times: 20
Loss after 11854378 batches: 0.0563
trigger times: 21
Loss after 11855341 batches: 0.0556
trigger times: 22
Loss after 11856304 batches: 0.0556
trigger times: 23
Loss after 11857267 batches: 0.0540
trigger times: 24
Loss after 11858230 batches: 0.0532
trigger times: 25
Early stopping!
Start to test process.
Loss after 11859193 batches: 0.0533
Time to train on one home:  69.44335007667542
trigger times: 0
Loss after 11860156 batches: 0.0268
trigger times: 1
Loss after 11861119 batches: 0.0226
trigger times: 2
Loss after 11862082 batches: 0.0205
trigger times: 3
Loss after 11863045 batches: 0.0185
trigger times: 0
Loss after 11864008 batches: 0.0175
trigger times: 1
Loss after 11864971 batches: 0.0168
trigger times: 2
Loss after 11865934 batches: 0.0160
trigger times: 3
Loss after 11866897 batches: 0.0159
trigger times: 4
Loss after 11867860 batches: 0.0151
trigger times: 5
Loss after 11868823 batches: 0.0148
trigger times: 6
Loss after 11869786 batches: 0.0150
trigger times: 7
Loss after 11870749 batches: 0.0148
trigger times: 8
Loss after 11871712 batches: 0.0143
trigger times: 9
Loss after 11872675 batches: 0.0148
trigger times: 10
Loss after 11873638 batches: 0.0144
trigger times: 11
Loss after 11874601 batches: 0.0136
trigger times: 12
Loss after 11875564 batches: 0.0137
trigger times: 13
Loss after 11876527 batches: 0.0136
trigger times: 14
Loss after 11877490 batches: 0.0133
trigger times: 15
Loss after 11878453 batches: 0.0130
trigger times: 16
Loss after 11879416 batches: 0.0129
trigger times: 17
Loss after 11880379 batches: 0.0140
trigger times: 18
Loss after 11881342 batches: 0.0141
trigger times: 0
Loss after 11882305 batches: 0.0134
trigger times: 1
Loss after 11883268 batches: 0.0131
trigger times: 2
Loss after 11884231 batches: 0.0134
trigger times: 3
Loss after 11885194 batches: 0.0126
trigger times: 4
Loss after 11886157 batches: 0.0128
trigger times: 5
Loss after 11887120 batches: 0.0131
trigger times: 6
Loss after 11888083 batches: 0.0124
trigger times: 7
Loss after 11889046 batches: 0.0124
trigger times: 8
Loss after 11890009 batches: 0.0124
trigger times: 9
Loss after 11890972 batches: 0.0124
trigger times: 10
Loss after 11891935 batches: 0.0124
trigger times: 11
Loss after 11892898 batches: 0.0124
trigger times: 12
Loss after 11893861 batches: 0.0130
trigger times: 13
Loss after 11894824 batches: 0.0125
trigger times: 14
Loss after 11895787 batches: 0.0122
trigger times: 15
Loss after 11896750 batches: 0.0121
trigger times: 16
Loss after 11897713 batches: 0.0116
trigger times: 17
Loss after 11898676 batches: 0.0119
trigger times: 18
Loss after 11899639 batches: 0.0115
trigger times: 19
Loss after 11900602 batches: 0.0113
trigger times: 20
Loss after 11901565 batches: 0.0107
trigger times: 21
Loss after 11902528 batches: 0.0110
trigger times: 22
Loss after 11903491 batches: 0.0112
trigger times: 0
Loss after 11904454 batches: 0.0124
trigger times: 1
Loss after 11905417 batches: 0.0121
trigger times: 2
Loss after 11906380 batches: 0.0124
trigger times: 3
Loss after 11907343 batches: 0.0114
trigger times: 4
Loss after 11908306 batches: 0.0111
trigger times: 5
Loss after 11909269 batches: 0.0109
trigger times: 6
Loss after 11910232 batches: 0.0106
trigger times: 7
Loss after 11911195 batches: 0.0105
trigger times: 8
Loss after 11912158 batches: 0.0108
trigger times: 9
Loss after 11913121 batches: 0.0106
trigger times: 10
Loss after 11914084 batches: 0.0105
trigger times: 11
Loss after 11915047 batches: 0.0110
trigger times: 12
Loss after 11916010 batches: 0.0111
trigger times: 13
Loss after 11916973 batches: 0.0109
trigger times: 14
Loss after 11917936 batches: 0.0109
trigger times: 15
Loss after 11918899 batches: 0.0112
trigger times: 16
Loss after 11919862 batches: 0.0099
trigger times: 17
Loss after 11920825 batches: 0.0107
trigger times: 18
Loss after 11921788 batches: 0.0097
trigger times: 19
Loss after 11922751 batches: 0.0101
trigger times: 20
Loss after 11923714 batches: 0.0106
trigger times: 21
Loss after 11924677 batches: 0.0099
trigger times: 22
Loss after 11925640 batches: 0.0096
trigger times: 23
Loss after 11926603 batches: 0.0098
trigger times: 24
Loss after 11927566 batches: 0.0101
trigger times: 25
Early stopping!
Start to test process.
Loss after 11928529 batches: 0.0100
Time to train on one home:  93.58506965637207
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11929492 batches: 0.0516
trigger times: 1
Loss after 11930455 batches: 0.0241
trigger times: 2
Loss after 11931418 batches: 0.0198
trigger times: 3
Loss after 11932381 batches: 0.0166
trigger times: 4
Loss after 11933344 batches: 0.0157
trigger times: 5
Loss after 11934307 batches: 0.0152
trigger times: 6
Loss after 11935270 batches: 0.0143
trigger times: 7
Loss after 11936233 batches: 0.0137
trigger times: 8
Loss after 11937196 batches: 0.0135
trigger times: 9
Loss after 11938159 batches: 0.0134
trigger times: 10
Loss after 11939122 batches: 0.0133
trigger times: 11
Loss after 11940085 batches: 0.0134
trigger times: 12
Loss after 11941048 batches: 0.0133
trigger times: 13
Loss after 11942011 batches: 0.0131
trigger times: 14
Loss after 11942974 batches: 0.0128
trigger times: 15
Loss after 11943937 batches: 0.0129
trigger times: 16
Loss after 11944900 batches: 0.0127
trigger times: 17
Loss after 11945863 batches: 0.0127
trigger times: 18
Loss after 11946826 batches: 0.0126
trigger times: 19
Loss after 11947789 batches: 0.0126
trigger times: 20
Loss after 11948752 batches: 0.0122
trigger times: 21
Loss after 11949715 batches: 0.0122
trigger times: 22
Loss after 11950678 batches: 0.0128
trigger times: 23
Loss after 11951641 batches: 0.0122
trigger times: 24
Loss after 11952604 batches: 0.0118
trigger times: 25
Early stopping!
Start to test process.
Loss after 11953567 batches: 0.0120
Time to train on one home:  57.04686975479126
trigger times: 0
Loss after 11954530 batches: 0.1021
trigger times: 0
Loss after 11955493 batches: 0.0938
trigger times: 0
Loss after 11956456 batches: 0.0862
trigger times: 1
Loss after 11957419 batches: 0.0830
trigger times: 2
Loss after 11958382 batches: 0.0804
trigger times: 3
Loss after 11959345 batches: 0.0772
trigger times: 4
Loss after 11960308 batches: 0.0759
trigger times: 5
Loss after 11961271 batches: 0.0743
trigger times: 6
Loss after 11962234 batches: 0.0732
trigger times: 7
Loss after 11963197 batches: 0.0714
trigger times: 8
Loss after 11964160 batches: 0.0709
trigger times: 9
Loss after 11965123 batches: 0.0704
trigger times: 10
Loss after 11966086 batches: 0.0697
trigger times: 11
Loss after 11967049 batches: 0.0695
trigger times: 12
Loss after 11968012 batches: 0.0673
trigger times: 13
Loss after 11968975 batches: 0.0658
trigger times: 14
Loss after 11969938 batches: 0.0668
trigger times: 15
Loss after 11970901 batches: 0.0643
trigger times: 16
Loss after 11971864 batches: 0.0632
trigger times: 17
Loss after 11972827 batches: 0.0644
trigger times: 18
Loss after 11973790 batches: 0.0644
trigger times: 19
Loss after 11974753 batches: 0.0639
trigger times: 20
Loss after 11975716 batches: 0.0634
trigger times: 21
Loss after 11976679 batches: 0.0630
trigger times: 22
Loss after 11977642 batches: 0.0630
trigger times: 23
Loss after 11978605 batches: 0.0626
trigger times: 24
Loss after 11979568 batches: 0.0635
trigger times: 25
Early stopping!
Start to test process.
Loss after 11980531 batches: 0.0616
Time to train on one home:  59.08428955078125
trigger times: 0
Loss after 11981494 batches: 0.0516
trigger times: 0
Loss after 11982457 batches: 0.0410
trigger times: 0
Loss after 11983420 batches: 0.0336
trigger times: 1
Loss after 11984383 batches: 0.0303
trigger times: 2
Loss after 11985346 batches: 0.0276
trigger times: 3
Loss after 11986309 batches: 0.0276
trigger times: 4
Loss after 11987272 batches: 0.0270
trigger times: 5
Loss after 11988235 batches: 0.0256
trigger times: 6
Loss after 11989198 batches: 0.0259
trigger times: 7
Loss after 11990161 batches: 0.0248
trigger times: 8
Loss after 11991124 batches: 0.0241
trigger times: 9
Loss after 11992087 batches: 0.0248
trigger times: 10
Loss after 11993050 batches: 0.0231
trigger times: 11
Loss after 11994013 batches: 0.0239
trigger times: 12
Loss after 11994976 batches: 0.0233
trigger times: 13
Loss after 11995939 batches: 0.0226
trigger times: 14
Loss after 11996902 batches: 0.0213
trigger times: 15
Loss after 11997865 batches: 0.0212
trigger times: 16
Loss after 11998828 batches: 0.0221
trigger times: 17
Loss after 11999791 batches: 0.0207
trigger times: 18
Loss after 12000754 batches: 0.0213
trigger times: 19
Loss after 12001717 batches: 0.0212
trigger times: 20
Loss after 12002680 batches: 0.0227
trigger times: 21
Loss after 12003643 batches: 0.0230
trigger times: 22
Loss after 12004606 batches: 0.0223
trigger times: 23
Loss after 12005569 batches: 0.0216
trigger times: 24
Loss after 12006532 batches: 0.0211
trigger times: 25
Early stopping!
Start to test process.
Loss after 12007495 batches: 0.0208
Time to train on one home:  57.20179891586304
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12008458 batches: 0.0777
trigger times: 1
Loss after 12009421 batches: 0.0723
trigger times: 2
Loss after 12010384 batches: 0.0693
trigger times: 3
Loss after 12011347 batches: 0.0668
trigger times: 4
Loss after 12012310 batches: 0.0645
trigger times: 5
Loss after 12013273 batches: 0.0640
trigger times: 6
Loss after 12014236 batches: 0.0617
trigger times: 7
Loss after 12015199 batches: 0.0619
trigger times: 8
Loss after 12016162 batches: 0.0605
trigger times: 9
Loss after 12017125 batches: 0.0601
trigger times: 10
Loss after 12018088 batches: 0.0602
trigger times: 11
Loss after 12019051 batches: 0.0590
trigger times: 12
Loss after 12020014 batches: 0.0587
trigger times: 13
Loss after 12020977 batches: 0.0589
trigger times: 14
Loss after 12021940 batches: 0.0582
trigger times: 15
Loss after 12022903 batches: 0.0572
trigger times: 16
Loss after 12023866 batches: 0.0561
trigger times: 17
Loss after 12024829 batches: 0.0566
trigger times: 18
Loss after 12025792 batches: 0.0561
trigger times: 19
Loss after 12026755 batches: 0.0574
trigger times: 20
Loss after 12027718 batches: 0.0577
trigger times: 21
Loss after 12028681 batches: 0.0562
trigger times: 22
Loss after 12029644 batches: 0.0567
trigger times: 23
Loss after 12030607 batches: 0.0547
trigger times: 24
Loss after 12031570 batches: 0.0556
trigger times: 25
Early stopping!
Start to test process.
Loss after 12032533 batches: 0.0556
Time to train on one home:  53.23297643661499
trigger times: 0
Loss after 12033496 batches: 0.1078
trigger times: 0
Loss after 12034459 batches: 0.0682
trigger times: 0
Loss after 12035422 batches: 0.0640
trigger times: 1
Loss after 12036385 batches: 0.0551
trigger times: 2
Loss after 12037348 batches: 0.0533
trigger times: 3
Loss after 12038311 batches: 0.0499
trigger times: 4
Loss after 12039274 batches: 0.0487
trigger times: 5
Loss after 12040237 batches: 0.0459
trigger times: 6
Loss after 12041200 batches: 0.0456
trigger times: 7
Loss after 12042163 batches: 0.0447
trigger times: 8
Loss after 12043126 batches: 0.0434
trigger times: 9
Loss after 12044089 batches: 0.0435
trigger times: 10
Loss after 12045052 batches: 0.0427
trigger times: 11
Loss after 12046015 batches: 0.0420
trigger times: 12
Loss after 12046978 batches: 0.0415
trigger times: 13
Loss after 12047941 batches: 0.0410
trigger times: 14
Loss after 12048904 batches: 0.0418
trigger times: 15
Loss after 12049867 batches: 0.0411
trigger times: 16
Loss after 12050830 batches: 0.0411
trigger times: 17
Loss after 12051793 batches: 0.0392
trigger times: 18
Loss after 12052756 batches: 0.0397
trigger times: 19
Loss after 12053719 batches: 0.0401
trigger times: 20
Loss after 12054682 batches: 0.0397
trigger times: 21
Loss after 12055645 batches: 0.0396
trigger times: 22
Loss after 12056608 batches: 0.0394
trigger times: 23
Loss after 12057571 batches: 0.0389
trigger times: 24
Loss after 12058534 batches: 0.0389
trigger times: 25
Early stopping!
Start to test process.
Loss after 12059497 batches: 0.0383
Time to train on one home:  58.63132429122925
trigger times: 0
Loss after 12060460 batches: 0.0775
trigger times: 1
Loss after 12061423 batches: 0.0698
trigger times: 2
Loss after 12062386 batches: 0.0685
trigger times: 3
Loss after 12063349 batches: 0.0656
trigger times: 4
Loss after 12064312 batches: 0.0628
trigger times: 5
Loss after 12065275 batches: 0.0617
trigger times: 6
Loss after 12066238 batches: 0.0610
trigger times: 7
Loss after 12067201 batches: 0.0592
trigger times: 8
Loss after 12068164 batches: 0.0580
trigger times: 9
Loss after 12069127 batches: 0.0583
trigger times: 10
Loss after 12070090 batches: 0.0574
trigger times: 11
Loss after 12071053 batches: 0.0561
trigger times: 12
Loss after 12072016 batches: 0.0558
trigger times: 13
Loss after 12072979 batches: 0.0576
trigger times: 14
Loss after 12073942 batches: 0.0556
trigger times: 15
Loss after 12074905 batches: 0.0547
trigger times: 16
Loss after 12075868 batches: 0.0546
trigger times: 17
Loss after 12076831 batches: 0.0533
trigger times: 18
Loss after 12077794 batches: 0.0540
trigger times: 19
Loss after 12078757 batches: 0.0508
trigger times: 20
Loss after 12079720 batches: 0.0518
trigger times: 21
Loss after 12080683 batches: 0.0519
trigger times: 22
Loss after 12081646 batches: 0.0519
trigger times: 23
Loss after 12082609 batches: 0.0530
trigger times: 24
Loss after 12083572 batches: 0.0520
trigger times: 25
Early stopping!
Start to test process.
Loss after 12084535 batches: 0.0532
Time to train on one home:  55.53621864318848
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12085498 batches: 0.0643
trigger times: 1
Loss after 12086461 batches: 0.0516
trigger times: 2
Loss after 12087424 batches: 0.0499
trigger times: 3
Loss after 12088387 batches: 0.0440
trigger times: 4
Loss after 12089350 batches: 0.0385
trigger times: 5
Loss after 12090313 batches: 0.0358
trigger times: 6
Loss after 12091276 batches: 0.0341
trigger times: 7
Loss after 12092239 batches: 0.0328
trigger times: 0
Loss after 12093202 batches: 0.0315
trigger times: 1
Loss after 12094165 batches: 0.0304
trigger times: 2
Loss after 12095128 batches: 0.0292
trigger times: 3
Loss after 12096091 batches: 0.0289
trigger times: 4
Loss after 12097054 batches: 0.0284
trigger times: 5
Loss after 12098017 batches: 0.0281
trigger times: 6
Loss after 12098980 batches: 0.0272
trigger times: 7
Loss after 12099943 batches: 0.0270
trigger times: 8
Loss after 12100906 batches: 0.0271
trigger times: 9
Loss after 12101869 batches: 0.0269
trigger times: 10
Loss after 12102832 batches: 0.0261
trigger times: 11
Loss after 12103795 batches: 0.0254
trigger times: 12
Loss after 12104758 batches: 0.0259
trigger times: 13
Loss after 12105721 batches: 0.0253
trigger times: 14
Loss after 12106684 batches: 0.0251
trigger times: 15
Loss after 12107647 batches: 0.0246
trigger times: 16
Loss after 12108610 batches: 0.0248
trigger times: 17
Loss after 12109573 batches: 0.0238
trigger times: 18
Loss after 12110536 batches: 0.0237
trigger times: 19
Loss after 12111499 batches: 0.0246
trigger times: 20
Loss after 12112462 batches: 0.0241
trigger times: 21
Loss after 12113425 batches: 0.0238
trigger times: 22
Loss after 12114388 batches: 0.0245
trigger times: 23
Loss after 12115351 batches: 0.0241
trigger times: 24
Loss after 12116314 batches: 0.0234
trigger times: 25
Early stopping!
Start to test process.
Loss after 12117277 batches: 0.0230
Time to train on one home:  65.73016262054443
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12118240 batches: 0.0936
trigger times: 0
Loss after 12119203 batches: 0.0548
trigger times: 1
Loss after 12120166 batches: 0.0471
trigger times: 0
Loss after 12121129 batches: 0.0429
trigger times: 0
Loss after 12122092 batches: 0.0403
trigger times: 1
Loss after 12123055 batches: 0.0355
trigger times: 2
Loss after 12124018 batches: 0.0335
trigger times: 3
Loss after 12124981 batches: 0.0300
trigger times: 4
Loss after 12125944 batches: 0.0285
trigger times: 5
Loss after 12126907 batches: 0.0265
trigger times: 6
Loss after 12127870 batches: 0.0260
trigger times: 7
Loss after 12128833 batches: 0.0258
trigger times: 8
Loss after 12129796 batches: 0.0243
trigger times: 9
Loss after 12130759 batches: 0.0231
trigger times: 10
Loss after 12131722 batches: 0.0240
trigger times: 11
Loss after 12132685 batches: 0.0226
trigger times: 12
Loss after 12133648 batches: 0.0233
trigger times: 13
Loss after 12134611 batches: 0.0235
trigger times: 14
Loss after 12135574 batches: 0.0208
trigger times: 15
Loss after 12136537 batches: 0.0222
trigger times: 16
Loss after 12137500 batches: 0.0246
trigger times: 17
Loss after 12138463 batches: 0.0235
trigger times: 18
Loss after 12139426 batches: 0.0224
trigger times: 19
Loss after 12140389 batches: 0.0226
trigger times: 20
Loss after 12141352 batches: 0.0217
trigger times: 21
Loss after 12142315 batches: 0.0221
trigger times: 22
Loss after 12143278 batches: 0.0200
trigger times: 23
Loss after 12144241 batches: 0.0197
trigger times: 24
Loss after 12145204 batches: 0.0198
trigger times: 25
Early stopping!
Start to test process.
Loss after 12146167 batches: 0.0189
Time to train on one home:  60.29682779312134
trigger times: 0
Loss after 12147096 batches: 0.1308
trigger times: 0
Loss after 12148025 batches: 0.0743
trigger times: 0
Loss after 12148954 batches: 0.0593
trigger times: 1
Loss after 12149883 batches: 0.0482
trigger times: 0
Loss after 12150812 batches: 0.0419
trigger times: 1
Loss after 12151741 batches: 0.0369
trigger times: 2
Loss after 12152670 batches: 0.0350
trigger times: 0
Loss after 12153599 batches: 0.0346
trigger times: 0
Loss after 12154528 batches: 0.0303
trigger times: 1
Loss after 12155457 batches: 0.0313
trigger times: 2
Loss after 12156386 batches: 0.0297
trigger times: 3
Loss after 12157315 batches: 0.0313
trigger times: 4
Loss after 12158244 batches: 0.0287
trigger times: 5
Loss after 12159173 batches: 0.0295
trigger times: 6
Loss after 12160102 batches: 0.0302
trigger times: 7
Loss after 12161031 batches: 0.0273
trigger times: 8
Loss after 12161960 batches: 0.0271
trigger times: 9
Loss after 12162889 batches: 0.0254
trigger times: 10
Loss after 12163818 batches: 0.0275
trigger times: 11
Loss after 12164747 batches: 0.0276
trigger times: 12
Loss after 12165676 batches: 0.0271
trigger times: 13
Loss after 12166605 batches: 0.0258
trigger times: 14
Loss after 12167534 batches: 0.0280
trigger times: 15
Loss after 12168463 batches: 0.0275
trigger times: 16
Loss after 12169392 batches: 0.0281
trigger times: 17
Loss after 12170321 batches: 0.0283
trigger times: 18
Loss after 12171250 batches: 0.0248
trigger times: 19
Loss after 12172179 batches: 0.0243
trigger times: 20
Loss after 12173108 batches: 0.0238
trigger times: 21
Loss after 12174037 batches: 0.0249
trigger times: 22
Loss after 12174966 batches: 0.0241
trigger times: 23
Loss after 12175895 batches: 0.0240
trigger times: 24
Loss after 12176824 batches: 0.0240
trigger times: 25
Early stopping!
Start to test process.
Loss after 12177753 batches: 0.0234
Time to train on one home:  62.480125188827515
trigger times: 0
Loss after 12178715 batches: 0.0720
trigger times: 1
Loss after 12179677 batches: 0.0654
trigger times: 2
Loss after 12180639 batches: 0.0640
trigger times: 3
Loss after 12181601 batches: 0.0611
trigger times: 4
Loss after 12182563 batches: 0.0591
trigger times: 5
Loss after 12183525 batches: 0.0570
trigger times: 6
Loss after 12184487 batches: 0.0564
trigger times: 7
Loss after 12185449 batches: 0.0552
trigger times: 8
Loss after 12186411 batches: 0.0542
trigger times: 9
Loss after 12187373 batches: 0.0543
trigger times: 10
Loss after 12188335 batches: 0.0538
trigger times: 11
Loss after 12189297 batches: 0.0546
trigger times: 12
Loss after 12190259 batches: 0.0542
trigger times: 13
Loss after 12191221 batches: 0.0538
trigger times: 14
Loss after 12192183 batches: 0.0528
trigger times: 15
Loss after 12193145 batches: 0.0528
trigger times: 16
Loss after 12194107 batches: 0.0528
trigger times: 17
Loss after 12195069 batches: 0.0510
trigger times: 18
Loss after 12196031 batches: 0.0514
trigger times: 19
Loss after 12196993 batches: 0.0514
trigger times: 20
Loss after 12197955 batches: 0.0520
trigger times: 21
Loss after 12198917 batches: 0.0521
trigger times: 22
Loss after 12199879 batches: 0.0531
trigger times: 23
Loss after 12200841 batches: 0.0524
trigger times: 24
Loss after 12201803 batches: 0.0508
trigger times: 25
Early stopping!
Start to test process.
Loss after 12202765 batches: 0.0514
Time to train on one home:  57.267696142196655
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12203728 batches: 0.0585
trigger times: 1
Loss after 12204691 batches: 0.0460
trigger times: 2
Loss after 12205654 batches: 0.0457
trigger times: 0
Loss after 12206617 batches: 0.0427
trigger times: 1
Loss after 12207580 batches: 0.0390
trigger times: 2
Loss after 12208543 batches: 0.0380
trigger times: 3
Loss after 12209506 batches: 0.0367
trigger times: 4
Loss after 12210469 batches: 0.0364
trigger times: 5
Loss after 12211432 batches: 0.0356
trigger times: 6
Loss after 12212395 batches: 0.0351
trigger times: 7
Loss after 12213358 batches: 0.0345
trigger times: 8
Loss after 12214321 batches: 0.0343
trigger times: 9
Loss after 12215284 batches: 0.0326
trigger times: 10
Loss after 12216247 batches: 0.0325
trigger times: 11
Loss after 12217210 batches: 0.0323
trigger times: 12
Loss after 12218173 batches: 0.0319
trigger times: 13
Loss after 12219136 batches: 0.0319
trigger times: 14
Loss after 12220099 batches: 0.0318
trigger times: 15
Loss after 12221062 batches: 0.0316
trigger times: 16
Loss after 12222025 batches: 0.0313
trigger times: 17
Loss after 12222988 batches: 0.0317
trigger times: 18
Loss after 12223951 batches: 0.0309
trigger times: 19
Loss after 12224914 batches: 0.0313
trigger times: 20
Loss after 12225877 batches: 0.0317
trigger times: 21
Loss after 12226840 batches: 0.0309
trigger times: 22
Loss after 12227803 batches: 0.0309
trigger times: 23
Loss after 12228766 batches: 0.0302
trigger times: 24
Loss after 12229729 batches: 0.0303
trigger times: 25
Early stopping!
Start to test process.
Loss after 12230692 batches: 0.0301
Time to train on one home:  59.55222845077515
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12231655 batches: 0.0562
trigger times: 0
Loss after 12232618 batches: 0.0472
trigger times: 0
Loss after 12233581 batches: 0.0456
trigger times: 1
Loss after 12234544 batches: 0.0421
trigger times: 2
Loss after 12235507 batches: 0.0396
trigger times: 3
Loss after 12236470 batches: 0.0386
trigger times: 4
Loss after 12237433 batches: 0.0375
trigger times: 5
Loss after 12238396 batches: 0.0374
trigger times: 6
Loss after 12239359 batches: 0.0358
trigger times: 7
Loss after 12240322 batches: 0.0347
trigger times: 8
Loss after 12241285 batches: 0.0345
trigger times: 9
Loss after 12242248 batches: 0.0333
trigger times: 10
Loss after 12243211 batches: 0.0322
trigger times: 11
Loss after 12244174 batches: 0.0322
trigger times: 12
Loss after 12245137 batches: 0.0326
trigger times: 13
Loss after 12246100 batches: 0.0320
trigger times: 14
Loss after 12247063 batches: 0.0314
trigger times: 15
Loss after 12248026 batches: 0.0319
trigger times: 16
Loss after 12248989 batches: 0.0307
trigger times: 17
Loss after 12249952 batches: 0.0306
trigger times: 18
Loss after 12250915 batches: 0.0320
trigger times: 19
Loss after 12251878 batches: 0.0317
trigger times: 20
Loss after 12252841 batches: 0.0311
trigger times: 21
Loss after 12253804 batches: 0.0303
trigger times: 22
Loss after 12254767 batches: 0.0292
trigger times: 23
Loss after 12255730 batches: 0.0288
trigger times: 24
Loss after 12256693 batches: 0.0295
trigger times: 25
Early stopping!
Start to test process.
Loss after 12257656 batches: 0.0286
Time to train on one home:  54.81608533859253
trigger times: 0
Loss after 12258619 batches: 0.1084
trigger times: 0
Loss after 12259582 batches: 0.0966
trigger times: 1
Loss after 12260545 batches: 0.0954
trigger times: 2
Loss after 12261508 batches: 0.0873
trigger times: 3
Loss after 12262471 batches: 0.0848
trigger times: 4
Loss after 12263434 batches: 0.0848
trigger times: 5
Loss after 12264397 batches: 0.0820
trigger times: 6
Loss after 12265360 batches: 0.0808
trigger times: 7
Loss after 12266323 batches: 0.0805
trigger times: 8
Loss after 12267286 batches: 0.0768
trigger times: 9
Loss after 12268249 batches: 0.0782
trigger times: 10
Loss after 12269212 batches: 0.0774
trigger times: 11
Loss after 12270175 batches: 0.0763
trigger times: 12
Loss after 12271138 batches: 0.0761
trigger times: 13
Loss after 12272101 batches: 0.0765
trigger times: 14
Loss after 12273064 batches: 0.0752
trigger times: 15
Loss after 12274027 batches: 0.0735
trigger times: 16
Loss after 12274990 batches: 0.0725
trigger times: 17
Loss after 12275953 batches: 0.0720
trigger times: 18
Loss after 12276916 batches: 0.0710
trigger times: 19
Loss after 12277879 batches: 0.0721
trigger times: 20
Loss after 12278842 batches: 0.0730
trigger times: 21
Loss after 12279805 batches: 0.0695
trigger times: 22
Loss after 12280768 batches: 0.0708
trigger times: 23
Loss after 12281731 batches: 0.0697
trigger times: 24
Loss after 12282694 batches: 0.0680
trigger times: 25
Early stopping!
Start to test process.
Loss after 12283657 batches: 0.0700
Time to train on one home:  56.37432813644409
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12284620 batches: 0.1094
trigger times: 1
Loss after 12285583 batches: 0.0614
trigger times: 2
Loss after 12286546 batches: 0.0619
trigger times: 0
Loss after 12287509 batches: 0.0528
trigger times: 0
Loss after 12288472 batches: 0.0468
trigger times: 1
Loss after 12289435 batches: 0.0425
trigger times: 2
Loss after 12290398 batches: 0.0401
trigger times: 3
Loss after 12291361 batches: 0.0377
trigger times: 4
Loss after 12292324 batches: 0.0369
trigger times: 5
Loss after 12293287 batches: 0.0362
trigger times: 6
Loss after 12294250 batches: 0.0344
trigger times: 7
Loss after 12295213 batches: 0.0346
trigger times: 8
Loss after 12296176 batches: 0.0325
trigger times: 9
Loss after 12297139 batches: 0.0329
trigger times: 10
Loss after 12298102 batches: 0.0327
trigger times: 11
Loss after 12299065 batches: 0.0319
trigger times: 12
Loss after 12300028 batches: 0.0310
trigger times: 13
Loss after 12300991 batches: 0.0314
trigger times: 14
Loss after 12301954 batches: 0.0307
trigger times: 15
Loss after 12302917 batches: 0.0309
trigger times: 16
Loss after 12303880 batches: 0.0298
trigger times: 17
Loss after 12304843 batches: 0.0299
trigger times: 0
Loss after 12305806 batches: 0.0305
trigger times: 1
Loss after 12306769 batches: 0.0298
trigger times: 2
Loss after 12307732 batches: 0.0297
trigger times: 3
Loss after 12308695 batches: 0.0294
trigger times: 4
Loss after 12309658 batches: 0.0288
trigger times: 0
Loss after 12310621 batches: 0.0287
trigger times: 1
Loss after 12311584 batches: 0.0284
trigger times: 2
Loss after 12312547 batches: 0.0284
trigger times: 3
Loss after 12313510 batches: 0.0278
trigger times: 4
Loss after 12314473 batches: 0.0277
trigger times: 5
Loss after 12315436 batches: 0.0272
trigger times: 6
Loss after 12316399 batches: 0.0268
trigger times: 7
Loss after 12317362 batches: 0.0275
trigger times: 8
Loss after 12318325 batches: 0.0269
trigger times: 9
Loss after 12319288 batches: 0.0258
trigger times: 10
Loss after 12320251 batches: 0.0269
trigger times: 11
Loss after 12321214 batches: 0.0265
trigger times: 12
Loss after 12322177 batches: 0.0256
trigger times: 13
Loss after 12323140 batches: 0.0252
trigger times: 14
Loss after 12324103 batches: 0.0267
trigger times: 15
Loss after 12325066 batches: 0.0271
trigger times: 0
Loss after 12326029 batches: 0.0258
trigger times: 1
Loss after 12326992 batches: 0.0248
trigger times: 2
Loss after 12327955 batches: 0.0279
trigger times: 3
Loss after 12328918 batches: 0.0270
trigger times: 4
Loss after 12329881 batches: 0.0258
trigger times: 5
Loss after 12330844 batches: 0.0252
trigger times: 6
Loss after 12331807 batches: 0.0246
trigger times: 7
Loss after 12332770 batches: 0.0250
trigger times: 8
Loss after 12333733 batches: 0.0262
trigger times: 9
Loss after 12334696 batches: 0.0256
trigger times: 10
Loss after 12335659 batches: 0.0247
trigger times: 11
Loss after 12336622 batches: 0.0242
trigger times: 12
Loss after 12337585 batches: 0.0234
trigger times: 13
Loss after 12338548 batches: 0.0237
trigger times: 14
Loss after 12339511 batches: 0.0246
trigger times: 15
Loss after 12340474 batches: 0.0264
trigger times: 16
Loss after 12341437 batches: 0.0267
trigger times: 17
Loss after 12342400 batches: 0.0239
trigger times: 18
Loss after 12343363 batches: 0.0250
trigger times: 19
Loss after 12344326 batches: 0.0226
trigger times: 20
Loss after 12345289 batches: 0.0225
trigger times: 21
Loss after 12346252 batches: 0.0237
trigger times: 22
Loss after 12347215 batches: 0.0227
trigger times: 23
Loss after 12348178 batches: 0.0226
trigger times: 24
Loss after 12349141 batches: 0.0246
trigger times: 25
Early stopping!
Start to test process.
Loss after 12350104 batches: 0.0224
Time to train on one home:  90.66821217536926
trigger times: 0
Loss after 12351063 batches: 0.1148
trigger times: 1
Loss after 12352022 batches: 0.0581
trigger times: 0
Loss after 12352981 batches: 0.0438
trigger times: 1
Loss after 12353940 batches: 0.0339
trigger times: 2
Loss after 12354899 batches: 0.0287
trigger times: 3
Loss after 12355858 batches: 0.0250
trigger times: 4
Loss after 12356817 batches: 0.0241
trigger times: 0
Loss after 12357776 batches: 0.0227
trigger times: 1
Loss after 12358735 batches: 0.0216
trigger times: 2
Loss after 12359694 batches: 0.0213
trigger times: 3
Loss after 12360653 batches: 0.0210
trigger times: 4
Loss after 12361612 batches: 0.0200
trigger times: 5
Loss after 12362571 batches: 0.0197
trigger times: 6
Loss after 12363530 batches: 0.0201
trigger times: 7
Loss after 12364489 batches: 0.0192
trigger times: 8
Loss after 12365448 batches: 0.0185
trigger times: 9
Loss after 12366407 batches: 0.0181
trigger times: 10
Loss after 12367366 batches: 0.0181
trigger times: 11
Loss after 12368325 batches: 0.0174
trigger times: 12
Loss after 12369284 batches: 0.0176
trigger times: 13
Loss after 12370243 batches: 0.0181
trigger times: 0
Loss after 12371202 batches: 0.0177
trigger times: 1
Loss after 12372161 batches: 0.0166
trigger times: 2
Loss after 12373120 batches: 0.0173
trigger times: 3
Loss after 12374079 batches: 0.0166
trigger times: 4
Loss after 12375038 batches: 0.0168
trigger times: 5
Loss after 12375997 batches: 0.0161
trigger times: 6
Loss after 12376956 batches: 0.0164
trigger times: 7
Loss after 12377915 batches: 0.0164
trigger times: 8
Loss after 12378874 batches: 0.0161
trigger times: 9
Loss after 12379833 batches: 0.0156
trigger times: 10
Loss after 12380792 batches: 0.0158
trigger times: 11
Loss after 12381751 batches: 0.0148
trigger times: 12
Loss after 12382710 batches: 0.0145
trigger times: 13
Loss after 12383669 batches: 0.0168
trigger times: 14
Loss after 12384628 batches: 0.0171
trigger times: 15
Loss after 12385587 batches: 0.0164
trigger times: 16
Loss after 12386546 batches: 0.0162
trigger times: 17
Loss after 12387505 batches: 0.0159
trigger times: 18
Loss after 12388464 batches: 0.0157
trigger times: 19
Loss after 12389423 batches: 0.0153
trigger times: 20
Loss after 12390382 batches: 0.0159
trigger times: 21
Loss after 12391341 batches: 0.0150
trigger times: 22
Loss after 12392300 batches: 0.0144
trigger times: 23
Loss after 12393259 batches: 0.0144
trigger times: 24
Loss after 12394218 batches: 0.0145
trigger times: 25
Early stopping!
Start to test process.
Loss after 12395177 batches: 0.0138
Time to train on one home:  77.93349409103394
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12396140 batches: 0.0513
trigger times: 1
Loss after 12397103 batches: 0.0281
trigger times: 2
Loss after 12398066 batches: 0.0262
trigger times: 3
Loss after 12399029 batches: 0.0261
trigger times: 4
Loss after 12399992 batches: 0.0247
trigger times: 5
Loss after 12400955 batches: 0.0244
trigger times: 6
Loss after 12401918 batches: 0.0233
trigger times: 7
Loss after 12402881 batches: 0.0221
trigger times: 8
Loss after 12403844 batches: 0.0215
trigger times: 9
Loss after 12404807 batches: 0.0208
trigger times: 10
Loss after 12405770 batches: 0.0202
trigger times: 11
Loss after 12406733 batches: 0.0198
trigger times: 12
Loss after 12407696 batches: 0.0198
trigger times: 13
Loss after 12408659 batches: 0.0191
trigger times: 14
Loss after 12409622 batches: 0.0188
trigger times: 15
Loss after 12410585 batches: 0.0189
trigger times: 16
Loss after 12411548 batches: 0.0186
trigger times: 17
Loss after 12412511 batches: 0.0186
trigger times: 18
Loss after 12413474 batches: 0.0185
trigger times: 19
Loss after 12414437 batches: 0.0181
trigger times: 20
Loss after 12415400 batches: 0.0182
trigger times: 21
Loss after 12416363 batches: 0.0179
trigger times: 22
Loss after 12417326 batches: 0.0175
trigger times: 23
Loss after 12418289 batches: 0.0179
trigger times: 24
Loss after 12419252 batches: 0.0181
trigger times: 25
Early stopping!
Start to test process.
Loss after 12420215 batches: 0.0178
Time to train on one home:  56.80839562416077
trigger times: 0
Loss after 12421160 batches: 0.0773
trigger times: 0
Loss after 12422105 batches: 0.0528
trigger times: 0
Loss after 12423050 batches: 0.0389
trigger times: 1
Loss after 12423995 batches: 0.0343
trigger times: 2
Loss after 12424940 batches: 0.0305
trigger times: 3
Loss after 12425885 batches: 0.0279
trigger times: 4
Loss after 12426830 batches: 0.0279
trigger times: 5
Loss after 12427775 batches: 0.0265
trigger times: 6
Loss after 12428720 batches: 0.0253
trigger times: 7
Loss after 12429665 batches: 0.0243
trigger times: 8
Loss after 12430610 batches: 0.0230
trigger times: 9
Loss after 12431555 batches: 0.0217
trigger times: 10
Loss after 12432500 batches: 0.0225
trigger times: 11
Loss after 12433445 batches: 0.0219
trigger times: 12
Loss after 12434390 batches: 0.0226
trigger times: 13
Loss after 12435335 batches: 0.0212
trigger times: 14
Loss after 12436280 batches: 0.0218
trigger times: 15
Loss after 12437225 batches: 0.0218
trigger times: 16
Loss after 12438170 batches: 0.0208
trigger times: 17
Loss after 12439115 batches: 0.0216
trigger times: 18
Loss after 12440060 batches: 0.0232
trigger times: 19
Loss after 12441005 batches: 0.0212
trigger times: 20
Loss after 12441950 batches: 0.0219
trigger times: 21
Loss after 12442895 batches: 0.0205
trigger times: 22
Loss after 12443840 batches: 0.0196
trigger times: 23
Loss after 12444785 batches: 0.0193
trigger times: 24
Loss after 12445730 batches: 0.0197
trigger times: 25
Early stopping!
Start to test process.
Loss after 12446675 batches: 0.0193
Time to train on one home:  58.007925033569336
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12447612 batches: 0.0872
trigger times: 1
Loss after 12448549 batches: 0.0722
trigger times: 2
Loss after 12449486 batches: 0.0684
trigger times: 3
Loss after 12450423 batches: 0.0649
trigger times: 4
Loss after 12451360 batches: 0.0627
trigger times: 5
Loss after 12452297 batches: 0.0597
trigger times: 6
Loss after 12453234 batches: 0.0594
trigger times: 7
Loss after 12454171 batches: 0.0580
trigger times: 8
Loss after 12455108 batches: 0.0562
trigger times: 9
Loss after 12456045 batches: 0.0566
trigger times: 10
Loss after 12456982 batches: 0.0548
trigger times: 11
Loss after 12457919 batches: 0.0537
trigger times: 12
Loss after 12458856 batches: 0.0531
trigger times: 13
Loss after 12459793 batches: 0.0545
trigger times: 14
Loss after 12460730 batches: 0.0536
trigger times: 15
Loss after 12461667 batches: 0.0511
trigger times: 16
Loss after 12462604 batches: 0.0529
trigger times: 17
Loss after 12463541 batches: 0.0520
trigger times: 18
Loss after 12464478 batches: 0.0509
trigger times: 19
Loss after 12465415 batches: 0.0517
trigger times: 20
Loss after 12466352 batches: 0.0512
trigger times: 21
Loss after 12467289 batches: 0.0515
trigger times: 22
Loss after 12468226 batches: 0.0529
trigger times: 23
Loss after 12469163 batches: 0.0523
trigger times: 24
Loss after 12470100 batches: 0.0503
trigger times: 25
Early stopping!
Start to test process.
Loss after 12471037 batches: 0.0492
Time to train on one home:  57.33303689956665
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 12472000 batches: 0.0253
trigger times: 1
Loss after 12472963 batches: 0.0148
trigger times: 2
Loss after 12473926 batches: 0.0137
trigger times: 3
Loss after 12474889 batches: 0.0134
trigger times: 4
Loss after 12475852 batches: 0.0131
trigger times: 5
Loss after 12476815 batches: 0.0127
trigger times: 6
Loss after 12477778 batches: 0.0119
trigger times: 7
Loss after 12478741 batches: 0.0115
trigger times: 8
Loss after 12479704 batches: 0.0109
trigger times: 9
Loss after 12480667 batches: 0.0104
trigger times: 10
Loss after 12481630 batches: 0.0099
trigger times: 11
Loss after 12482593 batches: 0.0095
trigger times: 12
Loss after 12483556 batches: 0.0090
trigger times: 13
Loss after 12484519 batches: 0.0087
trigger times: 14
Loss after 12485482 batches: 0.0087
trigger times: 15
Loss after 12486445 batches: 0.0084
trigger times: 16
Loss after 12487408 batches: 0.0081
trigger times: 17
Loss after 12488371 batches: 0.0080
trigger times: 18
Loss after 12489334 batches: 0.0079
trigger times: 19
Loss after 12490297 batches: 0.0077
trigger times: 20
Loss after 12491260 batches: 0.0079
trigger times: 21
Loss after 12492223 batches: 0.0077
trigger times: 22
Loss after 12493186 batches: 0.0074
trigger times: 23
Loss after 12494149 batches: 0.0074
trigger times: 24
Loss after 12495112 batches: 0.0071
trigger times: 25
Early stopping!
Start to test process.
Loss after 12496075 batches: 0.0069
Time to train on one home:  57.09303021430969
trigger times: 0
Loss after 12497038 batches: 0.0920
trigger times: 1
Loss after 12498001 batches: 0.0774
trigger times: 2
Loss after 12498964 batches: 0.0722
trigger times: 3
Loss after 12499927 batches: 0.0682
trigger times: 4
Loss after 12500890 batches: 0.0653
trigger times: 5
Loss after 12501853 batches: 0.0637
trigger times: 6
Loss after 12502816 batches: 0.0607
trigger times: 7
Loss after 12503779 batches: 0.0609
trigger times: 8
Loss after 12504742 batches: 0.0603
trigger times: 9
Loss after 12505705 batches: 0.0605
trigger times: 10
Loss after 12506668 batches: 0.0587
trigger times: 11
Loss after 12507631 batches: 0.0591
trigger times: 12
Loss after 12508594 batches: 0.0593
trigger times: 13
Loss after 12509557 batches: 0.0583
trigger times: 14
Loss after 12510520 batches: 0.0590
trigger times: 15
Loss after 12511483 batches: 0.0583
trigger times: 16
Loss after 12512446 batches: 0.0592
trigger times: 17
Loss after 12513409 batches: 0.0588
trigger times: 18
Loss after 12514372 batches: 0.0575
trigger times: 19
Loss after 12515335 batches: 0.0573
trigger times: 20
Loss after 12516298 batches: 0.0576
trigger times: 21
Loss after 12517261 batches: 0.0553
trigger times: 22
Loss after 12518224 batches: 0.0544
trigger times: 23
Loss after 12519187 batches: 0.0550
trigger times: 24
Loss after 12520150 batches: 0.0557
trigger times: 25
Early stopping!
Start to test process.
Loss after 12521113 batches: 0.0535
Time to train on one home:  53.1931312084198
trigger times: 0
Loss after 12522076 batches: 0.0658
trigger times: 1
Loss after 12523039 batches: 0.0515
trigger times: 2
Loss after 12524002 batches: 0.0469
trigger times: 3
Loss after 12524965 batches: 0.0443
trigger times: 4
Loss after 12525928 batches: 0.0421
trigger times: 5
Loss after 12526891 batches: 0.0410
trigger times: 6
Loss after 12527854 batches: 0.0406
trigger times: 7
Loss after 12528817 batches: 0.0381
trigger times: 8
Loss after 12529780 batches: 0.0372
trigger times: 9
Loss after 12530743 batches: 0.0365
trigger times: 10
Loss after 12531706 batches: 0.0352
trigger times: 11
Loss after 12532669 batches: 0.0351
trigger times: 12
Loss after 12533632 batches: 0.0349
trigger times: 13
Loss after 12534595 batches: 0.0345
trigger times: 14
Loss after 12535558 batches: 0.0339
trigger times: 15
Loss after 12536521 batches: 0.0330
trigger times: 16
Loss after 12537484 batches: 0.0331
trigger times: 17
Loss after 12538447 batches: 0.0332
trigger times: 18
Loss after 12539410 batches: 0.0334
trigger times: 19
Loss after 12540373 batches: 0.0321
trigger times: 20
Loss after 12541336 batches: 0.0326
trigger times: 21
Loss after 12542299 batches: 0.0321
trigger times: 22
Loss after 12543262 batches: 0.0323
trigger times: 23
Loss after 12544225 batches: 0.0315
trigger times: 24
Loss after 12545188 batches: 0.0306
trigger times: 25
Early stopping!
Start to test process.
Loss after 12546151 batches: 0.0323
Time to train on one home:  56.113221406936646
trigger times: 0
Loss after 12547047 batches: 0.1031
trigger times: 1
Loss after 12547943 batches: 0.0929
trigger times: 2
Loss after 12548839 batches: 0.0886
trigger times: 3
Loss after 12549735 batches: 0.0841
trigger times: 4
Loss after 12550631 batches: 0.0759
trigger times: 5
Loss after 12551527 batches: 0.0732
trigger times: 6
Loss after 12552423 batches: 0.0704
trigger times: 7
Loss after 12553319 batches: 0.0680
trigger times: 8
Loss after 12554215 batches: 0.0673
trigger times: 9
Loss after 12555111 batches: 0.0640
trigger times: 10
Loss after 12556007 batches: 0.0631
trigger times: 11
Loss after 12556903 batches: 0.0615
trigger times: 12
Loss after 12557799 batches: 0.0599
trigger times: 13
Loss after 12558695 batches: 0.0594
trigger times: 14
Loss after 12559591 batches: 0.0585
trigger times: 15
Loss after 12560487 batches: 0.0579
trigger times: 16
Loss after 12561383 batches: 0.0605
trigger times: 17
Loss after 12562279 batches: 0.0595
trigger times: 18
Loss after 12563175 batches: 0.0587
trigger times: 19
Loss after 12564071 batches: 0.0614
trigger times: 20
Loss after 12564967 batches: 0.0594
trigger times: 21
Loss after 12565863 batches: 0.0572
trigger times: 22
Loss after 12566759 batches: 0.0563
trigger times: 23
Loss after 12567655 batches: 0.0584
trigger times: 24
Loss after 12568551 batches: 0.0595
trigger times: 25
Early stopping!
Start to test process.
Loss after 12569447 batches: 0.0626
Time to train on one home:  55.36641001701355
trigger times: 0
Loss after 12570410 batches: 0.1550
trigger times: 1
Loss after 12571373 batches: 0.1153
trigger times: 2
Loss after 12572336 batches: 0.1010
trigger times: 3
Loss after 12573299 batches: 0.0906
trigger times: 4
Loss after 12574262 batches: 0.0840
trigger times: 5
Loss after 12575225 batches: 0.0798
trigger times: 6
Loss after 12576188 batches: 0.0750
trigger times: 7
Loss after 12577151 batches: 0.0704
trigger times: 8
Loss after 12578114 batches: 0.0670
trigger times: 9
Loss after 12579077 batches: 0.0635
trigger times: 10
Loss after 12580040 batches: 0.0597
trigger times: 11
Loss after 12581003 batches: 0.0567
trigger times: 12
Loss after 12581966 batches: 0.0548
trigger times: 13
Loss after 12582929 batches: 0.0530
trigger times: 14
Loss after 12583892 batches: 0.0513
trigger times: 15
Loss after 12584855 batches: 0.0523
trigger times: 16
Loss after 12585818 batches: 0.0508
trigger times: 17
Loss after 12586781 batches: 0.0483
trigger times: 18
Loss after 12587744 batches: 0.0465
trigger times: 19
Loss after 12588707 batches: 0.0477
trigger times: 20
Loss after 12589670 batches: 0.0473
trigger times: 21
Loss after 12590633 batches: 0.0461
trigger times: 22
Loss after 12591596 batches: 0.0469
trigger times: 23
Loss after 12592559 batches: 0.0460
trigger times: 24
Loss after 12593522 batches: 0.0442
trigger times: 25
Early stopping!
Start to test process.
Loss after 12594485 batches: 0.0437
Time to train on one home:  53.503901958465576
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12595448 batches: 0.0843
trigger times: 1
Loss after 12596411 batches: 0.0760
trigger times: 2
Loss after 12597374 batches: 0.0741
trigger times: 3
Loss after 12598337 batches: 0.0717
trigger times: 4
Loss after 12599300 batches: 0.0688
trigger times: 5
Loss after 12600263 batches: 0.0660
trigger times: 6
Loss after 12601226 batches: 0.0632
trigger times: 7
Loss after 12602189 batches: 0.0623
trigger times: 8
Loss after 12603152 batches: 0.0591
trigger times: 9
Loss after 12604115 batches: 0.0595
trigger times: 10
Loss after 12605078 batches: 0.0567
trigger times: 11
Loss after 12606041 batches: 0.0557
trigger times: 12
Loss after 12607004 batches: 0.0533
trigger times: 13
Loss after 12607967 batches: 0.0539
trigger times: 14
Loss after 12608930 batches: 0.0543
trigger times: 15
Loss after 12609893 batches: 0.0540
trigger times: 16
Loss after 12610856 batches: 0.0520
trigger times: 17
Loss after 12611819 batches: 0.0517
trigger times: 18
Loss after 12612782 batches: 0.0508
trigger times: 19
Loss after 12613745 batches: 0.0516
trigger times: 20
Loss after 12614708 batches: 0.0495
trigger times: 21
Loss after 12615671 batches: 0.0487
trigger times: 22
Loss after 12616634 batches: 0.0492
trigger times: 23
Loss after 12617597 batches: 0.0469
trigger times: 24
Loss after 12618560 batches: 0.0490
trigger times: 25
Early stopping!
Start to test process.
Loss after 12619523 batches: 0.0485
Time to train on one home:  62.801119327545166
trigger times: 0
Loss after 12620486 batches: 0.0913
trigger times: 0
Loss after 12621449 batches: 0.0649
trigger times: 1
Loss after 12622412 batches: 0.0598
trigger times: 2
Loss after 12623375 batches: 0.0527
trigger times: 3
Loss after 12624338 batches: 0.0503
trigger times: 4
Loss after 12625301 batches: 0.0481
trigger times: 5
Loss after 12626264 batches: 0.0455
trigger times: 6
Loss after 12627227 batches: 0.0438
trigger times: 7
Loss after 12628190 batches: 0.0425
trigger times: 8
Loss after 12629153 batches: 0.0429
trigger times: 9
Loss after 12630116 batches: 0.0422
trigger times: 10
Loss after 12631079 batches: 0.0413
trigger times: 11
Loss after 12632042 batches: 0.0404
trigger times: 12
Loss after 12633005 batches: 0.0395
trigger times: 13
Loss after 12633968 batches: 0.0396
trigger times: 14
Loss after 12634931 batches: 0.0399
trigger times: 15
Loss after 12635894 batches: 0.0396
trigger times: 16
Loss after 12636857 batches: 0.0390
trigger times: 17
Loss after 12637820 batches: 0.0379
trigger times: 18
Loss after 12638783 batches: 0.0380
trigger times: 19
Loss after 12639746 batches: 0.0377
trigger times: 20
Loss after 12640709 batches: 0.0382
trigger times: 21
Loss after 12641672 batches: 0.0385
trigger times: 22
Loss after 12642635 batches: 0.0427
trigger times: 23
Loss after 12643598 batches: 0.0429
trigger times: 24
Loss after 12644561 batches: 0.0410
trigger times: 25
Early stopping!
Start to test process.
Loss after 12645524 batches: 0.0390
Time to train on one home:  57.11480784416199
trigger times: 0
Loss after 12646487 batches: 0.0507
trigger times: 1
Loss after 12647450 batches: 0.0384
trigger times: 2
Loss after 12648413 batches: 0.0365
trigger times: 3
Loss after 12649376 batches: 0.0334
trigger times: 4
Loss after 12650339 batches: 0.0300
trigger times: 5
Loss after 12651302 batches: 0.0279
trigger times: 6
Loss after 12652265 batches: 0.0263
trigger times: 7
Loss after 12653228 batches: 0.0255
trigger times: 8
Loss after 12654191 batches: 0.0256
trigger times: 9
Loss after 12655154 batches: 0.0253
trigger times: 10
Loss after 12656117 batches: 0.0242
trigger times: 11
Loss after 12657080 batches: 0.0244
trigger times: 12
Loss after 12658043 batches: 0.0237
trigger times: 13
Loss after 12659006 batches: 0.0236
trigger times: 14
Loss after 12659969 batches: 0.0231
trigger times: 15
Loss after 12660932 batches: 0.0229
trigger times: 16
Loss after 12661895 batches: 0.0227
trigger times: 17
Loss after 12662858 batches: 0.0229
trigger times: 18
Loss after 12663821 batches: 0.0221
trigger times: 19
Loss after 12664784 batches: 0.0223
trigger times: 20
Loss after 12665747 batches: 0.0228
trigger times: 21
Loss after 12666710 batches: 0.0231
trigger times: 22
Loss after 12667673 batches: 0.0227
trigger times: 23
Loss after 12668636 batches: 0.0218
trigger times: 24
Loss after 12669599 batches: 0.0209
trigger times: 25
Early stopping!
Start to test process.
Loss after 12670562 batches: 0.0210
Time to train on one home:  56.45702600479126
trigger times: 0
Loss after 12671525 batches: 0.0623
trigger times: 1
Loss after 12672488 batches: 0.0460
trigger times: 2
Loss after 12673451 batches: 0.0460
trigger times: 3
Loss after 12674414 batches: 0.0442
trigger times: 4
Loss after 12675377 batches: 0.0420
trigger times: 5
Loss after 12676340 batches: 0.0404
trigger times: 6
Loss after 12677303 batches: 0.0390
trigger times: 7
Loss after 12678266 batches: 0.0382
trigger times: 8
Loss after 12679229 batches: 0.0371
trigger times: 9
Loss after 12680192 batches: 0.0372
trigger times: 10
Loss after 12681155 batches: 0.0369
trigger times: 11
Loss after 12682118 batches: 0.0361
trigger times: 12
Loss after 12683081 batches: 0.0364
trigger times: 13
Loss after 12684044 batches: 0.0362
trigger times: 14
Loss after 12685007 batches: 0.0352
trigger times: 15
Loss after 12685970 batches: 0.0356
trigger times: 16
Loss after 12686933 batches: 0.0350
trigger times: 17
Loss after 12687896 batches: 0.0350
trigger times: 18
Loss after 12688859 batches: 0.0342
trigger times: 19
Loss after 12689822 batches: 0.0340
trigger times: 20
Loss after 12690785 batches: 0.0334
trigger times: 21
Loss after 12691748 batches: 0.0329
trigger times: 22
Loss after 12692711 batches: 0.0334
trigger times: 23
Loss after 12693674 batches: 0.0334
trigger times: 24
Loss after 12694637 batches: 0.0341
trigger times: 25
Early stopping!
Start to test process.
Loss after 12695600 batches: 0.0346
Time to train on one home:  57.38666248321533
trigger times: 0
Loss after 12696495 batches: 0.0833
trigger times: 1
Loss after 12697390 batches: 0.0467
trigger times: 2
Loss after 12698285 batches: 0.0226
trigger times: 0
Loss after 12699180 batches: 0.0121
trigger times: 0
Loss after 12700075 batches: 0.0078
trigger times: 1
Loss after 12700970 batches: 0.0067
trigger times: 2
Loss after 12701865 batches: 0.0059
trigger times: 3
Loss after 12702760 batches: 0.0053
trigger times: 4
Loss after 12703655 batches: 0.0041
trigger times: 0
Loss after 12704550 batches: 0.0040
trigger times: 1
Loss after 12705445 batches: 0.0037
trigger times: 2
Loss after 12706340 batches: 0.0039
trigger times: 3
Loss after 12707235 batches: 0.0039
trigger times: 4
Loss after 12708130 batches: 0.0041
trigger times: 5
Loss after 12709025 batches: 0.0035
trigger times: 6
Loss after 12709920 batches: 0.0028
trigger times: 0
Loss after 12710815 batches: 0.0030
trigger times: 1
Loss after 12711710 batches: 0.0033
trigger times: 2
Loss after 12712605 batches: 0.0031
trigger times: 3
Loss after 12713500 batches: 0.0032
trigger times: 4
Loss after 12714395 batches: 0.0026
trigger times: 5
Loss after 12715290 batches: 0.0024
trigger times: 6
Loss after 12716185 batches: 0.0027
trigger times: 7
Loss after 12717080 batches: 0.0023
trigger times: 0
Loss after 12717975 batches: 0.0025
trigger times: 1
Loss after 12718870 batches: 0.0021
trigger times: 2
Loss after 12719765 batches: 0.0023
trigger times: 3
Loss after 12720660 batches: 0.0019
trigger times: 4
Loss after 12721555 batches: 0.0022
trigger times: 5
Loss after 12722450 batches: 0.0024
trigger times: 6
Loss after 12723345 batches: 0.0021
trigger times: 7
Loss after 12724240 batches: 0.0020
trigger times: 8
Loss after 12725135 batches: 0.0021
trigger times: 9
Loss after 12726030 batches: 0.0022
trigger times: 10
Loss after 12726925 batches: 0.0021
trigger times: 11
Loss after 12727820 batches: 0.0023
trigger times: 12
Loss after 12728715 batches: 0.0025
trigger times: 13
Loss after 12729610 batches: 0.0024
trigger times: 14
Loss after 12730505 batches: 0.0022
trigger times: 15
Loss after 12731400 batches: 0.0041
trigger times: 16
Loss after 12732295 batches: 0.0089
trigger times: 17
Loss after 12733190 batches: 0.0059
trigger times: 18
Loss after 12734085 batches: 0.0045
trigger times: 19
Loss after 12734980 batches: 0.0038
trigger times: 20
Loss after 12735875 batches: 0.0034
trigger times: 21
Loss after 12736770 batches: 0.0030
trigger times: 22
Loss after 12737665 batches: 0.0029
trigger times: 23
Loss after 12738560 batches: 0.0031
trigger times: 24
Loss after 12739455 batches: 0.0030
trigger times: 25
Early stopping!
Start to test process.
Loss after 12740350 batches: 0.0025
Time to train on one home:  73.50281357765198
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336, 0.034888222413376094, 0.034613204319843105, 0.033508375671873286]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171], [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71], [0.13675299286842346, 0.15565351472302724, 0.32920502623266784, 1.059933101560365, 0.6549021202780799, 35.010519125969424, 3184.605], [0.0885479673743248, 0.1559912263273432, 0.33358461589199145, 0.944871497166479, 0.654640173496829, 31.209933508474723, 3183.3313]]
Round_10_results:  [0.0885479673743248, 0.1559912263273432, 0.33358461589199145, 0.944871497166479, 0.654640173496829, 31.209933508474723, 3183.3313]
trigger times: 0
Loss after 12741313 batches: 0.0531
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 13355 < 13356; dropping {'Training_Loss': 0.053105218069893975, 'Validation_Loss': 0.06399154663085938, 'Training_R2': 0.2942283344431774, 'Validation_R2': 0.12084043485735485, 'Training_F1': 0.5012330583674017, 'Validation_F1': 0.3157827879242677, 'Training_NEP': 0.7826993637705789, 'Validation_NEP': 0.8905149610422062, 'Training_NDE': 0.5197686671789296, 'Validation_NDE': 0.6822259838671355, 'Training_MAE': 22.53796069677818, 'Validation_MAE': 29.663261601954332, 'Training_MSE': 1635.2859, 'Validation_MSE': 3379.3381}.
trigger times: 1
Loss after 12742276 batches: 0.0431
trigger times: 0
Loss after 12743239 batches: 0.0404
trigger times: 1
Loss after 12744202 batches: 0.0380
trigger times: 2
Loss after 12745165 batches: 0.0353
trigger times: 3
Loss after 12746128 batches: 0.0340
trigger times: 4
Loss after 12747091 batches: 0.0343
trigger times: 5
Loss after 12748054 batches: 0.0338
trigger times: 6
Loss after 12749017 batches: 0.0319
trigger times: 7
Loss after 12749980 batches: 0.0313
trigger times: 8
Loss after 12750943 batches: 0.0315
trigger times: 9
Loss after 12751906 batches: 0.0317
trigger times: 10
Loss after 12752869 batches: 0.0316
trigger times: 11
Loss after 12753832 batches: 0.0314
trigger times: 12
Loss after 12754795 batches: 0.0296
trigger times: 13
Loss after 12755758 batches: 0.0301
trigger times: 14
Loss after 12756721 batches: 0.0298
trigger times: 15
Loss after 12757684 batches: 0.0316
trigger times: 16
Loss after 12758647 batches: 0.0309
trigger times: 17
Loss after 12759610 batches: 0.0308
trigger times: 18
Loss after 12760573 batches: 0.0308
trigger times: 19
Loss after 12761536 batches: 0.0298
trigger times: 20
Loss after 12762499 batches: 0.0296
trigger times: 21
Loss after 12763462 batches: 0.0291
trigger times: 22
Loss after 12764425 batches: 0.0288
trigger times: 23
Loss after 12765388 batches: 0.0297
trigger times: 24
Loss after 12766351 batches: 0.0287
trigger times: 25
Early stopping!
Start to test process.
Loss after 12767314 batches: 0.0292
Time to train on one home:  54.29346251487732
trigger times: 0
Loss after 12768272 batches: 0.0638
trigger times: 0
Loss after 12769230 batches: 0.0409
trigger times: 1
Loss after 12770188 batches: 0.0347
trigger times: 0
Loss after 12771146 batches: 0.0295
trigger times: 1
Loss after 12772104 batches: 0.0254
trigger times: 0
Loss after 12773062 batches: 0.0241
trigger times: 1
Loss after 12774020 batches: 0.0220
trigger times: 2
Loss after 12774978 batches: 0.0206
trigger times: 3
Loss after 12775936 batches: 0.0200
trigger times: 4
Loss after 12776894 batches: 0.0203
trigger times: 5
Loss after 12777852 batches: 0.0189
trigger times: 6
Loss after 12778810 batches: 0.0196
trigger times: 0
Loss after 12779768 batches: 0.0194
trigger times: 1
Loss after 12780726 batches: 0.0180
trigger times: 2
Loss after 12781684 batches: 0.0204
trigger times: 3
Loss after 12782642 batches: 0.0225
trigger times: 4
Loss after 12783600 batches: 0.0205
trigger times: 5
Loss after 12784558 batches: 0.0193
trigger times: 6
Loss after 12785516 batches: 0.0187
trigger times: 7
Loss after 12786474 batches: 0.0204
trigger times: 8
Loss after 12787432 batches: 0.0209
trigger times: 9
Loss after 12788390 batches: 0.0192
trigger times: 10
Loss after 12789348 batches: 0.0176
trigger times: 11
Loss after 12790306 batches: 0.0170
trigger times: 12
Loss after 12791264 batches: 0.0178
trigger times: 0
Loss after 12792222 batches: 0.0179
trigger times: 1
Loss after 12793180 batches: 0.0175
trigger times: 2
Loss after 12794138 batches: 0.0165
trigger times: 3
Loss after 12795096 batches: 0.0171
trigger times: 0
Loss after 12796054 batches: 0.0165
trigger times: 1
Loss after 12797012 batches: 0.0159
trigger times: 2
Loss after 12797970 batches: 0.0160
trigger times: 3
Loss after 12798928 batches: 0.0150
trigger times: 4
Loss after 12799886 batches: 0.0155
trigger times: 5
Loss after 12800844 batches: 0.0175
trigger times: 6
Loss after 12801802 batches: 0.0186
trigger times: 7
Loss after 12802760 batches: 0.0219
trigger times: 8
Loss after 12803718 batches: 0.0203
trigger times: 9
Loss after 12804676 batches: 0.0191
trigger times: 10
Loss after 12805634 batches: 0.0183
trigger times: 11
Loss after 12806592 batches: 0.0169
trigger times: 12
Loss after 12807550 batches: 0.0166
trigger times: 13
Loss after 12808508 batches: 0.0156
trigger times: 14
Loss after 12809466 batches: 0.0162
trigger times: 15
Loss after 12810424 batches: 0.0169
trigger times: 16
Loss after 12811382 batches: 0.0162
trigger times: 17
Loss after 12812340 batches: 0.0153
trigger times: 18
Loss after 12813298 batches: 0.0152
trigger times: 19
Loss after 12814256 batches: 0.0154
trigger times: 20
Loss after 12815214 batches: 0.0158
trigger times: 21
Loss after 12816172 batches: 0.0142
trigger times: 22
Loss after 12817130 batches: 0.0144
trigger times: 23
Loss after 12818088 batches: 0.0141
trigger times: 24
Loss after 12819046 batches: 0.0147
trigger times: 25
Early stopping!
Start to test process.
Loss after 12820004 batches: 0.0141
Time to train on one home:  80.85541415214539
trigger times: 0
Loss after 12820967 batches: 0.1010
trigger times: 1
Loss after 12821930 batches: 0.0721
trigger times: 2
Loss after 12822893 batches: 0.0672
trigger times: 3
Loss after 12823856 batches: 0.0675
trigger times: 4
Loss after 12824819 batches: 0.0654
trigger times: 5
Loss after 12825782 batches: 0.0630
trigger times: 6
Loss after 12826745 batches: 0.0602
trigger times: 7
Loss after 12827708 batches: 0.0573
trigger times: 8
Loss after 12828671 batches: 0.0553
trigger times: 9
Loss after 12829634 batches: 0.0549
trigger times: 10
Loss after 12830597 batches: 0.0524
trigger times: 11
Loss after 12831560 batches: 0.0515
trigger times: 12
Loss after 12832523 batches: 0.0507
trigger times: 13
Loss after 12833486 batches: 0.0500
trigger times: 14
Loss after 12834449 batches: 0.0499
trigger times: 15
Loss after 12835412 batches: 0.0490
trigger times: 16
Loss after 12836375 batches: 0.0481
trigger times: 17
Loss after 12837338 batches: 0.0477
trigger times: 18
Loss after 12838301 batches: 0.0475
trigger times: 19
Loss after 12839264 batches: 0.0467
trigger times: 20
Loss after 12840227 batches: 0.0460
trigger times: 21
Loss after 12841190 batches: 0.0472
trigger times: 22
Loss after 12842153 batches: 0.0447
trigger times: 23
Loss after 12843116 batches: 0.0448
trigger times: 24
Loss after 12844079 batches: 0.0459
trigger times: 25
Early stopping!
Start to test process.
Loss after 12845042 batches: 0.0449
Time to train on one home:  54.49842119216919
trigger times: 0
Loss after 12846005 batches: 0.0994
trigger times: 1
Loss after 12846968 batches: 0.0778
trigger times: 2
Loss after 12847931 batches: 0.0764
trigger times: 3
Loss after 12848894 batches: 0.0748
trigger times: 4
Loss after 12849857 batches: 0.0718
trigger times: 5
Loss after 12850820 batches: 0.0681
trigger times: 6
Loss after 12851783 batches: 0.0671
trigger times: 7
Loss after 12852746 batches: 0.0650
trigger times: 8
Loss after 12853709 batches: 0.0640
trigger times: 9
Loss after 12854672 batches: 0.0630
trigger times: 10
Loss after 12855635 batches: 0.0615
trigger times: 11
Loss after 12856598 batches: 0.0611
trigger times: 12
Loss after 12857561 batches: 0.0600
trigger times: 13
Loss after 12858524 batches: 0.0603
trigger times: 14
Loss after 12859487 batches: 0.0598
trigger times: 15
Loss after 12860450 batches: 0.0602
trigger times: 16
Loss after 12861413 batches: 0.0600
trigger times: 17
Loss after 12862376 batches: 0.0602
trigger times: 18
Loss after 12863339 batches: 0.0600
trigger times: 19
Loss after 12864302 batches: 0.0588
trigger times: 20
Loss after 12865265 batches: 0.0587
trigger times: 21
Loss after 12866228 batches: 0.0565
trigger times: 22
Loss after 12867191 batches: 0.0568
trigger times: 23
Loss after 12868154 batches: 0.0569
trigger times: 24
Loss after 12869117 batches: 0.0578
trigger times: 25
Early stopping!
Start to test process.
Loss after 12870080 batches: 0.0587
Time to train on one home:  61.030521631240845
trigger times: 0
Loss after 12871043 batches: 0.0257
trigger times: 1
Loss after 12872006 batches: 0.0211
trigger times: 2
Loss after 12872969 batches: 0.0193
trigger times: 3
Loss after 12873932 batches: 0.0178
trigger times: 4
Loss after 12874895 batches: 0.0169
trigger times: 5
Loss after 12875858 batches: 0.0164
trigger times: 6
Loss after 12876821 batches: 0.0158
trigger times: 7
Loss after 12877784 batches: 0.0151
trigger times: 8
Loss after 12878747 batches: 0.0151
trigger times: 9
Loss after 12879710 batches: 0.0145
trigger times: 10
Loss after 12880673 batches: 0.0147
trigger times: 11
Loss after 12881636 batches: 0.0140
trigger times: 12
Loss after 12882599 batches: 0.0140
trigger times: 13
Loss after 12883562 batches: 0.0134
trigger times: 14
Loss after 12884525 batches: 0.0138
trigger times: 15
Loss after 12885488 batches: 0.0133
trigger times: 16
Loss after 12886451 batches: 0.0129
trigger times: 17
Loss after 12887414 batches: 0.0133
trigger times: 18
Loss after 12888377 batches: 0.0133
trigger times: 19
Loss after 12889340 batches: 0.0135
trigger times: 20
Loss after 12890303 batches: 0.0126
trigger times: 21
Loss after 12891266 batches: 0.0129
trigger times: 22
Loss after 12892229 batches: 0.0127
trigger times: 23
Loss after 12893192 batches: 0.0127
trigger times: 24
Loss after 12894155 batches: 0.0124
trigger times: 25
Early stopping!
Start to test process.
Loss after 12895118 batches: 0.0126
Time to train on one home:  56.613808155059814
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12896081 batches: 0.0842
trigger times: 1
Loss after 12897044 batches: 0.0263
trigger times: 2
Loss after 12898007 batches: 0.0206
trigger times: 3
Loss after 12898970 batches: 0.0185
trigger times: 4
Loss after 12899933 batches: 0.0175
trigger times: 5
Loss after 12900896 batches: 0.0166
trigger times: 6
Loss after 12901859 batches: 0.0155
trigger times: 7
Loss after 12902822 batches: 0.0149
trigger times: 8
Loss after 12903785 batches: 0.0142
trigger times: 9
Loss after 12904748 batches: 0.0140
trigger times: 10
Loss after 12905711 batches: 0.0142
trigger times: 11
Loss after 12906674 batches: 0.0135
trigger times: 12
Loss after 12907637 batches: 0.0132
trigger times: 13
Loss after 12908600 batches: 0.0130
trigger times: 14
Loss after 12909563 batches: 0.0129
trigger times: 15
Loss after 12910526 batches: 0.0130
trigger times: 16
Loss after 12911489 batches: 0.0128
trigger times: 17
Loss after 12912452 batches: 0.0126
trigger times: 18
Loss after 12913415 batches: 0.0125
trigger times: 19
Loss after 12914378 batches: 0.0127
trigger times: 20
Loss after 12915341 batches: 0.0124
trigger times: 21
Loss after 12916304 batches: 0.0122
trigger times: 22
Loss after 12917267 batches: 0.0127
trigger times: 23
Loss after 12918230 batches: 0.0126
trigger times: 24
Loss after 12919193 batches: 0.0121
trigger times: 25
Early stopping!
Start to test process.
Loss after 12920156 batches: 0.0119
Time to train on one home:  56.58386039733887
trigger times: 0
Loss after 12921119 batches: 0.0980
trigger times: 1
Loss after 12922082 batches: 0.0899
trigger times: 0
Loss after 12923045 batches: 0.0834
trigger times: 1
Loss after 12924008 batches: 0.0811
trigger times: 2
Loss after 12924971 batches: 0.0769
trigger times: 3
Loss after 12925934 batches: 0.0761
trigger times: 4
Loss after 12926897 batches: 0.0741
trigger times: 5
Loss after 12927860 batches: 0.0716
trigger times: 6
Loss after 12928823 batches: 0.0704
trigger times: 7
Loss after 12929786 batches: 0.0692
trigger times: 8
Loss after 12930749 batches: 0.0670
trigger times: 9
Loss after 12931712 batches: 0.0675
trigger times: 10
Loss after 12932675 batches: 0.0663
trigger times: 11
Loss after 12933638 batches: 0.0660
trigger times: 12
Loss after 12934601 batches: 0.0669
trigger times: 13
Loss after 12935564 batches: 0.0654
trigger times: 14
Loss after 12936527 batches: 0.0651
trigger times: 15
Loss after 12937490 batches: 0.0625
trigger times: 16
Loss after 12938453 batches: 0.0636
trigger times: 17
Loss after 12939416 batches: 0.0649
trigger times: 18
Loss after 12940379 batches: 0.0644
trigger times: 19
Loss after 12941342 batches: 0.0642
trigger times: 20
Loss after 12942305 batches: 0.0617
trigger times: 21
Loss after 12943268 batches: 0.0615
trigger times: 22
Loss after 12944231 batches: 0.0594
trigger times: 23
Loss after 12945194 batches: 0.0606
trigger times: 24
Loss after 12946157 batches: 0.0601
trigger times: 25
Early stopping!
Start to test process.
Loss after 12947120 batches: 0.0599
Time to train on one home:  59.25406002998352
trigger times: 0
Loss after 12948083 batches: 0.0574
trigger times: 1
Loss after 12949046 batches: 0.0464
trigger times: 0
Loss after 12950009 batches: 0.0349
trigger times: 1
Loss after 12950972 batches: 0.0318
trigger times: 2
Loss after 12951935 batches: 0.0287
trigger times: 3
Loss after 12952898 batches: 0.0275
trigger times: 4
Loss after 12953861 batches: 0.0267
trigger times: 5
Loss after 12954824 batches: 0.0263
trigger times: 6
Loss after 12955787 batches: 0.0255
trigger times: 7
Loss after 12956750 batches: 0.0247
trigger times: 8
Loss after 12957713 batches: 0.0224
trigger times: 9
Loss after 12958676 batches: 0.0225
trigger times: 10
Loss after 12959639 batches: 0.0224
trigger times: 11
Loss after 12960602 batches: 0.0225
trigger times: 12
Loss after 12961565 batches: 0.0228
trigger times: 13
Loss after 12962528 batches: 0.0217
trigger times: 14
Loss after 12963491 batches: 0.0219
trigger times: 15
Loss after 12964454 batches: 0.0210
trigger times: 16
Loss after 12965417 batches: 0.0217
trigger times: 17
Loss after 12966380 batches: 0.0203
trigger times: 18
Loss after 12967343 batches: 0.0193
trigger times: 19
Loss after 12968306 batches: 0.0197
trigger times: 20
Loss after 12969269 batches: 0.0197
trigger times: 21
Loss after 12970232 batches: 0.0190
trigger times: 22
Loss after 12971195 batches: 0.0200
trigger times: 23
Loss after 12972158 batches: 0.0205
trigger times: 24
Loss after 12973121 batches: 0.0197
trigger times: 25
Early stopping!
Start to test process.
Loss after 12974084 batches: 0.0194
Time to train on one home:  58.981149673461914
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12975047 batches: 0.0844
trigger times: 1
Loss after 12976010 batches: 0.0743
trigger times: 2
Loss after 12976973 batches: 0.0705
trigger times: 3
Loss after 12977936 batches: 0.0671
trigger times: 4
Loss after 12978899 batches: 0.0647
trigger times: 5
Loss after 12979862 batches: 0.0631
trigger times: 6
Loss after 12980825 batches: 0.0627
trigger times: 7
Loss after 12981788 batches: 0.0609
trigger times: 8
Loss after 12982751 batches: 0.0600
trigger times: 9
Loss after 12983714 batches: 0.0589
trigger times: 10
Loss after 12984677 batches: 0.0580
trigger times: 11
Loss after 12985640 batches: 0.0592
trigger times: 12
Loss after 12986603 batches: 0.0586
trigger times: 13
Loss after 12987566 batches: 0.0568
trigger times: 14
Loss after 12988529 batches: 0.0568
trigger times: 15
Loss after 12989492 batches: 0.0569
trigger times: 16
Loss after 12990455 batches: 0.0571
trigger times: 17
Loss after 12991418 batches: 0.0560
trigger times: 18
Loss after 12992381 batches: 0.0556
trigger times: 19
Loss after 12993344 batches: 0.0543
trigger times: 20
Loss after 12994307 batches: 0.0550
trigger times: 21
Loss after 12995270 batches: 0.0547
trigger times: 22
Loss after 12996233 batches: 0.0550
trigger times: 23
Loss after 12997196 batches: 0.0545
trigger times: 24
Loss after 12998159 batches: 0.0550
trigger times: 25
Early stopping!
Start to test process.
Loss after 12999122 batches: 0.0545
Time to train on one home:  54.03268647193909
trigger times: 0
Loss after 13000085 batches: 0.0927
trigger times: 0
Loss after 13001048 batches: 0.0616
trigger times: 0
Loss after 13002011 batches: 0.0570
trigger times: 1
Loss after 13002974 batches: 0.0528
trigger times: 2
Loss after 13003937 batches: 0.0502
trigger times: 3
Loss after 13004900 batches: 0.0474
trigger times: 4
Loss after 13005863 batches: 0.0467
trigger times: 5
Loss after 13006826 batches: 0.0448
trigger times: 6
Loss after 13007789 batches: 0.0435
trigger times: 7
Loss after 13008752 batches: 0.0431
trigger times: 8
Loss after 13009715 batches: 0.0428
trigger times: 9
Loss after 13010678 batches: 0.0420
trigger times: 10
Loss after 13011641 batches: 0.0425
trigger times: 11
Loss after 13012604 batches: 0.0412
trigger times: 12
Loss after 13013567 batches: 0.0412
trigger times: 13
Loss after 13014530 batches: 0.0406
trigger times: 14
Loss after 13015493 batches: 0.0407
trigger times: 15
Loss after 13016456 batches: 0.0385
trigger times: 16
Loss after 13017419 batches: 0.0399
trigger times: 17
Loss after 13018382 batches: 0.0400
trigger times: 18
Loss after 13019345 batches: 0.0411
trigger times: 19
Loss after 13020308 batches: 0.0401
trigger times: 20
Loss after 13021271 batches: 0.0392
trigger times: 21
Loss after 13022234 batches: 0.0379
trigger times: 22
Loss after 13023197 batches: 0.0371
trigger times: 23
Loss after 13024160 batches: 0.0387
trigger times: 24
Loss after 13025123 batches: 0.0378
trigger times: 25
Early stopping!
Start to test process.
Loss after 13026086 batches: 0.0382
Time to train on one home:  56.097996950149536
trigger times: 0
Loss after 13027049 batches: 0.0757
trigger times: 1
Loss after 13028012 batches: 0.0682
trigger times: 2
Loss after 13028975 batches: 0.0679
trigger times: 3
Loss after 13029938 batches: 0.0653
trigger times: 4
Loss after 13030901 batches: 0.0630
trigger times: 5
Loss after 13031864 batches: 0.0610
trigger times: 6
Loss after 13032827 batches: 0.0594
trigger times: 7
Loss after 13033790 batches: 0.0590
trigger times: 8
Loss after 13034753 batches: 0.0570
trigger times: 9
Loss after 13035716 batches: 0.0570
trigger times: 10
Loss after 13036679 batches: 0.0565
trigger times: 11
Loss after 13037642 batches: 0.0563
trigger times: 12
Loss after 13038605 batches: 0.0559
trigger times: 13
Loss after 13039568 batches: 0.0541
trigger times: 14
Loss after 13040531 batches: 0.0551
trigger times: 15
Loss after 13041494 batches: 0.0543
trigger times: 16
Loss after 13042457 batches: 0.0548
trigger times: 17
Loss after 13043420 batches: 0.0523
trigger times: 18
Loss after 13044383 batches: 0.0523
trigger times: 19
Loss after 13045346 batches: 0.0522
trigger times: 20
Loss after 13046309 batches: 0.0517
trigger times: 21
Loss after 13047272 batches: 0.0523
trigger times: 22
Loss after 13048235 batches: 0.0526
trigger times: 23
Loss after 13049198 batches: 0.0529
trigger times: 24
Loss after 13050161 batches: 0.0515
trigger times: 25
Early stopping!
Start to test process.
Loss after 13051124 batches: 0.0530
Time to train on one home:  56.56492900848389
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13052087 batches: 0.0648
trigger times: 1
Loss after 13053050 batches: 0.0511
trigger times: 2
Loss after 13054013 batches: 0.0472
trigger times: 3
Loss after 13054976 batches: 0.0433
trigger times: 4
Loss after 13055939 batches: 0.0379
trigger times: 5
Loss after 13056902 batches: 0.0357
trigger times: 6
Loss after 13057865 batches: 0.0342
trigger times: 7
Loss after 13058828 batches: 0.0322
trigger times: 8
Loss after 13059791 batches: 0.0306
trigger times: 9
Loss after 13060754 batches: 0.0295
trigger times: 10
Loss after 13061717 batches: 0.0295
trigger times: 11
Loss after 13062680 batches: 0.0286
trigger times: 12
Loss after 13063643 batches: 0.0281
trigger times: 13
Loss after 13064606 batches: 0.0283
trigger times: 14
Loss after 13065569 batches: 0.0276
trigger times: 15
Loss after 13066532 batches: 0.0272
trigger times: 16
Loss after 13067495 batches: 0.0271
trigger times: 17
Loss after 13068458 batches: 0.0264
trigger times: 18
Loss after 13069421 batches: 0.0255
trigger times: 19
Loss after 13070384 batches: 0.0263
trigger times: 20
Loss after 13071347 batches: 0.0258
trigger times: 21
Loss after 13072310 batches: 0.0245
trigger times: 22
Loss after 13073273 batches: 0.0247
trigger times: 23
Loss after 13074236 batches: 0.0243
trigger times: 24
Loss after 13075199 batches: 0.0240
trigger times: 25
Early stopping!
Start to test process.
Loss after 13076162 batches: 0.0232
Time to train on one home:  53.97364377975464
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13077125 batches: 0.0807
trigger times: 0
Loss after 13078088 batches: 0.0537
trigger times: 1
Loss after 13079051 batches: 0.0442
trigger times: 2
Loss after 13080014 batches: 0.0405
trigger times: 0
Loss after 13080977 batches: 0.0350
trigger times: 1
Loss after 13081940 batches: 0.0339
trigger times: 2
Loss after 13082903 batches: 0.0289
trigger times: 3
Loss after 13083866 batches: 0.0278
trigger times: 4
Loss after 13084829 batches: 0.0281
trigger times: 5
Loss after 13085792 batches: 0.0270
trigger times: 6
Loss after 13086755 batches: 0.0258
trigger times: 7
Loss after 13087718 batches: 0.0250
trigger times: 8
Loss after 13088681 batches: 0.0242
trigger times: 9
Loss after 13089644 batches: 0.0241
trigger times: 10
Loss after 13090607 batches: 0.0243
trigger times: 11
Loss after 13091570 batches: 0.0232
trigger times: 12
Loss after 13092533 batches: 0.0233
trigger times: 13
Loss after 13093496 batches: 0.0220
trigger times: 14
Loss after 13094459 batches: 0.0213
trigger times: 15
Loss after 13095422 batches: 0.0204
trigger times: 16
Loss after 13096385 batches: 0.0201
trigger times: 17
Loss after 13097348 batches: 0.0218
trigger times: 18
Loss after 13098311 batches: 0.0230
trigger times: 19
Loss after 13099274 batches: 0.0214
trigger times: 20
Loss after 13100237 batches: 0.0208
trigger times: 21
Loss after 13101200 batches: 0.0192
trigger times: 22
Loss after 13102163 batches: 0.0191
trigger times: 23
Loss after 13103126 batches: 0.0178
trigger times: 24
Loss after 13104089 batches: 0.0204
trigger times: 25
Early stopping!
Start to test process.
Loss after 13105052 batches: 0.0189
Time to train on one home:  63.88868474960327
trigger times: 0
Loss after 13105981 batches: 0.0904
trigger times: 0
Loss after 13106910 batches: 0.0635
trigger times: 0
Loss after 13107839 batches: 0.0475
trigger times: 1
Loss after 13108768 batches: 0.0411
trigger times: 2
Loss after 13109697 batches: 0.0377
trigger times: 0
Loss after 13110626 batches: 0.0341
trigger times: 0
Loss after 13111555 batches: 0.0322
trigger times: 1
Loss after 13112484 batches: 0.0312
trigger times: 2
Loss after 13113413 batches: 0.0293
trigger times: 3
Loss after 13114342 batches: 0.0287
trigger times: 4
Loss after 13115271 batches: 0.0283
trigger times: 0
Loss after 13116200 batches: 0.0272
trigger times: 1
Loss after 13117129 batches: 0.0265
trigger times: 0
Loss after 13118058 batches: 0.0265
trigger times: 1
Loss after 13118987 batches: 0.0258
trigger times: 2
Loss after 13119916 batches: 0.0253
trigger times: 3
Loss after 13120845 batches: 0.0279
trigger times: 0
Loss after 13121774 batches: 0.0241
trigger times: 1
Loss after 13122703 batches: 0.0240
trigger times: 2
Loss after 13123632 batches: 0.0237
trigger times: 3
Loss after 13124561 batches: 0.0232
trigger times: 4
Loss after 13125490 batches: 0.0247
trigger times: 5
Loss after 13126419 batches: 0.0233
trigger times: 6
Loss after 13127348 batches: 0.0234
trigger times: 7
Loss after 13128277 batches: 0.0234
trigger times: 8
Loss after 13129206 batches: 0.0232
trigger times: 9
Loss after 13130135 batches: 0.0250
trigger times: 10
Loss after 13131064 batches: 0.0243
trigger times: 11
Loss after 13131993 batches: 0.0237
trigger times: 12
Loss after 13132922 batches: 0.0226
trigger times: 13
Loss after 13133851 batches: 0.0229
trigger times: 14
Loss after 13134780 batches: 0.0238
trigger times: 15
Loss after 13135709 batches: 0.0231
trigger times: 16
Loss after 13136638 batches: 0.0244
trigger times: 17
Loss after 13137567 batches: 0.0220
trigger times: 18
Loss after 13138496 batches: 0.0235
trigger times: 19
Loss after 13139425 batches: 0.0223
trigger times: 20
Loss after 13140354 batches: 0.0205
trigger times: 21
Loss after 13141283 batches: 0.0227
trigger times: 22
Loss after 13142212 batches: 0.0208
trigger times: 23
Loss after 13143141 batches: 0.0242
trigger times: 24
Loss after 13144070 batches: 0.0264
trigger times: 25
Early stopping!
Start to test process.
Loss after 13144999 batches: 0.0229
Time to train on one home:  69.6679379940033
trigger times: 0
Loss after 13145961 batches: 0.0791
trigger times: 1
Loss after 13146923 batches: 0.0664
trigger times: 2
Loss after 13147885 batches: 0.0652
trigger times: 3
Loss after 13148847 batches: 0.0617
trigger times: 4
Loss after 13149809 batches: 0.0592
trigger times: 5
Loss after 13150771 batches: 0.0572
trigger times: 6
Loss after 13151733 batches: 0.0563
trigger times: 7
Loss after 13152695 batches: 0.0566
trigger times: 8
Loss after 13153657 batches: 0.0540
trigger times: 9
Loss after 13154619 batches: 0.0547
trigger times: 10
Loss after 13155581 batches: 0.0551
trigger times: 11
Loss after 13156543 batches: 0.0539
trigger times: 12
Loss after 13157505 batches: 0.0529
trigger times: 13
Loss after 13158467 batches: 0.0529
trigger times: 14
Loss after 13159429 batches: 0.0524
trigger times: 15
Loss after 13160391 batches: 0.0520
trigger times: 16
Loss after 13161353 batches: 0.0528
trigger times: 17
Loss after 13162315 batches: 0.0519
trigger times: 18
Loss after 13163277 batches: 0.0510
trigger times: 19
Loss after 13164239 batches: 0.0516
trigger times: 20
Loss after 13165201 batches: 0.0513
trigger times: 21
Loss after 13166163 batches: 0.0513
trigger times: 22
Loss after 13167125 batches: 0.0497
trigger times: 23
Loss after 13168087 batches: 0.0504
trigger times: 24
Loss after 13169049 batches: 0.0507
trigger times: 25
Early stopping!
Start to test process.
Loss after 13170011 batches: 0.0501
Time to train on one home:  56.66888499259949
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13170974 batches: 0.0542
trigger times: 1
Loss after 13171937 batches: 0.0444
trigger times: 2
Loss after 13172900 batches: 0.0446
trigger times: 3
Loss after 13173863 batches: 0.0421
trigger times: 4
Loss after 13174826 batches: 0.0384
trigger times: 5
Loss after 13175789 batches: 0.0377
trigger times: 6
Loss after 13176752 batches: 0.0363
trigger times: 7
Loss after 13177715 batches: 0.0358
trigger times: 8
Loss after 13178678 batches: 0.0350
trigger times: 9
Loss after 13179641 batches: 0.0347
trigger times: 10
Loss after 13180604 batches: 0.0336
trigger times: 11
Loss after 13181567 batches: 0.0334
trigger times: 12
Loss after 13182530 batches: 0.0331
trigger times: 13
Loss after 13183493 batches: 0.0330
trigger times: 14
Loss after 13184456 batches: 0.0322
trigger times: 15
Loss after 13185419 batches: 0.0314
trigger times: 16
Loss after 13186382 batches: 0.0312
trigger times: 17
Loss after 13187345 batches: 0.0312
trigger times: 18
Loss after 13188308 batches: 0.0311
trigger times: 19
Loss after 13189271 batches: 0.0310
trigger times: 20
Loss after 13190234 batches: 0.0304
trigger times: 21
Loss after 13191197 batches: 0.0305
trigger times: 22
Loss after 13192160 batches: 0.0298
trigger times: 23
Loss after 13193123 batches: 0.0298
trigger times: 24
Loss after 13194086 batches: 0.0299
trigger times: 25
Early stopping!
Start to test process.
Loss after 13195049 batches: 0.0297
Time to train on one home:  57.62154746055603
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13196012 batches: 0.0493
trigger times: 1
Loss after 13196975 batches: 0.0438
trigger times: 0
Loss after 13197938 batches: 0.0422
trigger times: 1
Loss after 13198901 batches: 0.0414
trigger times: 2
Loss after 13199864 batches: 0.0386
trigger times: 3
Loss after 13200827 batches: 0.0370
trigger times: 4
Loss after 13201790 batches: 0.0368
trigger times: 5
Loss after 13202753 batches: 0.0357
trigger times: 6
Loss after 13203716 batches: 0.0357
trigger times: 7
Loss after 13204679 batches: 0.0350
trigger times: 8
Loss after 13205642 batches: 0.0336
trigger times: 9
Loss after 13206605 batches: 0.0333
trigger times: 10
Loss after 13207568 batches: 0.0338
trigger times: 11
Loss after 13208531 batches: 0.0325
trigger times: 12
Loss after 13209494 batches: 0.0316
trigger times: 13
Loss after 13210457 batches: 0.0316
trigger times: 14
Loss after 13211420 batches: 0.0311
trigger times: 15
Loss after 13212383 batches: 0.0313
trigger times: 16
Loss after 13213346 batches: 0.0308
trigger times: 17
Loss after 13214309 batches: 0.0307
trigger times: 18
Loss after 13215272 batches: 0.0322
trigger times: 19
Loss after 13216235 batches: 0.0300
trigger times: 20
Loss after 13217198 batches: 0.0308
trigger times: 21
Loss after 13218161 batches: 0.0297
trigger times: 22
Loss after 13219124 batches: 0.0292
trigger times: 23
Loss after 13220087 batches: 0.0288
trigger times: 24
Loss after 13221050 batches: 0.0289
trigger times: 25
Early stopping!
Start to test process.
Loss after 13222013 batches: 0.0290
Time to train on one home:  58.113399267196655
trigger times: 0
Loss after 13222976 batches: 0.1007
trigger times: 0
Loss after 13223939 batches: 0.0944
trigger times: 1
Loss after 13224902 batches: 0.0914
trigger times: 2
Loss after 13225865 batches: 0.0870
trigger times: 3
Loss after 13226828 batches: 0.0839
trigger times: 4
Loss after 13227791 batches: 0.0822
trigger times: 5
Loss after 13228754 batches: 0.0795
trigger times: 6
Loss after 13229717 batches: 0.0787
trigger times: 7
Loss after 13230680 batches: 0.0803
trigger times: 8
Loss after 13231643 batches: 0.0767
trigger times: 9
Loss after 13232606 batches: 0.0753
trigger times: 10
Loss after 13233569 batches: 0.0751
trigger times: 11
Loss after 13234532 batches: 0.0753
trigger times: 12
Loss after 13235495 batches: 0.0757
trigger times: 13
Loss after 13236458 batches: 0.0741
trigger times: 14
Loss after 13237421 batches: 0.0734
trigger times: 15
Loss after 13238384 batches: 0.0725
trigger times: 16
Loss after 13239347 batches: 0.0725
trigger times: 17
Loss after 13240310 batches: 0.0706
trigger times: 18
Loss after 13241273 batches: 0.0704
trigger times: 19
Loss after 13242236 batches: 0.0697
trigger times: 20
Loss after 13243199 batches: 0.0709
trigger times: 21
Loss after 13244162 batches: 0.0671
trigger times: 22
Loss after 13245125 batches: 0.0691
trigger times: 23
Loss after 13246088 batches: 0.0694
trigger times: 24
Loss after 13247051 batches: 0.0666
trigger times: 25
Early stopping!
Start to test process.
Loss after 13248014 batches: 0.0679
Time to train on one home:  53.67410588264465
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13248977 batches: 0.0935
trigger times: 1
Loss after 13249940 batches: 0.0578
trigger times: 2
Loss after 13250903 batches: 0.0557
trigger times: 3
Loss after 13251866 batches: 0.0484
trigger times: 4
Loss after 13252829 batches: 0.0435
trigger times: 5
Loss after 13253792 batches: 0.0404
trigger times: 6
Loss after 13254755 batches: 0.0385
trigger times: 7
Loss after 13255718 batches: 0.0364
trigger times: 8
Loss after 13256681 batches: 0.0350
trigger times: 9
Loss after 13257644 batches: 0.0348
trigger times: 10
Loss after 13258607 batches: 0.0336
trigger times: 11
Loss after 13259570 batches: 0.0333
trigger times: 0
Loss after 13260533 batches: 0.0328
trigger times: 1
Loss after 13261496 batches: 0.0323
trigger times: 2
Loss after 13262459 batches: 0.0310
trigger times: 3
Loss after 13263422 batches: 0.0312
trigger times: 4
Loss after 13264385 batches: 0.0306
trigger times: 5
Loss after 13265348 batches: 0.0296
trigger times: 6
Loss after 13266311 batches: 0.0295
trigger times: 7
Loss after 13267274 batches: 0.0290
trigger times: 8
Loss after 13268237 batches: 0.0294
trigger times: 9
Loss after 13269200 batches: 0.0293
trigger times: 10
Loss after 13270163 batches: 0.0294
trigger times: 11
Loss after 13271126 batches: 0.0296
trigger times: 12
Loss after 13272089 batches: 0.0285
trigger times: 13
Loss after 13273052 batches: 0.0284
trigger times: 14
Loss after 13274015 batches: 0.0273
trigger times: 15
Loss after 13274978 batches: 0.0270
trigger times: 16
Loss after 13275941 batches: 0.0272
trigger times: 17
Loss after 13276904 batches: 0.0285
trigger times: 18
Loss after 13277867 batches: 0.0276
trigger times: 19
Loss after 13278830 batches: 0.0268
trigger times: 20
Loss after 13279793 batches: 0.0286
trigger times: 21
Loss after 13280756 batches: 0.0270
trigger times: 22
Loss after 13281719 batches: 0.0268
trigger times: 23
Loss after 13282682 batches: 0.0259
trigger times: 24
Loss after 13283645 batches: 0.0258
trigger times: 25
Early stopping!
Start to test process.
Loss after 13284608 batches: 0.0245
Time to train on one home:  66.82515811920166
trigger times: 0
Loss after 13285567 batches: 0.0924
trigger times: 1
Loss after 13286526 batches: 0.0491
trigger times: 0
Loss after 13287485 batches: 0.0356
trigger times: 1
Loss after 13288444 batches: 0.0303
trigger times: 2
Loss after 13289403 batches: 0.0260
trigger times: 3
Loss after 13290362 batches: 0.0244
trigger times: 4
Loss after 13291321 batches: 0.0226
trigger times: 0
Loss after 13292280 batches: 0.0215
trigger times: 1
Loss after 13293239 batches: 0.0208
trigger times: 2
Loss after 13294198 batches: 0.0201
trigger times: 3
Loss after 13295157 batches: 0.0198
trigger times: 4
Loss after 13296116 batches: 0.0191
trigger times: 5
Loss after 13297075 batches: 0.0185
trigger times: 6
Loss after 13298034 batches: 0.0181
trigger times: 0
Loss after 13298993 batches: 0.0177
trigger times: 1
Loss after 13299952 batches: 0.0181
trigger times: 2
Loss after 13300911 batches: 0.0185
trigger times: 3
Loss after 13301870 batches: 0.0177
trigger times: 4
Loss after 13302829 batches: 0.0175
trigger times: 5
Loss after 13303788 batches: 0.0169
trigger times: 6
Loss after 13304747 batches: 0.0164
trigger times: 7
Loss after 13305706 batches: 0.0165
trigger times: 8
Loss after 13306665 batches: 0.0155
trigger times: 9
Loss after 13307624 batches: 0.0157
trigger times: 10
Loss after 13308583 batches: 0.0158
trigger times: 11
Loss after 13309542 batches: 0.0153
trigger times: 12
Loss after 13310501 batches: 0.0156
trigger times: 13
Loss after 13311460 batches: 0.0148
trigger times: 14
Loss after 13312419 batches: 0.0147
trigger times: 15
Loss after 13313378 batches: 0.0142
trigger times: 16
Loss after 13314337 batches: 0.0149
trigger times: 17
Loss after 13315296 batches: 0.0148
trigger times: 18
Loss after 13316255 batches: 0.0144
trigger times: 19
Loss after 13317214 batches: 0.0149
trigger times: 20
Loss after 13318173 batches: 0.0147
trigger times: 21
Loss after 13319132 batches: 0.0146
trigger times: 22
Loss after 13320091 batches: 0.0142
trigger times: 23
Loss after 13321050 batches: 0.0137
trigger times: 24
Loss after 13322009 batches: 0.0131
trigger times: 25
Early stopping!
Start to test process.
Loss after 13322968 batches: 0.0135
Time to train on one home:  67.81818509101868
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13323931 batches: 0.0684
trigger times: 1
Loss after 13324894 batches: 0.0310
trigger times: 2
Loss after 13325857 batches: 0.0267
trigger times: 3
Loss after 13326820 batches: 0.0267
trigger times: 4
Loss after 13327783 batches: 0.0258
trigger times: 5
Loss after 13328746 batches: 0.0250
trigger times: 6
Loss after 13329709 batches: 0.0241
trigger times: 7
Loss after 13330672 batches: 0.0231
trigger times: 8
Loss after 13331635 batches: 0.0218
trigger times: 9
Loss after 13332598 batches: 0.0216
trigger times: 10
Loss after 13333561 batches: 0.0214
trigger times: 11
Loss after 13334524 batches: 0.0208
trigger times: 12
Loss after 13335487 batches: 0.0202
trigger times: 13
Loss after 13336450 batches: 0.0199
trigger times: 14
Loss after 13337413 batches: 0.0192
trigger times: 15
Loss after 13338376 batches: 0.0197
trigger times: 16
Loss after 13339339 batches: 0.0190
trigger times: 17
Loss after 13340302 batches: 0.0190
trigger times: 18
Loss after 13341265 batches: 0.0187
trigger times: 19
Loss after 13342228 batches: 0.0187
trigger times: 20
Loss after 13343191 batches: 0.0183
trigger times: 21
Loss after 13344154 batches: 0.0181
trigger times: 22
Loss after 13345117 batches: 0.0182
trigger times: 23
Loss after 13346080 batches: 0.0180
trigger times: 24
Loss after 13347043 batches: 0.0178
trigger times: 25
Early stopping!
Start to test process.
Loss after 13348006 batches: 0.0177
Time to train on one home:  56.68205809593201
trigger times: 0
Loss after 13348951 batches: 0.0646
trigger times: 0
Loss after 13349896 batches: 0.0494
trigger times: 0
Loss after 13350841 batches: 0.0354
trigger times: 1
Loss after 13351786 batches: 0.0321
trigger times: 2
Loss after 13352731 batches: 0.0300
trigger times: 0
Loss after 13353676 batches: 0.0283
trigger times: 1
Loss after 13354621 batches: 0.0278
trigger times: 2
Loss after 13355566 batches: 0.0266
trigger times: 3
Loss after 13356511 batches: 0.0247
trigger times: 4
Loss after 13357456 batches: 0.0252
trigger times: 5
Loss after 13358401 batches: 0.0217
trigger times: 6
Loss after 13359346 batches: 0.0225
trigger times: 7
Loss after 13360291 batches: 0.0211
trigger times: 8
Loss after 13361236 batches: 0.0224
trigger times: 9
Loss after 13362181 batches: 0.0221
trigger times: 10
Loss after 13363126 batches: 0.0230
trigger times: 11
Loss after 13364071 batches: 0.0219
trigger times: 12
Loss after 13365016 batches: 0.0213
trigger times: 13
Loss after 13365961 batches: 0.0205
trigger times: 14
Loss after 13366906 batches: 0.0206
trigger times: 15
Loss after 13367851 batches: 0.0200
trigger times: 16
Loss after 13368796 batches: 0.0198
trigger times: 17
Loss after 13369741 batches: 0.0197
trigger times: 18
Loss after 13370686 batches: 0.0205
trigger times: 19
Loss after 13371631 batches: 0.0212
trigger times: 20
Loss after 13372576 batches: 0.0189
trigger times: 21
Loss after 13373521 batches: 0.0197
trigger times: 22
Loss after 13374466 batches: 0.0193
trigger times: 23
Loss after 13375411 batches: 0.0195
trigger times: 24
Loss after 13376356 batches: 0.0192
trigger times: 25
Early stopping!
Start to test process.
Loss after 13377301 batches: 0.0194
Time to train on one home:  62.375415563583374
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13378238 batches: 0.0833
trigger times: 1
Loss after 13379175 batches: 0.0699
trigger times: 2
Loss after 13380112 batches: 0.0667
trigger times: 3
Loss after 13381049 batches: 0.0629
trigger times: 4
Loss after 13381986 batches: 0.0604
trigger times: 5
Loss after 13382923 batches: 0.0576
trigger times: 6
Loss after 13383860 batches: 0.0575
trigger times: 7
Loss after 13384797 batches: 0.0544
trigger times: 8
Loss after 13385734 batches: 0.0558
trigger times: 9
Loss after 13386671 batches: 0.0548
trigger times: 10
Loss after 13387608 batches: 0.0538
trigger times: 11
Loss after 13388545 batches: 0.0533
trigger times: 12
Loss after 13389482 batches: 0.0517
trigger times: 13
Loss after 13390419 batches: 0.0526
trigger times: 14
Loss after 13391356 batches: 0.0519
trigger times: 15
Loss after 13392293 batches: 0.0519
trigger times: 16
Loss after 13393230 batches: 0.0504
trigger times: 17
Loss after 13394167 batches: 0.0495
trigger times: 18
Loss after 13395104 batches: 0.0506
trigger times: 19
Loss after 13396041 batches: 0.0519
trigger times: 20
Loss after 13396978 batches: 0.0504
trigger times: 21
Loss after 13397915 batches: 0.0510
trigger times: 22
Loss after 13398852 batches: 0.0493
trigger times: 23
Loss after 13399789 batches: 0.0484
trigger times: 24
Loss after 13400726 batches: 0.0485
trigger times: 25
Early stopping!
Start to test process.
Loss after 13401663 batches: 0.0472
Time to train on one home:  56.29845404624939
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 13402626 batches: 0.0483
trigger times: 1
Loss after 13403589 batches: 0.0177
trigger times: 2
Loss after 13404552 batches: 0.0137
trigger times: 3
Loss after 13405515 batches: 0.0141
trigger times: 4
Loss after 13406478 batches: 0.0139
trigger times: 5
Loss after 13407441 batches: 0.0140
trigger times: 6
Loss after 13408404 batches: 0.0140
trigger times: 7
Loss after 13409367 batches: 0.0138
trigger times: 8
Loss after 13410330 batches: 0.0138
trigger times: 9
Loss after 13411293 batches: 0.0139
trigger times: 10
Loss after 13412256 batches: 0.0139
trigger times: 11
Loss after 13413219 batches: 0.0138
trigger times: 12
Loss after 13414182 batches: 0.0138
trigger times: 13
Loss after 13415145 batches: 0.0138
trigger times: 14
Loss after 13416108 batches: 0.0138
trigger times: 15
Loss after 13417071 batches: 0.0139
trigger times: 16
Loss after 13418034 batches: 0.0137
trigger times: 17
Loss after 13418997 batches: 0.0139
trigger times: 18
Loss after 13419960 batches: 0.0140
trigger times: 19
Loss after 13420923 batches: 0.0138
trigger times: 20
Loss after 13421886 batches: 0.0140
trigger times: 21
Loss after 13422849 batches: 0.0139
trigger times: 22
Loss after 13423812 batches: 0.0139
trigger times: 23
Loss after 13424775 batches: 0.0140
trigger times: 24
Loss after 13425738 batches: 0.0138
trigger times: 25
Early stopping!
Start to test process.
Loss after 13426701 batches: 0.0141
Time to train on one home:  57.55916953086853
trigger times: 0
Loss after 13427664 batches: 0.0945
trigger times: 1
Loss after 13428627 batches: 0.0783
trigger times: 2
Loss after 13429590 batches: 0.0723
trigger times: 3
Loss after 13430553 batches: 0.0676
trigger times: 4
Loss after 13431516 batches: 0.0665
trigger times: 5
Loss after 13432479 batches: 0.0643
trigger times: 6
Loss after 13433442 batches: 0.0623
trigger times: 7
Loss after 13434405 batches: 0.0624
trigger times: 8
Loss after 13435368 batches: 0.0608
trigger times: 9
Loss after 13436331 batches: 0.0604
trigger times: 10
Loss after 13437294 batches: 0.0595
trigger times: 11
Loss after 13438257 batches: 0.0583
trigger times: 12
Loss after 13439220 batches: 0.0575
trigger times: 13
Loss after 13440183 batches: 0.0583
trigger times: 14
Loss after 13441146 batches: 0.0582
trigger times: 15
Loss after 13442109 batches: 0.0579
trigger times: 16
Loss after 13443072 batches: 0.0562
trigger times: 17
Loss after 13444035 batches: 0.0550
trigger times: 18
Loss after 13444998 batches: 0.0559
trigger times: 19
Loss after 13445961 batches: 0.0555
trigger times: 20
Loss after 13446924 batches: 0.0554
trigger times: 21
Loss after 13447887 batches: 0.0558
trigger times: 22
Loss after 13448850 batches: 0.0543
trigger times: 23
Loss after 13449813 batches: 0.0553
trigger times: 24
Loss after 13450776 batches: 0.0536
trigger times: 25
Early stopping!
Start to test process.
Loss after 13451739 batches: 0.0535
Time to train on one home:  57.418298959732056
trigger times: 0
Loss after 13452702 batches: 0.0733
trigger times: 1
Loss after 13453665 batches: 0.0528
trigger times: 2
Loss after 13454628 batches: 0.0499
trigger times: 3
Loss after 13455591 batches: 0.0453
trigger times: 4
Loss after 13456554 batches: 0.0423
trigger times: 5
Loss after 13457517 batches: 0.0405
trigger times: 6
Loss after 13458480 batches: 0.0388
trigger times: 7
Loss after 13459443 batches: 0.0381
trigger times: 8
Loss after 13460406 batches: 0.0365
trigger times: 9
Loss after 13461369 batches: 0.0362
trigger times: 10
Loss after 13462332 batches: 0.0351
trigger times: 11
Loss after 13463295 batches: 0.0345
trigger times: 12
Loss after 13464258 batches: 0.0343
trigger times: 13
Loss after 13465221 batches: 0.0342
trigger times: 14
Loss after 13466184 batches: 0.0338
trigger times: 15
Loss after 13467147 batches: 0.0330
trigger times: 16
Loss after 13468110 batches: 0.0322
trigger times: 17
Loss after 13469073 batches: 0.0319
trigger times: 18
Loss after 13470036 batches: 0.0315
trigger times: 19
Loss after 13470999 batches: 0.0311
trigger times: 20
Loss after 13471962 batches: 0.0321
trigger times: 21
Loss after 13472925 batches: 0.0321
trigger times: 22
Loss after 13473888 batches: 0.0320
trigger times: 23
Loss after 13474851 batches: 0.0325
trigger times: 24
Loss after 13475814 batches: 0.0308
trigger times: 25
Early stopping!
Start to test process.
Loss after 13476777 batches: 0.0310
Time to train on one home:  55.516953229904175
trigger times: 0
Loss after 13477673 batches: 0.1034
trigger times: 1
Loss after 13478569 batches: 0.0925
trigger times: 2
Loss after 13479465 batches: 0.0857
trigger times: 3
Loss after 13480361 batches: 0.0809
trigger times: 4
Loss after 13481257 batches: 0.0759
trigger times: 5
Loss after 13482153 batches: 0.0717
trigger times: 6
Loss after 13483049 batches: 0.0678
trigger times: 7
Loss after 13483945 batches: 0.0651
trigger times: 8
Loss after 13484841 batches: 0.0628
trigger times: 9
Loss after 13485737 batches: 0.0623
trigger times: 10
Loss after 13486633 batches: 0.0617
trigger times: 11
Loss after 13487529 batches: 0.0593
trigger times: 12
Loss after 13488425 batches: 0.0596
trigger times: 13
Loss after 13489321 batches: 0.0590
trigger times: 14
Loss after 13490217 batches: 0.0584
trigger times: 15
Loss after 13491113 batches: 0.0582
trigger times: 16
Loss after 13492009 batches: 0.0583
trigger times: 17
Loss after 13492905 batches: 0.0580
trigger times: 18
Loss after 13493801 batches: 0.0607
trigger times: 19
Loss after 13494697 batches: 0.0588
trigger times: 20
Loss after 13495593 batches: 0.0588
trigger times: 21
Loss after 13496489 batches: 0.0599
trigger times: 22
Loss after 13497385 batches: 0.0577
trigger times: 23
Loss after 13498281 batches: 0.0562
trigger times: 24
Loss after 13499177 batches: 0.0580
trigger times: 25
Early stopping!
Start to test process.
Loss after 13500073 batches: 0.0560
Time to train on one home:  52.47789812088013
trigger times: 0
Loss after 13501036 batches: 0.1737
trigger times: 1
Loss after 13501999 batches: 0.1207
trigger times: 2
Loss after 13502962 batches: 0.0999
trigger times: 3
Loss after 13503925 batches: 0.0920
trigger times: 4
Loss after 13504888 batches: 0.0848
trigger times: 5
Loss after 13505851 batches: 0.0784
trigger times: 6
Loss after 13506814 batches: 0.0738
trigger times: 7
Loss after 13507777 batches: 0.0689
trigger times: 8
Loss after 13508740 batches: 0.0652
trigger times: 9
Loss after 13509703 batches: 0.0608
trigger times: 10
Loss after 13510666 batches: 0.0577
trigger times: 11
Loss after 13511629 batches: 0.0537
trigger times: 12
Loss after 13512592 batches: 0.0523
trigger times: 13
Loss after 13513555 batches: 0.0500
trigger times: 14
Loss after 13514518 batches: 0.0507
trigger times: 15
Loss after 13515481 batches: 0.0495
trigger times: 16
Loss after 13516444 batches: 0.0482
trigger times: 17
Loss after 13517407 batches: 0.0474
trigger times: 18
Loss after 13518370 batches: 0.0463
trigger times: 19
Loss after 13519333 batches: 0.0460
trigger times: 20
Loss after 13520296 batches: 0.0446
trigger times: 21
Loss after 13521259 batches: 0.0438
trigger times: 22
Loss after 13522222 batches: 0.0458
trigger times: 23
Loss after 13523185 batches: 0.0448
trigger times: 24
Loss after 13524148 batches: 0.0434
trigger times: 25
Early stopping!
Start to test process.
Loss after 13525111 batches: 0.0417
Time to train on one home:  57.33748745918274
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13526074 batches: 0.0871
trigger times: 1
Loss after 13527037 batches: 0.0758
trigger times: 2
Loss after 13528000 batches: 0.0751
trigger times: 3
Loss after 13528963 batches: 0.0720
trigger times: 4
Loss after 13529926 batches: 0.0675
trigger times: 5
Loss after 13530889 batches: 0.0634
trigger times: 6
Loss after 13531852 batches: 0.0589
trigger times: 7
Loss after 13532815 batches: 0.0585
trigger times: 8
Loss after 13533778 batches: 0.0564
trigger times: 9
Loss after 13534741 batches: 0.0554
trigger times: 10
Loss after 13535704 batches: 0.0531
trigger times: 11
Loss after 13536667 batches: 0.0535
trigger times: 12
Loss after 13537630 batches: 0.0542
trigger times: 13
Loss after 13538593 batches: 0.0557
trigger times: 14
Loss after 13539556 batches: 0.0546
trigger times: 15
Loss after 13540519 batches: 0.0514
trigger times: 16
Loss after 13541482 batches: 0.0513
trigger times: 17
Loss after 13542445 batches: 0.0497
trigger times: 18
Loss after 13543408 batches: 0.0495
trigger times: 19
Loss after 13544371 batches: 0.0486
trigger times: 20
Loss after 13545334 batches: 0.0481
trigger times: 21
Loss after 13546297 batches: 0.0499
trigger times: 22
Loss after 13547260 batches: 0.0477
trigger times: 23
Loss after 13548223 batches: 0.0477
trigger times: 24
Loss after 13549186 batches: 0.0470
trigger times: 25
Early stopping!
Start to test process.
Loss after 13550149 batches: 0.0471
Time to train on one home:  55.990933418273926
trigger times: 0
Loss after 13551112 batches: 0.0874
trigger times: 1
Loss after 13552075 batches: 0.0646
trigger times: 2
Loss after 13553038 batches: 0.0569
trigger times: 3
Loss after 13554001 batches: 0.0519
trigger times: 4
Loss after 13554964 batches: 0.0486
trigger times: 5
Loss after 13555927 batches: 0.0459
trigger times: 6
Loss after 13556890 batches: 0.0446
trigger times: 7
Loss after 13557853 batches: 0.0433
trigger times: 8
Loss after 13558816 batches: 0.0421
trigger times: 9
Loss after 13559779 batches: 0.0411
trigger times: 10
Loss after 13560742 batches: 0.0408
trigger times: 11
Loss after 13561705 batches: 0.0397
trigger times: 12
Loss after 13562668 batches: 0.0404
trigger times: 13
Loss after 13563631 batches: 0.0409
trigger times: 14
Loss after 13564594 batches: 0.0388
trigger times: 15
Loss after 13565557 batches: 0.0387
trigger times: 16
Loss after 13566520 batches: 0.0376
trigger times: 17
Loss after 13567483 batches: 0.0374
trigger times: 18
Loss after 13568446 batches: 0.0382
trigger times: 19
Loss after 13569409 batches: 0.0372
trigger times: 20
Loss after 13570372 batches: 0.0368
trigger times: 21
Loss after 13571335 batches: 0.0372
trigger times: 22
Loss after 13572298 batches: 0.0367
trigger times: 23
Loss after 13573261 batches: 0.0367
trigger times: 24
Loss after 13574224 batches: 0.0357
trigger times: 25
Early stopping!
Start to test process.
Loss after 13575187 batches: 0.0348
Time to train on one home:  55.93701672554016
trigger times: 0
Loss after 13576150 batches: 0.0449
trigger times: 1
Loss after 13577113 batches: 0.0368
trigger times: 2
Loss after 13578076 batches: 0.0338
trigger times: 3
Loss after 13579039 batches: 0.0315
trigger times: 4
Loss after 13580002 batches: 0.0280
trigger times: 5
Loss after 13580965 batches: 0.0273
trigger times: 6
Loss after 13581928 batches: 0.0255
trigger times: 7
Loss after 13582891 batches: 0.0255
trigger times: 8
Loss after 13583854 batches: 0.0243
trigger times: 9
Loss after 13584817 batches: 0.0242
trigger times: 10
Loss after 13585780 batches: 0.0238
trigger times: 11
Loss after 13586743 batches: 0.0234
trigger times: 12
Loss after 13587706 batches: 0.0230
trigger times: 13
Loss after 13588669 batches: 0.0228
trigger times: 14
Loss after 13589632 batches: 0.0224
trigger times: 15
Loss after 13590595 batches: 0.0224
trigger times: 16
Loss after 13591558 batches: 0.0219
trigger times: 17
Loss after 13592521 batches: 0.0216
trigger times: 18
Loss after 13593484 batches: 0.0212
trigger times: 19
Loss after 13594447 batches: 0.0209
trigger times: 20
Loss after 13595410 batches: 0.0211
trigger times: 21
Loss after 13596373 batches: 0.0208
trigger times: 22
Loss after 13597336 batches: 0.0215
trigger times: 23
Loss after 13598299 batches: 0.0239
trigger times: 24
Loss after 13599262 batches: 0.0229
trigger times: 25
Early stopping!
Start to test process.
Loss after 13600225 batches: 0.0218
Time to train on one home:  59.41690158843994
trigger times: 0
Loss after 13601188 batches: 0.0845
trigger times: 1
Loss after 13602151 batches: 0.0487
trigger times: 2
Loss after 13603114 batches: 0.0487
trigger times: 3
Loss after 13604077 batches: 0.0477
trigger times: 4
Loss after 13605040 batches: 0.0451
trigger times: 5
Loss after 13606003 batches: 0.0429
trigger times: 6
Loss after 13606966 batches: 0.0414
trigger times: 7
Loss after 13607929 batches: 0.0404
trigger times: 8
Loss after 13608892 batches: 0.0402
trigger times: 9
Loss after 13609855 batches: 0.0384
trigger times: 10
Loss after 13610818 batches: 0.0380
trigger times: 11
Loss after 13611781 batches: 0.0387
trigger times: 12
Loss after 13612744 batches: 0.0382
trigger times: 13
Loss after 13613707 batches: 0.0372
trigger times: 14
Loss after 13614670 batches: 0.0368
trigger times: 15
Loss after 13615633 batches: 0.0365
trigger times: 16
Loss after 13616596 batches: 0.0362
trigger times: 17
Loss after 13617559 batches: 0.0357
trigger times: 18
Loss after 13618522 batches: 0.0347
trigger times: 19
Loss after 13619485 batches: 0.0354
trigger times: 20
Loss after 13620448 batches: 0.0351
trigger times: 21
Loss after 13621411 batches: 0.0348
trigger times: 22
Loss after 13622374 batches: 0.0345
trigger times: 23
Loss after 13623337 batches: 0.0350
trigger times: 24
Loss after 13624300 batches: 0.0346
trigger times: 25
Early stopping!
Start to test process.
Loss after 13625263 batches: 0.0342
Time to train on one home:  56.27095317840576
trigger times: 0
Loss after 13626158 batches: 0.0622
trigger times: 1
Loss after 13627053 batches: 0.0309
trigger times: 2
Loss after 13627948 batches: 0.0137
trigger times: 0
Loss after 13628843 batches: 0.0087
trigger times: 1
Loss after 13629738 batches: 0.0066
trigger times: 0
Loss after 13630633 batches: 0.0051
trigger times: 1
Loss after 13631528 batches: 0.0049
trigger times: 2
Loss after 13632423 batches: 0.0044
trigger times: 3
Loss after 13633318 batches: 0.0043
trigger times: 4
Loss after 13634213 batches: 0.0097
trigger times: 5
Loss after 13635108 batches: 0.0083
trigger times: 6
Loss after 13636003 batches: 0.0060
trigger times: 7
Loss after 13636898 batches: 0.0048
trigger times: 0
Loss after 13637793 batches: 0.0035
trigger times: 1
Loss after 13638688 batches: 0.0041
trigger times: 2
Loss after 13639583 batches: 0.0045
trigger times: 3
Loss after 13640478 batches: 0.0037
trigger times: 4
Loss after 13641373 batches: 0.0042
trigger times: 5
Loss after 13642268 batches: 0.0037
trigger times: 6
Loss after 13643163 batches: 0.0030
trigger times: 7
Loss after 13644058 batches: 0.0028
trigger times: 8
Loss after 13644953 batches: 0.0027
trigger times: 9
Loss after 13645848 batches: 0.0030
trigger times: 10
Loss after 13646743 batches: 0.0026
trigger times: 11
Loss after 13647638 batches: 0.0023
trigger times: 12
Loss after 13648533 batches: 0.0026
trigger times: 13
Loss after 13649428 batches: 0.0026
trigger times: 14
Loss after 13650323 batches: 0.0024
trigger times: 15
Loss after 13651218 batches: 0.0025
trigger times: 16
Loss after 13652113 batches: 0.0024
trigger times: 17
Loss after 13653008 batches: 0.0023
trigger times: 18
Loss after 13653903 batches: 0.0022
trigger times: 19
Loss after 13654798 batches: 0.0019
trigger times: 20
Loss after 13655693 batches: 0.0021
trigger times: 21
Loss after 13656588 batches: 0.0018
trigger times: 22
Loss after 13657483 batches: 0.0018
trigger times: 23
Loss after 13658378 batches: 0.0020
trigger times: 24
Loss after 13659273 batches: 0.0026
trigger times: 25
Early stopping!
Start to test process.
Loss after 13660168 batches: 0.0029
Time to train on one home:  66.2678952217102
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336, 0.034888222413376094, 0.034613204319843105, 0.033508375671873286, 0.03325774057242148]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171], [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71], [0.13675299286842346, 0.15565351472302724, 0.32920502623266784, 1.059933101560365, 0.6549021202780799, 35.010519125969424, 3184.605], [0.0885479673743248, 0.1559912263273432, 0.33358461589199145, 0.944871497166479, 0.654640173496829, 31.209933508474723, 3183.3313], [0.1345348209142685, 0.15444619808844395, 0.3263722552493196, 1.0544752388510252, 0.6558385527073367, 34.830241138150235, 3189.1584]]
Round_11_results:  [0.1345348209142685, 0.15444619808844395, 0.3263722552493196, 1.0544752388510252, 0.6558385527073367, 34.830241138150235, 3189.1584]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 14318 < 14319; dropping {'Training_Loss': 0.06374299579433032, 'Validation_Loss': 0.07981936633586884, 'Training_R2': 0.30684891911321077, 'Validation_R2': 0.10229429438309257, 'Training_F1': 0.5052873703115358, 'Validation_F1': 0.2975641652749393, 'Training_NEP': 0.8140433310908217, 'Validation_NEP': 0.9294867434965013, 'Training_NDE': 0.5104741812756826, 'Validation_NDE': 0.6966177526116585, 'Training_MAE': 23.4405155425386, 'Validation_MAE': 30.961420789177033, 'Training_MSE': 1606.0436, 'Validation_MSE': 3450.626}.
trigger times: 0
Loss after 13661131 batches: 0.0637
trigger times: 0
Loss after 13662094 batches: 0.0448
trigger times: 0
Loss after 13663057 batches: 0.0432
trigger times: 1
Loss after 13664020 batches: 0.0397
trigger times: 2
Loss after 13664983 batches: 0.0363
trigger times: 3
Loss after 13665946 batches: 0.0340
trigger times: 4
Loss after 13666909 batches: 0.0327
trigger times: 5
Loss after 13667872 batches: 0.0331
trigger times: 6
Loss after 13668835 batches: 0.0311
trigger times: 7
Loss after 13669798 batches: 0.0314
trigger times: 8
Loss after 13670761 batches: 0.0309
trigger times: 9
Loss after 13671724 batches: 0.0312
trigger times: 10
Loss after 13672687 batches: 0.0311
trigger times: 11
Loss after 13673650 batches: 0.0308
trigger times: 12
Loss after 13674613 batches: 0.0312
trigger times: 13
Loss after 13675576 batches: 0.0293
trigger times: 14
Loss after 13676539 batches: 0.0283
trigger times: 15
Loss after 13677502 batches: 0.0291
trigger times: 16
Loss after 13678465 batches: 0.0289
trigger times: 17
Loss after 13679428 batches: 0.0278
trigger times: 18
Loss after 13680391 batches: 0.0283
trigger times: 19
Loss after 13681354 batches: 0.0269
trigger times: 20
Loss after 13682317 batches: 0.0279
trigger times: 21
Loss after 13683280 batches: 0.0276
trigger times: 22
Loss after 13684243 batches: 0.0266
trigger times: 23
Loss after 13685206 batches: 0.0272
trigger times: 24
Loss after 13686169 batches: 0.0295
trigger times: 25
Early stopping!
Start to test process.
Loss after 13687132 batches: 0.0293
Time to train on one home:  58.95409560203552
trigger times: 0
Loss after 13688090 batches: 0.0866
trigger times: 0
Loss after 13689048 batches: 0.0463
trigger times: 1
Loss after 13690006 batches: 0.0418
trigger times: 2
Loss after 13690964 batches: 0.0361
trigger times: 3
Loss after 13691922 batches: 0.0278
trigger times: 4
Loss after 13692880 batches: 0.0275
trigger times: 5
Loss after 13693838 batches: 0.0251
trigger times: 0
Loss after 13694796 batches: 0.0234
trigger times: 1
Loss after 13695754 batches: 0.0224
trigger times: 2
Loss after 13696712 batches: 0.0229
trigger times: 3
Loss after 13697670 batches: 0.0208
trigger times: 0
Loss after 13698628 batches: 0.0203
trigger times: 1
Loss after 13699586 batches: 0.0212
trigger times: 2
Loss after 13700544 batches: 0.0223
trigger times: 3
Loss after 13701502 batches: 0.0211
trigger times: 4
Loss after 13702460 batches: 0.0197
trigger times: 5
Loss after 13703418 batches: 0.0183
trigger times: 6
Loss after 13704376 batches: 0.0183
trigger times: 7
Loss after 13705334 batches: 0.0177
trigger times: 8
Loss after 13706292 batches: 0.0187
trigger times: 9
Loss after 13707250 batches: 0.0173
trigger times: 10
Loss after 13708208 batches: 0.0186
trigger times: 11
Loss after 13709166 batches: 0.0171
trigger times: 12
Loss after 13710124 batches: 0.0168
trigger times: 13
Loss after 13711082 batches: 0.0168
trigger times: 14
Loss after 13712040 batches: 0.0163
trigger times: 15
Loss after 13712998 batches: 0.0160
trigger times: 16
Loss after 13713956 batches: 0.0146
trigger times: 0
Loss after 13714914 batches: 0.0151
trigger times: 0
Loss after 13715872 batches: 0.0156
trigger times: 1
Loss after 13716830 batches: 0.0152
trigger times: 2
Loss after 13717788 batches: 0.0150
trigger times: 3
Loss after 13718746 batches: 0.0148
trigger times: 4
Loss after 13719704 batches: 0.0135
trigger times: 5
Loss after 13720662 batches: 0.0144
trigger times: 6
Loss after 13721620 batches: 0.0143
trigger times: 0
Loss after 13722578 batches: 0.0150
trigger times: 1
Loss after 13723536 batches: 0.0147
trigger times: 2
Loss after 13724494 batches: 0.0142
trigger times: 3
Loss after 13725452 batches: 0.0137
trigger times: 4
Loss after 13726410 batches: 0.0131
trigger times: 5
Loss after 13727368 batches: 0.0138
trigger times: 6
Loss after 13728326 batches: 0.0138
trigger times: 7
Loss after 13729284 batches: 0.0138
trigger times: 8
Loss after 13730242 batches: 0.0148
trigger times: 0
Loss after 13731200 batches: 0.0140
trigger times: 1
Loss after 13732158 batches: 0.0134
trigger times: 2
Loss after 13733116 batches: 0.0133
trigger times: 3
Loss after 13734074 batches: 0.0128
trigger times: 4
Loss after 13735032 batches: 0.0129
trigger times: 5
Loss after 13735990 batches: 0.0126
trigger times: 6
Loss after 13736948 batches: 0.0123
trigger times: 7
Loss after 13737906 batches: 0.0127
trigger times: 8
Loss after 13738864 batches: 0.0121
trigger times: 9
Loss after 13739822 batches: 0.0127
trigger times: 10
Loss after 13740780 batches: 0.0126
trigger times: 11
Loss after 13741738 batches: 0.0135
trigger times: 12
Loss after 13742696 batches: 0.0173
trigger times: 13
Loss after 13743654 batches: 0.0158
trigger times: 14
Loss after 13744612 batches: 0.0150
trigger times: 15
Loss after 13745570 batches: 0.0146
trigger times: 16
Loss after 13746528 batches: 0.0134
trigger times: 17
Loss after 13747486 batches: 0.0128
trigger times: 0
Loss after 13748444 batches: 0.0124
trigger times: 1
Loss after 13749402 batches: 0.0121
trigger times: 2
Loss after 13750360 batches: 0.0124
trigger times: 3
Loss after 13751318 batches: 0.0118
trigger times: 4
Loss after 13752276 batches: 0.0121
trigger times: 5
Loss after 13753234 batches: 0.0114
trigger times: 6
Loss after 13754192 batches: 0.0124
trigger times: 7
Loss after 13755150 batches: 0.0138
trigger times: 8
Loss after 13756108 batches: 0.0149
trigger times: 9
Loss after 13757066 batches: 0.0133
trigger times: 10
Loss after 13758024 batches: 0.0130
trigger times: 11
Loss after 13758982 batches: 0.0117
trigger times: 12
Loss after 13759940 batches: 0.0132
trigger times: 13
Loss after 13760898 batches: 0.0137
trigger times: 14
Loss after 13761856 batches: 0.0128
trigger times: 15
Loss after 13762814 batches: 0.0128
trigger times: 16
Loss after 13763772 batches: 0.0126
trigger times: 17
Loss after 13764730 batches: 0.0122
trigger times: 18
Loss after 13765688 batches: 0.0115
trigger times: 0
Loss after 13766646 batches: 0.0119
trigger times: 1
Loss after 13767604 batches: 0.0119
trigger times: 0
Loss after 13768562 batches: 0.0115
trigger times: 0
Loss after 13769520 batches: 0.0106
trigger times: 1
Loss after 13770478 batches: 0.0114
trigger times: 2
Loss after 13771436 batches: 0.0107
trigger times: 3
Loss after 13772394 batches: 0.0111
trigger times: 4
Loss after 13773352 batches: 0.0113
trigger times: 5
Loss after 13774310 batches: 0.0113
trigger times: 6
Loss after 13775268 batches: 0.0108
trigger times: 7
Loss after 13776226 batches: 0.0107
trigger times: 8
Loss after 13777184 batches: 0.0110
trigger times: 9
Loss after 13778142 batches: 0.0105
trigger times: 10
Loss after 13779100 batches: 0.0107
trigger times: 11
Loss after 13780058 batches: 0.0109
trigger times: 12
Loss after 13781016 batches: 0.0107
trigger times: 13
Loss after 13781974 batches: 0.0102
trigger times: 14
Loss after 13782932 batches: 0.0103
trigger times: 15
Loss after 13783890 batches: 0.0097
trigger times: 16
Loss after 13784848 batches: 0.0103
trigger times: 17
Loss after 13785806 batches: 0.0101
trigger times: 18
Loss after 13786764 batches: 0.0101
trigger times: 19
Loss after 13787722 batches: 0.0102
trigger times: 20
Loss after 13788680 batches: 0.0100
trigger times: 21
Loss after 13789638 batches: 0.0094
trigger times: 22
Loss after 13790596 batches: 0.0091
trigger times: 23
Loss after 13791554 batches: 0.0090
trigger times: 24
Loss after 13792512 batches: 0.0090
trigger times: 25
Early stopping!
Start to test process.
Loss after 13793470 batches: 0.0099
Time to train on one home:  123.83634686470032
trigger times: 0
Loss after 13794433 batches: 0.0846
trigger times: 1
Loss after 13795396 batches: 0.0696
trigger times: 2
Loss after 13796359 batches: 0.0671
trigger times: 3
Loss after 13797322 batches: 0.0659
trigger times: 4
Loss after 13798285 batches: 0.0632
trigger times: 5
Loss after 13799248 batches: 0.0608
trigger times: 6
Loss after 13800211 batches: 0.0590
trigger times: 7
Loss after 13801174 batches: 0.0572
trigger times: 8
Loss after 13802137 batches: 0.0537
trigger times: 9
Loss after 13803100 batches: 0.0536
trigger times: 10
Loss after 13804063 batches: 0.0516
trigger times: 11
Loss after 13805026 batches: 0.0513
trigger times: 12
Loss after 13805989 batches: 0.0510
trigger times: 13
Loss after 13806952 batches: 0.0510
trigger times: 14
Loss after 13807915 batches: 0.0499
trigger times: 15
Loss after 13808878 batches: 0.0486
trigger times: 16
Loss after 13809841 batches: 0.0479
trigger times: 17
Loss after 13810804 batches: 0.0472
trigger times: 18
Loss after 13811767 batches: 0.0472
trigger times: 19
Loss after 13812730 batches: 0.0468
trigger times: 20
Loss after 13813693 batches: 0.0460
trigger times: 21
Loss after 13814656 batches: 0.0443
trigger times: 22
Loss after 13815619 batches: 0.0442
trigger times: 23
Loss after 13816582 batches: 0.0457
trigger times: 24
Loss after 13817545 batches: 0.0459
trigger times: 25
Early stopping!
Start to test process.
Loss after 13818508 batches: 0.0449
Time to train on one home:  57.23059868812561
trigger times: 0
Loss after 13819471 batches: 0.0903
trigger times: 1
Loss after 13820434 batches: 0.0777
trigger times: 2
Loss after 13821397 batches: 0.0757
trigger times: 3
Loss after 13822360 batches: 0.0734
trigger times: 4
Loss after 13823323 batches: 0.0699
trigger times: 5
Loss after 13824286 batches: 0.0680
trigger times: 6
Loss after 13825249 batches: 0.0655
trigger times: 7
Loss after 13826212 batches: 0.0646
trigger times: 8
Loss after 13827175 batches: 0.0622
trigger times: 9
Loss after 13828138 batches: 0.0607
trigger times: 10
Loss after 13829101 batches: 0.0602
trigger times: 0
Loss after 13830064 batches: 0.0609
trigger times: 1
Loss after 13831027 batches: 0.0615
trigger times: 2
Loss after 13831990 batches: 0.0597
trigger times: 3
Loss after 13832953 batches: 0.0601
trigger times: 4
Loss after 13833916 batches: 0.0582
trigger times: 5
Loss after 13834879 batches: 0.0581
trigger times: 6
Loss after 13835842 batches: 0.0586
trigger times: 7
Loss after 13836805 batches: 0.0569
trigger times: 8
Loss after 13837768 batches: 0.0558
trigger times: 9
Loss after 13838731 batches: 0.0565
trigger times: 10
Loss after 13839694 batches: 0.0560
trigger times: 11
Loss after 13840657 batches: 0.0568
trigger times: 12
Loss after 13841620 batches: 0.0564
trigger times: 0
Loss after 13842583 batches: 0.0562
trigger times: 1
Loss after 13843546 batches: 0.0549
trigger times: 2
Loss after 13844509 batches: 0.0554
trigger times: 3
Loss after 13845472 batches: 0.0540
trigger times: 4
Loss after 13846435 batches: 0.0547
trigger times: 5
Loss after 13847398 batches: 0.0552
trigger times: 6
Loss after 13848361 batches: 0.0538
trigger times: 7
Loss after 13849324 batches: 0.0523
trigger times: 8
Loss after 13850287 batches: 0.0520
trigger times: 9
Loss after 13851250 batches: 0.0508
trigger times: 10
Loss after 13852213 batches: 0.0518
trigger times: 11
Loss after 13853176 batches: 0.0528
trigger times: 12
Loss after 13854139 batches: 0.0533
trigger times: 13
Loss after 13855102 batches: 0.0509
trigger times: 14
Loss after 13856065 batches: 0.0507
trigger times: 15
Loss after 13857028 batches: 0.0506
trigger times: 16
Loss after 13857991 batches: 0.0506
trigger times: 17
Loss after 13858954 batches: 0.0510
trigger times: 18
Loss after 13859917 batches: 0.0499
trigger times: 19
Loss after 13860880 batches: 0.0502
trigger times: 20
Loss after 13861843 batches: 0.0494
trigger times: 21
Loss after 13862806 batches: 0.0501
trigger times: 22
Loss after 13863769 batches: 0.0482
trigger times: 23
Loss after 13864732 batches: 0.0514
trigger times: 24
Loss after 13865695 batches: 0.0497
trigger times: 25
Early stopping!
Start to test process.
Loss after 13866658 batches: 0.0498
Time to train on one home:  74.15844798088074
trigger times: 0
Loss after 13867621 batches: 0.0258
trigger times: 1
Loss after 13868584 batches: 0.0217
trigger times: 2
Loss after 13869547 batches: 0.0203
trigger times: 3
Loss after 13870510 batches: 0.0180
trigger times: 0
Loss after 13871473 batches: 0.0167
trigger times: 1
Loss after 13872436 batches: 0.0158
trigger times: 2
Loss after 13873399 batches: 0.0154
trigger times: 3
Loss after 13874362 batches: 0.0155
trigger times: 4
Loss after 13875325 batches: 0.0147
trigger times: 5
Loss after 13876288 batches: 0.0147
trigger times: 6
Loss after 13877251 batches: 0.0137
trigger times: 7
Loss after 13878214 batches: 0.0137
trigger times: 8
Loss after 13879177 batches: 0.0135
trigger times: 9
Loss after 13880140 batches: 0.0136
trigger times: 10
Loss after 13881103 batches: 0.0132
trigger times: 11
Loss after 13882066 batches: 0.0131
trigger times: 12
Loss after 13883029 batches: 0.0133
trigger times: 13
Loss after 13883992 batches: 0.0136
trigger times: 14
Loss after 13884955 batches: 0.0138
trigger times: 15
Loss after 13885918 batches: 0.0135
trigger times: 16
Loss after 13886881 batches: 0.0130
trigger times: 17
Loss after 13887844 batches: 0.0135
trigger times: 18
Loss after 13888807 batches: 0.0130
trigger times: 19
Loss after 13889770 batches: 0.0133
trigger times: 20
Loss after 13890733 batches: 0.0129
trigger times: 21
Loss after 13891696 batches: 0.0128
trigger times: 22
Loss after 13892659 batches: 0.0122
trigger times: 23
Loss after 13893622 batches: 0.0122
trigger times: 24
Loss after 13894585 batches: 0.0119
trigger times: 25
Early stopping!
Start to test process.
Loss after 13895548 batches: 0.0118
Time to train on one home:  65.03130650520325
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13896511 batches: 0.0523
trigger times: 1
Loss after 13897474 batches: 0.0236
trigger times: 2
Loss after 13898437 batches: 0.0192
trigger times: 3
Loss after 13899400 batches: 0.0164
trigger times: 4
Loss after 13900363 batches: 0.0157
trigger times: 5
Loss after 13901326 batches: 0.0145
trigger times: 6
Loss after 13902289 batches: 0.0141
trigger times: 7
Loss after 13903252 batches: 0.0136
trigger times: 8
Loss after 13904215 batches: 0.0135
trigger times: 9
Loss after 13905178 batches: 0.0129
trigger times: 10
Loss after 13906141 batches: 0.0128
trigger times: 11
Loss after 13907104 batches: 0.0128
trigger times: 12
Loss after 13908067 batches: 0.0127
trigger times: 13
Loss after 13909030 batches: 0.0127
trigger times: 14
Loss after 13909993 batches: 0.0124
trigger times: 15
Loss after 13910956 batches: 0.0124
trigger times: 16
Loss after 13911919 batches: 0.0124
trigger times: 17
Loss after 13912882 batches: 0.0124
trigger times: 18
Loss after 13913845 batches: 0.0120
trigger times: 19
Loss after 13914808 batches: 0.0119
trigger times: 20
Loss after 13915771 batches: 0.0118
trigger times: 21
Loss after 13916734 batches: 0.0118
trigger times: 22
Loss after 13917697 batches: 0.0117
trigger times: 23
Loss after 13918660 batches: 0.0118
trigger times: 24
Loss after 13919623 batches: 0.0117
trigger times: 25
Early stopping!
Start to test process.
Loss after 13920586 batches: 0.0116
Time to train on one home:  56.8067467212677
trigger times: 0
Loss after 13921549 batches: 0.1012
trigger times: 0
Loss after 13922512 batches: 0.0922
trigger times: 0
Loss after 13923475 batches: 0.0868
trigger times: 0
Loss after 13924438 batches: 0.0813
trigger times: 1
Loss after 13925401 batches: 0.0778
trigger times: 2
Loss after 13926364 batches: 0.0749
trigger times: 3
Loss after 13927327 batches: 0.0736
trigger times: 4
Loss after 13928290 batches: 0.0721
trigger times: 5
Loss after 13929253 batches: 0.0695
trigger times: 6
Loss after 13930216 batches: 0.0686
trigger times: 7
Loss after 13931179 batches: 0.0677
trigger times: 8
Loss after 13932142 batches: 0.0676
trigger times: 9
Loss after 13933105 batches: 0.0657
trigger times: 10
Loss after 13934068 batches: 0.0651
trigger times: 11
Loss after 13935031 batches: 0.0656
trigger times: 12
Loss after 13935994 batches: 0.0646
trigger times: 13
Loss after 13936957 batches: 0.0645
trigger times: 14
Loss after 13937920 batches: 0.0628
trigger times: 15
Loss after 13938883 batches: 0.0648
trigger times: 16
Loss after 13939846 batches: 0.0640
trigger times: 17
Loss after 13940809 batches: 0.0631
trigger times: 18
Loss after 13941772 batches: 0.0627
trigger times: 19
Loss after 13942735 batches: 0.0604
trigger times: 20
Loss after 13943698 batches: 0.0603
trigger times: 21
Loss after 13944661 batches: 0.0595
trigger times: 22
Loss after 13945624 batches: 0.0594
trigger times: 23
Loss after 13946587 batches: 0.0580
trigger times: 24
Loss after 13947550 batches: 0.0586
trigger times: 25
Early stopping!
Start to test process.
Loss after 13948513 batches: 0.0576
Time to train on one home:  58.956138134002686
trigger times: 0
Loss after 13949476 batches: 0.0508
trigger times: 0
Loss after 13950439 batches: 0.0406
trigger times: 0
Loss after 13951402 batches: 0.0337
trigger times: 1
Loss after 13952365 batches: 0.0299
trigger times: 2
Loss after 13953328 batches: 0.0279
trigger times: 3
Loss after 13954291 batches: 0.0262
trigger times: 4
Loss after 13955254 batches: 0.0249
trigger times: 5
Loss after 13956217 batches: 0.0240
trigger times: 6
Loss after 13957180 batches: 0.0233
trigger times: 7
Loss after 13958143 batches: 0.0225
trigger times: 8
Loss after 13959106 batches: 0.0230
trigger times: 9
Loss after 13960069 batches: 0.0226
trigger times: 10
Loss after 13961032 batches: 0.0215
trigger times: 0
Loss after 13961995 batches: 0.0219
trigger times: 1
Loss after 13962958 batches: 0.0214
trigger times: 2
Loss after 13963921 batches: 0.0213
trigger times: 3
Loss after 13964884 batches: 0.0209
trigger times: 4
Loss after 13965847 batches: 0.0196
trigger times: 5
Loss after 13966810 batches: 0.0194
trigger times: 6
Loss after 13967773 batches: 0.0213
trigger times: 7
Loss after 13968736 batches: 0.0209
trigger times: 8
Loss after 13969699 batches: 0.0209
trigger times: 9
Loss after 13970662 batches: 0.0216
trigger times: 10
Loss after 13971625 batches: 0.0201
trigger times: 11
Loss after 13972588 batches: 0.0197
trigger times: 12
Loss after 13973551 batches: 0.0196
trigger times: 13
Loss after 13974514 batches: 0.0193
trigger times: 14
Loss after 13975477 batches: 0.0185
trigger times: 15
Loss after 13976440 batches: 0.0182
trigger times: 16
Loss after 13977403 batches: 0.0183
trigger times: 17
Loss after 13978366 batches: 0.0186
trigger times: 18
Loss after 13979329 batches: 0.0199
trigger times: 19
Loss after 13980292 batches: 0.0199
trigger times: 20
Loss after 13981255 batches: 0.0189
trigger times: 21
Loss after 13982218 batches: 0.0186
trigger times: 22
Loss after 13983181 batches: 0.0188
trigger times: 23
Loss after 13984144 batches: 0.0182
trigger times: 24
Loss after 13985107 batches: 0.0184
trigger times: 25
Early stopping!
Start to test process.
Loss after 13986070 batches: 0.0170
Time to train on one home:  68.36133217811584
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13987033 batches: 0.0776
trigger times: 1
Loss after 13987996 batches: 0.0708
trigger times: 2
Loss after 13988959 batches: 0.0680
trigger times: 3
Loss after 13989922 batches: 0.0665
trigger times: 4
Loss after 13990885 batches: 0.0626
trigger times: 5
Loss after 13991848 batches: 0.0620
trigger times: 6
Loss after 13992811 batches: 0.0605
trigger times: 7
Loss after 13993774 batches: 0.0601
trigger times: 8
Loss after 13994737 batches: 0.0590
trigger times: 9
Loss after 13995700 batches: 0.0584
trigger times: 10
Loss after 13996663 batches: 0.0583
trigger times: 11
Loss after 13997626 batches: 0.0582
trigger times: 12
Loss after 13998589 batches: 0.0575
trigger times: 13
Loss after 13999552 batches: 0.0569
trigger times: 14
Loss after 14000515 batches: 0.0563
trigger times: 15
Loss after 14001478 batches: 0.0569
trigger times: 16
Loss after 14002441 batches: 0.0552
trigger times: 17
Loss after 14003404 batches: 0.0556
trigger times: 18
Loss after 14004367 batches: 0.0556
trigger times: 19
Loss after 14005330 batches: 0.0547
trigger times: 20
Loss after 14006293 batches: 0.0542
trigger times: 21
Loss after 14007256 batches: 0.0547
trigger times: 22
Loss after 14008219 batches: 0.0548
trigger times: 23
Loss after 14009182 batches: 0.0541
trigger times: 24
Loss after 14010145 batches: 0.0548
trigger times: 25
Early stopping!
Start to test process.
Loss after 14011108 batches: 0.0544
Time to train on one home:  56.42492079734802
trigger times: 0
Loss after 14012071 batches: 0.1076
trigger times: 0
Loss after 14013034 batches: 0.0672
trigger times: 0
Loss after 14013997 batches: 0.0609
trigger times: 1
Loss after 14014960 batches: 0.0547
trigger times: 2
Loss after 14015923 batches: 0.0515
trigger times: 3
Loss after 14016886 batches: 0.0488
trigger times: 4
Loss after 14017849 batches: 0.0469
trigger times: 5
Loss after 14018812 batches: 0.0449
trigger times: 6
Loss after 14019775 batches: 0.0438
trigger times: 7
Loss after 14020738 batches: 0.0436
trigger times: 8
Loss after 14021701 batches: 0.0416
trigger times: 9
Loss after 14022664 batches: 0.0421
trigger times: 10
Loss after 14023627 batches: 0.0434
trigger times: 11
Loss after 14024590 batches: 0.0432
trigger times: 12
Loss after 14025553 batches: 0.0405
trigger times: 13
Loss after 14026516 batches: 0.0414
trigger times: 14
Loss after 14027479 batches: 0.0403
trigger times: 15
Loss after 14028442 batches: 0.0394
trigger times: 16
Loss after 14029405 batches: 0.0395
trigger times: 17
Loss after 14030368 batches: 0.0394
trigger times: 18
Loss after 14031331 batches: 0.0384
trigger times: 19
Loss after 14032294 batches: 0.0380
trigger times: 20
Loss after 14033257 batches: 0.0379
trigger times: 21
Loss after 14034220 batches: 0.0395
trigger times: 22
Loss after 14035183 batches: 0.0391
trigger times: 23
Loss after 14036146 batches: 0.0390
trigger times: 24
Loss after 14037109 batches: 0.0390
trigger times: 25
Early stopping!
Start to test process.
Loss after 14038072 batches: 0.0383
Time to train on one home:  54.71343946456909
trigger times: 0
Loss after 14039035 batches: 0.0758
trigger times: 1
Loss after 14039998 batches: 0.0698
trigger times: 2
Loss after 14040961 batches: 0.0674
trigger times: 3
Loss after 14041924 batches: 0.0639
trigger times: 4
Loss after 14042887 batches: 0.0610
trigger times: 5
Loss after 14043850 batches: 0.0624
trigger times: 6
Loss after 14044813 batches: 0.0582
trigger times: 7
Loss after 14045776 batches: 0.0586
trigger times: 8
Loss after 14046739 batches: 0.0571
trigger times: 9
Loss after 14047702 batches: 0.0564
trigger times: 10
Loss after 14048665 batches: 0.0557
trigger times: 11
Loss after 14049628 batches: 0.0563
trigger times: 12
Loss after 14050591 batches: 0.0582
trigger times: 13
Loss after 14051554 batches: 0.0577
trigger times: 14
Loss after 14052517 batches: 0.0547
trigger times: 15
Loss after 14053480 batches: 0.0536
trigger times: 16
Loss after 14054443 batches: 0.0523
trigger times: 17
Loss after 14055406 batches: 0.0525
trigger times: 18
Loss after 14056369 batches: 0.0519
trigger times: 19
Loss after 14057332 batches: 0.0532
trigger times: 20
Loss after 14058295 batches: 0.0511
trigger times: 21
Loss after 14059258 batches: 0.0518
trigger times: 22
Loss after 14060221 batches: 0.0512
trigger times: 23
Loss after 14061184 batches: 0.0513
trigger times: 24
Loss after 14062147 batches: 0.0506
trigger times: 25
Early stopping!
Start to test process.
Loss after 14063110 batches: 0.0484
Time to train on one home:  56.85676193237305
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14064073 batches: 0.0637
trigger times: 1
Loss after 14065036 batches: 0.0507
trigger times: 2
Loss after 14065999 batches: 0.0485
trigger times: 3
Loss after 14066962 batches: 0.0422
trigger times: 4
Loss after 14067925 batches: 0.0361
trigger times: 5
Loss after 14068888 batches: 0.0338
trigger times: 6
Loss after 14069851 batches: 0.0324
trigger times: 7
Loss after 14070814 batches: 0.0308
trigger times: 8
Loss after 14071777 batches: 0.0291
trigger times: 9
Loss after 14072740 batches: 0.0279
trigger times: 10
Loss after 14073703 batches: 0.0281
trigger times: 11
Loss after 14074666 batches: 0.0269
trigger times: 12
Loss after 14075629 batches: 0.0264
trigger times: 13
Loss after 14076592 batches: 0.0264
trigger times: 14
Loss after 14077555 batches: 0.0261
trigger times: 15
Loss after 14078518 batches: 0.0253
trigger times: 16
Loss after 14079481 batches: 0.0256
trigger times: 17
Loss after 14080444 batches: 0.0245
trigger times: 18
Loss after 14081407 batches: 0.0251
trigger times: 19
Loss after 14082370 batches: 0.0255
trigger times: 20
Loss after 14083333 batches: 0.0244
trigger times: 21
Loss after 14084296 batches: 0.0245
trigger times: 22
Loss after 14085259 batches: 0.0242
trigger times: 23
Loss after 14086222 batches: 0.0244
trigger times: 24
Loss after 14087185 batches: 0.0235
trigger times: 25
Early stopping!
Start to test process.
Loss after 14088148 batches: 0.0236
Time to train on one home:  56.77485370635986
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14089111 batches: 0.0920
trigger times: 0
Loss after 14090074 batches: 0.0536
trigger times: 1
Loss after 14091037 batches: 0.0454
trigger times: 0
Loss after 14092000 batches: 0.0439
trigger times: 0
Loss after 14092963 batches: 0.0401
trigger times: 1
Loss after 14093926 batches: 0.0336
trigger times: 2
Loss after 14094889 batches: 0.0319
trigger times: 3
Loss after 14095852 batches: 0.0288
trigger times: 4
Loss after 14096815 batches: 0.0278
trigger times: 5
Loss after 14097778 batches: 0.0262
trigger times: 6
Loss after 14098741 batches: 0.0269
trigger times: 7
Loss after 14099704 batches: 0.0237
trigger times: 8
Loss after 14100667 batches: 0.0238
trigger times: 9
Loss after 14101630 batches: 0.0227
trigger times: 10
Loss after 14102593 batches: 0.0235
trigger times: 11
Loss after 14103556 batches: 0.0230
trigger times: 12
Loss after 14104519 batches: 0.0210
trigger times: 13
Loss after 14105482 batches: 0.0210
trigger times: 14
Loss after 14106445 batches: 0.0215
trigger times: 15
Loss after 14107408 batches: 0.0209
trigger times: 16
Loss after 14108371 batches: 0.0210
trigger times: 17
Loss after 14109334 batches: 0.0199
trigger times: 18
Loss after 14110297 batches: 0.0197
trigger times: 19
Loss after 14111260 batches: 0.0201
trigger times: 20
Loss after 14112223 batches: 0.0203
trigger times: 21
Loss after 14113186 batches: 0.0194
trigger times: 22
Loss after 14114149 batches: 0.0198
trigger times: 23
Loss after 14115112 batches: 0.0193
trigger times: 24
Loss after 14116075 batches: 0.0210
trigger times: 25
Early stopping!
Start to test process.
Loss after 14117038 batches: 0.0209
Time to train on one home:  57.316630840301514
trigger times: 0
Loss after 14117967 batches: 0.1273
trigger times: 0
Loss after 14118896 batches: 0.0720
trigger times: 0
Loss after 14119825 batches: 0.0543
trigger times: 1
Loss after 14120754 batches: 0.0446
trigger times: 2
Loss after 14121683 batches: 0.0391
trigger times: 0
Loss after 14122612 batches: 0.0354
trigger times: 1
Loss after 14123541 batches: 0.0330
trigger times: 0
Loss after 14124470 batches: 0.0319
trigger times: 0
Loss after 14125399 batches: 0.0325
trigger times: 1
Loss after 14126328 batches: 0.0312
trigger times: 2
Loss after 14127257 batches: 0.0292
trigger times: 3
Loss after 14128186 batches: 0.0314
trigger times: 4
Loss after 14129115 batches: 0.0270
trigger times: 5
Loss after 14130044 batches: 0.0258
trigger times: 6
Loss after 14130973 batches: 0.0262
trigger times: 7
Loss after 14131902 batches: 0.0256
trigger times: 8
Loss after 14132831 batches: 0.0255
trigger times: 9
Loss after 14133760 batches: 0.0285
trigger times: 10
Loss after 14134689 batches: 0.0246
trigger times: 11
Loss after 14135618 batches: 0.0246
trigger times: 12
Loss after 14136547 batches: 0.0240
trigger times: 13
Loss after 14137476 batches: 0.0256
trigger times: 14
Loss after 14138405 batches: 0.0255
trigger times: 15
Loss after 14139334 batches: 0.0259
trigger times: 16
Loss after 14140263 batches: 0.0250
trigger times: 17
Loss after 14141192 batches: 0.0257
trigger times: 18
Loss after 14142121 batches: 0.0291
trigger times: 19
Loss after 14143050 batches: 0.0283
trigger times: 20
Loss after 14143979 batches: 0.0260
trigger times: 21
Loss after 14144908 batches: 0.0280
trigger times: 22
Loss after 14145837 batches: 0.0261
trigger times: 23
Loss after 14146766 batches: 0.0260
trigger times: 24
Loss after 14147695 batches: 0.0251
trigger times: 25
Early stopping!
Start to test process.
Loss after 14148624 batches: 0.0230
Time to train on one home:  66.15202593803406
trigger times: 0
Loss after 14149586 batches: 0.0717
trigger times: 1
Loss after 14150548 batches: 0.0653
trigger times: 2
Loss after 14151510 batches: 0.0635
trigger times: 3
Loss after 14152472 batches: 0.0604
trigger times: 4
Loss after 14153434 batches: 0.0592
trigger times: 5
Loss after 14154396 batches: 0.0566
trigger times: 6
Loss after 14155358 batches: 0.0551
trigger times: 7
Loss after 14156320 batches: 0.0537
trigger times: 8
Loss after 14157282 batches: 0.0535
trigger times: 9
Loss after 14158244 batches: 0.0532
trigger times: 10
Loss after 14159206 batches: 0.0531
trigger times: 11
Loss after 14160168 batches: 0.0525
trigger times: 12
Loss after 14161130 batches: 0.0536
trigger times: 13
Loss after 14162092 batches: 0.0532
trigger times: 14
Loss after 14163054 batches: 0.0534
trigger times: 15
Loss after 14164016 batches: 0.0532
trigger times: 16
Loss after 14164978 batches: 0.0523
trigger times: 17
Loss after 14165940 batches: 0.0514
trigger times: 18
Loss after 14166902 batches: 0.0506
trigger times: 19
Loss after 14167864 batches: 0.0511
trigger times: 20
Loss after 14168826 batches: 0.0510
trigger times: 21
Loss after 14169788 batches: 0.0508
trigger times: 22
Loss after 14170750 batches: 0.0506
trigger times: 23
Loss after 14171712 batches: 0.0494
trigger times: 24
Loss after 14172674 batches: 0.0485
trigger times: 25
Early stopping!
Start to test process.
Loss after 14173636 batches: 0.0498
Time to train on one home:  56.58389067649841
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14174599 batches: 0.0585
trigger times: 1
Loss after 14175562 batches: 0.0448
trigger times: 2
Loss after 14176525 batches: 0.0446
trigger times: 0
Loss after 14177488 batches: 0.0415
trigger times: 1
Loss after 14178451 batches: 0.0384
trigger times: 2
Loss after 14179414 batches: 0.0373
trigger times: 3
Loss after 14180377 batches: 0.0357
trigger times: 4
Loss after 14181340 batches: 0.0349
trigger times: 5
Loss after 14182303 batches: 0.0341
trigger times: 6
Loss after 14183266 batches: 0.0339
trigger times: 7
Loss after 14184229 batches: 0.0330
trigger times: 8
Loss after 14185192 batches: 0.0320
trigger times: 9
Loss after 14186155 batches: 0.0322
trigger times: 10
Loss after 14187118 batches: 0.0319
trigger times: 11
Loss after 14188081 batches: 0.0324
trigger times: 12
Loss after 14189044 batches: 0.0314
trigger times: 13
Loss after 14190007 batches: 0.0316
trigger times: 14
Loss after 14190970 batches: 0.0311
trigger times: 15
Loss after 14191933 batches: 0.0310
trigger times: 16
Loss after 14192896 batches: 0.0314
trigger times: 17
Loss after 14193859 batches: 0.0314
trigger times: 18
Loss after 14194822 batches: 0.0304
trigger times: 19
Loss after 14195785 batches: 0.0305
trigger times: 20
Loss after 14196748 batches: 0.0296
trigger times: 21
Loss after 14197711 batches: 0.0293
trigger times: 22
Loss after 14198674 batches: 0.0292
trigger times: 23
Loss after 14199637 batches: 0.0289
trigger times: 24
Loss after 14200600 batches: 0.0289
trigger times: 25
Early stopping!
Start to test process.
Loss after 14201563 batches: 0.0298
Time to train on one home:  59.45498299598694
trigger times: 0
Loss after 14202526 batches: 0.0545
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14203489 batches: 0.0451
trigger times: 0
Loss after 14204452 batches: 0.0439
trigger times: 1
Loss after 14205415 batches: 0.0402
trigger times: 2
Loss after 14206378 batches: 0.0390
trigger times: 3
Loss after 14207341 batches: 0.0373
trigger times: 4
Loss after 14208304 batches: 0.0367
trigger times: 5
Loss after 14209267 batches: 0.0349
trigger times: 6
Loss after 14210230 batches: 0.0347
trigger times: 7
Loss after 14211193 batches: 0.0336
trigger times: 8
Loss after 14212156 batches: 0.0331
trigger times: 9
Loss after 14213119 batches: 0.0314
trigger times: 10
Loss after 14214082 batches: 0.0312
trigger times: 11
Loss after 14215045 batches: 0.0315
trigger times: 12
Loss after 14216008 batches: 0.0306
trigger times: 13
Loss after 14216971 batches: 0.0307
trigger times: 14
Loss after 14217934 batches: 0.0297
trigger times: 15
Loss after 14218897 batches: 0.0296
trigger times: 16
Loss after 14219860 batches: 0.0299
trigger times: 17
Loss after 14220823 batches: 0.0296
trigger times: 18
Loss after 14221786 batches: 0.0296
trigger times: 19
Loss after 14222749 batches: 0.0290
trigger times: 20
Loss after 14223712 batches: 0.0287
trigger times: 21
Loss after 14224675 batches: 0.0296
trigger times: 22
Loss after 14225638 batches: 0.0295
trigger times: 23
Loss after 14226601 batches: 0.0274
trigger times: 24
Loss after 14227564 batches: 0.0293
trigger times: 25
Early stopping!
Start to test process.
Loss after 14228527 batches: 0.0288
Time to train on one home:  59.19109916687012
trigger times: 0
Loss after 14229490 batches: 0.1083
trigger times: 0
Loss after 14230453 batches: 0.0941
trigger times: 1
Loss after 14231416 batches: 0.0943
trigger times: 2
Loss after 14232379 batches: 0.0870
trigger times: 3
Loss after 14233342 batches: 0.0847
trigger times: 4
Loss after 14234305 batches: 0.0822
trigger times: 5
Loss after 14235268 batches: 0.0792
trigger times: 6
Loss after 14236231 batches: 0.0790
trigger times: 7
Loss after 14237194 batches: 0.0780
trigger times: 8
Loss after 14238157 batches: 0.0763
trigger times: 9
Loss after 14239120 batches: 0.0748
trigger times: 10
Loss after 14240083 batches: 0.0714
trigger times: 11
Loss after 14241046 batches: 0.0730
trigger times: 12
Loss after 14242009 batches: 0.0721
trigger times: 13
Loss after 14242972 batches: 0.0711
trigger times: 14
Loss after 14243935 batches: 0.0705
trigger times: 15
Loss after 14244898 batches: 0.0718
trigger times: 16
Loss after 14245861 batches: 0.0710
trigger times: 17
Loss after 14246824 batches: 0.0704
trigger times: 18
Loss after 14247787 batches: 0.0682
trigger times: 19
Loss after 14248750 batches: 0.0711
trigger times: 20
Loss after 14249713 batches: 0.0682
trigger times: 21
Loss after 14250676 batches: 0.0673
trigger times: 22
Loss after 14251639 batches: 0.0670
trigger times: 23
Loss after 14252602 batches: 0.0668
trigger times: 24
Loss after 14253565 batches: 0.0674
trigger times: 25
Early stopping!
Start to test process.
Loss after 14254528 batches: 0.0669
Time to train on one home:  57.43795847892761
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14255491 batches: 0.1072
trigger times: 1
Loss after 14256454 batches: 0.0595
trigger times: 2
Loss after 14257417 batches: 0.0599
trigger times: 0
Loss after 14258380 batches: 0.0495
trigger times: 0
Loss after 14259343 batches: 0.0443
trigger times: 1
Loss after 14260306 batches: 0.0405
trigger times: 2
Loss after 14261269 batches: 0.0381
trigger times: 3
Loss after 14262232 batches: 0.0364
trigger times: 4
Loss after 14263195 batches: 0.0346
trigger times: 5
Loss after 14264158 batches: 0.0342
trigger times: 6
Loss after 14265121 batches: 0.0326
trigger times: 7
Loss after 14266084 batches: 0.0329
trigger times: 8
Loss after 14267047 batches: 0.0324
trigger times: 9
Loss after 14268010 batches: 0.0308
trigger times: 10
Loss after 14268973 batches: 0.0311
trigger times: 11
Loss after 14269936 batches: 0.0291
trigger times: 12
Loss after 14270899 batches: 0.0297
trigger times: 13
Loss after 14271862 batches: 0.0302
trigger times: 14
Loss after 14272825 batches: 0.0290
trigger times: 15
Loss after 14273788 batches: 0.0290
trigger times: 16
Loss after 14274751 batches: 0.0287
trigger times: 17
Loss after 14275714 batches: 0.0290
trigger times: 18
Loss after 14276677 batches: 0.0283
trigger times: 19
Loss after 14277640 batches: 0.0286
trigger times: 20
Loss after 14278603 batches: 0.0282
trigger times: 21
Loss after 14279566 batches: 0.0277
trigger times: 22
Loss after 14280529 batches: 0.0277
trigger times: 23
Loss after 14281492 batches: 0.0272
trigger times: 24
Loss after 14282455 batches: 0.0276
trigger times: 25
Early stopping!
Start to test process.
Loss after 14283418 batches: 0.0285
Time to train on one home:  56.28435254096985
trigger times: 0
Loss after 14284377 batches: 0.1151
trigger times: 1
Loss after 14285336 batches: 0.0571
trigger times: 0
Loss after 14286295 batches: 0.0424
trigger times: 1
Loss after 14287254 batches: 0.0334
trigger times: 2
Loss after 14288213 batches: 0.0284
trigger times: 3
Loss after 14289172 batches: 0.0266
trigger times: 0
Loss after 14290131 batches: 0.0236
trigger times: 1
Loss after 14291090 batches: 0.0227
trigger times: 2
Loss after 14292049 batches: 0.0208
trigger times: 3
Loss after 14293008 batches: 0.0212
trigger times: 4
Loss after 14293967 batches: 0.0202
trigger times: 5
Loss after 14294926 batches: 0.0189
trigger times: 6
Loss after 14295885 batches: 0.0180
trigger times: 7
Loss after 14296844 batches: 0.0191
trigger times: 8
Loss after 14297803 batches: 0.0176
trigger times: 9
Loss after 14298762 batches: 0.0171
trigger times: 10
Loss after 14299721 batches: 0.0171
trigger times: 11
Loss after 14300680 batches: 0.0178
trigger times: 12
Loss after 14301639 batches: 0.0174
trigger times: 13
Loss after 14302598 batches: 0.0166
trigger times: 14
Loss after 14303557 batches: 0.0166
trigger times: 15
Loss after 14304516 batches: 0.0160
trigger times: 16
Loss after 14305475 batches: 0.0163
trigger times: 17
Loss after 14306434 batches: 0.0157
trigger times: 18
Loss after 14307393 batches: 0.0156
trigger times: 19
Loss after 14308352 batches: 0.0156
trigger times: 20
Loss after 14309311 batches: 0.0155
trigger times: 21
Loss after 14310270 batches: 0.0149
trigger times: 22
Loss after 14311229 batches: 0.0147
trigger times: 23
Loss after 14312188 batches: 0.0155
trigger times: 24
Loss after 14313147 batches: 0.0149
trigger times: 25
Early stopping!
Start to test process.
Loss after 14314106 batches: 0.0144
Time to train on one home:  61.274757862091064
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14315069 batches: 0.0523
trigger times: 1
Loss after 14316032 batches: 0.0284
trigger times: 2
Loss after 14316995 batches: 0.0265
trigger times: 3
Loss after 14317958 batches: 0.0259
trigger times: 4
Loss after 14318921 batches: 0.0247
trigger times: 5
Loss after 14319884 batches: 0.0245
trigger times: 6
Loss after 14320847 batches: 0.0233
trigger times: 7
Loss after 14321810 batches: 0.0219
trigger times: 8
Loss after 14322773 batches: 0.0210
trigger times: 9
Loss after 14323736 batches: 0.0204
trigger times: 10
Loss after 14324699 batches: 0.0196
trigger times: 11
Loss after 14325662 batches: 0.0194
trigger times: 12
Loss after 14326625 batches: 0.0192
trigger times: 13
Loss after 14327588 batches: 0.0189
trigger times: 14
Loss after 14328551 batches: 0.0188
trigger times: 15
Loss after 14329514 batches: 0.0185
trigger times: 16
Loss after 14330477 batches: 0.0182
trigger times: 17
Loss after 14331440 batches: 0.0180
trigger times: 18
Loss after 14332403 batches: 0.0179
trigger times: 19
Loss after 14333366 batches: 0.0179
trigger times: 20
Loss after 14334329 batches: 0.0179
trigger times: 21
Loss after 14335292 batches: 0.0179
trigger times: 22
Loss after 14336255 batches: 0.0177
trigger times: 23
Loss after 14337218 batches: 0.0173
trigger times: 24
Loss after 14338181 batches: 0.0173
trigger times: 25
Early stopping!
Start to test process.
Loss after 14339144 batches: 0.0173
Time to train on one home:  56.766096353530884
trigger times: 0
Loss after 14340089 batches: 0.0779
trigger times: 0
Loss after 14341034 batches: 0.0540
trigger times: 0
Loss after 14341979 batches: 0.0397
trigger times: 1
Loss after 14342924 batches: 0.0350
trigger times: 2
Loss after 14343869 batches: 0.0310
trigger times: 3
Loss after 14344814 batches: 0.0268
trigger times: 4
Loss after 14345759 batches: 0.0256
trigger times: 5
Loss after 14346704 batches: 0.0239
trigger times: 6
Loss after 14347649 batches: 0.0260
trigger times: 7
Loss after 14348594 batches: 0.0250
trigger times: 8
Loss after 14349539 batches: 0.0245
trigger times: 9
Loss after 14350484 batches: 0.0242
trigger times: 10
Loss after 14351429 batches: 0.0226
trigger times: 11
Loss after 14352374 batches: 0.0226
trigger times: 12
Loss after 14353319 batches: 0.0226
trigger times: 13
Loss after 14354264 batches: 0.0215
trigger times: 14
Loss after 14355209 batches: 0.0205
trigger times: 15
Loss after 14356154 batches: 0.0200
trigger times: 16
Loss after 14357099 batches: 0.0210
trigger times: 17
Loss after 14358044 batches: 0.0206
trigger times: 18
Loss after 14358989 batches: 0.0195
trigger times: 19
Loss after 14359934 batches: 0.0210
trigger times: 20
Loss after 14360879 batches: 0.0197
trigger times: 21
Loss after 14361824 batches: 0.0201
trigger times: 22
Loss after 14362769 batches: 0.0194
trigger times: 23
Loss after 14363714 batches: 0.0187
trigger times: 24
Loss after 14364659 batches: 0.0189
trigger times: 25
Early stopping!
Start to test process.
Loss after 14365604 batches: 0.0195
Time to train on one home:  56.24514865875244
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14366541 batches: 0.0869
trigger times: 1
Loss after 14367478 batches: 0.0713
trigger times: 2
Loss after 14368415 batches: 0.0686
trigger times: 3
Loss after 14369352 batches: 0.0633
trigger times: 4
Loss after 14370289 batches: 0.0609
trigger times: 5
Loss after 14371226 batches: 0.0590
trigger times: 6
Loss after 14372163 batches: 0.0583
trigger times: 7
Loss after 14373100 batches: 0.0550
trigger times: 8
Loss after 14374037 batches: 0.0542
trigger times: 9
Loss after 14374974 batches: 0.0528
trigger times: 10
Loss after 14375911 batches: 0.0525
trigger times: 11
Loss after 14376848 batches: 0.0519
trigger times: 12
Loss after 14377785 batches: 0.0524
trigger times: 13
Loss after 14378722 batches: 0.0524
trigger times: 14
Loss after 14379659 batches: 0.0501
trigger times: 15
Loss after 14380596 batches: 0.0511
trigger times: 16
Loss after 14381533 batches: 0.0510
trigger times: 17
Loss after 14382470 batches: 0.0500
trigger times: 18
Loss after 14383407 batches: 0.0481
trigger times: 19
Loss after 14384344 batches: 0.0492
trigger times: 20
Loss after 14385281 batches: 0.0499
trigger times: 21
Loss after 14386218 batches: 0.0483
trigger times: 22
Loss after 14387155 batches: 0.0482
trigger times: 23
Loss after 14388092 batches: 0.0467
trigger times: 24
Loss after 14389029 batches: 0.0472
trigger times: 25
Early stopping!
Start to test process.
Loss after 14389966 batches: 0.0485
Time to train on one home:  60.628857374191284
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 14390929 batches: 0.0256
trigger times: 1
Loss after 14391892 batches: 0.0147
trigger times: 2
Loss after 14392855 batches: 0.0136
trigger times: 3
Loss after 14393818 batches: 0.0136
trigger times: 4
Loss after 14394781 batches: 0.0133
trigger times: 5
Loss after 14395744 batches: 0.0131
trigger times: 6
Loss after 14396707 batches: 0.0123
trigger times: 7
Loss after 14397670 batches: 0.0115
trigger times: 8
Loss after 14398633 batches: 0.0108
trigger times: 9
Loss after 14399596 batches: 0.0105
trigger times: 10
Loss after 14400559 batches: 0.0098
trigger times: 11
Loss after 14401522 batches: 0.0095
trigger times: 12
Loss after 14402485 batches: 0.0091
trigger times: 13
Loss after 14403448 batches: 0.0089
trigger times: 14
Loss after 14404411 batches: 0.0087
trigger times: 15
Loss after 14405374 batches: 0.0082
trigger times: 16
Loss after 14406337 batches: 0.0080
trigger times: 17
Loss after 14407300 batches: 0.0082
trigger times: 18
Loss after 14408263 batches: 0.0075
trigger times: 19
Loss after 14409226 batches: 0.0075
trigger times: 20
Loss after 14410189 batches: 0.0077
trigger times: 21
Loss after 14411152 batches: 0.0073
trigger times: 22
Loss after 14412115 batches: 0.0073
trigger times: 23
Loss after 14413078 batches: 0.0071
trigger times: 24
Loss after 14414041 batches: 0.0069
trigger times: 25
Early stopping!
Start to test process.
Loss after 14415004 batches: 0.0070
Time to train on one home:  56.89491081237793
trigger times: 0
Loss after 14415967 batches: 0.0931
trigger times: 1
Loss after 14416930 batches: 0.0773
trigger times: 2
Loss after 14417893 batches: 0.0715
trigger times: 3
Loss after 14418856 batches: 0.0676
trigger times: 4
Loss after 14419819 batches: 0.0650
trigger times: 5
Loss after 14420782 batches: 0.0625
trigger times: 6
Loss after 14421745 batches: 0.0611
trigger times: 7
Loss after 14422708 batches: 0.0598
trigger times: 8
Loss after 14423671 batches: 0.0582
trigger times: 9
Loss after 14424634 batches: 0.0573
trigger times: 10
Loss after 14425597 batches: 0.0580
trigger times: 11
Loss after 14426560 batches: 0.0575
trigger times: 12
Loss after 14427523 batches: 0.0566
trigger times: 13
Loss after 14428486 batches: 0.0574
trigger times: 14
Loss after 14429449 batches: 0.0564
trigger times: 15
Loss after 14430412 batches: 0.0552
trigger times: 16
Loss after 14431375 batches: 0.0570
trigger times: 17
Loss after 14432338 batches: 0.0553
trigger times: 18
Loss after 14433301 batches: 0.0551
trigger times: 19
Loss after 14434264 batches: 0.0547
trigger times: 20
Loss after 14435227 batches: 0.0541
trigger times: 21
Loss after 14436190 batches: 0.0538
trigger times: 22
Loss after 14437153 batches: 0.0539
trigger times: 23
Loss after 14438116 batches: 0.0522
trigger times: 24
Loss after 14439079 batches: 0.0550
trigger times: 25
Early stopping!
Start to test process.
Loss after 14440042 batches: 0.0555
Time to train on one home:  56.30703043937683
trigger times: 0
Loss after 14441005 batches: 0.0640
trigger times: 1
Loss after 14441968 batches: 0.0499
trigger times: 2
Loss after 14442931 batches: 0.0456
trigger times: 3
Loss after 14443894 batches: 0.0427
trigger times: 4
Loss after 14444857 batches: 0.0404
trigger times: 5
Loss after 14445820 batches: 0.0387
trigger times: 6
Loss after 14446783 batches: 0.0374
trigger times: 7
Loss after 14447746 batches: 0.0362
trigger times: 8
Loss after 14448709 batches: 0.0355
trigger times: 9
Loss after 14449672 batches: 0.0342
trigger times: 10
Loss after 14450635 batches: 0.0338
trigger times: 11
Loss after 14451598 batches: 0.0336
trigger times: 12
Loss after 14452561 batches: 0.0325
trigger times: 13
Loss after 14453524 batches: 0.0325
trigger times: 14
Loss after 14454487 batches: 0.0323
trigger times: 15
Loss after 14455450 batches: 0.0335
trigger times: 16
Loss after 14456413 batches: 0.0324
trigger times: 17
Loss after 14457376 batches: 0.0317
trigger times: 18
Loss after 14458339 batches: 0.0311
trigger times: 19
Loss after 14459302 batches: 0.0309
trigger times: 20
Loss after 14460265 batches: 0.0307
trigger times: 21
Loss after 14461228 batches: 0.0299
trigger times: 22
Loss after 14462191 batches: 0.0299
trigger times: 23
Loss after 14463154 batches: 0.0297
trigger times: 24
Loss after 14464117 batches: 0.0296
trigger times: 25
Early stopping!
Start to test process.
Loss after 14465080 batches: 0.0298
Time to train on one home:  57.33077073097229
trigger times: 0
Loss after 14465976 batches: 0.1018
trigger times: 1
Loss after 14466872 batches: 0.0925
trigger times: 2
Loss after 14467768 batches: 0.0875
trigger times: 3
Loss after 14468664 batches: 0.0819
trigger times: 4
Loss after 14469560 batches: 0.0754
trigger times: 5
Loss after 14470456 batches: 0.0712
trigger times: 6
Loss after 14471352 batches: 0.0693
trigger times: 7
Loss after 14472248 batches: 0.0663
trigger times: 8
Loss after 14473144 batches: 0.0634
trigger times: 9
Loss after 14474040 batches: 0.0617
trigger times: 10
Loss after 14474936 batches: 0.0619
trigger times: 11
Loss after 14475832 batches: 0.0596
trigger times: 12
Loss after 14476728 batches: 0.0611
trigger times: 13
Loss after 14477624 batches: 0.0590
trigger times: 14
Loss after 14478520 batches: 0.0587
trigger times: 15
Loss after 14479416 batches: 0.0565
trigger times: 16
Loss after 14480312 batches: 0.0569
trigger times: 17
Loss after 14481208 batches: 0.0567
trigger times: 18
Loss after 14482104 batches: 0.0545
trigger times: 19
Loss after 14483000 batches: 0.0542
trigger times: 20
Loss after 14483896 batches: 0.0556
trigger times: 21
Loss after 14484792 batches: 0.0531
trigger times: 22
Loss after 14485688 batches: 0.0545
trigger times: 23
Loss after 14486584 batches: 0.0551
trigger times: 24
Loss after 14487480 batches: 0.0550
trigger times: 25
Early stopping!
Start to test process.
Loss after 14488376 batches: 0.0523
Time to train on one home:  55.5372269153595
trigger times: 0
Loss after 14489339 batches: 0.1540
trigger times: 1
Loss after 14490302 batches: 0.1149
trigger times: 2
Loss after 14491265 batches: 0.0996
trigger times: 3
Loss after 14492228 batches: 0.0897
trigger times: 4
Loss after 14493191 batches: 0.0810
trigger times: 5
Loss after 14494154 batches: 0.0779
trigger times: 6
Loss after 14495117 batches: 0.0718
trigger times: 7
Loss after 14496080 batches: 0.0677
trigger times: 8
Loss after 14497043 batches: 0.0636
trigger times: 9
Loss after 14498006 batches: 0.0595
trigger times: 10
Loss after 14498969 batches: 0.0562
trigger times: 11
Loss after 14499932 batches: 0.0526
trigger times: 12
Loss after 14500895 batches: 0.0525
trigger times: 13
Loss after 14501858 batches: 0.0512
trigger times: 14
Loss after 14502821 batches: 0.0490
trigger times: 15
Loss after 14503784 batches: 0.0483
trigger times: 16
Loss after 14504747 batches: 0.0475
trigger times: 17
Loss after 14505710 batches: 0.0469
trigger times: 18
Loss after 14506673 batches: 0.0446
trigger times: 19
Loss after 14507636 batches: 0.0451
trigger times: 20
Loss after 14508599 batches: 0.0456
trigger times: 21
Loss after 14509562 batches: 0.0442
trigger times: 22
Loss after 14510525 batches: 0.0433
trigger times: 23
Loss after 14511488 batches: 0.0414
trigger times: 24
Loss after 14512451 batches: 0.0417
trigger times: 25
Early stopping!
Start to test process.
Loss after 14513414 batches: 0.0422
Time to train on one home:  53.670849323272705
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14514377 batches: 0.0832
trigger times: 1
Loss after 14515340 batches: 0.0751
trigger times: 2
Loss after 14516303 batches: 0.0730
trigger times: 3
Loss after 14517266 batches: 0.0706
trigger times: 4
Loss after 14518229 batches: 0.0674
trigger times: 5
Loss after 14519192 batches: 0.0630
trigger times: 6
Loss after 14520155 batches: 0.0600
trigger times: 7
Loss after 14521118 batches: 0.0575
trigger times: 8
Loss after 14522081 batches: 0.0569
trigger times: 9
Loss after 14523044 batches: 0.0536
trigger times: 10
Loss after 14524007 batches: 0.0535
trigger times: 11
Loss after 14524970 batches: 0.0516
trigger times: 12
Loss after 14525933 batches: 0.0491
trigger times: 13
Loss after 14526896 batches: 0.0510
trigger times: 14
Loss after 14527859 batches: 0.0516
trigger times: 15
Loss after 14528822 batches: 0.0505
trigger times: 16
Loss after 14529785 batches: 0.0512
trigger times: 17
Loss after 14530748 batches: 0.0496
trigger times: 18
Loss after 14531711 batches: 0.0473
trigger times: 19
Loss after 14532674 batches: 0.0486
trigger times: 20
Loss after 14533637 batches: 0.0485
trigger times: 21
Loss after 14534600 batches: 0.0468
trigger times: 22
Loss after 14535563 batches: 0.0478
trigger times: 23
Loss after 14536526 batches: 0.0449
trigger times: 24
Loss after 14537489 batches: 0.0431
trigger times: 25
Early stopping!
Start to test process.
Loss after 14538452 batches: 0.0488
Time to train on one home:  54.64447236061096
trigger times: 0
Loss after 14539415 batches: 0.0896
trigger times: 0
Loss after 14540378 batches: 0.0639
trigger times: 1
Loss after 14541341 batches: 0.0591
trigger times: 2
Loss after 14542304 batches: 0.0518
trigger times: 3
Loss after 14543267 batches: 0.0478
trigger times: 4
Loss after 14544230 batches: 0.0459
trigger times: 5
Loss after 14545193 batches: 0.0447
trigger times: 6
Loss after 14546156 batches: 0.0432
trigger times: 7
Loss after 14547119 batches: 0.0428
trigger times: 8
Loss after 14548082 batches: 0.0405
trigger times: 9
Loss after 14549045 batches: 0.0401
trigger times: 10
Loss after 14550008 batches: 0.0391
trigger times: 11
Loss after 14550971 batches: 0.0408
trigger times: 12
Loss after 14551934 batches: 0.0396
trigger times: 13
Loss after 14552897 batches: 0.0382
trigger times: 14
Loss after 14553860 batches: 0.0372
trigger times: 15
Loss after 14554823 batches: 0.0373
trigger times: 16
Loss after 14555786 batches: 0.0373
trigger times: 17
Loss after 14556749 batches: 0.0358
trigger times: 18
Loss after 14557712 batches: 0.0364
trigger times: 19
Loss after 14558675 batches: 0.0348
trigger times: 20
Loss after 14559638 batches: 0.0357
trigger times: 21
Loss after 14560601 batches: 0.0348
trigger times: 22
Loss after 14561564 batches: 0.0357
trigger times: 23
Loss after 14562527 batches: 0.0369
trigger times: 24
Loss after 14563490 batches: 0.0370
trigger times: 25
Early stopping!
Start to test process.
Loss after 14564453 batches: 0.0350
Time to train on one home:  57.662590742111206
trigger times: 0
Loss after 14565416 batches: 0.0493
trigger times: 1
Loss after 14566379 batches: 0.0358
trigger times: 2
Loss after 14567342 batches: 0.0350
trigger times: 3
Loss after 14568305 batches: 0.0313
trigger times: 4
Loss after 14569268 batches: 0.0283
trigger times: 5
Loss after 14570231 batches: 0.0262
trigger times: 6
Loss after 14571194 batches: 0.0258
trigger times: 7
Loss after 14572157 batches: 0.0249
trigger times: 8
Loss after 14573120 batches: 0.0240
trigger times: 9
Loss after 14574083 batches: 0.0229
trigger times: 10
Loss after 14575046 batches: 0.0225
trigger times: 11
Loss after 14576009 batches: 0.0222
trigger times: 12
Loss after 14576972 batches: 0.0223
trigger times: 13
Loss after 14577935 batches: 0.0222
trigger times: 14
Loss after 14578898 batches: 0.0222
trigger times: 15
Loss after 14579861 batches: 0.0221
trigger times: 16
Loss after 14580824 batches: 0.0219
trigger times: 17
Loss after 14581787 batches: 0.0228
trigger times: 18
Loss after 14582750 batches: 0.0224
trigger times: 19
Loss after 14583713 batches: 0.0219
trigger times: 20
Loss after 14584676 batches: 0.0218
trigger times: 21
Loss after 14585639 batches: 0.0208
trigger times: 22
Loss after 14586602 batches: 0.0200
trigger times: 23
Loss after 14587565 batches: 0.0204
trigger times: 24
Loss after 14588528 batches: 0.0207
trigger times: 25
Early stopping!
Start to test process.
Loss after 14589491 batches: 0.0201
Time to train on one home:  54.219658851623535
trigger times: 0
Loss after 14590454 batches: 0.0625
trigger times: 1
Loss after 14591417 batches: 0.0456
trigger times: 2
Loss after 14592380 batches: 0.0461
trigger times: 3
Loss after 14593343 batches: 0.0439
trigger times: 4
Loss after 14594306 batches: 0.0414
trigger times: 5
Loss after 14595269 batches: 0.0397
trigger times: 6
Loss after 14596232 batches: 0.0389
trigger times: 7
Loss after 14597195 batches: 0.0383
trigger times: 8
Loss after 14598158 batches: 0.0376
trigger times: 9
Loss after 14599121 batches: 0.0379
trigger times: 10
Loss after 14600084 batches: 0.0364
trigger times: 11
Loss after 14601047 batches: 0.0364
trigger times: 12
Loss after 14602010 batches: 0.0367
trigger times: 13
Loss after 14602973 batches: 0.0357
trigger times: 14
Loss after 14603936 batches: 0.0348
trigger times: 15
Loss after 14604899 batches: 0.0347
trigger times: 16
Loss after 14605862 batches: 0.0350
trigger times: 17
Loss after 14606825 batches: 0.0344
trigger times: 18
Loss after 14607788 batches: 0.0343
trigger times: 19
Loss after 14608751 batches: 0.0341
trigger times: 20
Loss after 14609714 batches: 0.0338
trigger times: 21
Loss after 14610677 batches: 0.0334
trigger times: 22
Loss after 14611640 batches: 0.0333
trigger times: 23
Loss after 14612603 batches: 0.0326
trigger times: 24
Loss after 14613566 batches: 0.0332
trigger times: 25
Early stopping!
Start to test process.
Loss after 14614529 batches: 0.0334
Time to train on one home:  60.16582274436951
trigger times: 0
Loss after 14615424 batches: 0.0804
trigger times: 1
Loss after 14616319 batches: 0.0457
trigger times: 2
Loss after 14617214 batches: 0.0203
trigger times: 0
Loss after 14618109 batches: 0.0117
trigger times: 1
Loss after 14619004 batches: 0.0090
trigger times: 0
Loss after 14619899 batches: 0.0070
trigger times: 1
Loss after 14620794 batches: 0.0058
trigger times: 2
Loss after 14621689 batches: 0.0050
trigger times: 3
Loss after 14622584 batches: 0.0047
trigger times: 4
Loss after 14623479 batches: 0.0040
trigger times: 5
Loss after 14624374 batches: 0.0038
trigger times: 0
Loss after 14625269 batches: 0.0032
trigger times: 1
Loss after 14626164 batches: 0.0026
trigger times: 2
Loss after 14627059 batches: 0.0027
trigger times: 3
Loss after 14627954 batches: 0.0027
trigger times: 4
Loss after 14628849 batches: 0.0025
trigger times: 5
Loss after 14629744 batches: 0.0027
trigger times: 6
Loss after 14630639 batches: 0.0034
trigger times: 7
Loss after 14631534 batches: 0.0038
trigger times: 8
Loss after 14632429 batches: 0.0033
trigger times: 9
Loss after 14633324 batches: 0.0028
trigger times: 10
Loss after 14634219 batches: 0.0022
trigger times: 11
Loss after 14635114 batches: 0.0022
trigger times: 12
Loss after 14636009 batches: 0.0034
trigger times: 13
Loss after 14636904 batches: 0.0034
trigger times: 14
Loss after 14637799 batches: 0.0031
trigger times: 0
Loss after 14638694 batches: 0.0027
trigger times: 1
Loss after 14639589 batches: 0.0026
trigger times: 2
Loss after 14640484 batches: 0.0025
trigger times: 3
Loss after 14641379 batches: 0.0025
trigger times: 4
Loss after 14642274 batches: 0.0024
trigger times: 5
Loss after 14643169 batches: 0.0024
trigger times: 6
Loss after 14644064 batches: 0.0023
trigger times: 7
Loss after 14644959 batches: 0.0019
trigger times: 8
Loss after 14645854 batches: 0.0020
trigger times: 9
Loss after 14646749 batches: 0.0021
trigger times: 10
Loss after 14647644 batches: 0.0022
trigger times: 11
Loss after 14648539 batches: 0.0020
trigger times: 12
Loss after 14649434 batches: 0.0017
trigger times: 13
Loss after 14650329 batches: 0.0019
trigger times: 14
Loss after 14651224 batches: 0.0017
trigger times: 15
Loss after 14652119 batches: 0.0017
trigger times: 16
Loss after 14653014 batches: 0.0016
trigger times: 17
Loss after 14653909 batches: 0.0017
trigger times: 18
Loss after 14654804 batches: 0.0014
trigger times: 19
Loss after 14655699 batches: 0.0015
trigger times: 20
Loss after 14656594 batches: 0.0014
trigger times: 21
Loss after 14657489 batches: 0.0015
trigger times: 22
Loss after 14658384 batches: 0.0019
trigger times: 23
Loss after 14659279 batches: 0.0016
trigger times: 24
Loss after 14660174 batches: 0.0022
trigger times: 25
Early stopping!
Start to test process.
Loss after 14661069 batches: 0.0022
Time to train on one home:  75.12429571151733
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336, 0.034888222413376094, 0.034613204319843105, 0.033508375671873286, 0.03325774057242148, 0.03243952582662181]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171], [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71], [0.13675299286842346, 0.15565351472302724, 0.32920502623266784, 1.059933101560365, 0.6549021202780799, 35.010519125969424, 3184.605], [0.0885479673743248, 0.1559912263273432, 0.33358461589199145, 0.944871497166479, 0.654640173496829, 31.209933508474723, 3183.3313], [0.1345348209142685, 0.15444619808844395, 0.3263722552493196, 1.0544752388510252, 0.6558385527073367, 34.830241138150235, 3189.1584], [0.09016867727041245, 0.15349941943138035, 0.3319629557497865, 0.9480173077546179, 0.65657289912243, 31.313842388762147, 3192.7297]]
Round_12_results:  [0.09016867727041245, 0.15349941943138035, 0.3319629557497865, 0.9480173077546179, 0.65657289912243, 31.313842388762147, 3192.7297]
trigger times: 0
Loss after 14662032 batches: 0.0516
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 15366 < 15367; dropping {'Training_Loss': 0.0515828292284693, 'Validation_Loss': 0.06416677683591843, 'Training_R2': 0.29997904175701495, 'Validation_R2': 0.1307529436364494, 'Training_F1': 0.5088477899067921, 'Validation_F1': 0.3224779685827689, 'Training_NEP': 0.771933094188847, 'Validation_NEP': 0.8873197294145185, 'Training_NDE': 0.5155335339251483, 'Validation_NDE': 0.6745338942702348, 'Training_MAE': 22.22794414136021, 'Validation_MAE': 29.556827689221397, 'Training_MSE': 1621.9613, 'Validation_MSE': 3341.236}.
trigger times: 1
Loss after 14662995 batches: 0.0429
trigger times: 0
Loss after 14663958 batches: 0.0398
trigger times: 1
Loss after 14664921 batches: 0.0358
trigger times: 2
Loss after 14665884 batches: 0.0341
trigger times: 3
Loss after 14666847 batches: 0.0325
trigger times: 4
Loss after 14667810 batches: 0.0312
trigger times: 5
Loss after 14668773 batches: 0.0323
trigger times: 6
Loss after 14669736 batches: 0.0300
trigger times: 7
Loss after 14670699 batches: 0.0300
trigger times: 8
Loss after 14671662 batches: 0.0295
trigger times: 9
Loss after 14672625 batches: 0.0295
trigger times: 10
Loss after 14673588 batches: 0.0303
trigger times: 11
Loss after 14674551 batches: 0.0283
trigger times: 12
Loss after 14675514 batches: 0.0280
trigger times: 13
Loss after 14676477 batches: 0.0278
trigger times: 14
Loss after 14677440 batches: 0.0279
trigger times: 15
Loss after 14678403 batches: 0.0274
trigger times: 16
Loss after 14679366 batches: 0.0266
trigger times: 17
Loss after 14680329 batches: 0.0282
trigger times: 18
Loss after 14681292 batches: 0.0269
trigger times: 19
Loss after 14682255 batches: 0.0265
trigger times: 20
Loss after 14683218 batches: 0.0253
trigger times: 21
Loss after 14684181 batches: 0.0268
trigger times: 22
Loss after 14685144 batches: 0.0272
trigger times: 23
Loss after 14686107 batches: 0.0271
trigger times: 24
Loss after 14687070 batches: 0.0264
trigger times: 25
Early stopping!
Start to test process.
Loss after 14688033 batches: 0.0255
Time to train on one home:  58.4832866191864
trigger times: 0
Loss after 14688991 batches: 0.0607
trigger times: 0
Loss after 14689949 batches: 0.0399
trigger times: 1
Loss after 14690907 batches: 0.0332
trigger times: 0
Loss after 14691865 batches: 0.0287
trigger times: 1
Loss after 14692823 batches: 0.0254
trigger times: 0
Loss after 14693781 batches: 0.0230
trigger times: 1
Loss after 14694739 batches: 0.0233
trigger times: 2
Loss after 14695697 batches: 0.0223
trigger times: 3
Loss after 14696655 batches: 0.0255
trigger times: 4
Loss after 14697613 batches: 0.0253
trigger times: 5
Loss after 14698571 batches: 0.0258
trigger times: 6
Loss after 14699529 batches: 0.0224
trigger times: 7
Loss after 14700487 batches: 0.0216
trigger times: 8
Loss after 14701445 batches: 0.0203
trigger times: 9
Loss after 14702403 batches: 0.0202
trigger times: 10
Loss after 14703361 batches: 0.0191
trigger times: 11
Loss after 14704319 batches: 0.0189
trigger times: 12
Loss after 14705277 batches: 0.0198
trigger times: 13
Loss after 14706235 batches: 0.0191
trigger times: 14
Loss after 14707193 batches: 0.0179
trigger times: 15
Loss after 14708151 batches: 0.0167
trigger times: 16
Loss after 14709109 batches: 0.0161
trigger times: 17
Loss after 14710067 batches: 0.0164
trigger times: 18
Loss after 14711025 batches: 0.0164
trigger times: 19
Loss after 14711983 batches: 0.0159
trigger times: 20
Loss after 14712941 batches: 0.0162
trigger times: 21
Loss after 14713899 batches: 0.0173
trigger times: 22
Loss after 14714857 batches: 0.0176
trigger times: 23
Loss after 14715815 batches: 0.0170
trigger times: 24
Loss after 14716773 batches: 0.0160
trigger times: 25
Early stopping!
Start to test process.
Loss after 14717731 batches: 0.0170
Time to train on one home:  61.81835126876831
trigger times: 0
Loss after 14718694 batches: 0.1012
trigger times: 1
Loss after 14719657 batches: 0.0722
trigger times: 2
Loss after 14720620 batches: 0.0667
trigger times: 3
Loss after 14721583 batches: 0.0664
trigger times: 4
Loss after 14722546 batches: 0.0642
trigger times: 5
Loss after 14723509 batches: 0.0607
trigger times: 6
Loss after 14724472 batches: 0.0582
trigger times: 7
Loss after 14725435 batches: 0.0565
trigger times: 8
Loss after 14726398 batches: 0.0538
trigger times: 9
Loss after 14727361 batches: 0.0534
trigger times: 10
Loss after 14728324 batches: 0.0514
trigger times: 11
Loss after 14729287 batches: 0.0505
trigger times: 12
Loss after 14730250 batches: 0.0494
trigger times: 13
Loss after 14731213 batches: 0.0478
trigger times: 14
Loss after 14732176 batches: 0.0490
trigger times: 15
Loss after 14733139 batches: 0.0467
trigger times: 16
Loss after 14734102 batches: 0.0472
trigger times: 17
Loss after 14735065 batches: 0.0475
trigger times: 18
Loss after 14736028 batches: 0.0476
trigger times: 19
Loss after 14736991 batches: 0.0471
trigger times: 20
Loss after 14737954 batches: 0.0459
trigger times: 21
Loss after 14738917 batches: 0.0455
trigger times: 22
Loss after 14739880 batches: 0.0460
trigger times: 23
Loss after 14740843 batches: 0.0446
trigger times: 24
Loss after 14741806 batches: 0.0452
trigger times: 25
Early stopping!
Start to test process.
Loss after 14742769 batches: 0.0450
Time to train on one home:  56.92976903915405
trigger times: 0
Loss after 14743732 batches: 0.1016
trigger times: 1
Loss after 14744695 batches: 0.0781
trigger times: 2
Loss after 14745658 batches: 0.0761
trigger times: 3
Loss after 14746621 batches: 0.0753
trigger times: 4
Loss after 14747584 batches: 0.0711
trigger times: 5
Loss after 14748547 batches: 0.0690
trigger times: 6
Loss after 14749510 batches: 0.0659
trigger times: 7
Loss after 14750473 batches: 0.0655
trigger times: 8
Loss after 14751436 batches: 0.0631
trigger times: 9
Loss after 14752399 batches: 0.0624
trigger times: 10
Loss after 14753362 batches: 0.0619
trigger times: 11
Loss after 14754325 batches: 0.0615
trigger times: 12
Loss after 14755288 batches: 0.0605
trigger times: 13
Loss after 14756251 batches: 0.0596
trigger times: 14
Loss after 14757214 batches: 0.0585
trigger times: 15
Loss after 14758177 batches: 0.0598
trigger times: 16
Loss after 14759140 batches: 0.0583
trigger times: 17
Loss after 14760103 batches: 0.0572
trigger times: 18
Loss after 14761066 batches: 0.0556
trigger times: 19
Loss after 14762029 batches: 0.0568
trigger times: 20
Loss after 14762992 batches: 0.0564
trigger times: 21
Loss after 14763955 batches: 0.0586
trigger times: 22
Loss after 14764918 batches: 0.0581
trigger times: 23
Loss after 14765881 batches: 0.0580
trigger times: 24
Loss after 14766844 batches: 0.0574
trigger times: 25
Early stopping!
Start to test process.
Loss after 14767807 batches: 0.0559
Time to train on one home:  53.13997411727905
trigger times: 0
Loss after 14768770 batches: 0.0247
trigger times: 1
Loss after 14769733 batches: 0.0200
trigger times: 2
Loss after 14770696 batches: 0.0183
trigger times: 3
Loss after 14771659 batches: 0.0171
trigger times: 4
Loss after 14772622 batches: 0.0159
trigger times: 5
Loss after 14773585 batches: 0.0153
trigger times: 6
Loss after 14774548 batches: 0.0152
trigger times: 7
Loss after 14775511 batches: 0.0149
trigger times: 8
Loss after 14776474 batches: 0.0141
trigger times: 9
Loss after 14777437 batches: 0.0138
trigger times: 10
Loss after 14778400 batches: 0.0130
trigger times: 11
Loss after 14779363 batches: 0.0133
trigger times: 12
Loss after 14780326 batches: 0.0135
trigger times: 13
Loss after 14781289 batches: 0.0137
trigger times: 14
Loss after 14782252 batches: 0.0137
trigger times: 15
Loss after 14783215 batches: 0.0139
trigger times: 16
Loss after 14784178 batches: 0.0128
trigger times: 17
Loss after 14785141 batches: 0.0126
trigger times: 18
Loss after 14786104 batches: 0.0124
trigger times: 19
Loss after 14787067 batches: 0.0125
trigger times: 20
Loss after 14788030 batches: 0.0122
trigger times: 21
Loss after 14788993 batches: 0.0120
trigger times: 22
Loss after 14789956 batches: 0.0115
trigger times: 23
Loss after 14790919 batches: 0.0112
trigger times: 24
Loss after 14791882 batches: 0.0116
trigger times: 25
Early stopping!
Start to test process.
Loss after 14792845 batches: 0.0116
Time to train on one home:  55.349278926849365
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14793808 batches: 0.0874
trigger times: 1
Loss after 14794771 batches: 0.0278
trigger times: 2
Loss after 14795734 batches: 0.0203
trigger times: 3
Loss after 14796697 batches: 0.0183
trigger times: 4
Loss after 14797660 batches: 0.0178
trigger times: 5
Loss after 14798623 batches: 0.0158
trigger times: 6
Loss after 14799586 batches: 0.0149
trigger times: 7
Loss after 14800549 batches: 0.0145
trigger times: 8
Loss after 14801512 batches: 0.0138
trigger times: 9
Loss after 14802475 batches: 0.0139
trigger times: 10
Loss after 14803438 batches: 0.0134
trigger times: 11
Loss after 14804401 batches: 0.0132
trigger times: 12
Loss after 14805364 batches: 0.0129
trigger times: 13
Loss after 14806327 batches: 0.0128
trigger times: 14
Loss after 14807290 batches: 0.0125
trigger times: 15
Loss after 14808253 batches: 0.0128
trigger times: 16
Loss after 14809216 batches: 0.0126
trigger times: 17
Loss after 14810179 batches: 0.0125
trigger times: 18
Loss after 14811142 batches: 0.0122
trigger times: 19
Loss after 14812105 batches: 0.0124
trigger times: 20
Loss after 14813068 batches: 0.0120
trigger times: 21
Loss after 14814031 batches: 0.0124
trigger times: 22
Loss after 14814994 batches: 0.0121
trigger times: 23
Loss after 14815957 batches: 0.0121
trigger times: 24
Loss after 14816920 batches: 0.0121
trigger times: 25
Early stopping!
Start to test process.
Loss after 14817883 batches: 0.0123
Time to train on one home:  56.811792850494385
trigger times: 0
Loss after 14818846 batches: 0.0949
trigger times: 1
Loss after 14819809 batches: 0.0875
trigger times: 0
Loss after 14820772 batches: 0.0823
trigger times: 1
Loss after 14821735 batches: 0.0796
trigger times: 2
Loss after 14822698 batches: 0.0765
trigger times: 3
Loss after 14823661 batches: 0.0746
trigger times: 4
Loss after 14824624 batches: 0.0727
trigger times: 5
Loss after 14825587 batches: 0.0697
trigger times: 6
Loss after 14826550 batches: 0.0688
trigger times: 7
Loss after 14827513 batches: 0.0679
trigger times: 8
Loss after 14828476 batches: 0.0661
trigger times: 9
Loss after 14829439 batches: 0.0669
trigger times: 10
Loss after 14830402 batches: 0.0660
trigger times: 11
Loss after 14831365 batches: 0.0652
trigger times: 12
Loss after 14832328 batches: 0.0638
trigger times: 13
Loss after 14833291 batches: 0.0625
trigger times: 14
Loss after 14834254 batches: 0.0631
trigger times: 15
Loss after 14835217 batches: 0.0600
trigger times: 16
Loss after 14836180 batches: 0.0602
trigger times: 17
Loss after 14837143 batches: 0.0595
trigger times: 18
Loss after 14838106 batches: 0.0599
trigger times: 19
Loss after 14839069 batches: 0.0628
trigger times: 20
Loss after 14840032 batches: 0.0623
trigger times: 21
Loss after 14840995 batches: 0.0609
trigger times: 22
Loss after 14841958 batches: 0.0583
trigger times: 23
Loss after 14842921 batches: 0.0571
trigger times: 24
Loss after 14843884 batches: 0.0583
trigger times: 25
Early stopping!
Start to test process.
Loss after 14844847 batches: 0.0557
Time to train on one home:  55.32026529312134
trigger times: 0
Loss after 14845810 batches: 0.0586
trigger times: 1
Loss after 14846773 batches: 0.0443
trigger times: 0
Loss after 14847736 batches: 0.0355
trigger times: 1
Loss after 14848699 batches: 0.0312
trigger times: 2
Loss after 14849662 batches: 0.0281
trigger times: 3
Loss after 14850625 batches: 0.0273
trigger times: 4
Loss after 14851588 batches: 0.0247
trigger times: 5
Loss after 14852551 batches: 0.0244
trigger times: 6
Loss after 14853514 batches: 0.0237
trigger times: 7
Loss after 14854477 batches: 0.0225
trigger times: 8
Loss after 14855440 batches: 0.0218
trigger times: 9
Loss after 14856403 batches: 0.0210
trigger times: 10
Loss after 14857366 batches: 0.0205
trigger times: 11
Loss after 14858329 batches: 0.0213
trigger times: 12
Loss after 14859292 batches: 0.0207
trigger times: 13
Loss after 14860255 batches: 0.0205
trigger times: 14
Loss after 14861218 batches: 0.0203
trigger times: 15
Loss after 14862181 batches: 0.0206
trigger times: 16
Loss after 14863144 batches: 0.0211
trigger times: 17
Loss after 14864107 batches: 0.0206
trigger times: 18
Loss after 14865070 batches: 0.0204
trigger times: 19
Loss after 14866033 batches: 0.0191
trigger times: 20
Loss after 14866996 batches: 0.0200
trigger times: 21
Loss after 14867959 batches: 0.0197
trigger times: 22
Loss after 14868922 batches: 0.0186
trigger times: 23
Loss after 14869885 batches: 0.0195
trigger times: 24
Loss after 14870848 batches: 0.0186
trigger times: 25
Early stopping!
Start to test process.
Loss after 14871811 batches: 0.0178
Time to train on one home:  63.25887489318848
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14872774 batches: 0.0848
trigger times: 1
Loss after 14873737 batches: 0.0729
trigger times: 2
Loss after 14874700 batches: 0.0694
trigger times: 3
Loss after 14875663 batches: 0.0667
trigger times: 4
Loss after 14876626 batches: 0.0641
trigger times: 5
Loss after 14877589 batches: 0.0624
trigger times: 6
Loss after 14878552 batches: 0.0615
trigger times: 7
Loss after 14879515 batches: 0.0593
trigger times: 8
Loss after 14880478 batches: 0.0585
trigger times: 9
Loss after 14881441 batches: 0.0571
trigger times: 10
Loss after 14882404 batches: 0.0585
trigger times: 11
Loss after 14883367 batches: 0.0579
trigger times: 12
Loss after 14884330 batches: 0.0559
trigger times: 13
Loss after 14885293 batches: 0.0559
trigger times: 14
Loss after 14886256 batches: 0.0554
trigger times: 15
Loss after 14887219 batches: 0.0556
trigger times: 16
Loss after 14888182 batches: 0.0531
trigger times: 17
Loss after 14889145 batches: 0.0529
trigger times: 18
Loss after 14890108 batches: 0.0535
trigger times: 19
Loss after 14891071 batches: 0.0526
trigger times: 20
Loss after 14892034 batches: 0.0521
trigger times: 21
Loss after 14892997 batches: 0.0536
trigger times: 22
Loss after 14893960 batches: 0.0533
trigger times: 23
Loss after 14894923 batches: 0.0525
trigger times: 24
Loss after 14895886 batches: 0.0525
trigger times: 25
Early stopping!
Start to test process.
Loss after 14896849 batches: 0.0535
Time to train on one home:  56.59088611602783
trigger times: 0
Loss after 14897812 batches: 0.0898
trigger times: 0
Loss after 14898775 batches: 0.0615
trigger times: 0
Loss after 14899738 batches: 0.0557
trigger times: 1
Loss after 14900701 batches: 0.0502
trigger times: 2
Loss after 14901664 batches: 0.0479
trigger times: 3
Loss after 14902627 batches: 0.0478
trigger times: 4
Loss after 14903590 batches: 0.0454
trigger times: 5
Loss after 14904553 batches: 0.0432
trigger times: 6
Loss after 14905516 batches: 0.0432
trigger times: 7
Loss after 14906479 batches: 0.0416
trigger times: 8
Loss after 14907442 batches: 0.0418
trigger times: 9
Loss after 14908405 batches: 0.0406
trigger times: 10
Loss after 14909368 batches: 0.0411
trigger times: 11
Loss after 14910331 batches: 0.0404
trigger times: 12
Loss after 14911294 batches: 0.0391
trigger times: 13
Loss after 14912257 batches: 0.0395
trigger times: 14
Loss after 14913220 batches: 0.0400
trigger times: 15
Loss after 14914183 batches: 0.0391
trigger times: 16
Loss after 14915146 batches: 0.0403
trigger times: 17
Loss after 14916109 batches: 0.0403
trigger times: 18
Loss after 14917072 batches: 0.0398
trigger times: 19
Loss after 14918035 batches: 0.0388
trigger times: 20
Loss after 14918998 batches: 0.0386
trigger times: 21
Loss after 14919961 batches: 0.0381
trigger times: 22
Loss after 14920924 batches: 0.0381
trigger times: 23
Loss after 14921887 batches: 0.0372
trigger times: 24
Loss after 14922850 batches: 0.0369
trigger times: 25
Early stopping!
Start to test process.
Loss after 14923813 batches: 0.0360
Time to train on one home:  58.154449701309204
trigger times: 0
Loss after 14924776 batches: 0.0742
trigger times: 1
Loss after 14925739 batches: 0.0673
trigger times: 2
Loss after 14926702 batches: 0.0664
trigger times: 3
Loss after 14927665 batches: 0.0630
trigger times: 4
Loss after 14928628 batches: 0.0615
trigger times: 5
Loss after 14929591 batches: 0.0601
trigger times: 6
Loss after 14930554 batches: 0.0575
trigger times: 7
Loss after 14931517 batches: 0.0579
trigger times: 8
Loss after 14932480 batches: 0.0564
trigger times: 9
Loss after 14933443 batches: 0.0557
trigger times: 10
Loss after 14934406 batches: 0.0547
trigger times: 11
Loss after 14935369 batches: 0.0561
trigger times: 12
Loss after 14936332 batches: 0.0538
trigger times: 13
Loss after 14937295 batches: 0.0539
trigger times: 14
Loss after 14938258 batches: 0.0539
trigger times: 15
Loss after 14939221 batches: 0.0540
trigger times: 16
Loss after 14940184 batches: 0.0531
trigger times: 17
Loss after 14941147 batches: 0.0523
trigger times: 18
Loss after 14942110 batches: 0.0515
trigger times: 19
Loss after 14943073 batches: 0.0507
trigger times: 20
Loss after 14944036 batches: 0.0508
trigger times: 21
Loss after 14944999 batches: 0.0505
trigger times: 22
Loss after 14945962 batches: 0.0515
trigger times: 23
Loss after 14946925 batches: 0.0510
trigger times: 24
Loss after 14947888 batches: 0.0518
trigger times: 25
Early stopping!
Start to test process.
Loss after 14948851 batches: 0.0532
Time to train on one home:  57.403602838516235
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14949814 batches: 0.0639
trigger times: 1
Loss after 14950777 batches: 0.0490
trigger times: 2
Loss after 14951740 batches: 0.0462
trigger times: 3
Loss after 14952703 batches: 0.0412
trigger times: 4
Loss after 14953666 batches: 0.0364
trigger times: 5
Loss after 14954629 batches: 0.0339
trigger times: 6
Loss after 14955592 batches: 0.0320
trigger times: 7
Loss after 14956555 batches: 0.0309
trigger times: 8
Loss after 14957518 batches: 0.0298
trigger times: 9
Loss after 14958481 batches: 0.0295
trigger times: 10
Loss after 14959444 batches: 0.0280
trigger times: 11
Loss after 14960407 batches: 0.0280
trigger times: 12
Loss after 14961370 batches: 0.0272
trigger times: 13
Loss after 14962333 batches: 0.0269
trigger times: 14
Loss after 14963296 batches: 0.0263
trigger times: 15
Loss after 14964259 batches: 0.0268
trigger times: 16
Loss after 14965222 batches: 0.0257
trigger times: 17
Loss after 14966185 batches: 0.0253
trigger times: 18
Loss after 14967148 batches: 0.0247
trigger times: 19
Loss after 14968111 batches: 0.0240
trigger times: 20
Loss after 14969074 batches: 0.0236
trigger times: 21
Loss after 14970037 batches: 0.0241
trigger times: 22
Loss after 14971000 batches: 0.0235
trigger times: 23
Loss after 14971963 batches: 0.0229
trigger times: 24
Loss after 14972926 batches: 0.0236
trigger times: 25
Early stopping!
Start to test process.
Loss after 14973889 batches: 0.0231
Time to train on one home:  56.952776193618774
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14974852 batches: 0.0769
trigger times: 0
Loss after 14975815 batches: 0.0512
trigger times: 1
Loss after 14976778 batches: 0.0412
trigger times: 2
Loss after 14977741 batches: 0.0399
trigger times: 0
Loss after 14978704 batches: 0.0410
trigger times: 1
Loss after 14979667 batches: 0.0384
trigger times: 2
Loss after 14980630 batches: 0.0324
trigger times: 3
Loss after 14981593 batches: 0.0293
trigger times: 4
Loss after 14982556 batches: 0.0281
trigger times: 5
Loss after 14983519 batches: 0.0253
trigger times: 6
Loss after 14984482 batches: 0.0253
trigger times: 7
Loss after 14985445 batches: 0.0234
trigger times: 8
Loss after 14986408 batches: 0.0229
trigger times: 9
Loss after 14987371 batches: 0.0234
trigger times: 10
Loss after 14988334 batches: 0.0215
trigger times: 11
Loss after 14989297 batches: 0.0216
trigger times: 12
Loss after 14990260 batches: 0.0207
trigger times: 13
Loss after 14991223 batches: 0.0209
trigger times: 14
Loss after 14992186 batches: 0.0199
trigger times: 15
Loss after 14993149 batches: 0.0223
trigger times: 16
Loss after 14994112 batches: 0.0205
trigger times: 17
Loss after 14995075 batches: 0.0200
trigger times: 18
Loss after 14996038 batches: 0.0229
trigger times: 19
Loss after 14997001 batches: 0.0212
trigger times: 20
Loss after 14997964 batches: 0.0208
trigger times: 21
Loss after 14998927 batches: 0.0199
trigger times: 22
Loss after 14999890 batches: 0.0194
trigger times: 23
Loss after 15000853 batches: 0.0196
trigger times: 24
Loss after 15001816 batches: 0.0181
trigger times: 25
Early stopping!
Start to test process.
Loss after 15002779 batches: 0.0183
Time to train on one home:  57.0427782535553
trigger times: 0
Loss after 15003708 batches: 0.0932
trigger times: 0
Loss after 15004637 batches: 0.0615
trigger times: 0
Loss after 15005566 batches: 0.0429
trigger times: 1
Loss after 15006495 batches: 0.0394
trigger times: 2
Loss after 15007424 batches: 0.0347
trigger times: 3
Loss after 15008353 batches: 0.0348
trigger times: 4
Loss after 15009282 batches: 0.0338
trigger times: 5
Loss after 15010211 batches: 0.0372
trigger times: 6
Loss after 15011140 batches: 0.0368
trigger times: 7
Loss after 15012069 batches: 0.0362
trigger times: 8
Loss after 15012998 batches: 0.0357
trigger times: 0
Loss after 15013927 batches: 0.0346
trigger times: 1
Loss after 15014856 batches: 0.0295
trigger times: 2
Loss after 15015785 batches: 0.0270
trigger times: 3
Loss after 15016714 batches: 0.0303
trigger times: 4
Loss after 15017643 batches: 0.0273
trigger times: 5
Loss after 15018572 batches: 0.0282
trigger times: 6
Loss after 15019501 batches: 0.0271
trigger times: 7
Loss after 15020430 batches: 0.0251
trigger times: 8
Loss after 15021359 batches: 0.0264
trigger times: 9
Loss after 15022288 batches: 0.0253
trigger times: 10
Loss after 15023217 batches: 0.0266
trigger times: 11
Loss after 15024146 batches: 0.0261
trigger times: 12
Loss after 15025075 batches: 0.0266
trigger times: 13
Loss after 15026004 batches: 0.0246
trigger times: 14
Loss after 15026933 batches: 0.0258
trigger times: 15
Loss after 15027862 batches: 0.0235
trigger times: 16
Loss after 15028791 batches: 0.0235
trigger times: 17
Loss after 15029720 batches: 0.0234
trigger times: 18
Loss after 15030649 batches: 0.0239
trigger times: 19
Loss after 15031578 batches: 0.0224
trigger times: 20
Loss after 15032507 batches: 0.0226
trigger times: 21
Loss after 15033436 batches: 0.0229
trigger times: 22
Loss after 15034365 batches: 0.0258
trigger times: 23
Loss after 15035294 batches: 0.0245
trigger times: 24
Loss after 15036223 batches: 0.0246
trigger times: 25
Early stopping!
Start to test process.
Loss after 15037152 batches: 0.0219
Time to train on one home:  63.50082850456238
trigger times: 0
Loss after 15038114 batches: 0.0774
trigger times: 1
Loss after 15039076 batches: 0.0658
trigger times: 2
Loss after 15040038 batches: 0.0646
trigger times: 3
Loss after 15041000 batches: 0.0608
trigger times: 4
Loss after 15041962 batches: 0.0584
trigger times: 5
Loss after 15042924 batches: 0.0567
trigger times: 6
Loss after 15043886 batches: 0.0549
trigger times: 7
Loss after 15044848 batches: 0.0556
trigger times: 8
Loss after 15045810 batches: 0.0534
trigger times: 9
Loss after 15046772 batches: 0.0532
trigger times: 10
Loss after 15047734 batches: 0.0531
trigger times: 11
Loss after 15048696 batches: 0.0514
trigger times: 12
Loss after 15049658 batches: 0.0528
trigger times: 13
Loss after 15050620 batches: 0.0530
trigger times: 14
Loss after 15051582 batches: 0.0515
trigger times: 15
Loss after 15052544 batches: 0.0519
trigger times: 16
Loss after 15053506 batches: 0.0512
trigger times: 17
Loss after 15054468 batches: 0.0515
trigger times: 18
Loss after 15055430 batches: 0.0510
trigger times: 19
Loss after 15056392 batches: 0.0511
trigger times: 20
Loss after 15057354 batches: 0.0505
trigger times: 21
Loss after 15058316 batches: 0.0515
trigger times: 22
Loss after 15059278 batches: 0.0497
trigger times: 23
Loss after 15060240 batches: 0.0501
trigger times: 24
Loss after 15061202 batches: 0.0490
trigger times: 25
Early stopping!
Start to test process.
Loss after 15062164 batches: 0.0482
Time to train on one home:  57.338623046875
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15063127 batches: 0.0534
trigger times: 1
Loss after 15064090 batches: 0.0438
trigger times: 2
Loss after 15065053 batches: 0.0434
trigger times: 3
Loss after 15066016 batches: 0.0409
trigger times: 4
Loss after 15066979 batches: 0.0380
trigger times: 5
Loss after 15067942 batches: 0.0366
trigger times: 6
Loss after 15068905 batches: 0.0359
trigger times: 7
Loss after 15069868 batches: 0.0342
trigger times: 8
Loss after 15070831 batches: 0.0336
trigger times: 9
Loss after 15071794 batches: 0.0329
trigger times: 10
Loss after 15072757 batches: 0.0323
trigger times: 11
Loss after 15073720 batches: 0.0324
trigger times: 12
Loss after 15074683 batches: 0.0317
trigger times: 13
Loss after 15075646 batches: 0.0313
trigger times: 14
Loss after 15076609 batches: 0.0316
trigger times: 15
Loss after 15077572 batches: 0.0309
trigger times: 16
Loss after 15078535 batches: 0.0312
trigger times: 17
Loss after 15079498 batches: 0.0309
trigger times: 18
Loss after 15080461 batches: 0.0304
trigger times: 19
Loss after 15081424 batches: 0.0302
trigger times: 20
Loss after 15082387 batches: 0.0304
trigger times: 21
Loss after 15083350 batches: 0.0298
trigger times: 22
Loss after 15084313 batches: 0.0297
trigger times: 23
Loss after 15085276 batches: 0.0293
trigger times: 24
Loss after 15086239 batches: 0.0289
trigger times: 25
Early stopping!
Start to test process.
Loss after 15087202 batches: 0.0286
Time to train on one home:  53.64777588844299
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15088165 batches: 0.0482
trigger times: 1
Loss after 15089128 batches: 0.0435
trigger times: 0
Loss after 15090091 batches: 0.0408
trigger times: 1
Loss after 15091054 batches: 0.0390
trigger times: 2
Loss after 15092017 batches: 0.0370
trigger times: 3
Loss after 15092980 batches: 0.0356
trigger times: 4
Loss after 15093943 batches: 0.0351
trigger times: 5
Loss after 15094906 batches: 0.0332
trigger times: 6
Loss after 15095869 batches: 0.0328
trigger times: 7
Loss after 15096832 batches: 0.0317
trigger times: 8
Loss after 15097795 batches: 0.0317
trigger times: 9
Loss after 15098758 batches: 0.0303
trigger times: 10
Loss after 15099721 batches: 0.0300
trigger times: 11
Loss after 15100684 batches: 0.0300
trigger times: 12
Loss after 15101647 batches: 0.0303
trigger times: 13
Loss after 15102610 batches: 0.0294
trigger times: 14
Loss after 15103573 batches: 0.0303
trigger times: 15
Loss after 15104536 batches: 0.0301
trigger times: 16
Loss after 15105499 batches: 0.0298
trigger times: 17
Loss after 15106462 batches: 0.0287
trigger times: 18
Loss after 15107425 batches: 0.0290
trigger times: 19
Loss after 15108388 batches: 0.0285
trigger times: 20
Loss after 15109351 batches: 0.0285
trigger times: 21
Loss after 15110314 batches: 0.0274
trigger times: 22
Loss after 15111277 batches: 0.0269
trigger times: 23
Loss after 15112240 batches: 0.0279
trigger times: 24
Loss after 15113203 batches: 0.0274
trigger times: 25
Early stopping!
Start to test process.
Loss after 15114166 batches: 0.0277
Time to train on one home:  62.42515444755554
trigger times: 0
Loss after 15115129 batches: 0.0984
trigger times: 1
Loss after 15116092 batches: 0.0913
trigger times: 2
Loss after 15117055 batches: 0.0868
trigger times: 3
Loss after 15118018 batches: 0.0827
trigger times: 4
Loss after 15118981 batches: 0.0822
trigger times: 5
Loss after 15119944 batches: 0.0773
trigger times: 6
Loss after 15120907 batches: 0.0771
trigger times: 7
Loss after 15121870 batches: 0.0762
trigger times: 8
Loss after 15122833 batches: 0.0757
trigger times: 9
Loss after 15123796 batches: 0.0751
trigger times: 10
Loss after 15124759 batches: 0.0731
trigger times: 11
Loss after 15125722 batches: 0.0707
trigger times: 12
Loss after 15126685 batches: 0.0711
trigger times: 13
Loss after 15127648 batches: 0.0732
trigger times: 14
Loss after 15128611 batches: 0.0696
trigger times: 15
Loss after 15129574 batches: 0.0711
trigger times: 16
Loss after 15130537 batches: 0.0690
trigger times: 17
Loss after 15131500 batches: 0.0681
trigger times: 18
Loss after 15132463 batches: 0.0663
trigger times: 19
Loss after 15133426 batches: 0.0653
trigger times: 20
Loss after 15134389 batches: 0.0654
trigger times: 21
Loss after 15135352 batches: 0.0652
trigger times: 22
Loss after 15136315 batches: 0.0654
trigger times: 23
Loss after 15137278 batches: 0.0663
trigger times: 24
Loss after 15138241 batches: 0.0646
trigger times: 25
Early stopping!
Start to test process.
Loss after 15139204 batches: 0.0617
Time to train on one home:  56.40794515609741
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15140167 batches: 0.0913
trigger times: 1
Loss after 15141130 batches: 0.0553
trigger times: 2
Loss after 15142093 batches: 0.0521
trigger times: 3
Loss after 15143056 batches: 0.0458
trigger times: 4
Loss after 15144019 batches: 0.0408
trigger times: 5
Loss after 15144982 batches: 0.0389
trigger times: 6
Loss after 15145945 batches: 0.0367
trigger times: 7
Loss after 15146908 batches: 0.0350
trigger times: 8
Loss after 15147871 batches: 0.0340
trigger times: 9
Loss after 15148834 batches: 0.0329
trigger times: 10
Loss after 15149797 batches: 0.0318
trigger times: 11
Loss after 15150760 batches: 0.0311
trigger times: 12
Loss after 15151723 batches: 0.0316
trigger times: 13
Loss after 15152686 batches: 0.0313
trigger times: 14
Loss after 15153649 batches: 0.0294
trigger times: 15
Loss after 15154612 batches: 0.0296
trigger times: 16
Loss after 15155575 batches: 0.0283
trigger times: 17
Loss after 15156538 batches: 0.0287
trigger times: 18
Loss after 15157501 batches: 0.0280
trigger times: 19
Loss after 15158464 batches: 0.0278
trigger times: 20
Loss after 15159427 batches: 0.0261
trigger times: 21
Loss after 15160390 batches: 0.0276
trigger times: 22
Loss after 15161353 batches: 0.0270
trigger times: 23
Loss after 15162316 batches: 0.0257
trigger times: 24
Loss after 15163279 batches: 0.0271
trigger times: 25
Early stopping!
Start to test process.
Loss after 15164242 batches: 0.0266
Time to train on one home:  56.57990503311157
trigger times: 0
Loss after 15165201 batches: 0.0898
trigger times: 1
Loss after 15166160 batches: 0.0474
trigger times: 0
Loss after 15167119 batches: 0.0350
trigger times: 1
Loss after 15168078 batches: 0.0285
trigger times: 2
Loss after 15169037 batches: 0.0268
trigger times: 0
Loss after 15169996 batches: 0.0243
trigger times: 1
Loss after 15170955 batches: 0.0218
trigger times: 2
Loss after 15171914 batches: 0.0209
trigger times: 3
Loss after 15172873 batches: 0.0207
trigger times: 4
Loss after 15173832 batches: 0.0190
trigger times: 5
Loss after 15174791 batches: 0.0189
trigger times: 0
Loss after 15175750 batches: 0.0192
trigger times: 1
Loss after 15176709 batches: 0.0179
trigger times: 2
Loss after 15177668 batches: 0.0182
trigger times: 3
Loss after 15178627 batches: 0.0182
trigger times: 0
Loss after 15179586 batches: 0.0170
trigger times: 1
Loss after 15180545 batches: 0.0170
trigger times: 2
Loss after 15181504 batches: 0.0172
trigger times: 3
Loss after 15182463 batches: 0.0165
trigger times: 4
Loss after 15183422 batches: 0.0173
trigger times: 5
Loss after 15184381 batches: 0.0167
trigger times: 6
Loss after 15185340 batches: 0.0164
trigger times: 7
Loss after 15186299 batches: 0.0164
trigger times: 8
Loss after 15187258 batches: 0.0162
trigger times: 9
Loss after 15188217 batches: 0.0156
trigger times: 10
Loss after 15189176 batches: 0.0146
trigger times: 11
Loss after 15190135 batches: 0.0147
trigger times: 12
Loss after 15191094 batches: 0.0150
trigger times: 13
Loss after 15192053 batches: 0.0149
trigger times: 14
Loss after 15193012 batches: 0.0146
trigger times: 15
Loss after 15193971 batches: 0.0144
trigger times: 16
Loss after 15194930 batches: 0.0133
trigger times: 17
Loss after 15195889 batches: 0.0139
trigger times: 18
Loss after 15196848 batches: 0.0139
trigger times: 19
Loss after 15197807 batches: 0.0141
trigger times: 20
Loss after 15198766 batches: 0.0145
trigger times: 21
Loss after 15199725 batches: 0.0136
trigger times: 22
Loss after 15200684 batches: 0.0136
trigger times: 23
Loss after 15201643 batches: 0.0126
trigger times: 24
Loss after 15202602 batches: 0.0132
trigger times: 25
Early stopping!
Start to test process.
Loss after 15203561 batches: 0.0138
Time to train on one home:  69.79534482955933
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15204524 batches: 0.0687
trigger times: 1
Loss after 15205487 batches: 0.0311
trigger times: 2
Loss after 15206450 batches: 0.0264
trigger times: 3
Loss after 15207413 batches: 0.0266
trigger times: 4
Loss after 15208376 batches: 0.0258
trigger times: 5
Loss after 15209339 batches: 0.0247
trigger times: 6
Loss after 15210302 batches: 0.0239
trigger times: 7
Loss after 15211265 batches: 0.0230
trigger times: 8
Loss after 15212228 batches: 0.0221
trigger times: 9
Loss after 15213191 batches: 0.0213
trigger times: 10
Loss after 15214154 batches: 0.0206
trigger times: 11
Loss after 15215117 batches: 0.0200
trigger times: 12
Loss after 15216080 batches: 0.0199
trigger times: 13
Loss after 15217043 batches: 0.0193
trigger times: 14
Loss after 15218006 batches: 0.0189
trigger times: 15
Loss after 15218969 batches: 0.0191
trigger times: 16
Loss after 15219932 batches: 0.0188
trigger times: 17
Loss after 15220895 batches: 0.0183
trigger times: 18
Loss after 15221858 batches: 0.0180
trigger times: 19
Loss after 15222821 batches: 0.0181
trigger times: 20
Loss after 15223784 batches: 0.0178
trigger times: 21
Loss after 15224747 batches: 0.0178
trigger times: 22
Loss after 15225710 batches: 0.0176
trigger times: 23
Loss after 15226673 batches: 0.0174
trigger times: 24
Loss after 15227636 batches: 0.0174
trigger times: 25
Early stopping!
Start to test process.
Loss after 15228599 batches: 0.0175
Time to train on one home:  57.090903520584106
trigger times: 0
Loss after 15229544 batches: 0.0658
trigger times: 0
Loss after 15230489 batches: 0.0481
trigger times: 0
Loss after 15231434 batches: 0.0353
trigger times: 1
Loss after 15232379 batches: 0.0314
trigger times: 2
Loss after 15233324 batches: 0.0295
trigger times: 3
Loss after 15234269 batches: 0.0275
trigger times: 4
Loss after 15235214 batches: 0.0253
trigger times: 5
Loss after 15236159 batches: 0.0253
trigger times: 6
Loss after 15237104 batches: 0.0248
trigger times: 7
Loss after 15238049 batches: 0.0245
trigger times: 8
Loss after 15238994 batches: 0.0241
trigger times: 9
Loss after 15239939 batches: 0.0234
trigger times: 10
Loss after 15240884 batches: 0.0221
trigger times: 11
Loss after 15241829 batches: 0.0219
trigger times: 12
Loss after 15242774 batches: 0.0210
trigger times: 13
Loss after 15243719 batches: 0.0210
trigger times: 14
Loss after 15244664 batches: 0.0204
trigger times: 15
Loss after 15245609 batches: 0.0192
trigger times: 16
Loss after 15246554 batches: 0.0211
trigger times: 17
Loss after 15247499 batches: 0.0195
trigger times: 18
Loss after 15248444 batches: 0.0197
trigger times: 19
Loss after 15249389 batches: 0.0186
trigger times: 20
Loss after 15250334 batches: 0.0186
trigger times: 21
Loss after 15251279 batches: 0.0180
trigger times: 22
Loss after 15252224 batches: 0.0184
trigger times: 23
Loss after 15253169 batches: 0.0180
trigger times: 24
Loss after 15254114 batches: 0.0162
trigger times: 25
Early stopping!
Start to test process.
Loss after 15255059 batches: 0.0177
Time to train on one home:  55.03768301010132
trigger times: 0
Loss after 15255996 batches: 0.0829
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 15256933 batches: 0.0686
trigger times: 2
Loss after 15257870 batches: 0.0648
trigger times: 3
Loss after 15258807 batches: 0.0620
trigger times: 4
Loss after 15259744 batches: 0.0592
trigger times: 5
Loss after 15260681 batches: 0.0570
trigger times: 6
Loss after 15261618 batches: 0.0553
trigger times: 7
Loss after 15262555 batches: 0.0540
trigger times: 8
Loss after 15263492 batches: 0.0533
trigger times: 9
Loss after 15264429 batches: 0.0522
trigger times: 10
Loss after 15265366 batches: 0.0517
trigger times: 11
Loss after 15266303 batches: 0.0507
trigger times: 12
Loss after 15267240 batches: 0.0517
trigger times: 13
Loss after 15268177 batches: 0.0507
trigger times: 14
Loss after 15269114 batches: 0.0503
trigger times: 15
Loss after 15270051 batches: 0.0508
trigger times: 16
Loss after 15270988 batches: 0.0497
trigger times: 17
Loss after 15271925 batches: 0.0484
trigger times: 18
Loss after 15272862 batches: 0.0482
trigger times: 19
Loss after 15273799 batches: 0.0500
trigger times: 20
Loss after 15274736 batches: 0.0503
trigger times: 21
Loss after 15275673 batches: 0.0506
trigger times: 22
Loss after 15276610 batches: 0.0494
trigger times: 23
Loss after 15277547 batches: 0.0495
trigger times: 24
Loss after 15278484 batches: 0.0485
trigger times: 25
Early stopping!
Start to test process.
Loss after 15279421 batches: 0.0485
Time to train on one home:  54.52097773551941
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 15280384 batches: 0.0488
trigger times: 1
Loss after 15281347 batches: 0.0178
trigger times: 2
Loss after 15282310 batches: 0.0141
trigger times: 3
Loss after 15283273 batches: 0.0138
trigger times: 4
Loss after 15284236 batches: 0.0139
trigger times: 5
Loss after 15285199 batches: 0.0139
trigger times: 6
Loss after 15286162 batches: 0.0138
trigger times: 7
Loss after 15287125 batches: 0.0140
trigger times: 8
Loss after 15288088 batches: 0.0141
trigger times: 9
Loss after 15289051 batches: 0.0138
trigger times: 10
Loss after 15290014 batches: 0.0139
trigger times: 11
Loss after 15290977 batches: 0.0138
trigger times: 12
Loss after 15291940 batches: 0.0141
trigger times: 13
Loss after 15292903 batches: 0.0139
trigger times: 14
Loss after 15293866 batches: 0.0139
trigger times: 15
Loss after 15294829 batches: 0.0138
trigger times: 16
Loss after 15295792 batches: 0.0139
trigger times: 17
Loss after 15296755 batches: 0.0137
trigger times: 18
Loss after 15297718 batches: 0.0136
trigger times: 19
Loss after 15298681 batches: 0.0137
trigger times: 20
Loss after 15299644 batches: 0.0136
trigger times: 21
Loss after 15300607 batches: 0.0137
trigger times: 22
Loss after 15301570 batches: 0.0135
trigger times: 23
Loss after 15302533 batches: 0.0133
trigger times: 24
Loss after 15303496 batches: 0.0133
trigger times: 25
Early stopping!
Start to test process.
Loss after 15304459 batches: 0.0132
Time to train on one home:  57.00530958175659
trigger times: 0
Loss after 15305422 batches: 0.0934
trigger times: 1
Loss after 15306385 batches: 0.0774
trigger times: 2
Loss after 15307348 batches: 0.0706
trigger times: 3
Loss after 15308311 batches: 0.0670
trigger times: 4
Loss after 15309274 batches: 0.0635
trigger times: 5
Loss after 15310237 batches: 0.0611
trigger times: 6
Loss after 15311200 batches: 0.0597
trigger times: 7
Loss after 15312163 batches: 0.0591
trigger times: 8
Loss after 15313126 batches: 0.0583
trigger times: 9
Loss after 15314089 batches: 0.0583
trigger times: 10
Loss after 15315052 batches: 0.0565
trigger times: 11
Loss after 15316015 batches: 0.0566
trigger times: 12
Loss after 15316978 batches: 0.0567
trigger times: 13
Loss after 15317941 batches: 0.0572
trigger times: 14
Loss after 15318904 batches: 0.0548
trigger times: 15
Loss after 15319867 batches: 0.0547
trigger times: 16
Loss after 15320830 batches: 0.0537
trigger times: 17
Loss after 15321793 batches: 0.0536
trigger times: 18
Loss after 15322756 batches: 0.0528
trigger times: 19
Loss after 15323719 batches: 0.0527
trigger times: 20
Loss after 15324682 batches: 0.0535
trigger times: 21
Loss after 15325645 batches: 0.0522
trigger times: 22
Loss after 15326608 batches: 0.0529
trigger times: 23
Loss after 15327571 batches: 0.0530
trigger times: 24
Loss after 15328534 batches: 0.0519
trigger times: 25
Early stopping!
Start to test process.
Loss after 15329497 batches: 0.0532
Time to train on one home:  54.395171880722046
trigger times: 0
Loss after 15330460 batches: 0.0730
trigger times: 1
Loss after 15331423 batches: 0.0507
trigger times: 2
Loss after 15332386 batches: 0.0476
trigger times: 3
Loss after 15333349 batches: 0.0431
trigger times: 4
Loss after 15334312 batches: 0.0408
trigger times: 5
Loss after 15335275 batches: 0.0395
trigger times: 6
Loss after 15336238 batches: 0.0378
trigger times: 7
Loss after 15337201 batches: 0.0358
trigger times: 8
Loss after 15338164 batches: 0.0356
trigger times: 9
Loss after 15339127 batches: 0.0352
trigger times: 10
Loss after 15340090 batches: 0.0344
trigger times: 11
Loss after 15341053 batches: 0.0346
trigger times: 12
Loss after 15342016 batches: 0.0341
trigger times: 13
Loss after 15342979 batches: 0.0332
trigger times: 14
Loss after 15343942 batches: 0.0328
trigger times: 15
Loss after 15344905 batches: 0.0323
trigger times: 16
Loss after 15345868 batches: 0.0312
trigger times: 17
Loss after 15346831 batches: 0.0320
trigger times: 18
Loss after 15347794 batches: 0.0314
trigger times: 19
Loss after 15348757 batches: 0.0303
trigger times: 20
Loss after 15349720 batches: 0.0306
trigger times: 21
Loss after 15350683 batches: 0.0304
trigger times: 22
Loss after 15351646 batches: 0.0300
trigger times: 23
Loss after 15352609 batches: 0.0287
trigger times: 24
Loss after 15353572 batches: 0.0293
trigger times: 25
Early stopping!
Start to test process.
Loss after 15354535 batches: 0.0287
Time to train on one home:  58.75396704673767
trigger times: 0
Loss after 15355431 batches: 0.1040
trigger times: 1
Loss after 15356327 batches: 0.0917
trigger times: 2
Loss after 15357223 batches: 0.0854
trigger times: 3
Loss after 15358119 batches: 0.0806
trigger times: 4
Loss after 15359015 batches: 0.0732
trigger times: 5
Loss after 15359911 batches: 0.0702
trigger times: 6
Loss after 15360807 batches: 0.0661
trigger times: 7
Loss after 15361703 batches: 0.0644
trigger times: 8
Loss after 15362599 batches: 0.0612
trigger times: 9
Loss after 15363495 batches: 0.0600
trigger times: 10
Loss after 15364391 batches: 0.0585
trigger times: 11
Loss after 15365287 batches: 0.0578
trigger times: 12
Loss after 15366183 batches: 0.0588
trigger times: 13
Loss after 15367079 batches: 0.0591
trigger times: 14
Loss after 15367975 batches: 0.0588
trigger times: 15
Loss after 15368871 batches: 0.0580
trigger times: 16
Loss after 15369767 batches: 0.0564
trigger times: 17
Loss after 15370663 batches: 0.0592
trigger times: 18
Loss after 15371559 batches: 0.0594
trigger times: 19
Loss after 15372455 batches: 0.0576
trigger times: 20
Loss after 15373351 batches: 0.0543
trigger times: 21
Loss after 15374247 batches: 0.0551
trigger times: 22
Loss after 15375143 batches: 0.0557
trigger times: 23
Loss after 15376039 batches: 0.0557
trigger times: 24
Loss after 15376935 batches: 0.0542
trigger times: 25
Early stopping!
Start to test process.
Loss after 15377831 batches: 0.0568
Time to train on one home:  56.0637845993042
trigger times: 0
Loss after 15378794 batches: 0.1765
trigger times: 1
Loss after 15379757 batches: 0.1195
trigger times: 2
Loss after 15380720 batches: 0.0991
trigger times: 3
Loss after 15381683 batches: 0.0922
trigger times: 4
Loss after 15382646 batches: 0.0830
trigger times: 5
Loss after 15383609 batches: 0.0760
trigger times: 6
Loss after 15384572 batches: 0.0731
trigger times: 7
Loss after 15385535 batches: 0.0695
trigger times: 8
Loss after 15386498 batches: 0.0641
trigger times: 9
Loss after 15387461 batches: 0.0590
trigger times: 10
Loss after 15388424 batches: 0.0549
trigger times: 11
Loss after 15389387 batches: 0.0538
trigger times: 12
Loss after 15390350 batches: 0.0507
trigger times: 13
Loss after 15391313 batches: 0.0496
trigger times: 14
Loss after 15392276 batches: 0.0478
trigger times: 15
Loss after 15393239 batches: 0.0487
trigger times: 16
Loss after 15394202 batches: 0.0479
trigger times: 17
Loss after 15395165 batches: 0.0473
trigger times: 18
Loss after 15396128 batches: 0.0450
trigger times: 19
Loss after 15397091 batches: 0.0436
trigger times: 20
Loss after 15398054 batches: 0.0430
trigger times: 21
Loss after 15399017 batches: 0.0429
trigger times: 22
Loss after 15399980 batches: 0.0410
trigger times: 23
Loss after 15400943 batches: 0.0421
trigger times: 24
Loss after 15401906 batches: 0.0431
trigger times: 25
Early stopping!
Start to test process.
Loss after 15402869 batches: 0.0421
Time to train on one home:  56.85962104797363
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15403832 batches: 0.0870
trigger times: 1
Loss after 15404795 batches: 0.0752
trigger times: 2
Loss after 15405758 batches: 0.0746
trigger times: 3
Loss after 15406721 batches: 0.0703
trigger times: 4
Loss after 15407684 batches: 0.0654
trigger times: 5
Loss after 15408647 batches: 0.0607
trigger times: 6
Loss after 15409610 batches: 0.0582
trigger times: 7
Loss after 15410573 batches: 0.0563
trigger times: 8
Loss after 15411536 batches: 0.0536
trigger times: 9
Loss after 15412499 batches: 0.0541
trigger times: 10
Loss after 15413462 batches: 0.0513
trigger times: 11
Loss after 15414425 batches: 0.0514
trigger times: 12
Loss after 15415388 batches: 0.0501
trigger times: 13
Loss after 15416351 batches: 0.0484
trigger times: 14
Loss after 15417314 batches: 0.0477
trigger times: 15
Loss after 15418277 batches: 0.0477
trigger times: 16
Loss after 15419240 batches: 0.0493
trigger times: 17
Loss after 15420203 batches: 0.0502
trigger times: 18
Loss after 15421166 batches: 0.0503
trigger times: 19
Loss after 15422129 batches: 0.0478
trigger times: 20
Loss after 15423092 batches: 0.0518
trigger times: 21
Loss after 15424055 batches: 0.0590
trigger times: 22
Loss after 15425018 batches: 0.0560
trigger times: 23
Loss after 15425981 batches: 0.0549
trigger times: 24
Loss after 15426944 batches: 0.0531
trigger times: 25
Early stopping!
Start to test process.
Loss after 15427907 batches: 0.0530
Time to train on one home:  57.42548584938049
trigger times: 0
Loss after 15428870 batches: 0.0873
trigger times: 1
Loss after 15429833 batches: 0.0628
trigger times: 2
Loss after 15430796 batches: 0.0559
trigger times: 3
Loss after 15431759 batches: 0.0509
trigger times: 4
Loss after 15432722 batches: 0.0476
trigger times: 5
Loss after 15433685 batches: 0.0447
trigger times: 6
Loss after 15434648 batches: 0.0432
trigger times: 7
Loss after 15435611 batches: 0.0435
trigger times: 8
Loss after 15436574 batches: 0.0413
trigger times: 9
Loss after 15437537 batches: 0.0394
trigger times: 10
Loss after 15438500 batches: 0.0398
trigger times: 11
Loss after 15439463 batches: 0.0392
trigger times: 12
Loss after 15440426 batches: 0.0388
trigger times: 13
Loss after 15441389 batches: 0.0377
trigger times: 14
Loss after 15442352 batches: 0.0381
trigger times: 15
Loss after 15443315 batches: 0.0367
trigger times: 16
Loss after 15444278 batches: 0.0362
trigger times: 17
Loss after 15445241 batches: 0.0369
trigger times: 18
Loss after 15446204 batches: 0.0370
trigger times: 19
Loss after 15447167 batches: 0.0353
trigger times: 20
Loss after 15448130 batches: 0.0358
trigger times: 21
Loss after 15449093 batches: 0.0349
trigger times: 22
Loss after 15450056 batches: 0.0351
trigger times: 23
Loss after 15451019 batches: 0.0350
trigger times: 24
Loss after 15451982 batches: 0.0349
trigger times: 25
Early stopping!
Start to test process.
Loss after 15452945 batches: 0.0342
Time to train on one home:  57.59546375274658
trigger times: 0
Loss after 15453908 batches: 0.0427
trigger times: 1
Loss after 15454871 batches: 0.0341
trigger times: 2
Loss after 15455834 batches: 0.0314
trigger times: 3
Loss after 15456797 batches: 0.0292
trigger times: 4
Loss after 15457760 batches: 0.0273
trigger times: 5
Loss after 15458723 batches: 0.0254
trigger times: 6
Loss after 15459686 batches: 0.0242
trigger times: 7
Loss after 15460649 batches: 0.0238
trigger times: 8
Loss after 15461612 batches: 0.0235
trigger times: 9
Loss after 15462575 batches: 0.0230
trigger times: 10
Loss after 15463538 batches: 0.0237
trigger times: 11
Loss after 15464501 batches: 0.0228
trigger times: 12
Loss after 15465464 batches: 0.0222
trigger times: 13
Loss after 15466427 batches: 0.0219
trigger times: 14
Loss after 15467390 batches: 0.0214
trigger times: 15
Loss after 15468353 batches: 0.0212
trigger times: 16
Loss after 15469316 batches: 0.0210
trigger times: 17
Loss after 15470279 batches: 0.0212
trigger times: 18
Loss after 15471242 batches: 0.0210
trigger times: 19
Loss after 15472205 batches: 0.0206
trigger times: 20
Loss after 15473168 batches: 0.0200
trigger times: 21
Loss after 15474131 batches: 0.0200
trigger times: 22
Loss after 15475094 batches: 0.0209
trigger times: 23
Loss after 15476057 batches: 0.0200
trigger times: 24
Loss after 15477020 batches: 0.0209
trigger times: 25
Early stopping!
Start to test process.
Loss after 15477983 batches: 0.0200
Time to train on one home:  55.74405074119568
trigger times: 0
Loss after 15478946 batches: 0.0854
trigger times: 1
Loss after 15479909 batches: 0.0477
trigger times: 2
Loss after 15480872 batches: 0.0480
trigger times: 3
Loss after 15481835 batches: 0.0468
trigger times: 4
Loss after 15482798 batches: 0.0442
trigger times: 5
Loss after 15483761 batches: 0.0422
trigger times: 6
Loss after 15484724 batches: 0.0411
trigger times: 7
Loss after 15485687 batches: 0.0402
trigger times: 8
Loss after 15486650 batches: 0.0394
trigger times: 9
Loss after 15487613 batches: 0.0387
trigger times: 10
Loss after 15488576 batches: 0.0372
trigger times: 11
Loss after 15489539 batches: 0.0369
trigger times: 12
Loss after 15490502 batches: 0.0362
trigger times: 13
Loss after 15491465 batches: 0.0359
trigger times: 14
Loss after 15492428 batches: 0.0355
trigger times: 15
Loss after 15493391 batches: 0.0355
trigger times: 16
Loss after 15494354 batches: 0.0350
trigger times: 17
Loss after 15495317 batches: 0.0343
trigger times: 18
Loss after 15496280 batches: 0.0336
trigger times: 19
Loss after 15497243 batches: 0.0346
trigger times: 20
Loss after 15498206 batches: 0.0346
trigger times: 21
Loss after 15499169 batches: 0.0338
trigger times: 22
Loss after 15500132 batches: 0.0334
trigger times: 23
Loss after 15501095 batches: 0.0338
trigger times: 24
Loss after 15502058 batches: 0.0331
trigger times: 25
Early stopping!
Start to test process.
Loss after 15503021 batches: 0.0334
Time to train on one home:  52.93195676803589
trigger times: 0
Loss after 15503916 batches: 0.0645
trigger times: 1
Loss after 15504811 batches: 0.0320
trigger times: 2
Loss after 15505706 batches: 0.0132
trigger times: 0
Loss after 15506601 batches: 0.0087
trigger times: 1
Loss after 15507496 batches: 0.0058
trigger times: 2
Loss after 15508391 batches: 0.0050
trigger times: 0
Loss after 15509286 batches: 0.0043
trigger times: 1
Loss after 15510181 batches: 0.0038
trigger times: 2
Loss after 15511076 batches: 0.0035
trigger times: 0
Loss after 15511971 batches: 0.0028
trigger times: 1
Loss after 15512866 batches: 0.0028
trigger times: 2
Loss after 15513761 batches: 0.0024
trigger times: 3
Loss after 15514656 batches: 0.0039
trigger times: 0
Loss after 15515551 batches: 0.0038
trigger times: 1
Loss after 15516446 batches: 0.0056
trigger times: 2
Loss after 15517341 batches: 0.0054
trigger times: 3
Loss after 15518236 batches: 0.0038
trigger times: 4
Loss after 15519131 batches: 0.0031
trigger times: 5
Loss after 15520026 batches: 0.0029
trigger times: 6
Loss after 15520921 batches: 0.0032
trigger times: 0
Loss after 15521816 batches: 0.0028
trigger times: 1
Loss after 15522711 batches: 0.0022
trigger times: 2
Loss after 15523606 batches: 0.0023
trigger times: 3
Loss after 15524501 batches: 0.0025
trigger times: 4
Loss after 15525396 batches: 0.0026
trigger times: 5
Loss after 15526291 batches: 0.0018
trigger times: 6
Loss after 15527186 batches: 0.0021
trigger times: 7
Loss after 15528081 batches: 0.0020
trigger times: 8
Loss after 15528976 batches: 0.0019
trigger times: 9
Loss after 15529871 batches: 0.0031
trigger times: 10
Loss after 15530766 batches: 0.0050
trigger times: 11
Loss after 15531661 batches: 0.0050
trigger times: 12
Loss after 15532556 batches: 0.0052
trigger times: 13
Loss after 15533451 batches: 0.0043
trigger times: 14
Loss after 15534346 batches: 0.0038
trigger times: 15
Loss after 15535241 batches: 0.0035
trigger times: 16
Loss after 15536136 batches: 0.0034
trigger times: 17
Loss after 15537031 batches: 0.0025
trigger times: 18
Loss after 15537926 batches: 0.0023
trigger times: 19
Loss after 15538821 batches: 0.0027
trigger times: 20
Loss after 15539716 batches: 0.0031
trigger times: 21
Loss after 15540611 batches: 0.0028
trigger times: 22
Loss after 15541506 batches: 0.0032
trigger times: 23
Loss after 15542401 batches: 0.0025
trigger times: 24
Loss after 15543296 batches: 0.0026
trigger times: 25
Early stopping!
Start to test process.
Loss after 15544191 batches: 0.0028
Time to train on one home:  71.49926972389221
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336, 0.034888222413376094, 0.034613204319843105, 0.033508375671873286, 0.03325774057242148, 0.03243952582662181, 0.032562873532530595]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171], [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71], [0.13675299286842346, 0.15565351472302724, 0.32920502623266784, 1.059933101560365, 0.6549021202780799, 35.010519125969424, 3184.605], [0.0885479673743248, 0.1559912263273432, 0.33358461589199145, 0.944871497166479, 0.654640173496829, 31.209933508474723, 3183.3313], [0.1345348209142685, 0.15444619808844395, 0.3263722552493196, 1.0544752388510252, 0.6558385527073367, 34.830241138150235, 3189.1584], [0.09016867727041245, 0.15349941943138035, 0.3319629557497865, 0.9480173077546179, 0.65657289912243, 31.313842388762147, 3192.7297], [0.13274754583835602, 0.1683317466130475, 0.33338956387040825, 1.0413414780800958, 0.645068476656334, 34.39642151124207, 3136.7866]]
Round_13_results:  [0.13274754583835602, 0.1683317466130475, 0.33338956387040825, 1.0413414780800958, 0.645068476656334, 34.39642151124207, 3136.7866]
trigger times: 0
Loss after 15545154 batches: 0.0633
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 16291 < 16292; dropping {'Training_Loss': 0.06334352120757103, 'Validation_Loss': 0.07552003860473633, 'Training_R2': 0.3134876065148895, 'Validation_R2': 0.11008921313103937, 'Training_F1': 0.5092603449205633, 'Validation_F1': 0.30280607118204655, 'Training_NEP': 0.8111546335489561, 'Validation_NEP': 0.9176147511314401, 'Training_NDE': 0.505585091520286, 'Validation_NDE': 0.6905689083881538, 'Training_MAE': 23.35733500774193, 'Validation_MAE': 30.565961947195238, 'Training_MSE': 1590.6617, 'Validation_MSE': 3420.6638}.
trigger times: 0
Loss after 15546117 batches: 0.0456
trigger times: 0
Loss after 15547080 batches: 0.0434
trigger times: 1
Loss after 15548043 batches: 0.0396
trigger times: 2
Loss after 15549006 batches: 0.0348
trigger times: 3
Loss after 15549969 batches: 0.0337
trigger times: 4
Loss after 15550932 batches: 0.0321
trigger times: 5
Loss after 15551895 batches: 0.0320
trigger times: 6
Loss after 15552858 batches: 0.0315
trigger times: 7
Loss after 15553821 batches: 0.0303
trigger times: 8
Loss after 15554784 batches: 0.0289
trigger times: 9
Loss after 15555747 batches: 0.0289
trigger times: 10
Loss after 15556710 batches: 0.0308
trigger times: 11
Loss after 15557673 batches: 0.0293
trigger times: 12
Loss after 15558636 batches: 0.0282
trigger times: 13
Loss after 15559599 batches: 0.0297
trigger times: 14
Loss after 15560562 batches: 0.0292
trigger times: 15
Loss after 15561525 batches: 0.0277
trigger times: 16
Loss after 15562488 batches: 0.0267
trigger times: 17
Loss after 15563451 batches: 0.0273
trigger times: 18
Loss after 15564414 batches: 0.0273
trigger times: 19
Loss after 15565377 batches: 0.0261
trigger times: 20
Loss after 15566340 batches: 0.0274
trigger times: 21
Loss after 15567303 batches: 0.0276
trigger times: 22
Loss after 15568266 batches: 0.0256
trigger times: 23
Loss after 15569229 batches: 0.0266
trigger times: 24
Loss after 15570192 batches: 0.0270
trigger times: 25
Early stopping!
Start to test process.
Loss after 15571155 batches: 0.0259
Time to train on one home:  57.45457983016968
trigger times: 0
Loss after 15572113 batches: 0.0849
trigger times: 0
Loss after 15573071 batches: 0.0451
trigger times: 1
Loss after 15574029 batches: 0.0392
trigger times: 2
Loss after 15574987 batches: 0.0326
trigger times: 3
Loss after 15575945 batches: 0.0288
trigger times: 4
Loss after 15576903 batches: 0.0243
trigger times: 5
Loss after 15577861 batches: 0.0223
trigger times: 6
Loss after 15578819 batches: 0.0208
trigger times: 7
Loss after 15579777 batches: 0.0226
trigger times: 8
Loss after 15580735 batches: 0.0196
trigger times: 9
Loss after 15581693 batches: 0.0199
trigger times: 10
Loss after 15582651 batches: 0.0184
trigger times: 11
Loss after 15583609 batches: 0.0194
trigger times: 0
Loss after 15584567 batches: 0.0178
trigger times: 1
Loss after 15585525 batches: 0.0187
trigger times: 0
Loss after 15586483 batches: 0.0166
trigger times: 1
Loss after 15587441 batches: 0.0166
trigger times: 2
Loss after 15588399 batches: 0.0167
trigger times: 3
Loss after 15589357 batches: 0.0158
trigger times: 0
Loss after 15590315 batches: 0.0155
trigger times: 0
Loss after 15591273 batches: 0.0161
trigger times: 1
Loss after 15592231 batches: 0.0163
trigger times: 2
Loss after 15593189 batches: 0.0158
trigger times: 3
Loss after 15594147 batches: 0.0153
trigger times: 4
Loss after 15595105 batches: 0.0153
trigger times: 5
Loss after 15596063 batches: 0.0151
trigger times: 6
Loss after 15597021 batches: 0.0157
trigger times: 7
Loss after 15597979 batches: 0.0149
trigger times: 8
Loss after 15598937 batches: 0.0155
trigger times: 9
Loss after 15599895 batches: 0.0164
trigger times: 10
Loss after 15600853 batches: 0.0143
trigger times: 11
Loss after 15601811 batches: 0.0135
trigger times: 12
Loss after 15602769 batches: 0.0149
trigger times: 13
Loss after 15603727 batches: 0.0135
trigger times: 14
Loss after 15604685 batches: 0.0147
trigger times: 15
Loss after 15605643 batches: 0.0155
trigger times: 16
Loss after 15606601 batches: 0.0155
trigger times: 17
Loss after 15607559 batches: 0.0176
trigger times: 18
Loss after 15608517 batches: 0.0169
trigger times: 19
Loss after 15609475 batches: 0.0183
trigger times: 20
Loss after 15610433 batches: 0.0165
trigger times: 21
Loss after 15611391 batches: 0.0152
trigger times: 22
Loss after 15612349 batches: 0.0153
trigger times: 23
Loss after 15613307 batches: 0.0136
trigger times: 24
Loss after 15614265 batches: 0.0135
trigger times: 25
Early stopping!
Start to test process.
Loss after 15615223 batches: 0.0128
Time to train on one home:  73.71569585800171
trigger times: 0
Loss after 15616186 batches: 0.0851
trigger times: 1
Loss after 15617149 batches: 0.0699
trigger times: 2
Loss after 15618112 batches: 0.0668
trigger times: 3
Loss after 15619075 batches: 0.0656
trigger times: 4
Loss after 15620038 batches: 0.0639
trigger times: 5
Loss after 15621001 batches: 0.0622
trigger times: 6
Loss after 15621964 batches: 0.0587
trigger times: 7
Loss after 15622927 batches: 0.0564
trigger times: 8
Loss after 15623890 batches: 0.0546
trigger times: 9
Loss after 15624853 batches: 0.0525
trigger times: 10
Loss after 15625816 batches: 0.0503
trigger times: 11
Loss after 15626779 batches: 0.0505
trigger times: 12
Loss after 15627742 batches: 0.0493
trigger times: 13
Loss after 15628705 batches: 0.0488
trigger times: 14
Loss after 15629668 batches: 0.0489
trigger times: 15
Loss after 15630631 batches: 0.0479
trigger times: 16
Loss after 15631594 batches: 0.0464
trigger times: 17
Loss after 15632557 batches: 0.0458
trigger times: 18
Loss after 15633520 batches: 0.0464
trigger times: 19
Loss after 15634483 batches: 0.0469
trigger times: 20
Loss after 15635446 batches: 0.0454
trigger times: 21
Loss after 15636409 batches: 0.0448
trigger times: 22
Loss after 15637372 batches: 0.0439
trigger times: 23
Loss after 15638335 batches: 0.0442
trigger times: 24
Loss after 15639298 batches: 0.0436
trigger times: 25
Early stopping!
Start to test process.
Loss after 15640261 batches: 0.0432
Time to train on one home:  56.800798654556274
trigger times: 0
Loss after 15641224 batches: 0.0899
trigger times: 1
Loss after 15642187 batches: 0.0780
trigger times: 2
Loss after 15643150 batches: 0.0761
trigger times: 3
Loss after 15644113 batches: 0.0745
trigger times: 4
Loss after 15645076 batches: 0.0710
trigger times: 5
Loss after 15646039 batches: 0.0683
trigger times: 6
Loss after 15647002 batches: 0.0655
trigger times: 7
Loss after 15647965 batches: 0.0644
trigger times: 8
Loss after 15648928 batches: 0.0621
trigger times: 9
Loss after 15649891 batches: 0.0620
trigger times: 0
Loss after 15650854 batches: 0.0600
trigger times: 1
Loss after 15651817 batches: 0.0586
trigger times: 2
Loss after 15652780 batches: 0.0587
trigger times: 0
Loss after 15653743 batches: 0.0575
trigger times: 1
Loss after 15654706 batches: 0.0572
trigger times: 2
Loss after 15655669 batches: 0.0576
trigger times: 3
Loss after 15656632 batches: 0.0554
trigger times: 4
Loss after 15657595 batches: 0.0561
trigger times: 5
Loss after 15658558 batches: 0.0566
trigger times: 6
Loss after 15659521 batches: 0.0570
trigger times: 7
Loss after 15660484 batches: 0.0580
trigger times: 8
Loss after 15661447 batches: 0.0564
trigger times: 9
Loss after 15662410 batches: 0.0543
trigger times: 10
Loss after 15663373 batches: 0.0540
trigger times: 11
Loss after 15664336 batches: 0.0545
trigger times: 12
Loss after 15665299 batches: 0.0542
trigger times: 13
Loss after 15666262 batches: 0.0519
trigger times: 14
Loss after 15667225 batches: 0.0538
trigger times: 15
Loss after 15668188 batches: 0.0538
trigger times: 16
Loss after 15669151 batches: 0.0524
trigger times: 17
Loss after 15670114 batches: 0.0508
trigger times: 18
Loss after 15671077 batches: 0.0524
trigger times: 19
Loss after 15672040 batches: 0.0543
trigger times: 20
Loss after 15673003 batches: 0.0522
trigger times: 21
Loss after 15673966 batches: 0.0511
trigger times: 22
Loss after 15674929 batches: 0.0524
trigger times: 23
Loss after 15675892 batches: 0.0524
trigger times: 24
Loss after 15676855 batches: 0.0535
trigger times: 25
Early stopping!
Start to test process.
Loss after 15677818 batches: 0.0508
Time to train on one home:  66.74913287162781
trigger times: 0
Loss after 15678781 batches: 0.0268
trigger times: 1
Loss after 15679744 batches: 0.0214
trigger times: 2
Loss after 15680707 batches: 0.0197
trigger times: 3
Loss after 15681670 batches: 0.0181
trigger times: 0
Loss after 15682633 batches: 0.0163
trigger times: 1
Loss after 15683596 batches: 0.0158
trigger times: 2
Loss after 15684559 batches: 0.0148
trigger times: 3
Loss after 15685522 batches: 0.0145
trigger times: 0
Loss after 15686485 batches: 0.0143
trigger times: 1
Loss after 15687448 batches: 0.0148
trigger times: 2
Loss after 15688411 batches: 0.0140
trigger times: 3
Loss after 15689374 batches: 0.0141
trigger times: 4
Loss after 15690337 batches: 0.0133
trigger times: 5
Loss after 15691300 batches: 0.0127
trigger times: 6
Loss after 15692263 batches: 0.0125
trigger times: 7
Loss after 15693226 batches: 0.0126
trigger times: 8
Loss after 15694189 batches: 0.0122
trigger times: 9
Loss after 15695152 batches: 0.0123
trigger times: 10
Loss after 15696115 batches: 0.0118
trigger times: 11
Loss after 15697078 batches: 0.0118
trigger times: 12
Loss after 15698041 batches: 0.0115
trigger times: 13
Loss after 15699004 batches: 0.0119
trigger times: 14
Loss after 15699967 batches: 0.0115
trigger times: 15
Loss after 15700930 batches: 0.0121
trigger times: 16
Loss after 15701893 batches: 0.0117
trigger times: 17
Loss after 15702856 batches: 0.0120
trigger times: 18
Loss after 15703819 batches: 0.0118
trigger times: 19
Loss after 15704782 batches: 0.0114
trigger times: 20
Loss after 15705745 batches: 0.0110
trigger times: 21
Loss after 15706708 batches: 0.0112
trigger times: 22
Loss after 15707671 batches: 0.0110
trigger times: 23
Loss after 15708634 batches: 0.0110
trigger times: 24
Loss after 15709597 batches: 0.0109
trigger times: 25
Early stopping!
Start to test process.
Loss after 15710560 batches: 0.0107
Time to train on one home:  64.3948712348938
trigger times: 0
Loss after 15711523 batches: 0.0545
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 15712486 batches: 0.0253
trigger times: 2
Loss after 15713449 batches: 0.0201
trigger times: 3
Loss after 15714412 batches: 0.0166
trigger times: 4
Loss after 15715375 batches: 0.0159
trigger times: 5
Loss after 15716338 batches: 0.0144
trigger times: 6
Loss after 15717301 batches: 0.0141
trigger times: 7
Loss after 15718264 batches: 0.0139
trigger times: 8
Loss after 15719227 batches: 0.0132
trigger times: 9
Loss after 15720190 batches: 0.0132
trigger times: 10
Loss after 15721153 batches: 0.0133
trigger times: 11
Loss after 15722116 batches: 0.0127
trigger times: 12
Loss after 15723079 batches: 0.0127
trigger times: 13
Loss after 15724042 batches: 0.0124
trigger times: 14
Loss after 15725005 batches: 0.0126
trigger times: 15
Loss after 15725968 batches: 0.0123
trigger times: 16
Loss after 15726931 batches: 0.0118
trigger times: 17
Loss after 15727894 batches: 0.0119
trigger times: 18
Loss after 15728857 batches: 0.0119
trigger times: 19
Loss after 15729820 batches: 0.0118
trigger times: 20
Loss after 15730783 batches: 0.0117
trigger times: 21
Loss after 15731746 batches: 0.0116
trigger times: 22
Loss after 15732709 batches: 0.0115
trigger times: 23
Loss after 15733672 batches: 0.0115
trigger times: 24
Loss after 15734635 batches: 0.0120
trigger times: 25
Early stopping!
Start to test process.
Loss after 15735598 batches: 0.0115
Time to train on one home:  57.18072319030762
trigger times: 0
Loss after 15736561 batches: 0.1015
trigger times: 0
Loss after 15737524 batches: 0.0902
trigger times: 0
Loss after 15738487 batches: 0.0851
trigger times: 0
Loss after 15739450 batches: 0.0810
trigger times: 1
Loss after 15740413 batches: 0.0761
trigger times: 2
Loss after 15741376 batches: 0.0741
trigger times: 3
Loss after 15742339 batches: 0.0727
trigger times: 4
Loss after 15743302 batches: 0.0700
trigger times: 5
Loss after 15744265 batches: 0.0689
trigger times: 6
Loss after 15745228 batches: 0.0666
trigger times: 7
Loss after 15746191 batches: 0.0667
trigger times: 8
Loss after 15747154 batches: 0.0659
trigger times: 9
Loss after 15748117 batches: 0.0653
trigger times: 10
Loss after 15749080 batches: 0.0647
trigger times: 11
Loss after 15750043 batches: 0.0641
trigger times: 12
Loss after 15751006 batches: 0.0639
trigger times: 13
Loss after 15751969 batches: 0.0617
trigger times: 14
Loss after 15752932 batches: 0.0621
trigger times: 15
Loss after 15753895 batches: 0.0622
trigger times: 16
Loss after 15754858 batches: 0.0603
trigger times: 17
Loss after 15755821 batches: 0.0588
trigger times: 18
Loss after 15756784 batches: 0.0592
trigger times: 19
Loss after 15757747 batches: 0.0591
trigger times: 20
Loss after 15758710 batches: 0.0584
trigger times: 21
Loss after 15759673 batches: 0.0562
trigger times: 22
Loss after 15760636 batches: 0.0573
trigger times: 23
Loss after 15761599 batches: 0.0575
trigger times: 24
Loss after 15762562 batches: 0.0590
trigger times: 25
Early stopping!
Start to test process.
Loss after 15763525 batches: 0.0577
Time to train on one home:  56.470308780670166
trigger times: 0
Loss after 15764488 batches: 0.0498
trigger times: 0
Loss after 15765451 batches: 0.0393
trigger times: 0
Loss after 15766414 batches: 0.0325
trigger times: 1
Loss after 15767377 batches: 0.0284
trigger times: 2
Loss after 15768340 batches: 0.0270
trigger times: 0
Loss after 15769303 batches: 0.0256
trigger times: 1
Loss after 15770266 batches: 0.0231
trigger times: 2
Loss after 15771229 batches: 0.0235
trigger times: 3
Loss after 15772192 batches: 0.0223
trigger times: 4
Loss after 15773155 batches: 0.0217
trigger times: 5
Loss after 15774118 batches: 0.0222
trigger times: 6
Loss after 15775081 batches: 0.0214
trigger times: 7
Loss after 15776044 batches: 0.0217
trigger times: 8
Loss after 15777007 batches: 0.0214
trigger times: 9
Loss after 15777970 batches: 0.0216
trigger times: 10
Loss after 15778933 batches: 0.0217
trigger times: 11
Loss after 15779896 batches: 0.0215
trigger times: 12
Loss after 15780859 batches: 0.0201
trigger times: 13
Loss after 15781822 batches: 0.0192
trigger times: 14
Loss after 15782785 batches: 0.0186
trigger times: 15
Loss after 15783748 batches: 0.0193
trigger times: 16
Loss after 15784711 batches: 0.0188
trigger times: 17
Loss after 15785674 batches: 0.0184
trigger times: 18
Loss after 15786637 batches: 0.0191
trigger times: 19
Loss after 15787600 batches: 0.0189
trigger times: 20
Loss after 15788563 batches: 0.0188
trigger times: 21
Loss after 15789526 batches: 0.0183
trigger times: 22
Loss after 15790489 batches: 0.0184
trigger times: 23
Loss after 15791452 batches: 0.0183
trigger times: 24
Loss after 15792415 batches: 0.0189
trigger times: 25
Early stopping!
Start to test process.
Loss after 15793378 batches: 0.0169
Time to train on one home:  58.38832378387451
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15794341 batches: 0.0768
trigger times: 1
Loss after 15795304 batches: 0.0716
trigger times: 2
Loss after 15796267 batches: 0.0673
trigger times: 3
Loss after 15797230 batches: 0.0654
trigger times: 4
Loss after 15798193 batches: 0.0622
trigger times: 5
Loss after 15799156 batches: 0.0616
trigger times: 6
Loss after 15800119 batches: 0.0599
trigger times: 7
Loss after 15801082 batches: 0.0589
trigger times: 8
Loss after 15802045 batches: 0.0582
trigger times: 9
Loss after 15803008 batches: 0.0581
trigger times: 10
Loss after 15803971 batches: 0.0566
trigger times: 11
Loss after 15804934 batches: 0.0558
trigger times: 12
Loss after 15805897 batches: 0.0555
trigger times: 13
Loss after 15806860 batches: 0.0553
trigger times: 14
Loss after 15807823 batches: 0.0554
trigger times: 15
Loss after 15808786 batches: 0.0550
trigger times: 16
Loss after 15809749 batches: 0.0541
trigger times: 17
Loss after 15810712 batches: 0.0527
trigger times: 18
Loss after 15811675 batches: 0.0544
trigger times: 19
Loss after 15812638 batches: 0.0535
trigger times: 20
Loss after 15813601 batches: 0.0538
trigger times: 21
Loss after 15814564 batches: 0.0559
trigger times: 22
Loss after 15815527 batches: 0.0542
trigger times: 23
Loss after 15816490 batches: 0.0533
trigger times: 24
Loss after 15817453 batches: 0.0538
trigger times: 25
Early stopping!
Start to test process.
Loss after 15818416 batches: 0.0525
Time to train on one home:  56.94331383705139
trigger times: 0
Loss after 15819379 batches: 0.1070
trigger times: 0
Loss after 15820342 batches: 0.0647
trigger times: 0
Loss after 15821305 batches: 0.0598
trigger times: 1
Loss after 15822268 batches: 0.0518
trigger times: 2
Loss after 15823231 batches: 0.0493
trigger times: 3
Loss after 15824194 batches: 0.0462
trigger times: 4
Loss after 15825157 batches: 0.0453
trigger times: 5
Loss after 15826120 batches: 0.0436
trigger times: 6
Loss after 15827083 batches: 0.0439
trigger times: 7
Loss after 15828046 batches: 0.0426
trigger times: 8
Loss after 15829009 batches: 0.0418
trigger times: 9
Loss after 15829972 batches: 0.0401
trigger times: 10
Loss after 15830935 batches: 0.0408
trigger times: 11
Loss after 15831898 batches: 0.0404
trigger times: 12
Loss after 15832861 batches: 0.0401
trigger times: 13
Loss after 15833824 batches: 0.0386
trigger times: 14
Loss after 15834787 batches: 0.0393
trigger times: 15
Loss after 15835750 batches: 0.0387
trigger times: 16
Loss after 15836713 batches: 0.0385
trigger times: 17
Loss after 15837676 batches: 0.0387
trigger times: 18
Loss after 15838639 batches: 0.0380
trigger times: 19
Loss after 15839602 batches: 0.0375
trigger times: 20
Loss after 15840565 batches: 0.0376
trigger times: 21
Loss after 15841528 batches: 0.0371
trigger times: 22
Loss after 15842491 batches: 0.0372
trigger times: 23
Loss after 15843454 batches: 0.0371
trigger times: 24
Loss after 15844417 batches: 0.0356
trigger times: 25
Early stopping!
Start to test process.
Loss after 15845380 batches: 0.0369
Time to train on one home:  56.46714472770691
trigger times: 0
Loss after 15846343 batches: 0.0763
trigger times: 1
Loss after 15847306 batches: 0.0674
trigger times: 2
Loss after 15848269 batches: 0.0665
trigger times: 3
Loss after 15849232 batches: 0.0630
trigger times: 4
Loss after 15850195 batches: 0.0609
trigger times: 5
Loss after 15851158 batches: 0.0603
trigger times: 6
Loss after 15852121 batches: 0.0579
trigger times: 7
Loss after 15853084 batches: 0.0565
trigger times: 8
Loss after 15854047 batches: 0.0557
trigger times: 9
Loss after 15855010 batches: 0.0548
trigger times: 10
Loss after 15855973 batches: 0.0547
trigger times: 11
Loss after 15856936 batches: 0.0524
trigger times: 12
Loss after 15857899 batches: 0.0525
trigger times: 13
Loss after 15858862 batches: 0.0528
trigger times: 14
Loss after 15859825 batches: 0.0539
trigger times: 15
Loss after 15860788 batches: 0.0525
trigger times: 16
Loss after 15861751 batches: 0.0501
trigger times: 17
Loss after 15862714 batches: 0.0515
trigger times: 18
Loss after 15863677 batches: 0.0501
trigger times: 19
Loss after 15864640 batches: 0.0510
trigger times: 20
Loss after 15865603 batches: 0.0508
trigger times: 21
Loss after 15866566 batches: 0.0515
trigger times: 22
Loss after 15867529 batches: 0.0506
trigger times: 23
Loss after 15868492 batches: 0.0499
trigger times: 24
Loss after 15869455 batches: 0.0496
trigger times: 25
Early stopping!
Start to test process.
Loss after 15870418 batches: 0.0482
Time to train on one home:  58.92220664024353
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15871381 batches: 0.0635
trigger times: 1
Loss after 15872344 batches: 0.0505
trigger times: 2
Loss after 15873307 batches: 0.0470
trigger times: 3
Loss after 15874270 batches: 0.0403
trigger times: 4
Loss after 15875233 batches: 0.0362
trigger times: 5
Loss after 15876196 batches: 0.0327
trigger times: 6
Loss after 15877159 batches: 0.0314
trigger times: 7
Loss after 15878122 batches: 0.0299
trigger times: 8
Loss after 15879085 batches: 0.0288
trigger times: 9
Loss after 15880048 batches: 0.0273
trigger times: 10
Loss after 15881011 batches: 0.0265
trigger times: 11
Loss after 15881974 batches: 0.0256
trigger times: 12
Loss after 15882937 batches: 0.0259
trigger times: 13
Loss after 15883900 batches: 0.0249
trigger times: 14
Loss after 15884863 batches: 0.0259
trigger times: 15
Loss after 15885826 batches: 0.0250
trigger times: 16
Loss after 15886789 batches: 0.0248
trigger times: 17
Loss after 15887752 batches: 0.0243
trigger times: 18
Loss after 15888715 batches: 0.0237
trigger times: 19
Loss after 15889678 batches: 0.0236
trigger times: 20
Loss after 15890641 batches: 0.0230
trigger times: 21
Loss after 15891604 batches: 0.0234
trigger times: 22
Loss after 15892567 batches: 0.0227
trigger times: 23
Loss after 15893530 batches: 0.0230
trigger times: 24
Loss after 15894493 batches: 0.0231
trigger times: 25
Early stopping!
Start to test process.
Loss after 15895456 batches: 0.0229
Time to train on one home:  57.0665328502655
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15896419 batches: 0.0914
trigger times: 0
Loss after 15897382 batches: 0.0558
trigger times: 1
Loss after 15898345 batches: 0.0442
trigger times: 0
Loss after 15899308 batches: 0.0399
trigger times: 0
Loss after 15900271 batches: 0.0354
trigger times: 1
Loss after 15901234 batches: 0.0318
trigger times: 2
Loss after 15902197 batches: 0.0288
trigger times: 3
Loss after 15903160 batches: 0.0261
trigger times: 4
Loss after 15904123 batches: 0.0257
trigger times: 5
Loss after 15905086 batches: 0.0237
trigger times: 6
Loss after 15906049 batches: 0.0227
trigger times: 7
Loss after 15907012 batches: 0.0231
trigger times: 8
Loss after 15907975 batches: 0.0220
trigger times: 9
Loss after 15908938 batches: 0.0235
trigger times: 10
Loss after 15909901 batches: 0.0223
trigger times: 11
Loss after 15910864 batches: 0.0215
trigger times: 12
Loss after 15911827 batches: 0.0209
trigger times: 13
Loss after 15912790 batches: 0.0214
trigger times: 14
Loss after 15913753 batches: 0.0199
trigger times: 15
Loss after 15914716 batches: 0.0216
trigger times: 16
Loss after 15915679 batches: 0.0206
trigger times: 17
Loss after 15916642 batches: 0.0206
trigger times: 18
Loss after 15917605 batches: 0.0217
trigger times: 19
Loss after 15918568 batches: 0.0212
trigger times: 20
Loss after 15919531 batches: 0.0193
trigger times: 21
Loss after 15920494 batches: 0.0175
trigger times: 22
Loss after 15921457 batches: 0.0189
trigger times: 23
Loss after 15922420 batches: 0.0179
trigger times: 24
Loss after 15923383 batches: 0.0170
trigger times: 25
Early stopping!
Start to test process.
Loss after 15924346 batches: 0.0183
Time to train on one home:  59.67560958862305
trigger times: 0
Loss after 15925275 batches: 0.1264
trigger times: 0
Loss after 15926204 batches: 0.0664
trigger times: 0
Loss after 15927133 batches: 0.0524
trigger times: 1
Loss after 15928062 batches: 0.0431
trigger times: 0
Loss after 15928991 batches: 0.0378
trigger times: 1
Loss after 15929920 batches: 0.0376
trigger times: 2
Loss after 15930849 batches: 0.0331
trigger times: 3
Loss after 15931778 batches: 0.0289
trigger times: 0
Loss after 15932707 batches: 0.0297
trigger times: 1
Loss after 15933636 batches: 0.0281
trigger times: 2
Loss after 15934565 batches: 0.0281
trigger times: 3
Loss after 15935494 batches: 0.0277
trigger times: 4
Loss after 15936423 batches: 0.0275
trigger times: 5
Loss after 15937352 batches: 0.0253
trigger times: 6
Loss after 15938281 batches: 0.0260
trigger times: 7
Loss after 15939210 batches: 0.0254
trigger times: 8
Loss after 15940139 batches: 0.0250
trigger times: 9
Loss after 15941068 batches: 0.0257
trigger times: 10
Loss after 15941997 batches: 0.0253
trigger times: 11
Loss after 15942926 batches: 0.0247
trigger times: 12
Loss after 15943855 batches: 0.0247
trigger times: 13
Loss after 15944784 batches: 0.0239
trigger times: 14
Loss after 15945713 batches: 0.0220
trigger times: 15
Loss after 15946642 batches: 0.0223
trigger times: 16
Loss after 15947571 batches: 0.0223
trigger times: 17
Loss after 15948500 batches: 0.0233
trigger times: 18
Loss after 15949429 batches: 0.0227
trigger times: 19
Loss after 15950358 batches: 0.0232
trigger times: 0
Loss after 15951287 batches: 0.0224
trigger times: 1
Loss after 15952216 batches: 0.0228
trigger times: 2
Loss after 15953145 batches: 0.0259
trigger times: 3
Loss after 15954074 batches: 0.0229
trigger times: 4
Loss after 15955003 batches: 0.0238
trigger times: 5
Loss after 15955932 batches: 0.0229
trigger times: 6
Loss after 15956861 batches: 0.0217
trigger times: 7
Loss after 15957790 batches: 0.0215
trigger times: 8
Loss after 15958719 batches: 0.0227
trigger times: 9
Loss after 15959648 batches: 0.0250
trigger times: 0
Loss after 15960577 batches: 0.0254
trigger times: 1
Loss after 15961506 batches: 0.0239
trigger times: 2
Loss after 15962435 batches: 0.0223
trigger times: 3
Loss after 15963364 batches: 0.0209
trigger times: 4
Loss after 15964293 batches: 0.0211
trigger times: 5
Loss after 15965222 batches: 0.0224
trigger times: 6
Loss after 15966151 batches: 0.0210
trigger times: 7
Loss after 15967080 batches: 0.0214
trigger times: 8
Loss after 15968009 batches: 0.0224
trigger times: 9
Loss after 15968938 batches: 0.0222
trigger times: 10
Loss after 15969867 batches: 0.0205
trigger times: 11
Loss after 15970796 batches: 0.0192
trigger times: 12
Loss after 15971725 batches: 0.0276
trigger times: 13
Loss after 15972654 batches: 0.0265
trigger times: 14
Loss after 15973583 batches: 0.0256
trigger times: 15
Loss after 15974512 batches: 0.0254
trigger times: 16
Loss after 15975441 batches: 0.0232
trigger times: 17
Loss after 15976370 batches: 0.0221
trigger times: 18
Loss after 15977299 batches: 0.0212
trigger times: 19
Loss after 15978228 batches: 0.0209
trigger times: 20
Loss after 15979157 batches: 0.0196
trigger times: 21
Loss after 15980086 batches: 0.0209
trigger times: 22
Loss after 15981015 batches: 0.0211
trigger times: 23
Loss after 15981944 batches: 0.0186
trigger times: 24
Loss after 15982873 batches: 0.0191
trigger times: 25
Early stopping!
Start to test process.
Loss after 15983802 batches: 0.0190
Time to train on one home:  88.13008427619934
trigger times: 0
Loss after 15984764 batches: 0.0730
trigger times: 1
Loss after 15985726 batches: 0.0653
trigger times: 2
Loss after 15986688 batches: 0.0637
trigger times: 3
Loss after 15987650 batches: 0.0611
trigger times: 4
Loss after 15988612 batches: 0.0586
trigger times: 5
Loss after 15989574 batches: 0.0563
trigger times: 6
Loss after 15990536 batches: 0.0555
trigger times: 7
Loss after 15991498 batches: 0.0556
trigger times: 8
Loss after 15992460 batches: 0.0535
trigger times: 9
Loss after 15993422 batches: 0.0525
trigger times: 10
Loss after 15994384 batches: 0.0527
trigger times: 11
Loss after 15995346 batches: 0.0524
trigger times: 12
Loss after 15996308 batches: 0.0520
trigger times: 13
Loss after 15997270 batches: 0.0524
trigger times: 14
Loss after 15998232 batches: 0.0515
trigger times: 15
Loss after 15999194 batches: 0.0507
trigger times: 16
Loss after 16000156 batches: 0.0511
trigger times: 17
Loss after 16001118 batches: 0.0500
trigger times: 18
Loss after 16002080 batches: 0.0511
trigger times: 19
Loss after 16003042 batches: 0.0503
trigger times: 20
Loss after 16004004 batches: 0.0501
trigger times: 21
Loss after 16004966 batches: 0.0499
trigger times: 22
Loss after 16005928 batches: 0.0495
trigger times: 23
Loss after 16006890 batches: 0.0501
trigger times: 24
Loss after 16007852 batches: 0.0497
trigger times: 25
Early stopping!
Start to test process.
Loss after 16008814 batches: 0.0489
Time to train on one home:  57.226054191589355
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16009777 batches: 0.0577
trigger times: 1
Loss after 16010740 batches: 0.0445
trigger times: 2
Loss after 16011703 batches: 0.0440
trigger times: 3
Loss after 16012666 batches: 0.0407
trigger times: 4
Loss after 16013629 batches: 0.0373
trigger times: 5
Loss after 16014592 batches: 0.0364
trigger times: 6
Loss after 16015555 batches: 0.0355
trigger times: 7
Loss after 16016518 batches: 0.0342
trigger times: 8
Loss after 16017481 batches: 0.0334
trigger times: 9
Loss after 16018444 batches: 0.0327
trigger times: 10
Loss after 16019407 batches: 0.0331
trigger times: 11
Loss after 16020370 batches: 0.0320
trigger times: 12
Loss after 16021333 batches: 0.0319
trigger times: 13
Loss after 16022296 batches: 0.0312
trigger times: 14
Loss after 16023259 batches: 0.0306
trigger times: 15
Loss after 16024222 batches: 0.0309
trigger times: 16
Loss after 16025185 batches: 0.0311
trigger times: 17
Loss after 16026148 batches: 0.0302
trigger times: 18
Loss after 16027111 batches: 0.0302
trigger times: 19
Loss after 16028074 batches: 0.0297
trigger times: 20
Loss after 16029037 batches: 0.0287
trigger times: 21
Loss after 16030000 batches: 0.0289
trigger times: 22
Loss after 16030963 batches: 0.0292
trigger times: 23
Loss after 16031926 batches: 0.0293
trigger times: 24
Loss after 16032889 batches: 0.0294
trigger times: 25
Early stopping!
Start to test process.
Loss after 16033852 batches: 0.0285
Time to train on one home:  53.392125844955444
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16034815 batches: 0.0544
trigger times: 0
Loss after 16035778 batches: 0.0454
trigger times: 0
Loss after 16036741 batches: 0.0436
trigger times: 1
Loss after 16037704 batches: 0.0404
trigger times: 2
Loss after 16038667 batches: 0.0377
trigger times: 3
Loss after 16039630 batches: 0.0363
trigger times: 4
Loss after 16040593 batches: 0.0338
trigger times: 5
Loss after 16041556 batches: 0.0332
trigger times: 6
Loss after 16042519 batches: 0.0326
trigger times: 7
Loss after 16043482 batches: 0.0311
trigger times: 8
Loss after 16044445 batches: 0.0311
trigger times: 9
Loss after 16045408 batches: 0.0299
trigger times: 10
Loss after 16046371 batches: 0.0305
trigger times: 11
Loss after 16047334 batches: 0.0298
trigger times: 12
Loss after 16048297 batches: 0.0297
trigger times: 13
Loss after 16049260 batches: 0.0290
trigger times: 14
Loss after 16050223 batches: 0.0283
trigger times: 15
Loss after 16051186 batches: 0.0273
trigger times: 16
Loss after 16052149 batches: 0.0278
trigger times: 17
Loss after 16053112 batches: 0.0288
trigger times: 18
Loss after 16054075 batches: 0.0275
trigger times: 19
Loss after 16055038 batches: 0.0275
trigger times: 20
Loss after 16056001 batches: 0.0283
trigger times: 21
Loss after 16056964 batches: 0.0265
trigger times: 22
Loss after 16057927 batches: 0.0269
trigger times: 23
Loss after 16058890 batches: 0.0259
trigger times: 24
Loss after 16059853 batches: 0.0268
trigger times: 25
Early stopping!
Start to test process.
Loss after 16060816 batches: 0.0271
Time to train on one home:  56.4201340675354
trigger times: 0
Loss after 16061779 batches: 0.1095
trigger times: 0
Loss after 16062742 batches: 0.0935
trigger times: 1
Loss after 16063705 batches: 0.0911
trigger times: 2
Loss after 16064668 batches: 0.0854
trigger times: 3
Loss after 16065631 batches: 0.0818
trigger times: 4
Loss after 16066594 batches: 0.0800
trigger times: 5
Loss after 16067557 batches: 0.0788
trigger times: 6
Loss after 16068520 batches: 0.0770
trigger times: 7
Loss after 16069483 batches: 0.0769
trigger times: 8
Loss after 16070446 batches: 0.0726
trigger times: 9
Loss after 16071409 batches: 0.0720
trigger times: 10
Loss after 16072372 batches: 0.0713
trigger times: 11
Loss after 16073335 batches: 0.0706
trigger times: 12
Loss after 16074298 batches: 0.0701
trigger times: 13
Loss after 16075261 batches: 0.0716
trigger times: 14
Loss after 16076224 batches: 0.0675
trigger times: 15
Loss after 16077187 batches: 0.0687
trigger times: 16
Loss after 16078150 batches: 0.0679
trigger times: 17
Loss after 16079113 batches: 0.0666
trigger times: 18
Loss after 16080076 batches: 0.0677
trigger times: 19
Loss after 16081039 batches: 0.0652
trigger times: 20
Loss after 16082002 batches: 0.0643
trigger times: 21
Loss after 16082965 batches: 0.0664
trigger times: 22
Loss after 16083928 batches: 0.0667
trigger times: 23
Loss after 16084891 batches: 0.0659
trigger times: 24
Loss after 16085854 batches: 0.0668
trigger times: 25
Early stopping!
Start to test process.
Loss after 16086817 batches: 0.0675
Time to train on one home:  57.61380457878113
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16087780 batches: 0.1118
trigger times: 1
Loss after 16088743 batches: 0.0582
trigger times: 2
Loss after 16089706 batches: 0.0580
trigger times: 0
Loss after 16090669 batches: 0.0496
trigger times: 0
Loss after 16091632 batches: 0.0420
trigger times: 1
Loss after 16092595 batches: 0.0408
trigger times: 2
Loss after 16093558 batches: 0.0366
trigger times: 0
Loss after 16094521 batches: 0.0350
trigger times: 1
Loss after 16095484 batches: 0.0346
trigger times: 0
Loss after 16096447 batches: 0.0337
trigger times: 1
Loss after 16097410 batches: 0.0324
trigger times: 2
Loss after 16098373 batches: 0.0323
trigger times: 3
Loss after 16099336 batches: 0.0320
trigger times: 4
Loss after 16100299 batches: 0.0307
trigger times: 5
Loss after 16101262 batches: 0.0296
trigger times: 6
Loss after 16102225 batches: 0.0295
trigger times: 7
Loss after 16103188 batches: 0.0289
trigger times: 8
Loss after 16104151 batches: 0.0288
trigger times: 0
Loss after 16105114 batches: 0.0278
trigger times: 1
Loss after 16106077 batches: 0.0276
trigger times: 0
Loss after 16107040 batches: 0.0280
trigger times: 1
Loss after 16108003 batches: 0.0272
trigger times: 2
Loss after 16108966 batches: 0.0276
trigger times: 3
Loss after 16109929 batches: 0.0272
trigger times: 4
Loss after 16110892 batches: 0.0269
trigger times: 5
Loss after 16111855 batches: 0.0260
trigger times: 6
Loss after 16112818 batches: 0.0272
trigger times: 7
Loss after 16113781 batches: 0.0266
trigger times: 8
Loss after 16114744 batches: 0.0255
trigger times: 9
Loss after 16115707 batches: 0.0253
trigger times: 10
Loss after 16116670 batches: 0.0248
trigger times: 11
Loss after 16117633 batches: 0.0254
trigger times: 12
Loss after 16118596 batches: 0.0272
trigger times: 13
Loss after 16119559 batches: 0.0266
trigger times: 14
Loss after 16120522 batches: 0.0257
trigger times: 15
Loss after 16121485 batches: 0.0262
trigger times: 16
Loss after 16122448 batches: 0.0248
trigger times: 17
Loss after 16123411 batches: 0.0260
trigger times: 18
Loss after 16124374 batches: 0.0246
trigger times: 19
Loss after 16125337 batches: 0.0238
trigger times: 20
Loss after 16126300 batches: 0.0225
trigger times: 21
Loss after 16127263 batches: 0.0238
trigger times: 22
Loss after 16128226 batches: 0.0235
trigger times: 23
Loss after 16129189 batches: 0.0224
trigger times: 24
Loss after 16130152 batches: 0.0230
trigger times: 0
Loss after 16131115 batches: 0.0225
trigger times: 1
Loss after 16132078 batches: 0.0222
trigger times: 2
Loss after 16133041 batches: 0.0224
trigger times: 3
Loss after 16134004 batches: 0.0225
trigger times: 4
Loss after 16134967 batches: 0.0225
trigger times: 5
Loss after 16135930 batches: 0.0241
trigger times: 6
Loss after 16136893 batches: 0.0229
trigger times: 7
Loss after 16137856 batches: 0.0230
trigger times: 8
Loss after 16138819 batches: 0.0213
trigger times: 9
Loss after 16139782 batches: 0.0214
trigger times: 10
Loss after 16140745 batches: 0.0209
trigger times: 11
Loss after 16141708 batches: 0.0219
trigger times: 12
Loss after 16142671 batches: 0.0203
trigger times: 13
Loss after 16143634 batches: 0.0214
trigger times: 14
Loss after 16144597 batches: 0.0219
trigger times: 15
Loss after 16145560 batches: 0.0205
trigger times: 16
Loss after 16146523 batches: 0.0201
trigger times: 17
Loss after 16147486 batches: 0.0212
trigger times: 18
Loss after 16148449 batches: 0.0198
trigger times: 19
Loss after 16149412 batches: 0.0201
trigger times: 20
Loss after 16150375 batches: 0.0211
trigger times: 21
Loss after 16151338 batches: 0.0211
trigger times: 22
Loss after 16152301 batches: 0.0199
trigger times: 23
Loss after 16153264 batches: 0.0198
trigger times: 24
Loss after 16154227 batches: 0.0201
trigger times: 25
Early stopping!
Start to test process.
Loss after 16155190 batches: 0.0207
Time to train on one home:  92.53824758529663
trigger times: 0
Loss after 16156149 batches: 0.1155
trigger times: 1
Loss after 16157108 batches: 0.0536
trigger times: 0
Loss after 16158067 batches: 0.0407
trigger times: 1
Loss after 16159026 batches: 0.0327
trigger times: 2
Loss after 16159985 batches: 0.0285
trigger times: 0
Loss after 16160944 batches: 0.0253
trigger times: 1
Loss after 16161903 batches: 0.0235
trigger times: 2
Loss after 16162862 batches: 0.0226
trigger times: 3
Loss after 16163821 batches: 0.0216
trigger times: 4
Loss after 16164780 batches: 0.0202
trigger times: 5
Loss after 16165739 batches: 0.0190
trigger times: 6
Loss after 16166698 batches: 0.0185
trigger times: 7
Loss after 16167657 batches: 0.0181
trigger times: 8
Loss after 16168616 batches: 0.0178
trigger times: 9
Loss after 16169575 batches: 0.0178
trigger times: 10
Loss after 16170534 batches: 0.0165
trigger times: 11
Loss after 16171493 batches: 0.0160
trigger times: 12
Loss after 16172452 batches: 0.0167
trigger times: 13
Loss after 16173411 batches: 0.0169
trigger times: 14
Loss after 16174370 batches: 0.0168
trigger times: 0
Loss after 16175329 batches: 0.0169
trigger times: 1
Loss after 16176288 batches: 0.0166
trigger times: 2
Loss after 16177247 batches: 0.0156
trigger times: 3
Loss after 16178206 batches: 0.0149
trigger times: 4
Loss after 16179165 batches: 0.0153
trigger times: 5
Loss after 16180124 batches: 0.0156
trigger times: 6
Loss after 16181083 batches: 0.0147
trigger times: 0
Loss after 16182042 batches: 0.0148
trigger times: 0
Loss after 16183001 batches: 0.0142
trigger times: 0
Loss after 16183960 batches: 0.0158
trigger times: 1
Loss after 16184919 batches: 0.0155
trigger times: 2
Loss after 16185878 batches: 0.0151
trigger times: 3
Loss after 16186837 batches: 0.0142
trigger times: 4
Loss after 16187796 batches: 0.0138
trigger times: 5
Loss after 16188755 batches: 0.0138
trigger times: 6
Loss after 16189714 batches: 0.0145
trigger times: 7
Loss after 16190673 batches: 0.0144
trigger times: 8
Loss after 16191632 batches: 0.0138
trigger times: 9
Loss after 16192591 batches: 0.0134
trigger times: 10
Loss after 16193550 batches: 0.0133
trigger times: 11
Loss after 16194509 batches: 0.0133
trigger times: 0
Loss after 16195468 batches: 0.0134
trigger times: 1
Loss after 16196427 batches: 0.0132
trigger times: 2
Loss after 16197386 batches: 0.0130
trigger times: 3
Loss after 16198345 batches: 0.0128
trigger times: 4
Loss after 16199304 batches: 0.0122
trigger times: 5
Loss after 16200263 batches: 0.0121
trigger times: 6
Loss after 16201222 batches: 0.0121
trigger times: 7
Loss after 16202181 batches: 0.0123
trigger times: 8
Loss after 16203140 batches: 0.0120
trigger times: 9
Loss after 16204099 batches: 0.0118
trigger times: 10
Loss after 16205058 batches: 0.0119
trigger times: 11
Loss after 16206017 batches: 0.0116
trigger times: 12
Loss after 16206976 batches: 0.0114
trigger times: 13
Loss after 16207935 batches: 0.0127
trigger times: 14
Loss after 16208894 batches: 0.0116
trigger times: 15
Loss after 16209853 batches: 0.0119
trigger times: 16
Loss after 16210812 batches: 0.0112
trigger times: 17
Loss after 16211771 batches: 0.0106
trigger times: 18
Loss after 16212730 batches: 0.0107
trigger times: 19
Loss after 16213689 batches: 0.0109
trigger times: 20
Loss after 16214648 batches: 0.0107
trigger times: 21
Loss after 16215607 batches: 0.0108
trigger times: 22
Loss after 16216566 batches: 0.0104
trigger times: 23
Loss after 16217525 batches: 0.0106
trigger times: 24
Loss after 16218484 batches: 0.0113
trigger times: 25
Early stopping!
Start to test process.
Loss after 16219443 batches: 0.0112
Time to train on one home:  92.80780506134033
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16220406 batches: 0.0525
trigger times: 1
Loss after 16221369 batches: 0.0284
trigger times: 2
Loss after 16222332 batches: 0.0261
trigger times: 3
Loss after 16223295 batches: 0.0255
trigger times: 4
Loss after 16224258 batches: 0.0243
trigger times: 5
Loss after 16225221 batches: 0.0234
trigger times: 6
Loss after 16226184 batches: 0.0226
trigger times: 7
Loss after 16227147 batches: 0.0208
trigger times: 8
Loss after 16228110 batches: 0.0203
trigger times: 9
Loss after 16229073 batches: 0.0200
trigger times: 10
Loss after 16230036 batches: 0.0194
trigger times: 11
Loss after 16230999 batches: 0.0188
trigger times: 12
Loss after 16231962 batches: 0.0182
trigger times: 13
Loss after 16232925 batches: 0.0183
trigger times: 14
Loss after 16233888 batches: 0.0181
trigger times: 15
Loss after 16234851 batches: 0.0178
trigger times: 16
Loss after 16235814 batches: 0.0177
trigger times: 17
Loss after 16236777 batches: 0.0175
trigger times: 18
Loss after 16237740 batches: 0.0175
trigger times: 19
Loss after 16238703 batches: 0.0177
trigger times: 20
Loss after 16239666 batches: 0.0173
trigger times: 21
Loss after 16240629 batches: 0.0170
trigger times: 22
Loss after 16241592 batches: 0.0168
trigger times: 23
Loss after 16242555 batches: 0.0167
trigger times: 24
Loss after 16243518 batches: 0.0168
trigger times: 25
Early stopping!
Start to test process.
Loss after 16244481 batches: 0.0165
Time to train on one home:  56.795841455459595
trigger times: 0
Loss after 16245426 batches: 0.0773
trigger times: 0
Loss after 16246371 batches: 0.0536
trigger times: 0
Loss after 16247316 batches: 0.0377
trigger times: 1
Loss after 16248261 batches: 0.0319
trigger times: 2
Loss after 16249206 batches: 0.0295
trigger times: 3
Loss after 16250151 batches: 0.0302
trigger times: 4
Loss after 16251096 batches: 0.0283
trigger times: 5
Loss after 16252041 batches: 0.0274
trigger times: 6
Loss after 16252986 batches: 0.0253
trigger times: 7
Loss after 16253931 batches: 0.0252
trigger times: 8
Loss after 16254876 batches: 0.0252
trigger times: 9
Loss after 16255821 batches: 0.0222
trigger times: 10
Loss after 16256766 batches: 0.0218
trigger times: 11
Loss after 16257711 batches: 0.0206
trigger times: 12
Loss after 16258656 batches: 0.0214
trigger times: 13
Loss after 16259601 batches: 0.0201
trigger times: 14
Loss after 16260546 batches: 0.0195
trigger times: 15
Loss after 16261491 batches: 0.0201
trigger times: 16
Loss after 16262436 batches: 0.0191
trigger times: 17
Loss after 16263381 batches: 0.0195
trigger times: 18
Loss after 16264326 batches: 0.0181
trigger times: 19
Loss after 16265271 batches: 0.0189
trigger times: 20
Loss after 16266216 batches: 0.0181
trigger times: 21
Loss after 16267161 batches: 0.0191
trigger times: 22
Loss after 16268106 batches: 0.0185
trigger times: 23
Loss after 16269051 batches: 0.0176
trigger times: 24
Loss after 16269996 batches: 0.0184
trigger times: 25
Early stopping!
Start to test process.
Loss after 16270941 batches: 0.0170
Time to train on one home:  59.157604932785034
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16271878 batches: 0.0880
trigger times: 1
Loss after 16272815 batches: 0.0701
trigger times: 2
Loss after 16273752 batches: 0.0673
trigger times: 3
Loss after 16274689 batches: 0.0630
trigger times: 4
Loss after 16275626 batches: 0.0592
trigger times: 5
Loss after 16276563 batches: 0.0582
trigger times: 6
Loss after 16277500 batches: 0.0558
trigger times: 7
Loss after 16278437 batches: 0.0545
trigger times: 8
Loss after 16279374 batches: 0.0547
trigger times: 9
Loss after 16280311 batches: 0.0522
trigger times: 10
Loss after 16281248 batches: 0.0522
trigger times: 11
Loss after 16282185 batches: 0.0522
trigger times: 12
Loss after 16283122 batches: 0.0514
trigger times: 13
Loss after 16284059 batches: 0.0512
trigger times: 14
Loss after 16284996 batches: 0.0510
trigger times: 15
Loss after 16285933 batches: 0.0505
trigger times: 16
Loss after 16286870 batches: 0.0500
trigger times: 17
Loss after 16287807 batches: 0.0494
trigger times: 18
Loss after 16288744 batches: 0.0490
trigger times: 19
Loss after 16289681 batches: 0.0482
trigger times: 20
Loss after 16290618 batches: 0.0468
trigger times: 21
Loss after 16291555 batches: 0.0472
trigger times: 22
Loss after 16292492 batches: 0.0477
trigger times: 23
Loss after 16293429 batches: 0.0472
trigger times: 24
Loss after 16294366 batches: 0.0465
trigger times: 25
Early stopping!
Start to test process.
Loss after 16295303 batches: 0.0466
Time to train on one home:  57.03882646560669
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 16296266 batches: 0.0267
trigger times: 1
Loss after 16297229 batches: 0.0154
trigger times: 2
Loss after 16298192 batches: 0.0137
trigger times: 3
Loss after 16299155 batches: 0.0135
trigger times: 4
Loss after 16300118 batches: 0.0134
trigger times: 5
Loss after 16301081 batches: 0.0128
trigger times: 6
Loss after 16302044 batches: 0.0123
trigger times: 7
Loss after 16303007 batches: 0.0118
trigger times: 8
Loss after 16303970 batches: 0.0110
trigger times: 9
Loss after 16304933 batches: 0.0106
trigger times: 10
Loss after 16305896 batches: 0.0099
trigger times: 11
Loss after 16306859 batches: 0.0092
trigger times: 12
Loss after 16307822 batches: 0.0088
trigger times: 13
Loss after 16308785 batches: 0.0083
trigger times: 14
Loss after 16309748 batches: 0.0080
trigger times: 15
Loss after 16310711 batches: 0.0077
trigger times: 16
Loss after 16311674 batches: 0.0076
trigger times: 17
Loss after 16312637 batches: 0.0073
trigger times: 18
Loss after 16313600 batches: 0.0071
trigger times: 19
Loss after 16314563 batches: 0.0072
trigger times: 20
Loss after 16315526 batches: 0.0072
trigger times: 21
Loss after 16316489 batches: 0.0069
trigger times: 22
Loss after 16317452 batches: 0.0067
trigger times: 23
Loss after 16318415 batches: 0.0068
trigger times: 24
Loss after 16319378 batches: 0.0069
trigger times: 25
Early stopping!
Start to test process.
Loss after 16320341 batches: 0.0066
Time to train on one home:  54.27211833000183
trigger times: 0
Loss after 16321304 batches: 0.0940
trigger times: 1
Loss after 16322267 batches: 0.0783
trigger times: 2
Loss after 16323230 batches: 0.0716
trigger times: 3
Loss after 16324193 batches: 0.0676
trigger times: 4
Loss after 16325156 batches: 0.0655
trigger times: 5
Loss after 16326119 batches: 0.0629
trigger times: 6
Loss after 16327082 batches: 0.0615
trigger times: 7
Loss after 16328045 batches: 0.0605
trigger times: 8
Loss after 16329008 batches: 0.0602
trigger times: 9
Loss after 16329971 batches: 0.0594
trigger times: 10
Loss after 16330934 batches: 0.0575
trigger times: 11
Loss after 16331897 batches: 0.0568
trigger times: 12
Loss after 16332860 batches: 0.0562
trigger times: 13
Loss after 16333823 batches: 0.0554
trigger times: 14
Loss after 16334786 batches: 0.0553
trigger times: 15
Loss after 16335749 batches: 0.0547
trigger times: 16
Loss after 16336712 batches: 0.0546
trigger times: 17
Loss after 16337675 batches: 0.0538
trigger times: 18
Loss after 16338638 batches: 0.0545
trigger times: 19
Loss after 16339601 batches: 0.0538
trigger times: 20
Loss after 16340564 batches: 0.0534
trigger times: 21
Loss after 16341527 batches: 0.0523
trigger times: 22
Loss after 16342490 batches: 0.0520
trigger times: 23
Loss after 16343453 batches: 0.0517
trigger times: 24
Loss after 16344416 batches: 0.0514
trigger times: 25
Early stopping!
Start to test process.
Loss after 16345379 batches: 0.0526
Time to train on one home:  54.06746459007263
trigger times: 0
Loss after 16346342 batches: 0.0674
trigger times: 1
Loss after 16347305 batches: 0.0506
trigger times: 2
Loss after 16348268 batches: 0.0453
trigger times: 3
Loss after 16349231 batches: 0.0423
trigger times: 4
Loss after 16350194 batches: 0.0405
trigger times: 5
Loss after 16351157 batches: 0.0376
trigger times: 6
Loss after 16352120 batches: 0.0371
trigger times: 7
Loss after 16353083 batches: 0.0360
trigger times: 8
Loss after 16354046 batches: 0.0344
trigger times: 9
Loss after 16355009 batches: 0.0340
trigger times: 10
Loss after 16355972 batches: 0.0331
trigger times: 11
Loss after 16356935 batches: 0.0320
trigger times: 12
Loss after 16357898 batches: 0.0319
trigger times: 13
Loss after 16358861 batches: 0.0319
trigger times: 14
Loss after 16359824 batches: 0.0315
trigger times: 15
Loss after 16360787 batches: 0.0313
trigger times: 16
Loss after 16361750 batches: 0.0311
trigger times: 17
Loss after 16362713 batches: 0.0324
trigger times: 18
Loss after 16363676 batches: 0.0317
trigger times: 19
Loss after 16364639 batches: 0.0308
trigger times: 20
Loss after 16365602 batches: 0.0301
trigger times: 21
Loss after 16366565 batches: 0.0294
trigger times: 22
Loss after 16367528 batches: 0.0291
trigger times: 23
Loss after 16368491 batches: 0.0285
trigger times: 24
Loss after 16369454 batches: 0.0288
trigger times: 25
Early stopping!
Start to test process.
Loss after 16370417 batches: 0.0286
Time to train on one home:  57.10069131851196
trigger times: 0
Loss after 16371313 batches: 0.1021
trigger times: 1
Loss after 16372209 batches: 0.0920
trigger times: 2
Loss after 16373105 batches: 0.0876
trigger times: 3
Loss after 16374001 batches: 0.0815
trigger times: 4
Loss after 16374897 batches: 0.0737
trigger times: 5
Loss after 16375793 batches: 0.0709
trigger times: 6
Loss after 16376689 batches: 0.0675
trigger times: 7
Loss after 16377585 batches: 0.0643
trigger times: 8
Loss after 16378481 batches: 0.0623
trigger times: 9
Loss after 16379377 batches: 0.0608
trigger times: 10
Loss after 16380273 batches: 0.0609
trigger times: 11
Loss after 16381169 batches: 0.0605
trigger times: 12
Loss after 16382065 batches: 0.0580
trigger times: 13
Loss after 16382961 batches: 0.0591
trigger times: 14
Loss after 16383857 batches: 0.0563
trigger times: 15
Loss after 16384753 batches: 0.0544
trigger times: 16
Loss after 16385649 batches: 0.0554
trigger times: 17
Loss after 16386545 batches: 0.0552
trigger times: 18
Loss after 16387441 batches: 0.0542
trigger times: 19
Loss after 16388337 batches: 0.0548
trigger times: 20
Loss after 16389233 batches: 0.0529
trigger times: 21
Loss after 16390129 batches: 0.0525
trigger times: 22
Loss after 16391025 batches: 0.0519
trigger times: 23
Loss after 16391921 batches: 0.0540
trigger times: 24
Loss after 16392817 batches: 0.0537
trigger times: 25
Early stopping!
Start to test process.
Loss after 16393713 batches: 0.0561
Time to train on one home:  54.477755546569824
trigger times: 0
Loss after 16394676 batches: 0.1533
trigger times: 1
Loss after 16395639 batches: 0.1146
trigger times: 2
Loss after 16396602 batches: 0.0990
trigger times: 3
Loss after 16397565 batches: 0.0883
trigger times: 4
Loss after 16398528 batches: 0.0810
trigger times: 5
Loss after 16399491 batches: 0.0756
trigger times: 6
Loss after 16400454 batches: 0.0689
trigger times: 7
Loss after 16401417 batches: 0.0628
trigger times: 8
Loss after 16402380 batches: 0.0584
trigger times: 9
Loss after 16403343 batches: 0.0551
trigger times: 10
Loss after 16404306 batches: 0.0524
trigger times: 11
Loss after 16405269 batches: 0.0525
trigger times: 12
Loss after 16406232 batches: 0.0486
trigger times: 13
Loss after 16407195 batches: 0.0493
trigger times: 14
Loss after 16408158 batches: 0.0474
trigger times: 15
Loss after 16409121 batches: 0.0467
trigger times: 16
Loss after 16410084 batches: 0.0455
trigger times: 17
Loss after 16411047 batches: 0.0446
trigger times: 18
Loss after 16412010 batches: 0.0450
trigger times: 19
Loss after 16412973 batches: 0.0458
trigger times: 20
Loss after 16413936 batches: 0.0447
trigger times: 21
Loss after 16414899 batches: 0.0435
trigger times: 22
Loss after 16415862 batches: 0.0431
trigger times: 23
Loss after 16416825 batches: 0.0409
trigger times: 24
Loss after 16417788 batches: 0.0415
trigger times: 25
Early stopping!
Start to test process.
Loss after 16418751 batches: 0.0440
Time to train on one home:  56.31036424636841
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16419714 batches: 0.0835
trigger times: 1
Loss after 16420677 batches: 0.0757
trigger times: 2
Loss after 16421640 batches: 0.0735
trigger times: 3
Loss after 16422603 batches: 0.0707
trigger times: 4
Loss after 16423566 batches: 0.0677
trigger times: 5
Loss after 16424529 batches: 0.0636
trigger times: 6
Loss after 16425492 batches: 0.0609
trigger times: 7
Loss after 16426455 batches: 0.0575
trigger times: 8
Loss after 16427418 batches: 0.0553
trigger times: 9
Loss after 16428381 batches: 0.0539
trigger times: 10
Loss after 16429344 batches: 0.0537
trigger times: 11
Loss after 16430307 batches: 0.0514
trigger times: 12
Loss after 16431270 batches: 0.0487
trigger times: 13
Loss after 16432233 batches: 0.0489
trigger times: 14
Loss after 16433196 batches: 0.0471
trigger times: 15
Loss after 16434159 batches: 0.0464
trigger times: 16
Loss after 16435122 batches: 0.0465
trigger times: 17
Loss after 16436085 batches: 0.0456
trigger times: 18
Loss after 16437048 batches: 0.0467
trigger times: 19
Loss after 16438011 batches: 0.0469
trigger times: 20
Loss after 16438974 batches: 0.0475
trigger times: 21
Loss after 16439937 batches: 0.0470
trigger times: 22
Loss after 16440900 batches: 0.0500
trigger times: 23
Loss after 16441863 batches: 0.0473
trigger times: 24
Loss after 16442826 batches: 0.0457
trigger times: 25
Early stopping!
Start to test process.
Loss after 16443789 batches: 0.0442
Time to train on one home:  58.776695251464844
trigger times: 0
Loss after 16444752 batches: 0.0867
trigger times: 1
Loss after 16445715 batches: 0.0637
trigger times: 2
Loss after 16446678 batches: 0.0567
trigger times: 3
Loss after 16447641 batches: 0.0503
trigger times: 4
Loss after 16448604 batches: 0.0470
trigger times: 5
Loss after 16449567 batches: 0.0440
trigger times: 6
Loss after 16450530 batches: 0.0426
trigger times: 7
Loss after 16451493 batches: 0.0414
trigger times: 8
Loss after 16452456 batches: 0.0404
trigger times: 9
Loss after 16453419 batches: 0.0395
trigger times: 10
Loss after 16454382 batches: 0.0383
trigger times: 11
Loss after 16455345 batches: 0.0385
trigger times: 12
Loss after 16456308 batches: 0.0375
trigger times: 13
Loss after 16457271 batches: 0.0369
trigger times: 14
Loss after 16458234 batches: 0.0364
trigger times: 15
Loss after 16459197 batches: 0.0361
trigger times: 16
Loss after 16460160 batches: 0.0354
trigger times: 17
Loss after 16461123 batches: 0.0360
trigger times: 18
Loss after 16462086 batches: 0.0354
trigger times: 19
Loss after 16463049 batches: 0.0356
trigger times: 20
Loss after 16464012 batches: 0.0336
trigger times: 21
Loss after 16464975 batches: 0.0338
trigger times: 22
Loss after 16465938 batches: 0.0344
trigger times: 23
Loss after 16466901 batches: 0.0333
trigger times: 24
Loss after 16467864 batches: 0.0347
trigger times: 25
Early stopping!
Start to test process.
Loss after 16468827 batches: 0.0355
Time to train on one home:  56.43252730369568
trigger times: 0
Loss after 16469790 batches: 0.0506
trigger times: 1
Loss after 16470753 batches: 0.0364
trigger times: 2
Loss after 16471716 batches: 0.0343
trigger times: 3
Loss after 16472679 batches: 0.0323
trigger times: 4
Loss after 16473642 batches: 0.0289
trigger times: 5
Loss after 16474605 batches: 0.0263
trigger times: 6
Loss after 16475568 batches: 0.0255
trigger times: 7
Loss after 16476531 batches: 0.0237
trigger times: 8
Loss after 16477494 batches: 0.0232
trigger times: 9
Loss after 16478457 batches: 0.0226
trigger times: 10
Loss after 16479420 batches: 0.0223
trigger times: 11
Loss after 16480383 batches: 0.0218
trigger times: 12
Loss after 16481346 batches: 0.0213
trigger times: 13
Loss after 16482309 batches: 0.0209
trigger times: 14
Loss after 16483272 batches: 0.0208
trigger times: 15
Loss after 16484235 batches: 0.0205
trigger times: 16
Loss after 16485198 batches: 0.0201
trigger times: 17
Loss after 16486161 batches: 0.0201
trigger times: 18
Loss after 16487124 batches: 0.0198
trigger times: 19
Loss after 16488087 batches: 0.0197
trigger times: 20
Loss after 16489050 batches: 0.0193
trigger times: 21
Loss after 16490013 batches: 0.0200
trigger times: 22
Loss after 16490976 batches: 0.0193
trigger times: 23
Loss after 16491939 batches: 0.0201
trigger times: 24
Loss after 16492902 batches: 0.0195
trigger times: 25
Early stopping!
Start to test process.
Loss after 16493865 batches: 0.0197
Time to train on one home:  56.877471685409546
trigger times: 0
Loss after 16494828 batches: 0.0624
trigger times: 1
Loss after 16495791 batches: 0.0460
trigger times: 2
Loss after 16496754 batches: 0.0461
trigger times: 3
Loss after 16497717 batches: 0.0439
trigger times: 4
Loss after 16498680 batches: 0.0416
trigger times: 5
Loss after 16499643 batches: 0.0394
trigger times: 6
Loss after 16500606 batches: 0.0383
trigger times: 7
Loss after 16501569 batches: 0.0376
trigger times: 8
Loss after 16502532 batches: 0.0363
trigger times: 9
Loss after 16503495 batches: 0.0359
trigger times: 10
Loss after 16504458 batches: 0.0357
trigger times: 11
Loss after 16505421 batches: 0.0355
trigger times: 12
Loss after 16506384 batches: 0.0347
trigger times: 13
Loss after 16507347 batches: 0.0350
trigger times: 14
Loss after 16508310 batches: 0.0344
trigger times: 15
Loss after 16509273 batches: 0.0337
trigger times: 16
Loss after 16510236 batches: 0.0333
trigger times: 17
Loss after 16511199 batches: 0.0334
trigger times: 18
Loss after 16512162 batches: 0.0326
trigger times: 19
Loss after 16513125 batches: 0.0327
trigger times: 20
Loss after 16514088 batches: 0.0328
trigger times: 21
Loss after 16515051 batches: 0.0327
trigger times: 22
Loss after 16516014 batches: 0.0320
trigger times: 23
Loss after 16516977 batches: 0.0320
trigger times: 24
Loss after 16517940 batches: 0.0322
trigger times: 25
Early stopping!
Start to test process.
Loss after 16518903 batches: 0.0317
Time to train on one home:  57.64030456542969
trigger times: 0
Loss after 16519798 batches: 0.0780
trigger times: 1
Loss after 16520693 batches: 0.0425
trigger times: 2
Loss after 16521588 batches: 0.0185
trigger times: 0
Loss after 16522483 batches: 0.0111
trigger times: 0
Loss after 16523378 batches: 0.0077
trigger times: 0
Loss after 16524273 batches: 0.0052
trigger times: 1
Loss after 16525168 batches: 0.0045
trigger times: 2
Loss after 16526063 batches: 0.0041
trigger times: 3
Loss after 16526958 batches: 0.0040
trigger times: 4
Loss after 16527853 batches: 0.0040
trigger times: 5
Loss after 16528748 batches: 0.0038
trigger times: 6
Loss after 16529643 batches: 0.0033
trigger times: 0
Loss after 16530538 batches: 0.0028
trigger times: 1
Loss after 16531433 batches: 0.0026
trigger times: 2
Loss after 16532328 batches: 0.0030
trigger times: 3
Loss after 16533223 batches: 0.0034
trigger times: 4
Loss after 16534118 batches: 0.0028
trigger times: 5
Loss after 16535013 batches: 0.0023
trigger times: 6
Loss after 16535908 batches: 0.0021
trigger times: 0
Loss after 16536803 batches: 0.0019
trigger times: 0
Loss after 16537698 batches: 0.0021
trigger times: 1
Loss after 16538593 batches: 0.0022
trigger times: 2
Loss after 16539488 batches: 0.0018
trigger times: 3
Loss after 16540383 batches: 0.0019
trigger times: 4
Loss after 16541278 batches: 0.0016
trigger times: 5
Loss after 16542173 batches: 0.0017
trigger times: 6
Loss after 16543068 batches: 0.0017
trigger times: 7
Loss after 16543963 batches: 0.0018
trigger times: 8
Loss after 16544858 batches: 0.0023
trigger times: 9
Loss after 16545753 batches: 0.0024
trigger times: 10
Loss after 16546648 batches: 0.0020
trigger times: 11
Loss after 16547543 batches: 0.0018
trigger times: 12
Loss after 16548438 batches: 0.0016
trigger times: 13
Loss after 16549333 batches: 0.0014
trigger times: 14
Loss after 16550228 batches: 0.0015
trigger times: 15
Loss after 16551123 batches: 0.0014
trigger times: 16
Loss after 16552018 batches: 0.0014
trigger times: 17
Loss after 16552913 batches: 0.0014
trigger times: 18
Loss after 16553808 batches: 0.0019
trigger times: 19
Loss after 16554703 batches: 0.0019
trigger times: 0
Loss after 16555598 batches: 0.0020
trigger times: 1
Loss after 16556493 batches: 0.0018
trigger times: 2
Loss after 16557388 batches: 0.0015
trigger times: 3
Loss after 16558283 batches: 0.0015
trigger times: 4
Loss after 16559178 batches: 0.0023
trigger times: 5
Loss after 16560073 batches: 0.0023
trigger times: 6
Loss after 16560968 batches: 0.0020
trigger times: 7
Loss after 16561863 batches: 0.0017
trigger times: 8
Loss after 16562758 batches: 0.0015
trigger times: 9
Loss after 16563653 batches: 0.0014
trigger times: 0
Loss after 16564548 batches: 0.0015
trigger times: 1
Loss after 16565443 batches: 0.0014
trigger times: 2
Loss after 16566338 batches: 0.0012
trigger times: 3
Loss after 16567233 batches: 0.0012
trigger times: 4
Loss after 16568128 batches: 0.0011
trigger times: 5
Loss after 16569023 batches: 0.0014
trigger times: 6
Loss after 16569918 batches: 0.0011
trigger times: 7
Loss after 16570813 batches: 0.0011
trigger times: 8
Loss after 16571708 batches: 0.0012
trigger times: 9
Loss after 16572603 batches: 0.0014
trigger times: 10
Loss after 16573498 batches: 0.0013
trigger times: 11
Loss after 16574393 batches: 0.0012
trigger times: 12
Loss after 16575288 batches: 0.0016
trigger times: 13
Loss after 16576183 batches: 0.0016
trigger times: 14
Loss after 16577078 batches: 0.0012
trigger times: 15
Loss after 16577973 batches: 0.0013
trigger times: 16
Loss after 16578868 batches: 0.0031
trigger times: 17
Loss after 16579763 batches: 0.0040
trigger times: 18
Loss after 16580658 batches: 0.0031
trigger times: 19
Loss after 16581553 batches: 0.0029
trigger times: 20
Loss after 16582448 batches: 0.0024
trigger times: 21
Loss after 16583343 batches: 0.0018
trigger times: 22
Loss after 16584238 batches: 0.0019
trigger times: 23
Loss after 16585133 batches: 0.0016
trigger times: 24
Loss after 16586028 batches: 0.0016
trigger times: 25
Early stopping!
Start to test process.
Loss after 16586923 batches: 0.0014
Time to train on one home:  92.26777958869934
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336, 0.034888222413376094, 0.034613204319843105, 0.033508375671873286, 0.03325774057242148, 0.03243952582662181, 0.032562873532530595, 0.03126130941140662]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171], [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71], [0.13675299286842346, 0.15565351472302724, 0.32920502623266784, 1.059933101560365, 0.6549021202780799, 35.010519125969424, 3184.605], [0.0885479673743248, 0.1559912263273432, 0.33358461589199145, 0.944871497166479, 0.654640173496829, 31.209933508474723, 3183.3313], [0.1345348209142685, 0.15444619808844395, 0.3263722552493196, 1.0544752388510252, 0.6558385527073367, 34.830241138150235, 3189.1584], [0.09016867727041245, 0.15349941943138035, 0.3319629557497865, 0.9480173077546179, 0.65657289912243, 31.313842388762147, 3192.7297], [0.13274754583835602, 0.1683317466130475, 0.33338956387040825, 1.0413414780800958, 0.645068476656334, 34.39642151124207, 3136.7866], [0.08685638755559921, 0.16053285093432457, 0.33813218961147645, 0.938309926482314, 0.6511175433777052, 30.99319907910727, 3166.2017]]
Round_14_results:  [0.08685638755559921, 0.16053285093432457, 0.33813218961147645, 0.938309926482314, 0.6511175433777052, 30.99319907910727, 3166.2017]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 17385 < 17386; dropping {'Training_Loss': 0.05189421293990953, 'Validation_Loss': 0.0624321848154068, 'Training_R2': 0.3120688980730377, 'Validation_R2': 0.13972411120973216, 'Training_F1': 0.5167459276048655, 'Validation_F1': 0.3285913358585457, 'Training_NEP': 0.7634533457273215, 'Validation_NEP': 0.8819090287269857, 'Training_NDE': 0.5066299066161988, 'Validation_NDE': 0.6675722926498469, 'Training_MAE': 21.98376834872414, 'Validation_MAE': 29.376595984010827, 'Training_MSE': 1593.9491, 'Validation_MSE': 3306.7527}.
trigger times: 0
Loss after 16587886 batches: 0.0519
trigger times: 1
Loss after 16588849 batches: 0.0428
trigger times: 0
Loss after 16589812 batches: 0.0386
trigger times: 1
Loss after 16590775 batches: 0.0353
trigger times: 2
Loss after 16591738 batches: 0.0329
trigger times: 3
Loss after 16592701 batches: 0.0318
trigger times: 4
Loss after 16593664 batches: 0.0309
trigger times: 5
Loss after 16594627 batches: 0.0304
trigger times: 6
Loss after 16595590 batches: 0.0283
trigger times: 7
Loss after 16596553 batches: 0.0297
trigger times: 8
Loss after 16597516 batches: 0.0314
trigger times: 9
Loss after 16598479 batches: 0.0293
trigger times: 10
Loss after 16599442 batches: 0.0285
trigger times: 11
Loss after 16600405 batches: 0.0276
trigger times: 12
Loss after 16601368 batches: 0.0260
trigger times: 13
Loss after 16602331 batches: 0.0262
trigger times: 14
Loss after 16603294 batches: 0.0256
trigger times: 15
Loss after 16604257 batches: 0.0265
trigger times: 16
Loss after 16605220 batches: 0.0259
trigger times: 17
Loss after 16606183 batches: 0.0266
trigger times: 18
Loss after 16607146 batches: 0.0250
trigger times: 19
Loss after 16608109 batches: 0.0245
trigger times: 20
Loss after 16609072 batches: 0.0250
trigger times: 21
Loss after 16610035 batches: 0.0257
trigger times: 22
Loss after 16610998 batches: 0.0253
trigger times: 23
Loss after 16611961 batches: 0.0254
trigger times: 24
Loss after 16612924 batches: 0.0249
trigger times: 25
Early stopping!
Start to test process.
Loss after 16613887 batches: 0.0248
Time to train on one home:  55.632063150405884
trigger times: 0
Loss after 16614845 batches: 0.0579
trigger times: 0
Loss after 16615803 batches: 0.0388
trigger times: 1
Loss after 16616761 batches: 0.0309
trigger times: 2
Loss after 16617719 batches: 0.0297
trigger times: 3
Loss after 16618677 batches: 0.0283
trigger times: 4
Loss after 16619635 batches: 0.0246
trigger times: 5
Loss after 16620593 batches: 0.0225
trigger times: 6
Loss after 16621551 batches: 0.0209
trigger times: 7
Loss after 16622509 batches: 0.0195
trigger times: 8
Loss after 16623467 batches: 0.0201
trigger times: 9
Loss after 16624425 batches: 0.0183
trigger times: 10
Loss after 16625383 batches: 0.0178
trigger times: 11
Loss after 16626341 batches: 0.0179
trigger times: 12
Loss after 16627299 batches: 0.0173
trigger times: 13
Loss after 16628257 batches: 0.0166
trigger times: 14
Loss after 16629215 batches: 0.0174
trigger times: 0
Loss after 16630173 batches: 0.0159
trigger times: 1
Loss after 16631131 batches: 0.0156
trigger times: 2
Loss after 16632089 batches: 0.0155
trigger times: 3
Loss after 16633047 batches: 0.0148
trigger times: 4
Loss after 16634005 batches: 0.0153
trigger times: 5
Loss after 16634963 batches: 0.0159
trigger times: 6
Loss after 16635921 batches: 0.0143
trigger times: 7
Loss after 16636879 batches: 0.0153
trigger times: 8
Loss after 16637837 batches: 0.0151
trigger times: 9
Loss after 16638795 batches: 0.0141
trigger times: 10
Loss after 16639753 batches: 0.0147
trigger times: 11
Loss after 16640711 batches: 0.0155
trigger times: 12
Loss after 16641669 batches: 0.0158
trigger times: 13
Loss after 16642627 batches: 0.0168
trigger times: 14
Loss after 16643585 batches: 0.0199
trigger times: 15
Loss after 16644543 batches: 0.0180
trigger times: 16
Loss after 16645501 batches: 0.0166
trigger times: 17
Loss after 16646459 batches: 0.0158
trigger times: 18
Loss after 16647417 batches: 0.0151
trigger times: 19
Loss after 16648375 batches: 0.0155
trigger times: 20
Loss after 16649333 batches: 0.0140
trigger times: 21
Loss after 16650291 batches: 0.0143
trigger times: 22
Loss after 16651249 batches: 0.0143
trigger times: 23
Loss after 16652207 batches: 0.0151
trigger times: 24
Loss after 16653165 batches: 0.0144
trigger times: 25
Early stopping!
Start to test process.
Loss after 16654123 batches: 0.0140
Time to train on one home:  70.224600315094
trigger times: 0
Loss after 16655086 batches: 0.1052
trigger times: 1
Loss after 16656049 batches: 0.0726
trigger times: 2
Loss after 16657012 batches: 0.0674
trigger times: 3
Loss after 16657975 batches: 0.0670
trigger times: 4
Loss after 16658938 batches: 0.0654
trigger times: 5
Loss after 16659901 batches: 0.0625
trigger times: 6
Loss after 16660864 batches: 0.0590
trigger times: 7
Loss after 16661827 batches: 0.0567
trigger times: 8
Loss after 16662790 batches: 0.0542
trigger times: 9
Loss after 16663753 batches: 0.0534
trigger times: 10
Loss after 16664716 batches: 0.0519
trigger times: 11
Loss after 16665679 batches: 0.0504
trigger times: 12
Loss after 16666642 batches: 0.0492
trigger times: 13
Loss after 16667605 batches: 0.0473
trigger times: 14
Loss after 16668568 batches: 0.0468
trigger times: 15
Loss after 16669531 batches: 0.0467
trigger times: 16
Loss after 16670494 batches: 0.0459
trigger times: 17
Loss after 16671457 batches: 0.0465
trigger times: 18
Loss after 16672420 batches: 0.0455
trigger times: 19
Loss after 16673383 batches: 0.0447
trigger times: 20
Loss after 16674346 batches: 0.0437
trigger times: 21
Loss after 16675309 batches: 0.0450
trigger times: 22
Loss after 16676272 batches: 0.0449
trigger times: 23
Loss after 16677235 batches: 0.0447
trigger times: 24
Loss after 16678198 batches: 0.0439
trigger times: 25
Early stopping!
Start to test process.
Loss after 16679161 batches: 0.0429
Time to train on one home:  54.88635563850403
trigger times: 0
Loss after 16680124 batches: 0.1040
trigger times: 1
Loss after 16681087 batches: 0.0777
trigger times: 2
Loss after 16682050 batches: 0.0766
trigger times: 3
Loss after 16683013 batches: 0.0747
trigger times: 4
Loss after 16683976 batches: 0.0716
trigger times: 5
Loss after 16684939 batches: 0.0681
trigger times: 6
Loss after 16685902 batches: 0.0667
trigger times: 7
Loss after 16686865 batches: 0.0649
trigger times: 8
Loss after 16687828 batches: 0.0635
trigger times: 9
Loss after 16688791 batches: 0.0616
trigger times: 10
Loss after 16689754 batches: 0.0615
trigger times: 11
Loss after 16690717 batches: 0.0601
trigger times: 12
Loss after 16691680 batches: 0.0595
trigger times: 13
Loss after 16692643 batches: 0.0585
trigger times: 14
Loss after 16693606 batches: 0.0576
trigger times: 15
Loss after 16694569 batches: 0.0576
trigger times: 16
Loss after 16695532 batches: 0.0566
trigger times: 17
Loss after 16696495 batches: 0.0569
trigger times: 18
Loss after 16697458 batches: 0.0573
trigger times: 19
Loss after 16698421 batches: 0.0552
trigger times: 20
Loss after 16699384 batches: 0.0558
trigger times: 21
Loss after 16700347 batches: 0.0554
trigger times: 22
Loss after 16701310 batches: 0.0549
trigger times: 23
Loss after 16702273 batches: 0.0532
trigger times: 24
Loss after 16703236 batches: 0.0538
trigger times: 25
Early stopping!
Start to test process.
Loss after 16704199 batches: 0.0521
Time to train on one home:  60.36064147949219
trigger times: 0
Loss after 16705162 batches: 0.0246
trigger times: 1
Loss after 16706125 batches: 0.0198
trigger times: 2
Loss after 16707088 batches: 0.0184
trigger times: 3
Loss after 16708051 batches: 0.0165
trigger times: 4
Loss after 16709014 batches: 0.0152
trigger times: 5
Loss after 16709977 batches: 0.0144
trigger times: 6
Loss after 16710940 batches: 0.0145
trigger times: 7
Loss after 16711903 batches: 0.0137
trigger times: 8
Loss after 16712866 batches: 0.0134
trigger times: 9
Loss after 16713829 batches: 0.0130
trigger times: 10
Loss after 16714792 batches: 0.0131
trigger times: 11
Loss after 16715755 batches: 0.0127
trigger times: 12
Loss after 16716718 batches: 0.0122
trigger times: 13
Loss after 16717681 batches: 0.0123
trigger times: 14
Loss after 16718644 batches: 0.0128
trigger times: 15
Loss after 16719607 batches: 0.0122
trigger times: 16
Loss after 16720570 batches: 0.0119
trigger times: 17
Loss after 16721533 batches: 0.0123
trigger times: 18
Loss after 16722496 batches: 0.0120
trigger times: 19
Loss after 16723459 batches: 0.0118
trigger times: 20
Loss after 16724422 batches: 0.0122
trigger times: 21
Loss after 16725385 batches: 0.0120
trigger times: 22
Loss after 16726348 batches: 0.0113
trigger times: 23
Loss after 16727311 batches: 0.0115
trigger times: 24
Loss after 16728274 batches: 0.0115
trigger times: 25
Early stopping!
Start to test process.
Loss after 16729237 batches: 0.0109
Time to train on one home:  56.84664702415466
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16730200 batches: 0.0924
trigger times: 1
Loss after 16731163 batches: 0.0278
trigger times: 2
Loss after 16732126 batches: 0.0197
trigger times: 3
Loss after 16733089 batches: 0.0191
trigger times: 4
Loss after 16734052 batches: 0.0178
trigger times: 5
Loss after 16735015 batches: 0.0157
trigger times: 6
Loss after 16735978 batches: 0.0149
trigger times: 7
Loss after 16736941 batches: 0.0141
trigger times: 8
Loss after 16737904 batches: 0.0139
trigger times: 9
Loss after 16738867 batches: 0.0134
trigger times: 10
Loss after 16739830 batches: 0.0134
trigger times: 11
Loss after 16740793 batches: 0.0131
trigger times: 12
Loss after 16741756 batches: 0.0130
trigger times: 13
Loss after 16742719 batches: 0.0127
trigger times: 14
Loss after 16743682 batches: 0.0126
trigger times: 15
Loss after 16744645 batches: 0.0129
trigger times: 16
Loss after 16745608 batches: 0.0122
trigger times: 17
Loss after 16746571 batches: 0.0122
trigger times: 18
Loss after 16747534 batches: 0.0119
trigger times: 19
Loss after 16748497 batches: 0.0121
trigger times: 20
Loss after 16749460 batches: 0.0122
trigger times: 21
Loss after 16750423 batches: 0.0119
trigger times: 22
Loss after 16751386 batches: 0.0119
trigger times: 23
Loss after 16752349 batches: 0.0119
trigger times: 24
Loss after 16753312 batches: 0.0117
trigger times: 25
Early stopping!
Start to test process.
Loss after 16754275 batches: 0.0116
Time to train on one home:  56.77764654159546
trigger times: 0
Loss after 16755238 batches: 0.0948
trigger times: 1
Loss after 16756201 batches: 0.0873
trigger times: 0
Loss after 16757164 batches: 0.0822
trigger times: 1
Loss after 16758127 batches: 0.0784
trigger times: 2
Loss after 16759090 batches: 0.0739
trigger times: 3
Loss after 16760053 batches: 0.0726
trigger times: 4
Loss after 16761016 batches: 0.0694
trigger times: 5
Loss after 16761979 batches: 0.0683
trigger times: 6
Loss after 16762942 batches: 0.0666
trigger times: 7
Loss after 16763905 batches: 0.0648
trigger times: 8
Loss after 16764868 batches: 0.0644
trigger times: 9
Loss after 16765831 batches: 0.0643
trigger times: 10
Loss after 16766794 batches: 0.0628
trigger times: 11
Loss after 16767757 batches: 0.0621
trigger times: 12
Loss after 16768720 batches: 0.0624
trigger times: 13
Loss after 16769683 batches: 0.0589
trigger times: 14
Loss after 16770646 batches: 0.0614
trigger times: 15
Loss after 16771609 batches: 0.0597
trigger times: 16
Loss after 16772572 batches: 0.0602
trigger times: 17
Loss after 16773535 batches: 0.0602
trigger times: 18
Loss after 16774498 batches: 0.0610
trigger times: 19
Loss after 16775461 batches: 0.0612
trigger times: 20
Loss after 16776424 batches: 0.0584
trigger times: 21
Loss after 16777387 batches: 0.0584
trigger times: 22
Loss after 16778350 batches: 0.0591
trigger times: 23
Loss after 16779313 batches: 0.0564
trigger times: 24
Loss after 16780276 batches: 0.0563
trigger times: 25
Early stopping!
Start to test process.
Loss after 16781239 batches: 0.0579
Time to train on one home:  59.27867937088013
trigger times: 0
Loss after 16782202 batches: 0.0590
trigger times: 1
Loss after 16783165 batches: 0.0438
trigger times: 0
Loss after 16784128 batches: 0.0324
trigger times: 1
Loss after 16785091 batches: 0.0305
trigger times: 2
Loss after 16786054 batches: 0.0263
trigger times: 3
Loss after 16787017 batches: 0.0256
trigger times: 4
Loss after 16787980 batches: 0.0231
trigger times: 5
Loss after 16788943 batches: 0.0230
trigger times: 6
Loss after 16789906 batches: 0.0216
trigger times: 7
Loss after 16790869 batches: 0.0214
trigger times: 8
Loss after 16791832 batches: 0.0210
trigger times: 9
Loss after 16792795 batches: 0.0207
trigger times: 10
Loss after 16793758 batches: 0.0199
trigger times: 11
Loss after 16794721 batches: 0.0197
trigger times: 12
Loss after 16795684 batches: 0.0213
trigger times: 13
Loss after 16796647 batches: 0.0198
trigger times: 14
Loss after 16797610 batches: 0.0203
trigger times: 15
Loss after 16798573 batches: 0.0202
trigger times: 16
Loss after 16799536 batches: 0.0191
trigger times: 17
Loss after 16800499 batches: 0.0190
trigger times: 18
Loss after 16801462 batches: 0.0188
trigger times: 19
Loss after 16802425 batches: 0.0191
trigger times: 20
Loss after 16803388 batches: 0.0188
trigger times: 21
Loss after 16804351 batches: 0.0177
trigger times: 22
Loss after 16805314 batches: 0.0180
trigger times: 23
Loss after 16806277 batches: 0.0167
trigger times: 24
Loss after 16807240 batches: 0.0174
trigger times: 25
Early stopping!
Start to test process.
Loss after 16808203 batches: 0.0170
Time to train on one home:  59.04665470123291
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16809166 batches: 0.0861
trigger times: 1
Loss after 16810129 batches: 0.0725
trigger times: 2
Loss after 16811092 batches: 0.0697
trigger times: 3
Loss after 16812055 batches: 0.0668
trigger times: 4
Loss after 16813018 batches: 0.0648
trigger times: 5
Loss after 16813981 batches: 0.0615
trigger times: 6
Loss after 16814944 batches: 0.0593
trigger times: 7
Loss after 16815907 batches: 0.0591
trigger times: 8
Loss after 16816870 batches: 0.0566
trigger times: 9
Loss after 16817833 batches: 0.0550
trigger times: 10
Loss after 16818796 batches: 0.0565
trigger times: 11
Loss after 16819759 batches: 0.0556
trigger times: 12
Loss after 16820722 batches: 0.0554
trigger times: 13
Loss after 16821685 batches: 0.0558
trigger times: 14
Loss after 16822648 batches: 0.0546
trigger times: 15
Loss after 16823611 batches: 0.0535
trigger times: 16
Loss after 16824574 batches: 0.0534
trigger times: 17
Loss after 16825537 batches: 0.0540
trigger times: 18
Loss after 16826500 batches: 0.0533
trigger times: 19
Loss after 16827463 batches: 0.0531
trigger times: 20
Loss after 16828426 batches: 0.0525
trigger times: 21
Loss after 16829389 batches: 0.0529
trigger times: 22
Loss after 16830352 batches: 0.0517
trigger times: 23
Loss after 16831315 batches: 0.0509
trigger times: 24
Loss after 16832278 batches: 0.0509
trigger times: 25
Early stopping!
Start to test process.
Loss after 16833241 batches: 0.0502
Time to train on one home:  56.227421045303345
trigger times: 0
Loss after 16834204 batches: 0.0898
trigger times: 0
Loss after 16835167 batches: 0.0595
trigger times: 0
Loss after 16836130 batches: 0.0547
trigger times: 1
Loss after 16837093 batches: 0.0495
trigger times: 2
Loss after 16838056 batches: 0.0472
trigger times: 3
Loss after 16839019 batches: 0.0455
trigger times: 4
Loss after 16839982 batches: 0.0439
trigger times: 5
Loss after 16840945 batches: 0.0425
trigger times: 6
Loss after 16841908 batches: 0.0416
trigger times: 7
Loss after 16842871 batches: 0.0411
trigger times: 8
Loss after 16843834 batches: 0.0425
trigger times: 9
Loss after 16844797 batches: 0.0403
trigger times: 10
Loss after 16845760 batches: 0.0399
trigger times: 11
Loss after 16846723 batches: 0.0400
trigger times: 12
Loss after 16847686 batches: 0.0404
trigger times: 13
Loss after 16848649 batches: 0.0389
trigger times: 14
Loss after 16849612 batches: 0.0377
trigger times: 15
Loss after 16850575 batches: 0.0376
trigger times: 16
Loss after 16851538 batches: 0.0377
trigger times: 17
Loss after 16852501 batches: 0.0387
trigger times: 18
Loss after 16853464 batches: 0.0376
trigger times: 19
Loss after 16854427 batches: 0.0368
trigger times: 20
Loss after 16855390 batches: 0.0364
trigger times: 21
Loss after 16856353 batches: 0.0366
trigger times: 22
Loss after 16857316 batches: 0.0358
trigger times: 23
Loss after 16858279 batches: 0.0357
trigger times: 24
Loss after 16859242 batches: 0.0356
trigger times: 25
Early stopping!
Start to test process.
Loss after 16860205 batches: 0.0356
Time to train on one home:  54.97467851638794
trigger times: 0
Loss after 16861168 batches: 0.0743
trigger times: 1
Loss after 16862131 batches: 0.0676
trigger times: 2
Loss after 16863094 batches: 0.0657
trigger times: 3
Loss after 16864057 batches: 0.0614
trigger times: 4
Loss after 16865020 batches: 0.0609
trigger times: 5
Loss after 16865983 batches: 0.0590
trigger times: 6
Loss after 16866946 batches: 0.0577
trigger times: 7
Loss after 16867909 batches: 0.0556
trigger times: 8
Loss after 16868872 batches: 0.0535
trigger times: 9
Loss after 16869835 batches: 0.0539
trigger times: 10
Loss after 16870798 batches: 0.0535
trigger times: 11
Loss after 16871761 batches: 0.0529
trigger times: 12
Loss after 16872724 batches: 0.0514
trigger times: 13
Loss after 16873687 batches: 0.0508
trigger times: 14
Loss after 16874650 batches: 0.0524
trigger times: 15
Loss after 16875613 batches: 0.0528
trigger times: 16
Loss after 16876576 batches: 0.0504
trigger times: 17
Loss after 16877539 batches: 0.0513
trigger times: 18
Loss after 16878502 batches: 0.0512
trigger times: 19
Loss after 16879465 batches: 0.0499
trigger times: 20
Loss after 16880428 batches: 0.0497
trigger times: 21
Loss after 16881391 batches: 0.0499
trigger times: 22
Loss after 16882354 batches: 0.0482
trigger times: 23
Loss after 16883317 batches: 0.0493
trigger times: 24
Loss after 16884280 batches: 0.0470
trigger times: 25
Early stopping!
Start to test process.
Loss after 16885243 batches: 0.0487
Time to train on one home:  57.99572134017944
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16886206 batches: 0.0659
trigger times: 1
Loss after 16887169 batches: 0.0497
trigger times: 2
Loss after 16888132 batches: 0.0459
trigger times: 3
Loss after 16889095 batches: 0.0410
trigger times: 4
Loss after 16890058 batches: 0.0372
trigger times: 5
Loss after 16891021 batches: 0.0342
trigger times: 6
Loss after 16891984 batches: 0.0323
trigger times: 7
Loss after 16892947 batches: 0.0315
trigger times: 8
Loss after 16893910 batches: 0.0295
trigger times: 9
Loss after 16894873 batches: 0.0290
trigger times: 10
Loss after 16895836 batches: 0.0275
trigger times: 11
Loss after 16896799 batches: 0.0273
trigger times: 12
Loss after 16897762 batches: 0.0262
trigger times: 13
Loss after 16898725 batches: 0.0260
trigger times: 14
Loss after 16899688 batches: 0.0251
trigger times: 15
Loss after 16900651 batches: 0.0261
trigger times: 16
Loss after 16901614 batches: 0.0256
trigger times: 17
Loss after 16902577 batches: 0.0253
trigger times: 18
Loss after 16903540 batches: 0.0245
trigger times: 19
Loss after 16904503 batches: 0.0240
trigger times: 20
Loss after 16905466 batches: 0.0237
trigger times: 21
Loss after 16906429 batches: 0.0236
trigger times: 22
Loss after 16907392 batches: 0.0233
trigger times: 23
Loss after 16908355 batches: 0.0227
trigger times: 24
Loss after 16909318 batches: 0.0237
trigger times: 25
Early stopping!
Start to test process.
Loss after 16910281 batches: 0.0224
Time to train on one home:  57.132917642593384
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16911244 batches: 0.0756
trigger times: 0
Loss after 16912207 batches: 0.0504
trigger times: 1
Loss after 16913170 batches: 0.0413
trigger times: 0
Loss after 16914133 batches: 0.0361
trigger times: 1
Loss after 16915096 batches: 0.0327
trigger times: 2
Loss after 16916059 batches: 0.0302
trigger times: 3
Loss after 16917022 batches: 0.0279
trigger times: 4
Loss after 16917985 batches: 0.0255
trigger times: 5
Loss after 16918948 batches: 0.0236
trigger times: 6
Loss after 16919911 batches: 0.0234
trigger times: 7
Loss after 16920874 batches: 0.0234
trigger times: 0
Loss after 16921837 batches: 0.0227
trigger times: 1
Loss after 16922800 batches: 0.0222
trigger times: 2
Loss after 16923763 batches: 0.0227
trigger times: 0
Loss after 16924726 batches: 0.0205
trigger times: 0
Loss after 16925689 batches: 0.0216
trigger times: 1
Loss after 16926652 batches: 0.0208
trigger times: 2
Loss after 16927615 batches: 0.0218
trigger times: 3
Loss after 16928578 batches: 0.0209
trigger times: 4
Loss after 16929541 batches: 0.0223
trigger times: 5
Loss after 16930504 batches: 0.0193
trigger times: 6
Loss after 16931467 batches: 0.0206
trigger times: 7
Loss after 16932430 batches: 0.0196
trigger times: 8
Loss after 16933393 batches: 0.0195
trigger times: 9
Loss after 16934356 batches: 0.0185
trigger times: 10
Loss after 16935319 batches: 0.0182
trigger times: 11
Loss after 16936282 batches: 0.0184
trigger times: 12
Loss after 16937245 batches: 0.0217
trigger times: 13
Loss after 16938208 batches: 0.0215
trigger times: 14
Loss after 16939171 batches: 0.0199
trigger times: 15
Loss after 16940134 batches: 0.0187
trigger times: 16
Loss after 16941097 batches: 0.0184
trigger times: 17
Loss after 16942060 batches: 0.0168
trigger times: 18
Loss after 16943023 batches: 0.0168
trigger times: 19
Loss after 16943986 batches: 0.0172
trigger times: 20
Loss after 16944949 batches: 0.0177
trigger times: 21
Loss after 16945912 batches: 0.0160
trigger times: 0
Loss after 16946875 batches: 0.0178
trigger times: 1
Loss after 16947838 batches: 0.0168
trigger times: 2
Loss after 16948801 batches: 0.0159
trigger times: 3
Loss after 16949764 batches: 0.0173
trigger times: 4
Loss after 16950727 batches: 0.0162
trigger times: 5
Loss after 16951690 batches: 0.0164
trigger times: 6
Loss after 16952653 batches: 0.0159
trigger times: 7
Loss after 16953616 batches: 0.0148
trigger times: 8
Loss after 16954579 batches: 0.0151
trigger times: 9
Loss after 16955542 batches: 0.0147
trigger times: 10
Loss after 16956505 batches: 0.0146
trigger times: 11
Loss after 16957468 batches: 0.0152
trigger times: 12
Loss after 16958431 batches: 0.0155
trigger times: 13
Loss after 16959394 batches: 0.0150
trigger times: 14
Loss after 16960357 batches: 0.0164
trigger times: 15
Loss after 16961320 batches: 0.0174
trigger times: 16
Loss after 16962283 batches: 0.0158
trigger times: 17
Loss after 16963246 batches: 0.0151
trigger times: 18
Loss after 16964209 batches: 0.0144
trigger times: 19
Loss after 16965172 batches: 0.0141
trigger times: 20
Loss after 16966135 batches: 0.0139
trigger times: 21
Loss after 16967098 batches: 0.0140
trigger times: 22
Loss after 16968061 batches: 0.0171
trigger times: 23
Loss after 16969024 batches: 0.0164
trigger times: 24
Loss after 16969987 batches: 0.0177
trigger times: 25
Early stopping!
Start to test process.
Loss after 16970950 batches: 0.0177
Time to train on one home:  87.05091071128845
trigger times: 0
Loss after 16971879 batches: 0.0894
trigger times: 0
Loss after 16972808 batches: 0.0596
trigger times: 0
Loss after 16973737 batches: 0.0441
trigger times: 0
Loss after 16974666 batches: 0.0370
trigger times: 1
Loss after 16975595 batches: 0.0357
trigger times: 0
Loss after 16976524 batches: 0.0312
trigger times: 1
Loss after 16977453 batches: 0.0310
trigger times: 2
Loss after 16978382 batches: 0.0266
trigger times: 3
Loss after 16979311 batches: 0.0278
trigger times: 4
Loss after 16980240 batches: 0.0271
trigger times: 5
Loss after 16981169 batches: 0.0276
trigger times: 6
Loss after 16982098 batches: 0.0259
trigger times: 7
Loss after 16983027 batches: 0.0239
trigger times: 8
Loss after 16983956 batches: 0.0236
trigger times: 9
Loss after 16984885 batches: 0.0248
trigger times: 10
Loss after 16985814 batches: 0.0231
trigger times: 11
Loss after 16986743 batches: 0.0231
trigger times: 12
Loss after 16987672 batches: 0.0228
trigger times: 13
Loss after 16988601 batches: 0.0235
trigger times: 14
Loss after 16989530 batches: 0.0236
trigger times: 15
Loss after 16990459 batches: 0.0225
trigger times: 16
Loss after 16991388 batches: 0.0229
trigger times: 17
Loss after 16992317 batches: 0.0240
trigger times: 18
Loss after 16993246 batches: 0.0246
trigger times: 19
Loss after 16994175 batches: 0.0254
trigger times: 20
Loss after 16995104 batches: 0.0274
trigger times: 21
Loss after 16996033 batches: 0.0275
trigger times: 22
Loss after 16996962 batches: 0.0249
trigger times: 23
Loss after 16997891 batches: 0.0241
trigger times: 24
Loss after 16998820 batches: 0.0237
trigger times: 25
Early stopping!
Start to test process.
Loss after 16999749 batches: 0.0232
Time to train on one home:  60.62975192070007
trigger times: 0
Loss after 17000711 batches: 0.0807
trigger times: 1
Loss after 17001673 batches: 0.0650
trigger times: 2
Loss after 17002635 batches: 0.0642
trigger times: 3
Loss after 17003597 batches: 0.0609
trigger times: 4
Loss after 17004559 batches: 0.0575
trigger times: 5
Loss after 17005521 batches: 0.0561
trigger times: 6
Loss after 17006483 batches: 0.0544
trigger times: 7
Loss after 17007445 batches: 0.0542
trigger times: 8
Loss after 17008407 batches: 0.0533
trigger times: 9
Loss after 17009369 batches: 0.0532
trigger times: 10
Loss after 17010331 batches: 0.0518
trigger times: 11
Loss after 17011293 batches: 0.0519
trigger times: 12
Loss after 17012255 batches: 0.0522
trigger times: 13
Loss after 17013217 batches: 0.0509
trigger times: 14
Loss after 17014179 batches: 0.0501
trigger times: 15
Loss after 17015141 batches: 0.0500
trigger times: 16
Loss after 17016103 batches: 0.0507
trigger times: 17
Loss after 17017065 batches: 0.0500
trigger times: 18
Loss after 17018027 batches: 0.0488
trigger times: 19
Loss after 17018989 batches: 0.0485
trigger times: 20
Loss after 17019951 batches: 0.0491
trigger times: 21
Loss after 17020913 batches: 0.0484
trigger times: 22
Loss after 17021875 batches: 0.0484
trigger times: 23
Loss after 17022837 batches: 0.0480
trigger times: 24
Loss after 17023799 batches: 0.0483
trigger times: 25
Early stopping!
Start to test process.
Loss after 17024761 batches: 0.0474
Time to train on one home:  56.60797071456909
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17025724 batches: 0.0544
trigger times: 1
Loss after 17026687 batches: 0.0435
trigger times: 2
Loss after 17027650 batches: 0.0439
trigger times: 3
Loss after 17028613 batches: 0.0414
trigger times: 4
Loss after 17029576 batches: 0.0378
trigger times: 5
Loss after 17030539 batches: 0.0365
trigger times: 6
Loss after 17031502 batches: 0.0353
trigger times: 7
Loss after 17032465 batches: 0.0337
trigger times: 8
Loss after 17033428 batches: 0.0329
trigger times: 9
Loss after 17034391 batches: 0.0322
trigger times: 10
Loss after 17035354 batches: 0.0319
trigger times: 11
Loss after 17036317 batches: 0.0316
trigger times: 12
Loss after 17037280 batches: 0.0316
trigger times: 13
Loss after 17038243 batches: 0.0314
trigger times: 14
Loss after 17039206 batches: 0.0308
trigger times: 15
Loss after 17040169 batches: 0.0304
trigger times: 16
Loss after 17041132 batches: 0.0298
trigger times: 17
Loss after 17042095 batches: 0.0295
trigger times: 18
Loss after 17043058 batches: 0.0297
trigger times: 19
Loss after 17044021 batches: 0.0294
trigger times: 20
Loss after 17044984 batches: 0.0291
trigger times: 21
Loss after 17045947 batches: 0.0286
trigger times: 22
Loss after 17046910 batches: 0.0288
trigger times: 23
Loss after 17047873 batches: 0.0288
trigger times: 24
Loss after 17048836 batches: 0.0283
trigger times: 25
Early stopping!
Start to test process.
Loss after 17049799 batches: 0.0283
Time to train on one home:  57.4587299823761
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17050762 batches: 0.0467
trigger times: 1
Loss after 17051725 batches: 0.0420
trigger times: 0
Loss after 17052688 batches: 0.0396
trigger times: 1
Loss after 17053651 batches: 0.0377
trigger times: 2
Loss after 17054614 batches: 0.0357
trigger times: 3
Loss after 17055577 batches: 0.0343
trigger times: 4
Loss after 17056540 batches: 0.0337
trigger times: 5
Loss after 17057503 batches: 0.0328
trigger times: 6
Loss after 17058466 batches: 0.0328
trigger times: 7
Loss after 17059429 batches: 0.0324
trigger times: 8
Loss after 17060392 batches: 0.0312
trigger times: 9
Loss after 17061355 batches: 0.0309
trigger times: 10
Loss after 17062318 batches: 0.0300
trigger times: 11
Loss after 17063281 batches: 0.0292
trigger times: 12
Loss after 17064244 batches: 0.0282
trigger times: 13
Loss after 17065207 batches: 0.0287
trigger times: 14
Loss after 17066170 batches: 0.0283
trigger times: 15
Loss after 17067133 batches: 0.0272
trigger times: 16
Loss after 17068096 batches: 0.0283
trigger times: 17
Loss after 17069059 batches: 0.0283
trigger times: 18
Loss after 17070022 batches: 0.0271
trigger times: 19
Loss after 17070985 batches: 0.0275
trigger times: 20
Loss after 17071948 batches: 0.0267
trigger times: 21
Loss after 17072911 batches: 0.0257
trigger times: 22
Loss after 17073874 batches: 0.0259
trigger times: 23
Loss after 17074837 batches: 0.0256
trigger times: 24
Loss after 17075800 batches: 0.0264
trigger times: 25
Early stopping!
Start to test process.
Loss after 17076763 batches: 0.0263
Time to train on one home:  59.35210967063904
trigger times: 0
Loss after 17077726 batches: 0.0969
trigger times: 1
Loss after 17078689 batches: 0.0908
trigger times: 2
Loss after 17079652 batches: 0.0871
trigger times: 3
Loss after 17080615 batches: 0.0840
trigger times: 4
Loss after 17081578 batches: 0.0807
trigger times: 5
Loss after 17082541 batches: 0.0784
trigger times: 6
Loss after 17083504 batches: 0.0775
trigger times: 7
Loss after 17084467 batches: 0.0756
trigger times: 8
Loss after 17085430 batches: 0.0744
trigger times: 9
Loss after 17086393 batches: 0.0724
trigger times: 10
Loss after 17087356 batches: 0.0722
trigger times: 11
Loss after 17088319 batches: 0.0711
trigger times: 12
Loss after 17089282 batches: 0.0687
trigger times: 13
Loss after 17090245 batches: 0.0685
trigger times: 14
Loss after 17091208 batches: 0.0700
trigger times: 15
Loss after 17092171 batches: 0.0689
trigger times: 16
Loss after 17093134 batches: 0.0682
trigger times: 17
Loss after 17094097 batches: 0.0660
trigger times: 18
Loss after 17095060 batches: 0.0647
trigger times: 19
Loss after 17096023 batches: 0.0660
trigger times: 20
Loss after 17096986 batches: 0.0649
trigger times: 21
Loss after 17097949 batches: 0.0636
trigger times: 22
Loss after 17098912 batches: 0.0613
trigger times: 23
Loss after 17099875 batches: 0.0656
trigger times: 24
Loss after 17100838 batches: 0.0663
trigger times: 25
Early stopping!
Start to test process.
Loss after 17101801 batches: 0.0643
Time to train on one home:  55.72119474411011
trigger times: 0
Loss after 17102764 batches: 0.0880
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 17103727 batches: 0.0517
trigger times: 2
Loss after 17104690 batches: 0.0515
trigger times: 3
Loss after 17105653 batches: 0.0454
trigger times: 4
Loss after 17106616 batches: 0.0404
trigger times: 5
Loss after 17107579 batches: 0.0371
trigger times: 6
Loss after 17108542 batches: 0.0358
trigger times: 7
Loss after 17109505 batches: 0.0338
trigger times: 8
Loss after 17110468 batches: 0.0337
trigger times: 9
Loss after 17111431 batches: 0.0316
trigger times: 10
Loss after 17112394 batches: 0.0309
trigger times: 11
Loss after 17113357 batches: 0.0301
trigger times: 12
Loss after 17114320 batches: 0.0299
trigger times: 13
Loss after 17115283 batches: 0.0295
trigger times: 14
Loss after 17116246 batches: 0.0295
trigger times: 15
Loss after 17117209 batches: 0.0292
trigger times: 16
Loss after 17118172 batches: 0.0292
trigger times: 17
Loss after 17119135 batches: 0.0281
trigger times: 18
Loss after 17120098 batches: 0.0275
trigger times: 19
Loss after 17121061 batches: 0.0274
trigger times: 20
Loss after 17122024 batches: 0.0267
trigger times: 21
Loss after 17122987 batches: 0.0264
trigger times: 22
Loss after 17123950 batches: 0.0260
trigger times: 23
Loss after 17124913 batches: 0.0268
trigger times: 24
Loss after 17125876 batches: 0.0248
trigger times: 25
Early stopping!
Start to test process.
Loss after 17126839 batches: 0.0248
Time to train on one home:  52.76407051086426
trigger times: 0
Loss after 17127798 batches: 0.0890
trigger times: 1
Loss after 17128757 batches: 0.0450
trigger times: 0
Loss after 17129716 batches: 0.0332
trigger times: 0
Loss after 17130675 batches: 0.0280
trigger times: 1
Loss after 17131634 batches: 0.0261
trigger times: 2
Loss after 17132593 batches: 0.0229
trigger times: 0
Loss after 17133552 batches: 0.0233
trigger times: 0
Loss after 17134511 batches: 0.0213
trigger times: 1
Loss after 17135470 batches: 0.0200
trigger times: 2
Loss after 17136429 batches: 0.0190
trigger times: 3
Loss after 17137388 batches: 0.0192
trigger times: 4
Loss after 17138347 batches: 0.0188
trigger times: 5
Loss after 17139306 batches: 0.0182
trigger times: 6
Loss after 17140265 batches: 0.0179
trigger times: 7
Loss after 17141224 batches: 0.0175
trigger times: 8
Loss after 17142183 batches: 0.0169
trigger times: 9
Loss after 17143142 batches: 0.0160
trigger times: 10
Loss after 17144101 batches: 0.0160
trigger times: 11
Loss after 17145060 batches: 0.0158
trigger times: 12
Loss after 17146019 batches: 0.0152
trigger times: 13
Loss after 17146978 batches: 0.0157
trigger times: 14
Loss after 17147937 batches: 0.0159
trigger times: 15
Loss after 17148896 batches: 0.0154
trigger times: 16
Loss after 17149855 batches: 0.0156
trigger times: 17
Loss after 17150814 batches: 0.0147
trigger times: 18
Loss after 17151773 batches: 0.0152
trigger times: 19
Loss after 17152732 batches: 0.0148
trigger times: 20
Loss after 17153691 batches: 0.0140
trigger times: 21
Loss after 17154650 batches: 0.0144
trigger times: 22
Loss after 17155609 batches: 0.0134
trigger times: 23
Loss after 17156568 batches: 0.0141
trigger times: 24
Loss after 17157527 batches: 0.0134
trigger times: 25
Early stopping!
Start to test process.
Loss after 17158486 batches: 0.0162
Time to train on one home:  63.60976696014404
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17159449 batches: 0.0722
trigger times: 1
Loss after 17160412 batches: 0.0318
trigger times: 2
Loss after 17161375 batches: 0.0263
trigger times: 3
Loss after 17162338 batches: 0.0260
trigger times: 4
Loss after 17163301 batches: 0.0248
trigger times: 5
Loss after 17164264 batches: 0.0243
trigger times: 6
Loss after 17165227 batches: 0.0234
trigger times: 7
Loss after 17166190 batches: 0.0220
trigger times: 8
Loss after 17167153 batches: 0.0213
trigger times: 9
Loss after 17168116 batches: 0.0201
trigger times: 10
Loss after 17169079 batches: 0.0196
trigger times: 11
Loss after 17170042 batches: 0.0193
trigger times: 12
Loss after 17171005 batches: 0.0191
trigger times: 13
Loss after 17171968 batches: 0.0187
trigger times: 14
Loss after 17172931 batches: 0.0186
trigger times: 15
Loss after 17173894 batches: 0.0179
trigger times: 16
Loss after 17174857 batches: 0.0183
trigger times: 17
Loss after 17175820 batches: 0.0177
trigger times: 18
Loss after 17176783 batches: 0.0175
trigger times: 19
Loss after 17177746 batches: 0.0174
trigger times: 20
Loss after 17178709 batches: 0.0174
trigger times: 21
Loss after 17179672 batches: 0.0172
trigger times: 22
Loss after 17180635 batches: 0.0174
trigger times: 23
Loss after 17181598 batches: 0.0175
trigger times: 24
Loss after 17182561 batches: 0.0171
trigger times: 25
Early stopping!
Start to test process.
Loss after 17183524 batches: 0.0168
Time to train on one home:  56.76488542556763
trigger times: 0
Loss after 17184469 batches: 0.0650
trigger times: 0
Loss after 17185414 batches: 0.0462
trigger times: 0
Loss after 17186359 batches: 0.0342
trigger times: 1
Loss after 17187304 batches: 0.0313
trigger times: 2
Loss after 17188249 batches: 0.0286
trigger times: 3
Loss after 17189194 batches: 0.0262
trigger times: 4
Loss after 17190139 batches: 0.0244
trigger times: 5
Loss after 17191084 batches: 0.0239
trigger times: 6
Loss after 17192029 batches: 0.0244
trigger times: 7
Loss after 17192974 batches: 0.0221
trigger times: 8
Loss after 17193919 batches: 0.0214
trigger times: 9
Loss after 17194864 batches: 0.0213
trigger times: 10
Loss after 17195809 batches: 0.0204
trigger times: 11
Loss after 17196754 batches: 0.0205
trigger times: 12
Loss after 17197699 batches: 0.0200
trigger times: 13
Loss after 17198644 batches: 0.0186
trigger times: 14
Loss after 17199589 batches: 0.0185
trigger times: 15
Loss after 17200534 batches: 0.0182
trigger times: 16
Loss after 17201479 batches: 0.0195
trigger times: 17
Loss after 17202424 batches: 0.0194
trigger times: 18
Loss after 17203369 batches: 0.0191
trigger times: 19
Loss after 17204314 batches: 0.0186
trigger times: 20
Loss after 17205259 batches: 0.0192
trigger times: 21
Loss after 17206204 batches: 0.0192
trigger times: 22
Loss after 17207149 batches: 0.0179
trigger times: 23
Loss after 17208094 batches: 0.0178
trigger times: 24
Loss after 17209039 batches: 0.0186
trigger times: 25
Early stopping!
Start to test process.
Loss after 17209984 batches: 0.0184
Time to train on one home:  57.083064556121826
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17210921 batches: 0.0826
trigger times: 1
Loss after 17211858 batches: 0.0678
trigger times: 2
Loss after 17212795 batches: 0.0645
trigger times: 3
Loss after 17213732 batches: 0.0615
trigger times: 4
Loss after 17214669 batches: 0.0585
trigger times: 5
Loss after 17215606 batches: 0.0568
trigger times: 6
Loss after 17216543 batches: 0.0541
trigger times: 7
Loss after 17217480 batches: 0.0546
trigger times: 8
Loss after 17218417 batches: 0.0534
trigger times: 9
Loss after 17219354 batches: 0.0523
trigger times: 10
Loss after 17220291 batches: 0.0520
trigger times: 11
Loss after 17221228 batches: 0.0495
trigger times: 12
Loss after 17222165 batches: 0.0510
trigger times: 13
Loss after 17223102 batches: 0.0498
trigger times: 14
Loss after 17224039 batches: 0.0502
trigger times: 15
Loss after 17224976 batches: 0.0493
trigger times: 16
Loss after 17225913 batches: 0.0497
trigger times: 17
Loss after 17226850 batches: 0.0481
trigger times: 18
Loss after 17227787 batches: 0.0482
trigger times: 19
Loss after 17228724 batches: 0.0474
trigger times: 20
Loss after 17229661 batches: 0.0483
trigger times: 21
Loss after 17230598 batches: 0.0481
trigger times: 22
Loss after 17231535 batches: 0.0471
trigger times: 23
Loss after 17232472 batches: 0.0462
trigger times: 24
Loss after 17233409 batches: 0.0475
trigger times: 25
Early stopping!
Start to test process.
Loss after 17234346 batches: 0.0477
Time to train on one home:  60.08047080039978
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 17235309 batches: 0.0527
trigger times: 1
Loss after 17236272 batches: 0.0179
trigger times: 2
Loss after 17237235 batches: 0.0142
trigger times: 3
Loss after 17238198 batches: 0.0140
trigger times: 4
Loss after 17239161 batches: 0.0139
trigger times: 5
Loss after 17240124 batches: 0.0139
trigger times: 6
Loss after 17241087 batches: 0.0139
trigger times: 7
Loss after 17242050 batches: 0.0138
trigger times: 8
Loss after 17243013 batches: 0.0138
trigger times: 9
Loss after 17243976 batches: 0.0140
trigger times: 10
Loss after 17244939 batches: 0.0138
trigger times: 11
Loss after 17245902 batches: 0.0138
trigger times: 12
Loss after 17246865 batches: 0.0137
trigger times: 13
Loss after 17247828 batches: 0.0139
trigger times: 14
Loss after 17248791 batches: 0.0138
trigger times: 15
Loss after 17249754 batches: 0.0137
trigger times: 16
Loss after 17250717 batches: 0.0139
trigger times: 17
Loss after 17251680 batches: 0.0139
trigger times: 18
Loss after 17252643 batches: 0.0139
trigger times: 19
Loss after 17253606 batches: 0.0138
trigger times: 20
Loss after 17254569 batches: 0.0139
trigger times: 21
Loss after 17255532 batches: 0.0138
trigger times: 22
Loss after 17256495 batches: 0.0139
trigger times: 23
Loss after 17257458 batches: 0.0140
trigger times: 24
Loss after 17258421 batches: 0.0139
trigger times: 25
Early stopping!
Start to test process.
Loss after 17259384 batches: 0.0139
Time to train on one home:  57.26987266540527
trigger times: 0
Loss after 17260347 batches: 0.0959
trigger times: 1
Loss after 17261310 batches: 0.0783
trigger times: 2
Loss after 17262273 batches: 0.0725
trigger times: 3
Loss after 17263236 batches: 0.0663
trigger times: 4
Loss after 17264199 batches: 0.0640
trigger times: 5
Loss after 17265162 batches: 0.0610
trigger times: 6
Loss after 17266125 batches: 0.0605
trigger times: 7
Loss after 17267088 batches: 0.0586
trigger times: 8
Loss after 17268051 batches: 0.0572
trigger times: 9
Loss after 17269014 batches: 0.0566
trigger times: 10
Loss after 17269977 batches: 0.0571
trigger times: 11
Loss after 17270940 batches: 0.0547
trigger times: 12
Loss after 17271903 batches: 0.0547
trigger times: 13
Loss after 17272866 batches: 0.0554
trigger times: 14
Loss after 17273829 batches: 0.0557
trigger times: 15
Loss after 17274792 batches: 0.0546
trigger times: 16
Loss after 17275755 batches: 0.0534
trigger times: 17
Loss after 17276718 batches: 0.0528
trigger times: 18
Loss after 17277681 batches: 0.0528
trigger times: 19
Loss after 17278644 batches: 0.0509
trigger times: 20
Loss after 17279607 batches: 0.0519
trigger times: 21
Loss after 17280570 batches: 0.0515
trigger times: 22
Loss after 17281533 batches: 0.0499
trigger times: 23
Loss after 17282496 batches: 0.0500
trigger times: 24
Loss after 17283459 batches: 0.0514
trigger times: 25
Early stopping!
Start to test process.
Loss after 17284422 batches: 0.0508
Time to train on one home:  57.03830075263977
trigger times: 0
Loss after 17285385 batches: 0.0759
trigger times: 1
Loss after 17286348 batches: 0.0502
trigger times: 2
Loss after 17287311 batches: 0.0472
trigger times: 3
Loss after 17288274 batches: 0.0428
trigger times: 4
Loss after 17289237 batches: 0.0407
trigger times: 5
Loss after 17290200 batches: 0.0383
trigger times: 6
Loss after 17291163 batches: 0.0359
trigger times: 7
Loss after 17292126 batches: 0.0363
trigger times: 8
Loss after 17293089 batches: 0.0350
trigger times: 9
Loss after 17294052 batches: 0.0342
trigger times: 10
Loss after 17295015 batches: 0.0332
trigger times: 11
Loss after 17295978 batches: 0.0338
trigger times: 12
Loss after 17296941 batches: 0.0319
trigger times: 13
Loss after 17297904 batches: 0.0320
trigger times: 14
Loss after 17298867 batches: 0.0310
trigger times: 15
Loss after 17299830 batches: 0.0311
trigger times: 16
Loss after 17300793 batches: 0.0293
trigger times: 17
Loss after 17301756 batches: 0.0297
trigger times: 18
Loss after 17302719 batches: 0.0297
trigger times: 19
Loss after 17303682 batches: 0.0280
trigger times: 20
Loss after 17304645 batches: 0.0288
trigger times: 21
Loss after 17305608 batches: 0.0282
trigger times: 22
Loss after 17306571 batches: 0.0285
trigger times: 23
Loss after 17307534 batches: 0.0282
trigger times: 24
Loss after 17308497 batches: 0.0271
trigger times: 25
Early stopping!
Start to test process.
Loss after 17309460 batches: 0.0275
Time to train on one home:  57.759366512298584
trigger times: 0
Loss after 17310356 batches: 0.1036
trigger times: 1
Loss after 17311252 batches: 0.0908
trigger times: 2
Loss after 17312148 batches: 0.0841
trigger times: 3
Loss after 17313044 batches: 0.0784
trigger times: 4
Loss after 17313940 batches: 0.0741
trigger times: 5
Loss after 17314836 batches: 0.0694
trigger times: 6
Loss after 17315732 batches: 0.0658
trigger times: 7
Loss after 17316628 batches: 0.0626
trigger times: 8
Loss after 17317524 batches: 0.0623
trigger times: 9
Loss after 17318420 batches: 0.0622
trigger times: 10
Loss after 17319316 batches: 0.0605
trigger times: 11
Loss after 17320212 batches: 0.0585
trigger times: 12
Loss after 17321108 batches: 0.0596
trigger times: 13
Loss after 17322004 batches: 0.0580
trigger times: 14
Loss after 17322900 batches: 0.0567
trigger times: 15
Loss after 17323796 batches: 0.0564
trigger times: 16
Loss after 17324692 batches: 0.0557
trigger times: 17
Loss after 17325588 batches: 0.0544
trigger times: 18
Loss after 17326484 batches: 0.0557
trigger times: 19
Loss after 17327380 batches: 0.0585
trigger times: 20
Loss after 17328276 batches: 0.0614
trigger times: 21
Loss after 17329172 batches: 0.0569
trigger times: 22
Loss after 17330068 batches: 0.0550
trigger times: 23
Loss after 17330964 batches: 0.0542
trigger times: 24
Loss after 17331860 batches: 0.0542
trigger times: 25
Early stopping!
Start to test process.
Loss after 17332756 batches: 0.0529
Time to train on one home:  55.77322769165039
trigger times: 0
Loss after 17333719 batches: 0.1806
trigger times: 1
Loss after 17334682 batches: 0.1198
trigger times: 2
Loss after 17335645 batches: 0.0989
trigger times: 3
Loss after 17336608 batches: 0.0900
trigger times: 4
Loss after 17337571 batches: 0.0805
trigger times: 5
Loss after 17338534 batches: 0.0755
trigger times: 6
Loss after 17339497 batches: 0.0699
trigger times: 7
Loss after 17340460 batches: 0.0639
trigger times: 8
Loss after 17341423 batches: 0.0594
trigger times: 9
Loss after 17342386 batches: 0.0568
trigger times: 10
Loss after 17343349 batches: 0.0532
trigger times: 11
Loss after 17344312 batches: 0.0525
trigger times: 12
Loss after 17345275 batches: 0.0494
trigger times: 13
Loss after 17346238 batches: 0.0474
trigger times: 14
Loss after 17347201 batches: 0.0441
trigger times: 15
Loss after 17348164 batches: 0.0450
trigger times: 16
Loss after 17349127 batches: 0.0447
trigger times: 17
Loss after 17350090 batches: 0.0436
trigger times: 18
Loss after 17351053 batches: 0.0426
trigger times: 19
Loss after 17352016 batches: 0.0430
trigger times: 20
Loss after 17352979 batches: 0.0435
trigger times: 21
Loss after 17353942 batches: 0.0412
trigger times: 22
Loss after 17354905 batches: 0.0418
trigger times: 23
Loss after 17355868 batches: 0.0404
trigger times: 24
Loss after 17356831 batches: 0.0418
trigger times: 25
Early stopping!
Start to test process.
Loss after 17357794 batches: 0.0398
Time to train on one home:  53.603055238723755
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17358757 batches: 0.0892
trigger times: 1
Loss after 17359720 batches: 0.0760
trigger times: 2
Loss after 17360683 batches: 0.0745
trigger times: 3
Loss after 17361646 batches: 0.0697
trigger times: 4
Loss after 17362609 batches: 0.0654
trigger times: 5
Loss after 17363572 batches: 0.0616
trigger times: 6
Loss after 17364535 batches: 0.0613
trigger times: 7
Loss after 17365498 batches: 0.0574
trigger times: 8
Loss after 17366461 batches: 0.0543
trigger times: 9
Loss after 17367424 batches: 0.0522
trigger times: 10
Loss after 17368387 batches: 0.0524
trigger times: 11
Loss after 17369350 batches: 0.0507
trigger times: 12
Loss after 17370313 batches: 0.0520
trigger times: 13
Loss after 17371276 batches: 0.0523
trigger times: 14
Loss after 17372239 batches: 0.0487
trigger times: 15
Loss after 17373202 batches: 0.0470
trigger times: 16
Loss after 17374165 batches: 0.0457
trigger times: 17
Loss after 17375128 batches: 0.0455
trigger times: 18
Loss after 17376091 batches: 0.0443
trigger times: 19
Loss after 17377054 batches: 0.0435
trigger times: 20
Loss after 17378017 batches: 0.0421
trigger times: 21
Loss after 17378980 batches: 0.0430
trigger times: 22
Loss after 17379943 batches: 0.0429
trigger times: 23
Loss after 17380906 batches: 0.0417
trigger times: 24
Loss after 17381869 batches: 0.0431
trigger times: 25
Early stopping!
Start to test process.
Loss after 17382832 batches: 0.0427
Time to train on one home:  54.631932973861694
trigger times: 0
Loss after 17383795 batches: 0.0864
trigger times: 1
Loss after 17384758 batches: 0.0632
trigger times: 2
Loss after 17385721 batches: 0.0546
trigger times: 3
Loss after 17386684 batches: 0.0501
trigger times: 4
Loss after 17387647 batches: 0.0465
trigger times: 5
Loss after 17388610 batches: 0.0435
trigger times: 6
Loss after 17389573 batches: 0.0417
trigger times: 7
Loss after 17390536 batches: 0.0406
trigger times: 8
Loss after 17391499 batches: 0.0397
trigger times: 9
Loss after 17392462 batches: 0.0388
trigger times: 10
Loss after 17393425 batches: 0.0382
trigger times: 11
Loss after 17394388 batches: 0.0372
trigger times: 12
Loss after 17395351 batches: 0.0361
trigger times: 13
Loss after 17396314 batches: 0.0362
trigger times: 14
Loss after 17397277 batches: 0.0368
trigger times: 15
Loss after 17398240 batches: 0.0363
trigger times: 16
Loss after 17399203 batches: 0.0359
trigger times: 17
Loss after 17400166 batches: 0.0350
trigger times: 18
Loss after 17401129 batches: 0.0357
trigger times: 19
Loss after 17402092 batches: 0.0344
trigger times: 20
Loss after 17403055 batches: 0.0352
trigger times: 21
Loss after 17404018 batches: 0.0347
trigger times: 22
Loss after 17404981 batches: 0.0349
trigger times: 23
Loss after 17405944 batches: 0.0336
trigger times: 24
Loss after 17406907 batches: 0.0334
trigger times: 25
Early stopping!
Start to test process.
Loss after 17407870 batches: 0.0322
Time to train on one home:  57.31618332862854
trigger times: 0
Loss after 17408833 batches: 0.0429
trigger times: 1
Loss after 17409796 batches: 0.0335
trigger times: 2
Loss after 17410759 batches: 0.0319
trigger times: 3
Loss after 17411722 batches: 0.0300
trigger times: 4
Loss after 17412685 batches: 0.0265
trigger times: 5
Loss after 17413648 batches: 0.0250
trigger times: 6
Loss after 17414611 batches: 0.0240
trigger times: 7
Loss after 17415574 batches: 0.0228
trigger times: 8
Loss after 17416537 batches: 0.0223
trigger times: 9
Loss after 17417500 batches: 0.0217
trigger times: 10
Loss after 17418463 batches: 0.0208
trigger times: 11
Loss after 17419426 batches: 0.0214
trigger times: 12
Loss after 17420389 batches: 0.0213
trigger times: 13
Loss after 17421352 batches: 0.0206
trigger times: 14
Loss after 17422315 batches: 0.0204
trigger times: 15
Loss after 17423278 batches: 0.0200
trigger times: 16
Loss after 17424241 batches: 0.0201
trigger times: 17
Loss after 17425204 batches: 0.0206
trigger times: 18
Loss after 17426167 batches: 0.0203
trigger times: 19
Loss after 17427130 batches: 0.0195
trigger times: 20
Loss after 17428093 batches: 0.0187
trigger times: 21
Loss after 17429056 batches: 0.0194
trigger times: 22
Loss after 17430019 batches: 0.0195
trigger times: 23
Loss after 17430982 batches: 0.0206
trigger times: 24
Loss after 17431945 batches: 0.0195
trigger times: 25
Early stopping!
Start to test process.
Loss after 17432908 batches: 0.0189
Time to train on one home:  55.29086470603943
trigger times: 0
Loss after 17433871 batches: 0.0887
trigger times: 1
Loss after 17434834 batches: 0.0484
trigger times: 2
Loss after 17435797 batches: 0.0479
trigger times: 3
Loss after 17436760 batches: 0.0474
trigger times: 4
Loss after 17437723 batches: 0.0442
trigger times: 5
Loss after 17438686 batches: 0.0417
trigger times: 6
Loss after 17439649 batches: 0.0399
trigger times: 7
Loss after 17440612 batches: 0.0393
trigger times: 8
Loss after 17441575 batches: 0.0380
trigger times: 9
Loss after 17442538 batches: 0.0377
trigger times: 10
Loss after 17443501 batches: 0.0370
trigger times: 11
Loss after 17444464 batches: 0.0367
trigger times: 12
Loss after 17445427 batches: 0.0356
trigger times: 13
Loss after 17446390 batches: 0.0357
trigger times: 14
Loss after 17447353 batches: 0.0346
trigger times: 15
Loss after 17448316 batches: 0.0350
trigger times: 16
Loss after 17449279 batches: 0.0349
trigger times: 17
Loss after 17450242 batches: 0.0343
trigger times: 18
Loss after 17451205 batches: 0.0346
trigger times: 19
Loss after 17452168 batches: 0.0341
trigger times: 20
Loss after 17453131 batches: 0.0334
trigger times: 21
Loss after 17454094 batches: 0.0332
trigger times: 22
Loss after 17455057 batches: 0.0329
trigger times: 23
Loss after 17456020 batches: 0.0320
trigger times: 24
Loss after 17456983 batches: 0.0332
trigger times: 25
Early stopping!
Start to test process.
Loss after 17457946 batches: 0.0335
Time to train on one home:  57.47533464431763
trigger times: 0
Loss after 17458841 batches: 0.0653
trigger times: 1
Loss after 17459736 batches: 0.0318
trigger times: 0
Loss after 17460631 batches: 0.0118
trigger times: 1
Loss after 17461526 batches: 0.0081
trigger times: 2
Loss after 17462421 batches: 0.0062
trigger times: 3
Loss after 17463316 batches: 0.0213
trigger times: 4
Loss after 17464211 batches: 0.0220
trigger times: 5
Loss after 17465106 batches: 0.0135
trigger times: 6
Loss after 17466001 batches: 0.0066
trigger times: 7
Loss after 17466896 batches: 0.0053
trigger times: 8
Loss after 17467791 batches: 0.0058
trigger times: 9
Loss after 17468686 batches: 0.0060
trigger times: 10
Loss after 17469581 batches: 0.0051
trigger times: 11
Loss after 17470476 batches: 0.0041
trigger times: 12
Loss after 17471371 batches: 0.0043
trigger times: 13
Loss after 17472266 batches: 0.0033
trigger times: 14
Loss after 17473161 batches: 0.0034
trigger times: 15
Loss after 17474056 batches: 0.0031
trigger times: 16
Loss after 17474951 batches: 0.0030
trigger times: 17
Loss after 17475846 batches: 0.0029
trigger times: 18
Loss after 17476741 batches: 0.0027
trigger times: 19
Loss after 17477636 batches: 0.0024
trigger times: 20
Loss after 17478531 batches: 0.0025
trigger times: 21
Loss after 17479426 batches: 0.0021
trigger times: 22
Loss after 17480321 batches: 0.0024
trigger times: 23
Loss after 17481216 batches: 0.0022
trigger times: 24
Loss after 17482111 batches: 0.0024
trigger times: 25
Early stopping!
Start to test process.
Loss after 17483006 batches: 0.0025
Time to train on one home:  58.542038917541504
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336, 0.034888222413376094, 0.034613204319843105, 0.033508375671873286, 0.03325774057242148, 0.03243952582662181, 0.032562873532530595, 0.03126130941140662, 0.03133166822639105]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171], [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71], [0.13675299286842346, 0.15565351472302724, 0.32920502623266784, 1.059933101560365, 0.6549021202780799, 35.010519125969424, 3184.605], [0.0885479673743248, 0.1559912263273432, 0.33358461589199145, 0.944871497166479, 0.654640173496829, 31.209933508474723, 3183.3313], [0.1345348209142685, 0.15444619808844395, 0.3263722552493196, 1.0544752388510252, 0.6558385527073367, 34.830241138150235, 3189.1584], [0.09016867727041245, 0.15349941943138035, 0.3319629557497865, 0.9480173077546179, 0.65657289912243, 31.313842388762147, 3192.7297], [0.13274754583835602, 0.1683317466130475, 0.33338956387040825, 1.0413414780800958, 0.645068476656334, 34.39642151124207, 3136.7866], [0.08685638755559921, 0.16053285093432457, 0.33813218961147645, 0.938309926482314, 0.6511175433777052, 30.99319907910727, 3166.2017], [0.13567858934402466, 0.1592208654199878, 0.3293138590809072, 1.0581373625723918, 0.6521351625870662, 34.951204293654875, 3171.1501]]
Round_15_results:  [0.13567858934402466, 0.1592208654199878, 0.3293138590809072, 1.0581373625723918, 0.6521351625870662, 34.951204293654875, 3171.1501]
trigger times: 0
Loss after 17483969 batches: 0.0642
trigger times: 0
Loss after 17484932 batches: 0.0451
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 18322 < 18323; dropping {'Training_Loss': 0.06417767384222575, 'Validation_Loss': 0.07915351539850235, 'Training_R2': 0.31230930713677907, 'Validation_R2': 0.10722342941781615, 'Training_F1': 0.5074705716364397, 'Validation_F1': 0.2986275038992142, 'Training_NEP': 0.8199500622324657, 'Validation_NEP': 0.9287259232426421, 'Training_NDE': 0.506452855380236, 'Validation_NDE': 0.6927927564155276, 'Training_MAE': 23.610600865817116, 'Validation_MAE': 30.93607768860085, 'Training_MSE': 1593.3918, 'Validation_MSE': 3431.6794}.
trigger times: 0
Loss after 17485895 batches: 0.0420
trigger times: 1
Loss after 17486858 batches: 0.0371
trigger times: 2
Loss after 17487821 batches: 0.0345
trigger times: 3
Loss after 17488784 batches: 0.0317
trigger times: 4
Loss after 17489747 batches: 0.0305
trigger times: 5
Loss after 17490710 batches: 0.0295
trigger times: 6
Loss after 17491673 batches: 0.0314
trigger times: 7
Loss after 17492636 batches: 0.0306
trigger times: 8
Loss after 17493599 batches: 0.0301
trigger times: 9
Loss after 17494562 batches: 0.0290
trigger times: 10
Loss after 17495525 batches: 0.0281
trigger times: 11
Loss after 17496488 batches: 0.0272
trigger times: 12
Loss after 17497451 batches: 0.0281
trigger times: 13
Loss after 17498414 batches: 0.0281
trigger times: 14
Loss after 17499377 batches: 0.0291
trigger times: 15
Loss after 17500340 batches: 0.0268
trigger times: 16
Loss after 17501303 batches: 0.0255
trigger times: 17
Loss after 17502266 batches: 0.0252
trigger times: 18
Loss after 17503229 batches: 0.0245
trigger times: 19
Loss after 17504192 batches: 0.0245
trigger times: 20
Loss after 17505155 batches: 0.0241
trigger times: 21
Loss after 17506118 batches: 0.0245
trigger times: 22
Loss after 17507081 batches: 0.0245
trigger times: 23
Loss after 17508044 batches: 0.0252
trigger times: 24
Loss after 17509007 batches: 0.0252
trigger times: 25
Early stopping!
Start to test process.
Loss after 17509970 batches: 0.0252
Time to train on one home:  58.15824222564697
trigger times: 0
Loss after 17510928 batches: 0.0861
trigger times: 0
Loss after 17511886 batches: 0.0459
trigger times: 1
Loss after 17512844 batches: 0.0402
trigger times: 0
Loss after 17513802 batches: 0.0334
trigger times: 1
Loss after 17514760 batches: 0.0272
trigger times: 2
Loss after 17515718 batches: 0.0438
trigger times: 3
Loss after 17516676 batches: 0.0367
trigger times: 4
Loss after 17517634 batches: 0.0311
trigger times: 5
Loss after 17518592 batches: 0.0275
trigger times: 6
Loss after 17519550 batches: 0.0240
trigger times: 7
Loss after 17520508 batches: 0.0261
trigger times: 8
Loss after 17521466 batches: 0.0270
trigger times: 9
Loss after 17522424 batches: 0.0248
trigger times: 10
Loss after 17523382 batches: 0.0266
trigger times: 11
Loss after 17524340 batches: 0.0249
trigger times: 12
Loss after 17525298 batches: 0.0237
trigger times: 13
Loss after 17526256 batches: 0.0238
trigger times: 14
Loss after 17527214 batches: 0.0227
trigger times: 15
Loss after 17528172 batches: 0.0221
trigger times: 16
Loss after 17529130 batches: 0.0212
trigger times: 17
Loss after 17530088 batches: 0.0205
trigger times: 18
Loss after 17531046 batches: 0.0187
trigger times: 19
Loss after 17532004 batches: 0.0190
trigger times: 20
Loss after 17532962 batches: 0.0183
trigger times: 21
Loss after 17533920 batches: 0.0179
trigger times: 22
Loss after 17534878 batches: 0.0175
trigger times: 23
Loss after 17535836 batches: 0.0168
trigger times: 0
Loss after 17536794 batches: 0.0182
trigger times: 1
Loss after 17537752 batches: 0.0173
trigger times: 2
Loss after 17538710 batches: 0.0166
trigger times: 3
Loss after 17539668 batches: 0.0171
trigger times: 4
Loss after 17540626 batches: 0.0169
trigger times: 5
Loss after 17541584 batches: 0.0167
trigger times: 6
Loss after 17542542 batches: 0.0159
trigger times: 7
Loss after 17543500 batches: 0.0154
trigger times: 8
Loss after 17544458 batches: 0.0152
trigger times: 9
Loss after 17545416 batches: 0.0159
trigger times: 10
Loss after 17546374 batches: 0.0162
trigger times: 11
Loss after 17547332 batches: 0.0163
trigger times: 12
Loss after 17548290 batches: 0.0154
trigger times: 13
Loss after 17549248 batches: 0.0158
trigger times: 14
Loss after 17550206 batches: 0.0153
trigger times: 15
Loss after 17551164 batches: 0.0178
trigger times: 16
Loss after 17552122 batches: 0.0162
trigger times: 17
Loss after 17553080 batches: 0.0160
trigger times: 18
Loss after 17554038 batches: 0.0152
trigger times: 19
Loss after 17554996 batches: 0.0159
trigger times: 20
Loss after 17555954 batches: 0.0162
trigger times: 21
Loss after 17556912 batches: 0.0159
trigger times: 22
Loss after 17557870 batches: 0.0147
trigger times: 23
Loss after 17558828 batches: 0.0139
trigger times: 24
Loss after 17559786 batches: 0.0139
trigger times: 25
Early stopping!
Start to test process.
Loss after 17560744 batches: 0.0144
Time to train on one home:  79.78565859794617
trigger times: 0
Loss after 17561707 batches: 0.0860
trigger times: 1
Loss after 17562670 batches: 0.0698
trigger times: 2
Loss after 17563633 batches: 0.0663
trigger times: 3
Loss after 17564596 batches: 0.0653
trigger times: 4
Loss after 17565559 batches: 0.0633
trigger times: 5
Loss after 17566522 batches: 0.0608
trigger times: 6
Loss after 17567485 batches: 0.0578
trigger times: 7
Loss after 17568448 batches: 0.0557
trigger times: 8
Loss after 17569411 batches: 0.0541
trigger times: 9
Loss after 17570374 batches: 0.0519
trigger times: 10
Loss after 17571337 batches: 0.0493
trigger times: 11
Loss after 17572300 batches: 0.0489
trigger times: 12
Loss after 17573263 batches: 0.0486
trigger times: 13
Loss after 17574226 batches: 0.0484
trigger times: 14
Loss after 17575189 batches: 0.0475
trigger times: 15
Loss after 17576152 batches: 0.0457
trigger times: 16
Loss after 17577115 batches: 0.0465
trigger times: 17
Loss after 17578078 batches: 0.0450
trigger times: 18
Loss after 17579041 batches: 0.0457
trigger times: 19
Loss after 17580004 batches: 0.0447
trigger times: 20
Loss after 17580967 batches: 0.0443
trigger times: 21
Loss after 17581930 batches: 0.0439
trigger times: 22
Loss after 17582893 batches: 0.0430
trigger times: 23
Loss after 17583856 batches: 0.0421
trigger times: 24
Loss after 17584819 batches: 0.0420
trigger times: 25
Early stopping!
Start to test process.
Loss after 17585782 batches: 0.0420
Time to train on one home:  57.672407150268555
trigger times: 0
Loss after 17586745 batches: 0.0906
trigger times: 1
Loss after 17587708 batches: 0.0781
trigger times: 2
Loss after 17588671 batches: 0.0758
trigger times: 3
Loss after 17589634 batches: 0.0746
trigger times: 4
Loss after 17590597 batches: 0.0705
trigger times: 5
Loss after 17591560 batches: 0.0675
trigger times: 6
Loss after 17592523 batches: 0.0649
trigger times: 7
Loss after 17593486 batches: 0.0618
trigger times: 8
Loss after 17594449 batches: 0.0618
trigger times: 9
Loss after 17595412 batches: 0.0610
trigger times: 10
Loss after 17596375 batches: 0.0593
trigger times: 11
Loss after 17597338 batches: 0.0579
trigger times: 12
Loss after 17598301 batches: 0.0596
trigger times: 13
Loss after 17599264 batches: 0.0589
trigger times: 14
Loss after 17600227 batches: 0.0601
trigger times: 15
Loss after 17601190 batches: 0.0576
trigger times: 16
Loss after 17602153 batches: 0.0569
trigger times: 17
Loss after 17603116 batches: 0.0578
trigger times: 18
Loss after 17604079 batches: 0.0566
trigger times: 19
Loss after 17605042 batches: 0.0555
trigger times: 20
Loss after 17606005 batches: 0.0557
trigger times: 21
Loss after 17606968 batches: 0.0549
trigger times: 22
Loss after 17607931 batches: 0.0544
trigger times: 23
Loss after 17608894 batches: 0.0535
trigger times: 24
Loss after 17609857 batches: 0.0531
trigger times: 25
Early stopping!
Start to test process.
Loss after 17610820 batches: 0.0522
Time to train on one home:  54.57144808769226
trigger times: 0
Loss after 17611783 batches: 0.0264
trigger times: 1
Loss after 17612746 batches: 0.0208
trigger times: 2
Loss after 17613709 batches: 0.0192
trigger times: 3
Loss after 17614672 batches: 0.0169
trigger times: 4
Loss after 17615635 batches: 0.0162
trigger times: 5
Loss after 17616598 batches: 0.0154
trigger times: 6
Loss after 17617561 batches: 0.0143
trigger times: 7
Loss after 17618524 batches: 0.0137
trigger times: 8
Loss after 17619487 batches: 0.0135
trigger times: 9
Loss after 17620450 batches: 0.0140
trigger times: 10
Loss after 17621413 batches: 0.0137
trigger times: 11
Loss after 17622376 batches: 0.0136
trigger times: 12
Loss after 17623339 batches: 0.0130
trigger times: 13
Loss after 17624302 batches: 0.0128
trigger times: 14
Loss after 17625265 batches: 0.0120
trigger times: 15
Loss after 17626228 batches: 0.0118
trigger times: 16
Loss after 17627191 batches: 0.0122
trigger times: 17
Loss after 17628154 batches: 0.0120
trigger times: 18
Loss after 17629117 batches: 0.0121
trigger times: 19
Loss after 17630080 batches: 0.0120
trigger times: 20
Loss after 17631043 batches: 0.0119
trigger times: 21
Loss after 17632006 batches: 0.0117
trigger times: 22
Loss after 17632969 batches: 0.0115
trigger times: 23
Loss after 17633932 batches: 0.0119
trigger times: 24
Loss after 17634895 batches: 0.0119
trigger times: 25
Early stopping!
Start to test process.
Loss after 17635858 batches: 0.0116
Time to train on one home:  53.72264099121094
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17636821 batches: 0.0555
trigger times: 1
Loss after 17637784 batches: 0.0245
trigger times: 2
Loss after 17638747 batches: 0.0209
trigger times: 3
Loss after 17639710 batches: 0.0165
trigger times: 4
Loss after 17640673 batches: 0.0159
trigger times: 5
Loss after 17641636 batches: 0.0147
trigger times: 6
Loss after 17642599 batches: 0.0144
trigger times: 7
Loss after 17643562 batches: 0.0139
trigger times: 8
Loss after 17644525 batches: 0.0135
trigger times: 9
Loss after 17645488 batches: 0.0129
trigger times: 10
Loss after 17646451 batches: 0.0126
trigger times: 11
Loss after 17647414 batches: 0.0125
trigger times: 12
Loss after 17648377 batches: 0.0123
trigger times: 13
Loss after 17649340 batches: 0.0122
trigger times: 14
Loss after 17650303 batches: 0.0122
trigger times: 15
Loss after 17651266 batches: 0.0121
trigger times: 16
Loss after 17652229 batches: 0.0118
trigger times: 17
Loss after 17653192 batches: 0.0116
trigger times: 18
Loss after 17654155 batches: 0.0118
trigger times: 19
Loss after 17655118 batches: 0.0116
trigger times: 20
Loss after 17656081 batches: 0.0114
trigger times: 21
Loss after 17657044 batches: 0.0113
trigger times: 22
Loss after 17658007 batches: 0.0115
trigger times: 23
Loss after 17658970 batches: 0.0116
trigger times: 24
Loss after 17659933 batches: 0.0112
trigger times: 25
Early stopping!
Start to test process.
Loss after 17660896 batches: 0.0112
Time to train on one home:  57.99939513206482
trigger times: 0
Loss after 17661859 batches: 0.1011
trigger times: 0
Loss after 17662822 batches: 0.0909
trigger times: 0
Loss after 17663785 batches: 0.0855
trigger times: 1
Loss after 17664748 batches: 0.0799
trigger times: 2
Loss after 17665711 batches: 0.0743
trigger times: 3
Loss after 17666674 batches: 0.0727
trigger times: 4
Loss after 17667637 batches: 0.0694
trigger times: 5
Loss after 17668600 batches: 0.0670
trigger times: 6
Loss after 17669563 batches: 0.0672
trigger times: 7
Loss after 17670526 batches: 0.0660
trigger times: 8
Loss after 17671489 batches: 0.0626
trigger times: 9
Loss after 17672452 batches: 0.0643
trigger times: 10
Loss after 17673415 batches: 0.0627
trigger times: 11
Loss after 17674378 batches: 0.0628
trigger times: 12
Loss after 17675341 batches: 0.0609
trigger times: 13
Loss after 17676304 batches: 0.0598
trigger times: 14
Loss after 17677267 batches: 0.0600
trigger times: 15
Loss after 17678230 batches: 0.0605
trigger times: 16
Loss after 17679193 batches: 0.0599
trigger times: 17
Loss after 17680156 batches: 0.0746
trigger times: 0
Loss after 17681119 batches: 0.0743
trigger times: 1
Loss after 17682082 batches: 0.0697
trigger times: 2
Loss after 17683045 batches: 0.0668
trigger times: 3
Loss after 17684008 batches: 0.0656
trigger times: 4
Loss after 17684971 batches: 0.0628
trigger times: 5
Loss after 17685934 batches: 0.0614
trigger times: 6
Loss after 17686897 batches: 0.0607
trigger times: 7
Loss after 17687860 batches: 0.0597
trigger times: 8
Loss after 17688823 batches: 0.0586
trigger times: 9
Loss after 17689786 batches: 0.0569
trigger times: 10
Loss after 17690749 batches: 0.0564
trigger times: 11
Loss after 17691712 batches: 0.0574
trigger times: 12
Loss after 17692675 batches: 0.0560
trigger times: 13
Loss after 17693638 batches: 0.0563
trigger times: 14
Loss after 17694601 batches: 0.0547
trigger times: 15
Loss after 17695564 batches: 0.0561
trigger times: 16
Loss after 17696527 batches: 0.0557
trigger times: 17
Loss after 17697490 batches: 0.0552
trigger times: 18
Loss after 17698453 batches: 0.0553
trigger times: 19
Loss after 17699416 batches: 0.0542
trigger times: 20
Loss after 17700379 batches: 0.0532
trigger times: 21
Loss after 17701342 batches: 0.0534
trigger times: 22
Loss after 17702305 batches: 0.0551
trigger times: 23
Loss after 17703268 batches: 0.0529
trigger times: 24
Loss after 17704231 batches: 0.0506
trigger times: 25
Early stopping!
Start to test process.
Loss after 17705194 batches: 0.0520
Time to train on one home:  71.93933415412903
trigger times: 0
Loss after 17706157 batches: 0.0488
trigger times: 0
Loss after 17707120 batches: 0.0381
trigger times: 0
Loss after 17708083 batches: 0.0313
trigger times: 1
Loss after 17709046 batches: 0.0282
trigger times: 2
Loss after 17710009 batches: 0.0277
trigger times: 3
Loss after 17710972 batches: 0.0249
trigger times: 4
Loss after 17711935 batches: 0.0241
trigger times: 5
Loss after 17712898 batches: 0.0233
trigger times: 0
Loss after 17713861 batches: 0.0236
trigger times: 1
Loss after 17714824 batches: 0.0211
trigger times: 2
Loss after 17715787 batches: 0.0203
trigger times: 3
Loss after 17716750 batches: 0.0202
trigger times: 4
Loss after 17717713 batches: 0.0193
trigger times: 5
Loss after 17718676 batches: 0.0204
trigger times: 6
Loss after 17719639 batches: 0.0188
trigger times: 7
Loss after 17720602 batches: 0.0190
trigger times: 8
Loss after 17721565 batches: 0.0194
trigger times: 9
Loss after 17722528 batches: 0.0189
trigger times: 10
Loss after 17723491 batches: 0.0195
trigger times: 11
Loss after 17724454 batches: 0.0194
trigger times: 12
Loss after 17725417 batches: 0.0198
trigger times: 13
Loss after 17726380 batches: 0.0185
trigger times: 14
Loss after 17727343 batches: 0.0191
trigger times: 15
Loss after 17728306 batches: 0.0181
trigger times: 16
Loss after 17729269 batches: 0.0187
trigger times: 17
Loss after 17730232 batches: 0.0185
trigger times: 18
Loss after 17731195 batches: 0.0178
trigger times: 19
Loss after 17732158 batches: 0.0167
trigger times: 20
Loss after 17733121 batches: 0.0171
trigger times: 21
Loss after 17734084 batches: 0.0167
trigger times: 22
Loss after 17735047 batches: 0.0162
trigger times: 23
Loss after 17736010 batches: 0.0173
trigger times: 24
Loss after 17736973 batches: 0.0173
trigger times: 25
Early stopping!
Start to test process.
Loss after 17737936 batches: 0.0181
Time to train on one home:  65.7251296043396
trigger times: 0
Loss after 17738899 batches: 0.0769
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 17739862 batches: 0.0709
trigger times: 2
Loss after 17740825 batches: 0.0663
trigger times: 3
Loss after 17741788 batches: 0.0645
trigger times: 4
Loss after 17742751 batches: 0.0612
trigger times: 5
Loss after 17743714 batches: 0.0588
trigger times: 6
Loss after 17744677 batches: 0.0581
trigger times: 7
Loss after 17745640 batches: 0.0592
trigger times: 8
Loss after 17746603 batches: 0.0572
trigger times: 9
Loss after 17747566 batches: 0.0565
trigger times: 10
Loss after 17748529 batches: 0.0558
trigger times: 11
Loss after 17749492 batches: 0.0560
trigger times: 12
Loss after 17750455 batches: 0.0538
trigger times: 13
Loss after 17751418 batches: 0.0530
trigger times: 14
Loss after 17752381 batches: 0.0541
trigger times: 15
Loss after 17753344 batches: 0.0533
trigger times: 16
Loss after 17754307 batches: 0.0534
trigger times: 17
Loss after 17755270 batches: 0.0526
trigger times: 18
Loss after 17756233 batches: 0.0524
trigger times: 19
Loss after 17757196 batches: 0.0516
trigger times: 20
Loss after 17758159 batches: 0.0524
trigger times: 21
Loss after 17759122 batches: 0.0523
trigger times: 22
Loss after 17760085 batches: 0.0508
trigger times: 23
Loss after 17761048 batches: 0.0502
trigger times: 24
Loss after 17762011 batches: 0.0518
trigger times: 25
Early stopping!
Start to test process.
Loss after 17762974 batches: 0.0504
Time to train on one home:  56.72786784172058
trigger times: 0
Loss after 17763937 batches: 0.1060
trigger times: 0
Loss after 17764900 batches: 0.0636
trigger times: 0
Loss after 17765863 batches: 0.0606
trigger times: 1
Loss after 17766826 batches: 0.0516
trigger times: 2
Loss after 17767789 batches: 0.0495
trigger times: 3
Loss after 17768752 batches: 0.0470
trigger times: 4
Loss after 17769715 batches: 0.0448
trigger times: 5
Loss after 17770678 batches: 0.0431
trigger times: 6
Loss after 17771641 batches: 0.0413
trigger times: 7
Loss after 17772604 batches: 0.0403
trigger times: 8
Loss after 17773567 batches: 0.0414
trigger times: 9
Loss after 17774530 batches: 0.0397
trigger times: 10
Loss after 17775493 batches: 0.0386
trigger times: 11
Loss after 17776456 batches: 0.0390
trigger times: 12
Loss after 17777419 batches: 0.0388
trigger times: 13
Loss after 17778382 batches: 0.0392
trigger times: 14
Loss after 17779345 batches: 0.0377
trigger times: 15
Loss after 17780308 batches: 0.0381
trigger times: 16
Loss after 17781271 batches: 0.0375
trigger times: 17
Loss after 17782234 batches: 0.0365
trigger times: 18
Loss after 17783197 batches: 0.0376
trigger times: 19
Loss after 17784160 batches: 0.0362
trigger times: 20
Loss after 17785123 batches: 0.0358
trigger times: 21
Loss after 17786086 batches: 0.0377
trigger times: 22
Loss after 17787049 batches: 0.0368
trigger times: 23
Loss after 17788012 batches: 0.0370
trigger times: 24
Loss after 17788975 batches: 0.0356
trigger times: 25
Early stopping!
Start to test process.
Loss after 17789938 batches: 0.0368
Time to train on one home:  58.3263783454895
trigger times: 0
Loss after 17790901 batches: 0.0747
trigger times: 1
Loss after 17791864 batches: 0.0689
trigger times: 2
Loss after 17792827 batches: 0.0649
trigger times: 3
Loss after 17793790 batches: 0.0621
trigger times: 4
Loss after 17794753 batches: 0.0600
trigger times: 5
Loss after 17795716 batches: 0.0592
trigger times: 6
Loss after 17796679 batches: 0.0564
trigger times: 7
Loss after 17797642 batches: 0.0562
trigger times: 8
Loss after 17798605 batches: 0.0560
trigger times: 9
Loss after 17799568 batches: 0.0540
trigger times: 10
Loss after 17800531 batches: 0.0547
trigger times: 11
Loss after 17801494 batches: 0.0541
trigger times: 12
Loss after 17802457 batches: 0.0538
trigger times: 13
Loss after 17803420 batches: 0.0515
trigger times: 14
Loss after 17804383 batches: 0.0520
trigger times: 15
Loss after 17805346 batches: 0.0514
trigger times: 16
Loss after 17806309 batches: 0.0508
trigger times: 17
Loss after 17807272 batches: 0.0506
trigger times: 18
Loss after 17808235 batches: 0.0491
trigger times: 19
Loss after 17809198 batches: 0.0481
trigger times: 20
Loss after 17810161 batches: 0.0487
trigger times: 21
Loss after 17811124 batches: 0.0486
trigger times: 22
Loss after 17812087 batches: 0.0481
trigger times: 23
Loss after 17813050 batches: 0.0479
trigger times: 24
Loss after 17814013 batches: 0.0467
trigger times: 25
Early stopping!
Start to test process.
Loss after 17814976 batches: 0.0467
Time to train on one home:  57.57163643836975
trigger times: 0
Loss after 17815939 batches: 0.0631
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 17816902 batches: 0.0494
trigger times: 2
Loss after 17817865 batches: 0.0462
trigger times: 3
Loss after 17818828 batches: 0.0392
trigger times: 4
Loss after 17819791 batches: 0.0349
trigger times: 5
Loss after 17820754 batches: 0.0323
trigger times: 6
Loss after 17821717 batches: 0.0315
trigger times: 7
Loss after 17822680 batches: 0.0294
trigger times: 8
Loss after 17823643 batches: 0.0284
trigger times: 9
Loss after 17824606 batches: 0.0278
trigger times: 10
Loss after 17825569 batches: 0.0265
trigger times: 11
Loss after 17826532 batches: 0.0256
trigger times: 12
Loss after 17827495 batches: 0.0255
trigger times: 13
Loss after 17828458 batches: 0.0266
trigger times: 14
Loss after 17829421 batches: 0.0252
trigger times: 15
Loss after 17830384 batches: 0.0247
trigger times: 16
Loss after 17831347 batches: 0.0237
trigger times: 17
Loss after 17832310 batches: 0.0236
trigger times: 18
Loss after 17833273 batches: 0.0237
trigger times: 19
Loss after 17834236 batches: 0.0232
trigger times: 20
Loss after 17835199 batches: 0.0222
trigger times: 21
Loss after 17836162 batches: 0.0224
trigger times: 22
Loss after 17837125 batches: 0.0222
trigger times: 23
Loss after 17838088 batches: 0.0220
trigger times: 24
Loss after 17839051 batches: 0.0213
trigger times: 25
Early stopping!
Start to test process.
Loss after 17840014 batches: 0.0214
Time to train on one home:  57.25265622138977
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17840977 batches: 0.0906
trigger times: 0
Loss after 17841940 batches: 0.0547
trigger times: 1
Loss after 17842903 batches: 0.0432
trigger times: 0
Loss after 17843866 batches: 0.0397
trigger times: 0
Loss after 17844829 batches: 0.0360
trigger times: 1
Loss after 17845792 batches: 0.0297
trigger times: 2
Loss after 17846755 batches: 0.0266
trigger times: 3
Loss after 17847718 batches: 0.0272
trigger times: 4
Loss after 17848681 batches: 0.0265
trigger times: 5
Loss after 17849644 batches: 0.0243
trigger times: 6
Loss after 17850607 batches: 0.0236
trigger times: 7
Loss after 17851570 batches: 0.0219
trigger times: 8
Loss after 17852533 batches: 0.0264
trigger times: 9
Loss after 17853496 batches: 0.0253
trigger times: 10
Loss after 17854459 batches: 0.0238
trigger times: 11
Loss after 17855422 batches: 0.0233
trigger times: 12
Loss after 17856385 batches: 0.0214
trigger times: 13
Loss after 17857348 batches: 0.0200
trigger times: 14
Loss after 17858311 batches: 0.0197
trigger times: 15
Loss after 17859274 batches: 0.0192
trigger times: 16
Loss after 17860237 batches: 0.0190
trigger times: 17
Loss after 17861200 batches: 0.0184
trigger times: 18
Loss after 17862163 batches: 0.0193
trigger times: 19
Loss after 17863126 batches: 0.0198
trigger times: 20
Loss after 17864089 batches: 0.0195
trigger times: 21
Loss after 17865052 batches: 0.0186
trigger times: 22
Loss after 17866015 batches: 0.0180
trigger times: 23
Loss after 17866978 batches: 0.0175
trigger times: 24
Loss after 17867941 batches: 0.0189
trigger times: 25
Early stopping!
Start to test process.
Loss after 17868904 batches: 0.0186
Time to train on one home:  58.69329833984375
trigger times: 0
Loss after 17869833 batches: 0.1296
trigger times: 0
Loss after 17870762 batches: 0.0690
trigger times: 0
Loss after 17871691 batches: 0.0529
trigger times: 1
Loss after 17872620 batches: 0.0419
trigger times: 2
Loss after 17873549 batches: 0.0389
trigger times: 3
Loss after 17874478 batches: 0.0369
trigger times: 0
Loss after 17875407 batches: 0.0323
trigger times: 1
Loss after 17876336 batches: 0.0295
trigger times: 2
Loss after 17877265 batches: 0.0285
trigger times: 3
Loss after 17878194 batches: 0.0260
trigger times: 4
Loss after 17879123 batches: 0.0266
trigger times: 5
Loss after 17880052 batches: 0.0264
trigger times: 6
Loss after 17880981 batches: 0.0266
trigger times: 0
Loss after 17881910 batches: 0.0269
trigger times: 1
Loss after 17882839 batches: 0.0300
trigger times: 2
Loss after 17883768 batches: 0.0275
trigger times: 3
Loss after 17884697 batches: 0.0244
trigger times: 4
Loss after 17885626 batches: 0.0239
trigger times: 5
Loss after 17886555 batches: 0.0233
trigger times: 6
Loss after 17887484 batches: 0.0242
trigger times: 7
Loss after 17888413 batches: 0.0241
trigger times: 8
Loss after 17889342 batches: 0.0218
trigger times: 9
Loss after 17890271 batches: 0.0212
trigger times: 10
Loss after 17891200 batches: 0.0209
trigger times: 11
Loss after 17892129 batches: 0.0227
trigger times: 12
Loss after 17893058 batches: 0.0235
trigger times: 13
Loss after 17893987 batches: 0.0217
trigger times: 14
Loss after 17894916 batches: 0.0217
trigger times: 15
Loss after 17895845 batches: 0.0214
trigger times: 16
Loss after 17896774 batches: 0.0210
trigger times: 17
Loss after 17897703 batches: 0.0201
trigger times: 18
Loss after 17898632 batches: 0.0205
trigger times: 19
Loss after 17899561 batches: 0.0209
trigger times: 20
Loss after 17900490 batches: 0.0215
trigger times: 0
Loss after 17901419 batches: 0.0223
trigger times: 1
Loss after 17902348 batches: 0.0219
trigger times: 2
Loss after 17903277 batches: 0.0253
trigger times: 3
Loss after 17904206 batches: 0.0237
trigger times: 4
Loss after 17905135 batches: 0.0224
trigger times: 5
Loss after 17906064 batches: 0.0208
trigger times: 6
Loss after 17906993 batches: 0.0211
trigger times: 7
Loss after 17907922 batches: 0.0199
trigger times: 8
Loss after 17908851 batches: 0.0188
trigger times: 9
Loss after 17909780 batches: 0.0184
trigger times: 10
Loss after 17910709 batches: 0.0182
trigger times: 11
Loss after 17911638 batches: 0.0197
trigger times: 12
Loss after 17912567 batches: 0.0207
trigger times: 13
Loss after 17913496 batches: 0.0206
trigger times: 14
Loss after 17914425 batches: 0.0205
trigger times: 15
Loss after 17915354 batches: 0.0208
trigger times: 16
Loss after 17916283 batches: 0.0204
trigger times: 17
Loss after 17917212 batches: 0.0201
trigger times: 18
Loss after 17918141 batches: 0.0197
trigger times: 19
Loss after 17919070 batches: 0.0195
trigger times: 20
Loss after 17919999 batches: 0.0193
trigger times: 21
Loss after 17920928 batches: 0.0217
trigger times: 22
Loss after 17921857 batches: 0.0220
trigger times: 23
Loss after 17922786 batches: 0.0213
trigger times: 24
Loss after 17923715 batches: 0.0206
trigger times: 25
Early stopping!
Start to test process.
Loss after 17924644 batches: 0.0193
Time to train on one home:  80.77456307411194
trigger times: 0
Loss after 17925606 batches: 0.0730
trigger times: 1
Loss after 17926568 batches: 0.0643
trigger times: 2
Loss after 17927530 batches: 0.0637
trigger times: 3
Loss after 17928492 batches: 0.0595
trigger times: 4
Loss after 17929454 batches: 0.0570
trigger times: 5
Loss after 17930416 batches: 0.0552
trigger times: 6
Loss after 17931378 batches: 0.0552
trigger times: 7
Loss after 17932340 batches: 0.0546
trigger times: 8
Loss after 17933302 batches: 0.0532
trigger times: 9
Loss after 17934264 batches: 0.0523
trigger times: 10
Loss after 17935226 batches: 0.0518
trigger times: 11
Loss after 17936188 batches: 0.0519
trigger times: 12
Loss after 17937150 batches: 0.0512
trigger times: 13
Loss after 17938112 batches: 0.0509
trigger times: 14
Loss after 17939074 batches: 0.0502
trigger times: 15
Loss after 17940036 batches: 0.0504
trigger times: 16
Loss after 17940998 batches: 0.0495
trigger times: 17
Loss after 17941960 batches: 0.0503
trigger times: 18
Loss after 17942922 batches: 0.0493
trigger times: 19
Loss after 17943884 batches: 0.0497
trigger times: 20
Loss after 17944846 batches: 0.0498
trigger times: 21
Loss after 17945808 batches: 0.0495
trigger times: 22
Loss after 17946770 batches: 0.0483
trigger times: 23
Loss after 17947732 batches: 0.0482
trigger times: 24
Loss after 17948694 batches: 0.0470
trigger times: 25
Early stopping!
Start to test process.
Loss after 17949656 batches: 0.0489
Time to train on one home:  57.201658487319946
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17950619 batches: 0.0582
trigger times: 1
Loss after 17951582 batches: 0.0447
trigger times: 2
Loss after 17952545 batches: 0.0443
trigger times: 3
Loss after 17953508 batches: 0.0402
trigger times: 4
Loss after 17954471 batches: 0.0375
trigger times: 5
Loss after 17955434 batches: 0.0364
trigger times: 6
Loss after 17956397 batches: 0.0344
trigger times: 7
Loss after 17957360 batches: 0.0336
trigger times: 8
Loss after 17958323 batches: 0.0325
trigger times: 9
Loss after 17959286 batches: 0.0327
trigger times: 10
Loss after 17960249 batches: 0.0318
trigger times: 11
Loss after 17961212 batches: 0.0308
trigger times: 12
Loss after 17962175 batches: 0.0308
trigger times: 13
Loss after 17963138 batches: 0.0312
trigger times: 14
Loss after 17964101 batches: 0.0303
trigger times: 15
Loss after 17965064 batches: 0.0301
trigger times: 16
Loss after 17966027 batches: 0.0293
trigger times: 17
Loss after 17966990 batches: 0.0293
trigger times: 18
Loss after 17967953 batches: 0.0291
trigger times: 19
Loss after 17968916 batches: 0.0282
trigger times: 20
Loss after 17969879 batches: 0.0293
trigger times: 21
Loss after 17970842 batches: 0.0281
trigger times: 22
Loss after 17971805 batches: 0.0279
trigger times: 23
Loss after 17972768 batches: 0.0282
trigger times: 24
Loss after 17973731 batches: 0.0279
trigger times: 25
Early stopping!
Start to test process.
Loss after 17974694 batches: 0.0282
Time to train on one home:  54.92841386795044
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17975657 batches: 0.0548
trigger times: 0
Loss after 17976620 batches: 0.0464
trigger times: 0
Loss after 17977583 batches: 0.0427
trigger times: 1
Loss after 17978546 batches: 0.0398
trigger times: 0
Loss after 17979509 batches: 0.0363
trigger times: 1
Loss after 17980472 batches: 0.0350
trigger times: 2
Loss after 17981435 batches: 0.0329
trigger times: 3
Loss after 17982398 batches: 0.0320
trigger times: 4
Loss after 17983361 batches: 0.0324
trigger times: 5
Loss after 17984324 batches: 0.0323
trigger times: 6
Loss after 17985287 batches: 0.0306
trigger times: 7
Loss after 17986250 batches: 0.0305
trigger times: 8
Loss after 17987213 batches: 0.0296
trigger times: 9
Loss after 17988176 batches: 0.0288
trigger times: 10
Loss after 17989139 batches: 0.0287
trigger times: 11
Loss after 17990102 batches: 0.0293
trigger times: 12
Loss after 17991065 batches: 0.0294
trigger times: 13
Loss after 17992028 batches: 0.0284
trigger times: 14
Loss after 17992991 batches: 0.0271
trigger times: 15
Loss after 17993954 batches: 0.0274
trigger times: 16
Loss after 17994917 batches: 0.0272
trigger times: 17
Loss after 17995880 batches: 0.0267
trigger times: 18
Loss after 17996843 batches: 0.0260
trigger times: 19
Loss after 17997806 batches: 0.0260
trigger times: 20
Loss after 17998769 batches: 0.0257
trigger times: 21
Loss after 17999732 batches: 0.0263
trigger times: 22
Loss after 18000695 batches: 0.0258
trigger times: 23
Loss after 18001658 batches: 0.0261
trigger times: 24
Loss after 18002621 batches: 0.0256
trigger times: 25
Early stopping!
Start to test process.
Loss after 18003584 batches: 0.0280
Time to train on one home:  62.26516604423523
trigger times: 0
Loss after 18004547 batches: 0.1094
trigger times: 0
Loss after 18005510 batches: 0.0937
trigger times: 1
Loss after 18006473 batches: 0.0893
trigger times: 2
Loss after 18007436 batches: 0.0853
trigger times: 3
Loss after 18008399 batches: 0.0817
trigger times: 4
Loss after 18009362 batches: 0.0792
trigger times: 5
Loss after 18010325 batches: 0.0771
trigger times: 6
Loss after 18011288 batches: 0.0756
trigger times: 7
Loss after 18012251 batches: 0.0752
trigger times: 8
Loss after 18013214 batches: 0.0735
trigger times: 9
Loss after 18014177 batches: 0.0716
trigger times: 10
Loss after 18015140 batches: 0.0692
trigger times: 11
Loss after 18016103 batches: 0.0692
trigger times: 12
Loss after 18017066 batches: 0.0696
trigger times: 13
Loss after 18018029 batches: 0.0683
trigger times: 14
Loss after 18018992 batches: 0.0689
trigger times: 15
Loss after 18019955 batches: 0.0674
trigger times: 16
Loss after 18020918 batches: 0.0674
trigger times: 17
Loss after 18021881 batches: 0.0641
trigger times: 18
Loss after 18022844 batches: 0.0654
trigger times: 19
Loss after 18023807 batches: 0.0674
trigger times: 20
Loss after 18024770 batches: 0.0641
trigger times: 21
Loss after 18025733 batches: 0.0642
trigger times: 22
Loss after 18026696 batches: 0.0653
trigger times: 23
Loss after 18027659 batches: 0.0633
trigger times: 24
Loss after 18028622 batches: 0.0607
trigger times: 25
Early stopping!
Start to test process.
Loss after 18029585 batches: 0.0596
Time to train on one home:  58.30332803726196
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18030548 batches: 0.1116
trigger times: 1
Loss after 18031511 batches: 0.0564
trigger times: 2
Loss after 18032474 batches: 0.0576
trigger times: 0
Loss after 18033437 batches: 0.0478
trigger times: 1
Loss after 18034400 batches: 0.0422
trigger times: 2
Loss after 18035363 batches: 0.0390
trigger times: 3
Loss after 18036326 batches: 0.0357
trigger times: 4
Loss after 18037289 batches: 0.0353
trigger times: 5
Loss after 18038252 batches: 0.0333
trigger times: 6
Loss after 18039215 batches: 0.0310
trigger times: 7
Loss after 18040178 batches: 0.0302
trigger times: 8
Loss after 18041141 batches: 0.0304
trigger times: 9
Loss after 18042104 batches: 0.0302
trigger times: 10
Loss after 18043067 batches: 0.0295
trigger times: 11
Loss after 18044030 batches: 0.0287
trigger times: 12
Loss after 18044993 batches: 0.0287
trigger times: 13
Loss after 18045956 batches: 0.0275
trigger times: 14
Loss after 18046919 batches: 0.0271
trigger times: 15
Loss after 18047882 batches: 0.0268
trigger times: 16
Loss after 18048845 batches: 0.0262
trigger times: 17
Loss after 18049808 batches: 0.0258
trigger times: 18
Loss after 18050771 batches: 0.0261
trigger times: 19
Loss after 18051734 batches: 0.0262
trigger times: 20
Loss after 18052697 batches: 0.0251
trigger times: 21
Loss after 18053660 batches: 0.0250
trigger times: 22
Loss after 18054623 batches: 0.0255
trigger times: 23
Loss after 18055586 batches: 0.0253
trigger times: 24
Loss after 18056549 batches: 0.0248
trigger times: 25
Early stopping!
Start to test process.
Loss after 18057512 batches: 0.0251
Time to train on one home:  58.94222712516785
trigger times: 0
Loss after 18058471 batches: 0.1178
trigger times: 1
Loss after 18059430 batches: 0.0566
trigger times: 0
Loss after 18060389 batches: 0.0438
trigger times: 1
Loss after 18061348 batches: 0.0325
trigger times: 0
Loss after 18062307 batches: 0.0277
trigger times: 0
Loss after 18063266 batches: 0.0246
trigger times: 1
Loss after 18064225 batches: 0.0226
trigger times: 2
Loss after 18065184 batches: 0.0213
trigger times: 3
Loss after 18066143 batches: 0.0209
trigger times: 0
Loss after 18067102 batches: 0.0195
trigger times: 1
Loss after 18068061 batches: 0.0198
trigger times: 2
Loss after 18069020 batches: 0.0189
trigger times: 3
Loss after 18069979 batches: 0.0185
trigger times: 4
Loss after 18070938 batches: 0.0183
trigger times: 5
Loss after 18071897 batches: 0.0169
trigger times: 0
Loss after 18072856 batches: 0.0173
trigger times: 1
Loss after 18073815 batches: 0.0162
trigger times: 2
Loss after 18074774 batches: 0.0164
trigger times: 3
Loss after 18075733 batches: 0.0168
trigger times: 4
Loss after 18076692 batches: 0.0156
trigger times: 0
Loss after 18077651 batches: 0.0149
trigger times: 1
Loss after 18078610 batches: 0.0158
trigger times: 2
Loss after 18079569 batches: 0.0153
trigger times: 3
Loss after 18080528 batches: 0.0148
trigger times: 4
Loss after 18081487 batches: 0.0150
trigger times: 5
Loss after 18082446 batches: 0.0150
trigger times: 6
Loss after 18083405 batches: 0.0154
trigger times: 7
Loss after 18084364 batches: 0.0140
trigger times: 8
Loss after 18085323 batches: 0.0137
trigger times: 9
Loss after 18086282 batches: 0.0137
trigger times: 10
Loss after 18087241 batches: 0.0129
trigger times: 11
Loss after 18088200 batches: 0.0140
trigger times: 12
Loss after 18089159 batches: 0.0134
trigger times: 13
Loss after 18090118 batches: 0.0139
trigger times: 14
Loss after 18091077 batches: 0.0135
trigger times: 15
Loss after 18092036 batches: 0.0128
trigger times: 16
Loss after 18092995 batches: 0.0125
trigger times: 17
Loss after 18093954 batches: 0.0129
trigger times: 18
Loss after 18094913 batches: 0.0124
trigger times: 19
Loss after 18095872 batches: 0.0121
trigger times: 20
Loss after 18096831 batches: 0.0123
trigger times: 21
Loss after 18097790 batches: 0.0128
trigger times: 22
Loss after 18098749 batches: 0.0124
trigger times: 23
Loss after 18099708 batches: 0.0120
trigger times: 24
Loss after 18100667 batches: 0.0123
trigger times: 25
Early stopping!
Start to test process.
Loss after 18101626 batches: 0.0129
Time to train on one home:  74.12757658958435
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18102589 batches: 0.0535
trigger times: 1
Loss after 18103552 batches: 0.0288
trigger times: 2
Loss after 18104515 batches: 0.0258
trigger times: 3
Loss after 18105478 batches: 0.0255
trigger times: 4
Loss after 18106441 batches: 0.0242
trigger times: 5
Loss after 18107404 batches: 0.0227
trigger times: 6
Loss after 18108367 batches: 0.0216
trigger times: 7
Loss after 18109330 batches: 0.0207
trigger times: 8
Loss after 18110293 batches: 0.0198
trigger times: 9
Loss after 18111256 batches: 0.0193
trigger times: 10
Loss after 18112219 batches: 0.0187
trigger times: 11
Loss after 18113182 batches: 0.0184
trigger times: 12
Loss after 18114145 batches: 0.0182
trigger times: 13
Loss after 18115108 batches: 0.0177
trigger times: 14
Loss after 18116071 batches: 0.0177
trigger times: 15
Loss after 18117034 batches: 0.0174
trigger times: 16
Loss after 18117997 batches: 0.0172
trigger times: 17
Loss after 18118960 batches: 0.0171
trigger times: 18
Loss after 18119923 batches: 0.0172
trigger times: 19
Loss after 18120886 batches: 0.0170
trigger times: 20
Loss after 18121849 batches: 0.0168
trigger times: 21
Loss after 18122812 batches: 0.0169
trigger times: 22
Loss after 18123775 batches: 0.0167
trigger times: 23
Loss after 18124738 batches: 0.0168
trigger times: 24
Loss after 18125701 batches: 0.0164
trigger times: 25
Early stopping!
Start to test process.
Loss after 18126664 batches: 0.0164
Time to train on one home:  57.94646668434143
trigger times: 0
Loss after 18127609 batches: 0.0797
trigger times: 0
Loss after 18128554 batches: 0.0526
trigger times: 0
Loss after 18129499 batches: 0.0379
trigger times: 1
Loss after 18130444 batches: 0.0342
trigger times: 2
Loss after 18131389 batches: 0.0310
trigger times: 3
Loss after 18132334 batches: 0.0278
trigger times: 4
Loss after 18133279 batches: 0.0260
trigger times: 5
Loss after 18134224 batches: 0.0244
trigger times: 6
Loss after 18135169 batches: 0.0230
trigger times: 7
Loss after 18136114 batches: 0.0225
trigger times: 8
Loss after 18137059 batches: 0.0220
trigger times: 9
Loss after 18138004 batches: 0.0213
trigger times: 10
Loss after 18138949 batches: 0.0198
trigger times: 11
Loss after 18139894 batches: 0.0199
trigger times: 12
Loss after 18140839 batches: 0.0191
trigger times: 13
Loss after 18141784 batches: 0.0194
trigger times: 14
Loss after 18142729 batches: 0.0197
trigger times: 15
Loss after 18143674 batches: 0.0199
trigger times: 16
Loss after 18144619 batches: 0.0187
trigger times: 17
Loss after 18145564 batches: 0.0183
trigger times: 18
Loss after 18146509 batches: 0.0191
trigger times: 19
Loss after 18147454 batches: 0.0194
trigger times: 20
Loss after 18148399 batches: 0.0191
trigger times: 21
Loss after 18149344 batches: 0.0193
trigger times: 22
Loss after 18150289 batches: 0.0204
trigger times: 23
Loss after 18151234 batches: 0.0190
trigger times: 24
Loss after 18152179 batches: 0.0182
trigger times: 25
Early stopping!
Start to test process.
Loss after 18153124 batches: 0.0173
Time to train on one home:  56.48591327667236
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18154061 batches: 0.0888
trigger times: 1
Loss after 18154998 batches: 0.0703
trigger times: 2
Loss after 18155935 batches: 0.0667
trigger times: 3
Loss after 18156872 batches: 0.0623
trigger times: 4
Loss after 18157809 batches: 0.0603
trigger times: 5
Loss after 18158746 batches: 0.0569
trigger times: 6
Loss after 18159683 batches: 0.0551
trigger times: 7
Loss after 18160620 batches: 0.0537
trigger times: 8
Loss after 18161557 batches: 0.0526
trigger times: 9
Loss after 18162494 batches: 0.0516
trigger times: 10
Loss after 18163431 batches: 0.0519
trigger times: 11
Loss after 18164368 batches: 0.0499
trigger times: 12
Loss after 18165305 batches: 0.0497
trigger times: 13
Loss after 18166242 batches: 0.0494
trigger times: 14
Loss after 18167179 batches: 0.0487
trigger times: 15
Loss after 18168116 batches: 0.0493
trigger times: 16
Loss after 18169053 batches: 0.0475
trigger times: 17
Loss after 18169990 batches: 0.0476
trigger times: 18
Loss after 18170927 batches: 0.0473
trigger times: 19
Loss after 18171864 batches: 0.0470
trigger times: 20
Loss after 18172801 batches: 0.0464
trigger times: 21
Loss after 18173738 batches: 0.0464
trigger times: 22
Loss after 18174675 batches: 0.0460
trigger times: 23
Loss after 18175612 batches: 0.0467
trigger times: 24
Loss after 18176549 batches: 0.0464
trigger times: 25
Early stopping!
Start to test process.
Loss after 18177486 batches: 0.0477
Time to train on one home:  53.17704153060913
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 18178449 batches: 0.0284
trigger times: 1
Loss after 18179412 batches: 0.0148
trigger times: 2
Loss after 18180375 batches: 0.0136
trigger times: 3
Loss after 18181338 batches: 0.0136
trigger times: 4
Loss after 18182301 batches: 0.0133
trigger times: 5
Loss after 18183264 batches: 0.0132
trigger times: 6
Loss after 18184227 batches: 0.0126
trigger times: 7
Loss after 18185190 batches: 0.0120
trigger times: 8
Loss after 18186153 batches: 0.0114
trigger times: 9
Loss after 18187116 batches: 0.0104
trigger times: 10
Loss after 18188079 batches: 0.0100
trigger times: 11
Loss after 18189042 batches: 0.0092
trigger times: 12
Loss after 18190005 batches: 0.0090
trigger times: 13
Loss after 18190968 batches: 0.0087
trigger times: 14
Loss after 18191931 batches: 0.0083
trigger times: 15
Loss after 18192894 batches: 0.0083
trigger times: 16
Loss after 18193857 batches: 0.0076
trigger times: 17
Loss after 18194820 batches: 0.0076
trigger times: 18
Loss after 18195783 batches: 0.0072
trigger times: 19
Loss after 18196746 batches: 0.0070
trigger times: 20
Loss after 18197709 batches: 0.0070
trigger times: 21
Loss after 18198672 batches: 0.0070
trigger times: 22
Loss after 18199635 batches: 0.0069
trigger times: 23
Loss after 18200598 batches: 0.0067
trigger times: 24
Loss after 18201561 batches: 0.0065
trigger times: 25
Early stopping!
Start to test process.
Loss after 18202524 batches: 0.0062
Time to train on one home:  57.92495131492615
trigger times: 0
Loss after 18203487 batches: 0.0951
trigger times: 1
Loss after 18204450 batches: 0.0777
trigger times: 2
Loss after 18205413 batches: 0.0720
trigger times: 3
Loss after 18206376 batches: 0.0667
trigger times: 4
Loss after 18207339 batches: 0.0638
trigger times: 5
Loss after 18208302 batches: 0.0621
trigger times: 6
Loss after 18209265 batches: 0.0599
trigger times: 7
Loss after 18210228 batches: 0.0581
trigger times: 8
Loss after 18211191 batches: 0.0566
trigger times: 9
Loss after 18212154 batches: 0.0576
trigger times: 10
Loss after 18213117 batches: 0.0561
trigger times: 11
Loss after 18214080 batches: 0.0554
trigger times: 12
Loss after 18215043 batches: 0.0542
trigger times: 13
Loss after 18216006 batches: 0.0539
trigger times: 14
Loss after 18216969 batches: 0.0543
trigger times: 15
Loss after 18217932 batches: 0.0533
trigger times: 16
Loss after 18218895 batches: 0.0532
trigger times: 17
Loss after 18219858 batches: 0.0526
trigger times: 18
Loss after 18220821 batches: 0.0539
trigger times: 19
Loss after 18221784 batches: 0.0522
trigger times: 20
Loss after 18222747 batches: 0.0526
trigger times: 21
Loss after 18223710 batches: 0.0518
trigger times: 22
Loss after 18224673 batches: 0.0515
trigger times: 23
Loss after 18225636 batches: 0.0511
trigger times: 24
Loss after 18226599 batches: 0.0504
trigger times: 25
Early stopping!
Start to test process.
Loss after 18227562 batches: 0.0501
Time to train on one home:  56.933207273483276
trigger times: 0
Loss after 18228525 batches: 0.0655
trigger times: 1
Loss after 18229488 batches: 0.0477
trigger times: 2
Loss after 18230451 batches: 0.0445
trigger times: 3
Loss after 18231414 batches: 0.0410
trigger times: 4
Loss after 18232377 batches: 0.0387
trigger times: 5
Loss after 18233340 batches: 0.0367
trigger times: 6
Loss after 18234303 batches: 0.0362
trigger times: 7
Loss after 18235266 batches: 0.0350
trigger times: 8
Loss after 18236229 batches: 0.0334
trigger times: 9
Loss after 18237192 batches: 0.0323
trigger times: 10
Loss after 18238155 batches: 0.0323
trigger times: 11
Loss after 18239118 batches: 0.0314
trigger times: 12
Loss after 18240081 batches: 0.0313
trigger times: 13
Loss after 18241044 batches: 0.0307
trigger times: 14
Loss after 18242007 batches: 0.0300
trigger times: 15
Loss after 18242970 batches: 0.0301
trigger times: 16
Loss after 18243933 batches: 0.0294
trigger times: 17
Loss after 18244896 batches: 0.0299
trigger times: 18
Loss after 18245859 batches: 0.0293
trigger times: 19
Loss after 18246822 batches: 0.0284
trigger times: 20
Loss after 18247785 batches: 0.0291
trigger times: 21
Loss after 18248748 batches: 0.0286
trigger times: 22
Loss after 18249711 batches: 0.0285
trigger times: 23
Loss after 18250674 batches: 0.0279
trigger times: 24
Loss after 18251637 batches: 0.0268
trigger times: 25
Early stopping!
Start to test process.
Loss after 18252600 batches: 0.0268
Time to train on one home:  54.24585962295532
trigger times: 0
Loss after 18253496 batches: 0.1015
trigger times: 1
Loss after 18254392 batches: 0.0921
trigger times: 2
Loss after 18255288 batches: 0.0866
trigger times: 3
Loss after 18256184 batches: 0.0808
trigger times: 4
Loss after 18257080 batches: 0.0738
trigger times: 5
Loss after 18257976 batches: 0.0688
trigger times: 6
Loss after 18258872 batches: 0.0649
trigger times: 7
Loss after 18259768 batches: 0.0618
trigger times: 8
Loss after 18260664 batches: 0.0595
trigger times: 9
Loss after 18261560 batches: 0.0587
trigger times: 10
Loss after 18262456 batches: 0.0589
trigger times: 11
Loss after 18263352 batches: 0.0556
trigger times: 12
Loss after 18264248 batches: 0.0559
trigger times: 13
Loss after 18265144 batches: 0.0559
trigger times: 14
Loss after 18266040 batches: 0.0547
trigger times: 15
Loss after 18266936 batches: 0.0541
trigger times: 16
Loss after 18267832 batches: 0.0527
trigger times: 17
Loss after 18268728 batches: 0.0526
trigger times: 18
Loss after 18269624 batches: 0.0538
trigger times: 19
Loss after 18270520 batches: 0.0525
trigger times: 20
Loss after 18271416 batches: 0.0527
trigger times: 21
Loss after 18272312 batches: 0.0521
trigger times: 22
Loss after 18273208 batches: 0.0525
trigger times: 23
Loss after 18274104 batches: 0.0526
trigger times: 24
Loss after 18275000 batches: 0.0532
trigger times: 25
Early stopping!
Start to test process.
Loss after 18275896 batches: 0.0519
Time to train on one home:  59.021446228027344
trigger times: 0
Loss after 18276859 batches: 0.1541
trigger times: 1
Loss after 18277822 batches: 0.1148
trigger times: 2
Loss after 18278785 batches: 0.0972
trigger times: 3
Loss after 18279748 batches: 0.0875
trigger times: 4
Loss after 18280711 batches: 0.0784
trigger times: 5
Loss after 18281674 batches: 0.0733
trigger times: 6
Loss after 18282637 batches: 0.0675
trigger times: 7
Loss after 18283600 batches: 0.0606
trigger times: 8
Loss after 18284563 batches: 0.0553
trigger times: 9
Loss after 18285526 batches: 0.0541
trigger times: 10
Loss after 18286489 batches: 0.0511
trigger times: 11
Loss after 18287452 batches: 0.0493
trigger times: 12
Loss after 18288415 batches: 0.0468
trigger times: 13
Loss after 18289378 batches: 0.0460
trigger times: 14
Loss after 18290341 batches: 0.0475
trigger times: 15
Loss after 18291304 batches: 0.0452
trigger times: 16
Loss after 18292267 batches: 0.0448
trigger times: 17
Loss after 18293230 batches: 0.0425
trigger times: 18
Loss after 18294193 batches: 0.0426
trigger times: 19
Loss after 18295156 batches: 0.0420
trigger times: 20
Loss after 18296119 batches: 0.0407
trigger times: 21
Loss after 18297082 batches: 0.0413
trigger times: 22
Loss after 18298045 batches: 0.0408
trigger times: 23
Loss after 18299008 batches: 0.0428
trigger times: 24
Loss after 18299971 batches: 0.0411
trigger times: 25
Early stopping!
Start to test process.
Loss after 18300934 batches: 0.0412
Time to train on one home:  56.52206611633301
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18301897 batches: 0.0846
trigger times: 1
Loss after 18302860 batches: 0.0756
trigger times: 2
Loss after 18303823 batches: 0.0730
trigger times: 3
Loss after 18304786 batches: 0.0701
trigger times: 4
Loss after 18305749 batches: 0.0650
trigger times: 5
Loss after 18306712 batches: 0.0600
trigger times: 6
Loss after 18307675 batches: 0.0575
trigger times: 7
Loss after 18308638 batches: 0.0546
trigger times: 8
Loss after 18309601 batches: 0.0548
trigger times: 9
Loss after 18310564 batches: 0.0524
trigger times: 10
Loss after 18311527 batches: 0.0490
trigger times: 11
Loss after 18312490 batches: 0.0487
trigger times: 12
Loss after 18313453 batches: 0.0484
trigger times: 13
Loss after 18314416 batches: 0.0451
trigger times: 14
Loss after 18315379 batches: 0.0469
trigger times: 15
Loss after 18316342 batches: 0.0477
trigger times: 16
Loss after 18317305 batches: 0.0472
trigger times: 17
Loss after 18318268 batches: 0.0476
trigger times: 18
Loss after 18319231 batches: 0.0441
trigger times: 19
Loss after 18320194 batches: 0.0428
trigger times: 20
Loss after 18321157 batches: 0.0441
trigger times: 21
Loss after 18322120 batches: 0.0434
trigger times: 22
Loss after 18323083 batches: 0.0418
trigger times: 23
Loss after 18324046 batches: 0.0418
trigger times: 24
Loss after 18325009 batches: 0.0422
trigger times: 25
Early stopping!
Start to test process.
Loss after 18325972 batches: 0.0423
Time to train on one home:  56.60407257080078
trigger times: 0
Loss after 18326935 batches: 0.0841
trigger times: 1
Loss after 18327898 batches: 0.0617
trigger times: 2
Loss after 18328861 batches: 0.0546
trigger times: 3
Loss after 18329824 batches: 0.0481
trigger times: 4
Loss after 18330787 batches: 0.0460
trigger times: 5
Loss after 18331750 batches: 0.0427
trigger times: 6
Loss after 18332713 batches: 0.0415
trigger times: 7
Loss after 18333676 batches: 0.0407
trigger times: 8
Loss after 18334639 batches: 0.0400
trigger times: 9
Loss after 18335602 batches: 0.0382
trigger times: 10
Loss after 18336565 batches: 0.0370
trigger times: 11
Loss after 18337528 batches: 0.0365
trigger times: 12
Loss after 18338491 batches: 0.0351
trigger times: 13
Loss after 18339454 batches: 0.0353
trigger times: 14
Loss after 18340417 batches: 0.0351
trigger times: 15
Loss after 18341380 batches: 0.0348
trigger times: 16
Loss after 18342343 batches: 0.0348
trigger times: 17
Loss after 18343306 batches: 0.0351
trigger times: 18
Loss after 18344269 batches: 0.0349
trigger times: 19
Loss after 18345232 batches: 0.0346
trigger times: 20
Loss after 18346195 batches: 0.0340
trigger times: 21
Loss after 18347158 batches: 0.0337
trigger times: 22
Loss after 18348121 batches: 0.0353
trigger times: 23
Loss after 18349084 batches: 0.0334
trigger times: 24
Loss after 18350047 batches: 0.0325
trigger times: 25
Early stopping!
Start to test process.
Loss after 18351010 batches: 0.0332
Time to train on one home:  58.85032796859741
trigger times: 0
Loss after 18351973 batches: 0.0490
trigger times: 1
Loss after 18352936 batches: 0.0347
trigger times: 2
Loss after 18353899 batches: 0.0337
trigger times: 3
Loss after 18354862 batches: 0.0304
trigger times: 4
Loss after 18355825 batches: 0.0266
trigger times: 5
Loss after 18356788 batches: 0.0250
trigger times: 6
Loss after 18357751 batches: 0.0241
trigger times: 7
Loss after 18358714 batches: 0.0237
trigger times: 8
Loss after 18359677 batches: 0.0226
trigger times: 9
Loss after 18360640 batches: 0.0227
trigger times: 10
Loss after 18361603 batches: 0.0212
trigger times: 11
Loss after 18362566 batches: 0.0210
trigger times: 12
Loss after 18363529 batches: 0.0209
trigger times: 13
Loss after 18364492 batches: 0.0203
trigger times: 14
Loss after 18365455 batches: 0.0208
trigger times: 15
Loss after 18366418 batches: 0.0204
trigger times: 16
Loss after 18367381 batches: 0.0199
trigger times: 17
Loss after 18368344 batches: 0.0204
trigger times: 18
Loss after 18369307 batches: 0.0199
trigger times: 19
Loss after 18370270 batches: 0.0194
trigger times: 20
Loss after 18371233 batches: 0.0195
trigger times: 21
Loss after 18372196 batches: 0.0189
trigger times: 22
Loss after 18373159 batches: 0.0187
trigger times: 23
Loss after 18374122 batches: 0.0184
trigger times: 24
Loss after 18375085 batches: 0.0183
trigger times: 25
Early stopping!
Start to test process.
Loss after 18376048 batches: 0.0183
Time to train on one home:  58.301466941833496
trigger times: 0
Loss after 18377011 batches: 0.0628
trigger times: 1
Loss after 18377974 batches: 0.0456
trigger times: 2
Loss after 18378937 batches: 0.0457
trigger times: 3
Loss after 18379900 batches: 0.0440
trigger times: 4
Loss after 18380863 batches: 0.0409
trigger times: 5
Loss after 18381826 batches: 0.0394
trigger times: 6
Loss after 18382789 batches: 0.0381
trigger times: 7
Loss after 18383752 batches: 0.0376
trigger times: 8
Loss after 18384715 batches: 0.0362
trigger times: 9
Loss after 18385678 batches: 0.0365
trigger times: 10
Loss after 18386641 batches: 0.0356
trigger times: 11
Loss after 18387604 batches: 0.0350
trigger times: 12
Loss after 18388567 batches: 0.0352
trigger times: 13
Loss after 18389530 batches: 0.0345
trigger times: 14
Loss after 18390493 batches: 0.0336
trigger times: 15
Loss after 18391456 batches: 0.0334
trigger times: 16
Loss after 18392419 batches: 0.0335
trigger times: 17
Loss after 18393382 batches: 0.0333
trigger times: 18
Loss after 18394345 batches: 0.0333
trigger times: 19
Loss after 18395308 batches: 0.0326
trigger times: 20
Loss after 18396271 batches: 0.0316
trigger times: 21
Loss after 18397234 batches: 0.0336
trigger times: 22
Loss after 18398197 batches: 0.0329
trigger times: 23
Loss after 18399160 batches: 0.0316
trigger times: 24
Loss after 18400123 batches: 0.0324
trigger times: 25
Early stopping!
Start to test process.
Loss after 18401086 batches: 0.0311
Time to train on one home:  55.82718586921692
trigger times: 0
Loss after 18401981 batches: 0.0800
trigger times: 1
Loss after 18402876 batches: 0.0436
trigger times: 2
Loss after 18403771 batches: 0.0202
trigger times: 0
Loss after 18404666 batches: 0.0106
trigger times: 1
Loss after 18405561 batches: 0.0071
trigger times: 2
Loss after 18406456 batches: 0.0056
trigger times: 3
Loss after 18407351 batches: 0.0046
trigger times: 4
Loss after 18408246 batches: 0.0042
trigger times: 5
Loss after 18409141 batches: 0.0036
trigger times: 6
Loss after 18410036 batches: 0.0028
trigger times: 7
Loss after 18410931 batches: 0.0032
trigger times: 8
Loss after 18411826 batches: 0.0030
trigger times: 9
Loss after 18412721 batches: 0.0026
trigger times: 10
Loss after 18413616 batches: 0.0029
trigger times: 11
Loss after 18414511 batches: 0.0032
trigger times: 12
Loss after 18415406 batches: 0.0039
trigger times: 13
Loss after 18416301 batches: 0.0030
trigger times: 14
Loss after 18417196 batches: 0.0027
trigger times: 15
Loss after 18418091 batches: 0.0028
trigger times: 16
Loss after 18418986 batches: 0.0028
trigger times: 17
Loss after 18419881 batches: 0.0025
trigger times: 18
Loss after 18420776 batches: 0.0021
trigger times: 19
Loss after 18421671 batches: 0.0020
trigger times: 20
Loss after 18422566 batches: 0.0017
trigger times: 21
Loss after 18423461 batches: 0.0027
trigger times: 22
Loss after 18424356 batches: 0.0027
trigger times: 23
Loss after 18425251 batches: 0.0025
trigger times: 24
Loss after 18426146 batches: 0.0023
trigger times: 25
Early stopping!
Start to test process.
Loss after 18427041 batches: 0.0018
Time to train on one home:  56.11705493927002
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336, 0.034888222413376094, 0.034613204319843105, 0.033508375671873286, 0.03325774057242148, 0.03243952582662181, 0.032562873532530595, 0.03126130941140662, 0.03133166822639105, 0.03049944561800663]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171], [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71], [0.13675299286842346, 0.15565351472302724, 0.32920502623266784, 1.059933101560365, 0.6549021202780799, 35.010519125969424, 3184.605], [0.0885479673743248, 0.1559912263273432, 0.33358461589199145, 0.944871497166479, 0.654640173496829, 31.209933508474723, 3183.3313], [0.1345348209142685, 0.15444619808844395, 0.3263722552493196, 1.0544752388510252, 0.6558385527073367, 34.830241138150235, 3189.1584], [0.09016867727041245, 0.15349941943138035, 0.3319629557497865, 0.9480173077546179, 0.65657289912243, 31.313842388762147, 3192.7297], [0.13274754583835602, 0.1683317466130475, 0.33338956387040825, 1.0413414780800958, 0.645068476656334, 34.39642151124207, 3136.7866], [0.08685638755559921, 0.16053285093432457, 0.33813218961147645, 0.938309926482314, 0.6511175433777052, 30.99319907910727, 3166.2017], [0.13567858934402466, 0.1592208654199878, 0.3293138590809072, 1.0581373625723918, 0.6521351625870662, 34.951204293654875, 3171.1501], [0.08736597001552582, 0.1548414758248644, 0.3338051677093621, 0.9435825164148742, 0.6555319615580705, 31.16735734476153, 3187.668]]
Round_16_results:  [0.08736597001552582, 0.1548414758248644, 0.3338051677093621, 0.9435825164148742, 0.6555319615580705, 31.16735734476153, 3187.668]
trigger times: 0
Loss after 18428004 batches: 0.0524
trigger times: 1
Loss after 18428967 batches: 0.0433
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 19310 < 19311; dropping {'Training_Loss': 0.0523912071117333, 'Validation_Loss': 0.06307332217693329, 'Training_R2': 0.30578262860538263, 'Validation_R2': 0.14096528434238076, 'Training_F1': 0.5103875325070172, 'Validation_F1': 0.3282006387978977, 'Training_NEP': 0.7740999017689916, 'Validation_NEP': 0.8843118923006008, 'Training_NDE': 0.5112594584273201, 'Validation_NDE': 0.6666091449723514, 'Training_MAE': 22.29033773248761, 'Validation_MAE': 29.456635931564918, 'Training_MSE': 1608.5145, 'Validation_MSE': 3301.982}.
trigger times: 0
Loss after 18429930 batches: 0.0376
trigger times: 1
Loss after 18430893 batches: 0.0343
trigger times: 2
Loss after 18431856 batches: 0.0308
trigger times: 3
Loss after 18432819 batches: 0.0302
trigger times: 4
Loss after 18433782 batches: 0.0292
trigger times: 5
Loss after 18434745 batches: 0.0284
trigger times: 6
Loss after 18435708 batches: 0.0282
trigger times: 7
Loss after 18436671 batches: 0.0280
trigger times: 8
Loss after 18437634 batches: 0.0272
trigger times: 9
Loss after 18438597 batches: 0.0260
trigger times: 10
Loss after 18439560 batches: 0.0264
trigger times: 11
Loss after 18440523 batches: 0.0280
trigger times: 12
Loss after 18441486 batches: 0.0272
trigger times: 13
Loss after 18442449 batches: 0.0269
trigger times: 14
Loss after 18443412 batches: 0.0273
trigger times: 15
Loss after 18444375 batches: 0.0272
trigger times: 16
Loss after 18445338 batches: 0.0256
trigger times: 17
Loss after 18446301 batches: 0.0262
trigger times: 18
Loss after 18447264 batches: 0.0250
trigger times: 19
Loss after 18448227 batches: 0.0240
trigger times: 20
Loss after 18449190 batches: 0.0246
trigger times: 21
Loss after 18450153 batches: 0.0243
trigger times: 22
Loss after 18451116 batches: 0.0246
trigger times: 23
Loss after 18452079 batches: 0.0243
trigger times: 24
Loss after 18453042 batches: 0.0237
trigger times: 25
Early stopping!
Start to test process.
Loss after 18454005 batches: 0.0233
Time to train on one home:  59.67603254318237
trigger times: 0
Loss after 18454963 batches: 0.0568
trigger times: 0
Loss after 18455921 batches: 0.0398
trigger times: 1
Loss after 18456879 batches: 0.0300
trigger times: 2
Loss after 18457837 batches: 0.0265
trigger times: 3
Loss after 18458795 batches: 0.0235
trigger times: 0
Loss after 18459753 batches: 0.0206
trigger times: 1
Loss after 18460711 batches: 0.0199
trigger times: 2
Loss after 18461669 batches: 0.0195
trigger times: 0
Loss after 18462627 batches: 0.0178
trigger times: 1
Loss after 18463585 batches: 0.0178
trigger times: 2
Loss after 18464543 batches: 0.0179
trigger times: 0
Loss after 18465501 batches: 0.0183
trigger times: 1
Loss after 18466459 batches: 0.0172
trigger times: 2
Loss after 18467417 batches: 0.0171
trigger times: 3
Loss after 18468375 batches: 0.0171
trigger times: 4
Loss after 18469333 batches: 0.0164
trigger times: 5
Loss after 18470291 batches: 0.0155
trigger times: 6
Loss after 18471249 batches: 0.0157
trigger times: 7
Loss after 18472207 batches: 0.0147
trigger times: 8
Loss after 18473165 batches: 0.0152
trigger times: 9
Loss after 18474123 batches: 0.0149
trigger times: 10
Loss after 18475081 batches: 0.0146
trigger times: 11
Loss after 18476039 batches: 0.0141
trigger times: 12
Loss after 18476997 batches: 0.0147
trigger times: 13
Loss after 18477955 batches: 0.0144
trigger times: 14
Loss after 18478913 batches: 0.0137
trigger times: 15
Loss after 18479871 batches: 0.0137
trigger times: 16
Loss after 18480829 batches: 0.0138
trigger times: 17
Loss after 18481787 batches: 0.0129
trigger times: 18
Loss after 18482745 batches: 0.0132
trigger times: 19
Loss after 18483703 batches: 0.0126
trigger times: 20
Loss after 18484661 batches: 0.0126
trigger times: 21
Loss after 18485619 batches: 0.0142
trigger times: 22
Loss after 18486577 batches: 0.0141
trigger times: 23
Loss after 18487535 batches: 0.0142
trigger times: 24
Loss after 18488493 batches: 0.0156
trigger times: 25
Early stopping!
Start to test process.
Loss after 18489451 batches: 0.0169
Time to train on one home:  66.41595077514648
trigger times: 0
Loss after 18490414 batches: 0.1057
trigger times: 1
Loss after 18491377 batches: 0.0730
trigger times: 2
Loss after 18492340 batches: 0.0669
trigger times: 3
Loss after 18493303 batches: 0.0669
trigger times: 4
Loss after 18494266 batches: 0.0648
trigger times: 5
Loss after 18495229 batches: 0.0615
trigger times: 6
Loss after 18496192 batches: 0.0584
trigger times: 7
Loss after 18497155 batches: 0.0558
trigger times: 8
Loss after 18498118 batches: 0.0539
trigger times: 9
Loss after 18499081 batches: 0.0524
trigger times: 10
Loss after 18500044 batches: 0.0504
trigger times: 11
Loss after 18501007 batches: 0.0497
trigger times: 12
Loss after 18501970 batches: 0.0486
trigger times: 13
Loss after 18502933 batches: 0.0471
trigger times: 14
Loss after 18503896 batches: 0.0472
trigger times: 15
Loss after 18504859 batches: 0.0476
trigger times: 16
Loss after 18505822 batches: 0.0453
trigger times: 17
Loss after 18506785 batches: 0.0462
trigger times: 18
Loss after 18507748 batches: 0.0444
trigger times: 19
Loss after 18508711 batches: 0.0438
trigger times: 20
Loss after 18509674 batches: 0.0432
trigger times: 21
Loss after 18510637 batches: 0.0423
trigger times: 22
Loss after 18511600 batches: 0.0427
trigger times: 23
Loss after 18512563 batches: 0.0416
trigger times: 24
Loss after 18513526 batches: 0.0417
trigger times: 25
Early stopping!
Start to test process.
Loss after 18514489 batches: 0.0405
Time to train on one home:  60.01384377479553
trigger times: 0
Loss after 18515452 batches: 0.1045
trigger times: 1
Loss after 18516415 batches: 0.0784
trigger times: 2
Loss after 18517378 batches: 0.0760
trigger times: 3
Loss after 18518341 batches: 0.0740
trigger times: 4
Loss after 18519304 batches: 0.0705
trigger times: 5
Loss after 18520267 batches: 0.0678
trigger times: 6
Loss after 18521230 batches: 0.0657
trigger times: 7
Loss after 18522193 batches: 0.0639
trigger times: 8
Loss after 18523156 batches: 0.0621
trigger times: 9
Loss after 18524119 batches: 0.0618
trigger times: 10
Loss after 18525082 batches: 0.0592
trigger times: 11
Loss after 18526045 batches: 0.0586
trigger times: 12
Loss after 18527008 batches: 0.0577
trigger times: 13
Loss after 18527971 batches: 0.0578
trigger times: 14
Loss after 18528934 batches: 0.0566
trigger times: 15
Loss after 18529897 batches: 0.0571
trigger times: 16
Loss after 18530860 batches: 0.0565
trigger times: 17
Loss after 18531823 batches: 0.0549
trigger times: 18
Loss after 18532786 batches: 0.0546
trigger times: 19
Loss after 18533749 batches: 0.0549
trigger times: 20
Loss after 18534712 batches: 0.0541
trigger times: 21
Loss after 18535675 batches: 0.0536
trigger times: 22
Loss after 18536638 batches: 0.0541
trigger times: 23
Loss after 18537601 batches: 0.0551
trigger times: 24
Loss after 18538564 batches: 0.0527
trigger times: 25
Early stopping!
Start to test process.
Loss after 18539527 batches: 0.0525
Time to train on one home:  58.584333419799805
trigger times: 0
Loss after 18540490 batches: 0.0245
trigger times: 1
Loss after 18541453 batches: 0.0191
trigger times: 2
Loss after 18542416 batches: 0.0178
trigger times: 3
Loss after 18543379 batches: 0.0166
trigger times: 4
Loss after 18544342 batches: 0.0154
trigger times: 5
Loss after 18545305 batches: 0.0142
trigger times: 6
Loss after 18546268 batches: 0.0141
trigger times: 7
Loss after 18547231 batches: 0.0136
trigger times: 8
Loss after 18548194 batches: 0.0133
trigger times: 9
Loss after 18549157 batches: 0.0129
trigger times: 10
Loss after 18550120 batches: 0.0126
trigger times: 11
Loss after 18551083 batches: 0.0125
trigger times: 12
Loss after 18552046 batches: 0.0123
trigger times: 13
Loss after 18553009 batches: 0.0119
trigger times: 14
Loss after 18553972 batches: 0.0118
trigger times: 15
Loss after 18554935 batches: 0.0119
trigger times: 16
Loss after 18555898 batches: 0.0119
trigger times: 17
Loss after 18556861 batches: 0.0121
trigger times: 18
Loss after 18557824 batches: 0.0118
trigger times: 19
Loss after 18558787 batches: 0.0118
trigger times: 20
Loss after 18559750 batches: 0.0111
trigger times: 21
Loss after 18560713 batches: 0.0113
trigger times: 22
Loss after 18561676 batches: 0.0110
trigger times: 23
Loss after 18562639 batches: 0.0108
trigger times: 24
Loss after 18563602 batches: 0.0106
trigger times: 25
Early stopping!
Start to test process.
Loss after 18564565 batches: 0.0110
Time to train on one home:  56.552873849868774
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18565528 batches: 0.0935
trigger times: 1
Loss after 18566491 batches: 0.0278
trigger times: 2
Loss after 18567454 batches: 0.0195
trigger times: 3
Loss after 18568417 batches: 0.0189
trigger times: 4
Loss after 18569380 batches: 0.0173
trigger times: 5
Loss after 18570343 batches: 0.0159
trigger times: 6
Loss after 18571306 batches: 0.0147
trigger times: 7
Loss after 18572269 batches: 0.0142
trigger times: 8
Loss after 18573232 batches: 0.0138
trigger times: 9
Loss after 18574195 batches: 0.0132
trigger times: 10
Loss after 18575158 batches: 0.0131
trigger times: 11
Loss after 18576121 batches: 0.0126
trigger times: 12
Loss after 18577084 batches: 0.0126
trigger times: 13
Loss after 18578047 batches: 0.0125
trigger times: 14
Loss after 18579010 batches: 0.0123
trigger times: 15
Loss after 18579973 batches: 0.0123
trigger times: 16
Loss after 18580936 batches: 0.0122
trigger times: 17
Loss after 18581899 batches: 0.0122
trigger times: 18
Loss after 18582862 batches: 0.0120
trigger times: 19
Loss after 18583825 batches: 0.0120
trigger times: 20
Loss after 18584788 batches: 0.0117
trigger times: 21
Loss after 18585751 batches: 0.0116
trigger times: 22
Loss after 18586714 batches: 0.0118
trigger times: 23
Loss after 18587677 batches: 0.0119
trigger times: 24
Loss after 18588640 batches: 0.0114
trigger times: 25
Early stopping!
Start to test process.
Loss after 18589603 batches: 0.0114
Time to train on one home:  57.19370913505554
trigger times: 0
Loss after 18590566 batches: 0.0951
trigger times: 1
Loss after 18591529 batches: 0.0882
trigger times: 0
Loss after 18592492 batches: 0.0823
trigger times: 1
Loss after 18593455 batches: 0.0778
trigger times: 2
Loss after 18594418 batches: 0.0745
trigger times: 3
Loss after 18595381 batches: 0.0716
trigger times: 4
Loss after 18596344 batches: 0.0692
trigger times: 5
Loss after 18597307 batches: 0.0665
trigger times: 6
Loss after 18598270 batches: 0.0649
trigger times: 7
Loss after 18599233 batches: 0.0637
trigger times: 8
Loss after 18600196 batches: 0.0638
trigger times: 9
Loss after 18601159 batches: 0.0627
trigger times: 10
Loss after 18602122 batches: 0.0616
trigger times: 11
Loss after 18603085 batches: 0.0614
trigger times: 12
Loss after 18604048 batches: 0.0604
trigger times: 13
Loss after 18605011 batches: 0.0598
trigger times: 14
Loss after 18605974 batches: 0.0592
trigger times: 15
Loss after 18606937 batches: 0.0589
trigger times: 16
Loss after 18607900 batches: 0.0573
trigger times: 17
Loss after 18608863 batches: 0.0601
trigger times: 18
Loss after 18609826 batches: 0.0579
trigger times: 19
Loss after 18610789 batches: 0.0574
trigger times: 20
Loss after 18611752 batches: 0.0561
trigger times: 21
Loss after 18612715 batches: 0.0571
trigger times: 22
Loss after 18613678 batches: 0.0563
trigger times: 23
Loss after 18614641 batches: 0.0564
trigger times: 24
Loss after 18615604 batches: 0.0570
trigger times: 25
Early stopping!
Start to test process.
Loss after 18616567 batches: 0.0562
Time to train on one home:  58.84024524688721
trigger times: 0
Loss after 18617530 batches: 0.0563
trigger times: 1
Loss after 18618493 batches: 0.0419
trigger times: 0
Loss after 18619456 batches: 0.0326
trigger times: 1
Loss after 18620419 batches: 0.0292
trigger times: 2
Loss after 18621382 batches: 0.0259
trigger times: 3
Loss after 18622345 batches: 0.0244
trigger times: 4
Loss after 18623308 batches: 0.0235
trigger times: 5
Loss after 18624271 batches: 0.0227
trigger times: 6
Loss after 18625234 batches: 0.0211
trigger times: 7
Loss after 18626197 batches: 0.0211
trigger times: 8
Loss after 18627160 batches: 0.0207
trigger times: 9
Loss after 18628123 batches: 0.0197
trigger times: 10
Loss after 18629086 batches: 0.0193
trigger times: 11
Loss after 18630049 batches: 0.0208
trigger times: 12
Loss after 18631012 batches: 0.0221
trigger times: 13
Loss after 18631975 batches: 0.0221
trigger times: 14
Loss after 18632938 batches: 0.0223
trigger times: 15
Loss after 18633901 batches: 0.0197
trigger times: 16
Loss after 18634864 batches: 0.0184
trigger times: 17
Loss after 18635827 batches: 0.0185
trigger times: 18
Loss after 18636790 batches: 0.0181
trigger times: 19
Loss after 18637753 batches: 0.0178
trigger times: 20
Loss after 18638716 batches: 0.0180
trigger times: 21
Loss after 18639679 batches: 0.0172
trigger times: 22
Loss after 18640642 batches: 0.0164
trigger times: 23
Loss after 18641605 batches: 0.0161
trigger times: 24
Loss after 18642568 batches: 0.0164
trigger times: 25
Early stopping!
Start to test process.
Loss after 18643531 batches: 0.0155
Time to train on one home:  57.8887243270874
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18644494 batches: 0.0880
trigger times: 1
Loss after 18645457 batches: 0.0727
trigger times: 2
Loss after 18646420 batches: 0.0692
trigger times: 3
Loss after 18647383 batches: 0.0666
trigger times: 4
Loss after 18648346 batches: 0.0637
trigger times: 5
Loss after 18649309 batches: 0.0609
trigger times: 6
Loss after 18650272 batches: 0.0592
trigger times: 7
Loss after 18651235 batches: 0.0586
trigger times: 8
Loss after 18652198 batches: 0.0566
trigger times: 9
Loss after 18653161 batches: 0.0554
trigger times: 10
Loss after 18654124 batches: 0.0547
trigger times: 11
Loss after 18655087 batches: 0.0538
trigger times: 12
Loss after 18656050 batches: 0.0547
trigger times: 13
Loss after 18657013 batches: 0.0544
trigger times: 14
Loss after 18657976 batches: 0.0557
trigger times: 15
Loss after 18658939 batches: 0.0541
trigger times: 16
Loss after 18659902 batches: 0.0529
trigger times: 17
Loss after 18660865 batches: 0.0520
trigger times: 18
Loss after 18661828 batches: 0.0522
trigger times: 19
Loss after 18662791 batches: 0.0514
trigger times: 20
Loss after 18663754 batches: 0.0532
trigger times: 21
Loss after 18664717 batches: 0.0511
trigger times: 22
Loss after 18665680 batches: 0.0506
trigger times: 23
Loss after 18666643 batches: 0.0509
trigger times: 24
Loss after 18667606 batches: 0.0520
trigger times: 25
Early stopping!
Start to test process.
Loss after 18668569 batches: 0.0523
Time to train on one home:  52.778363943099976
trigger times: 0
Loss after 18669532 batches: 0.0863
trigger times: 0
Loss after 18670495 batches: 0.0587
trigger times: 0
Loss after 18671458 batches: 0.0527
trigger times: 1
Loss after 18672421 batches: 0.0488
trigger times: 2
Loss after 18673384 batches: 0.0450
trigger times: 3
Loss after 18674347 batches: 0.0452
trigger times: 4
Loss after 18675310 batches: 0.0424
trigger times: 5
Loss after 18676273 batches: 0.0407
trigger times: 6
Loss after 18677236 batches: 0.0412
trigger times: 7
Loss after 18678199 batches: 0.0394
trigger times: 8
Loss after 18679162 batches: 0.0390
trigger times: 9
Loss after 18680125 batches: 0.0388
trigger times: 10
Loss after 18681088 batches: 0.0384
trigger times: 11
Loss after 18682051 batches: 0.0383
trigger times: 12
Loss after 18683014 batches: 0.0373
trigger times: 13
Loss after 18683977 batches: 0.0380
trigger times: 14
Loss after 18684940 batches: 0.0372
trigger times: 15
Loss after 18685903 batches: 0.0378
trigger times: 16
Loss after 18686866 batches: 0.0361
trigger times: 17
Loss after 18687829 batches: 0.0362
trigger times: 18
Loss after 18688792 batches: 0.0368
trigger times: 19
Loss after 18689755 batches: 0.0360
trigger times: 20
Loss after 18690718 batches: 0.0353
trigger times: 21
Loss after 18691681 batches: 0.0357
trigger times: 22
Loss after 18692644 batches: 0.0366
trigger times: 23
Loss after 18693607 batches: 0.0364
trigger times: 24
Loss after 18694570 batches: 0.0346
trigger times: 25
Early stopping!
Start to test process.
Loss after 18695533 batches: 0.0351
Time to train on one home:  58.45964193344116
trigger times: 0
Loss after 18696496 batches: 0.0746
trigger times: 1
Loss after 18697459 batches: 0.0683
trigger times: 2
Loss after 18698422 batches: 0.0635
trigger times: 3
Loss after 18699385 batches: 0.0618
trigger times: 4
Loss after 18700348 batches: 0.0603
trigger times: 5
Loss after 18701311 batches: 0.0581
trigger times: 6
Loss after 18702274 batches: 0.0573
trigger times: 7
Loss after 18703237 batches: 0.0554
trigger times: 8
Loss after 18704200 batches: 0.0548
trigger times: 9
Loss after 18705163 batches: 0.0530
trigger times: 10
Loss after 18706126 batches: 0.0537
trigger times: 11
Loss after 18707089 batches: 0.0534
trigger times: 12
Loss after 18708052 batches: 0.0511
trigger times: 13
Loss after 18709015 batches: 0.0517
trigger times: 14
Loss after 18709978 batches: 0.0514
trigger times: 15
Loss after 18710941 batches: 0.0500
trigger times: 16
Loss after 18711904 batches: 0.0488
trigger times: 17
Loss after 18712867 batches: 0.0492
trigger times: 18
Loss after 18713830 batches: 0.0486
trigger times: 19
Loss after 18714793 batches: 0.0493
trigger times: 20
Loss after 18715756 batches: 0.0481
trigger times: 21
Loss after 18716719 batches: 0.0482
trigger times: 22
Loss after 18717682 batches: 0.0478
trigger times: 23
Loss after 18718645 batches: 0.0468
trigger times: 24
Loss after 18719608 batches: 0.0489
trigger times: 25
Early stopping!
Start to test process.
Loss after 18720571 batches: 0.0477
Time to train on one home:  57.13090920448303
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18721534 batches: 0.0657
trigger times: 1
Loss after 18722497 batches: 0.0485
trigger times: 2
Loss after 18723460 batches: 0.0451
trigger times: 3
Loss after 18724423 batches: 0.0395
trigger times: 4
Loss after 18725386 batches: 0.0356
trigger times: 5
Loss after 18726349 batches: 0.0329
trigger times: 6
Loss after 18727312 batches: 0.0312
trigger times: 7
Loss after 18728275 batches: 0.0298
trigger times: 8
Loss after 18729238 batches: 0.0288
trigger times: 9
Loss after 18730201 batches: 0.0275
trigger times: 10
Loss after 18731164 batches: 0.0277
trigger times: 11
Loss after 18732127 batches: 0.0261
trigger times: 12
Loss after 18733090 batches: 0.0260
trigger times: 13
Loss after 18734053 batches: 0.0258
trigger times: 14
Loss after 18735016 batches: 0.0251
trigger times: 15
Loss after 18735979 batches: 0.0254
trigger times: 16
Loss after 18736942 batches: 0.0243
trigger times: 17
Loss after 18737905 batches: 0.0237
trigger times: 18
Loss after 18738868 batches: 0.0228
trigger times: 19
Loss after 18739831 batches: 0.0228
trigger times: 20
Loss after 18740794 batches: 0.0230
trigger times: 21
Loss after 18741757 batches: 0.0233
trigger times: 22
Loss after 18742720 batches: 0.0232
trigger times: 23
Loss after 18743683 batches: 0.0225
trigger times: 24
Loss after 18744646 batches: 0.0220
trigger times: 25
Early stopping!
Start to test process.
Loss after 18745609 batches: 0.0219
Time to train on one home:  53.73594856262207
trigger times: 0
Loss after 18746572 batches: 0.0746
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18747535 batches: 0.0494
trigger times: 1
Loss after 18748498 batches: 0.0394
trigger times: 0
Loss after 18749461 batches: 0.0359
trigger times: 0
Loss after 18750424 batches: 0.0315
trigger times: 1
Loss after 18751387 batches: 0.0316
trigger times: 2
Loss after 18752350 batches: 0.0308
trigger times: 3
Loss after 18753313 batches: 0.0273
trigger times: 4
Loss after 18754276 batches: 0.0245
trigger times: 0
Loss after 18755239 batches: 0.0238
trigger times: 1
Loss after 18756202 batches: 0.0241
trigger times: 2
Loss after 18757165 batches: 0.0221
trigger times: 3
Loss after 18758128 batches: 0.0218
trigger times: 4
Loss after 18759091 batches: 0.0209
trigger times: 5
Loss after 18760054 batches: 0.0217
trigger times: 0
Loss after 18761017 batches: 0.0208
trigger times: 1
Loss after 18761980 batches: 0.0217
trigger times: 2
Loss after 18762943 batches: 0.0207
trigger times: 3
Loss after 18763906 batches: 0.0199
trigger times: 4
Loss after 18764869 batches: 0.0183
trigger times: 5
Loss after 18765832 batches: 0.0186
trigger times: 6
Loss after 18766795 batches: 0.0185
trigger times: 7
Loss after 18767758 batches: 0.0173
trigger times: 8
Loss after 18768721 batches: 0.0172
trigger times: 9
Loss after 18769684 batches: 0.0184
trigger times: 10
Loss after 18770647 batches: 0.0192
trigger times: 11
Loss after 18771610 batches: 0.0187
trigger times: 12
Loss after 18772573 batches: 0.0186
trigger times: 13
Loss after 18773536 batches: 0.0183
trigger times: 14
Loss after 18774499 batches: 0.0177
trigger times: 15
Loss after 18775462 batches: 0.0166
trigger times: 16
Loss after 18776425 batches: 0.0156
trigger times: 17
Loss after 18777388 batches: 0.0161
trigger times: 18
Loss after 18778351 batches: 0.0162
trigger times: 19
Loss after 18779314 batches: 0.0159
trigger times: 20
Loss after 18780277 batches: 0.0164
trigger times: 21
Loss after 18781240 batches: 0.0168
trigger times: 22
Loss after 18782203 batches: 0.0158
trigger times: 23
Loss after 18783166 batches: 0.0152
trigger times: 24
Loss after 18784129 batches: 0.0144
trigger times: 25
Early stopping!
Start to test process.
Loss after 18785092 batches: 0.0152
Time to train on one home:  72.00040125846863
trigger times: 0
Loss after 18786021 batches: 0.0884
trigger times: 0
Loss after 18786950 batches: 0.0602
trigger times: 0
Loss after 18787879 batches: 0.0442
trigger times: 1
Loss after 18788808 batches: 0.0353
trigger times: 2
Loss after 18789737 batches: 0.0371
trigger times: 0
Loss after 18790666 batches: 0.0302
trigger times: 1
Loss after 18791595 batches: 0.0280
trigger times: 2
Loss after 18792524 batches: 0.0298
trigger times: 3
Loss after 18793453 batches: 0.0303
trigger times: 0
Loss after 18794382 batches: 0.0290
trigger times: 1
Loss after 18795311 batches: 0.0262
trigger times: 2
Loss after 18796240 batches: 0.0252
trigger times: 3
Loss after 18797169 batches: 0.0256
trigger times: 0
Loss after 18798098 batches: 0.0235
trigger times: 1
Loss after 18799027 batches: 0.0247
trigger times: 2
Loss after 18799956 batches: 0.0226
trigger times: 3
Loss after 18800885 batches: 0.0240
trigger times: 4
Loss after 18801814 batches: 0.0245
trigger times: 5
Loss after 18802743 batches: 0.0239
trigger times: 6
Loss after 18803672 batches: 0.0232
trigger times: 7
Loss after 18804601 batches: 0.0216
trigger times: 8
Loss after 18805530 batches: 0.0218
trigger times: 9
Loss after 18806459 batches: 0.0212
trigger times: 10
Loss after 18807388 batches: 0.0203
trigger times: 11
Loss after 18808317 batches: 0.0198
trigger times: 12
Loss after 18809246 batches: 0.0217
trigger times: 13
Loss after 18810175 batches: 0.0208
trigger times: 14
Loss after 18811104 batches: 0.0204
trigger times: 15
Loss after 18812033 batches: 0.0199
trigger times: 16
Loss after 18812962 batches: 0.0187
trigger times: 17
Loss after 18813891 batches: 0.0192
trigger times: 18
Loss after 18814820 batches: 0.0193
trigger times: 19
Loss after 18815749 batches: 0.0202
trigger times: 20
Loss after 18816678 batches: 0.0197
trigger times: 21
Loss after 18817607 batches: 0.0225
trigger times: 22
Loss after 18818536 batches: 0.0212
trigger times: 23
Loss after 18819465 batches: 0.0202
trigger times: 24
Loss after 18820394 batches: 0.0202
trigger times: 25
Early stopping!
Start to test process.
Loss after 18821323 batches: 0.0192
Time to train on one home:  66.7689254283905
trigger times: 0
Loss after 18822285 batches: 0.0817
trigger times: 1
Loss after 18823247 batches: 0.0651
trigger times: 2
Loss after 18824209 batches: 0.0646
trigger times: 3
Loss after 18825171 batches: 0.0623
trigger times: 4
Loss after 18826133 batches: 0.0583
trigger times: 5
Loss after 18827095 batches: 0.0563
trigger times: 6
Loss after 18828057 batches: 0.0544
trigger times: 7
Loss after 18829019 batches: 0.0535
trigger times: 8
Loss after 18829981 batches: 0.0523
trigger times: 9
Loss after 18830943 batches: 0.0524
trigger times: 10
Loss after 18831905 batches: 0.0512
trigger times: 11
Loss after 18832867 batches: 0.0509
trigger times: 12
Loss after 18833829 batches: 0.0499
trigger times: 13
Loss after 18834791 batches: 0.0500
trigger times: 14
Loss after 18835753 batches: 0.0491
trigger times: 15
Loss after 18836715 batches: 0.0502
trigger times: 16
Loss after 18837677 batches: 0.0502
trigger times: 17
Loss after 18838639 batches: 0.0510
trigger times: 18
Loss after 18839601 batches: 0.0493
trigger times: 19
Loss after 18840563 batches: 0.0490
trigger times: 20
Loss after 18841525 batches: 0.0488
trigger times: 21
Loss after 18842487 batches: 0.0479
trigger times: 22
Loss after 18843449 batches: 0.0477
trigger times: 23
Loss after 18844411 batches: 0.0478
trigger times: 24
Loss after 18845373 batches: 0.0476
trigger times: 25
Early stopping!
Start to test process.
Loss after 18846335 batches: 0.0475
Time to train on one home:  56.62691950798035
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18847298 batches: 0.0537
trigger times: 1
Loss after 18848261 batches: 0.0425
trigger times: 2
Loss after 18849224 batches: 0.0431
trigger times: 3
Loss after 18850187 batches: 0.0403
trigger times: 4
Loss after 18851150 batches: 0.0375
trigger times: 5
Loss after 18852113 batches: 0.0358
trigger times: 6
Loss after 18853076 batches: 0.0342
trigger times: 7
Loss after 18854039 batches: 0.0331
trigger times: 8
Loss after 18855002 batches: 0.0328
trigger times: 9
Loss after 18855965 batches: 0.0324
trigger times: 10
Loss after 18856928 batches: 0.0311
trigger times: 11
Loss after 18857891 batches: 0.0305
trigger times: 12
Loss after 18858854 batches: 0.0305
trigger times: 13
Loss after 18859817 batches: 0.0304
trigger times: 14
Loss after 18860780 batches: 0.0290
trigger times: 15
Loss after 18861743 batches: 0.0297
trigger times: 16
Loss after 18862706 batches: 0.0302
trigger times: 17
Loss after 18863669 batches: 0.0292
trigger times: 18
Loss after 18864632 batches: 0.0288
trigger times: 19
Loss after 18865595 batches: 0.0293
trigger times: 20
Loss after 18866558 batches: 0.0289
trigger times: 21
Loss after 18867521 batches: 0.0285
trigger times: 22
Loss after 18868484 batches: 0.0286
trigger times: 23
Loss after 18869447 batches: 0.0285
trigger times: 24
Loss after 18870410 batches: 0.0283
trigger times: 25
Early stopping!
Start to test process.
Loss after 18871373 batches: 0.0276
Time to train on one home:  57.714646100997925
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18872336 batches: 0.0473
trigger times: 1
Loss after 18873299 batches: 0.0415
trigger times: 0
Loss after 18874262 batches: 0.0394
trigger times: 1
Loss after 18875225 batches: 0.0367
trigger times: 2
Loss after 18876188 batches: 0.0349
trigger times: 3
Loss after 18877151 batches: 0.0355
trigger times: 4
Loss after 18878114 batches: 0.0360
trigger times: 5
Loss after 18879077 batches: 0.0335
trigger times: 6
Loss after 18880040 batches: 0.0323
trigger times: 7
Loss after 18881003 batches: 0.0314
trigger times: 8
Loss after 18881966 batches: 0.0318
trigger times: 9
Loss after 18882929 batches: 0.0302
trigger times: 10
Loss after 18883892 batches: 0.0300
trigger times: 11
Loss after 18884855 batches: 0.0298
trigger times: 12
Loss after 18885818 batches: 0.0294
trigger times: 13
Loss after 18886781 batches: 0.0285
trigger times: 14
Loss after 18887744 batches: 0.0294
trigger times: 15
Loss after 18888707 batches: 0.0279
trigger times: 16
Loss after 18889670 batches: 0.0286
trigger times: 17
Loss after 18890633 batches: 0.0279
trigger times: 18
Loss after 18891596 batches: 0.0293
trigger times: 19
Loss after 18892559 batches: 0.0275
trigger times: 20
Loss after 18893522 batches: 0.0274
trigger times: 21
Loss after 18894485 batches: 0.0269
trigger times: 22
Loss after 18895448 batches: 0.0265
trigger times: 23
Loss after 18896411 batches: 0.0263
trigger times: 24
Loss after 18897374 batches: 0.0261
trigger times: 25
Early stopping!
Start to test process.
Loss after 18898337 batches: 0.0258
Time to train on one home:  58.805230140686035
trigger times: 0
Loss after 18899300 batches: 0.0967
trigger times: 1
Loss after 18900263 batches: 0.0895
trigger times: 2
Loss after 18901226 batches: 0.0841
trigger times: 3
Loss after 18902189 batches: 0.0833
trigger times: 4
Loss after 18903152 batches: 0.0792
trigger times: 5
Loss after 18904115 batches: 0.0772
trigger times: 6
Loss after 18905078 batches: 0.0737
trigger times: 7
Loss after 18906041 batches: 0.0708
trigger times: 8
Loss after 18907004 batches: 0.0710
trigger times: 9
Loss after 18907967 batches: 0.0698
trigger times: 10
Loss after 18908930 batches: 0.0696
trigger times: 11
Loss after 18909893 batches: 0.0690
trigger times: 12
Loss after 18910856 batches: 0.0680
trigger times: 13
Loss after 18911819 batches: 0.0674
trigger times: 14
Loss after 18912782 batches: 0.0688
trigger times: 15
Loss after 18913745 batches: 0.0665
trigger times: 16
Loss after 18914708 batches: 0.0640
trigger times: 17
Loss after 18915671 batches: 0.0652
trigger times: 18
Loss after 18916634 batches: 0.0677
trigger times: 19
Loss after 18917597 batches: 0.0634
trigger times: 20
Loss after 18918560 batches: 0.0635
trigger times: 21
Loss after 18919523 batches: 0.0622
trigger times: 22
Loss after 18920486 batches: 0.0625
trigger times: 23
Loss after 18921449 batches: 0.0626
trigger times: 24
Loss after 18922412 batches: 0.0614
trigger times: 25
Early stopping!
Start to test process.
Loss after 18923375 batches: 0.0603
Time to train on one home:  54.386616230010986
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18924338 batches: 0.0899
trigger times: 1
Loss after 18925301 batches: 0.0507
trigger times: 2
Loss after 18926264 batches: 0.0507
trigger times: 3
Loss after 18927227 batches: 0.0424
trigger times: 4
Loss after 18928190 batches: 0.0397
trigger times: 5
Loss after 18929153 batches: 0.0369
trigger times: 6
Loss after 18930116 batches: 0.0348
trigger times: 7
Loss after 18931079 batches: 0.0330
trigger times: 8
Loss after 18932042 batches: 0.0324
trigger times: 9
Loss after 18933005 batches: 0.0306
trigger times: 10
Loss after 18933968 batches: 0.0299
trigger times: 11
Loss after 18934931 batches: 0.0290
trigger times: 12
Loss after 18935894 batches: 0.0293
trigger times: 13
Loss after 18936857 batches: 0.0281
trigger times: 14
Loss after 18937820 batches: 0.0276
trigger times: 15
Loss after 18938783 batches: 0.0278
trigger times: 16
Loss after 18939746 batches: 0.0271
trigger times: 17
Loss after 18940709 batches: 0.0276
trigger times: 18
Loss after 18941672 batches: 0.0257
trigger times: 19
Loss after 18942635 batches: 0.0268
trigger times: 20
Loss after 18943598 batches: 0.0253
trigger times: 21
Loss after 18944561 batches: 0.0254
trigger times: 22
Loss after 18945524 batches: 0.0250
trigger times: 23
Loss after 18946487 batches: 0.0253
trigger times: 24
Loss after 18947450 batches: 0.0256
trigger times: 25
Early stopping!
Start to test process.
Loss after 18948413 batches: 0.0250
Time to train on one home:  54.0176956653595
trigger times: 0
Loss after 18949372 batches: 0.0906
trigger times: 1
Loss after 18950331 batches: 0.0448
trigger times: 0
Loss after 18951290 batches: 0.0353
trigger times: 1
Loss after 18952249 batches: 0.0278
trigger times: 0
Loss after 18953208 batches: 0.0251
trigger times: 0
Loss after 18954167 batches: 0.0233
trigger times: 1
Loss after 18955126 batches: 0.0216
trigger times: 0
Loss after 18956085 batches: 0.0201
trigger times: 1
Loss after 18957044 batches: 0.0194
trigger times: 2
Loss after 18958003 batches: 0.0198
trigger times: 3
Loss after 18958962 batches: 0.0182
trigger times: 4
Loss after 18959921 batches: 0.0178
trigger times: 0
Loss after 18960880 batches: 0.0171
trigger times: 1
Loss after 18961839 batches: 0.0166
trigger times: 2
Loss after 18962798 batches: 0.0169
trigger times: 3
Loss after 18963757 batches: 0.0155
trigger times: 4
Loss after 18964716 batches: 0.0173
trigger times: 5
Loss after 18965675 batches: 0.0173
trigger times: 6
Loss after 18966634 batches: 0.0162
trigger times: 7
Loss after 18967593 batches: 0.0160
trigger times: 8
Loss after 18968552 batches: 0.0149
trigger times: 9
Loss after 18969511 batches: 0.0150
trigger times: 10
Loss after 18970470 batches: 0.0149
trigger times: 11
Loss after 18971429 batches: 0.0147
trigger times: 12
Loss after 18972388 batches: 0.0141
trigger times: 13
Loss after 18973347 batches: 0.0146
trigger times: 14
Loss after 18974306 batches: 0.0139
trigger times: 15
Loss after 18975265 batches: 0.0141
trigger times: 16
Loss after 18976224 batches: 0.0140
trigger times: 17
Loss after 18977183 batches: 0.0136
trigger times: 18
Loss after 18978142 batches: 0.0134
trigger times: 19
Loss after 18979101 batches: 0.0126
trigger times: 20
Loss after 18980060 batches: 0.0136
trigger times: 21
Loss after 18981019 batches: 0.0133
trigger times: 22
Loss after 18981978 batches: 0.0134
trigger times: 23
Loss after 18982937 batches: 0.0134
trigger times: 24
Loss after 18983896 batches: 0.0130
trigger times: 25
Early stopping!
Start to test process.
Loss after 18984855 batches: 0.0130
Time to train on one home:  67.09069991111755
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18985818 batches: 0.0733
trigger times: 1
Loss after 18986781 batches: 0.0327
trigger times: 2
Loss after 18987744 batches: 0.0262
trigger times: 3
Loss after 18988707 batches: 0.0265
trigger times: 4
Loss after 18989670 batches: 0.0254
trigger times: 5
Loss after 18990633 batches: 0.0241
trigger times: 6
Loss after 18991596 batches: 0.0230
trigger times: 7
Loss after 18992559 batches: 0.0217
trigger times: 8
Loss after 18993522 batches: 0.0209
trigger times: 9
Loss after 18994485 batches: 0.0201
trigger times: 10
Loss after 18995448 batches: 0.0192
trigger times: 11
Loss after 18996411 batches: 0.0195
trigger times: 12
Loss after 18997374 batches: 0.0188
trigger times: 13
Loss after 18998337 batches: 0.0184
trigger times: 14
Loss after 18999300 batches: 0.0182
trigger times: 15
Loss after 19000263 batches: 0.0181
trigger times: 16
Loss after 19001226 batches: 0.0180
trigger times: 17
Loss after 19002189 batches: 0.0174
trigger times: 18
Loss after 19003152 batches: 0.0174
trigger times: 19
Loss after 19004115 batches: 0.0174
trigger times: 20
Loss after 19005078 batches: 0.0176
trigger times: 21
Loss after 19006041 batches: 0.0169
trigger times: 22
Loss after 19007004 batches: 0.0169
trigger times: 23
Loss after 19007967 batches: 0.0170
trigger times: 24
Loss after 19008930 batches: 0.0163
trigger times: 25
Early stopping!
Start to test process.
Loss after 19009893 batches: 0.0165
Time to train on one home:  55.35427379608154
trigger times: 0
Loss after 19010838 batches: 0.0640
trigger times: 0
Loss after 19011783 batches: 0.0456
trigger times: 0
Loss after 19012728 batches: 0.0326
trigger times: 1
Loss after 19013673 batches: 0.0302
trigger times: 2
Loss after 19014618 batches: 0.0283
trigger times: 3
Loss after 19015563 batches: 0.0272
trigger times: 4
Loss after 19016508 batches: 0.0241
trigger times: 5
Loss after 19017453 batches: 0.0233
trigger times: 6
Loss after 19018398 batches: 0.0236
trigger times: 7
Loss after 19019343 batches: 0.0211
trigger times: 8
Loss after 19020288 batches: 0.0203
trigger times: 9
Loss after 19021233 batches: 0.0211
trigger times: 10
Loss after 19022178 batches: 0.0192
trigger times: 11
Loss after 19023123 batches: 0.0202
trigger times: 12
Loss after 19024068 batches: 0.0190
trigger times: 13
Loss after 19025013 batches: 0.0200
trigger times: 14
Loss after 19025958 batches: 0.0195
trigger times: 15
Loss after 19026903 batches: 0.0184
trigger times: 16
Loss after 19027848 batches: 0.0175
trigger times: 17
Loss after 19028793 batches: 0.0172
trigger times: 18
Loss after 19029738 batches: 0.0175
trigger times: 19
Loss after 19030683 batches: 0.0171
trigger times: 20
Loss after 19031628 batches: 0.0169
trigger times: 21
Loss after 19032573 batches: 0.0161
trigger times: 22
Loss after 19033518 batches: 0.0170
trigger times: 23
Loss after 19034463 batches: 0.0174
trigger times: 24
Loss after 19035408 batches: 0.0166
trigger times: 25
Early stopping!
Start to test process.
Loss after 19036353 batches: 0.0156
Time to train on one home:  57.67756724357605
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19037290 batches: 0.0844
trigger times: 1
Loss after 19038227 batches: 0.0676
trigger times: 2
Loss after 19039164 batches: 0.0638
trigger times: 3
Loss after 19040101 batches: 0.0615
trigger times: 4
Loss after 19041038 batches: 0.0576
trigger times: 5
Loss after 19041975 batches: 0.0564
trigger times: 6
Loss after 19042912 batches: 0.0533
trigger times: 7
Loss after 19043849 batches: 0.0520
trigger times: 8
Loss after 19044786 batches: 0.0512
trigger times: 9
Loss after 19045723 batches: 0.0506
trigger times: 10
Loss after 19046660 batches: 0.0500
trigger times: 11
Loss after 19047597 batches: 0.0498
trigger times: 12
Loss after 19048534 batches: 0.0490
trigger times: 13
Loss after 19049471 batches: 0.0486
trigger times: 14
Loss after 19050408 batches: 0.0478
trigger times: 15
Loss after 19051345 batches: 0.0483
trigger times: 16
Loss after 19052282 batches: 0.0473
trigger times: 17
Loss after 19053219 batches: 0.0479
trigger times: 18
Loss after 19054156 batches: 0.0462
trigger times: 19
Loss after 19055093 batches: 0.0463
trigger times: 20
Loss after 19056030 batches: 0.0466
trigger times: 21
Loss after 19056967 batches: 0.0462
trigger times: 22
Loss after 19057904 batches: 0.0471
trigger times: 23
Loss after 19058841 batches: 0.0474
trigger times: 24
Loss after 19059778 batches: 0.0465
trigger times: 25
Early stopping!
Start to test process.
Loss after 19060715 batches: 0.0462
Time to train on one home:  59.16209626197815
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 19061678 batches: 0.0512
trigger times: 1
Loss after 19062641 batches: 0.0180
trigger times: 2
Loss after 19063604 batches: 0.0140
trigger times: 3
Loss after 19064567 batches: 0.0137
trigger times: 4
Loss after 19065530 batches: 0.0139
trigger times: 5
Loss after 19066493 batches: 0.0138
trigger times: 6
Loss after 19067456 batches: 0.0140
trigger times: 7
Loss after 19068419 batches: 0.0138
trigger times: 8
Loss after 19069382 batches: 0.0138
trigger times: 9
Loss after 19070345 batches: 0.0138
trigger times: 10
Loss after 19071308 batches: 0.0137
trigger times: 11
Loss after 19072271 batches: 0.0140
trigger times: 12
Loss after 19073234 batches: 0.0136
trigger times: 13
Loss after 19074197 batches: 0.0139
trigger times: 14
Loss after 19075160 batches: 0.0139
trigger times: 15
Loss after 19076123 batches: 0.0136
trigger times: 16
Loss after 19077086 batches: 0.0136
trigger times: 17
Loss after 19078049 batches: 0.0135
trigger times: 18
Loss after 19079012 batches: 0.0136
trigger times: 19
Loss after 19079975 batches: 0.0135
trigger times: 20
Loss after 19080938 batches: 0.0134
trigger times: 21
Loss after 19081901 batches: 0.0131
trigger times: 22
Loss after 19082864 batches: 0.0132
trigger times: 23
Loss after 19083827 batches: 0.0131
trigger times: 24
Loss after 19084790 batches: 0.0130
trigger times: 25
Early stopping!
Start to test process.
Loss after 19085753 batches: 0.0131
Time to train on one home:  56.9728147983551
trigger times: 0
Loss after 19086716 batches: 0.0965
trigger times: 1
Loss after 19087679 batches: 0.0776
trigger times: 2
Loss after 19088642 batches: 0.0726
trigger times: 3
Loss after 19089605 batches: 0.0668
trigger times: 4
Loss after 19090568 batches: 0.0643
trigger times: 5
Loss after 19091531 batches: 0.0612
trigger times: 6
Loss after 19092494 batches: 0.0590
trigger times: 7
Loss after 19093457 batches: 0.0571
trigger times: 8
Loss after 19094420 batches: 0.0565
trigger times: 9
Loss after 19095383 batches: 0.0551
trigger times: 10
Loss after 19096346 batches: 0.0544
trigger times: 11
Loss after 19097309 batches: 0.0543
trigger times: 12
Loss after 19098272 batches: 0.0540
trigger times: 13
Loss after 19099235 batches: 0.0527
trigger times: 14
Loss after 19100198 batches: 0.0542
trigger times: 15
Loss after 19101161 batches: 0.0534
trigger times: 16
Loss after 19102124 batches: 0.0531
trigger times: 17
Loss after 19103087 batches: 0.0529
trigger times: 18
Loss after 19104050 batches: 0.0515
trigger times: 19
Loss after 19105013 batches: 0.0503
trigger times: 20
Loss after 19105976 batches: 0.0505
trigger times: 21
Loss after 19106939 batches: 0.0506
trigger times: 22
Loss after 19107902 batches: 0.0506
trigger times: 23
Loss after 19108865 batches: 0.0500
trigger times: 24
Loss after 19109828 batches: 0.0491
trigger times: 25
Early stopping!
Start to test process.
Loss after 19110791 batches: 0.0494
Time to train on one home:  56.99210238456726
trigger times: 0
Loss after 19111754 batches: 0.0763
trigger times: 1
Loss after 19112717 batches: 0.0490
trigger times: 2
Loss after 19113680 batches: 0.0471
trigger times: 3
Loss after 19114643 batches: 0.0423
trigger times: 4
Loss after 19115606 batches: 0.0390
trigger times: 5
Loss after 19116569 batches: 0.0373
trigger times: 6
Loss after 19117532 batches: 0.0359
trigger times: 7
Loss after 19118495 batches: 0.0344
trigger times: 8
Loss after 19119458 batches: 0.0333
trigger times: 9
Loss after 19120421 batches: 0.0335
trigger times: 10
Loss after 19121384 batches: 0.0320
trigger times: 11
Loss after 19122347 batches: 0.0313
trigger times: 12
Loss after 19123310 batches: 0.0310
trigger times: 13
Loss after 19124273 batches: 0.0303
trigger times: 14
Loss after 19125236 batches: 0.0298
trigger times: 15
Loss after 19126199 batches: 0.0299
trigger times: 16
Loss after 19127162 batches: 0.0297
trigger times: 17
Loss after 19128125 batches: 0.0293
trigger times: 18
Loss after 19129088 batches: 0.0290
trigger times: 19
Loss after 19130051 batches: 0.0286
trigger times: 20
Loss after 19131014 batches: 0.0272
trigger times: 21
Loss after 19131977 batches: 0.0279
trigger times: 22
Loss after 19132940 batches: 0.0275
trigger times: 23
Loss after 19133903 batches: 0.0282
trigger times: 24
Loss after 19134866 batches: 0.0275
trigger times: 25
Early stopping!
Start to test process.
Loss after 19135829 batches: 0.0264
Time to train on one home:  57.55716681480408
trigger times: 0
Loss after 19136725 batches: 0.1038
trigger times: 1
Loss after 19137621 batches: 0.0914
trigger times: 2
Loss after 19138517 batches: 0.0840
trigger times: 3
Loss after 19139413 batches: 0.0784
trigger times: 4
Loss after 19140309 batches: 0.0720
trigger times: 5
Loss after 19141205 batches: 0.0690
trigger times: 6
Loss after 19142101 batches: 0.0688
trigger times: 7
Loss after 19142997 batches: 0.0643
trigger times: 8
Loss after 19143893 batches: 0.0615
trigger times: 9
Loss after 19144789 batches: 0.0595
trigger times: 10
Loss after 19145685 batches: 0.0587
trigger times: 11
Loss after 19146581 batches: 0.0565
trigger times: 12
Loss after 19147477 batches: 0.0552
trigger times: 13
Loss after 19148373 batches: 0.0546
trigger times: 14
Loss after 19149269 batches: 0.0557
trigger times: 15
Loss after 19150165 batches: 0.0543
trigger times: 16
Loss after 19151061 batches: 0.0552
trigger times: 17
Loss after 19151957 batches: 0.0574
trigger times: 18
Loss after 19152853 batches: 0.0543
trigger times: 19
Loss after 19153749 batches: 0.0520
trigger times: 20
Loss after 19154645 batches: 0.0526
trigger times: 21
Loss after 19155541 batches: 0.0511
trigger times: 22
Loss after 19156437 batches: 0.0511
trigger times: 23
Loss after 19157333 batches: 0.0513
trigger times: 24
Loss after 19158229 batches: 0.0548
trigger times: 25
Early stopping!
Start to test process.
Loss after 19159125 batches: 0.0525
Time to train on one home:  55.270244121551514
trigger times: 0
Loss after 19160088 batches: 0.1806
trigger times: 1
Loss after 19161051 batches: 0.1210
trigger times: 2
Loss after 19162014 batches: 0.0969
trigger times: 3
Loss after 19162977 batches: 0.0886
trigger times: 4
Loss after 19163940 batches: 0.0784
trigger times: 5
Loss after 19164903 batches: 0.0709
trigger times: 6
Loss after 19165866 batches: 0.0693
trigger times: 7
Loss after 19166829 batches: 0.0626
trigger times: 8
Loss after 19167792 batches: 0.0581
trigger times: 9
Loss after 19168755 batches: 0.0554
trigger times: 10
Loss after 19169718 batches: 0.0517
trigger times: 11
Loss after 19170681 batches: 0.0512
trigger times: 12
Loss after 19171644 batches: 0.0490
trigger times: 13
Loss after 19172607 batches: 0.0483
trigger times: 14
Loss after 19173570 batches: 0.0454
trigger times: 15
Loss after 19174533 batches: 0.0438
trigger times: 16
Loss after 19175496 batches: 0.0448
trigger times: 17
Loss after 19176459 batches: 0.0446
trigger times: 18
Loss after 19177422 batches: 0.0422
trigger times: 19
Loss after 19178385 batches: 0.0413
trigger times: 20
Loss after 19179348 batches: 0.0397
trigger times: 21
Loss after 19180311 batches: 0.0400
trigger times: 22
Loss after 19181274 batches: 0.0414
trigger times: 23
Loss after 19182237 batches: 0.0414
trigger times: 24
Loss after 19183200 batches: 0.0397
trigger times: 25
Early stopping!
Start to test process.
Loss after 19184163 batches: 0.0402
Time to train on one home:  53.15144348144531
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19185126 batches: 0.0895
trigger times: 1
Loss after 19186089 batches: 0.0756
trigger times: 2
Loss after 19187052 batches: 0.0744
trigger times: 3
Loss after 19188015 batches: 0.0706
trigger times: 4
Loss after 19188978 batches: 0.0655
trigger times: 5
Loss after 19189941 batches: 0.0602
trigger times: 6
Loss after 19190904 batches: 0.0566
trigger times: 7
Loss after 19191867 batches: 0.0550
trigger times: 8
Loss after 19192830 batches: 0.0515
trigger times: 9
Loss after 19193793 batches: 0.0489
trigger times: 10
Loss after 19194756 batches: 0.0480
trigger times: 11
Loss after 19195719 batches: 0.0483
trigger times: 12
Loss after 19196682 batches: 0.0490
trigger times: 13
Loss after 19197645 batches: 0.0478
trigger times: 14
Loss after 19198608 batches: 0.0461
trigger times: 15
Loss after 19199571 batches: 0.0458
trigger times: 16
Loss after 19200534 batches: 0.0435
trigger times: 17
Loss after 19201497 batches: 0.0441
trigger times: 18
Loss after 19202460 batches: 0.0462
trigger times: 19
Loss after 19203423 batches: 0.0422
trigger times: 20
Loss after 19204386 batches: 0.0450
trigger times: 21
Loss after 19205349 batches: 0.0439
trigger times: 22
Loss after 19206312 batches: 0.0447
trigger times: 23
Loss after 19207275 batches: 0.0452
trigger times: 24
Loss after 19208238 batches: 0.0473
trigger times: 25
Early stopping!
Start to test process.
Loss after 19209201 batches: 0.0434
Time to train on one home:  55.37953996658325
trigger times: 0
Loss after 19210164 batches: 0.0847
trigger times: 1
Loss after 19211127 batches: 0.0612
trigger times: 2
Loss after 19212090 batches: 0.0535
trigger times: 3
Loss after 19213053 batches: 0.0493
trigger times: 4
Loss after 19214016 batches: 0.0451
trigger times: 5
Loss after 19214979 batches: 0.0426
trigger times: 6
Loss after 19215942 batches: 0.0410
trigger times: 7
Loss after 19216905 batches: 0.0397
trigger times: 8
Loss after 19217868 batches: 0.0393
trigger times: 9
Loss after 19218831 batches: 0.0380
trigger times: 10
Loss after 19219794 batches: 0.0366
trigger times: 11
Loss after 19220757 batches: 0.0359
trigger times: 12
Loss after 19221720 batches: 0.0355
trigger times: 13
Loss after 19222683 batches: 0.0350
trigger times: 14
Loss after 19223646 batches: 0.0350
trigger times: 15
Loss after 19224609 batches: 0.0345
trigger times: 16
Loss after 19225572 batches: 0.0340
trigger times: 17
Loss after 19226535 batches: 0.0346
trigger times: 18
Loss after 19227498 batches: 0.0339
trigger times: 19
Loss after 19228461 batches: 0.0337
trigger times: 20
Loss after 19229424 batches: 0.0337
trigger times: 21
Loss after 19230387 batches: 0.0333
trigger times: 22
Loss after 19231350 batches: 0.0326
trigger times: 23
Loss after 19232313 batches: 0.0326
trigger times: 24
Loss after 19233276 batches: 0.0319
trigger times: 25
Early stopping!
Start to test process.
Loss after 19234239 batches: 0.0313
Time to train on one home:  56.692909479141235
trigger times: 0
Loss after 19235202 batches: 0.0427
trigger times: 1
Loss after 19236165 batches: 0.0327
trigger times: 2
Loss after 19237128 batches: 0.0310
trigger times: 3
Loss after 19238091 batches: 0.0287
trigger times: 4
Loss after 19239054 batches: 0.0260
trigger times: 5
Loss after 19240017 batches: 0.0242
trigger times: 6
Loss after 19240980 batches: 0.0231
trigger times: 7
Loss after 19241943 batches: 0.0223
trigger times: 8
Loss after 19242906 batches: 0.0211
trigger times: 9
Loss after 19243869 batches: 0.0215
trigger times: 10
Loss after 19244832 batches: 0.0212
trigger times: 11
Loss after 19245795 batches: 0.0206
trigger times: 12
Loss after 19246758 batches: 0.0204
trigger times: 13
Loss after 19247721 batches: 0.0196
trigger times: 14
Loss after 19248684 batches: 0.0199
trigger times: 15
Loss after 19249647 batches: 0.0197
trigger times: 16
Loss after 19250610 batches: 0.0196
trigger times: 17
Loss after 19251573 batches: 0.0190
trigger times: 18
Loss after 19252536 batches: 0.0195
trigger times: 19
Loss after 19253499 batches: 0.0190
trigger times: 20
Loss after 19254462 batches: 0.0186
trigger times: 21
Loss after 19255425 batches: 0.0183
trigger times: 22
Loss after 19256388 batches: 0.0185
trigger times: 23
Loss after 19257351 batches: 0.0184
trigger times: 24
Loss after 19258314 batches: 0.0174
trigger times: 25
Early stopping!
Start to test process.
Loss after 19259277 batches: 0.0184
Time to train on one home:  54.7322359085083
trigger times: 0
Loss after 19260240 batches: 0.0891
trigger times: 1
Loss after 19261203 batches: 0.0480
trigger times: 2
Loss after 19262166 batches: 0.0478
trigger times: 3
Loss after 19263129 batches: 0.0471
trigger times: 4
Loss after 19264092 batches: 0.0437
trigger times: 5
Loss after 19265055 batches: 0.0418
trigger times: 6
Loss after 19266018 batches: 0.0420
trigger times: 7
Loss after 19266981 batches: 0.0393
trigger times: 8
Loss after 19267944 batches: 0.0391
trigger times: 9
Loss after 19268907 batches: 0.0376
trigger times: 10
Loss after 19269870 batches: 0.0370
trigger times: 11
Loss after 19270833 batches: 0.0368
trigger times: 12
Loss after 19271796 batches: 0.0368
trigger times: 13
Loss after 19272759 batches: 0.0358
trigger times: 14
Loss after 19273722 batches: 0.0350
trigger times: 15
Loss after 19274685 batches: 0.0341
trigger times: 16
Loss after 19275648 batches: 0.0342
trigger times: 17
Loss after 19276611 batches: 0.0344
trigger times: 18
Loss after 19277574 batches: 0.0332
trigger times: 19
Loss after 19278537 batches: 0.0336
trigger times: 20
Loss after 19279500 batches: 0.0336
trigger times: 21
Loss after 19280463 batches: 0.0324
trigger times: 22
Loss after 19281426 batches: 0.0323
trigger times: 23
Loss after 19282389 batches: 0.0323
trigger times: 24
Loss after 19283352 batches: 0.0314
trigger times: 25
Early stopping!
Start to test process.
Loss after 19284315 batches: 0.0325
Time to train on one home:  57.02747559547424
trigger times: 0
Loss after 19285210 batches: 0.0643
trigger times: 1
Loss after 19286105 batches: 0.0327
trigger times: 0
Loss after 19287000 batches: 0.0116
trigger times: 0
Loss after 19287895 batches: 0.0083
trigger times: 1
Loss after 19288790 batches: 0.0057
trigger times: 2
Loss after 19289685 batches: 0.0047
trigger times: 3
Loss after 19290580 batches: 0.0038
trigger times: 4
Loss after 19291475 batches: 0.0034
trigger times: 5
Loss after 19292370 batches: 0.0032
trigger times: 6
Loss after 19293265 batches: 0.0030
trigger times: 7
Loss after 19294160 batches: 0.0029
trigger times: 8
Loss after 19295055 batches: 0.0026
trigger times: 9
Loss after 19295950 batches: 0.0022
trigger times: 10
Loss after 19296845 batches: 0.0022
trigger times: 11
Loss after 19297740 batches: 0.0022
trigger times: 12
Loss after 19298635 batches: 0.0019
trigger times: 13
Loss after 19299530 batches: 0.0020
trigger times: 14
Loss after 19300425 batches: 0.0022
trigger times: 15
Loss after 19301320 batches: 0.0032
trigger times: 16
Loss after 19302215 batches: 0.0058
trigger times: 17
Loss after 19303110 batches: 0.0052
trigger times: 18
Loss after 19304005 batches: 0.0066
trigger times: 19
Loss after 19304900 batches: 0.0055
trigger times: 20
Loss after 19305795 batches: 0.0038
trigger times: 21
Loss after 19306690 batches: 0.0030
trigger times: 22
Loss after 19307585 batches: 0.0023
trigger times: 23
Loss after 19308480 batches: 0.0025
trigger times: 24
Loss after 19309375 batches: 0.0028
trigger times: 25
Early stopping!
Start to test process.
Loss after 19310270 batches: 0.0022
Time to train on one home:  59.96444249153137
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336, 0.034888222413376094, 0.034613204319843105, 0.033508375671873286, 0.03325774057242148, 0.03243952582662181, 0.032562873532530595, 0.03126130941140662, 0.03133166822639105, 0.03049944561800663, 0.030467629500461172]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171], [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71], [0.13675299286842346, 0.15565351472302724, 0.32920502623266784, 1.059933101560365, 0.6549021202780799, 35.010519125969424, 3184.605], [0.0885479673743248, 0.1559912263273432, 0.33358461589199145, 0.944871497166479, 0.654640173496829, 31.209933508474723, 3183.3313], [0.1345348209142685, 0.15444619808844395, 0.3263722552493196, 1.0544752388510252, 0.6558385527073367, 34.830241138150235, 3189.1584], [0.09016867727041245, 0.15349941943138035, 0.3319629557497865, 0.9480173077546179, 0.65657289912243, 31.313842388762147, 3192.7297], [0.13274754583835602, 0.1683317466130475, 0.33338956387040825, 1.0413414780800958, 0.645068476656334, 34.39642151124207, 3136.7866], [0.08685638755559921, 0.16053285093432457, 0.33813218961147645, 0.938309926482314, 0.6511175433777052, 30.99319907910727, 3166.2017], [0.13567858934402466, 0.1592208654199878, 0.3293138590809072, 1.0581373625723918, 0.6521351625870662, 34.951204293654875, 3171.1501], [0.08736597001552582, 0.1548414758248644, 0.3338051677093621, 0.9435825164148742, 0.6555319615580705, 31.16735734476153, 3187.668], [0.13322944939136505, 0.16497793935194727, 0.3304458768859095, 1.0481311306385603, 0.6476697950280367, 34.620689684777616, 3149.4363]]
Round_17_results:  [0.13322944939136505, 0.16497793935194727, 0.3304458768859095, 1.0481311306385603, 0.6476697950280367, 34.620689684777616, 3149.4363]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 20234 < 20235; dropping {'Training_Loss': 0.06460941423262868, 'Validation_Loss': 0.07692039012908936, 'Training_R2': 0.3182873282756268, 'Validation_R2': 0.11448415521482269, 'Training_F1': 0.5103403998644345, 'Validation_F1': 0.3038050719294986, 'Training_NEP': 0.8154101665579457, 'Validation_NEP': 0.9220284923305228, 'Training_NDE': 0.5020503164098576, 'Validation_NDE': 0.6871584484452194, 'Training_MAE': 23.479873801230163, 'Validation_MAE': 30.71298469870352, 'Training_MSE': 1579.5408, 'Validation_MSE': 3403.7705}.
trigger times: 0
Loss after 19311233 batches: 0.0646
trigger times: 1
Loss after 19312196 batches: 0.0454
trigger times: 0
Loss after 19313159 batches: 0.0416
trigger times: 1
Loss after 19314122 batches: 0.0360
trigger times: 2
Loss after 19315085 batches: 0.0326
trigger times: 3
Loss after 19316048 batches: 0.0304
trigger times: 4
Loss after 19317011 batches: 0.0302
trigger times: 5
Loss after 19317974 batches: 0.0292
trigger times: 6
Loss after 19318937 batches: 0.0279
trigger times: 7
Loss after 19319900 batches: 0.0274
trigger times: 8
Loss after 19320863 batches: 0.0268
trigger times: 9
Loss after 19321826 batches: 0.0264
trigger times: 10
Loss after 19322789 batches: 0.0275
trigger times: 11
Loss after 19323752 batches: 0.0287
trigger times: 12
Loss after 19324715 batches: 0.0280
trigger times: 13
Loss after 19325678 batches: 0.0284
trigger times: 14
Loss after 19326641 batches: 0.0279
trigger times: 15
Loss after 19327604 batches: 0.0272
trigger times: 16
Loss after 19328567 batches: 0.0252
trigger times: 17
Loss after 19329530 batches: 0.0253
trigger times: 18
Loss after 19330493 batches: 0.0253
trigger times: 19
Loss after 19331456 batches: 0.0247
trigger times: 20
Loss after 19332419 batches: 0.0233
trigger times: 21
Loss after 19333382 batches: 0.0250
trigger times: 22
Loss after 19334345 batches: 0.0241
trigger times: 23
Loss after 19335308 batches: 0.0233
trigger times: 24
Loss after 19336271 batches: 0.0236
trigger times: 25
Early stopping!
Start to test process.
Loss after 19337234 batches: 0.0224
Time to train on one home:  58.30880880355835
trigger times: 0
Loss after 19338192 batches: 0.0843
trigger times: 0
Loss after 19339150 batches: 0.0437
trigger times: 1
Loss after 19340108 batches: 0.0382
trigger times: 0
Loss after 19341066 batches: 0.0320
trigger times: 0
Loss after 19342024 batches: 0.0281
trigger times: 1
Loss after 19342982 batches: 0.0241
trigger times: 2
Loss after 19343940 batches: 0.0212
trigger times: 0
Loss after 19344898 batches: 0.0204
trigger times: 1
Loss after 19345856 batches: 0.0195
trigger times: 2
Loss after 19346814 batches: 0.0185
trigger times: 3
Loss after 19347772 batches: 0.0176
trigger times: 4
Loss after 19348730 batches: 0.0162
trigger times: 5
Loss after 19349688 batches: 0.0163
trigger times: 0
Loss after 19350646 batches: 0.0163
trigger times: 1
Loss after 19351604 batches: 0.0165
trigger times: 2
Loss after 19352562 batches: 0.0171
trigger times: 3
Loss after 19353520 batches: 0.0172
trigger times: 4
Loss after 19354478 batches: 0.0165
trigger times: 5
Loss after 19355436 batches: 0.0154
trigger times: 6
Loss after 19356394 batches: 0.0152
trigger times: 7
Loss after 19357352 batches: 0.0144
trigger times: 0
Loss after 19358310 batches: 0.0147
trigger times: 1
Loss after 19359268 batches: 0.0143
trigger times: 2
Loss after 19360226 batches: 0.0143
trigger times: 3
Loss after 19361184 batches: 0.0145
trigger times: 0
Loss after 19362142 batches: 0.0138
trigger times: 0
Loss after 19363100 batches: 0.0127
trigger times: 1
Loss after 19364058 batches: 0.0129
trigger times: 2
Loss after 19365016 batches: 0.0129
trigger times: 3
Loss after 19365974 batches: 0.0129
trigger times: 4
Loss after 19366932 batches: 0.0130
trigger times: 5
Loss after 19367890 batches: 0.0135
trigger times: 6
Loss after 19368848 batches: 0.0121
trigger times: 7
Loss after 19369806 batches: 0.0120
trigger times: 8
Loss after 19370764 batches: 0.0128
trigger times: 9
Loss after 19371722 batches: 0.0128
trigger times: 10
Loss after 19372680 batches: 0.0130
trigger times: 11
Loss after 19373638 batches: 0.0130
trigger times: 12
Loss after 19374596 batches: 0.0140
trigger times: 13
Loss after 19375554 batches: 0.0127
trigger times: 14
Loss after 19376512 batches: 0.0128
trigger times: 15
Loss after 19377470 batches: 0.0127
trigger times: 16
Loss after 19378428 batches: 0.0123
trigger times: 17
Loss after 19379386 batches: 0.0126
trigger times: 18
Loss after 19380344 batches: 0.0122
trigger times: 19
Loss after 19381302 batches: 0.0124
trigger times: 20
Loss after 19382260 batches: 0.0119
trigger times: 21
Loss after 19383218 batches: 0.0114
trigger times: 22
Loss after 19384176 batches: 0.0116
trigger times: 0
Loss after 19385134 batches: 0.0117
trigger times: 1
Loss after 19386092 batches: 0.0119
trigger times: 0
Loss after 19387050 batches: 0.0164
trigger times: 0
Loss after 19388008 batches: 0.0145
trigger times: 1
Loss after 19388966 batches: 0.0141
trigger times: 2
Loss after 19389924 batches: 0.0127
trigger times: 3
Loss after 19390882 batches: 0.0129
trigger times: 4
Loss after 19391840 batches: 0.0115
trigger times: 5
Loss after 19392798 batches: 0.0117
trigger times: 6
Loss after 19393756 batches: 0.0108
trigger times: 7
Loss after 19394714 batches: 0.0111
trigger times: 8
Loss after 19395672 batches: 0.0109
trigger times: 9
Loss after 19396630 batches: 0.0121
trigger times: 10
Loss after 19397588 batches: 0.0100
trigger times: 11
Loss after 19398546 batches: 0.0110
trigger times: 12
Loss after 19399504 batches: 0.0102
trigger times: 13
Loss after 19400462 batches: 0.0107
trigger times: 14
Loss after 19401420 batches: 0.0112
trigger times: 15
Loss after 19402378 batches: 0.0113
trigger times: 16
Loss after 19403336 batches: 0.0108
trigger times: 17
Loss after 19404294 batches: 0.0104
trigger times: 18
Loss after 19405252 batches: 0.0111
trigger times: 19
Loss after 19406210 batches: 0.0108
trigger times: 20
Loss after 19407168 batches: 0.0107
trigger times: 21
Loss after 19408126 batches: 0.0110
trigger times: 0
Loss after 19409084 batches: 0.0113
trigger times: 1
Loss after 19410042 batches: 0.0108
trigger times: 2
Loss after 19411000 batches: 0.0106
trigger times: 3
Loss after 19411958 batches: 0.0099
trigger times: 4
Loss after 19412916 batches: 0.0094
trigger times: 5
Loss after 19413874 batches: 0.0097
trigger times: 6
Loss after 19414832 batches: 0.0095
trigger times: 7
Loss after 19415790 batches: 0.0095
trigger times: 8
Loss after 19416748 batches: 0.0095
trigger times: 9
Loss after 19417706 batches: 0.0092
trigger times: 10
Loss after 19418664 batches: 0.0099
trigger times: 11
Loss after 19419622 batches: 0.0095
trigger times: 12
Loss after 19420580 batches: 0.0097
trigger times: 13
Loss after 19421538 batches: 0.0106
trigger times: 14
Loss after 19422496 batches: 0.0108
trigger times: 15
Loss after 19423454 batches: 0.0109
trigger times: 16
Loss after 19424412 batches: 0.0100
trigger times: 17
Loss after 19425370 batches: 0.0101
trigger times: 18
Loss after 19426328 batches: 0.0090
trigger times: 19
Loss after 19427286 batches: 0.0101
trigger times: 20
Loss after 19428244 batches: 0.0090
trigger times: 21
Loss after 19429202 batches: 0.0087
trigger times: 22
Loss after 19430160 batches: 0.0085
trigger times: 23
Loss after 19431118 batches: 0.0086
trigger times: 24
Loss after 19432076 batches: 0.0086
trigger times: 25
Early stopping!
Start to test process.
Loss after 19433034 batches: 0.0085
Time to train on one home:  118.54783177375793
trigger times: 0
Loss after 19433997 batches: 0.0863
trigger times: 1
Loss after 19434960 batches: 0.0700
trigger times: 2
Loss after 19435923 batches: 0.0663
trigger times: 3
Loss after 19436886 batches: 0.0650
trigger times: 4
Loss after 19437849 batches: 0.0630
trigger times: 5
Loss after 19438812 batches: 0.0601
trigger times: 6
Loss after 19439775 batches: 0.0567
trigger times: 7
Loss after 19440738 batches: 0.0559
trigger times: 8
Loss after 19441701 batches: 0.0523
trigger times: 9
Loss after 19442664 batches: 0.0513
trigger times: 10
Loss after 19443627 batches: 0.0495
trigger times: 11
Loss after 19444590 batches: 0.0477
trigger times: 12
Loss after 19445553 batches: 0.0474
trigger times: 13
Loss after 19446516 batches: 0.0462
trigger times: 14
Loss after 19447479 batches: 0.0464
trigger times: 15
Loss after 19448442 batches: 0.0456
trigger times: 16
Loss after 19449405 batches: 0.0461
trigger times: 17
Loss after 19450368 batches: 0.0444
trigger times: 18
Loss after 19451331 batches: 0.0444
trigger times: 19
Loss after 19452294 batches: 0.0443
trigger times: 20
Loss after 19453257 batches: 0.0440
trigger times: 21
Loss after 19454220 batches: 0.0423
trigger times: 22
Loss after 19455183 batches: 0.0414
trigger times: 23
Loss after 19456146 batches: 0.0412
trigger times: 24
Loss after 19457109 batches: 0.0408
trigger times: 25
Early stopping!
Start to test process.
Loss after 19458072 batches: 0.0403
Time to train on one home:  56.42615270614624
trigger times: 0
Loss after 19459035 batches: 0.0909
trigger times: 1
Loss after 19459998 batches: 0.0777
trigger times: 2
Loss after 19460961 batches: 0.0760
trigger times: 3
Loss after 19461924 batches: 0.0735
trigger times: 4
Loss after 19462887 batches: 0.0705
trigger times: 5
Loss after 19463850 batches: 0.0679
trigger times: 6
Loss after 19464813 batches: 0.0635
trigger times: 7
Loss after 19465776 batches: 0.0629
trigger times: 8
Loss after 19466739 batches: 0.0620
trigger times: 9
Loss after 19467702 batches: 0.0601
trigger times: 10
Loss after 19468665 batches: 0.0597
trigger times: 11
Loss after 19469628 batches: 0.0590
trigger times: 12
Loss after 19470591 batches: 0.0587
trigger times: 13
Loss after 19471554 batches: 0.0565
trigger times: 14
Loss after 19472517 batches: 0.0566
trigger times: 15
Loss after 19473480 batches: 0.0567
trigger times: 16
Loss after 19474443 batches: 0.0559
trigger times: 17
Loss after 19475406 batches: 0.0555
trigger times: 18
Loss after 19476369 batches: 0.0556
trigger times: 19
Loss after 19477332 batches: 0.0541
trigger times: 20
Loss after 19478295 batches: 0.0544
trigger times: 21
Loss after 19479258 batches: 0.0552
trigger times: 22
Loss after 19480221 batches: 0.0540
trigger times: 23
Loss after 19481184 batches: 0.0537
trigger times: 24
Loss after 19482147 batches: 0.0525
trigger times: 25
Early stopping!
Start to test process.
Loss after 19483110 batches: 0.0508
Time to train on one home:  52.90116548538208
trigger times: 0
Loss after 19484073 batches: 0.0245
trigger times: 1
Loss after 19485036 batches: 0.0207
trigger times: 2
Loss after 19485999 batches: 0.0191
trigger times: 0
Loss after 19486962 batches: 0.0167
trigger times: 1
Loss after 19487925 batches: 0.0154
trigger times: 2
Loss after 19488888 batches: 0.0145
trigger times: 3
Loss after 19489851 batches: 0.0137
trigger times: 4
Loss after 19490814 batches: 0.0138
trigger times: 5
Loss after 19491777 batches: 0.0133
trigger times: 6
Loss after 19492740 batches: 0.0132
trigger times: 7
Loss after 19493703 batches: 0.0126
trigger times: 8
Loss after 19494666 batches: 0.0119
trigger times: 9
Loss after 19495629 batches: 0.0125
trigger times: 10
Loss after 19496592 batches: 0.0120
trigger times: 11
Loss after 19497555 batches: 0.0119
trigger times: 12
Loss after 19498518 batches: 0.0116
trigger times: 13
Loss after 19499481 batches: 0.0114
trigger times: 14
Loss after 19500444 batches: 0.0115
trigger times: 15
Loss after 19501407 batches: 0.0118
trigger times: 16
Loss after 19502370 batches: 0.0120
trigger times: 17
Loss after 19503333 batches: 0.0113
trigger times: 18
Loss after 19504296 batches: 0.0122
trigger times: 19
Loss after 19505259 batches: 0.0125
trigger times: 20
Loss after 19506222 batches: 0.0115
trigger times: 21
Loss after 19507185 batches: 0.0115
trigger times: 22
Loss after 19508148 batches: 0.0112
trigger times: 23
Loss after 19509111 batches: 0.0111
trigger times: 24
Loss after 19510074 batches: 0.0109
trigger times: 25
Early stopping!
Start to test process.
Loss after 19511037 batches: 0.0108
Time to train on one home:  58.546408891677856
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19512000 batches: 0.0567
trigger times: 1
Loss after 19512963 batches: 0.0245
trigger times: 2
Loss after 19513926 batches: 0.0198
trigger times: 3
Loss after 19514889 batches: 0.0160
trigger times: 4
Loss after 19515852 batches: 0.0154
trigger times: 5
Loss after 19516815 batches: 0.0142
trigger times: 6
Loss after 19517778 batches: 0.0141
trigger times: 7
Loss after 19518741 batches: 0.0132
trigger times: 8
Loss after 19519704 batches: 0.0130
trigger times: 9
Loss after 19520667 batches: 0.0125
trigger times: 10
Loss after 19521630 batches: 0.0125
trigger times: 11
Loss after 19522593 batches: 0.0123
trigger times: 12
Loss after 19523556 batches: 0.0120
trigger times: 13
Loss after 19524519 batches: 0.0118
trigger times: 14
Loss after 19525482 batches: 0.0119
trigger times: 15
Loss after 19526445 batches: 0.0118
trigger times: 16
Loss after 19527408 batches: 0.0116
trigger times: 17
Loss after 19528371 batches: 0.0117
trigger times: 18
Loss after 19529334 batches: 0.0116
trigger times: 19
Loss after 19530297 batches: 0.0116
trigger times: 20
Loss after 19531260 batches: 0.0115
trigger times: 21
Loss after 19532223 batches: 0.0112
trigger times: 22
Loss after 19533186 batches: 0.0113
trigger times: 23
Loss after 19534149 batches: 0.0112
trigger times: 24
Loss after 19535112 batches: 0.0111
trigger times: 25
Early stopping!
Start to test process.
Loss after 19536075 batches: 0.0110
Time to train on one home:  57.004876136779785
trigger times: 0
Loss after 19537038 batches: 0.0996
trigger times: 0
Loss after 19538001 batches: 0.0905
trigger times: 0
Loss after 19538964 batches: 0.0847
trigger times: 1
Loss after 19539927 batches: 0.0794
trigger times: 2
Loss after 19540890 batches: 0.0757
trigger times: 3
Loss after 19541853 batches: 0.0717
trigger times: 4
Loss after 19542816 batches: 0.0696
trigger times: 5
Loss after 19543779 batches: 0.0675
trigger times: 6
Loss after 19544742 batches: 0.0658
trigger times: 7
Loss after 19545705 batches: 0.0629
trigger times: 8
Loss after 19546668 batches: 0.0625
trigger times: 9
Loss after 19547631 batches: 0.0624
trigger times: 10
Loss after 19548594 batches: 0.0608
trigger times: 11
Loss after 19549557 batches: 0.0595
trigger times: 12
Loss after 19550520 batches: 0.0595
trigger times: 13
Loss after 19551483 batches: 0.0568
trigger times: 14
Loss after 19552446 batches: 0.0583
trigger times: 15
Loss after 19553409 batches: 0.0576
trigger times: 16
Loss after 19554372 batches: 0.0561
trigger times: 17
Loss after 19555335 batches: 0.0601
trigger times: 18
Loss after 19556298 batches: 0.0588
trigger times: 19
Loss after 19557261 batches: 0.0561
trigger times: 20
Loss after 19558224 batches: 0.0565
trigger times: 21
Loss after 19559187 batches: 0.0591
trigger times: 22
Loss after 19560150 batches: 0.0570
trigger times: 23
Loss after 19561113 batches: 0.0573
trigger times: 24
Loss after 19562076 batches: 0.0550
trigger times: 25
Early stopping!
Start to test process.
Loss after 19563039 batches: 0.0546
Time to train on one home:  56.061137676239014
trigger times: 0
Loss after 19564002 batches: 0.0501
trigger times: 0
Loss after 19564965 batches: 0.0395
trigger times: 0
Loss after 19565928 batches: 0.0308
trigger times: 1
Loss after 19566891 batches: 0.0274
trigger times: 2
Loss after 19567854 batches: 0.0254
trigger times: 3
Loss after 19568817 batches: 0.0231
trigger times: 4
Loss after 19569780 batches: 0.0221
trigger times: 5
Loss after 19570743 batches: 0.0222
trigger times: 6
Loss after 19571706 batches: 0.0222
trigger times: 7
Loss after 19572669 batches: 0.0209
trigger times: 8
Loss after 19573632 batches: 0.0194
trigger times: 9
Loss after 19574595 batches: 0.0196
trigger times: 10
Loss after 19575558 batches: 0.0195
trigger times: 11
Loss after 19576521 batches: 0.0182
trigger times: 12
Loss after 19577484 batches: 0.0183
trigger times: 13
Loss after 19578447 batches: 0.0182
trigger times: 14
Loss after 19579410 batches: 0.0168
trigger times: 15
Loss after 19580373 batches: 0.0178
trigger times: 16
Loss after 19581336 batches: 0.0189
trigger times: 17
Loss after 19582299 batches: 0.0192
trigger times: 18
Loss after 19583262 batches: 0.0187
trigger times: 19
Loss after 19584225 batches: 0.0185
trigger times: 20
Loss after 19585188 batches: 0.0174
trigger times: 21
Loss after 19586151 batches: 0.0168
trigger times: 22
Loss after 19587114 batches: 0.0176
trigger times: 23
Loss after 19588077 batches: 0.0168
trigger times: 24
Loss after 19589040 batches: 0.0167
trigger times: 25
Early stopping!
Start to test process.
Loss after 19590003 batches: 0.0172
Time to train on one home:  61.78139615058899
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19590966 batches: 0.0762
trigger times: 1
Loss after 19591929 batches: 0.0707
trigger times: 2
Loss after 19592892 batches: 0.0662
trigger times: 3
Loss after 19593855 batches: 0.0629
trigger times: 4
Loss after 19594818 batches: 0.0596
trigger times: 5
Loss after 19595781 batches: 0.0587
trigger times: 6
Loss after 19596744 batches: 0.0586
trigger times: 7
Loss after 19597707 batches: 0.0575
trigger times: 8
Loss after 19598670 batches: 0.0569
trigger times: 9
Loss after 19599633 batches: 0.0550
trigger times: 10
Loss after 19600596 batches: 0.0547
trigger times: 11
Loss after 19601559 batches: 0.0545
trigger times: 12
Loss after 19602522 batches: 0.0555
trigger times: 13
Loss after 19603485 batches: 0.0542
trigger times: 14
Loss after 19604448 batches: 0.0536
trigger times: 15
Loss after 19605411 batches: 0.0527
trigger times: 16
Loss after 19606374 batches: 0.0517
trigger times: 17
Loss after 19607337 batches: 0.0524
trigger times: 18
Loss after 19608300 batches: 0.0524
trigger times: 19
Loss after 19609263 batches: 0.0526
trigger times: 20
Loss after 19610226 batches: 0.0527
trigger times: 21
Loss after 19611189 batches: 0.0514
trigger times: 22
Loss after 19612152 batches: 0.0515
trigger times: 23
Loss after 19613115 batches: 0.0507
trigger times: 24
Loss after 19614078 batches: 0.0507
trigger times: 25
Early stopping!
Start to test process.
Loss after 19615041 batches: 0.0516
Time to train on one home:  56.288684606552124
trigger times: 0
Loss after 19616004 batches: 0.1051
trigger times: 0
Loss after 19616967 batches: 0.0640
trigger times: 0
Loss after 19617930 batches: 0.0590
trigger times: 1
Loss after 19618893 batches: 0.0500
trigger times: 2
Loss after 19619856 batches: 0.0484
trigger times: 3
Loss after 19620819 batches: 0.0451
trigger times: 4
Loss after 19621782 batches: 0.0437
trigger times: 5
Loss after 19622745 batches: 0.0413
trigger times: 6
Loss after 19623708 batches: 0.0402
trigger times: 7
Loss after 19624671 batches: 0.0398
trigger times: 8
Loss after 19625634 batches: 0.0405
trigger times: 9
Loss after 19626597 batches: 0.0405
trigger times: 10
Loss after 19627560 batches: 0.0389
trigger times: 11
Loss after 19628523 batches: 0.0391
trigger times: 12
Loss after 19629486 batches: 0.0378
trigger times: 13
Loss after 19630449 batches: 0.0376
trigger times: 14
Loss after 19631412 batches: 0.0363
trigger times: 15
Loss after 19632375 batches: 0.0366
trigger times: 16
Loss after 19633338 batches: 0.0365
trigger times: 17
Loss after 19634301 batches: 0.0361
trigger times: 18
Loss after 19635264 batches: 0.0355
trigger times: 19
Loss after 19636227 batches: 0.0356
trigger times: 20
Loss after 19637190 batches: 0.0357
trigger times: 21
Loss after 19638153 batches: 0.0346
trigger times: 22
Loss after 19639116 batches: 0.0361
trigger times: 23
Loss after 19640079 batches: 0.0360
trigger times: 24
Loss after 19641042 batches: 0.0358
trigger times: 25
Early stopping!
Start to test process.
Loss after 19642005 batches: 0.0361
Time to train on one home:  58.41854810714722
trigger times: 0
Loss after 19642968 batches: 0.0748
trigger times: 1
Loss after 19643931 batches: 0.0676
trigger times: 2
Loss after 19644894 batches: 0.0655
trigger times: 3
Loss after 19645857 batches: 0.0624
trigger times: 4
Loss after 19646820 batches: 0.0603
trigger times: 5
Loss after 19647783 batches: 0.0579
trigger times: 6
Loss after 19648746 batches: 0.0569
trigger times: 7
Loss after 19649709 batches: 0.0560
trigger times: 8
Loss after 19650672 batches: 0.0547
trigger times: 9
Loss after 19651635 batches: 0.0531
trigger times: 10
Loss after 19652598 batches: 0.0510
trigger times: 11
Loss after 19653561 batches: 0.0521
trigger times: 12
Loss after 19654524 batches: 0.0516
trigger times: 13
Loss after 19655487 batches: 0.0505
trigger times: 14
Loss after 19656450 batches: 0.0496
trigger times: 15
Loss after 19657413 batches: 0.0506
trigger times: 16
Loss after 19658376 batches: 0.0493
trigger times: 17
Loss after 19659339 batches: 0.0503
trigger times: 18
Loss after 19660302 batches: 0.0494
trigger times: 19
Loss after 19661265 batches: 0.0486
trigger times: 20
Loss after 19662228 batches: 0.0487
trigger times: 21
Loss after 19663191 batches: 0.0461
trigger times: 22
Loss after 19664154 batches: 0.0466
trigger times: 23
Loss after 19665117 batches: 0.0462
trigger times: 24
Loss after 19666080 batches: 0.0465
trigger times: 25
Early stopping!
Start to test process.
Loss after 19667043 batches: 0.0456
Time to train on one home:  57.7480731010437
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19668006 batches: 0.0634
trigger times: 1
Loss after 19668969 batches: 0.0491
trigger times: 2
Loss after 19669932 batches: 0.0457
trigger times: 3
Loss after 19670895 batches: 0.0385
trigger times: 4
Loss after 19671858 batches: 0.0346
trigger times: 5
Loss after 19672821 batches: 0.0326
trigger times: 6
Loss after 19673784 batches: 0.0304
trigger times: 7
Loss after 19674747 batches: 0.0285
trigger times: 8
Loss after 19675710 batches: 0.0272
trigger times: 9
Loss after 19676673 batches: 0.0261
trigger times: 10
Loss after 19677636 batches: 0.0254
trigger times: 11
Loss after 19678599 batches: 0.0260
trigger times: 12
Loss after 19679562 batches: 0.0259
trigger times: 13
Loss after 19680525 batches: 0.0249
trigger times: 14
Loss after 19681488 batches: 0.0243
trigger times: 15
Loss after 19682451 batches: 0.0238
trigger times: 16
Loss after 19683414 batches: 0.0233
trigger times: 17
Loss after 19684377 batches: 0.0235
trigger times: 18
Loss after 19685340 batches: 0.0225
trigger times: 19
Loss after 19686303 batches: 0.0222
trigger times: 20
Loss after 19687266 batches: 0.0227
trigger times: 21
Loss after 19688229 batches: 0.0245
trigger times: 22
Loss after 19689192 batches: 0.0248
trigger times: 23
Loss after 19690155 batches: 0.0237
trigger times: 24
Loss after 19691118 batches: 0.0228
trigger times: 25
Early stopping!
Start to test process.
Loss after 19692081 batches: 0.0228
Time to train on one home:  57.384549617767334
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19693044 batches: 0.0908
trigger times: 0
Loss after 19694007 batches: 0.0532
trigger times: 1
Loss after 19694970 batches: 0.0410
trigger times: 0
Loss after 19695933 batches: 0.0411
trigger times: 0
Loss after 19696896 batches: 0.0398
trigger times: 0
Loss after 19697859 batches: 0.0355
trigger times: 1
Loss after 19698822 batches: 0.0316
trigger times: 2
Loss after 19699785 batches: 0.0293
trigger times: 3
Loss after 19700748 batches: 0.0266
trigger times: 4
Loss after 19701711 batches: 0.0242
trigger times: 5
Loss after 19702674 batches: 0.0235
trigger times: 6
Loss after 19703637 batches: 0.0234
trigger times: 7
Loss after 19704600 batches: 0.0217
trigger times: 8
Loss after 19705563 batches: 0.0216
trigger times: 9
Loss after 19706526 batches: 0.0227
trigger times: 10
Loss after 19707489 batches: 0.0216
trigger times: 11
Loss after 19708452 batches: 0.0215
trigger times: 12
Loss after 19709415 batches: 0.0209
trigger times: 13
Loss after 19710378 batches: 0.0208
trigger times: 14
Loss after 19711341 batches: 0.0198
trigger times: 15
Loss after 19712304 batches: 0.0207
trigger times: 16
Loss after 19713267 batches: 0.0187
trigger times: 17
Loss after 19714230 batches: 0.0189
trigger times: 18
Loss after 19715193 batches: 0.0178
trigger times: 19
Loss after 19716156 batches: 0.0174
trigger times: 20
Loss after 19717119 batches: 0.0181
trigger times: 21
Loss after 19718082 batches: 0.0177
trigger times: 22
Loss after 19719045 batches: 0.0177
trigger times: 23
Loss after 19720008 batches: 0.0173
trigger times: 24
Loss after 19720971 batches: 0.0171
trigger times: 25
Early stopping!
Start to test process.
Loss after 19721934 batches: 0.0166
Time to train on one home:  57.994580030441284
trigger times: 0
Loss after 19722863 batches: 0.1247
trigger times: 0
Loss after 19723792 batches: 0.0701
trigger times: 0
Loss after 19724721 batches: 0.0526
trigger times: 0
Loss after 19725650 batches: 0.0415
trigger times: 1
Loss after 19726579 batches: 0.0360
trigger times: 0
Loss after 19727508 batches: 0.0324
trigger times: 0
Loss after 19728437 batches: 0.0312
trigger times: 1
Loss after 19729366 batches: 0.0281
trigger times: 2
Loss after 19730295 batches: 0.0291
trigger times: 3
Loss after 19731224 batches: 0.0269
trigger times: 4
Loss after 19732153 batches: 0.0261
trigger times: 5
Loss after 19733082 batches: 0.0259
trigger times: 6
Loss after 19734011 batches: 0.0245
trigger times: 7
Loss after 19734940 batches: 0.0234
trigger times: 8
Loss after 19735869 batches: 0.0238
trigger times: 9
Loss after 19736798 batches: 0.0244
trigger times: 10
Loss after 19737727 batches: 0.0257
trigger times: 11
Loss after 19738656 batches: 0.0254
trigger times: 12
Loss after 19739585 batches: 0.0276
trigger times: 13
Loss after 19740514 batches: 0.0259
trigger times: 14
Loss after 19741443 batches: 0.0248
trigger times: 15
Loss after 19742372 batches: 0.0216
trigger times: 16
Loss after 19743301 batches: 0.0211
trigger times: 17
Loss after 19744230 batches: 0.0215
trigger times: 18
Loss after 19745159 batches: 0.0228
trigger times: 19
Loss after 19746088 batches: 0.0202
trigger times: 20
Loss after 19747017 batches: 0.0205
trigger times: 21
Loss after 19747946 batches: 0.0223
trigger times: 22
Loss after 19748875 batches: 0.0207
trigger times: 23
Loss after 19749804 batches: 0.0220
trigger times: 24
Loss after 19750733 batches: 0.0215
trigger times: 25
Early stopping!
Start to test process.
Loss after 19751662 batches: 0.0277
Time to train on one home:  58.553566217422485
trigger times: 0
Loss after 19752624 batches: 0.0725
trigger times: 1
Loss after 19753586 batches: 0.0649
trigger times: 2
Loss after 19754548 batches: 0.0626
trigger times: 3
Loss after 19755510 batches: 0.0603
trigger times: 4
Loss after 19756472 batches: 0.0571
trigger times: 5
Loss after 19757434 batches: 0.0548
trigger times: 6
Loss after 19758396 batches: 0.0536
trigger times: 7
Loss after 19759358 batches: 0.0529
trigger times: 8
Loss after 19760320 batches: 0.0533
trigger times: 9
Loss after 19761282 batches: 0.0523
trigger times: 10
Loss after 19762244 batches: 0.0514
trigger times: 11
Loss after 19763206 batches: 0.0507
trigger times: 12
Loss after 19764168 batches: 0.0500
trigger times: 13
Loss after 19765130 batches: 0.0500
trigger times: 14
Loss after 19766092 batches: 0.0498
trigger times: 15
Loss after 19767054 batches: 0.0489
trigger times: 16
Loss after 19768016 batches: 0.0483
trigger times: 17
Loss after 19768978 batches: 0.0481
trigger times: 18
Loss after 19769940 batches: 0.0487
trigger times: 19
Loss after 19770902 batches: 0.0483
trigger times: 20
Loss after 19771864 batches: 0.0476
trigger times: 21
Loss after 19772826 batches: 0.0486
trigger times: 22
Loss after 19773788 batches: 0.0493
trigger times: 23
Loss after 19774750 batches: 0.0492
trigger times: 24
Loss after 19775712 batches: 0.0485
trigger times: 25
Early stopping!
Start to test process.
Loss after 19776674 batches: 0.0469
Time to train on one home:  56.94717478752136
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19777637 batches: 0.0580
trigger times: 1
Loss after 19778600 batches: 0.0439
trigger times: 2
Loss after 19779563 batches: 0.0441
trigger times: 3
Loss after 19780526 batches: 0.0405
trigger times: 4
Loss after 19781489 batches: 0.0367
trigger times: 5
Loss after 19782452 batches: 0.0355
trigger times: 6
Loss after 19783415 batches: 0.0345
trigger times: 7
Loss after 19784378 batches: 0.0328
trigger times: 8
Loss after 19785341 batches: 0.0326
trigger times: 9
Loss after 19786304 batches: 0.0315
trigger times: 10
Loss after 19787267 batches: 0.0318
trigger times: 11
Loss after 19788230 batches: 0.0312
trigger times: 12
Loss after 19789193 batches: 0.0306
trigger times: 13
Loss after 19790156 batches: 0.0301
trigger times: 14
Loss after 19791119 batches: 0.0292
trigger times: 15
Loss after 19792082 batches: 0.0288
trigger times: 16
Loss after 19793045 batches: 0.0291
trigger times: 17
Loss after 19794008 batches: 0.0291
trigger times: 18
Loss after 19794971 batches: 0.0283
trigger times: 19
Loss after 19795934 batches: 0.0277
trigger times: 20
Loss after 19796897 batches: 0.0276
trigger times: 21
Loss after 19797860 batches: 0.0275
trigger times: 22
Loss after 19798823 batches: 0.0274
trigger times: 23
Loss after 19799786 batches: 0.0268
trigger times: 24
Loss after 19800749 batches: 0.0275
trigger times: 25
Early stopping!
Start to test process.
Loss after 19801712 batches: 0.0265
Time to train on one home:  55.56675171852112
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19802675 batches: 0.0536
trigger times: 0
Loss after 19803638 batches: 0.0426
trigger times: 0
Loss after 19804601 batches: 0.0418
trigger times: 1
Loss after 19805564 batches: 0.0380
trigger times: 0
Loss after 19806527 batches: 0.0355
trigger times: 1
Loss after 19807490 batches: 0.0336
trigger times: 2
Loss after 19808453 batches: 0.0326
trigger times: 3
Loss after 19809416 batches: 0.0316
trigger times: 4
Loss after 19810379 batches: 0.0324
trigger times: 5
Loss after 19811342 batches: 0.0321
trigger times: 6
Loss after 19812305 batches: 0.0303
trigger times: 7
Loss after 19813268 batches: 0.0298
trigger times: 8
Loss after 19814231 batches: 0.0295
trigger times: 9
Loss after 19815194 batches: 0.0288
trigger times: 10
Loss after 19816157 batches: 0.0282
trigger times: 11
Loss after 19817120 batches: 0.0270
trigger times: 12
Loss after 19818083 batches: 0.0279
trigger times: 13
Loss after 19819046 batches: 0.0261
trigger times: 14
Loss after 19820009 batches: 0.0269
trigger times: 15
Loss after 19820972 batches: 0.0260
trigger times: 16
Loss after 19821935 batches: 0.0260
trigger times: 17
Loss after 19822898 batches: 0.0252
trigger times: 18
Loss after 19823861 batches: 0.0251
trigger times: 19
Loss after 19824824 batches: 0.0258
trigger times: 20
Loss after 19825787 batches: 0.0266
trigger times: 21
Loss after 19826750 batches: 0.0268
trigger times: 22
Loss after 19827713 batches: 0.0292
trigger times: 23
Loss after 19828676 batches: 0.0282
trigger times: 24
Loss after 19829639 batches: 0.0274
trigger times: 25
Early stopping!
Start to test process.
Loss after 19830602 batches: 0.0259
Time to train on one home:  58.70192217826843
trigger times: 0
Loss after 19831565 batches: 0.1072
trigger times: 0
Loss after 19832528 batches: 0.0927
trigger times: 1
Loss after 19833491 batches: 0.0879
trigger times: 2
Loss after 19834454 batches: 0.0830
trigger times: 3
Loss after 19835417 batches: 0.0801
trigger times: 4
Loss after 19836380 batches: 0.0771
trigger times: 5
Loss after 19837343 batches: 0.0773
trigger times: 6
Loss after 19838306 batches: 0.0723
trigger times: 7
Loss after 19839269 batches: 0.0715
trigger times: 8
Loss after 19840232 batches: 0.0715
trigger times: 9
Loss after 19841195 batches: 0.0698
trigger times: 10
Loss after 19842158 batches: 0.0687
trigger times: 11
Loss after 19843121 batches: 0.0681
trigger times: 12
Loss after 19844084 batches: 0.0709
trigger times: 13
Loss after 19845047 batches: 0.0756
trigger times: 14
Loss after 19846010 batches: 0.0737
trigger times: 15
Loss after 19846973 batches: 0.0688
trigger times: 16
Loss after 19847936 batches: 0.0691
trigger times: 17
Loss after 19848899 batches: 0.0674
trigger times: 18
Loss after 19849862 batches: 0.0647
trigger times: 19
Loss after 19850825 batches: 0.0641
trigger times: 20
Loss after 19851788 batches: 0.0636
trigger times: 21
Loss after 19852751 batches: 0.0646
trigger times: 22
Loss after 19853714 batches: 0.0627
trigger times: 23
Loss after 19854677 batches: 0.0618
trigger times: 24
Loss after 19855640 batches: 0.0606
trigger times: 25
Early stopping!
Start to test process.
Loss after 19856603 batches: 0.0615
Time to train on one home:  59.920531034469604
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19857566 batches: 0.1092
trigger times: 1
Loss after 19858529 batches: 0.0537
trigger times: 2
Loss after 19859492 batches: 0.0554
trigger times: 0
Loss after 19860455 batches: 0.0461
trigger times: 0
Loss after 19861418 batches: 0.0417
trigger times: 1
Loss after 19862381 batches: 0.0379
trigger times: 2
Loss after 19863344 batches: 0.0359
trigger times: 0
Loss after 19864307 batches: 0.0341
trigger times: 1
Loss after 19865270 batches: 0.0324
trigger times: 2
Loss after 19866233 batches: 0.0315
trigger times: 0
Loss after 19867196 batches: 0.0303
trigger times: 1
Loss after 19868159 batches: 0.0288
trigger times: 2
Loss after 19869122 batches: 0.0283
trigger times: 3
Loss after 19870085 batches: 0.0281
trigger times: 4
Loss after 19871048 batches: 0.0272
trigger times: 5
Loss after 19872011 batches: 0.0267
trigger times: 6
Loss after 19872974 batches: 0.0270
trigger times: 7
Loss after 19873937 batches: 0.0271
trigger times: 8
Loss after 19874900 batches: 0.0276
trigger times: 9
Loss after 19875863 batches: 0.0274
trigger times: 10
Loss after 19876826 batches: 0.0266
trigger times: 11
Loss after 19877789 batches: 0.0262
trigger times: 12
Loss after 19878752 batches: 0.0250
trigger times: 13
Loss after 19879715 batches: 0.0276
trigger times: 14
Loss after 19880678 batches: 0.0274
trigger times: 15
Loss after 19881641 batches: 0.0257
trigger times: 16
Loss after 19882604 batches: 0.0251
trigger times: 17
Loss after 19883567 batches: 0.0243
trigger times: 18
Loss after 19884530 batches: 0.0233
trigger times: 19
Loss after 19885493 batches: 0.0235
trigger times: 20
Loss after 19886456 batches: 0.0234
trigger times: 21
Loss after 19887419 batches: 0.0237
trigger times: 22
Loss after 19888382 batches: 0.0231
trigger times: 23
Loss after 19889345 batches: 0.0232
trigger times: 24
Loss after 19890308 batches: 0.0235
trigger times: 25
Early stopping!
Start to test process.
Loss after 19891271 batches: 0.0241
Time to train on one home:  64.89515805244446
trigger times: 0
Loss after 19892230 batches: 0.1158
trigger times: 1
Loss after 19893189 batches: 0.0537
trigger times: 0
Loss after 19894148 batches: 0.0411
trigger times: 1
Loss after 19895107 batches: 0.0307
trigger times: 0
Loss after 19896066 batches: 0.0257
trigger times: 0
Loss after 19897025 batches: 0.0238
trigger times: 0
Loss after 19897984 batches: 0.0231
trigger times: 1
Loss after 19898943 batches: 0.0218
trigger times: 2
Loss after 19899902 batches: 0.0198
trigger times: 3
Loss after 19900861 batches: 0.0192
trigger times: 4
Loss after 19901820 batches: 0.0187
trigger times: 5
Loss after 19902779 batches: 0.0178
trigger times: 6
Loss after 19903738 batches: 0.0172
trigger times: 7
Loss after 19904697 batches: 0.0172
trigger times: 8
Loss after 19905656 batches: 0.0168
trigger times: 9
Loss after 19906615 batches: 0.0164
trigger times: 10
Loss after 19907574 batches: 0.0168
trigger times: 11
Loss after 19908533 batches: 0.0156
trigger times: 12
Loss after 19909492 batches: 0.0149
trigger times: 13
Loss after 19910451 batches: 0.0160
trigger times: 14
Loss after 19911410 batches: 0.0152
trigger times: 15
Loss after 19912369 batches: 0.0157
trigger times: 16
Loss after 19913328 batches: 0.0146
trigger times: 17
Loss after 19914287 batches: 0.0150
trigger times: 18
Loss after 19915246 batches: 0.0154
trigger times: 19
Loss after 19916205 batches: 0.0146
trigger times: 20
Loss after 19917164 batches: 0.0148
trigger times: 21
Loss after 19918123 batches: 0.0142
trigger times: 22
Loss after 19919082 batches: 0.0148
trigger times: 23
Loss after 19920041 batches: 0.0144
trigger times: 24
Loss after 19921000 batches: 0.0148
trigger times: 25
Early stopping!
Start to test process.
Loss after 19921959 batches: 0.0135
Time to train on one home:  61.906090259552
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19922922 batches: 0.0532
trigger times: 1
Loss after 19923885 batches: 0.0280
trigger times: 2
Loss after 19924848 batches: 0.0259
trigger times: 3
Loss after 19925811 batches: 0.0254
trigger times: 4
Loss after 19926774 batches: 0.0240
trigger times: 5
Loss after 19927737 batches: 0.0227
trigger times: 6
Loss after 19928700 batches: 0.0218
trigger times: 7
Loss after 19929663 batches: 0.0202
trigger times: 8
Loss after 19930626 batches: 0.0196
trigger times: 9
Loss after 19931589 batches: 0.0188
trigger times: 10
Loss after 19932552 batches: 0.0184
trigger times: 11
Loss after 19933515 batches: 0.0181
trigger times: 12
Loss after 19934478 batches: 0.0176
trigger times: 13
Loss after 19935441 batches: 0.0175
trigger times: 14
Loss after 19936404 batches: 0.0172
trigger times: 15
Loss after 19937367 batches: 0.0171
trigger times: 16
Loss after 19938330 batches: 0.0171
trigger times: 17
Loss after 19939293 batches: 0.0169
trigger times: 18
Loss after 19940256 batches: 0.0167
trigger times: 19
Loss after 19941219 batches: 0.0170
trigger times: 20
Loss after 19942182 batches: 0.0165
trigger times: 21
Loss after 19943145 batches: 0.0166
trigger times: 22
Loss after 19944108 batches: 0.0159
trigger times: 23
Loss after 19945071 batches: 0.0164
trigger times: 24
Loss after 19946034 batches: 0.0159
trigger times: 25
Early stopping!
Start to test process.
Loss after 19946997 batches: 0.0161
Time to train on one home:  57.373478412628174
trigger times: 0
Loss after 19947942 batches: 0.0773
trigger times: 0
Loss after 19948887 batches: 0.0530
trigger times: 0
Loss after 19949832 batches: 0.0382
trigger times: 1
Loss after 19950777 batches: 0.0336
trigger times: 2
Loss after 19951722 batches: 0.0324
trigger times: 3
Loss after 19952667 batches: 0.0300
trigger times: 4
Loss after 19953612 batches: 0.0265
trigger times: 5
Loss after 19954557 batches: 0.0237
trigger times: 6
Loss after 19955502 batches: 0.0229
trigger times: 7
Loss after 19956447 batches: 0.0219
trigger times: 8
Loss after 19957392 batches: 0.0220
trigger times: 9
Loss after 19958337 batches: 0.0211
trigger times: 10
Loss after 19959282 batches: 0.0213
trigger times: 11
Loss after 19960227 batches: 0.0203
trigger times: 12
Loss after 19961172 batches: 0.0196
trigger times: 13
Loss after 19962117 batches: 0.0197
trigger times: 14
Loss after 19963062 batches: 0.0200
trigger times: 15
Loss after 19964007 batches: 0.0187
trigger times: 16
Loss after 19964952 batches: 0.0198
trigger times: 17
Loss after 19965897 batches: 0.0185
trigger times: 18
Loss after 19966842 batches: 0.0193
trigger times: 19
Loss after 19967787 batches: 0.0184
trigger times: 20
Loss after 19968732 batches: 0.0177
trigger times: 21
Loss after 19969677 batches: 0.0175
trigger times: 22
Loss after 19970622 batches: 0.0169
trigger times: 23
Loss after 19971567 batches: 0.0175
trigger times: 24
Loss after 19972512 batches: 0.0169
trigger times: 25
Early stopping!
Start to test process.
Loss after 19973457 batches: 0.0176
Time to train on one home:  58.63515853881836
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19974394 batches: 0.0852
trigger times: 1
Loss after 19975331 batches: 0.0689
trigger times: 2
Loss after 19976268 batches: 0.0652
trigger times: 3
Loss after 19977205 batches: 0.0604
trigger times: 4
Loss after 19978142 batches: 0.0572
trigger times: 5
Loss after 19979079 batches: 0.0557
trigger times: 6
Loss after 19980016 batches: 0.0553
trigger times: 7
Loss after 19980953 batches: 0.0520
trigger times: 8
Loss after 19981890 batches: 0.0519
trigger times: 9
Loss after 19982827 batches: 0.0526
trigger times: 10
Loss after 19983764 batches: 0.0504
trigger times: 11
Loss after 19984701 batches: 0.0497
trigger times: 12
Loss after 19985638 batches: 0.0485
trigger times: 13
Loss after 19986575 batches: 0.0496
trigger times: 14
Loss after 19987512 batches: 0.0473
trigger times: 15
Loss after 19988449 batches: 0.0495
trigger times: 16
Loss after 19989386 batches: 0.0480
trigger times: 17
Loss after 19990323 batches: 0.0478
trigger times: 18
Loss after 19991260 batches: 0.0464
trigger times: 19
Loss after 19992197 batches: 0.0471
trigger times: 20
Loss after 19993134 batches: 0.0460
trigger times: 21
Loss after 19994071 batches: 0.0468
trigger times: 22
Loss after 19995008 batches: 0.0463
trigger times: 23
Loss after 19995945 batches: 0.0461
trigger times: 24
Loss after 19996882 batches: 0.0452
trigger times: 25
Early stopping!
Start to test process.
Loss after 19997819 batches: 0.0479
Time to train on one home:  52.93891406059265
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 19998782 batches: 0.0286
trigger times: 1
Loss after 19999745 batches: 0.0153
trigger times: 2
Loss after 20000708 batches: 0.0135
trigger times: 3
Loss after 20001671 batches: 0.0136
trigger times: 4
Loss after 20002634 batches: 0.0133
trigger times: 5
Loss after 20003597 batches: 0.0128
trigger times: 6
Loss after 20004560 batches: 0.0123
trigger times: 7
Loss after 20005523 batches: 0.0115
trigger times: 8
Loss after 20006486 batches: 0.0107
trigger times: 9
Loss after 20007449 batches: 0.0098
trigger times: 10
Loss after 20008412 batches: 0.0092
trigger times: 11
Loss after 20009375 batches: 0.0088
trigger times: 12
Loss after 20010338 batches: 0.0084
trigger times: 13
Loss after 20011301 batches: 0.0082
trigger times: 14
Loss after 20012264 batches: 0.0078
trigger times: 15
Loss after 20013227 batches: 0.0075
trigger times: 16
Loss after 20014190 batches: 0.0073
trigger times: 17
Loss after 20015153 batches: 0.0071
trigger times: 18
Loss after 20016116 batches: 0.0070
trigger times: 19
Loss after 20017079 batches: 0.0069
trigger times: 20
Loss after 20018042 batches: 0.0064
trigger times: 21
Loss after 20019005 batches: 0.0063
trigger times: 22
Loss after 20019968 batches: 0.0061
trigger times: 23
Loss after 20020931 batches: 0.0061
trigger times: 24
Loss after 20021894 batches: 0.0062
trigger times: 25
Early stopping!
Start to test process.
Loss after 20022857 batches: 0.0062
Time to train on one home:  55.84607791900635
trigger times: 0
Loss after 20023820 batches: 0.0944
trigger times: 1
Loss after 20024783 batches: 0.0770
trigger times: 2
Loss after 20025746 batches: 0.0715
trigger times: 3
Loss after 20026709 batches: 0.0667
trigger times: 4
Loss after 20027672 batches: 0.0638
trigger times: 5
Loss after 20028635 batches: 0.0606
trigger times: 6
Loss after 20029598 batches: 0.0598
trigger times: 7
Loss after 20030561 batches: 0.0581
trigger times: 8
Loss after 20031524 batches: 0.0577
trigger times: 9
Loss after 20032487 batches: 0.0566
trigger times: 10
Loss after 20033450 batches: 0.0544
trigger times: 11
Loss after 20034413 batches: 0.0558
trigger times: 12
Loss after 20035376 batches: 0.0562
trigger times: 13
Loss after 20036339 batches: 0.0548
trigger times: 14
Loss after 20037302 batches: 0.0548
trigger times: 15
Loss after 20038265 batches: 0.0547
trigger times: 16
Loss after 20039228 batches: 0.0528
trigger times: 17
Loss after 20040191 batches: 0.0518
trigger times: 18
Loss after 20041154 batches: 0.0524
trigger times: 19
Loss after 20042117 batches: 0.0512
trigger times: 20
Loss after 20043080 batches: 0.0511
trigger times: 21
Loss after 20044043 batches: 0.0500
trigger times: 22
Loss after 20045006 batches: 0.0512
trigger times: 23
Loss after 20045969 batches: 0.0509
trigger times: 24
Loss after 20046932 batches: 0.0521
trigger times: 25
Early stopping!
Start to test process.
Loss after 20047895 batches: 0.0509
Time to train on one home:  57.09064841270447
trigger times: 0
Loss after 20048858 batches: 0.0658
trigger times: 1
Loss after 20049821 batches: 0.0462
trigger times: 2
Loss after 20050784 batches: 0.0441
trigger times: 3
Loss after 20051747 batches: 0.0401
trigger times: 4
Loss after 20052710 batches: 0.0380
trigger times: 5
Loss after 20053673 batches: 0.0361
trigger times: 6
Loss after 20054636 batches: 0.0348
trigger times: 7
Loss after 20055599 batches: 0.0337
trigger times: 8
Loss after 20056562 batches: 0.0327
trigger times: 9
Loss after 20057525 batches: 0.0318
trigger times: 10
Loss after 20058488 batches: 0.0313
trigger times: 11
Loss after 20059451 batches: 0.0310
trigger times: 12
Loss after 20060414 batches: 0.0309
trigger times: 13
Loss after 20061377 batches: 0.0299
trigger times: 14
Loss after 20062340 batches: 0.0296
trigger times: 15
Loss after 20063303 batches: 0.0290
trigger times: 16
Loss after 20064266 batches: 0.0291
trigger times: 17
Loss after 20065229 batches: 0.0291
trigger times: 18
Loss after 20066192 batches: 0.0280
trigger times: 19
Loss after 20067155 batches: 0.0278
trigger times: 20
Loss after 20068118 batches: 0.0281
trigger times: 21
Loss after 20069081 batches: 0.0275
trigger times: 22
Loss after 20070044 batches: 0.0262
trigger times: 23
Loss after 20071007 batches: 0.0268
trigger times: 24
Loss after 20071970 batches: 0.0272
trigger times: 25
Early stopping!
Start to test process.
Loss after 20072933 batches: 0.0268
Time to train on one home:  54.127593994140625
trigger times: 0
Loss after 20073829 batches: 0.1022
trigger times: 1
Loss after 20074725 batches: 0.0919
trigger times: 2
Loss after 20075621 batches: 0.0858
trigger times: 3
Loss after 20076517 batches: 0.0789
trigger times: 4
Loss after 20077413 batches: 0.0709
trigger times: 5
Loss after 20078309 batches: 0.0677
trigger times: 6
Loss after 20079205 batches: 0.0643
trigger times: 7
Loss after 20080101 batches: 0.0616
trigger times: 8
Loss after 20080997 batches: 0.0609
trigger times: 9
Loss after 20081893 batches: 0.0592
trigger times: 10
Loss after 20082789 batches: 0.0571
trigger times: 11
Loss after 20083685 batches: 0.0567
trigger times: 12
Loss after 20084581 batches: 0.0600
trigger times: 13
Loss after 20085477 batches: 0.0584
trigger times: 14
Loss after 20086373 batches: 0.0574
trigger times: 15
Loss after 20087269 batches: 0.0549
trigger times: 16
Loss after 20088165 batches: 0.0539
trigger times: 17
Loss after 20089061 batches: 0.0530
trigger times: 18
Loss after 20089957 batches: 0.0528
trigger times: 19
Loss after 20090853 batches: 0.0530
trigger times: 20
Loss after 20091749 batches: 0.0522
trigger times: 21
Loss after 20092645 batches: 0.0524
trigger times: 22
Loss after 20093541 batches: 0.0503
trigger times: 23
Loss after 20094437 batches: 0.0513
trigger times: 24
Loss after 20095333 batches: 0.0504
trigger times: 25
Early stopping!
Start to test process.
Loss after 20096229 batches: 0.0511
Time to train on one home:  61.2131462097168
trigger times: 0
Loss after 20097192 batches: 0.1534
trigger times: 1
Loss after 20098155 batches: 0.1147
trigger times: 2
Loss after 20099118 batches: 0.0959
trigger times: 3
Loss after 20100081 batches: 0.0847
trigger times: 4
Loss after 20101044 batches: 0.0767
trigger times: 5
Loss after 20102007 batches: 0.0704
trigger times: 6
Loss after 20102970 batches: 0.0652
trigger times: 7
Loss after 20103933 batches: 0.0607
trigger times: 8
Loss after 20104896 batches: 0.0566
trigger times: 9
Loss after 20105859 batches: 0.0522
trigger times: 10
Loss after 20106822 batches: 0.0521
trigger times: 11
Loss after 20107785 batches: 0.0512
trigger times: 12
Loss after 20108748 batches: 0.0486
trigger times: 13
Loss after 20109711 batches: 0.0470
trigger times: 14
Loss after 20110674 batches: 0.0439
trigger times: 15
Loss after 20111637 batches: 0.0432
trigger times: 16
Loss after 20112600 batches: 0.0420
trigger times: 17
Loss after 20113563 batches: 0.0422
trigger times: 18
Loss after 20114526 batches: 0.0432
trigger times: 19
Loss after 20115489 batches: 0.0400
trigger times: 20
Loss after 20116452 batches: 0.0391
trigger times: 21
Loss after 20117415 batches: 0.0395
trigger times: 22
Loss after 20118378 batches: 0.0388
trigger times: 23
Loss after 20119341 batches: 0.0365
trigger times: 24
Loss after 20120304 batches: 0.0384
trigger times: 25
Early stopping!
Start to test process.
Loss after 20121267 batches: 0.0380
Time to train on one home:  56.74833154678345
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20122230 batches: 0.0833
trigger times: 1
Loss after 20123193 batches: 0.0741
trigger times: 2
Loss after 20124156 batches: 0.0718
trigger times: 3
Loss after 20125119 batches: 0.0682
trigger times: 4
Loss after 20126082 batches: 0.0646
trigger times: 5
Loss after 20127045 batches: 0.0591
trigger times: 6
Loss after 20128008 batches: 0.0559
trigger times: 7
Loss after 20128971 batches: 0.0529
trigger times: 8
Loss after 20129934 batches: 0.0497
trigger times: 9
Loss after 20130897 batches: 0.0495
trigger times: 10
Loss after 20131860 batches: 0.0482
trigger times: 11
Loss after 20132823 batches: 0.0484
trigger times: 12
Loss after 20133786 batches: 0.0446
trigger times: 13
Loss after 20134749 batches: 0.0435
trigger times: 14
Loss after 20135712 batches: 0.0444
trigger times: 15
Loss after 20136675 batches: 0.0445
trigger times: 16
Loss after 20137638 batches: 0.0459
trigger times: 17
Loss after 20138601 batches: 0.0442
trigger times: 18
Loss after 20139564 batches: 0.0424
trigger times: 19
Loss after 20140527 batches: 0.0433
trigger times: 20
Loss after 20141490 batches: 0.0435
trigger times: 21
Loss after 20142453 batches: 0.0445
trigger times: 22
Loss after 20143416 batches: 0.0430
trigger times: 23
Loss after 20144379 batches: 0.0434
trigger times: 24
Loss after 20145342 batches: 0.0423
trigger times: 25
Early stopping!
Start to test process.
Loss after 20146305 batches: 0.0408
Time to train on one home:  56.69344997406006
trigger times: 0
Loss after 20147268 batches: 0.0844
trigger times: 1
Loss after 20148231 batches: 0.0614
trigger times: 2
Loss after 20149194 batches: 0.0525
trigger times: 3
Loss after 20150157 batches: 0.0478
trigger times: 4
Loss after 20151120 batches: 0.0445
trigger times: 5
Loss after 20152083 batches: 0.0429
trigger times: 6
Loss after 20153046 batches: 0.0394
trigger times: 7
Loss after 20154009 batches: 0.0388
trigger times: 8
Loss after 20154972 batches: 0.0376
trigger times: 9
Loss after 20155935 batches: 0.0378
trigger times: 10
Loss after 20156898 batches: 0.0368
trigger times: 11
Loss after 20157861 batches: 0.0362
trigger times: 12
Loss after 20158824 batches: 0.0365
trigger times: 13
Loss after 20159787 batches: 0.0357
trigger times: 14
Loss after 20160750 batches: 0.0343
trigger times: 15
Loss after 20161713 batches: 0.0341
trigger times: 16
Loss after 20162676 batches: 0.0344
trigger times: 17
Loss after 20163639 batches: 0.0328
trigger times: 18
Loss after 20164602 batches: 0.0333
trigger times: 19
Loss after 20165565 batches: 0.0315
trigger times: 20
Loss after 20166528 batches: 0.0324
trigger times: 21
Loss after 20167491 batches: 0.0319
trigger times: 22
Loss after 20168454 batches: 0.0314
trigger times: 23
Loss after 20169417 batches: 0.0325
trigger times: 24
Loss after 20170380 batches: 0.0312
trigger times: 25
Early stopping!
Start to test process.
Loss after 20171343 batches: 0.0317
Time to train on one home:  57.27238750457764
trigger times: 0
Loss after 20172306 batches: 0.0486
trigger times: 1
Loss after 20173269 batches: 0.0336
trigger times: 2
Loss after 20174232 batches: 0.0329
trigger times: 3
Loss after 20175195 batches: 0.0295
trigger times: 4
Loss after 20176158 batches: 0.0257
trigger times: 5
Loss after 20177121 batches: 0.0243
trigger times: 6
Loss after 20178084 batches: 0.0234
trigger times: 7
Loss after 20179047 batches: 0.0226
trigger times: 8
Loss after 20180010 batches: 0.0215
trigger times: 9
Loss after 20180973 batches: 0.0213
trigger times: 10
Loss after 20181936 batches: 0.0206
trigger times: 11
Loss after 20182899 batches: 0.0207
trigger times: 12
Loss after 20183862 batches: 0.0202
trigger times: 13
Loss after 20184825 batches: 0.0205
trigger times: 14
Loss after 20185788 batches: 0.0201
trigger times: 15
Loss after 20186751 batches: 0.0200
trigger times: 16
Loss after 20187714 batches: 0.0201
trigger times: 17
Loss after 20188677 batches: 0.0190
trigger times: 18
Loss after 20189640 batches: 0.0196
trigger times: 19
Loss after 20190603 batches: 0.0196
trigger times: 20
Loss after 20191566 batches: 0.0193
trigger times: 21
Loss after 20192529 batches: 0.0187
trigger times: 22
Loss after 20193492 batches: 0.0186
trigger times: 23
Loss after 20194455 batches: 0.0200
trigger times: 24
Loss after 20195418 batches: 0.0210
trigger times: 25
Early stopping!
Start to test process.
Loss after 20196381 batches: 0.0204
Time to train on one home:  57.53033900260925
trigger times: 0
Loss after 20197344 batches: 0.0637
trigger times: 1
Loss after 20198307 batches: 0.0456
trigger times: 2
Loss after 20199270 batches: 0.0454
trigger times: 3
Loss after 20200233 batches: 0.0430
trigger times: 4
Loss after 20201196 batches: 0.0406
trigger times: 5
Loss after 20202159 batches: 0.0383
trigger times: 6
Loss after 20203122 batches: 0.0376
trigger times: 7
Loss after 20204085 batches: 0.0363
trigger times: 8
Loss after 20205048 batches: 0.0357
trigger times: 9
Loss after 20206011 batches: 0.0349
trigger times: 10
Loss after 20206974 batches: 0.0348
trigger times: 11
Loss after 20207937 batches: 0.0335
trigger times: 12
Loss after 20208900 batches: 0.0332
trigger times: 13
Loss after 20209863 batches: 0.0337
trigger times: 14
Loss after 20210826 batches: 0.0323
trigger times: 15
Loss after 20211789 batches: 0.0327
trigger times: 16
Loss after 20212752 batches: 0.0328
trigger times: 17
Loss after 20213715 batches: 0.0324
trigger times: 18
Loss after 20214678 batches: 0.0327
trigger times: 19
Loss after 20215641 batches: 0.0317
trigger times: 20
Loss after 20216604 batches: 0.0319
trigger times: 21
Loss after 20217567 batches: 0.0318
trigger times: 22
Loss after 20218530 batches: 0.0305
trigger times: 23
Loss after 20219493 batches: 0.0319
trigger times: 24
Loss after 20220456 batches: 0.0313
trigger times: 25
Early stopping!
Start to test process.
Loss after 20221419 batches: 0.0312
Time to train on one home:  55.300079584121704
trigger times: 0
Loss after 20222314 batches: 0.0809
trigger times: 1
Loss after 20223209 batches: 0.0420
trigger times: 2
Loss after 20224104 batches: 0.0192
trigger times: 0
Loss after 20224999 batches: 0.0107
trigger times: 1
Loss after 20225894 batches: 0.0062
trigger times: 2
Loss after 20226789 batches: 0.0049
trigger times: 3
Loss after 20227684 batches: 0.0044
trigger times: 4
Loss after 20228579 batches: 0.0037
trigger times: 5
Loss after 20229474 batches: 0.0032
trigger times: 6
Loss after 20230369 batches: 0.0029
trigger times: 7
Loss after 20231264 batches: 0.0036
trigger times: 8
Loss after 20232159 batches: 0.0089
trigger times: 9
Loss after 20233054 batches: 0.0055
trigger times: 10
Loss after 20233949 batches: 0.0044
trigger times: 11
Loss after 20234844 batches: 0.0034
trigger times: 12
Loss after 20235739 batches: 0.0028
trigger times: 13
Loss after 20236634 batches: 0.0026
trigger times: 14
Loss after 20237529 batches: 0.0028
trigger times: 15
Loss after 20238424 batches: 0.0026
trigger times: 16
Loss after 20239319 batches: 0.0024
trigger times: 17
Loss after 20240214 batches: 0.0025
trigger times: 18
Loss after 20241109 batches: 0.0023
trigger times: 19
Loss after 20242004 batches: 0.0020
trigger times: 20
Loss after 20242899 batches: 0.0021
trigger times: 21
Loss after 20243794 batches: 0.0017
trigger times: 22
Loss after 20244689 batches: 0.0019
trigger times: 23
Loss after 20245584 batches: 0.0015
trigger times: 24
Loss after 20246479 batches: 0.0018
trigger times: 25
Early stopping!
Start to test process.
Loss after 20247374 batches: 0.0021
Time to train on one home:  54.09048104286194
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336, 0.034888222413376094, 0.034613204319843105, 0.033508375671873286, 0.03325774057242148, 0.03243952582662181, 0.032562873532530595, 0.03126130941140662, 0.03133166822639105, 0.03049944561800663, 0.030467629500461172, 0.03015643608279951]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171], [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71], [0.13675299286842346, 0.15565351472302724, 0.32920502623266784, 1.059933101560365, 0.6549021202780799, 35.010519125969424, 3184.605], [0.0885479673743248, 0.1559912263273432, 0.33358461589199145, 0.944871497166479, 0.654640173496829, 31.209933508474723, 3183.3313], [0.1345348209142685, 0.15444619808844395, 0.3263722552493196, 1.0544752388510252, 0.6558385527073367, 34.830241138150235, 3189.1584], [0.09016867727041245, 0.15349941943138035, 0.3319629557497865, 0.9480173077546179, 0.65657289912243, 31.313842388762147, 3192.7297], [0.13274754583835602, 0.1683317466130475, 0.33338956387040825, 1.0413414780800958, 0.645068476656334, 34.39642151124207, 3136.7866], [0.08685638755559921, 0.16053285093432457, 0.33813218961147645, 0.938309926482314, 0.6511175433777052, 30.99319907910727, 3166.2017], [0.13567858934402466, 0.1592208654199878, 0.3293138590809072, 1.0581373625723918, 0.6521351625870662, 34.951204293654875, 3171.1501], [0.08736597001552582, 0.1548414758248644, 0.3338051677093621, 0.9435825164148742, 0.6555319615580705, 31.16735734476153, 3187.668], [0.13322944939136505, 0.16497793935194727, 0.3304458768859095, 1.0481311306385603, 0.6476697950280367, 34.620689684777616, 3149.4363], [0.08741147071123123, 0.15130263388043563, 0.33007529206673664, 0.9466815576133256, 0.6582767919114704, 31.269721391125234, 3201.015]]
Round_18_results:  [0.08741147071123123, 0.15130263388043563, 0.33007529206673664, 0.9466815576133256, 0.6582767919114704, 31.269721391125234, 3201.015]
trigger times: 0
Loss after 20248337 batches: 0.0525
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 21214 < 21215; dropping {'Training_Loss': 0.052520805171557834, 'Validation_Loss': 0.06266362965106964, 'Training_R2': 0.2970487712008233, 'Validation_R2': 0.12682866607838705, 'Training_F1': 0.5032348444634819, 'Validation_F1': 0.31801480307167024, 'Training_NEP': 0.7814482981546991, 'Validation_NEP': 0.889765722722303, 'Training_NDE': 0.5176915464953677, 'Validation_NDE': 0.6775791349044494, 'Training_MAE': 22.501936050553915, 'Validation_MAE': 29.63830429830669, 'Training_MSE': 1628.7509, 'Validation_MSE': 3356.3203}.
trigger times: 1
Loss after 20249300 batches: 0.0430
trigger times: 0
Loss after 20250263 batches: 0.0389
trigger times: 1
Loss after 20251226 batches: 0.0348
trigger times: 2
Loss after 20252189 batches: 0.0315
trigger times: 3
Loss after 20253152 batches: 0.0306
trigger times: 4
Loss after 20254115 batches: 0.0293
trigger times: 5
Loss after 20255078 batches: 0.0273
trigger times: 6
Loss after 20256041 batches: 0.0271
trigger times: 7
Loss after 20257004 batches: 0.0277
trigger times: 8
Loss after 20257967 batches: 0.0271
trigger times: 9
Loss after 20258930 batches: 0.0272
trigger times: 10
Loss after 20259893 batches: 0.0265
trigger times: 11
Loss after 20260856 batches: 0.0269
trigger times: 12
Loss after 20261819 batches: 0.0268
trigger times: 13
Loss after 20262782 batches: 0.0251
trigger times: 14
Loss after 20263745 batches: 0.0253
trigger times: 15
Loss after 20264708 batches: 0.0258
trigger times: 16
Loss after 20265671 batches: 0.0258
trigger times: 17
Loss after 20266634 batches: 0.0249
trigger times: 18
Loss after 20267597 batches: 0.0259
trigger times: 19
Loss after 20268560 batches: 0.0295
trigger times: 20
Loss after 20269523 batches: 0.0279
trigger times: 21
Loss after 20270486 batches: 0.0265
trigger times: 22
Loss after 20271449 batches: 0.0259
trigger times: 23
Loss after 20272412 batches: 0.0242
trigger times: 24
Loss after 20273375 batches: 0.0235
trigger times: 25
Early stopping!
Start to test process.
Loss after 20274338 batches: 0.0239
Time to train on one home:  60.30758595466614
trigger times: 0
Loss after 20275296 batches: 0.0586
trigger times: 0
Loss after 20276254 batches: 0.0396
trigger times: 1
Loss after 20277212 batches: 0.0296
trigger times: 2
Loss after 20278170 batches: 0.0269
trigger times: 3
Loss after 20279128 batches: 0.0242
trigger times: 0
Loss after 20280086 batches: 0.0213
trigger times: 1
Loss after 20281044 batches: 0.0193
trigger times: 2
Loss after 20282002 batches: 0.0192
trigger times: 3
Loss after 20282960 batches: 0.0183
trigger times: 4
Loss after 20283918 batches: 0.0181
trigger times: 5
Loss after 20284876 batches: 0.0176
trigger times: 6
Loss after 20285834 batches: 0.0171
trigger times: 7
Loss after 20286792 batches: 0.0165
trigger times: 8
Loss after 20287750 batches: 0.0164
trigger times: 9
Loss after 20288708 batches: 0.0164
trigger times: 10
Loss after 20289666 batches: 0.0153
trigger times: 11
Loss after 20290624 batches: 0.0164
trigger times: 12
Loss after 20291582 batches: 0.0149
trigger times: 13
Loss after 20292540 batches: 0.0146
trigger times: 14
Loss after 20293498 batches: 0.0151
trigger times: 15
Loss after 20294456 batches: 0.0151
trigger times: 16
Loss after 20295414 batches: 0.0148
trigger times: 17
Loss after 20296372 batches: 0.0148
trigger times: 18
Loss after 20297330 batches: 0.0144
trigger times: 19
Loss after 20298288 batches: 0.0151
trigger times: 20
Loss after 20299246 batches: 0.0139
trigger times: 0
Loss after 20300204 batches: 0.0143
trigger times: 1
Loss after 20301162 batches: 0.0130
trigger times: 2
Loss after 20302120 batches: 0.0140
trigger times: 3
Loss after 20303078 batches: 0.0129
trigger times: 4
Loss after 20304036 batches: 0.0128
trigger times: 5
Loss after 20304994 batches: 0.0125
trigger times: 6
Loss after 20305952 batches: 0.0124
trigger times: 7
Loss after 20306910 batches: 0.0117
trigger times: 8
Loss after 20307868 batches: 0.0121
trigger times: 9
Loss after 20308826 batches: 0.0135
trigger times: 10
Loss after 20309784 batches: 0.0122
trigger times: 11
Loss after 20310742 batches: 0.0122
trigger times: 12
Loss after 20311700 batches: 0.0120
trigger times: 13
Loss after 20312658 batches: 0.0125
trigger times: 14
Loss after 20313616 batches: 0.0136
trigger times: 15
Loss after 20314574 batches: 0.0132
trigger times: 16
Loss after 20315532 batches: 0.0112
trigger times: 17
Loss after 20316490 batches: 0.0120
trigger times: 18
Loss after 20317448 batches: 0.0123
trigger times: 19
Loss after 20318406 batches: 0.0120
trigger times: 20
Loss after 20319364 batches: 0.0125
trigger times: 21
Loss after 20320322 batches: 0.0119
trigger times: 22
Loss after 20321280 batches: 0.0120
trigger times: 23
Loss after 20322238 batches: 0.0129
trigger times: 24
Loss after 20323196 batches: 0.0123
trigger times: 25
Early stopping!
Start to test process.
Loss after 20324154 batches: 0.0119
Time to train on one home:  77.1155436038971
trigger times: 0
Loss after 20325117 batches: 0.1058
trigger times: 1
Loss after 20326080 batches: 0.0733
trigger times: 2
Loss after 20327043 batches: 0.0672
trigger times: 3
Loss after 20328006 batches: 0.0673
trigger times: 4
Loss after 20328969 batches: 0.0651
trigger times: 5
Loss after 20329932 batches: 0.0617
trigger times: 6
Loss after 20330895 batches: 0.0591
trigger times: 7
Loss after 20331858 batches: 0.0554
trigger times: 8
Loss after 20332821 batches: 0.0540
trigger times: 9
Loss after 20333784 batches: 0.0517
trigger times: 10
Loss after 20334747 batches: 0.0490
trigger times: 11
Loss after 20335710 batches: 0.0482
trigger times: 12
Loss after 20336673 batches: 0.0466
trigger times: 13
Loss after 20337636 batches: 0.0462
trigger times: 14
Loss after 20338599 batches: 0.0452
trigger times: 15
Loss after 20339562 batches: 0.0459
trigger times: 16
Loss after 20340525 batches: 0.0449
trigger times: 17
Loss after 20341488 batches: 0.0436
trigger times: 18
Loss after 20342451 batches: 0.0437
trigger times: 19
Loss after 20343414 batches: 0.0425
trigger times: 20
Loss after 20344377 batches: 0.0424
trigger times: 21
Loss after 20345340 batches: 0.0414
trigger times: 22
Loss after 20346303 batches: 0.0412
trigger times: 23
Loss after 20347266 batches: 0.0419
trigger times: 24
Loss after 20348229 batches: 0.0408
trigger times: 25
Early stopping!
Start to test process.
Loss after 20349192 batches: 0.0411
Time to train on one home:  59.97078466415405
trigger times: 0
Loss after 20350155 batches: 0.1028
trigger times: 1
Loss after 20351118 batches: 0.0785
trigger times: 2
Loss after 20352081 batches: 0.0758
trigger times: 3
Loss after 20353044 batches: 0.0739
trigger times: 4
Loss after 20354007 batches: 0.0705
trigger times: 5
Loss after 20354970 batches: 0.0667
trigger times: 6
Loss after 20355933 batches: 0.0648
trigger times: 7
Loss after 20356896 batches: 0.0637
trigger times: 8
Loss after 20357859 batches: 0.0618
trigger times: 9
Loss after 20358822 batches: 0.0613
trigger times: 10
Loss after 20359785 batches: 0.0601
trigger times: 11
Loss after 20360748 batches: 0.0594
trigger times: 12
Loss after 20361711 batches: 0.0582
trigger times: 13
Loss after 20362674 batches: 0.0573
trigger times: 14
Loss after 20363637 batches: 0.0560
trigger times: 15
Loss after 20364600 batches: 0.0553
trigger times: 16
Loss after 20365563 batches: 0.0552
trigger times: 17
Loss after 20366526 batches: 0.0546
trigger times: 18
Loss after 20367489 batches: 0.0551
trigger times: 19
Loss after 20368452 batches: 0.0549
trigger times: 20
Loss after 20369415 batches: 0.0536
trigger times: 21
Loss after 20370378 batches: 0.0532
trigger times: 22
Loss after 20371341 batches: 0.0531
trigger times: 23
Loss after 20372304 batches: 0.0527
trigger times: 24
Loss after 20373267 batches: 0.0527
trigger times: 25
Early stopping!
Start to test process.
Loss after 20374230 batches: 0.0532
Time to train on one home:  56.70779228210449
trigger times: 0
Loss after 20375193 batches: 0.0236
trigger times: 1
Loss after 20376156 batches: 0.0188
trigger times: 2
Loss after 20377119 batches: 0.0173
trigger times: 3
Loss after 20378082 batches: 0.0160
trigger times: 4
Loss after 20379045 batches: 0.0150
trigger times: 5
Loss after 20380008 batches: 0.0140
trigger times: 6
Loss after 20380971 batches: 0.0132
trigger times: 7
Loss after 20381934 batches: 0.0135
trigger times: 8
Loss after 20382897 batches: 0.0127
trigger times: 9
Loss after 20383860 batches: 0.0130
trigger times: 10
Loss after 20384823 batches: 0.0125
trigger times: 11
Loss after 20385786 batches: 0.0123
trigger times: 12
Loss after 20386749 batches: 0.0121
trigger times: 13
Loss after 20387712 batches: 0.0127
trigger times: 14
Loss after 20388675 batches: 0.0115
trigger times: 15
Loss after 20389638 batches: 0.0113
trigger times: 16
Loss after 20390601 batches: 0.0113
trigger times: 17
Loss after 20391564 batches: 0.0112
trigger times: 18
Loss after 20392527 batches: 0.0113
trigger times: 19
Loss after 20393490 batches: 0.0110
trigger times: 20
Loss after 20394453 batches: 0.0110
trigger times: 21
Loss after 20395416 batches: 0.0111
trigger times: 22
Loss after 20396379 batches: 0.0109
trigger times: 23
Loss after 20397342 batches: 0.0108
trigger times: 24
Loss after 20398305 batches: 0.0111
trigger times: 25
Early stopping!
Start to test process.
Loss after 20399268 batches: 0.0114
Time to train on one home:  56.43888473510742
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20400231 batches: 0.0926
trigger times: 1
Loss after 20401194 batches: 0.0269
trigger times: 2
Loss after 20402157 batches: 0.0206
trigger times: 3
Loss after 20403120 batches: 0.0191
trigger times: 4
Loss after 20404083 batches: 0.0175
trigger times: 5
Loss after 20405046 batches: 0.0160
trigger times: 6
Loss after 20406009 batches: 0.0147
trigger times: 7
Loss after 20406972 batches: 0.0141
trigger times: 8
Loss after 20407935 batches: 0.0138
trigger times: 9
Loss after 20408898 batches: 0.0134
trigger times: 10
Loss after 20409861 batches: 0.0132
trigger times: 11
Loss after 20410824 batches: 0.0130
trigger times: 12
Loss after 20411787 batches: 0.0126
trigger times: 13
Loss after 20412750 batches: 0.0127
trigger times: 14
Loss after 20413713 batches: 0.0124
trigger times: 15
Loss after 20414676 batches: 0.0121
trigger times: 16
Loss after 20415639 batches: 0.0120
trigger times: 17
Loss after 20416602 batches: 0.0120
trigger times: 18
Loss after 20417565 batches: 0.0119
trigger times: 19
Loss after 20418528 batches: 0.0118
trigger times: 20
Loss after 20419491 batches: 0.0116
trigger times: 21
Loss after 20420454 batches: 0.0118
trigger times: 22
Loss after 20421417 batches: 0.0117
trigger times: 23
Loss after 20422380 batches: 0.0116
trigger times: 24
Loss after 20423343 batches: 0.0116
trigger times: 25
Early stopping!
Start to test process.
Loss after 20424306 batches: 0.0114
Time to train on one home:  57.30263352394104
trigger times: 0
Loss after 20425269 batches: 0.0933
trigger times: 1
Loss after 20426232 batches: 0.0870
trigger times: 0
Loss after 20427195 batches: 0.0803
trigger times: 1
Loss after 20428158 batches: 0.0780
trigger times: 2
Loss after 20429121 batches: 0.0734
trigger times: 3
Loss after 20430084 batches: 0.0704
trigger times: 4
Loss after 20431047 batches: 0.0675
trigger times: 5
Loss after 20432010 batches: 0.0654
trigger times: 6
Loss after 20432973 batches: 0.0639
trigger times: 7
Loss after 20433936 batches: 0.0621
trigger times: 8
Loss after 20434899 batches: 0.0598
trigger times: 9
Loss after 20435862 batches: 0.0614
trigger times: 10
Loss after 20436825 batches: 0.0598
trigger times: 11
Loss after 20437788 batches: 0.0579
trigger times: 12
Loss after 20438751 batches: 0.0606
trigger times: 13
Loss after 20439714 batches: 0.0592
trigger times: 14
Loss after 20440677 batches: 0.0569
trigger times: 15
Loss after 20441640 batches: 0.0566
trigger times: 16
Loss after 20442603 batches: 0.0569
trigger times: 17
Loss after 20443566 batches: 0.0546
trigger times: 18
Loss after 20444529 batches: 0.0531
trigger times: 19
Loss after 20445492 batches: 0.0548
trigger times: 20
Loss after 20446455 batches: 0.0540
trigger times: 21
Loss after 20447418 batches: 0.0540
trigger times: 22
Loss after 20448381 batches: 0.0544
trigger times: 23
Loss after 20449344 batches: 0.0545
trigger times: 24
Loss after 20450307 batches: 0.0533
trigger times: 25
Early stopping!
Start to test process.
Loss after 20451270 batches: 0.0523
Time to train on one home:  58.989163637161255
trigger times: 0
Loss after 20452233 batches: 0.0585
trigger times: 1
Loss after 20453196 batches: 0.0435
trigger times: 0
Loss after 20454159 batches: 0.0335
trigger times: 0
Loss after 20455122 batches: 0.0280
trigger times: 0
Loss after 20456085 batches: 0.0248
trigger times: 1
Loss after 20457048 batches: 0.0239
trigger times: 2
Loss after 20458011 batches: 0.0234
trigger times: 3
Loss after 20458974 batches: 0.0220
trigger times: 4
Loss after 20459937 batches: 0.0207
trigger times: 5
Loss after 20460900 batches: 0.0200
trigger times: 6
Loss after 20461863 batches: 0.0211
trigger times: 7
Loss after 20462826 batches: 0.0200
trigger times: 8
Loss after 20463789 batches: 0.0194
trigger times: 9
Loss after 20464752 batches: 0.0180
trigger times: 10
Loss after 20465715 batches: 0.0192
trigger times: 11
Loss after 20466678 batches: 0.0183
trigger times: 12
Loss after 20467641 batches: 0.0187
trigger times: 13
Loss after 20468604 batches: 0.0194
trigger times: 14
Loss after 20469567 batches: 0.0183
trigger times: 15
Loss after 20470530 batches: 0.0180
trigger times: 16
Loss after 20471493 batches: 0.0173
trigger times: 17
Loss after 20472456 batches: 0.0163
trigger times: 18
Loss after 20473419 batches: 0.0167
trigger times: 19
Loss after 20474382 batches: 0.0175
trigger times: 20
Loss after 20475345 batches: 0.0164
trigger times: 21
Loss after 20476308 batches: 0.0165
trigger times: 22
Loss after 20477271 batches: 0.0161
trigger times: 23
Loss after 20478234 batches: 0.0160
trigger times: 24
Loss after 20479197 batches: 0.0172
trigger times: 25
Early stopping!
Start to test process.
Loss after 20480160 batches: 0.0185
Time to train on one home:  58.92312955856323
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20481123 batches: 0.0850
trigger times: 1
Loss after 20482086 batches: 0.0727
trigger times: 2
Loss after 20483049 batches: 0.0687
trigger times: 3
Loss after 20484012 batches: 0.0653
trigger times: 4
Loss after 20484975 batches: 0.0644
trigger times: 5
Loss after 20485938 batches: 0.0609
trigger times: 6
Loss after 20486901 batches: 0.0591
trigger times: 7
Loss after 20487864 batches: 0.0587
trigger times: 8
Loss after 20488827 batches: 0.0567
trigger times: 9
Loss after 20489790 batches: 0.0555
trigger times: 10
Loss after 20490753 batches: 0.0545
trigger times: 11
Loss after 20491716 batches: 0.0555
trigger times: 12
Loss after 20492679 batches: 0.0539
trigger times: 13
Loss after 20493642 batches: 0.0534
trigger times: 14
Loss after 20494605 batches: 0.0534
trigger times: 15
Loss after 20495568 batches: 0.0520
trigger times: 16
Loss after 20496531 batches: 0.0531
trigger times: 17
Loss after 20497494 batches: 0.0524
trigger times: 18
Loss after 20498457 batches: 0.0513
trigger times: 19
Loss after 20499420 batches: 0.0515
trigger times: 20
Loss after 20500383 batches: 0.0526
trigger times: 21
Loss after 20501346 batches: 0.0522
trigger times: 22
Loss after 20502309 batches: 0.0511
trigger times: 23
Loss after 20503272 batches: 0.0505
trigger times: 24
Loss after 20504235 batches: 0.0497
trigger times: 25
Early stopping!
Start to test process.
Loss after 20505198 batches: 0.0498
Time to train on one home:  53.27287244796753
trigger times: 0
Loss after 20506161 batches: 0.0876
trigger times: 0
Loss after 20507124 batches: 0.0574
trigger times: 0
Loss after 20508087 batches: 0.0544
trigger times: 1
Loss after 20509050 batches: 0.0485
trigger times: 2
Loss after 20510013 batches: 0.0473
trigger times: 3
Loss after 20510976 batches: 0.0439
trigger times: 4
Loss after 20511939 batches: 0.0420
trigger times: 5
Loss after 20512902 batches: 0.0424
trigger times: 6
Loss after 20513865 batches: 0.0405
trigger times: 7
Loss after 20514828 batches: 0.0393
trigger times: 8
Loss after 20515791 batches: 0.0386
trigger times: 9
Loss after 20516754 batches: 0.0386
trigger times: 10
Loss after 20517717 batches: 0.0374
trigger times: 11
Loss after 20518680 batches: 0.0369
trigger times: 12
Loss after 20519643 batches: 0.0360
trigger times: 13
Loss after 20520606 batches: 0.0364
trigger times: 14
Loss after 20521569 batches: 0.0376
trigger times: 15
Loss after 20522532 batches: 0.0357
trigger times: 16
Loss after 20523495 batches: 0.0365
trigger times: 17
Loss after 20524458 batches: 0.0357
trigger times: 18
Loss after 20525421 batches: 0.0357
trigger times: 19
Loss after 20526384 batches: 0.0345
trigger times: 20
Loss after 20527347 batches: 0.0355
trigger times: 21
Loss after 20528310 batches: 0.0350
trigger times: 22
Loss after 20529273 batches: 0.0355
trigger times: 23
Loss after 20530236 batches: 0.0346
trigger times: 24
Loss after 20531199 batches: 0.0350
trigger times: 25
Early stopping!
Start to test process.
Loss after 20532162 batches: 0.0346
Time to train on one home:  59.92386531829834
trigger times: 0
Loss after 20533125 batches: 0.0726
trigger times: 1
Loss after 20534088 batches: 0.0667
trigger times: 2
Loss after 20535051 batches: 0.0639
trigger times: 3
Loss after 20536014 batches: 0.0610
trigger times: 4
Loss after 20536977 batches: 0.0589
trigger times: 5
Loss after 20537940 batches: 0.0567
trigger times: 6
Loss after 20538903 batches: 0.0555
trigger times: 7
Loss after 20539866 batches: 0.0537
trigger times: 8
Loss after 20540829 batches: 0.0528
trigger times: 9
Loss after 20541792 batches: 0.0522
trigger times: 10
Loss after 20542755 batches: 0.0517
trigger times: 11
Loss after 20543718 batches: 0.0506
trigger times: 12
Loss after 20544681 batches: 0.0495
trigger times: 13
Loss after 20545644 batches: 0.0512
trigger times: 14
Loss after 20546607 batches: 0.0500
trigger times: 15
Loss after 20547570 batches: 0.0489
trigger times: 16
Loss after 20548533 batches: 0.0485
trigger times: 17
Loss after 20549496 batches: 0.0494
trigger times: 18
Loss after 20550459 batches: 0.0480
trigger times: 19
Loss after 20551422 batches: 0.0478
trigger times: 20
Loss after 20552385 batches: 0.0462
trigger times: 21
Loss after 20553348 batches: 0.0467
trigger times: 22
Loss after 20554311 batches: 0.0464
trigger times: 23
Loss after 20555274 batches: 0.0475
trigger times: 24
Loss after 20556237 batches: 0.0472
trigger times: 25
Early stopping!
Start to test process.
Loss after 20557200 batches: 0.0452
Time to train on one home:  56.59692072868347
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20558163 batches: 0.0640
trigger times: 1
Loss after 20559126 batches: 0.0475
trigger times: 2
Loss after 20560089 batches: 0.0442
trigger times: 3
Loss after 20561052 batches: 0.0384
trigger times: 4
Loss after 20562015 batches: 0.0344
trigger times: 5
Loss after 20562978 batches: 0.0322
trigger times: 6
Loss after 20563941 batches: 0.0302
trigger times: 7
Loss after 20564904 batches: 0.0291
trigger times: 8
Loss after 20565867 batches: 0.0277
trigger times: 9
Loss after 20566830 batches: 0.0271
trigger times: 10
Loss after 20567793 batches: 0.0259
trigger times: 11
Loss after 20568756 batches: 0.0259
trigger times: 12
Loss after 20569719 batches: 0.0256
trigger times: 13
Loss after 20570682 batches: 0.0252
trigger times: 14
Loss after 20571645 batches: 0.0246
trigger times: 15
Loss after 20572608 batches: 0.0240
trigger times: 16
Loss after 20573571 batches: 0.0233
trigger times: 17
Loss after 20574534 batches: 0.0226
trigger times: 18
Loss after 20575497 batches: 0.0224
trigger times: 19
Loss after 20576460 batches: 0.0224
trigger times: 20
Loss after 20577423 batches: 0.0220
trigger times: 21
Loss after 20578386 batches: 0.0222
trigger times: 22
Loss after 20579349 batches: 0.0219
trigger times: 23
Loss after 20580312 batches: 0.0212
trigger times: 24
Loss after 20581275 batches: 0.0209
trigger times: 25
Early stopping!
Start to test process.
Loss after 20582238 batches: 0.0212
Time to train on one home:  54.06191945075989
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20583201 batches: 0.0755
trigger times: 0
Loss after 20584164 batches: 0.0504
trigger times: 1
Loss after 20585127 batches: 0.0375
trigger times: 0
Loss after 20586090 batches: 0.0346
trigger times: 1
Loss after 20587053 batches: 0.0303
trigger times: 2
Loss after 20588016 batches: 0.0286
trigger times: 3
Loss after 20588979 batches: 0.0281
trigger times: 4
Loss after 20589942 batches: 0.0259
trigger times: 5
Loss after 20590905 batches: 0.0237
trigger times: 6
Loss after 20591868 batches: 0.0234
trigger times: 7
Loss after 20592831 batches: 0.0218
trigger times: 8
Loss after 20593794 batches: 0.0210
trigger times: 9
Loss after 20594757 batches: 0.0202
trigger times: 0
Loss after 20595720 batches: 0.0201
trigger times: 1
Loss after 20596683 batches: 0.0195
trigger times: 2
Loss after 20597646 batches: 0.0188
trigger times: 3
Loss after 20598609 batches: 0.0185
trigger times: 4
Loss after 20599572 batches: 0.0177
trigger times: 5
Loss after 20600535 batches: 0.0186
trigger times: 6
Loss after 20601498 batches: 0.0174
trigger times: 7
Loss after 20602461 batches: 0.0179
trigger times: 0
Loss after 20603424 batches: 0.0185
trigger times: 1
Loss after 20604387 batches: 0.0177
trigger times: 2
Loss after 20605350 batches: 0.0163
trigger times: 3
Loss after 20606313 batches: 0.0167
trigger times: 4
Loss after 20607276 batches: 0.0174
trigger times: 5
Loss after 20608239 batches: 0.0173
trigger times: 6
Loss after 20609202 batches: 0.0177
trigger times: 7
Loss after 20610165 batches: 0.0182
trigger times: 8
Loss after 20611128 batches: 0.0182
trigger times: 9
Loss after 20612091 batches: 0.0189
trigger times: 10
Loss after 20613054 batches: 0.0180
trigger times: 11
Loss after 20614017 batches: 0.0174
trigger times: 12
Loss after 20614980 batches: 0.0180
trigger times: 13
Loss after 20615943 batches: 0.0173
trigger times: 14
Loss after 20616906 batches: 0.0168
trigger times: 15
Loss after 20617869 batches: 0.0158
trigger times: 16
Loss after 20618832 batches: 0.0157
trigger times: 17
Loss after 20619795 batches: 0.0152
trigger times: 18
Loss after 20620758 batches: 0.0149
trigger times: 19
Loss after 20621721 batches: 0.0143
trigger times: 20
Loss after 20622684 batches: 0.0139
trigger times: 21
Loss after 20623647 batches: 0.0136
trigger times: 22
Loss after 20624610 batches: 0.0141
trigger times: 23
Loss after 20625573 batches: 0.0143
trigger times: 24
Loss after 20626536 batches: 0.0146
trigger times: 25
Early stopping!
Start to test process.
Loss after 20627499 batches: 0.0162
Time to train on one home:  77.67300033569336
trigger times: 0
Loss after 20628428 batches: 0.0880
trigger times: 0
Loss after 20629357 batches: 0.0575
trigger times: 0
Loss after 20630286 batches: 0.0431
trigger times: 1
Loss after 20631215 batches: 0.0386
trigger times: 2
Loss after 20632144 batches: 0.0329
trigger times: 3
Loss after 20633073 batches: 0.0303
trigger times: 0
Loss after 20634002 batches: 0.0279
trigger times: 1
Loss after 20634931 batches: 0.0263
trigger times: 2
Loss after 20635860 batches: 0.0269
trigger times: 0
Loss after 20636789 batches: 0.0265
trigger times: 1
Loss after 20637718 batches: 0.0333
trigger times: 2
Loss after 20638647 batches: 0.0390
trigger times: 3
Loss after 20639576 batches: 0.0374
trigger times: 4
Loss after 20640505 batches: 0.0356
trigger times: 5
Loss after 20641434 batches: 0.0336
trigger times: 6
Loss after 20642363 batches: 0.0310
trigger times: 7
Loss after 20643292 batches: 0.0314
trigger times: 8
Loss after 20644221 batches: 0.0301
trigger times: 9
Loss after 20645150 batches: 0.0280
trigger times: 10
Loss after 20646079 batches: 0.0261
trigger times: 11
Loss after 20647008 batches: 0.0259
trigger times: 12
Loss after 20647937 batches: 0.0266
trigger times: 13
Loss after 20648866 batches: 0.0260
trigger times: 14
Loss after 20649795 batches: 0.0263
trigger times: 15
Loss after 20650724 batches: 0.0261
trigger times: 16
Loss after 20651653 batches: 0.0248
trigger times: 17
Loss after 20652582 batches: 0.0245
trigger times: 18
Loss after 20653511 batches: 0.0274
trigger times: 19
Loss after 20654440 batches: 0.0263
trigger times: 20
Loss after 20655369 batches: 0.0233
trigger times: 21
Loss after 20656298 batches: 0.0238
trigger times: 22
Loss after 20657227 batches: 0.0299
trigger times: 23
Loss after 20658156 batches: 0.0417
trigger times: 24
Loss after 20659085 batches: 0.0384
trigger times: 25
Early stopping!
Start to test process.
Loss after 20660014 batches: 0.0336
Time to train on one home:  63.65565371513367
trigger times: 0
Loss after 20660976 batches: 0.0799
trigger times: 1
Loss after 20661938 batches: 0.0648
trigger times: 2
Loss after 20662900 batches: 0.0640
trigger times: 3
Loss after 20663862 batches: 0.0609
trigger times: 4
Loss after 20664824 batches: 0.0573
trigger times: 5
Loss after 20665786 batches: 0.0557
trigger times: 6
Loss after 20666748 batches: 0.0545
trigger times: 7
Loss after 20667710 batches: 0.0528
trigger times: 8
Loss after 20668672 batches: 0.0524
trigger times: 9
Loss after 20669634 batches: 0.0525
trigger times: 10
Loss after 20670596 batches: 0.0514
trigger times: 11
Loss after 20671558 batches: 0.0507
trigger times: 12
Loss after 20672520 batches: 0.0511
trigger times: 13
Loss after 20673482 batches: 0.0513
trigger times: 14
Loss after 20674444 batches: 0.0514
trigger times: 15
Loss after 20675406 batches: 0.0511
trigger times: 16
Loss after 20676368 batches: 0.0493
trigger times: 17
Loss after 20677330 batches: 0.0499
trigger times: 18
Loss after 20678292 batches: 0.0489
trigger times: 19
Loss after 20679254 batches: 0.0486
trigger times: 20
Loss after 20680216 batches: 0.0473
trigger times: 21
Loss after 20681178 batches: 0.0487
trigger times: 22
Loss after 20682140 batches: 0.0472
trigger times: 23
Loss after 20683102 batches: 0.0482
trigger times: 24
Loss after 20684064 batches: 0.0478
trigger times: 25
Early stopping!
Start to test process.
Loss after 20685026 batches: 0.0471
Time to train on one home:  57.13632392883301
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20685989 batches: 0.0533
trigger times: 1
Loss after 20686952 batches: 0.0428
trigger times: 2
Loss after 20687915 batches: 0.0427
trigger times: 3
Loss after 20688878 batches: 0.0399
trigger times: 4
Loss after 20689841 batches: 0.0370
trigger times: 5
Loss after 20690804 batches: 0.0355
trigger times: 6
Loss after 20691767 batches: 0.0338
trigger times: 7
Loss after 20692730 batches: 0.0332
trigger times: 8
Loss after 20693693 batches: 0.0324
trigger times: 9
Loss after 20694656 batches: 0.0319
trigger times: 10
Loss after 20695619 batches: 0.0312
trigger times: 11
Loss after 20696582 batches: 0.0304
trigger times: 12
Loss after 20697545 batches: 0.0308
trigger times: 13
Loss after 20698508 batches: 0.0291
trigger times: 14
Loss after 20699471 batches: 0.0290
trigger times: 15
Loss after 20700434 batches: 0.0288
trigger times: 16
Loss after 20701397 batches: 0.0288
trigger times: 17
Loss after 20702360 batches: 0.0286
trigger times: 18
Loss after 20703323 batches: 0.0281
trigger times: 19
Loss after 20704286 batches: 0.0279
trigger times: 20
Loss after 20705249 batches: 0.0276
trigger times: 21
Loss after 20706212 batches: 0.0272
trigger times: 22
Loss after 20707175 batches: 0.0274
trigger times: 23
Loss after 20708138 batches: 0.0273
trigger times: 24
Loss after 20709101 batches: 0.0272
trigger times: 25
Early stopping!
Start to test process.
Loss after 20710064 batches: 0.0277
Time to train on one home:  57.648115158081055
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20711027 batches: 0.0469
trigger times: 1
Loss after 20711990 batches: 0.0417
trigger times: 0
Loss after 20712953 batches: 0.0392
trigger times: 1
Loss after 20713916 batches: 0.0357
trigger times: 2
Loss after 20714879 batches: 0.0341
trigger times: 3
Loss after 20715842 batches: 0.0321
trigger times: 4
Loss after 20716805 batches: 0.0309
trigger times: 5
Loss after 20717768 batches: 0.0300
trigger times: 6
Loss after 20718731 batches: 0.0307
trigger times: 7
Loss after 20719694 batches: 0.0330
trigger times: 8
Loss after 20720657 batches: 0.0320
trigger times: 9
Loss after 20721620 batches: 0.0301
trigger times: 10
Loss after 20722583 batches: 0.0285
trigger times: 11
Loss after 20723546 batches: 0.0282
trigger times: 12
Loss after 20724509 batches: 0.0274
trigger times: 13
Loss after 20725472 batches: 0.0268
trigger times: 14
Loss after 20726435 batches: 0.0276
trigger times: 15
Loss after 20727398 batches: 0.0275
trigger times: 16
Loss after 20728361 batches: 0.0269
trigger times: 17
Loss after 20729324 batches: 0.0271
trigger times: 18
Loss after 20730287 batches: 0.0263
trigger times: 19
Loss after 20731250 batches: 0.0250
trigger times: 20
Loss after 20732213 batches: 0.0252
trigger times: 21
Loss after 20733176 batches: 0.0263
trigger times: 22
Loss after 20734139 batches: 0.0249
trigger times: 23
Loss after 20735102 batches: 0.0251
trigger times: 24
Loss after 20736065 batches: 0.0256
trigger times: 25
Early stopping!
Start to test process.
Loss after 20737028 batches: 0.0247
Time to train on one home:  58.289790391922
trigger times: 0
Loss after 20737991 batches: 0.0973
trigger times: 1
Loss after 20738954 batches: 0.0884
trigger times: 2
Loss after 20739917 batches: 0.0841
trigger times: 3
Loss after 20740880 batches: 0.0807
trigger times: 4
Loss after 20741843 batches: 0.0775
trigger times: 5
Loss after 20742806 batches: 0.0753
trigger times: 6
Loss after 20743769 batches: 0.0723
trigger times: 7
Loss after 20744732 batches: 0.0701
trigger times: 8
Loss after 20745695 batches: 0.0713
trigger times: 9
Loss after 20746658 batches: 0.0712
trigger times: 10
Loss after 20747621 batches: 0.0677
trigger times: 11
Loss after 20748584 batches: 0.0654
trigger times: 12
Loss after 20749547 batches: 0.0667
trigger times: 13
Loss after 20750510 batches: 0.0641
trigger times: 14
Loss after 20751473 batches: 0.0651
trigger times: 15
Loss after 20752436 batches: 0.0631
trigger times: 16
Loss after 20753399 batches: 0.0653
trigger times: 17
Loss after 20754362 batches: 0.0628
trigger times: 18
Loss after 20755325 batches: 0.0628
trigger times: 19
Loss after 20756288 batches: 0.0627
trigger times: 20
Loss after 20757251 batches: 0.0634
trigger times: 21
Loss after 20758214 batches: 0.0639
trigger times: 22
Loss after 20759177 batches: 0.0628
trigger times: 23
Loss after 20760140 batches: 0.0620
trigger times: 24
Loss after 20761103 batches: 0.0599
trigger times: 25
Early stopping!
Start to test process.
Loss after 20762066 batches: 0.0589
Time to train on one home:  53.26719307899475
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20763029 batches: 0.0873
trigger times: 1
Loss after 20763992 batches: 0.0487
trigger times: 2
Loss after 20764955 batches: 0.0505
trigger times: 3
Loss after 20765918 batches: 0.0417
trigger times: 4
Loss after 20766881 batches: 0.0393
trigger times: 5
Loss after 20767844 batches: 0.0354
trigger times: 6
Loss after 20768807 batches: 0.0338
trigger times: 7
Loss after 20769770 batches: 0.0323
trigger times: 8
Loss after 20770733 batches: 0.0315
trigger times: 9
Loss after 20771696 batches: 0.0309
trigger times: 10
Loss after 20772659 batches: 0.0299
trigger times: 11
Loss after 20773622 batches: 0.0286
trigger times: 12
Loss after 20774585 batches: 0.0276
trigger times: 13
Loss after 20775548 batches: 0.0264
trigger times: 14
Loss after 20776511 batches: 0.0275
trigger times: 15
Loss after 20777474 batches: 0.0264
trigger times: 16
Loss after 20778437 batches: 0.0266
trigger times: 17
Loss after 20779400 batches: 0.0266
trigger times: 18
Loss after 20780363 batches: 0.0253
trigger times: 19
Loss after 20781326 batches: 0.0257
trigger times: 20
Loss after 20782289 batches: 0.0252
trigger times: 21
Loss after 20783252 batches: 0.0246
trigger times: 22
Loss after 20784215 batches: 0.0244
trigger times: 23
Loss after 20785178 batches: 0.0239
trigger times: 24
Loss after 20786141 batches: 0.0235
trigger times: 25
Early stopping!
Start to test process.
Loss after 20787104 batches: 0.0244
Time to train on one home:  54.71170687675476
trigger times: 0
Loss after 20788063 batches: 0.0897
trigger times: 1
Loss after 20789022 batches: 0.0451
trigger times: 0
Loss after 20789981 batches: 0.0338
trigger times: 1
Loss after 20790940 batches: 0.0273
trigger times: 0
Loss after 20791899 batches: 0.0245
trigger times: 0
Loss after 20792858 batches: 0.0225
trigger times: 0
Loss after 20793817 batches: 0.0211
trigger times: 1
Loss after 20794776 batches: 0.0202
trigger times: 2
Loss after 20795735 batches: 0.0190
trigger times: 3
Loss after 20796694 batches: 0.0198
trigger times: 4
Loss after 20797653 batches: 0.0176
trigger times: 5
Loss after 20798612 batches: 0.0172
trigger times: 6
Loss after 20799571 batches: 0.0167
trigger times: 7
Loss after 20800530 batches: 0.0165
trigger times: 8
Loss after 20801489 batches: 0.0158
trigger times: 9
Loss after 20802448 batches: 0.0156
trigger times: 10
Loss after 20803407 batches: 0.0157
trigger times: 11
Loss after 20804366 batches: 0.0150
trigger times: 12
Loss after 20805325 batches: 0.0157
trigger times: 13
Loss after 20806284 batches: 0.0150
trigger times: 0
Loss after 20807243 batches: 0.0151
trigger times: 1
Loss after 20808202 batches: 0.0144
trigger times: 0
Loss after 20809161 batches: 0.0142
trigger times: 1
Loss after 20810120 batches: 0.0151
trigger times: 2
Loss after 20811079 batches: 0.0147
trigger times: 3
Loss after 20812038 batches: 0.0154
trigger times: 4
Loss after 20812997 batches: 0.0140
trigger times: 5
Loss after 20813956 batches: 0.0135
trigger times: 6
Loss after 20814915 batches: 0.0130
trigger times: 7
Loss after 20815874 batches: 0.0134
trigger times: 8
Loss after 20816833 batches: 0.0129
trigger times: 9
Loss after 20817792 batches: 0.0123
trigger times: 10
Loss after 20818751 batches: 0.0133
trigger times: 11
Loss after 20819710 batches: 0.0122
trigger times: 12
Loss after 20820669 batches: 0.0120
trigger times: 13
Loss after 20821628 batches: 0.0118
trigger times: 14
Loss after 20822587 batches: 0.0117
trigger times: 15
Loss after 20823546 batches: 0.0120
trigger times: 0
Loss after 20824505 batches: 0.0120
trigger times: 1
Loss after 20825464 batches: 0.0121
trigger times: 2
Loss after 20826423 batches: 0.0115
trigger times: 3
Loss after 20827382 batches: 0.0114
trigger times: 4
Loss after 20828341 batches: 0.0117
trigger times: 5
Loss after 20829300 batches: 0.0114
trigger times: 6
Loss after 20830259 batches: 0.0132
trigger times: 7
Loss after 20831218 batches: 0.0130
trigger times: 0
Loss after 20832177 batches: 0.0127
trigger times: 1
Loss after 20833136 batches: 0.0129
trigger times: 2
Loss after 20834095 batches: 0.0121
trigger times: 3
Loss after 20835054 batches: 0.0120
trigger times: 4
Loss after 20836013 batches: 0.0117
trigger times: 5
Loss after 20836972 batches: 0.0111
trigger times: 0
Loss after 20837931 batches: 0.0106
trigger times: 1
Loss after 20838890 batches: 0.0103
trigger times: 2
Loss after 20839849 batches: 0.0104
trigger times: 3
Loss after 20840808 batches: 0.0106
trigger times: 4
Loss after 20841767 batches: 0.0102
trigger times: 5
Loss after 20842726 batches: 0.0098
trigger times: 6
Loss after 20843685 batches: 0.0101
trigger times: 0
Loss after 20844644 batches: 0.0100
trigger times: 0
Loss after 20845603 batches: 0.0103
trigger times: 1
Loss after 20846562 batches: 0.0104
trigger times: 2
Loss after 20847521 batches: 0.0100
trigger times: 3
Loss after 20848480 batches: 0.0103
trigger times: 4
Loss after 20849439 batches: 0.0100
trigger times: 5
Loss after 20850398 batches: 0.0095
trigger times: 6
Loss after 20851357 batches: 0.0097
trigger times: 7
Loss after 20852316 batches: 0.0096
trigger times: 8
Loss after 20853275 batches: 0.0093
trigger times: 0
Loss after 20854234 batches: 0.0097
trigger times: 0
Loss after 20855193 batches: 0.0113
trigger times: 1
Loss after 20856152 batches: 0.0109
trigger times: 2
Loss after 20857111 batches: 0.0113
trigger times: 3
Loss after 20858070 batches: 0.0107
trigger times: 4
Loss after 20859029 batches: 0.0103
trigger times: 5
Loss after 20859988 batches: 0.0103
trigger times: 6
Loss after 20860947 batches: 0.0103
trigger times: 7
Loss after 20861906 batches: 0.0094
trigger times: 8
Loss after 20862865 batches: 0.0092
trigger times: 9
Loss after 20863824 batches: 0.0089
trigger times: 10
Loss after 20864783 batches: 0.0088
trigger times: 11
Loss after 20865742 batches: 0.0091
trigger times: 12
Loss after 20866701 batches: 0.0090
trigger times: 13
Loss after 20867660 batches: 0.0087
trigger times: 14
Loss after 20868619 batches: 0.0082
trigger times: 15
Loss after 20869578 batches: 0.0087
trigger times: 16
Loss after 20870537 batches: 0.0087
trigger times: 17
Loss after 20871496 batches: 0.0087
trigger times: 18
Loss after 20872455 batches: 0.0089
trigger times: 19
Loss after 20873414 batches: 0.0085
trigger times: 20
Loss after 20874373 batches: 0.0086
trigger times: 21
Loss after 20875332 batches: 0.0081
trigger times: 22
Loss after 20876291 batches: 0.0085
trigger times: 23
Loss after 20877250 batches: 0.0083
trigger times: 24
Loss after 20878209 batches: 0.0082
trigger times: 25
Early stopping!
Start to test process.
Loss after 20879168 batches: 0.0083
Time to train on one home:  113.68599939346313
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20880131 batches: 0.0721
trigger times: 1
Loss after 20881094 batches: 0.0321
trigger times: 2
Loss after 20882057 batches: 0.0262
trigger times: 3
Loss after 20883020 batches: 0.0264
trigger times: 4
Loss after 20883983 batches: 0.0247
trigger times: 5
Loss after 20884946 batches: 0.0234
trigger times: 6
Loss after 20885909 batches: 0.0223
trigger times: 7
Loss after 20886872 batches: 0.0215
trigger times: 8
Loss after 20887835 batches: 0.0209
trigger times: 9
Loss after 20888798 batches: 0.0197
trigger times: 10
Loss after 20889761 batches: 0.0192
trigger times: 11
Loss after 20890724 batches: 0.0186
trigger times: 12
Loss after 20891687 batches: 0.0181
trigger times: 13
Loss after 20892650 batches: 0.0179
trigger times: 14
Loss after 20893613 batches: 0.0178
trigger times: 15
Loss after 20894576 batches: 0.0177
trigger times: 16
Loss after 20895539 batches: 0.0172
trigger times: 17
Loss after 20896502 batches: 0.0170
trigger times: 18
Loss after 20897465 batches: 0.0171
trigger times: 19
Loss after 20898428 batches: 0.0168
trigger times: 20
Loss after 20899391 batches: 0.0166
trigger times: 21
Loss after 20900354 batches: 0.0167
trigger times: 22
Loss after 20901317 batches: 0.0167
trigger times: 23
Loss after 20902280 batches: 0.0164
trigger times: 24
Loss after 20903243 batches: 0.0164
trigger times: 25
Early stopping!
Start to test process.
Loss after 20904206 batches: 0.0161
Time to train on one home:  59.40417122840881
trigger times: 0
Loss after 20905151 batches: 0.0653
trigger times: 0
Loss after 20906096 batches: 0.0465
trigger times: 0
Loss after 20907041 batches: 0.0348
trigger times: 1
Loss after 20907986 batches: 0.0300
trigger times: 2
Loss after 20908931 batches: 0.0277
trigger times: 3
Loss after 20909876 batches: 0.0258
trigger times: 4
Loss after 20910821 batches: 0.0243
trigger times: 5
Loss after 20911766 batches: 0.0231
trigger times: 6
Loss after 20912711 batches: 0.0220
trigger times: 7
Loss after 20913656 batches: 0.0209
trigger times: 8
Loss after 20914601 batches: 0.0205
trigger times: 9
Loss after 20915546 batches: 0.0201
trigger times: 10
Loss after 20916491 batches: 0.0203
trigger times: 11
Loss after 20917436 batches: 0.0202
trigger times: 12
Loss after 20918381 batches: 0.0188
trigger times: 13
Loss after 20919326 batches: 0.0192
trigger times: 14
Loss after 20920271 batches: 0.0186
trigger times: 15
Loss after 20921216 batches: 0.0185
trigger times: 16
Loss after 20922161 batches: 0.0179
trigger times: 17
Loss after 20923106 batches: 0.0171
trigger times: 18
Loss after 20924051 batches: 0.0160
trigger times: 19
Loss after 20924996 batches: 0.0161
trigger times: 20
Loss after 20925941 batches: 0.0167
trigger times: 21
Loss after 20926886 batches: 0.0165
trigger times: 22
Loss after 20927831 batches: 0.0181
trigger times: 23
Loss after 20928776 batches: 0.0206
trigger times: 24
Loss after 20929721 batches: 0.0196
trigger times: 25
Early stopping!
Start to test process.
Loss after 20930666 batches: 0.0185
Time to train on one home:  58.31744050979614
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20931603 batches: 0.0797
trigger times: 1
Loss after 20932540 batches: 0.0670
trigger times: 2
Loss after 20933477 batches: 0.0639
trigger times: 3
Loss after 20934414 batches: 0.0594
trigger times: 4
Loss after 20935351 batches: 0.0565
trigger times: 5
Loss after 20936288 batches: 0.0549
trigger times: 6
Loss after 20937225 batches: 0.0516
trigger times: 7
Loss after 20938162 batches: 0.0511
trigger times: 8
Loss after 20939099 batches: 0.0505
trigger times: 9
Loss after 20940036 batches: 0.0503
trigger times: 10
Loss after 20940973 batches: 0.0490
trigger times: 11
Loss after 20941910 batches: 0.0483
trigger times: 12
Loss after 20942847 batches: 0.0484
trigger times: 13
Loss after 20943784 batches: 0.0473
trigger times: 14
Loss after 20944721 batches: 0.0482
trigger times: 15
Loss after 20945658 batches: 0.0470
trigger times: 16
Loss after 20946595 batches: 0.0486
trigger times: 17
Loss after 20947532 batches: 0.0476
trigger times: 18
Loss after 20948469 batches: 0.0479
trigger times: 19
Loss after 20949406 batches: 0.0456
trigger times: 20
Loss after 20950343 batches: 0.0466
trigger times: 21
Loss after 20951280 batches: 0.0459
trigger times: 22
Loss after 20952217 batches: 0.0463
trigger times: 23
Loss after 20953154 batches: 0.0452
trigger times: 24
Loss after 20954091 batches: 0.0459
trigger times: 25
Early stopping!
Start to test process.
Loss after 20955028 batches: 0.0436
Time to train on one home:  56.40205979347229
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 20955991 batches: 0.0528
trigger times: 1
Loss after 20956954 batches: 0.0182
trigger times: 2
Loss after 20957917 batches: 0.0139
trigger times: 3
Loss after 20958880 batches: 0.0141
trigger times: 4
Loss after 20959843 batches: 0.0138
trigger times: 5
Loss after 20960806 batches: 0.0139
trigger times: 6
Loss after 20961769 batches: 0.0141
trigger times: 7
Loss after 20962732 batches: 0.0138
trigger times: 8
Loss after 20963695 batches: 0.0139
trigger times: 9
Loss after 20964658 batches: 0.0139
trigger times: 10
Loss after 20965621 batches: 0.0138
trigger times: 11
Loss after 20966584 batches: 0.0140
trigger times: 12
Loss after 20967547 batches: 0.0137
trigger times: 13
Loss after 20968510 batches: 0.0138
trigger times: 14
Loss after 20969473 batches: 0.0138
trigger times: 15
Loss after 20970436 batches: 0.0140
trigger times: 16
Loss after 20971399 batches: 0.0140
trigger times: 17
Loss after 20972362 batches: 0.0138
trigger times: 18
Loss after 20973325 batches: 0.0139
trigger times: 19
Loss after 20974288 batches: 0.0141
trigger times: 20
Loss after 20975251 batches: 0.0138
trigger times: 21
Loss after 20976214 batches: 0.0140
trigger times: 22
Loss after 20977177 batches: 0.0137
trigger times: 23
Loss after 20978140 batches: 0.0139
trigger times: 24
Loss after 20979103 batches: 0.0141
trigger times: 25
Early stopping!
Start to test process.
Loss after 20980066 batches: 0.0141
Time to train on one home:  57.42969989776611
trigger times: 0
Loss after 20981029 batches: 0.0962
trigger times: 1
Loss after 20981992 batches: 0.0767
trigger times: 2
Loss after 20982955 batches: 0.0720
trigger times: 3
Loss after 20983918 batches: 0.0653
trigger times: 4
Loss after 20984881 batches: 0.0620
trigger times: 5
Loss after 20985844 batches: 0.0604
trigger times: 6
Loss after 20986807 batches: 0.0585
trigger times: 7
Loss after 20987770 batches: 0.0566
trigger times: 8
Loss after 20988733 batches: 0.0562
trigger times: 9
Loss after 20989696 batches: 0.0549
trigger times: 10
Loss after 20990659 batches: 0.0536
trigger times: 11
Loss after 20991622 batches: 0.0535
trigger times: 12
Loss after 20992585 batches: 0.0524
trigger times: 13
Loss after 20993548 batches: 0.0526
trigger times: 14
Loss after 20994511 batches: 0.0534
trigger times: 15
Loss after 20995474 batches: 0.0517
trigger times: 16
Loss after 20996437 batches: 0.0523
trigger times: 17
Loss after 20997400 batches: 0.0511
trigger times: 18
Loss after 20998363 batches: 0.0519
trigger times: 19
Loss after 20999326 batches: 0.0509
trigger times: 20
Loss after 21000289 batches: 0.0498
trigger times: 21
Loss after 21001252 batches: 0.0502
trigger times: 22
Loss after 21002215 batches: 0.0505
trigger times: 23
Loss after 21003178 batches: 0.0489
trigger times: 24
Loss after 21004141 batches: 0.0492
trigger times: 25
Early stopping!
Start to test process.
Loss after 21005104 batches: 0.0481
Time to train on one home:  57.530659198760986
trigger times: 0
Loss after 21006067 batches: 0.0733
trigger times: 1
Loss after 21007030 batches: 0.0482
trigger times: 2
Loss after 21007993 batches: 0.0455
trigger times: 3
Loss after 21008956 batches: 0.0412
trigger times: 4
Loss after 21009919 batches: 0.0386
trigger times: 5
Loss after 21010882 batches: 0.0366
trigger times: 6
Loss after 21011845 batches: 0.0354
trigger times: 7
Loss after 21012808 batches: 0.0340
trigger times: 8
Loss after 21013771 batches: 0.0325
trigger times: 9
Loss after 21014734 batches: 0.0319
trigger times: 10
Loss after 21015697 batches: 0.0307
trigger times: 11
Loss after 21016660 batches: 0.0304
trigger times: 12
Loss after 21017623 batches: 0.0301
trigger times: 13
Loss after 21018586 batches: 0.0289
trigger times: 14
Loss after 21019549 batches: 0.0294
trigger times: 15
Loss after 21020512 batches: 0.0293
trigger times: 16
Loss after 21021475 batches: 0.0283
trigger times: 17
Loss after 21022438 batches: 0.0285
trigger times: 18
Loss after 21023401 batches: 0.0286
trigger times: 19
Loss after 21024364 batches: 0.0276
trigger times: 20
Loss after 21025327 batches: 0.0270
trigger times: 21
Loss after 21026290 batches: 0.0273
trigger times: 22
Loss after 21027253 batches: 0.0265
trigger times: 23
Loss after 21028216 batches: 0.0258
trigger times: 24
Loss after 21029179 batches: 0.0257
trigger times: 25
Early stopping!
Start to test process.
Loss after 21030142 batches: 0.0260
Time to train on one home:  56.32997131347656
trigger times: 0
Loss after 21031038 batches: 0.1040
trigger times: 1
Loss after 21031934 batches: 0.0919
trigger times: 2
Loss after 21032830 batches: 0.0838
trigger times: 3
Loss after 21033726 batches: 0.0783
trigger times: 4
Loss after 21034622 batches: 0.0715
trigger times: 5
Loss after 21035518 batches: 0.0692
trigger times: 6
Loss after 21036414 batches: 0.0647
trigger times: 7
Loss after 21037310 batches: 0.0616
trigger times: 8
Loss after 21038206 batches: 0.0604
trigger times: 9
Loss after 21039102 batches: 0.0584
trigger times: 10
Loss after 21039998 batches: 0.0571
trigger times: 11
Loss after 21040894 batches: 0.0572
trigger times: 12
Loss after 21041790 batches: 0.0559
trigger times: 13
Loss after 21042686 batches: 0.0577
trigger times: 14
Loss after 21043582 batches: 0.0558
trigger times: 15
Loss after 21044478 batches: 0.0567
trigger times: 16
Loss after 21045374 batches: 0.0574
trigger times: 17
Loss after 21046270 batches: 0.0562
trigger times: 18
Loss after 21047166 batches: 0.0553
trigger times: 19
Loss after 21048062 batches: 0.0549
trigger times: 20
Loss after 21048958 batches: 0.0529
trigger times: 21
Loss after 21049854 batches: 0.0514
trigger times: 22
Loss after 21050750 batches: 0.0532
trigger times: 23
Loss after 21051646 batches: 0.0555
trigger times: 24
Loss after 21052542 batches: 0.0523
trigger times: 25
Early stopping!
Start to test process.
Loss after 21053438 batches: 0.0525
Time to train on one home:  51.80939769744873
trigger times: 0
Loss after 21054401 batches: 0.1794
trigger times: 1
Loss after 21055364 batches: 0.1205
trigger times: 2
Loss after 21056327 batches: 0.0951
trigger times: 3
Loss after 21057290 batches: 0.0862
trigger times: 4
Loss after 21058253 batches: 0.0752
trigger times: 5
Loss after 21059216 batches: 0.0699
trigger times: 6
Loss after 21060179 batches: 0.0642
trigger times: 7
Loss after 21061142 batches: 0.0579
trigger times: 8
Loss after 21062105 batches: 0.0547
trigger times: 9
Loss after 21063068 batches: 0.0514
trigger times: 10
Loss after 21064031 batches: 0.0506
trigger times: 11
Loss after 21064994 batches: 0.0487
trigger times: 12
Loss after 21065957 batches: 0.0462
trigger times: 13
Loss after 21066920 batches: 0.0471
trigger times: 14
Loss after 21067883 batches: 0.0444
trigger times: 15
Loss after 21068846 batches: 0.0439
trigger times: 16
Loss after 21069809 batches: 0.0412
trigger times: 17
Loss after 21070772 batches: 0.0412
trigger times: 18
Loss after 21071735 batches: 0.0411
trigger times: 19
Loss after 21072698 batches: 0.0388
trigger times: 20
Loss after 21073661 batches: 0.0395
trigger times: 21
Loss after 21074624 batches: 0.0390
trigger times: 22
Loss after 21075587 batches: 0.0394
trigger times: 23
Loss after 21076550 batches: 0.0387
trigger times: 24
Loss after 21077513 batches: 0.0370
trigger times: 25
Early stopping!
Start to test process.
Loss after 21078476 batches: 0.0397
Time to train on one home:  55.824782848358154
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 21079439 batches: 0.0884
trigger times: 1
Loss after 21080402 batches: 0.0749
trigger times: 2
Loss after 21081365 batches: 0.0734
trigger times: 3
Loss after 21082328 batches: 0.0692
trigger times: 4
Loss after 21083291 batches: 0.0653
trigger times: 5
Loss after 21084254 batches: 0.0613
trigger times: 6
Loss after 21085217 batches: 0.0581
trigger times: 7
Loss after 21086180 batches: 0.0536
trigger times: 8
Loss after 21087143 batches: 0.0516
trigger times: 9
Loss after 21088106 batches: 0.0527
trigger times: 10
Loss after 21089069 batches: 0.0486
trigger times: 11
Loss after 21090032 batches: 0.0493
trigger times: 12
Loss after 21090995 batches: 0.0471
trigger times: 13
Loss after 21091958 batches: 0.0471
trigger times: 14
Loss after 21092921 batches: 0.0479
trigger times: 15
Loss after 21093884 batches: 0.0464
trigger times: 16
Loss after 21094847 batches: 0.0462
trigger times: 17
Loss after 21095810 batches: 0.0442
trigger times: 18
Loss after 21096773 batches: 0.0440
trigger times: 19
Loss after 21097736 batches: 0.0428
trigger times: 20
Loss after 21098699 batches: 0.0419
trigger times: 21
Loss after 21099662 batches: 0.0419
trigger times: 22
Loss after 21100625 batches: 0.0405
trigger times: 23
Loss after 21101588 batches: 0.0410
trigger times: 24
Loss after 21102551 batches: 0.0406
trigger times: 25
Early stopping!
Start to test process.
Loss after 21103514 batches: 0.0413
Time to train on one home:  57.5237181186676
trigger times: 0
Loss after 21104477 batches: 0.0823
trigger times: 1
Loss after 21105440 batches: 0.0605
trigger times: 2
Loss after 21106403 batches: 0.0526
trigger times: 3
Loss after 21107366 batches: 0.0478
trigger times: 4
Loss after 21108329 batches: 0.0436
trigger times: 5
Loss after 21109292 batches: 0.0424
trigger times: 6
Loss after 21110255 batches: 0.0390
trigger times: 7
Loss after 21111218 batches: 0.0380
trigger times: 8
Loss after 21112181 batches: 0.0382
trigger times: 9
Loss after 21113144 batches: 0.0374
trigger times: 10
Loss after 21114107 batches: 0.0368
trigger times: 11
Loss after 21115070 batches: 0.0361
trigger times: 12
Loss after 21116033 batches: 0.0364
trigger times: 13
Loss after 21116996 batches: 0.0366
trigger times: 14
Loss after 21117959 batches: 0.0354
trigger times: 15
Loss after 21118922 batches: 0.0354
trigger times: 16
Loss after 21119885 batches: 0.0334
trigger times: 17
Loss after 21120848 batches: 0.0339
trigger times: 18
Loss after 21121811 batches: 0.0338
trigger times: 19
Loss after 21122774 batches: 0.0326
trigger times: 20
Loss after 21123737 batches: 0.0327
trigger times: 21
Loss after 21124700 batches: 0.0313
trigger times: 22
Loss after 21125663 batches: 0.0315
trigger times: 23
Loss after 21126626 batches: 0.0316
trigger times: 24
Loss after 21127589 batches: 0.0297
trigger times: 25
Early stopping!
Start to test process.
Loss after 21128552 batches: 0.0304
Time to train on one home:  54.84473657608032
trigger times: 0
Loss after 21129515 batches: 0.0428
trigger times: 1
Loss after 21130478 batches: 0.0316
trigger times: 2
Loss after 21131441 batches: 0.0305
trigger times: 3
Loss after 21132404 batches: 0.0283
trigger times: 4
Loss after 21133367 batches: 0.0254
trigger times: 5
Loss after 21134330 batches: 0.0238
trigger times: 6
Loss after 21135293 batches: 0.0233
trigger times: 7
Loss after 21136256 batches: 0.0211
trigger times: 8
Loss after 21137219 batches: 0.0212
trigger times: 9
Loss after 21138182 batches: 0.0217
trigger times: 10
Loss after 21139145 batches: 0.0204
trigger times: 11
Loss after 21140108 batches: 0.0205
trigger times: 12
Loss after 21141071 batches: 0.0204
trigger times: 13
Loss after 21142034 batches: 0.0201
trigger times: 14
Loss after 21142997 batches: 0.0199
trigger times: 15
Loss after 21143960 batches: 0.0202
trigger times: 16
Loss after 21144923 batches: 0.0198
trigger times: 17
Loss after 21145886 batches: 0.0202
trigger times: 18
Loss after 21146849 batches: 0.0184
trigger times: 19
Loss after 21147812 batches: 0.0189
trigger times: 20
Loss after 21148775 batches: 0.0193
trigger times: 21
Loss after 21149738 batches: 0.0205
trigger times: 22
Loss after 21150701 batches: 0.0196
trigger times: 23
Loss after 21151664 batches: 0.0187
trigger times: 24
Loss after 21152627 batches: 0.0182
trigger times: 25
Early stopping!
Start to test process.
Loss after 21153590 batches: 0.0178
Time to train on one home:  60.12222671508789
trigger times: 0
Loss after 21154553 batches: 0.0890
trigger times: 1
Loss after 21155516 batches: 0.0484
trigger times: 2
Loss after 21156479 batches: 0.0472
trigger times: 3
Loss after 21157442 batches: 0.0466
trigger times: 4
Loss after 21158405 batches: 0.0434
trigger times: 5
Loss after 21159368 batches: 0.0414
trigger times: 6
Loss after 21160331 batches: 0.0393
trigger times: 7
Loss after 21161294 batches: 0.0385
trigger times: 8
Loss after 21162257 batches: 0.0381
trigger times: 9
Loss after 21163220 batches: 0.0377
trigger times: 10
Loss after 21164183 batches: 0.0365
trigger times: 11
Loss after 21165146 batches: 0.0356
trigger times: 12
Loss after 21166109 batches: 0.0348
trigger times: 13
Loss after 21167072 batches: 0.0343
trigger times: 14
Loss after 21168035 batches: 0.0341
trigger times: 15
Loss after 21168998 batches: 0.0338
trigger times: 16
Loss after 21169961 batches: 0.0336
trigger times: 17
Loss after 21170924 batches: 0.0329
trigger times: 18
Loss after 21171887 batches: 0.0327
trigger times: 19
Loss after 21172850 batches: 0.0322
trigger times: 20
Loss after 21173813 batches: 0.0319
trigger times: 21
Loss after 21174776 batches: 0.0322
trigger times: 22
Loss after 21175739 batches: 0.0317
trigger times: 23
Loss after 21176702 batches: 0.0314
trigger times: 24
Loss after 21177665 batches: 0.0315
trigger times: 25
Early stopping!
Start to test process.
Loss after 21178628 batches: 0.0313
Time to train on one home:  56.74143743515015
trigger times: 0
Loss after 21179523 batches: 0.0641
trigger times: 1
Loss after 21180418 batches: 0.0346
trigger times: 2
Loss after 21181313 batches: 0.0122
trigger times: 0
Loss after 21182208 batches: 0.0086
trigger times: 1
Loss after 21183103 batches: 0.0056
trigger times: 2
Loss after 21183998 batches: 0.0046
trigger times: 3
Loss after 21184893 batches: 0.0043
trigger times: 4
Loss after 21185788 batches: 0.0041
trigger times: 5
Loss after 21186683 batches: 0.0036
trigger times: 6
Loss after 21187578 batches: 0.0041
trigger times: 7
Loss after 21188473 batches: 0.0042
trigger times: 8
Loss after 21189368 batches: 0.0032
trigger times: 9
Loss after 21190263 batches: 0.0027
trigger times: 10
Loss after 21191158 batches: 0.0023
trigger times: 11
Loss after 21192053 batches: 0.0024
trigger times: 12
Loss after 21192948 batches: 0.0025
trigger times: 13
Loss after 21193843 batches: 0.0025
trigger times: 14
Loss after 21194738 batches: 0.0021
trigger times: 15
Loss after 21195633 batches: 0.0022
trigger times: 16
Loss after 21196528 batches: 0.0018
trigger times: 17
Loss after 21197423 batches: 0.0018
trigger times: 18
Loss after 21198318 batches: 0.0034
trigger times: 19
Loss after 21199213 batches: 0.0052
trigger times: 20
Loss after 21200108 batches: 0.0039
trigger times: 21
Loss after 21201003 batches: 0.0026
trigger times: 22
Loss after 21201898 batches: 0.0023
trigger times: 23
Loss after 21202793 batches: 0.0020
trigger times: 24
Loss after 21203688 batches: 0.0017
trigger times: 25
Early stopping!
Start to test process.
Loss after 21204583 batches: 0.0017
Time to train on one home:  57.794124603271484
train_results:  [0.08167134079891343, 0.05326533742725424, 0.04550632822440204, 0.043288951247213554, 0.04044017604079157, 0.03895779660920387, 0.03720831167862333, 0.03590612433584336, 0.034888222413376094, 0.034613204319843105, 0.033508375671873286, 0.03325774057242148, 0.03243952582662181, 0.032562873532530595, 0.03126130941140662, 0.03133166822639105, 0.03049944561800663, 0.030467629500461172, 0.03015643608279951, 0.030195612542982676]
test_results:  [[0.1372801512479782, -0.17630011535437906, 0.09165623225234511, 1.1074626263238363, 0.9123759614671366, 36.58046097732784, 4436.628], [0.15019331872463226, -0.03945367500616581, 0.18360298702085523, 1.1433348997800294, 0.8062334854689305, 37.76535360317488, 3920.487], [0.10503555089235306, 0.11604646509476468, 0.2941523892332113, 1.0160329939295982, 0.6856226066458603, 33.56046010283253, 3333.9902], [0.12885217368602753, 0.08478938334622399, 0.2717248960955158, 1.0659429087240713, 0.709866598204025, 35.209028322765505, 3451.882], [0.0979808047413826, 0.13873112342958427, 0.31531024815199815, 0.9756221648036203, 0.6680276622646856, 32.225655006238654, 3248.4307], [0.12634874880313873, 0.12620499423095177, 0.3036101279672065, 1.0441931429634264, 0.6777433262115133, 34.490614501150326, 3295.6755], [0.09346123039722443, 0.1737103640286468, 0.34391388215749696, 0.9528860989456474, 0.6408966437771907, 31.474662828149263, 3116.5005], [0.1353289783000946, 0.14834475745401599, 0.32130749263260416, 1.0592306287028919, 0.6605710190489519, 34.98731583193527, 3212.171], [0.08650185167789459, 0.15801192772828487, 0.33556989960356126, 0.9421185599161759, 0.6530728568769362, 31.119001578796855, 3175.71], [0.13675299286842346, 0.15565351472302724, 0.32920502623266784, 1.059933101560365, 0.6549021202780799, 35.010519125969424, 3184.605], [0.0885479673743248, 0.1559912263273432, 0.33358461589199145, 0.944871497166479, 0.654640173496829, 31.209933508474723, 3183.3313], [0.1345348209142685, 0.15444619808844395, 0.3263722552493196, 1.0544752388510252, 0.6558385527073367, 34.830241138150235, 3189.1584], [0.09016867727041245, 0.15349941943138035, 0.3319629557497865, 0.9480173077546179, 0.65657289912243, 31.313842388762147, 3192.7297], [0.13274754583835602, 0.1683317466130475, 0.33338956387040825, 1.0413414780800958, 0.645068476656334, 34.39642151124207, 3136.7866], [0.08685638755559921, 0.16053285093432457, 0.33813218961147645, 0.938309926482314, 0.6511175433777052, 30.99319907910727, 3166.2017], [0.13567858934402466, 0.1592208654199878, 0.3293138590809072, 1.0581373625723918, 0.6521351625870662, 34.951204293654875, 3171.1501], [0.08736597001552582, 0.1548414758248644, 0.3338051677093621, 0.9435825164148742, 0.6555319615580705, 31.16735734476153, 3187.668], [0.13322944939136505, 0.16497793935194727, 0.3304458768859095, 1.0481311306385603, 0.6476697950280367, 34.620689684777616, 3149.4363], [0.08741147071123123, 0.15130263388043563, 0.33007529206673664, 0.9466815576133256, 0.6582767919114704, 31.269721391125234, 3201.015], [0.13793548941612244, 0.14146804396514878, 0.32086843455623654, 1.0714886450573733, 0.6659048185629647, 35.39220885338468, 3238.1082]]
Round_19_results:  [0.13793548941612244, 0.14146804396514878, 0.32086843455623654, 1.0714886450573733, 0.6659048185629647, 35.39220885338468, 3238.1082]