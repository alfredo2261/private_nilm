LSTM(
  (conv1): Conv1d(1, 16, kernel_size=(7,), stride=(1,), padding=(3,))
  (lstm1): LSTM(16, 64, batch_first=True, bidirectional=True)
  (lstm2): LSTM(128, 85, batch_first=True, bidirectional=True)
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=170, out_features=170, bias=True)
  )
  (linear1): Linear(in_features=170, out_features=211, bias=True)
  (linear2): Linear(in_features=211, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (maxpool): MaxPool1d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)
  (dropout): Dropout(p=0.5, inplace=False)
)
Window Length:  136
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 00962 batches: 0.2717
trigger times: 1
Loss after 01924 batches: 0.2338
trigger times: 2
Loss after 02886 batches: 0.1311
trigger times: 3
Loss after 03848 batches: 0.1002
trigger times: 4
Loss after 04810 batches: 0.0889
trigger times: 5
Loss after 05772 batches: 0.0810
trigger times: 6
Loss after 06734 batches: 0.0794
trigger times: 7
Loss after 07696 batches: 0.0791
trigger times: 8
Loss after 08658 batches: 0.0783
trigger times: 9
Loss after 09620 batches: 0.0784
trigger times: 10
Loss after 10582 batches: 0.0778
trigger times: 11
Loss after 11544 batches: 0.0779
trigger times: 12
Loss after 12506 batches: 0.0777
trigger times: 13
Loss after 13468 batches: 0.0776
trigger times: 14
Loss after 14430 batches: 0.0773
trigger times: 15
Loss after 15392 batches: 0.0769
trigger times: 16
Loss after 16354 batches: 0.0770
trigger times: 17
Loss after 17316 batches: 0.0769
trigger times: 18
Loss after 18278 batches: 0.0774
trigger times: 19
Loss after 19240 batches: 0.0773
trigger times: 20
Loss after 20202 batches: 0.0772
trigger times: 21
Loss after 21164 batches: 0.0771
trigger times: 22
Loss after 22126 batches: 0.0769
trigger times: 23
Loss after 23088 batches: 0.0767
trigger times: 24
Loss after 24050 batches: 0.0766
trigger times: 25
Early stopping!
Start to test process.
Loss after 25012 batches: 0.0770
Time to train on one home:  48.68761658668518
trigger times: 0
Loss after 25941 batches: 0.2070
trigger times: 1
Loss after 26870 batches: 0.1878
trigger times: 2
Loss after 27799 batches: 0.1468
trigger times: 3
Loss after 28728 batches: 0.1400
trigger times: 4
Loss after 29657 batches: 0.1387
trigger times: 5
Loss after 30586 batches: 0.1354
trigger times: 6
Loss after 31515 batches: 0.1359
trigger times: 7
Loss after 32444 batches: 0.1364
trigger times: 8
Loss after 33373 batches: 0.1368
trigger times: 9
Loss after 34302 batches: 0.1362
trigger times: 10
Loss after 35231 batches: 0.1356
trigger times: 11
Loss after 36160 batches: 0.1362
trigger times: 12
Loss after 37089 batches: 0.1355
trigger times: 13
Loss after 38018 batches: 0.1360
trigger times: 14
Loss after 38947 batches: 0.1354
trigger times: 15
Loss after 39876 batches: 0.1318
trigger times: 16
Loss after 40805 batches: 0.1020
trigger times: 17
Loss after 41734 batches: 0.0816
trigger times: 18
Loss after 42663 batches: 0.0740
trigger times: 19
Loss after 43592 batches: 0.0673
trigger times: 20
Loss after 44521 batches: 0.0642
trigger times: 21
Loss after 45450 batches: 0.0665
trigger times: 22
Loss after 46379 batches: 0.0642
trigger times: 23
Loss after 47308 batches: 0.0582
trigger times: 24
Loss after 48237 batches: 0.0585
trigger times: 25
Early stopping!
Start to test process.
Loss after 49166 batches: 0.0562
Time to train on one home:  48.13014793395996
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 50129 batches: 0.3421
trigger times: 1
Loss after 51092 batches: 0.2946
trigger times: 2
Loss after 52055 batches: 0.1442
trigger times: 3
Loss after 53018 batches: 0.0454
trigger times: 4
Loss after 53981 batches: 0.0347
trigger times: 5
Loss after 54944 batches: 0.0239
trigger times: 6
Loss after 55907 batches: 0.0210
trigger times: 7
Loss after 56870 batches: 0.0191
trigger times: 8
Loss after 57833 batches: 0.0180
trigger times: 9
Loss after 58796 batches: 0.0173
trigger times: 10
Loss after 59759 batches: 0.0169
trigger times: 11
Loss after 60722 batches: 0.0167
trigger times: 12
Loss after 61685 batches: 0.0164
trigger times: 13
Loss after 62648 batches: 0.0161
trigger times: 14
Loss after 63611 batches: 0.0160
trigger times: 15
Loss after 64574 batches: 0.0157
trigger times: 16
Loss after 65537 batches: 0.0155
trigger times: 17
Loss after 66500 batches: 0.0155
trigger times: 18
Loss after 67463 batches: 0.0157
trigger times: 19
Loss after 68426 batches: 0.0154
trigger times: 20
Loss after 69389 batches: 0.0152
trigger times: 21
Loss after 70352 batches: 0.0151
trigger times: 22
Loss after 71315 batches: 0.0150
trigger times: 23
Loss after 72278 batches: 0.0149
trigger times: 24
Loss after 73241 batches: 0.0147
trigger times: 25
Early stopping!
Start to test process.
Loss after 74204 batches: 0.0146
Time to train on one home:  46.74775433540344
trigger times: 0
Loss after 75167 batches: 0.2076
trigger times: 1
Loss after 76130 batches: 0.1753
trigger times: 2
Loss after 77093 batches: 0.0919
trigger times: 3
Loss after 78056 batches: 0.0700
trigger times: 4
Loss after 79019 batches: 0.0625
trigger times: 5
Loss after 79982 batches: 0.0571
trigger times: 6
Loss after 80945 batches: 0.0560
trigger times: 7
Loss after 81908 batches: 0.0553
trigger times: 8
Loss after 82871 batches: 0.0550
trigger times: 9
Loss after 83834 batches: 0.0547
trigger times: 10
Loss after 84797 batches: 0.0546
trigger times: 11
Loss after 85760 batches: 0.0545
trigger times: 12
Loss after 86723 batches: 0.0542
trigger times: 13
Loss after 87686 batches: 0.0543
trigger times: 14
Loss after 88649 batches: 0.0540
trigger times: 15
Loss after 89612 batches: 0.0542
trigger times: 16
Loss after 90575 batches: 0.0543
trigger times: 17
Loss after 91538 batches: 0.0541
trigger times: 18
Loss after 92501 batches: 0.0540
trigger times: 19
Loss after 93464 batches: 0.0539
trigger times: 20
Loss after 94427 batches: 0.0539
trigger times: 21
Loss after 95390 batches: 0.0539
trigger times: 22
Loss after 96353 batches: 0.0538
trigger times: 23
Loss after 97316 batches: 0.0538
trigger times: 24
Loss after 98279 batches: 0.0539
trigger times: 25
Early stopping!
Start to test process.
Loss after 99242 batches: 0.0538
Time to train on one home:  46.535027265548706
trigger times: 0
Loss after 100205 batches: 0.2643
trigger times: 1
Loss after 101168 batches: 0.2341
trigger times: 2
Loss after 102131 batches: 0.1619
trigger times: 3
Loss after 103094 batches: 0.1425
trigger times: 4
Loss after 104057 batches: 0.1375
trigger times: 5
Loss after 105020 batches: 0.1341
trigger times: 6
Loss after 105983 batches: 0.1324
trigger times: 7
Loss after 106946 batches: 0.1324
trigger times: 8
Loss after 107909 batches: 0.1321
trigger times: 9
Loss after 108872 batches: 0.1319
trigger times: 10
Loss after 109835 batches: 0.1317
trigger times: 11
Loss after 110798 batches: 0.1313
trigger times: 12
Loss after 111761 batches: 0.1312
trigger times: 13
Loss after 112724 batches: 0.1308
trigger times: 14
Loss after 113687 batches: 0.1311
trigger times: 15
Loss after 114650 batches: 0.1312
trigger times: 16
Loss after 115613 batches: 0.1310
trigger times: 17
Loss after 116576 batches: 0.1310
trigger times: 18
Loss after 117539 batches: 0.1309
trigger times: 19
Loss after 118502 batches: 0.1309
trigger times: 20
Loss after 119465 batches: 0.1309
trigger times: 21
Loss after 120428 batches: 0.1315
trigger times: 22
Loss after 121391 batches: 0.1309
trigger times: 23
Loss after 122354 batches: 0.1309
trigger times: 24
Loss after 123317 batches: 0.1308
trigger times: 25
Early stopping!
Start to test process.
Loss after 124280 batches: 0.1308
Time to train on one home:  46.63251209259033
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 125243 batches: 0.3649
trigger times: 1
Loss after 126206 batches: 0.3238
trigger times: 2
Loss after 127169 batches: 0.1960
trigger times: 3
Loss after 128132 batches: 0.1366
trigger times: 4
Loss after 129095 batches: 0.1272
trigger times: 5
Loss after 130058 batches: 0.1172
trigger times: 6
Loss after 131021 batches: 0.1151
trigger times: 7
Loss after 131984 batches: 0.1148
trigger times: 8
Loss after 132947 batches: 0.1135
trigger times: 9
Loss after 133910 batches: 0.1134
trigger times: 10
Loss after 134873 batches: 0.1131
trigger times: 11
Loss after 135836 batches: 0.1129
trigger times: 12
Loss after 136799 batches: 0.1128
trigger times: 13
Loss after 137762 batches: 0.1125
trigger times: 14
Loss after 138725 batches: 0.1122
trigger times: 15
Loss after 139688 batches: 0.1124
trigger times: 16
Loss after 140651 batches: 0.1125
trigger times: 17
Loss after 141614 batches: 0.1123
trigger times: 18
Loss after 142577 batches: 0.1120
trigger times: 19
Loss after 143540 batches: 0.1118
trigger times: 20
Loss after 144503 batches: 0.1118
trigger times: 21
Loss after 145466 batches: 0.1118
trigger times: 22
Loss after 146429 batches: 0.1119
trigger times: 23
Loss after 147392 batches: 0.1119
trigger times: 24
Loss after 148355 batches: 0.1113
trigger times: 25
Early stopping!
Start to test process.
Loss after 149318 batches: 0.1117
Time to train on one home:  46.265809774398804
trigger times: 0
Loss after 150281 batches: 0.2392
trigger times: 1
Loss after 151244 batches: 0.2107
trigger times: 2
Loss after 152207 batches: 0.1339
trigger times: 3
Loss after 153170 batches: 0.1167
trigger times: 4
Loss after 154133 batches: 0.1108
trigger times: 5
Loss after 155096 batches: 0.1054
trigger times: 6
Loss after 156059 batches: 0.1037
trigger times: 7
Loss after 157022 batches: 0.1040
trigger times: 8
Loss after 157985 batches: 0.1033
trigger times: 9
Loss after 158948 batches: 0.1032
trigger times: 10
Loss after 159911 batches: 0.1030
trigger times: 11
Loss after 160874 batches: 0.1025
trigger times: 12
Loss after 161837 batches: 0.1024
trigger times: 13
Loss after 162800 batches: 0.1029
trigger times: 14
Loss after 163763 batches: 0.1025
trigger times: 15
Loss after 164726 batches: 0.1024
trigger times: 16
Loss after 165689 batches: 0.1023
trigger times: 17
Loss after 166652 batches: 0.1025
trigger times: 18
Loss after 167615 batches: 0.1023
trigger times: 19
Loss after 168578 batches: 0.1020
trigger times: 20
Loss after 169541 batches: 0.1023
trigger times: 21
Loss after 170504 batches: 0.1022
trigger times: 22
Loss after 171467 batches: 0.1022
trigger times: 23
Loss after 172430 batches: 0.1022
trigger times: 24
Loss after 173393 batches: 0.1023
trigger times: 25
Early stopping!
Start to test process.
Loss after 174356 batches: 0.1020
Time to train on one home:  48.89695882797241
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 175319 batches: 0.2166
trigger times: 1
Loss after 176282 batches: 0.1860
trigger times: 2
Loss after 177245 batches: 0.1059
trigger times: 3
Loss after 178208 batches: 0.0830
trigger times: 4
Loss after 179171 batches: 0.0776
trigger times: 5
Loss after 180134 batches: 0.0725
trigger times: 6
Loss after 181097 batches: 0.0714
trigger times: 7
Loss after 182060 batches: 0.0710
trigger times: 8
Loss after 183023 batches: 0.0706
trigger times: 9
Loss after 183986 batches: 0.0701
trigger times: 10
Loss after 184949 batches: 0.0699
trigger times: 11
Loss after 185912 batches: 0.0699
trigger times: 12
Loss after 186875 batches: 0.0699
trigger times: 13
Loss after 187838 batches: 0.0699
trigger times: 14
Loss after 188801 batches: 0.0696
trigger times: 15
Loss after 189764 batches: 0.0695
Loss after 190727 batches: 0.0697
trigger times: 17
Loss after 191690 batches: 0.0695
trigger times: 18
Loss after 192653 batches: 0.0695
trigger times: 19
Loss after 193616 batches: 0.0694
trigger times: 20
Loss after 194579 batches: 0.0696
trigger times: 21
Loss after 195542 batches: 0.0695
trigger times: 22
Loss after 196505 batches: 0.0694
trigger times: 23
Loss after 197468 batches: 0.0695
trigger times: 24
Loss after 198431 batches: 0.0695
trigger times: 25
Early stopping!
Start to test process.
Loss after 199394 batches: 0.0695
Time to train on one home:  46.71533393859863
trigger times: 0
Loss after 200352 batches: 0.2123
trigger times: 1
Loss after 201310 batches: 0.1909
trigger times: 2
Loss after 202268 batches: 0.1513
trigger times: 3
Loss after 203226 batches: 0.1442
trigger times: 4
Loss after 204184 batches: 0.1437
trigger times: 5
Loss after 205142 batches: 0.1407
trigger times: 6
Loss after 206100 batches: 0.1402
trigger times: 7
Loss after 207058 batches: 0.1395
trigger times: 8
Loss after 208016 batches: 0.1397
trigger times: 9
Loss after 208974 batches: 0.1392
trigger times: 10
Loss after 209932 batches: 0.1388
trigger times: 11
Loss after 210890 batches: 0.1392
trigger times: 12
Loss after 211848 batches: 0.1385
trigger times: 13
Loss after 212806 batches: 0.1395
trigger times: 14
Loss after 213764 batches: 0.1395
trigger times: 15
Loss after 214722 batches: 0.1396
trigger times: 16
Loss after 215680 batches: 0.1390
trigger times: 17
Loss after 216638 batches: 0.1404
trigger times: 18
Loss after 217596 batches: 0.1392
trigger times: 19
Loss after 218554 batches: 0.1333
trigger times: 20
Loss after 219512 batches: 0.1368
trigger times: 21
Loss after 220470 batches: 0.0945
trigger times: 22
Loss after 221428 batches: 0.0823
trigger times: 23
Loss after 222386 batches: 0.0750
trigger times: 24
Loss after 223344 batches: 0.0746
trigger times: 25
Early stopping!
Start to test process.
Loss after 224302 batches: 0.0704
Time to train on one home:  46.945531606674194
trigger times: 0
Loss after 225264 batches: 0.2719
trigger times: 1
Loss after 226226 batches: 0.2358
trigger times: 2
Loss after 227188 batches: 0.1304
trigger times: 3
Loss after 228150 batches: 0.0974
trigger times: 4
Loss after 229112 batches: 0.0889
trigger times: 5
Loss after 230074 batches: 0.0818
trigger times: 6
Loss after 231036 batches: 0.0788
trigger times: 7
Loss after 231998 batches: 0.0793
trigger times: 8
Loss after 232960 batches: 0.0782
trigger times: 9
Loss after 233922 batches: 0.0782
trigger times: 10
Loss after 234884 batches: 0.0774
trigger times: 11
Loss after 235846 batches: 0.0773
trigger times: 12
Loss after 236808 batches: 0.0774
trigger times: 13
Loss after 237770 batches: 0.0773
trigger times: 14
Loss after 238732 batches: 0.0772
trigger times: 15
Loss after 239694 batches: 0.0774
trigger times: 16
Loss after 240656 batches: 0.0774
trigger times: 17
Loss after 241618 batches: 0.0776
trigger times: 18
Loss after 242580 batches: 0.0766
trigger times: 19
Loss after 243542 batches: 0.0768
trigger times: 20
Loss after 244504 batches: 0.0770
trigger times: 21
Loss after 245466 batches: 0.0770
trigger times: 22
Loss after 246428 batches: 0.0767
trigger times: 23
Loss after 247390 batches: 0.0769
trigger times: 24
Loss after 248352 batches: 0.0769
trigger times: 25
Early stopping!
Start to test process.
Loss after 249314 batches: 0.0772
Time to train on one home:  47.08325004577637
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 250277 batches: 0.4870
trigger times: 1
Loss after 251240 batches: 0.4299
trigger times: 2
Loss after 252203 batches: 0.2382
trigger times: 3
Loss after 253166 batches: 0.0703
trigger times: 4
Loss after 254129 batches: 0.0523
trigger times: 5
Loss after 255092 batches: 0.0374
trigger times: 6
Loss after 256055 batches: 0.0333
trigger times: 7
Loss after 257018 batches: 0.0304
trigger times: 8
Loss after 257981 batches: 0.0294
trigger times: 9
Loss after 258944 batches: 0.0282
trigger times: 10
Loss after 259907 batches: 0.0275
trigger times: 11
Loss after 260870 batches: 0.0271
trigger times: 12
Loss after 261833 batches: 0.0265
trigger times: 13
Loss after 262796 batches: 0.0263
trigger times: 14
Loss after 263759 batches: 0.0261
trigger times: 15
Loss after 264722 batches: 0.0260
trigger times: 16
Loss after 265685 batches: 0.0254
trigger times: 17
Loss after 266648 batches: 0.0254
trigger times: 18
Loss after 267611 batches: 0.0252
trigger times: 19
Loss after 268574 batches: 0.0250
trigger times: 20
Loss after 269537 batches: 0.0250
trigger times: 21
Loss after 270500 batches: 0.0248
trigger times: 22
Loss after 271463 batches: 0.0248
trigger times: 23
Loss after 272426 batches: 0.0245
trigger times: 24
Loss after 273389 batches: 0.0245
trigger times: 25
Early stopping!
Start to test process.
Loss after 274352 batches: 0.0242
Time to train on one home:  46.52058124542236
trigger times: 0
Loss after 275315 batches: 0.1859
trigger times: 1
Loss after 276278 batches: 0.1614
trigger times: 2
Loss after 277241 batches: 0.1128
trigger times: 3
Loss after 278204 batches: 0.1018
trigger times: 4
Loss after 279167 batches: 0.0998
trigger times: 5
Loss after 280130 batches: 0.0967
trigger times: 6
Loss after 281093 batches: 0.0966
trigger times: 7
Loss after 282056 batches: 0.0962
trigger times: 8
Loss after 283019 batches: 0.0959
trigger times: 9
Loss after 283982 batches: 0.0957
trigger times: 10
Loss after 284945 batches: 0.0954
trigger times: 11
Loss after 285908 batches: 0.0949
trigger times: 12
Loss after 286871 batches: 0.0953
trigger times: 13
Loss after 287834 batches: 0.0950
trigger times: 14
Loss after 288797 batches: 0.0955
trigger times: 15
Loss after 289760 batches: 0.0951
trigger times: 16
Loss after 290723 batches: 0.0951
trigger times: 17
Loss after 291686 batches: 0.0954
trigger times: 18
Loss after 292649 batches: 0.0949
trigger times: 19
Loss after 293612 batches: 0.0955
trigger times: 20
Loss after 294575 batches: 0.0954
trigger times: 21
Loss after 295538 batches: 0.0956
trigger times: 22
Loss after 296501 batches: 0.0951
trigger times: 23
Loss after 297464 batches: 0.0950
trigger times: 24
Loss after 298427 batches: 0.0952
trigger times: 25
Early stopping!
Start to test process.
Loss after 299390 batches: 0.0953
Time to train on one home:  47.14177894592285
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 300353 batches: 0.2168
trigger times: 1
Loss after 301316 batches: 0.1863
trigger times: 2
Loss after 302279 batches: 0.1056
trigger times: 3
Loss after 303242 batches: 0.0847
trigger times: 4
Loss after 304205 batches: 0.0779
trigger times: 5
Loss after 305168 batches: 0.0724
trigger times: 6
Loss after 306131 batches: 0.0713
trigger times: 7
Loss after 307094 batches: 0.0710
trigger times: 8
Loss after 308057 batches: 0.0704
trigger times: 9
Loss after 309020 batches: 0.0705
trigger times: 10
Loss after 309983 batches: 0.0700
trigger times: 11
Loss after 310946 batches: 0.0700
trigger times: 12
Loss after 311909 batches: 0.0700
trigger times: 13
Loss after 312872 batches: 0.0699
trigger times: 14
Loss after 313835 batches: 0.0698
trigger times: 15
Loss after 314798 batches: 0.0696
trigger times: 16
Loss after 315761 batches: 0.0696
trigger times: 17
Loss after 316724 batches: 0.0696
trigger times: 18
Loss after 317687 batches: 0.0696
trigger times: 19
Loss after 318650 batches: 0.0695
trigger times: 20
Loss after 319613 batches: 0.0694
trigger times: 21
Loss after 320576 batches: 0.0696
trigger times: 22
Loss after 321539 batches: 0.0696
trigger times: 23
Loss after 322502 batches: 0.0695
trigger times: 24
Loss after 323465 batches: 0.0692
trigger times: 25
Early stopping!
Start to test process.
Loss after 324428 batches: 0.0693
Time to train on one home:  46.656067848205566
trigger times: 0
Loss after 325391 batches: 0.1865
trigger times: 1
Loss after 326354 batches: 0.1615
trigger times: 2
Loss after 327317 batches: 0.1119
trigger times: 3
Loss after 328280 batches: 0.1023
trigger times: 4
Loss after 329243 batches: 0.0992
trigger times: 5
Loss after 330206 batches: 0.0971
trigger times: 6
Loss after 331169 batches: 0.0959
trigger times: 7
Loss after 332132 batches: 0.0964
trigger times: 8
Loss after 333095 batches: 0.0958
trigger times: 9
Loss after 334058 batches: 0.0955
trigger times: 10
Loss after 335021 batches: 0.0959
trigger times: 11
Loss after 335984 batches: 0.0952
trigger times: 12
Loss after 336947 batches: 0.0951
trigger times: 13
Loss after 337910 batches: 0.0957
trigger times: 14
Loss after 338873 batches: 0.0950
trigger times: 15
Loss after 339836 batches: 0.0950
trigger times: 16
Loss after 340799 batches: 0.0952
trigger times: 17
Loss after 341762 batches: 0.0953
trigger times: 18
Loss after 342725 batches: 0.0951
trigger times: 19
Loss after 343688 batches: 0.0950
trigger times: 20
Loss after 344651 batches: 0.0953
trigger times: 21
Loss after 345614 batches: 0.0951
trigger times: 22
Loss after 346577 batches: 0.0951
trigger times: 23
Loss after 347540 batches: 0.0951
trigger times: 24
Loss after 348503 batches: 0.0949
trigger times: 25
Early stopping!
Start to test process.
Loss after 349466 batches: 0.0950
Time to train on one home:  46.6309859752655
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 350429 batches: 0.2048
trigger times: 1
Loss after 351392 batches: 0.1780
trigger times: 2
Loss after 352355 batches: 0.1120
trigger times: 3
Loss after 353318 batches: 0.0962
trigger times: 4
Loss after 354281 batches: 0.0915
trigger times: 5
Loss after 355244 batches: 0.0886
trigger times: 6
Loss after 356207 batches: 0.0875
trigger times: 7
Loss after 357170 batches: 0.0864
trigger times: 8
Loss after 358133 batches: 0.0864
trigger times: 9
Loss after 359096 batches: 0.0863
trigger times: 10
Loss after 360059 batches: 0.0863
trigger times: 11
Loss after 361022 batches: 0.0863
trigger times: 12
Loss after 361985 batches: 0.0862
trigger times: 13
Loss after 362948 batches: 0.0860
trigger times: 14
Loss after 363911 batches: 0.0857
trigger times: 15
Loss after 364874 batches: 0.0859
trigger times: 16
Loss after 365837 batches: 0.0858
trigger times: 17
Loss after 366800 batches: 0.0856
trigger times: 18
Loss after 367763 batches: 0.0856
trigger times: 19
Loss after 368726 batches: 0.0857
trigger times: 20
Loss after 369689 batches: 0.0855
trigger times: 21
Loss after 370652 batches: 0.0856
trigger times: 22
Loss after 371615 batches: 0.0853
trigger times: 23
Loss after 372578 batches: 0.0854
trigger times: 24
Loss after 373541 batches: 0.0855
trigger times: 25
Early stopping!
Start to test process.
Loss after 374504 batches: 0.0850
Time to train on one home:  46.52479648590088
trigger times: 0
Loss after 375467 batches: 0.3810
trigger times: 1
Loss after 376430 batches: 0.3339
trigger times: 2
Loss after 377393 batches: 0.1816
trigger times: 3
Loss after 378356 batches: 0.0852
trigger times: 4
Loss after 379319 batches: 0.0744
trigger times: 5
Loss after 380282 batches: 0.0630
trigger times: 6
Loss after 381245 batches: 0.0593
trigger times: 7
Loss after 382208 batches: 0.0575
trigger times: 8
Loss after 383171 batches: 0.0565
trigger times: 9
Loss after 384134 batches: 0.0558
trigger times: 10
Loss after 385097 batches: 0.0558
trigger times: 11
Loss after 386060 batches: 0.0551
trigger times: 12
Loss after 387023 batches: 0.0548
trigger times: 13
Loss after 387986 batches: 0.0548
trigger times: 14
Loss after 388949 batches: 0.0548
trigger times: 15
Loss after 389912 batches: 0.0546
trigger times: 16
Loss after 390875 batches: 0.0544
trigger times: 17
Loss after 391838 batches: 0.0541
trigger times: 18
Loss after 392801 batches: 0.0540
trigger times: 19
Loss after 393764 batches: 0.0540
trigger times: 20
Loss after 394727 batches: 0.0537
trigger times: 21
Loss after 395690 batches: 0.0540
trigger times: 22
Loss after 396653 batches: 0.0538
trigger times: 23
Loss after 397616 batches: 0.0538
trigger times: 24
Loss after 398579 batches: 0.0539
trigger times: 25
Early stopping!
Start to test process.
Loss after 399542 batches: 0.0538
Time to train on one home:  46.29866433143616
trigger times: 0
Loss after 400505 batches: 0.2650
trigger times: 1
Loss after 401468 batches: 0.2339
trigger times: 2
Loss after 402431 batches: 0.1610
trigger times: 3
Loss after 403394 batches: 0.1436
trigger times: 4
Loss after 404357 batches: 0.1381
trigger times: 5
Loss after 405320 batches: 0.1335
trigger times: 6
Loss after 406283 batches: 0.1322
trigger times: 7
Loss after 407246 batches: 0.1324
trigger times: 8
Loss after 408209 batches: 0.1318
trigger times: 9
Loss after 409172 batches: 0.1318
trigger times: 10
Loss after 410135 batches: 0.1314
trigger times: 11
Loss after 411098 batches: 0.1317
trigger times: 12
Loss after 412061 batches: 0.1315
trigger times: 13
Loss after 413024 batches: 0.1311
trigger times: 14
Loss after 413987 batches: 0.1309
trigger times: 15
Loss after 414950 batches: 0.1310
trigger times: 16
Loss after 415913 batches: 0.1314
trigger times: 17
Loss after 416876 batches: 0.1314
trigger times: 18
Loss after 417839 batches: 0.1311
trigger times: 19
Loss after 418802 batches: 0.1309
trigger times: 20
Loss after 419765 batches: 0.1308
trigger times: 21
Loss after 420728 batches: 0.1309
trigger times: 22
Loss after 421691 batches: 0.1307
trigger times: 23
Loss after 422654 batches: 0.1308
trigger times: 24
Loss after 423617 batches: 0.1313
trigger times: 25
Early stopping!
Start to test process.
Loss after 424580 batches: 0.1306
Time to train on one home:  46.772496938705444
trigger times: 0
Loss after 425543 batches: 0.1985
trigger times: 1
Loss after 426506 batches: 0.1753
trigger times: 2
Loss after 427469 batches: 0.1330
trigger times: 3
Loss after 428432 batches: 0.1244
trigger times: 4
Loss after 429395 batches: 0.1230
trigger times: 5
Loss after 430358 batches: 0.1205
trigger times: 6
Loss after 431321 batches: 0.1197
trigger times: 7
Loss after 432284 batches: 0.1199
trigger times: 8
Loss after 433247 batches: 0.1197
trigger times: 9
Loss after 434210 batches: 0.1196
trigger times: 10
Loss after 435173 batches: 0.1194
trigger times: 11
Loss after 436136 batches: 0.1199
trigger times: 12
Loss after 437099 batches: 0.1194
trigger times: 13
Loss after 438062 batches: 0.1196
trigger times: 14
Loss after 439025 batches: 0.1198
trigger times: 15
Loss after 439988 batches: 0.1192
trigger times: 16
Loss after 440951 batches: 0.1196
trigger times: 17
Loss after 441914 batches: 0.1193
trigger times: 18
Loss after 442877 batches: 0.1193
trigger times: 19
Loss after 443840 batches: 0.1191
trigger times: 20
Loss after 444803 batches: 0.1188
trigger times: 21
Loss after 445766 batches: 0.1197
trigger times: 22
Loss after 446729 batches: 0.1193
trigger times: 23
Loss after 447692 batches: 0.1191
trigger times: 24
Loss after 448655 batches: 0.1196
trigger times: 25
Early stopping!
Start to test process.
Loss after 449618 batches: 0.1194
Time to train on one home:  46.35740852355957
trigger times: 0
Loss after 450547 batches: 0.2039
trigger times: 1
Loss after 451476 batches: 0.1843
trigger times: 2
Loss after 452405 batches: 0.1457
trigger times: 3
Loss after 453334 batches: 0.1378
trigger times: 4
Loss after 454263 batches: 0.1390
trigger times: 5
Loss after 455192 batches: 0.1359
trigger times: 6
Loss after 456121 batches: 0.1340
trigger times: 7
Loss after 457050 batches: 0.1369
trigger times: 8
Loss after 457979 batches: 0.1368
trigger times: 9
Loss after 458908 batches: 0.1366
trigger times: 10
Loss after 459837 batches: 0.1351
trigger times: 11
Loss after 460766 batches: 0.1348
trigger times: 12
Loss after 461695 batches: 0.1356
trigger times: 13
Loss after 462624 batches: 0.1363
trigger times: 14
Loss after 463553 batches: 0.1355
trigger times: 15
Loss after 464482 batches: 0.1344
trigger times: 16
Loss after 465411 batches: 0.1131
trigger times: 17
Loss after 466340 batches: 0.0947
trigger times: 18
Loss after 467269 batches: 0.0844
trigger times: 19
Loss after 468198 batches: 0.0757
trigger times: 20
Loss after 469127 batches: 0.0683
trigger times: 21
Loss after 470056 batches: 0.0659
trigger times: 22
Loss after 470985 batches: 0.0632
trigger times: 23
Loss after 471914 batches: 0.0637
trigger times: 24
Loss after 472843 batches: 0.0608
trigger times: 25
Early stopping!
Start to test process.
Loss after 473772 batches: 0.0587
Time to train on one home:  46.81872200965881
trigger times: 0
Loss after 474735 batches: 0.3452
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 475698 batches: 0.2982
trigger times: 2
Loss after 476661 batches: 0.1470
trigger times: 3
Loss after 477624 batches: 0.0610
trigger times: 4
Loss after 478587 batches: 0.0494
trigger times: 5
Loss after 479550 batches: 0.0371
trigger times: 6
Loss after 480513 batches: 0.0334
trigger times: 7
Loss after 481476 batches: 0.0324
trigger times: 8
Loss after 482439 batches: 0.0313
trigger times: 9
Loss after 483402 batches: 0.0309
trigger times: 10
Loss after 484365 batches: 0.0305
trigger times: 11
Loss after 485328 batches: 0.0302
trigger times: 12
Loss after 486291 batches: 0.0299
trigger times: 13
Loss after 487254 batches: 0.0297
trigger times: 14
Loss after 488217 batches: 0.0296
trigger times: 15
Loss after 489180 batches: 0.0293
trigger times: 16
Loss after 490143 batches: 0.0292
trigger times: 17
Loss after 491106 batches: 0.0291
trigger times: 18
Loss after 492069 batches: 0.0290
trigger times: 19
Loss after 493032 batches: 0.0289
trigger times: 20
Loss after 493995 batches: 0.0288
trigger times: 21
Loss after 494958 batches: 0.0287
trigger times: 22
Loss after 495921 batches: 0.0287
trigger times: 23
Loss after 496884 batches: 0.0284
trigger times: 24
Loss after 497847 batches: 0.0284
trigger times: 25
Early stopping!
Start to test process.
Loss after 498810 batches: 0.0282
Time to train on one home:  47.444916009902954
trigger times: 0
Loss after 499773 batches: 0.5364
trigger times: 1
Loss after 500736 batches: 0.4785
trigger times: 2
Loss after 501699 batches: 0.2945
trigger times: 3
Loss after 502662 batches: 0.1570
trigger times: 4
Loss after 503625 batches: 0.1405
trigger times: 5
Loss after 504588 batches: 0.1283
trigger times: 6
Loss after 505551 batches: 0.1256
trigger times: 7
Loss after 506514 batches: 0.1224
trigger times: 8
Loss after 507477 batches: 0.1218
trigger times: 9
Loss after 508440 batches: 0.1200
trigger times: 10
Loss after 509403 batches: 0.1207
trigger times: 11
Loss after 510366 batches: 0.1200
trigger times: 12
Loss after 511329 batches: 0.1197
trigger times: 13
Loss after 512292 batches: 0.1194
trigger times: 14
Loss after 513255 batches: 0.1196
trigger times: 15
Loss after 514218 batches: 0.1191
trigger times: 16
Loss after 515181 batches: 0.1187
trigger times: 17
Loss after 516144 batches: 0.1186
trigger times: 18
Loss after 517107 batches: 0.1187
trigger times: 19
Loss after 518070 batches: 0.1187
trigger times: 20
Loss after 519033 batches: 0.1185
trigger times: 21
Loss after 519996 batches: 0.1182
trigger times: 22
Loss after 520959 batches: 0.1182
trigger times: 23
Loss after 521922 batches: 0.1183
trigger times: 24
Loss after 522885 batches: 0.1182
trigger times: 25
Early stopping!
Start to test process.
Loss after 523848 batches: 0.1182
Time to train on one home:  49.647040605545044
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 524811 batches: 0.3636
trigger times: 1
Loss after 525774 batches: 0.3221
trigger times: 2
Loss after 526737 batches: 0.1967
trigger times: 3
Loss after 527700 batches: 0.1371
trigger times: 4
Loss after 528663 batches: 0.1263
trigger times: 5
Loss after 529626 batches: 0.1177
trigger times: 6
Loss after 530589 batches: 0.1153
trigger times: 7
Loss after 531552 batches: 0.1141
trigger times: 8
Loss after 532515 batches: 0.1138
trigger times: 9
Loss after 533478 batches: 0.1130
trigger times: 10
Loss after 534441 batches: 0.1129
trigger times: 11
Loss after 535404 batches: 0.1127
trigger times: 12
Loss after 536367 batches: 0.1125
trigger times: 13
Loss after 537330 batches: 0.1123
trigger times: 14
Loss after 538293 batches: 0.1124
trigger times: 15
Loss after 539256 batches: 0.1122
trigger times: 16
Loss after 540219 batches: 0.1119
trigger times: 17
Loss after 541182 batches: 0.1119
trigger times: 18
Loss after 542145 batches: 0.1120
trigger times: 19
Loss after 543108 batches: 0.1118
trigger times: 20
Loss after 544071 batches: 0.1118
trigger times: 21
Loss after 545034 batches: 0.1117
trigger times: 22
Loss after 545997 batches: 0.1118
trigger times: 23
Loss after 546960 batches: 0.1117
trigger times: 24
Loss after 547923 batches: 0.1116
trigger times: 25
Early stopping!
Start to test process.
Loss after 548886 batches: 0.1116
Time to train on one home:  47.207767724990845
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 549849 batches: 0.2608
trigger times: 1
Loss after 550812 batches: 0.2268
trigger times: 2
Loss after 551775 batches: 0.1314
trigger times: 3
Loss after 552738 batches: 0.1027
trigger times: 4
Loss after 553701 batches: 0.0955
trigger times: 5
Loss after 554664 batches: 0.0888
trigger times: 6
Loss after 555627 batches: 0.0862
trigger times: 7
Loss after 556590 batches: 0.0864
trigger times: 8
Loss after 557553 batches: 0.0859
trigger times: 9
Loss after 558516 batches: 0.0851
trigger times: 10
Loss after 559479 batches: 0.0849
trigger times: 11
Loss after 560442 batches: 0.0848
trigger times: 12
Loss after 561405 batches: 0.0848
trigger times: 13
Loss after 562368 batches: 0.0851
trigger times: 14
Loss after 563331 batches: 0.0847
trigger times: 15
Loss after 564294 batches: 0.0846
trigger times: 16
Loss after 565257 batches: 0.0844
trigger times: 17
Loss after 566220 batches: 0.0845
trigger times: 18
Loss after 567183 batches: 0.0846
trigger times: 19
Loss after 568146 batches: 0.0843
trigger times: 20
Loss after 569109 batches: 0.0842
trigger times: 21
Loss after 570072 batches: 0.0842
trigger times: 22
Loss after 571035 batches: 0.0843
trigger times: 23
Loss after 571998 batches: 0.0842
trigger times: 24
Loss after 572961 batches: 0.0842
trigger times: 25
Early stopping!
Start to test process.
Loss after 573924 batches: 0.0843
Time to train on one home:  46.24345779418945
trigger times: 0
Loss after 574887 batches: 0.2079
trigger times: 1
Loss after 575850 batches: 0.1750
trigger times: 2
Loss after 576813 batches: 0.0916
trigger times: 3
Loss after 577776 batches: 0.0696
trigger times: 4
Loss after 578739 batches: 0.0632
trigger times: 5
Loss after 579702 batches: 0.0570
trigger times: 6
Loss after 580665 batches: 0.0561
trigger times: 7
Loss after 581628 batches: 0.0555
trigger times: 8
Loss after 582591 batches: 0.0548
trigger times: 9
Loss after 583554 batches: 0.0547
trigger times: 10
Loss after 584517 batches: 0.0546
trigger times: 11
Loss after 585480 batches: 0.0544
trigger times: 12
Loss after 586443 batches: 0.0544
trigger times: 13
Loss after 587406 batches: 0.0541
trigger times: 14
Loss after 588369 batches: 0.0544
trigger times: 15
Loss after 589332 batches: 0.0541
trigger times: 16
Loss after 590295 batches: 0.0540
trigger times: 17
Loss after 591258 batches: 0.0538
trigger times: 18
Loss after 592221 batches: 0.0541
trigger times: 19
Loss after 593184 batches: 0.0541
trigger times: 20
Loss after 594147 batches: 0.0540
trigger times: 21
Loss after 595110 batches: 0.0541
trigger times: 22
Loss after 596073 batches: 0.0540
trigger times: 23
Loss after 597036 batches: 0.0540
trigger times: 24
Loss after 597999 batches: 0.0539
trigger times: 25
Early stopping!
Start to test process.
Loss after 598962 batches: 0.0538
Time to train on one home:  46.611908197402954
trigger times: 0
Loss after 599925 batches: 0.1932
trigger times: 1
Loss after 600888 batches: 0.1651
trigger times: 2
Loss after 601851 batches: 0.1011
trigger times: 3
Loss after 602814 batches: 0.0886
trigger times: 4
Loss after 603777 batches: 0.0840
trigger times: 5
Loss after 604740 batches: 0.0796
trigger times: 6
Loss after 605703 batches: 0.0790
trigger times: 7
Loss after 606666 batches: 0.0791
trigger times: 8
Loss after 607629 batches: 0.0787
trigger times: 9
Loss after 608592 batches: 0.0781
trigger times: 10
Loss after 609555 batches: 0.0780
trigger times: 11
Loss after 610518 batches: 0.0780
trigger times: 12
Loss after 611481 batches: 0.0777
trigger times: 13
Loss after 612444 batches: 0.0779
trigger times: 14
Loss after 613407 batches: 0.0776
trigger times: 15
Loss after 614370 batches: 0.0778
trigger times: 16
Loss after 615333 batches: 0.0782
trigger times: 17
Loss after 616296 batches: 0.0775
trigger times: 18
Loss after 617259 batches: 0.0775
trigger times: 19
Loss after 618222 batches: 0.0778
trigger times: 20
Loss after 619185 batches: 0.0779
trigger times: 21
Loss after 620148 batches: 0.0777
trigger times: 22
Loss after 621111 batches: 0.0777
trigger times: 23
Loss after 622074 batches: 0.0775
trigger times: 24
Loss after 623037 batches: 0.0775
trigger times: 25
Early stopping!
Start to test process.
Loss after 624000 batches: 0.0775
Time to train on one home:  46.02103233337402
trigger times: 0
Loss after 624963 batches: 0.3608
trigger times: 1
Loss after 625926 batches: 0.3247
trigger times: 2
Loss after 626889 batches: 0.2160
trigger times: 3
Loss after 627852 batches: 0.1739
trigger times: 4
Loss after 628815 batches: 0.1650
trigger times: 5
Loss after 629778 batches: 0.1583
trigger times: 6
Loss after 630741 batches: 0.1571
trigger times: 7
Loss after 631704 batches: 0.1563
trigger times: 8
Loss after 632667 batches: 0.1555
trigger times: 9
Loss after 633630 batches: 0.1556
trigger times: 10
Loss after 634593 batches: 0.1551
trigger times: 11
Loss after 635556 batches: 0.1550
trigger times: 12
Loss after 636519 batches: 0.1550
trigger times: 13
Loss after 637482 batches: 0.1546
trigger times: 14
Loss after 638445 batches: 0.1547
trigger times: 15
Loss after 639408 batches: 0.1549
trigger times: 16
Loss after 640371 batches: 0.1548
trigger times: 17
Loss after 641334 batches: 0.1547
trigger times: 18
Loss after 642297 batches: 0.1544
trigger times: 19
Loss after 643260 batches: 0.1546
trigger times: 20
Loss after 644223 batches: 0.1542
trigger times: 21
Loss after 645186 batches: 0.1543
trigger times: 22
Loss after 646149 batches: 0.1544
trigger times: 23
Loss after 647112 batches: 0.1542
trigger times: 24
Loss after 648075 batches: 0.1540
trigger times: 25
Early stopping!
Start to test process.
Loss after 649038 batches: 0.1540
Time to train on one home:  46.38992714881897
trigger times: 0
Loss after 650001 batches: 0.1977
trigger times: 1
Loss after 650964 batches: 0.1749
trigger times: 2
Loss after 651927 batches: 0.1329
trigger times: 3
Loss after 652890 batches: 0.1248
trigger times: 4
Loss after 653853 batches: 0.1229
trigger times: 5
Loss after 654816 batches: 0.1204
trigger times: 6
Loss after 655779 batches: 0.1197
trigger times: 7
Loss after 656742 batches: 0.1198
trigger times: 8
Loss after 657705 batches: 0.1196
trigger times: 9
Loss after 658668 batches: 0.1192
trigger times: 10
Loss after 659631 batches: 0.1194
trigger times: 11
Loss after 660594 batches: 0.1192
trigger times: 12
Loss after 661557 batches: 0.1194
trigger times: 13
Loss after 662520 batches: 0.1195
trigger times: 14
Loss after 663483 batches: 0.1194
trigger times: 15
Loss after 664446 batches: 0.1194
trigger times: 16
Loss after 665409 batches: 0.1196
trigger times: 17
Loss after 666372 batches: 0.1191
trigger times: 18
Loss after 667335 batches: 0.1195
trigger times: 19
Loss after 668298 batches: 0.1199
trigger times: 20
Loss after 669261 batches: 0.1194
trigger times: 21
Loss after 670224 batches: 0.1191
trigger times: 22
Loss after 671187 batches: 0.1193
trigger times: 23
Loss after 672150 batches: 0.1193
trigger times: 24
Loss after 673113 batches: 0.1193
trigger times: 25
Early stopping!
Start to test process.
Loss after 674076 batches: 0.1191
Time to train on one home:  47.36101412773132
trigger times: 0
Loss after 675039 batches: 0.1848
trigger times: 1
Loss after 676002 batches: 0.1605
trigger times: 2
Loss after 676965 batches: 0.1126
trigger times: 3
Loss after 677928 batches: 0.1019
trigger times: 4
Loss after 678891 batches: 0.0993
trigger times: 5
Loss after 679854 batches: 0.0970
trigger times: 6
Loss after 680817 batches: 0.0960
trigger times: 7
Loss after 681780 batches: 0.0960
trigger times: 8
Loss after 682743 batches: 0.0959
trigger times: 9
Loss after 683706 batches: 0.0959
trigger times: 10
Loss after 684669 batches: 0.0952
trigger times: 11
Loss after 685632 batches: 0.0946
trigger times: 12
Loss after 686595 batches: 0.0952
trigger times: 13
Loss after 687558 batches: 0.0955
trigger times: 14
Loss after 688521 batches: 0.0953
trigger times: 15
Loss after 689484 batches: 0.0953
trigger times: 16
Loss after 690447 batches: 0.0953
trigger times: 17
Loss after 691410 batches: 0.0946
trigger times: 18
Loss after 692373 batches: 0.0954
trigger times: 19
Loss after 693336 batches: 0.0952
trigger times: 20
Loss after 694299 batches: 0.0954
trigger times: 21
Loss after 695262 batches: 0.0948
trigger times: 22
Loss after 696225 batches: 0.0958
trigger times: 23
Loss after 697188 batches: 0.0956
trigger times: 24
Loss after 698151 batches: 0.0957
trigger times: 25
Early stopping!
Start to test process.
Loss after 699114 batches: 0.0948
Time to train on one home:  48.538713455200195
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 700077 batches: 0.1353
trigger times: 1
Loss after 701040 batches: 0.1169
trigger times: 2
Loss after 702003 batches: 0.0927
trigger times: 3
Loss after 702966 batches: 0.0885
trigger times: 4
Loss after 703929 batches: 0.0884
trigger times: 5
Loss after 704892 batches: 0.0858
trigger times: 6
Loss after 705855 batches: 0.0862
trigger times: 7
Loss after 706818 batches: 0.0858
trigger times: 8
Loss after 707781 batches: 0.0854
trigger times: 9
Loss after 708744 batches: 0.0860
trigger times: 10
Loss after 709707 batches: 0.0853
trigger times: 11
Loss after 710670 batches: 0.0855
trigger times: 12
Loss after 711633 batches: 0.0857
trigger times: 13
Loss after 712596 batches: 0.0855
trigger times: 14
Loss after 713559 batches: 0.0855
trigger times: 15
Loss after 714522 batches: 0.0854
trigger times: 16
Loss after 715485 batches: 0.0858
trigger times: 17
Loss after 716448 batches: 0.0860
trigger times: 18
Loss after 717411 batches: 0.0855
trigger times: 19
Loss after 718374 batches: 0.0857
trigger times: 20
Loss after 719337 batches: 0.0858
trigger times: 21
Loss after 720300 batches: 0.0859
trigger times: 22
Loss after 721263 batches: 0.0856
trigger times: 23
Loss after 722226 batches: 0.0854
trigger times: 24
Loss after 723189 batches: 0.0859
trigger times: 25
Early stopping!
Start to test process.
Loss after 724152 batches: 0.0855
Time to train on one home:  49.36190629005432
trigger times: 0
Loss after 725115 batches: 0.3380
trigger times: 1
Loss after 726078 batches: 0.2948
trigger times: 2
Loss after 727041 batches: 0.1689
trigger times: 3
Loss after 728004 batches: 0.1126
trigger times: 4
Loss after 728967 batches: 0.1019
trigger times: 5
Loss after 729930 batches: 0.0922
trigger times: 6
Loss after 730893 batches: 0.0899
trigger times: 7
Loss after 731856 batches: 0.0886
trigger times: 8
Loss after 732819 batches: 0.0881
trigger times: 9
Loss after 733782 batches: 0.0880
trigger times: 10
Loss after 734745 batches: 0.0876
trigger times: 11
Loss after 735708 batches: 0.0872
trigger times: 12
Loss after 736671 batches: 0.0872
trigger times: 13
Loss after 737634 batches: 0.0868
trigger times: 14
Loss after 738597 batches: 0.0869
trigger times: 15
Loss after 739560 batches: 0.0865
trigger times: 16
Loss after 740523 batches: 0.0865
trigger times: 17
Loss after 741486 batches: 0.0864
trigger times: 18
Loss after 742449 batches: 0.0865
trigger times: 19
Loss after 743412 batches: 0.0864
trigger times: 20
Loss after 744375 batches: 0.0863
trigger times: 21
Loss after 745338 batches: 0.0861
trigger times: 22
Loss after 746301 batches: 0.0860
trigger times: 23
Loss after 747264 batches: 0.0862
trigger times: 24
Loss after 748227 batches: 0.0863
trigger times: 25
Early stopping!
Start to test process.
Loss after 749190 batches: 0.0861
Time to train on one home:  46.61400818824768
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 750153 batches: 0.1341
trigger times: 1
Loss after 751116 batches: 0.1172
trigger times: 2
Loss after 752079 batches: 0.0924
trigger times: 3
Loss after 753042 batches: 0.0877
trigger times: 4
Loss after 754005 batches: 0.0873
trigger times: 5
Loss after 754968 batches: 0.0860
trigger times: 6
Loss after 755931 batches: 0.0864
trigger times: 7
Loss after 756894 batches: 0.0857
trigger times: 8
Loss after 757857 batches: 0.0858
trigger times: 9
Loss after 758820 batches: 0.0856
trigger times: 10
Loss after 759783 batches: 0.0853
trigger times: 11
Loss after 760746 batches: 0.0856
trigger times: 12
Loss after 761709 batches: 0.0857
trigger times: 13
Loss after 762672 batches: 0.0856
trigger times: 14
Loss after 763635 batches: 0.0856
trigger times: 15
Loss after 764598 batches: 0.0855
trigger times: 16
Loss after 765561 batches: 0.0856
trigger times: 17
Loss after 766524 batches: 0.0853
trigger times: 18
Loss after 767487 batches: 0.0860
trigger times: 19
Loss after 768450 batches: 0.0863
trigger times: 20
Loss after 769413 batches: 0.0855
trigger times: 21
Loss after 770376 batches: 0.0855
trigger times: 22
Loss after 771339 batches: 0.0857
trigger times: 23
Loss after 772302 batches: 0.0854
trigger times: 24
Loss after 773265 batches: 0.0854
trigger times: 25
Early stopping!
Start to test process.
Loss after 774228 batches: 0.0857
Time to train on one home:  46.60490942001343
trigger times: 0
Loss after 775123 batches: 0.1890
trigger times: 1
Loss after 776018 batches: 0.1703
trigger times: 2
Loss after 776913 batches: 0.1262
trigger times: 3
Loss after 777808 batches: 0.0958
trigger times: 4
Loss after 778703 batches: 0.0881
trigger times: 5
Loss after 779598 batches: 0.0886
trigger times: 6
Loss after 780493 batches: 0.0865
trigger times: 7
Loss after 781388 batches: 0.0853
trigger times: 8
Loss after 782283 batches: 0.0854
trigger times: 9
Loss after 783178 batches: 0.0850
trigger times: 10
Loss after 784073 batches: 0.0846
trigger times: 11
Loss after 784968 batches: 0.0846
trigger times: 12
Loss after 785863 batches: 0.0844
trigger times: 13
Loss after 786758 batches: 0.0843
trigger times: 14
Loss after 787653 batches: 0.0844
trigger times: 15
Loss after 788548 batches: 0.0848
trigger times: 16
Loss after 789443 batches: 0.0846
trigger times: 17
Loss after 790338 batches: 0.0845
trigger times: 18
Loss after 791233 batches: 0.0841
trigger times: 19
Loss after 792128 batches: 0.0841
trigger times: 20
Loss after 793023 batches: 0.0839
trigger times: 21
Loss after 793918 batches: 0.0818
trigger times: 22
Loss after 794813 batches: 0.0562
trigger times: 23
Loss after 795708 batches: 0.0333
trigger times: 24
Loss after 796603 batches: 0.0277
trigger times: 25
Early stopping!
Start to test process.
Loss after 797498 batches: 0.0230
Time to train on one home:  47.01255917549133
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 798435 batches: 0.2143
trigger times: 1
Loss after 799372 batches: 0.1844
trigger times: 2
Loss after 800309 batches: 0.1177
trigger times: 3
Loss after 801246 batches: 0.1075
trigger times: 4
Loss after 802183 batches: 0.1004
trigger times: 5
Loss after 803120 batches: 0.0966
trigger times: 6
Loss after 804057 batches: 0.0948
trigger times: 7
Loss after 804994 batches: 0.0956
trigger times: 8
Loss after 805931 batches: 0.0949
trigger times: 9
Loss after 806868 batches: 0.0949
trigger times: 10
Loss after 807805 batches: 0.0947
trigger times: 11
Loss after 808742 batches: 0.0947
trigger times: 12
Loss after 809679 batches: 0.0947
trigger times: 13
Loss after 810616 batches: 0.0943
trigger times: 14
Loss after 811553 batches: 0.0943
trigger times: 15
Loss after 812490 batches: 0.0942
trigger times: 16
Loss after 813427 batches: 0.0943
trigger times: 17
Loss after 814364 batches: 0.0943
trigger times: 18
Loss after 815301 batches: 0.0949
trigger times: 19
Loss after 816238 batches: 0.0943
trigger times: 20
Loss after 817175 batches: 0.0946
trigger times: 21
Loss after 818112 batches: 0.0942
trigger times: 22
Loss after 819049 batches: 0.0938
trigger times: 23
Loss after 819986 batches: 0.0946
trigger times: 24
Loss after 820923 batches: 0.0943
trigger times: 25
Early stopping!
Start to test process.
Loss after 821860 batches: 0.0939
Time to train on one home:  50.95118427276611
train_results:  [0.08213193090377217]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355]]
Round_0_results:  [0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 859 < 860; dropping {'Training_Loss': 0.08526041784456798, 'Validation_Loss': 0.10636470466852188, 'Training_R2': -0.43879324557169785, 'Validation_R2': -0.04748230832089706, 'Training_F1': 0.19080162479099125, 'Validation_F1': 0.14475076536653153, 'Training_NEP': 0.9494058561714036, 'Validation_NEP': 1.140164915035997, 'Training_NDE': 0.8762599766982312, 'Validation_NDE': 0.9463753723939377, 'Training_MAE': 36.07772799624023, 'Validation_MAE': 40.54343631038365, 'Training_MSE': 3236.367, 'Validation_MSE': 12397.528}.
trigger times: 0
Loss after 822822 batches: 0.0853
trigger times: 1
Loss after 823784 batches: 0.0785
trigger times: 2
Loss after 824746 batches: 0.0798
trigger times: 3
Loss after 825708 batches: 0.0780
trigger times: 4
Loss after 826670 batches: 0.0772
trigger times: 5
Loss after 827632 batches: 0.0771
trigger times: 6
Loss after 828594 batches: 0.0771
trigger times: 7
Loss after 829556 batches: 0.0767
trigger times: 8
Loss after 830518 batches: 0.0770
trigger times: 9
Loss after 831480 batches: 0.0767
trigger times: 10
Loss after 832442 batches: 0.0765
trigger times: 11
Loss after 833404 batches: 0.0767
trigger times: 12
Loss after 834366 batches: 0.0763
trigger times: 13
Loss after 835328 batches: 0.0766
trigger times: 14
Loss after 836290 batches: 0.0765
trigger times: 15
Loss after 837252 batches: 0.0768
trigger times: 16
Loss after 838214 batches: 0.0764
trigger times: 17
Loss after 839176 batches: 0.0760
trigger times: 18
Loss after 840138 batches: 0.0767
trigger times: 19
Loss after 841100 batches: 0.0753
trigger times: 20
Loss after 842062 batches: 0.0734
trigger times: 21
Loss after 843024 batches: 0.0723
trigger times: 22
Loss after 843986 batches: 0.0712
trigger times: 23
Loss after 844948 batches: 0.0702
trigger times: 24
Loss after 845910 batches: 0.0696
trigger times: 25
Early stopping!
Start to test process.
Loss after 846872 batches: 0.0691
Time to train on one home:  47.61095070838928
trigger times: 0
Loss after 847801 batches: 0.1412
trigger times: 0
Loss after 848730 batches: 0.1373
trigger times: 0
Loss after 849659 batches: 0.1357
trigger times: 1
Loss after 850588 batches: 0.1357
trigger times: 2
Loss after 851517 batches: 0.1360
trigger times: 3
Loss after 852446 batches: 0.1362
trigger times: 4
Loss after 853375 batches: 0.1360
trigger times: 5
Loss after 854304 batches: 0.1355
trigger times: 6
Loss after 855233 batches: 0.1349
trigger times: 7
Loss after 856162 batches: 0.1354
trigger times: 8
Loss after 857091 batches: 0.1359
trigger times: 9
Loss after 858020 batches: 0.1350
trigger times: 10
Loss after 858949 batches: 0.1369
trigger times: 11
Loss after 859878 batches: 0.1336
trigger times: 12
Loss after 860807 batches: 0.1368
trigger times: 13
Loss after 861736 batches: 0.1339
trigger times: 14
Loss after 862665 batches: 0.1362
trigger times: 15
Loss after 863594 batches: 0.1363
trigger times: 16
Loss after 864523 batches: 0.1347
trigger times: 17
Loss after 865452 batches: 0.1340
trigger times: 18
Loss after 866381 batches: 0.1331
trigger times: 19
Loss after 867310 batches: 0.1131
trigger times: 20
Loss after 868239 batches: 0.0821
trigger times: 21
Loss after 869168 batches: 0.0711
trigger times: 22
Loss after 870097 batches: 0.0677
trigger times: 23
Loss after 871026 batches: 0.0645
trigger times: 24
Loss after 871955 batches: 0.0626
trigger times: 25
Early stopping!
Start to test process.
Loss after 872884 batches: 0.0609
Time to train on one home:  54.10022568702698
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 873847 batches: 0.0639
trigger times: 1
Loss after 874810 batches: 0.0310
trigger times: 2
Loss after 875773 batches: 0.0270
trigger times: 3
Loss after 876736 batches: 0.0187
trigger times: 4
Loss after 877699 batches: 0.0179
trigger times: 5
Loss after 878662 batches: 0.0170
trigger times: 6
Loss after 879625 batches: 0.0159
trigger times: 7
Loss after 880588 batches: 0.0154
trigger times: 8
Loss after 881551 batches: 0.0151
trigger times: 9
Loss after 882514 batches: 0.0151
trigger times: 10
Loss after 883477 batches: 0.0148
trigger times: 11
Loss after 884440 batches: 0.0148
trigger times: 12
Loss after 885403 batches: 0.0148
trigger times: 13
Loss after 886366 batches: 0.0147
trigger times: 14
Loss after 887329 batches: 0.0145
trigger times: 15
Loss after 888292 batches: 0.0144
trigger times: 16
Loss after 889255 batches: 0.0145
trigger times: 17
Loss after 890218 batches: 0.0146
trigger times: 18
Loss after 891181 batches: 0.0144
trigger times: 19
Loss after 892144 batches: 0.0144
trigger times: 20
Loss after 893107 batches: 0.0144
trigger times: 21
Loss after 894070 batches: 0.0142
trigger times: 22
Loss after 895033 batches: 0.0144
trigger times: 23
Loss after 895996 batches: 0.0144
trigger times: 24
Loss after 896959 batches: 0.0144
trigger times: 25
Early stopping!
Start to test process.
Loss after 897922 batches: 0.0142
Time to train on one home:  50.94076657295227
trigger times: 0
Loss after 898885 batches: 0.0558
trigger times: 1
Loss after 899848 batches: 0.0543
trigger times: 2
Loss after 900811 batches: 0.0546
trigger times: 3
Loss after 901774 batches: 0.0542
trigger times: 4
Loss after 902737 batches: 0.0539
trigger times: 5
Loss after 903700 batches: 0.0536
trigger times: 6
Loss after 904663 batches: 0.0537
trigger times: 7
Loss after 905626 batches: 0.0538
trigger times: 8
Loss after 906589 batches: 0.0537
trigger times: 9
Loss after 907552 batches: 0.0536
trigger times: 10
Loss after 908515 batches: 0.0537
trigger times: 11
Loss after 909478 batches: 0.0535
trigger times: 12
Loss after 910441 batches: 0.0533
trigger times: 13
Loss after 911404 batches: 0.0519
trigger times: 14
Loss after 912367 batches: 0.0501
trigger times: 15
Loss after 913330 batches: 0.0475
trigger times: 16
Loss after 914293 batches: 0.0463
trigger times: 17
Loss after 915256 batches: 0.0457
trigger times: 18
Loss after 916219 batches: 0.0449
trigger times: 0
Loss after 917182 batches: 0.0442
trigger times: 1
Loss after 918145 batches: 0.0445
trigger times: 0
Loss after 919108 batches: 0.0440
trigger times: 1
Loss after 920071 batches: 0.0409
trigger times: 2
Loss after 921034 batches: 0.0384
trigger times: 3
Loss after 921997 batches: 0.0370
trigger times: 4
Loss after 922960 batches: 0.0357
trigger times: 0
Loss after 923923 batches: 0.0345
trigger times: 1
Loss after 924886 batches: 0.0344
trigger times: 2
Loss after 925849 batches: 0.0336
trigger times: 3
Loss after 926812 batches: 0.0327
trigger times: 4
Loss after 927775 batches: 0.0323
trigger times: 5
Loss after 928738 batches: 0.0317
trigger times: 0
Loss after 929701 batches: 0.0318
trigger times: 0
Loss after 930664 batches: 0.0317
trigger times: 0
Loss after 931627 batches: 0.0310
trigger times: 1
Loss after 932590 batches: 0.0301
trigger times: 2
Loss after 933553 batches: 0.0297
trigger times: 0
Loss after 934516 batches: 0.0299
trigger times: 1
Loss after 935479 batches: 0.0292
trigger times: 0
Loss after 936442 batches: 0.0290
trigger times: 0
Loss after 937405 batches: 0.0289
trigger times: 0
Loss after 938368 batches: 0.0293
trigger times: 1
Loss after 939331 batches: 0.0279
trigger times: 2
Loss after 940294 batches: 0.0279
trigger times: 0
Loss after 941257 batches: 0.0284
trigger times: 0
Loss after 942220 batches: 0.0279
trigger times: 0
Loss after 943183 batches: 0.0275
trigger times: 1
Loss after 944146 batches: 0.0269
trigger times: 2
Loss after 945109 batches: 0.0270
trigger times: 3
Loss after 946072 batches: 0.0263
trigger times: 4
Loss after 947035 batches: 0.0268
trigger times: 5
Loss after 947998 batches: 0.0255
trigger times: 6
Loss after 948961 batches: 0.0252
trigger times: 7
Loss after 949924 batches: 0.0255
trigger times: 8
Loss after 950887 batches: 0.0250
trigger times: 0
Loss after 951850 batches: 0.0251
trigger times: 0
Loss after 952813 batches: 0.0241
trigger times: 1
Loss after 953776 batches: 0.0246
trigger times: 2
Loss after 954739 batches: 0.0241
trigger times: 3
Loss after 955702 batches: 0.0239
trigger times: 4
Loss after 956665 batches: 0.0233
trigger times: 5
Loss after 957628 batches: 0.0231
trigger times: 0
Loss after 958591 batches: 0.0227
trigger times: 1
Loss after 959554 batches: 0.0229
trigger times: 2
Loss after 960517 batches: 0.0226
trigger times: 3
Loss after 961480 batches: 0.0231
trigger times: 4
Loss after 962443 batches: 0.0227
trigger times: 5
Loss after 963406 batches: 0.0222
trigger times: 6
Loss after 964369 batches: 0.0224
trigger times: 0
Loss after 965332 batches: 0.0223
trigger times: 1
Loss after 966295 batches: 0.0221
trigger times: 2
Loss after 967258 batches: 0.0221
trigger times: 3
Loss after 968221 batches: 0.0221
trigger times: 4
Loss after 969184 batches: 0.0217
trigger times: 0
Loss after 970147 batches: 0.0214
trigger times: 1
Loss after 971110 batches: 0.0212
trigger times: 2
Loss after 972073 batches: 0.0207
trigger times: 3
Loss after 973036 batches: 0.0206
trigger times: 0
Loss after 973999 batches: 0.0212
trigger times: 1
Loss after 974962 batches: 0.0216
trigger times: 2
Loss after 975925 batches: 0.0210
trigger times: 3
Loss after 976888 batches: 0.0206
trigger times: 4
Loss after 977851 batches: 0.0202
trigger times: 5
Loss after 978814 batches: 0.0204
trigger times: 6
Loss after 979777 batches: 0.0204
trigger times: 7
Loss after 980740 batches: 0.0200
trigger times: 8
Loss after 981703 batches: 0.0207
trigger times: 9
Loss after 982666 batches: 0.0197
trigger times: 0
Loss after 983629 batches: 0.0199
trigger times: 1
Loss after 984592 batches: 0.0195
trigger times: 2
Loss after 985555 batches: 0.0196
trigger times: 3
Loss after 986518 batches: 0.0193
trigger times: 4
Loss after 987481 batches: 0.0197
trigger times: 5
Loss after 988444 batches: 0.0189
trigger times: 6
Loss after 989407 batches: 0.0189
trigger times: 7
Loss after 990370 batches: 0.0187
trigger times: 8
Loss after 991333 batches: 0.0187
trigger times: 9
Loss after 992296 batches: 0.0184
trigger times: 10
Loss after 993259 batches: 0.0195
trigger times: 11
Loss after 994222 batches: 0.0192
trigger times: 12
Loss after 995185 batches: 0.0186
trigger times: 13
Loss after 996148 batches: 0.0183
trigger times: 14
Loss after 997111 batches: 0.0186
trigger times: 15
Loss after 998074 batches: 0.0183
trigger times: 16
Loss after 999037 batches: 0.0182
trigger times: 17
Loss after 1000000 batches: 0.0183
trigger times: 18
Loss after 1000963 batches: 0.0182
trigger times: 19
Loss after 1001926 batches: 0.0178
trigger times: 20
Loss after 1002889 batches: 0.0187
trigger times: 21
Loss after 1003852 batches: 0.0177
trigger times: 0
Loss after 1004815 batches: 0.0181
trigger times: 1
Loss after 1005778 batches: 0.0174
trigger times: 2
Loss after 1006741 batches: 0.0178
trigger times: 0
Loss after 1007704 batches: 0.0182
trigger times: 1
Loss after 1008667 batches: 0.0175
trigger times: 2
Loss after 1009630 batches: 0.0176
trigger times: 3
Loss after 1010593 batches: 0.0176
trigger times: 4
Loss after 1011556 batches: 0.0173
trigger times: 5
Loss after 1012519 batches: 0.0171
trigger times: 6
Loss after 1013482 batches: 0.0169
trigger times: 7
Loss after 1014445 batches: 0.0167
trigger times: 8
Loss after 1015408 batches: 0.0168
trigger times: 9
Loss after 1016371 batches: 0.0169
trigger times: 10
Loss after 1017334 batches: 0.0170
trigger times: 11
Loss after 1018297 batches: 0.0164
trigger times: 12
Loss after 1019260 batches: 0.0168
trigger times: 13
Loss after 1020223 batches: 0.0168
trigger times: 14
Loss after 1021186 batches: 0.0170
trigger times: 15
Loss after 1022149 batches: 0.0170
trigger times: 16
Loss after 1023112 batches: 0.0166
trigger times: 17
Loss after 1024075 batches: 0.0162
trigger times: 18
Loss after 1025038 batches: 0.0162
trigger times: 19
Loss after 1026001 batches: 0.0162
trigger times: 20
Loss after 1026964 batches: 0.0161
trigger times: 21
Loss after 1027927 batches: 0.0161
trigger times: 22
Loss after 1028890 batches: 0.0158
trigger times: 23
Loss after 1029853 batches: 0.0160
trigger times: 24
Loss after 1030816 batches: 0.0168
trigger times: 25
Early stopping!
Start to test process.
Loss after 1031779 batches: 0.0161
Time to train on one home:  127.64948844909668
trigger times: 0
Loss after 1032742 batches: 0.1318
trigger times: 1
Loss after 1033705 batches: 0.1310
trigger times: 2
Loss after 1034668 batches: 0.1311
trigger times: 3
Loss after 1035631 batches: 0.1310
trigger times: 4
Loss after 1036594 batches: 0.1309
trigger times: 5
Loss after 1037557 batches: 0.1305
trigger times: 6
Loss after 1038520 batches: 0.1308
trigger times: 7
Loss after 1039483 batches: 0.1298
trigger times: 0
Loss after 1040446 batches: 0.1248
trigger times: 1
Loss after 1041409 batches: 0.1215
trigger times: 0
Loss after 1042372 batches: 0.1210
trigger times: 1
Loss after 1043335 batches: 0.1179
trigger times: 0
Loss after 1044298 batches: 0.1130
trigger times: 1
Loss after 1045261 batches: 0.1104
trigger times: 2
Loss after 1046224 batches: 0.1084
trigger times: 3
Loss after 1047187 batches: 0.1071
trigger times: 4
Loss after 1048150 batches: 0.1054
trigger times: 5
Loss after 1049113 batches: 0.1028
trigger times: 6
Loss after 1050076 batches: 0.1015
trigger times: 7
Loss after 1051039 batches: 0.1013
trigger times: 8
Loss after 1052002 batches: 0.1002
trigger times: 9
Loss after 1052965 batches: 0.0984
trigger times: 10
Loss after 1053928 batches: 0.0971
trigger times: 11
Loss after 1054891 batches: 0.0964
trigger times: 0
Loss after 1055854 batches: 0.0957
trigger times: 1
Loss after 1056817 batches: 0.0966
trigger times: 2
Loss after 1057780 batches: 0.0961
trigger times: 0
Loss after 1058743 batches: 0.0945
trigger times: 1
Loss after 1059706 batches: 0.0934
trigger times: 2
Loss after 1060669 batches: 0.0935
trigger times: 0
Loss after 1061632 batches: 0.0927
trigger times: 1
Loss after 1062595 batches: 0.0907
trigger times: 2
Loss after 1063558 batches: 0.0908
trigger times: 0
Loss after 1064521 batches: 0.0896
trigger times: 1
Loss after 1065484 batches: 0.0886
trigger times: 2
Loss after 1066447 batches: 0.0875
trigger times: 3
Loss after 1067410 batches: 0.0868
trigger times: 0
Loss after 1068373 batches: 0.0860
trigger times: 1
Loss after 1069336 batches: 0.0883
trigger times: 2
Loss after 1070299 batches: 0.0860
trigger times: 3
Loss after 1071262 batches: 0.0840
trigger times: 4
Loss after 1072225 batches: 0.0836
trigger times: 5
Loss after 1073188 batches: 0.0845
trigger times: 6
Loss after 1074151 batches: 0.0824
trigger times: 7
Loss after 1075114 batches: 0.0820
trigger times: 8
Loss after 1076077 batches: 0.0841
trigger times: 9
Loss after 1077040 batches: 0.0836
trigger times: 10
Loss after 1078003 batches: 0.0818
trigger times: 11
Loss after 1078966 batches: 0.0805
trigger times: 12
Loss after 1079929 batches: 0.0806
trigger times: 13
Loss after 1080892 batches: 0.0791
trigger times: 14
Loss after 1081855 batches: 0.0795
trigger times: 15
Loss after 1082818 batches: 0.0800
trigger times: 16
Loss after 1083781 batches: 0.0785
trigger times: 17
Loss after 1084744 batches: 0.0780
trigger times: 18
Loss after 1085707 batches: 0.0780
trigger times: 19
Loss after 1086670 batches: 0.0771
trigger times: 20
Loss after 1087633 batches: 0.0757
trigger times: 21
Loss after 1088596 batches: 0.0777
trigger times: 22
Loss after 1089559 batches: 0.0758
trigger times: 23
Loss after 1090522 batches: 0.0766
trigger times: 24
Loss after 1091485 batches: 0.0776
trigger times: 25
Early stopping!
Start to test process.
Loss after 1092448 batches: 0.0762
Time to train on one home:  72.81161284446716
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 1093411 batches: 0.1354
trigger times: 1
Loss after 1094374 batches: 0.1173
trigger times: 2
Loss after 1095337 batches: 0.1189
trigger times: 3
Loss after 1096300 batches: 0.1144
trigger times: 4
Loss after 1097263 batches: 0.1135
trigger times: 5
Loss after 1098226 batches: 0.1127
trigger times: 6
Loss after 1099189 batches: 0.1123
trigger times: 7
Loss after 1100152 batches: 0.1118
trigger times: 8
Loss after 1101115 batches: 0.1114
trigger times: 9
Loss after 1102078 batches: 0.1115
trigger times: 10
Loss after 1103041 batches: 0.1110
trigger times: 11
Loss after 1104004 batches: 0.1113
trigger times: 12
Loss after 1104967 batches: 0.1115
trigger times: 13
Loss after 1105930 batches: 0.1110
trigger times: 14
Loss after 1106893 batches: 0.1113
trigger times: 15
Loss after 1107856 batches: 0.1113
trigger times: 16
Loss after 1108819 batches: 0.1114
trigger times: 17
Loss after 1109782 batches: 0.1114
trigger times: 18
Loss after 1110745 batches: 0.1108
trigger times: 19
Loss after 1111708 batches: 0.1100
trigger times: 20
Loss after 1112671 batches: 0.1070
trigger times: 21
Loss after 1113634 batches: 0.1049
trigger times: 22
Loss after 1114597 batches: 0.1018
trigger times: 23
Loss after 1115560 batches: 0.1002
trigger times: 24
Loss after 1116523 batches: 0.0973
trigger times: 25
Early stopping!
Start to test process.
Loss after 1117486 batches: 0.0957
Time to train on one home:  46.77099132537842
trigger times: 0
Loss after 1118449 batches: 0.1028
trigger times: 1
Loss after 1119412 batches: 0.1023
trigger times: 2
Loss after 1120375 batches: 0.1026
trigger times: 3
Loss after 1121338 batches: 0.1022
trigger times: 4
Loss after 1122301 batches: 0.1023
trigger times: 5
Loss after 1123264 batches: 0.1023
trigger times: 6
Loss after 1124227 batches: 0.1023
trigger times: 7
Loss after 1125190 batches: 0.1020
trigger times: 8
Loss after 1126153 batches: 0.1019
trigger times: 9
Loss after 1127116 batches: 0.1017
trigger times: 0
Loss after 1128079 batches: 0.1006
trigger times: 1
Loss after 1129042 batches: 0.0992
trigger times: 2
Loss after 1130005 batches: 0.0976
trigger times: 3
Loss after 1130968 batches: 0.0956
trigger times: 4
Loss after 1131931 batches: 0.0946
trigger times: 5
Loss after 1132894 batches: 0.0940
trigger times: 0
Loss after 1133857 batches: 0.0910
trigger times: 1
Loss after 1134820 batches: 0.0895
trigger times: 2
Loss after 1135783 batches: 0.0871
trigger times: 3
Loss after 1136746 batches: 0.0838
trigger times: 4
Loss after 1137709 batches: 0.0821
trigger times: 5
Loss after 1138672 batches: 0.0818
trigger times: 6
Loss after 1139635 batches: 0.0811
trigger times: 7
Loss after 1140598 batches: 0.0802
trigger times: 8
Loss after 1141561 batches: 0.0794
trigger times: 9
Loss after 1142524 batches: 0.0778
trigger times: 10
Loss after 1143487 batches: 0.0781
trigger times: 11
Loss after 1144450 batches: 0.0760
trigger times: 12
Loss after 1145413 batches: 0.0757
trigger times: 13
Loss after 1146376 batches: 0.0752
trigger times: 14
Loss after 1147339 batches: 0.0734
trigger times: 15
Loss after 1148302 batches: 0.0741
trigger times: 16
Loss after 1149265 batches: 0.0747
trigger times: 17
Loss after 1150228 batches: 0.0730
trigger times: 18
Loss after 1151191 batches: 0.0739
trigger times: 19
Loss after 1152154 batches: 0.0733
trigger times: 20
Loss after 1153117 batches: 0.0723
trigger times: 21
Loss after 1154080 batches: 0.0715
trigger times: 22
Loss after 1155043 batches: 0.0700
trigger times: 23
Loss after 1156006 batches: 0.0761
trigger times: 24
Loss after 1156969 batches: 0.0737
trigger times: 25
Early stopping!
Start to test process.
Loss after 1157932 batches: 0.0711
Time to train on one home:  57.646600008010864
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 1158895 batches: 0.0710
trigger times: 1
Loss after 1159858 batches: 0.0696
trigger times: 2
Loss after 1160821 batches: 0.0700
trigger times: 3
Loss after 1161784 batches: 0.0695
trigger times: 4
Loss after 1162747 batches: 0.0694
trigger times: 5
Loss after 1163710 batches: 0.0696
trigger times: 6
Loss after 1164673 batches: 0.0694
trigger times: 7
Loss after 1165636 batches: 0.0693
trigger times: 8
Loss after 1166599 batches: 0.0693
trigger times: 9
Loss after 1167562 batches: 0.0694
trigger times: 10
Loss after 1168525 batches: 0.0693
trigger times: 11
Loss after 1169488 batches: 0.0694
trigger times: 12
Loss after 1170451 batches: 0.0692
trigger times: 13
Loss after 1171414 batches: 0.0693
trigger times: 14
Loss after 1172377 batches: 0.0692
trigger times: 15
Loss after 1173340 batches: 0.0691
trigger times: 16
Loss after 1174303 batches: 0.0691
trigger times: 17
Loss after 1175266 batches: 0.0694
trigger times: 18
Loss after 1176229 batches: 0.0692
trigger times: 19
Loss after 1177192 batches: 0.0691
trigger times: 20
Loss after 1178155 batches: 0.0693
trigger times: 21
Loss after 1179118 batches: 0.0691
trigger times: 22
Loss after 1180081 batches: 0.0691
trigger times: 23
Loss after 1181044 batches: 0.0689
trigger times: 0
Loss after 1182007 batches: 0.0680
trigger times: 0
Loss after 1182970 batches: 0.0668
trigger times: 0
Loss after 1183933 batches: 0.0652
trigger times: 1
Loss after 1184896 batches: 0.0637
trigger times: 2
Loss after 1185859 batches: 0.0607
trigger times: 0
Loss after 1186822 batches: 0.0610
trigger times: 0
Loss after 1187785 batches: 0.0581
trigger times: 0
Loss after 1188748 batches: 0.0559
trigger times: 0
Loss after 1189711 batches: 0.0539
trigger times: 0
Loss after 1190674 batches: 0.0527
trigger times: 0
Loss after 1191637 batches: 0.0518
trigger times: 1
Loss after 1192600 batches: 0.0513
trigger times: 2
Loss after 1193563 batches: 0.0506
trigger times: 0
Loss after 1194526 batches: 0.0504
trigger times: 0
Loss after 1195489 batches: 0.0498
trigger times: 1
Loss after 1196452 batches: 0.0493
trigger times: 0
Loss after 1197415 batches: 0.0479
trigger times: 0
Loss after 1198378 batches: 0.0469
trigger times: 0
Loss after 1199341 batches: 0.0454
trigger times: 1
Loss after 1200304 batches: 0.0446
trigger times: 2
Loss after 1201267 batches: 0.0437
trigger times: 3
Loss after 1202230 batches: 0.0429
trigger times: 4
Loss after 1203193 batches: 0.0423
trigger times: 5
Loss after 1204156 batches: 0.0419
trigger times: 0
Loss after 1205119 batches: 0.0423
trigger times: 1
Loss after 1206082 batches: 0.0411
trigger times: 0
Loss after 1207045 batches: 0.0409
trigger times: 0
Loss after 1208008 batches: 0.0399
trigger times: 1
Loss after 1208971 batches: 0.0402
trigger times: 0
Loss after 1209934 batches: 0.0399
trigger times: 1
Loss after 1210897 batches: 0.0387
trigger times: 2
Loss after 1211860 batches: 0.0389
trigger times: 3
Loss after 1212823 batches: 0.0373
trigger times: 4
Loss after 1213786 batches: 0.0375
trigger times: 5
Loss after 1214749 batches: 0.0374
trigger times: 6
Loss after 1215712 batches: 0.0369
trigger times: 7
Loss after 1216675 batches: 0.0365
trigger times: 8
Loss after 1217638 batches: 0.0370
trigger times: 9
Loss after 1218601 batches: 0.0365
trigger times: 10
Loss after 1219564 batches: 0.0360
trigger times: 11
Loss after 1220527 batches: 0.0354
trigger times: 12
Loss after 1221490 batches: 0.0353
trigger times: 13
Loss after 1222453 batches: 0.0350
trigger times: 14
Loss after 1223416 batches: 0.0344
trigger times: 15
Loss after 1224379 batches: 0.0338
trigger times: 16
Loss after 1225342 batches: 0.0341
trigger times: 17
Loss after 1226305 batches: 0.0345
trigger times: 18
Loss after 1227268 batches: 0.0338
trigger times: 19
Loss after 1228231 batches: 0.0336
trigger times: 20
Loss after 1229194 batches: 0.0329
trigger times: 21
Loss after 1230157 batches: 0.0330
trigger times: 22
Loss after 1231120 batches: 0.0332
trigger times: 23
Loss after 1232083 batches: 0.0327
trigger times: 24
Loss after 1233046 batches: 0.0324
trigger times: 25
Early stopping!
Start to test process.
Loss after 1234009 batches: 0.0330
Time to train on one home:  84.59856295585632
trigger times: 0
Loss after 1234967 batches: 0.1439
trigger times: 0
Loss after 1235925 batches: 0.1399
trigger times: 0
Loss after 1236883 batches: 0.1389
trigger times: 1
Loss after 1237841 batches: 0.1406
trigger times: 2
Loss after 1238799 batches: 0.1394
trigger times: 3
Loss after 1239757 batches: 0.1395
trigger times: 4
Loss after 1240715 batches: 0.1393
trigger times: 5
Loss after 1241673 batches: 0.1388
trigger times: 6
Loss after 1242631 batches: 0.1391
trigger times: 7
Loss after 1243589 batches: 0.1400
trigger times: 8
Loss after 1244547 batches: 0.1393
trigger times: 9
Loss after 1245505 batches: 0.1388
trigger times: 10
Loss after 1246463 batches: 0.1382
trigger times: 11
Loss after 1247421 batches: 0.1391
trigger times: 12
Loss after 1248379 batches: 0.1389
trigger times: 13
Loss after 1249337 batches: 0.1394
trigger times: 14
Loss after 1250295 batches: 0.1389
trigger times: 15
Loss after 1251253 batches: 0.1390
trigger times: 16
Loss after 1252211 batches: 0.1394
trigger times: 17
Loss after 1253169 batches: 0.1391
trigger times: 18
Loss after 1254127 batches: 0.1390
trigger times: 19
Loss after 1255085 batches: 0.1399
trigger times: 20
Loss after 1256043 batches: 0.1389
trigger times: 21
Loss after 1257001 batches: 0.1384
trigger times: 22
Loss after 1257959 batches: 0.1388
trigger times: 23
Loss after 1258917 batches: 0.1388
trigger times: 24
Loss after 1259875 batches: 0.1391
trigger times: 25
Early stopping!
Start to test process.
Loss after 1260833 batches: 0.1382
Time to train on one home:  48.02793478965759
trigger times: 0
Loss after 1261795 batches: 0.0853
trigger times: 1
Loss after 1262757 batches: 0.0788
trigger times: 2
Loss after 1263719 batches: 0.0790
trigger times: 3
Loss after 1264681 batches: 0.0776
trigger times: 4
Loss after 1265643 batches: 0.0774
trigger times: 5
Loss after 1266605 batches: 0.0772
trigger times: 6
Loss after 1267567 batches: 0.0768
trigger times: 7
Loss after 1268529 batches: 0.0767
trigger times: 8
Loss after 1269491 batches: 0.0767
trigger times: 9
Loss after 1270453 batches: 0.0768
trigger times: 10
Loss after 1271415 batches: 0.0765
trigger times: 11
Loss after 1272377 batches: 0.0766
trigger times: 12
Loss after 1273339 batches: 0.0765
trigger times: 13
Loss after 1274301 batches: 0.0763
trigger times: 14
Loss after 1275263 batches: 0.0765
trigger times: 15
Loss after 1276225 batches: 0.0761
trigger times: 16
Loss after 1277187 batches: 0.0750
trigger times: 17
Loss after 1278149 batches: 0.0741
trigger times: 18
Loss after 1279111 batches: 0.0717
trigger times: 19
Loss after 1280073 batches: 0.0711
trigger times: 20
Loss after 1281035 batches: 0.0708
trigger times: 21
Loss after 1281997 batches: 0.0698
trigger times: 22
Loss after 1282959 batches: 0.0697
trigger times: 23
Loss after 1283921 batches: 0.0697
trigger times: 24
Loss after 1284883 batches: 0.0692
trigger times: 25
Early stopping!
Start to test process.
Loss after 1285845 batches: 0.0698
Time to train on one home:  46.61497187614441
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 1286808 batches: 0.1328
trigger times: 1
Loss after 1287771 batches: 0.0709
trigger times: 2
Loss after 1288734 batches: 0.0471
trigger times: 3
Loss after 1289697 batches: 0.0314
trigger times: 4
Loss after 1290660 batches: 0.0317
trigger times: 5
Loss after 1291623 batches: 0.0285
trigger times: 6
Loss after 1292586 batches: 0.0267
trigger times: 7
Loss after 1293549 batches: 0.0253
trigger times: 8
Loss after 1294512 batches: 0.0247
trigger times: 9
Loss after 1295475 batches: 0.0245
trigger times: 10
Loss after 1296438 batches: 0.0244
trigger times: 11
Loss after 1297401 batches: 0.0242
trigger times: 12
Loss after 1298364 batches: 0.0239
trigger times: 13
Loss after 1299327 batches: 0.0242
trigger times: 14
Loss after 1300290 batches: 0.0240
trigger times: 15
Loss after 1301253 batches: 0.0241
trigger times: 16
Loss after 1302216 batches: 0.0239
trigger times: 17
Loss after 1303179 batches: 0.0244
trigger times: 18
Loss after 1304142 batches: 0.0237
trigger times: 19
Loss after 1305105 batches: 0.0235
trigger times: 20
Loss after 1306068 batches: 0.0236
trigger times: 21
Loss after 1307031 batches: 0.0241
trigger times: 22
Loss after 1307994 batches: 0.0238
trigger times: 23
Loss after 1308957 batches: 0.0236
trigger times: 24
Loss after 1309920 batches: 0.0234
trigger times: 25
Early stopping!
Start to test process.
Loss after 1310883 batches: 0.0235
Time to train on one home:  47.01812720298767
trigger times: 0
Loss after 1311846 batches: 0.0967
trigger times: 0
Loss after 1312809 batches: 0.0954
trigger times: 1
Loss after 1313772 batches: 0.0955
trigger times: 2
Loss after 1314735 batches: 0.0952
trigger times: 3
Loss after 1315698 batches: 0.0949
trigger times: 4
Loss after 1316661 batches: 0.0947
trigger times: 5
Loss after 1317624 batches: 0.0948
trigger times: 6
Loss after 1318587 batches: 0.0949
trigger times: 7
Loss after 1319550 batches: 0.0952
trigger times: 8
Loss after 1320513 batches: 0.0954
trigger times: 9
Loss after 1321476 batches: 0.0954
trigger times: 10
Loss after 1322439 batches: 0.0912
trigger times: 11
Loss after 1323402 batches: 0.0840
trigger times: 12
Loss after 1324365 batches: 0.0794
trigger times: 13
Loss after 1325328 batches: 0.0768
trigger times: 14
Loss after 1326291 batches: 0.0741
trigger times: 15
Loss after 1327254 batches: 0.0719
trigger times: 0
Loss after 1328217 batches: 0.0668
trigger times: 1
Loss after 1329180 batches: 0.0633
trigger times: 2
Loss after 1330143 batches: 0.0616
trigger times: 3
Loss after 1331106 batches: 0.0590
trigger times: 4
Loss after 1332069 batches: 0.0580
trigger times: 5
Loss after 1333032 batches: 0.0556
trigger times: 0
Loss after 1333995 batches: 0.0545
trigger times: 0
Loss after 1334958 batches: 0.0541
trigger times: 1
Loss after 1335921 batches: 0.0533
trigger times: 2
Loss after 1336884 batches: 0.0528
trigger times: 0
Loss after 1337847 batches: 0.0524
trigger times: 1
Loss after 1338810 batches: 0.0511
trigger times: 2
Loss after 1339773 batches: 0.0494
trigger times: 0
Loss after 1340736 batches: 0.0489
trigger times: 0
Loss after 1341699 batches: 0.0490
trigger times: 1
Loss after 1342662 batches: 0.0486
trigger times: 0
Loss after 1343625 batches: 0.0485
trigger times: 0
Loss after 1344588 batches: 0.0481
trigger times: 1
Loss after 1345551 batches: 0.0492
trigger times: 2
Loss after 1346514 batches: 0.0476
trigger times: 0
Loss after 1347477 batches: 0.0473
trigger times: 1
Loss after 1348440 batches: 0.0476
trigger times: 2
Loss after 1349403 batches: 0.0480
trigger times: 0
Loss after 1350366 batches: 0.0459
trigger times: 1
Loss after 1351329 batches: 0.0470
trigger times: 2
Loss after 1352292 batches: 0.0468
trigger times: 0
Loss after 1353255 batches: 0.0454
trigger times: 1
Loss after 1354218 batches: 0.0446
trigger times: 2
Loss after 1355181 batches: 0.0446
trigger times: 3
Loss after 1356144 batches: 0.0445
trigger times: 4
Loss after 1357107 batches: 0.0442
trigger times: 0
Loss after 1358070 batches: 0.0444
trigger times: 1
Loss after 1359033 batches: 0.0448
trigger times: 2
Loss after 1359996 batches: 0.0450
trigger times: 3
Loss after 1360959 batches: 0.0440
trigger times: 4
Loss after 1361922 batches: 0.0439
trigger times: 5
Loss after 1362885 batches: 0.0439
trigger times: 6
Loss after 1363848 batches: 0.0434
trigger times: 7
Loss after 1364811 batches: 0.0431
trigger times: 8
Loss after 1365774 batches: 0.0439
trigger times: 0
Loss after 1366737 batches: 0.0429
trigger times: 1
Loss after 1367700 batches: 0.0432
trigger times: 2
Loss after 1368663 batches: 0.0437
trigger times: 3
Loss after 1369626 batches: 0.0434
trigger times: 4
Loss after 1370589 batches: 0.0434
trigger times: 0
Loss after 1371552 batches: 0.0432
trigger times: 1
Loss after 1372515 batches: 0.0431
trigger times: 2
Loss after 1373478 batches: 0.0426
trigger times: 0
Loss after 1374441 batches: 0.0418
trigger times: 1
Loss after 1375404 batches: 0.0419
trigger times: 2
Loss after 1376367 batches: 0.0423
trigger times: 3
Loss after 1377330 batches: 0.0405
trigger times: 4
Loss after 1378293 batches: 0.0414
trigger times: 5
Loss after 1379256 batches: 0.0409
trigger times: 6
Loss after 1380219 batches: 0.0408
trigger times: 7
Loss after 1381182 batches: 0.0403
trigger times: 8
Loss after 1382145 batches: 0.0402
trigger times: 9
Loss after 1383108 batches: 0.0408
trigger times: 10
Loss after 1384071 batches: 0.0415
trigger times: 11
Loss after 1385034 batches: 0.0396
trigger times: 12
Loss after 1385997 batches: 0.0405
trigger times: 13
Loss after 1386960 batches: 0.0400
trigger times: 14
Loss after 1387923 batches: 0.0407
trigger times: 15
Loss after 1388886 batches: 0.0399
trigger times: 16
Loss after 1389849 batches: 0.0391
trigger times: 17
Loss after 1390812 batches: 0.0395
trigger times: 18
Loss after 1391775 batches: 0.0401
trigger times: 19
Loss after 1392738 batches: 0.0389
trigger times: 20
Loss after 1393701 batches: 0.0386
trigger times: 21
Loss after 1394664 batches: 0.0388
trigger times: 22
Loss after 1395627 batches: 0.0386
trigger times: 23
Loss after 1396590 batches: 0.0383
trigger times: 24
Loss after 1397553 batches: 0.0380
trigger times: 25
Early stopping!
Start to test process.
Loss after 1398516 batches: 0.0377
Time to train on one home:  92.49412393569946
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 1399479 batches: 0.0711
trigger times: 1
Loss after 1400442 batches: 0.0696
trigger times: 2
Loss after 1401405 batches: 0.0702
trigger times: 3
Loss after 1402368 batches: 0.0697
trigger times: 4
Loss after 1403331 batches: 0.0696
trigger times: 5
Loss after 1404294 batches: 0.0693
trigger times: 6
Loss after 1405257 batches: 0.0693
trigger times: 7
Loss after 1406220 batches: 0.0695
trigger times: 8
Loss after 1407183 batches: 0.0695
trigger times: 9
Loss after 1408146 batches: 0.0693
trigger times: 10
Loss after 1409109 batches: 0.0693
trigger times: 11
Loss after 1410072 batches: 0.0691
trigger times: 12
Loss after 1411035 batches: 0.0695
trigger times: 13
Loss after 1411998 batches: 0.0694
trigger times: 14
Loss after 1412961 batches: 0.0694
trigger times: 15
Loss after 1413924 batches: 0.0692
trigger times: 16
Loss after 1414887 batches: 0.0691
trigger times: 17
Loss after 1415850 batches: 0.0692
trigger times: 18
Loss after 1416813 batches: 0.0691
trigger times: 19
Loss after 1417776 batches: 0.0694
trigger times: 20
Loss after 1418739 batches: 0.0692
trigger times: 21
Loss after 1419702 batches: 0.0691
trigger times: 22
Loss after 1420665 batches: 0.0687
trigger times: 23
Loss after 1421628 batches: 0.0675
trigger times: 24
Loss after 1422591 batches: 0.0665
trigger times: 0
Loss after 1423554 batches: 0.0650
trigger times: 1
Loss after 1424517 batches: 0.0629
trigger times: 2
Loss after 1425480 batches: 0.0612
trigger times: 0
Loss after 1426443 batches: 0.0593
trigger times: 0
Loss after 1427406 batches: 0.0574
trigger times: 1
Loss after 1428369 batches: 0.0563
trigger times: 2
Loss after 1429332 batches: 0.0539
trigger times: 0
Loss after 1430295 batches: 0.0527
trigger times: 0
Loss after 1431258 batches: 0.0523
trigger times: 0
Loss after 1432221 batches: 0.0511
trigger times: 0
Loss after 1433184 batches: 0.0503
trigger times: 1
Loss after 1434147 batches: 0.0494
trigger times: 0
Loss after 1435110 batches: 0.0488
trigger times: 0
Loss after 1436073 batches: 0.0483
trigger times: 0
Loss after 1437036 batches: 0.0471
trigger times: 0
Loss after 1437999 batches: 0.0447
trigger times: 1
Loss after 1438962 batches: 0.0444
trigger times: 0
Loss after 1439925 batches: 0.0442
trigger times: 0
Loss after 1440888 batches: 0.0432
trigger times: 1
Loss after 1441851 batches: 0.0436
trigger times: 2
Loss after 1442814 batches: 0.0426
trigger times: 3
Loss after 1443777 batches: 0.0417
trigger times: 4
Loss after 1444740 batches: 0.0423
trigger times: 5
Loss after 1445703 batches: 0.0410
trigger times: 6
Loss after 1446666 batches: 0.0404
trigger times: 7
Loss after 1447629 batches: 0.0401
trigger times: 8
Loss after 1448592 batches: 0.0400
trigger times: 9
Loss after 1449555 batches: 0.0392
trigger times: 10
Loss after 1450518 batches: 0.0387
trigger times: 11
Loss after 1451481 batches: 0.0390
trigger times: 12
Loss after 1452444 batches: 0.0383
trigger times: 13
Loss after 1453407 batches: 0.0379
trigger times: 14
Loss after 1454370 batches: 0.0377
trigger times: 15
Loss after 1455333 batches: 0.0369
trigger times: 16
Loss after 1456296 batches: 0.0364
trigger times: 17
Loss after 1457259 batches: 0.0368
trigger times: 18
Loss after 1458222 batches: 0.0372
trigger times: 19
Loss after 1459185 batches: 0.0357
trigger times: 20
Loss after 1460148 batches: 0.0354
trigger times: 21
Loss after 1461111 batches: 0.0353
trigger times: 22
Loss after 1462074 batches: 0.0346
trigger times: 23
Loss after 1463037 batches: 0.0352
trigger times: 24
Loss after 1464000 batches: 0.0353
trigger times: 25
Early stopping!
Start to test process.
Loss after 1464963 batches: 0.0348
Time to train on one home:  78.22066974639893
trigger times: 0
Loss after 1465926 batches: 0.0964
trigger times: 0
Loss after 1466889 batches: 0.0950
trigger times: 1
Loss after 1467852 batches: 0.0958
trigger times: 2
Loss after 1468815 batches: 0.0953
trigger times: 3
Loss after 1469778 batches: 0.0951
trigger times: 4
Loss after 1470741 batches: 0.0955
trigger times: 5
Loss after 1471704 batches: 0.0947
trigger times: 6
Loss after 1472667 batches: 0.0948
trigger times: 7
Loss after 1473630 batches: 0.0946
trigger times: 8
Loss after 1474593 batches: 0.0947
trigger times: 9
Loss after 1475556 batches: 0.0948
trigger times: 10
Loss after 1476519 batches: 0.0903
trigger times: 0
Loss after 1477482 batches: 0.0837
trigger times: 1
Loss after 1478445 batches: 0.0791
trigger times: 2
Loss after 1479408 batches: 0.0761
trigger times: 3
Loss after 1480371 batches: 0.0726
trigger times: 4
Loss after 1481334 batches: 0.0680
trigger times: 0
Loss after 1482297 batches: 0.0666
trigger times: 1
Loss after 1483260 batches: 0.0619
trigger times: 2
Loss after 1484223 batches: 0.0615
trigger times: 3
Loss after 1485186 batches: 0.0589
trigger times: 4
Loss after 1486149 batches: 0.0574
trigger times: 5
Loss after 1487112 batches: 0.0558
trigger times: 6
Loss after 1488075 batches: 0.0544
trigger times: 0
Loss after 1489038 batches: 0.0545
trigger times: 0
Loss after 1490001 batches: 0.0541
trigger times: 1
Loss after 1490964 batches: 0.0526
trigger times: 2
Loss after 1491927 batches: 0.0519
trigger times: 3
Loss after 1492890 batches: 0.0522
trigger times: 0
Loss after 1493853 batches: 0.0507
trigger times: 0
Loss after 1494816 batches: 0.0515
trigger times: 1
Loss after 1495779 batches: 0.0511
trigger times: 0
Loss after 1496742 batches: 0.0498
trigger times: 0
Loss after 1497705 batches: 0.0504
trigger times: 1
Loss after 1498668 batches: 0.0487
trigger times: 0
Loss after 1499631 batches: 0.0488
trigger times: 1
Loss after 1500594 batches: 0.0476
trigger times: 0
Loss after 1501557 batches: 0.0480
trigger times: 1
Loss after 1502520 batches: 0.0472
trigger times: 2
Loss after 1503483 batches: 0.0476
trigger times: 3
Loss after 1504446 batches: 0.0469
trigger times: 4
Loss after 1505409 batches: 0.0467
trigger times: 5
Loss after 1506372 batches: 0.0468
trigger times: 6
Loss after 1507335 batches: 0.0452
trigger times: 0
Loss after 1508298 batches: 0.0468
trigger times: 0
Loss after 1509261 batches: 0.0455
trigger times: 0
Loss after 1510224 batches: 0.0451
trigger times: 1
Loss after 1511187 batches: 0.0449
trigger times: 2
Loss after 1512150 batches: 0.0463
trigger times: 3
Loss after 1513113 batches: 0.0444
trigger times: 4
Loss after 1514076 batches: 0.0445
trigger times: 5
Loss after 1515039 batches: 0.0451
trigger times: 6
Loss after 1516002 batches: 0.0439
trigger times: 0
Loss after 1516965 batches: 0.0447
trigger times: 1
Loss after 1517928 batches: 0.0430
trigger times: 0
Loss after 1518891 batches: 0.0436
trigger times: 0
Loss after 1519854 batches: 0.0432
trigger times: 1
Loss after 1520817 batches: 0.0430
trigger times: 2
Loss after 1521780 batches: 0.0441
trigger times: 3
Loss after 1522743 batches: 0.0448
trigger times: 4
Loss after 1523706 batches: 0.0425
trigger times: 0
Loss after 1524669 batches: 0.0419
trigger times: 0
Loss after 1525632 batches: 0.0408
trigger times: 0
Loss after 1526595 batches: 0.0419
trigger times: 1
Loss after 1527558 batches: 0.0429
trigger times: 2
Loss after 1528521 batches: 0.0429
trigger times: 3
Loss after 1529484 batches: 0.0419
trigger times: 4
Loss after 1530447 batches: 0.0414
trigger times: 5
Loss after 1531410 batches: 0.0404
trigger times: 6
Loss after 1532373 batches: 0.0409
trigger times: 7
Loss after 1533336 batches: 0.0414
trigger times: 8
Loss after 1534299 batches: 0.0404
trigger times: 9
Loss after 1535262 batches: 0.0397
trigger times: 10
Loss after 1536225 batches: 0.0394
trigger times: 11
Loss after 1537188 batches: 0.0396
trigger times: 0
Loss after 1538151 batches: 0.0411
trigger times: 1
Loss after 1539114 batches: 0.0417
trigger times: 0
Loss after 1540077 batches: 0.0429
trigger times: 0
Loss after 1541040 batches: 0.0412
trigger times: 1
Loss after 1542003 batches: 0.0413
trigger times: 2
Loss after 1542966 batches: 0.0400
trigger times: 3
Loss after 1543929 batches: 0.0400
trigger times: 4
Loss after 1544892 batches: 0.0387
trigger times: 5
Loss after 1545855 batches: 0.0388
trigger times: 6
Loss after 1546818 batches: 0.0394
trigger times: 0
Loss after 1547781 batches: 0.0382
trigger times: 1
Loss after 1548744 batches: 0.0376
trigger times: 2
Loss after 1549707 batches: 0.0385
trigger times: 3
Loss after 1550670 batches: 0.0377
trigger times: 0
Loss after 1551633 batches: 0.0380
trigger times: 1
Loss after 1552596 batches: 0.0380
trigger times: 2
Loss after 1553559 batches: 0.0385
trigger times: 3
Loss after 1554522 batches: 0.0380
trigger times: 4
Loss after 1555485 batches: 0.0386
trigger times: 5
Loss after 1556448 batches: 0.0380
trigger times: 6
Loss after 1557411 batches: 0.0369
trigger times: 7
Loss after 1558374 batches: 0.0370
trigger times: 8
Loss after 1559337 batches: 0.0369
trigger times: 9
Loss after 1560300 batches: 0.0363
trigger times: 10
Loss after 1561263 batches: 0.0366
trigger times: 0
Loss after 1562226 batches: 0.0361
trigger times: 1
Loss after 1563189 batches: 0.0363
trigger times: 2
Loss after 1564152 batches: 0.0384
trigger times: 3
Loss after 1565115 batches: 0.0383
trigger times: 4
Loss after 1566078 batches: 0.0370
trigger times: 5
Loss after 1567041 batches: 0.0368
trigger times: 6
Loss after 1568004 batches: 0.0371
trigger times: 7
Loss after 1568967 batches: 0.0373
trigger times: 8
Loss after 1569930 batches: 0.0373
trigger times: 9
Loss after 1570893 batches: 0.0363
trigger times: 10
Loss after 1571856 batches: 0.0358
trigger times: 0
Loss after 1572819 batches: 0.0361
trigger times: 1
Loss after 1573782 batches: 0.0365
trigger times: 2
Loss after 1574745 batches: 0.0363
trigger times: 3
Loss after 1575708 batches: 0.0367
trigger times: 4
Loss after 1576671 batches: 0.0363
trigger times: 5
Loss after 1577634 batches: 0.0346
trigger times: 6
Loss after 1578597 batches: 0.0367
trigger times: 0
Loss after 1579560 batches: 0.0361
trigger times: 1
Loss after 1580523 batches: 0.0364
trigger times: 0
Loss after 1581486 batches: 0.0361
trigger times: 1
Loss after 1582449 batches: 0.0358
trigger times: 2
Loss after 1583412 batches: 0.0364
trigger times: 3
Loss after 1584375 batches: 0.0350
trigger times: 4
Loss after 1585338 batches: 0.0353
trigger times: 5
Loss after 1586301 batches: 0.0346
trigger times: 6
Loss after 1587264 batches: 0.0347
trigger times: 7
Loss after 1588227 batches: 0.0351
trigger times: 8
Loss after 1589190 batches: 0.0342
trigger times: 9
Loss after 1590153 batches: 0.0343
trigger times: 10
Loss after 1591116 batches: 0.0342
trigger times: 11
Loss after 1592079 batches: 0.0344
trigger times: 12
Loss after 1593042 batches: 0.0344
trigger times: 13
Loss after 1594005 batches: 0.0336
trigger times: 14
Loss after 1594968 batches: 0.0337
trigger times: 15
Loss after 1595931 batches: 0.0337
trigger times: 16
Loss after 1596894 batches: 0.0332
trigger times: 17
Loss after 1597857 batches: 0.0331
trigger times: 18
Loss after 1598820 batches: 0.0328
trigger times: 19
Loss after 1599783 batches: 0.0333
trigger times: 20
Loss after 1600746 batches: 0.0339
trigger times: 21
Loss after 1601709 batches: 0.0333
trigger times: 22
Loss after 1602672 batches: 0.0331
trigger times: 23
Loss after 1603635 batches: 0.0327
trigger times: 24
Loss after 1604598 batches: 0.0325
trigger times: 25
Early stopping!
Start to test process.
Loss after 1605561 batches: 0.0318
Time to train on one home:  131.47978043556213
trigger times: 0
Loss after 1606524 batches: 0.0852
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 1607487 batches: 0.0856
trigger times: 2
Loss after 1608450 batches: 0.0853
trigger times: 3
Loss after 1609413 batches: 0.0854
trigger times: 4
Loss after 1610376 batches: 0.0853
trigger times: 5
Loss after 1611339 batches: 0.0855
trigger times: 0
Loss after 1612302 batches: 0.0854
trigger times: 1
Loss after 1613265 batches: 0.0857
trigger times: 2
Loss after 1614228 batches: 0.0854
trigger times: 3
Loss after 1615191 batches: 0.0851
trigger times: 0
Loss after 1616154 batches: 0.0842
trigger times: 1
Loss after 1617117 batches: 0.0817
trigger times: 0
Loss after 1618080 batches: 0.0791
trigger times: 0
Loss after 1619043 batches: 0.0776
trigger times: 0
Loss after 1620006 batches: 0.0767
trigger times: 0
Loss after 1620969 batches: 0.0738
trigger times: 1
Loss after 1621932 batches: 0.0735
trigger times: 2
Loss after 1622895 batches: 0.0717
trigger times: 3
Loss after 1623858 batches: 0.0691
trigger times: 4
Loss after 1624821 batches: 0.0666
trigger times: 5
Loss after 1625784 batches: 0.0635
trigger times: 0
Loss after 1626747 batches: 0.0612
trigger times: 1
Loss after 1627710 batches: 0.0601
trigger times: 0
Loss after 1628673 batches: 0.0587
trigger times: 1
Loss after 1629636 batches: 0.0579
trigger times: 0
Loss after 1630599 batches: 0.0569
trigger times: 1
Loss after 1631562 batches: 0.0569
trigger times: 0
Loss after 1632525 batches: 0.0564
trigger times: 0
Loss after 1633488 batches: 0.0547
trigger times: 1
Loss after 1634451 batches: 0.0544
trigger times: 2
Loss after 1635414 batches: 0.0540
trigger times: 0
Loss after 1636377 batches: 0.0528
trigger times: 1
Loss after 1637340 batches: 0.0529
trigger times: 0
Loss after 1638303 batches: 0.0514
trigger times: 1
Loss after 1639266 batches: 0.0518
trigger times: 2
Loss after 1640229 batches: 0.0511
trigger times: 3
Loss after 1641192 batches: 0.0500
trigger times: 4
Loss after 1642155 batches: 0.0497
trigger times: 5
Loss after 1643118 batches: 0.0501
trigger times: 6
Loss after 1644081 batches: 0.0494
trigger times: 7
Loss after 1645044 batches: 0.0496
trigger times: 8
Loss after 1646007 batches: 0.0487
trigger times: 0
Loss after 1646970 batches: 0.0481
trigger times: 1
Loss after 1647933 batches: 0.0477
trigger times: 2
Loss after 1648896 batches: 0.0472
trigger times: 0
Loss after 1649859 batches: 0.0473
trigger times: 1
Loss after 1650822 batches: 0.0473
trigger times: 2
Loss after 1651785 batches: 0.0459
trigger times: 3
Loss after 1652748 batches: 0.0462
trigger times: 4
Loss after 1653711 batches: 0.0451
trigger times: 5
Loss after 1654674 batches: 0.0443
trigger times: 6
Loss after 1655637 batches: 0.0437
trigger times: 7
Loss after 1656600 batches: 0.0439
trigger times: 8
Loss after 1657563 batches: 0.0438
trigger times: 9
Loss after 1658526 batches: 0.0445
trigger times: 10
Loss after 1659489 batches: 0.0440
trigger times: 0
Loss after 1660452 batches: 0.0423
trigger times: 1
Loss after 1661415 batches: 0.0422
trigger times: 2
Loss after 1662378 batches: 0.0411
trigger times: 0
Loss after 1663341 batches: 0.0421
trigger times: 0
Loss after 1664304 batches: 0.0418
trigger times: 1
Loss after 1665267 batches: 0.0420
trigger times: 2
Loss after 1666230 batches: 0.0416
trigger times: 3
Loss after 1667193 batches: 0.0410
trigger times: 0
Loss after 1668156 batches: 0.0422
trigger times: 0
Loss after 1669119 batches: 0.0397
trigger times: 0
Loss after 1670082 batches: 0.0401
trigger times: 1
Loss after 1671045 batches: 0.0394
trigger times: 2
Loss after 1672008 batches: 0.0407
trigger times: 0
Loss after 1672971 batches: 0.0401
trigger times: 1
Loss after 1673934 batches: 0.0397
trigger times: 2
Loss after 1674897 batches: 0.0399
trigger times: 3
Loss after 1675860 batches: 0.0378
trigger times: 4
Loss after 1676823 batches: 0.0382
trigger times: 5
Loss after 1677786 batches: 0.0387
trigger times: 6
Loss after 1678749 batches: 0.0378
trigger times: 7
Loss after 1679712 batches: 0.0388
trigger times: 8
Loss after 1680675 batches: 0.0384
trigger times: 9
Loss after 1681638 batches: 0.0373
trigger times: 10
Loss after 1682601 batches: 0.0371
trigger times: 11
Loss after 1683564 batches: 0.0371
trigger times: 12
Loss after 1684527 batches: 0.0378
trigger times: 13
Loss after 1685490 batches: 0.0380
trigger times: 14
Loss after 1686453 batches: 0.0373
trigger times: 15
Loss after 1687416 batches: 0.0361
trigger times: 16
Loss after 1688379 batches: 0.0366
trigger times: 17
Loss after 1689342 batches: 0.0362
trigger times: 0
Loss after 1690305 batches: 0.0368
trigger times: 1
Loss after 1691268 batches: 0.0365
trigger times: 2
Loss after 1692231 batches: 0.0377
trigger times: 3
Loss after 1693194 batches: 0.0369
trigger times: 4
Loss after 1694157 batches: 0.0366
trigger times: 5
Loss after 1695120 batches: 0.0354
trigger times: 6
Loss after 1696083 batches: 0.0353
trigger times: 7
Loss after 1697046 batches: 0.0345
trigger times: 8
Loss after 1698009 batches: 0.0348
trigger times: 9
Loss after 1698972 batches: 0.0347
trigger times: 10
Loss after 1699935 batches: 0.0362
trigger times: 11
Loss after 1700898 batches: 0.0345
trigger times: 12
Loss after 1701861 batches: 0.0344
trigger times: 13
Loss after 1702824 batches: 0.0342
trigger times: 0
Loss after 1703787 batches: 0.0330
trigger times: 1
Loss after 1704750 batches: 0.0331
trigger times: 2
Loss after 1705713 batches: 0.0345
trigger times: 3
Loss after 1706676 batches: 0.0342
trigger times: 4
Loss after 1707639 batches: 0.0335
trigger times: 5
Loss after 1708602 batches: 0.0334
trigger times: 6
Loss after 1709565 batches: 0.0328
trigger times: 7
Loss after 1710528 batches: 0.0326
trigger times: 8
Loss after 1711491 batches: 0.0321
trigger times: 9
Loss after 1712454 batches: 0.0324
trigger times: 0
Loss after 1713417 batches: 0.0335
trigger times: 1
Loss after 1714380 batches: 0.0321
trigger times: 2
Loss after 1715343 batches: 0.0317
trigger times: 3
Loss after 1716306 batches: 0.0320
trigger times: 4
Loss after 1717269 batches: 0.0326
trigger times: 5
Loss after 1718232 batches: 0.0312
trigger times: 6
Loss after 1719195 batches: 0.0320
trigger times: 7
Loss after 1720158 batches: 0.0310
trigger times: 8
Loss after 1721121 batches: 0.0303
trigger times: 9
Loss after 1722084 batches: 0.0305
trigger times: 10
trigger times: 11
Loss after 1724010 batches: 0.0304
trigger times: 12
Loss after 1724973 batches: 0.0310
trigger times: 13
Loss after 1725936 batches: 0.0311
trigger times: 14
Loss after 1726899 batches: 0.0299
trigger times: 15
Loss after 1727862 batches: 0.0314
trigger times: 16
Loss after 1728825 batches: 0.0308
trigger times: 17
Loss after 1729788 batches: 0.0301
trigger times: 18
Loss after 1730751 batches: 0.0304
trigger times: 19
Loss after 1731714 batches: 0.0306
trigger times: 20
Loss after 1732677 batches: 0.0295
trigger times: 21
Loss after 1733640 batches: 0.0305
trigger times: 22
Loss after 1734603 batches: 0.0300
trigger times: 0
Loss after 1735566 batches: 0.0295
trigger times: 1
Loss after 1736529 batches: 0.0284
trigger times: 2
Loss after 1737492 batches: 0.0281
trigger times: 3
Loss after 1738455 batches: 0.0291
trigger times: 4
Loss after 1739418 batches: 0.0281
trigger times: 5
Loss after 1740381 batches: 0.0352
trigger times: 6
Loss after 1741344 batches: 0.0343
trigger times: 7
Loss after 1742307 batches: 0.0325
trigger times: 8
Loss after 1743270 batches: 0.0303
trigger times: 9
Loss after 1744233 batches: 0.0296
trigger times: 10
Loss after 1745196 batches: 0.0290
trigger times: 11
Loss after 1746159 batches: 0.0286
trigger times: 12
Loss after 1747122 batches: 0.0286
trigger times: 13
Loss after 1748085 batches: 0.0281
trigger times: 14
Loss after 1749048 batches: 0.0274
trigger times: 15
Loss after 1750011 batches: 0.0275
trigger times: 0
Loss after 1750974 batches: 0.0261
trigger times: 1
Loss after 1751937 batches: 0.0275
trigger times: 2
Loss after 1752900 batches: 0.0269
trigger times: 3
Loss after 1753863 batches: 0.0264
trigger times: 4
Loss after 1754826 batches: 0.0265
trigger times: 5
Loss after 1755789 batches: 0.0268
trigger times: 6
Loss after 1756752 batches: 0.0290
trigger times: 7
Loss after 1757715 batches: 0.0272
trigger times: 8
Loss after 1758678 batches: 0.0264
trigger times: 9
Loss after 1759641 batches: 0.0268
trigger times: 10
Loss after 1760604 batches: 0.0263
trigger times: 11
Loss after 1761567 batches: 0.0255
trigger times: 12
Loss after 1762530 batches: 0.0261
trigger times: 13
Loss after 1763493 batches: 0.0252
trigger times: 14
Loss after 1764456 batches: 0.0254
trigger times: 15
Loss after 1765419 batches: 0.0254
trigger times: 16
Loss after 1766382 batches: 0.0254
trigger times: 17
Loss after 1767345 batches: 0.0251
trigger times: 18
Loss after 1768308 batches: 0.0250
trigger times: 19
Loss after 1769271 batches: 0.0254
trigger times: 20
Loss after 1770234 batches: 0.0262
trigger times: 21
Loss after 1771197 batches: 0.0261
trigger times: 22
Loss after 1772160 batches: 0.0261
trigger times: 23
Loss after 1773123 batches: 0.0262
trigger times: 24
Loss after 1774086 batches: 0.0256
trigger times: 25
Early stopping!
Start to test process.
Loss after 1775049 batches: 0.0258
Time to train on one home:  152.80020546913147
trigger times: 0
Loss after 1776012 batches: 0.1035
trigger times: 1
Loss after 1776975 batches: 0.0692
trigger times: 2
Loss after 1777938 batches: 0.0671
trigger times: 3
Loss after 1778901 batches: 0.0580
trigger times: 4
Loss after 1779864 batches: 0.0569
trigger times: 5
Loss after 1780827 batches: 0.0557
trigger times: 6
Loss after 1781790 batches: 0.0546
trigger times: 7
Loss after 1782753 batches: 0.0541
trigger times: 8
Loss after 1783716 batches: 0.0538
trigger times: 9
Loss after 1784679 batches: 0.0537
trigger times: 10
Loss after 1785642 batches: 0.0537
trigger times: 11
Loss after 1786605 batches: 0.0534
trigger times: 12
Loss after 1787568 batches: 0.0532
trigger times: 13
Loss after 1788531 batches: 0.0533
trigger times: 14
Loss after 1789494 batches: 0.0532
trigger times: 15
Loss after 1790457 batches: 0.0531
trigger times: 16
Loss after 1791420 batches: 0.0533
trigger times: 17
Loss after 1792383 batches: 0.0533
trigger times: 18
Loss after 1793346 batches: 0.0532
trigger times: 19
Loss after 1794309 batches: 0.0533
trigger times: 20
Loss after 1795272 batches: 0.0532
trigger times: 21
Loss after 1796235 batches: 0.0529
trigger times: 22
Loss after 1797198 batches: 0.0532
trigger times: 23
Loss after 1798161 batches: 0.0529
trigger times: 24
Loss after 1799124 batches: 0.0532
trigger times: 25
Early stopping!
Start to test process.
Loss after 1800087 batches: 0.0529
Time to train on one home:  46.44704270362854
trigger times: 0
Loss after 1801050 batches: 0.1313
trigger times: 1
Loss after 1802013 batches: 0.1311
trigger times: 2
Loss after 1802976 batches: 0.1310
trigger times: 3
Loss after 1803939 batches: 0.1309
trigger times: 4
Loss after 1804902 batches: 0.1308
trigger times: 5
Loss after 1805865 batches: 0.1305
trigger times: 6
Loss after 1806828 batches: 0.1308
trigger times: 7
Loss after 1807791 batches: 0.1298
trigger times: 0
Loss after 1808754 batches: 0.1254
trigger times: 1
Loss after 1809717 batches: 0.1247
trigger times: 2
Loss after 1810680 batches: 0.1222
trigger times: 3
Loss after 1811643 batches: 0.1194
trigger times: 4
Loss after 1812606 batches: 0.1142
trigger times: 5
Loss after 1813569 batches: 0.1100
trigger times: 6
Loss after 1814532 batches: 0.1087
trigger times: 7
Loss after 1815495 batches: 0.1055
trigger times: 8
Loss after 1816458 batches: 0.1044
trigger times: 9
Loss after 1817421 batches: 0.1028
trigger times: 10
Loss after 1818384 batches: 0.1020
trigger times: 11
Loss after 1819347 batches: 0.1014
trigger times: 12
Loss after 1820310 batches: 0.1004
trigger times: 13
Loss after 1821273 batches: 0.0989
trigger times: 14
Loss after 1822236 batches: 0.0979
trigger times: 15
Loss after 1823199 batches: 0.0971
trigger times: 16
Loss after 1824162 batches: 0.0961
trigger times: 17
Loss after 1825125 batches: 0.0959
trigger times: 18
Loss after 1826088 batches: 0.0955
trigger times: 19
Loss after 1827051 batches: 0.0939
trigger times: 20
Loss after 1828014 batches: 0.0947
trigger times: 21
Loss after 1828977 batches: 0.0931
trigger times: 22
Loss after 1829940 batches: 0.0931
trigger times: 23
Loss after 1830903 batches: 0.0920
trigger times: 24
Loss after 1831866 batches: 0.0915
trigger times: 25
Early stopping!
Start to test process.
Loss after 1832829 batches: 0.0915
Time to train on one home:  52.489144802093506
trigger times: 0
Loss after 1833792 batches: 0.1222
trigger times: 0
Loss after 1834755 batches: 0.1197
trigger times: 1
Loss after 1835718 batches: 0.1206
trigger times: 2
Loss after 1836681 batches: 0.1192
trigger times: 3
Loss after 1837644 batches: 0.1194
trigger times: 4
Loss after 1838607 batches: 0.1195
trigger times: 5
Loss after 1839570 batches: 0.1196
trigger times: 6
Loss after 1840533 batches: 0.1190
trigger times: 7
Loss after 1841496 batches: 0.1191
trigger times: 8
Loss after 1842459 batches: 0.1192
trigger times: 9
Loss after 1843422 batches: 0.1192
trigger times: 10
Loss after 1844385 batches: 0.1191
trigger times: 11
Loss after 1845348 batches: 0.1195
trigger times: 12
Loss after 1846311 batches: 0.1195
trigger times: 13
Loss after 1847274 batches: 0.1192
trigger times: 14
Loss after 1848237 batches: 0.1197
trigger times: 15
Loss after 1849200 batches: 0.1193
trigger times: 16
Loss after 1850163 batches: 0.1192
trigger times: 17
Loss after 1851126 batches: 0.1193
trigger times: 18
Loss after 1852089 batches: 0.1189
trigger times: 19
Loss after 1853052 batches: 0.1190
trigger times: 20
Loss after 1854015 batches: 0.1192
trigger times: 21
Loss after 1854978 batches: 0.1198
trigger times: 22
Loss after 1855941 batches: 0.1150
trigger times: 0
Loss after 1856904 batches: 0.1080
trigger times: 0
Loss after 1857867 batches: 0.0992
trigger times: 1
Loss after 1858830 batches: 0.0921
trigger times: 0
Loss after 1859793 batches: 0.0852
trigger times: 1
Loss after 1860756 batches: 0.0797
trigger times: 2
Loss after 1861719 batches: 0.0765
trigger times: 0
Loss after 1862682 batches: 0.0739
trigger times: 0
Loss after 1863645 batches: 0.0724
trigger times: 1
Loss after 1864608 batches: 0.0712
trigger times: 0
Loss after 1865571 batches: 0.0688
trigger times: 1
Loss after 1866534 batches: 0.0682
trigger times: 2
Loss after 1867497 batches: 0.0675
trigger times: 3
Loss after 1868460 batches: 0.0660
trigger times: 4
Loss after 1869423 batches: 0.0632
trigger times: 5
Loss after 1870386 batches: 0.0615
trigger times: 6
Loss after 1871349 batches: 0.0627
trigger times: 7
Loss after 1872312 batches: 0.0603
trigger times: 0
Loss after 1873275 batches: 0.0612
trigger times: 0
Loss after 1874238 batches: 0.0604
trigger times: 1
Loss after 1875201 batches: 0.0588
trigger times: 0
Loss after 1876164 batches: 0.0586
trigger times: 1
Loss after 1877127 batches: 0.0580
trigger times: 2
Loss after 1878090 batches: 0.0560
trigger times: 3
Loss after 1879053 batches: 0.0560
trigger times: 0
Loss after 1880016 batches: 0.0549
trigger times: 1
Loss after 1880979 batches: 0.0549
trigger times: 0
Loss after 1881942 batches: 0.0530
trigger times: 1
Loss after 1882905 batches: 0.0534
trigger times: 0
Loss after 1883868 batches: 0.0525
trigger times: 1
Loss after 1884831 batches: 0.0518
trigger times: 2
Loss after 1885794 batches: 0.0529
trigger times: 3
Loss after 1886757 batches: 0.0520
trigger times: 4
Loss after 1887720 batches: 0.0512
trigger times: 5
Loss after 1888683 batches: 0.0502
trigger times: 0
Loss after 1889646 batches: 0.0505
trigger times: 1
Loss after 1890609 batches: 0.0495
trigger times: 2
Loss after 1891572 batches: 0.0488
trigger times: 3
Loss after 1892535 batches: 0.0490
trigger times: 4
Loss after 1893498 batches: 0.0488
trigger times: 5
Loss after 1894461 batches: 0.0493
trigger times: 6
Loss after 1895424 batches: 0.0485
trigger times: 7
Loss after 1896387 batches: 0.0469
trigger times: 8
Loss after 1897350 batches: 0.0467
trigger times: 9
Loss after 1898313 batches: 0.0472
trigger times: 10
Loss after 1899276 batches: 0.0472
trigger times: 11
Loss after 1900239 batches: 0.0480
trigger times: 12
Loss after 1901202 batches: 0.0471
trigger times: 0
Loss after 1902165 batches: 0.0464
trigger times: 1
Loss after 1903128 batches: 0.0452
trigger times: 2
Loss after 1904091 batches: 0.0461
trigger times: 3
Loss after 1905054 batches: 0.0450
trigger times: 4
Loss after 1906017 batches: 0.0448
trigger times: 5
Loss after 1906980 batches: 0.0455
trigger times: 6
Loss after 1907943 batches: 0.0452
trigger times: 7
Loss after 1908906 batches: 0.0449
trigger times: 8
Loss after 1909869 batches: 0.0442
trigger times: 9
Loss after 1910832 batches: 0.0443
trigger times: 10
Loss after 1911795 batches: 0.0447
trigger times: 11
Loss after 1912758 batches: 0.0459
trigger times: 12
Loss after 1913721 batches: 0.0440
trigger times: 13
Loss after 1914684 batches: 0.0442
trigger times: 14
Loss after 1915647 batches: 0.0448
trigger times: 15
Loss after 1916610 batches: 0.0440
trigger times: 16
Loss after 1917573 batches: 0.0450
trigger times: 17
Loss after 1918536 batches: 0.0441
trigger times: 18
Loss after 1919499 batches: 0.0430
trigger times: 19
Loss after 1920462 batches: 0.0426
trigger times: 20
Loss after 1921425 batches: 0.0433
trigger times: 21
Loss after 1922388 batches: 0.0435
trigger times: 22
Loss after 1923351 batches: 0.0432
trigger times: 23
Loss after 1924314 batches: 0.0431
trigger times: 24
Loss after 1925277 batches: 0.0425
trigger times: 25
Early stopping!
Start to test process.
Loss after 1926240 batches: 0.0420
Time to train on one home:  96.4130871295929
trigger times: 0
Loss after 1927169 batches: 0.1414
trigger times: 0
Loss after 1928098 batches: 0.1359
trigger times: 0
Loss after 1929027 batches: 0.1346
trigger times: 1
Loss after 1929956 batches: 0.1358
trigger times: 2
Loss after 1930885 batches: 0.1364
trigger times: 3
Loss after 1931814 batches: 0.1357
trigger times: 4
Loss after 1932743 batches: 0.1359
trigger times: 5
Loss after 1933672 batches: 0.1348
trigger times: 6
Loss after 1934601 batches: 0.1350
trigger times: 7
Loss after 1935530 batches: 0.1361
trigger times: 8
Loss after 1936459 batches: 0.1347
trigger times: 9
Loss after 1937388 batches: 0.1341
trigger times: 10
Loss after 1938317 batches: 0.1357
trigger times: 11
Loss after 1939246 batches: 0.1357
trigger times: 12
Loss after 1940175 batches: 0.1351
trigger times: 13
Loss after 1941104 batches: 0.1352
trigger times: 14
Loss after 1942033 batches: 0.1348
trigger times: 15
Loss after 1942962 batches: 0.1362
trigger times: 16
Loss after 1943891 batches: 0.1352
trigger times: 17
Loss after 1944820 batches: 0.1351
trigger times: 18
Loss after 1945749 batches: 0.1328
trigger times: 19
Loss after 1946678 batches: 0.1259
trigger times: 20
Loss after 1947607 batches: 0.0884
trigger times: 21
Loss after 1948536 batches: 0.0752
trigger times: 22
Loss after 1949465 batches: 0.0697
trigger times: 23
Loss after 1950394 batches: 0.0663
trigger times: 24
Loss after 1951323 batches: 0.0660
trigger times: 25
Early stopping!
Start to test process.
Loss after 1952252 batches: 0.0678
Time to train on one home:  47.878560066223145
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 1953215 batches: 0.0733
trigger times: 1
Loss after 1954178 batches: 0.0428
trigger times: 2
Loss after 1955141 batches: 0.0397
trigger times: 3
Loss after 1956104 batches: 0.0321
trigger times: 4
Loss after 1957067 batches: 0.0316
trigger times: 5
Loss after 1958030 batches: 0.0301
trigger times: 6
Loss after 1958993 batches: 0.0293
trigger times: 7
Loss after 1959956 batches: 0.0292
trigger times: 8
Loss after 1960919 batches: 0.0288
trigger times: 9
Loss after 1961882 batches: 0.0286
trigger times: 10
Loss after 1962845 batches: 0.0284
trigger times: 11
Loss after 1963808 batches: 0.0284
trigger times: 12
Loss after 1964771 batches: 0.0284
trigger times: 13
Loss after 1965734 batches: 0.0282
trigger times: 14
Loss after 1966697 batches: 0.0282
trigger times: 15
Loss after 1967660 batches: 0.0282
trigger times: 16
Loss after 1968623 batches: 0.0280
trigger times: 17
Loss after 1969586 batches: 0.0280
trigger times: 18
Loss after 1970549 batches: 0.0281
trigger times: 19
Loss after 1971512 batches: 0.0279
trigger times: 20
Loss after 1972475 batches: 0.0281
trigger times: 21
Loss after 1973438 batches: 0.0281
trigger times: 22
Loss after 1974401 batches: 0.0280
trigger times: 23
Loss after 1975364 batches: 0.0278
trigger times: 24
Loss after 1976327 batches: 0.0280
trigger times: 25
Early stopping!
Start to test process.
Loss after 1977290 batches: 0.0280
Time to train on one home:  46.77506399154663
trigger times: 0
Loss after 1978253 batches: 0.2052
trigger times: 1
Loss after 1979216 batches: 0.1520
trigger times: 2
Loss after 1980179 batches: 0.1370
trigger times: 3
Loss after 1981142 batches: 0.1241
trigger times: 4
Loss after 1982105 batches: 0.1226
trigger times: 5
Loss after 1983068 batches: 0.1217
trigger times: 6
Loss after 1984031 batches: 0.1202
trigger times: 7
Loss after 1984994 batches: 0.1191
trigger times: 8
Loss after 1985957 batches: 0.1180
trigger times: 9
Loss after 1986920 batches: 0.1174
trigger times: 10
Loss after 1987883 batches: 0.1178
trigger times: 11
Loss after 1988846 batches: 0.1175
trigger times: 12
Loss after 1989809 batches: 0.1172
trigger times: 13
Loss after 1990772 batches: 0.1172
trigger times: 14
Loss after 1991735 batches: 0.1174
trigger times: 15
Loss after 1992698 batches: 0.1172
trigger times: 16
Loss after 1993661 batches: 0.1170
trigger times: 17
Loss after 1994624 batches: 0.1170
trigger times: 18
Loss after 1995587 batches: 0.1171
trigger times: 19
Loss after 1996550 batches: 0.1168
trigger times: 20
Loss after 1997513 batches: 0.1163
trigger times: 21
Loss after 1998476 batches: 0.1167
trigger times: 22
Loss after 1999439 batches: 0.1162
trigger times: 23
Loss after 2000402 batches: 0.1141
trigger times: 24
Loss after 2001365 batches: 0.1053
trigger times: 25
Early stopping!
Start to test process.
Loss after 2002328 batches: 0.1018
Time to train on one home:  46.506458044052124
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 2003291 batches: 0.1352
trigger times: 1
Loss after 2004254 batches: 0.1181
trigger times: 2
Loss after 2005217 batches: 0.1177
trigger times: 3
Loss after 2006180 batches: 0.1140
trigger times: 4
Loss after 2007143 batches: 0.1127
trigger times: 5
Loss after 2008106 batches: 0.1126
trigger times: 6
Loss after 2009069 batches: 0.1121
trigger times: 7
Loss after 2010032 batches: 0.1117
trigger times: 8
Loss after 2010995 batches: 0.1115
trigger times: 9
Loss after 2011958 batches: 0.1116
trigger times: 10
Loss after 2012921 batches: 0.1116
trigger times: 11
Loss after 2013884 batches: 0.1112
trigger times: 12
Loss after 2014847 batches: 0.1112
trigger times: 13
Loss after 2015810 batches: 0.1112
trigger times: 14
Loss after 2016773 batches: 0.1113
trigger times: 15
Loss after 2017736 batches: 0.1111
trigger times: 16
Loss after 2018699 batches: 0.1114
trigger times: 17
Loss after 2019662 batches: 0.1105
trigger times: 18
Loss after 2020625 batches: 0.1097
trigger times: 19
Loss after 2021588 batches: 0.1056
trigger times: 20
Loss after 2022551 batches: 0.1026
trigger times: 21
Loss after 2023514 batches: 0.1010
trigger times: 22
Loss after 2024477 batches: 0.0991
trigger times: 23
Loss after 2025440 batches: 0.0977
trigger times: 24
Loss after 2026403 batches: 0.0957
trigger times: 25
Early stopping!
Start to test process.
Loss after 2027366 batches: 0.0936
Time to train on one home:  46.657182693481445
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 2028329 batches: 0.0892
trigger times: 1
Loss after 2029292 batches: 0.0852
trigger times: 2
Loss after 2030255 batches: 0.0862
trigger times: 3
Loss after 2031218 batches: 0.0852
trigger times: 4
Loss after 2032181 batches: 0.0846
trigger times: 5
Loss after 2033144 batches: 0.0843
trigger times: 6
Loss after 2034107 batches: 0.0844
trigger times: 7
Loss after 2035070 batches: 0.0844
trigger times: 8
Loss after 2036033 batches: 0.0844
trigger times: 9
Loss after 2036996 batches: 0.0840
trigger times: 10
Loss after 2037959 batches: 0.0839
trigger times: 11
Loss after 2038922 batches: 0.0839
trigger times: 12
Loss after 2039885 batches: 0.0839
trigger times: 13
Loss after 2040848 batches: 0.0837
trigger times: 14
Loss after 2041811 batches: 0.0839
trigger times: 15
Loss after 2042774 batches: 0.0831
trigger times: 16
Loss after 2043737 batches: 0.0815
trigger times: 17
Loss after 2044700 batches: 0.0789
trigger times: 18
Loss after 2045663 batches: 0.0772
trigger times: 19
Loss after 2046626 batches: 0.0763
trigger times: 20
Loss after 2047589 batches: 0.0758
trigger times: 21
Loss after 2048552 batches: 0.0759
trigger times: 22
Loss after 2049515 batches: 0.0748
trigger times: 23
Loss after 2050478 batches: 0.0758
trigger times: 24
Loss after 2051441 batches: 0.0753
trigger times: 25
Early stopping!
Start to test process.
Loss after 2052404 batches: 0.0751
Time to train on one home:  46.4907009601593
trigger times: 0
Loss after 2053367 batches: 0.0559
trigger times: 1
Loss after 2054330 batches: 0.0544
trigger times: 2
Loss after 2055293 batches: 0.0546
trigger times: 3
Loss after 2056256 batches: 0.0543
trigger times: 4
Loss after 2057219 batches: 0.0539
trigger times: 5
Loss after 2058182 batches: 0.0540
trigger times: 6
Loss after 2059145 batches: 0.0537
trigger times: 7
Loss after 2060108 batches: 0.0538
trigger times: 8
Loss after 2061071 batches: 0.0535
trigger times: 9
Loss after 2062034 batches: 0.0538
trigger times: 10
Loss after 2062997 batches: 0.0537
trigger times: 11
Loss after 2063960 batches: 0.0536
trigger times: 12
Loss after 2064923 batches: 0.0528
trigger times: 13
Loss after 2065886 batches: 0.0504
trigger times: 14
Loss after 2066849 batches: 0.0489
trigger times: 15
Loss after 2067812 batches: 0.0475
trigger times: 16
Loss after 2068775 batches: 0.0466
trigger times: 17
Loss after 2069738 batches: 0.0458
trigger times: 18
Loss after 2070701 batches: 0.0454
trigger times: 19
Loss after 2071664 batches: 0.0455
trigger times: 0
Loss after 2072627 batches: 0.0434
trigger times: 1
Loss after 2073590 batches: 0.0411
trigger times: 2
Loss after 2074553 batches: 0.0396
trigger times: 0
Loss after 2075516 batches: 0.0375
trigger times: 1
Loss after 2076479 batches: 0.0366
trigger times: 2
Loss after 2077442 batches: 0.0353
trigger times: 0
Loss after 2078405 batches: 0.0347
trigger times: 0
Loss after 2079368 batches: 0.0348
trigger times: 0
Loss after 2080331 batches: 0.0335
trigger times: 0
Loss after 2081294 batches: 0.0324
trigger times: 1
Loss after 2082257 batches: 0.0329
trigger times: 2
Loss after 2083220 batches: 0.0323
trigger times: 3
Loss after 2084183 batches: 0.0315
trigger times: 4
Loss after 2085146 batches: 0.0308
trigger times: 0
Loss after 2086109 batches: 0.0307
trigger times: 0
Loss after 2087072 batches: 0.0308
trigger times: 1
Loss after 2088035 batches: 0.0301
trigger times: 2
Loss after 2088998 batches: 0.0298
trigger times: 3
Loss after 2089961 batches: 0.0293
trigger times: 0
Loss after 2090924 batches: 0.0294
trigger times: 1
Loss after 2091887 batches: 0.0289
trigger times: 2
Loss after 2092850 batches: 0.0289
trigger times: 3
Loss after 2093813 batches: 0.0279
trigger times: 0
Loss after 2094776 batches: 0.0281
trigger times: 1
Loss after 2095739 batches: 0.0276
trigger times: 2
Loss after 2096702 batches: 0.0276
trigger times: 3
Loss after 2097665 batches: 0.0277
trigger times: 4
Loss after 2098628 batches: 0.0273
trigger times: 5
Loss after 2099591 batches: 0.0267
trigger times: 6
Loss after 2100554 batches: 0.0266
trigger times: 0
Loss after 2101517 batches: 0.0266
trigger times: 1
Loss after 2102480 batches: 0.0263
trigger times: 2
Loss after 2103443 batches: 0.0251
trigger times: 3
Loss after 2104406 batches: 0.0250
trigger times: 4
Loss after 2105369 batches: 0.0248
trigger times: 5
Loss after 2106332 batches: 0.0247
trigger times: 6
Loss after 2107295 batches: 0.0239
trigger times: 7
Loss after 2108258 batches: 0.0241
trigger times: 8
Loss after 2109221 batches: 0.0237
trigger times: 0
Loss after 2110184 batches: 0.0235
trigger times: 0
Loss after 2111147 batches: 0.0233
trigger times: 1
Loss after 2112110 batches: 0.0235
trigger times: 2
Loss after 2113073 batches: 0.0230
trigger times: 3
Loss after 2114036 batches: 0.0231
trigger times: 4
Loss after 2114999 batches: 0.0225
trigger times: 5
Loss after 2115962 batches: 0.0225
trigger times: 6
Loss after 2116925 batches: 0.0219
trigger times: 7
Loss after 2117888 batches: 0.0227
trigger times: 8
Loss after 2118851 batches: 0.0219
trigger times: 9
Loss after 2119814 batches: 0.0217
trigger times: 10
Loss after 2120777 batches: 0.0220
trigger times: 11
Loss after 2121740 batches: 0.0213
trigger times: 12
Loss after 2122703 batches: 0.0217
trigger times: 13
Loss after 2123666 batches: 0.0215
trigger times: 14
Loss after 2124629 batches: 0.0213
trigger times: 15
Loss after 2125592 batches: 0.0209
trigger times: 16
Loss after 2126555 batches: 0.0212
trigger times: 17
Loss after 2127518 batches: 0.0205
trigger times: 18
Loss after 2128481 batches: 0.0207
trigger times: 19
Loss after 2129444 batches: 0.0205
trigger times: 20
Loss after 2130407 batches: 0.0206
trigger times: 0
Loss after 2131370 batches: 0.0204
trigger times: 1
Loss after 2132333 batches: 0.0196
trigger times: 2
Loss after 2133296 batches: 0.0202
trigger times: 3
Loss after 2134259 batches: 0.0200
trigger times: 4
Loss after 2135222 batches: 0.0205
trigger times: 5
Loss after 2136185 batches: 0.0207
trigger times: 6
Loss after 2137148 batches: 0.0202
trigger times: 0
Loss after 2138111 batches: 0.0192
trigger times: 1
Loss after 2139074 batches: 0.0197
trigger times: 2
Loss after 2140037 batches: 0.0196
trigger times: 3
Loss after 2141000 batches: 0.0191
trigger times: 4
Loss after 2141963 batches: 0.0196
trigger times: 5
Loss after 2142926 batches: 0.0190
trigger times: 0
Loss after 2143889 batches: 0.0190
trigger times: 1
Loss after 2144852 batches: 0.0191
trigger times: 2
Loss after 2145815 batches: 0.0190
trigger times: 3
Loss after 2146778 batches: 0.0192
trigger times: 4
Loss after 2147741 batches: 0.0194
trigger times: 0
Loss after 2148704 batches: 0.0191
trigger times: 1
Loss after 2149667 batches: 0.0190
trigger times: 2
Loss after 2150630 batches: 0.0187
trigger times: 3
Loss after 2151593 batches: 0.0194
trigger times: 4
Loss after 2152556 batches: 0.0194
trigger times: 5
Loss after 2153519 batches: 0.0189
trigger times: 6
Loss after 2154482 batches: 0.0187
trigger times: 7
Loss after 2155445 batches: 0.0184
trigger times: 8
Loss after 2156408 batches: 0.0188
trigger times: 9
Loss after 2157371 batches: 0.0183
trigger times: 10
Loss after 2158334 batches: 0.0180
trigger times: 0
Loss after 2159297 batches: 0.0182
trigger times: 1
Loss after 2160260 batches: 0.0177
trigger times: 2
Loss after 2161223 batches: 0.0180
trigger times: 3
Loss after 2162186 batches: 0.0179
trigger times: 4
Loss after 2163149 batches: 0.0174
trigger times: 5
Loss after 2164112 batches: 0.0177
trigger times: 6
Loss after 2165075 batches: 0.0174
trigger times: 7
Loss after 2166038 batches: 0.0173
trigger times: 8
Loss after 2167001 batches: 0.0172
trigger times: 9
Loss after 2167964 batches: 0.0166
trigger times: 10
Loss after 2168927 batches: 0.0167
trigger times: 11
Loss after 2169890 batches: 0.0164
trigger times: 12
Loss after 2170853 batches: 0.0169
trigger times: 13
Loss after 2171816 batches: 0.0168
trigger times: 14
Loss after 2172779 batches: 0.0166
trigger times: 15
Loss after 2173742 batches: 0.0165
trigger times: 16
Loss after 2174705 batches: 0.0166
trigger times: 17
Loss after 2175668 batches: 0.0170
trigger times: 18
Loss after 2176631 batches: 0.0167
trigger times: 19
Loss after 2177594 batches: 0.0164
trigger times: 20
Loss after 2178557 batches: 0.0160
trigger times: 21
Loss after 2179520 batches: 0.0167
trigger times: 22
Loss after 2180483 batches: 0.0163
trigger times: 23
Loss after 2181446 batches: 0.0164
trigger times: 24
Loss after 2182409 batches: 0.0163
trigger times: 25
Early stopping!
Start to test process.
Loss after 2183372 batches: 0.0162
Time to train on one home:  124.52948713302612
trigger times: 0
Loss after 2184335 batches: 0.0776
trigger times: 0
Loss after 2185298 batches: 0.0774
trigger times: 1
Loss after 2186261 batches: 0.0777
trigger times: 2
Loss after 2187224 batches: 0.0775
trigger times: 0
Loss after 2188187 batches: 0.0777
trigger times: 1
Loss after 2189150 batches: 0.0779
trigger times: 2
Loss after 2190113 batches: 0.0776
trigger times: 3
Loss after 2191076 batches: 0.0775
trigger times: 4
Loss after 2192039 batches: 0.0772
trigger times: 5
Loss after 2193002 batches: 0.0777
trigger times: 6
Loss after 2193965 batches: 0.0778
trigger times: 0
Loss after 2194928 batches: 0.0773
trigger times: 0
Loss after 2195891 batches: 0.0763
trigger times: 1
Loss after 2196854 batches: 0.0742
trigger times: 2
Loss after 2197817 batches: 0.0726
trigger times: 3
Loss after 2198780 batches: 0.0685
trigger times: 0
Loss after 2199743 batches: 0.0629
trigger times: 1
Loss after 2200706 batches: 0.0611
trigger times: 2
Loss after 2201669 batches: 0.0575
trigger times: 0
Loss after 2202632 batches: 0.0555
trigger times: 1
Loss after 2203595 batches: 0.0536
trigger times: 0
Loss after 2204558 batches: 0.0516
trigger times: 1
Loss after 2205521 batches: 0.0500
trigger times: 0
Loss after 2206484 batches: 0.0496
trigger times: 1
Loss after 2207447 batches: 0.0485
trigger times: 2
Loss after 2208410 batches: 0.0471
trigger times: 3
Loss after 2209373 batches: 0.0476
trigger times: 0
Loss after 2210336 batches: 0.0477
trigger times: 1
Loss after 2211299 batches: 0.0471
trigger times: 2
Loss after 2212262 batches: 0.0463
trigger times: 0
Loss after 2213225 batches: 0.0457
trigger times: 0
Loss after 2214188 batches: 0.0451
trigger times: 0
Loss after 2215151 batches: 0.0445
trigger times: 1
Loss after 2216114 batches: 0.0443
trigger times: 2
Loss after 2217077 batches: 0.0442
trigger times: 0
Loss after 2218040 batches: 0.0428
trigger times: 0
Loss after 2219003 batches: 0.0419
trigger times: 1
Loss after 2219966 batches: 0.0414
trigger times: 0
Loss after 2220929 batches: 0.0413
trigger times: 1
Loss after 2221892 batches: 0.0415
trigger times: 0
Loss after 2222855 batches: 0.0404
trigger times: 1
Loss after 2223818 batches: 0.0399
trigger times: 0
Loss after 2224781 batches: 0.0403
trigger times: 1
Loss after 2225744 batches: 0.0393
trigger times: 2
Loss after 2226707 batches: 0.0383
trigger times: 0
Loss after 2227670 batches: 0.0377
trigger times: 1
Loss after 2228633 batches: 0.0365
trigger times: 2
Loss after 2229596 batches: 0.0373
trigger times: 0
Loss after 2230559 batches: 0.0383
trigger times: 1
Loss after 2231522 batches: 0.0374
trigger times: 2
Loss after 2232485 batches: 0.0358
trigger times: 3
Loss after 2233448 batches: 0.0355
trigger times: 4
Loss after 2234411 batches: 0.0340
trigger times: 5
Loss after 2235374 batches: 0.0339
trigger times: 6
Loss after 2236337 batches: 0.0336
trigger times: 7
Loss after 2237300 batches: 0.0329
trigger times: 8
Loss after 2238263 batches: 0.0334
trigger times: 9
Loss after 2239226 batches: 0.0338
trigger times: 10
Loss after 2240189 batches: 0.0325
trigger times: 11
Loss after 2241152 batches: 0.0327
trigger times: 12
Loss after 2242115 batches: 0.0321
trigger times: 13
Loss after 2243078 batches: 0.0312
trigger times: 14
Loss after 2244041 batches: 0.0316
trigger times: 15
Loss after 2245004 batches: 0.0310
trigger times: 16
Loss after 2245967 batches: 0.0303
trigger times: 17
Loss after 2246930 batches: 0.0302
trigger times: 18
Loss after 2247893 batches: 0.0305
trigger times: 19
Loss after 2248856 batches: 0.0302
trigger times: 20
Loss after 2249819 batches: 0.0299
trigger times: 21
Loss after 2250782 batches: 0.0291
trigger times: 22
Loss after 2251745 batches: 0.0290
trigger times: 23
Loss after 2252708 batches: 0.0292
trigger times: 24
Loss after 2253671 batches: 0.0288
trigger times: 25
Early stopping!
Start to test process.
Loss after 2254634 batches: 0.0294
Time to train on one home:  80.9380464553833
trigger times: 0
Loss after 2255597 batches: 0.1653
trigger times: 1
Loss after 2256560 batches: 0.1563
trigger times: 2
Loss after 2257523 batches: 0.1577
trigger times: 3
Loss after 2258486 batches: 0.1553
trigger times: 4
Loss after 2259449 batches: 0.1550
trigger times: 5
Loss after 2260412 batches: 0.1545
trigger times: 6
Loss after 2261375 batches: 0.1542
trigger times: 7
Loss after 2262338 batches: 0.1541
trigger times: 8
Loss after 2263301 batches: 0.1540
trigger times: 9
Loss after 2264264 batches: 0.1539
trigger times: 10
Loss after 2265227 batches: 0.1534
trigger times: 11
Loss after 2266190 batches: 0.1502
trigger times: 12
Loss after 2267153 batches: 0.1418
trigger times: 13
Loss after 2268116 batches: 0.1330
trigger times: 14
Loss after 2269079 batches: 0.1165
trigger times: 15
Loss after 2270042 batches: 0.1018
trigger times: 16
Loss after 2271005 batches: 0.0919
trigger times: 17
Loss after 2271968 batches: 0.0825
trigger times: 18
Loss after 2272931 batches: 0.0784
trigger times: 19
Loss after 2273894 batches: 0.0752
trigger times: 20
Loss after 2274857 batches: 0.0720
trigger times: 21
Loss after 2275820 batches: 0.0697
trigger times: 22
Loss after 2276783 batches: 0.0688
trigger times: 23
Loss after 2277746 batches: 0.0678
trigger times: 24
Loss after 2278709 batches: 0.0657
trigger times: 25
Early stopping!
Start to test process.
Loss after 2279672 batches: 0.0662
Time to train on one home:  46.91340708732605
trigger times: 0
Loss after 2280635 batches: 0.1223
trigger times: 0
Loss after 2281598 batches: 0.1198
trigger times: 1
Loss after 2282561 batches: 0.1206
trigger times: 2
Loss after 2283524 batches: 0.1198
trigger times: 3
Loss after 2284487 batches: 0.1195
trigger times: 4
Loss after 2285450 batches: 0.1195
trigger times: 5
Loss after 2286413 batches: 0.1193
trigger times: 6
Loss after 2287376 batches: 0.1190
trigger times: 7
Loss after 2288339 batches: 0.1191
trigger times: 8
Loss after 2289302 batches: 0.1189
trigger times: 9
Loss after 2290265 batches: 0.1194
trigger times: 10
Loss after 2291228 batches: 0.1192
trigger times: 11
Loss after 2292191 batches: 0.1192
trigger times: 12
Loss after 2293154 batches: 0.1191
trigger times: 13
Loss after 2294117 batches: 0.1191
trigger times: 14
Loss after 2295080 batches: 0.1193
trigger times: 15
Loss after 2296043 batches: 0.1190
trigger times: 16
Loss after 2297006 batches: 0.1191
trigger times: 17
Loss after 2297969 batches: 0.1192
trigger times: 18
Loss after 2298932 batches: 0.1191
trigger times: 19
Loss after 2299895 batches: 0.1192
trigger times: 20
Loss after 2300858 batches: 0.1190
trigger times: 21
Loss after 2301821 batches: 0.1167
trigger times: 22
Loss after 2302784 batches: 0.1112
trigger times: 23
Loss after 2303747 batches: 0.1061
trigger times: 24
Loss after 2304710 batches: 0.0944
trigger times: 25
Early stopping!
Start to test process.
Loss after 2305673 batches: 0.0907
Time to train on one home:  49.093984603881836
trigger times: 0
Loss after 2306636 batches: 0.0969
trigger times: 0
Loss after 2307599 batches: 0.0952
trigger times: 1
Loss after 2308562 batches: 0.0960
trigger times: 2
Loss after 2309525 batches: 0.0958
trigger times: 3
Loss after 2310488 batches: 0.0950
trigger times: 4
Loss after 2311451 batches: 0.0949
trigger times: 5
Loss after 2312414 batches: 0.0947
trigger times: 6
Loss after 2313377 batches: 0.0947
trigger times: 7
Loss after 2314340 batches: 0.0953
trigger times: 8
Loss after 2315303 batches: 0.0948
trigger times: 9
Loss after 2316266 batches: 0.0943
trigger times: 10
Loss after 2317229 batches: 0.0897
trigger times: 11
Loss after 2318192 batches: 0.0816
trigger times: 12
Loss after 2319155 batches: 0.0788
trigger times: 13
Loss after 2320118 batches: 0.0763
trigger times: 14
Loss after 2321081 batches: 0.0740
trigger times: 15
Loss after 2322044 batches: 0.0694
trigger times: 16
Loss after 2323007 batches: 0.0692
trigger times: 0
Loss after 2323970 batches: 0.0633
trigger times: 1
Loss after 2324933 batches: 0.0609
trigger times: 0
Loss after 2325896 batches: 0.0582
trigger times: 1
Loss after 2326859 batches: 0.0573
trigger times: 0
Loss after 2327822 batches: 0.0561
trigger times: 1
Loss after 2328785 batches: 0.0540
trigger times: 2
Loss after 2329748 batches: 0.0535
trigger times: 3
Loss after 2330711 batches: 0.0537
trigger times: 0
Loss after 2331674 batches: 0.0517
trigger times: 0
Loss after 2332637 batches: 0.0513
trigger times: 1
Loss after 2333600 batches: 0.0499
trigger times: 2
Loss after 2334563 batches: 0.0501
trigger times: 0
Loss after 2335526 batches: 0.0502
trigger times: 0
Loss after 2336489 batches: 0.0489
trigger times: 1
Loss after 2337452 batches: 0.0490
trigger times: 2
Loss after 2338415 batches: 0.0489
trigger times: 3
Loss after 2339378 batches: 0.0473
trigger times: 4
Loss after 2340341 batches: 0.0481
trigger times: 0
Loss after 2341304 batches: 0.0475
trigger times: 0
Loss after 2342267 batches: 0.0478
trigger times: 1
Loss after 2343230 batches: 0.0488
trigger times: 2
Loss after 2344193 batches: 0.0467
trigger times: 3
Loss after 2345156 batches: 0.0456
trigger times: 4
Loss after 2346119 batches: 0.0455
trigger times: 0
Loss after 2347082 batches: 0.0454
trigger times: 1
Loss after 2348045 batches: 0.0452
trigger times: 2
Loss after 2349008 batches: 0.0458
trigger times: 3
Loss after 2349971 batches: 0.0448
trigger times: 4
Loss after 2350934 batches: 0.0452
trigger times: 5
Loss after 2351897 batches: 0.0447
trigger times: 6
Loss after 2352860 batches: 0.0442
trigger times: 7
Loss after 2353823 batches: 0.0449
trigger times: 8
Loss after 2354786 batches: 0.0436
trigger times: 9
Loss after 2355749 batches: 0.0432
trigger times: 10
Loss after 2356712 batches: 0.0433
trigger times: 0
Loss after 2357675 batches: 0.0440
trigger times: 0
Loss after 2358638 batches: 0.0434
trigger times: 1
Loss after 2359601 batches: 0.0428
trigger times: 2
Loss after 2360564 batches: 0.0426
trigger times: 3
Loss after 2361527 batches: 0.0423
trigger times: 0
Loss after 2362490 batches: 0.0425
trigger times: 1
Loss after 2363453 batches: 0.0425
trigger times: 0
Loss after 2364416 batches: 0.0418
trigger times: 1
Loss after 2365379 batches: 0.0421
trigger times: 2
Loss after 2366342 batches: 0.0409
trigger times: 3
Loss after 2367305 batches: 0.0407
trigger times: 4
Loss after 2368268 batches: 0.0409
trigger times: 5
Loss after 2369231 batches: 0.0406
trigger times: 6
Loss after 2370194 batches: 0.0407
trigger times: 7
Loss after 2371157 batches: 0.0413
trigger times: 8
Loss after 2372120 batches: 0.0399
trigger times: 9
Loss after 2373083 batches: 0.0411
trigger times: 10
Loss after 2374046 batches: 0.0401
trigger times: 11
Loss after 2375009 batches: 0.0397
trigger times: 12
Loss after 2375972 batches: 0.0400
trigger times: 13
Loss after 2376935 batches: 0.0392
trigger times: 14
Loss after 2377898 batches: 0.0401
trigger times: 15
Loss after 2378861 batches: 0.0389
trigger times: 16
Loss after 2379824 batches: 0.0396
trigger times: 17
Loss after 2380787 batches: 0.0414
trigger times: 18
Loss after 2381750 batches: 0.0392
trigger times: 0
Loss after 2382713 batches: 0.0396
trigger times: 1
Loss after 2383676 batches: 0.0378
trigger times: 2
Loss after 2384639 batches: 0.0384
trigger times: 3
Loss after 2385602 batches: 0.0388
trigger times: 4
Loss after 2386565 batches: 0.0379
trigger times: 5
Loss after 2387528 batches: 0.0376
trigger times: 6
Loss after 2388491 batches: 0.0376
trigger times: 7
Loss after 2389454 batches: 0.0380
trigger times: 0
Loss after 2390417 batches: 0.0387
trigger times: 1
Loss after 2391380 batches: 0.0363
trigger times: 2
Loss after 2392343 batches: 0.0374
trigger times: 3
Loss after 2393306 batches: 0.0379
trigger times: 4
Loss after 2394269 batches: 0.0384
trigger times: 5
Loss after 2395232 batches: 0.0384
trigger times: 6
Loss after 2396195 batches: 0.0375
trigger times: 7
Loss after 2397158 batches: 0.0373
trigger times: 8
Loss after 2398121 batches: 0.0365
trigger times: 9
Loss after 2399084 batches: 0.0369
trigger times: 10
Loss after 2400047 batches: 0.0354
trigger times: 11
Loss after 2401010 batches: 0.0363
trigger times: 12
Loss after 2401973 batches: 0.0356
trigger times: 13
Loss after 2402936 batches: 0.0350
trigger times: 14
Loss after 2403899 batches: 0.0352
trigger times: 15
Loss after 2404862 batches: 0.0363
trigger times: 16
Loss after 2405825 batches: 0.0363
trigger times: 17
Loss after 2406788 batches: 0.0367
trigger times: 18
Loss after 2407751 batches: 0.0352
trigger times: 19
Loss after 2408714 batches: 0.0360
trigger times: 20
Loss after 2409677 batches: 0.0357
trigger times: 21
Loss after 2410640 batches: 0.0355
trigger times: 22
Loss after 2411603 batches: 0.0351
trigger times: 23
Loss after 2412566 batches: 0.0350
trigger times: 24
Loss after 2413529 batches: 0.0343
trigger times: 25
Early stopping!
Start to test process.
Loss after 2414492 batches: 0.0348
Time to train on one home:  108.45876288414001
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 2415455 batches: 0.0968
trigger times: 0
Loss after 2416418 batches: 0.0861
trigger times: 0
Loss after 2417381 batches: 0.0873
trigger times: 1
Loss after 2418344 batches: 0.0892
trigger times: 2
Loss after 2419307 batches: 0.0861
trigger times: 3
Loss after 2420270 batches: 0.0862
trigger times: 4
Loss after 2421233 batches: 0.0853
trigger times: 5
Loss after 2422196 batches: 0.0865
trigger times: 6
Loss after 2423159 batches: 0.0857
trigger times: 7
Loss after 2424122 batches: 0.0852
trigger times: 8
Loss after 2425085 batches: 0.0851
trigger times: 9
Loss after 2426048 batches: 0.0853
trigger times: 10
Loss after 2427011 batches: 0.0862
trigger times: 11
Loss after 2427974 batches: 0.0857
trigger times: 12
Loss after 2428937 batches: 0.0858
trigger times: 13
Loss after 2429900 batches: 0.0856
trigger times: 14
Loss after 2430863 batches: 0.0855
trigger times: 15
Loss after 2431826 batches: 0.0855
trigger times: 16
Loss after 2432789 batches: 0.0858
trigger times: 17
Loss after 2433752 batches: 0.0854
trigger times: 18
Loss after 2434715 batches: 0.0858
trigger times: 19
Loss after 2435678 batches: 0.0857
trigger times: 20
Loss after 2436641 batches: 0.0854
trigger times: 21
Loss after 2437604 batches: 0.0852
trigger times: 22
Loss after 2438567 batches: 0.0859
trigger times: 23
Loss after 2439530 batches: 0.0856
trigger times: 24
Loss after 2440493 batches: 0.0854
trigger times: 25
Early stopping!
Start to test process.
Loss after 2441456 batches: 0.0854
Time to train on one home:  48.263009786605835
trigger times: 0
Loss after 2442419 batches: 0.1090
trigger times: 1
Loss after 2443382 batches: 0.0913
trigger times: 2
Loss after 2444345 batches: 0.0934
trigger times: 3
Loss after 2445308 batches: 0.0881
trigger times: 4
Loss after 2446271 batches: 0.0875
trigger times: 5
Loss after 2447234 batches: 0.0872
trigger times: 6
Loss after 2448197 batches: 0.0867
trigger times: 7
Loss after 2449160 batches: 0.0863
trigger times: 8
Loss after 2450123 batches: 0.0863
trigger times: 9
Loss after 2451086 batches: 0.0860
trigger times: 10
Loss after 2452049 batches: 0.0859
trigger times: 11
Loss after 2453012 batches: 0.0859
trigger times: 12
Loss after 2453975 batches: 0.0858
trigger times: 13
Loss after 2454938 batches: 0.0859
trigger times: 14
Loss after 2455901 batches: 0.0858
trigger times: 15
Loss after 2456864 batches: 0.0858
trigger times: 16
Loss after 2457827 batches: 0.0858
trigger times: 17
Loss after 2458790 batches: 0.0859
trigger times: 18
Loss after 2459753 batches: 0.0859
trigger times: 19
Loss after 2460716 batches: 0.0858
trigger times: 20
Loss after 2461679 batches: 0.0857
trigger times: 21
Loss after 2462642 batches: 0.0854
trigger times: 22
Loss after 2463605 batches: 0.0855
trigger times: 23
Loss after 2464568 batches: 0.0851
trigger times: 24
Loss after 2465531 batches: 0.0827
trigger times: 25
Early stopping!
Start to test process.
Loss after 2466494 batches: 0.0817
Time to train on one home:  46.52205944061279
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 2467457 batches: 0.0964
trigger times: 0
Loss after 2468420 batches: 0.0865
trigger times: 0
Loss after 2469383 batches: 0.0885
trigger times: 1
Loss after 2470346 batches: 0.0881
trigger times: 2
Loss after 2471309 batches: 0.0860
trigger times: 3
Loss after 2472272 batches: 0.0857
trigger times: 4
Loss after 2473235 batches: 0.0854
trigger times: 5
Loss after 2474198 batches: 0.0852
trigger times: 6
Loss after 2475161 batches: 0.0853
trigger times: 7
Loss after 2476124 batches: 0.0860
trigger times: 8
Loss after 2477087 batches: 0.0857
trigger times: 9
Loss after 2478050 batches: 0.0859
trigger times: 10
Loss after 2479013 batches: 0.0860
trigger times: 11
Loss after 2479976 batches: 0.0859
trigger times: 12
Loss after 2480939 batches: 0.0857
trigger times: 13
Loss after 2481902 batches: 0.0853
trigger times: 14
Loss after 2482865 batches: 0.0850
trigger times: 15
Loss after 2483828 batches: 0.0854
trigger times: 16
Loss after 2484791 batches: 0.0860
trigger times: 17
Loss after 2485754 batches: 0.0856
trigger times: 18
Loss after 2486717 batches: 0.0861
trigger times: 19
Loss after 2487680 batches: 0.0855
trigger times: 20
Loss after 2488643 batches: 0.0854
trigger times: 21
Loss after 2489606 batches: 0.0857
trigger times: 22
Loss after 2490569 batches: 0.0856
trigger times: 23
Loss after 2491532 batches: 0.0853
trigger times: 24
Loss after 2492495 batches: 0.0858
trigger times: 25
Early stopping!
Start to test process.
Loss after 2493458 batches: 0.0862
Time to train on one home:  47.969680309295654
trigger times: 0
Loss after 2494353 batches: 0.0844
trigger times: 0
Loss after 2495248 batches: 0.0843
trigger times: 0
Loss after 2496143 batches: 0.0841
trigger times: 1
Loss after 2497038 batches: 0.0842
trigger times: 2
Loss after 2497933 batches: 0.0840
trigger times: 3
Loss after 2498828 batches: 0.0840
trigger times: 4
Loss after 2499723 batches: 0.0840
trigger times: 5
Loss after 2500618 batches: 0.0841
trigger times: 6
Loss after 2501513 batches: 0.0841
trigger times: 7
Loss after 2502408 batches: 0.0840
trigger times: 8
Loss after 2503303 batches: 0.0840
trigger times: 9
Loss after 2504198 batches: 0.0839
trigger times: 10
Loss after 2505093 batches: 0.0840
trigger times: 11
Loss after 2505988 batches: 0.0839
trigger times: 12
Loss after 2506883 batches: 0.0838
trigger times: 13
Loss after 2507778 batches: 0.0837
trigger times: 14
Loss after 2508673 batches: 0.0832
trigger times: 15
Loss after 2509568 batches: 0.0795
trigger times: 16
Loss after 2510463 batches: 0.0389
trigger times: 17
Loss after 2511358 batches: 0.0264
trigger times: 0
Loss after 2512253 batches: 0.0213
trigger times: 1
Loss after 2513148 batches: 0.0205
trigger times: 2
Loss after 2514043 batches: 0.0179
trigger times: 3
Loss after 2514938 batches: 0.0157
trigger times: 4
Loss after 2515833 batches: 0.0145
trigger times: 0
Loss after 2516728 batches: 0.0126
trigger times: 1
Loss after 2517623 batches: 0.0118
trigger times: 0
Loss after 2518518 batches: 0.0102
trigger times: 1
Loss after 2519413 batches: 0.0105
trigger times: 2
Loss after 2520308 batches: 0.0094
trigger times: 3
Loss after 2521203 batches: 0.0078
trigger times: 0
Loss after 2522098 batches: 0.0095
trigger times: 0
Loss after 2522993 batches: 0.0080
trigger times: 0
Loss after 2523888 batches: 0.0091
trigger times: 1
Loss after 2524783 batches: 0.0085
trigger times: 2
Loss after 2525678 batches: 0.0081
trigger times: 0
Loss after 2526573 batches: 0.0080
trigger times: 1
Loss after 2527468 batches: 0.0090
trigger times: 2
Loss after 2528363 batches: 0.0076
trigger times: 3
Loss after 2529258 batches: 0.0073
trigger times: 4
Loss after 2530153 batches: 0.0073
trigger times: 5
Loss after 2531048 batches: 0.0072
trigger times: 6
Loss after 2531943 batches: 0.0069
trigger times: 7
Loss after 2532838 batches: 0.0070
trigger times: 8
Loss after 2533733 batches: 0.0062
trigger times: 9
Loss after 2534628 batches: 0.0062
trigger times: 10
Loss after 2535523 batches: 0.0067
trigger times: 11
Loss after 2536418 batches: 0.0064
trigger times: 12
Loss after 2537313 batches: 0.0062
trigger times: 13
Loss after 2538208 batches: 0.0064
trigger times: 14
Loss after 2539103 batches: 0.0062
trigger times: 15
Loss after 2539998 batches: 0.0065
trigger times: 16
Loss after 2540893 batches: 0.0058
trigger times: 17
Loss after 2541788 batches: 0.0060
trigger times: 18
Loss after 2542683 batches: 0.0065
trigger times: 19
Loss after 2543578 batches: 0.0078
trigger times: 20
Loss after 2544473 batches: 0.0065
trigger times: 21
Loss after 2545368 batches: 0.0061
trigger times: 22
Loss after 2546263 batches: 0.0057
trigger times: 23
Loss after 2547158 batches: 0.0060
trigger times: 0
Loss after 2548053 batches: 0.0051
trigger times: 0
Loss after 2548948 batches: 0.0049
trigger times: 1
Loss after 2549843 batches: 0.0050
trigger times: 2
Loss after 2550738 batches: 0.0051
trigger times: 3
Loss after 2551633 batches: 0.0050
trigger times: 4
Loss after 2552528 batches: 0.0049
trigger times: 5
Loss after 2553423 batches: 0.0051
trigger times: 6
Loss after 2554318 batches: 0.0050
trigger times: 7
Loss after 2555213 batches: 0.0046
trigger times: 8
Loss after 2556108 batches: 0.0047
trigger times: 9
Loss after 2557003 batches: 0.0069
trigger times: 10
Loss after 2557898 batches: 0.0080
trigger times: 11
Loss after 2558793 batches: 0.0071
trigger times: 12
Loss after 2559688 batches: 0.0070
trigger times: 0
Loss after 2560583 batches: 0.0057
trigger times: 1
Loss after 2561478 batches: 0.0065
trigger times: 2
Loss after 2562373 batches: 0.0057
trigger times: 3
Loss after 2563268 batches: 0.0052
trigger times: 4
Loss after 2564163 batches: 0.0055
trigger times: 5
Loss after 2565058 batches: 0.0059
trigger times: 6
Loss after 2565953 batches: 0.0060
trigger times: 7
Loss after 2566848 batches: 0.0052
trigger times: 8
Loss after 2567743 batches: 0.0046
trigger times: 9
Loss after 2568638 batches: 0.0043
trigger times: 10
Loss after 2569533 batches: 0.0045
trigger times: 11
Loss after 2570428 batches: 0.0047
trigger times: 12
Loss after 2571323 batches: 0.0044
trigger times: 13
Loss after 2572218 batches: 0.0043
trigger times: 14
Loss after 2573113 batches: 0.0041
trigger times: 15
Loss after 2574008 batches: 0.0043
trigger times: 16
Loss after 2574903 batches: 0.0038
trigger times: 17
Loss after 2575798 batches: 0.0052
trigger times: 18
Loss after 2576693 batches: 0.0052
trigger times: 19
Loss after 2577588 batches: 0.0047
trigger times: 20
Loss after 2578483 batches: 0.0048
trigger times: 21
Loss after 2579378 batches: 0.0050
trigger times: 22
Loss after 2580273 batches: 0.0043
trigger times: 23
Loss after 2581168 batches: 0.0047
trigger times: 24
Loss after 2582063 batches: 0.0047
trigger times: 25
Early stopping!
Start to test process.
Loss after 2582958 batches: 0.0051
Time to train on one home:  95.13003087043762
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 2583895 batches: 0.0942
trigger times: 1
Loss after 2584832 batches: 0.0946
trigger times: 0
Loss after 2585769 batches: 0.0940
trigger times: 1
Loss after 2586706 batches: 0.0938
trigger times: 2
Loss after 2587643 batches: 0.0939
trigger times: 3
Loss after 2588580 batches: 0.0938
trigger times: 4
Loss after 2589517 batches: 0.0935
trigger times: 5
Loss after 2590454 batches: 0.0938
trigger times: 0
Loss after 2591391 batches: 0.0937
trigger times: 1
Loss after 2592328 batches: 0.0935
trigger times: 0
Loss after 2593265 batches: 0.0926
trigger times: 1
Loss after 2594202 batches: 0.0918
trigger times: 2
Loss after 2595139 batches: 0.0902
trigger times: 3
Loss after 2596076 batches: 0.0894
trigger times: 4
Loss after 2597013 batches: 0.0883
trigger times: 5
Loss after 2597950 batches: 0.0869
trigger times: 6
Loss after 2598887 batches: 0.0864
trigger times: 7
Loss after 2599824 batches: 0.0846
trigger times: 8
Loss after 2600761 batches: 0.0848
trigger times: 9
Loss after 2601698 batches: 0.0846
trigger times: 10
Loss after 2602635 batches: 0.0830
trigger times: 11
Loss after 2603572 batches: 0.0822
trigger times: 12
Loss after 2604509 batches: 0.0805
trigger times: 13
Loss after 2605446 batches: 0.0791
trigger times: 14
Loss after 2606383 batches: 0.0779
trigger times: 15
Loss after 2607320 batches: 0.0776
trigger times: 16
Loss after 2608257 batches: 0.0759
trigger times: 17
Loss after 2609194 batches: 0.0743
trigger times: 18
Loss after 2610131 batches: 0.0739
trigger times: 19
Loss after 2611068 batches: 0.0724
trigger times: 20
Loss after 2612005 batches: 0.0717
trigger times: 21
Loss after 2612942 batches: 0.0713
trigger times: 22
Loss after 2613879 batches: 0.0687
trigger times: 23
Loss after 2614816 batches: 0.0703
trigger times: 24
Loss after 2615753 batches: 0.0696
trigger times: 25
Early stopping!
Start to test process.
Loss after 2616690 batches: 0.0694
Time to train on one home:  53.773618936538696
train_results:  [0.08213193090377217, 0.05804704250025322]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982]]
Round_1_results:  [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2733 < 2734; dropping {'Training_Loss': 0.07214353233575821, 'Validation_Loss': 0.1359797716140747, 'Training_R2': -0.2850040897269852, 'Validation_R2': 0.005249023090595606, 'Training_F1': 0.2901029048769573, 'Validation_F1': 0.24864555635657215, 'Training_NEP': 0.9209448850740386, 'Validation_NEP': 1.2480530777059011, 'Training_NDE': 0.7825986507871586, 'Validation_NDE': 0.8987339076662941, 'Training_MAE': 34.99620193751091, 'Validation_MAE': 44.379861018921076, 'Training_MSE': 2890.4395, 'Validation_MSE': 11773.424}.
trigger times: 0
Loss after 2617652 batches: 0.0721
trigger times: 1
Loss after 2618614 batches: 0.0709
trigger times: 2
Loss after 2619576 batches: 0.0697
trigger times: 3
Loss after 2620538 batches: 0.0693
trigger times: 4
Loss after 2621500 batches: 0.0690
trigger times: 5
Loss after 2622462 batches: 0.0687
trigger times: 6
Loss after 2623424 batches: 0.0682
trigger times: 7
Loss after 2624386 batches: 0.0679
trigger times: 8
Loss after 2625348 batches: 0.0671
trigger times: 9
Loss after 2626310 batches: 0.0682
trigger times: 0
Loss after 2627272 batches: 0.0669
trigger times: 0
Loss after 2628234 batches: 0.0664
trigger times: 1
Loss after 2629196 batches: 0.0666
trigger times: 2
Loss after 2630158 batches: 0.0659
trigger times: 3
Loss after 2631120 batches: 0.0662
trigger times: 4
Loss after 2632082 batches: 0.0657
trigger times: 5
Loss after 2633044 batches: 0.0646
trigger times: 0
Loss after 2634006 batches: 0.0642
trigger times: 1
Loss after 2634968 batches: 0.0647
trigger times: 0
Loss after 2635930 batches: 0.0634
trigger times: 1
Loss after 2636892 batches: 0.0631
trigger times: 0
Loss after 2637854 batches: 0.0624
trigger times: 0
Loss after 2638816 batches: 0.0618
trigger times: 0
Loss after 2639778 batches: 0.0622
trigger times: 0
Loss after 2640740 batches: 0.0624
trigger times: 1
Loss after 2641702 batches: 0.0637
trigger times: 2
Loss after 2642664 batches: 0.0629
trigger times: 3
Loss after 2643626 batches: 0.0615
trigger times: 4
Loss after 2644588 batches: 0.0616
trigger times: 5
Loss after 2645550 batches: 0.0617
trigger times: 6
Loss after 2646512 batches: 0.0610
trigger times: 7
Loss after 2647474 batches: 0.0611
trigger times: 8
Loss after 2648436 batches: 0.0624
trigger times: 9
Loss after 2649398 batches: 0.0620
trigger times: 0
Loss after 2650360 batches: 0.0612
trigger times: 1
Loss after 2651322 batches: 0.0614
trigger times: 2
Loss after 2652284 batches: 0.0619
trigger times: 3
Loss after 2653246 batches: 0.0612
trigger times: 4
Loss after 2654208 batches: 0.0608
trigger times: 5
Loss after 2655170 batches: 0.0608
trigger times: 6
Loss after 2656132 batches: 0.0608
trigger times: 7
Loss after 2657094 batches: 0.0602
trigger times: 8
Loss after 2658056 batches: 0.0597
trigger times: 9
Loss after 2659018 batches: 0.0596
trigger times: 10
Loss after 2659980 batches: 0.0597
trigger times: 11
Loss after 2660942 batches: 0.0596
trigger times: 12
Loss after 2661904 batches: 0.0595
trigger times: 13
Loss after 2662866 batches: 0.0593
trigger times: 14
Loss after 2663828 batches: 0.0589
trigger times: 15
Loss after 2664790 batches: 0.0590
trigger times: 16
Loss after 2665752 batches: 0.0594
trigger times: 17
Loss after 2666714 batches: 0.0585
trigger times: 18
Loss after 2667676 batches: 0.0590
trigger times: 19
Loss after 2668638 batches: 0.0592
trigger times: 20
Loss after 2669600 batches: 0.0590
trigger times: 21
Loss after 2670562 batches: 0.0583
trigger times: 22
Loss after 2671524 batches: 0.0586
trigger times: 23
Loss after 2672486 batches: 0.0588
trigger times: 24
Loss after 2673448 batches: 0.0586
trigger times: 25
Early stopping!
Start to test process.
Loss after 2674410 batches: 0.0590
Time to train on one home:  70.49964165687561
trigger times: 0
Loss after 2675339 batches: 0.1336
trigger times: 0
Loss after 2676268 batches: 0.0975
trigger times: 1
Loss after 2677197 batches: 0.0830
trigger times: 2
Loss after 2678126 batches: 0.0698
trigger times: 3
Loss after 2679055 batches: 0.0619
trigger times: 4
Loss after 2679984 batches: 0.0580
trigger times: 5
Loss after 2680913 batches: 0.0532
trigger times: 6
Loss after 2681842 batches: 0.0503
trigger times: 7
Loss after 2682771 batches: 0.0478
trigger times: 8
Loss after 2683700 batches: 0.0458
trigger times: 9
Loss after 2684629 batches: 0.0462
trigger times: 10
Loss after 2685558 batches: 0.0444
trigger times: 11
Loss after 2686487 batches: 0.0433
trigger times: 12
Loss after 2687416 batches: 0.0426
trigger times: 13
Loss after 2688345 batches: 0.0438
trigger times: 14
Loss after 2689274 batches: 0.0424
trigger times: 15
Loss after 2690203 batches: 0.0419
trigger times: 16
Loss after 2691132 batches: 0.0436
trigger times: 17
Loss after 2692061 batches: 0.0435
trigger times: 18
Loss after 2692990 batches: 0.0412
trigger times: 19
Loss after 2693919 batches: 0.0406
trigger times: 20
Loss after 2694848 batches: 0.0397
trigger times: 21
Loss after 2695777 batches: 0.0398
trigger times: 22
Loss after 2696706 batches: 0.0358
trigger times: 23
Loss after 2697635 batches: 0.0374
trigger times: 24
Loss after 2698564 batches: 0.0386
trigger times: 25
Early stopping!
Start to test process.
Loss after 2699493 batches: 0.0390
Time to train on one home:  47.09024930000305
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 2700456 batches: 0.0305
trigger times: 1
Loss after 2701419 batches: 0.0173
trigger times: 2
Loss after 2702382 batches: 0.0171
trigger times: 3
Loss after 2703345 batches: 0.0156
trigger times: 4
Loss after 2704308 batches: 0.0148
trigger times: 5
Loss after 2705271 batches: 0.0144
trigger times: 6
Loss after 2706234 batches: 0.0142
trigger times: 7
Loss after 2707197 batches: 0.0142
trigger times: 8
Loss after 2708160 batches: 0.0141
trigger times: 9
Loss after 2709123 batches: 0.0142
trigger times: 10
Loss after 2710086 batches: 0.0139
trigger times: 11
Loss after 2711049 batches: 0.0138
trigger times: 12
Loss after 2712012 batches: 0.0138
trigger times: 13
Loss after 2712975 batches: 0.0140
trigger times: 14
Loss after 2713938 batches: 0.0140
trigger times: 15
Loss after 2714901 batches: 0.0140
trigger times: 16
Loss after 2715864 batches: 0.0140
trigger times: 17
Loss after 2716827 batches: 0.0136
trigger times: 18
Loss after 2717790 batches: 0.0140
trigger times: 19
Loss after 2718753 batches: 0.0141
trigger times: 20
Loss after 2719716 batches: 0.0140
trigger times: 21
Loss after 2720679 batches: 0.0139
trigger times: 22
Loss after 2721642 batches: 0.0138
trigger times: 23
Loss after 2722605 batches: 0.0140
trigger times: 24
Loss after 2723568 batches: 0.0139
trigger times: 25
Early stopping!
Start to test process.
Loss after 2724531 batches: 0.0140
Time to train on one home:  47.07791471481323
trigger times: 0
Loss after 2725494 batches: 0.0471
trigger times: 0
Loss after 2726457 batches: 0.0457
trigger times: 1
Loss after 2727420 batches: 0.0448
trigger times: 0
Loss after 2728383 batches: 0.0439
trigger times: 1
Loss after 2729346 batches: 0.0433
trigger times: 0
Loss after 2730309 batches: 0.0424
trigger times: 0
Loss after 2731272 batches: 0.0407
trigger times: 1
Loss after 2732235 batches: 0.0388
trigger times: 0
Loss after 2733198 batches: 0.0373
trigger times: 1
Loss after 2734161 batches: 0.0364
trigger times: 0
Loss after 2735124 batches: 0.0346
trigger times: 1
Loss after 2736087 batches: 0.0338
trigger times: 0
Loss after 2737050 batches: 0.0332
trigger times: 0
Loss after 2738013 batches: 0.0325
trigger times: 0
Loss after 2738976 batches: 0.0319
trigger times: 0
Loss after 2739939 batches: 0.0325
trigger times: 0
Loss after 2740902 batches: 0.0314
trigger times: 0
Loss after 2741865 batches: 0.0305
trigger times: 1
Loss after 2742828 batches: 0.0293
trigger times: 0
Loss after 2743791 batches: 0.0290
trigger times: 1
Loss after 2744754 batches: 0.0292
trigger times: 2
Loss after 2745717 batches: 0.0295
trigger times: 3
Loss after 2746680 batches: 0.0291
trigger times: 4
Loss after 2747643 batches: 0.0281
trigger times: 0
Loss after 2748606 batches: 0.0279
trigger times: 1
Loss after 2749569 batches: 0.0276
trigger times: 2
Loss after 2750532 batches: 0.0278
trigger times: 0
Loss after 2751495 batches: 0.0273
trigger times: 1
Loss after 2752458 batches: 0.0263
trigger times: 0
Loss after 2753421 batches: 0.0261
trigger times: 1
Loss after 2754384 batches: 0.0253
trigger times: 2
Loss after 2755347 batches: 0.0253
trigger times: 3
Loss after 2756310 batches: 0.0248
trigger times: 4
Loss after 2757273 batches: 0.0247
trigger times: 5
Loss after 2758236 batches: 0.0248
trigger times: 6
Loss after 2759199 batches: 0.0243
trigger times: 0
Loss after 2760162 batches: 0.0248
trigger times: 1
Loss after 2761125 batches: 0.0237
trigger times: 2
Loss after 2762088 batches: 0.0232
trigger times: 3
Loss after 2763051 batches: 0.0233
trigger times: 4
Loss after 2764014 batches: 0.0229
trigger times: 0
Loss after 2764977 batches: 0.0222
trigger times: 1
Loss after 2765940 batches: 0.0221
trigger times: 0
Loss after 2766903 batches: 0.0222
trigger times: 1
Loss after 2767866 batches: 0.0223
trigger times: 2
Loss after 2768829 batches: 0.0217
trigger times: 0
Loss after 2769792 batches: 0.0217
trigger times: 1
Loss after 2770755 batches: 0.0210
trigger times: 2
Loss after 2771718 batches: 0.0211
trigger times: 3
Loss after 2772681 batches: 0.0208
trigger times: 0
Loss after 2773644 batches: 0.0214
trigger times: 1
Loss after 2774607 batches: 0.0202
trigger times: 2
Loss after 2775570 batches: 0.0208
trigger times: 3
Loss after 2776533 batches: 0.0210
trigger times: 4
Loss after 2777496 batches: 0.0210
trigger times: 0
Loss after 2778459 batches: 0.0206
trigger times: 1
Loss after 2779422 batches: 0.0211
trigger times: 2
Loss after 2780385 batches: 0.0202
trigger times: 0
Loss after 2781348 batches: 0.0208
trigger times: 1
Loss after 2782311 batches: 0.0205
trigger times: 2
Loss after 2783274 batches: 0.0203
trigger times: 3
Loss after 2784237 batches: 0.0200
trigger times: 4
Loss after 2785200 batches: 0.0196
trigger times: 5
Loss after 2786163 batches: 0.0197
trigger times: 6
Loss after 2787126 batches: 0.0195
trigger times: 7
Loss after 2788089 batches: 0.0193
trigger times: 8
Loss after 2789052 batches: 0.0193
trigger times: 9
Loss after 2790015 batches: 0.0191
trigger times: 10
Loss after 2790978 batches: 0.0191
trigger times: 11
Loss after 2791941 batches: 0.0187
trigger times: 12
Loss after 2792904 batches: 0.0186
trigger times: 13
Loss after 2793867 batches: 0.0187
trigger times: 14
Loss after 2794830 batches: 0.0184
trigger times: 15
Loss after 2795793 batches: 0.0183
trigger times: 16
Loss after 2796756 batches: 0.0182
trigger times: 17
Loss after 2797719 batches: 0.0178
trigger times: 0
Loss after 2798682 batches: 0.0181
trigger times: 1
Loss after 2799645 batches: 0.0181
trigger times: 2
Loss after 2800608 batches: 0.0179
trigger times: 3
Loss after 2801571 batches: 0.0182
trigger times: 4
Loss after 2802534 batches: 0.0175
trigger times: 5
Loss after 2803497 batches: 0.0174
trigger times: 6
Loss after 2804460 batches: 0.0175
trigger times: 7
Loss after 2805423 batches: 0.0172
trigger times: 8
Loss after 2806386 batches: 0.0174
trigger times: 9
Loss after 2807349 batches: 0.0171
trigger times: 10
Loss after 2808312 batches: 0.0171
trigger times: 0
Loss after 2809275 batches: 0.0170
trigger times: 1
Loss after 2810238 batches: 0.0168
trigger times: 0
Loss after 2811201 batches: 0.0172
trigger times: 1
Loss after 2812164 batches: 0.0170
trigger times: 2
Loss after 2813127 batches: 0.0165
trigger times: 3
Loss after 2814090 batches: 0.0166
trigger times: 4
Loss after 2815053 batches: 0.0167
trigger times: 5
Loss after 2816016 batches: 0.0165
trigger times: 6
Loss after 2816979 batches: 0.0163
trigger times: 7
Loss after 2817942 batches: 0.0165
trigger times: 8
Loss after 2818905 batches: 0.0162
trigger times: 9
Loss after 2819868 batches: 0.0164
trigger times: 10
Loss after 2820831 batches: 0.0163
trigger times: 11
Loss after 2821794 batches: 0.0164
trigger times: 12
Loss after 2822757 batches: 0.0164
trigger times: 0
Loss after 2823720 batches: 0.0169
trigger times: 1
Loss after 2824683 batches: 0.0162
trigger times: 2
Loss after 2825646 batches: 0.0164
trigger times: 0
Loss after 2826609 batches: 0.0160
trigger times: 1
Loss after 2827572 batches: 0.0156
trigger times: 2
Loss after 2828535 batches: 0.0158
trigger times: 3
Loss after 2829498 batches: 0.0158
trigger times: 0
Loss after 2830461 batches: 0.0155
trigger times: 1
Loss after 2831424 batches: 0.0155
trigger times: 2
Loss after 2832387 batches: 0.0152
trigger times: 3
Loss after 2833350 batches: 0.0149
trigger times: 0
Loss after 2834313 batches: 0.0149
trigger times: 1
Loss after 2835276 batches: 0.0150
trigger times: 2
Loss after 2836239 batches: 0.0150
trigger times: 3
Loss after 2837202 batches: 0.0144
trigger times: 4
Loss after 2838165 batches: 0.0145
trigger times: 5
Loss after 2839128 batches: 0.0151
trigger times: 6
Loss after 2840091 batches: 0.0151
trigger times: 7
Loss after 2841054 batches: 0.0147
trigger times: 8
Loss after 2842017 batches: 0.0149
trigger times: 9
Loss after 2842980 batches: 0.0147
trigger times: 10
Loss after 2843943 batches: 0.0142
trigger times: 11
Loss after 2844906 batches: 0.0152
trigger times: 12
Loss after 2845869 batches: 0.0145
trigger times: 13
Loss after 2846832 batches: 0.0146
trigger times: 14
Loss after 2847795 batches: 0.0145
trigger times: 15
Loss after 2848758 batches: 0.0140
trigger times: 16
Loss after 2849721 batches: 0.0136
trigger times: 17
Loss after 2850684 batches: 0.0131
trigger times: 18
Loss after 2851647 batches: 0.0136
trigger times: 19
Loss after 2852610 batches: 0.0138
trigger times: 20
Loss after 2853573 batches: 0.0139
trigger times: 21
Loss after 2854536 batches: 0.0138
trigger times: 22
Loss after 2855499 batches: 0.0142
trigger times: 23
Loss after 2856462 batches: 0.0147
trigger times: 24
Loss after 2857425 batches: 0.0140
trigger times: 25
Early stopping!
Start to test process.
Loss after 2858388 batches: 0.0137
Time to train on one home:  126.54350686073303
trigger times: 0
Loss after 2859351 batches: 0.1226
trigger times: 0
Loss after 2860314 batches: 0.1198
trigger times: 1
Loss after 2861277 batches: 0.1179
trigger times: 2
Loss after 2862240 batches: 0.1157
trigger times: 3
Loss after 2863203 batches: 0.1116
trigger times: 4
Loss after 2864166 batches: 0.1080
trigger times: 5
Loss after 2865129 batches: 0.1052
trigger times: 6
Loss after 2866092 batches: 0.1043
trigger times: 7
Loss after 2867055 batches: 0.1019
trigger times: 8
Loss after 2868018 batches: 0.0998
trigger times: 9
Loss after 2868981 batches: 0.0993
trigger times: 10
Loss after 2869944 batches: 0.0984
trigger times: 11
Loss after 2870907 batches: 0.0961
trigger times: 12
Loss after 2871870 batches: 0.0961
trigger times: 13
Loss after 2872833 batches: 0.0955
trigger times: 14
Loss after 2873796 batches: 0.0946
trigger times: 15
Loss after 2874759 batches: 0.0945
trigger times: 16
Loss after 2875722 batches: 0.0940
trigger times: 17
Loss after 2876685 batches: 0.0940
trigger times: 18
Loss after 2877648 batches: 0.0935
trigger times: 19
Loss after 2878611 batches: 0.0923
trigger times: 20
Loss after 2879574 batches: 0.0915
trigger times: 21
Loss after 2880537 batches: 0.0913
trigger times: 22
Loss after 2881500 batches: 0.0905
trigger times: 23
Loss after 2882463 batches: 0.0905
trigger times: 24
Loss after 2883426 batches: 0.0893
trigger times: 25
Early stopping!
Start to test process.
Loss after 2884389 batches: 0.0887
Time to train on one home:  47.826233863830566
trigger times: 0
Loss after 2885352 batches: 0.1074
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 2886315 batches: 0.0997
trigger times: 0
Loss after 2887278 batches: 0.0977
trigger times: 1
Loss after 2888241 batches: 0.0941
trigger times: 2
Loss after 2889204 batches: 0.0900
trigger times: 3
Loss after 2890167 batches: 0.0879
trigger times: 4
Loss after 2891130 batches: 0.0850
trigger times: 5
Loss after 2892093 batches: 0.0829
trigger times: 6
Loss after 2893056 batches: 0.0819
trigger times: 7
Loss after 2894019 batches: 0.0814
trigger times: 8
Loss after 2894982 batches: 0.0811
trigger times: 9
Loss after 2895945 batches: 0.0810
trigger times: 10
Loss after 2896908 batches: 0.0801
trigger times: 11
Loss after 2897871 batches: 0.0803
trigger times: 12
Loss after 2898834 batches: 0.0795
trigger times: 13
Loss after 2899797 batches: 0.0783
trigger times: 14
Loss after 2900760 batches: 0.0784
trigger times: 15
Loss after 2901723 batches: 0.0770
trigger times: 16
Loss after 2902686 batches: 0.0766
trigger times: 17
Loss after 2903649 batches: 0.0756
trigger times: 18
Loss after 2904612 batches: 0.0751
trigger times: 19
Loss after 2905575 batches: 0.0743
trigger times: 20
Loss after 2906538 batches: 0.0738
trigger times: 21
Loss after 2907501 batches: 0.0744
trigger times: 22
Loss after 2908464 batches: 0.0734
trigger times: 23
Loss after 2909427 batches: 0.0728
trigger times: 24
Loss after 2910390 batches: 0.0726
trigger times: 25
Early stopping!
Start to test process.
Loss after 2911353 batches: 0.0726
Time to train on one home:  48.6932532787323
trigger times: 0
Loss after 2912316 batches: 0.0964
trigger times: 0
Loss after 2913279 batches: 0.0956
trigger times: 1
Loss after 2914242 batches: 0.0952
trigger times: 0
Loss after 2915205 batches: 0.0940
trigger times: 1
Loss after 2916168 batches: 0.0936
trigger times: 0
Loss after 2917131 batches: 0.0931
trigger times: 0
Loss after 2918094 batches: 0.0906
trigger times: 1
Loss after 2919057 batches: 0.0890
trigger times: 2
Loss after 2920020 batches: 0.0845
trigger times: 3
Loss after 2920983 batches: 0.0826
trigger times: 4
Loss after 2921946 batches: 0.0812
trigger times: 5
Loss after 2922909 batches: 0.0806
trigger times: 6
Loss after 2923872 batches: 0.0786
trigger times: 7
Loss after 2924835 batches: 0.0772
trigger times: 8
Loss after 2925798 batches: 0.0757
trigger times: 9
Loss after 2926761 batches: 0.0750
trigger times: 10
Loss after 2927724 batches: 0.0735
trigger times: 11
Loss after 2928687 batches: 0.0753
trigger times: 12
Loss after 2929650 batches: 0.0733
trigger times: 13
Loss after 2930613 batches: 0.0737
trigger times: 14
Loss after 2931576 batches: 0.0727
trigger times: 15
Loss after 2932539 batches: 0.0718
trigger times: 16
Loss after 2933502 batches: 0.0694
trigger times: 17
Loss after 2934465 batches: 0.0702
trigger times: 18
Loss after 2935428 batches: 0.0696
trigger times: 19
Loss after 2936391 batches: 0.0694
trigger times: 20
Loss after 2937354 batches: 0.0687
trigger times: 21
Loss after 2938317 batches: 0.0689
trigger times: 0
Loss after 2939280 batches: 0.0684
trigger times: 0
Loss after 2940243 batches: 0.0682
trigger times: 1
Loss after 2941206 batches: 0.0685
trigger times: 2
Loss after 2942169 batches: 0.0705
trigger times: 3
Loss after 2943132 batches: 0.0697
trigger times: 0
Loss after 2944095 batches: 0.0688
trigger times: 1
Loss after 2945058 batches: 0.0679
trigger times: 0
Loss after 2946021 batches: 0.0666
trigger times: 1
Loss after 2946984 batches: 0.0658
trigger times: 2
Loss after 2947947 batches: 0.0667
trigger times: 3
Loss after 2948910 batches: 0.0661
trigger times: 4
Loss after 2949873 batches: 0.0665
trigger times: 5
Loss after 2950836 batches: 0.0656
trigger times: 6
Loss after 2951799 batches: 0.0655
trigger times: 7
Loss after 2952762 batches: 0.0643
trigger times: 8
Loss after 2953725 batches: 0.0634
trigger times: 9
Loss after 2954688 batches: 0.0636
trigger times: 10
Loss after 2955651 batches: 0.0618
trigger times: 11
Loss after 2956614 batches: 0.0627
trigger times: 12
Loss after 2957577 batches: 0.0631
trigger times: 13
Loss after 2958540 batches: 0.0626
trigger times: 14
Loss after 2959503 batches: 0.0613
trigger times: 15
Loss after 2960466 batches: 0.0618
trigger times: 16
Loss after 2961429 batches: 0.0623
trigger times: 17
Loss after 2962392 batches: 0.0620
trigger times: 18
Loss after 2963355 batches: 0.0621
trigger times: 19
Loss after 2964318 batches: 0.0630
trigger times: 20
Loss after 2965281 batches: 0.0616
trigger times: 21
Loss after 2966244 batches: 0.0614
trigger times: 22
Loss after 2967207 batches: 0.0607
trigger times: 23
Loss after 2968170 batches: 0.0600
trigger times: 24
Loss after 2969133 batches: 0.0603
trigger times: 25
Early stopping!
Start to test process.
Loss after 2970096 batches: 0.0603
Time to train on one home:  71.60940623283386
trigger times: 0
Loss after 2971059 batches: 0.0671
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 2972022 batches: 0.0658
trigger times: 1
Loss after 2972985 batches: 0.0647
trigger times: 2
Loss after 2973948 batches: 0.0637
trigger times: 3
Loss after 2974911 batches: 0.0622
trigger times: 0
Loss after 2975874 batches: 0.0608
trigger times: 1
Loss after 2976837 batches: 0.0592
trigger times: 0
Loss after 2977800 batches: 0.0566
trigger times: 0
Loss after 2978763 batches: 0.0546
trigger times: 1
Loss after 2979726 batches: 0.0533
trigger times: 2
Loss after 2980689 batches: 0.0521
trigger times: 3
Loss after 2981652 batches: 0.0509
trigger times: 0
Loss after 2982615 batches: 0.0503
trigger times: 0
Loss after 2983578 batches: 0.0496
trigger times: 0
Loss after 2984541 batches: 0.0490
trigger times: 0
Loss after 2985504 batches: 0.0481
trigger times: 0
Loss after 2986467 batches: 0.0478
trigger times: 0
Loss after 2987430 batches: 0.0475
trigger times: 0
Loss after 2988393 batches: 0.0467
trigger times: 1
Loss after 2989356 batches: 0.0465
trigger times: 2
Loss after 2990319 batches: 0.0460
trigger times: 3
Loss after 2991282 batches: 0.0451
trigger times: 0
Loss after 2992245 batches: 0.0443
trigger times: 1
Loss after 2993208 batches: 0.0445
trigger times: 2
Loss after 2994171 batches: 0.0436
trigger times: 3
Loss after 2995134 batches: 0.0422
trigger times: 4
Loss after 2996097 batches: 0.0410
trigger times: 5
Loss after 2997060 batches: 0.0404
trigger times: 6
Loss after 2998023 batches: 0.0407
trigger times: 7
Loss after 2998986 batches: 0.0396
trigger times: 8
Loss after 2999949 batches: 0.0393
trigger times: 9
Loss after 3000912 batches: 0.0389
trigger times: 10
Loss after 3001875 batches: 0.0392
trigger times: 11
Loss after 3002838 batches: 0.0387
trigger times: 12
Loss after 3003801 batches: 0.0376
trigger times: 13
Loss after 3004764 batches: 0.0378
trigger times: 14
Loss after 3005727 batches: 0.0369
trigger times: 15
Loss after 3006690 batches: 0.0365
trigger times: 16
Loss after 3007653 batches: 0.0361
trigger times: 17
Loss after 3008616 batches: 0.0355
trigger times: 18
Loss after 3009579 batches: 0.0356
trigger times: 19
Loss after 3010542 batches: 0.0353
trigger times: 20
Loss after 3011505 batches: 0.0358
trigger times: 21
Loss after 3012468 batches: 0.0349
trigger times: 22
Loss after 3013431 batches: 0.0352
trigger times: 23
Loss after 3014394 batches: 0.0337
trigger times: 24
Loss after 3015357 batches: 0.0336
trigger times: 25
Early stopping!
Start to test process.
Loss after 3016320 batches: 0.0350
Time to train on one home:  62.280845642089844
trigger times: 0
Loss after 3017278 batches: 0.1309
trigger times: 0
Loss after 3018236 batches: 0.0992
trigger times: 1
Loss after 3019194 batches: 0.0813
trigger times: 2
Loss after 3020152 batches: 0.0698
trigger times: 3
Loss after 3021110 batches: 0.0669
trigger times: 4
Loss after 3022068 batches: 0.0617
trigger times: 5
Loss after 3023026 batches: 0.0556
trigger times: 6
Loss after 3023984 batches: 0.0484
trigger times: 7
Loss after 3024942 batches: 0.0449
trigger times: 8
Loss after 3025900 batches: 0.0441
trigger times: 9
Loss after 3026858 batches: 0.0433
trigger times: 10
Loss after 3027816 batches: 0.0417
trigger times: 11
Loss after 3028774 batches: 0.0402
trigger times: 12
Loss after 3029732 batches: 0.0405
trigger times: 13
Loss after 3030690 batches: 0.0386
trigger times: 14
Loss after 3031648 batches: 0.0381
trigger times: 15
Loss after 3032606 batches: 0.0363
trigger times: 16
Loss after 3033564 batches: 0.0349
trigger times: 17
Loss after 3034522 batches: 0.0336
trigger times: 18
Loss after 3035480 batches: 0.0325
trigger times: 19
Loss after 3036438 batches: 0.0316
trigger times: 20
Loss after 3037396 batches: 0.0320
trigger times: 21
Loss after 3038354 batches: 0.0305
trigger times: 22
Loss after 3039312 batches: 0.0324
trigger times: 23
Loss after 3040270 batches: 0.0293
trigger times: 24
Loss after 3041228 batches: 0.0321
trigger times: 25
Early stopping!
Start to test process.
Loss after 3042186 batches: 0.0304
Time to train on one home:  47.618579626083374
trigger times: 0
Loss after 3043148 batches: 0.0720
trigger times: 1
Loss after 3044110 batches: 0.0710
trigger times: 2
Loss after 3045072 batches: 0.0703
trigger times: 3
Loss after 3046034 batches: 0.0694
trigger times: 4
Loss after 3046996 batches: 0.0690
trigger times: 5
Loss after 3047958 batches: 0.0691
trigger times: 6
Loss after 3048920 batches: 0.0683
trigger times: 7
Loss after 3049882 batches: 0.0680
trigger times: 8
Loss after 3050844 batches: 0.0680
trigger times: 0
Loss after 3051806 batches: 0.0681
trigger times: 1
Loss after 3052768 batches: 0.0676
trigger times: 2
Loss after 3053730 batches: 0.0666
trigger times: 3
Loss after 3054692 batches: 0.0660
trigger times: 4
Loss after 3055654 batches: 0.0653
trigger times: 5
Loss after 3056616 batches: 0.0647
trigger times: 0
Loss after 3057578 batches: 0.0647
trigger times: 1
Loss after 3058540 batches: 0.0635
trigger times: 0
Loss after 3059502 batches: 0.0634
trigger times: 0
Loss after 3060464 batches: 0.0625
trigger times: 1
Loss after 3061426 batches: 0.0623
trigger times: 2
Loss after 3062388 batches: 0.0623
trigger times: 3
Loss after 3063350 batches: 0.0621
trigger times: 4
Loss after 3064312 batches: 0.0625
trigger times: 5
Loss after 3065274 batches: 0.0617
trigger times: 6
Loss after 3066236 batches: 0.0616
trigger times: 0
Loss after 3067198 batches: 0.0617
trigger times: 1
Loss after 3068160 batches: 0.0620
trigger times: 2
Loss after 3069122 batches: 0.0624
trigger times: 3
Loss after 3070084 batches: 0.0615
trigger times: 4
Loss after 3071046 batches: 0.0609
trigger times: 5
Loss after 3072008 batches: 0.0631
trigger times: 6
Loss after 3072970 batches: 0.0616
trigger times: 7
Loss after 3073932 batches: 0.0618
trigger times: 8
Loss after 3074894 batches: 0.0614
trigger times: 9
Loss after 3075856 batches: 0.0609
trigger times: 10
Loss after 3076818 batches: 0.0601
trigger times: 11
Loss after 3077780 batches: 0.0598
trigger times: 12
Loss after 3078742 batches: 0.0602
trigger times: 13
Loss after 3079704 batches: 0.0614
trigger times: 14
Loss after 3080666 batches: 0.0601
trigger times: 15
Loss after 3081628 batches: 0.0597
trigger times: 16
Loss after 3082590 batches: 0.0604
trigger times: 17
Loss after 3083552 batches: 0.0594
trigger times: 18
Loss after 3084514 batches: 0.0598
trigger times: 19
Loss after 3085476 batches: 0.0593
trigger times: 20
Loss after 3086438 batches: 0.0597
trigger times: 0
Loss after 3087400 batches: 0.0596
trigger times: 1
Loss after 3088362 batches: 0.0604
trigger times: 2
Loss after 3089324 batches: 0.0591
trigger times: 3
Loss after 3090286 batches: 0.0594
trigger times: 4
Loss after 3091248 batches: 0.0592
trigger times: 5
Loss after 3092210 batches: 0.0635
trigger times: 6
Loss after 3093172 batches: 0.0630
trigger times: 7
Loss after 3094134 batches: 0.0618
trigger times: 8
Loss after 3095096 batches: 0.0613
trigger times: 9
Loss after 3096058 batches: 0.0612
trigger times: 10
Loss after 3097020 batches: 0.0598
trigger times: 11
Loss after 3097982 batches: 0.0587
trigger times: 12
Loss after 3098944 batches: 0.0589
trigger times: 13
Loss after 3099906 batches: 0.0589
trigger times: 14
Loss after 3100868 batches: 0.0587
trigger times: 15
Loss after 3101830 batches: 0.0584
trigger times: 16
Loss after 3102792 batches: 0.0589
trigger times: 17
Loss after 3103754 batches: 0.0585
trigger times: 18
Loss after 3104716 batches: 0.0581
trigger times: 19
Loss after 3105678 batches: 0.0581
trigger times: 20
Loss after 3106640 batches: 0.0584
trigger times: 21
Loss after 3107602 batches: 0.0577
trigger times: 22
Loss after 3108564 batches: 0.0576
trigger times: 23
Loss after 3109526 batches: 0.0581
trigger times: 24
Loss after 3110488 batches: 0.0580
trigger times: 25
Early stopping!
Start to test process.
Loss after 3111450 batches: 0.0569
Time to train on one home:  78.95523309707642
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3112413 batches: 0.0764
trigger times: 1
Loss after 3113376 batches: 0.0380
trigger times: 2
Loss after 3114339 batches: 0.0316
trigger times: 3
Loss after 3115302 batches: 0.0271
trigger times: 4
Loss after 3116265 batches: 0.0242
trigger times: 5
Loss after 3117228 batches: 0.0238
trigger times: 6
Loss after 3118191 batches: 0.0235
trigger times: 7
Loss after 3119154 batches: 0.0233
trigger times: 8
Loss after 3120117 batches: 0.0231
trigger times: 9
Loss after 3121080 batches: 0.0229
trigger times: 10
Loss after 3122043 batches: 0.0231
trigger times: 11
Loss after 3123006 batches: 0.0229
trigger times: 12
Loss after 3123969 batches: 0.0227
trigger times: 13
Loss after 3124932 batches: 0.0227
trigger times: 14
Loss after 3125895 batches: 0.0228
trigger times: 15
Loss after 3126858 batches: 0.0229
trigger times: 16
Loss after 3127821 batches: 0.0225
trigger times: 17
Loss after 3128784 batches: 0.0227
trigger times: 18
Loss after 3129747 batches: 0.0227
trigger times: 19
Loss after 3130710 batches: 0.0227
trigger times: 20
Loss after 3131673 batches: 0.0226
trigger times: 21
Loss after 3132636 batches: 0.0226
trigger times: 22
Loss after 3133599 batches: 0.0228
trigger times: 23
Loss after 3134562 batches: 0.0232
trigger times: 24
Loss after 3135525 batches: 0.0225
trigger times: 25
Early stopping!
Start to test process.
Loss after 3136488 batches: 0.0225
Time to train on one home:  50.0461802482605
trigger times: 0
Loss after 3137451 batches: 0.0925
trigger times: 0
Loss after 3138414 batches: 0.0836
trigger times: 1
Loss after 3139377 batches: 0.0773
trigger times: 2
Loss after 3140340 batches: 0.0764
trigger times: 3
Loss after 3141303 batches: 0.0743
trigger times: 4
Loss after 3142266 batches: 0.0725
trigger times: 5
Loss after 3143229 batches: 0.0702
trigger times: 0
Loss after 3144192 batches: 0.0674
trigger times: 0
Loss after 3145155 batches: 0.0614
trigger times: 0
Loss after 3146118 batches: 0.0573
trigger times: 1
Loss after 3147081 batches: 0.0562
trigger times: 0
Loss after 3148044 batches: 0.0545
trigger times: 1
Loss after 3149007 batches: 0.0533
trigger times: 0
Loss after 3149970 batches: 0.0521
trigger times: 1
Loss after 3150933 batches: 0.0519
trigger times: 2
Loss after 3151896 batches: 0.0514
trigger times: 0
Loss after 3152859 batches: 0.0500
trigger times: 0
Loss after 3153822 batches: 0.0493
trigger times: 1
Loss after 3154785 batches: 0.0475
trigger times: 2
Loss after 3155748 batches: 0.0483
trigger times: 3
Loss after 3156711 batches: 0.0482
trigger times: 4
Loss after 3157674 batches: 0.0478
trigger times: 5
Loss after 3158637 batches: 0.0469
trigger times: 6
Loss after 3159600 batches: 0.0461
trigger times: 0
Loss after 3160563 batches: 0.0452
trigger times: 0
Loss after 3161526 batches: 0.0445
trigger times: 1
Loss after 3162489 batches: 0.0449
trigger times: 0
Loss after 3163452 batches: 0.0445
trigger times: 0
Loss after 3164415 batches: 0.0433
trigger times: 1
Loss after 3165378 batches: 0.0434
trigger times: 2
Loss after 3166341 batches: 0.0438
trigger times: 3
Loss after 3167304 batches: 0.0431
trigger times: 0
Loss after 3168267 batches: 0.0427
trigger times: 1
Loss after 3169230 batches: 0.0431
trigger times: 2
Loss after 3170193 batches: 0.0434
trigger times: 3
Loss after 3171156 batches: 0.0423
trigger times: 4
Loss after 3172119 batches: 0.0421
trigger times: 5
Loss after 3173082 batches: 0.0416
trigger times: 6
Loss after 3174045 batches: 0.0419
trigger times: 7
Loss after 3175008 batches: 0.0422
trigger times: 8
Loss after 3175971 batches: 0.0412
trigger times: 9
Loss after 3176934 batches: 0.0411
trigger times: 10
Loss after 3177897 batches: 0.0411
trigger times: 0
Loss after 3178860 batches: 0.0408
trigger times: 1
Loss after 3179823 batches: 0.0410
trigger times: 2
Loss after 3180786 batches: 0.0422
trigger times: 3
Loss after 3181749 batches: 0.0396
trigger times: 4
Loss after 3182712 batches: 0.0412
trigger times: 5
Loss after 3183675 batches: 0.0404
trigger times: 6
Loss after 3184638 batches: 0.0393
trigger times: 7
Loss after 3185601 batches: 0.0397
trigger times: 8
Loss after 3186564 batches: 0.0394
trigger times: 9
Loss after 3187527 batches: 0.0384
trigger times: 10
Loss after 3188490 batches: 0.0394
trigger times: 11
Loss after 3189453 batches: 0.0390
trigger times: 12
Loss after 3190416 batches: 0.0384
trigger times: 13
Loss after 3191379 batches: 0.0384
trigger times: 14
Loss after 3192342 batches: 0.0384
trigger times: 15
Loss after 3193305 batches: 0.0389
trigger times: 0
Loss after 3194268 batches: 0.0384
trigger times: 1
Loss after 3195231 batches: 0.0383
trigger times: 2
Loss after 3196194 batches: 0.0376
trigger times: 3
Loss after 3197157 batches: 0.0377
trigger times: 4
Loss after 3198120 batches: 0.0375
trigger times: 5
Loss after 3199083 batches: 0.0377
trigger times: 6
Loss after 3200046 batches: 0.0384
trigger times: 7
Loss after 3201009 batches: 0.0377
trigger times: 8
Loss after 3201972 batches: 0.0378
trigger times: 9
Loss after 3202935 batches: 0.0373
trigger times: 10
Loss after 3203898 batches: 0.0368
trigger times: 11
Loss after 3204861 batches: 0.0371
trigger times: 12
Loss after 3205824 batches: 0.0371
trigger times: 13
Loss after 3206787 batches: 0.0377
trigger times: 14
Loss after 3207750 batches: 0.0381
trigger times: 15
Loss after 3208713 batches: 0.0378
trigger times: 16
Loss after 3209676 batches: 0.0367
trigger times: 17
Loss after 3210639 batches: 0.0365
trigger times: 18
Loss after 3211602 batches: 0.0363
trigger times: 19
Loss after 3212565 batches: 0.0364
trigger times: 20
Loss after 3213528 batches: 0.0357
trigger times: 21
Loss after 3214491 batches: 0.0360
trigger times: 22
Loss after 3215454 batches: 0.0364
trigger times: 23
Loss after 3216417 batches: 0.0364
trigger times: 24
Loss after 3217380 batches: 0.0359
trigger times: 25
Early stopping!
Start to test process.
Loss after 3218343 batches: 0.0347
Time to train on one home:  90.99266815185547
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3219306 batches: 0.0672
trigger times: 0
Loss after 3220269 batches: 0.0660
trigger times: 1
Loss after 3221232 batches: 0.0647
trigger times: 2
Loss after 3222195 batches: 0.0629
trigger times: 0
Loss after 3223158 batches: 0.0619
trigger times: 1
Loss after 3224121 batches: 0.0601
trigger times: 0
Loss after 3225084 batches: 0.0585
trigger times: 0
Loss after 3226047 batches: 0.0555
trigger times: 0
Loss after 3227010 batches: 0.0542
trigger times: 1
Loss after 3227973 batches: 0.0527
trigger times: 2
Loss after 3228936 batches: 0.0512
trigger times: 3
Loss after 3229899 batches: 0.0500
trigger times: 0
Loss after 3230862 batches: 0.0497
trigger times: 0
Loss after 3231825 batches: 0.0493
trigger times: 1
Loss after 3232788 batches: 0.0485
trigger times: 2
Loss after 3233751 batches: 0.0481
trigger times: 0
Loss after 3234714 batches: 0.0474
trigger times: 1
Loss after 3235677 batches: 0.0467
trigger times: 0
Loss after 3236640 batches: 0.0460
trigger times: 1
Loss after 3237603 batches: 0.0458
trigger times: 2
Loss after 3238566 batches: 0.0448
trigger times: 0
Loss after 3239529 batches: 0.0439
trigger times: 1
Loss after 3240492 batches: 0.0430
trigger times: 2
Loss after 3241455 batches: 0.0418
trigger times: 3
Loss after 3242418 batches: 0.0410
trigger times: 4
Loss after 3243381 batches: 0.0403
trigger times: 0
Loss after 3244344 batches: 0.0404
trigger times: 0
Loss after 3245307 batches: 0.0396
trigger times: 1
Loss after 3246270 batches: 0.0390
trigger times: 2
Loss after 3247233 batches: 0.0386
trigger times: 3
Loss after 3248196 batches: 0.0385
trigger times: 4
Loss after 3249159 batches: 0.0381
trigger times: 5
Loss after 3250122 batches: 0.0376
trigger times: 6
Loss after 3251085 batches: 0.0375
trigger times: 7
Loss after 3252048 batches: 0.0377
trigger times: 8
Loss after 3253011 batches: 0.0366
trigger times: 9
Loss after 3253974 batches: 0.0367
trigger times: 10
Loss after 3254937 batches: 0.0370
trigger times: 11
Loss after 3255900 batches: 0.0360
trigger times: 12
Loss after 3256863 batches: 0.0349
trigger times: 13
Loss after 3257826 batches: 0.0341
trigger times: 14
Loss after 3258789 batches: 0.0346
trigger times: 15
Loss after 3259752 batches: 0.0339
trigger times: 16
Loss after 3260715 batches: 0.0336
trigger times: 17
Loss after 3261678 batches: 0.0337
trigger times: 18
Loss after 3262641 batches: 0.0336
trigger times: 19
Loss after 3263604 batches: 0.0334
trigger times: 20
Loss after 3264567 batches: 0.0326
trigger times: 21
Loss after 3265530 batches: 0.0322
trigger times: 22
Loss after 3266493 batches: 0.0320
trigger times: 23
Loss after 3267456 batches: 0.0322
trigger times: 24
Loss after 3268419 batches: 0.0316
trigger times: 25
Early stopping!
Start to test process.
Loss after 3269382 batches: 0.0315
Time to train on one home:  67.12219548225403
trigger times: 0
Loss after 3270345 batches: 0.0917
trigger times: 0
Loss after 3271308 batches: 0.0850
trigger times: 1
Loss after 3272271 batches: 0.0769
trigger times: 2
Loss after 3273234 batches: 0.0761
trigger times: 3
Loss after 3274197 batches: 0.0743
trigger times: 4
Loss after 3275160 batches: 0.0736
trigger times: 5
Loss after 3276123 batches: 0.0716
trigger times: 6
Loss after 3277086 batches: 0.0688
trigger times: 7
Loss after 3278049 batches: 0.0680
trigger times: 8
Loss after 3279012 batches: 0.0637
trigger times: 0
Loss after 3279975 batches: 0.0593
trigger times: 1
Loss after 3280938 batches: 0.0576
trigger times: 2
Loss after 3281901 batches: 0.0550
trigger times: 3
Loss after 3282864 batches: 0.0548
trigger times: 4
Loss after 3283827 batches: 0.0538
trigger times: 5
Loss after 3284790 batches: 0.0531
trigger times: 6
Loss after 3285753 batches: 0.0520
trigger times: 7
Loss after 3286716 batches: 0.0529
trigger times: 0
Loss after 3287679 batches: 0.0505
trigger times: 1
Loss after 3288642 batches: 0.0505
trigger times: 2
Loss after 3289605 batches: 0.0504
trigger times: 0
Loss after 3290568 batches: 0.0495
trigger times: 0
Loss after 3291531 batches: 0.0497
trigger times: 1
Loss after 3292494 batches: 0.0479
trigger times: 2
Loss after 3293457 batches: 0.0476
trigger times: 3
Loss after 3294420 batches: 0.0484
trigger times: 4
Loss after 3295383 batches: 0.0466
trigger times: 0
Loss after 3296346 batches: 0.0474
trigger times: 1
Loss after 3297309 batches: 0.0461
trigger times: 2
Loss after 3298272 batches: 0.0457
trigger times: 0
Loss after 3299235 batches: 0.0459
trigger times: 1
Loss after 3300198 batches: 0.0456
trigger times: 0
Loss after 3301161 batches: 0.0458
trigger times: 0
Loss after 3302124 batches: 0.0459
trigger times: 0
Loss after 3303087 batches: 0.0455
trigger times: 0
Loss after 3304050 batches: 0.0452
trigger times: 1
Loss after 3305013 batches: 0.0445
trigger times: 2
Loss after 3305976 batches: 0.0452
trigger times: 3
Loss after 3306939 batches: 0.0442
trigger times: 0
Loss after 3307902 batches: 0.0440
trigger times: 0
Loss after 3308865 batches: 0.0432
trigger times: 1
Loss after 3309828 batches: 0.0428
trigger times: 0
Loss after 3310791 batches: 0.0440
trigger times: 1
Loss after 3311754 batches: 0.0434
trigger times: 0
Loss after 3312717 batches: 0.0436
trigger times: 1
Loss after 3313680 batches: 0.0437
trigger times: 2
Loss after 3314643 batches: 0.0420
trigger times: 3
Loss after 3315606 batches: 0.0419
trigger times: 4
Loss after 3316569 batches: 0.0415
trigger times: 5
Loss after 3317532 batches: 0.0404
trigger times: 6
Loss after 3318495 batches: 0.0399
trigger times: 7
Loss after 3319458 batches: 0.0406
trigger times: 0
Loss after 3320421 batches: 0.0417
trigger times: 1
Loss after 3321384 batches: 0.0402
trigger times: 2
Loss after 3322347 batches: 0.0405
trigger times: 3
Loss after 3323310 batches: 0.0406
trigger times: 0
Loss after 3324273 batches: 0.0404
trigger times: 1
Loss after 3325236 batches: 0.0397
trigger times: 2
Loss after 3326199 batches: 0.0394
trigger times: 3
Loss after 3327162 batches: 0.0409
trigger times: 4
Loss after 3328125 batches: 0.0404
trigger times: 5
Loss after 3329088 batches: 0.0396
trigger times: 6
Loss after 3330051 batches: 0.0386
trigger times: 7
Loss after 3331014 batches: 0.0398
trigger times: 8
Loss after 3331977 batches: 0.0383
trigger times: 9
Loss after 3332940 batches: 0.0385
trigger times: 10
Loss after 3333903 batches: 0.0386
trigger times: 11
Loss after 3334866 batches: 0.0386
trigger times: 12
Loss after 3335829 batches: 0.0391
trigger times: 13
Loss after 3336792 batches: 0.0382
trigger times: 14
Loss after 3337755 batches: 0.0387
trigger times: 15
Loss after 3338718 batches: 0.0379
trigger times: 16
Loss after 3339681 batches: 0.0377
trigger times: 17
Loss after 3340644 batches: 0.0372
trigger times: 18
Loss after 3341607 batches: 0.0372
trigger times: 19
Loss after 3342570 batches: 0.0369
trigger times: 20
Loss after 3343533 batches: 0.0407
trigger times: 21
Loss after 3344496 batches: 0.0395
trigger times: 22
Loss after 3345459 batches: 0.0388
trigger times: 23
Loss after 3346422 batches: 0.0381
trigger times: 24
Loss after 3347385 batches: 0.0370
trigger times: 25
Early stopping!
Start to test process.
Loss after 3348348 batches: 0.0368
Time to train on one home:  89.69641160964966
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3349311 batches: 0.0806
trigger times: 0
Loss after 3350274 batches: 0.0790
trigger times: 1
Loss after 3351237 batches: 0.0762
trigger times: 2
Loss after 3352200 batches: 0.0755
trigger times: 3
Loss after 3353163 batches: 0.0746
trigger times: 4
Loss after 3354126 batches: 0.0724
trigger times: 5
Loss after 3355089 batches: 0.0705
trigger times: 6
Loss after 3356052 batches: 0.0676
trigger times: 7
Loss after 3357015 batches: 0.0646
trigger times: 0
Loss after 3357978 batches: 0.0612
trigger times: 0
Loss after 3358941 batches: 0.0631
trigger times: 0
Loss after 3359904 batches: 0.0598
trigger times: 0
Loss after 3360867 batches: 0.0609
trigger times: 1
Loss after 3361830 batches: 0.0592
trigger times: 2
Loss after 3362793 batches: 0.0578
trigger times: 0
Loss after 3363756 batches: 0.0560
trigger times: 0
Loss after 3364719 batches: 0.0552
trigger times: 0
Loss after 3365682 batches: 0.0551
trigger times: 1
Loss after 3366645 batches: 0.0543
trigger times: 0
Loss after 3367608 batches: 0.0544
trigger times: 0
Loss after 3368571 batches: 0.0538
trigger times: 1
Loss after 3369534 batches: 0.0531
trigger times: 0
Loss after 3370497 batches: 0.0529
trigger times: 0
Loss after 3371460 batches: 0.0522
trigger times: 0
Loss after 3372423 batches: 0.0519
trigger times: 1
Loss after 3373386 batches: 0.0507
trigger times: 0
Loss after 3374349 batches: 0.0515
trigger times: 1
Loss after 3375312 batches: 0.0513
trigger times: 2
Loss after 3376275 batches: 0.0512
trigger times: 0
Loss after 3377238 batches: 0.0502
trigger times: 1
Loss after 3378201 batches: 0.0498
trigger times: 0
Loss after 3379164 batches: 0.0500
trigger times: 0
Loss after 3380127 batches: 0.0494
trigger times: 1
Loss after 3381090 batches: 0.0487
trigger times: 2
Loss after 3382053 batches: 0.0484
trigger times: 0
Loss after 3383016 batches: 0.0477
trigger times: 1
Loss after 3383979 batches: 0.0479
trigger times: 0
Loss after 3384942 batches: 0.0478
trigger times: 1
Loss after 3385905 batches: 0.0475
trigger times: 2
Loss after 3386868 batches: 0.0473
trigger times: 3
Loss after 3387831 batches: 0.0478
trigger times: 4
Loss after 3388794 batches: 0.0463
trigger times: 5
Loss after 3389757 batches: 0.0464
trigger times: 0
Loss after 3390720 batches: 0.0458
trigger times: 1
Loss after 3391683 batches: 0.0465
trigger times: 2
Loss after 3392646 batches: 0.0453
trigger times: 3
Loss after 3393609 batches: 0.0458
trigger times: 4
Loss after 3394572 batches: 0.0447
trigger times: 5
Loss after 3395535 batches: 0.0452
trigger times: 0
Loss after 3396498 batches: 0.0446
trigger times: 1
Loss after 3397461 batches: 0.0445
trigger times: 2
Loss after 3398424 batches: 0.0442
trigger times: 0
Loss after 3399387 batches: 0.0444
trigger times: 1
Loss after 3400350 batches: 0.0446
trigger times: 2
Loss after 3401313 batches: 0.0438
trigger times: 0
Loss after 3402276 batches: 0.0434
trigger times: 1
Loss after 3403239 batches: 0.0436
trigger times: 2
Loss after 3404202 batches: 0.0437
trigger times: 3
Loss after 3405165 batches: 0.0453
trigger times: 4
Loss after 3406128 batches: 0.0455
trigger times: 5
Loss after 3407091 batches: 0.0448
trigger times: 6
Loss after 3408054 batches: 0.0442
trigger times: 0
Loss after 3409017 batches: 0.0424
trigger times: 0
Loss after 3409980 batches: 0.0423
trigger times: 1
Loss after 3410943 batches: 0.0414
trigger times: 2
Loss after 3411906 batches: 0.0413
trigger times: 3
Loss after 3412869 batches: 0.0424
trigger times: 4
Loss after 3413832 batches: 0.0417
trigger times: 5
Loss after 3414795 batches: 0.0419
trigger times: 0
Loss after 3415758 batches: 0.0408
trigger times: 1
Loss after 3416721 batches: 0.0407
trigger times: 2
Loss after 3417684 batches: 0.0416
trigger times: 3
Loss after 3418647 batches: 0.0417
trigger times: 4
Loss after 3419610 batches: 0.0412
trigger times: 0
Loss after 3420573 batches: 0.0408
trigger times: 1
Loss after 3421536 batches: 0.0409
trigger times: 2
Loss after 3422499 batches: 0.0412
trigger times: 3
Loss after 3423462 batches: 0.0415
trigger times: 4
Loss after 3424425 batches: 0.0404
trigger times: 5
Loss after 3425388 batches: 0.0397
trigger times: 6
Loss after 3426351 batches: 0.0400
trigger times: 0
Loss after 3427314 batches: 0.0402
trigger times: 1
Loss after 3428277 batches: 0.0394
trigger times: 2
Loss after 3429240 batches: 0.0395
trigger times: 3
Loss after 3430203 batches: 0.0392
trigger times: 4
Loss after 3431166 batches: 0.0402
trigger times: 5
Loss after 3432129 batches: 0.0391
trigger times: 6
Loss after 3433092 batches: 0.0389
trigger times: 7
Loss after 3434055 batches: 0.0388
trigger times: 8
Loss after 3435018 batches: 0.0384
trigger times: 9
Loss after 3435981 batches: 0.0386
trigger times: 10
Loss after 3436944 batches: 0.0381
trigger times: 11
Loss after 3437907 batches: 0.0384
trigger times: 12
Loss after 3438870 batches: 0.0391
trigger times: 13
Loss after 3439833 batches: 0.0381
trigger times: 14
Loss after 3440796 batches: 0.0380
trigger times: 15
Loss after 3441759 batches: 0.0381
trigger times: 16
Loss after 3442722 batches: 0.0366
trigger times: 17
Loss after 3443685 batches: 0.0370
trigger times: 18
Loss after 3444648 batches: 0.0368
trigger times: 19
Loss after 3445611 batches: 0.0370
trigger times: 20
Loss after 3446574 batches: 0.0366
trigger times: 21
Loss after 3447537 batches: 0.0366
trigger times: 22
Loss after 3448500 batches: 0.0371
trigger times: 23
Loss after 3449463 batches: 0.0367
trigger times: 24
Loss after 3450426 batches: 0.0358
trigger times: 25
Early stopping!
Start to test process.
Loss after 3451389 batches: 0.0368
Time to train on one home:  105.4909257888794
trigger times: 0
Loss after 3452352 batches: 0.0689
trigger times: 1
Loss after 3453315 batches: 0.0533
trigger times: 2
Loss after 3454278 batches: 0.0553
trigger times: 3
Loss after 3455241 batches: 0.0543
trigger times: 4
Loss after 3456204 batches: 0.0533
trigger times: 5
Loss after 3457167 batches: 0.0516
trigger times: 6
Loss after 3458130 batches: 0.0497
trigger times: 7
Loss after 3459093 batches: 0.0492
trigger times: 8
Loss after 3460056 batches: 0.0480
trigger times: 9
Loss after 3461019 batches: 0.0480
trigger times: 10
Loss after 3461982 batches: 0.0475
trigger times: 11
Loss after 3462945 batches: 0.0475
trigger times: 12
Loss after 3463908 batches: 0.0459
trigger times: 13
Loss after 3464871 batches: 0.0455
trigger times: 14
Loss after 3465834 batches: 0.0456
trigger times: 15
Loss after 3466797 batches: 0.0444
trigger times: 16
Loss after 3467760 batches: 0.0440
trigger times: 17
Loss after 3468723 batches: 0.0442
trigger times: 18
Loss after 3469686 batches: 0.0437
trigger times: 19
Loss after 3470649 batches: 0.0431
trigger times: 20
Loss after 3471612 batches: 0.0429
trigger times: 21
Loss after 3472575 batches: 0.0429
trigger times: 22
Loss after 3473538 batches: 0.0428
trigger times: 23
Loss after 3474501 batches: 0.0414
trigger times: 24
Loss after 3475464 batches: 0.0419
trigger times: 25
Early stopping!
Start to test process.
Loss after 3476427 batches: 0.0411
Time to train on one home:  47.47203016281128
trigger times: 0
Loss after 3477390 batches: 0.1223
trigger times: 0
Loss after 3478353 batches: 0.1192
trigger times: 1
Loss after 3479316 batches: 0.1178
trigger times: 2
Loss after 3480279 batches: 0.1139
trigger times: 3
Loss after 3481242 batches: 0.1079
trigger times: 4
Loss after 3482205 batches: 0.1048
trigger times: 5
Loss after 3483168 batches: 0.1038
trigger times: 6
Loss after 3484131 batches: 0.1010
trigger times: 7
Loss after 3485094 batches: 0.0990
trigger times: 8
Loss after 3486057 batches: 0.0962
trigger times: 9
Loss after 3487020 batches: 0.0962
trigger times: 10
Loss after 3487983 batches: 0.0935
trigger times: 11
Loss after 3488946 batches: 0.0938
trigger times: 12
Loss after 3489909 batches: 0.0927
trigger times: 13
Loss after 3490872 batches: 0.0928
trigger times: 14
Loss after 3491835 batches: 0.0908
trigger times: 15
Loss after 3492798 batches: 0.0902
trigger times: 16
Loss after 3493761 batches: 0.0891
trigger times: 17
Loss after 3494724 batches: 0.0882
trigger times: 18
Loss after 3495687 batches: 0.0895
trigger times: 19
Loss after 3496650 batches: 0.0883
trigger times: 20
Loss after 3497613 batches: 0.0865
trigger times: 21
Loss after 3498576 batches: 0.0855
trigger times: 22
Loss after 3499539 batches: 0.0846
trigger times: 23
Loss after 3500502 batches: 0.0852
trigger times: 24
Loss after 3501465 batches: 0.0846
trigger times: 0
Loss after 3502428 batches: 0.0837
trigger times: 1
Loss after 3503391 batches: 0.0833
trigger times: 2
Loss after 3504354 batches: 0.0838
trigger times: 3
Loss after 3505317 batches: 0.0820
trigger times: 4
Loss after 3506280 batches: 0.0833
trigger times: 0
Loss after 3507243 batches: 0.0821
trigger times: 1
Loss after 3508206 batches: 0.0797
trigger times: 2
Loss after 3509169 batches: 0.0807
trigger times: 0
Loss after 3510132 batches: 0.0817
trigger times: 0
Loss after 3511095 batches: 0.0811
trigger times: 1
Loss after 3512058 batches: 0.0791
trigger times: 2
Loss after 3513021 batches: 0.0789
trigger times: 3
Loss after 3513984 batches: 0.0784
trigger times: 4
Loss after 3514947 batches: 0.0799
trigger times: 5
Loss after 3515910 batches: 0.0784
trigger times: 6
Loss after 3516873 batches: 0.0781
trigger times: 7
Loss after 3517836 batches: 0.0775
trigger times: 8
Loss after 3518799 batches: 0.0768
trigger times: 9
Loss after 3519762 batches: 0.0765
trigger times: 10
Loss after 3520725 batches: 0.0760
trigger times: 11
Loss after 3521688 batches: 0.0769
trigger times: 12
Loss after 3522651 batches: 0.0766
trigger times: 13
Loss after 3523614 batches: 0.0756
trigger times: 14
Loss after 3524577 batches: 0.0744
trigger times: 15
Loss after 3525540 batches: 0.0748
trigger times: 16
Loss after 3526503 batches: 0.0738
trigger times: 17
Loss after 3527466 batches: 0.0740
trigger times: 18
Loss after 3528429 batches: 0.0727
trigger times: 19
Loss after 3529392 batches: 0.0724
trigger times: 20
Loss after 3530355 batches: 0.0727
trigger times: 21
Loss after 3531318 batches: 0.0738
trigger times: 22
Loss after 3532281 batches: 0.0732
trigger times: 23
Loss after 3533244 batches: 0.0725
trigger times: 24
Loss after 3534207 batches: 0.0712
trigger times: 25
Early stopping!
Start to test process.
Loss after 3535170 batches: 0.0724
Time to train on one home:  72.13066530227661
trigger times: 0
Loss after 3536133 batches: 0.1224
trigger times: 0
Loss after 3537096 batches: 0.1113
trigger times: 1
Loss after 3538059 batches: 0.1076
trigger times: 2
Loss after 3539022 batches: 0.1019
trigger times: 3
Loss after 3539985 batches: 0.0963
trigger times: 4
Loss after 3540948 batches: 0.0907
trigger times: 0
Loss after 3541911 batches: 0.0796
trigger times: 1
Loss after 3542874 batches: 0.0758
trigger times: 2
Loss after 3543837 batches: 0.0723
trigger times: 0
Loss after 3544800 batches: 0.0714
trigger times: 1
Loss after 3545763 batches: 0.0691
trigger times: 0
Loss after 3546726 batches: 0.0683
trigger times: 1
Loss after 3547689 batches: 0.0668
trigger times: 2
Loss after 3548652 batches: 0.0662
trigger times: 3
Loss after 3549615 batches: 0.0648
trigger times: 0
Loss after 3550578 batches: 0.0654
trigger times: 1
Loss after 3551541 batches: 0.0631
trigger times: 2
Loss after 3552504 batches: 0.0632
trigger times: 0
Loss after 3553467 batches: 0.0623
trigger times: 0
Loss after 3554430 batches: 0.0614
trigger times: 0
Loss after 3555393 batches: 0.0605
trigger times: 1
Loss after 3556356 batches: 0.0605
trigger times: 2
Loss after 3557319 batches: 0.0593
trigger times: 3
Loss after 3558282 batches: 0.0587
trigger times: 4
Loss after 3559245 batches: 0.0575
trigger times: 0
Loss after 3560208 batches: 0.0583
trigger times: 1
Loss after 3561171 batches: 0.0570
trigger times: 0
Loss after 3562134 batches: 0.0561
trigger times: 0
Loss after 3563097 batches: 0.0554
trigger times: 0
Loss after 3564060 batches: 0.0542
trigger times: 1
Loss after 3565023 batches: 0.0543
trigger times: 2
Loss after 3565986 batches: 0.0542
trigger times: 3
Loss after 3566949 batches: 0.0538
trigger times: 4
Loss after 3567912 batches: 0.0533
trigger times: 0
Loss after 3568875 batches: 0.0530
trigger times: 1
Loss after 3569838 batches: 0.0507
trigger times: 2
Loss after 3570801 batches: 0.0524
trigger times: 0
Loss after 3571764 batches: 0.0508
trigger times: 1
Loss after 3572727 batches: 0.0517
trigger times: 2
Loss after 3573690 batches: 0.0517
trigger times: 3
Loss after 3574653 batches: 0.0493
trigger times: 4
Loss after 3575616 batches: 0.0499
trigger times: 5
Loss after 3576579 batches: 0.0498
trigger times: 6
Loss after 3577542 batches: 0.0484
trigger times: 7
Loss after 3578505 batches: 0.0514
trigger times: 8
Loss after 3579468 batches: 0.0497
trigger times: 9
Loss after 3580431 batches: 0.0488
trigger times: 10
Loss after 3581394 batches: 0.0486
trigger times: 11
Loss after 3582357 batches: 0.0472
trigger times: 12
Loss after 3583320 batches: 0.0480
trigger times: 13
Loss after 3584283 batches: 0.0475
trigger times: 14
Loss after 3585246 batches: 0.0481
trigger times: 15
Loss after 3586209 batches: 0.0476
trigger times: 16
Loss after 3587172 batches: 0.0486
trigger times: 17
Loss after 3588135 batches: 0.0472
trigger times: 18
Loss after 3589098 batches: 0.0473
trigger times: 19
Loss after 3590061 batches: 0.0469
trigger times: 20
Loss after 3591024 batches: 0.0461
trigger times: 21
Loss after 3591987 batches: 0.0457
trigger times: 22
Loss after 3592950 batches: 0.0451
trigger times: 23
Loss after 3593913 batches: 0.0451
trigger times: 24
Loss after 3594876 batches: 0.0441
trigger times: 25
Early stopping!
Start to test process.
Loss after 3595839 batches: 0.0439
Time to train on one home:  74.02340865135193
trigger times: 0
Loss after 3596768 batches: 0.1314
trigger times: 0
Loss after 3597697 batches: 0.0976
trigger times: 1
Loss after 3598626 batches: 0.0827
trigger times: 2
Loss after 3599555 batches: 0.0699
trigger times: 3
Loss after 3600484 batches: 0.0618
trigger times: 4
Loss after 3601413 batches: 0.0594
trigger times: 5
Loss after 3602342 batches: 0.0559
trigger times: 6
Loss after 3603271 batches: 0.0513
trigger times: 7
Loss after 3604200 batches: 0.0488
trigger times: 8
Loss after 3605129 batches: 0.0453
trigger times: 9
Loss after 3606058 batches: 0.0466
trigger times: 10
Loss after 3606987 batches: 0.0462
trigger times: 11
Loss after 3607916 batches: 0.0485
trigger times: 12
Loss after 3608845 batches: 0.0431
trigger times: 13
Loss after 3609774 batches: 0.0437
trigger times: 14
Loss after 3610703 batches: 0.0453
trigger times: 15
Loss after 3611632 batches: 0.0408
trigger times: 16
Loss after 3612561 batches: 0.0418
trigger times: 17
Loss after 3613490 batches: 0.0405
trigger times: 18
Loss after 3614419 batches: 0.0402
trigger times: 19
Loss after 3615348 batches: 0.0387
trigger times: 20
Loss after 3616277 batches: 0.0421
trigger times: 21
Loss after 3617206 batches: 0.0439
trigger times: 22
Loss after 3618135 batches: 0.0423
trigger times: 23
Loss after 3619064 batches: 0.0428
trigger times: 24
Loss after 3619993 batches: 0.0400
trigger times: 25
Early stopping!
Start to test process.
Loss after 3620922 batches: 0.0410
Time to train on one home:  50.72687077522278
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3621885 batches: 0.0436
trigger times: 1
Loss after 3622848 batches: 0.0294
trigger times: 2
Loss after 3623811 batches: 0.0301
trigger times: 3
Loss after 3624774 batches: 0.0289
trigger times: 4
Loss after 3625737 batches: 0.0278
trigger times: 5
Loss after 3626700 batches: 0.0278
trigger times: 6
Loss after 3627663 batches: 0.0276
trigger times: 7
Loss after 3628626 batches: 0.0273
trigger times: 8
Loss after 3629589 batches: 0.0270
trigger times: 9
Loss after 3630552 batches: 0.0267
trigger times: 10
Loss after 3631515 batches: 0.0265
trigger times: 11
Loss after 3632478 batches: 0.0261
trigger times: 12
Loss after 3633441 batches: 0.0259
trigger times: 13
Loss after 3634404 batches: 0.0256
trigger times: 14
Loss after 3635367 batches: 0.0252
trigger times: 15
Loss after 3636330 batches: 0.0253
trigger times: 16
Loss after 3637293 batches: 0.0251
trigger times: 17
Loss after 3638256 batches: 0.0250
trigger times: 18
Loss after 3639219 batches: 0.0247
trigger times: 19
Loss after 3640182 batches: 0.0245
trigger times: 20
Loss after 3641145 batches: 0.0246
trigger times: 21
Loss after 3642108 batches: 0.0245
trigger times: 22
Loss after 3643071 batches: 0.0244
trigger times: 23
Loss after 3644034 batches: 0.0244
trigger times: 24
Loss after 3644997 batches: 0.0243
trigger times: 25
Early stopping!
Start to test process.
Loss after 3645960 batches: 0.0240
Time to train on one home:  52.23248648643494
trigger times: 0
Loss after 3646923 batches: 0.1534
trigger times: 1
Loss after 3647886 batches: 0.1187
trigger times: 2
Loss after 3648849 batches: 0.1158
trigger times: 3
Loss after 3649812 batches: 0.1071
trigger times: 4
Loss after 3650775 batches: 0.1004
trigger times: 5
Loss after 3651738 batches: 0.0945
trigger times: 6
Loss after 3652701 batches: 0.0923
trigger times: 7
Loss after 3653664 batches: 0.0875
trigger times: 8
Loss after 3654627 batches: 0.0868
trigger times: 9
Loss after 3655590 batches: 0.0859
trigger times: 10
Loss after 3656553 batches: 0.0851
trigger times: 11
Loss after 3657516 batches: 0.0833
trigger times: 12
Loss after 3658479 batches: 0.0835
trigger times: 13
Loss after 3659442 batches: 0.0824
trigger times: 14
Loss after 3660405 batches: 0.0825
trigger times: 15
Loss after 3661368 batches: 0.0792
trigger times: 16
Loss after 3662331 batches: 0.0784
trigger times: 17
Loss after 3663294 batches: 0.0776
trigger times: 18
Loss after 3664257 batches: 0.0763
trigger times: 19
Loss after 3665220 batches: 0.0743
trigger times: 20
Loss after 3666183 batches: 0.0733
trigger times: 21
Loss after 3667146 batches: 0.0714
trigger times: 22
Loss after 3668109 batches: 0.0708
trigger times: 23
Loss after 3669072 batches: 0.0705
trigger times: 24
Loss after 3670035 batches: 0.0709
trigger times: 25
Early stopping!
Start to test process.
Loss after 3670998 batches: 0.0672
Time to train on one home:  52.50641489028931
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3671961 batches: 0.1075
trigger times: 1
Loss after 3672924 batches: 0.1002
trigger times: 2
Loss after 3673887 batches: 0.0983
trigger times: 3
Loss after 3674850 batches: 0.0935
trigger times: 0
Loss after 3675813 batches: 0.0903
trigger times: 1
Loss after 3676776 batches: 0.0875
trigger times: 2
Loss after 3677739 batches: 0.0844
trigger times: 3
Loss after 3678702 batches: 0.0830
trigger times: 4
Loss after 3679665 batches: 0.0812
trigger times: 5
Loss after 3680628 batches: 0.0807
trigger times: 6
Loss after 3681591 batches: 0.0806
trigger times: 7
Loss after 3682554 batches: 0.0793
trigger times: 8
Loss after 3683517 batches: 0.0785
trigger times: 9
Loss after 3684480 batches: 0.0791
trigger times: 10
Loss after 3685443 batches: 0.0782
trigger times: 11
Loss after 3686406 batches: 0.0764
trigger times: 12
Loss after 3687369 batches: 0.0768
trigger times: 13
Loss after 3688332 batches: 0.0750
trigger times: 14
Loss after 3689295 batches: 0.0760
trigger times: 15
Loss after 3690258 batches: 0.0752
trigger times: 16
Loss after 3691221 batches: 0.0750
trigger times: 17
Loss after 3692184 batches: 0.0745
trigger times: 18
Loss after 3693147 batches: 0.0730
trigger times: 19
Loss after 3694110 batches: 0.0725
trigger times: 20
Loss after 3695073 batches: 0.0732
trigger times: 21
Loss after 3696036 batches: 0.0715
trigger times: 22
Loss after 3696999 batches: 0.0721
trigger times: 23
Loss after 3697962 batches: 0.0718
trigger times: 24
Loss after 3698925 batches: 0.0711
trigger times: 25
Early stopping!
Start to test process.
Loss after 3699888 batches: 0.0705
Time to train on one home:  55.744710206985474
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 3700851 batches: 0.0787
trigger times: 1
Loss after 3701814 batches: 0.0775
trigger times: 2
Loss after 3702777 batches: 0.0768
trigger times: 3
Loss after 3703740 batches: 0.0756
trigger times: 4
Loss after 3704703 batches: 0.0758
trigger times: 5
Loss after 3705666 batches: 0.0760
trigger times: 6
Loss after 3706629 batches: 0.0751
trigger times: 7
Loss after 3707592 batches: 0.0755
trigger times: 8
Loss after 3708555 batches: 0.0757
trigger times: 9
Loss after 3709518 batches: 0.0747
trigger times: 10
Loss after 3710481 batches: 0.0749
trigger times: 11
Loss after 3711444 batches: 0.0745
trigger times: 0
Loss after 3712407 batches: 0.0741
trigger times: 0
Loss after 3713370 batches: 0.0731
trigger times: 1
Loss after 3714333 batches: 0.0727
trigger times: 2
Loss after 3715296 batches: 0.0713
trigger times: 3
Loss after 3716259 batches: 0.0710
trigger times: 4
Loss after 3717222 batches: 0.0717
trigger times: 5
Loss after 3718185 batches: 0.0716
trigger times: 6
Loss after 3719148 batches: 0.0712
trigger times: 7
Loss after 3720111 batches: 0.0710
trigger times: 8
Loss after 3721074 batches: 0.0705
trigger times: 9
Loss after 3722037 batches: 0.0711
trigger times: 10
Loss after 3723000 batches: 0.0703
trigger times: 11
Loss after 3723963 batches: 0.0702
trigger times: 12
Loss after 3724926 batches: 0.0702
trigger times: 13
Loss after 3725889 batches: 0.0692
trigger times: 14
Loss after 3726852 batches: 0.0700
trigger times: 15
Loss after 3727815 batches: 0.0703
trigger times: 16
Loss after 3728778 batches: 0.0699
trigger times: 17
Loss after 3729741 batches: 0.0694
trigger times: 18
Loss after 3730704 batches: 0.0698
trigger times: 19
Loss after 3731667 batches: 0.0706
trigger times: 20
Loss after 3732630 batches: 0.0703
trigger times: 21
Loss after 3733593 batches: 0.0690
trigger times: 22
Loss after 3734556 batches: 0.0689
trigger times: 23
Loss after 3735519 batches: 0.0696
trigger times: 24
Loss after 3736482 batches: 0.0685
trigger times: 25
Early stopping!
Start to test process.
Loss after 3737445 batches: 0.0690
Time to train on one home:  62.667136430740356
trigger times: 0
Loss after 3738408 batches: 0.0467
trigger times: 1
Loss after 3739371 batches: 0.0458
trigger times: 2
Loss after 3740334 batches: 0.0447
trigger times: 0
Loss after 3741297 batches: 0.0438
trigger times: 0
Loss after 3742260 batches: 0.0434
trigger times: 0
Loss after 3743223 batches: 0.0422
trigger times: 0
Loss after 3744186 batches: 0.0409
trigger times: 0
Loss after 3745149 batches: 0.0396
trigger times: 1
Loss after 3746112 batches: 0.0382
trigger times: 0
Loss after 3747075 batches: 0.0363
trigger times: 0
Loss after 3748038 batches: 0.0349
trigger times: 1
Loss after 3749001 batches: 0.0335
trigger times: 0
Loss after 3749964 batches: 0.0329
trigger times: 0
Loss after 3750927 batches: 0.0325
trigger times: 1
Loss after 3751890 batches: 0.0318
trigger times: 0
Loss after 3752853 batches: 0.0313
trigger times: 0
Loss after 3753816 batches: 0.0309
trigger times: 0
Loss after 3754779 batches: 0.0311
trigger times: 0
Loss after 3755742 batches: 0.0302
trigger times: 0
Loss after 3756705 batches: 0.0295
trigger times: 0
Loss after 3757668 batches: 0.0295
trigger times: 1
Loss after 3758631 batches: 0.0291
trigger times: 0
Loss after 3759594 batches: 0.0285
trigger times: 1
Loss after 3760557 batches: 0.0281
trigger times: 0
Loss after 3761520 batches: 0.0286
trigger times: 1
Loss after 3762483 batches: 0.0283
trigger times: 2
Loss after 3763446 batches: 0.0278
trigger times: 3
Loss after 3764409 batches: 0.0275
trigger times: 4
Loss after 3765372 batches: 0.0265
trigger times: 5
Loss after 3766335 batches: 0.0258
trigger times: 0
Loss after 3767298 batches: 0.0252
trigger times: 1
Loss after 3768261 batches: 0.0245
trigger times: 2
Loss after 3769224 batches: 0.0247
trigger times: 3
Loss after 3770187 batches: 0.0237
trigger times: 0
Loss after 3771150 batches: 0.0242
trigger times: 1
Loss after 3772113 batches: 0.0231
trigger times: 2
Loss after 3773076 batches: 0.0232
trigger times: 3
Loss after 3774039 batches: 0.0233
trigger times: 4
Loss after 3775002 batches: 0.0232
trigger times: 5
Loss after 3775965 batches: 0.0223
trigger times: 6
Loss after 3776928 batches: 0.0221
trigger times: 7
Loss after 3777891 batches: 0.0225
trigger times: 8
Loss after 3778854 batches: 0.0223
trigger times: 0
Loss after 3779817 batches: 0.0217
trigger times: 1
Loss after 3780780 batches: 0.0214
trigger times: 0
Loss after 3781743 batches: 0.0217
trigger times: 1
Loss after 3782706 batches: 0.0216
trigger times: 2
Loss after 3783669 batches: 0.0215
trigger times: 3
Loss after 3784632 batches: 0.0215
trigger times: 4
Loss after 3785595 batches: 0.0208
trigger times: 5
Loss after 3786558 batches: 0.0205
trigger times: 0
Loss after 3787521 batches: 0.0199
trigger times: 1
Loss after 3788484 batches: 0.0204
trigger times: 2
Loss after 3789447 batches: 0.0207
trigger times: 3
Loss after 3790410 batches: 0.0203
trigger times: 4
Loss after 3791373 batches: 0.0201
trigger times: 5
Loss after 3792336 batches: 0.0205
trigger times: 6
Loss after 3793299 batches: 0.0202
trigger times: 7
Loss after 3794262 batches: 0.0201
trigger times: 8
Loss after 3795225 batches: 0.0198
trigger times: 9
Loss after 3796188 batches: 0.0200
trigger times: 10
Loss after 3797151 batches: 0.0200
trigger times: 11
Loss after 3798114 batches: 0.0193
trigger times: 12
Loss after 3799077 batches: 0.0194
trigger times: 13
Loss after 3800040 batches: 0.0186
trigger times: 14
Loss after 3801003 batches: 0.0189
trigger times: 15
Loss after 3801966 batches: 0.0190
trigger times: 16
Loss after 3802929 batches: 0.0181
trigger times: 0
Loss after 3803892 batches: 0.0179
trigger times: 1
Loss after 3804855 batches: 0.0182
trigger times: 2
Loss after 3805818 batches: 0.0184
trigger times: 3
Loss after 3806781 batches: 0.0183
trigger times: 4
Loss after 3807744 batches: 0.0185
trigger times: 5
Loss after 3808707 batches: 0.0178
trigger times: 6
Loss after 3809670 batches: 0.0181
trigger times: 7
Loss after 3810633 batches: 0.0179
trigger times: 8
Loss after 3811596 batches: 0.0177
trigger times: 9
Loss after 3812559 batches: 0.0170
trigger times: 10
Loss after 3813522 batches: 0.0178
trigger times: 11
Loss after 3814485 batches: 0.0175
trigger times: 12
Loss after 3815448 batches: 0.0172
trigger times: 13
Loss after 3816411 batches: 0.0175
trigger times: 14
Loss after 3817374 batches: 0.0171
trigger times: 0
Loss after 3818337 batches: 0.0168
trigger times: 1
Loss after 3819300 batches: 0.0169
trigger times: 2
Loss after 3820263 batches: 0.0170
trigger times: 3
Loss after 3821226 batches: 0.0169
trigger times: 4
Loss after 3822189 batches: 0.0170
trigger times: 5
Loss after 3823152 batches: 0.0165
trigger times: 6
Loss after 3824115 batches: 0.0164
trigger times: 7
Loss after 3825078 batches: 0.0167
trigger times: 0
Loss after 3826041 batches: 0.0160
trigger times: 1
Loss after 3827004 batches: 0.0167
trigger times: 2
Loss after 3827967 batches: 0.0164
trigger times: 3
Loss after 3828930 batches: 0.0158
trigger times: 4
Loss after 3829893 batches: 0.0171
trigger times: 5
Loss after 3830856 batches: 0.0163
trigger times: 6
Loss after 3831819 batches: 0.0167
trigger times: 7
Loss after 3832782 batches: 0.0165
trigger times: 8
Loss after 3833745 batches: 0.0168
trigger times: 9
Loss after 3834708 batches: 0.0171
trigger times: 10
Loss after 3835671 batches: 0.0165
trigger times: 11
Loss after 3836634 batches: 0.0154
trigger times: 12
Loss after 3837597 batches: 0.0159
trigger times: 0
Loss after 3838560 batches: 0.0157
trigger times: 1
Loss after 3839523 batches: 0.0153
trigger times: 2
Loss after 3840486 batches: 0.0162
trigger times: 3
Loss after 3841449 batches: 0.0154
trigger times: 4
Loss after 3842412 batches: 0.0157
trigger times: 5
Loss after 3843375 batches: 0.0155
trigger times: 6
Loss after 3844338 batches: 0.0163
trigger times: 7
Loss after 3845301 batches: 0.0160
trigger times: 8
Loss after 3846264 batches: 0.0157
trigger times: 9
Loss after 3847227 batches: 0.0158
trigger times: 10
Loss after 3848190 batches: 0.0150
trigger times: 11
Loss after 3849153 batches: 0.0146
trigger times: 12
Loss after 3850116 batches: 0.0147
trigger times: 13
Loss after 3851079 batches: 0.0147
trigger times: 14
Loss after 3852042 batches: 0.0149
trigger times: 15
Loss after 3853005 batches: 0.0152
trigger times: 16
Loss after 3853968 batches: 0.0144
trigger times: 17
Loss after 3854931 batches: 0.0139
trigger times: 18
Loss after 3855894 batches: 0.0140
trigger times: 19
Loss after 3856857 batches: 0.0141
trigger times: 20
Loss after 3857820 batches: 0.0139
trigger times: 21
Loss after 3858783 batches: 0.0134
trigger times: 22
Loss after 3859746 batches: 0.0139
trigger times: 23
Loss after 3860709 batches: 0.0141
trigger times: 24
Loss after 3861672 batches: 0.0144
trigger times: 25
Early stopping!
Start to test process.
Loss after 3862635 batches: 0.0137
Time to train on one home:  131.27714443206787
trigger times: 0
Loss after 3863598 batches: 0.0755
trigger times: 0
Loss after 3864561 batches: 0.0725
trigger times: 1
Loss after 3865524 batches: 0.0705
trigger times: 2
Loss after 3866487 batches: 0.0677
trigger times: 3
Loss after 3867450 batches: 0.0639
trigger times: 4
Loss after 3868413 batches: 0.0616
trigger times: 5
Loss after 3869376 batches: 0.0597
trigger times: 6
Loss after 3870339 batches: 0.0592
trigger times: 7
Loss after 3871302 batches: 0.0578
trigger times: 0
Loss after 3872265 batches: 0.0549
trigger times: 0
Loss after 3873228 batches: 0.0512
trigger times: 0
Loss after 3874191 batches: 0.0497
trigger times: 0
Loss after 3875154 batches: 0.0483
trigger times: 1
Loss after 3876117 batches: 0.0492
trigger times: 0
Loss after 3877080 batches: 0.0488
trigger times: 1
Loss after 3878043 batches: 0.0484
trigger times: 0
Loss after 3879006 batches: 0.0474
trigger times: 0
Loss after 3879969 batches: 0.0466
trigger times: 1
Loss after 3880932 batches: 0.0454
trigger times: 0
Loss after 3881895 batches: 0.0454
trigger times: 1
Loss after 3882858 batches: 0.0450
trigger times: 2
Loss after 3883821 batches: 0.0453
trigger times: 3
Loss after 3884784 batches: 0.0442
trigger times: 0
Loss after 3885747 batches: 0.0439
trigger times: 1
Loss after 3886710 batches: 0.0440
trigger times: 2
Loss after 3887673 batches: 0.0443
trigger times: 0
Loss after 3888636 batches: 0.0429
trigger times: 0
Loss after 3889599 batches: 0.0427
trigger times: 1
Loss after 3890562 batches: 0.0425
trigger times: 0
Loss after 3891525 batches: 0.0419
trigger times: 0
Loss after 3892488 batches: 0.0414
trigger times: 1
Loss after 3893451 batches: 0.0405
trigger times: 2
Loss after 3894414 batches: 0.0406
trigger times: 3
Loss after 3895377 batches: 0.0405
trigger times: 4
Loss after 3896340 batches: 0.0401
trigger times: 5
Loss after 3897303 batches: 0.0409
trigger times: 6
Loss after 3898266 batches: 0.0401
trigger times: 0
Loss after 3899229 batches: 0.0391
trigger times: 1
Loss after 3900192 batches: 0.0392
trigger times: 2
Loss after 3901155 batches: 0.0391
trigger times: 3
Loss after 3902118 batches: 0.0380
trigger times: 0
Loss after 3903081 batches: 0.0379
trigger times: 1
Loss after 3904044 batches: 0.0389
trigger times: 2
Loss after 3905007 batches: 0.0381
trigger times: 0
Loss after 3905970 batches: 0.0366
trigger times: 1
Loss after 3906933 batches: 0.0366
trigger times: 2
Loss after 3907896 batches: 0.0366
trigger times: 3
Loss after 3908859 batches: 0.0362
trigger times: 0
Loss after 3909822 batches: 0.0363
trigger times: 1
Loss after 3910785 batches: 0.0347
trigger times: 2
Loss after 3911748 batches: 0.0335
trigger times: 3
Loss after 3912711 batches: 0.0339
trigger times: 4
Loss after 3913674 batches: 0.0333
trigger times: 5
Loss after 3914637 batches: 0.0331
trigger times: 6
Loss after 3915600 batches: 0.0332
trigger times: 7
Loss after 3916563 batches: 0.0326
trigger times: 8
Loss after 3917526 batches: 0.0328
trigger times: 0
Loss after 3918489 batches: 0.0317
trigger times: 1
Loss after 3919452 batches: 0.0316
trigger times: 2
Loss after 3920415 batches: 0.0311
trigger times: 0
Loss after 3921378 batches: 0.0323
trigger times: 1
Loss after 3922341 batches: 0.0331
trigger times: 2
Loss after 3923304 batches: 0.0320
trigger times: 3
Loss after 3924267 batches: 0.0302
trigger times: 4
Loss after 3925230 batches: 0.0306
trigger times: 5
Loss after 3926193 batches: 0.0304
trigger times: 6
Loss after 3927156 batches: 0.0297
trigger times: 7
Loss after 3928119 batches: 0.0284
trigger times: 8
Loss after 3929082 batches: 0.0287
trigger times: 9
Loss after 3930045 batches: 0.0296
trigger times: 10
Loss after 3931008 batches: 0.0287
trigger times: 11
Loss after 3931971 batches: 0.0283
trigger times: 12
Loss after 3932934 batches: 0.0283
trigger times: 13
Loss after 3933897 batches: 0.0286
trigger times: 14
Loss after 3934860 batches: 0.0279
trigger times: 15
Loss after 3935823 batches: 0.0279
trigger times: 16
Loss after 3936786 batches: 0.0278
trigger times: 17
Loss after 3937749 batches: 0.0276
trigger times: 18
Loss after 3938712 batches: 0.0277
trigger times: 19
Loss after 3939675 batches: 0.0273
trigger times: 20
Loss after 3940638 batches: 0.0271
trigger times: 21
Loss after 3941601 batches: 0.0269
trigger times: 22
Loss after 3942564 batches: 0.0256
trigger times: 23
Loss after 3943527 batches: 0.0264
trigger times: 24
Loss after 3944490 batches: 0.0275
trigger times: 25
Early stopping!
Start to test process.
Loss after 3945453 batches: 0.0264
Time to train on one home:  97.60280275344849
trigger times: 0
Loss after 3946416 batches: 0.1368
trigger times: 1
Loss after 3947379 batches: 0.1264
trigger times: 2
Loss after 3948342 batches: 0.1152
trigger times: 3
Loss after 3949305 batches: 0.0974
trigger times: 4
Loss after 3950268 batches: 0.0887
trigger times: 5
Loss after 3951231 batches: 0.0810
trigger times: 6
Loss after 3952194 batches: 0.0743
trigger times: 7
Loss after 3953157 batches: 0.0697
trigger times: 8
Loss after 3954120 batches: 0.0666
trigger times: 9
Loss after 3955083 batches: 0.0636
trigger times: 10
Loss after 3956046 batches: 0.0628
trigger times: 11
Loss after 3957009 batches: 0.0617
trigger times: 12
Loss after 3957972 batches: 0.0616
trigger times: 13
Loss after 3958935 batches: 0.0594
trigger times: 14
Loss after 3959898 batches: 0.0583
trigger times: 15
Loss after 3960861 batches: 0.0570
trigger times: 16
Loss after 3961824 batches: 0.0571
trigger times: 17
Loss after 3962787 batches: 0.0563
trigger times: 18
Loss after 3963750 batches: 0.0549
trigger times: 19
Loss after 3964713 batches: 0.0542
trigger times: 20
Loss after 3965676 batches: 0.0538
trigger times: 21
Loss after 3966639 batches: 0.0535
trigger times: 22
Loss after 3967602 batches: 0.0537
trigger times: 23
Loss after 3968565 batches: 0.0537
trigger times: 24
Loss after 3969528 batches: 0.0522
trigger times: 25
Early stopping!
Start to test process.
Loss after 3970491 batches: 0.0522
Time to train on one home:  51.10759449005127
trigger times: 0
Loss after 3971454 batches: 0.1217
trigger times: 0
Loss after 3972417 batches: 0.1106
trigger times: 1
Loss after 3973380 batches: 0.1043
trigger times: 2
Loss after 3974343 batches: 0.0984
trigger times: 3
Loss after 3975306 batches: 0.0870
trigger times: 0
Loss after 3976269 batches: 0.0763
trigger times: 1
Loss after 3977232 batches: 0.0733
trigger times: 2
Loss after 3978195 batches: 0.0698
trigger times: 3
Loss after 3979158 batches: 0.0687
trigger times: 0
Loss after 3980121 batches: 0.0663
trigger times: 0
Loss after 3981084 batches: 0.0653
trigger times: 0
Loss after 3982047 batches: 0.0621
trigger times: 1
Loss after 3983010 batches: 0.0606
trigger times: 2
Loss after 3983973 batches: 0.0592
trigger times: 3
Loss after 3984936 batches: 0.0576
trigger times: 4
Loss after 3985899 batches: 0.0572
trigger times: 5
Loss after 3986862 batches: 0.0555
trigger times: 0
Loss after 3987825 batches: 0.0555
trigger times: 0
Loss after 3988788 batches: 0.0553
trigger times: 1
Loss after 3989751 batches: 0.0548
trigger times: 2
Loss after 3990714 batches: 0.0545
trigger times: 3
Loss after 3991677 batches: 0.0534
trigger times: 4
Loss after 3992640 batches: 0.0513
trigger times: 5
Loss after 3993603 batches: 0.0511
trigger times: 6
Loss after 3994566 batches: 0.0517
trigger times: 7
Loss after 3995529 batches: 0.0504
trigger times: 8
Loss after 3996492 batches: 0.0496
trigger times: 9
Loss after 3997455 batches: 0.0495
trigger times: 10
Loss after 3998418 batches: 0.0488
trigger times: 11
Loss after 3999381 batches: 0.0498
trigger times: 0
Loss after 4000344 batches: 0.0489
trigger times: 0
Loss after 4001307 batches: 0.0474
trigger times: 0
Loss after 4002270 batches: 0.0486
trigger times: 1
Loss after 4003233 batches: 0.0487
trigger times: 2
Loss after 4004196 batches: 0.0476
trigger times: 3
Loss after 4005159 batches: 0.0471
trigger times: 4
Loss after 4006122 batches: 0.0468
trigger times: 5
Loss after 4007085 batches: 0.0471
trigger times: 6
Loss after 4008048 batches: 0.0473
trigger times: 7
Loss after 4009011 batches: 0.0453
trigger times: 8
Loss after 4009974 batches: 0.0460
trigger times: 9
Loss after 4010937 batches: 0.0456
trigger times: 10
Loss after 4011900 batches: 0.0456
trigger times: 11
Loss after 4012863 batches: 0.0445
trigger times: 12
Loss after 4013826 batches: 0.0451
trigger times: 13
Loss after 4014789 batches: 0.0454
trigger times: 14
Loss after 4015752 batches: 0.0450
trigger times: 15
Loss after 4016715 batches: 0.0441
trigger times: 16
Loss after 4017678 batches: 0.0442
trigger times: 17
Loss after 4018641 batches: 0.0448
trigger times: 18
Loss after 4019604 batches: 0.0448
trigger times: 19
Loss after 4020567 batches: 0.0439
trigger times: 20
Loss after 4021530 batches: 0.0439
trigger times: 21
Loss after 4022493 batches: 0.0423
trigger times: 22
Loss after 4023456 batches: 0.0427
trigger times: 23
Loss after 4024419 batches: 0.0433
trigger times: 24
Loss after 4025382 batches: 0.0434
trigger times: 25
Early stopping!
Start to test process.
Loss after 4026345 batches: 0.0427
Time to train on one home:  76.92379140853882
trigger times: 0
Loss after 4027308 batches: 0.0918
trigger times: 0
Loss after 4028271 batches: 0.0835
trigger times: 1
Loss after 4029234 batches: 0.0778
trigger times: 2
Loss after 4030197 batches: 0.0767
trigger times: 3
Loss after 4031160 batches: 0.0742
trigger times: 4
Loss after 4032123 batches: 0.0728
trigger times: 5
Loss after 4033086 batches: 0.0710
trigger times: 6
Loss after 4034049 batches: 0.0689
trigger times: 0
Loss after 4035012 batches: 0.0653
trigger times: 0
Loss after 4035975 batches: 0.0593
trigger times: 1
Loss after 4036938 batches: 0.0567
trigger times: 0
Loss after 4037901 batches: 0.0562
trigger times: 0
Loss after 4038864 batches: 0.0533
trigger times: 1
Loss after 4039827 batches: 0.0527
trigger times: 0
Loss after 4040790 batches: 0.0516
trigger times: 0
Loss after 4041753 batches: 0.0513
trigger times: 0
Loss after 4042716 batches: 0.0502
trigger times: 0
Loss after 4043679 batches: 0.0500
trigger times: 0
Loss after 4044642 batches: 0.0489
trigger times: 1
Loss after 4045605 batches: 0.0486
trigger times: 2
Loss after 4046568 batches: 0.0481
trigger times: 0
Loss after 4047531 batches: 0.0484
trigger times: 0
Loss after 4048494 batches: 0.0469
trigger times: 1
Loss after 4049457 batches: 0.0473
trigger times: 0
Loss after 4050420 batches: 0.0458
trigger times: 1
Loss after 4051383 batches: 0.0464
trigger times: 0
Loss after 4052346 batches: 0.0461
trigger times: 0
Loss after 4053309 batches: 0.0449
trigger times: 1
Loss after 4054272 batches: 0.0436
trigger times: 2
Loss after 4055235 batches: 0.0434
trigger times: 3
Loss after 4056198 batches: 0.0440
trigger times: 4
Loss after 4057161 batches: 0.0438
trigger times: 0
Loss after 4058124 batches: 0.0456
trigger times: 1
Loss after 4059087 batches: 0.0440
trigger times: 0
Loss after 4060050 batches: 0.0447
trigger times: 1
Loss after 4061013 batches: 0.0424
trigger times: 2
Loss after 4061976 batches: 0.0423
trigger times: 3
Loss after 4062939 batches: 0.0425
trigger times: 0
Loss after 4063902 batches: 0.0416
trigger times: 1
Loss after 4064865 batches: 0.0404
trigger times: 2
Loss after 4065828 batches: 0.0418
trigger times: 3
Loss after 4066791 batches: 0.0413
trigger times: 4
Loss after 4067754 batches: 0.0420
trigger times: 0
Loss after 4068717 batches: 0.0418
trigger times: 1
Loss after 4069680 batches: 0.0410
trigger times: 2
Loss after 4070643 batches: 0.0413
trigger times: 0
Loss after 4071606 batches: 0.0411
trigger times: 1
Loss after 4072569 batches: 0.0402
trigger times: 2
Loss after 4073532 batches: 0.0392
trigger times: 3
Loss after 4074495 batches: 0.0397
trigger times: 4
Loss after 4075458 batches: 0.0399
trigger times: 5
Loss after 4076421 batches: 0.0402
trigger times: 6
Loss after 4077384 batches: 0.0403
trigger times: 7
Loss after 4078347 batches: 0.0396
trigger times: 8
Loss after 4079310 batches: 0.0400
trigger times: 9
Loss after 4080273 batches: 0.0399
trigger times: 10
Loss after 4081236 batches: 0.0399
trigger times: 11
Loss after 4082199 batches: 0.0393
trigger times: 12
Loss after 4083162 batches: 0.0402
trigger times: 13
Loss after 4084125 batches: 0.0397
trigger times: 14
Loss after 4085088 batches: 0.0395
trigger times: 15
Loss after 4086051 batches: 0.0382
trigger times: 16
Loss after 4087014 batches: 0.0401
trigger times: 17
Loss after 4087977 batches: 0.0384
trigger times: 18
Loss after 4088940 batches: 0.0390
trigger times: 19
Loss after 4089903 batches: 0.0401
trigger times: 20
Loss after 4090866 batches: 0.0409
trigger times: 21
Loss after 4091829 batches: 0.0400
trigger times: 22
Loss after 4092792 batches: 0.0382
trigger times: 23
Loss after 4093755 batches: 0.0387
trigger times: 24
Loss after 4094718 batches: 0.0388
trigger times: 25
Early stopping!
Start to test process.
Loss after 4095681 batches: 0.0379
Time to train on one home:  90.36198806762695
trigger times: 0
Loss after 4096644 batches: 0.1038
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4097607 batches: 0.0857
trigger times: 1
Loss after 4098570 batches: 0.0841
trigger times: 2
Loss after 4099533 batches: 0.0828
trigger times: 3
Loss after 4100496 batches: 0.0811
trigger times: 4
Loss after 4101459 batches: 0.0802
trigger times: 5
Loss after 4102422 batches: 0.0796
trigger times: 6
Loss after 4103385 batches: 0.0793
trigger times: 7
Loss after 4104348 batches: 0.0784
trigger times: 8
Loss after 4105311 batches: 0.0774
trigger times: 9
Loss after 4106274 batches: 0.0770
trigger times: 10
Loss after 4107237 batches: 0.0770
trigger times: 11
Loss after 4108200 batches: 0.0762
trigger times: 12
Loss after 4109163 batches: 0.0749
trigger times: 13
Loss after 4110126 batches: 0.0737
trigger times: 0
Loss after 4111089 batches: 0.0716
trigger times: 0
Loss after 4112052 batches: 0.0684
trigger times: 1
Loss after 4113015 batches: 0.0668
trigger times: 2
Loss after 4113978 batches: 0.0636
trigger times: 0
Loss after 4114941 batches: 0.0617
trigger times: 1
Loss after 4115904 batches: 0.0600
trigger times: 0
Loss after 4116867 batches: 0.0578
trigger times: 0
Loss after 4117830 batches: 0.0544
trigger times: 0
Loss after 4118793 batches: 0.0533
trigger times: 0
Loss after 4119756 batches: 0.0518
trigger times: 0
Loss after 4120719 batches: 0.0519
trigger times: 1
Loss after 4121682 batches: 0.0510
trigger times: 2
Loss after 4122645 batches: 0.0488
trigger times: 0
Loss after 4123608 batches: 0.0472
trigger times: 1
Loss after 4124571 batches: 0.0468
trigger times: 2
Loss after 4125534 batches: 0.0466
trigger times: 0
Loss after 4126497 batches: 0.0461
trigger times: 0
Loss after 4127460 batches: 0.0445
trigger times: 0
Loss after 4128423 batches: 0.0442
trigger times: 1
Loss after 4129386 batches: 0.0436
trigger times: 2
Loss after 4130349 batches: 0.0437
trigger times: 3
Loss after 4131312 batches: 0.0424
trigger times: 4
Loss after 4132275 batches: 0.0434
trigger times: 5
Loss after 4133238 batches: 0.0431
trigger times: 6
Loss after 4134201 batches: 0.0415
trigger times: 7
Loss after 4135164 batches: 0.0403
trigger times: 8
Loss after 4136127 batches: 0.0422
trigger times: 9
Loss after 4137090 batches: 0.0401
trigger times: 10
Loss after 4138053 batches: 0.0386
trigger times: 11
Loss after 4139016 batches: 0.0385
trigger times: 12
Loss after 4139979 batches: 0.0374
trigger times: 13
Loss after 4140942 batches: 0.0373
trigger times: 0
Loss after 4141905 batches: 0.0371
trigger times: 0
Loss after 4142868 batches: 0.0358
trigger times: 0
Loss after 4143831 batches: 0.0357
trigger times: 1
Loss after 4144794 batches: 0.0365
trigger times: 2
Loss after 4145757 batches: 0.0356
trigger times: 3
Loss after 4146720 batches: 0.0351
trigger times: 0
Loss after 4147683 batches: 0.0353
trigger times: 1
Loss after 4148646 batches: 0.0353
trigger times: 2
Loss after 4149609 batches: 0.0340
trigger times: 3
Loss after 4150572 batches: 0.0345
trigger times: 4
Loss after 4151535 batches: 0.0335
trigger times: 5
Loss after 4152498 batches: 0.0338
trigger times: 6
Loss after 4153461 batches: 0.0346
trigger times: 0
Loss after 4154424 batches: 0.0338
trigger times: 1
Loss after 4155387 batches: 0.0327
trigger times: 2
Loss after 4156350 batches: 0.0329
trigger times: 3
Loss after 4157313 batches: 0.0323
trigger times: 0
Loss after 4158276 batches: 0.0324
trigger times: 1
Loss after 4159239 batches: 0.0315
trigger times: 2
Loss after 4160202 batches: 0.0322
trigger times: 0
Loss after 4161165 batches: 0.0311
trigger times: 1
Loss after 4162128 batches: 0.0308
trigger times: 2
Loss after 4163091 batches: 0.0320
trigger times: 3
Loss after 4164054 batches: 0.0310
trigger times: 4
Loss after 4165017 batches: 0.0304
trigger times: 5
Loss after 4165980 batches: 0.0298
trigger times: 6
Loss after 4166943 batches: 0.0291
trigger times: 7
Loss after 4167906 batches: 0.0301
trigger times: 8
Loss after 4168869 batches: 0.0294
trigger times: 9
Loss after 4169832 batches: 0.0286
trigger times: 0
Loss after 4170795 batches: 0.0294
trigger times: 1
Loss after 4171758 batches: 0.0296
trigger times: 2
Loss after 4172721 batches: 0.0289
trigger times: 3
Loss after 4173684 batches: 0.0293
trigger times: 4
Loss after 4174647 batches: 0.0291
trigger times: 5
Loss after 4175610 batches: 0.0277
trigger times: 6
Loss after 4176573 batches: 0.0273
trigger times: 7
Loss after 4177536 batches: 0.0281
trigger times: 8
Loss after 4178499 batches: 0.0276
trigger times: 9
Loss after 4179462 batches: 0.0274
trigger times: 10
Loss after 4180425 batches: 0.0290
trigger times: 0
Loss after 4181388 batches: 0.0278
trigger times: 1
Loss after 4182351 batches: 0.0288
trigger times: 2
Loss after 4183314 batches: 0.0283
trigger times: 3
Loss after 4184277 batches: 0.0362
trigger times: 4
Loss after 4185240 batches: 0.0347
trigger times: 5
Loss after 4186203 batches: 0.0335
trigger times: 6
Loss after 4187166 batches: 0.0310
trigger times: 7
Loss after 4188129 batches: 0.0303
trigger times: 8
Loss after 4189092 batches: 0.0288
trigger times: 9
Loss after 4190055 batches: 0.0293
trigger times: 10
Loss after 4191018 batches: 0.0285
trigger times: 11
Loss after 4191981 batches: 0.0284
trigger times: 12
Loss after 4192944 batches: 0.0285
trigger times: 13
Loss after 4193907 batches: 0.0281
trigger times: 14
Loss after 4194870 batches: 0.0271
trigger times: 15
Loss after 4195833 batches: 0.0280
trigger times: 16
Loss after 4196796 batches: 0.0270
trigger times: 0
Loss after 4197759 batches: 0.0260
trigger times: 1
Loss after 4198722 batches: 0.0255
trigger times: 2
Loss after 4199685 batches: 0.0261
trigger times: 3
Loss after 4200648 batches: 0.0256
trigger times: 4
Loss after 4201611 batches: 0.0258
trigger times: 5
Loss after 4202574 batches: 0.0250
trigger times: 6
Loss after 4203537 batches: 0.0246
trigger times: 7
Loss after 4204500 batches: 0.0250
trigger times: 8
Loss after 4205463 batches: 0.0249
trigger times: 9
Loss after 4206426 batches: 0.0253
trigger times: 10
Loss after 4207389 batches: 0.0251
trigger times: 11
Loss after 4208352 batches: 0.0242
trigger times: 0
Loss after 4209315 batches: 0.0238
trigger times: 0
Loss after 4210278 batches: 0.0239
trigger times: 1
Loss after 4211241 batches: 0.0245
trigger times: 2
Loss after 4212204 batches: 0.0241
trigger times: 0
Loss after 4213167 batches: 0.0238
trigger times: 0
Loss after 4214130 batches: 0.0237
trigger times: 0
Loss after 4215093 batches: 0.0229
trigger times: 1
Loss after 4216056 batches: 0.0231
trigger times: 2
Loss after 4217019 batches: 0.0240
trigger times: 3
Loss after 4217982 batches: 0.0233
trigger times: 4
Loss after 4218945 batches: 0.0230
trigger times: 5
Loss after 4219908 batches: 0.0235
trigger times: 6
Loss after 4220871 batches: 0.0231
trigger times: 7
Loss after 4221834 batches: 0.0222
trigger times: 8
Loss after 4222797 batches: 0.0225
trigger times: 9
Loss after 4223760 batches: 0.0218
trigger times: 10
Loss after 4224723 batches: 0.0222
trigger times: 11
Loss after 4225686 batches: 0.0228
trigger times: 12
Loss after 4226649 batches: 0.0224
trigger times: 13
Loss after 4227612 batches: 0.0231
trigger times: 14
Loss after 4228575 batches: 0.0241
trigger times: 15
Loss after 4229538 batches: 0.0233
trigger times: 16
Loss after 4230501 batches: 0.0224
trigger times: 17
Loss after 4231464 batches: 0.0223
trigger times: 18
Loss after 4232427 batches: 0.0219
trigger times: 19
Loss after 4233390 batches: 0.0219
trigger times: 20
Loss after 4234353 batches: 0.0223
trigger times: 21
Loss after 4235316 batches: 0.0217
trigger times: 22
Loss after 4236279 batches: 0.0212
trigger times: 23
Loss after 4237242 batches: 0.0221
trigger times: 24
Loss after 4238205 batches: 0.0229
trigger times: 25
Early stopping!
Start to test process.
Loss after 4239168 batches: 0.0216
Time to train on one home:  147.19669461250305
trigger times: 0
Loss after 4240131 batches: 0.0862
trigger times: 1
Loss after 4241094 batches: 0.0827
trigger times: 0
Loss after 4242057 batches: 0.0825
trigger times: 1
Loss after 4243020 batches: 0.0808
trigger times: 2
Loss after 4243983 batches: 0.0805
trigger times: 3
Loss after 4244946 batches: 0.0795
trigger times: 4
Loss after 4245909 batches: 0.0777
trigger times: 5
Loss after 4246872 batches: 0.0768
trigger times: 6
Loss after 4247835 batches: 0.0756
trigger times: 7
Loss after 4248798 batches: 0.0749
trigger times: 0
Loss after 4249761 batches: 0.0744
trigger times: 1
Loss after 4250724 batches: 0.0746
trigger times: 2
Loss after 4251687 batches: 0.0743
trigger times: 3
Loss after 4252650 batches: 0.0737
trigger times: 4
Loss after 4253613 batches: 0.0730
trigger times: 5
Loss after 4254576 batches: 0.0727
trigger times: 6
Loss after 4255539 batches: 0.0736
trigger times: 7
Loss after 4256502 batches: 0.0717
trigger times: 8
Loss after 4257465 batches: 0.0713
trigger times: 9
Loss after 4258428 batches: 0.0710
trigger times: 0
Loss after 4259391 batches: 0.0714
trigger times: 1
Loss after 4260354 batches: 0.0706
trigger times: 2
Loss after 4261317 batches: 0.0700
trigger times: 3
Loss after 4262280 batches: 0.0709
trigger times: 4
Loss after 4263243 batches: 0.0705
trigger times: 5
Loss after 4264206 batches: 0.0699
trigger times: 6
Loss after 4265169 batches: 0.0703
trigger times: 7
Loss after 4266132 batches: 0.0689
trigger times: 8
Loss after 4267095 batches: 0.0686
trigger times: 9
Loss after 4268058 batches: 0.0693
trigger times: 10
Loss after 4269021 batches: 0.0679
trigger times: 11
Loss after 4269984 batches: 0.0674
trigger times: 12
Loss after 4270947 batches: 0.0684
trigger times: 13
Loss after 4271910 batches: 0.0689
trigger times: 14
Loss after 4272873 batches: 0.0689
trigger times: 15
Loss after 4273836 batches: 0.0676
trigger times: 16
Loss after 4274799 batches: 0.0674
trigger times: 17
Loss after 4275762 batches: 0.0672
trigger times: 18
Loss after 4276725 batches: 0.0669
trigger times: 19
Loss after 4277688 batches: 0.0668
trigger times: 20
Loss after 4278651 batches: 0.0671
trigger times: 21
Loss after 4279614 batches: 0.0665
trigger times: 22
Loss after 4280577 batches: 0.0666
trigger times: 23
Loss after 4281540 batches: 0.0681
trigger times: 24
Loss after 4282503 batches: 0.0684
trigger times: 25
Early stopping!
Start to test process.
Loss after 4283466 batches: 0.0680
Time to train on one home:  67.1319420337677
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4284429 batches: 0.1042
trigger times: 0
Loss after 4285392 batches: 0.0857
trigger times: 1
Loss after 4286355 batches: 0.0842
trigger times: 2
Loss after 4287318 batches: 0.0829
trigger times: 3
Loss after 4288281 batches: 0.0815
trigger times: 4
Loss after 4289244 batches: 0.0800
trigger times: 5
Loss after 4290207 batches: 0.0799
trigger times: 6
Loss after 4291170 batches: 0.0788
trigger times: 7
Loss after 4292133 batches: 0.0778
trigger times: 8
Loss after 4293096 batches: 0.0783
trigger times: 9
Loss after 4294059 batches: 0.0770
trigger times: 10
Loss after 4295022 batches: 0.0772
trigger times: 11
Loss after 4295985 batches: 0.0758
trigger times: 12
Loss after 4296948 batches: 0.0753
trigger times: 13
Loss after 4297911 batches: 0.0746
trigger times: 14
Loss after 4298874 batches: 0.0734
trigger times: 15
Loss after 4299837 batches: 0.0695
trigger times: 0
Loss after 4300800 batches: 0.0677
trigger times: 0
Loss after 4301763 batches: 0.0638
trigger times: 0
Loss after 4302726 batches: 0.0607
trigger times: 0
Loss after 4303689 batches: 0.0578
trigger times: 1
Loss after 4304652 batches: 0.0575
trigger times: 0
Loss after 4305615 batches: 0.0551
trigger times: 1
Loss after 4306578 batches: 0.0529
trigger times: 2
Loss after 4307541 batches: 0.0522
trigger times: 0
Loss after 4308504 batches: 0.0507
trigger times: 1
Loss after 4309467 batches: 0.0497
trigger times: 0
Loss after 4310430 batches: 0.0500
trigger times: 0
Loss after 4311393 batches: 0.0488
trigger times: 1
Loss after 4312356 batches: 0.0485
trigger times: 2
Loss after 4313319 batches: 0.0478
trigger times: 3
Loss after 4314282 batches: 0.0455
trigger times: 4
Loss after 4315245 batches: 0.0462
trigger times: 5
Loss after 4316208 batches: 0.0443
trigger times: 6
Loss after 4317171 batches: 0.0438
trigger times: 7
Loss after 4318134 batches: 0.0435
trigger times: 8
Loss after 4319097 batches: 0.0435
trigger times: 9
Loss after 4320060 batches: 0.0429
trigger times: 10
Loss after 4321023 batches: 0.0438
trigger times: 11
Loss after 4321986 batches: 0.0410
trigger times: 12
Loss after 4322949 batches: 0.0413
trigger times: 0
Loss after 4323912 batches: 0.0411
trigger times: 1
Loss after 4324875 batches: 0.0399
trigger times: 2
Loss after 4325838 batches: 0.0397
trigger times: 0
Loss after 4326801 batches: 0.0381
trigger times: 1
Loss after 4327764 batches: 0.0378
trigger times: 2
Loss after 4328727 batches: 0.0379
trigger times: 3
Loss after 4329690 batches: 0.0370
trigger times: 4
Loss after 4330653 batches: 0.0358
trigger times: 0
Loss after 4331616 batches: 0.0368
trigger times: 1
Loss after 4332579 batches: 0.0356
trigger times: 2
Loss after 4333542 batches: 0.0366
trigger times: 3
Loss after 4334505 batches: 0.0355
trigger times: 4
Loss after 4335468 batches: 0.0350
trigger times: 5
Loss after 4336431 batches: 0.0355
trigger times: 6
Loss after 4337394 batches: 0.0348
trigger times: 7
Loss after 4338357 batches: 0.0345
trigger times: 8
Loss after 4339320 batches: 0.0349
trigger times: 9
Loss after 4340283 batches: 0.0347
trigger times: 10
Loss after 4341246 batches: 0.0346
trigger times: 11
Loss after 4342209 batches: 0.0328
trigger times: 12
Loss after 4343172 batches: 0.0322
trigger times: 13
Loss after 4344135 batches: 0.0331
trigger times: 14
Loss after 4345098 batches: 0.0321
trigger times: 15
Loss after 4346061 batches: 0.0316
trigger times: 16
Loss after 4347024 batches: 0.0308
trigger times: 17
Loss after 4347987 batches: 0.0319
trigger times: 18
Loss after 4348950 batches: 0.0310
trigger times: 19
Loss after 4349913 batches: 0.0311
trigger times: 20
Loss after 4350876 batches: 0.0317
trigger times: 21
Loss after 4351839 batches: 0.0307
trigger times: 0
Loss after 4352802 batches: 0.0295
trigger times: 1
Loss after 4353765 batches: 0.0293
trigger times: 2
Loss after 4354728 batches: 0.0297
trigger times: 0
Loss after 4355691 batches: 0.0302
trigger times: 1
Loss after 4356654 batches: 0.0293
trigger times: 2
Loss after 4357617 batches: 0.0286
trigger times: 3
Loss after 4358580 batches: 0.0293
trigger times: 4
Loss after 4359543 batches: 0.0287
trigger times: 0
Loss after 4360506 batches: 0.0293
trigger times: 1
Loss after 4361469 batches: 0.0291
trigger times: 2
Loss after 4362432 batches: 0.0293
trigger times: 3
Loss after 4363395 batches: 0.0279
trigger times: 4
Loss after 4364358 batches: 0.0278
trigger times: 5
Loss after 4365321 batches: 0.0277
trigger times: 6
Loss after 4366284 batches: 0.0277
trigger times: 7
Loss after 4367247 batches: 0.0286
trigger times: 8
Loss after 4368210 batches: 0.0284
trigger times: 9
Loss after 4369173 batches: 0.0281
trigger times: 10
Loss after 4370136 batches: 0.0274
trigger times: 11
Loss after 4371099 batches: 0.0271
trigger times: 12
Loss after 4372062 batches: 0.0266
trigger times: 13
Loss after 4373025 batches: 0.0261
trigger times: 14
Loss after 4373988 batches: 0.0275
trigger times: 0
Loss after 4374951 batches: 0.0270
trigger times: 0
Loss after 4375914 batches: 0.0249
trigger times: 1
Loss after 4376877 batches: 0.0254
trigger times: 2
Loss after 4377840 batches: 0.0257
trigger times: 3
Loss after 4378803 batches: 0.0268
trigger times: 4
Loss after 4379766 batches: 0.0270
trigger times: 5
Loss after 4380729 batches: 0.0268
trigger times: 6
Loss after 4381692 batches: 0.0270
trigger times: 7
Loss after 4382655 batches: 0.0271
trigger times: 8
Loss after 4383618 batches: 0.0262
trigger times: 9
Loss after 4384581 batches: 0.0246
trigger times: 10
Loss after 4385544 batches: 0.0251
trigger times: 11
Loss after 4386507 batches: 0.0249
trigger times: 12
Loss after 4387470 batches: 0.0248
trigger times: 13
Loss after 4388433 batches: 0.0248
trigger times: 14
Loss after 4389396 batches: 0.0250
trigger times: 15
Loss after 4390359 batches: 0.0242
trigger times: 16
Loss after 4391322 batches: 0.0248
trigger times: 0
Loss after 4392285 batches: 0.0242
trigger times: 1
Loss after 4393248 batches: 0.0243
trigger times: 2
Loss after 4394211 batches: 0.0241
trigger times: 3
Loss after 4395174 batches: 0.0241
trigger times: 4
Loss after 4396137 batches: 0.0221
trigger times: 5
Loss after 4397100 batches: 0.0235
trigger times: 6
Loss after 4398063 batches: 0.0235
trigger times: 7
Loss after 4399026 batches: 0.0240
trigger times: 8
Loss after 4399989 batches: 0.0228
trigger times: 9
Loss after 4400952 batches: 0.0225
trigger times: 10
Loss after 4401915 batches: 0.0235
trigger times: 0
Loss after 4402878 batches: 0.0241
trigger times: 1
Loss after 4403841 batches: 0.0241
trigger times: 2
Loss after 4404804 batches: 0.0246
trigger times: 3
Loss after 4405767 batches: 0.0235
trigger times: 4
Loss after 4406730 batches: 0.0228
trigger times: 5
Loss after 4407693 batches: 0.0228
trigger times: 6
Loss after 4408656 batches: 0.0214
trigger times: 7
Loss after 4409619 batches: 0.0216
trigger times: 8
Loss after 4410582 batches: 0.0219
trigger times: 9
Loss after 4411545 batches: 0.0223
trigger times: 10
Loss after 4412508 batches: 0.0215
trigger times: 11
Loss after 4413471 batches: 0.0223
trigger times: 12
Loss after 4414434 batches: 0.0216
trigger times: 13
Loss after 4415397 batches: 0.0214
trigger times: 14
Loss after 4416360 batches: 0.0204
trigger times: 15
Loss after 4417323 batches: 0.0207
trigger times: 16
Loss after 4418286 batches: 0.0201
trigger times: 17
Loss after 4419249 batches: 0.0205
trigger times: 18
Loss after 4420212 batches: 0.0205
trigger times: 19
Loss after 4421175 batches: 0.0206
trigger times: 20
Loss after 4422138 batches: 0.0212
trigger times: 21
Loss after 4423101 batches: 0.0200
trigger times: 22
Loss after 4424064 batches: 0.0206
trigger times: 23
Loss after 4425027 batches: 0.0205
trigger times: 24
Loss after 4425990 batches: 0.0211
trigger times: 25
Early stopping!
Start to test process.
Loss after 4426953 batches: 0.0199
Time to train on one home:  142.76318550109863
trigger times: 0
Loss after 4427848 batches: 0.0717
trigger times: 0
Loss after 4428743 batches: 0.0438
trigger times: 1
Loss after 4429638 batches: 0.0255
trigger times: 2
Loss after 4430533 batches: 0.0240
trigger times: 3
Loss after 4431428 batches: 0.0208
trigger times: 4
Loss after 4432323 batches: 0.0189
trigger times: 5
Loss after 4433218 batches: 0.0173
trigger times: 6
Loss after 4434113 batches: 0.0157
trigger times: 7
Loss after 4435008 batches: 0.0136
trigger times: 8
Loss after 4435903 batches: 0.0112
trigger times: 9
Loss after 4436798 batches: 0.0102
trigger times: 10
Loss after 4437693 batches: 0.0093
trigger times: 11
Loss after 4438588 batches: 0.0086
trigger times: 12
Loss after 4439483 batches: 0.0074
trigger times: 0
Loss after 4440378 batches: 0.0076
trigger times: 1
Loss after 4441273 batches: 0.0071
trigger times: 2
Loss after 4442168 batches: 0.0063
trigger times: 3
Loss after 4443063 batches: 0.0062
trigger times: 4
Loss after 4443958 batches: 0.0062
trigger times: 5
Loss after 4444853 batches: 0.0064
trigger times: 6
Loss after 4445748 batches: 0.0063
trigger times: 7
Loss after 4446643 batches: 0.0056
trigger times: 8
Loss after 4447538 batches: 0.0057
trigger times: 9
Loss after 4448433 batches: 0.0056
trigger times: 10
Loss after 4449328 batches: 0.0058
trigger times: 11
Loss after 4450223 batches: 0.0055
trigger times: 12
Loss after 4451118 batches: 0.0084
trigger times: 13
Loss after 4452013 batches: 0.0065
trigger times: 14
Loss after 4452908 batches: 0.0066
trigger times: 15
Loss after 4453803 batches: 0.0058
trigger times: 16
Loss after 4454698 batches: 0.0057
trigger times: 17
Loss after 4455593 batches: 0.0053
trigger times: 18
Loss after 4456488 batches: 0.0051
trigger times: 19
Loss after 4457383 batches: 0.0069
trigger times: 20
Loss after 4458278 batches: 0.0047
trigger times: 21
Loss after 4459173 batches: 0.0051
trigger times: 22
Loss after 4460068 batches: 0.0048
trigger times: 23
Loss after 4460963 batches: 0.0049
trigger times: 24
Loss after 4461858 batches: 0.0041
trigger times: 25
Early stopping!
Start to test process.
Loss after 4462753 batches: 0.0041
Time to train on one home:  59.72906184196472
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4463690 batches: 0.0927
trigger times: 0
Loss after 4464627 batches: 0.0899
trigger times: 1
Loss after 4465564 batches: 0.0897
trigger times: 2
Loss after 4466501 batches: 0.0872
trigger times: 3
Loss after 4467438 batches: 0.0860
trigger times: 0
Loss after 4468375 batches: 0.0851
trigger times: 0
Loss after 4469312 batches: 0.0846
trigger times: 1
Loss after 4470249 batches: 0.0827
trigger times: 2
Loss after 4471186 batches: 0.0800
trigger times: 3
Loss after 4472123 batches: 0.0790
trigger times: 4
Loss after 4473060 batches: 0.0773
trigger times: 5
Loss after 4473997 batches: 0.0768
trigger times: 6
Loss after 4474934 batches: 0.0744
trigger times: 0
Loss after 4475871 batches: 0.0722
trigger times: 0
Loss after 4476808 batches: 0.0710
trigger times: 0
Loss after 4477745 batches: 0.0698
trigger times: 0
Loss after 4478682 batches: 0.0696
trigger times: 1
Loss after 4479619 batches: 0.0685
trigger times: 2
Loss after 4480556 batches: 0.0674
trigger times: 0
Loss after 4481493 batches: 0.0685
trigger times: 1
Loss after 4482430 batches: 0.0684
trigger times: 2
Loss after 4483367 batches: 0.0663
trigger times: 3
Loss after 4484304 batches: 0.0647
trigger times: 4
Loss after 4485241 batches: 0.0663
trigger times: 5
Loss after 4486178 batches: 0.0644
trigger times: 6
Loss after 4487115 batches: 0.0647
trigger times: 7
Loss after 4488052 batches: 0.0652
trigger times: 8
Loss after 4488989 batches: 0.0643
trigger times: 0
Loss after 4489926 batches: 0.0650
trigger times: 1
Loss after 4490863 batches: 0.0650
trigger times: 2
Loss after 4491800 batches: 0.0631
trigger times: 3
Loss after 4492737 batches: 0.0625
trigger times: 4
Loss after 4493674 batches: 0.0628
trigger times: 5
Loss after 4494611 batches: 0.0624
trigger times: 6
Loss after 4495548 batches: 0.0615
trigger times: 7
Loss after 4496485 batches: 0.0615
trigger times: 8
Loss after 4497422 batches: 0.0612
trigger times: 9
Loss after 4498359 batches: 0.0612
trigger times: 10
Loss after 4499296 batches: 0.0619
trigger times: 11
Loss after 4500233 batches: 0.0604
trigger times: 12
Loss after 4501170 batches: 0.0589
trigger times: 13
Loss after 4502107 batches: 0.0592
trigger times: 14
Loss after 4503044 batches: 0.0589
trigger times: 15
Loss after 4503981 batches: 0.0586
trigger times: 16
Loss after 4504918 batches: 0.0577
trigger times: 17
Loss after 4505855 batches: 0.0588
trigger times: 18
Loss after 4506792 batches: 0.0584
trigger times: 19
Loss after 4507729 batches: 0.0577
trigger times: 20
Loss after 4508666 batches: 0.0584
trigger times: 21
Loss after 4509603 batches: 0.0574
trigger times: 22
Loss after 4510540 batches: 0.0563
trigger times: 23
Loss after 4511477 batches: 0.0574
trigger times: 24
Loss after 4512414 batches: 0.0575
trigger times: 25
Early stopping!
Start to test process.
Loss after 4513351 batches: 0.0566
Time to train on one home:  72.11637234687805
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934]]
Round_2_results:  [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4709 < 4710; dropping {'Training_Loss': 0.08159178814717702, 'Validation_Loss': 0.11651576310396194, 'Training_R2': -0.19907287051694555, 'Validation_R2': 0.04191973597348708, 'Training_F1': 0.3425097632042003, 'Validation_F1': 0.3532308612195265, 'Training_NEP': 0.8940894437230443, 'Validation_NEP': 1.1284053932516027, 'Training_NDE': 0.7302644530037465, 'Validation_NDE': 0.8656027450605291, 'Training_MAE': 33.97568652570663, 'Validation_MAE': 40.12527625632594, 'Training_MSE': 2697.149, 'Validation_MSE': 11339.407}.
trigger times: 0
Loss after 4514313 batches: 0.0816
trigger times: 1
Loss after 4515275 batches: 0.0712
trigger times: 2
Loss after 4516237 batches: 0.0728
trigger times: 3
Loss after 4517199 batches: 0.0706
trigger times: 4
Loss after 4518161 batches: 0.0685
trigger times: 5
Loss after 4519123 batches: 0.0662
trigger times: 6
Loss after 4520085 batches: 0.0663
trigger times: 7
Loss after 4521047 batches: 0.0650
trigger times: 8
Loss after 4522009 batches: 0.0630
trigger times: 9
Loss after 4522971 batches: 0.0630
trigger times: 10
Loss after 4523933 batches: 0.0623
trigger times: 11
Loss after 4524895 batches: 0.0620
trigger times: 12
Loss after 4525857 batches: 0.0626
trigger times: 13
Loss after 4526819 batches: 0.0625
trigger times: 14
Loss after 4527781 batches: 0.0613
trigger times: 15
Loss after 4528743 batches: 0.0613
trigger times: 16
Loss after 4529705 batches: 0.0605
trigger times: 17
Loss after 4530667 batches: 0.0600
trigger times: 18
Loss after 4531629 batches: 0.0608
trigger times: 19
Loss after 4532591 batches: 0.0610
trigger times: 20
Loss after 4533553 batches: 0.0601
trigger times: 21
Loss after 4534515 batches: 0.0598
trigger times: 22
Loss after 4535477 batches: 0.0604
trigger times: 0
Loss after 4536439 batches: 0.0606
trigger times: 1
Loss after 4537401 batches: 0.0609
trigger times: 2
Loss after 4538363 batches: 0.0601
trigger times: 3
Loss after 4539325 batches: 0.0607
trigger times: 4
Loss after 4540287 batches: 0.0590
trigger times: 5
Loss after 4541249 batches: 0.0594
trigger times: 6
Loss after 4542211 batches: 0.0595
trigger times: 7
Loss after 4543173 batches: 0.0587
trigger times: 8
Loss after 4544135 batches: 0.0596
trigger times: 9
Loss after 4545097 batches: 0.0592
trigger times: 10
Loss after 4546059 batches: 0.0594
trigger times: 11
Loss after 4547021 batches: 0.0590
trigger times: 12
Loss after 4547983 batches: 0.0600
trigger times: 13
Loss after 4548945 batches: 0.0584
trigger times: 14
Loss after 4549907 batches: 0.0576
trigger times: 15
Loss after 4550869 batches: 0.0574
trigger times: 16
Loss after 4551831 batches: 0.0577
trigger times: 17
Loss after 4552793 batches: 0.0577
trigger times: 18
Loss after 4553755 batches: 0.0581
trigger times: 19
Loss after 4554717 batches: 0.0577
trigger times: 20
Loss after 4555679 batches: 0.0570
trigger times: 21
Loss after 4556641 batches: 0.0577
trigger times: 22
Loss after 4557603 batches: 0.0565
trigger times: 23
Loss after 4558565 batches: 0.0572
trigger times: 24
Loss after 4559527 batches: 0.0569
trigger times: 25
Early stopping!
Start to test process.
Loss after 4560489 batches: 0.0575
Time to train on one home:  68.16771602630615
trigger times: 0
Loss after 4561418 batches: 0.0825
trigger times: 1
Loss after 4562347 batches: 0.0643
trigger times: 2
Loss after 4563276 batches: 0.0533
trigger times: 3
Loss after 4564205 batches: 0.0498
trigger times: 4
Loss after 4565134 batches: 0.0458
trigger times: 5
Loss after 4566063 batches: 0.0454
trigger times: 6
Loss after 4566992 batches: 0.0426
trigger times: 7
Loss after 4567921 batches: 0.0452
trigger times: 8
Loss after 4568850 batches: 0.0404
trigger times: 9
Loss after 4569779 batches: 0.0407
trigger times: 10
Loss after 4570708 batches: 0.0414
trigger times: 11
Loss after 4571637 batches: 0.0490
trigger times: 12
Loss after 4572566 batches: 0.0433
trigger times: 13
Loss after 4573495 batches: 0.0420
trigger times: 14
Loss after 4574424 batches: 0.0394
trigger times: 15
Loss after 4575353 batches: 0.0401
trigger times: 16
Loss after 4576282 batches: 0.0404
trigger times: 17
Loss after 4577211 batches: 0.0376
trigger times: 18
Loss after 4578140 batches: 0.0379
trigger times: 19
Loss after 4579069 batches: 0.0361
trigger times: 20
Loss after 4579998 batches: 0.0363
trigger times: 21
Loss after 4580927 batches: 0.0346
trigger times: 22
Loss after 4581856 batches: 0.0362
trigger times: 23
Loss after 4582785 batches: 0.0351
trigger times: 24
Loss after 4583714 batches: 0.0370
trigger times: 25
Early stopping!
Start to test process.
Loss after 4584643 batches: 0.0365
Time to train on one home:  50.886242389678955
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 4585606 batches: 0.0642
trigger times: 1
Loss after 4586569 batches: 0.0156
trigger times: 2
Loss after 4587532 batches: 0.0151
trigger times: 3
Loss after 4588495 batches: 0.0143
trigger times: 4
Loss after 4589458 batches: 0.0142
trigger times: 5
Loss after 4590421 batches: 0.0141
trigger times: 6
Loss after 4591384 batches: 0.0139
trigger times: 7
Loss after 4592347 batches: 0.0138
trigger times: 8
Loss after 4593310 batches: 0.0138
trigger times: 9
Loss after 4594273 batches: 0.0138
trigger times: 10
Loss after 4595236 batches: 0.0137
trigger times: 11
Loss after 4596199 batches: 0.0139
trigger times: 12
Loss after 4597162 batches: 0.0138
trigger times: 13
Loss after 4598125 batches: 0.0140
trigger times: 14
Loss after 4599088 batches: 0.0140
trigger times: 15
Loss after 4600051 batches: 0.0139
trigger times: 16
Loss after 4601014 batches: 0.0139
trigger times: 17
Loss after 4601977 batches: 0.0139
trigger times: 18
Loss after 4602940 batches: 0.0139
trigger times: 19
Loss after 4603903 batches: 0.0139
trigger times: 20
Loss after 4604866 batches: 0.0138
trigger times: 21
Loss after 4605829 batches: 0.0139
trigger times: 22
Loss after 4606792 batches: 0.0141
trigger times: 23
Loss after 4607755 batches: 0.0139
trigger times: 24
Loss after 4608718 batches: 0.0139
trigger times: 25
Early stopping!
Start to test process.
Loss after 4609681 batches: 0.0138
Time to train on one home:  51.30831289291382
trigger times: 0
Loss after 4610644 batches: 0.0411
trigger times: 1
Loss after 4611607 batches: 0.0382
trigger times: 0
Loss after 4612570 batches: 0.0367
trigger times: 0
Loss after 4613533 batches: 0.0339
trigger times: 0
Loss after 4614496 batches: 0.0318
trigger times: 0
Loss after 4615459 batches: 0.0299
trigger times: 0
Loss after 4616422 batches: 0.0292
trigger times: 0
Loss after 4617385 batches: 0.0276
trigger times: 0
Loss after 4618348 batches: 0.0273
trigger times: 1
Loss after 4619311 batches: 0.0266
trigger times: 2
Loss after 4620274 batches: 0.0257
trigger times: 0
Loss after 4621237 batches: 0.0255
trigger times: 1
Loss after 4622200 batches: 0.0244
trigger times: 2
Loss after 4623163 batches: 0.0241
trigger times: 3
Loss after 4624126 batches: 0.0238
trigger times: 4
Loss after 4625089 batches: 0.0237
trigger times: 0
Loss after 4626052 batches: 0.0238
trigger times: 0
Loss after 4627015 batches: 0.0230
trigger times: 1
Loss after 4627978 batches: 0.0223
trigger times: 2
Loss after 4628941 batches: 0.0218
trigger times: 3
Loss after 4629904 batches: 0.0227
trigger times: 4
Loss after 4630867 batches: 0.0223
trigger times: 5
Loss after 4631830 batches: 0.0213
trigger times: 6
Loss after 4632793 batches: 0.0215
trigger times: 7
Loss after 4633756 batches: 0.0205
trigger times: 0
Loss after 4634719 batches: 0.0208
trigger times: 1
Loss after 4635682 batches: 0.0205
trigger times: 2
Loss after 4636645 batches: 0.0205
trigger times: 3
Loss after 4637608 batches: 0.0203
trigger times: 4
Loss after 4638571 batches: 0.0202
trigger times: 5
Loss after 4639534 batches: 0.0201
trigger times: 6
Loss after 4640497 batches: 0.0195
trigger times: 0
Loss after 4641460 batches: 0.0198
trigger times: 1
Loss after 4642423 batches: 0.0191
trigger times: 0
Loss after 4643386 batches: 0.0192
trigger times: 1
Loss after 4644349 batches: 0.0193
trigger times: 2
Loss after 4645312 batches: 0.0191
trigger times: 3
Loss after 4646275 batches: 0.0188
trigger times: 0
Loss after 4647238 batches: 0.0184
trigger times: 1
Loss after 4648201 batches: 0.0179
trigger times: 2
Loss after 4649164 batches: 0.0184
trigger times: 3
Loss after 4650127 batches: 0.0185
trigger times: 4
Loss after 4651090 batches: 0.0179
trigger times: 5
Loss after 4652053 batches: 0.0180
trigger times: 6
Loss after 4653016 batches: 0.0183
trigger times: 7
Loss after 4653979 batches: 0.0176
trigger times: 8
Loss after 4654942 batches: 0.0173
trigger times: 9
Loss after 4655905 batches: 0.0175
trigger times: 10
Loss after 4656868 batches: 0.0173
trigger times: 11
Loss after 4657831 batches: 0.0174
trigger times: 12
Loss after 4658794 batches: 0.0171
trigger times: 13
Loss after 4659757 batches: 0.0171
trigger times: 14
Loss after 4660720 batches: 0.0170
trigger times: 15
Loss after 4661683 batches: 0.0169
trigger times: 16
Loss after 4662646 batches: 0.0171
trigger times: 17
Loss after 4663609 batches: 0.0170
trigger times: 18
Loss after 4664572 batches: 0.0169
trigger times: 19
Loss after 4665535 batches: 0.0172
trigger times: 20
Loss after 4666498 batches: 0.0169
trigger times: 21
Loss after 4667461 batches: 0.0164
trigger times: 22
Loss after 4668424 batches: 0.0162
trigger times: 23
Loss after 4669387 batches: 0.0157
trigger times: 24
Loss after 4670350 batches: 0.0164
trigger times: 25
Early stopping!
Start to test process.
Loss after 4671313 batches: 0.0170
Time to train on one home:  78.48969721794128
trigger times: 0
Loss after 4672276 batches: 0.1050
trigger times: 0
Loss after 4673239 batches: 0.1006
trigger times: 1
Loss after 4674202 batches: 0.0969
trigger times: 2
Loss after 4675165 batches: 0.0950
trigger times: 3
Loss after 4676128 batches: 0.0939
trigger times: 4
Loss after 4677091 batches: 0.0914
trigger times: 5
Loss after 4678054 batches: 0.0893
trigger times: 6
Loss after 4679017 batches: 0.0880
trigger times: 7
Loss after 4679980 batches: 0.0865
trigger times: 8
Loss after 4680943 batches: 0.0865
trigger times: 0
Loss after 4681906 batches: 0.0858
trigger times: 1
Loss after 4682869 batches: 0.0847
trigger times: 2
Loss after 4683832 batches: 0.0862
trigger times: 3
Loss after 4684795 batches: 0.0828
trigger times: 4
Loss after 4685758 batches: 0.0822
trigger times: 5
Loss after 4686721 batches: 0.0830
trigger times: 6
Loss after 4687684 batches: 0.0806
trigger times: 7
Loss after 4688647 batches: 0.0815
trigger times: 8
Loss after 4689610 batches: 0.0810
trigger times: 9
Loss after 4690573 batches: 0.0792
trigger times: 10
Loss after 4691536 batches: 0.0797
trigger times: 11
Loss after 4692499 batches: 0.0787
trigger times: 12
Loss after 4693462 batches: 0.0775
trigger times: 13
Loss after 4694425 batches: 0.0777
trigger times: 0
Loss after 4695388 batches: 0.0773
trigger times: 1
Loss after 4696351 batches: 0.0770
trigger times: 2
Loss after 4697314 batches: 0.0757
trigger times: 3
Loss after 4698277 batches: 0.0760
trigger times: 4
Loss after 4699240 batches: 0.0756
trigger times: 5
Loss after 4700203 batches: 0.0748
trigger times: 6
Loss after 4701166 batches: 0.0754
trigger times: 7
Loss after 4702129 batches: 0.0757
trigger times: 8
Loss after 4703092 batches: 0.0748
trigger times: 9
Loss after 4704055 batches: 0.0739
trigger times: 10
Loss after 4705018 batches: 0.0744
trigger times: 11
Loss after 4705981 batches: 0.0725
trigger times: 12
Loss after 4706944 batches: 0.0708
trigger times: 13
Loss after 4707907 batches: 0.0736
trigger times: 14
Loss after 4708870 batches: 0.0741
trigger times: 15
Loss after 4709833 batches: 0.0714
trigger times: 16
Loss after 4710796 batches: 0.0722
trigger times: 17
Loss after 4711759 batches: 0.0712
trigger times: 18
Loss after 4712722 batches: 0.0710
trigger times: 19
Loss after 4713685 batches: 0.0711
trigger times: 20
Loss after 4714648 batches: 0.0710
trigger times: 21
Loss after 4715611 batches: 0.0722
trigger times: 22
Loss after 4716574 batches: 0.0695
trigger times: 23
Loss after 4717537 batches: 0.0691
trigger times: 24
Loss after 4718500 batches: 0.0694
trigger times: 25
Early stopping!
Start to test process.
Loss after 4719463 batches: 0.0673
Time to train on one home:  68.63795399665833
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4720426 batches: 0.0967
trigger times: 1
Loss after 4721389 batches: 0.0897
trigger times: 2
Loss after 4722352 batches: 0.0845
trigger times: 3
Loss after 4723315 batches: 0.0826
trigger times: 4
Loss after 4724278 batches: 0.0797
trigger times: 5
Loss after 4725241 batches: 0.0783
trigger times: 6
Loss after 4726204 batches: 0.0776
trigger times: 7
Loss after 4727167 batches: 0.0758
trigger times: 8
Loss after 4728130 batches: 0.0762
trigger times: 9
Loss after 4729093 batches: 0.0762
trigger times: 10
Loss after 4730056 batches: 0.0750
trigger times: 11
Loss after 4731019 batches: 0.0734
trigger times: 12
Loss after 4731982 batches: 0.0737
trigger times: 13
Loss after 4732945 batches: 0.0727
trigger times: 14
Loss after 4733908 batches: 0.0716
trigger times: 15
Loss after 4734871 batches: 0.0712
trigger times: 16
Loss after 4735834 batches: 0.0706
trigger times: 17
Loss after 4736797 batches: 0.0710
trigger times: 18
Loss after 4737760 batches: 0.0711
trigger times: 19
Loss after 4738723 batches: 0.0692
trigger times: 20
Loss after 4739686 batches: 0.0697
trigger times: 21
Loss after 4740649 batches: 0.0680
trigger times: 22
Loss after 4741612 batches: 0.0683
trigger times: 23
Loss after 4742575 batches: 0.0670
trigger times: 24
Loss after 4743538 batches: 0.0672
trigger times: 25
Early stopping!
Start to test process.
Loss after 4744501 batches: 0.0668
Time to train on one home:  51.129756927490234
trigger times: 0
Loss after 4745464 batches: 0.0898
trigger times: 0
Loss after 4746427 batches: 0.0844
trigger times: 1
Loss after 4747390 batches: 0.0818
trigger times: 2
Loss after 4748353 batches: 0.0780
trigger times: 3
Loss after 4749316 batches: 0.0751
trigger times: 0
Loss after 4750279 batches: 0.0732
trigger times: 1
Loss after 4751242 batches: 0.0721
trigger times: 0
Loss after 4752205 batches: 0.0704
trigger times: 0
Loss after 4753168 batches: 0.0698
trigger times: 1
Loss after 4754131 batches: 0.0685
trigger times: 0
Loss after 4755094 batches: 0.0679
trigger times: 1
Loss after 4756057 batches: 0.0684
trigger times: 2
Loss after 4757020 batches: 0.0691
trigger times: 0
Loss after 4757983 batches: 0.0668
trigger times: 1
Loss after 4758946 batches: 0.0655
trigger times: 0
Loss after 4759909 batches: 0.0657
trigger times: 1
Loss after 4760872 batches: 0.0654
trigger times: 2
Loss after 4761835 batches: 0.0656
trigger times: 0
Loss after 4762798 batches: 0.0649
trigger times: 1
Loss after 4763761 batches: 0.0650
trigger times: 2
Loss after 4764724 batches: 0.0634
trigger times: 3
Loss after 4765687 batches: 0.0631
trigger times: 4
Loss after 4766650 batches: 0.0634
trigger times: 5
Loss after 4767613 batches: 0.0625
trigger times: 0
Loss after 4768576 batches: 0.0636
trigger times: 1
Loss after 4769539 batches: 0.0620
trigger times: 2
Loss after 4770502 batches: 0.0612
trigger times: 3
Loss after 4771465 batches: 0.0623
trigger times: 4
Loss after 4772428 batches: 0.0616
trigger times: 5
Loss after 4773391 batches: 0.0621
trigger times: 6
Loss after 4774354 batches: 0.0624
trigger times: 7
Loss after 4775317 batches: 0.0620
trigger times: 8
Loss after 4776280 batches: 0.0620
trigger times: 9
Loss after 4777243 batches: 0.0609
trigger times: 10
Loss after 4778206 batches: 0.0602
trigger times: 11
Loss after 4779169 batches: 0.0606
trigger times: 12
Loss after 4780132 batches: 0.0605
trigger times: 13
Loss after 4781095 batches: 0.0596
trigger times: 14
Loss after 4782058 batches: 0.0600
trigger times: 15
Loss after 4783021 batches: 0.0594
trigger times: 16
Loss after 4783984 batches: 0.0599
trigger times: 17
Loss after 4784947 batches: 0.0592
trigger times: 18
Loss after 4785910 batches: 0.0602
trigger times: 19
Loss after 4786873 batches: 0.0591
trigger times: 20
Loss after 4787836 batches: 0.0597
trigger times: 21
Loss after 4788799 batches: 0.0578
trigger times: 22
Loss after 4789762 batches: 0.0577
trigger times: 23
Loss after 4790725 batches: 0.0569
trigger times: 24
Loss after 4791688 batches: 0.0565
trigger times: 25
Early stopping!
Start to test process.
Loss after 4792651 batches: 0.0576
Time to train on one home:  68.89520454406738
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4793614 batches: 0.0651
trigger times: 1
Loss after 4794577 batches: 0.0588
trigger times: 0
Loss after 4795540 batches: 0.0589
trigger times: 0
Loss after 4796503 batches: 0.0542
trigger times: 1
Loss after 4797466 batches: 0.0527
trigger times: 2
Loss after 4798429 batches: 0.0509
trigger times: 3
Loss after 4799392 batches: 0.0475
trigger times: 4
Loss after 4800355 batches: 0.0463
trigger times: 5
Loss after 4801318 batches: 0.0448
trigger times: 6
Loss after 4802281 batches: 0.0426
trigger times: 7
Loss after 4803244 batches: 0.0419
trigger times: 8
Loss after 4804207 batches: 0.0405
trigger times: 9
Loss after 4805170 batches: 0.0395
trigger times: 0
Loss after 4806133 batches: 0.0398
trigger times: 1
Loss after 4807096 batches: 0.0387
trigger times: 2
Loss after 4808059 batches: 0.0370
trigger times: 3
Loss after 4809022 batches: 0.0375
trigger times: 4
Loss after 4809985 batches: 0.0378
trigger times: 5
Loss after 4810948 batches: 0.0368
trigger times: 6
Loss after 4811911 batches: 0.0362
trigger times: 7
Loss after 4812874 batches: 0.0354
trigger times: 8
Loss after 4813837 batches: 0.0354
trigger times: 9
Loss after 4814800 batches: 0.0352
trigger times: 10
Loss after 4815763 batches: 0.0341
trigger times: 11
Loss after 4816726 batches: 0.0348
trigger times: 12
Loss after 4817689 batches: 0.0338
trigger times: 13
Loss after 4818652 batches: 0.0333
trigger times: 14
Loss after 4819615 batches: 0.0332
trigger times: 15
Loss after 4820578 batches: 0.0331
trigger times: 16
Loss after 4821541 batches: 0.0334
trigger times: 17
Loss after 4822504 batches: 0.0322
trigger times: 18
Loss after 4823467 batches: 0.0319
trigger times: 19
Loss after 4824430 batches: 0.0313
trigger times: 20
Loss after 4825393 batches: 0.0311
trigger times: 21
Loss after 4826356 batches: 0.0318
trigger times: 22
Loss after 4827319 batches: 0.0310
trigger times: 23
Loss after 4828282 batches: 0.0307
trigger times: 24
Loss after 4829245 batches: 0.0305
trigger times: 25
Early stopping!
Start to test process.
Loss after 4830208 batches: 0.0300
Time to train on one home:  60.51757478713989
trigger times: 0
Loss after 4831166 batches: 0.0747
trigger times: 1
Loss after 4832124 batches: 0.0532
trigger times: 2
Loss after 4833082 batches: 0.0491
trigger times: 3
Loss after 4834040 batches: 0.0447
trigger times: 4
Loss after 4834998 batches: 0.0415
trigger times: 5
Loss after 4835956 batches: 0.0396
trigger times: 6
Loss after 4836914 batches: 0.0373
trigger times: 7
Loss after 4837872 batches: 0.0351
trigger times: 8
Loss after 4838830 batches: 0.0320
trigger times: 9
Loss after 4839788 batches: 0.0325
trigger times: 10
Loss after 4840746 batches: 0.0324
trigger times: 11
Loss after 4841704 batches: 0.0302
trigger times: 12
Loss after 4842662 batches: 0.0290
trigger times: 13
Loss after 4843620 batches: 0.0290
trigger times: 14
Loss after 4844578 batches: 0.0285
trigger times: 15
Loss after 4845536 batches: 0.0283
trigger times: 16
Loss after 4846494 batches: 0.0273
trigger times: 17
Loss after 4847452 batches: 0.0312
trigger times: 18
Loss after 4848410 batches: 0.0314
trigger times: 19
Loss after 4849368 batches: 0.0293
trigger times: 20
Loss after 4850326 batches: 0.0283
trigger times: 21
Loss after 4851284 batches: 0.0279
trigger times: 22
Loss after 4852242 batches: 0.0281
trigger times: 23
Loss after 4853200 batches: 0.0271
trigger times: 24
Loss after 4854158 batches: 0.0271
trigger times: 25
Early stopping!
Start to test process.
Loss after 4855116 batches: 0.0260
Time to train on one home:  50.96549868583679
trigger times: 0
Loss after 4856078 batches: 0.0818
trigger times: 1
Loss after 4857040 batches: 0.0712
trigger times: 2
Loss after 4858002 batches: 0.0730
trigger times: 3
Loss after 4858964 batches: 0.0700
trigger times: 4
Loss after 4859926 batches: 0.0684
trigger times: 5
Loss after 4860888 batches: 0.0664
trigger times: 6
Loss after 4861850 batches: 0.0654
trigger times: 0
Loss after 4862812 batches: 0.0642
trigger times: 1
Loss after 4863774 batches: 0.0643
trigger times: 2
Loss after 4864736 batches: 0.0641
trigger times: 3
Loss after 4865698 batches: 0.0630
trigger times: 4
Loss after 4866660 batches: 0.0627
trigger times: 5
Loss after 4867622 batches: 0.0621
trigger times: 6
Loss after 4868584 batches: 0.0625
trigger times: 7
Loss after 4869546 batches: 0.0619
trigger times: 8
Loss after 4870508 batches: 0.0618
trigger times: 9
Loss after 4871470 batches: 0.0615
trigger times: 10
Loss after 4872432 batches: 0.0607
trigger times: 11
Loss after 4873394 batches: 0.0616
trigger times: 12
Loss after 4874356 batches: 0.0611
trigger times: 0
Loss after 4875318 batches: 0.0612
trigger times: 1
Loss after 4876280 batches: 0.0606
trigger times: 2
Loss after 4877242 batches: 0.0601
trigger times: 3
Loss after 4878204 batches: 0.0599
trigger times: 4
Loss after 4879166 batches: 0.0594
trigger times: 0
Loss after 4880128 batches: 0.0601
trigger times: 1
Loss after 4881090 batches: 0.0595
trigger times: 2
Loss after 4882052 batches: 0.0595
trigger times: 3
Loss after 4883014 batches: 0.0587
trigger times: 4
Loss after 4883976 batches: 0.0593
trigger times: 5
Loss after 4884938 batches: 0.0594
trigger times: 6
Loss after 4885900 batches: 0.0582
trigger times: 7
Loss after 4886862 batches: 0.0587
trigger times: 8
Loss after 4887824 batches: 0.0585
trigger times: 9
Loss after 4888786 batches: 0.0581
trigger times: 10
Loss after 4889748 batches: 0.0577
trigger times: 11
Loss after 4890710 batches: 0.0590
trigger times: 12
Loss after 4891672 batches: 0.0590
trigger times: 13
Loss after 4892634 batches: 0.0585
trigger times: 14
Loss after 4893596 batches: 0.0585
trigger times: 0
Loss after 4894558 batches: 0.0581
trigger times: 1
Loss after 4895520 batches: 0.0578
trigger times: 2
Loss after 4896482 batches: 0.0568
trigger times: 3
Loss after 4897444 batches: 0.0579
trigger times: 4
Loss after 4898406 batches: 0.0578
trigger times: 5
Loss after 4899368 batches: 0.0578
trigger times: 6
Loss after 4900330 batches: 0.0572
trigger times: 7
Loss after 4901292 batches: 0.0563
trigger times: 8
Loss after 4902254 batches: 0.0561
trigger times: 9
Loss after 4903216 batches: 0.0565
trigger times: 10
Loss after 4904178 batches: 0.0585
trigger times: 11
Loss after 4905140 batches: 0.0581
trigger times: 12
Loss after 4906102 batches: 0.0587
trigger times: 13
Loss after 4907064 batches: 0.0576
trigger times: 14
Loss after 4908026 batches: 0.0572
trigger times: 15
Loss after 4908988 batches: 0.0568
trigger times: 16
Loss after 4909950 batches: 0.0577
trigger times: 17
Loss after 4910912 batches: 0.0572
trigger times: 18
Loss after 4911874 batches: 0.0562
trigger times: 19
Loss after 4912836 batches: 0.0560
trigger times: 20
Loss after 4913798 batches: 0.0560
trigger times: 21
Loss after 4914760 batches: 0.0561
trigger times: 22
Loss after 4915722 batches: 0.0563
trigger times: 23
Loss after 4916684 batches: 0.0555
trigger times: 24
Loss after 4917646 batches: 0.0575
trigger times: 25
Early stopping!
Start to test process.
Loss after 4918608 batches: 0.0570
Time to train on one home:  80.5572783946991
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4919571 batches: 0.1020
trigger times: 1
Loss after 4920534 batches: 0.0277
trigger times: 2
Loss after 4921497 batches: 0.0276
trigger times: 3
Loss after 4922460 batches: 0.0248
trigger times: 4
Loss after 4923423 batches: 0.0227
trigger times: 5
Loss after 4924386 batches: 0.0227
trigger times: 6
Loss after 4925349 batches: 0.0225
trigger times: 7
Loss after 4926312 batches: 0.0225
trigger times: 8
Loss after 4927275 batches: 0.0222
trigger times: 9
Loss after 4928238 batches: 0.0225
trigger times: 10
Loss after 4929201 batches: 0.0226
trigger times: 11
Loss after 4930164 batches: 0.0225
trigger times: 12
Loss after 4931127 batches: 0.0222
trigger times: 13
Loss after 4932090 batches: 0.0223
trigger times: 14
Loss after 4933053 batches: 0.0222
trigger times: 15
Loss after 4934016 batches: 0.0227
trigger times: 16
Loss after 4934979 batches: 0.0219
trigger times: 17
Loss after 4935942 batches: 0.0216
trigger times: 18
Loss after 4936905 batches: 0.0209
trigger times: 19
Loss after 4937868 batches: 0.0201
trigger times: 20
Loss after 4938831 batches: 0.0194
trigger times: 21
Loss after 4939794 batches: 0.0184
trigger times: 22
Loss after 4940757 batches: 0.0177
trigger times: 23
Loss after 4941720 batches: 0.0172
trigger times: 24
Loss after 4942683 batches: 0.0170
trigger times: 25
Early stopping!
Start to test process.
Loss after 4943646 batches: 0.0167
Time to train on one home:  51.62612771987915
trigger times: 0
Loss after 4944609 batches: 0.0677
trigger times: 0
Loss after 4945572 batches: 0.0606
trigger times: 0
Loss after 4946535 batches: 0.0564
trigger times: 1
Loss after 4947498 batches: 0.0530
trigger times: 0
Loss after 4948461 batches: 0.0510
trigger times: 0
Loss after 4949424 batches: 0.0490
trigger times: 0
Loss after 4950387 batches: 0.0472
trigger times: 1
Loss after 4951350 batches: 0.0469
trigger times: 0
Loss after 4952313 batches: 0.0447
trigger times: 0
Loss after 4953276 batches: 0.0440
trigger times: 1
Loss after 4954239 batches: 0.0440
trigger times: 0
Loss after 4955202 batches: 0.0433
trigger times: 0
Loss after 4956165 batches: 0.0424
trigger times: 0
Loss after 4957128 batches: 0.0432
trigger times: 0
Loss after 4958091 batches: 0.0418
trigger times: 1
Loss after 4959054 batches: 0.0415
trigger times: 2
Loss after 4960017 batches: 0.0416
trigger times: 3
Loss after 4960980 batches: 0.0407
trigger times: 4
Loss after 4961943 batches: 0.0401
trigger times: 5
Loss after 4962906 batches: 0.0399
trigger times: 6
Loss after 4963869 batches: 0.0391
trigger times: 0
Loss after 4964832 batches: 0.0385
trigger times: 1
Loss after 4965795 batches: 0.0400
trigger times: 0
Loss after 4966758 batches: 0.0398
trigger times: 1
Loss after 4967721 batches: 0.0376
trigger times: 2
Loss after 4968684 batches: 0.0390
trigger times: 3
Loss after 4969647 batches: 0.0388
trigger times: 4
Loss after 4970610 batches: 0.0392
trigger times: 5
Loss after 4971573 batches: 0.0391
trigger times: 6
Loss after 4972536 batches: 0.0385
trigger times: 7
Loss after 4973499 batches: 0.0387
trigger times: 8
Loss after 4974462 batches: 0.0364
trigger times: 9
Loss after 4975425 batches: 0.0369
trigger times: 10
Loss after 4976388 batches: 0.0363
trigger times: 11
Loss after 4977351 batches: 0.0363
trigger times: 12
Loss after 4978314 batches: 0.0382
trigger times: 13
Loss after 4979277 batches: 0.0366
trigger times: 14
Loss after 4980240 batches: 0.0365
trigger times: 15
Loss after 4981203 batches: 0.0363
trigger times: 16
Loss after 4982166 batches: 0.0365
trigger times: 17
Loss after 4983129 batches: 0.0357
trigger times: 18
Loss after 4984092 batches: 0.0361
trigger times: 19
Loss after 4985055 batches: 0.0347
trigger times: 20
Loss after 4986018 batches: 0.0354
trigger times: 21
Loss after 4986981 batches: 0.0360
trigger times: 22
Loss after 4987944 batches: 0.0346
trigger times: 23
Loss after 4988907 batches: 0.0347
trigger times: 24
Loss after 4989870 batches: 0.0343
trigger times: 25
Early stopping!
Start to test process.
Loss after 4990833 batches: 0.0338
Time to train on one home:  68.37441968917847
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 4991796 batches: 0.0650
trigger times: 0
Loss after 4992759 batches: 0.0589
trigger times: 0
Loss after 4993722 batches: 0.0586
trigger times: 0
Loss after 4994685 batches: 0.0543
trigger times: 1
Loss after 4995648 batches: 0.0530
trigger times: 2
Loss after 4996611 batches: 0.0508
trigger times: 3
Loss after 4997574 batches: 0.0492
trigger times: 0
Loss after 4998537 batches: 0.0469
trigger times: 0
Loss after 4999500 batches: 0.0450
trigger times: 1
Loss after 5000463 batches: 0.0429
trigger times: 2
Loss after 5001426 batches: 0.0428
trigger times: 0
Loss after 5002389 batches: 0.0414
trigger times: 1
Loss after 5003352 batches: 0.0397
trigger times: 2
Loss after 5004315 batches: 0.0393
trigger times: 3
Loss after 5005278 batches: 0.0382
trigger times: 4
Loss after 5006241 batches: 0.0375
trigger times: 5
Loss after 5007204 batches: 0.0371
trigger times: 6
Loss after 5008167 batches: 0.0374
trigger times: 7
Loss after 5009130 batches: 0.0363
trigger times: 8
Loss after 5010093 batches: 0.0361
trigger times: 9
Loss after 5011056 batches: 0.0355
trigger times: 10
Loss after 5012019 batches: 0.0352
trigger times: 11
Loss after 5012982 batches: 0.0349
trigger times: 12
Loss after 5013945 batches: 0.0346
trigger times: 13
Loss after 5014908 batches: 0.0343
trigger times: 14
Loss after 5015871 batches: 0.0345
trigger times: 15
Loss after 5016834 batches: 0.0336
trigger times: 16
Loss after 5017797 batches: 0.0346
trigger times: 17
Loss after 5018760 batches: 0.0331
trigger times: 18
Loss after 5019723 batches: 0.0328
trigger times: 19
Loss after 5020686 batches: 0.0320
trigger times: 20
Loss after 5021649 batches: 0.0317
trigger times: 21
Loss after 5022612 batches: 0.0314
trigger times: 22
Loss after 5023575 batches: 0.0321
trigger times: 23
Loss after 5024538 batches: 0.0312
trigger times: 24
Loss after 5025501 batches: 0.0308
trigger times: 25
Early stopping!
Start to test process.
Loss after 5026464 batches: 0.0305
Time to train on one home:  59.241288900375366
trigger times: 0
Loss after 5027427 batches: 0.0671
trigger times: 0
Loss after 5028390 batches: 0.0617
trigger times: 0
Loss after 5029353 batches: 0.0566
trigger times: 1
Loss after 5030316 batches: 0.0539
trigger times: 0
Loss after 5031279 batches: 0.0518
trigger times: 0
Loss after 5032242 batches: 0.0485
trigger times: 0
Loss after 5033205 batches: 0.0475
trigger times: 1
Loss after 5034168 batches: 0.0456
trigger times: 2
Loss after 5035131 batches: 0.0450
trigger times: 0
Loss after 5036094 batches: 0.0437
trigger times: 1
Loss after 5037057 batches: 0.0438
trigger times: 2
Loss after 5038020 batches: 0.0442
trigger times: 3
Loss after 5038983 batches: 0.0444
trigger times: 4
Loss after 5039946 batches: 0.0433
trigger times: 5
Loss after 5040909 batches: 0.0435
trigger times: 6
Loss after 5041872 batches: 0.0423
trigger times: 0
Loss after 5042835 batches: 0.0415
trigger times: 1
Loss after 5043798 batches: 0.0409
trigger times: 2
Loss after 5044761 batches: 0.0413
trigger times: 3
Loss after 5045724 batches: 0.0407
trigger times: 4
Loss after 5046687 batches: 0.0398
trigger times: 0
Loss after 5047650 batches: 0.0398
trigger times: 1
Loss after 5048613 batches: 0.0395
trigger times: 2
Loss after 5049576 batches: 0.0393
trigger times: 3
Loss after 5050539 batches: 0.0396
trigger times: 4
Loss after 5051502 batches: 0.0381
trigger times: 5
Loss after 5052465 batches: 0.0388
trigger times: 6
Loss after 5053428 batches: 0.0397
trigger times: 7
Loss after 5054391 batches: 0.0388
trigger times: 8
Loss after 5055354 batches: 0.0385
trigger times: 9
Loss after 5056317 batches: 0.0388
trigger times: 10
Loss after 5057280 batches: 0.0386
trigger times: 11
Loss after 5058243 batches: 0.0380
trigger times: 12
Loss after 5059206 batches: 0.0378
trigger times: 0
Loss after 5060169 batches: 0.0388
trigger times: 1
Loss after 5061132 batches: 0.0374
trigger times: 2
Loss after 5062095 batches: 0.0372
trigger times: 3
Loss after 5063058 batches: 0.0371
trigger times: 4
Loss after 5064021 batches: 0.0363
trigger times: 5
Loss after 5064984 batches: 0.0366
trigger times: 6
Loss after 5065947 batches: 0.0362
trigger times: 7
Loss after 5066910 batches: 0.0349
trigger times: 8
Loss after 5067873 batches: 0.0361
trigger times: 9
Loss after 5068836 batches: 0.0351
trigger times: 10
Loss after 5069799 batches: 0.0363
trigger times: 11
Loss after 5070762 batches: 0.0358
trigger times: 12
Loss after 5071725 batches: 0.0363
trigger times: 13
Loss after 5072688 batches: 0.0352
trigger times: 14
Loss after 5073651 batches: 0.0353
trigger times: 15
Loss after 5074614 batches: 0.0370
trigger times: 16
Loss after 5075577 batches: 0.0361
trigger times: 17
Loss after 5076540 batches: 0.0358
trigger times: 18
Loss after 5077503 batches: 0.0346
trigger times: 19
Loss after 5078466 batches: 0.0346
trigger times: 20
Loss after 5079429 batches: 0.0341
trigger times: 21
Loss after 5080392 batches: 0.0351
trigger times: 22
Loss after 5081355 batches: 0.0362
trigger times: 23
Loss after 5082318 batches: 0.0353
trigger times: 24
Loss after 5083281 batches: 0.0339
trigger times: 25
Early stopping!
Start to test process.
Loss after 5084244 batches: 0.0340
Time to train on one home:  76.86525344848633
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5085207 batches: 0.0644
trigger times: 0
Loss after 5086170 batches: 0.0617
trigger times: 0
Loss after 5087133 batches: 0.0587
trigger times: 1
Loss after 5088096 batches: 0.0560
trigger times: 0
Loss after 5089059 batches: 0.0546
trigger times: 1
Loss after 5090022 batches: 0.0525
trigger times: 0
Loss after 5090985 batches: 0.0514
trigger times: 1
Loss after 5091948 batches: 0.0502
trigger times: 2
Loss after 5092911 batches: 0.0499
trigger times: 3
Loss after 5093874 batches: 0.0498
trigger times: 0
Loss after 5094837 batches: 0.0492
trigger times: 0
Loss after 5095800 batches: 0.0478
trigger times: 0
Loss after 5096763 batches: 0.0475
trigger times: 1
Loss after 5097726 batches: 0.0472
trigger times: 2
Loss after 5098689 batches: 0.0463
trigger times: 3
Loss after 5099652 batches: 0.0468
trigger times: 4
Loss after 5100615 batches: 0.0473
trigger times: 5
Loss after 5101578 batches: 0.0454
trigger times: 6
Loss after 5102541 batches: 0.0441
trigger times: 0
Loss after 5103504 batches: 0.0442
trigger times: 1
Loss after 5104467 batches: 0.0449
trigger times: 2
Loss after 5105430 batches: 0.0442
trigger times: 3
Loss after 5106393 batches: 0.0444
trigger times: 0
Loss after 5107356 batches: 0.0434
trigger times: 0
Loss after 5108319 batches: 0.0421
trigger times: 1
Loss after 5109282 batches: 0.0417
trigger times: 2
Loss after 5110245 batches: 0.0417
trigger times: 3
Loss after 5111208 batches: 0.0426
trigger times: 0
Loss after 5112171 batches: 0.0416
trigger times: 1
Loss after 5113134 batches: 0.0405
trigger times: 2
Loss after 5114097 batches: 0.0418
trigger times: 3
Loss after 5115060 batches: 0.0410
trigger times: 4
Loss after 5116023 batches: 0.0395
trigger times: 5
Loss after 5116986 batches: 0.0394
trigger times: 0
Loss after 5117949 batches: 0.0400
trigger times: 1
Loss after 5118912 batches: 0.0394
trigger times: 2
Loss after 5119875 batches: 0.0406
trigger times: 3
Loss after 5120838 batches: 0.0405
trigger times: 4
Loss after 5121801 batches: 0.0399
trigger times: 5
Loss after 5122764 batches: 0.0391
trigger times: 0
Loss after 5123727 batches: 0.0386
trigger times: 1
Loss after 5124690 batches: 0.0380
trigger times: 2
Loss after 5125653 batches: 0.0393
trigger times: 3
Loss after 5126616 batches: 0.0392
trigger times: 4
Loss after 5127579 batches: 0.0394
trigger times: 5
Loss after 5128542 batches: 0.0388
trigger times: 0
Loss after 5129505 batches: 0.0379
trigger times: 0
Loss after 5130468 batches: 0.0377
trigger times: 1
Loss after 5131431 batches: 0.0369
trigger times: 2
Loss after 5132394 batches: 0.0375
trigger times: 0
Loss after 5133357 batches: 0.0366
trigger times: 1
Loss after 5134320 batches: 0.0366
trigger times: 2
Loss after 5135283 batches: 0.0365
trigger times: 3
Loss after 5136246 batches: 0.0367
trigger times: 0
Loss after 5137209 batches: 0.0372
trigger times: 1
Loss after 5138172 batches: 0.0376
trigger times: 2
Loss after 5139135 batches: 0.0373
trigger times: 0
Loss after 5140098 batches: 0.0368
trigger times: 1
Loss after 5141061 batches: 0.0354
trigger times: 2
Loss after 5142024 batches: 0.0358
trigger times: 3
Loss after 5142987 batches: 0.0351
trigger times: 4
Loss after 5143950 batches: 0.0358
trigger times: 5
Loss after 5144913 batches: 0.0350
trigger times: 6
Loss after 5145876 batches: 0.0345
trigger times: 0
Loss after 5146839 batches: 0.0342
trigger times: 1
Loss after 5147802 batches: 0.0350
trigger times: 2
Loss after 5148765 batches: 0.0346
trigger times: 3
Loss after 5149728 batches: 0.0354
trigger times: 4
Loss after 5150691 batches: 0.0337
trigger times: 5
Loss after 5151654 batches: 0.0335
trigger times: 6
Loss after 5152617 batches: 0.0359
trigger times: 7
Loss after 5153580 batches: 0.0350
trigger times: 8
Loss after 5154543 batches: 0.0337
trigger times: 9
Loss after 5155506 batches: 0.0333
trigger times: 10
Loss after 5156469 batches: 0.0339
trigger times: 11
Loss after 5157432 batches: 0.0351
trigger times: 12
Loss after 5158395 batches: 0.0329
trigger times: 13
Loss after 5159358 batches: 0.0339
trigger times: 14
Loss after 5160321 batches: 0.0334
trigger times: 15
Loss after 5161284 batches: 0.0325
trigger times: 16
Loss after 5162247 batches: 0.0316
trigger times: 17
Loss after 5163210 batches: 0.0332
trigger times: 18
Loss after 5164173 batches: 0.0338
trigger times: 19
Loss after 5165136 batches: 0.0329
trigger times: 20
Loss after 5166099 batches: 0.0327
trigger times: 21
Loss after 5167062 batches: 0.0357
trigger times: 22
Loss after 5168025 batches: 0.0333
trigger times: 23
Loss after 5168988 batches: 0.0328
trigger times: 24
Loss after 5169951 batches: 0.0323
trigger times: 25
Early stopping!
Start to test process.
Loss after 5170914 batches: 0.0321
Time to train on one home:  99.5632643699646
trigger times: 0
Loss after 5171877 batches: 0.0860
trigger times: 1
Loss after 5172840 batches: 0.0499
trigger times: 2
Loss after 5173803 batches: 0.0514
trigger times: 3
Loss after 5174766 batches: 0.0500
trigger times: 4
Loss after 5175729 batches: 0.0477
trigger times: 5
Loss after 5176692 batches: 0.0464
trigger times: 6
Loss after 5177655 batches: 0.0460
trigger times: 7
Loss after 5178618 batches: 0.0454
trigger times: 8
Loss after 5179581 batches: 0.0444
trigger times: 9
Loss after 5180544 batches: 0.0437
trigger times: 10
Loss after 5181507 batches: 0.0433
trigger times: 11
Loss after 5182470 batches: 0.0432
trigger times: 12
Loss after 5183433 batches: 0.0425
trigger times: 13
Loss after 5184396 batches: 0.0420
trigger times: 14
Loss after 5185359 batches: 0.0406
trigger times: 15
Loss after 5186322 batches: 0.0409
trigger times: 16
Loss after 5187285 batches: 0.0403
trigger times: 17
Loss after 5188248 batches: 0.0408
trigger times: 18
Loss after 5189211 batches: 0.0413
trigger times: 19
Loss after 5190174 batches: 0.0406
trigger times: 20
Loss after 5191137 batches: 0.0402
trigger times: 21
Loss after 5192100 batches: 0.0400
trigger times: 22
Loss after 5193063 batches: 0.0394
trigger times: 23
Loss after 5194026 batches: 0.0393
trigger times: 24
Loss after 5194989 batches: 0.0391
trigger times: 25
Early stopping!
Start to test process.
Loss after 5195952 batches: 0.0390
Time to train on one home:  51.117624282836914
trigger times: 0
Loss after 5196915 batches: 0.1048
trigger times: 0
Loss after 5197878 batches: 0.1002
trigger times: 1
Loss after 5198841 batches: 0.0983
trigger times: 2
Loss after 5199804 batches: 0.0953
trigger times: 3
Loss after 5200767 batches: 0.0937
trigger times: 4
Loss after 5201730 batches: 0.0922
trigger times: 5
Loss after 5202693 batches: 0.0907
trigger times: 6
Loss after 5203656 batches: 0.0887
trigger times: 7
Loss after 5204619 batches: 0.0867
trigger times: 8
Loss after 5205582 batches: 0.0865
trigger times: 9
Loss after 5206545 batches: 0.0852
trigger times: 0
Loss after 5207508 batches: 0.0855
trigger times: 1
Loss after 5208471 batches: 0.0840
trigger times: 2
Loss after 5209434 batches: 0.0837
trigger times: 0
Loss after 5210397 batches: 0.0842
trigger times: 1
Loss after 5211360 batches: 0.0822
trigger times: 2
Loss after 5212323 batches: 0.0831
trigger times: 3
Loss after 5213286 batches: 0.0810
trigger times: 4
Loss after 5214249 batches: 0.0811
trigger times: 5
Loss after 5215212 batches: 0.0816
trigger times: 6
Loss after 5216175 batches: 0.0810
trigger times: 7
Loss after 5217138 batches: 0.0806
trigger times: 8
Loss after 5218101 batches: 0.0813
trigger times: 9
Loss after 5219064 batches: 0.0820
trigger times: 10
Loss after 5220027 batches: 0.0791
trigger times: 11
Loss after 5220990 batches: 0.0788
trigger times: 12
Loss after 5221953 batches: 0.0780
trigger times: 13
Loss after 5222916 batches: 0.0766
trigger times: 14
Loss after 5223879 batches: 0.0756
trigger times: 15
Loss after 5224842 batches: 0.0763
trigger times: 16
Loss after 5225805 batches: 0.0766
trigger times: 17
Loss after 5226768 batches: 0.0780
trigger times: 18
Loss after 5227731 batches: 0.0771
trigger times: 19
Loss after 5228694 batches: 0.0761
trigger times: 20
Loss after 5229657 batches: 0.0737
trigger times: 21
Loss after 5230620 batches: 0.0745
trigger times: 22
Loss after 5231583 batches: 0.0752
trigger times: 23
Loss after 5232546 batches: 0.0740
trigger times: 24
Loss after 5233509 batches: 0.0746
trigger times: 25
Early stopping!
Start to test process.
Loss after 5234472 batches: 0.0726
Time to train on one home:  61.829379081726074
trigger times: 0
Loss after 5235435 batches: 0.0895
trigger times: 0
Loss after 5236398 batches: 0.0790
trigger times: 1
Loss after 5237361 batches: 0.0727
trigger times: 2
Loss after 5238324 batches: 0.0681
trigger times: 3
Loss after 5239287 batches: 0.0651
trigger times: 0
Loss after 5240250 batches: 0.0631
trigger times: 0
Loss after 5241213 batches: 0.0613
trigger times: 1
Loss after 5242176 batches: 0.0592
trigger times: 2
Loss after 5243139 batches: 0.0580
trigger times: 0
Loss after 5244102 batches: 0.0571
trigger times: 1
Loss after 5245065 batches: 0.0549
trigger times: 2
Loss after 5246028 batches: 0.0559
trigger times: 3
Loss after 5246991 batches: 0.0536
trigger times: 4
Loss after 5247954 batches: 0.0544
trigger times: 5
Loss after 5248917 batches: 0.0533
trigger times: 0
Loss after 5249880 batches: 0.0527
trigger times: 1
Loss after 5250843 batches: 0.0516
trigger times: 2
Loss after 5251806 batches: 0.0518
trigger times: 3
Loss after 5252769 batches: 0.0505
trigger times: 4
Loss after 5253732 batches: 0.0506
trigger times: 5
Loss after 5254695 batches: 0.0498
trigger times: 6
Loss after 5255658 batches: 0.0496
trigger times: 7
Loss after 5256621 batches: 0.0493
trigger times: 8
Loss after 5257584 batches: 0.0495
trigger times: 9
Loss after 5258547 batches: 0.0479
trigger times: 10
Loss after 5259510 batches: 0.0495
trigger times: 11
Loss after 5260473 batches: 0.0482
trigger times: 12
Loss after 5261436 batches: 0.0478
trigger times: 0
Loss after 5262399 batches: 0.0480
trigger times: 1
Loss after 5263362 batches: 0.0464
trigger times: 2
Loss after 5264325 batches: 0.0473
trigger times: 3
Loss after 5265288 batches: 0.0467
trigger times: 4
Loss after 5266251 batches: 0.0460
trigger times: 5
Loss after 5267214 batches: 0.0460
trigger times: 6
Loss after 5268177 batches: 0.0461
trigger times: 7
Loss after 5269140 batches: 0.0450
trigger times: 8
Loss after 5270103 batches: 0.0454
trigger times: 9
Loss after 5271066 batches: 0.0444
trigger times: 10
Loss after 5272029 batches: 0.0446
trigger times: 11
Loss after 5272992 batches: 0.0445
trigger times: 12
Loss after 5273955 batches: 0.0447
trigger times: 13
Loss after 5274918 batches: 0.0436
trigger times: 14
Loss after 5275881 batches: 0.0436
trigger times: 15
Loss after 5276844 batches: 0.0440
trigger times: 16
Loss after 5277807 batches: 0.0441
trigger times: 17
Loss after 5278770 batches: 0.0431
trigger times: 18
Loss after 5279733 batches: 0.0441
trigger times: 19
Loss after 5280696 batches: 0.0438
trigger times: 20
Loss after 5281659 batches: 0.0425
trigger times: 21
Loss after 5282622 batches: 0.0434
trigger times: 22
Loss after 5283585 batches: 0.0420
trigger times: 23
Loss after 5284548 batches: 0.0432
trigger times: 24
Loss after 5285511 batches: 0.0420
trigger times: 25
Early stopping!
Start to test process.
Loss after 5286474 batches: 0.0415
Time to train on one home:  71.79037094116211
trigger times: 0
Loss after 5287403 batches: 0.0791
trigger times: 1
Loss after 5288332 batches: 0.0639
trigger times: 2
Loss after 5289261 batches: 0.0554
trigger times: 3
Loss after 5290190 batches: 0.0490
trigger times: 4
Loss after 5291119 batches: 0.0460
trigger times: 5
Loss after 5292048 batches: 0.0447
trigger times: 6
Loss after 5292977 batches: 0.0432
trigger times: 7
Loss after 5293906 batches: 0.0438
trigger times: 8
Loss after 5294835 batches: 0.0507
trigger times: 9
Loss after 5295764 batches: 0.0437
trigger times: 10
Loss after 5296693 batches: 0.0435
trigger times: 11
Loss after 5297622 batches: 0.0418
trigger times: 12
Loss after 5298551 batches: 0.0378
trigger times: 13
Loss after 5299480 batches: 0.0374
trigger times: 14
Loss after 5300409 batches: 0.0406
trigger times: 15
Loss after 5301338 batches: 0.0385
trigger times: 16
Loss after 5302267 batches: 0.0365
trigger times: 17
Loss after 5303196 batches: 0.0366
trigger times: 18
Loss after 5304125 batches: 0.0355
trigger times: 19
Loss after 5305054 batches: 0.0357
trigger times: 20
Loss after 5305983 batches: 0.0375
trigger times: 21
Loss after 5306912 batches: 0.0364
trigger times: 22
Loss after 5307841 batches: 0.0349
trigger times: 23
Loss after 5308770 batches: 0.0339
trigger times: 24
Loss after 5309699 batches: 0.0351
trigger times: 25
Early stopping!
Start to test process.
Loss after 5310628 batches: 0.0366
Time to train on one home:  50.493656158447266
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5311591 batches: 0.0573
trigger times: 1
Loss after 5312554 batches: 0.0294
trigger times: 2
Loss after 5313517 batches: 0.0275
trigger times: 3
Loss after 5314480 batches: 0.0274
trigger times: 4
Loss after 5315443 batches: 0.0275
trigger times: 5
Loss after 5316406 batches: 0.0270
trigger times: 6
Loss after 5317369 batches: 0.0263
trigger times: 7
Loss after 5318332 batches: 0.0258
trigger times: 8
Loss after 5319295 batches: 0.0253
trigger times: 9
Loss after 5320258 batches: 0.0248
trigger times: 10
Loss after 5321221 batches: 0.0248
trigger times: 11
Loss after 5322184 batches: 0.0251
trigger times: 12
Loss after 5323147 batches: 0.0242
trigger times: 13
Loss after 5324110 batches: 0.0241
trigger times: 14
Loss after 5325073 batches: 0.0239
trigger times: 15
Loss after 5326036 batches: 0.0239
trigger times: 16
Loss after 5326999 batches: 0.0236
trigger times: 17
Loss after 5327962 batches: 0.0233
trigger times: 18
Loss after 5328925 batches: 0.0231
trigger times: 19
Loss after 5329888 batches: 0.0230
trigger times: 20
Loss after 5330851 batches: 0.0225
trigger times: 21
Loss after 5331814 batches: 0.0224
trigger times: 22
Loss after 5332777 batches: 0.0225
trigger times: 23
Loss after 5333740 batches: 0.0217
trigger times: 24
Loss after 5334703 batches: 0.0216
trigger times: 25
Early stopping!
Start to test process.
Loss after 5335666 batches: 0.0217
Time to train on one home:  51.81532835960388
trigger times: 0
Loss after 5336629 batches: 0.1603
trigger times: 1
Loss after 5337592 batches: 0.1168
trigger times: 2
Loss after 5338555 batches: 0.1107
trigger times: 3
Loss after 5339518 batches: 0.1045
trigger times: 4
Loss after 5340481 batches: 0.0964
trigger times: 5
Loss after 5341444 batches: 0.0876
trigger times: 6
Loss after 5342407 batches: 0.0861
trigger times: 7
Loss after 5343370 batches: 0.0866
trigger times: 8
Loss after 5344333 batches: 0.0836
trigger times: 9
Loss after 5345296 batches: 0.0820
trigger times: 10
Loss after 5346259 batches: 0.0821
trigger times: 11
Loss after 5347222 batches: 0.0819
trigger times: 12
Loss after 5348185 batches: 0.0802
trigger times: 13
Loss after 5349148 batches: 0.0789
trigger times: 14
Loss after 5350111 batches: 0.0786
trigger times: 15
Loss after 5351074 batches: 0.0762
trigger times: 16
Loss after 5352037 batches: 0.0745
trigger times: 17
Loss after 5353000 batches: 0.0729
trigger times: 18
Loss after 5353963 batches: 0.0714
trigger times: 19
Loss after 5354926 batches: 0.0703
trigger times: 20
Loss after 5355889 batches: 0.0701
trigger times: 21
Loss after 5356852 batches: 0.0700
trigger times: 22
Loss after 5357815 batches: 0.0690
trigger times: 23
Loss after 5358778 batches: 0.0678
trigger times: 24
Loss after 5359741 batches: 0.0650
trigger times: 25
Early stopping!
Start to test process.
Loss after 5360704 batches: 0.0634
Time to train on one home:  51.105260610580444
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5361667 batches: 0.0963
trigger times: 1
Loss after 5362630 batches: 0.0887
trigger times: 2
Loss after 5363593 batches: 0.0853
trigger times: 3
Loss after 5364556 batches: 0.0813
trigger times: 4
Loss after 5365519 batches: 0.0799
trigger times: 5
Loss after 5366482 batches: 0.0787
trigger times: 6
Loss after 5367445 batches: 0.0779
trigger times: 7
Loss after 5368408 batches: 0.0765
trigger times: 8
Loss after 5369371 batches: 0.0750
trigger times: 9
Loss after 5370334 batches: 0.0747
trigger times: 10
Loss after 5371297 batches: 0.0752
trigger times: 11
Loss after 5372260 batches: 0.0749
trigger times: 12
Loss after 5373223 batches: 0.0732
trigger times: 13
Loss after 5374186 batches: 0.0728
trigger times: 14
Loss after 5375149 batches: 0.0708
trigger times: 15
Loss after 5376112 batches: 0.0701
trigger times: 16
Loss after 5377075 batches: 0.0708
trigger times: 17
Loss after 5378038 batches: 0.0696
trigger times: 18
Loss after 5379001 batches: 0.0695
trigger times: 19
Loss after 5379964 batches: 0.0701
trigger times: 20
Loss after 5380927 batches: 0.0689
trigger times: 21
Loss after 5381890 batches: 0.0681
trigger times: 22
Loss after 5382853 batches: 0.0690
trigger times: 23
Loss after 5383816 batches: 0.0690
trigger times: 24
Loss after 5384779 batches: 0.0685
trigger times: 25
Early stopping!
Start to test process.
Loss after 5385742 batches: 0.0671
Time to train on one home:  51.11707592010498
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5386705 batches: 0.0890
trigger times: 1
Loss after 5387668 batches: 0.0801
trigger times: 2
Loss after 5388631 batches: 0.0814
trigger times: 3
Loss after 5389594 batches: 0.0805
trigger times: 4
Loss after 5390557 batches: 0.0781
trigger times: 5
Loss after 5391520 batches: 0.0766
trigger times: 6
Loss after 5392483 batches: 0.0755
trigger times: 7
Loss after 5393446 batches: 0.0750
trigger times: 8
Loss after 5394409 batches: 0.0748
trigger times: 9
Loss after 5395372 batches: 0.0730
trigger times: 10
Loss after 5396335 batches: 0.0729
trigger times: 11
Loss after 5397298 batches: 0.0722
trigger times: 12
Loss after 5398261 batches: 0.0724
trigger times: 13
Loss after 5399224 batches: 0.0710
trigger times: 14
Loss after 5400187 batches: 0.0708
trigger times: 15
Loss after 5401150 batches: 0.0705
trigger times: 16
Loss after 5402113 batches: 0.0705
trigger times: 17
Loss after 5403076 batches: 0.0709
trigger times: 18
Loss after 5404039 batches: 0.0703
trigger times: 19
Loss after 5405002 batches: 0.0684
trigger times: 20
Loss after 5405965 batches: 0.0687
trigger times: 21
Loss after 5406928 batches: 0.0685
trigger times: 22
Loss after 5407891 batches: 0.0673
trigger times: 23
Loss after 5408854 batches: 0.0693
trigger times: 24
Loss after 5409817 batches: 0.0687
trigger times: 25
Early stopping!
Start to test process.
Loss after 5410780 batches: 0.0673
Time to train on one home:  52.282185316085815
trigger times: 0
Loss after 5411743 batches: 0.0411
trigger times: 1
Loss after 5412706 batches: 0.0383
trigger times: 0
Loss after 5413669 batches: 0.0362
trigger times: 0
Loss after 5414632 batches: 0.0337
trigger times: 0
Loss after 5415595 batches: 0.0321
trigger times: 0
Loss after 5416558 batches: 0.0303
trigger times: 0
Loss after 5417521 batches: 0.0286
trigger times: 1
Loss after 5418484 batches: 0.0281
trigger times: 0
Loss after 5419447 batches: 0.0268
trigger times: 0
Loss after 5420410 batches: 0.0263
trigger times: 0
Loss after 5421373 batches: 0.0258
trigger times: 0
Loss after 5422336 batches: 0.0247
trigger times: 1
Loss after 5423299 batches: 0.0245
trigger times: 2
Loss after 5424262 batches: 0.0243
trigger times: 3
Loss after 5425225 batches: 0.0240
trigger times: 0
Loss after 5426188 batches: 0.0235
trigger times: 1
Loss after 5427151 batches: 0.0229
trigger times: 0
Loss after 5428114 batches: 0.0222
trigger times: 1
Loss after 5429077 batches: 0.0224
trigger times: 2
Loss after 5430040 batches: 0.0221
trigger times: 3
Loss after 5431003 batches: 0.0223
trigger times: 0
Loss after 5431966 batches: 0.0220
trigger times: 1
Loss after 5432929 batches: 0.0212
trigger times: 0
Loss after 5433892 batches: 0.0216
trigger times: 0
Loss after 5434855 batches: 0.0212
trigger times: 0
Loss after 5435818 batches: 0.0215
trigger times: 1
Loss after 5436781 batches: 0.0206
trigger times: 2
Loss after 5437744 batches: 0.0202
trigger times: 3
Loss after 5438707 batches: 0.0206
trigger times: 4
Loss after 5439670 batches: 0.0205
trigger times: 5
Loss after 5440633 batches: 0.0202
trigger times: 6
Loss after 5441596 batches: 0.0196
trigger times: 0
Loss after 5442559 batches: 0.0191
trigger times: 1
Loss after 5443522 batches: 0.0196
trigger times: 2
Loss after 5444485 batches: 0.0193
trigger times: 0
Loss after 5445448 batches: 0.0192
trigger times: 1
Loss after 5446411 batches: 0.0193
trigger times: 2
Loss after 5447374 batches: 0.0185
trigger times: 3
Loss after 5448337 batches: 0.0181
trigger times: 4
Loss after 5449300 batches: 0.0177
trigger times: 5
Loss after 5450263 batches: 0.0179
trigger times: 6
Loss after 5451226 batches: 0.0182
trigger times: 7
Loss after 5452189 batches: 0.0183
trigger times: 8
Loss after 5453152 batches: 0.0178
trigger times: 9
Loss after 5454115 batches: 0.0180
trigger times: 10
Loss after 5455078 batches: 0.0181
trigger times: 11
Loss after 5456041 batches: 0.0180
trigger times: 12
Loss after 5457004 batches: 0.0175
trigger times: 0
Loss after 5457967 batches: 0.0172
trigger times: 1
Loss after 5458930 batches: 0.0173
trigger times: 2
Loss after 5459893 batches: 0.0173
trigger times: 3
Loss after 5460856 batches: 0.0169
trigger times: 4
Loss after 5461819 batches: 0.0171
trigger times: 5
Loss after 5462782 batches: 0.0167
trigger times: 6
Loss after 5463745 batches: 0.0165
trigger times: 7
Loss after 5464708 batches: 0.0166
trigger times: 8
Loss after 5465671 batches: 0.0162
trigger times: 9
Loss after 5466634 batches: 0.0162
trigger times: 10
Loss after 5467597 batches: 0.0163
trigger times: 11
Loss after 5468560 batches: 0.0163
trigger times: 12
Loss after 5469523 batches: 0.0168
trigger times: 13
Loss after 5470486 batches: 0.0162
trigger times: 14
Loss after 5471449 batches: 0.0163
trigger times: 15
Loss after 5472412 batches: 0.0162
trigger times: 16
Loss after 5473375 batches: 0.0158
trigger times: 17
Loss after 5474338 batches: 0.0162
trigger times: 18
Loss after 5475301 batches: 0.0158
trigger times: 19
Loss after 5476264 batches: 0.0155
trigger times: 20
Loss after 5477227 batches: 0.0153
trigger times: 21
Loss after 5478190 batches: 0.0151
trigger times: 22
Loss after 5479153 batches: 0.0151
trigger times: 23
Loss after 5480116 batches: 0.0156
trigger times: 24
Loss after 5481079 batches: 0.0150
trigger times: 25
Early stopping!
Start to test process.
Loss after 5482042 batches: 0.0150
Time to train on one home:  90.01364707946777
trigger times: 0
Loss after 5483005 batches: 0.0610
trigger times: 0
Loss after 5483968 batches: 0.0529
trigger times: 0
Loss after 5484931 batches: 0.0508
trigger times: 1
Loss after 5485894 batches: 0.0487
trigger times: 0
Loss after 5486857 batches: 0.0471
trigger times: 1
Loss after 5487820 batches: 0.0460
trigger times: 0
Loss after 5488783 batches: 0.0441
trigger times: 1
Loss after 5489746 batches: 0.0430
trigger times: 0
Loss after 5490709 batches: 0.0429
trigger times: 0
Loss after 5491672 batches: 0.0426
trigger times: 1
Loss after 5492635 batches: 0.0419
trigger times: 0
Loss after 5493598 batches: 0.0396
trigger times: 0
Loss after 5494561 batches: 0.0396
trigger times: 1
Loss after 5495524 batches: 0.0388
trigger times: 2
Loss after 5496487 batches: 0.0383
trigger times: 3
Loss after 5497450 batches: 0.0377
trigger times: 4
Loss after 5498413 batches: 0.0375
trigger times: 5
Loss after 5499376 batches: 0.0362
trigger times: 6
Loss after 5500339 batches: 0.0364
trigger times: 7
Loss after 5501302 batches: 0.0364
trigger times: 8
Loss after 5502265 batches: 0.0354
trigger times: 9
Loss after 5503228 batches: 0.0363
trigger times: 10
Loss after 5504191 batches: 0.0347
trigger times: 11
Loss after 5505154 batches: 0.0336
trigger times: 12
Loss after 5506117 batches: 0.0343
trigger times: 0
Loss after 5507080 batches: 0.0326
trigger times: 1
Loss after 5508043 batches: 0.0315
trigger times: 2
Loss after 5509006 batches: 0.0311
trigger times: 3
Loss after 5509969 batches: 0.0322
trigger times: 4
Loss after 5510932 batches: 0.0319
trigger times: 5
Loss after 5511895 batches: 0.0319
trigger times: 6
Loss after 5512858 batches: 0.0314
trigger times: 7
Loss after 5513821 batches: 0.0307
trigger times: 8
Loss after 5514784 batches: 0.0315
trigger times: 0
Loss after 5515747 batches: 0.0307
trigger times: 0
Loss after 5516710 batches: 0.0293
trigger times: 1
Loss after 5517673 batches: 0.0289
trigger times: 2
Loss after 5518636 batches: 0.0289
trigger times: 3
Loss after 5519599 batches: 0.0283
trigger times: 4
Loss after 5520562 batches: 0.0270
trigger times: 5
Loss after 5521525 batches: 0.0270
trigger times: 6
Loss after 5522488 batches: 0.0268
trigger times: 7
Loss after 5523451 batches: 0.0265
trigger times: 8
Loss after 5524414 batches: 0.0272
trigger times: 9
Loss after 5525377 batches: 0.0269
trigger times: 10
Loss after 5526340 batches: 0.0272
trigger times: 11
Loss after 5527303 batches: 0.0269
trigger times: 12
Loss after 5528266 batches: 0.0269
trigger times: 13
Loss after 5529229 batches: 0.0259
trigger times: 14
Loss after 5530192 batches: 0.0264
trigger times: 15
Loss after 5531155 batches: 0.0260
trigger times: 16
Loss after 5532118 batches: 0.0256
trigger times: 17
Loss after 5533081 batches: 0.0250
trigger times: 0
Loss after 5534044 batches: 0.0250
trigger times: 1
Loss after 5535007 batches: 0.0247
trigger times: 2
Loss after 5535970 batches: 0.0253
trigger times: 3
Loss after 5536933 batches: 0.0251
trigger times: 4
Loss after 5537896 batches: 0.0247
trigger times: 5
Loss after 5538859 batches: 0.0251
trigger times: 6
Loss after 5539822 batches: 0.0242
trigger times: 7
Loss after 5540785 batches: 0.0242
trigger times: 8
Loss after 5541748 batches: 0.0245
trigger times: 9
Loss after 5542711 batches: 0.0249
trigger times: 10
Loss after 5543674 batches: 0.0252
trigger times: 0
Loss after 5544637 batches: 0.0249
trigger times: 1
Loss after 5545600 batches: 0.0237
trigger times: 2
Loss after 5546563 batches: 0.0237
trigger times: 3
Loss after 5547526 batches: 0.0240
trigger times: 4
Loss after 5548489 batches: 0.0233
trigger times: 5
Loss after 5549452 batches: 0.0236
trigger times: 6
Loss after 5550415 batches: 0.0234
trigger times: 7
Loss after 5551378 batches: 0.0233
trigger times: 8
Loss after 5552341 batches: 0.0240
trigger times: 9
Loss after 5553304 batches: 0.0232
trigger times: 0
Loss after 5554267 batches: 0.0233
trigger times: 1
Loss after 5555230 batches: 0.0232
trigger times: 2
Loss after 5556193 batches: 0.0224
trigger times: 3
Loss after 5557156 batches: 0.0222
trigger times: 4
Loss after 5558119 batches: 0.0225
trigger times: 5
Loss after 5559082 batches: 0.0228
trigger times: 6
Loss after 5560045 batches: 0.0231
trigger times: 7
Loss after 5561008 batches: 0.0236
trigger times: 8
Loss after 5561971 batches: 0.0233
trigger times: 9
Loss after 5562934 batches: 0.0227
trigger times: 10
Loss after 5563897 batches: 0.0228
trigger times: 11
Loss after 5564860 batches: 0.0236
trigger times: 12
Loss after 5565823 batches: 0.0236
trigger times: 13
Loss after 5566786 batches: 0.0233
trigger times: 14
Loss after 5567749 batches: 0.0226
trigger times: 15
Loss after 5568712 batches: 0.0225
trigger times: 16
Loss after 5569675 batches: 0.0215
trigger times: 17
Loss after 5570638 batches: 0.0215
trigger times: 18
Loss after 5571601 batches: 0.0208
trigger times: 19
Loss after 5572564 batches: 0.0217
trigger times: 20
Loss after 5573527 batches: 0.0213
trigger times: 21
Loss after 5574490 batches: 0.0215
trigger times: 22
Loss after 5575453 batches: 0.0215
trigger times: 23
Loss after 5576416 batches: 0.0210
trigger times: 24
Loss after 5577379 batches: 0.0207
trigger times: 25
Early stopping!
Start to test process.
Loss after 5578342 batches: 0.0209
Time to train on one home:  110.90346384048462
trigger times: 0
Loss after 5579305 batches: 0.0942
trigger times: 1
Loss after 5580268 batches: 0.0827
trigger times: 2
Loss after 5581231 batches: 0.0752
trigger times: 3
Loss after 5582194 batches: 0.0691
trigger times: 4
Loss after 5583157 batches: 0.0648
trigger times: 5
Loss after 5584120 batches: 0.0603
trigger times: 6
Loss after 5585083 batches: 0.0587
trigger times: 7
Loss after 5586046 batches: 0.0564
trigger times: 8
Loss after 5587009 batches: 0.0543
trigger times: 9
Loss after 5587972 batches: 0.0549
trigger times: 10
Loss after 5588935 batches: 0.0531
trigger times: 11
Loss after 5589898 batches: 0.0528
trigger times: 12
Loss after 5590861 batches: 0.0510
trigger times: 13
Loss after 5591824 batches: 0.0524
trigger times: 14
Loss after 5592787 batches: 0.0518
trigger times: 15
Loss after 5593750 batches: 0.0509
trigger times: 16
Loss after 5594713 batches: 0.0507
trigger times: 17
Loss after 5595676 batches: 0.0496
trigger times: 18
Loss after 5596639 batches: 0.0487
trigger times: 19
Loss after 5597602 batches: 0.0490
trigger times: 20
Loss after 5598565 batches: 0.0482
trigger times: 21
Loss after 5599528 batches: 0.0478
trigger times: 22
Loss after 5600491 batches: 0.0474
trigger times: 23
Loss after 5601454 batches: 0.0461
trigger times: 24
Loss after 5602417 batches: 0.0475
trigger times: 25
Early stopping!
Start to test process.
Loss after 5603380 batches: 0.0474
Time to train on one home:  51.28955841064453
trigger times: 0
Loss after 5604343 batches: 0.0891
trigger times: 0
Loss after 5605306 batches: 0.0789
trigger times: 1
Loss after 5606269 batches: 0.0717
trigger times: 2
Loss after 5607232 batches: 0.0692
trigger times: 3
Loss after 5608195 batches: 0.0659
trigger times: 0
Loss after 5609158 batches: 0.0623
trigger times: 1
Loss after 5610121 batches: 0.0601
trigger times: 0
Loss after 5611084 batches: 0.0594
trigger times: 0
Loss after 5612047 batches: 0.0576
trigger times: 0
Loss after 5613010 batches: 0.0556
trigger times: 1
Loss after 5613973 batches: 0.0555
trigger times: 0
Loss after 5614936 batches: 0.0553
trigger times: 1
Loss after 5615899 batches: 0.0539
trigger times: 2
Loss after 5616862 batches: 0.0522
trigger times: 3
Loss after 5617825 batches: 0.0519
trigger times: 0
Loss after 5618788 batches: 0.0520
trigger times: 1
Loss after 5619751 batches: 0.0514
trigger times: 2
Loss after 5620714 batches: 0.0509
trigger times: 3
Loss after 5621677 batches: 0.0502
trigger times: 4
Loss after 5622640 batches: 0.0493
trigger times: 0
Loss after 5623603 batches: 0.0482
trigger times: 0
Loss after 5624566 batches: 0.0495
trigger times: 1
Loss after 5625529 batches: 0.0491
trigger times: 2
Loss after 5626492 batches: 0.0479
trigger times: 3
Loss after 5627455 batches: 0.0476
trigger times: 4
Loss after 5628418 batches: 0.0474
trigger times: 5
Loss after 5629381 batches: 0.0479
trigger times: 6
Loss after 5630344 batches: 0.0460
trigger times: 7
Loss after 5631307 batches: 0.0465
trigger times: 8
Loss after 5632270 batches: 0.0467
trigger times: 9
Loss after 5633233 batches: 0.0459
trigger times: 10
Loss after 5634196 batches: 0.0458
trigger times: 0
Loss after 5635159 batches: 0.0460
trigger times: 1
Loss after 5636122 batches: 0.0459
trigger times: 2
Loss after 5637085 batches: 0.0447
trigger times: 3
Loss after 5638048 batches: 0.0453
trigger times: 0
Loss after 5639011 batches: 0.0451
trigger times: 1
Loss after 5639974 batches: 0.0438
trigger times: 2
Loss after 5640937 batches: 0.0446
trigger times: 3
Loss after 5641900 batches: 0.0437
trigger times: 4
Loss after 5642863 batches: 0.0436
trigger times: 5
Loss after 5643826 batches: 0.0433
trigger times: 6
Loss after 5644789 batches: 0.0426
trigger times: 7
Loss after 5645752 batches: 0.0436
trigger times: 8
Loss after 5646715 batches: 0.0422
trigger times: 9
Loss after 5647678 batches: 0.0427
trigger times: 10
Loss after 5648641 batches: 0.0427
trigger times: 11
Loss after 5649604 batches: 0.0431
trigger times: 0
Loss after 5650567 batches: 0.0431
trigger times: 1
Loss after 5651530 batches: 0.0426
trigger times: 2
Loss after 5652493 batches: 0.0418
trigger times: 3
Loss after 5653456 batches: 0.0415
trigger times: 4
Loss after 5654419 batches: 0.0421
trigger times: 5
Loss after 5655382 batches: 0.0405
trigger times: 6
Loss after 5656345 batches: 0.0415
trigger times: 7
Loss after 5657308 batches: 0.0404
trigger times: 8
Loss after 5658271 batches: 0.0391
trigger times: 9
Loss after 5659234 batches: 0.0396
trigger times: 10
Loss after 5660197 batches: 0.0403
trigger times: 11
Loss after 5661160 batches: 0.0396
trigger times: 12
Loss after 5662123 batches: 0.0392
trigger times: 13
Loss after 5663086 batches: 0.0399
trigger times: 14
Loss after 5664049 batches: 0.0392
trigger times: 15
Loss after 5665012 batches: 0.0391
trigger times: 16
Loss after 5665975 batches: 0.0392
trigger times: 17
Loss after 5666938 batches: 0.0390
trigger times: 18
Loss after 5667901 batches: 0.0381
trigger times: 19
Loss after 5668864 batches: 0.0396
trigger times: 20
Loss after 5669827 batches: 0.0385
trigger times: 21
Loss after 5670790 batches: 0.0382
trigger times: 22
Loss after 5671753 batches: 0.0382
trigger times: 23
Loss after 5672716 batches: 0.0390
trigger times: 24
Loss after 5673679 batches: 0.0385
trigger times: 25
Early stopping!
Start to test process.
Loss after 5674642 batches: 0.0376
Time to train on one home:  86.50695729255676
trigger times: 0
Loss after 5675605 batches: 0.0670
trigger times: 0
Loss after 5676568 batches: 0.0611
trigger times: 0
Loss after 5677531 batches: 0.0574
trigger times: 1
Loss after 5678494 batches: 0.0540
trigger times: 0
Loss after 5679457 batches: 0.0508
trigger times: 0
Loss after 5680420 batches: 0.0481
trigger times: 0
Loss after 5681383 batches: 0.0478
trigger times: 0
Loss after 5682346 batches: 0.0462
trigger times: 1
Loss after 5683309 batches: 0.0454
trigger times: 2
Loss after 5684272 batches: 0.0439
trigger times: 0
Loss after 5685235 batches: 0.0445
trigger times: 1
Loss after 5686198 batches: 0.0438
trigger times: 2
Loss after 5687161 batches: 0.0431
trigger times: 3
Loss after 5688124 batches: 0.0424
trigger times: 4
Loss after 5689087 batches: 0.0420
trigger times: 5
Loss after 5690050 batches: 0.0419
trigger times: 0
Loss after 5691013 batches: 0.0415
trigger times: 1
Loss after 5691976 batches: 0.0411
trigger times: 2
Loss after 5692939 batches: 0.0409
trigger times: 3
Loss after 5693902 batches: 0.0406
trigger times: 4
Loss after 5694865 batches: 0.0397
trigger times: 5
Loss after 5695828 batches: 0.0385
trigger times: 6
Loss after 5696791 batches: 0.0398
trigger times: 7
Loss after 5697754 batches: 0.0394
trigger times: 8
Loss after 5698717 batches: 0.0386
trigger times: 9
Loss after 5699680 batches: 0.0390
trigger times: 10
Loss after 5700643 batches: 0.0385
trigger times: 11
Loss after 5701606 batches: 0.0382
trigger times: 12
Loss after 5702569 batches: 0.0375
trigger times: 13
Loss after 5703532 batches: 0.0372
trigger times: 14
Loss after 5704495 batches: 0.0364
trigger times: 15
Loss after 5705458 batches: 0.0400
trigger times: 16
Loss after 5706421 batches: 0.0427
trigger times: 17
Loss after 5707384 batches: 0.0440
trigger times: 18
Loss after 5708347 batches: 0.0417
trigger times: 19
Loss after 5709310 batches: 0.0398
trigger times: 20
Loss after 5710273 batches: 0.0385
trigger times: 21
Loss after 5711236 batches: 0.0398
trigger times: 22
Loss after 5712199 batches: 0.0401
trigger times: 23
Loss after 5713162 batches: 0.0388
trigger times: 24
Loss after 5714125 batches: 0.0392
trigger times: 25
Early stopping!
Start to test process.
Loss after 5715088 batches: 0.0378
Time to train on one home:  62.60964012145996
trigger times: 0
Loss after 5716051 batches: 0.1170
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5717014 batches: 0.0831
trigger times: 0
Loss after 5717977 batches: 0.0828
trigger times: 1
Loss after 5718940 batches: 0.0825
trigger times: 2
Loss after 5719903 batches: 0.0803
trigger times: 3
Loss after 5720866 batches: 0.0791
trigger times: 4
Loss after 5721829 batches: 0.0781
trigger times: 0
Loss after 5722792 batches: 0.0768
trigger times: 1
Loss after 5723755 batches: 0.0749
trigger times: 0
Loss after 5724718 batches: 0.0720
trigger times: 1
Loss after 5725681 batches: 0.0694
trigger times: 0
Loss after 5726644 batches: 0.0668
trigger times: 0
Loss after 5727607 batches: 0.0638
trigger times: 0
Loss after 5728570 batches: 0.0614
trigger times: 0
Loss after 5729533 batches: 0.0588
trigger times: 0
Loss after 5730496 batches: 0.0565
trigger times: 1
Loss after 5731459 batches: 0.0554
trigger times: 0
Loss after 5732422 batches: 0.0528
trigger times: 0
Loss after 5733385 batches: 0.0516
trigger times: 0
Loss after 5734348 batches: 0.0497
trigger times: 0
Loss after 5735311 batches: 0.0487
trigger times: 0
Loss after 5736274 batches: 0.0474
trigger times: 0
Loss after 5737237 batches: 0.0455
trigger times: 0
Loss after 5738200 batches: 0.0466
trigger times: 1
Loss after 5739163 batches: 0.0452
trigger times: 0
Loss after 5740126 batches: 0.0442
trigger times: 1
Loss after 5741089 batches: 0.0436
trigger times: 0
Loss after 5742052 batches: 0.0436
trigger times: 1
Loss after 5743015 batches: 0.0440
trigger times: 0
Loss after 5743978 batches: 0.0421
trigger times: 0
Loss after 5744941 batches: 0.0409
trigger times: 1
Loss after 5745904 batches: 0.0400
trigger times: 0
Loss after 5746867 batches: 0.0395
trigger times: 1
Loss after 5747830 batches: 0.0400
trigger times: 0
Loss after 5748793 batches: 0.0403
trigger times: 1
Loss after 5749756 batches: 0.0402
trigger times: 2
Loss after 5750719 batches: 0.0392
trigger times: 3
Loss after 5751682 batches: 0.0384
trigger times: 4
Loss after 5752645 batches: 0.0388
trigger times: 5
Loss after 5753608 batches: 0.0381
trigger times: 6
Loss after 5754571 batches: 0.0375
trigger times: 0
Loss after 5755534 batches: 0.0366
trigger times: 1
Loss after 5756497 batches: 0.0375
trigger times: 2
Loss after 5757460 batches: 0.0374
trigger times: 3
Loss after 5758423 batches: 0.0356
trigger times: 0
Loss after 5759386 batches: 0.0352
trigger times: 1
Loss after 5760349 batches: 0.0347
trigger times: 0
Loss after 5761312 batches: 0.0346
trigger times: 1
Loss after 5762275 batches: 0.0350
trigger times: 2
Loss after 5763238 batches: 0.0350
trigger times: 3
Loss after 5764201 batches: 0.0342
trigger times: 4
Loss after 5765164 batches: 0.0338
trigger times: 5
Loss after 5766127 batches: 0.0330
trigger times: 6
Loss after 5767090 batches: 0.0330
trigger times: 7
Loss after 5768053 batches: 0.0334
trigger times: 8
Loss after 5769016 batches: 0.0332
trigger times: 9
Loss after 5769979 batches: 0.0334
trigger times: 10
Loss after 5770942 batches: 0.0335
trigger times: 11
Loss after 5771905 batches: 0.0321
trigger times: 12
Loss after 5772868 batches: 0.0315
trigger times: 13
Loss after 5773831 batches: 0.0317
trigger times: 14
Loss after 5774794 batches: 0.0314
trigger times: 15
Loss after 5775757 batches: 0.0316
trigger times: 16
Loss after 5776720 batches: 0.0304
trigger times: 17
Loss after 5777683 batches: 0.0315
trigger times: 18
Loss after 5778646 batches: 0.0307
trigger times: 19
Loss after 5779609 batches: 0.0309
trigger times: 20
Loss after 5780572 batches: 0.0307
trigger times: 21
Loss after 5781535 batches: 0.0301
trigger times: 22
Loss after 5782498 batches: 0.0293
trigger times: 23
Loss after 5783461 batches: 0.0291
trigger times: 24
Loss after 5784424 batches: 0.0305
trigger times: 25
Early stopping!
Start to test process.
Loss after 5785387 batches: 0.0303
Time to train on one home:  85.9279682636261
trigger times: 0
Loss after 5786350 batches: 0.0881
trigger times: 1
Loss after 5787313 batches: 0.0792
trigger times: 2
Loss after 5788276 batches: 0.0796
trigger times: 3
Loss after 5789239 batches: 0.0784
trigger times: 4
Loss after 5790202 batches: 0.0767
trigger times: 5
Loss after 5791165 batches: 0.0764
trigger times: 6
Loss after 5792128 batches: 0.0750
trigger times: 7
Loss after 5793091 batches: 0.0744
trigger times: 8
Loss after 5794054 batches: 0.0740
trigger times: 9
Loss after 5795017 batches: 0.0726
trigger times: 10
Loss after 5795980 batches: 0.0718
trigger times: 11
Loss after 5796943 batches: 0.0715
trigger times: 12
Loss after 5797906 batches: 0.0706
trigger times: 13
Loss after 5798869 batches: 0.0693
trigger times: 14
Loss after 5799832 batches: 0.0698
trigger times: 15
Loss after 5800795 batches: 0.0689
trigger times: 16
Loss after 5801758 batches: 0.0696
trigger times: 17
Loss after 5802721 batches: 0.0681
trigger times: 18
Loss after 5803684 batches: 0.0691
trigger times: 19
Loss after 5804647 batches: 0.0710
trigger times: 20
Loss after 5805610 batches: 0.0703
trigger times: 21
Loss after 5806573 batches: 0.0694
trigger times: 22
Loss after 5807536 batches: 0.0667
trigger times: 23
Loss after 5808499 batches: 0.0664
trigger times: 24
Loss after 5809462 batches: 0.0667
trigger times: 25
Early stopping!
Start to test process.
Loss after 5810425 batches: 0.0654
Time to train on one home:  51.235716104507446
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5811388 batches: 0.1163
trigger times: 0
Loss after 5812351 batches: 0.0825
trigger times: 0
Loss after 5813314 batches: 0.0823
trigger times: 1
Loss after 5814277 batches: 0.0819
trigger times: 2
Loss after 5815240 batches: 0.0806
trigger times: 0
Loss after 5816203 batches: 0.0799
trigger times: 0
Loss after 5817166 batches: 0.0784
trigger times: 1
Loss after 5818129 batches: 0.0774
trigger times: 2
Loss after 5819092 batches: 0.0758
trigger times: 0
Loss after 5820055 batches: 0.0735
trigger times: 0
Loss after 5821018 batches: 0.0693
trigger times: 1
Loss after 5821981 batches: 0.0655
trigger times: 0
Loss after 5822944 batches: 0.0637
trigger times: 1
Loss after 5823907 batches: 0.0611
trigger times: 0
Loss after 5824870 batches: 0.0595
trigger times: 0
Loss after 5825833 batches: 0.0573
trigger times: 0
Loss after 5826796 batches: 0.0547
trigger times: 0
Loss after 5827759 batches: 0.0521
trigger times: 0
Loss after 5828722 batches: 0.0512
trigger times: 0
Loss after 5829685 batches: 0.0494
trigger times: 0
Loss after 5830648 batches: 0.0483
trigger times: 0
Loss after 5831611 batches: 0.0474
trigger times: 0
Loss after 5832574 batches: 0.0472
trigger times: 1
Loss after 5833537 batches: 0.0460
trigger times: 0
Loss after 5834500 batches: 0.0439
trigger times: 1
Loss after 5835463 batches: 0.0438
trigger times: 2
Loss after 5836426 batches: 0.0442
trigger times: 0
Loss after 5837389 batches: 0.0421
trigger times: 0
Loss after 5838352 batches: 0.0407
trigger times: 0
Loss after 5839315 batches: 0.0407
trigger times: 1
Loss after 5840278 batches: 0.0397
trigger times: 2
Loss after 5841241 batches: 0.0401
trigger times: 3
Loss after 5842204 batches: 0.0386
trigger times: 4
Loss after 5843167 batches: 0.0391
trigger times: 5
Loss after 5844130 batches: 0.0384
trigger times: 6
Loss after 5845093 batches: 0.0390
trigger times: 7
Loss after 5846056 batches: 0.0377
trigger times: 8
Loss after 5847019 batches: 0.0387
trigger times: 0
Loss after 5847982 batches: 0.0381
trigger times: 1
Loss after 5848945 batches: 0.0365
trigger times: 2
Loss after 5849908 batches: 0.0370
trigger times: 3
Loss after 5850871 batches: 0.0369
trigger times: 4
Loss after 5851834 batches: 0.0361
trigger times: 0
Loss after 5852797 batches: 0.0355
trigger times: 1
Loss after 5853760 batches: 0.0355
trigger times: 0
Loss after 5854723 batches: 0.0350
trigger times: 1
Loss after 5855686 batches: 0.0340
trigger times: 0
Loss after 5856649 batches: 0.0349
trigger times: 1
Loss after 5857612 batches: 0.0341
trigger times: 2
Loss after 5858575 batches: 0.0335
trigger times: 3
Loss after 5859538 batches: 0.0338
trigger times: 4
Loss after 5860501 batches: 0.0330
trigger times: 5
Loss after 5861464 batches: 0.0323
trigger times: 6
Loss after 5862427 batches: 0.0326
trigger times: 7
Loss after 5863390 batches: 0.0325
trigger times: 8
Loss after 5864353 batches: 0.0326
trigger times: 0
Loss after 5865316 batches: 0.0324
trigger times: 1
Loss after 5866279 batches: 0.0324
trigger times: 2
Loss after 5867242 batches: 0.0320
trigger times: 3
Loss after 5868205 batches: 0.0311
trigger times: 4
Loss after 5869168 batches: 0.0306
trigger times: 5
Loss after 5870131 batches: 0.0314
trigger times: 6
Loss after 5871094 batches: 0.0317
trigger times: 7
Loss after 5872057 batches: 0.0309
trigger times: 8
Loss after 5873020 batches: 0.0304
trigger times: 9
Loss after 5873983 batches: 0.0316
trigger times: 0
Loss after 5874946 batches: 0.0313
trigger times: 1
Loss after 5875909 batches: 0.0304
trigger times: 0
Loss after 5876872 batches: 0.0289
trigger times: 1
Loss after 5877835 batches: 0.0303
trigger times: 2
Loss after 5878798 batches: 0.0294
trigger times: 3
Loss after 5879761 batches: 0.0287
trigger times: 4
Loss after 5880724 batches: 0.0310
trigger times: 0
Loss after 5881687 batches: 0.0297
trigger times: 1
Loss after 5882650 batches: 0.0293
trigger times: 2
Loss after 5883613 batches: 0.0292
trigger times: 3
Loss after 5884576 batches: 0.0293
trigger times: 4
Loss after 5885539 batches: 0.0287
trigger times: 5
Loss after 5886502 batches: 0.0290
trigger times: 6
Loss after 5887465 batches: 0.0283
trigger times: 7
Loss after 5888428 batches: 0.0287
trigger times: 0
Loss after 5889391 batches: 0.0283
trigger times: 1
Loss after 5890354 batches: 0.0275
trigger times: 2
Loss after 5891317 batches: 0.0280
trigger times: 3
Loss after 5892280 batches: 0.0273
trigger times: 4
Loss after 5893243 batches: 0.0264
trigger times: 5
Loss after 5894206 batches: 0.0269
trigger times: 6
Loss after 5895169 batches: 0.0279
trigger times: 0
Loss after 5896132 batches: 0.0275
trigger times: 1
Loss after 5897095 batches: 0.0273
trigger times: 2
Loss after 5898058 batches: 0.0267
trigger times: 3
Loss after 5899021 batches: 0.0261
trigger times: 4
Loss after 5899984 batches: 0.0256
trigger times: 5
Loss after 5900947 batches: 0.0253
trigger times: 6
Loss after 5901910 batches: 0.0250
trigger times: 7
Loss after 5902873 batches: 0.0256
trigger times: 8
Loss after 5903836 batches: 0.0263
trigger times: 9
Loss after 5904799 batches: 0.0256
trigger times: 10
Loss after 5905762 batches: 0.0250
trigger times: 11
Loss after 5906725 batches: 0.0248
trigger times: 12
Loss after 5907688 batches: 0.0277
trigger times: 13
Loss after 5908651 batches: 0.0276
trigger times: 14
Loss after 5909614 batches: 0.0259
trigger times: 15
Loss after 5910577 batches: 0.0264
trigger times: 16
Loss after 5911540 batches: 0.0266
trigger times: 17
Loss after 5912503 batches: 0.0273
trigger times: 18
Loss after 5913466 batches: 0.0257
trigger times: 19
Loss after 5914429 batches: 0.0247
trigger times: 20
Loss after 5915392 batches: 0.0247
trigger times: 21
Loss after 5916355 batches: 0.0253
trigger times: 22
Loss after 5917318 batches: 0.0249
trigger times: 23
Loss after 5918281 batches: 0.0248
trigger times: 24
Loss after 5919244 batches: 0.0247
trigger times: 25
Early stopping!
Start to test process.
Loss after 5920207 batches: 0.0243
Time to train on one home:  117.37373876571655
trigger times: 0
Loss after 5921102 batches: 0.0347
trigger times: 1
Loss after 5921997 batches: 0.0205
trigger times: 0
Loss after 5922892 batches: 0.0139
trigger times: 0
Loss after 5923787 batches: 0.0109
trigger times: 1
Loss after 5924682 batches: 0.0096
trigger times: 2
Loss after 5925577 batches: 0.0098
trigger times: 3
Loss after 5926472 batches: 0.0086
trigger times: 4
Loss after 5927367 batches: 0.0095
trigger times: 0
Loss after 5928262 batches: 0.0085
trigger times: 1
Loss after 5929157 batches: 0.0070
trigger times: 2
Loss after 5930052 batches: 0.0064
trigger times: 3
Loss after 5930947 batches: 0.0056
trigger times: 4
Loss after 5931842 batches: 0.0058
trigger times: 5
Loss after 5932737 batches: 0.0054
trigger times: 6
Loss after 5933632 batches: 0.0050
trigger times: 7
Loss after 5934527 batches: 0.0054
trigger times: 8
Loss after 5935422 batches: 0.0048
trigger times: 9
Loss after 5936317 batches: 0.0052
trigger times: 10
Loss after 5937212 batches: 0.0056
trigger times: 11
Loss after 5938107 batches: 0.0047
trigger times: 12
Loss after 5939002 batches: 0.0047
trigger times: 13
Loss after 5939897 batches: 0.0043
trigger times: 14
Loss after 5940792 batches: 0.0044
trigger times: 15
Loss after 5941687 batches: 0.0044
trigger times: 16
Loss after 5942582 batches: 0.0050
trigger times: 17
Loss after 5943477 batches: 0.0042
trigger times: 18
Loss after 5944372 batches: 0.0042
trigger times: 19
Loss after 5945267 batches: 0.0052
trigger times: 20
Loss after 5946162 batches: 0.0048
trigger times: 21
Loss after 5947057 batches: 0.0043
trigger times: 22
Loss after 5947952 batches: 0.0042
trigger times: 23
Loss after 5948847 batches: 0.0040
trigger times: 24
Loss after 5949742 batches: 0.0035
trigger times: 25
Early stopping!
Start to test process.
Loss after 5950637 batches: 0.0039
Time to train on one home:  55.64597153663635
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 5951574 batches: 0.0913
trigger times: 0
Loss after 5952511 batches: 0.0816
trigger times: 0
Loss after 5953448 batches: 0.0822
trigger times: 0
Loss after 5954385 batches: 0.0768
trigger times: 0
Loss after 5955322 batches: 0.0746
trigger times: 0
Loss after 5956259 batches: 0.0714
trigger times: 1
Loss after 5957196 batches: 0.0700
trigger times: 2
Loss after 5958133 batches: 0.0686
trigger times: 3
Loss after 5959070 batches: 0.0678
trigger times: 4
Loss after 5960007 batches: 0.0664
trigger times: 5
Loss after 5960944 batches: 0.0656
trigger times: 6
Loss after 5961881 batches: 0.0657
trigger times: 7
Loss after 5962818 batches: 0.0648
trigger times: 8
Loss after 5963755 batches: 0.0642
trigger times: 0
Loss after 5964692 batches: 0.0631
trigger times: 1
Loss after 5965629 batches: 0.0627
trigger times: 2
Loss after 5966566 batches: 0.0641
trigger times: 3
Loss after 5967503 batches: 0.0610
trigger times: 4
Loss after 5968440 batches: 0.0615
trigger times: 5
Loss after 5969377 batches: 0.0609
trigger times: 6
Loss after 5970314 batches: 0.0606
trigger times: 0
Loss after 5971251 batches: 0.0591
trigger times: 1
Loss after 5972188 batches: 0.0606
trigger times: 2
Loss after 5973125 batches: 0.0594
trigger times: 3
Loss after 5974062 batches: 0.0592
trigger times: 4
Loss after 5974999 batches: 0.0601
trigger times: 5
Loss after 5975936 batches: 0.0580
trigger times: 6
Loss after 5976873 batches: 0.0574
trigger times: 7
Loss after 5977810 batches: 0.0578
trigger times: 8
Loss after 5978747 batches: 0.0638
trigger times: 9
Loss after 5979684 batches: 0.0609
trigger times: 10
Loss after 5980621 batches: 0.0599
trigger times: 11
Loss after 5981558 batches: 0.0578
trigger times: 12
Loss after 5982495 batches: 0.0570
trigger times: 13
Loss after 5983432 batches: 0.0572
trigger times: 14
Loss after 5984369 batches: 0.0568
trigger times: 15
Loss after 5985306 batches: 0.0577
trigger times: 16
Loss after 5986243 batches: 0.0568
trigger times: 17
Loss after 5987180 batches: 0.0570
trigger times: 18
Loss after 5988117 batches: 0.0556
trigger times: 19
Loss after 5989054 batches: 0.0550
trigger times: 20
Loss after 5989991 batches: 0.0562
trigger times: 21
Loss after 5990928 batches: 0.0548
trigger times: 22
Loss after 5991865 batches: 0.0548
trigger times: 23
Loss after 5992802 batches: 0.0551
trigger times: 24
Loss after 5993739 batches: 0.0549
trigger times: 25
Early stopping!
Start to test process.
Loss after 5994676 batches: 0.0551
Time to train on one home:  66.32833290100098
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214]]
Round_3_results:  [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214]
trigger times: 0
Loss after 5995638 batches: 0.0692
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 6253 < 6254; dropping {'Training_Loss': 0.06917415239981242, 'Validation_Loss': 0.10637839138507843, 'Training_R2': -0.17162579211778195, 'Validation_R2': 0.07182071082657593, 'Training_F1': 0.36156156979211584, 'Validation_F1': 0.451722678496025, 'Training_NEP': 0.8805288076665545, 'Validation_NEP': 0.9812148904750261, 'Training_NDE': 0.7135485208398544, 'Validation_NDE': 0.8385879770947623, 'Training_MAE': 33.46037799256259, 'Validation_MAE': 34.89128887773073, 'Training_MSE': 2635.4104, 'Validation_MSE': 10985.512}.
trigger times: 1
Loss after 5996600 batches: 0.0673
trigger times: 2
Loss after 5997562 batches: 0.0649
trigger times: 3
Loss after 5998524 batches: 0.0633
trigger times: 4
Loss after 5999486 batches: 0.0615
trigger times: 5
Loss after 6000448 batches: 0.0607
trigger times: 6
Loss after 6001410 batches: 0.0606
trigger times: 7
Loss after 6002372 batches: 0.0603
trigger times: 8
Loss after 6003334 batches: 0.0590
trigger times: 9
Loss after 6004296 batches: 0.0602
trigger times: 10
Loss after 6005258 batches: 0.0593
trigger times: 11
Loss after 6006220 batches: 0.0584
trigger times: 12
Loss after 6007182 batches: 0.0598
trigger times: 13
Loss after 6008144 batches: 0.0587
trigger times: 14
Loss after 6009106 batches: 0.0580
trigger times: 15
Loss after 6010068 batches: 0.0572
trigger times: 16
Loss after 6011030 batches: 0.0570
trigger times: 17
Loss after 6011992 batches: 0.0571
trigger times: 18
Loss after 6012954 batches: 0.0562
trigger times: 19
Loss after 6013916 batches: 0.0576
trigger times: 20
Loss after 6014878 batches: 0.0567
trigger times: 21
Loss after 6015840 batches: 0.0561
trigger times: 22
Loss after 6016802 batches: 0.0560
trigger times: 23
Loss after 6017764 batches: 0.0561
trigger times: 24
Loss after 6018726 batches: 0.0551
trigger times: 25
Early stopping!
Start to test process.
Loss after 6019688 batches: 0.0556
Time to train on one home:  52.109315156936646
trigger times: 0
Loss after 6020617 batches: 0.1140
trigger times: 1
Loss after 6021546 batches: 0.0666
trigger times: 2
Loss after 6022475 batches: 0.0619
trigger times: 3
Loss after 6023404 batches: 0.0507
trigger times: 4
Loss after 6024333 batches: 0.0454
trigger times: 5
Loss after 6025262 batches: 0.0460
trigger times: 6
Loss after 6026191 batches: 0.0433
trigger times: 7
Loss after 6027120 batches: 0.0410
trigger times: 8
Loss after 6028049 batches: 0.0383
trigger times: 9
Loss after 6028978 batches: 0.0371
trigger times: 10
Loss after 6029907 batches: 0.0369
trigger times: 11
Loss after 6030836 batches: 0.0360
trigger times: 12
Loss after 6031765 batches: 0.0359
trigger times: 13
Loss after 6032694 batches: 0.0393
trigger times: 14
Loss after 6033623 batches: 0.0379
trigger times: 15
Loss after 6034552 batches: 0.0352
trigger times: 16
Loss after 6035481 batches: 0.0367
trigger times: 17
Loss after 6036410 batches: 0.0381
trigger times: 18
Loss after 6037339 batches: 0.0397
trigger times: 19
Loss after 6038268 batches: 0.0381
trigger times: 20
Loss after 6039197 batches: 0.0387
trigger times: 21
Loss after 6040126 batches: 0.0348
trigger times: 22
Loss after 6041055 batches: 0.0364
trigger times: 23
Loss after 6041984 batches: 0.0339
trigger times: 24
Loss after 6042913 batches: 0.0354
trigger times: 25
Early stopping!
Start to test process.
Loss after 6043842 batches: 0.0365
Time to train on one home:  52.501763582229614
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 6044805 batches: 0.0272
trigger times: 1
Loss after 6045768 batches: 0.0155
trigger times: 2
Loss after 6046731 batches: 0.0146
trigger times: 3
Loss after 6047694 batches: 0.0145
trigger times: 4
Loss after 6048657 batches: 0.0139
trigger times: 5
Loss after 6049620 batches: 0.0133
trigger times: 6
Loss after 6050583 batches: 0.0126
trigger times: 7
Loss after 6051546 batches: 0.0123
trigger times: 8
Loss after 6052509 batches: 0.0119
trigger times: 9
Loss after 6053472 batches: 0.0114
trigger times: 10
Loss after 6054435 batches: 0.0108
trigger times: 11
Loss after 6055398 batches: 0.0104
trigger times: 12
Loss after 6056361 batches: 0.0102
trigger times: 13
Loss after 6057324 batches: 0.0099
trigger times: 14
Loss after 6058287 batches: 0.0098
trigger times: 15
Loss after 6059250 batches: 0.0098
trigger times: 16
Loss after 6060213 batches: 0.0097
trigger times: 17
Loss after 6061176 batches: 0.0096
trigger times: 18
Loss after 6062139 batches: 0.0094
trigger times: 19
Loss after 6063102 batches: 0.0092
trigger times: 20
Loss after 6064065 batches: 0.0090
trigger times: 21
Loss after 6065028 batches: 0.0090
trigger times: 22
Loss after 6065991 batches: 0.0089
trigger times: 23
Loss after 6066954 batches: 0.0090
trigger times: 24
Loss after 6067917 batches: 0.0087
trigger times: 25
Early stopping!
Start to test process.
Loss after 6068880 batches: 0.0086
Time to train on one home:  52.628577709198
trigger times: 0
Loss after 6069843 batches: 0.0317
trigger times: 0
Loss after 6070806 batches: 0.0303
trigger times: 0
Loss after 6071769 batches: 0.0280
trigger times: 1
Loss after 6072732 batches: 0.0261
trigger times: 0
Loss after 6073695 batches: 0.0238
trigger times: 1
Loss after 6074658 batches: 0.0229
trigger times: 2
Loss after 6075621 batches: 0.0214
trigger times: 0
Loss after 6076584 batches: 0.0212
trigger times: 1
Loss after 6077547 batches: 0.0210
trigger times: 0
Loss after 6078510 batches: 0.0206
trigger times: 0
Loss after 6079473 batches: 0.0198
trigger times: 1
Loss after 6080436 batches: 0.0196
trigger times: 2
Loss after 6081399 batches: 0.0191
trigger times: 3
Loss after 6082362 batches: 0.0188
trigger times: 4
Loss after 6083325 batches: 0.0190
trigger times: 5
Loss after 6084288 batches: 0.0183
trigger times: 6
Loss after 6085251 batches: 0.0185
trigger times: 7
Loss after 6086214 batches: 0.0185
trigger times: 0
Loss after 6087177 batches: 0.0181
trigger times: 1
Loss after 6088140 batches: 0.0176
trigger times: 2
Loss after 6089103 batches: 0.0174
trigger times: 3
Loss after 6090066 batches: 0.0176
trigger times: 4
Loss after 6091029 batches: 0.0175
trigger times: 5
Loss after 6091992 batches: 0.0178
trigger times: 6
Loss after 6092955 batches: 0.0178
trigger times: 7
Loss after 6093918 batches: 0.0172
trigger times: 8
Loss after 6094881 batches: 0.0172
trigger times: 9
Loss after 6095844 batches: 0.0164
trigger times: 0
Loss after 6096807 batches: 0.0163
trigger times: 1
Loss after 6097770 batches: 0.0160
trigger times: 2
Loss after 6098733 batches: 0.0165
trigger times: 3
Loss after 6099696 batches: 0.0163
trigger times: 4
Loss after 6100659 batches: 0.0162
trigger times: 5
Loss after 6101622 batches: 0.0163
trigger times: 6
Loss after 6102585 batches: 0.0159
trigger times: 7
Loss after 6103548 batches: 0.0159
trigger times: 8
Loss after 6104511 batches: 0.0155
trigger times: 9
Loss after 6105474 batches: 0.0165
trigger times: 10
Loss after 6106437 batches: 0.0155
trigger times: 11
Loss after 6107400 batches: 0.0152
trigger times: 12
Loss after 6108363 batches: 0.0155
trigger times: 13
Loss after 6109326 batches: 0.0150
trigger times: 14
Loss after 6110289 batches: 0.0153
trigger times: 15
Loss after 6111252 batches: 0.0150
trigger times: 16
Loss after 6112215 batches: 0.0150
trigger times: 17
Loss after 6113178 batches: 0.0148
trigger times: 18
Loss after 6114141 batches: 0.0143
trigger times: 19
Loss after 6115104 batches: 0.0147
trigger times: 20
Loss after 6116067 batches: 0.0144
trigger times: 21
Loss after 6117030 batches: 0.0145
trigger times: 22
Loss after 6117993 batches: 0.0143
trigger times: 23
Loss after 6118956 batches: 0.0148
trigger times: 24
Loss after 6119919 batches: 0.0148
trigger times: 25
Early stopping!
Start to test process.
Loss after 6120882 batches: 0.0147
Time to train on one home:  75.19850015640259
trigger times: 0
Loss after 6121845 batches: 0.1036
trigger times: 0
Loss after 6122808 batches: 0.0956
trigger times: 1
Loss after 6123771 batches: 0.0922
trigger times: 2
Loss after 6124734 batches: 0.0892
trigger times: 3
Loss after 6125697 batches: 0.0868
trigger times: 4
Loss after 6126660 batches: 0.0845
trigger times: 5
Loss after 6127623 batches: 0.0826
trigger times: 6
Loss after 6128586 batches: 0.0814
trigger times: 7
Loss after 6129549 batches: 0.0806
trigger times: 8
Loss after 6130512 batches: 0.0794
trigger times: 9
Loss after 6131475 batches: 0.0790
trigger times: 10
Loss after 6132438 batches: 0.0780
trigger times: 11
Loss after 6133401 batches: 0.0769
trigger times: 12
Loss after 6134364 batches: 0.0767
trigger times: 13
Loss after 6135327 batches: 0.0776
trigger times: 14
Loss after 6136290 batches: 0.0780
trigger times: 15
Loss after 6137253 batches: 0.0777
trigger times: 16
Loss after 6138216 batches: 0.0753
trigger times: 17
Loss after 6139179 batches: 0.0751
trigger times: 18
Loss after 6140142 batches: 0.0732
trigger times: 19
Loss after 6141105 batches: 0.0726
trigger times: 20
Loss after 6142068 batches: 0.0734
trigger times: 21
Loss after 6143031 batches: 0.0708
trigger times: 22
Loss after 6143994 batches: 0.0724
trigger times: 23
Loss after 6144957 batches: 0.0724
trigger times: 24
Loss after 6145920 batches: 0.0741
trigger times: 25
Early stopping!
Start to test process.
Loss after 6146883 batches: 0.0733
Time to train on one home:  53.767740964889526
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6147846 batches: 0.0842
trigger times: 1
Loss after 6148809 batches: 0.0784
trigger times: 2
Loss after 6149772 batches: 0.0765
trigger times: 3
Loss after 6150735 batches: 0.0748
trigger times: 4
Loss after 6151698 batches: 0.0726
trigger times: 5
Loss after 6152661 batches: 0.0712
trigger times: 6
Loss after 6153624 batches: 0.0694
trigger times: 7
Loss after 6154587 batches: 0.0692
trigger times: 8
Loss after 6155550 batches: 0.0686
trigger times: 9
Loss after 6156513 batches: 0.0689
trigger times: 10
Loss after 6157476 batches: 0.0679
trigger times: 11
Loss after 6158439 batches: 0.0666
trigger times: 12
Loss after 6159402 batches: 0.0657
trigger times: 13
Loss after 6160365 batches: 0.0662
trigger times: 14
Loss after 6161328 batches: 0.0660
trigger times: 15
Loss after 6162291 batches: 0.0661
trigger times: 16
Loss after 6163254 batches: 0.0652
trigger times: 17
Loss after 6164217 batches: 0.0652
trigger times: 18
Loss after 6165180 batches: 0.0651
trigger times: 19
Loss after 6166143 batches: 0.0638
trigger times: 20
Loss after 6167106 batches: 0.0637
trigger times: 21
Loss after 6168069 batches: 0.0635
trigger times: 22
Loss after 6169032 batches: 0.0633
trigger times: 23
Loss after 6169995 batches: 0.0626
trigger times: 24
Loss after 6170958 batches: 0.0638
trigger times: 25
Early stopping!
Start to test process.
Loss after 6171921 batches: 0.0628
Time to train on one home:  51.13389182090759
trigger times: 0
Loss after 6172884 batches: 0.0816
trigger times: 0
Loss after 6173847 batches: 0.0758
trigger times: 0
Loss after 6174810 batches: 0.0722
trigger times: 1
Loss after 6175773 batches: 0.0707
trigger times: 0
Loss after 6176736 batches: 0.0688
trigger times: 1
Loss after 6177699 batches: 0.0673
trigger times: 2
Loss after 6178662 batches: 0.0663
trigger times: 3
Loss after 6179625 batches: 0.0659
trigger times: 4
Loss after 6180588 batches: 0.0650
trigger times: 5
Loss after 6181551 batches: 0.0634
trigger times: 6
Loss after 6182514 batches: 0.0647
trigger times: 7
Loss after 6183477 batches: 0.0629
trigger times: 8
Loss after 6184440 batches: 0.0624
trigger times: 9
Loss after 6185403 batches: 0.0615
trigger times: 10
Loss after 6186366 batches: 0.0614
trigger times: 11
Loss after 6187329 batches: 0.0615
trigger times: 12
Loss after 6188292 batches: 0.0607
trigger times: 13
Loss after 6189255 batches: 0.0600
trigger times: 14
Loss after 6190218 batches: 0.0610
trigger times: 15
Loss after 6191181 batches: 0.0602
trigger times: 16
Loss after 6192144 batches: 0.0598
trigger times: 17
Loss after 6193107 batches: 0.0585
trigger times: 18
Loss after 6194070 batches: 0.0586
trigger times: 19
Loss after 6195033 batches: 0.0593
trigger times: 20
Loss after 6195996 batches: 0.0585
trigger times: 21
Loss after 6196959 batches: 0.0593
trigger times: 22
Loss after 6197922 batches: 0.0580
trigger times: 23
Loss after 6198885 batches: 0.0570
trigger times: 24
Loss after 6199848 batches: 0.0577
trigger times: 25
Early stopping!
Start to test process.
Loss after 6200811 batches: 0.0578
Time to train on one home:  53.77926468849182
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6201774 batches: 0.0569
trigger times: 1
Loss after 6202737 batches: 0.0533
trigger times: 0
Loss after 6203700 batches: 0.0503
trigger times: 1
Loss after 6204663 batches: 0.0472
trigger times: 2
Loss after 6205626 batches: 0.0439
trigger times: 3
Loss after 6206589 batches: 0.0413
trigger times: 4
Loss after 6207552 batches: 0.0392
trigger times: 5
Loss after 6208515 batches: 0.0378
trigger times: 6
Loss after 6209478 batches: 0.0362
trigger times: 7
Loss after 6210441 batches: 0.0355
trigger times: 8
Loss after 6211404 batches: 0.0342
trigger times: 9
Loss after 6212367 batches: 0.0334
trigger times: 10
Loss after 6213330 batches: 0.0329
trigger times: 11
Loss after 6214293 batches: 0.0331
trigger times: 12
Loss after 6215256 batches: 0.0317
trigger times: 13
Loss after 6216219 batches: 0.0319
trigger times: 14
Loss after 6217182 batches: 0.0318
trigger times: 15
Loss after 6218145 batches: 0.0314
trigger times: 16
Loss after 6219108 batches: 0.0312
trigger times: 17
Loss after 6220071 batches: 0.0315
trigger times: 18
Loss after 6221034 batches: 0.0318
trigger times: 19
Loss after 6221997 batches: 0.0309
trigger times: 20
Loss after 6222960 batches: 0.0301
trigger times: 21
Loss after 6223923 batches: 0.0287
trigger times: 22
Loss after 6224886 batches: 0.0286
trigger times: 23
Loss after 6225849 batches: 0.0278
trigger times: 24
Loss after 6226812 batches: 0.0286
trigger times: 25
Early stopping!
Start to test process.
Loss after 6227775 batches: 0.0296
Time to train on one home:  52.37860059738159
trigger times: 0
Loss after 6228733 batches: 0.0952
trigger times: 1
Loss after 6229691 batches: 0.0509
trigger times: 2
Loss after 6230649 batches: 0.0509
trigger times: 3
Loss after 6231607 batches: 0.0418
trigger times: 4
Loss after 6232565 batches: 0.0390
trigger times: 5
Loss after 6233523 batches: 0.0362
trigger times: 6
Loss after 6234481 batches: 0.0353
trigger times: 7
Loss after 6235439 batches: 0.0313
trigger times: 8
Loss after 6236397 batches: 0.0290
trigger times: 9
Loss after 6237355 batches: 0.0294
trigger times: 10
Loss after 6238313 batches: 0.0289
trigger times: 11
Loss after 6239271 batches: 0.0273
trigger times: 12
Loss after 6240229 batches: 0.0273
trigger times: 13
Loss after 6241187 batches: 0.0277
trigger times: 14
Loss after 6242145 batches: 0.0273
trigger times: 15
Loss after 6243103 batches: 0.0265
trigger times: 16
Loss after 6244061 batches: 0.0255
trigger times: 17
Loss after 6245019 batches: 0.0247
trigger times: 18
Loss after 6245977 batches: 0.0238
trigger times: 19
Loss after 6246935 batches: 0.0240
trigger times: 20
Loss after 6247893 batches: 0.0250
trigger times: 21
Loss after 6248851 batches: 0.0251
trigger times: 22
Loss after 6249809 batches: 0.0258
trigger times: 23
Loss after 6250767 batches: 0.0259
trigger times: 24
Loss after 6251725 batches: 0.0245
trigger times: 25
Early stopping!
Start to test process.
Loss after 6252683 batches: 0.0237
Time to train on one home:  51.84784698486328
trigger times: 0
Loss after 6253645 batches: 0.0693
trigger times: 1
Loss after 6254607 batches: 0.0675
trigger times: 2
Loss after 6255569 batches: 0.0650
trigger times: 3
Loss after 6256531 batches: 0.0629
trigger times: 4
Loss after 6257493 batches: 0.0616
trigger times: 5
Loss after 6258455 batches: 0.0608
trigger times: 6
Loss after 6259417 batches: 0.0605
trigger times: 7
Loss after 6260379 batches: 0.0607
trigger times: 8
Loss after 6261341 batches: 0.0597
trigger times: 9
Loss after 6262303 batches: 0.0598
trigger times: 10
Loss after 6263265 batches: 0.0587
trigger times: 11
Loss after 6264227 batches: 0.0589
trigger times: 12
Loss after 6265189 batches: 0.0590
trigger times: 13
Loss after 6266151 batches: 0.0586
trigger times: 14
Loss after 6267113 batches: 0.0570
trigger times: 15
Loss after 6268075 batches: 0.0570
trigger times: 16
Loss after 6269037 batches: 0.0570
trigger times: 17
Loss after 6269999 batches: 0.0579
trigger times: 18
Loss after 6270961 batches: 0.0573
trigger times: 19
Loss after 6271923 batches: 0.0566
trigger times: 20
Loss after 6272885 batches: 0.0562
trigger times: 21
Loss after 6273847 batches: 0.0563
trigger times: 22
Loss after 6274809 batches: 0.0564
trigger times: 23
Loss after 6275771 batches: 0.0562
trigger times: 24
Loss after 6276733 batches: 0.0557
trigger times: 25
Early stopping!
Start to test process.
Loss after 6277695 batches: 0.0542
Time to train on one home:  51.03423571586609
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6278658 batches: 0.0628
trigger times: 1
Loss after 6279621 batches: 0.0259
trigger times: 2
Loss after 6280584 batches: 0.0249
trigger times: 3
Loss after 6281547 batches: 0.0198
trigger times: 4
Loss after 6282510 batches: 0.0181
trigger times: 5
Loss after 6283473 batches: 0.0168
trigger times: 6
Loss after 6284436 batches: 0.0159
trigger times: 7
Loss after 6285399 batches: 0.0155
trigger times: 8
Loss after 6286362 batches: 0.0149
trigger times: 9
Loss after 6287325 batches: 0.0145
trigger times: 10
Loss after 6288288 batches: 0.0147
trigger times: 11
Loss after 6289251 batches: 0.0144
trigger times: 12
Loss after 6290214 batches: 0.0140
trigger times: 13
Loss after 6291177 batches: 0.0141
trigger times: 14
Loss after 6292140 batches: 0.0138
trigger times: 15
Loss after 6293103 batches: 0.0145
trigger times: 16
Loss after 6294066 batches: 0.0140
trigger times: 17
Loss after 6295029 batches: 0.0139
trigger times: 18
Loss after 6295992 batches: 0.0136
trigger times: 19
Loss after 6296955 batches: 0.0135
trigger times: 20
Loss after 6297918 batches: 0.0134
trigger times: 21
Loss after 6298881 batches: 0.0138
trigger times: 22
Loss after 6299844 batches: 0.0134
trigger times: 23
Loss after 6300807 batches: 0.0132
trigger times: 24
Loss after 6301770 batches: 0.0133
trigger times: 25
Early stopping!
Start to test process.
Loss after 6302733 batches: 0.0133
Time to train on one home:  50.65924692153931
trigger times: 0
Loss after 6303696 batches: 0.0667
trigger times: 0
Loss after 6304659 batches: 0.0517
trigger times: 1
Loss after 6305622 batches: 0.0501
trigger times: 2
Loss after 6306585 batches: 0.0457
trigger times: 3
Loss after 6307548 batches: 0.0426
trigger times: 4
Loss after 6308511 batches: 0.0422
trigger times: 0
Loss after 6309474 batches: 0.0406
trigger times: 1
Loss after 6310437 batches: 0.0389
trigger times: 2
Loss after 6311400 batches: 0.0397
trigger times: 3
Loss after 6312363 batches: 0.0385
trigger times: 4
Loss after 6313326 batches: 0.0381
trigger times: 5
Loss after 6314289 batches: 0.0377
trigger times: 6
Loss after 6315252 batches: 0.0376
trigger times: 7
Loss after 6316215 batches: 0.0389
trigger times: 8
Loss after 6317178 batches: 0.0380
trigger times: 9
Loss after 6318141 batches: 0.0385
trigger times: 10
Loss after 6319104 batches: 0.0378
trigger times: 11
Loss after 6320067 batches: 0.0363
trigger times: 12
Loss after 6321030 batches: 0.0369
trigger times: 13
Loss after 6321993 batches: 0.0366
trigger times: 14
Loss after 6322956 batches: 0.0363
trigger times: 15
Loss after 6323919 batches: 0.0359
trigger times: 16
Loss after 6324882 batches: 0.0361
trigger times: 17
Loss after 6325845 batches: 0.0358
trigger times: 18
Loss after 6326808 batches: 0.0357
trigger times: 19
Loss after 6327771 batches: 0.0356
trigger times: 20
Loss after 6328734 batches: 0.0359
trigger times: 21
Loss after 6329697 batches: 0.0343
trigger times: 22
Loss after 6330660 batches: 0.0360
trigger times: 0
Loss after 6331623 batches: 0.0350
trigger times: 1
Loss after 6332586 batches: 0.0346
trigger times: 2
Loss after 6333549 batches: 0.0348
trigger times: 3
Loss after 6334512 batches: 0.0347
trigger times: 4
Loss after 6335475 batches: 0.0341
trigger times: 5
Loss after 6336438 batches: 0.0341
trigger times: 6
Loss after 6337401 batches: 0.0329
trigger times: 7
Loss after 6338364 batches: 0.0351
trigger times: 8
Loss after 6339327 batches: 0.0359
trigger times: 9
Loss after 6340290 batches: 0.0349
trigger times: 10
Loss after 6341253 batches: 0.0354
trigger times: 11
Loss after 6342216 batches: 0.0345
trigger times: 12
Loss after 6343179 batches: 0.0337
trigger times: 13
Loss after 6344142 batches: 0.0324
trigger times: 14
Loss after 6345105 batches: 0.0325
trigger times: 15
Loss after 6346068 batches: 0.0331
trigger times: 16
Loss after 6347031 batches: 0.0317
trigger times: 17
Loss after 6347994 batches: 0.0330
trigger times: 18
Loss after 6348957 batches: 0.0323
trigger times: 19
Loss after 6349920 batches: 0.0311
trigger times: 20
Loss after 6350883 batches: 0.0355
trigger times: 21
Loss after 6351846 batches: 0.0354
trigger times: 22
Loss after 6352809 batches: 0.0338
trigger times: 23
Loss after 6353772 batches: 0.0337
trigger times: 24
Loss after 6354735 batches: 0.0321
trigger times: 25
Early stopping!
Start to test process.
Loss after 6355698 batches: 0.0312
Time to train on one home:  72.69868516921997
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6356661 batches: 0.0573
trigger times: 1
Loss after 6357624 batches: 0.0533
trigger times: 0
Loss after 6358587 batches: 0.0507
trigger times: 0
Loss after 6359550 batches: 0.0476
trigger times: 1
Loss after 6360513 batches: 0.0438
trigger times: 2
Loss after 6361476 batches: 0.0412
trigger times: 3
Loss after 6362439 batches: 0.0386
trigger times: 4
Loss after 6363402 batches: 0.0370
trigger times: 5
Loss after 6364365 batches: 0.0361
trigger times: 6
Loss after 6365328 batches: 0.0352
trigger times: 7
Loss after 6366291 batches: 0.0341
trigger times: 8
Loss after 6367254 batches: 0.0338
trigger times: 9
Loss after 6368217 batches: 0.0331
trigger times: 10
Loss after 6369180 batches: 0.0326
trigger times: 11
Loss after 6370143 batches: 0.0317
trigger times: 12
Loss after 6371106 batches: 0.0322
trigger times: 13
Loss after 6372069 batches: 0.0313
trigger times: 14
Loss after 6373032 batches: 0.0309
trigger times: 15
Loss after 6373995 batches: 0.0307
trigger times: 16
Loss after 6374958 batches: 0.0300
trigger times: 17
Loss after 6375921 batches: 0.0307
trigger times: 18
Loss after 6376884 batches: 0.0317
trigger times: 19
Loss after 6377847 batches: 0.0305
trigger times: 20
Loss after 6378810 batches: 0.0301
trigger times: 21
Loss after 6379773 batches: 0.0300
trigger times: 22
Loss after 6380736 batches: 0.0288
trigger times: 23
Loss after 6381699 batches: 0.0286
trigger times: 24
Loss after 6382662 batches: 0.0282
trigger times: 25
Early stopping!
Start to test process.
Loss after 6383625 batches: 0.0275
Time to train on one home:  52.959556102752686
trigger times: 0
Loss after 6384588 batches: 0.0666
trigger times: 0
Loss after 6385551 batches: 0.0527
trigger times: 1
Loss after 6386514 batches: 0.0502
trigger times: 2
Loss after 6387477 batches: 0.0452
trigger times: 3
Loss after 6388440 batches: 0.0419
trigger times: 4
Loss after 6389403 batches: 0.0419
trigger times: 5
Loss after 6390366 batches: 0.0403
trigger times: 0
Loss after 6391329 batches: 0.0402
trigger times: 1
Loss after 6392292 batches: 0.0400
trigger times: 2
Loss after 6393255 batches: 0.0388
trigger times: 3
Loss after 6394218 batches: 0.0379
trigger times: 4
Loss after 6395181 batches: 0.0390
trigger times: 5
Loss after 6396144 batches: 0.0388
trigger times: 6
Loss after 6397107 batches: 0.0385
trigger times: 7
Loss after 6398070 batches: 0.0365
trigger times: 8
Loss after 6399033 batches: 0.0365
trigger times: 9
Loss after 6399996 batches: 0.0357
trigger times: 10
Loss after 6400959 batches: 0.0363
trigger times: 11
Loss after 6401922 batches: 0.0363
trigger times: 12
Loss after 6402885 batches: 0.0367
trigger times: 13
Loss after 6403848 batches: 0.0359
trigger times: 14
Loss after 6404811 batches: 0.0357
trigger times: 15
Loss after 6405774 batches: 0.0364
trigger times: 16
Loss after 6406737 batches: 0.0343
trigger times: 17
Loss after 6407700 batches: 0.0358
trigger times: 18
Loss after 6408663 batches: 0.0362
trigger times: 19
Loss after 6409626 batches: 0.0356
trigger times: 0
Loss after 6410589 batches: 0.0367
trigger times: 1
Loss after 6411552 batches: 0.0352
trigger times: 2
Loss after 6412515 batches: 0.0346
trigger times: 3
Loss after 6413478 batches: 0.0335
trigger times: 4
Loss after 6414441 batches: 0.0339
trigger times: 5
Loss after 6415404 batches: 0.0339
trigger times: 6
Loss after 6416367 batches: 0.0346
trigger times: 7
Loss after 6417330 batches: 0.0344
trigger times: 8
Loss after 6418293 batches: 0.0374
trigger times: 9
Loss after 6419256 batches: 0.0383
trigger times: 10
Loss after 6420219 batches: 0.0372
trigger times: 11
Loss after 6421182 batches: 0.0354
trigger times: 12
Loss after 6422145 batches: 0.0352
trigger times: 13
Loss after 6423108 batches: 0.0340
trigger times: 14
Loss after 6424071 batches: 0.0344
trigger times: 15
Loss after 6425034 batches: 0.0337
trigger times: 16
Loss after 6425997 batches: 0.0331
trigger times: 17
Loss after 6426960 batches: 0.0328
trigger times: 18
Loss after 6427923 batches: 0.0337
trigger times: 19
Loss after 6428886 batches: 0.0327
trigger times: 20
Loss after 6429849 batches: 0.0315
trigger times: 21
Loss after 6430812 batches: 0.0322
trigger times: 22
Loss after 6431775 batches: 0.0328
trigger times: 23
Loss after 6432738 batches: 0.0319
trigger times: 24
Loss after 6433701 batches: 0.0317
trigger times: 25
Early stopping!
Start to test process.
Loss after 6434664 batches: 0.0317
Time to train on one home:  71.2389395236969
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6435627 batches: 0.0601
trigger times: 0
Loss after 6436590 batches: 0.0539
trigger times: 0
Loss after 6437553 batches: 0.0529
trigger times: 1
Loss after 6438516 batches: 0.0498
trigger times: 2
Loss after 6439479 batches: 0.0482
trigger times: 3
Loss after 6440442 batches: 0.0464
trigger times: 4
Loss after 6441405 batches: 0.0454
trigger times: 5
Loss after 6442368 batches: 0.0450
trigger times: 0
Loss after 6443331 batches: 0.0437
trigger times: 1
Loss after 6444294 batches: 0.0425
trigger times: 0
Loss after 6445257 batches: 0.0431
trigger times: 1
Loss after 6446220 batches: 0.0422
trigger times: 2
Loss after 6447183 batches: 0.0413
trigger times: 3
Loss after 6448146 batches: 0.0405
trigger times: 4
Loss after 6449109 batches: 0.0400
trigger times: 0
Loss after 6450072 batches: 0.0396
trigger times: 1
Loss after 6451035 batches: 0.0399
trigger times: 2
Loss after 6451998 batches: 0.0394
trigger times: 0
Loss after 6452961 batches: 0.0390
trigger times: 1
Loss after 6453924 batches: 0.0378
trigger times: 2
Loss after 6454887 batches: 0.0385
trigger times: 3
Loss after 6455850 batches: 0.0376
trigger times: 0
Loss after 6456813 batches: 0.0382
trigger times: 0
Loss after 6457776 batches: 0.0383
trigger times: 1
Loss after 6458739 batches: 0.0386
trigger times: 2
Loss after 6459702 batches: 0.0384
trigger times: 3
Loss after 6460665 batches: 0.0368
trigger times: 4
Loss after 6461628 batches: 0.0378
trigger times: 5
Loss after 6462591 batches: 0.0362
trigger times: 6
Loss after 6463554 batches: 0.0359
trigger times: 7
Loss after 6464517 batches: 0.0358
trigger times: 0
Loss after 6465480 batches: 0.0363
trigger times: 1
Loss after 6466443 batches: 0.0358
trigger times: 2
Loss after 6467406 batches: 0.0355
trigger times: 3
Loss after 6468369 batches: 0.0369
trigger times: 4
Loss after 6469332 batches: 0.0370
trigger times: 5
Loss after 6470295 batches: 0.0358
trigger times: 6
Loss after 6471258 batches: 0.0351
trigger times: 0
Loss after 6472221 batches: 0.0353
trigger times: 1
Loss after 6473184 batches: 0.0344
trigger times: 0
Loss after 6474147 batches: 0.0346
trigger times: 1
Loss after 6475110 batches: 0.0346
trigger times: 2
Loss after 6476073 batches: 0.0335
trigger times: 3
Loss after 6477036 batches: 0.0339
trigger times: 4
Loss after 6477999 batches: 0.0339
trigger times: 5
Loss after 6478962 batches: 0.0334
trigger times: 6
Loss after 6479925 batches: 0.0333
trigger times: 7
Loss after 6480888 batches: 0.0334
trigger times: 8
Loss after 6481851 batches: 0.0339
trigger times: 9
Loss after 6482814 batches: 0.0333
trigger times: 10
Loss after 6483777 batches: 0.0337
trigger times: 11
Loss after 6484740 batches: 0.0334
trigger times: 12
Loss after 6485703 batches: 0.0340
trigger times: 13
Loss after 6486666 batches: 0.0340
trigger times: 14
Loss after 6487629 batches: 0.0325
trigger times: 15
Loss after 6488592 batches: 0.0319
trigger times: 16
Loss after 6489555 batches: 0.0314
trigger times: 17
Loss after 6490518 batches: 0.0315
trigger times: 18
Loss after 6491481 batches: 0.0317
trigger times: 19
Loss after 6492444 batches: 0.0302
trigger times: 20
Loss after 6493407 batches: 0.0308
trigger times: 21
Loss after 6494370 batches: 0.0330
trigger times: 22
Loss after 6495333 batches: 0.0313
trigger times: 23
Loss after 6496296 batches: 0.0318
trigger times: 24
Loss after 6497259 batches: 0.0304
trigger times: 25
Early stopping!
Start to test process.
Loss after 6498222 batches: 0.0308
Time to train on one home:  80.7694480419159
trigger times: 0
Loss after 6499185 batches: 0.0657
trigger times: 1
Loss after 6500148 batches: 0.0492
trigger times: 2
Loss after 6501111 batches: 0.0490
trigger times: 3
Loss after 6502074 batches: 0.0463
trigger times: 4
Loss after 6503037 batches: 0.0453
trigger times: 5
Loss after 6504000 batches: 0.0435
trigger times: 6
Loss after 6504963 batches: 0.0423
trigger times: 7
Loss after 6505926 batches: 0.0412
trigger times: 8
Loss after 6506889 batches: 0.0405
trigger times: 9
Loss after 6507852 batches: 0.0394
trigger times: 10
Loss after 6508815 batches: 0.0406
trigger times: 11
Loss after 6509778 batches: 0.0403
trigger times: 12
Loss after 6510741 batches: 0.0400
trigger times: 13
Loss after 6511704 batches: 0.0395
trigger times: 14
Loss after 6512667 batches: 0.0394
trigger times: 15
Loss after 6513630 batches: 0.0389
trigger times: 16
Loss after 6514593 batches: 0.0388
trigger times: 17
Loss after 6515556 batches: 0.0380
trigger times: 18
Loss after 6516519 batches: 0.0383
trigger times: 19
Loss after 6517482 batches: 0.0377
trigger times: 20
Loss after 6518445 batches: 0.0377
trigger times: 21
Loss after 6519408 batches: 0.0369
trigger times: 22
Loss after 6520371 batches: 0.0371
trigger times: 23
Loss after 6521334 batches: 0.0376
trigger times: 24
Loss after 6522297 batches: 0.0379
trigger times: 25
Early stopping!
Start to test process.
Loss after 6523260 batches: 0.0374
Time to train on one home:  51.16459131240845
trigger times: 0
Loss after 6524223 batches: 0.1038
trigger times: 0
Loss after 6525186 batches: 0.0952
trigger times: 1
Loss after 6526149 batches: 0.0917
trigger times: 2
Loss after 6527112 batches: 0.0887
trigger times: 3
Loss after 6528075 batches: 0.0861
trigger times: 4
Loss after 6529038 batches: 0.0852
trigger times: 5
Loss after 6530001 batches: 0.0827
trigger times: 6
Loss after 6530964 batches: 0.0826
trigger times: 0
Loss after 6531927 batches: 0.0806
trigger times: 1
Loss after 6532890 batches: 0.0791
trigger times: 2
Loss after 6533853 batches: 0.0803
trigger times: 3
Loss after 6534816 batches: 0.0791
trigger times: 4
Loss after 6535779 batches: 0.0764
trigger times: 5
Loss after 6536742 batches: 0.0769
trigger times: 6
Loss after 6537705 batches: 0.0761
trigger times: 7
Loss after 6538668 batches: 0.0768
trigger times: 8
Loss after 6539631 batches: 0.0773
trigger times: 9
Loss after 6540594 batches: 0.0769
trigger times: 10
Loss after 6541557 batches: 0.0749
trigger times: 11
Loss after 6542520 batches: 0.0731
trigger times: 12
Loss after 6543483 batches: 0.0737
trigger times: 13
Loss after 6544446 batches: 0.0738
trigger times: 14
Loss after 6545409 batches: 0.0740
trigger times: 15
Loss after 6546372 batches: 0.0721
trigger times: 16
Loss after 6547335 batches: 0.0708
trigger times: 17
Loss after 6548298 batches: 0.0712
trigger times: 18
Loss after 6549261 batches: 0.0713
trigger times: 19
Loss after 6550224 batches: 0.0706
trigger times: 20
Loss after 6551187 batches: 0.0692
trigger times: 21
Loss after 6552150 batches: 0.0693
trigger times: 22
Loss after 6553113 batches: 0.0690
trigger times: 23
Loss after 6554076 batches: 0.0678
trigger times: 24
Loss after 6555039 batches: 0.0676
trigger times: 25
Early stopping!
Start to test process.
Loss after 6556002 batches: 0.0677
Time to train on one home:  56.60117316246033
trigger times: 0
Loss after 6556965 batches: 0.0955
trigger times: 1
Loss after 6557928 batches: 0.0716
trigger times: 2
Loss after 6558891 batches: 0.0667
trigger times: 3
Loss after 6559854 batches: 0.0627
trigger times: 4
Loss after 6560817 batches: 0.0588
trigger times: 5
Loss after 6561780 batches: 0.0562
trigger times: 0
Loss after 6562743 batches: 0.0547
trigger times: 0
Loss after 6563706 batches: 0.0531
trigger times: 1
Loss after 6564669 batches: 0.0514
trigger times: 2
Loss after 6565632 batches: 0.0508
trigger times: 3
Loss after 6566595 batches: 0.0498
trigger times: 4
Loss after 6567558 batches: 0.0506
trigger times: 5
Loss after 6568521 batches: 0.0490
trigger times: 6
Loss after 6569484 batches: 0.0484
trigger times: 7
Loss after 6570447 batches: 0.0477
trigger times: 8
Loss after 6571410 batches: 0.0480
trigger times: 9
Loss after 6572373 batches: 0.0459
trigger times: 0
Loss after 6573336 batches: 0.0461
trigger times: 1
Loss after 6574299 batches: 0.0463
trigger times: 0
Loss after 6575262 batches: 0.0455
trigger times: 1
Loss after 6576225 batches: 0.0446
trigger times: 0
Loss after 6577188 batches: 0.0447
trigger times: 0
Loss after 6578151 batches: 0.0453
trigger times: 1
Loss after 6579114 batches: 0.0446
trigger times: 2
Loss after 6580077 batches: 0.0444
trigger times: 3
Loss after 6581040 batches: 0.0447
trigger times: 4
Loss after 6582003 batches: 0.0435
trigger times: 5
Loss after 6582966 batches: 0.0437
trigger times: 6
Loss after 6583929 batches: 0.0429
trigger times: 7
Loss after 6584892 batches: 0.0417
trigger times: 8
Loss after 6585855 batches: 0.0420
trigger times: 9
Loss after 6586818 batches: 0.0411
trigger times: 10
Loss after 6587781 batches: 0.0428
trigger times: 11
Loss after 6588744 batches: 0.0421
trigger times: 12
Loss after 6589707 batches: 0.0426
trigger times: 13
Loss after 6590670 batches: 0.0428
trigger times: 14
Loss after 6591633 batches: 0.0413
trigger times: 15
Loss after 6592596 batches: 0.0407
trigger times: 16
Loss after 6593559 batches: 0.0402
trigger times: 17
Loss after 6594522 batches: 0.0406
trigger times: 18
Loss after 6595485 batches: 0.0400
trigger times: 19
Loss after 6596448 batches: 0.0407
trigger times: 20
Loss after 6597411 batches: 0.0413
trigger times: 21
Loss after 6598374 batches: 0.0405
trigger times: 22
Loss after 6599337 batches: 0.0402
trigger times: 23
Loss after 6600300 batches: 0.0392
trigger times: 24
Loss after 6601263 batches: 0.0389
trigger times: 25
Early stopping!
Start to test process.
Loss after 6602226 batches: 0.0388
Time to train on one home:  67.59163522720337
trigger times: 0
Loss after 6603155 batches: 0.1147
trigger times: 1
Loss after 6604084 batches: 0.0674
trigger times: 2
Loss after 6605013 batches: 0.0608
trigger times: 3
Loss after 6605942 batches: 0.0482
trigger times: 4
Loss after 6606871 batches: 0.0430
trigger times: 5
Loss after 6607800 batches: 0.0423
trigger times: 6
Loss after 6608729 batches: 0.0405
trigger times: 7
Loss after 6609658 batches: 0.0410
trigger times: 8
Loss after 6610587 batches: 0.0427
trigger times: 9
Loss after 6611516 batches: 0.0398
trigger times: 10
Loss after 6612445 batches: 0.0400
trigger times: 11
Loss after 6613374 batches: 0.0434
trigger times: 12
Loss after 6614303 batches: 0.0392
trigger times: 13
Loss after 6615232 batches: 0.0367
trigger times: 14
Loss after 6616161 batches: 0.0354
trigger times: 15
Loss after 6617090 batches: 0.0365
trigger times: 16
Loss after 6618019 batches: 0.0630
trigger times: 17
Loss after 6618948 batches: 0.0483
trigger times: 18
Loss after 6619877 batches: 0.0443
trigger times: 19
Loss after 6620806 batches: 0.0436
trigger times: 20
Loss after 6621735 batches: 0.0393
trigger times: 21
Loss after 6622664 batches: 0.0374
trigger times: 22
Loss after 6623593 batches: 0.0367
trigger times: 23
Loss after 6624522 batches: 0.0354
trigger times: 24
Loss after 6625451 batches: 0.0387
trigger times: 25
Early stopping!
Start to test process.
Loss after 6626380 batches: 0.0372
Time to train on one home:  50.52691388130188
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6627343 batches: 0.0496
trigger times: 1
Loss after 6628306 batches: 0.0282
trigger times: 2
Loss after 6629269 batches: 0.0277
trigger times: 3
Loss after 6630232 batches: 0.0273
trigger times: 4
Loss after 6631195 batches: 0.0263
trigger times: 5
Loss after 6632158 batches: 0.0256
trigger times: 6
Loss after 6633121 batches: 0.0252
trigger times: 7
Loss after 6634084 batches: 0.0245
trigger times: 8
Loss after 6635047 batches: 0.0242
trigger times: 9
Loss after 6636010 batches: 0.0236
trigger times: 10
Loss after 6636973 batches: 0.0231
trigger times: 11
Loss after 6637936 batches: 0.0229
trigger times: 12
Loss after 6638899 batches: 0.0221
trigger times: 13
Loss after 6639862 batches: 0.0219
trigger times: 14
Loss after 6640825 batches: 0.0215
trigger times: 15
Loss after 6641788 batches: 0.0211
trigger times: 16
Loss after 6642751 batches: 0.0208
trigger times: 17
Loss after 6643714 batches: 0.0203
trigger times: 18
Loss after 6644677 batches: 0.0207
trigger times: 19
Loss after 6645640 batches: 0.0202
trigger times: 20
Loss after 6646603 batches: 0.0202
trigger times: 21
Loss after 6647566 batches: 0.0199
trigger times: 22
Loss after 6648529 batches: 0.0198
trigger times: 23
Loss after 6649492 batches: 0.0197
trigger times: 24
Loss after 6650455 batches: 0.0198
trigger times: 25
Early stopping!
Start to test process.
Loss after 6651418 batches: 0.0195
Time to train on one home:  50.9978551864624
trigger times: 0
Loss after 6652381 batches: 0.1581
trigger times: 1
Loss after 6653344 batches: 0.1171
trigger times: 2
Loss after 6654307 batches: 0.1103
trigger times: 3
Loss after 6655270 batches: 0.0973
trigger times: 4
Loss after 6656233 batches: 0.0899
trigger times: 5
Loss after 6657196 batches: 0.0871
trigger times: 6
Loss after 6658159 batches: 0.0857
trigger times: 7
Loss after 6659122 batches: 0.0845
trigger times: 8
Loss after 6660085 batches: 0.0821
trigger times: 9
Loss after 6661048 batches: 0.0823
trigger times: 10
Loss after 6662011 batches: 0.0820
trigger times: 11
Loss after 6662974 batches: 0.0817
trigger times: 12
Loss after 6663937 batches: 0.0797
trigger times: 13
Loss after 6664900 batches: 0.0777
trigger times: 14
Loss after 6665863 batches: 0.0771
trigger times: 15
Loss after 6666826 batches: 0.0744
trigger times: 16
Loss after 6667789 batches: 0.0730
trigger times: 17
Loss after 6668752 batches: 0.0681
trigger times: 18
Loss after 6669715 batches: 0.0667
trigger times: 19
Loss after 6670678 batches: 0.0647
trigger times: 20
Loss after 6671641 batches: 0.0664
trigger times: 21
Loss after 6672604 batches: 0.0630
trigger times: 22
Loss after 6673567 batches: 0.0615
trigger times: 23
Loss after 6674530 batches: 0.0610
trigger times: 24
Loss after 6675493 batches: 0.0604
trigger times: 25
Early stopping!
Start to test process.
Loss after 6676456 batches: 0.0584
Time to train on one home:  51.26571726799011
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6677419 batches: 0.0843
trigger times: 1
Loss after 6678382 batches: 0.0791
trigger times: 2
Loss after 6679345 batches: 0.0772
trigger times: 3
Loss after 6680308 batches: 0.0745
trigger times: 4
Loss after 6681271 batches: 0.0734
trigger times: 5
Loss after 6682234 batches: 0.0721
trigger times: 6
Loss after 6683197 batches: 0.0707
trigger times: 7
Loss after 6684160 batches: 0.0704
trigger times: 8
Loss after 6685123 batches: 0.0680
trigger times: 9
Loss after 6686086 batches: 0.0674
trigger times: 10
Loss after 6687049 batches: 0.0681
trigger times: 11
Loss after 6688012 batches: 0.0667
trigger times: 12
Loss after 6688975 batches: 0.0651
trigger times: 13
Loss after 6689938 batches: 0.0671
trigger times: 14
Loss after 6690901 batches: 0.0660
trigger times: 15
Loss after 6691864 batches: 0.0653
trigger times: 16
Loss after 6692827 batches: 0.0641
trigger times: 17
Loss after 6693790 batches: 0.0652
trigger times: 18
Loss after 6694753 batches: 0.0649
trigger times: 19
Loss after 6695716 batches: 0.0643
trigger times: 20
Loss after 6696679 batches: 0.0627
trigger times: 21
Loss after 6697642 batches: 0.0627
trigger times: 22
Loss after 6698605 batches: 0.0635
trigger times: 23
Loss after 6699568 batches: 0.0624
trigger times: 24
Loss after 6700531 batches: 0.0617
trigger times: 25
Early stopping!
Start to test process.
Loss after 6701494 batches: 0.0623
Time to train on one home:  53.35888957977295
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6702457 batches: 0.0800
trigger times: 1
Loss after 6703420 batches: 0.0777
trigger times: 2
Loss after 6704383 batches: 0.0755
trigger times: 3
Loss after 6705346 batches: 0.0727
trigger times: 4
Loss after 6706309 batches: 0.0712
trigger times: 5
Loss after 6707272 batches: 0.0697
trigger times: 6
Loss after 6708235 batches: 0.0701
trigger times: 7
Loss after 6709198 batches: 0.0703
trigger times: 8
Loss after 6710161 batches: 0.0734
trigger times: 9
Loss after 6711124 batches: 0.0724
trigger times: 10
Loss after 6712087 batches: 0.0709
trigger times: 11
Loss after 6713050 batches: 0.0694
trigger times: 12
Loss after 6714013 batches: 0.0680
trigger times: 13
Loss after 6714976 batches: 0.0687
trigger times: 14
Loss after 6715939 batches: 0.0688
trigger times: 15
Loss after 6716902 batches: 0.0687
trigger times: 16
Loss after 6717865 batches: 0.0689
trigger times: 17
Loss after 6718828 batches: 0.0672
trigger times: 18
Loss after 6719791 batches: 0.0672
trigger times: 19
Loss after 6720754 batches: 0.0668
trigger times: 20
Loss after 6721717 batches: 0.0672
trigger times: 21
Loss after 6722680 batches: 0.0650
trigger times: 22
Loss after 6723643 batches: 0.0649
trigger times: 23
Loss after 6724606 batches: 0.0661
trigger times: 24
Loss after 6725569 batches: 0.0642
trigger times: 25
Early stopping!
Start to test process.
Loss after 6726532 batches: 0.0669
Time to train on one home:  54.31656765937805
trigger times: 0
Loss after 6727495 batches: 0.0322
trigger times: 0
Loss after 6728458 batches: 0.0306
trigger times: 0
Loss after 6729421 batches: 0.0278
trigger times: 1
Loss after 6730384 batches: 0.0263
trigger times: 0
Loss after 6731347 batches: 0.0243
trigger times: 1
Loss after 6732310 batches: 0.0229
trigger times: 2
Loss after 6733273 batches: 0.0214
trigger times: 3
Loss after 6734236 batches: 0.0210
trigger times: 0
Loss after 6735199 batches: 0.0208
trigger times: 1
Loss after 6736162 batches: 0.0210
trigger times: 2
Loss after 6737125 batches: 0.0196
trigger times: 3
Loss after 6738088 batches: 0.0197
trigger times: 0
Loss after 6739051 batches: 0.0197
trigger times: 1
Loss after 6740014 batches: 0.0196
trigger times: 2
Loss after 6740977 batches: 0.0187
trigger times: 3
Loss after 6741940 batches: 0.0181
trigger times: 4
Loss after 6742903 batches: 0.0185
trigger times: 5
Loss after 6743866 batches: 0.0179
trigger times: 6
Loss after 6744829 batches: 0.0183
trigger times: 7
Loss after 6745792 batches: 0.0176
trigger times: 8
Loss after 6746755 batches: 0.0174
trigger times: 9
Loss after 6747718 batches: 0.0176
trigger times: 10
Loss after 6748681 batches: 0.0172
trigger times: 11
Loss after 6749644 batches: 0.0170
trigger times: 12
Loss after 6750607 batches: 0.0165
trigger times: 0
Loss after 6751570 batches: 0.0161
trigger times: 1
Loss after 6752533 batches: 0.0162
trigger times: 2
Loss after 6753496 batches: 0.0168
trigger times: 3
Loss after 6754459 batches: 0.0167
trigger times: 4
Loss after 6755422 batches: 0.0171
trigger times: 5
Loss after 6756385 batches: 0.0169
trigger times: 6
Loss after 6757348 batches: 0.0161
trigger times: 7
Loss after 6758311 batches: 0.0161
trigger times: 0
Loss after 6759274 batches: 0.0159
trigger times: 1
Loss after 6760237 batches: 0.0158
trigger times: 2
Loss after 6761200 batches: 0.0156
trigger times: 3
Loss after 6762163 batches: 0.0153
trigger times: 4
Loss after 6763126 batches: 0.0155
trigger times: 5
Loss after 6764089 batches: 0.0159
trigger times: 6
Loss after 6765052 batches: 0.0157
trigger times: 7
Loss after 6766015 batches: 0.0157
trigger times: 8
Loss after 6766978 batches: 0.0157
trigger times: 9
Loss after 6767941 batches: 0.0158
trigger times: 10
Loss after 6768904 batches: 0.0154
trigger times: 11
Loss after 6769867 batches: 0.0150
trigger times: 12
Loss after 6770830 batches: 0.0146
trigger times: 13
Loss after 6771793 batches: 0.0146
trigger times: 14
Loss after 6772756 batches: 0.0144
trigger times: 15
Loss after 6773719 batches: 0.0146
trigger times: 16
Loss after 6774682 batches: 0.0140
trigger times: 17
Loss after 6775645 batches: 0.0145
trigger times: 18
Loss after 6776608 batches: 0.0148
trigger times: 19
Loss after 6777571 batches: 0.0141
trigger times: 20
Loss after 6778534 batches: 0.0140
trigger times: 21
Loss after 6779497 batches: 0.0139
trigger times: 22
Loss after 6780460 batches: 0.0141
trigger times: 23
Loss after 6781423 batches: 0.0137
trigger times: 24
Loss after 6782386 batches: 0.0135
trigger times: 25
Early stopping!
Start to test process.
Loss after 6783349 batches: 0.0132
Time to train on one home:  78.34429407119751
trigger times: 0
Loss after 6784312 batches: 0.0515
trigger times: 0
Loss after 6785275 batches: 0.0460
trigger times: 0
Loss after 6786238 batches: 0.0437
trigger times: 0
Loss after 6787201 batches: 0.0417
trigger times: 1
Loss after 6788164 batches: 0.0396
trigger times: 0
Loss after 6789127 batches: 0.0386
trigger times: 1
Loss after 6790090 batches: 0.0370
trigger times: 2
Loss after 6791053 batches: 0.0359
trigger times: 0
Loss after 6792016 batches: 0.0343
trigger times: 1
Loss after 6792979 batches: 0.0337
trigger times: 2
Loss after 6793942 batches: 0.0351
trigger times: 3
Loss after 6794905 batches: 0.0331
trigger times: 4
Loss after 6795868 batches: 0.0315
trigger times: 5
Loss after 6796831 batches: 0.0310
trigger times: 6
Loss after 6797794 batches: 0.0293
trigger times: 7
Loss after 6798757 batches: 0.0286
trigger times: 8
Loss after 6799720 batches: 0.0285
trigger times: 9
Loss after 6800683 batches: 0.0283
trigger times: 10
Loss after 6801646 batches: 0.0284
trigger times: 11
Loss after 6802609 batches: 0.0271
trigger times: 0
Loss after 6803572 batches: 0.0267
trigger times: 1
Loss after 6804535 batches: 0.0265
trigger times: 2
Loss after 6805498 batches: 0.0262
trigger times: 3
Loss after 6806461 batches: 0.0257
trigger times: 4
Loss after 6807424 batches: 0.0260
trigger times: 5
Loss after 6808387 batches: 0.0256
trigger times: 6
Loss after 6809350 batches: 0.0250
trigger times: 7
Loss after 6810313 batches: 0.0254
trigger times: 8
Loss after 6811276 batches: 0.0251
trigger times: 9
Loss after 6812239 batches: 0.0249
trigger times: 10
Loss after 6813202 batches: 0.0244
trigger times: 11
Loss after 6814165 batches: 0.0245
trigger times: 12
Loss after 6815128 batches: 0.0245
trigger times: 13
Loss after 6816091 batches: 0.0241
trigger times: 14
Loss after 6817054 batches: 0.0248
trigger times: 15
Loss after 6818017 batches: 0.0244
trigger times: 16
Loss after 6818980 batches: 0.0236
trigger times: 17
Loss after 6819943 batches: 0.0240
trigger times: 18
Loss after 6820906 batches: 0.0240
trigger times: 19
Loss after 6821869 batches: 0.0233
trigger times: 20
Loss after 6822832 batches: 0.0239
trigger times: 21
Loss after 6823795 batches: 0.0237
trigger times: 22
Loss after 6824758 batches: 0.0233
trigger times: 23
Loss after 6825721 batches: 0.0235
trigger times: 24
Loss after 6826684 batches: 0.0227
trigger times: 25
Early stopping!
Start to test process.
Loss after 6827647 batches: 0.0230
Time to train on one home:  67.76149392127991
trigger times: 0
Loss after 6828610 batches: 0.0936
trigger times: 1
Loss after 6829573 batches: 0.0725
trigger times: 2
Loss after 6830536 batches: 0.0682
trigger times: 3
Loss after 6831499 batches: 0.0619
trigger times: 4
Loss after 6832462 batches: 0.0565
trigger times: 5
Loss after 6833425 batches: 0.0549
trigger times: 6
Loss after 6834388 batches: 0.0533
trigger times: 7
Loss after 6835351 batches: 0.0520
trigger times: 8
Loss after 6836314 batches: 0.0510
trigger times: 9
Loss after 6837277 batches: 0.0503
trigger times: 10
Loss after 6838240 batches: 0.0486
trigger times: 11
Loss after 6839203 batches: 0.0483
trigger times: 12
Loss after 6840166 batches: 0.0473
trigger times: 13
Loss after 6841129 batches: 0.0478
trigger times: 14
Loss after 6842092 batches: 0.0460
trigger times: 15
Loss after 6843055 batches: 0.0458
trigger times: 16
Loss after 6844018 batches: 0.0467
trigger times: 17
Loss after 6844981 batches: 0.0452
trigger times: 18
Loss after 6845944 batches: 0.0455
trigger times: 19
Loss after 6846907 batches: 0.0453
trigger times: 20
Loss after 6847870 batches: 0.0458
trigger times: 21
Loss after 6848833 batches: 0.0448
trigger times: 22
Loss after 6849796 batches: 0.0448
trigger times: 23
Loss after 6850759 batches: 0.0433
trigger times: 24
Loss after 6851722 batches: 0.0433
trigger times: 25
Early stopping!
Start to test process.
Loss after 6852685 batches: 0.0443
Time to train on one home:  52.51029372215271
trigger times: 0
Loss after 6853648 batches: 0.0958
trigger times: 1
Loss after 6854611 batches: 0.0704
trigger times: 2
Loss after 6855574 batches: 0.0671
trigger times: 3
Loss after 6856537 batches: 0.0622
trigger times: 4
Loss after 6857500 batches: 0.0575
trigger times: 0
Loss after 6858463 batches: 0.0557
trigger times: 1
Loss after 6859426 batches: 0.0532
trigger times: 2
Loss after 6860389 batches: 0.0535
trigger times: 3
Loss after 6861352 batches: 0.0514
trigger times: 0
Loss after 6862315 batches: 0.0518
trigger times: 1
Loss after 6863278 batches: 0.0541
trigger times: 2
Loss after 6864241 batches: 0.0523
trigger times: 3
Loss after 6865204 batches: 0.0504
trigger times: 4
Loss after 6866167 batches: 0.0489
trigger times: 5
Loss after 6867130 batches: 0.0483
trigger times: 6
Loss after 6868093 batches: 0.0481
trigger times: 7
Loss after 6869056 batches: 0.0477
trigger times: 8
Loss after 6870019 batches: 0.0478
trigger times: 9
Loss after 6870982 batches: 0.0472
trigger times: 0
Loss after 6871945 batches: 0.0468
trigger times: 1
Loss after 6872908 batches: 0.0470
trigger times: 2
Loss after 6873871 batches: 0.0456
trigger times: 3
Loss after 6874834 batches: 0.0444
trigger times: 4
Loss after 6875797 batches: 0.0451
trigger times: 5
Loss after 6876760 batches: 0.0449
trigger times: 6
Loss after 6877723 batches: 0.0435
trigger times: 7
Loss after 6878686 batches: 0.0443
trigger times: 8
Loss after 6879649 batches: 0.0425
trigger times: 9
Loss after 6880612 batches: 0.0425
trigger times: 10
Loss after 6881575 batches: 0.0437
trigger times: 11
Loss after 6882538 batches: 0.0429
trigger times: 12
Loss after 6883501 batches: 0.0429
trigger times: 13
Loss after 6884464 batches: 0.0430
trigger times: 0
Loss after 6885427 batches: 0.0422
trigger times: 1
Loss after 6886390 batches: 0.0414
trigger times: 2
Loss after 6887353 batches: 0.0421
trigger times: 3
Loss after 6888316 batches: 0.0421
trigger times: 4
Loss after 6889279 batches: 0.0419
trigger times: 5
Loss after 6890242 batches: 0.0403
trigger times: 6
Loss after 6891205 batches: 0.0405
trigger times: 7
Loss after 6892168 batches: 0.0411
trigger times: 8
Loss after 6893131 batches: 0.0411
trigger times: 9
Loss after 6894094 batches: 0.0410
trigger times: 10
Loss after 6895057 batches: 0.0403
trigger times: 11
Loss after 6896020 batches: 0.0397
trigger times: 12
Loss after 6896983 batches: 0.0408
trigger times: 13
Loss after 6897946 batches: 0.0395
trigger times: 14
Loss after 6898909 batches: 0.0395
trigger times: 15
Loss after 6899872 batches: 0.0408
trigger times: 16
Loss after 6900835 batches: 0.0407
trigger times: 17
Loss after 6901798 batches: 0.0399
trigger times: 18
Loss after 6902761 batches: 0.0396
trigger times: 19
Loss after 6903724 batches: 0.0383
trigger times: 20
Loss after 6904687 batches: 0.0387
trigger times: 21
Loss after 6905650 batches: 0.0376
trigger times: 22
Loss after 6906613 batches: 0.0388
trigger times: 23
Loss after 6907576 batches: 0.0392
trigger times: 24
Loss after 6908539 batches: 0.0376
trigger times: 25
Early stopping!
Start to test process.
Loss after 6909502 batches: 0.0378
Time to train on one home:  77.39194130897522
trigger times: 0
Loss after 6910465 batches: 0.0661
trigger times: 0
Loss after 6911428 batches: 0.0531
trigger times: 1
Loss after 6912391 batches: 0.0500
trigger times: 2
Loss after 6913354 batches: 0.0451
trigger times: 3
Loss after 6914317 batches: 0.0437
trigger times: 4
Loss after 6915280 batches: 0.0421
trigger times: 5
Loss after 6916243 batches: 0.0397
trigger times: 6
Loss after 6917206 batches: 0.0402
trigger times: 7
Loss after 6918169 batches: 0.0406
trigger times: 8
Loss after 6919132 batches: 0.0393
trigger times: 9
Loss after 6920095 batches: 0.0385
trigger times: 10
Loss after 6921058 batches: 0.0376
trigger times: 11
Loss after 6922021 batches: 0.0372
trigger times: 12
Loss after 6922984 batches: 0.0377
trigger times: 13
Loss after 6923947 batches: 0.0369
trigger times: 14
Loss after 6924910 batches: 0.0372
trigger times: 15
Loss after 6925873 batches: 0.0367
trigger times: 16
Loss after 6926836 batches: 0.0364
trigger times: 17
Loss after 6927799 batches: 0.0368
trigger times: 18
Loss after 6928762 batches: 0.0356
trigger times: 19
Loss after 6929725 batches: 0.0350
trigger times: 20
Loss after 6930688 batches: 0.0350
trigger times: 21
Loss after 6931651 batches: 0.0362
trigger times: 22
Loss after 6932614 batches: 0.0361
trigger times: 23
Loss after 6933577 batches: 0.0353
trigger times: 24
Loss after 6934540 batches: 0.0349
trigger times: 25
Early stopping!
Start to test process.
Loss after 6935503 batches: 0.0351
Time to train on one home:  53.16210150718689
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 6936466 batches: 0.1143
trigger times: 1
Loss after 6937429 batches: 0.0792
trigger times: 2
Loss after 6938392 batches: 0.0787
trigger times: 3
Loss after 6939355 batches: 0.0732
trigger times: 0
Loss after 6940318 batches: 0.0648
trigger times: 0
Loss after 6941281 batches: 0.0598
trigger times: 0
Loss after 6942244 batches: 0.0548
trigger times: 1
Loss after 6943207 batches: 0.0507
trigger times: 2
Loss after 6944170 batches: 0.0491
trigger times: 0
Loss after 6945133 batches: 0.0466
trigger times: 0
Loss after 6946096 batches: 0.0443
trigger times: 1
Loss after 6947059 batches: 0.0432
trigger times: 0
Loss after 6948022 batches: 0.0429
trigger times: 1
Loss after 6948985 batches: 0.0417
trigger times: 2
Loss after 6949948 batches: 0.0405
trigger times: 0
Loss after 6950911 batches: 0.0395
trigger times: 1
Loss after 6951874 batches: 0.0399
trigger times: 2
Loss after 6952837 batches: 0.0392
trigger times: 3
Loss after 6953800 batches: 0.0376
trigger times: 4
Loss after 6954763 batches: 0.0370
trigger times: 5
Loss after 6955726 batches: 0.0372
trigger times: 6
Loss after 6956689 batches: 0.0365
trigger times: 7
Loss after 6957652 batches: 0.0353
trigger times: 8
Loss after 6958615 batches: 0.0354
trigger times: 9
Loss after 6959578 batches: 0.0367
trigger times: 10
Loss after 6960541 batches: 0.0355
trigger times: 0
Loss after 6961504 batches: 0.0356
trigger times: 1
Loss after 6962467 batches: 0.0339
trigger times: 2
Loss after 6963430 batches: 0.0340
trigger times: 3
Loss after 6964393 batches: 0.0338
trigger times: 0
Loss after 6965356 batches: 0.0340
trigger times: 1
Loss after 6966319 batches: 0.0331
trigger times: 0
Loss after 6967282 batches: 0.0334
trigger times: 1
Loss after 6968245 batches: 0.0327
trigger times: 2
Loss after 6969208 batches: 0.0333
trigger times: 3
Loss after 6970171 batches: 0.0329
trigger times: 4
Loss after 6971134 batches: 0.0329
trigger times: 5
Loss after 6972097 batches: 0.0336
trigger times: 6
Loss after 6973060 batches: 0.0313
trigger times: 0
Loss after 6974023 batches: 0.0324
trigger times: 1
Loss after 6974986 batches: 0.0318
trigger times: 2
Loss after 6975949 batches: 0.0301
trigger times: 3
Loss after 6976912 batches: 0.0307
trigger times: 4
Loss after 6977875 batches: 0.0299
trigger times: 5
Loss after 6978838 batches: 0.0301
trigger times: 6
Loss after 6979801 batches: 0.0311
trigger times: 7
Loss after 6980764 batches: 0.0301
trigger times: 8
Loss after 6981727 batches: 0.0291
trigger times: 9
Loss after 6982690 batches: 0.0293
trigger times: 10
Loss after 6983653 batches: 0.0293
trigger times: 11
Loss after 6984616 batches: 0.0284
trigger times: 12
Loss after 6985579 batches: 0.0300
trigger times: 13
Loss after 6986542 batches: 0.0282
trigger times: 14
Loss after 6987505 batches: 0.0281
trigger times: 15
Loss after 6988468 batches: 0.0286
trigger times: 16
Loss after 6989431 batches: 0.0289
trigger times: 17
Loss after 6990394 batches: 0.0291
trigger times: 18
Loss after 6991357 batches: 0.0300
trigger times: 19
Loss after 6992320 batches: 0.0284
trigger times: 20
Loss after 6993283 batches: 0.0276
trigger times: 21
Loss after 6994246 batches: 0.0277
trigger times: 0
Loss after 6995209 batches: 0.0294
trigger times: 1
Loss after 6996172 batches: 0.0297
trigger times: 2
Loss after 6997135 batches: 0.0286
trigger times: 3
Loss after 6998098 batches: 0.0279
trigger times: 4
Loss after 6999061 batches: 0.0274
trigger times: 5
Loss after 7000024 batches: 0.0277
trigger times: 6
Loss after 7000987 batches: 0.0278
trigger times: 7
Loss after 7001950 batches: 0.0271
trigger times: 8
Loss after 7002913 batches: 0.0274
trigger times: 9
Loss after 7003876 batches: 0.0269
trigger times: 10
Loss after 7004839 batches: 0.0263
trigger times: 0
Loss after 7005802 batches: 0.0267
trigger times: 1
Loss after 7006765 batches: 0.0273
trigger times: 2
Loss after 7007728 batches: 0.0273
trigger times: 3
Loss after 7008691 batches: 0.0256
trigger times: 4
Loss after 7009654 batches: 0.0258
trigger times: 5
Loss after 7010617 batches: 0.0252
trigger times: 6
Loss after 7011580 batches: 0.0260
trigger times: 7
Loss after 7012543 batches: 0.0253
trigger times: 8
Loss after 7013506 batches: 0.0248
trigger times: 9
Loss after 7014469 batches: 0.0254
trigger times: 10
Loss after 7015432 batches: 0.0254
trigger times: 11
Loss after 7016395 batches: 0.0252
trigger times: 12
Loss after 7017358 batches: 0.0240
trigger times: 13
Loss after 7018321 batches: 0.0240
trigger times: 14
Loss after 7019284 batches: 0.0237
trigger times: 15
Loss after 7020247 batches: 0.0237
trigger times: 16
Loss after 7021210 batches: 0.0251
trigger times: 17
Loss after 7022173 batches: 0.0248
trigger times: 18
Loss after 7023136 batches: 0.0246
trigger times: 19
Loss after 7024099 batches: 0.0260
trigger times: 20
Loss after 7025062 batches: 0.0240
trigger times: 21
Loss after 7026025 batches: 0.0245
trigger times: 22
Loss after 7026988 batches: 0.0238
trigger times: 23
Loss after 7027951 batches: 0.0231
trigger times: 24
Loss after 7028914 batches: 0.0236
trigger times: 25
Early stopping!
Start to test process.
Loss after 7029877 batches: 0.0250
Time to train on one home:  106.60300779342651
trigger times: 0
Loss after 7030840 batches: 0.0857
trigger times: 1
Loss after 7031803 batches: 0.0792
trigger times: 2
Loss after 7032766 batches: 0.0782
trigger times: 3
Loss after 7033729 batches: 0.0771
trigger times: 4
Loss after 7034692 batches: 0.0756
trigger times: 5
Loss after 7035655 batches: 0.0747
trigger times: 6
Loss after 7036618 batches: 0.0729
trigger times: 7
Loss after 7037581 batches: 0.0712
trigger times: 8
Loss after 7038544 batches: 0.0688
trigger times: 9
Loss after 7039507 batches: 0.0692
trigger times: 10
Loss after 7040470 batches: 0.0690
trigger times: 11
Loss after 7041433 batches: 0.0688
trigger times: 12
Loss after 7042396 batches: 0.0685
trigger times: 13
Loss after 7043359 batches: 0.0691
trigger times: 14
Loss after 7044322 batches: 0.0674
trigger times: 15
Loss after 7045285 batches: 0.0665
trigger times: 16
Loss after 7046248 batches: 0.0669
trigger times: 17
Loss after 7047211 batches: 0.0658
trigger times: 18
Loss after 7048174 batches: 0.0648
trigger times: 19
Loss after 7049137 batches: 0.0636
trigger times: 20
Loss after 7050100 batches: 0.0646
trigger times: 21
Loss after 7051063 batches: 0.0641
trigger times: 22
Loss after 7052026 batches: 0.0647
trigger times: 23
Loss after 7052989 batches: 0.0637
trigger times: 24
Loss after 7053952 batches: 0.0629
trigger times: 25
Early stopping!
Start to test process.
Loss after 7054915 batches: 0.0633
Time to train on one home:  52.35225200653076
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7055878 batches: 0.1132
trigger times: 1
Loss after 7056841 batches: 0.0804
trigger times: 2
Loss after 7057804 batches: 0.0788
trigger times: 3
Loss after 7058767 batches: 0.0729
trigger times: 0
Loss after 7059730 batches: 0.0654
trigger times: 0
Loss after 7060693 batches: 0.0619
trigger times: 0
Loss after 7061656 batches: 0.0562
trigger times: 0
Loss after 7062619 batches: 0.0513
trigger times: 0
Loss after 7063582 batches: 0.0493
trigger times: 1
Loss after 7064545 batches: 0.0472
trigger times: 0
Loss after 7065508 batches: 0.0450
trigger times: 1
Loss after 7066471 batches: 0.0449
trigger times: 2
Loss after 7067434 batches: 0.0436
trigger times: 0
Loss after 7068397 batches: 0.0432
trigger times: 1
Loss after 7069360 batches: 0.0412
trigger times: 0
Loss after 7070323 batches: 0.0409
trigger times: 1
Loss after 7071286 batches: 0.0399
trigger times: 2
Loss after 7072249 batches: 0.0391
trigger times: 0
Loss after 7073212 batches: 0.0380
trigger times: 0
Loss after 7074175 batches: 0.0379
trigger times: 0
Loss after 7075138 batches: 0.0379
trigger times: 1
Loss after 7076101 batches: 0.0369
trigger times: 0
Loss after 7077064 batches: 0.0362
trigger times: 1
Loss after 7078027 batches: 0.0369
trigger times: 2
Loss after 7078990 batches: 0.0358
trigger times: 3
Loss after 7079953 batches: 0.0347
trigger times: 0
Loss after 7080916 batches: 0.0354
trigger times: 1
Loss after 7081879 batches: 0.0333
trigger times: 2
Loss after 7082842 batches: 0.0340
trigger times: 3
Loss after 7083805 batches: 0.0333
trigger times: 0
Loss after 7084768 batches: 0.0336
trigger times: 1
Loss after 7085731 batches: 0.0326
trigger times: 2
Loss after 7086694 batches: 0.0324
trigger times: 3
Loss after 7087657 batches: 0.0327
trigger times: 4
Loss after 7088620 batches: 0.0328
trigger times: 5
Loss after 7089583 batches: 0.0332
trigger times: 6
Loss after 7090546 batches: 0.0324
trigger times: 7
Loss after 7091509 batches: 0.0312
trigger times: 8
Loss after 7092472 batches: 0.0323
trigger times: 9
Loss after 7093435 batches: 0.0316
trigger times: 10
Loss after 7094398 batches: 0.0318
trigger times: 11
Loss after 7095361 batches: 0.0311
trigger times: 12
Loss after 7096324 batches: 0.0310
trigger times: 13
Loss after 7097287 batches: 0.0306
trigger times: 14
Loss after 7098250 batches: 0.0310
trigger times: 15
Loss after 7099213 batches: 0.0311
trigger times: 16
Loss after 7100176 batches: 0.0307
trigger times: 17
Loss after 7101139 batches: 0.0301
trigger times: 18
Loss after 7102102 batches: 0.0307
trigger times: 19
Loss after 7103065 batches: 0.0301
trigger times: 20
Loss after 7104028 batches: 0.0303
trigger times: 21
Loss after 7104991 batches: 0.0287
trigger times: 0
Loss after 7105954 batches: 0.0288
trigger times: 1
Loss after 7106917 batches: 0.0296
trigger times: 2
Loss after 7107880 batches: 0.0297
trigger times: 3
Loss after 7108843 batches: 0.0292
trigger times: 4
Loss after 7109806 batches: 0.0284
trigger times: 0
Loss after 7110769 batches: 0.0291
trigger times: 0
Loss after 7111732 batches: 0.0293
trigger times: 1
Loss after 7112695 batches: 0.0285
trigger times: 2
Loss after 7113658 batches: 0.0283
trigger times: 3
Loss after 7114621 batches: 0.0286
trigger times: 4
Loss after 7115584 batches: 0.0267
trigger times: 5
Loss after 7116547 batches: 0.0278
trigger times: 6
Loss after 7117510 batches: 0.0268
trigger times: 7
Loss after 7118473 batches: 0.0275
trigger times: 0
Loss after 7119436 batches: 0.0273
trigger times: 1
Loss after 7120399 batches: 0.0282
trigger times: 2
Loss after 7121362 batches: 0.0284
trigger times: 3
Loss after 7122325 batches: 0.0266
trigger times: 4
Loss after 7123288 batches: 0.0273
trigger times: 5
Loss after 7124251 batches: 0.0272
trigger times: 6
Loss after 7125214 batches: 0.0264
trigger times: 7
Loss after 7126177 batches: 0.0257
trigger times: 8
Loss after 7127140 batches: 0.0267
trigger times: 9
Loss after 7128103 batches: 0.0271
trigger times: 10
Loss after 7129066 batches: 0.0264
trigger times: 11
Loss after 7130029 batches: 0.0259
trigger times: 0
Loss after 7130992 batches: 0.0259
trigger times: 1
Loss after 7131955 batches: 0.0264
trigger times: 0
Loss after 7132918 batches: 0.0261
trigger times: 1
Loss after 7133881 batches: 0.0250
trigger times: 2
Loss after 7134844 batches: 0.0286
trigger times: 3
Loss after 7135807 batches: 0.0266
trigger times: 4
Loss after 7136770 batches: 0.0269
trigger times: 5
Loss after 7137733 batches: 0.0265
trigger times: 6
Loss after 7138696 batches: 0.0246
trigger times: 7
Loss after 7139659 batches: 0.0252
trigger times: 8
Loss after 7140622 batches: 0.0244
trigger times: 9
Loss after 7141585 batches: 0.0247
trigger times: 10
Loss after 7142548 batches: 0.0241
trigger times: 11
Loss after 7143511 batches: 0.0235
trigger times: 12
Loss after 7144474 batches: 0.0261
trigger times: 13
Loss after 7145437 batches: 0.0265
trigger times: 14
Loss after 7146400 batches: 0.0252
trigger times: 15
Loss after 7147363 batches: 0.0247
trigger times: 16
Loss after 7148326 batches: 0.0237
trigger times: 0
Loss after 7149289 batches: 0.0234
trigger times: 1
Loss after 7150252 batches: 0.0227
trigger times: 2
Loss after 7151215 batches: 0.0228
trigger times: 3
Loss after 7152178 batches: 0.0234
trigger times: 4
Loss after 7153141 batches: 0.0236
trigger times: 5
Loss after 7154104 batches: 0.0241
trigger times: 6
Loss after 7155067 batches: 0.0236
trigger times: 7
Loss after 7156030 batches: 0.0236
trigger times: 8
Loss after 7156993 batches: 0.0234
trigger times: 9
Loss after 7157956 batches: 0.0225
trigger times: 10
Loss after 7158919 batches: 0.0230
trigger times: 11
Loss after 7159882 batches: 0.0217
trigger times: 12
Loss after 7160845 batches: 0.0224
trigger times: 13
Loss after 7161808 batches: 0.0218
trigger times: 0
Loss after 7162771 batches: 0.0227
trigger times: 1
Loss after 7163734 batches: 0.0221
trigger times: 2
Loss after 7164697 batches: 0.0214
trigger times: 3
Loss after 7165660 batches: 0.0213
trigger times: 4
Loss after 7166623 batches: 0.0209
trigger times: 5
Loss after 7167586 batches: 0.0212
trigger times: 6
Loss after 7168549 batches: 0.0213
trigger times: 7
Loss after 7169512 batches: 0.0218
trigger times: 0
Loss after 7170475 batches: 0.0209
trigger times: 0
Loss after 7171438 batches: 0.0218
trigger times: 1
Loss after 7172401 batches: 0.0205
trigger times: 2
Loss after 7173364 batches: 0.0214
trigger times: 3
Loss after 7174327 batches: 0.0207
trigger times: 4
Loss after 7175290 batches: 0.0201
trigger times: 5
Loss after 7176253 batches: 0.0200
trigger times: 6
Loss after 7177216 batches: 0.0206
trigger times: 7
Loss after 7178179 batches: 0.0197
trigger times: 8
Loss after 7179142 batches: 0.0202
trigger times: 9
Loss after 7180105 batches: 0.0195
trigger times: 10
Loss after 7181068 batches: 0.0192
trigger times: 11
Loss after 7182031 batches: 0.0193
trigger times: 0
Loss after 7182994 batches: 0.0196
trigger times: 1
Loss after 7183957 batches: 0.0186
trigger times: 2
Loss after 7184920 batches: 0.0195
trigger times: 3
Loss after 7185883 batches: 0.0190
trigger times: 4
Loss after 7186846 batches: 0.0192
trigger times: 5
Loss after 7187809 batches: 0.0196
trigger times: 6
Loss after 7188772 batches: 0.0196
trigger times: 7
Loss after 7189735 batches: 0.0186
trigger times: 8
Loss after 7190698 batches: 0.0190
trigger times: 9
Loss after 7191661 batches: 0.0189
trigger times: 10
Loss after 7192624 batches: 0.0222
trigger times: 11
Loss after 7193587 batches: 0.0210
trigger times: 12
Loss after 7194550 batches: 0.0209
trigger times: 13
Loss after 7195513 batches: 0.0197
trigger times: 14
Loss after 7196476 batches: 0.0192
trigger times: 15
Loss after 7197439 batches: 0.0194
trigger times: 0
Loss after 7198402 batches: 0.0185
trigger times: 1
Loss after 7199365 batches: 0.0174
trigger times: 2
Loss after 7200328 batches: 0.0176
trigger times: 3
Loss after 7201291 batches: 0.0191
trigger times: 4
Loss after 7202254 batches: 0.0179
trigger times: 5
Loss after 7203217 batches: 0.0185
trigger times: 6
Loss after 7204180 batches: 0.0184
trigger times: 7
Loss after 7205143 batches: 0.0187
trigger times: 8
Loss after 7206106 batches: 0.0185
trigger times: 9
Loss after 7207069 batches: 0.0175
trigger times: 10
Loss after 7208032 batches: 0.0177
trigger times: 11
Loss after 7208995 batches: 0.0173
trigger times: 12
Loss after 7209958 batches: 0.0174
trigger times: 13
Loss after 7210921 batches: 0.0173
trigger times: 14
Loss after 7211884 batches: 0.0182
trigger times: 15
Loss after 7212847 batches: 0.0188
trigger times: 16
Loss after 7213810 batches: 0.0186
trigger times: 17
Loss after 7214773 batches: 0.0178
trigger times: 18
Loss after 7215736 batches: 0.0182
trigger times: 19
Loss after 7216699 batches: 0.0193
trigger times: 20
Loss after 7217662 batches: 0.0193
trigger times: 21
Loss after 7218625 batches: 0.0188
trigger times: 22
Loss after 7219588 batches: 0.0193
trigger times: 23
Loss after 7220551 batches: 0.0174
trigger times: 0
Loss after 7221514 batches: 0.0168
trigger times: 1
Loss after 7222477 batches: 0.0175
trigger times: 2
Loss after 7223440 batches: 0.0174
trigger times: 3
Loss after 7224403 batches: 0.0167
trigger times: 4
Loss after 7225366 batches: 0.0166
trigger times: 5
Loss after 7226329 batches: 0.0159
trigger times: 6
Loss after 7227292 batches: 0.0160
trigger times: 7
Loss after 7228255 batches: 0.0170
trigger times: 8
Loss after 7229218 batches: 0.0172
trigger times: 9
Loss after 7230181 batches: 0.0182
trigger times: 10
Loss after 7231144 batches: 0.0170
trigger times: 11
Loss after 7232107 batches: 0.0156
trigger times: 12
Loss after 7233070 batches: 0.0161
trigger times: 13
Loss after 7234033 batches: 0.0153
trigger times: 14
Loss after 7234996 batches: 0.0152
trigger times: 15
Loss after 7235959 batches: 0.0153
trigger times: 16
Loss after 7236922 batches: 0.0158
trigger times: 17
Loss after 7237885 batches: 0.0154
trigger times: 18
Loss after 7238848 batches: 0.0156
trigger times: 19
Loss after 7239811 batches: 0.0163
trigger times: 20
Loss after 7240774 batches: 0.0167
trigger times: 21
Loss after 7241737 batches: 0.0160
trigger times: 22
Loss after 7242700 batches: 0.0159
trigger times: 23
Loss after 7243663 batches: 0.0158
trigger times: 24
Loss after 7244626 batches: 0.0153
trigger times: 25
Early stopping!
Start to test process.
Loss after 7245589 batches: 0.0153
Time to train on one home:  183.01480054855347
trigger times: 0
Loss after 7246484 batches: 0.0700
trigger times: 1
Loss after 7247379 batches: 0.0359
trigger times: 2
Loss after 7248274 batches: 0.0186
trigger times: 3
Loss after 7249169 batches: 0.0124
trigger times: 4
Loss after 7250064 batches: 0.0089
trigger times: 5
Loss after 7250959 batches: 0.0075
trigger times: 6
Loss after 7251854 batches: 0.0066
trigger times: 7
Loss after 7252749 batches: 0.0064
trigger times: 8
Loss after 7253644 batches: 0.0066
trigger times: 9
Loss after 7254539 batches: 0.0078
trigger times: 10
Loss after 7255434 batches: 0.0085
trigger times: 11
Loss after 7256329 batches: 0.0065
trigger times: 12
Loss after 7257224 batches: 0.0055
trigger times: 13
Loss after 7258119 batches: 0.0055
trigger times: 14
Loss after 7259014 batches: 0.0050
trigger times: 15
Loss after 7259909 batches: 0.0047
trigger times: 16
Loss after 7260804 batches: 0.0043
trigger times: 17
Loss after 7261699 batches: 0.0040
trigger times: 18
Loss after 7262594 batches: 0.0043
trigger times: 19
Loss after 7263489 batches: 0.0046
trigger times: 20
Loss after 7264384 batches: 0.0046
trigger times: 21
Loss after 7265279 batches: 0.0045
trigger times: 22
Loss after 7266174 batches: 0.0041
trigger times: 23
Loss after 7267069 batches: 0.0039
trigger times: 24
Loss after 7267964 batches: 0.0041
trigger times: 25
Early stopping!
Start to test process.
Loss after 7268859 batches: 0.0040
Time to train on one home:  50.82964253425598
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7269796 batches: 0.0840
trigger times: 1
Loss after 7270733 batches: 0.0749
trigger times: 2
Loss after 7271670 batches: 0.0740
trigger times: 0
Loss after 7272607 batches: 0.0698
trigger times: 1
Loss after 7273544 batches: 0.0680
trigger times: 2
Loss after 7274481 batches: 0.0657
trigger times: 3
Loss after 7275418 batches: 0.0641
trigger times: 4
Loss after 7276355 batches: 0.0630
trigger times: 5
Loss after 7277292 batches: 0.0631
trigger times: 6
Loss after 7278229 batches: 0.0631
trigger times: 7
Loss after 7279166 batches: 0.0638
trigger times: 8
Loss after 7280103 batches: 0.0619
trigger times: 9
Loss after 7281040 batches: 0.0605
trigger times: 10
Loss after 7281977 batches: 0.0595
trigger times: 11
Loss after 7282914 batches: 0.0583
trigger times: 12
Loss after 7283851 batches: 0.0579
trigger times: 13
Loss after 7284788 batches: 0.0585
trigger times: 14
Loss after 7285725 batches: 0.0578
trigger times: 15
Loss after 7286662 batches: 0.0564
trigger times: 16
Loss after 7287599 batches: 0.0565
trigger times: 17
Loss after 7288536 batches: 0.0569
trigger times: 18
Loss after 7289473 batches: 0.0561
trigger times: 19
Loss after 7290410 batches: 0.0575
trigger times: 20
Loss after 7291347 batches: 0.0559
trigger times: 21
Loss after 7292284 batches: 0.0549
trigger times: 22
Loss after 7293221 batches: 0.0549
trigger times: 23
Loss after 7294158 batches: 0.0549
trigger times: 24
Loss after 7295095 batches: 0.0545
trigger times: 25
Early stopping!
Start to test process.
Loss after 7296032 batches: 0.0540
Time to train on one home:  54.112619161605835
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887]]
Round_4_results:  [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887]
trigger times: 0
Loss after 7296994 batches: 0.0790
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 7609 < 7610; dropping {'Training_Loss': 0.0790489816239902, 'Validation_Loss': 0.08777032047510147, 'Training_R2': -0.2070627111536194, 'Validation_R2': 0.11121237908759174, 'Training_F1': 0.3372032982651931, 'Validation_F1': 0.5663398561666377, 'Training_NEP': 0.8805346715817307, 'Validation_NEP': 0.8131502581466822, 'Training_NDE': 0.7351304627512985, 'Validation_NDE': 0.8029985111079239, 'Training_MAE': 33.460600823225946, 'Validation_MAE': 28.915032612542, 'Training_MSE': 2715.1208, 'Validation_MSE': 10519.29}.
trigger times: 1
Loss after 7297956 batches: 0.0689
trigger times: 2
Loss after 7298918 batches: 0.0661
trigger times: 3
Loss after 7299880 batches: 0.0633
trigger times: 4
Loss after 7300842 batches: 0.0608
trigger times: 5
Loss after 7301804 batches: 0.0597
trigger times: 6
Loss after 7302766 batches: 0.0593
trigger times: 7
Loss after 7303728 batches: 0.0584
trigger times: 8
Loss after 7304690 batches: 0.0583
trigger times: 9
Loss after 7305652 batches: 0.0582
trigger times: 10
Loss after 7306614 batches: 0.0564
trigger times: 11
Loss after 7307576 batches: 0.0562
trigger times: 12
Loss after 7308538 batches: 0.0560
trigger times: 13
Loss after 7309500 batches: 0.0569
trigger times: 14
Loss after 7310462 batches: 0.0557
trigger times: 15
Loss after 7311424 batches: 0.0559
trigger times: 16
Loss after 7312386 batches: 0.0553
trigger times: 17
Loss after 7313348 batches: 0.0547
trigger times: 18
Loss after 7314310 batches: 0.0543
trigger times: 19
Loss after 7315272 batches: 0.0549
trigger times: 20
Loss after 7316234 batches: 0.0546
trigger times: 21
Loss after 7317196 batches: 0.0541
trigger times: 22
Loss after 7318158 batches: 0.0537
trigger times: 23
Loss after 7319120 batches: 0.0539
trigger times: 24
Loss after 7320082 batches: 0.0529
trigger times: 25
Early stopping!
Start to test process.
Loss after 7321044 batches: 0.0528
Time to train on one home:  51.96731448173523
trigger times: 0
Loss after 7321973 batches: 0.0815
trigger times: 1
Loss after 7322902 batches: 0.0632
trigger times: 2
Loss after 7323831 batches: 0.0521
trigger times: 3
Loss after 7324760 batches: 0.0457
trigger times: 4
Loss after 7325689 batches: 0.0417
trigger times: 5
Loss after 7326618 batches: 0.0380
trigger times: 6
Loss after 7327547 batches: 0.0380
trigger times: 7
Loss after 7328476 batches: 0.0371
trigger times: 8
Loss after 7329405 batches: 0.0372
trigger times: 9
Loss after 7330334 batches: 0.0386
trigger times: 10
Loss after 7331263 batches: 0.0351
trigger times: 11
Loss after 7332192 batches: 0.0315
trigger times: 12
Loss after 7333121 batches: 0.0347
trigger times: 13
Loss after 7334050 batches: 0.0350
trigger times: 14
Loss after 7334979 batches: 0.0425
trigger times: 15
Loss after 7335908 batches: 0.0464
trigger times: 16
Loss after 7336837 batches: 0.0451
trigger times: 17
Loss after 7337766 batches: 0.0398
trigger times: 18
Loss after 7338695 batches: 0.0393
trigger times: 19
Loss after 7339624 batches: 0.0365
trigger times: 20
Loss after 7340553 batches: 0.0381
trigger times: 21
Loss after 7341482 batches: 0.0380
trigger times: 22
Loss after 7342411 batches: 0.0343
trigger times: 23
Loss after 7343340 batches: 0.0326
trigger times: 24
Loss after 7344269 batches: 0.0330
trigger times: 25
Early stopping!
Start to test process.
Loss after 7345198 batches: 0.0315
Time to train on one home:  51.88930344581604
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 7346161 batches: 0.0579
trigger times: 1
Loss after 7347124 batches: 0.0164
trigger times: 2
Loss after 7348087 batches: 0.0144
trigger times: 3
Loss after 7349050 batches: 0.0141
trigger times: 4
Loss after 7350013 batches: 0.0143
trigger times: 5
Loss after 7350976 batches: 0.0142
trigger times: 6
Loss after 7351939 batches: 0.0140
trigger times: 7
Loss after 7352902 batches: 0.0139
trigger times: 8
Loss after 7353865 batches: 0.0137
trigger times: 9
Loss after 7354828 batches: 0.0137
trigger times: 10
Loss after 7355791 batches: 0.0138
trigger times: 11
Loss after 7356754 batches: 0.0134
trigger times: 12
Loss after 7357717 batches: 0.0132
trigger times: 13
Loss after 7358680 batches: 0.0130
trigger times: 14
Loss after 7359643 batches: 0.0128
trigger times: 15
Loss after 7360606 batches: 0.0123
trigger times: 16
Loss after 7361569 batches: 0.0121
trigger times: 17
Loss after 7362532 batches: 0.0118
trigger times: 18
Loss after 7363495 batches: 0.0116
trigger times: 19
Loss after 7364458 batches: 0.0110
trigger times: 20
Loss after 7365421 batches: 0.0108
trigger times: 21
Loss after 7366384 batches: 0.0105
trigger times: 22
Loss after 7367347 batches: 0.0099
trigger times: 23
Loss after 7368310 batches: 0.0095
trigger times: 24
Loss after 7369273 batches: 0.0095
trigger times: 25
Early stopping!
Start to test process.
Loss after 7370236 batches: 0.0092
Time to train on one home:  51.935287952423096
trigger times: 0
Loss after 7371199 batches: 0.0292
trigger times: 0
Loss after 7372162 batches: 0.0255
trigger times: 1
Loss after 7373125 batches: 0.0224
trigger times: 2
Loss after 7374088 batches: 0.0218
trigger times: 0
Loss after 7375051 batches: 0.0203
trigger times: 1
Loss after 7376014 batches: 0.0192
trigger times: 2
Loss after 7376977 batches: 0.0183
trigger times: 0
Loss after 7377940 batches: 0.0183
trigger times: 1
Loss after 7378903 batches: 0.0184
trigger times: 2
Loss after 7379866 batches: 0.0177
trigger times: 3
Loss after 7380829 batches: 0.0174
trigger times: 4
Loss after 7381792 batches: 0.0170
trigger times: 5
Loss after 7382755 batches: 0.0165
trigger times: 6
Loss after 7383718 batches: 0.0165
trigger times: 7
Loss after 7384681 batches: 0.0162
trigger times: 8
Loss after 7385644 batches: 0.0162
trigger times: 0
Loss after 7386607 batches: 0.0164
trigger times: 1
Loss after 7387570 batches: 0.0161
trigger times: 2
Loss after 7388533 batches: 0.0162
trigger times: 3
Loss after 7389496 batches: 0.0159
trigger times: 4
Loss after 7390459 batches: 0.0154
trigger times: 5
Loss after 7391422 batches: 0.0159
trigger times: 6
Loss after 7392385 batches: 0.0155
trigger times: 7
Loss after 7393348 batches: 0.0150
trigger times: 8
Loss after 7394311 batches: 0.0149
trigger times: 9
Loss after 7395274 batches: 0.0152
trigger times: 10
Loss after 7396237 batches: 0.0145
trigger times: 11
Loss after 7397200 batches: 0.0148
trigger times: 12
Loss after 7398163 batches: 0.0152
trigger times: 13
Loss after 7399126 batches: 0.0141
trigger times: 14
Loss after 7400089 batches: 0.0150
trigger times: 15
Loss after 7401052 batches: 0.0149
trigger times: 16
Loss after 7402015 batches: 0.0147
trigger times: 17
Loss after 7402978 batches: 0.0148
trigger times: 18
Loss after 7403941 batches: 0.0146
trigger times: 19
Loss after 7404904 batches: 0.0138
trigger times: 20
Loss after 7405867 batches: 0.0140
trigger times: 21
Loss after 7406830 batches: 0.0143
trigger times: 22
Loss after 7407793 batches: 0.0139
trigger times: 23
Loss after 7408756 batches: 0.0140
trigger times: 24
Loss after 7409719 batches: 0.0141
trigger times: 25
Early stopping!
Start to test process.
Loss after 7410682 batches: 0.0133
Time to train on one home:  63.95564270019531
trigger times: 0
Loss after 7411645 batches: 0.0940
trigger times: 1
Loss after 7412608 batches: 0.0903
trigger times: 2
Loss after 7413571 batches: 0.0869
trigger times: 3
Loss after 7414534 batches: 0.0843
trigger times: 4
Loss after 7415497 batches: 0.0813
trigger times: 5
Loss after 7416460 batches: 0.0798
trigger times: 6
Loss after 7417423 batches: 0.0788
trigger times: 7
Loss after 7418386 batches: 0.0778
trigger times: 8
Loss after 7419349 batches: 0.0773
trigger times: 9
Loss after 7420312 batches: 0.0760
trigger times: 10
Loss after 7421275 batches: 0.0756
trigger times: 11
Loss after 7422238 batches: 0.0746
trigger times: 12
Loss after 7423201 batches: 0.0767
trigger times: 13
Loss after 7424164 batches: 0.0738
trigger times: 14
Loss after 7425127 batches: 0.0732
trigger times: 15
Loss after 7426090 batches: 0.0742
trigger times: 16
Loss after 7427053 batches: 0.0723
trigger times: 17
Loss after 7428016 batches: 0.0723
trigger times: 18
Loss after 7428979 batches: 0.0718
trigger times: 19
Loss after 7429942 batches: 0.0698
trigger times: 20
Loss after 7430905 batches: 0.0691
trigger times: 21
Loss after 7431868 batches: 0.0690
trigger times: 22
Loss after 7432831 batches: 0.0701
trigger times: 23
Loss after 7433794 batches: 0.0680
trigger times: 24
Loss after 7434757 batches: 0.0673
trigger times: 25
Early stopping!
Start to test process.
Loss after 7435720 batches: 0.0678
Time to train on one home:  51.894290924072266
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7436683 batches: 0.0912
trigger times: 1
Loss after 7437646 batches: 0.0780
trigger times: 2
Loss after 7438609 batches: 0.0754
trigger times: 3
Loss after 7439572 batches: 0.0729
trigger times: 4
Loss after 7440535 batches: 0.0706
trigger times: 5
Loss after 7441498 batches: 0.0688
trigger times: 6
Loss after 7442461 batches: 0.0682
trigger times: 7
Loss after 7443424 batches: 0.0676
trigger times: 8
Loss after 7444387 batches: 0.0662
trigger times: 9
Loss after 7445350 batches: 0.0656
trigger times: 10
Loss after 7446313 batches: 0.0658
trigger times: 11
Loss after 7447276 batches: 0.0652
trigger times: 12
Loss after 7448239 batches: 0.0632
trigger times: 13
Loss after 7449202 batches: 0.0634
trigger times: 14
Loss after 7450165 batches: 0.0630
trigger times: 15
Loss after 7451128 batches: 0.0629
trigger times: 16
Loss after 7452091 batches: 0.0641
trigger times: 17
Loss after 7453054 batches: 0.0633
trigger times: 18
Loss after 7454017 batches: 0.0628
trigger times: 19
Loss after 7454980 batches: 0.0610
trigger times: 20
Loss after 7455943 batches: 0.0622
trigger times: 21
Loss after 7456906 batches: 0.0628
trigger times: 22
Loss after 7457869 batches: 0.0609
trigger times: 23
Loss after 7458832 batches: 0.0610
trigger times: 24
Loss after 7459795 batches: 0.0606
trigger times: 25
Early stopping!
Start to test process.
Loss after 7460758 batches: 0.0600
Time to train on one home:  51.87829899787903
trigger times: 0
Loss after 7461721 batches: 0.0762
trigger times: 1
Loss after 7462684 batches: 0.0720
trigger times: 2
Loss after 7463647 batches: 0.0693
trigger times: 3
Loss after 7464610 batches: 0.0683
trigger times: 4
Loss after 7465573 batches: 0.0660
trigger times: 5
Loss after 7466536 batches: 0.0643
trigger times: 6
Loss after 7467499 batches: 0.0634
trigger times: 7
Loss after 7468462 batches: 0.0617
trigger times: 8
Loss after 7469425 batches: 0.0616
trigger times: 9
Loss after 7470388 batches: 0.0606
trigger times: 10
Loss after 7471351 batches: 0.0608
trigger times: 11
Loss after 7472314 batches: 0.0602
trigger times: 12
Loss after 7473277 batches: 0.0601
trigger times: 13
Loss after 7474240 batches: 0.0601
trigger times: 14
Loss after 7475203 batches: 0.0594
trigger times: 15
Loss after 7476166 batches: 0.0593
trigger times: 16
Loss after 7477129 batches: 0.0586
trigger times: 17
Loss after 7478092 batches: 0.0589
trigger times: 18
Loss after 7479055 batches: 0.0595
trigger times: 19
Loss after 7480018 batches: 0.0580
trigger times: 20
Loss after 7480981 batches: 0.0569
trigger times: 21
Loss after 7481944 batches: 0.0572
trigger times: 22
Loss after 7482907 batches: 0.0557
trigger times: 23
Loss after 7483870 batches: 0.0558
trigger times: 24
Loss after 7484833 batches: 0.0554
trigger times: 25
Early stopping!
Start to test process.
Loss after 7485796 batches: 0.0551
Time to train on one home:  51.96626043319702
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7486759 batches: 0.0634
trigger times: 1
Loss after 7487722 batches: 0.0528
trigger times: 0
Loss after 7488685 batches: 0.0508
trigger times: 1
Loss after 7489648 batches: 0.0467
trigger times: 2
Loss after 7490611 batches: 0.0413
trigger times: 3
Loss after 7491574 batches: 0.0387
trigger times: 4
Loss after 7492537 batches: 0.0367
trigger times: 5
Loss after 7493500 batches: 0.0354
trigger times: 6
Loss after 7494463 batches: 0.0342
trigger times: 7
Loss after 7495426 batches: 0.0328
trigger times: 8
Loss after 7496389 batches: 0.0322
trigger times: 9
Loss after 7497352 batches: 0.0320
trigger times: 10
Loss after 7498315 batches: 0.0316
trigger times: 11
Loss after 7499278 batches: 0.0301
trigger times: 12
Loss after 7500241 batches: 0.0300
trigger times: 13
Loss after 7501204 batches: 0.0297
trigger times: 14
Loss after 7502167 batches: 0.0292
trigger times: 15
Loss after 7503130 batches: 0.0294
trigger times: 16
Loss after 7504093 batches: 0.0295
trigger times: 17
Loss after 7505056 batches: 0.0286
trigger times: 18
Loss after 7506019 batches: 0.0290
trigger times: 19
Loss after 7506982 batches: 0.0289
trigger times: 20
Loss after 7507945 batches: 0.0290
trigger times: 21
Loss after 7508908 batches: 0.0284
trigger times: 22
Loss after 7509871 batches: 0.0277
trigger times: 23
Loss after 7510834 batches: 0.0277
trigger times: 24
Loss after 7511797 batches: 0.0281
trigger times: 25
Early stopping!
Start to test process.
Loss after 7512760 batches: 0.0273
Time to train on one home:  53.7620849609375
trigger times: 0
Loss after 7513718 batches: 0.0666
trigger times: 1
Loss after 7514676 batches: 0.0465
trigger times: 2
Loss after 7515634 batches: 0.0421
trigger times: 3
Loss after 7516592 batches: 0.0357
trigger times: 4
Loss after 7517550 batches: 0.0329
trigger times: 5
Loss after 7518508 batches: 0.0313
trigger times: 6
Loss after 7519466 batches: 0.0291
trigger times: 7
Loss after 7520424 batches: 0.0281
trigger times: 8
Loss after 7521382 batches: 0.0283
trigger times: 9
Loss after 7522340 batches: 0.0273
trigger times: 10
Loss after 7523298 batches: 0.0269
trigger times: 11
Loss after 7524256 batches: 0.0282
trigger times: 12
Loss after 7525214 batches: 0.0274
trigger times: 13
Loss after 7526172 batches: 0.0262
trigger times: 14
Loss after 7527130 batches: 0.0255
trigger times: 15
Loss after 7528088 batches: 0.0237
trigger times: 16
Loss after 7529046 batches: 0.0231
trigger times: 17
Loss after 7530004 batches: 0.0237
trigger times: 18
Loss after 7530962 batches: 0.0230
trigger times: 19
Loss after 7531920 batches: 0.0218
trigger times: 20
Loss after 7532878 batches: 0.0220
trigger times: 21
Loss after 7533836 batches: 0.0216
trigger times: 22
Loss after 7534794 batches: 0.0219
trigger times: 23
Loss after 7535752 batches: 0.0214
trigger times: 24
Loss after 7536710 batches: 0.0218
trigger times: 25
Early stopping!
Start to test process.
Loss after 7537668 batches: 0.0219
Time to train on one home:  51.91740560531616
trigger times: 0
Loss after 7538630 batches: 0.0782
trigger times: 1
Loss after 7539592 batches: 0.0691
trigger times: 2
Loss after 7540554 batches: 0.0667
trigger times: 3
Loss after 7541516 batches: 0.0647
trigger times: 4
Loss after 7542478 batches: 0.0615
trigger times: 5
Loss after 7543440 batches: 0.0600
trigger times: 6
Loss after 7544402 batches: 0.0600
trigger times: 7
Loss after 7545364 batches: 0.0580
trigger times: 8
Loss after 7546326 batches: 0.0567
trigger times: 9
Loss after 7547288 batches: 0.0585
trigger times: 10
Loss after 7548250 batches: 0.0568
trigger times: 11
Loss after 7549212 batches: 0.0569
trigger times: 12
Loss after 7550174 batches: 0.0565
trigger times: 13
Loss after 7551136 batches: 0.0560
trigger times: 14
Loss after 7552098 batches: 0.0549
trigger times: 15
Loss after 7553060 batches: 0.0563
trigger times: 16
Loss after 7554022 batches: 0.0552
trigger times: 17
Loss after 7554984 batches: 0.0564
trigger times: 18
Loss after 7555946 batches: 0.0552
trigger times: 19
Loss after 7556908 batches: 0.0555
trigger times: 20
Loss after 7557870 batches: 0.0537
trigger times: 21
Loss after 7558832 batches: 0.0527
trigger times: 22
Loss after 7559794 batches: 0.0527
trigger times: 23
Loss after 7560756 batches: 0.0538
trigger times: 24
Loss after 7561718 batches: 0.0528
trigger times: 25
Early stopping!
Start to test process.
Loss after 7562680 batches: 0.0526
Time to train on one home:  51.893755197525024
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7563643 batches: 0.0992
trigger times: 1
Loss after 7564606 batches: 0.0278
trigger times: 2
Loss after 7565569 batches: 0.0243
trigger times: 3
Loss after 7566532 batches: 0.0202
trigger times: 4
Loss after 7567495 batches: 0.0186
trigger times: 5
Loss after 7568458 batches: 0.0174
trigger times: 6
Loss after 7569421 batches: 0.0161
trigger times: 7
Loss after 7570384 batches: 0.0151
trigger times: 8
Loss after 7571347 batches: 0.0150
trigger times: 9
Loss after 7572310 batches: 0.0145
trigger times: 10
Loss after 7573273 batches: 0.0144
trigger times: 11
Loss after 7574236 batches: 0.0142
trigger times: 12
Loss after 7575199 batches: 0.0144
trigger times: 13
Loss after 7576162 batches: 0.0139
trigger times: 14
Loss after 7577125 batches: 0.0140
trigger times: 15
Loss after 7578088 batches: 0.0137
trigger times: 16
Loss after 7579051 batches: 0.0136
trigger times: 17
Loss after 7580014 batches: 0.0134
trigger times: 18
Loss after 7580977 batches: 0.0135
trigger times: 19
Loss after 7581940 batches: 0.0133
trigger times: 20
Loss after 7582903 batches: 0.0131
trigger times: 21
Loss after 7583866 batches: 0.0131
trigger times: 22
Loss after 7584829 batches: 0.0133
trigger times: 23
Loss after 7585792 batches: 0.0132
trigger times: 24
Loss after 7586755 batches: 0.0131
trigger times: 25
Early stopping!
Start to test process.
Loss after 7587718 batches: 0.0136
Time to train on one home:  51.96900987625122
trigger times: 0
Loss after 7588681 batches: 0.0536
trigger times: 1
Loss after 7589644 batches: 0.0466
trigger times: 2
Loss after 7590607 batches: 0.0432
trigger times: 3
Loss after 7591570 batches: 0.0409
trigger times: 4
Loss after 7592533 batches: 0.0387
trigger times: 5
Loss after 7593496 batches: 0.0394
trigger times: 6
Loss after 7594459 batches: 0.0384
trigger times: 7
Loss after 7595422 batches: 0.0378
trigger times: 8
Loss after 7596385 batches: 0.0371
trigger times: 9
Loss after 7597348 batches: 0.0369
trigger times: 10
Loss after 7598311 batches: 0.0363
trigger times: 11
Loss after 7599274 batches: 0.0354
trigger times: 12
Loss after 7600237 batches: 0.0367
trigger times: 13
Loss after 7601200 batches: 0.0355
trigger times: 14
Loss after 7602163 batches: 0.0340
trigger times: 15
Loss after 7603126 batches: 0.0347
trigger times: 16
Loss after 7604089 batches: 0.0338
trigger times: 17
Loss after 7605052 batches: 0.0350
trigger times: 18
Loss after 7606015 batches: 0.0333
trigger times: 19
Loss after 7606978 batches: 0.0337
trigger times: 20
Loss after 7607941 batches: 0.0331
trigger times: 21
Loss after 7608904 batches: 0.0347
trigger times: 22
Loss after 7609867 batches: 0.0337
trigger times: 23
Loss after 7610830 batches: 0.0335
trigger times: 24
Loss after 7611793 batches: 0.0337
trigger times: 25
Early stopping!
Start to test process.
Loss after 7612756 batches: 0.0321
Time to train on one home:  51.94524836540222
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7613719 batches: 0.0636
trigger times: 1
Loss after 7614682 batches: 0.0530
trigger times: 0
Loss after 7615645 batches: 0.0509
trigger times: 1
Loss after 7616608 batches: 0.0466
trigger times: 2
Loss after 7617571 batches: 0.0415
trigger times: 3
Loss after 7618534 batches: 0.0389
trigger times: 4
Loss after 7619497 batches: 0.0369
trigger times: 5
Loss after 7620460 batches: 0.0346
trigger times: 6
Loss after 7621423 batches: 0.0338
trigger times: 7
Loss after 7622386 batches: 0.0328
trigger times: 8
Loss after 7623349 batches: 0.0319
trigger times: 9
Loss after 7624312 batches: 0.0312
trigger times: 10
Loss after 7625275 batches: 0.0306
trigger times: 11
Loss after 7626238 batches: 0.0313
trigger times: 12
Loss after 7627201 batches: 0.0297
trigger times: 13
Loss after 7628164 batches: 0.0306
trigger times: 14
Loss after 7629127 batches: 0.0302
trigger times: 15
Loss after 7630090 batches: 0.0293
trigger times: 16
Loss after 7631053 batches: 0.0293
trigger times: 17
Loss after 7632016 batches: 0.0285
trigger times: 18
Loss after 7632979 batches: 0.0305
trigger times: 19
Loss after 7633942 batches: 0.0283
trigger times: 20
Loss after 7634905 batches: 0.0289
trigger times: 21
Loss after 7635868 batches: 0.0286
trigger times: 22
Loss after 7636831 batches: 0.0280
trigger times: 23
Loss after 7637794 batches: 0.0278
trigger times: 24
Loss after 7638757 batches: 0.0268
trigger times: 25
Early stopping!
Start to test process.
Loss after 7639720 batches: 0.0272
Time to train on one home:  53.880820989608765
trigger times: 0
Loss after 7640683 batches: 0.0537
trigger times: 1
Loss after 7641646 batches: 0.0464
trigger times: 2
Loss after 7642609 batches: 0.0444
trigger times: 3
Loss after 7643572 batches: 0.0416
trigger times: 4
Loss after 7644535 batches: 0.0398
trigger times: 5
Loss after 7645498 batches: 0.0389
trigger times: 6
Loss after 7646461 batches: 0.0385
trigger times: 7
Loss after 7647424 batches: 0.0372
trigger times: 8
Loss after 7648387 batches: 0.0366
trigger times: 9
Loss after 7649350 batches: 0.0359
trigger times: 10
Loss after 7650313 batches: 0.0345
trigger times: 11
Loss after 7651276 batches: 0.0365
trigger times: 12
Loss after 7652239 batches: 0.0349
trigger times: 13
Loss after 7653202 batches: 0.0348
trigger times: 14
Loss after 7654165 batches: 0.0357
trigger times: 15
Loss after 7655128 batches: 0.0346
trigger times: 16
Loss after 7656091 batches: 0.0355
trigger times: 17
Loss after 7657054 batches: 0.0342
trigger times: 18
Loss after 7658017 batches: 0.0344
trigger times: 19
Loss after 7658980 batches: 0.0339
trigger times: 20
Loss after 7659943 batches: 0.0339
trigger times: 21
Loss after 7660906 batches: 0.0338
trigger times: 22
Loss after 7661869 batches: 0.0331
trigger times: 23
Loss after 7662832 batches: 0.0327
trigger times: 24
Loss after 7663795 batches: 0.0329
trigger times: 25
Early stopping!
Start to test process.
Loss after 7664758 batches: 0.0341
Time to train on one home:  51.84534573554993
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7665721 batches: 0.0513
trigger times: 1
Loss after 7666684 batches: 0.0494
trigger times: 0
Loss after 7667647 batches: 0.0473
trigger times: 1
Loss after 7668610 batches: 0.0447
trigger times: 2
Loss after 7669573 batches: 0.0439
trigger times: 3
Loss after 7670536 batches: 0.0428
trigger times: 4
Loss after 7671499 batches: 0.0422
trigger times: 5
Loss after 7672462 batches: 0.0408
trigger times: 0
Loss after 7673425 batches: 0.0404
trigger times: 1
Loss after 7674388 batches: 0.0397
trigger times: 2
Loss after 7675351 batches: 0.0391
trigger times: 3
Loss after 7676314 batches: 0.0392
trigger times: 0
Loss after 7677277 batches: 0.0384
trigger times: 1
Loss after 7678240 batches: 0.0384
trigger times: 2
Loss after 7679203 batches: 0.0378
trigger times: 3
Loss after 7680166 batches: 0.0375
trigger times: 4
Loss after 7681129 batches: 0.0374
trigger times: 5
Loss after 7682092 batches: 0.0375
trigger times: 6
Loss after 7683055 batches: 0.0359
trigger times: 0
Loss after 7684018 batches: 0.0352
trigger times: 1
Loss after 7684981 batches: 0.0353
trigger times: 2
Loss after 7685944 batches: 0.0351
trigger times: 3
Loss after 7686907 batches: 0.0340
trigger times: 4
Loss after 7687870 batches: 0.0358
trigger times: 5
Loss after 7688833 batches: 0.0355
trigger times: 6
Loss after 7689796 batches: 0.0353
trigger times: 7
Loss after 7690759 batches: 0.0340
trigger times: 8
Loss after 7691722 batches: 0.0356
trigger times: 9
Loss after 7692685 batches: 0.0351
trigger times: 10
Loss after 7693648 batches: 0.0337
trigger times: 0
Loss after 7694611 batches: 0.0333
trigger times: 1
Loss after 7695574 batches: 0.0354
trigger times: 2
Loss after 7696537 batches: 0.0358
trigger times: 3
Loss after 7697500 batches: 0.0338
trigger times: 4
Loss after 7698463 batches: 0.0330
trigger times: 5
Loss after 7699426 batches: 0.0335
trigger times: 6
Loss after 7700389 batches: 0.0335
trigger times: 7
Loss after 7701352 batches: 0.0341
trigger times: 8
Loss after 7702315 batches: 0.0327
trigger times: 9
Loss after 7703278 batches: 0.0318
trigger times: 10
Loss after 7704241 batches: 0.0320
trigger times: 11
Loss after 7705204 batches: 0.0322
trigger times: 12
Loss after 7706167 batches: 0.0321
trigger times: 13
Loss after 7707130 batches: 0.0312
trigger times: 14
Loss after 7708093 batches: 0.0309
trigger times: 15
Loss after 7709056 batches: 0.0313
trigger times: 16
Loss after 7710019 batches: 0.0318
trigger times: 17
Loss after 7710982 batches: 0.0315
trigger times: 18
Loss after 7711945 batches: 0.0308
trigger times: 19
Loss after 7712908 batches: 0.0326
trigger times: 20
Loss after 7713871 batches: 0.0330
trigger times: 21
Loss after 7714834 batches: 0.0344
trigger times: 22
Loss after 7715797 batches: 0.0325
trigger times: 23
Loss after 7716760 batches: 0.0311
trigger times: 24
Loss after 7717723 batches: 0.0298
trigger times: 25
Early stopping!
Start to test process.
Loss after 7718686 batches: 0.0289
Time to train on one home:  75.10645937919617
trigger times: 0
Loss after 7719649 batches: 0.0899
trigger times: 1
Loss after 7720612 batches: 0.0506
trigger times: 2
Loss after 7721575 batches: 0.0495
trigger times: 3
Loss after 7722538 batches: 0.0477
trigger times: 4
Loss after 7723501 batches: 0.0463
trigger times: 5
Loss after 7724464 batches: 0.0447
trigger times: 6
Loss after 7725427 batches: 0.0429
trigger times: 7
Loss after 7726390 batches: 0.0418
trigger times: 8
Loss after 7727353 batches: 0.0414
trigger times: 9
Loss after 7728316 batches: 0.0404
trigger times: 10
Loss after 7729279 batches: 0.0392
trigger times: 11
Loss after 7730242 batches: 0.0390
trigger times: 12
Loss after 7731205 batches: 0.0383
trigger times: 13
Loss after 7732168 batches: 0.0388
trigger times: 14
Loss after 7733131 batches: 0.0384
trigger times: 15
Loss after 7734094 batches: 0.0380
trigger times: 16
Loss after 7735057 batches: 0.0378
trigger times: 17
Loss after 7736020 batches: 0.0379
trigger times: 18
Loss after 7736983 batches: 0.0375
trigger times: 19
Loss after 7737946 batches: 0.0378
trigger times: 20
Loss after 7738909 batches: 0.0368
trigger times: 21
Loss after 7739872 batches: 0.0371
trigger times: 22
Loss after 7740835 batches: 0.0367
trigger times: 23
Loss after 7741798 batches: 0.0370
trigger times: 24
Loss after 7742761 batches: 0.0369
trigger times: 25
Early stopping!
Start to test process.
Loss after 7743724 batches: 0.0367
Time to train on one home:  52.32476735115051
trigger times: 0
Loss after 7744687 batches: 0.0940
trigger times: 1
Loss after 7745650 batches: 0.0906
trigger times: 2
Loss after 7746613 batches: 0.0859
trigger times: 3
Loss after 7747576 batches: 0.0853
trigger times: 4
Loss after 7748539 batches: 0.0820
trigger times: 5
Loss after 7749502 batches: 0.0798
trigger times: 6
Loss after 7750465 batches: 0.0788
trigger times: 7
Loss after 7751428 batches: 0.0799
trigger times: 8
Loss after 7752391 batches: 0.0767
trigger times: 9
Loss after 7753354 batches: 0.0762
trigger times: 10
Loss after 7754317 batches: 0.0758
trigger times: 11
Loss after 7755280 batches: 0.0748
trigger times: 12
Loss after 7756243 batches: 0.0737
trigger times: 13
Loss after 7757206 batches: 0.0736
trigger times: 14
Loss after 7758169 batches: 0.0724
trigger times: 15
Loss after 7759132 batches: 0.0735
trigger times: 16
Loss after 7760095 batches: 0.0722
trigger times: 17
Loss after 7761058 batches: 0.0716
trigger times: 18
Loss after 7762021 batches: 0.0710
trigger times: 19
Loss after 7762984 batches: 0.0707
trigger times: 20
Loss after 7763947 batches: 0.0703
trigger times: 21
Loss after 7764910 batches: 0.0716
trigger times: 22
Loss after 7765873 batches: 0.0713
trigger times: 23
Loss after 7766836 batches: 0.0716
trigger times: 24
Loss after 7767799 batches: 0.0683
trigger times: 25
Early stopping!
Start to test process.
Loss after 7768762 batches: 0.0671
Time to train on one home:  52.64171528816223
trigger times: 0
Loss after 7769725 batches: 0.0828
trigger times: 1
Loss after 7770688 batches: 0.0646
trigger times: 2
Loss after 7771651 batches: 0.0624
trigger times: 3
Loss after 7772614 batches: 0.0585
trigger times: 4
Loss after 7773577 batches: 0.0548
trigger times: 5
Loss after 7774540 batches: 0.0518
trigger times: 6
Loss after 7775503 batches: 0.0499
trigger times: 7
Loss after 7776466 batches: 0.0499
trigger times: 8
Loss after 7777429 batches: 0.0481
trigger times: 9
Loss after 7778392 batches: 0.0475
trigger times: 10
Loss after 7779355 batches: 0.0472
trigger times: 11
Loss after 7780318 batches: 0.0460
trigger times: 12
Loss after 7781281 batches: 0.0462
trigger times: 13
Loss after 7782244 batches: 0.0457
trigger times: 14
Loss after 7783207 batches: 0.0439
trigger times: 15
Loss after 7784170 batches: 0.0437
trigger times: 16
Loss after 7785133 batches: 0.0435
trigger times: 17
Loss after 7786096 batches: 0.0434
trigger times: 18
Loss after 7787059 batches: 0.0435
trigger times: 19
Loss after 7788022 batches: 0.0423
trigger times: 20
Loss after 7788985 batches: 0.0426
trigger times: 21
Loss after 7789948 batches: 0.0415
trigger times: 22
Loss after 7790911 batches: 0.0417
trigger times: 23
Loss after 7791874 batches: 0.0414
trigger times: 24
Loss after 7792837 batches: 0.0407
trigger times: 25
Early stopping!
Start to test process.
Loss after 7793800 batches: 0.0402
Time to train on one home:  52.78573966026306
trigger times: 0
Loss after 7794729 batches: 0.0829
trigger times: 1
Loss after 7795658 batches: 0.0658
trigger times: 2
Loss after 7796587 batches: 0.0516
trigger times: 3
Loss after 7797516 batches: 0.0449
trigger times: 4
Loss after 7798445 batches: 0.0408
trigger times: 5
Loss after 7799374 batches: 0.0395
trigger times: 6
Loss after 7800303 batches: 0.0373
trigger times: 7
Loss after 7801232 batches: 0.0384
trigger times: 8
Loss after 7802161 batches: 0.0372
trigger times: 9
Loss after 7803090 batches: 0.0374
trigger times: 10
Loss after 7804019 batches: 0.0346
trigger times: 11
Loss after 7804948 batches: 0.0333
trigger times: 12
Loss after 7805877 batches: 0.0337
trigger times: 13
Loss after 7806806 batches: 0.0314
trigger times: 14
Loss after 7807735 batches: 0.0320
trigger times: 15
Loss after 7808664 batches: 0.0313
trigger times: 16
Loss after 7809593 batches: 0.0320
trigger times: 17
Loss after 7810522 batches: 0.0311
trigger times: 18
Loss after 7811451 batches: 0.0322
trigger times: 19
Loss after 7812380 batches: 0.0291
trigger times: 20
Loss after 7813309 batches: 0.0292
trigger times: 21
Loss after 7814238 batches: 0.0309
trigger times: 22
Loss after 7815167 batches: 0.0336
trigger times: 23
Loss after 7816096 batches: 0.0300
trigger times: 24
Loss after 7817025 batches: 0.0296
trigger times: 25
Early stopping!
Start to test process.
Loss after 7817954 batches: 0.0412
Time to train on one home:  51.937047243118286
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7818917 batches: 0.0694
trigger times: 1
Loss after 7819880 batches: 0.0317
trigger times: 2
Loss after 7820843 batches: 0.0272
trigger times: 3
Loss after 7821806 batches: 0.0269
trigger times: 4
Loss after 7822769 batches: 0.0264
trigger times: 5
Loss after 7823732 batches: 0.0257
trigger times: 6
Loss after 7824695 batches: 0.0251
trigger times: 7
Loss after 7825658 batches: 0.0243
trigger times: 8
Loss after 7826621 batches: 0.0239
trigger times: 9
Loss after 7827584 batches: 0.0233
trigger times: 10
Loss after 7828547 batches: 0.0228
trigger times: 11
Loss after 7829510 batches: 0.0225
trigger times: 12
Loss after 7830473 batches: 0.0221
trigger times: 13
Loss after 7831436 batches: 0.0223
trigger times: 14
Loss after 7832399 batches: 0.0217
trigger times: 15
Loss after 7833362 batches: 0.0214
trigger times: 16
Loss after 7834325 batches: 0.0211
trigger times: 17
Loss after 7835288 batches: 0.0209
trigger times: 18
Loss after 7836251 batches: 0.0205
trigger times: 19
Loss after 7837214 batches: 0.0206
trigger times: 20
Loss after 7838177 batches: 0.0202
trigger times: 21
Loss after 7839140 batches: 0.0205
trigger times: 22
Loss after 7840103 batches: 0.0198
trigger times: 23
Loss after 7841066 batches: 0.0198
trigger times: 24
Loss after 7842029 batches: 0.0194
trigger times: 25
Early stopping!
Start to test process.
Loss after 7842992 batches: 0.0191
Time to train on one home:  51.990070819854736
trigger times: 0
Loss after 7843955 batches: 0.1787
trigger times: 1
Loss after 7844918 batches: 0.1189
trigger times: 2
Loss after 7845881 batches: 0.1024
trigger times: 3
Loss after 7846844 batches: 0.0904
trigger times: 4
Loss after 7847807 batches: 0.0864
trigger times: 5
Loss after 7848770 batches: 0.0849
trigger times: 6
Loss after 7849733 batches: 0.0824
trigger times: 7
Loss after 7850696 batches: 0.0799
trigger times: 8
Loss after 7851659 batches: 0.0798
trigger times: 9
Loss after 7852622 batches: 0.0762
trigger times: 10
Loss after 7853585 batches: 0.0750
trigger times: 11
Loss after 7854548 batches: 0.0726
trigger times: 12
Loss after 7855511 batches: 0.0699
trigger times: 13
Loss after 7856474 batches: 0.0673
trigger times: 14
Loss after 7857437 batches: 0.0643
trigger times: 15
Loss after 7858400 batches: 0.0644
trigger times: 16
Loss after 7859363 batches: 0.0614
trigger times: 17
Loss after 7860326 batches: 0.0589
trigger times: 18
Loss after 7861289 batches: 0.0590
trigger times: 19
Loss after 7862252 batches: 0.0552
trigger times: 20
Loss after 7863215 batches: 0.0540
trigger times: 21
Loss after 7864178 batches: 0.0540
trigger times: 22
Loss after 7865141 batches: 0.0533
trigger times: 23
Loss after 7866104 batches: 0.0508
trigger times: 24
Loss after 7867067 batches: 0.0510
trigger times: 25
Early stopping!
Start to test process.
Loss after 7868030 batches: 0.0504
Time to train on one home:  51.73617482185364
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7868993 batches: 0.0907
trigger times: 1
Loss after 7869956 batches: 0.0775
trigger times: 2
Loss after 7870919 batches: 0.0758
trigger times: 3
Loss after 7871882 batches: 0.0729
trigger times: 4
Loss after 7872845 batches: 0.0708
trigger times: 5
Loss after 7873808 batches: 0.0697
trigger times: 6
Loss after 7874771 batches: 0.0680
trigger times: 7
Loss after 7875734 batches: 0.0670
trigger times: 8
Loss after 7876697 batches: 0.0659
trigger times: 9
Loss after 7877660 batches: 0.0658
trigger times: 10
Loss after 7878623 batches: 0.0656
trigger times: 11
Loss after 7879586 batches: 0.0642
trigger times: 12
Loss after 7880549 batches: 0.0642
trigger times: 13
Loss after 7881512 batches: 0.0638
trigger times: 14
Loss after 7882475 batches: 0.0639
trigger times: 15
Loss after 7883438 batches: 0.0646
trigger times: 16
Loss after 7884401 batches: 0.0634
trigger times: 17
Loss after 7885364 batches: 0.0631
trigger times: 18
Loss after 7886327 batches: 0.0623
trigger times: 19
Loss after 7887290 batches: 0.0627
trigger times: 20
Loss after 7888253 batches: 0.0620
trigger times: 21
Loss after 7889216 batches: 0.0619
trigger times: 22
Loss after 7890179 batches: 0.0611
trigger times: 23
Loss after 7891142 batches: 0.0609
trigger times: 24
Loss after 7892105 batches: 0.0588
trigger times: 25
Early stopping!
Start to test process.
Loss after 7893068 batches: 0.0602
Time to train on one home:  52.69392704963684
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 7894031 batches: 0.0874
trigger times: 1
Loss after 7894994 batches: 0.0780
trigger times: 2
Loss after 7895957 batches: 0.0773
trigger times: 3
Loss after 7896920 batches: 0.0749
trigger times: 4
Loss after 7897883 batches: 0.0714
trigger times: 5
Loss after 7898846 batches: 0.0706
trigger times: 6
Loss after 7899809 batches: 0.0698
trigger times: 7
Loss after 7900772 batches: 0.0668
trigger times: 8
Loss after 7901735 batches: 0.0669
trigger times: 9
Loss after 7902698 batches: 0.0658
trigger times: 10
Loss after 7903661 batches: 0.0628
trigger times: 11
Loss after 7904624 batches: 0.0616
trigger times: 12
Loss after 7905587 batches: 0.0625
trigger times: 13
Loss after 7906550 batches: 0.0642
trigger times: 14
Loss after 7907513 batches: 0.0639
trigger times: 15
Loss after 7908476 batches: 0.0624
trigger times: 16
Loss after 7909439 batches: 0.0614
trigger times: 17
Loss after 7910402 batches: 0.0604
trigger times: 18
Loss after 7911365 batches: 0.0589
trigger times: 19
Loss after 7912328 batches: 0.0603
trigger times: 20
Loss after 7913291 batches: 0.0591
trigger times: 21
Loss after 7914254 batches: 0.0578
trigger times: 22
Loss after 7915217 batches: 0.0559
trigger times: 23
Loss after 7916180 batches: 0.0558
trigger times: 24
Loss after 7917143 batches: 0.0561
trigger times: 25
Early stopping!
Start to test process.
Loss after 7918106 batches: 0.0551
Time to train on one home:  52.051132678985596
trigger times: 0
Loss after 7919069 batches: 0.0288
trigger times: 0
Loss after 7920032 batches: 0.0253
trigger times: 1
Loss after 7920995 batches: 0.0229
trigger times: 2
Loss after 7921958 batches: 0.0210
trigger times: 0
Loss after 7922921 batches: 0.0199
trigger times: 1
Loss after 7923884 batches: 0.0196
trigger times: 2
Loss after 7924847 batches: 0.0190
trigger times: 3
Loss after 7925810 batches: 0.0181
trigger times: 4
Loss after 7926773 batches: 0.0179
trigger times: 5
Loss after 7927736 batches: 0.0177
trigger times: 6
Loss after 7928699 batches: 0.0173
trigger times: 7
Loss after 7929662 batches: 0.0173
trigger times: 8
Loss after 7930625 batches: 0.0170
trigger times: 9
Loss after 7931588 batches: 0.0167
trigger times: 10
Loss after 7932551 batches: 0.0166
trigger times: 11
Loss after 7933514 batches: 0.0164
trigger times: 12
Loss after 7934477 batches: 0.0158
trigger times: 13
Loss after 7935440 batches: 0.0165
trigger times: 14
Loss after 7936403 batches: 0.0162
trigger times: 15
Loss after 7937366 batches: 0.0157
trigger times: 16
Loss after 7938329 batches: 0.0154
trigger times: 17
Loss after 7939292 batches: 0.0152
trigger times: 18
Loss after 7940255 batches: 0.0153
trigger times: 19
Loss after 7941218 batches: 0.0153
trigger times: 20
Loss after 7942181 batches: 0.0153
trigger times: 21
Loss after 7943144 batches: 0.0148
trigger times: 22
Loss after 7944107 batches: 0.0149
trigger times: 23
Loss after 7945070 batches: 0.0147
trigger times: 24
Loss after 7946033 batches: 0.0149
trigger times: 25
Early stopping!
Start to test process.
Loss after 7946996 batches: 0.0149
Time to train on one home:  55.111217975616455
trigger times: 0
Loss after 7947959 batches: 0.0461
trigger times: 1
Loss after 7948922 batches: 0.0409
trigger times: 2
Loss after 7949885 batches: 0.0393
trigger times: 0
Loss after 7950848 batches: 0.0364
trigger times: 0
Loss after 7951811 batches: 0.0340
trigger times: 1
Loss after 7952774 batches: 0.0329
trigger times: 2
Loss after 7953737 batches: 0.0315
trigger times: 3
Loss after 7954700 batches: 0.0299
trigger times: 4
Loss after 7955663 batches: 0.0304
trigger times: 5
Loss after 7956626 batches: 0.0284
trigger times: 6
Loss after 7957589 batches: 0.0279
trigger times: 7
Loss after 7958552 batches: 0.0275
trigger times: 8
Loss after 7959515 batches: 0.0263
trigger times: 9
Loss after 7960478 batches: 0.0256
trigger times: 10
Loss after 7961441 batches: 0.0256
trigger times: 11
Loss after 7962404 batches: 0.0250
trigger times: 12
Loss after 7963367 batches: 0.0254
trigger times: 13
Loss after 7964330 batches: 0.0258
trigger times: 14
Loss after 7965293 batches: 0.0253
trigger times: 15
Loss after 7966256 batches: 0.0243
trigger times: 16
Loss after 7967219 batches: 0.0243
trigger times: 17
Loss after 7968182 batches: 0.0248
trigger times: 18
Loss after 7969145 batches: 0.0247
trigger times: 19
Loss after 7970108 batches: 0.0247
trigger times: 20
Loss after 7971071 batches: 0.0248
trigger times: 21
Loss after 7972034 batches: 0.0242
trigger times: 22
Loss after 7972997 batches: 0.0243
trigger times: 23
Loss after 7973960 batches: 0.0234
trigger times: 24
Loss after 7974923 batches: 0.0238
trigger times: 25
Early stopping!
Start to test process.
Loss after 7975886 batches: 0.0231
Time to train on one home:  55.33916997909546
trigger times: 0
Loss after 7976849 batches: 0.0935
trigger times: 1
Loss after 7977812 batches: 0.0700
trigger times: 2
Loss after 7978775 batches: 0.0639
trigger times: 3
Loss after 7979738 batches: 0.0579
trigger times: 4
Loss after 7980701 batches: 0.0550
trigger times: 5
Loss after 7981664 batches: 0.0514
trigger times: 6
Loss after 7982627 batches: 0.0501
trigger times: 7
Loss after 7983590 batches: 0.0481
trigger times: 8
Loss after 7984553 batches: 0.0480
trigger times: 9
Loss after 7985516 batches: 0.0464
trigger times: 10
Loss after 7986479 batches: 0.0465
trigger times: 11
Loss after 7987442 batches: 0.0455
trigger times: 12
Loss after 7988405 batches: 0.0448
trigger times: 13
Loss after 7989368 batches: 0.0458
trigger times: 14
Loss after 7990331 batches: 0.0448
trigger times: 15
Loss after 7991294 batches: 0.0456
trigger times: 16
Loss after 7992257 batches: 0.0443
trigger times: 17
Loss after 7993220 batches: 0.0434
trigger times: 18
Loss after 7994183 batches: 0.0439
trigger times: 19
Loss after 7995146 batches: 0.0436
trigger times: 20
Loss after 7996109 batches: 0.0428
trigger times: 21
Loss after 7997072 batches: 0.0415
trigger times: 22
Loss after 7998035 batches: 0.0416
trigger times: 23
Loss after 7998998 batches: 0.0416
trigger times: 24
Loss after 7999961 batches: 0.0418
trigger times: 25
Early stopping!
Start to test process.
Loss after 8000924 batches: 0.0413
Time to train on one home:  52.3031165599823
trigger times: 0
Loss after 8001887 batches: 0.0826
trigger times: 1
Loss after 8002850 batches: 0.0659
trigger times: 2
Loss after 8003813 batches: 0.0619
trigger times: 3
Loss after 8004776 batches: 0.0574
trigger times: 4
Loss after 8005739 batches: 0.0554
trigger times: 5
Loss after 8006702 batches: 0.0520
trigger times: 6
Loss after 8007665 batches: 0.0508
trigger times: 7
Loss after 8008628 batches: 0.0488
trigger times: 8
Loss after 8009591 batches: 0.0480
trigger times: 9
Loss after 8010554 batches: 0.0469
trigger times: 10
Loss after 8011517 batches: 0.0470
trigger times: 11
Loss after 8012480 batches: 0.0455
trigger times: 12
Loss after 8013443 batches: 0.0458
trigger times: 13
Loss after 8014406 batches: 0.0453
trigger times: 14
Loss after 8015369 batches: 0.0450
trigger times: 15
Loss after 8016332 batches: 0.0444
trigger times: 16
Loss after 8017295 batches: 0.0450
trigger times: 17
Loss after 8018258 batches: 0.0440
trigger times: 18
Loss after 8019221 batches: 0.0423
trigger times: 19
Loss after 8020184 batches: 0.0430
trigger times: 20
Loss after 8021147 batches: 0.0426
trigger times: 21
Loss after 8022110 batches: 0.0426
trigger times: 22
Loss after 8023073 batches: 0.0417
trigger times: 23
Loss after 8024036 batches: 0.0424
trigger times: 24
Loss after 8024999 batches: 0.0411
trigger times: 25
Early stopping!
Start to test process.
Loss after 8025962 batches: 0.0417
Time to train on one home:  52.059351444244385
trigger times: 0
Loss after 8026925 batches: 0.0533
trigger times: 1
Loss after 8027888 batches: 0.0463
trigger times: 2
Loss after 8028851 batches: 0.0432
trigger times: 3
Loss after 8029814 batches: 0.0404
trigger times: 4
Loss after 8030777 batches: 0.0396
trigger times: 5
Loss after 8031740 batches: 0.0370
trigger times: 6
Loss after 8032703 batches: 0.0384
trigger times: 7
Loss after 8033666 batches: 0.0382
trigger times: 8
Loss after 8034629 batches: 0.0370
trigger times: 9
Loss after 8035592 batches: 0.0372
trigger times: 10
Loss after 8036555 batches: 0.0370
trigger times: 11
Loss after 8037518 batches: 0.0349
trigger times: 12
Loss after 8038481 batches: 0.0354
trigger times: 13
Loss after 8039444 batches: 0.0349
trigger times: 14
Loss after 8040407 batches: 0.0353
trigger times: 15
Loss after 8041370 batches: 0.0354
trigger times: 16
Loss after 8042333 batches: 0.0357
trigger times: 17
Loss after 8043296 batches: 0.0354
trigger times: 18
Loss after 8044259 batches: 0.0332
trigger times: 19
Loss after 8045222 batches: 0.0341
trigger times: 20
Loss after 8046185 batches: 0.0334
trigger times: 21
Loss after 8047148 batches: 0.0337
trigger times: 22
Loss after 8048111 batches: 0.0329
trigger times: 23
Loss after 8049074 batches: 0.0328
trigger times: 24
Loss after 8050037 batches: 0.0327
trigger times: 25
Early stopping!
Start to test process.
Loss after 8051000 batches: 0.0330
Time to train on one home:  52.40566325187683
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8051963 batches: 0.0936
trigger times: 1
Loss after 8052926 batches: 0.0680
trigger times: 2
Loss after 8053889 batches: 0.0635
trigger times: 0
Loss after 8054852 batches: 0.0573
trigger times: 0
Loss after 8055815 batches: 0.0493
trigger times: 1
Loss after 8056778 batches: 0.0462
trigger times: 0
Loss after 8057741 batches: 0.0434
trigger times: 0
Loss after 8058704 batches: 0.0428
trigger times: 1
Loss after 8059667 batches: 0.0411
trigger times: 0
Loss after 8060630 batches: 0.0382
trigger times: 1
Loss after 8061593 batches: 0.0389
trigger times: 2
Loss after 8062556 batches: 0.0379
trigger times: 0
Loss after 8063519 batches: 0.0367
trigger times: 1
Loss after 8064482 batches: 0.0351
trigger times: 2
Loss after 8065445 batches: 0.0352
trigger times: 3
Loss after 8066408 batches: 0.0343
trigger times: 4
Loss after 8067371 batches: 0.0343
trigger times: 5
Loss after 8068334 batches: 0.0335
trigger times: 6
Loss after 8069297 batches: 0.0332
trigger times: 7
Loss after 8070260 batches: 0.0331
trigger times: 8
Loss after 8071223 batches: 0.0333
trigger times: 9
Loss after 8072186 batches: 0.0327
trigger times: 10
Loss after 8073149 batches: 0.0329
trigger times: 11
Loss after 8074112 batches: 0.0329
trigger times: 12
Loss after 8075075 batches: 0.0332
trigger times: 13
Loss after 8076038 batches: 0.0324
trigger times: 0
Loss after 8077001 batches: 0.0323
trigger times: 1
Loss after 8077964 batches: 0.0323
trigger times: 2
Loss after 8078927 batches: 0.0317
trigger times: 3
Loss after 8079890 batches: 0.0310
trigger times: 0
Loss after 8080853 batches: 0.0305
trigger times: 1
Loss after 8081816 batches: 0.0308
trigger times: 0
Loss after 8082779 batches: 0.0303
trigger times: 1
Loss after 8083742 batches: 0.0297
trigger times: 2
Loss after 8084705 batches: 0.0305
trigger times: 3
Loss after 8085668 batches: 0.0307
trigger times: 4
Loss after 8086631 batches: 0.0295
trigger times: 5
Loss after 8087594 batches: 0.0291
trigger times: 6
Loss after 8088557 batches: 0.0283
trigger times: 0
Loss after 8089520 batches: 0.0286
trigger times: 0
Loss after 8090483 batches: 0.0275
trigger times: 1
Loss after 8091446 batches: 0.0276
trigger times: 2
Loss after 8092409 batches: 0.0274
trigger times: 3
Loss after 8093372 batches: 0.0275
trigger times: 4
Loss after 8094335 batches: 0.0274
trigger times: 5
Loss after 8095298 batches: 0.0280
trigger times: 6
Loss after 8096261 batches: 0.0270
trigger times: 7
Loss after 8097224 batches: 0.0272
trigger times: 8
Loss after 8098187 batches: 0.0268
trigger times: 9
Loss after 8099150 batches: 0.0265
trigger times: 10
Loss after 8100113 batches: 0.0257
trigger times: 11
Loss after 8101076 batches: 0.0273
trigger times: 12
Loss after 8102039 batches: 0.0269
trigger times: 13
Loss after 8103002 batches: 0.0259
trigger times: 14
Loss after 8103965 batches: 0.0266
trigger times: 0
Loss after 8104928 batches: 0.0252
trigger times: 1
Loss after 8105891 batches: 0.0259
trigger times: 2
Loss after 8106854 batches: 0.0257
trigger times: 3
Loss after 8107817 batches: 0.0256
trigger times: 4
Loss after 8108780 batches: 0.0257
trigger times: 5
Loss after 8109743 batches: 0.0250
trigger times: 6
Loss after 8110706 batches: 0.0254
trigger times: 7
Loss after 8111669 batches: 0.0267
trigger times: 0
Loss after 8112632 batches: 0.0267
trigger times: 1
Loss after 8113595 batches: 0.0246
trigger times: 2
Loss after 8114558 batches: 0.0240
trigger times: 3
Loss after 8115521 batches: 0.0238
trigger times: 4
Loss after 8116484 batches: 0.0248
trigger times: 5
Loss after 8117447 batches: 0.0243
trigger times: 6
Loss after 8118410 batches: 0.0241
trigger times: 7
Loss after 8119373 batches: 0.0243
trigger times: 8
Loss after 8120336 batches: 0.0243
trigger times: 9
Loss after 8121299 batches: 0.0239
trigger times: 10
Loss after 8122262 batches: 0.0237
trigger times: 11
Loss after 8123225 batches: 0.0235
trigger times: 12
Loss after 8124188 batches: 0.0231
trigger times: 13
Loss after 8125151 batches: 0.0239
trigger times: 14
Loss after 8126114 batches: 0.0241
trigger times: 15
Loss after 8127077 batches: 0.0235
trigger times: 16
Loss after 8128040 batches: 0.0223
trigger times: 17
Loss after 8129003 batches: 0.0230
trigger times: 18
Loss after 8129966 batches: 0.0232
trigger times: 19
Loss after 8130929 batches: 0.0237
trigger times: 20
Loss after 8131892 batches: 0.0232
trigger times: 21
Loss after 8132855 batches: 0.0238
trigger times: 22
Loss after 8133818 batches: 0.0233
trigger times: 23
Loss after 8134781 batches: 0.0236
trigger times: 24
Loss after 8135744 batches: 0.0228
trigger times: 25
Early stopping!
Start to test process.
Loss after 8136707 batches: 0.0217
Time to train on one home:  100.03146171569824
trigger times: 0
Loss after 8137670 batches: 0.0992
trigger times: 1
Loss after 8138633 batches: 0.0786
trigger times: 2
Loss after 8139596 batches: 0.0784
trigger times: 3
Loss after 8140559 batches: 0.0771
trigger times: 4
Loss after 8141522 batches: 0.0752
trigger times: 5
Loss after 8142485 batches: 0.0732
trigger times: 6
Loss after 8143448 batches: 0.0713
trigger times: 7
Loss after 8144411 batches: 0.0700
trigger times: 8
Loss after 8145374 batches: 0.0675
trigger times: 9
Loss after 8146337 batches: 0.0688
trigger times: 10
Loss after 8147300 batches: 0.0675
trigger times: 11
Loss after 8148263 batches: 0.0669
trigger times: 12
Loss after 8149226 batches: 0.0656
trigger times: 13
Loss after 8150189 batches: 0.0651
trigger times: 14
Loss after 8151152 batches: 0.0647
trigger times: 15
Loss after 8152115 batches: 0.0651
trigger times: 16
Loss after 8153078 batches: 0.0646
trigger times: 17
Loss after 8154041 batches: 0.0678
trigger times: 18
Loss after 8155004 batches: 0.0668
trigger times: 19
Loss after 8155967 batches: 0.0639
trigger times: 20
Loss after 8156930 batches: 0.0637
trigger times: 21
Loss after 8157893 batches: 0.0653
trigger times: 22
Loss after 8158856 batches: 0.0628
trigger times: 23
Loss after 8159819 batches: 0.0623
trigger times: 24
Loss after 8160782 batches: 0.0618
trigger times: 25
Early stopping!
Start to test process.
Loss after 8161745 batches: 0.0608
Time to train on one home:  52.749316930770874
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8162708 batches: 0.0917
trigger times: 1
Loss after 8163671 batches: 0.0694
trigger times: 2
Loss after 8164634 batches: 0.0631
trigger times: 0
Loss after 8165597 batches: 0.0558
trigger times: 0
Loss after 8166560 batches: 0.0505
trigger times: 1
Loss after 8167523 batches: 0.0462
trigger times: 2
Loss after 8168486 batches: 0.0441
trigger times: 0
Loss after 8169449 batches: 0.0424
trigger times: 1
Loss after 8170412 batches: 0.0413
trigger times: 2
Loss after 8171375 batches: 0.0411
trigger times: 0
Loss after 8172338 batches: 0.0386
trigger times: 0
Loss after 8173301 batches: 0.0375
trigger times: 0
Loss after 8174264 batches: 0.0367
trigger times: 1
Loss after 8175227 batches: 0.0365
trigger times: 2
Loss after 8176190 batches: 0.0357
trigger times: 3
Loss after 8177153 batches: 0.0359
trigger times: 4
Loss after 8178116 batches: 0.0357
trigger times: 5
Loss after 8179079 batches: 0.0343
trigger times: 6
Loss after 8180042 batches: 0.0333
trigger times: 7
Loss after 8181005 batches: 0.0337
trigger times: 8
Loss after 8181968 batches: 0.0334
trigger times: 9
Loss after 8182931 batches: 0.0327
trigger times: 10
Loss after 8183894 batches: 0.0322
trigger times: 11
Loss after 8184857 batches: 0.0323
trigger times: 12
Loss after 8185820 batches: 0.0326
trigger times: 13
Loss after 8186783 batches: 0.0326
trigger times: 14
Loss after 8187746 batches: 0.0310
trigger times: 0
Loss after 8188709 batches: 0.0325
trigger times: 1
Loss after 8189672 batches: 0.0309
trigger times: 2
Loss after 8190635 batches: 0.0308
trigger times: 3
Loss after 8191598 batches: 0.0312
trigger times: 4
Loss after 8192561 batches: 0.0311
trigger times: 5
Loss after 8193524 batches: 0.0311
trigger times: 6
Loss after 8194487 batches: 0.0306
trigger times: 7
Loss after 8195450 batches: 0.0300
trigger times: 8
Loss after 8196413 batches: 0.0292
trigger times: 0
Loss after 8197376 batches: 0.0302
trigger times: 1
Loss after 8198339 batches: 0.0295
trigger times: 2
Loss after 8199302 batches: 0.0291
trigger times: 3
Loss after 8200265 batches: 0.0300
trigger times: 4
Loss after 8201228 batches: 0.0302
trigger times: 5
Loss after 8202191 batches: 0.0292
trigger times: 6
Loss after 8203154 batches: 0.0302
trigger times: 7
Loss after 8204117 batches: 0.0303
trigger times: 8
Loss after 8205080 batches: 0.0281
trigger times: 0
Loss after 8206043 batches: 0.0273
trigger times: 1
Loss after 8207006 batches: 0.0284
trigger times: 2
Loss after 8207969 batches: 0.0279
trigger times: 3
Loss after 8208932 batches: 0.0283
trigger times: 4
Loss after 8209895 batches: 0.0286
trigger times: 5
Loss after 8210858 batches: 0.0269
trigger times: 6
Loss after 8211821 batches: 0.0268
trigger times: 7
Loss after 8212784 batches: 0.0277
trigger times: 8
Loss after 8213747 batches: 0.0271
trigger times: 9
Loss after 8214710 batches: 0.0267
trigger times: 10
Loss after 8215673 batches: 0.0265
trigger times: 11
Loss after 8216636 batches: 0.0260
trigger times: 12
Loss after 8217599 batches: 0.0257
trigger times: 13
Loss after 8218562 batches: 0.0269
trigger times: 14
Loss after 8219525 batches: 0.0271
trigger times: 15
Loss after 8220488 batches: 0.0268
trigger times: 16
Loss after 8221451 batches: 0.0269
trigger times: 17
Loss after 8222414 batches: 0.0269
trigger times: 18
Loss after 8223377 batches: 0.0268
trigger times: 19
Loss after 8224340 batches: 0.0261
trigger times: 20
Loss after 8225303 batches: 0.0254
trigger times: 21
Loss after 8226266 batches: 0.0263
trigger times: 22
Loss after 8227229 batches: 0.0260
trigger times: 23
Loss after 8228192 batches: 0.0246
trigger times: 24
Loss after 8229155 batches: 0.0255
trigger times: 25
Early stopping!
Start to test process.
Loss after 8230118 batches: 0.0261
Time to train on one home:  86.87804651260376
trigger times: 0
Loss after 8231013 batches: 0.0546
trigger times: 1
Loss after 8231908 batches: 0.0331
trigger times: 2
Loss after 8232803 batches: 0.0179
trigger times: 3
Loss after 8233698 batches: 0.0113
trigger times: 4
Loss after 8234593 batches: 0.0083
trigger times: 5
Loss after 8235488 batches: 0.0068
trigger times: 6
Loss after 8236383 batches: 0.0059
trigger times: 7
Loss after 8237278 batches: 0.0052
trigger times: 8
Loss after 8238173 batches: 0.0052
trigger times: 9
Loss after 8239068 batches: 0.0049
trigger times: 10
Loss after 8239963 batches: 0.0051
trigger times: 11
Loss after 8240858 batches: 0.0050
trigger times: 12
Loss after 8241753 batches: 0.0044
trigger times: 13
Loss after 8242648 batches: 0.0054
trigger times: 14
Loss after 8243543 batches: 0.0051
trigger times: 15
Loss after 8244438 batches: 0.0053
trigger times: 16
Loss after 8245333 batches: 0.0051
trigger times: 17
Loss after 8246228 batches: 0.0040
trigger times: 18
Loss after 8247123 batches: 0.0040
trigger times: 19
Loss after 8248018 batches: 0.0045
trigger times: 20
Loss after 8248913 batches: 0.0039
trigger times: 21
Loss after 8249808 batches: 0.0041
trigger times: 22
Loss after 8250703 batches: 0.0041
trigger times: 23
Loss after 8251598 batches: 0.0041
trigger times: 24
Loss after 8252493 batches: 0.0040
trigger times: 25
Early stopping!
Start to test process.
Loss after 8253388 batches: 0.0042
Time to train on one home:  50.49986791610718
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8254325 batches: 0.0803
trigger times: 1
Loss after 8255262 batches: 0.0721
trigger times: 2
Loss after 8256199 batches: 0.0685
trigger times: 3
Loss after 8257136 batches: 0.0661
trigger times: 4
Loss after 8258073 batches: 0.0639
trigger times: 5
Loss after 8259010 batches: 0.0619
trigger times: 6
Loss after 8259947 batches: 0.0607
trigger times: 7
Loss after 8260884 batches: 0.0600
trigger times: 8
Loss after 8261821 batches: 0.0579
trigger times: 9
Loss after 8262758 batches: 0.0583
trigger times: 10
Loss after 8263695 batches: 0.0569
trigger times: 11
Loss after 8264632 batches: 0.0566
trigger times: 12
Loss after 8265569 batches: 0.0559
trigger times: 13
Loss after 8266506 batches: 0.0562
trigger times: 14
Loss after 8267443 batches: 0.0549
trigger times: 15
Loss after 8268380 batches: 0.0560
trigger times: 16
Loss after 8269317 batches: 0.0558
trigger times: 17
Loss after 8270254 batches: 0.0552
trigger times: 18
Loss after 8271191 batches: 0.0532
trigger times: 19
Loss after 8272128 batches: 0.0550
trigger times: 20
Loss after 8273065 batches: 0.0554
trigger times: 21
Loss after 8274002 batches: 0.0533
trigger times: 22
Loss after 8274939 batches: 0.0530
trigger times: 23
Loss after 8275876 batches: 0.0532
trigger times: 24
Loss after 8276813 batches: 0.0521
trigger times: 25
Early stopping!
Start to test process.
Loss after 8277750 batches: 0.0531
Time to train on one home:  52.048367500305176
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124]]
Round_5_results:  [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124]
trigger times: 0
Loss after 8278712 batches: 0.0695
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 8633 < 8634; dropping {'Training_Loss': 0.06947267161948341, 'Validation_Loss': 0.08687984943389893, 'Training_R2': -0.11548694169356422, 'Validation_R2': 0.11276360140348773, 'Training_F1': 0.4158947114205882, 'Validation_F1': 0.5786580031459705, 'Training_NEP': 0.8453118151378299, 'Validation_NEP': 0.8112013943979605, 'Training_NDE': 0.6793585979679786, 'Validation_NDE': 0.8015970023114117, 'Training_MAE': 32.12212094576009, 'Validation_MAE': 28.845732433039966, 'Training_MSE': 2509.1338, 'Validation_MSE': 10500.931}.
trigger times: 1
Loss after 8279674 batches: 0.0654
trigger times: 2
Loss after 8280636 batches: 0.0637
trigger times: 3
Loss after 8281598 batches: 0.0606
trigger times: 4
Loss after 8282560 batches: 0.0597
trigger times: 5
Loss after 8283522 batches: 0.0574
trigger times: 6
Loss after 8284484 batches: 0.0569
trigger times: 7
Loss after 8285446 batches: 0.0588
trigger times: 8
Loss after 8286408 batches: 0.0563
trigger times: 9
Loss after 8287370 batches: 0.0562
trigger times: 10
Loss after 8288332 batches: 0.0558
trigger times: 11
Loss after 8289294 batches: 0.0545
trigger times: 12
Loss after 8290256 batches: 0.0542
trigger times: 13
Loss after 8291218 batches: 0.0545
trigger times: 14
Loss after 8292180 batches: 0.0548
trigger times: 15
Loss after 8293142 batches: 0.0547
trigger times: 16
Loss after 8294104 batches: 0.0538
trigger times: 17
Loss after 8295066 batches: 0.0542
trigger times: 18
Loss after 8296028 batches: 0.0536
trigger times: 19
Loss after 8296990 batches: 0.0540
trigger times: 20
Loss after 8297952 batches: 0.0524
trigger times: 21
Loss after 8298914 batches: 0.0534
trigger times: 22
Loss after 8299876 batches: 0.0523
trigger times: 23
Loss after 8300838 batches: 0.0530
trigger times: 24
Loss after 8301800 batches: 0.0520
trigger times: 25
Early stopping!
Start to test process.
Loss after 8302762 batches: 0.0515
Time to train on one home:  52.40926504135132
trigger times: 0
Loss after 8303691 batches: 0.1256
trigger times: 1
Loss after 8304620 batches: 0.0726
trigger times: 2
Loss after 8305549 batches: 0.0595
trigger times: 3
Loss after 8306478 batches: 0.0508
trigger times: 4
Loss after 8307407 batches: 0.0428
trigger times: 5
Loss after 8308336 batches: 0.0418
trigger times: 6
Loss after 8309265 batches: 0.0374
trigger times: 7
Loss after 8310194 batches: 0.0361
trigger times: 8
Loss after 8311123 batches: 0.0359
trigger times: 9
Loss after 8312052 batches: 0.0383
trigger times: 10
Loss after 8312981 batches: 0.0353
trigger times: 11
Loss after 8313910 batches: 0.0351
trigger times: 12
Loss after 8314839 batches: 0.0342
trigger times: 13
Loss after 8315768 batches: 0.0325
trigger times: 14
Loss after 8316697 batches: 0.0319
trigger times: 15
Loss after 8317626 batches: 0.0321
trigger times: 16
Loss after 8318555 batches: 0.0329
trigger times: 17
Loss after 8319484 batches: 0.0328
trigger times: 18
Loss after 8320413 batches: 0.0297
trigger times: 19
Loss after 8321342 batches: 0.0301
trigger times: 20
Loss after 8322271 batches: 0.0329
trigger times: 21
Loss after 8323200 batches: 0.0344
trigger times: 22
Loss after 8324129 batches: 0.0352
trigger times: 23
Loss after 8325058 batches: 0.0364
trigger times: 24
Loss after 8325987 batches: 0.0335
trigger times: 25
Early stopping!
Start to test process.
Loss after 8326916 batches: 0.0352
Time to train on one home:  51.906362533569336
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 8327879 batches: 0.0321
trigger times: 1
Loss after 8328842 batches: 0.0168
trigger times: 2
Loss after 8329805 batches: 0.0140
trigger times: 3
Loss after 8330768 batches: 0.0139
trigger times: 4
Loss after 8331731 batches: 0.0137
trigger times: 5
Loss after 8332694 batches: 0.0134
trigger times: 6
Loss after 8333657 batches: 0.0127
trigger times: 7
Loss after 8334620 batches: 0.0123
trigger times: 8
Loss after 8335583 batches: 0.0117
trigger times: 9
Loss after 8336546 batches: 0.0109
trigger times: 10
Loss after 8337509 batches: 0.0110
trigger times: 11
Loss after 8338472 batches: 0.0102
trigger times: 12
Loss after 8339435 batches: 0.0098
trigger times: 13
Loss after 8340398 batches: 0.0097
trigger times: 14
Loss after 8341361 batches: 0.0093
trigger times: 15
Loss after 8342324 batches: 0.0090
trigger times: 16
Loss after 8343287 batches: 0.0087
trigger times: 17
Loss after 8344250 batches: 0.0084
trigger times: 18
Loss after 8345213 batches: 0.0082
trigger times: 19
Loss after 8346176 batches: 0.0079
trigger times: 20
Loss after 8347139 batches: 0.0076
trigger times: 21
Loss after 8348102 batches: 0.0077
trigger times: 22
Loss after 8349065 batches: 0.0077
trigger times: 23
Loss after 8350028 batches: 0.0078
trigger times: 24
Loss after 8350991 batches: 0.0078
trigger times: 25
Early stopping!
Start to test process.
Loss after 8351954 batches: 0.0075
Time to train on one home:  52.704076528549194
trigger times: 0
Loss after 8352917 batches: 0.0262
trigger times: 0
Loss after 8353880 batches: 0.0228
trigger times: 1
Loss after 8354843 batches: 0.0212
trigger times: 2
Loss after 8355806 batches: 0.0202
trigger times: 3
Loss after 8356769 batches: 0.0190
trigger times: 0
Loss after 8357732 batches: 0.0176
trigger times: 1
Loss after 8358695 batches: 0.0174
trigger times: 2
Loss after 8359658 batches: 0.0169
trigger times: 3
Loss after 8360621 batches: 0.0168
trigger times: 4
Loss after 8361584 batches: 0.0162
trigger times: 5
Loss after 8362547 batches: 0.0160
trigger times: 6
Loss after 8363510 batches: 0.0160
trigger times: 7
Loss after 8364473 batches: 0.0155
trigger times: 8
Loss after 8365436 batches: 0.0157
trigger times: 9
Loss after 8366399 batches: 0.0152
trigger times: 10
Loss after 8367362 batches: 0.0153
trigger times: 11
Loss after 8368325 batches: 0.0153
trigger times: 12
Loss after 8369288 batches: 0.0156
trigger times: 0
Loss after 8370251 batches: 0.0152
trigger times: 1
Loss after 8371214 batches: 0.0155
trigger times: 2
Loss after 8372177 batches: 0.0152
trigger times: 3
Loss after 8373140 batches: 0.0152
trigger times: 4
Loss after 8374103 batches: 0.0152
trigger times: 5
Loss after 8375066 batches: 0.0150
trigger times: 6
Loss after 8376029 batches: 0.0141
trigger times: 7
Loss after 8376992 batches: 0.0147
trigger times: 8
Loss after 8377955 batches: 0.0143
trigger times: 9
Loss after 8378918 batches: 0.0141
trigger times: 10
Loss after 8379881 batches: 0.0140
trigger times: 11
Loss after 8380844 batches: 0.0148
trigger times: 12
Loss after 8381807 batches: 0.0138
trigger times: 13
Loss after 8382770 batches: 0.0136
trigger times: 14
Loss after 8383733 batches: 0.0136
trigger times: 15
Loss after 8384696 batches: 0.0128
trigger times: 16
Loss after 8385659 batches: 0.0135
trigger times: 17
Loss after 8386622 batches: 0.0135
trigger times: 18
Loss after 8387585 batches: 0.0131
trigger times: 19
Loss after 8388548 batches: 0.0135
trigger times: 20
Loss after 8389511 batches: 0.0130
trigger times: 21
Loss after 8390474 batches: 0.0131
trigger times: 22
Loss after 8391437 batches: 0.0123
trigger times: 23
Loss after 8392400 batches: 0.0126
trigger times: 24
Loss after 8393363 batches: 0.0129
trigger times: 25
Early stopping!
Start to test process.
Loss after 8394326 batches: 0.0123
Time to train on one home:  65.67617416381836
trigger times: 0
Loss after 8395289 batches: 0.0991
trigger times: 1
Loss after 8396252 batches: 0.0921
trigger times: 2
Loss after 8397215 batches: 0.0873
trigger times: 3
Loss after 8398178 batches: 0.0841
trigger times: 4
Loss after 8399141 batches: 0.0806
trigger times: 5
Loss after 8400104 batches: 0.0787
trigger times: 6
Loss after 8401067 batches: 0.0780
trigger times: 7
Loss after 8402030 batches: 0.0770
trigger times: 8
Loss after 8402993 batches: 0.0746
trigger times: 9
Loss after 8403956 batches: 0.0744
trigger times: 10
Loss after 8404919 batches: 0.0738
trigger times: 11
Loss after 8405882 batches: 0.0728
trigger times: 12
Loss after 8406845 batches: 0.0721
trigger times: 13
Loss after 8407808 batches: 0.0704
trigger times: 14
Loss after 8408771 batches: 0.0713
trigger times: 15
Loss after 8409734 batches: 0.0703
trigger times: 16
Loss after 8410697 batches: 0.0685
trigger times: 17
Loss after 8411660 batches: 0.0689
trigger times: 18
Loss after 8412623 batches: 0.0689
trigger times: 19
Loss after 8413586 batches: 0.0682
trigger times: 20
Loss after 8414549 batches: 0.0673
trigger times: 21
Loss after 8415512 batches: 0.0672
trigger times: 22
Loss after 8416475 batches: 0.0665
trigger times: 23
Loss after 8417438 batches: 0.0662
trigger times: 24
Loss after 8418401 batches: 0.0666
trigger times: 25
Early stopping!
Start to test process.
Loss after 8419364 batches: 0.0663
Time to train on one home:  52.06929564476013
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8420327 batches: 0.0800
trigger times: 1
Loss after 8421290 batches: 0.0747
trigger times: 2
Loss after 8422253 batches: 0.0725
trigger times: 3
Loss after 8423216 batches: 0.0684
trigger times: 4
Loss after 8424179 batches: 0.0673
trigger times: 5
Loss after 8425142 batches: 0.0670
trigger times: 6
Loss after 8426105 batches: 0.0656
trigger times: 7
Loss after 8427068 batches: 0.0647
trigger times: 8
Loss after 8428031 batches: 0.0643
trigger times: 9
Loss after 8428994 batches: 0.0635
trigger times: 10
Loss after 8429957 batches: 0.0634
trigger times: 11
Loss after 8430920 batches: 0.0628
trigger times: 12
Loss after 8431883 batches: 0.0620
trigger times: 13
Loss after 8432846 batches: 0.0626
trigger times: 14
Loss after 8433809 batches: 0.0613
trigger times: 15
Loss after 8434772 batches: 0.0611
trigger times: 16
Loss after 8435735 batches: 0.0618
trigger times: 17
Loss after 8436698 batches: 0.0608
trigger times: 18
Loss after 8437661 batches: 0.0603
trigger times: 19
Loss after 8438624 batches: 0.0599
trigger times: 20
Loss after 8439587 batches: 0.0615
trigger times: 21
Loss after 8440550 batches: 0.0600
trigger times: 22
Loss after 8441513 batches: 0.0592
trigger times: 23
Loss after 8442476 batches: 0.0581
trigger times: 24
Loss after 8443439 batches: 0.0585
trigger times: 25
Early stopping!
Start to test process.
Loss after 8444402 batches: 0.0586
Time to train on one home:  52.17522859573364
trigger times: 0
Loss after 8445365 batches: 0.0756
trigger times: 1
Loss after 8446328 batches: 0.0705
trigger times: 2
Loss after 8447291 batches: 0.0694
trigger times: 3
Loss after 8448254 batches: 0.0673
trigger times: 4
Loss after 8449217 batches: 0.0648
trigger times: 5
Loss after 8450180 batches: 0.0626
trigger times: 6
Loss after 8451143 batches: 0.0620
trigger times: 7
Loss after 8452106 batches: 0.0613
trigger times: 8
Loss after 8453069 batches: 0.0613
trigger times: 9
Loss after 8454032 batches: 0.0609
trigger times: 10
Loss after 8454995 batches: 0.0594
trigger times: 11
Loss after 8455958 batches: 0.0604
trigger times: 12
Loss after 8456921 batches: 0.0588
trigger times: 13
Loss after 8457884 batches: 0.0596
trigger times: 14
Loss after 8458847 batches: 0.0581
trigger times: 15
Loss after 8459810 batches: 0.0580
trigger times: 16
Loss after 8460773 batches: 0.0570
trigger times: 17
Loss after 8461736 batches: 0.0559
trigger times: 18
Loss after 8462699 batches: 0.0562
trigger times: 19
Loss after 8463662 batches: 0.0560
trigger times: 20
Loss after 8464625 batches: 0.0556
trigger times: 21
Loss after 8465588 batches: 0.0569
trigger times: 22
Loss after 8466551 batches: 0.0555
trigger times: 23
Loss after 8467514 batches: 0.0545
trigger times: 24
Loss after 8468477 batches: 0.0541
trigger times: 25
Early stopping!
Start to test process.
Loss after 8469440 batches: 0.0540
Time to train on one home:  52.238200664520264
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8470403 batches: 0.0624
trigger times: 1
Loss after 8471366 batches: 0.0523
trigger times: 2
Loss after 8472329 batches: 0.0513
trigger times: 3
Loss after 8473292 batches: 0.0467
trigger times: 4
Loss after 8474255 batches: 0.0393
trigger times: 5
Loss after 8475218 batches: 0.0369
trigger times: 6
Loss after 8476181 batches: 0.0351
trigger times: 7
Loss after 8477144 batches: 0.0335
trigger times: 8
Loss after 8478107 batches: 0.0326
trigger times: 9
Loss after 8479070 batches: 0.0312
trigger times: 10
Loss after 8480033 batches: 0.0306
trigger times: 11
Loss after 8480996 batches: 0.0294
trigger times: 12
Loss after 8481959 batches: 0.0292
trigger times: 13
Loss after 8482922 batches: 0.0283
trigger times: 14
Loss after 8483885 batches: 0.0291
trigger times: 15
Loss after 8484848 batches: 0.0286
trigger times: 16
Loss after 8485811 batches: 0.0281
trigger times: 17
Loss after 8486774 batches: 0.0278
trigger times: 18
Loss after 8487737 batches: 0.0274
trigger times: 19
Loss after 8488700 batches: 0.0280
trigger times: 20
Loss after 8489663 batches: 0.0267
trigger times: 21
Loss after 8490626 batches: 0.0269
trigger times: 22
Loss after 8491589 batches: 0.0267
trigger times: 23
Loss after 8492552 batches: 0.0265
trigger times: 24
Loss after 8493515 batches: 0.0268
trigger times: 25
Early stopping!
Start to test process.
Loss after 8494478 batches: 0.0263
Time to train on one home:  52.40814471244812
trigger times: 0
Loss after 8495436 batches: 0.0910
trigger times: 1
Loss after 8496394 batches: 0.0501
trigger times: 2
Loss after 8497352 batches: 0.0424
trigger times: 3
Loss after 8498310 batches: 0.0359
trigger times: 4
Loss after 8499268 batches: 0.0328
trigger times: 5
Loss after 8500226 batches: 0.0309
trigger times: 6
Loss after 8501184 batches: 0.0320
trigger times: 7
Loss after 8502142 batches: 0.0297
trigger times: 8
Loss after 8503100 batches: 0.0317
trigger times: 9
Loss after 8504058 batches: 0.0287
trigger times: 10
Loss after 8505016 batches: 0.0267
trigger times: 11
Loss after 8505974 batches: 0.0251
trigger times: 12
Loss after 8506932 batches: 0.0251
trigger times: 13
Loss after 8507890 batches: 0.0238
trigger times: 14
Loss after 8508848 batches: 0.0230
trigger times: 15
Loss after 8509806 batches: 0.0237
trigger times: 16
Loss after 8510764 batches: 0.0230
trigger times: 17
Loss after 8511722 batches: 0.0242
trigger times: 18
Loss after 8512680 batches: 0.0256
trigger times: 19
Loss after 8513638 batches: 0.0254
trigger times: 20
Loss after 8514596 batches: 0.0229
trigger times: 21
Loss after 8515554 batches: 0.0234
trigger times: 22
Loss after 8516512 batches: 0.0236
trigger times: 23
Loss after 8517470 batches: 0.0232
trigger times: 24
Loss after 8518428 batches: 0.0232
trigger times: 25
Early stopping!
Start to test process.
Loss after 8519386 batches: 0.0226
Time to train on one home:  52.448126554489136
trigger times: 0
Loss after 8520348 batches: 0.0700
trigger times: 1
Loss after 8521310 batches: 0.0651
trigger times: 2
Loss after 8522272 batches: 0.0630
trigger times: 3
Loss after 8523234 batches: 0.0612
trigger times: 4
Loss after 8524196 batches: 0.0586
trigger times: 5
Loss after 8525158 batches: 0.0581
trigger times: 6
Loss after 8526120 batches: 0.0575
trigger times: 7
Loss after 8527082 batches: 0.0564
trigger times: 8
Loss after 8528044 batches: 0.0558
trigger times: 9
Loss after 8529006 batches: 0.0542
trigger times: 10
Loss after 8529968 batches: 0.0552
trigger times: 11
Loss after 8530930 batches: 0.0546
trigger times: 12
Loss after 8531892 batches: 0.0534
trigger times: 13
Loss after 8532854 batches: 0.0533
trigger times: 14
Loss after 8533816 batches: 0.0534
trigger times: 15
Loss after 8534778 batches: 0.0532
trigger times: 16
Loss after 8535740 batches: 0.0545
trigger times: 17
Loss after 8536702 batches: 0.0540
trigger times: 18
Loss after 8537664 batches: 0.0532
trigger times: 19
Loss after 8538626 batches: 0.0526
trigger times: 20
Loss after 8539588 batches: 0.0522
trigger times: 21
Loss after 8540550 batches: 0.0528
trigger times: 22
Loss after 8541512 batches: 0.0522
trigger times: 23
Loss after 8542474 batches: 0.0516
trigger times: 24
Loss after 8543436 batches: 0.0507
trigger times: 25
Early stopping!
Start to test process.
Loss after 8544398 batches: 0.0525
Time to train on one home:  52.233187437057495
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8545361 batches: 0.0660
trigger times: 1
Loss after 8546324 batches: 0.0256
trigger times: 2
Loss after 8547287 batches: 0.0215
trigger times: 3
Loss after 8548250 batches: 0.0178
trigger times: 4
Loss after 8549213 batches: 0.0162
trigger times: 5
Loss after 8550176 batches: 0.0154
trigger times: 6
Loss after 8551139 batches: 0.0144
trigger times: 7
Loss after 8552102 batches: 0.0143
trigger times: 8
Loss after 8553065 batches: 0.0139
trigger times: 9
Loss after 8554028 batches: 0.0140
trigger times: 10
Loss after 8554991 batches: 0.0138
trigger times: 11
Loss after 8555954 batches: 0.0137
trigger times: 12
Loss after 8556917 batches: 0.0133
trigger times: 13
Loss after 8557880 batches: 0.0132
trigger times: 14
Loss after 8558843 batches: 0.0131
trigger times: 15
Loss after 8559806 batches: 0.0132
trigger times: 16
Loss after 8560769 batches: 0.0132
trigger times: 17
Loss after 8561732 batches: 0.0132
trigger times: 18
Loss after 8562695 batches: 0.0129
trigger times: 19
Loss after 8563658 batches: 0.0131
trigger times: 20
Loss after 8564621 batches: 0.0130
trigger times: 21
Loss after 8565584 batches: 0.0127
trigger times: 22
Loss after 8566547 batches: 0.0126
trigger times: 23
Loss after 8567510 batches: 0.0128
trigger times: 24
Loss after 8568473 batches: 0.0125
trigger times: 25
Early stopping!
Start to test process.
Loss after 8569436 batches: 0.0127
Time to train on one home:  52.31515693664551
trigger times: 0
Loss after 8570399 batches: 0.0621
trigger times: 1
Loss after 8571362 batches: 0.0463
trigger times: 2
Loss after 8572325 batches: 0.0439
trigger times: 3
Loss after 8573288 batches: 0.0417
trigger times: 4
Loss after 8574251 batches: 0.0387
trigger times: 5
Loss after 8575214 batches: 0.0374
trigger times: 6
Loss after 8576177 batches: 0.0362
trigger times: 7
Loss after 8577140 batches: 0.0363
trigger times: 8
Loss after 8578103 batches: 0.0366
trigger times: 9
Loss after 8579066 batches: 0.0365
trigger times: 10
Loss after 8580029 batches: 0.0343
trigger times: 11
Loss after 8580992 batches: 0.0341
trigger times: 12
Loss after 8581955 batches: 0.0338
trigger times: 13
Loss after 8582918 batches: 0.0346
trigger times: 14
Loss after 8583881 batches: 0.0349
trigger times: 15
Loss after 8584844 batches: 0.0344
trigger times: 16
Loss after 8585807 batches: 0.0339
trigger times: 17
Loss after 8586770 batches: 0.0334
trigger times: 18
Loss after 8587733 batches: 0.0323
trigger times: 19
Loss after 8588696 batches: 0.0321
trigger times: 20
Loss after 8589659 batches: 0.0317
trigger times: 21
Loss after 8590622 batches: 0.0317
trigger times: 22
Loss after 8591585 batches: 0.0319
trigger times: 23
Loss after 8592548 batches: 0.0315
trigger times: 24
Loss after 8593511 batches: 0.0314
trigger times: 25
Early stopping!
Start to test process.
Loss after 8594474 batches: 0.0307
Time to train on one home:  52.37393879890442
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8595437 batches: 0.0622
trigger times: 1
Loss after 8596400 batches: 0.0521
trigger times: 2
Loss after 8597363 batches: 0.0509
trigger times: 3
Loss after 8598326 batches: 0.0455
trigger times: 4
Loss after 8599289 batches: 0.0394
trigger times: 5
Loss after 8600252 batches: 0.0368
trigger times: 6
Loss after 8601215 batches: 0.0343
trigger times: 7
Loss after 8602178 batches: 0.0329
trigger times: 8
Loss after 8603141 batches: 0.0318
trigger times: 9
Loss after 8604104 batches: 0.0309
trigger times: 10
Loss after 8605067 batches: 0.0305
trigger times: 11
Loss after 8606030 batches: 0.0295
trigger times: 12
Loss after 8606993 batches: 0.0296
trigger times: 13
Loss after 8607956 batches: 0.0285
trigger times: 14
Loss after 8608919 batches: 0.0289
trigger times: 15
Loss after 8609882 batches: 0.0289
trigger times: 16
Loss after 8610845 batches: 0.0281
trigger times: 17
Loss after 8611808 batches: 0.0287
trigger times: 18
Loss after 8612771 batches: 0.0287
trigger times: 19
Loss after 8613734 batches: 0.0285
trigger times: 20
Loss after 8614697 batches: 0.0271
trigger times: 21
Loss after 8615660 batches: 0.0276
trigger times: 22
Loss after 8616623 batches: 0.0271
trigger times: 23
Loss after 8617586 batches: 0.0264
trigger times: 24
Loss after 8618549 batches: 0.0263
trigger times: 25
Early stopping!
Start to test process.
Loss after 8619512 batches: 0.0268
Time to train on one home:  52.533366441726685
trigger times: 0
Loss after 8620475 batches: 0.0621
trigger times: 1
Loss after 8621438 batches: 0.0475
trigger times: 2
Loss after 8622401 batches: 0.0430
trigger times: 3
Loss after 8623364 batches: 0.0406
trigger times: 4
Loss after 8624327 batches: 0.0379
trigger times: 5
Loss after 8625290 batches: 0.0379
trigger times: 6
Loss after 8626253 batches: 0.0369
trigger times: 7
Loss after 8627216 batches: 0.0367
trigger times: 8
Loss after 8628179 batches: 0.0355
trigger times: 9
Loss after 8629142 batches: 0.0349
trigger times: 10
Loss after 8630105 batches: 0.0356
trigger times: 11
Loss after 8631068 batches: 0.0350
trigger times: 12
Loss after 8632031 batches: 0.0337
trigger times: 13
Loss after 8632994 batches: 0.0334
trigger times: 14
Loss after 8633957 batches: 0.0329
trigger times: 15
Loss after 8634920 batches: 0.0342
trigger times: 16
Loss after 8635883 batches: 0.0343
trigger times: 17
Loss after 8636846 batches: 0.0356
trigger times: 18
Loss after 8637809 batches: 0.0334
trigger times: 19
Loss after 8638772 batches: 0.0338
trigger times: 20
Loss after 8639735 batches: 0.0326
trigger times: 21
Loss after 8640698 batches: 0.0320
trigger times: 22
Loss after 8641661 batches: 0.0337
trigger times: 23
Loss after 8642624 batches: 0.0322
trigger times: 24
Loss after 8643587 batches: 0.0324
trigger times: 25
Early stopping!
Start to test process.
Loss after 8644550 batches: 0.0316
Time to train on one home:  52.79868793487549
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8645513 batches: 0.0561
trigger times: 1
Loss after 8646476 batches: 0.0497
trigger times: 2
Loss after 8647439 batches: 0.0472
trigger times: 3
Loss after 8648402 batches: 0.0448
trigger times: 0
Loss after 8649365 batches: 0.0430
trigger times: 0
Loss after 8650328 batches: 0.0416
trigger times: 1
Loss after 8651291 batches: 0.0401
trigger times: 2
Loss after 8652254 batches: 0.0399
trigger times: 3
Loss after 8653217 batches: 0.0389
trigger times: 4
Loss after 8654180 batches: 0.0389
trigger times: 5
Loss after 8655143 batches: 0.0374
trigger times: 6
Loss after 8656106 batches: 0.0370
trigger times: 7
Loss after 8657069 batches: 0.0370
trigger times: 8
Loss after 8658032 batches: 0.0361
trigger times: 9
Loss after 8658995 batches: 0.0364
trigger times: 10
Loss after 8659958 batches: 0.0354
trigger times: 0
Loss after 8660921 batches: 0.0352
trigger times: 0
Loss after 8661884 batches: 0.0357
trigger times: 1
Loss after 8662847 batches: 0.0344
trigger times: 2
Loss after 8663810 batches: 0.0349
trigger times: 0
Loss after 8664773 batches: 0.0341
trigger times: 1
Loss after 8665736 batches: 0.0338
trigger times: 2
Loss after 8666699 batches: 0.0338
trigger times: 3
Loss after 8667662 batches: 0.0345
trigger times: 4
Loss after 8668625 batches: 0.0341
trigger times: 5
Loss after 8669588 batches: 0.0336
trigger times: 6
Loss after 8670551 batches: 0.0329
trigger times: 7
Loss after 8671514 batches: 0.0336
trigger times: 8
Loss after 8672477 batches: 0.0329
trigger times: 9
Loss after 8673440 batches: 0.0320
trigger times: 10
Loss after 8674403 batches: 0.0324
trigger times: 11
Loss after 8675366 batches: 0.0318
trigger times: 12
Loss after 8676329 batches: 0.0316
trigger times: 13
Loss after 8677292 batches: 0.0332
trigger times: 14
Loss after 8678255 batches: 0.0339
trigger times: 15
Loss after 8679218 batches: 0.0323
trigger times: 16
Loss after 8680181 batches: 0.0310
trigger times: 17
Loss after 8681144 batches: 0.0316
trigger times: 18
Loss after 8682107 batches: 0.0319
trigger times: 19
Loss after 8683070 batches: 0.0316
trigger times: 20
Loss after 8684033 batches: 0.0316
trigger times: 21
Loss after 8684996 batches: 0.0313
trigger times: 22
Loss after 8685959 batches: 0.0298
trigger times: 23
Loss after 8686922 batches: 0.0310
trigger times: 24
Loss after 8687885 batches: 0.0303
trigger times: 25
Early stopping!
Start to test process.
Loss after 8688848 batches: 0.0300
Time to train on one home:  67.46669721603394
trigger times: 0
Loss after 8689811 batches: 0.0656
trigger times: 1
Loss after 8690774 batches: 0.0475
trigger times: 2
Loss after 8691737 batches: 0.0469
trigger times: 3
Loss after 8692700 batches: 0.0448
trigger times: 4
Loss after 8693663 batches: 0.0419
trigger times: 5
Loss after 8694626 batches: 0.0410
trigger times: 6
Loss after 8695589 batches: 0.0399
trigger times: 7
Loss after 8696552 batches: 0.0397
trigger times: 8
Loss after 8697515 batches: 0.0387
trigger times: 9
Loss after 8698478 batches: 0.0380
trigger times: 10
Loss after 8699441 batches: 0.0378
trigger times: 11
Loss after 8700404 batches: 0.0379
trigger times: 12
Loss after 8701367 batches: 0.0370
trigger times: 13
Loss after 8702330 batches: 0.0364
trigger times: 14
Loss after 8703293 batches: 0.0367
trigger times: 15
Loss after 8704256 batches: 0.0370
trigger times: 16
Loss after 8705219 batches: 0.0373
trigger times: 17
Loss after 8706182 batches: 0.0362
trigger times: 18
Loss after 8707145 batches: 0.0358
trigger times: 19
Loss after 8708108 batches: 0.0364
trigger times: 20
Loss after 8709071 batches: 0.0356
trigger times: 21
Loss after 8710034 batches: 0.0357
trigger times: 22
Loss after 8710997 batches: 0.0350
trigger times: 23
Loss after 8711960 batches: 0.0354
trigger times: 24
Loss after 8712923 batches: 0.0354
trigger times: 25
Early stopping!
Start to test process.
Loss after 8713886 batches: 0.0350
Time to train on one home:  52.28282833099365
trigger times: 0
Loss after 8714849 batches: 0.0995
trigger times: 1
Loss after 8715812 batches: 0.0904
trigger times: 2
Loss after 8716775 batches: 0.0863
trigger times: 3
Loss after 8717738 batches: 0.0844
trigger times: 4
Loss after 8718701 batches: 0.0805
trigger times: 5
Loss after 8719664 batches: 0.0795
trigger times: 6
Loss after 8720627 batches: 0.0774
trigger times: 7
Loss after 8721590 batches: 0.0756
trigger times: 8
Loss after 8722553 batches: 0.0743
trigger times: 9
Loss after 8723516 batches: 0.0728
trigger times: 10
Loss after 8724479 batches: 0.0743
trigger times: 11
Loss after 8725442 batches: 0.0726
trigger times: 12
Loss after 8726405 batches: 0.0713
trigger times: 13
Loss after 8727368 batches: 0.0715
trigger times: 14
Loss after 8728331 batches: 0.0697
trigger times: 15
Loss after 8729294 batches: 0.0694
trigger times: 16
Loss after 8730257 batches: 0.0695
trigger times: 17
Loss after 8731220 batches: 0.0696
trigger times: 18
Loss after 8732183 batches: 0.0694
trigger times: 19
Loss after 8733146 batches: 0.0661
trigger times: 20
Loss after 8734109 batches: 0.0666
trigger times: 21
Loss after 8735072 batches: 0.0676
trigger times: 22
Loss after 8736035 batches: 0.0664
trigger times: 23
Loss after 8736998 batches: 0.0667
trigger times: 24
Loss after 8737961 batches: 0.0658
trigger times: 25
Early stopping!
Start to test process.
Loss after 8738924 batches: 0.0665
Time to train on one home:  52.42947864532471
trigger times: 0
Loss after 8739887 batches: 0.1017
trigger times: 1
Loss after 8740850 batches: 0.0711
trigger times: 2
Loss after 8741813 batches: 0.0640
trigger times: 3
Loss after 8742776 batches: 0.0570
trigger times: 4
Loss after 8743739 batches: 0.0549
trigger times: 5
Loss after 8744702 batches: 0.0528
trigger times: 6
Loss after 8745665 batches: 0.0502
trigger times: 7
Loss after 8746628 batches: 0.0487
trigger times: 8
Loss after 8747591 batches: 0.0467
trigger times: 9
Loss after 8748554 batches: 0.0467
trigger times: 0
Loss after 8749517 batches: 0.0469
trigger times: 1
Loss after 8750480 batches: 0.0461
trigger times: 2
Loss after 8751443 batches: 0.0450
trigger times: 3
Loss after 8752406 batches: 0.0446
trigger times: 4
Loss after 8753369 batches: 0.0446
trigger times: 5
Loss after 8754332 batches: 0.0433
trigger times: 6
Loss after 8755295 batches: 0.0425
trigger times: 7
Loss after 8756258 batches: 0.0421
trigger times: 8
Loss after 8757221 batches: 0.0418
trigger times: 9
Loss after 8758184 batches: 0.0424
trigger times: 10
Loss after 8759147 batches: 0.0419
trigger times: 11
Loss after 8760110 batches: 0.0405
trigger times: 12
Loss after 8761073 batches: 0.0403
trigger times: 13
Loss after 8762036 batches: 0.0408
trigger times: 14
Loss after 8762999 batches: 0.0404
trigger times: 15
Loss after 8763962 batches: 0.0408
trigger times: 16
Loss after 8764925 batches: 0.0397
trigger times: 17
Loss after 8765888 batches: 0.0390
trigger times: 18
Loss after 8766851 batches: 0.0394
trigger times: 19
Loss after 8767814 batches: 0.0395
trigger times: 20
Loss after 8768777 batches: 0.0394
trigger times: 21
Loss after 8769740 batches: 0.0403
trigger times: 22
Loss after 8770703 batches: 0.0398
trigger times: 23
Loss after 8771666 batches: 0.0384
trigger times: 24
Loss after 8772629 batches: 0.0390
trigger times: 25
Early stopping!
Start to test process.
Loss after 8773592 batches: 0.0396
Time to train on one home:  59.82013010978699
trigger times: 0
Loss after 8774521 batches: 0.1231
trigger times: 1
Loss after 8775450 batches: 0.0705
trigger times: 2
Loss after 8776379 batches: 0.0582
trigger times: 3
Loss after 8777308 batches: 0.0494
trigger times: 4
Loss after 8778237 batches: 0.0441
trigger times: 5
Loss after 8779166 batches: 0.0412
trigger times: 6
Loss after 8780095 batches: 0.0385
trigger times: 7
Loss after 8781024 batches: 0.0369
trigger times: 8
Loss after 8781953 batches: 0.0378
trigger times: 9
Loss after 8782882 batches: 0.0371
trigger times: 10
Loss after 8783811 batches: 0.0330
trigger times: 11
Loss after 8784740 batches: 0.0333
trigger times: 12
Loss after 8785669 batches: 0.0337
trigger times: 13
Loss after 8786598 batches: 0.0340
trigger times: 14
Loss after 8787527 batches: 0.0324
trigger times: 15
Loss after 8788456 batches: 0.0326
trigger times: 16
Loss after 8789385 batches: 0.0338
trigger times: 17
Loss after 8790314 batches: 0.0314
trigger times: 18
Loss after 8791243 batches: 0.0304
trigger times: 19
Loss after 8792172 batches: 0.0309
trigger times: 20
Loss after 8793101 batches: 0.0313
trigger times: 21
Loss after 8794030 batches: 0.0316
trigger times: 22
Loss after 8794959 batches: 0.0334
trigger times: 23
Loss after 8795888 batches: 0.0329
trigger times: 24
Loss after 8796817 batches: 0.0394
trigger times: 25
Early stopping!
Start to test process.
Loss after 8797746 batches: 0.0335
Time to train on one home:  51.96213412284851
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8798709 batches: 0.0548
trigger times: 1
Loss after 8799672 batches: 0.0283
trigger times: 2
Loss after 8800635 batches: 0.0274
trigger times: 3
Loss after 8801598 batches: 0.0271
trigger times: 4
Loss after 8802561 batches: 0.0265
trigger times: 5
Loss after 8803524 batches: 0.0253
trigger times: 6
Loss after 8804487 batches: 0.0246
trigger times: 7
Loss after 8805450 batches: 0.0237
trigger times: 8
Loss after 8806413 batches: 0.0229
trigger times: 9
Loss after 8807376 batches: 0.0223
trigger times: 10
Loss after 8808339 batches: 0.0220
trigger times: 11
Loss after 8809302 batches: 0.0213
trigger times: 12
Loss after 8810265 batches: 0.0209
trigger times: 13
Loss after 8811228 batches: 0.0207
trigger times: 14
Loss after 8812191 batches: 0.0202
trigger times: 15
Loss after 8813154 batches: 0.0202
trigger times: 16
Loss after 8814117 batches: 0.0197
trigger times: 17
Loss after 8815080 batches: 0.0200
trigger times: 18
Loss after 8816043 batches: 0.0195
trigger times: 19
Loss after 8817006 batches: 0.0193
trigger times: 20
Loss after 8817969 batches: 0.0191
trigger times: 21
Loss after 8818932 batches: 0.0191
trigger times: 22
Loss after 8819895 batches: 0.0188
trigger times: 23
Loss after 8820858 batches: 0.0187
trigger times: 24
Loss after 8821821 batches: 0.0186
trigger times: 25
Early stopping!
Start to test process.
Loss after 8822784 batches: 0.0184
Time to train on one home:  52.43081259727478
trigger times: 0
Loss after 8823747 batches: 0.1604
trigger times: 1
Loss after 8824710 batches: 0.1147
trigger times: 2
Loss after 8825673 batches: 0.1033
trigger times: 3
Loss after 8826636 batches: 0.0884
trigger times: 4
Loss after 8827599 batches: 0.0840
trigger times: 5
Loss after 8828562 batches: 0.0800
trigger times: 6
Loss after 8829525 batches: 0.0786
trigger times: 7
Loss after 8830488 batches: 0.0763
trigger times: 8
Loss after 8831451 batches: 0.0750
trigger times: 9
Loss after 8832414 batches: 0.0740
trigger times: 10
Loss after 8833377 batches: 0.0724
trigger times: 11
Loss after 8834340 batches: 0.0689
trigger times: 12
Loss after 8835303 batches: 0.0666
trigger times: 13
Loss after 8836266 batches: 0.0634
trigger times: 14
Loss after 8837229 batches: 0.0621
trigger times: 15
Loss after 8838192 batches: 0.0583
trigger times: 16
Loss after 8839155 batches: 0.0557
trigger times: 17
Loss after 8840118 batches: 0.0547
trigger times: 18
Loss after 8841081 batches: 0.0547
trigger times: 19
Loss after 8842044 batches: 0.0532
trigger times: 20
Loss after 8843007 batches: 0.0512
trigger times: 21
Loss after 8843970 batches: 0.0507
trigger times: 22
Loss after 8844933 batches: 0.0511
trigger times: 23
Loss after 8845896 batches: 0.0499
trigger times: 24
Loss after 8846859 batches: 0.0484
trigger times: 25
Early stopping!
Start to test process.
Loss after 8847822 batches: 0.0481
Time to train on one home:  52.14981651306152
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8848785 batches: 0.0798
trigger times: 1
Loss after 8849748 batches: 0.0745
trigger times: 2
Loss after 8850711 batches: 0.0714
trigger times: 3
Loss after 8851674 batches: 0.0692
trigger times: 4
Loss after 8852637 batches: 0.0680
trigger times: 5
Loss after 8853600 batches: 0.0669
trigger times: 6
Loss after 8854563 batches: 0.0663
trigger times: 7
Loss after 8855526 batches: 0.0650
trigger times: 8
Loss after 8856489 batches: 0.0637
trigger times: 9
Loss after 8857452 batches: 0.0635
trigger times: 10
Loss after 8858415 batches: 0.0632
trigger times: 11
Loss after 8859378 batches: 0.0625
trigger times: 12
Loss after 8860341 batches: 0.0610
trigger times: 13
Loss after 8861304 batches: 0.0618
trigger times: 14
Loss after 8862267 batches: 0.0626
trigger times: 15
Loss after 8863230 batches: 0.0612
trigger times: 16
Loss after 8864193 batches: 0.0615
trigger times: 17
Loss after 8865156 batches: 0.0607
trigger times: 18
Loss after 8866119 batches: 0.0606
trigger times: 19
Loss after 8867082 batches: 0.0604
trigger times: 20
Loss after 8868045 batches: 0.0592
trigger times: 21
Loss after 8869008 batches: 0.0596
trigger times: 22
Loss after 8869971 batches: 0.0594
trigger times: 23
Loss after 8870934 batches: 0.0597
trigger times: 24
Loss after 8871897 batches: 0.0578
trigger times: 25
Early stopping!
Start to test process.
Loss after 8872860 batches: 0.0578
Time to train on one home:  51.922749042510986
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 8873823 batches: 0.0822
trigger times: 1
Loss after 8874786 batches: 0.0760
trigger times: 2
Loss after 8875749 batches: 0.0747
trigger times: 3
Loss after 8876712 batches: 0.0718
trigger times: 4
Loss after 8877675 batches: 0.0694
trigger times: 5
Loss after 8878638 batches: 0.0662
trigger times: 6
Loss after 8879601 batches: 0.0671
trigger times: 7
Loss after 8880564 batches: 0.0635
trigger times: 8
Loss after 8881527 batches: 0.0645
trigger times: 9
Loss after 8882490 batches: 0.0611
trigger times: 10
Loss after 8883453 batches: 0.0609
trigger times: 11
Loss after 8884416 batches: 0.0605
trigger times: 12
Loss after 8885379 batches: 0.0601
trigger times: 13
Loss after 8886342 batches: 0.0596
trigger times: 14
Loss after 8887305 batches: 0.0597
trigger times: 15
Loss after 8888268 batches: 0.0578
trigger times: 16
Loss after 8889231 batches: 0.0566
trigger times: 17
Loss after 8890194 batches: 0.0560
trigger times: 18
Loss after 8891157 batches: 0.0574
trigger times: 19
Loss after 8892120 batches: 0.0593
trigger times: 20
Loss after 8893083 batches: 0.0561
trigger times: 21
Loss after 8894046 batches: 0.0582
trigger times: 22
Loss after 8895009 batches: 0.0633
trigger times: 23
Loss after 8895972 batches: 0.0628
trigger times: 24
Loss after 8896935 batches: 0.0599
trigger times: 25
Early stopping!
Start to test process.
Loss after 8897898 batches: 0.0599
Time to train on one home:  52.13759183883667
trigger times: 0
Loss after 8898861 batches: 0.0261
trigger times: 0
Loss after 8899824 batches: 0.0238
trigger times: 1
Loss after 8900787 batches: 0.0212
trigger times: 2
Loss after 8901750 batches: 0.0196
trigger times: 3
Loss after 8902713 batches: 0.0184
trigger times: 0
Loss after 8903676 batches: 0.0178
trigger times: 1
Loss after 8904639 batches: 0.0176
trigger times: 2
Loss after 8905602 batches: 0.0172
trigger times: 3
Loss after 8906565 batches: 0.0165
trigger times: 4
Loss after 8907528 batches: 0.0159
trigger times: 5
Loss after 8908491 batches: 0.0160
trigger times: 6
Loss after 8909454 batches: 0.0156
trigger times: 7
Loss after 8910417 batches: 0.0156
trigger times: 8
Loss after 8911380 batches: 0.0154
trigger times: 9
Loss after 8912343 batches: 0.0156
trigger times: 10
Loss after 8913306 batches: 0.0156
trigger times: 11
Loss after 8914269 batches: 0.0158
trigger times: 12
Loss after 8915232 batches: 0.0153
trigger times: 13
Loss after 8916195 batches: 0.0151
trigger times: 14
Loss after 8917158 batches: 0.0149
trigger times: 15
Loss after 8918121 batches: 0.0147
trigger times: 16
Loss after 8919084 batches: 0.0144
trigger times: 17
Loss after 8920047 batches: 0.0142
trigger times: 18
Loss after 8921010 batches: 0.0149
trigger times: 19
Loss after 8921973 batches: 0.0147
trigger times: 20
Loss after 8922936 batches: 0.0149
trigger times: 21
Loss after 8923899 batches: 0.0148
trigger times: 22
Loss after 8924862 batches: 0.0142
trigger times: 23
Loss after 8925825 batches: 0.0147
trigger times: 24
Loss after 8926788 batches: 0.0141
trigger times: 25
Early stopping!
Start to test process.
Loss after 8927751 batches: 0.0137
Time to train on one home:  55.87641954421997
trigger times: 0
Loss after 8928714 batches: 0.0469
trigger times: 1
Loss after 8929677 batches: 0.0412
trigger times: 2
Loss after 8930640 batches: 0.0380
trigger times: 0
Loss after 8931603 batches: 0.0350
trigger times: 1
Loss after 8932566 batches: 0.0320
trigger times: 2
Loss after 8933529 batches: 0.0307
trigger times: 3
Loss after 8934492 batches: 0.0295
trigger times: 4
Loss after 8935455 batches: 0.0297
trigger times: 5
Loss after 8936418 batches: 0.0287
trigger times: 6
Loss after 8937381 batches: 0.0285
trigger times: 7
Loss after 8938344 batches: 0.0279
trigger times: 8
Loss after 8939307 batches: 0.0262
trigger times: 9
Loss after 8940270 batches: 0.0265
trigger times: 10
Loss after 8941233 batches: 0.0252
trigger times: 11
Loss after 8942196 batches: 0.0246
trigger times: 12
Loss after 8943159 batches: 0.0245
trigger times: 13
Loss after 8944122 batches: 0.0243
trigger times: 14
Loss after 8945085 batches: 0.0235
trigger times: 15
Loss after 8946048 batches: 0.0241
trigger times: 16
Loss after 8947011 batches: 0.0233
trigger times: 17
Loss after 8947974 batches: 0.0232
trigger times: 18
Loss after 8948937 batches: 0.0241
trigger times: 19
Loss after 8949900 batches: 0.0241
trigger times: 20
Loss after 8950863 batches: 0.0234
trigger times: 21
Loss after 8951826 batches: 0.0233
trigger times: 22
Loss after 8952789 batches: 0.0230
trigger times: 23
Loss after 8953752 batches: 0.0228
trigger times: 24
Loss after 8954715 batches: 0.0228
trigger times: 25
Early stopping!
Start to test process.
Loss after 8955678 batches: 0.0224
Time to train on one home:  54.88264584541321
trigger times: 0
Loss after 8956641 batches: 0.0920
trigger times: 1
Loss after 8957604 batches: 0.0673
trigger times: 2
Loss after 8958567 batches: 0.0644
trigger times: 3
Loss after 8959530 batches: 0.0570
trigger times: 4
Loss after 8960493 batches: 0.0542
trigger times: 5
Loss after 8961456 batches: 0.0505
trigger times: 6
Loss after 8962419 batches: 0.0499
trigger times: 7
Loss after 8963382 batches: 0.0487
trigger times: 8
Loss after 8964345 batches: 0.0464
trigger times: 9
Loss after 8965308 batches: 0.0466
trigger times: 10
Loss after 8966271 batches: 0.0455
trigger times: 11
Loss after 8967234 batches: 0.0457
trigger times: 12
Loss after 8968197 batches: 0.0452
trigger times: 13
Loss after 8969160 batches: 0.0436
trigger times: 14
Loss after 8970123 batches: 0.0437
trigger times: 15
Loss after 8971086 batches: 0.0429
trigger times: 16
Loss after 8972049 batches: 0.0419
trigger times: 17
Loss after 8973012 batches: 0.0417
trigger times: 18
Loss after 8973975 batches: 0.0429
trigger times: 19
Loss after 8974938 batches: 0.0418
trigger times: 20
Loss after 8975901 batches: 0.0416
trigger times: 21
Loss after 8976864 batches: 0.0419
trigger times: 22
Loss after 8977827 batches: 0.0418
trigger times: 23
Loss after 8978790 batches: 0.0408
trigger times: 24
Loss after 8979753 batches: 0.0400
trigger times: 25
Early stopping!
Start to test process.
Loss after 8980716 batches: 0.0401
Time to train on one home:  51.98546290397644
trigger times: 0
Loss after 8981679 batches: 0.1017
trigger times: 1
Loss after 8982642 batches: 0.0713
trigger times: 2
Loss after 8983605 batches: 0.0628
trigger times: 3
Loss after 8984568 batches: 0.0579
trigger times: 4
Loss after 8985531 batches: 0.0544
trigger times: 5
Loss after 8986494 batches: 0.0521
trigger times: 6
Loss after 8987457 batches: 0.0499
trigger times: 7
Loss after 8988420 batches: 0.0481
trigger times: 8
Loss after 8989383 batches: 0.0469
trigger times: 9
Loss after 8990346 batches: 0.0451
trigger times: 10
Loss after 8991309 batches: 0.0453
trigger times: 11
Loss after 8992272 batches: 0.0450
trigger times: 12
Loss after 8993235 batches: 0.0449
trigger times: 13
Loss after 8994198 batches: 0.0444
trigger times: 14
Loss after 8995161 batches: 0.0434
trigger times: 15
Loss after 8996124 batches: 0.0437
trigger times: 16
Loss after 8997087 batches: 0.0426
trigger times: 17
Loss after 8998050 batches: 0.0433
trigger times: 18
Loss after 8999013 batches: 0.0429
trigger times: 19
Loss after 8999976 batches: 0.0422
trigger times: 20
Loss after 9000939 batches: 0.0416
trigger times: 21
Loss after 9001902 batches: 0.0409
trigger times: 22
Loss after 9002865 batches: 0.0408
trigger times: 23
Loss after 9003828 batches: 0.0403
trigger times: 24
Loss after 9004791 batches: 0.0411
trigger times: 25
Early stopping!
Start to test process.
Loss after 9005754 batches: 0.0398
Time to train on one home:  52.65622663497925
trigger times: 0
Loss after 9006717 batches: 0.0618
trigger times: 1
Loss after 9007680 batches: 0.0458
trigger times: 2
Loss after 9008643 batches: 0.0442
trigger times: 3
Loss after 9009606 batches: 0.0413
trigger times: 4
Loss after 9010569 batches: 0.0391
trigger times: 5
Loss after 9011532 batches: 0.0368
trigger times: 6
Loss after 9012495 batches: 0.0367
trigger times: 7
Loss after 9013458 batches: 0.0360
trigger times: 8
Loss after 9014421 batches: 0.0350
trigger times: 9
Loss after 9015384 batches: 0.0352
trigger times: 10
Loss after 9016347 batches: 0.0345
trigger times: 11
Loss after 9017310 batches: 0.0340
trigger times: 12
Loss after 9018273 batches: 0.0341
trigger times: 13
Loss after 9019236 batches: 0.0338
trigger times: 14
Loss after 9020199 batches: 0.0336
trigger times: 15
Loss after 9021162 batches: 0.0329
trigger times: 16
Loss after 9022125 batches: 0.0344
trigger times: 17
Loss after 9023088 batches: 0.0341
trigger times: 18
Loss after 9024051 batches: 0.0327
trigger times: 19
Loss after 9025014 batches: 0.0322
trigger times: 20
Loss after 9025977 batches: 0.0323
trigger times: 21
Loss after 9026940 batches: 0.0321
trigger times: 22
Loss after 9027903 batches: 0.0326
trigger times: 23
Loss after 9028866 batches: 0.0314
trigger times: 24
Loss after 9029829 batches: 0.0313
trigger times: 25
Early stopping!
Start to test process.
Loss after 9030792 batches: 0.0321
Time to train on one home:  52.3852744102478
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9031755 batches: 0.0992
trigger times: 1
Loss after 9032718 batches: 0.0638
trigger times: 2
Loss after 9033681 batches: 0.0611
trigger times: 0
Loss after 9034644 batches: 0.0514
trigger times: 0
Loss after 9035607 batches: 0.0472
trigger times: 0
Loss after 9036570 batches: 0.0446
trigger times: 1
Loss after 9037533 batches: 0.0420
trigger times: 2
Loss after 9038496 batches: 0.0394
trigger times: 3
Loss after 9039459 batches: 0.0379
trigger times: 0
Loss after 9040422 batches: 0.0364
trigger times: 1
Loss after 9041385 batches: 0.0353
trigger times: 2
Loss after 9042348 batches: 0.0345
trigger times: 3
Loss after 9043311 batches: 0.0347
trigger times: 4
Loss after 9044274 batches: 0.0328
trigger times: 5
Loss after 9045237 batches: 0.0330
trigger times: 0
Loss after 9046200 batches: 0.0333
trigger times: 1
Loss after 9047163 batches: 0.0331
trigger times: 2
Loss after 9048126 batches: 0.0323
trigger times: 3
Loss after 9049089 batches: 0.0320
trigger times: 4
Loss after 9050052 batches: 0.0314
trigger times: 5
Loss after 9051015 batches: 0.0321
trigger times: 6
Loss after 9051978 batches: 0.0306
trigger times: 7
Loss after 9052941 batches: 0.0314
trigger times: 8
Loss after 9053904 batches: 0.0311
trigger times: 9
Loss after 9054867 batches: 0.0300
trigger times: 10
Loss after 9055830 batches: 0.0297
trigger times: 11
Loss after 9056793 batches: 0.0292
trigger times: 12
Loss after 9057756 batches: 0.0287
trigger times: 0
Loss after 9058719 batches: 0.0294
trigger times: 1
Loss after 9059682 batches: 0.0280
trigger times: 2
Loss after 9060645 batches: 0.0290
trigger times: 3
Loss after 9061608 batches: 0.0288
trigger times: 4
Loss after 9062571 batches: 0.0295
trigger times: 5
Loss after 9063534 batches: 0.0276
trigger times: 6
Loss after 9064497 batches: 0.0271
trigger times: 7
Loss after 9065460 batches: 0.0268
trigger times: 8
Loss after 9066423 batches: 0.0276
trigger times: 9
Loss after 9067386 batches: 0.0265
trigger times: 10
Loss after 9068349 batches: 0.0270
trigger times: 11
Loss after 9069312 batches: 0.0268
trigger times: 12
Loss after 9070275 batches: 0.0268
trigger times: 13
Loss after 9071238 batches: 0.0262
trigger times: 0
Loss after 9072201 batches: 0.0263
trigger times: 1
Loss after 9073164 batches: 0.0251
trigger times: 2
Loss after 9074127 batches: 0.0243
trigger times: 3
Loss after 9075090 batches: 0.0262
trigger times: 4
Loss after 9076053 batches: 0.0260
trigger times: 5
Loss after 9077016 batches: 0.0248
trigger times: 6
Loss after 9077979 batches: 0.0252
trigger times: 7
Loss after 9078942 batches: 0.0267
trigger times: 8
Loss after 9079905 batches: 0.0272
trigger times: 9
Loss after 9080868 batches: 0.0256
trigger times: 10
Loss after 9081831 batches: 0.0253
trigger times: 11
Loss after 9082794 batches: 0.0256
trigger times: 12
Loss after 9083757 batches: 0.0245
trigger times: 13
Loss after 9084720 batches: 0.0238
trigger times: 14
Loss after 9085683 batches: 0.0234
trigger times: 15
Loss after 9086646 batches: 0.0237
trigger times: 16
Loss after 9087609 batches: 0.0228
trigger times: 17
Loss after 9088572 batches: 0.0240
trigger times: 18
Loss after 9089535 batches: 0.0253
trigger times: 19
Loss after 9090498 batches: 0.0241
trigger times: 20
Loss after 9091461 batches: 0.0245
trigger times: 21
Loss after 9092424 batches: 0.0227
trigger times: 22
Loss after 9093387 batches: 0.0236
trigger times: 23
Loss after 9094350 batches: 0.0243
trigger times: 24
Loss after 9095313 batches: 0.0236
trigger times: 25
Early stopping!
Start to test process.
Loss after 9096276 batches: 0.0234
Time to train on one home:  84.48842000961304
trigger times: 0
Loss after 9097239 batches: 0.0921
trigger times: 1
Loss after 9098202 batches: 0.0788
trigger times: 2
Loss after 9099165 batches: 0.0777
trigger times: 3
Loss after 9100128 batches: 0.0762
trigger times: 4
Loss after 9101091 batches: 0.0742
trigger times: 5
Loss after 9102054 batches: 0.0727
trigger times: 6
Loss after 9103017 batches: 0.0704
trigger times: 7
Loss after 9103980 batches: 0.0681
trigger times: 8
Loss after 9104943 batches: 0.0677
trigger times: 9
Loss after 9105906 batches: 0.0676
trigger times: 10
Loss after 9106869 batches: 0.0661
trigger times: 11
Loss after 9107832 batches: 0.0662
trigger times: 12
Loss after 9108795 batches: 0.0638
trigger times: 13
Loss after 9109758 batches: 0.0655
trigger times: 14
Loss after 9110721 batches: 0.0635
trigger times: 15
Loss after 9111684 batches: 0.0639
trigger times: 16
Loss after 9112647 batches: 0.0626
trigger times: 17
Loss after 9113610 batches: 0.0627
trigger times: 18
Loss after 9114573 batches: 0.0625
trigger times: 19
Loss after 9115536 batches: 0.0620
trigger times: 20
Loss after 9116499 batches: 0.0622
trigger times: 21
Loss after 9117462 batches: 0.0624
trigger times: 22
Loss after 9118425 batches: 0.0624
trigger times: 23
Loss after 9119388 batches: 0.0614
trigger times: 24
Loss after 9120351 batches: 0.0604
trigger times: 25
Early stopping!
Start to test process.
Loss after 9121314 batches: 0.0602
Time to train on one home:  52.26027178764343
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9122277 batches: 0.0990
trigger times: 1
Loss after 9123240 batches: 0.0655
trigger times: 2
Loss after 9124203 batches: 0.0616
trigger times: 0
Loss after 9125166 batches: 0.0526
trigger times: 1
Loss after 9126129 batches: 0.0463
trigger times: 2
Loss after 9127092 batches: 0.0425
trigger times: 3
Loss after 9128055 batches: 0.0412
trigger times: 4
Loss after 9129018 batches: 0.0392
trigger times: 5
Loss after 9129981 batches: 0.0377
trigger times: 6
Loss after 9130944 batches: 0.0370
trigger times: 0
Loss after 9131907 batches: 0.0361
trigger times: 0
Loss after 9132870 batches: 0.0348
trigger times: 1
Loss after 9133833 batches: 0.0351
trigger times: 2
Loss after 9134796 batches: 0.0334
trigger times: 3
Loss after 9135759 batches: 0.0329
trigger times: 4
Loss after 9136722 batches: 0.0322
trigger times: 0
Loss after 9137685 batches: 0.0323
trigger times: 1
Loss after 9138648 batches: 0.0320
trigger times: 0
Loss after 9139611 batches: 0.0317
trigger times: 1
Loss after 9140574 batches: 0.0316
trigger times: 2
Loss after 9141537 batches: 0.0317
trigger times: 0
Loss after 9142500 batches: 0.0317
trigger times: 1
Loss after 9143463 batches: 0.0313
trigger times: 2
Loss after 9144426 batches: 0.0300
trigger times: 3
Loss after 9145389 batches: 0.0303
trigger times: 4
Loss after 9146352 batches: 0.0296
trigger times: 5
Loss after 9147315 batches: 0.0306
trigger times: 6
Loss after 9148278 batches: 0.0298
trigger times: 7
Loss after 9149241 batches: 0.0287
trigger times: 8
Loss after 9150204 batches: 0.0287
trigger times: 9
Loss after 9151167 batches: 0.0289
trigger times: 0
Loss after 9152130 batches: 0.0273
trigger times: 1
Loss after 9153093 batches: 0.0270
trigger times: 0
Loss after 9154056 batches: 0.0278
trigger times: 1
Loss after 9155019 batches: 0.0267
trigger times: 2
Loss after 9155982 batches: 0.0266
trigger times: 3
Loss after 9156945 batches: 0.0271
trigger times: 4
Loss after 9157908 batches: 0.0265
trigger times: 5
Loss after 9158871 batches: 0.0303
trigger times: 6
Loss after 9159834 batches: 0.0297
trigger times: 0
Loss after 9160797 batches: 0.0286
trigger times: 1
Loss after 9161760 batches: 0.0271
trigger times: 2
Loss after 9162723 batches: 0.0257
trigger times: 3
Loss after 9163686 batches: 0.0263
trigger times: 4
Loss after 9164649 batches: 0.0265
trigger times: 5
Loss after 9165612 batches: 0.0260
trigger times: 6
Loss after 9166575 batches: 0.0250
trigger times: 7
Loss after 9167538 batches: 0.0262
trigger times: 8
Loss after 9168501 batches: 0.0266
trigger times: 9
Loss after 9169464 batches: 0.0273
trigger times: 10
Loss after 9170427 batches: 0.0261
trigger times: 11
Loss after 9171390 batches: 0.0252
trigger times: 12
Loss after 9172353 batches: 0.0253
trigger times: 13
Loss after 9173316 batches: 0.0256
trigger times: 14
Loss after 9174279 batches: 0.0255
trigger times: 15
Loss after 9175242 batches: 0.0257
trigger times: 16
Loss after 9176205 batches: 0.0244
trigger times: 17
Loss after 9177168 batches: 0.0247
trigger times: 18
Loss after 9178131 batches: 0.0243
trigger times: 19
Loss after 9179094 batches: 0.0243
trigger times: 20
Loss after 9180057 batches: 0.0241
trigger times: 21
Loss after 9181020 batches: 0.0239
trigger times: 22
Loss after 9181983 batches: 0.0242
trigger times: 23
Loss after 9182946 batches: 0.0249
trigger times: 24
Loss after 9183909 batches: 0.0244
trigger times: 25
Early stopping!
Start to test process.
Loss after 9184872 batches: 0.0246
Time to train on one home:  83.03332614898682
trigger times: 0
Loss after 9185767 batches: 0.0797
trigger times: 1
Loss after 9186662 batches: 0.0468
trigger times: 2
Loss after 9187557 batches: 0.0214
trigger times: 3
Loss after 9188452 batches: 0.0125
trigger times: 4
Loss after 9189347 batches: 0.0102
trigger times: 5
Loss after 9190242 batches: 0.0075
trigger times: 6
Loss after 9191137 batches: 0.0056
trigger times: 7
Loss after 9192032 batches: 0.0058
trigger times: 8
Loss after 9192927 batches: 0.0053
trigger times: 9
Loss after 9193822 batches: 0.0048
trigger times: 10
Loss after 9194717 batches: 0.0051
trigger times: 11
Loss after 9195612 batches: 0.0057
trigger times: 12
Loss after 9196507 batches: 0.0048
trigger times: 13
Loss after 9197402 batches: 0.0064
trigger times: 14
Loss after 9198297 batches: 0.0056
trigger times: 15
Loss after 9199192 batches: 0.0050
trigger times: 16
Loss after 9200087 batches: 0.0044
trigger times: 17
Loss after 9200982 batches: 0.0052
trigger times: 18
Loss after 9201877 batches: 0.0043
trigger times: 19
Loss after 9202772 batches: 0.0037
trigger times: 20
Loss after 9203667 batches: 0.0035
trigger times: 21
Loss after 9204562 batches: 0.0034
trigger times: 22
Loss after 9205457 batches: 0.0038
trigger times: 23
Loss after 9206352 batches: 0.0046
trigger times: 24
Loss after 9207247 batches: 0.0048
trigger times: 25
Early stopping!
Start to test process.
Loss after 9208142 batches: 0.0044
Time to train on one home:  51.26572394371033
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9209079 batches: 0.0843
trigger times: 1
Loss after 9210016 batches: 0.0735
trigger times: 2
Loss after 9210953 batches: 0.0691
trigger times: 3
Loss after 9211890 batches: 0.0663
trigger times: 4
Loss after 9212827 batches: 0.0635
trigger times: 5
Loss after 9213764 batches: 0.0607
trigger times: 6
Loss after 9214701 batches: 0.0601
trigger times: 7
Loss after 9215638 batches: 0.0590
trigger times: 8
Loss after 9216575 batches: 0.0574
trigger times: 9
Loss after 9217512 batches: 0.0561
trigger times: 10
Loss after 9218449 batches: 0.0555
trigger times: 11
Loss after 9219386 batches: 0.0568
trigger times: 12
Loss after 9220323 batches: 0.0551
trigger times: 13
Loss after 9221260 batches: 0.0553
trigger times: 14
Loss after 9222197 batches: 0.0540
trigger times: 15
Loss after 9223134 batches: 0.0542
trigger times: 16
Loss after 9224071 batches: 0.0531
trigger times: 17
Loss after 9225008 batches: 0.0525
trigger times: 18
Loss after 9225945 batches: 0.0530
trigger times: 19
Loss after 9226882 batches: 0.0526
trigger times: 20
Loss after 9227819 batches: 0.0527
trigger times: 21
Loss after 9228756 batches: 0.0531
trigger times: 22
Loss after 9229693 batches: 0.0525
trigger times: 23
Loss after 9230630 batches: 0.0509
trigger times: 24
Loss after 9231567 batches: 0.0516
trigger times: 25
Early stopping!
Start to test process.
Loss after 9232504 batches: 0.0503
Time to train on one home:  52.25735950469971
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51]]
Round_6_results:  [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51]
trigger times: 0
Loss after 9233466 batches: 0.0800
trigger times: 1
Loss after 9234428 batches: 0.0662
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 9629 < 9630; dropping {'Training_Loss': 0.08003294893673488, 'Validation_Loss': 0.08138938993215561, 'Training_R2': -0.17427380699258954, 'Validation_R2': 0.11193260267296201, 'Training_F1': 0.3668596355858995, 'Validation_F1': 0.5914317224826848, 'Training_NEP': 0.8573691843896551, 'Validation_NEP': 0.7960320664928612, 'Training_NDE': 0.7151612264759195, 'Validation_NDE': 0.8023477885211483, 'Training_MAE': 32.58030485666598, 'Validation_MAE': 28.306322149772043, 'Training_MSE': 2641.3665, 'Validation_MSE': 10510.766}.
trigger times: 2
Loss after 9235390 batches: 0.0650
trigger times: 3
Loss after 9236352 batches: 0.0619
trigger times: 4
Loss after 9237314 batches: 0.0608
trigger times: 5
Loss after 9238276 batches: 0.0585
trigger times: 6
Loss after 9239238 batches: 0.0577
trigger times: 7
Loss after 9240200 batches: 0.0569
trigger times: 8
Loss after 9241162 batches: 0.0576
trigger times: 9
Loss after 9242124 batches: 0.0562
trigger times: 10
Loss after 9243086 batches: 0.0541
trigger times: 11
Loss after 9244048 batches: 0.0545
trigger times: 12
Loss after 9245010 batches: 0.0548
trigger times: 13
Loss after 9245972 batches: 0.0543
trigger times: 14
Loss after 9246934 batches: 0.0534
trigger times: 15
Loss after 9247896 batches: 0.0549
trigger times: 16
Loss after 9248858 batches: 0.0533
trigger times: 17
Loss after 9249820 batches: 0.0544
trigger times: 18
Loss after 9250782 batches: 0.0536
trigger times: 19
Loss after 9251744 batches: 0.0534
trigger times: 20
Loss after 9252706 batches: 0.0531
trigger times: 21
Loss after 9253668 batches: 0.0516
trigger times: 22
Loss after 9254630 batches: 0.0522
trigger times: 23
Loss after 9255592 batches: 0.0517
trigger times: 24
Loss after 9256554 batches: 0.0513
trigger times: 25
Early stopping!
Start to test process.
Loss after 9257516 batches: 0.0510
Time to train on one home:  52.59225845336914
trigger times: 0
Loss after 9258445 batches: 0.0866
trigger times: 1
Loss after 9259374 batches: 0.0631
trigger times: 2
Loss after 9260303 batches: 0.0544
trigger times: 3
Loss after 9261232 batches: 0.0427
trigger times: 4
Loss after 9262161 batches: 0.0404
trigger times: 5
Loss after 9263090 batches: 0.0397
trigger times: 6
Loss after 9264019 batches: 0.0344
trigger times: 7
Loss after 9264948 batches: 0.0354
trigger times: 8
Loss after 9265877 batches: 0.0344
trigger times: 9
Loss after 9266806 batches: 0.0322
trigger times: 10
Loss after 9267735 batches: 0.0323
trigger times: 11
Loss after 9268664 batches: 0.0323
trigger times: 12
Loss after 9269593 batches: 0.0316
trigger times: 13
Loss after 9270522 batches: 0.0314
trigger times: 14
Loss after 9271451 batches: 0.0304
trigger times: 15
Loss after 9272380 batches: 0.0310
trigger times: 16
Loss after 9273309 batches: 0.0294
trigger times: 17
Loss after 9274238 batches: 0.0282
trigger times: 18
Loss after 9275167 batches: 0.0331
trigger times: 19
Loss after 9276096 batches: 0.0310
trigger times: 20
Loss after 9277025 batches: 0.0297
trigger times: 21
Loss after 9277954 batches: 0.0299
trigger times: 22
Loss after 9278883 batches: 0.0295
trigger times: 23
Loss after 9279812 batches: 0.0274
trigger times: 24
Loss after 9280741 batches: 0.0282
trigger times: 25
Early stopping!
Start to test process.
Loss after 9281670 batches: 0.0339
Time to train on one home:  53.2929892539978
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 9282633 batches: 0.0658
trigger times: 1
Loss after 9283596 batches: 0.0191
trigger times: 2
Loss after 9284559 batches: 0.0143
trigger times: 3
Loss after 9285522 batches: 0.0138
trigger times: 4
Loss after 9286485 batches: 0.0136
trigger times: 5
Loss after 9287448 batches: 0.0132
trigger times: 6
Loss after 9288411 batches: 0.0126
trigger times: 7
Loss after 9289374 batches: 0.0120
trigger times: 8
Loss after 9290337 batches: 0.0116
trigger times: 9
Loss after 9291300 batches: 0.0110
trigger times: 10
Loss after 9292263 batches: 0.0109
trigger times: 11
Loss after 9293226 batches: 0.0103
trigger times: 12
Loss after 9294189 batches: 0.0099
trigger times: 13
Loss after 9295152 batches: 0.0100
trigger times: 14
Loss after 9296115 batches: 0.0098
trigger times: 15
Loss after 9297078 batches: 0.0093
trigger times: 16
Loss after 9298041 batches: 0.0090
trigger times: 17
Loss after 9299004 batches: 0.0089
trigger times: 18
Loss after 9299967 batches: 0.0087
trigger times: 19
Loss after 9300930 batches: 0.0085
trigger times: 20
Loss after 9301893 batches: 0.0086
trigger times: 21
Loss after 9302856 batches: 0.0087
trigger times: 22
Loss after 9303819 batches: 0.0083
trigger times: 23
Loss after 9304782 batches: 0.0083
trigger times: 24
Loss after 9305745 batches: 0.0080
trigger times: 25
Early stopping!
Start to test process.
Loss after 9306708 batches: 0.0076
Time to train on one home:  53.37597060203552
trigger times: 0
Loss after 9307671 batches: 0.0264
trigger times: 0
Loss after 9308634 batches: 0.0217
trigger times: 1
Loss after 9309597 batches: 0.0199
trigger times: 0
Loss after 9310560 batches: 0.0182
trigger times: 0
Loss after 9311523 batches: 0.0176
trigger times: 1
Loss after 9312486 batches: 0.0165
trigger times: 2
Loss after 9313449 batches: 0.0164
trigger times: 3
Loss after 9314412 batches: 0.0162
trigger times: 4
Loss after 9315375 batches: 0.0156
trigger times: 5
Loss after 9316338 batches: 0.0154
trigger times: 6
Loss after 9317301 batches: 0.0156
trigger times: 7
Loss after 9318264 batches: 0.0154
trigger times: 8
Loss after 9319227 batches: 0.0152
trigger times: 9
Loss after 9320190 batches: 0.0149
trigger times: 10
Loss after 9321153 batches: 0.0144
trigger times: 11
Loss after 9322116 batches: 0.0145
trigger times: 12
Loss after 9323079 batches: 0.0138
trigger times: 13
Loss after 9324042 batches: 0.0138
trigger times: 14
Loss after 9325005 batches: 0.0146
trigger times: 15
Loss after 9325968 batches: 0.0137
trigger times: 16
Loss after 9326931 batches: 0.0143
trigger times: 17
Loss after 9327894 batches: 0.0141
trigger times: 18
Loss after 9328857 batches: 0.0139
trigger times: 19
Loss after 9329820 batches: 0.0141
trigger times: 20
Loss after 9330783 batches: 0.0135
trigger times: 21
Loss after 9331746 batches: 0.0134
trigger times: 22
Loss after 9332709 batches: 0.0138
trigger times: 23
Loss after 9333672 batches: 0.0162
trigger times: 24
Loss after 9334635 batches: 0.0162
trigger times: 25
Early stopping!
Start to test process.
Loss after 9335598 batches: 0.0150
Time to train on one home:  55.7062201499939
trigger times: 0
Loss after 9336561 batches: 0.0952
trigger times: 1
Loss after 9337524 batches: 0.0903
trigger times: 2
Loss after 9338487 batches: 0.0843
trigger times: 3
Loss after 9339450 batches: 0.0818
trigger times: 4
Loss after 9340413 batches: 0.0775
trigger times: 5
Loss after 9341376 batches: 0.0771
trigger times: 6
Loss after 9342339 batches: 0.0759
trigger times: 7
Loss after 9343302 batches: 0.0729
trigger times: 8
Loss after 9344265 batches: 0.0731
trigger times: 9
Loss after 9345228 batches: 0.0718
trigger times: 10
Loss after 9346191 batches: 0.0712
trigger times: 11
Loss after 9347154 batches: 0.0711
trigger times: 12
Loss after 9348117 batches: 0.0707
trigger times: 13
Loss after 9349080 batches: 0.0683
trigger times: 14
Loss after 9350043 batches: 0.0697
trigger times: 15
Loss after 9351006 batches: 0.0700
trigger times: 16
Loss after 9351969 batches: 0.0681
trigger times: 17
Loss after 9352932 batches: 0.0690
trigger times: 18
Loss after 9353895 batches: 0.0680
trigger times: 19
Loss after 9354858 batches: 0.0695
trigger times: 20
Loss after 9355821 batches: 0.0661
trigger times: 21
Loss after 9356784 batches: 0.0665
trigger times: 22
Loss after 9357747 batches: 0.0658
trigger times: 23
Loss after 9358710 batches: 0.0660
trigger times: 24
Loss after 9359673 batches: 0.0650
trigger times: 25
Early stopping!
Start to test process.
Loss after 9360636 batches: 0.0676
Time to train on one home:  52.369243144989014
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9361599 batches: 0.0876
trigger times: 1
Loss after 9362562 batches: 0.0748
trigger times: 2
Loss after 9363525 batches: 0.0730
trigger times: 3
Loss after 9364488 batches: 0.0695
trigger times: 4
Loss after 9365451 batches: 0.0666
trigger times: 5
Loss after 9366414 batches: 0.0669
trigger times: 6
Loss after 9367377 batches: 0.0662
trigger times: 7
Loss after 9368340 batches: 0.0643
trigger times: 8
Loss after 9369303 batches: 0.0642
trigger times: 9
Loss after 9370266 batches: 0.0628
trigger times: 10
Loss after 9371229 batches: 0.0632
trigger times: 11
Loss after 9372192 batches: 0.0616
trigger times: 12
Loss after 9373155 batches: 0.0618
trigger times: 13
Loss after 9374118 batches: 0.0616
trigger times: 14
Loss after 9375081 batches: 0.0608
trigger times: 15
Loss after 9376044 batches: 0.0611
trigger times: 16
Loss after 9377007 batches: 0.0593
trigger times: 17
Loss after 9377970 batches: 0.0595
trigger times: 18
Loss after 9378933 batches: 0.0577
trigger times: 19
Loss after 9379896 batches: 0.0596
trigger times: 20
Loss after 9380859 batches: 0.0576
trigger times: 21
Loss after 9381822 batches: 0.0591
trigger times: 22
Loss after 9382785 batches: 0.0583
trigger times: 23
Loss after 9383748 batches: 0.0579
trigger times: 24
Loss after 9384711 batches: 0.0585
trigger times: 25
Early stopping!
Start to test process.
Loss after 9385674 batches: 0.0561
Time to train on one home:  52.47319555282593
trigger times: 0
Loss after 9386637 batches: 0.0758
trigger times: 1
Loss after 9387600 batches: 0.0698
trigger times: 2
Loss after 9388563 batches: 0.0682
trigger times: 3
Loss after 9389526 batches: 0.0655
trigger times: 4
Loss after 9390489 batches: 0.0637
trigger times: 5
Loss after 9391452 batches: 0.0618
trigger times: 6
Loss after 9392415 batches: 0.0619
trigger times: 7
Loss after 9393378 batches: 0.0604
trigger times: 8
Loss after 9394341 batches: 0.0607
trigger times: 9
Loss after 9395304 batches: 0.0594
trigger times: 10
Loss after 9396267 batches: 0.0586
trigger times: 11
Loss after 9397230 batches: 0.0573
trigger times: 12
Loss after 9398193 batches: 0.0577
trigger times: 13
Loss after 9399156 batches: 0.0584
trigger times: 14
Loss after 9400119 batches: 0.0575
trigger times: 15
Loss after 9401082 batches: 0.0563
trigger times: 16
Loss after 9402045 batches: 0.0566
trigger times: 17
Loss after 9403008 batches: 0.0555
trigger times: 18
Loss after 9403971 batches: 0.0565
trigger times: 19
Loss after 9404934 batches: 0.0561
trigger times: 20
Loss after 9405897 batches: 0.0552
trigger times: 21
Loss after 9406860 batches: 0.0538
trigger times: 22
Loss after 9407823 batches: 0.0541
trigger times: 23
Loss after 9408786 batches: 0.0541
trigger times: 24
Loss after 9409749 batches: 0.0533
trigger times: 25
Early stopping!
Start to test process.
Loss after 9410712 batches: 0.0542
Time to train on one home:  52.27020812034607
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9411675 batches: 0.0671
trigger times: 1
Loss after 9412638 batches: 0.0525
trigger times: 2
Loss after 9413601 batches: 0.0514
trigger times: 3
Loss after 9414564 batches: 0.0475
trigger times: 4
Loss after 9415527 batches: 0.0415
trigger times: 5
Loss after 9416490 batches: 0.0384
trigger times: 6
Loss after 9417453 batches: 0.0362
trigger times: 7
Loss after 9418416 batches: 0.0342
trigger times: 8
Loss after 9419379 batches: 0.0331
trigger times: 9
Loss after 9420342 batches: 0.0313
trigger times: 10
Loss after 9421305 batches: 0.0304
trigger times: 11
Loss after 9422268 batches: 0.0297
trigger times: 12
Loss after 9423231 batches: 0.0298
trigger times: 13
Loss after 9424194 batches: 0.0295
trigger times: 14
Loss after 9425157 batches: 0.0291
trigger times: 15
Loss after 9426120 batches: 0.0288
trigger times: 16
Loss after 9427083 batches: 0.0283
trigger times: 17
Loss after 9428046 batches: 0.0275
trigger times: 18
Loss after 9429009 batches: 0.0278
trigger times: 19
Loss after 9429972 batches: 0.0264
trigger times: 20
Loss after 9430935 batches: 0.0260
trigger times: 21
Loss after 9431898 batches: 0.0264
trigger times: 22
Loss after 9432861 batches: 0.0261
trigger times: 23
Loss after 9433824 batches: 0.0258
trigger times: 24
Loss after 9434787 batches: 0.0257
trigger times: 25
Early stopping!
Start to test process.
Loss after 9435750 batches: 0.0257
Time to train on one home:  52.47714877128601
trigger times: 0
Loss after 9436708 batches: 0.0619
trigger times: 1
Loss after 9437666 batches: 0.0445
trigger times: 2
Loss after 9438624 batches: 0.0393
trigger times: 3
Loss after 9439582 batches: 0.0346
trigger times: 4
Loss after 9440540 batches: 0.0329
trigger times: 5
Loss after 9441498 batches: 0.0294
trigger times: 6
Loss after 9442456 batches: 0.0285
trigger times: 7
Loss after 9443414 batches: 0.0272
trigger times: 8
Loss after 9444372 batches: 0.0249
trigger times: 9
Loss after 9445330 batches: 0.0234
trigger times: 10
Loss after 9446288 batches: 0.0232
trigger times: 11
Loss after 9447246 batches: 0.0235
trigger times: 12
Loss after 9448204 batches: 0.0233
trigger times: 13
Loss after 9449162 batches: 0.0214
trigger times: 14
Loss after 9450120 batches: 0.0210
trigger times: 15
Loss after 9451078 batches: 0.0207
trigger times: 16
Loss after 9452036 batches: 0.0202
trigger times: 17
Loss after 9452994 batches: 0.0205
trigger times: 18
Loss after 9453952 batches: 0.0192
trigger times: 19
Loss after 9454910 batches: 0.0190
trigger times: 20
Loss after 9455868 batches: 0.0185
trigger times: 21
Loss after 9456826 batches: 0.0188
trigger times: 22
Loss after 9457784 batches: 0.0188
trigger times: 23
Loss after 9458742 batches: 0.0180
trigger times: 24
Loss after 9459700 batches: 0.0183
trigger times: 25
Early stopping!
Start to test process.
Loss after 9460658 batches: 0.0190
Time to train on one home:  52.696104288101196
trigger times: 0
Loss after 9461620 batches: 0.0794
trigger times: 1
Loss after 9462582 batches: 0.0666
trigger times: 2
Loss after 9463544 batches: 0.0655
trigger times: 3
Loss after 9464506 batches: 0.0626
trigger times: 4
Loss after 9465468 batches: 0.0590
trigger times: 5
Loss after 9466430 batches: 0.0573
trigger times: 6
Loss after 9467392 batches: 0.0571
trigger times: 7
Loss after 9468354 batches: 0.0565
trigger times: 8
Loss after 9469316 batches: 0.0556
trigger times: 9
Loss after 9470278 batches: 0.0570
trigger times: 10
Loss after 9471240 batches: 0.0549
trigger times: 11
Loss after 9472202 batches: 0.0547
trigger times: 12
Loss after 9473164 batches: 0.0536
trigger times: 13
Loss after 9474126 batches: 0.0531
trigger times: 14
Loss after 9475088 batches: 0.0532
trigger times: 15
Loss after 9476050 batches: 0.0530
trigger times: 16
Loss after 9477012 batches: 0.0532
trigger times: 17
Loss after 9477974 batches: 0.0526
trigger times: 18
Loss after 9478936 batches: 0.0519
trigger times: 19
Loss after 9479898 batches: 0.0519
trigger times: 20
Loss after 9480860 batches: 0.0527
trigger times: 21
Loss after 9481822 batches: 0.0522
trigger times: 22
Loss after 9482784 batches: 0.0518
trigger times: 23
Loss after 9483746 batches: 0.0519
trigger times: 24
Loss after 9484708 batches: 0.0516
trigger times: 25
Early stopping!
Start to test process.
Loss after 9485670 batches: 0.0502
Time to train on one home:  52.22920799255371
trigger times: 0
Loss after 9486633 batches: 0.1052
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 9487596 batches: 0.0296
trigger times: 2
Loss after 9488559 batches: 0.0213
trigger times: 3
Loss after 9489522 batches: 0.0210
trigger times: 4
Loss after 9490485 batches: 0.0178
trigger times: 5
Loss after 9491448 batches: 0.0166
trigger times: 6
Loss after 9492411 batches: 0.0163
trigger times: 7
Loss after 9493374 batches: 0.0150
trigger times: 8
Loss after 9494337 batches: 0.0149
trigger times: 9
Loss after 9495300 batches: 0.0144
trigger times: 10
Loss after 9496263 batches: 0.0144
trigger times: 11
Loss after 9497226 batches: 0.0139
trigger times: 12
Loss after 9498189 batches: 0.0138
trigger times: 13
Loss after 9499152 batches: 0.0139
trigger times: 14
Loss after 9500115 batches: 0.0135
trigger times: 15
Loss after 9501078 batches: 0.0133
trigger times: 16
Loss after 9502041 batches: 0.0135
trigger times: 17
Loss after 9503004 batches: 0.0132
trigger times: 18
Loss after 9503967 batches: 0.0132
trigger times: 19
Loss after 9504930 batches: 0.0133
trigger times: 20
Loss after 9505893 batches: 0.0132
trigger times: 21
Loss after 9506856 batches: 0.0131
trigger times: 22
Loss after 9507819 batches: 0.0131
trigger times: 23
Loss after 9508782 batches: 0.0128
trigger times: 24
Loss after 9509745 batches: 0.0131
trigger times: 25
Early stopping!
Start to test process.
Loss after 9510708 batches: 0.0127
Time to train on one home:  52.38516187667847
trigger times: 0
Loss after 9511671 batches: 0.0523
trigger times: 1
Loss after 9512634 batches: 0.0438
trigger times: 2
Loss after 9513597 batches: 0.0410
trigger times: 3
Loss after 9514560 batches: 0.0382
trigger times: 4
Loss after 9515523 batches: 0.0370
trigger times: 5
Loss after 9516486 batches: 0.0350
trigger times: 6
Loss after 9517449 batches: 0.0351
trigger times: 7
Loss after 9518412 batches: 0.0348
trigger times: 8
Loss after 9519375 batches: 0.0349
trigger times: 9
Loss after 9520338 batches: 0.0356
trigger times: 10
Loss after 9521301 batches: 0.0338
trigger times: 11
Loss after 9522264 batches: 0.0339
trigger times: 12
Loss after 9523227 batches: 0.0337
trigger times: 13
Loss after 9524190 batches: 0.0334
trigger times: 14
Loss after 9525153 batches: 0.0333
trigger times: 15
Loss after 9526116 batches: 0.0329
trigger times: 16
Loss after 9527079 batches: 0.0325
trigger times: 17
Loss after 9528042 batches: 0.0323
trigger times: 18
Loss after 9529005 batches: 0.0326
trigger times: 19
Loss after 9529968 batches: 0.0313
trigger times: 20
Loss after 9530931 batches: 0.0306
trigger times: 21
Loss after 9531894 batches: 0.0311
trigger times: 22
Loss after 9532857 batches: 0.0307
trigger times: 23
Loss after 9533820 batches: 0.0309
trigger times: 24
Loss after 9534783 batches: 0.0298
trigger times: 25
Early stopping!
Start to test process.
Loss after 9535746 batches: 0.0298
Time to train on one home:  52.415123462677
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9536709 batches: 0.0669
trigger times: 1
Loss after 9537672 batches: 0.0526
trigger times: 2
Loss after 9538635 batches: 0.0517
trigger times: 3
Loss after 9539598 batches: 0.0477
trigger times: 4
Loss after 9540561 batches: 0.0416
trigger times: 5
Loss after 9541524 batches: 0.0385
trigger times: 6
Loss after 9542487 batches: 0.0364
trigger times: 7
Loss after 9543450 batches: 0.0346
trigger times: 8
Loss after 9544413 batches: 0.0323
trigger times: 9
Loss after 9545376 batches: 0.0313
trigger times: 10
Loss after 9546339 batches: 0.0302
trigger times: 11
Loss after 9547302 batches: 0.0293
trigger times: 12
Loss after 9548265 batches: 0.0297
trigger times: 13
Loss after 9549228 batches: 0.0288
trigger times: 14
Loss after 9550191 batches: 0.0286
trigger times: 15
Loss after 9551154 batches: 0.0285
trigger times: 16
Loss after 9552117 batches: 0.0286
trigger times: 17
Loss after 9553080 batches: 0.0284
trigger times: 18
Loss after 9554043 batches: 0.0272
trigger times: 19
Loss after 9555006 batches: 0.0274
trigger times: 20
Loss after 9555969 batches: 0.0266
trigger times: 21
Loss after 9556932 batches: 0.0271
trigger times: 22
Loss after 9557895 batches: 0.0272
trigger times: 23
Loss after 9558858 batches: 0.0276
trigger times: 24
Loss after 9559821 batches: 0.0280
trigger times: 25
Early stopping!
Start to test process.
Loss after 9560784 batches: 0.0271
Time to train on one home:  52.4571578502655
trigger times: 0
Loss after 9561747 batches: 0.0509
trigger times: 1
Loss after 9562710 batches: 0.0434
trigger times: 2
Loss after 9563673 batches: 0.0402
trigger times: 3
Loss after 9564636 batches: 0.0378
trigger times: 4
Loss after 9565599 batches: 0.0372
trigger times: 5
Loss after 9566562 batches: 0.0363
trigger times: 6
Loss after 9567525 batches: 0.0340
trigger times: 7
Loss after 9568488 batches: 0.0348
trigger times: 8
Loss after 9569451 batches: 0.0353
trigger times: 9
Loss after 9570414 batches: 0.0344
trigger times: 10
Loss after 9571377 batches: 0.0336
trigger times: 11
Loss after 9572340 batches: 0.0334
trigger times: 12
Loss after 9573303 batches: 0.0330
trigger times: 13
Loss after 9574266 batches: 0.0322
trigger times: 14
Loss after 9575229 batches: 0.0317
trigger times: 15
Loss after 9576192 batches: 0.0325
trigger times: 16
Loss after 9577155 batches: 0.0305
trigger times: 17
Loss after 9578118 batches: 0.0304
trigger times: 18
Loss after 9579081 batches: 0.0308
trigger times: 19
Loss after 9580044 batches: 0.0310
trigger times: 20
Loss after 9581007 batches: 0.0311
trigger times: 21
Loss after 9581970 batches: 0.0309
trigger times: 22
Loss after 9582933 batches: 0.0314
trigger times: 23
Loss after 9583896 batches: 0.0309
trigger times: 24
Loss after 9584859 batches: 0.0311
trigger times: 25
Early stopping!
Start to test process.
Loss after 9585822 batches: 0.0307
Time to train on one home:  52.4171404838562
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9586785 batches: 0.0499
trigger times: 1
Loss after 9587748 batches: 0.0466
trigger times: 0
Loss after 9588711 batches: 0.0440
trigger times: 1
Loss after 9589674 batches: 0.0427
trigger times: 0
Loss after 9590637 batches: 0.0408
trigger times: 1
Loss after 9591600 batches: 0.0398
trigger times: 2
Loss after 9592563 batches: 0.0382
trigger times: 3
Loss after 9593526 batches: 0.0382
trigger times: 4
Loss after 9594489 batches: 0.0371
trigger times: 5
Loss after 9595452 batches: 0.0361
trigger times: 6
Loss after 9596415 batches: 0.0360
trigger times: 7
Loss after 9597378 batches: 0.0350
trigger times: 8
Loss after 9598341 batches: 0.0351
trigger times: 9
Loss after 9599304 batches: 0.0352
trigger times: 10
Loss after 9600267 batches: 0.0355
trigger times: 11
Loss after 9601230 batches: 0.0343
trigger times: 12
Loss after 9602193 batches: 0.0339
trigger times: 13
Loss after 9603156 batches: 0.0338
trigger times: 14
Loss after 9604119 batches: 0.0346
trigger times: 15
Loss after 9605082 batches: 0.0342
trigger times: 16
Loss after 9606045 batches: 0.0331
trigger times: 17
Loss after 9607008 batches: 0.0321
trigger times: 18
Loss after 9607971 batches: 0.0331
trigger times: 19
Loss after 9608934 batches: 0.0318
trigger times: 20
Loss after 9609897 batches: 0.0323
trigger times: 21
Loss after 9610860 batches: 0.0314
trigger times: 22
Loss after 9611823 batches: 0.0313
trigger times: 23
Loss after 9612786 batches: 0.0307
trigger times: 24
Loss after 9613749 batches: 0.0308
trigger times: 25
Early stopping!
Start to test process.
Loss after 9614712 batches: 0.0309
Time to train on one home:  55.87309908866882
trigger times: 0
Loss after 9615675 batches: 0.0909
trigger times: 1
Loss after 9616638 batches: 0.0501
trigger times: 2
Loss after 9617601 batches: 0.0497
trigger times: 3
Loss after 9618564 batches: 0.0490
trigger times: 4
Loss after 9619527 batches: 0.0453
trigger times: 5
Loss after 9620490 batches: 0.0434
trigger times: 6
Loss after 9621453 batches: 0.0422
trigger times: 7
Loss after 9622416 batches: 0.0413
trigger times: 8
Loss after 9623379 batches: 0.0399
trigger times: 9
Loss after 9624342 batches: 0.0404
trigger times: 10
Loss after 9625305 batches: 0.0391
trigger times: 11
Loss after 9626268 batches: 0.0390
trigger times: 12
Loss after 9627231 batches: 0.0386
trigger times: 13
Loss after 9628194 batches: 0.0378
trigger times: 14
Loss after 9629157 batches: 0.0372
trigger times: 15
Loss after 9630120 batches: 0.0366
trigger times: 16
Loss after 9631083 batches: 0.0366
trigger times: 17
Loss after 9632046 batches: 0.0374
trigger times: 18
Loss after 9633009 batches: 0.0363
trigger times: 19
Loss after 9633972 batches: 0.0364
trigger times: 20
Loss after 9634935 batches: 0.0360
trigger times: 21
Loss after 9635898 batches: 0.0356
trigger times: 22
Loss after 9636861 batches: 0.0363
trigger times: 23
Loss after 9637824 batches: 0.0361
trigger times: 24
Loss after 9638787 batches: 0.0361
trigger times: 25
Early stopping!
Start to test process.
Loss after 9639750 batches: 0.0360
Time to train on one home:  53.00338935852051
trigger times: 0
Loss after 9640713 batches: 0.0964
trigger times: 1
Loss after 9641676 batches: 0.0888
trigger times: 2
Loss after 9642639 batches: 0.0849
trigger times: 3
Loss after 9643602 batches: 0.0825
trigger times: 4
Loss after 9644565 batches: 0.0797
trigger times: 5
Loss after 9645528 batches: 0.0768
trigger times: 6
Loss after 9646491 batches: 0.0768
trigger times: 7
Loss after 9647454 batches: 0.0734
trigger times: 8
Loss after 9648417 batches: 0.0724
trigger times: 9
Loss after 9649380 batches: 0.0710
trigger times: 10
Loss after 9650343 batches: 0.0710
trigger times: 11
Loss after 9651306 batches: 0.0704
trigger times: 12
Loss after 9652269 batches: 0.0699
trigger times: 13
Loss after 9653232 batches: 0.0710
trigger times: 14
Loss after 9654195 batches: 0.0673
trigger times: 15
Loss after 9655158 batches: 0.0677
trigger times: 16
Loss after 9656121 batches: 0.0657
trigger times: 17
Loss after 9657084 batches: 0.0656
trigger times: 18
Loss after 9658047 batches: 0.0646
trigger times: 19
Loss after 9659010 batches: 0.0651
trigger times: 20
Loss after 9659973 batches: 0.0668
trigger times: 21
Loss after 9660936 batches: 0.0640
trigger times: 22
Loss after 9661899 batches: 0.0660
trigger times: 23
Loss after 9662862 batches: 0.0643
trigger times: 24
Loss after 9663825 batches: 0.0662
trigger times: 25
Early stopping!
Start to test process.
Loss after 9664788 batches: 0.0653
Time to train on one home:  52.19829273223877
trigger times: 0
Loss after 9665751 batches: 0.0922
trigger times: 1
Loss after 9666714 batches: 0.0647
trigger times: 2
Loss after 9667677 batches: 0.0601
trigger times: 3
Loss after 9668640 batches: 0.0553
trigger times: 4
Loss after 9669603 batches: 0.0516
trigger times: 5
Loss after 9670566 batches: 0.0496
trigger times: 6
Loss after 9671529 batches: 0.0477
trigger times: 7
Loss after 9672492 batches: 0.0468
trigger times: 8
Loss after 9673455 batches: 0.0468
trigger times: 9
Loss after 9674418 batches: 0.0439
trigger times: 10
Loss after 9675381 batches: 0.0439
trigger times: 11
Loss after 9676344 batches: 0.0436
trigger times: 12
Loss after 9677307 batches: 0.0422
trigger times: 13
Loss after 9678270 batches: 0.0426
trigger times: 14
Loss after 9679233 batches: 0.0426
trigger times: 15
Loss after 9680196 batches: 0.0415
trigger times: 16
Loss after 9681159 batches: 0.0408
trigger times: 17
Loss after 9682122 batches: 0.0415
trigger times: 18
Loss after 9683085 batches: 0.0416
trigger times: 19
Loss after 9684048 batches: 0.0405
trigger times: 20
Loss after 9685011 batches: 0.0403
trigger times: 21
Loss after 9685974 batches: 0.0405
trigger times: 22
Loss after 9686937 batches: 0.0402
trigger times: 23
Loss after 9687900 batches: 0.0393
trigger times: 24
Loss after 9688863 batches: 0.0400
trigger times: 25
Early stopping!
Start to test process.
Loss after 9689826 batches: 0.0391
Time to train on one home:  52.806416511535645
trigger times: 0
Loss after 9690755 batches: 0.0840
trigger times: 1
Loss after 9691684 batches: 0.0620
trigger times: 2
Loss after 9692613 batches: 0.0527
trigger times: 3
Loss after 9693542 batches: 0.0443
trigger times: 4
Loss after 9694471 batches: 0.0403
trigger times: 5
Loss after 9695400 batches: 0.0392
trigger times: 6
Loss after 9696329 batches: 0.0360
trigger times: 7
Loss after 9697258 batches: 0.0354
trigger times: 8
Loss after 9698187 batches: 0.0341
trigger times: 9
Loss after 9699116 batches: 0.0338
trigger times: 10
Loss after 9700045 batches: 0.0341
trigger times: 11
Loss after 9700974 batches: 0.0331
trigger times: 12
Loss after 9701903 batches: 0.0319
trigger times: 13
Loss after 9702832 batches: 0.0344
trigger times: 14
Loss after 9703761 batches: 0.0353
trigger times: 15
Loss after 9704690 batches: 0.0351
trigger times: 16
Loss after 9705619 batches: 0.0343
trigger times: 17
Loss after 9706548 batches: 0.0307
trigger times: 18
Loss after 9707477 batches: 0.0317
trigger times: 19
Loss after 9708406 batches: 0.0327
trigger times: 20
Loss after 9709335 batches: 0.0324
trigger times: 21
Loss after 9710264 batches: 0.0284
trigger times: 22
Loss after 9711193 batches: 0.0310
trigger times: 23
Loss after 9712122 batches: 0.0271
trigger times: 24
Loss after 9713051 batches: 0.0296
trigger times: 25
Early stopping!
Start to test process.
Loss after 9713980 batches: 0.0265
Time to train on one home:  52.24589991569519
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9714943 batches: 0.0737
trigger times: 1
Loss after 9715906 batches: 0.0334
trigger times: 2
Loss after 9716869 batches: 0.0268
trigger times: 3
Loss after 9717832 batches: 0.0269
trigger times: 4
Loss after 9718795 batches: 0.0259
trigger times: 5
Loss after 9719758 batches: 0.0250
trigger times: 6
Loss after 9720721 batches: 0.0247
trigger times: 7
Loss after 9721684 batches: 0.0237
trigger times: 8
Loss after 9722647 batches: 0.0231
trigger times: 9
Loss after 9723610 batches: 0.0226
trigger times: 10
Loss after 9724573 batches: 0.0218
trigger times: 11
Loss after 9725536 batches: 0.0214
trigger times: 12
Loss after 9726499 batches: 0.0210
trigger times: 13
Loss after 9727462 batches: 0.0202
trigger times: 14
Loss after 9728425 batches: 0.0204
trigger times: 15
Loss after 9729388 batches: 0.0199
trigger times: 16
Loss after 9730351 batches: 0.0200
trigger times: 17
Loss after 9731314 batches: 0.0194
trigger times: 18
Loss after 9732277 batches: 0.0197
trigger times: 19
Loss after 9733240 batches: 0.0190
trigger times: 20
Loss after 9734203 batches: 0.0191
trigger times: 21
Loss after 9735166 batches: 0.0190
trigger times: 22
Loss after 9736129 batches: 0.0190
trigger times: 23
Loss after 9737092 batches: 0.0187
trigger times: 24
Loss after 9738055 batches: 0.0187
trigger times: 25
Early stopping!
Start to test process.
Loss after 9739018 batches: 0.0184
Time to train on one home:  52.328110694885254
trigger times: 0
Loss after 9739981 batches: 0.1824
trigger times: 1
Loss after 9740944 batches: 0.1227
trigger times: 2
Loss after 9741907 batches: 0.1002
trigger times: 3
Loss after 9742870 batches: 0.0917
trigger times: 4
Loss after 9743833 batches: 0.0846
trigger times: 5
Loss after 9744796 batches: 0.0810
trigger times: 6
Loss after 9745759 batches: 0.0769
trigger times: 7
Loss after 9746722 batches: 0.0732
trigger times: 8
Loss after 9747685 batches: 0.0694
trigger times: 9
Loss after 9748648 batches: 0.0657
trigger times: 10
Loss after 9749611 batches: 0.0642
trigger times: 11
Loss after 9750574 batches: 0.0601
trigger times: 12
Loss after 9751537 batches: 0.0593
trigger times: 13
Loss after 9752500 batches: 0.0564
trigger times: 14
Loss after 9753463 batches: 0.0550
trigger times: 15
Loss after 9754426 batches: 0.0570
trigger times: 16
Loss after 9755389 batches: 0.0550
trigger times: 17
Loss after 9756352 batches: 0.0527
trigger times: 18
Loss after 9757315 batches: 0.0508
trigger times: 19
Loss after 9758278 batches: 0.0516
trigger times: 20
Loss after 9759241 batches: 0.0503
trigger times: 21
Loss after 9760204 batches: 0.0494
trigger times: 22
Loss after 9761167 batches: 0.0493
trigger times: 23
Loss after 9762130 batches: 0.0471
trigger times: 24
Loss after 9763093 batches: 0.0483
trigger times: 25
Early stopping!
Start to test process.
Loss after 9764056 batches: 0.0473
Time to train on one home:  53.01008868217468
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9765019 batches: 0.0877
trigger times: 1
Loss after 9765982 batches: 0.0758
trigger times: 2
Loss after 9766945 batches: 0.0729
trigger times: 3
Loss after 9767908 batches: 0.0698
trigger times: 4
Loss after 9768871 batches: 0.0690
trigger times: 5
Loss after 9769834 batches: 0.0677
trigger times: 6
Loss after 9770797 batches: 0.0649
trigger times: 7
Loss after 9771760 batches: 0.0640
trigger times: 8
Loss after 9772723 batches: 0.0638
trigger times: 9
Loss after 9773686 batches: 0.0626
trigger times: 10
Loss after 9774649 batches: 0.0614
trigger times: 11
Loss after 9775612 batches: 0.0616
trigger times: 12
Loss after 9776575 batches: 0.0612
trigger times: 13
Loss after 9777538 batches: 0.0607
trigger times: 14
Loss after 9778501 batches: 0.0598
trigger times: 15
Loss after 9779464 batches: 0.0601
trigger times: 16
Loss after 9780427 batches: 0.0586
trigger times: 17
Loss after 9781390 batches: 0.0583
trigger times: 18
Loss after 9782353 batches: 0.0592
trigger times: 19
Loss after 9783316 batches: 0.0587
trigger times: 20
Loss after 9784279 batches: 0.0596
trigger times: 21
Loss after 9785242 batches: 0.0585
trigger times: 22
Loss after 9786205 batches: 0.0576
trigger times: 23
Loss after 9787168 batches: 0.0582
trigger times: 24
Loss after 9788131 batches: 0.0590
trigger times: 25
Early stopping!
Start to test process.
Loss after 9789094 batches: 0.0580
Time to train on one home:  52.58835244178772
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9790057 batches: 0.0871
trigger times: 1
Loss after 9791020 batches: 0.0759
trigger times: 2
Loss after 9791983 batches: 0.0755
trigger times: 3
Loss after 9792946 batches: 0.0716
trigger times: 4
Loss after 9793909 batches: 0.0692
trigger times: 5
Loss after 9794872 batches: 0.0674
trigger times: 6
Loss after 9795835 batches: 0.0647
trigger times: 7
Loss after 9796798 batches: 0.0640
trigger times: 8
Loss after 9797761 batches: 0.0608
trigger times: 9
Loss after 9798724 batches: 0.0600
trigger times: 10
Loss after 9799687 batches: 0.0585
trigger times: 11
Loss after 9800650 batches: 0.0570
trigger times: 12
Loss after 9801613 batches: 0.0561
trigger times: 13
Loss after 9802576 batches: 0.0568
trigger times: 14
Loss after 9803539 batches: 0.0550
trigger times: 15
Loss after 9804502 batches: 0.0553
trigger times: 16
Loss after 9805465 batches: 0.0557
trigger times: 17
Loss after 9806428 batches: 0.0542
trigger times: 18
Loss after 9807391 batches: 0.0553
trigger times: 19
Loss after 9808354 batches: 0.0540
trigger times: 20
Loss after 9809317 batches: 0.0563
trigger times: 21
Loss after 9810280 batches: 0.0550
trigger times: 22
Loss after 9811243 batches: 0.0533
trigger times: 23
Loss after 9812206 batches: 0.0515
trigger times: 24
Loss after 9813169 batches: 0.0548
trigger times: 25
Early stopping!
Start to test process.
Loss after 9814132 batches: 0.0546
Time to train on one home:  52.95043468475342
trigger times: 0
Loss after 9815095 batches: 0.0264
trigger times: 1
Loss after 9816058 batches: 0.0216
trigger times: 2
Loss after 9817021 batches: 0.0206
trigger times: 3
Loss after 9817984 batches: 0.0190
trigger times: 0
Loss after 9818947 batches: 0.0173
trigger times: 1
Loss after 9819910 batches: 0.0167
trigger times: 2
Loss after 9820873 batches: 0.0161
trigger times: 0
Loss after 9821836 batches: 0.0161
trigger times: 1
Loss after 9822799 batches: 0.0156
trigger times: 2
Loss after 9823762 batches: 0.0154
trigger times: 3
Loss after 9824725 batches: 0.0151
trigger times: 4
Loss after 9825688 batches: 0.0154
trigger times: 5
Loss after 9826651 batches: 0.0148
trigger times: 6
Loss after 9827614 batches: 0.0147
trigger times: 7
Loss after 9828577 batches: 0.0150
trigger times: 8
Loss after 9829540 batches: 0.0147
trigger times: 9
Loss after 9830503 batches: 0.0144
trigger times: 10
Loss after 9831466 batches: 0.0147
trigger times: 11
Loss after 9832429 batches: 0.0138
trigger times: 12
Loss after 9833392 batches: 0.0140
trigger times: 13
Loss after 9834355 batches: 0.0139
trigger times: 14
Loss after 9835318 batches: 0.0135
trigger times: 15
Loss after 9836281 batches: 0.0137
trigger times: 16
Loss after 9837244 batches: 0.0139
trigger times: 17
Loss after 9838207 batches: 0.0141
trigger times: 18
Loss after 9839170 batches: 0.0136
trigger times: 19
Loss after 9840133 batches: 0.0141
trigger times: 20
Loss after 9841096 batches: 0.0142
trigger times: 21
Loss after 9842059 batches: 0.0149
trigger times: 22
Loss after 9843022 batches: 0.0143
trigger times: 23
Loss after 9843985 batches: 0.0137
trigger times: 24
Loss after 9844948 batches: 0.0130
trigger times: 25
Early stopping!
Start to test process.
Loss after 9845911 batches: 0.0129
Time to train on one home:  57.857983350753784
trigger times: 0
Loss after 9846874 batches: 0.0441
trigger times: 1
Loss after 9847837 batches: 0.0376
trigger times: 2
Loss after 9848800 batches: 0.0361
trigger times: 3
Loss after 9849763 batches: 0.0325
trigger times: 4
Loss after 9850726 batches: 0.0305
trigger times: 5
Loss after 9851689 batches: 0.0291
trigger times: 6
Loss after 9852652 batches: 0.0278
trigger times: 7
Loss after 9853615 batches: 0.0268
trigger times: 8
Loss after 9854578 batches: 0.0258
trigger times: 9
Loss after 9855541 batches: 0.0248
trigger times: 10
Loss after 9856504 batches: 0.0252
trigger times: 11
Loss after 9857467 batches: 0.0254
trigger times: 12
Loss after 9858430 batches: 0.0249
trigger times: 13
Loss after 9859393 batches: 0.0241
trigger times: 14
Loss after 9860356 batches: 0.0239
trigger times: 15
Loss after 9861319 batches: 0.0232
trigger times: 16
Loss after 9862282 batches: 0.0225
trigger times: 17
Loss after 9863245 batches: 0.0233
trigger times: 18
Loss after 9864208 batches: 0.0227
trigger times: 19
Loss after 9865171 batches: 0.0228
trigger times: 20
Loss after 9866134 batches: 0.0228
trigger times: 21
Loss after 9867097 batches: 0.0222
trigger times: 22
Loss after 9868060 batches: 0.0219
trigger times: 23
Loss after 9869023 batches: 0.0224
trigger times: 24
Loss after 9869986 batches: 0.0228
trigger times: 25
Early stopping!
Start to test process.
Loss after 9870949 batches: 0.0219
Time to train on one home:  52.48378086090088
trigger times: 0
Loss after 9871912 batches: 0.0936
trigger times: 1
Loss after 9872875 batches: 0.0671
trigger times: 2
Loss after 9873838 batches: 0.0621
trigger times: 3
Loss after 9874801 batches: 0.0558
trigger times: 4
Loss after 9875764 batches: 0.0516
trigger times: 5
Loss after 9876727 batches: 0.0494
trigger times: 6
Loss after 9877690 batches: 0.0478
trigger times: 7
Loss after 9878653 batches: 0.0472
trigger times: 8
Loss after 9879616 batches: 0.0456
trigger times: 9
Loss after 9880579 batches: 0.0458
trigger times: 10
Loss after 9881542 batches: 0.0437
trigger times: 11
Loss after 9882505 batches: 0.0438
trigger times: 12
Loss after 9883468 batches: 0.0437
trigger times: 13
Loss after 9884431 batches: 0.0421
trigger times: 14
Loss after 9885394 batches: 0.0423
trigger times: 15
Loss after 9886357 batches: 0.0420
trigger times: 16
Loss after 9887320 batches: 0.0411
trigger times: 17
Loss after 9888283 batches: 0.0416
trigger times: 18
Loss after 9889246 batches: 0.0410
trigger times: 19
Loss after 9890209 batches: 0.0401
trigger times: 20
Loss after 9891172 batches: 0.0389
trigger times: 21
Loss after 9892135 batches: 0.0392
trigger times: 22
Loss after 9893098 batches: 0.0397
trigger times: 23
Loss after 9894061 batches: 0.0403
trigger times: 24
Loss after 9895024 batches: 0.0407
trigger times: 25
Early stopping!
Start to test process.
Loss after 9895987 batches: 0.0387
Time to train on one home:  52.32785439491272
trigger times: 0
Loss after 9896950 batches: 0.0914
trigger times: 1
Loss after 9897913 batches: 0.0644
trigger times: 2
Loss after 9898876 batches: 0.0592
trigger times: 3
Loss after 9899839 batches: 0.0554
trigger times: 4
Loss after 9900802 batches: 0.0522
trigger times: 5
Loss after 9901765 batches: 0.0500
trigger times: 6
Loss after 9902728 batches: 0.0479
trigger times: 7
Loss after 9903691 batches: 0.0466
trigger times: 8
Loss after 9904654 batches: 0.0455
trigger times: 9
Loss after 9905617 batches: 0.0451
trigger times: 10
Loss after 9906580 batches: 0.0440
trigger times: 11
Loss after 9907543 batches: 0.0421
trigger times: 12
Loss after 9908506 batches: 0.0435
trigger times: 13
Loss after 9909469 batches: 0.0429
trigger times: 14
Loss after 9910432 batches: 0.0417
trigger times: 15
Loss after 9911395 batches: 0.0422
trigger times: 16
Loss after 9912358 batches: 0.0419
trigger times: 17
Loss after 9913321 batches: 0.0418
trigger times: 18
Loss after 9914284 batches: 0.0416
trigger times: 19
Loss after 9915247 batches: 0.0408
trigger times: 20
Loss after 9916210 batches: 0.0394
trigger times: 21
Loss after 9917173 batches: 0.0404
trigger times: 22
Loss after 9918136 batches: 0.0396
trigger times: 23
Loss after 9919099 batches: 0.0396
trigger times: 24
Loss after 9920062 batches: 0.0394
trigger times: 25
Early stopping!
Start to test process.
Loss after 9921025 batches: 0.0382
Time to train on one home:  52.33291959762573
trigger times: 0
Loss after 9921988 batches: 0.0506
trigger times: 1
Loss after 9922951 batches: 0.0436
trigger times: 2
Loss after 9923914 batches: 0.0400
trigger times: 3
Loss after 9924877 batches: 0.0384
trigger times: 4
Loss after 9925840 batches: 0.0354
trigger times: 5
Loss after 9926803 batches: 0.0361
trigger times: 6
Loss after 9927766 batches: 0.0353
trigger times: 7
Loss after 9928729 batches: 0.0352
trigger times: 8
Loss after 9929692 batches: 0.0351
trigger times: 9
Loss after 9930655 batches: 0.0332
trigger times: 10
Loss after 9931618 batches: 0.0324
trigger times: 11
Loss after 9932581 batches: 0.0324
trigger times: 12
Loss after 9933544 batches: 0.0326
trigger times: 13
Loss after 9934507 batches: 0.0321
trigger times: 14
Loss after 9935470 batches: 0.0316
trigger times: 15
Loss after 9936433 batches: 0.0317
trigger times: 16
Loss after 9937396 batches: 0.0337
trigger times: 17
Loss after 9938359 batches: 0.0333
trigger times: 18
Loss after 9939322 batches: 0.0327
trigger times: 19
Loss after 9940285 batches: 0.0333
trigger times: 20
Loss after 9941248 batches: 0.0328
trigger times: 21
Loss after 9942211 batches: 0.0338
trigger times: 22
Loss after 9943174 batches: 0.0321
trigger times: 23
Loss after 9944137 batches: 0.0313
trigger times: 24
Loss after 9945100 batches: 0.0332
trigger times: 25
Early stopping!
Start to test process.
Loss after 9946063 batches: 0.0324
Time to train on one home:  52.18400526046753
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 9947026 batches: 0.0844
trigger times: 1
Loss after 9947989 batches: 0.0563
trigger times: 2
Loss after 9948952 batches: 0.0557
trigger times: 0
Loss after 9949915 batches: 0.0474
trigger times: 0
Loss after 9950878 batches: 0.0442
trigger times: 1
Loss after 9951841 batches: 0.0405
trigger times: 2
Loss after 9952804 batches: 0.0381
trigger times: 3
Loss after 9953767 batches: 0.0377
trigger times: 4
Loss after 9954730 batches: 0.0360
trigger times: 5
Loss after 9955693 batches: 0.0344
trigger times: 6
Loss after 9956656 batches: 0.0342
trigger times: 0
Loss after 9957619 batches: 0.0335
trigger times: 1
Loss after 9958582 batches: 0.0329
trigger times: 2
Loss after 9959545 batches: 0.0321
trigger times: 3
Loss after 9960508 batches: 0.0311
trigger times: 4
Loss after 9961471 batches: 0.0315
trigger times: 5
Loss after 9962434 batches: 0.0305
trigger times: 6
Loss after 9963397 batches: 0.0310
trigger times: 7
Loss after 9964360 batches: 0.0296
trigger times: 8
Loss after 9965323 batches: 0.0302
trigger times: 9
Loss after 9966286 batches: 0.0296
trigger times: 10
Loss after 9967249 batches: 0.0293
trigger times: 11
Loss after 9968212 batches: 0.0298
trigger times: 12
Loss after 9969175 batches: 0.0289
trigger times: 13
Loss after 9970138 batches: 0.0275
trigger times: 14
Loss after 9971101 batches: 0.0285
trigger times: 15
Loss after 9972064 batches: 0.0282
trigger times: 16
Loss after 9973027 batches: 0.0272
trigger times: 17
Loss after 9973990 batches: 0.0266
trigger times: 18
Loss after 9974953 batches: 0.0282
trigger times: 19
Loss after 9975916 batches: 0.0278
trigger times: 20
Loss after 9976879 batches: 0.0284
trigger times: 21
Loss after 9977842 batches: 0.0274
trigger times: 22
Loss after 9978805 batches: 0.0263
trigger times: 23
Loss after 9979768 batches: 0.0273
trigger times: 24
Loss after 9980731 batches: 0.0259
trigger times: 25
Early stopping!
Start to test process.
Loss after 9981694 batches: 0.0263
Time to train on one home:  60.923370361328125
trigger times: 0
Loss after 9982657 batches: 0.1016
trigger times: 1
Loss after 9983620 batches: 0.0788
trigger times: 2
Loss after 9984583 batches: 0.0774
trigger times: 3
Loss after 9985546 batches: 0.0766
trigger times: 4
Loss after 9986509 batches: 0.0734
trigger times: 5
Loss after 9987472 batches: 0.0714
trigger times: 6
Loss after 9988435 batches: 0.0698
trigger times: 7
Loss after 9989398 batches: 0.0665
trigger times: 8
Loss after 9990361 batches: 0.0661
trigger times: 9
Loss after 9991324 batches: 0.0662
trigger times: 10
Loss after 9992287 batches: 0.0647
trigger times: 11
Loss after 9993250 batches: 0.0637
trigger times: 12
Loss after 9994213 batches: 0.0643
trigger times: 13
Loss after 9995176 batches: 0.0627
trigger times: 14
Loss after 9996139 batches: 0.0621
trigger times: 15
Loss after 9997102 batches: 0.0618
trigger times: 16
Loss after 9998065 batches: 0.0606
trigger times: 17
Loss after 9999028 batches: 0.0607
trigger times: 18
Loss after 9999991 batches: 0.0605
trigger times: 19
Loss after 10000954 batches: 0.0610
trigger times: 20
Loss after 10001917 batches: 0.0610
trigger times: 21
Loss after 10002880 batches: 0.0613
trigger times: 22
Loss after 10003843 batches: 0.0618
trigger times: 23
Loss after 10004806 batches: 0.0606
trigger times: 24
Loss after 10005769 batches: 0.0595
trigger times: 25
Early stopping!
Start to test process.
Loss after 10006732 batches: 0.0594
Time to train on one home:  52.70192360877991
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10007695 batches: 0.0839
trigger times: 1
Loss after 10008658 batches: 0.0585
trigger times: 2
Loss after 10009621 batches: 0.0565
trigger times: 0
Loss after 10010584 batches: 0.0481
trigger times: 1
Loss after 10011547 batches: 0.0444
trigger times: 2
Loss after 10012510 batches: 0.0404
trigger times: 0
Loss after 10013473 batches: 0.0388
trigger times: 1
Loss after 10014436 batches: 0.0385
trigger times: 2
Loss after 10015399 batches: 0.0356
trigger times: 3
Loss after 10016362 batches: 0.0354
trigger times: 4
Loss after 10017325 batches: 0.0339
trigger times: 5
Loss after 10018288 batches: 0.0334
trigger times: 6
Loss after 10019251 batches: 0.0333
trigger times: 7
Loss after 10020214 batches: 0.0331
trigger times: 8
Loss after 10021177 batches: 0.0323
trigger times: 9
Loss after 10022140 batches: 0.0322
trigger times: 10
Loss after 10023103 batches: 0.0309
trigger times: 0
Loss after 10024066 batches: 0.0308
trigger times: 1
Loss after 10025029 batches: 0.0309
trigger times: 2
Loss after 10025992 batches: 0.0303
trigger times: 3
Loss after 10026955 batches: 0.0298
trigger times: 4
Loss after 10027918 batches: 0.0292
trigger times: 5
Loss after 10028881 batches: 0.0284
trigger times: 6
Loss after 10029844 batches: 0.0299
trigger times: 0
Loss after 10030807 batches: 0.0284
trigger times: 1
Loss after 10031770 batches: 0.0277
trigger times: 2
Loss after 10032733 batches: 0.0277
trigger times: 0
Loss after 10033696 batches: 0.0281
trigger times: 1
Loss after 10034659 batches: 0.0283
trigger times: 2
Loss after 10035622 batches: 0.0278
trigger times: 3
Loss after 10036585 batches: 0.0277
trigger times: 4
Loss after 10037548 batches: 0.0265
trigger times: 5
Loss after 10038511 batches: 0.0261
trigger times: 6
Loss after 10039474 batches: 0.0259
trigger times: 0
Loss after 10040437 batches: 0.0269
trigger times: 1
Loss after 10041400 batches: 0.0261
trigger times: 2
Loss after 10042363 batches: 0.0275
trigger times: 3
Loss after 10043326 batches: 0.0263
trigger times: 4
Loss after 10044289 batches: 0.0255
trigger times: 5
Loss after 10045252 batches: 0.0268
trigger times: 6
Loss after 10046215 batches: 0.0259
trigger times: 7
Loss after 10047178 batches: 0.0257
trigger times: 8
Loss after 10048141 batches: 0.0258
trigger times: 9
Loss after 10049104 batches: 0.0253
trigger times: 10
Loss after 10050067 batches: 0.0259
trigger times: 11
Loss after 10051030 batches: 0.0262
trigger times: 12
Loss after 10051993 batches: 0.0263
trigger times: 13
Loss after 10052956 batches: 0.0238
trigger times: 14
Loss after 10053919 batches: 0.0251
trigger times: 15
Loss after 10054882 batches: 0.0264
trigger times: 16
Loss after 10055845 batches: 0.0256
trigger times: 17
Loss after 10056808 batches: 0.0250
trigger times: 18
Loss after 10057771 batches: 0.0235
trigger times: 19
Loss after 10058734 batches: 0.0234
trigger times: 20
Loss after 10059697 batches: 0.0242
trigger times: 21
Loss after 10060660 batches: 0.0233
trigger times: 22
Loss after 10061623 batches: 0.0236
trigger times: 23
Loss after 10062586 batches: 0.0231
trigger times: 24
Loss after 10063549 batches: 0.0223
trigger times: 25
Early stopping!
Start to test process.
Loss after 10064512 batches: 0.0225
Time to train on one home:  78.0062358379364
trigger times: 0
Loss after 10065407 batches: 0.0584
trigger times: 1
Loss after 10066302 batches: 0.0315
trigger times: 2
Loss after 10067197 batches: 0.0144
trigger times: 3
Loss after 10068092 batches: 0.0100
trigger times: 4
Loss after 10068987 batches: 0.0089
trigger times: 5
Loss after 10069882 batches: 0.0063
trigger times: 6
Loss after 10070777 batches: 0.0071
trigger times: 7
Loss after 10071672 batches: 0.0058
trigger times: 8
Loss after 10072567 batches: 0.0056
trigger times: 9
Loss after 10073462 batches: 0.0056
trigger times: 10
Loss after 10074357 batches: 0.0045
trigger times: 11
Loss after 10075252 batches: 0.0039
trigger times: 12
Loss after 10076147 batches: 0.0038
trigger times: 13
Loss after 10077042 batches: 0.0034
trigger times: 14
Loss after 10077937 batches: 0.0039
trigger times: 15
Loss after 10078832 batches: 0.0037
trigger times: 16
Loss after 10079727 batches: 0.0041
trigger times: 17
Loss after 10080622 batches: 0.0039
trigger times: 18
Loss after 10081517 batches: 0.0089
trigger times: 19
Loss after 10082412 batches: 0.0046
trigger times: 20
Loss after 10083307 batches: 0.0072
trigger times: 21
Loss after 10084202 batches: 0.0052
trigger times: 22
Loss after 10085097 batches: 0.0041
trigger times: 23
Loss after 10085992 batches: 0.0041
trigger times: 24
Loss after 10086887 batches: 0.0041
trigger times: 25
Early stopping!
Start to test process.
Loss after 10087782 batches: 0.0037
Time to train on one home:  51.32339525222778
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10088719 batches: 0.0814
trigger times: 1
Loss after 10089656 batches: 0.0693
trigger times: 2
Loss after 10090593 batches: 0.0670
trigger times: 3
Loss after 10091530 batches: 0.0635
trigger times: 4
Loss after 10092467 batches: 0.0613
trigger times: 5
Loss after 10093404 batches: 0.0601
trigger times: 6
Loss after 10094341 batches: 0.0583
trigger times: 7
Loss after 10095278 batches: 0.0570
trigger times: 8
Loss after 10096215 batches: 0.0555
trigger times: 9
Loss after 10097152 batches: 0.0553
trigger times: 10
Loss after 10098089 batches: 0.0548
trigger times: 11
Loss after 10099026 batches: 0.0541
trigger times: 12
Loss after 10099963 batches: 0.0542
trigger times: 13
Loss after 10100900 batches: 0.0548
trigger times: 14
Loss after 10101837 batches: 0.0531
trigger times: 15
Loss after 10102774 batches: 0.0548
trigger times: 16
Loss after 10103711 batches: 0.0523
trigger times: 17
Loss after 10104648 batches: 0.0528
trigger times: 18
Loss after 10105585 batches: 0.0514
trigger times: 19
Loss after 10106522 batches: 0.0509
trigger times: 20
Loss after 10107459 batches: 0.0511
trigger times: 21
Loss after 10108396 batches: 0.0507
trigger times: 22
Loss after 10109333 batches: 0.0495
trigger times: 23
Loss after 10110270 batches: 0.0509
trigger times: 24
Loss after 10111207 batches: 0.0513
trigger times: 25
Early stopping!
Start to test process.
Loss after 10112144 batches: 0.0500
Time to train on one home:  52.55402636528015
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863]]
Round_7_results:  [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 10547 < 10548; dropping {'Training_Loss': 0.07126072155577796, 'Validation_Loss': 0.08228766173124313, 'Training_R2': -0.11164349268185281, 'Validation_R2': 0.12267119568175266, 'Training_F1': 0.42200632482433664, 'Validation_F1': 0.5992908639664182, 'Training_NEP': 0.838457540271965, 'Validation_NEP': 0.7727315955609516, 'Training_NDE': 0.6770178448420064, 'Validation_NDE': 0.7926457153869582, 'Training_MAE': 31.861656295563638, 'Validation_MAE': 27.47777432588115, 'Training_MSE': 2500.4885, 'Validation_MSE': 10383.669}.
trigger times: 0
Loss after 10113106 batches: 0.0713
trigger times: 1
Loss after 10114068 batches: 0.0651
trigger times: 2
Loss after 10115030 batches: 0.0633
trigger times: 3
Loss after 10115992 batches: 0.0603
trigger times: 4
Loss after 10116954 batches: 0.0588
trigger times: 5
Loss after 10117916 batches: 0.0566
trigger times: 6
Loss after 10118878 batches: 0.0551
trigger times: 7
Loss after 10119840 batches: 0.0545
trigger times: 8
Loss after 10120802 batches: 0.0545
trigger times: 9
Loss after 10121764 batches: 0.0541
trigger times: 10
Loss after 10122726 batches: 0.0534
trigger times: 11
Loss after 10123688 batches: 0.0527
trigger times: 12
Loss after 10124650 batches: 0.0525
trigger times: 13
Loss after 10125612 batches: 0.0527
trigger times: 14
Loss after 10126574 batches: 0.0525
trigger times: 15
Loss after 10127536 batches: 0.0526
trigger times: 16
Loss after 10128498 batches: 0.0512
trigger times: 17
Loss after 10129460 batches: 0.0516
trigger times: 18
Loss after 10130422 batches: 0.0527
trigger times: 19
Loss after 10131384 batches: 0.0515
trigger times: 20
Loss after 10132346 batches: 0.0512
trigger times: 21
Loss after 10133308 batches: 0.0505
trigger times: 22
Loss after 10134270 batches: 0.0512
trigger times: 23
Loss after 10135232 batches: 0.0515
trigger times: 24
Loss after 10136194 batches: 0.0522
trigger times: 25
Early stopping!
Start to test process.
Loss after 10137156 batches: 0.0519
Time to train on one home:  52.680989503860474
trigger times: 0
Loss after 10138085 batches: 0.1158
trigger times: 1
Loss after 10139014 batches: 0.0733
trigger times: 2
Loss after 10139943 batches: 0.0566
trigger times: 3
Loss after 10140872 batches: 0.0470
trigger times: 4
Loss after 10141801 batches: 0.0400
trigger times: 5
Loss after 10142730 batches: 0.0374
trigger times: 6
Loss after 10143659 batches: 0.0362
trigger times: 7
Loss after 10144588 batches: 0.0350
trigger times: 8
Loss after 10145517 batches: 0.0331
trigger times: 9
Loss after 10146446 batches: 0.0323
trigger times: 10
Loss after 10147375 batches: 0.0343
trigger times: 11
Loss after 10148304 batches: 0.0325
trigger times: 12
Loss after 10149233 batches: 0.0316
trigger times: 13
Loss after 10150162 batches: 0.0325
trigger times: 14
Loss after 10151091 batches: 0.0316
trigger times: 15
Loss after 10152020 batches: 0.0349
trigger times: 16
Loss after 10152949 batches: 0.0289
trigger times: 17
Loss after 10153878 batches: 0.0284
trigger times: 18
Loss after 10154807 batches: 0.0273
trigger times: 19
Loss after 10155736 batches: 0.0282
trigger times: 20
Loss after 10156665 batches: 0.0284
trigger times: 21
Loss after 10157594 batches: 0.0351
trigger times: 22
Loss after 10158523 batches: 0.0355
trigger times: 23
Loss after 10159452 batches: 0.0343
trigger times: 24
Loss after 10160381 batches: 0.0306
trigger times: 25
Early stopping!
Start to test process.
Loss after 10161310 batches: 0.0294
Time to train on one home:  52.47922706604004
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 10162273 batches: 0.0347
trigger times: 1
Loss after 10163236 batches: 0.0173
trigger times: 2
Loss after 10164199 batches: 0.0142
trigger times: 3
Loss after 10165162 batches: 0.0139
trigger times: 4
Loss after 10166125 batches: 0.0137
trigger times: 5
Loss after 10167088 batches: 0.0132
trigger times: 6
Loss after 10168051 batches: 0.0127
trigger times: 7
Loss after 10169014 batches: 0.0123
trigger times: 8
Loss after 10169977 batches: 0.0116
trigger times: 9
Loss after 10170940 batches: 0.0109
trigger times: 10
Loss after 10171903 batches: 0.0105
trigger times: 11
Loss after 10172866 batches: 0.0099
trigger times: 12
Loss after 10173829 batches: 0.0094
trigger times: 13
Loss after 10174792 batches: 0.0089
trigger times: 14
Loss after 10175755 batches: 0.0088
trigger times: 15
Loss after 10176718 batches: 0.0085
trigger times: 16
Loss after 10177681 batches: 0.0082
trigger times: 17
Loss after 10178644 batches: 0.0079
trigger times: 18
Loss after 10179607 batches: 0.0081
trigger times: 19
Loss after 10180570 batches: 0.0076
trigger times: 20
Loss after 10181533 batches: 0.0075
trigger times: 21
Loss after 10182496 batches: 0.0074
trigger times: 22
Loss after 10183459 batches: 0.0074
trigger times: 23
Loss after 10184422 batches: 0.0074
trigger times: 24
Loss after 10185385 batches: 0.0072
trigger times: 25
Early stopping!
Start to test process.
Loss after 10186348 batches: 0.0070
Time to train on one home:  53.28410863876343
trigger times: 0
Loss after 10187311 batches: 0.0230
trigger times: 0
Loss after 10188274 batches: 0.0213
trigger times: 1
Loss after 10189237 batches: 0.0192
trigger times: 2
Loss after 10190200 batches: 0.0184
trigger times: 3
Loss after 10191163 batches: 0.0169
trigger times: 4
Loss after 10192126 batches: 0.0161
trigger times: 5
Loss after 10193089 batches: 0.0155
trigger times: 6
Loss after 10194052 batches: 0.0149
trigger times: 0
Loss after 10195015 batches: 0.0150
trigger times: 1
Loss after 10195978 batches: 0.0147
trigger times: 2
Loss after 10196941 batches: 0.0148
trigger times: 3
Loss after 10197904 batches: 0.0148
trigger times: 4
Loss after 10198867 batches: 0.0146
trigger times: 5
Loss after 10199830 batches: 0.0146
trigger times: 6
Loss after 10200793 batches: 0.0140
trigger times: 7
Loss after 10201756 batches: 0.0137
trigger times: 8
Loss after 10202719 batches: 0.0140
trigger times: 9
Loss after 10203682 batches: 0.0143
trigger times: 10
Loss after 10204645 batches: 0.0137
trigger times: 11
Loss after 10205608 batches: 0.0139
trigger times: 12
Loss after 10206571 batches: 0.0131
trigger times: 13
Loss after 10207534 batches: 0.0130
trigger times: 14
Loss after 10208497 batches: 0.0134
trigger times: 15
Loss after 10209460 batches: 0.0132
trigger times: 16
Loss after 10210423 batches: 0.0127
trigger times: 17
Loss after 10211386 batches: 0.0133
trigger times: 18
Loss after 10212349 batches: 0.0128
trigger times: 19
Loss after 10213312 batches: 0.0124
trigger times: 20
Loss after 10214275 batches: 0.0124
trigger times: 21
Loss after 10215238 batches: 0.0128
trigger times: 22
Loss after 10216201 batches: 0.0130
trigger times: 23
Loss after 10217164 batches: 0.0121
trigger times: 24
Loss after 10218127 batches: 0.0120
trigger times: 25
Early stopping!
Start to test process.
Loss after 10219090 batches: 0.0124
Time to train on one home:  59.05231952667236
trigger times: 0
Loss after 10220053 batches: 0.0978
trigger times: 1
Loss after 10221016 batches: 0.0891
trigger times: 2
Loss after 10221979 batches: 0.0834
trigger times: 3
Loss after 10222942 batches: 0.0802
trigger times: 4
Loss after 10223905 batches: 0.0787
trigger times: 5
Loss after 10224868 batches: 0.0771
trigger times: 6
Loss after 10225831 batches: 0.0758
trigger times: 7
Loss after 10226794 batches: 0.0732
trigger times: 8
Loss after 10227757 batches: 0.0720
trigger times: 9
Loss after 10228720 batches: 0.0697
trigger times: 10
Loss after 10229683 batches: 0.0698
trigger times: 11
Loss after 10230646 batches: 0.0700
trigger times: 12
Loss after 10231609 batches: 0.0670
trigger times: 13
Loss after 10232572 batches: 0.0671
trigger times: 14
Loss after 10233535 batches: 0.0671
trigger times: 15
Loss after 10234498 batches: 0.0674
trigger times: 16
Loss after 10235461 batches: 0.0657
trigger times: 17
Loss after 10236424 batches: 0.0651
trigger times: 18
Loss after 10237387 batches: 0.0657
trigger times: 19
Loss after 10238350 batches: 0.0660
trigger times: 20
Loss after 10239313 batches: 0.0643
trigger times: 21
Loss after 10240276 batches: 0.0636
trigger times: 22
Loss after 10241239 batches: 0.0635
trigger times: 23
Loss after 10242202 batches: 0.0611
trigger times: 24
Loss after 10243165 batches: 0.0622
trigger times: 25
Early stopping!
Start to test process.
Loss after 10244128 batches: 0.0616
Time to train on one home:  52.98110580444336
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10245091 batches: 0.0794
trigger times: 1
Loss after 10246054 batches: 0.0726
trigger times: 2
Loss after 10247017 batches: 0.0694
trigger times: 3
Loss after 10247980 batches: 0.0678
trigger times: 4
Loss after 10248943 batches: 0.0657
trigger times: 5
Loss after 10249906 batches: 0.0650
trigger times: 6
Loss after 10250869 batches: 0.0638
trigger times: 7
Loss after 10251832 batches: 0.0612
trigger times: 8
Loss after 10252795 batches: 0.0629
trigger times: 9
Loss after 10253758 batches: 0.0616
trigger times: 10
Loss after 10254721 batches: 0.0614
trigger times: 11
Loss after 10255684 batches: 0.0598
trigger times: 12
Loss after 10256647 batches: 0.0598
trigger times: 13
Loss after 10257610 batches: 0.0599
trigger times: 14
Loss after 10258573 batches: 0.0592
trigger times: 15
Loss after 10259536 batches: 0.0580
trigger times: 16
Loss after 10260499 batches: 0.0585
trigger times: 17
Loss after 10261462 batches: 0.0584
trigger times: 18
Loss after 10262425 batches: 0.0581
trigger times: 19
Loss after 10263388 batches: 0.0579
trigger times: 20
Loss after 10264351 batches: 0.0572
trigger times: 21
Loss after 10265314 batches: 0.0567
trigger times: 22
Loss after 10266277 batches: 0.0558
trigger times: 23
Loss after 10267240 batches: 0.0565
trigger times: 24
Loss after 10268203 batches: 0.0553
trigger times: 25
Early stopping!
Start to test process.
Loss after 10269166 batches: 0.0548
Time to train on one home:  52.58620262145996
trigger times: 0
Loss after 10270129 batches: 0.0746
trigger times: 1
Loss after 10271092 batches: 0.0710
trigger times: 2
Loss after 10272055 batches: 0.0671
trigger times: 3
Loss after 10273018 batches: 0.0645
trigger times: 4
Loss after 10273981 batches: 0.0636
trigger times: 5
Loss after 10274944 batches: 0.0619
trigger times: 6
Loss after 10275907 batches: 0.0604
trigger times: 7
Loss after 10276870 batches: 0.0598
trigger times: 8
Loss after 10277833 batches: 0.0585
trigger times: 9
Loss after 10278796 batches: 0.0580
trigger times: 10
Loss after 10279759 batches: 0.0573
trigger times: 11
Loss after 10280722 batches: 0.0559
trigger times: 12
Loss after 10281685 batches: 0.0562
trigger times: 13
Loss after 10282648 batches: 0.0567
trigger times: 14
Loss after 10283611 batches: 0.0563
trigger times: 15
Loss after 10284574 batches: 0.0552
trigger times: 16
Loss after 10285537 batches: 0.0553
trigger times: 17
Loss after 10286500 batches: 0.0541
trigger times: 18
Loss after 10287463 batches: 0.0538
trigger times: 19
Loss after 10288426 batches: 0.0527
trigger times: 20
Loss after 10289389 batches: 0.0539
trigger times: 21
Loss after 10290352 batches: 0.0532
trigger times: 22
Loss after 10291315 batches: 0.0541
trigger times: 23
Loss after 10292278 batches: 0.0524
trigger times: 24
Loss after 10293241 batches: 0.0527
trigger times: 25
Early stopping!
Start to test process.
Loss after 10294204 batches: 0.0546
Time to train on one home:  52.8540940284729
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10295167 batches: 0.0634
trigger times: 1
Loss after 10296130 batches: 0.0515
trigger times: 2
Loss after 10297093 batches: 0.0498
trigger times: 3
Loss after 10298056 batches: 0.0448
trigger times: 4
Loss after 10299019 batches: 0.0390
trigger times: 5
Loss after 10299982 batches: 0.0354
trigger times: 6
Loss after 10300945 batches: 0.0340
trigger times: 7
Loss after 10301908 batches: 0.0322
trigger times: 8
Loss after 10302871 batches: 0.0310
trigger times: 9
Loss after 10303834 batches: 0.0307
trigger times: 10
Loss after 10304797 batches: 0.0285
trigger times: 11
Loss after 10305760 batches: 0.0290
trigger times: 12
Loss after 10306723 batches: 0.0286
trigger times: 13
Loss after 10307686 batches: 0.0282
trigger times: 14
Loss after 10308649 batches: 0.0273
trigger times: 15
Loss after 10309612 batches: 0.0273
trigger times: 16
Loss after 10310575 batches: 0.0273
trigger times: 17
Loss after 10311538 batches: 0.0262
trigger times: 18
Loss after 10312501 batches: 0.0265
trigger times: 19
Loss after 10313464 batches: 0.0265
trigger times: 20
Loss after 10314427 batches: 0.0253
trigger times: 21
Loss after 10315390 batches: 0.0259
trigger times: 22
Loss after 10316353 batches: 0.0254
trigger times: 23
Loss after 10317316 batches: 0.0251
trigger times: 24
Loss after 10318279 batches: 0.0246
trigger times: 25
Early stopping!
Start to test process.
Loss after 10319242 batches: 0.0243
Time to train on one home:  52.65915632247925
trigger times: 0
Loss after 10320200 batches: 0.0833
trigger times: 1
Loss after 10321158 batches: 0.0484
trigger times: 2
Loss after 10322116 batches: 0.0401
trigger times: 3
Loss after 10323074 batches: 0.0367
trigger times: 4
Loss after 10324032 batches: 0.0331
trigger times: 5
Loss after 10324990 batches: 0.0293
trigger times: 6
Loss after 10325948 batches: 0.0271
trigger times: 7
Loss after 10326906 batches: 0.0273
trigger times: 8
Loss after 10327864 batches: 0.0260
trigger times: 9
Loss after 10328822 batches: 0.0239
trigger times: 10
Loss after 10329780 batches: 0.0243
trigger times: 11
Loss after 10330738 batches: 0.0230
trigger times: 12
Loss after 10331696 batches: 0.0216
trigger times: 13
Loss after 10332654 batches: 0.0214
trigger times: 14
Loss after 10333612 batches: 0.0202
trigger times: 15
Loss after 10334570 batches: 0.0206
trigger times: 16
Loss after 10335528 batches: 0.0197
trigger times: 17
Loss after 10336486 batches: 0.0189
trigger times: 18
Loss after 10337444 batches: 0.0186
trigger times: 19
Loss after 10338402 batches: 0.0192
trigger times: 20
Loss after 10339360 batches: 0.0198
trigger times: 21
Loss after 10340318 batches: 0.0185
trigger times: 22
Loss after 10341276 batches: 0.0200
trigger times: 23
Loss after 10342234 batches: 0.0195
trigger times: 24
Loss after 10343192 batches: 0.0187
trigger times: 25
Early stopping!
Start to test process.
Loss after 10344150 batches: 0.0182
Time to train on one home:  52.95902466773987
trigger times: 0
Loss after 10345112 batches: 0.0706
trigger times: 1
Loss after 10346074 batches: 0.0649
trigger times: 2
Loss after 10347036 batches: 0.0631
trigger times: 3
Loss after 10347998 batches: 0.0599
trigger times: 4
Loss after 10348960 batches: 0.0578
trigger times: 5
Loss after 10349922 batches: 0.0568
trigger times: 6
Loss after 10350884 batches: 0.0552
trigger times: 7
Loss after 10351846 batches: 0.0542
trigger times: 8
Loss after 10352808 batches: 0.0542
trigger times: 9
Loss after 10353770 batches: 0.0539
trigger times: 10
Loss after 10354732 batches: 0.0539
trigger times: 11
Loss after 10355694 batches: 0.0543
trigger times: 12
Loss after 10356656 batches: 0.0534
trigger times: 13
Loss after 10357618 batches: 0.0528
trigger times: 14
Loss after 10358580 batches: 0.0522
trigger times: 15
Loss after 10359542 batches: 0.0522
trigger times: 16
Loss after 10360504 batches: 0.0507
trigger times: 17
Loss after 10361466 batches: 0.0511
trigger times: 18
Loss after 10362428 batches: 0.0512
trigger times: 19
Loss after 10363390 batches: 0.0516
trigger times: 20
Loss after 10364352 batches: 0.0511
trigger times: 21
Loss after 10365314 batches: 0.0512
trigger times: 22
Loss after 10366276 batches: 0.0514
trigger times: 23
Loss after 10367238 batches: 0.0500
trigger times: 24
Loss after 10368200 batches: 0.0511
trigger times: 25
Early stopping!
Start to test process.
Loss after 10369162 batches: 0.0502
Time to train on one home:  52.75507879257202
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10370125 batches: 0.0668
trigger times: 1
Loss after 10371088 batches: 0.0264
trigger times: 2
Loss after 10372051 batches: 0.0193
trigger times: 3
Loss after 10373014 batches: 0.0166
trigger times: 4
Loss after 10373977 batches: 0.0156
trigger times: 5
Loss after 10374940 batches: 0.0145
trigger times: 6
Loss after 10375903 batches: 0.0148
trigger times: 7
Loss after 10376866 batches: 0.0138
trigger times: 8
Loss after 10377829 batches: 0.0135
trigger times: 9
Loss after 10378792 batches: 0.0135
trigger times: 10
Loss after 10379755 batches: 0.0133
trigger times: 11
Loss after 10380718 batches: 0.0131
trigger times: 12
Loss after 10381681 batches: 0.0130
trigger times: 13
Loss after 10382644 batches: 0.0135
trigger times: 14
Loss after 10383607 batches: 0.0129
trigger times: 15
Loss after 10384570 batches: 0.0128
trigger times: 16
Loss after 10385533 batches: 0.0128
trigger times: 17
Loss after 10386496 batches: 0.0129
trigger times: 18
Loss after 10387459 batches: 0.0128
trigger times: 19
Loss after 10388422 batches: 0.0127
trigger times: 20
Loss after 10389385 batches: 0.0125
trigger times: 21
Loss after 10390348 batches: 0.0129
trigger times: 22
Loss after 10391311 batches: 0.0124
trigger times: 23
Loss after 10392274 batches: 0.0124
trigger times: 24
Loss after 10393237 batches: 0.0123
trigger times: 25
Early stopping!
Start to test process.
Loss after 10394200 batches: 0.0129
Time to train on one home:  52.913020849227905
trigger times: 0
Loss after 10395163 batches: 0.0578
trigger times: 1
Loss after 10396126 batches: 0.0441
trigger times: 2
Loss after 10397089 batches: 0.0420
trigger times: 3
Loss after 10398052 batches: 0.0389
trigger times: 4
Loss after 10399015 batches: 0.0357
trigger times: 5
Loss after 10399978 batches: 0.0360
trigger times: 6
Loss after 10400941 batches: 0.0342
trigger times: 7
Loss after 10401904 batches: 0.0335
trigger times: 8
Loss after 10402867 batches: 0.0334
trigger times: 9
Loss after 10403830 batches: 0.0341
trigger times: 10
Loss after 10404793 batches: 0.0328
trigger times: 11
Loss after 10405756 batches: 0.0355
trigger times: 12
Loss after 10406719 batches: 0.0345
trigger times: 13
Loss after 10407682 batches: 0.0340
trigger times: 14
Loss after 10408645 batches: 0.0322
trigger times: 15
Loss after 10409608 batches: 0.0315
trigger times: 16
Loss after 10410571 batches: 0.0305
trigger times: 17
Loss after 10411534 batches: 0.0297
trigger times: 18
Loss after 10412497 batches: 0.0312
trigger times: 19
Loss after 10413460 batches: 0.0313
trigger times: 20
Loss after 10414423 batches: 0.0303
trigger times: 21
Loss after 10415386 batches: 0.0311
trigger times: 22
Loss after 10416349 batches: 0.0306
trigger times: 23
Loss after 10417312 batches: 0.0303
trigger times: 24
Loss after 10418275 batches: 0.0297
trigger times: 25
Early stopping!
Start to test process.
Loss after 10419238 batches: 0.0284
Time to train on one home:  53.04499959945679
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10420201 batches: 0.0638
trigger times: 1
Loss after 10421164 batches: 0.0518
trigger times: 2
Loss after 10422127 batches: 0.0498
trigger times: 3
Loss after 10423090 batches: 0.0457
trigger times: 4
Loss after 10424053 batches: 0.0392
trigger times: 5
Loss after 10425016 batches: 0.0361
trigger times: 6
Loss after 10425979 batches: 0.0334
trigger times: 7
Loss after 10426942 batches: 0.0313
trigger times: 8
Loss after 10427905 batches: 0.0306
trigger times: 9
Loss after 10428868 batches: 0.0291
trigger times: 10
Loss after 10429831 batches: 0.0289
trigger times: 11
Loss after 10430794 batches: 0.0283
trigger times: 12
Loss after 10431757 batches: 0.0277
trigger times: 13
Loss after 10432720 batches: 0.0275
trigger times: 14
Loss after 10433683 batches: 0.0281
trigger times: 15
Loss after 10434646 batches: 0.0275
trigger times: 16
Loss after 10435609 batches: 0.0268
trigger times: 17
Loss after 10436572 batches: 0.0267
trigger times: 18
Loss after 10437535 batches: 0.0259
trigger times: 19
Loss after 10438498 batches: 0.0264
trigger times: 20
Loss after 10439461 batches: 0.0252
trigger times: 21
Loss after 10440424 batches: 0.0256
trigger times: 22
Loss after 10441387 batches: 0.0256
trigger times: 23
Loss after 10442350 batches: 0.0252
trigger times: 24
Loss after 10443313 batches: 0.0247
trigger times: 25
Early stopping!
Start to test process.
Loss after 10444276 batches: 0.0249
Time to train on one home:  52.40515637397766
trigger times: 0
Loss after 10445239 batches: 0.0580
trigger times: 1
Loss after 10446202 batches: 0.0447
trigger times: 2
Loss after 10447165 batches: 0.0410
trigger times: 3
Loss after 10448128 batches: 0.0379
trigger times: 4
Loss after 10449091 batches: 0.0366
trigger times: 5
Loss after 10450054 batches: 0.0360
trigger times: 6
Loss after 10451017 batches: 0.0349
trigger times: 7
Loss after 10451980 batches: 0.0337
trigger times: 8
Loss after 10452943 batches: 0.0327
trigger times: 9
Loss after 10453906 batches: 0.0322
trigger times: 10
Loss after 10454869 batches: 0.0311
trigger times: 11
Loss after 10455832 batches: 0.0333
trigger times: 12
Loss after 10456795 batches: 0.0325
trigger times: 13
Loss after 10457758 batches: 0.0321
trigger times: 14
Loss after 10458721 batches: 0.0316
trigger times: 15
Loss after 10459684 batches: 0.0318
trigger times: 16
Loss after 10460647 batches: 0.0308
trigger times: 17
Loss after 10461610 batches: 0.0294
trigger times: 18
Loss after 10462573 batches: 0.0317
trigger times: 19
Loss after 10463536 batches: 0.0310
trigger times: 20
Loss after 10464499 batches: 0.0305
trigger times: 21
Loss after 10465462 batches: 0.0292
trigger times: 22
Loss after 10466425 batches: 0.0299
trigger times: 23
Loss after 10467388 batches: 0.0286
trigger times: 24
Loss after 10468351 batches: 0.0295
trigger times: 25
Early stopping!
Start to test process.
Loss after 10469314 batches: 0.0301
Time to train on one home:  53.000969886779785
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10470277 batches: 0.0532
trigger times: 1
Loss after 10471240 batches: 0.0478
trigger times: 2
Loss after 10472203 batches: 0.0453
trigger times: 3
Loss after 10473166 batches: 0.0422
trigger times: 0
Loss after 10474129 batches: 0.0409
trigger times: 1
Loss after 10475092 batches: 0.0396
trigger times: 2
Loss after 10476055 batches: 0.0379
trigger times: 3
Loss after 10477018 batches: 0.0373
trigger times: 4
Loss after 10477981 batches: 0.0365
trigger times: 5
Loss after 10478944 batches: 0.0362
trigger times: 6
Loss after 10479907 batches: 0.0353
trigger times: 7
Loss after 10480870 batches: 0.0343
trigger times: 8
Loss after 10481833 batches: 0.0338
trigger times: 9
Loss after 10482796 batches: 0.0345
trigger times: 10
Loss after 10483759 batches: 0.0336
trigger times: 11
Loss after 10484722 batches: 0.0316
trigger times: 12
Loss after 10485685 batches: 0.0321
trigger times: 13
Loss after 10486648 batches: 0.0324
trigger times: 14
Loss after 10487611 batches: 0.0326
trigger times: 15
Loss after 10488574 batches: 0.0330
trigger times: 0
Loss after 10489537 batches: 0.0324
trigger times: 1
Loss after 10490500 batches: 0.0313
trigger times: 2
Loss after 10491463 batches: 0.0327
trigger times: 3
Loss after 10492426 batches: 0.0334
trigger times: 4
Loss after 10493389 batches: 0.0336
trigger times: 5
Loss after 10494352 batches: 0.0316
trigger times: 6
Loss after 10495315 batches: 0.0319
trigger times: 7
Loss after 10496278 batches: 0.0318
trigger times: 8
Loss after 10497241 batches: 0.0312
trigger times: 9
Loss after 10498204 batches: 0.0294
trigger times: 10
Loss after 10499167 batches: 0.0296
trigger times: 11
Loss after 10500130 batches: 0.0298
trigger times: 12
Loss after 10501093 batches: 0.0297
trigger times: 0
Loss after 10502056 batches: 0.0297
trigger times: 1
Loss after 10503019 batches: 0.0298
trigger times: 0
Loss after 10503982 batches: 0.0289
trigger times: 1
Loss after 10504945 batches: 0.0309
trigger times: 2
Loss after 10505908 batches: 0.0296
trigger times: 3
Loss after 10506871 batches: 0.0288
trigger times: 4
Loss after 10507834 batches: 0.0279
trigger times: 5
Loss after 10508797 batches: 0.0282
trigger times: 6
Loss after 10509760 batches: 0.0277
trigger times: 7
Loss after 10510723 batches: 0.0269
trigger times: 8
Loss after 10511686 batches: 0.0290
trigger times: 9
Loss after 10512649 batches: 0.0273
trigger times: 10
Loss after 10513612 batches: 0.0274
trigger times: 11
Loss after 10514575 batches: 0.0277
trigger times: 12
Loss after 10515538 batches: 0.0268
trigger times: 13
Loss after 10516501 batches: 0.0275
trigger times: 14
Loss after 10517464 batches: 0.0276
trigger times: 15
Loss after 10518427 batches: 0.0275
trigger times: 16
Loss after 10519390 batches: 0.0272
trigger times: 0
Loss after 10520353 batches: 0.0258
trigger times: 1
Loss after 10521316 batches: 0.0249
trigger times: 2
Loss after 10522279 batches: 0.0248
trigger times: 3
Loss after 10523242 batches: 0.0254
trigger times: 4
Loss after 10524205 batches: 0.0253
trigger times: 5
Loss after 10525168 batches: 0.0262
trigger times: 6
Loss after 10526131 batches: 0.0261
trigger times: 7
Loss after 10527094 batches: 0.0248
trigger times: 8
Loss after 10528057 batches: 0.0251
trigger times: 9
Loss after 10529020 batches: 0.0253
trigger times: 10
Loss after 10529983 batches: 0.0255
trigger times: 11
Loss after 10530946 batches: 0.0259
trigger times: 12
Loss after 10531909 batches: 0.0259
trigger times: 13
Loss after 10532872 batches: 0.0268
trigger times: 14
Loss after 10533835 batches: 0.0260
trigger times: 0
Loss after 10534798 batches: 0.0243
trigger times: 1
Loss after 10535761 batches: 0.0246
trigger times: 2
Loss after 10536724 batches: 0.0244
trigger times: 3
Loss after 10537687 batches: 0.0243
trigger times: 4
Loss after 10538650 batches: 0.0242
trigger times: 5
Loss after 10539613 batches: 0.0234
trigger times: 6
Loss after 10540576 batches: 0.0249
trigger times: 7
Loss after 10541539 batches: 0.0267
trigger times: 8
Loss after 10542502 batches: 0.0246
trigger times: 9
Loss after 10543465 batches: 0.0248
trigger times: 10
Loss after 10544428 batches: 0.0249
trigger times: 11
Loss after 10545391 batches: 0.0299
trigger times: 12
Loss after 10546354 batches: 0.0290
trigger times: 13
Loss after 10547317 batches: 0.0282
trigger times: 14
Loss after 10548280 batches: 0.0261
trigger times: 15
Loss after 10549243 batches: 0.0274
trigger times: 16
Loss after 10550206 batches: 0.0246
trigger times: 17
Loss after 10551169 batches: 0.0248
trigger times: 18
Loss after 10552132 batches: 0.0255
trigger times: 19
Loss after 10553095 batches: 0.0232
trigger times: 20
Loss after 10554058 batches: 0.0235
trigger times: 21
Loss after 10555021 batches: 0.0236
trigger times: 22
Loss after 10555984 batches: 0.0234
trigger times: 23
Loss after 10556947 batches: 0.0240
trigger times: 24
Loss after 10557910 batches: 0.0236
trigger times: 25
Early stopping!
Start to test process.
Loss after 10558873 batches: 0.0235
Time to train on one home:  103.75761270523071
trigger times: 0
Loss after 10559836 batches: 0.0694
trigger times: 1
Loss after 10560799 batches: 0.0472
trigger times: 2
Loss after 10561762 batches: 0.0466
trigger times: 3
Loss after 10562725 batches: 0.0434
trigger times: 4
Loss after 10563688 batches: 0.0414
trigger times: 5
Loss after 10564651 batches: 0.0406
trigger times: 6
Loss after 10565614 batches: 0.0394
trigger times: 7
Loss after 10566577 batches: 0.0391
trigger times: 8
Loss after 10567540 batches: 0.0380
trigger times: 9
Loss after 10568503 batches: 0.0379
trigger times: 10
Loss after 10569466 batches: 0.0372
trigger times: 11
Loss after 10570429 batches: 0.0368
trigger times: 12
Loss after 10571392 batches: 0.0373
trigger times: 13
Loss after 10572355 batches: 0.0369
trigger times: 14
Loss after 10573318 batches: 0.0361
trigger times: 15
Loss after 10574281 batches: 0.0360
trigger times: 16
Loss after 10575244 batches: 0.0355
trigger times: 17
Loss after 10576207 batches: 0.0354
trigger times: 18
Loss after 10577170 batches: 0.0355
trigger times: 19
Loss after 10578133 batches: 0.0350
trigger times: 20
Loss after 10579096 batches: 0.0349
trigger times: 21
Loss after 10580059 batches: 0.0347
trigger times: 22
Loss after 10581022 batches: 0.0350
trigger times: 23
Loss after 10581985 batches: 0.0356
trigger times: 24
Loss after 10582948 batches: 0.0359
trigger times: 25
Early stopping!
Start to test process.
Loss after 10583911 batches: 0.0346
Time to train on one home:  52.5051064491272
trigger times: 0
Loss after 10584874 batches: 0.0989
trigger times: 1
Loss after 10585837 batches: 0.0895
trigger times: 2
Loss after 10586800 batches: 0.0836
trigger times: 3
Loss after 10587763 batches: 0.0830
trigger times: 4
Loss after 10588726 batches: 0.0767
trigger times: 5
Loss after 10589689 batches: 0.0758
trigger times: 6
Loss after 10590652 batches: 0.0742
trigger times: 7
Loss after 10591615 batches: 0.0725
trigger times: 8
Loss after 10592578 batches: 0.0717
trigger times: 9
Loss after 10593541 batches: 0.0721
trigger times: 10
Loss after 10594504 batches: 0.0724
trigger times: 11
Loss after 10595467 batches: 0.0695
trigger times: 12
Loss after 10596430 batches: 0.0696
trigger times: 13
Loss after 10597393 batches: 0.0687
trigger times: 14
Loss after 10598356 batches: 0.0669
trigger times: 15
Loss after 10599319 batches: 0.0657
trigger times: 16
Loss after 10600282 batches: 0.0669
trigger times: 17
Loss after 10601245 batches: 0.0652
trigger times: 18
Loss after 10602208 batches: 0.0644
trigger times: 19
Loss after 10603171 batches: 0.0661
trigger times: 20
Loss after 10604134 batches: 0.0635
trigger times: 21
Loss after 10605097 batches: 0.0632
trigger times: 22
Loss after 10606060 batches: 0.0643
trigger times: 23
Loss after 10607023 batches: 0.0615
trigger times: 24
Loss after 10607986 batches: 0.0633
trigger times: 25
Early stopping!
Start to test process.
Loss after 10608949 batches: 0.0624
Time to train on one home:  52.417136430740356
trigger times: 0
Loss after 10609912 batches: 0.1023
trigger times: 1
Loss after 10610875 batches: 0.0685
trigger times: 2
Loss after 10611838 batches: 0.0637
trigger times: 3
Loss after 10612801 batches: 0.0561
trigger times: 4
Loss after 10613764 batches: 0.0528
trigger times: 5
Loss after 10614727 batches: 0.0491
trigger times: 6
Loss after 10615690 batches: 0.0473
trigger times: 7
Loss after 10616653 batches: 0.0458
trigger times: 8
Loss after 10617616 batches: 0.0449
trigger times: 9
Loss after 10618579 batches: 0.0438
trigger times: 10
Loss after 10619542 batches: 0.0436
trigger times: 11
Loss after 10620505 batches: 0.0429
trigger times: 12
Loss after 10621468 batches: 0.0426
trigger times: 13
Loss after 10622431 batches: 0.0414
trigger times: 14
Loss after 10623394 batches: 0.0407
trigger times: 15
Loss after 10624357 batches: 0.0417
trigger times: 16
Loss after 10625320 batches: 0.0406
trigger times: 17
Loss after 10626283 batches: 0.0403
trigger times: 18
Loss after 10627246 batches: 0.0404
trigger times: 19
Loss after 10628209 batches: 0.0404
trigger times: 20
Loss after 10629172 batches: 0.0399
trigger times: 21
Loss after 10630135 batches: 0.0399
trigger times: 22
Loss after 10631098 batches: 0.0386
trigger times: 23
Loss after 10632061 batches: 0.0400
trigger times: 24
Loss after 10633024 batches: 0.0394
trigger times: 25
Early stopping!
Start to test process.
Loss after 10633987 batches: 0.0408
Time to train on one home:  52.84201407432556
trigger times: 0
Loss after 10634916 batches: 0.1150
trigger times: 1
Loss after 10635845 batches: 0.0718
trigger times: 2
Loss after 10636774 batches: 0.0550
trigger times: 3
Loss after 10637703 batches: 0.0461
trigger times: 4
Loss after 10638632 batches: 0.0407
trigger times: 5
Loss after 10639561 batches: 0.0390
trigger times: 6
Loss after 10640490 batches: 0.0374
trigger times: 7
Loss after 10641419 batches: 0.0355
trigger times: 8
Loss after 10642348 batches: 0.0340
trigger times: 9
Loss after 10643277 batches: 0.0311
trigger times: 10
Loss after 10644206 batches: 0.0309
trigger times: 11
Loss after 10645135 batches: 0.0300
trigger times: 12
Loss after 10646064 batches: 0.0321
trigger times: 13
Loss after 10646993 batches: 0.0302
trigger times: 14
Loss after 10647922 batches: 0.0295
trigger times: 15
Loss after 10648851 batches: 0.0316
trigger times: 16
Loss after 10649780 batches: 0.0327
trigger times: 17
Loss after 10650709 batches: 0.0317
trigger times: 18
Loss after 10651638 batches: 0.0316
trigger times: 19
Loss after 10652567 batches: 0.0308
trigger times: 20
Loss after 10653496 batches: 0.0299
trigger times: 21
Loss after 10654425 batches: 0.0282
trigger times: 22
Loss after 10655354 batches: 0.0286
trigger times: 23
Loss after 10656283 batches: 0.0313
trigger times: 24
Loss after 10657212 batches: 0.0312
trigger times: 25
Early stopping!
Start to test process.
Loss after 10658141 batches: 0.0284
Time to train on one home:  52.372138023376465
trigger times: 0
Loss after 10659104 batches: 0.0576
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 10660067 batches: 0.0296
trigger times: 2
Loss after 10661030 batches: 0.0268
trigger times: 3
Loss after 10661993 batches: 0.0265
trigger times: 4
Loss after 10662956 batches: 0.0252
trigger times: 5
Loss after 10663919 batches: 0.0243
trigger times: 6
Loss after 10664882 batches: 0.0235
trigger times: 7
Loss after 10665845 batches: 0.0229
trigger times: 8
Loss after 10666808 batches: 0.0221
trigger times: 9
Loss after 10667771 batches: 0.0215
trigger times: 10
Loss after 10668734 batches: 0.0207
trigger times: 11
Loss after 10669697 batches: 0.0207
trigger times: 12
Loss after 10670660 batches: 0.0201
trigger times: 13
Loss after 10671623 batches: 0.0197
trigger times: 14
Loss after 10672586 batches: 0.0196
trigger times: 15
Loss after 10673549 batches: 0.0193
trigger times: 16
Loss after 10674512 batches: 0.0190
trigger times: 17
Loss after 10675475 batches: 0.0188
trigger times: 18
Loss after 10676438 batches: 0.0186
trigger times: 19
Loss after 10677401 batches: 0.0183
trigger times: 20
Loss after 10678364 batches: 0.0186
trigger times: 21
Loss after 10679327 batches: 0.0181
trigger times: 22
Loss after 10680290 batches: 0.0184
trigger times: 23
Loss after 10681253 batches: 0.0181
trigger times: 24
Loss after 10682216 batches: 0.0179
trigger times: 25
Early stopping!
Start to test process.
Loss after 10683179 batches: 0.0177
Time to train on one home:  53.13611078262329
trigger times: 0
Loss after 10684142 batches: 0.1623
trigger times: 1
Loss after 10685105 batches: 0.1180
trigger times: 2
Loss after 10686068 batches: 0.0993
trigger times: 3
Loss after 10687031 batches: 0.0872
trigger times: 4
Loss after 10687994 batches: 0.0812
trigger times: 5
Loss after 10688957 batches: 0.0789
trigger times: 6
Loss after 10689920 batches: 0.0752
trigger times: 7
Loss after 10690883 batches: 0.0723
trigger times: 8
Loss after 10691846 batches: 0.0672
trigger times: 9
Loss after 10692809 batches: 0.0630
trigger times: 10
Loss after 10693772 batches: 0.0600
trigger times: 11
Loss after 10694735 batches: 0.0571
trigger times: 12
Loss after 10695698 batches: 0.0572
trigger times: 13
Loss after 10696661 batches: 0.0536
trigger times: 14
Loss after 10697624 batches: 0.0520
trigger times: 15
Loss after 10698587 batches: 0.0504
trigger times: 16
Loss after 10699550 batches: 0.0498
trigger times: 17
Loss after 10700513 batches: 0.0488
trigger times: 18
Loss after 10701476 batches: 0.0498
trigger times: 19
Loss after 10702439 batches: 0.0481
trigger times: 20
Loss after 10703402 batches: 0.0480
trigger times: 21
Loss after 10704365 batches: 0.0482
trigger times: 22
Loss after 10705328 batches: 0.0461
trigger times: 23
Loss after 10706291 batches: 0.0456
trigger times: 24
Loss after 10707254 batches: 0.0454
trigger times: 25
Early stopping!
Start to test process.
Loss after 10708217 batches: 0.0454
Time to train on one home:  52.13309073448181
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10709180 batches: 0.0799
trigger times: 1
Loss after 10710143 batches: 0.0732
trigger times: 2
Loss after 10711106 batches: 0.0686
trigger times: 3
Loss after 10712069 batches: 0.0670
trigger times: 4
Loss after 10713032 batches: 0.0666
trigger times: 5
Loss after 10713995 batches: 0.0645
trigger times: 6
Loss after 10714958 batches: 0.0633
trigger times: 7
Loss after 10715921 batches: 0.0620
trigger times: 8
Loss after 10716884 batches: 0.0615
trigger times: 9
Loss after 10717847 batches: 0.0609
trigger times: 10
Loss after 10718810 batches: 0.0598
trigger times: 11
Loss after 10719773 batches: 0.0609
trigger times: 12
Loss after 10720736 batches: 0.0585
trigger times: 13
Loss after 10721699 batches: 0.0597
trigger times: 14
Loss after 10722662 batches: 0.0592
trigger times: 15
Loss after 10723625 batches: 0.0600
trigger times: 16
Loss after 10724588 batches: 0.0593
trigger times: 17
Loss after 10725551 batches: 0.0592
trigger times: 18
Loss after 10726514 batches: 0.0579
trigger times: 19
Loss after 10727477 batches: 0.0585
trigger times: 20
Loss after 10728440 batches: 0.0584
trigger times: 21
Loss after 10729403 batches: 0.0567
trigger times: 22
Loss after 10730366 batches: 0.0568
trigger times: 23
Loss after 10731329 batches: 0.0569
trigger times: 24
Loss after 10732292 batches: 0.0560
trigger times: 25
Early stopping!
Start to test process.
Loss after 10733255 batches: 0.0556
Time to train on one home:  52.82172989845276
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10734218 batches: 0.0833
trigger times: 1
Loss after 10735181 batches: 0.0753
trigger times: 2
Loss after 10736144 batches: 0.0735
trigger times: 3
Loss after 10737107 batches: 0.0696
trigger times: 4
Loss after 10738070 batches: 0.0662
trigger times: 5
Loss after 10739033 batches: 0.0637
trigger times: 6
Loss after 10739996 batches: 0.0628
trigger times: 7
Loss after 10740959 batches: 0.0590
trigger times: 8
Loss after 10741922 batches: 0.0587
trigger times: 9
Loss after 10742885 batches: 0.0563
trigger times: 10
Loss after 10743848 batches: 0.0592
trigger times: 11
Loss after 10744811 batches: 0.0558
trigger times: 12
Loss after 10745774 batches: 0.0558
trigger times: 13
Loss after 10746737 batches: 0.0530
trigger times: 14
Loss after 10747700 batches: 0.0571
trigger times: 15
Loss after 10748663 batches: 0.0564
trigger times: 16
Loss after 10749626 batches: 0.0531
trigger times: 17
Loss after 10750589 batches: 0.0595
trigger times: 18
Loss after 10751552 batches: 0.0579
trigger times: 19
Loss after 10752515 batches: 0.0556
trigger times: 20
Loss after 10753478 batches: 0.0547
trigger times: 21
Loss after 10754441 batches: 0.0530
trigger times: 22
Loss after 10755404 batches: 0.0526
trigger times: 23
Loss after 10756367 batches: 0.0536
trigger times: 24
Loss after 10757330 batches: 0.0535
trigger times: 25
Early stopping!
Start to test process.
Loss after 10758293 batches: 0.0530
Time to train on one home:  52.67364311218262
trigger times: 0
Loss after 10759256 batches: 0.0235
trigger times: 0
Loss after 10760219 batches: 0.0208
trigger times: 1
Loss after 10761182 batches: 0.0191
trigger times: 2
Loss after 10762145 batches: 0.0179
trigger times: 3
Loss after 10763108 batches: 0.0173
trigger times: 0
Loss after 10764071 batches: 0.0158
trigger times: 1
Loss after 10765034 batches: 0.0157
trigger times: 2
Loss after 10765997 batches: 0.0157
trigger times: 3
Loss after 10766960 batches: 0.0154
trigger times: 4
Loss after 10767923 batches: 0.0147
trigger times: 5
Loss after 10768886 batches: 0.0146
trigger times: 6
Loss after 10769849 batches: 0.0145
trigger times: 7
Loss after 10770812 batches: 0.0142
trigger times: 8
Loss after 10771775 batches: 0.0141
trigger times: 9
Loss after 10772738 batches: 0.0138
trigger times: 10
Loss after 10773701 batches: 0.0135
trigger times: 11
Loss after 10774664 batches: 0.0139
trigger times: 12
Loss after 10775627 batches: 0.0136
trigger times: 13
Loss after 10776590 batches: 0.0133
trigger times: 14
Loss after 10777553 batches: 0.0138
trigger times: 15
Loss after 10778516 batches: 0.0138
trigger times: 16
Loss after 10779479 batches: 0.0133
trigger times: 17
Loss after 10780442 batches: 0.0130
trigger times: 18
Loss after 10781405 batches: 0.0130
trigger times: 19
Loss after 10782368 batches: 0.0126
trigger times: 20
Loss after 10783331 batches: 0.0128
trigger times: 21
Loss after 10784294 batches: 0.0126
trigger times: 22
Loss after 10785257 batches: 0.0130
trigger times: 23
Loss after 10786220 batches: 0.0127
trigger times: 24
Loss after 10787183 batches: 0.0119
trigger times: 25
Early stopping!
Start to test process.
Loss after 10788146 batches: 0.0119
Time to train on one home:  56.44441533088684
trigger times: 0
Loss after 10789109 batches: 0.0448
trigger times: 1
Loss after 10790072 batches: 0.0375
trigger times: 2
Loss after 10791035 batches: 0.0337
trigger times: 0
Loss after 10791998 batches: 0.0322
trigger times: 1
Loss after 10792961 batches: 0.0298
trigger times: 2
Loss after 10793924 batches: 0.0280
trigger times: 3
Loss after 10794887 batches: 0.0260
trigger times: 4
Loss after 10795850 batches: 0.0258
trigger times: 5
Loss after 10796813 batches: 0.0250
trigger times: 6
Loss after 10797776 batches: 0.0248
trigger times: 7
Loss after 10798739 batches: 0.0239
trigger times: 8
Loss after 10799702 batches: 0.0234
trigger times: 9
Loss after 10800665 batches: 0.0232
trigger times: 10
Loss after 10801628 batches: 0.0228
trigger times: 11
Loss after 10802591 batches: 0.0222
trigger times: 12
Loss after 10803554 batches: 0.0224
trigger times: 13
Loss after 10804517 batches: 0.0223
trigger times: 14
Loss after 10805480 batches: 0.0224
trigger times: 15
Loss after 10806443 batches: 0.0220
trigger times: 16
Loss after 10807406 batches: 0.0218
trigger times: 17
Loss after 10808369 batches: 0.0217
trigger times: 18
Loss after 10809332 batches: 0.0221
trigger times: 19
Loss after 10810295 batches: 0.0212
trigger times: 20
Loss after 10811258 batches: 0.0211
trigger times: 21
Loss after 10812221 batches: 0.0213
trigger times: 22
Loss after 10813184 batches: 0.0221
trigger times: 23
Loss after 10814147 batches: 0.0215
trigger times: 24
Loss after 10815110 batches: 0.0212
trigger times: 25
Early stopping!
Start to test process.
Loss after 10816073 batches: 0.0212
Time to train on one home:  54.96175503730774
trigger times: 0
Loss after 10817036 batches: 0.0932
trigger times: 1
Loss after 10817999 batches: 0.0670
trigger times: 2
Loss after 10818962 batches: 0.0599
trigger times: 3
Loss after 10819925 batches: 0.0553
trigger times: 4
Loss after 10820888 batches: 0.0517
trigger times: 5
Loss after 10821851 batches: 0.0491
trigger times: 6
Loss after 10822814 batches: 0.0469
trigger times: 7
Loss after 10823777 batches: 0.0448
trigger times: 8
Loss after 10824740 batches: 0.0443
trigger times: 9
Loss after 10825703 batches: 0.0440
trigger times: 10
Loss after 10826666 batches: 0.0433
trigger times: 11
Loss after 10827629 batches: 0.0430
trigger times: 12
Loss after 10828592 batches: 0.0427
trigger times: 13
Loss after 10829555 batches: 0.0415
trigger times: 14
Loss after 10830518 batches: 0.0406
trigger times: 15
Loss after 10831481 batches: 0.0408
trigger times: 16
Loss after 10832444 batches: 0.0405
trigger times: 17
Loss after 10833407 batches: 0.0397
trigger times: 18
Loss after 10834370 batches: 0.0390
trigger times: 19
Loss after 10835333 batches: 0.0401
trigger times: 20
Loss after 10836296 batches: 0.0389
trigger times: 21
Loss after 10837259 batches: 0.0387
trigger times: 22
Loss after 10838222 batches: 0.0387
trigger times: 23
Loss after 10839185 batches: 0.0390
trigger times: 24
Loss after 10840148 batches: 0.0377
trigger times: 25
Early stopping!
Start to test process.
Loss after 10841111 batches: 0.0380
Time to train on one home:  52.809319496154785
trigger times: 0
Loss after 10842074 batches: 0.1038
trigger times: 1
Loss after 10843037 batches: 0.0674
trigger times: 2
Loss after 10844000 batches: 0.0626
trigger times: 3
Loss after 10844963 batches: 0.0553
trigger times: 4
Loss after 10845926 batches: 0.0539
trigger times: 5
Loss after 10846889 batches: 0.0497
trigger times: 6
Loss after 10847852 batches: 0.0480
trigger times: 7
Loss after 10848815 batches: 0.0462
trigger times: 8
Loss after 10849778 batches: 0.0454
trigger times: 9
Loss after 10850741 batches: 0.0439
trigger times: 10
Loss after 10851704 batches: 0.0436
trigger times: 11
Loss after 10852667 batches: 0.0419
trigger times: 12
Loss after 10853630 batches: 0.0417
trigger times: 13
Loss after 10854593 batches: 0.0419
trigger times: 14
Loss after 10855556 batches: 0.0408
trigger times: 15
Loss after 10856519 batches: 0.0401
trigger times: 16
Loss after 10857482 batches: 0.0402
trigger times: 17
Loss after 10858445 batches: 0.0413
trigger times: 18
Loss after 10859408 batches: 0.0398
trigger times: 19
Loss after 10860371 batches: 0.0385
trigger times: 20
Loss after 10861334 batches: 0.0395
trigger times: 21
Loss after 10862297 batches: 0.0393
trigger times: 22
Loss after 10863260 batches: 0.0391
trigger times: 23
Loss after 10864223 batches: 0.0389
trigger times: 24
Loss after 10865186 batches: 0.0382
trigger times: 25
Early stopping!
Start to test process.
Loss after 10866149 batches: 0.0386
Time to train on one home:  52.9152295589447
trigger times: 0
Loss after 10867112 batches: 0.0585
trigger times: 1
Loss after 10868075 batches: 0.0446
trigger times: 2
Loss after 10869038 batches: 0.0406
trigger times: 3
Loss after 10870001 batches: 0.0382
trigger times: 4
Loss after 10870964 batches: 0.0363
trigger times: 5
Loss after 10871927 batches: 0.0361
trigger times: 6
Loss after 10872890 batches: 0.0350
trigger times: 7
Loss after 10873853 batches: 0.0328
trigger times: 8
Loss after 10874816 batches: 0.0332
trigger times: 9
Loss after 10875779 batches: 0.0323
trigger times: 10
Loss after 10876742 batches: 0.0333
trigger times: 11
Loss after 10877705 batches: 0.0348
trigger times: 12
Loss after 10878668 batches: 0.0336
trigger times: 13
Loss after 10879631 batches: 0.0333
trigger times: 14
Loss after 10880594 batches: 0.0316
trigger times: 15
Loss after 10881557 batches: 0.0326
trigger times: 16
Loss after 10882520 batches: 0.0308
trigger times: 17
Loss after 10883483 batches: 0.0308
trigger times: 18
Loss after 10884446 batches: 0.0307
trigger times: 19
Loss after 10885409 batches: 0.0299
trigger times: 20
Loss after 10886372 batches: 0.0317
trigger times: 21
Loss after 10887335 batches: 0.0306
trigger times: 22
Loss after 10888298 batches: 0.0294
trigger times: 23
Loss after 10889261 batches: 0.0295
trigger times: 24
Loss after 10890224 batches: 0.0281
trigger times: 25
Early stopping!
Start to test process.
Loss after 10891187 batches: 0.0294
Time to train on one home:  53.007185220718384
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10892150 batches: 0.0922
trigger times: 1
Loss after 10893113 batches: 0.0577
trigger times: 2
Loss after 10894076 batches: 0.0557
trigger times: 3
Loss after 10895039 batches: 0.0476
trigger times: 0
Loss after 10896002 batches: 0.0423
trigger times: 0
Loss after 10896965 batches: 0.0398
trigger times: 0
Loss after 10897928 batches: 0.0372
trigger times: 1
Loss after 10898891 batches: 0.0361
trigger times: 2
Loss after 10899854 batches: 0.0344
trigger times: 3
Loss after 10900817 batches: 0.0344
trigger times: 4
Loss after 10901780 batches: 0.0322
trigger times: 5
Loss after 10902743 batches: 0.0320
trigger times: 6
Loss after 10903706 batches: 0.0316
trigger times: 7
Loss after 10904669 batches: 0.0312
trigger times: 8
Loss after 10905632 batches: 0.0306
trigger times: 9
Loss after 10906595 batches: 0.0293
trigger times: 0
Loss after 10907558 batches: 0.0296
trigger times: 1
Loss after 10908521 batches: 0.0309
trigger times: 2
Loss after 10909484 batches: 0.0302
trigger times: 3
Loss after 10910447 batches: 0.0298
trigger times: 4
Loss after 10911410 batches: 0.0289
trigger times: 5
Loss after 10912373 batches: 0.0276
trigger times: 0
Loss after 10913336 batches: 0.0287
trigger times: 1
Loss after 10914299 batches: 0.0280
trigger times: 2
Loss after 10915262 batches: 0.0286
trigger times: 3
Loss after 10916225 batches: 0.0270
trigger times: 4
Loss after 10917188 batches: 0.0275
trigger times: 5
Loss after 10918151 batches: 0.0278
trigger times: 6
Loss after 10919114 batches: 0.0271
trigger times: 7
Loss after 10920077 batches: 0.0259
trigger times: 8
Loss after 10921040 batches: 0.0258
trigger times: 9
Loss after 10922003 batches: 0.0264
trigger times: 10
Loss after 10922966 batches: 0.0249
trigger times: 11
Loss after 10923929 batches: 0.0258
trigger times: 12
Loss after 10924892 batches: 0.0258
trigger times: 13
Loss after 10925855 batches: 0.0253
trigger times: 14
Loss after 10926818 batches: 0.0247
trigger times: 15
Loss after 10927781 batches: 0.0250
trigger times: 16
Loss after 10928744 batches: 0.0252
trigger times: 17
Loss after 10929707 batches: 0.0258
trigger times: 18
Loss after 10930670 batches: 0.0254
trigger times: 19
Loss after 10931633 batches: 0.0263
trigger times: 20
Loss after 10932596 batches: 0.0253
trigger times: 21
Loss after 10933559 batches: 0.0259
trigger times: 22
Loss after 10934522 batches: 0.0255
trigger times: 23
Loss after 10935485 batches: 0.0239
trigger times: 24
Loss after 10936448 batches: 0.0241
trigger times: 25
Early stopping!
Start to test process.
Loss after 10937411 batches: 0.0237
Time to train on one home:  69.36020851135254
trigger times: 0
Loss after 10938374 batches: 0.0937
trigger times: 1
Loss after 10939337 batches: 0.0785
trigger times: 2
Loss after 10940300 batches: 0.0764
trigger times: 3
Loss after 10941263 batches: 0.0752
trigger times: 4
Loss after 10942226 batches: 0.0719
trigger times: 5
Loss after 10943189 batches: 0.0695
trigger times: 6
Loss after 10944152 batches: 0.0673
trigger times: 7
Loss after 10945115 batches: 0.0661
trigger times: 8
Loss after 10946078 batches: 0.0655
trigger times: 9
Loss after 10947041 batches: 0.0643
trigger times: 10
Loss after 10948004 batches: 0.0634
trigger times: 11
Loss after 10948967 batches: 0.0640
trigger times: 12
Loss after 10949930 batches: 0.0631
trigger times: 13
Loss after 10950893 batches: 0.0626
trigger times: 14
Loss after 10951856 batches: 0.0623
trigger times: 15
Loss after 10952819 batches: 0.0654
trigger times: 16
Loss after 10953782 batches: 0.0640
trigger times: 17
Loss after 10954745 batches: 0.0629
trigger times: 18
Loss after 10955708 batches: 0.0636
trigger times: 19
Loss after 10956671 batches: 0.0602
trigger times: 20
Loss after 10957634 batches: 0.0601
trigger times: 21
Loss after 10958597 batches: 0.0590
trigger times: 22
Loss after 10959560 batches: 0.0598
trigger times: 23
Loss after 10960523 batches: 0.0591
trigger times: 24
Loss after 10961486 batches: 0.0584
trigger times: 25
Early stopping!
Start to test process.
Loss after 10962449 batches: 0.0600
Time to train on one home:  53.12304186820984
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 10963412 batches: 0.0916
trigger times: 1
Loss after 10964375 batches: 0.0582
trigger times: 2
Loss after 10965338 batches: 0.0569
trigger times: 0
Loss after 10966301 batches: 0.0461
trigger times: 0
Loss after 10967264 batches: 0.0424
trigger times: 0
Loss after 10968227 batches: 0.0394
trigger times: 0
Loss after 10969190 batches: 0.0365
trigger times: 1
Loss after 10970153 batches: 0.0362
trigger times: 0
Loss after 10971116 batches: 0.0345
trigger times: 1
Loss after 10972079 batches: 0.0329
trigger times: 0
Loss after 10973042 batches: 0.0333
trigger times: 1
Loss after 10974005 batches: 0.0337
trigger times: 2
Loss after 10974968 batches: 0.0329
trigger times: 3
Loss after 10975931 batches: 0.0325
trigger times: 0
Loss after 10976894 batches: 0.0306
trigger times: 0
Loss after 10977857 batches: 0.0306
trigger times: 1
Loss after 10978820 batches: 0.0296
trigger times: 2
Loss after 10979783 batches: 0.0291
trigger times: 3
Loss after 10980746 batches: 0.0290
trigger times: 4
Loss after 10981709 batches: 0.0289
trigger times: 5
Loss after 10982672 batches: 0.0279
trigger times: 6
Loss after 10983635 batches: 0.0278
trigger times: 7
Loss after 10984598 batches: 0.0273
trigger times: 8
Loss after 10985561 batches: 0.0264
trigger times: 9
Loss after 10986524 batches: 0.0272
trigger times: 10
Loss after 10987487 batches: 0.0295
trigger times: 0
Loss after 10988450 batches: 0.0289
trigger times: 1
Loss after 10989413 batches: 0.0266
trigger times: 2
Loss after 10990376 batches: 0.0271
trigger times: 0
Loss after 10991339 batches: 0.0275
trigger times: 1
Loss after 10992302 batches: 0.0263
trigger times: 2
Loss after 10993265 batches: 0.0266
trigger times: 3
Loss after 10994228 batches: 0.0258
trigger times: 4
Loss after 10995191 batches: 0.0247
trigger times: 5
Loss after 10996154 batches: 0.0256
trigger times: 6
Loss after 10997117 batches: 0.0254
trigger times: 7
Loss after 10998080 batches: 0.0242
trigger times: 8
Loss after 10999043 batches: 0.0253
trigger times: 9
Loss after 11000006 batches: 0.0280
trigger times: 10
Loss after 11000969 batches: 0.0261
trigger times: 11
Loss after 11001932 batches: 0.0253
trigger times: 12
Loss after 11002895 batches: 0.0252
trigger times: 13
Loss after 11003858 batches: 0.0250
trigger times: 14
Loss after 11004821 batches: 0.0230
trigger times: 15
Loss after 11005784 batches: 0.0277
trigger times: 16
Loss after 11006747 batches: 0.0254
trigger times: 17
Loss after 11007710 batches: 0.0249
trigger times: 18
Loss after 11008673 batches: 0.0247
trigger times: 19
Loss after 11009636 batches: 0.0228
trigger times: 20
Loss after 11010599 batches: 0.0230
trigger times: 21
Loss after 11011562 batches: 0.0215
trigger times: 22
Loss after 11012525 batches: 0.0218
trigger times: 23
Loss after 11013488 batches: 0.0222
trigger times: 24
Loss after 11014451 batches: 0.0231
trigger times: 25
Early stopping!
Start to test process.
Loss after 11015414 batches: 0.0217
Time to train on one home:  74.55655884742737
trigger times: 0
Loss after 11016309 batches: 0.0747
trigger times: 1
Loss after 11017204 batches: 0.0382
trigger times: 2
Loss after 11018099 batches: 0.0183
trigger times: 3
Loss after 11018994 batches: 0.0108
trigger times: 4
Loss after 11019889 batches: 0.0083
trigger times: 5
Loss after 11020784 batches: 0.0068
trigger times: 6
Loss after 11021679 batches: 0.0055
trigger times: 7
Loss after 11022574 batches: 0.0049
trigger times: 8
Loss after 11023469 batches: 0.0067
trigger times: 9
Loss after 11024364 batches: 0.0104
trigger times: 10
Loss after 11025259 batches: 0.0053
trigger times: 11
Loss after 11026154 batches: 0.0047
trigger times: 12
Loss after 11027049 batches: 0.0045
trigger times: 13
Loss after 11027944 batches: 0.0047
trigger times: 14
Loss after 11028839 batches: 0.0049
trigger times: 15
Loss after 11029734 batches: 0.0045
trigger times: 16
Loss after 11030629 batches: 0.0037
trigger times: 17
Loss after 11031524 batches: 0.0035
trigger times: 18
Loss after 11032419 batches: 0.0117
trigger times: 19
Loss after 11033314 batches: 0.0132
trigger times: 20
Loss after 11034209 batches: 0.0164
trigger times: 21
Loss after 11035104 batches: 0.0187
trigger times: 22
Loss after 11035999 batches: 0.0101
trigger times: 23
Loss after 11036894 batches: 0.0083
trigger times: 24
Loss after 11037789 batches: 0.0068
trigger times: 25
Early stopping!
Start to test process.
Loss after 11038684 batches: 0.0072
Time to train on one home:  51.25355553627014
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11039621 batches: 0.0823
trigger times: 1
Loss after 11040558 batches: 0.0707
trigger times: 2
Loss after 11041495 batches: 0.0676
trigger times: 3
Loss after 11042432 batches: 0.0633
trigger times: 4
Loss after 11043369 batches: 0.0608
trigger times: 5
Loss after 11044306 batches: 0.0596
trigger times: 6
Loss after 11045243 batches: 0.0576
trigger times: 7
Loss after 11046180 batches: 0.0569
trigger times: 8
Loss after 11047117 batches: 0.0546
trigger times: 9
Loss after 11048054 batches: 0.0555
trigger times: 10
Loss after 11048991 batches: 0.0542
trigger times: 11
Loss after 11049928 batches: 0.0524
trigger times: 12
Loss after 11050865 batches: 0.0523
trigger times: 13
Loss after 11051802 batches: 0.0530
trigger times: 14
Loss after 11052739 batches: 0.0520
trigger times: 15
Loss after 11053676 batches: 0.0512
trigger times: 16
Loss after 11054613 batches: 0.0528
trigger times: 17
Loss after 11055550 batches: 0.0520
trigger times: 18
Loss after 11056487 batches: 0.0522
trigger times: 19
Loss after 11057424 batches: 0.0499
trigger times: 20
Loss after 11058361 batches: 0.0494
trigger times: 21
Loss after 11059298 batches: 0.0503
trigger times: 22
Loss after 11060235 batches: 0.0499
trigger times: 23
Loss after 11061172 batches: 0.0498
trigger times: 24
Loss after 11062109 batches: 0.0500
trigger times: 25
Early stopping!
Start to test process.
Loss after 11063046 batches: 0.0488
Time to train on one home:  52.66812443733215
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066, 0.03401813118363776]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863], [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861]]
Round_8_results:  [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861]
trigger times: 0
Loss after 11064008 batches: 0.0776
trigger times: 1
Loss after 11064970 batches: 0.0657
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 11539 < 11540; dropping {'Training_Loss': 0.07764334444488798, 'Validation_Loss': 0.07900631427764893, 'Training_R2': -0.1469178471321233, 'Validation_R2': 0.11312614740733085, 'Training_F1': 0.38859944843526883, 'Validation_F1': 0.6033074146945652, 'Training_NEP': 0.8425460299837942, 'Validation_NEP': 0.7855291596013677, 'Training_NDE': 0.6985007815134361, 'Validation_NDE': 0.8012694688347176, 'Training_MAE': 32.01702022004334, 'Validation_MAE': 27.932846408663384, 'Training_MSE': 2579.8333, 'Validation_MSE': 10496.64}.
trigger times: 2
Loss after 11065932 batches: 0.0640
trigger times: 3
Loss after 11066894 batches: 0.0616
trigger times: 4
Loss after 11067856 batches: 0.0585
trigger times: 5
Loss after 11068818 batches: 0.0570
trigger times: 6
Loss after 11069780 batches: 0.0558
trigger times: 7
Loss after 11070742 batches: 0.0543
trigger times: 8
Loss after 11071704 batches: 0.0541
trigger times: 9
Loss after 11072666 batches: 0.0532
trigger times: 10
Loss after 11073628 batches: 0.0527
trigger times: 11
Loss after 11074590 batches: 0.0540
trigger times: 12
Loss after 11075552 batches: 0.0517
trigger times: 13
Loss after 11076514 batches: 0.0510
trigger times: 14
Loss after 11077476 batches: 0.0517
trigger times: 15
Loss after 11078438 batches: 0.0513
trigger times: 16
Loss after 11079400 batches: 0.0509
trigger times: 17
Loss after 11080362 batches: 0.0513
trigger times: 18
Loss after 11081324 batches: 0.0505
trigger times: 19
Loss after 11082286 batches: 0.0499
trigger times: 20
Loss after 11083248 batches: 0.0504
trigger times: 21
Loss after 11084210 batches: 0.0502
trigger times: 22
Loss after 11085172 batches: 0.0495
trigger times: 23
Loss after 11086134 batches: 0.0505
trigger times: 24
Loss after 11087096 batches: 0.0500
trigger times: 25
Early stopping!
Start to test process.
Loss after 11088058 batches: 0.0502
Time to train on one home:  52.691126108169556
trigger times: 0
Loss after 11088987 batches: 0.0878
trigger times: 1
Loss after 11089916 batches: 0.0631
trigger times: 2
Loss after 11090845 batches: 0.0510
trigger times: 3
Loss after 11091774 batches: 0.0426
trigger times: 4
Loss after 11092703 batches: 0.0404
trigger times: 5
Loss after 11093632 batches: 0.0394
trigger times: 6
Loss after 11094561 batches: 0.0348
trigger times: 7
Loss after 11095490 batches: 0.0349
trigger times: 8
Loss after 11096419 batches: 0.0357
trigger times: 9
Loss after 11097348 batches: 0.0328
trigger times: 10
Loss after 11098277 batches: 0.0316
trigger times: 11
Loss after 11099206 batches: 0.0297
trigger times: 12
Loss after 11100135 batches: 0.0287
trigger times: 13
Loss after 11101064 batches: 0.0284
trigger times: 14
Loss after 11101993 batches: 0.0321
trigger times: 15
Loss after 11102922 batches: 0.0297
trigger times: 16
Loss after 11103851 batches: 0.0294
trigger times: 17
Loss after 11104780 batches: 0.0281
trigger times: 18
Loss after 11105709 batches: 0.0289
trigger times: 19
Loss after 11106638 batches: 0.0279
trigger times: 20
Loss after 11107567 batches: 0.0264
trigger times: 21
Loss after 11108496 batches: 0.0272
trigger times: 22
Loss after 11109425 batches: 0.0283
trigger times: 23
Loss after 11110354 batches: 0.0280
trigger times: 24
Loss after 11111283 batches: 0.0258
trigger times: 25
Early stopping!
Start to test process.
Loss after 11112212 batches: 0.0282
Time to train on one home:  52.821045875549316
trigger times: 0
Loss after 11113175 batches: 0.0559
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 1
Loss after 11114138 batches: 0.0191
trigger times: 2
Loss after 11115101 batches: 0.0140
trigger times: 3
Loss after 11116064 batches: 0.0141
trigger times: 4
Loss after 11117027 batches: 0.0142
trigger times: 5
Loss after 11117990 batches: 0.0139
trigger times: 6
Loss after 11118953 batches: 0.0140
trigger times: 7
Loss after 11119916 batches: 0.0139
trigger times: 8
Loss after 11120879 batches: 0.0138
trigger times: 9
Loss after 11121842 batches: 0.0140
trigger times: 10
Loss after 11122805 batches: 0.0138
trigger times: 11
Loss after 11123768 batches: 0.0137
trigger times: 12
Loss after 11124731 batches: 0.0137
trigger times: 13
Loss after 11125694 batches: 0.0137
trigger times: 14
Loss after 11126657 batches: 0.0135
trigger times: 15
Loss after 11127620 batches: 0.0134
trigger times: 16
Loss after 11128583 batches: 0.0132
trigger times: 17
Loss after 11129546 batches: 0.0131
trigger times: 18
Loss after 11130509 batches: 0.0128
trigger times: 19
Loss after 11131472 batches: 0.0125
trigger times: 20
Loss after 11132435 batches: 0.0122
trigger times: 21
Loss after 11133398 batches: 0.0121
trigger times: 22
Loss after 11134361 batches: 0.0114
trigger times: 23
Loss after 11135324 batches: 0.0111
trigger times: 24
Loss after 11136287 batches: 0.0108
trigger times: 25
Early stopping!
Start to test process.
Loss after 11137250 batches: 0.0103
Time to train on one home:  52.98399209976196
trigger times: 0
Loss after 11138213 batches: 0.0247
trigger times: 0
Loss after 11139176 batches: 0.0200
trigger times: 1
Loss after 11140139 batches: 0.0189
trigger times: 2
Loss after 11141102 batches: 0.0177
trigger times: 0
Loss after 11142065 batches: 0.0165
trigger times: 1
Loss after 11143028 batches: 0.0158
trigger times: 2
Loss after 11143991 batches: 0.0152
trigger times: 3
Loss after 11144954 batches: 0.0148
trigger times: 4
Loss after 11145917 batches: 0.0143
trigger times: 5
Loss after 11146880 batches: 0.0141
trigger times: 6
Loss after 11147843 batches: 0.0137
trigger times: 7
Loss after 11148806 batches: 0.0137
trigger times: 8
Loss after 11149769 batches: 0.0140
trigger times: 9
Loss after 11150732 batches: 0.0133
trigger times: 10
Loss after 11151695 batches: 0.0131
trigger times: 11
Loss after 11152658 batches: 0.0132
trigger times: 12
Loss after 11153621 batches: 0.0127
trigger times: 13
Loss after 11154584 batches: 0.0132
trigger times: 14
Loss after 11155547 batches: 0.0133
trigger times: 15
Loss after 11156510 batches: 0.0136
trigger times: 16
Loss after 11157473 batches: 0.0126
trigger times: 17
Loss after 11158436 batches: 0.0127
trigger times: 18
Loss after 11159399 batches: 0.0127
trigger times: 19
Loss after 11160362 batches: 0.0128
trigger times: 20
Loss after 11161325 batches: 0.0127
trigger times: 21
Loss after 11162288 batches: 0.0126
trigger times: 22
Loss after 11163251 batches: 0.0127
trigger times: 23
Loss after 11164214 batches: 0.0125
trigger times: 24
Loss after 11165177 batches: 0.0122
trigger times: 25
Early stopping!
Start to test process.
Loss after 11166140 batches: 0.0129
Time to train on one home:  55.47323226928711
trigger times: 0
Loss after 11167103 batches: 0.0963
trigger times: 1
Loss after 11168066 batches: 0.0887
trigger times: 2
Loss after 11169029 batches: 0.0840
trigger times: 3
Loss after 11169992 batches: 0.0807
trigger times: 4
Loss after 11170955 batches: 0.0772
trigger times: 5
Loss after 11171918 batches: 0.0739
trigger times: 6
Loss after 11172881 batches: 0.0729
trigger times: 7
Loss after 11173844 batches: 0.0706
trigger times: 8
Loss after 11174807 batches: 0.0691
trigger times: 9
Loss after 11175770 batches: 0.0700
trigger times: 10
Loss after 11176733 batches: 0.0685
trigger times: 11
Loss after 11177696 batches: 0.0672
trigger times: 12
Loss after 11178659 batches: 0.0679
trigger times: 13
Loss after 11179622 batches: 0.0657
trigger times: 14
Loss after 11180585 batches: 0.0685
trigger times: 15
Loss after 11181548 batches: 0.0664
trigger times: 16
Loss after 11182511 batches: 0.0661
trigger times: 17
Loss after 11183474 batches: 0.0647
trigger times: 18
Loss after 11184437 batches: 0.0648
trigger times: 19
Loss after 11185400 batches: 0.0628
trigger times: 20
Loss after 11186363 batches: 0.0636
trigger times: 21
Loss after 11187326 batches: 0.0627
trigger times: 22
Loss after 11188289 batches: 0.0609
trigger times: 23
Loss after 11189252 batches: 0.0627
trigger times: 24
Loss after 11190215 batches: 0.0628
trigger times: 25
Early stopping!
Start to test process.
Loss after 11191178 batches: 0.0613
Time to train on one home:  52.16023063659668
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11192141 batches: 0.0853
trigger times: 1
Loss after 11193104 batches: 0.0741
trigger times: 2
Loss after 11194067 batches: 0.0715
trigger times: 3
Loss after 11195030 batches: 0.0674
trigger times: 4
Loss after 11195993 batches: 0.0662
trigger times: 5
Loss after 11196956 batches: 0.0651
trigger times: 6
Loss after 11197919 batches: 0.0643
trigger times: 7
Loss after 11198882 batches: 0.0631
trigger times: 8
Loss after 11199845 batches: 0.0607
trigger times: 9
Loss after 11200808 batches: 0.0610
trigger times: 10
Loss after 11201771 batches: 0.0602
trigger times: 11
Loss after 11202734 batches: 0.0602
trigger times: 12
Loss after 11203697 batches: 0.0595
trigger times: 13
Loss after 11204660 batches: 0.0588
trigger times: 14
Loss after 11205623 batches: 0.0572
trigger times: 15
Loss after 11206586 batches: 0.0580
trigger times: 16
Loss after 11207549 batches: 0.0579
trigger times: 17
Loss after 11208512 batches: 0.0565
trigger times: 18
Loss after 11209475 batches: 0.0558
trigger times: 19
Loss after 11210438 batches: 0.0564
trigger times: 20
Loss after 11211401 batches: 0.0570
trigger times: 21
Loss after 11212364 batches: 0.0562
trigger times: 22
Loss after 11213327 batches: 0.0565
trigger times: 23
Loss after 11214290 batches: 0.0561
trigger times: 24
Loss after 11215253 batches: 0.0550
trigger times: 25
Early stopping!
Start to test process.
Loss after 11216216 batches: 0.0553
Time to train on one home:  52.182589054107666
trigger times: 0
Loss after 11217179 batches: 0.0755
trigger times: 1
Loss after 11218142 batches: 0.0703
trigger times: 2
Loss after 11219105 batches: 0.0663
trigger times: 3
Loss after 11220068 batches: 0.0649
trigger times: 4
Loss after 11221031 batches: 0.0624
trigger times: 5
Loss after 11221994 batches: 0.0606
trigger times: 6
Loss after 11222957 batches: 0.0594
trigger times: 7
Loss after 11223920 batches: 0.0596
trigger times: 8
Loss after 11224883 batches: 0.0574
trigger times: 9
Loss after 11225846 batches: 0.0567
trigger times: 10
Loss after 11226809 batches: 0.0552
trigger times: 11
Loss after 11227772 batches: 0.0558
trigger times: 12
Loss after 11228735 batches: 0.0554
trigger times: 13
Loss after 11229698 batches: 0.0553
trigger times: 14
Loss after 11230661 batches: 0.0551
trigger times: 15
Loss after 11231624 batches: 0.0550
trigger times: 16
Loss after 11232587 batches: 0.0534
trigger times: 17
Loss after 11233550 batches: 0.0541
trigger times: 18
Loss after 11234513 batches: 0.0525
trigger times: 19
Loss after 11235476 batches: 0.0533
trigger times: 20
Loss after 11236439 batches: 0.0528
trigger times: 21
Loss after 11237402 batches: 0.0520
trigger times: 22
Loss after 11238365 batches: 0.0524
trigger times: 23
Loss after 11239328 batches: 0.0523
trigger times: 24
Loss after 11240291 batches: 0.0518
trigger times: 25
Early stopping!
Start to test process.
Loss after 11241254 batches: 0.0521
Time to train on one home:  52.51860451698303
trigger times: 0
Loss after 11242217 batches: 0.0658
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 11243180 batches: 0.0514
trigger times: 2
Loss after 11244143 batches: 0.0498
trigger times: 3
Loss after 11245106 batches: 0.0452
trigger times: 4
Loss after 11246069 batches: 0.0382
trigger times: 5
Loss after 11247032 batches: 0.0357
trigger times: 6
Loss after 11247995 batches: 0.0336
trigger times: 7
Loss after 11248958 batches: 0.0324
trigger times: 8
Loss after 11249921 batches: 0.0308
trigger times: 9
Loss after 11250884 batches: 0.0297
trigger times: 10
Loss after 11251847 batches: 0.0289
trigger times: 11
Loss after 11252810 batches: 0.0285
trigger times: 12
Loss after 11253773 batches: 0.0277
trigger times: 13
Loss after 11254736 batches: 0.0272
trigger times: 14
Loss after 11255699 batches: 0.0273
trigger times: 15
Loss after 11256662 batches: 0.0270
trigger times: 16
Loss after 11257625 batches: 0.0271
trigger times: 17
Loss after 11258588 batches: 0.0269
trigger times: 18
Loss after 11259551 batches: 0.0259
trigger times: 19
Loss after 11260514 batches: 0.0256
trigger times: 20
Loss after 11261477 batches: 0.0253
trigger times: 21
Loss after 11262440 batches: 0.0256
trigger times: 22
Loss after 11263403 batches: 0.0246
trigger times: 23
Loss after 11264366 batches: 0.0266
trigger times: 24
Loss after 11265329 batches: 0.0252
trigger times: 25
Early stopping!
Start to test process.
Loss after 11266292 batches: 0.0259
Time to train on one home:  52.3785502910614
trigger times: 0
Loss after 11267250 batches: 0.0655
trigger times: 1
Loss after 11268208 batches: 0.0436
trigger times: 2
Loss after 11269166 batches: 0.0374
trigger times: 3
Loss after 11270124 batches: 0.0324
trigger times: 4
Loss after 11271082 batches: 0.0299
trigger times: 5
Loss after 11272040 batches: 0.0279
trigger times: 6
Loss after 11272998 batches: 0.0253
trigger times: 7
Loss after 11273956 batches: 0.0256
trigger times: 8
Loss after 11274914 batches: 0.0230
trigger times: 9
Loss after 11275872 batches: 0.0222
trigger times: 10
Loss after 11276830 batches: 0.0220
trigger times: 11
Loss after 11277788 batches: 0.0218
trigger times: 12
Loss after 11278746 batches: 0.0199
trigger times: 13
Loss after 11279704 batches: 0.0201
trigger times: 14
Loss after 11280662 batches: 0.0192
trigger times: 15
Loss after 11281620 batches: 0.0191
trigger times: 16
Loss after 11282578 batches: 0.0186
trigger times: 17
Loss after 11283536 batches: 0.0185
trigger times: 18
Loss after 11284494 batches: 0.0179
trigger times: 19
Loss after 11285452 batches: 0.0171
trigger times: 20
Loss after 11286410 batches: 0.0169
trigger times: 21
Loss after 11287368 batches: 0.0169
trigger times: 22
Loss after 11288326 batches: 0.0180
trigger times: 23
Loss after 11289284 batches: 0.0173
trigger times: 24
Loss after 11290242 batches: 0.0179
trigger times: 25
Early stopping!
Start to test process.
Loss after 11291200 batches: 0.0167
Time to train on one home:  52.35151433944702
trigger times: 0
Loss after 11292162 batches: 0.0772
trigger times: 1
Loss after 11293124 batches: 0.0649
trigger times: 2
Loss after 11294086 batches: 0.0643
trigger times: 3
Loss after 11295048 batches: 0.0611
trigger times: 4
Loss after 11296010 batches: 0.0585
trigger times: 5
Loss after 11296972 batches: 0.0572
trigger times: 6
Loss after 11297934 batches: 0.0554
trigger times: 7
Loss after 11298896 batches: 0.0548
trigger times: 8
Loss after 11299858 batches: 0.0546
trigger times: 9
Loss after 11300820 batches: 0.0534
trigger times: 10
Loss after 11301782 batches: 0.0529
trigger times: 11
Loss after 11302744 batches: 0.0526
trigger times: 12
Loss after 11303706 batches: 0.0519
trigger times: 13
Loss after 11304668 batches: 0.0515
trigger times: 14
Loss after 11305630 batches: 0.0504
trigger times: 15
Loss after 11306592 batches: 0.0517
trigger times: 16
Loss after 11307554 batches: 0.0507
trigger times: 17
Loss after 11308516 batches: 0.0512
trigger times: 18
Loss after 11309478 batches: 0.0507
trigger times: 19
Loss after 11310440 batches: 0.0518
trigger times: 20
Loss after 11311402 batches: 0.0512
trigger times: 21
Loss after 11312364 batches: 0.0501
trigger times: 22
Loss after 11313326 batches: 0.0506
trigger times: 23
Loss after 11314288 batches: 0.0495
trigger times: 24
Loss after 11315250 batches: 0.0498
trigger times: 25
Early stopping!
Start to test process.
Loss after 11316212 batches: 0.0494
Time to train on one home:  52.23945689201355
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11317175 batches: 0.0948
trigger times: 1
Loss after 11318138 batches: 0.0289
trigger times: 2
Loss after 11319101 batches: 0.0211
trigger times: 3
Loss after 11320064 batches: 0.0187
trigger times: 4
Loss after 11321027 batches: 0.0177
trigger times: 5
Loss after 11321990 batches: 0.0165
trigger times: 6
Loss after 11322953 batches: 0.0156
trigger times: 7
Loss after 11323916 batches: 0.0146
trigger times: 8
Loss after 11324879 batches: 0.0145
trigger times: 9
Loss after 11325842 batches: 0.0138
trigger times: 10
Loss after 11326805 batches: 0.0138
trigger times: 11
Loss after 11327768 batches: 0.0136
trigger times: 12
Loss after 11328731 batches: 0.0136
trigger times: 13
Loss after 11329694 batches: 0.0133
trigger times: 14
Loss after 11330657 batches: 0.0130
trigger times: 15
Loss after 11331620 batches: 0.0131
trigger times: 16
Loss after 11332583 batches: 0.0129
trigger times: 17
Loss after 11333546 batches: 0.0129
trigger times: 18
Loss after 11334509 batches: 0.0134
trigger times: 19
Loss after 11335472 batches: 0.0127
trigger times: 20
Loss after 11336435 batches: 0.0126
trigger times: 21
Loss after 11337398 batches: 0.0125
trigger times: 22
Loss after 11338361 batches: 0.0124
trigger times: 23
Loss after 11339324 batches: 0.0126
trigger times: 24
Loss after 11340287 batches: 0.0123
trigger times: 25
Early stopping!
Start to test process.
Loss after 11341250 batches: 0.0125
Time to train on one home:  52.80026817321777
trigger times: 0
Loss after 11342213 batches: 0.0519
trigger times: 1
Loss after 11343176 batches: 0.0433
trigger times: 2
Loss after 11344139 batches: 0.0395
trigger times: 3
Loss after 11345102 batches: 0.0372
trigger times: 4
Loss after 11346065 batches: 0.0341
trigger times: 5
Loss after 11347028 batches: 0.0334
trigger times: 6
Loss after 11347991 batches: 0.0325
trigger times: 7
Loss after 11348954 batches: 0.0309
trigger times: 8
Loss after 11349917 batches: 0.0319
trigger times: 9
Loss after 11350880 batches: 0.0317
trigger times: 10
Loss after 11351843 batches: 0.0328
trigger times: 11
Loss after 11352806 batches: 0.0303
trigger times: 12
Loss after 11353769 batches: 0.0314
trigger times: 13
Loss after 11354732 batches: 0.0305
trigger times: 14
Loss after 11355695 batches: 0.0306
trigger times: 15
Loss after 11356658 batches: 0.0314
trigger times: 16
Loss after 11357621 batches: 0.0316
trigger times: 17
Loss after 11358584 batches: 0.0312
trigger times: 18
Loss after 11359547 batches: 0.0298
trigger times: 19
Loss after 11360510 batches: 0.0295
trigger times: 20
Loss after 11361473 batches: 0.0285
trigger times: 21
Loss after 11362436 batches: 0.0284
trigger times: 22
Loss after 11363399 batches: 0.0285
trigger times: 23
Loss after 11364362 batches: 0.0289
trigger times: 24
Loss after 11365325 batches: 0.0295
trigger times: 25
Early stopping!
Start to test process.
Loss after 11366288 batches: 0.0287
Time to train on one home:  52.16438174247742
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11367251 batches: 0.0659
trigger times: 1
Loss after 11368214 batches: 0.0512
trigger times: 2
Loss after 11369177 batches: 0.0501
trigger times: 3
Loss after 11370140 batches: 0.0451
trigger times: 4
Loss after 11371103 batches: 0.0397
trigger times: 5
Loss after 11372066 batches: 0.0362
trigger times: 6
Loss after 11373029 batches: 0.0339
trigger times: 7
Loss after 11373992 batches: 0.0326
trigger times: 8
Loss after 11374955 batches: 0.0306
trigger times: 9
Loss after 11375918 batches: 0.0298
trigger times: 10
Loss after 11376881 batches: 0.0294
trigger times: 11
Loss after 11377844 batches: 0.0287
trigger times: 12
Loss after 11378807 batches: 0.0280
trigger times: 13
Loss after 11379770 batches: 0.0284
trigger times: 14
Loss after 11380733 batches: 0.0274
trigger times: 15
Loss after 11381696 batches: 0.0271
trigger times: 16
Loss after 11382659 batches: 0.0264
trigger times: 17
Loss after 11383622 batches: 0.0272
trigger times: 18
Loss after 11384585 batches: 0.0273
trigger times: 19
Loss after 11385548 batches: 0.0262
trigger times: 20
Loss after 11386511 batches: 0.0265
trigger times: 21
Loss after 11387474 batches: 0.0259
trigger times: 22
Loss after 11388437 batches: 0.0251
trigger times: 23
Loss after 11389400 batches: 0.0251
trigger times: 24
Loss after 11390363 batches: 0.0251
trigger times: 25
Early stopping!
Start to test process.
Loss after 11391326 batches: 0.0248
Time to train on one home:  52.66423416137695
trigger times: 0
Loss after 11392289 batches: 0.0515
trigger times: 1
Loss after 11393252 batches: 0.0434
trigger times: 2
Loss after 11394215 batches: 0.0389
trigger times: 3
Loss after 11395178 batches: 0.0360
trigger times: 4
Loss after 11396141 batches: 0.0333
trigger times: 5
Loss after 11397104 batches: 0.0352
trigger times: 6
Loss after 11398067 batches: 0.0338
trigger times: 7
Loss after 11399030 batches: 0.0328
trigger times: 8
Loss after 11399993 batches: 0.0342
trigger times: 9
Loss after 11400956 batches: 0.0334
trigger times: 10
Loss after 11401919 batches: 0.0310
trigger times: 11
Loss after 11402882 batches: 0.0322
trigger times: 12
Loss after 11403845 batches: 0.0321
trigger times: 13
Loss after 11404808 batches: 0.0314
trigger times: 14
Loss after 11405771 batches: 0.0311
trigger times: 15
Loss after 11406734 batches: 0.0303
trigger times: 16
Loss after 11407697 batches: 0.0282
trigger times: 17
Loss after 11408660 batches: 0.0292
trigger times: 18
Loss after 11409623 batches: 0.0303
trigger times: 19
Loss after 11410586 batches: 0.0293
trigger times: 20
Loss after 11411549 batches: 0.0286
trigger times: 21
Loss after 11412512 batches: 0.0298
trigger times: 22
Loss after 11413475 batches: 0.0290
trigger times: 23
Loss after 11414438 batches: 0.0295
trigger times: 24
Loss after 11415401 batches: 0.0286
trigger times: 25
Early stopping!
Start to test process.
Loss after 11416364 batches: 0.0291
Time to train on one home:  52.203315019607544
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11417327 batches: 0.0481
trigger times: 1
Loss after 11418290 batches: 0.0452
trigger times: 2
Loss after 11419253 batches: 0.0431
trigger times: 3
Loss after 11420216 batches: 0.0408
trigger times: 0
Loss after 11421179 batches: 0.0385
trigger times: 1
Loss after 11422142 batches: 0.0381
trigger times: 2
Loss after 11423105 batches: 0.0372
trigger times: 0
Loss after 11424068 batches: 0.0348
trigger times: 1
Loss after 11425031 batches: 0.0347
trigger times: 2
Loss after 11425994 batches: 0.0340
trigger times: 3
Loss after 11426957 batches: 0.0334
trigger times: 4
Loss after 11427920 batches: 0.0330
trigger times: 5
Loss after 11428883 batches: 0.0323
trigger times: 6
Loss after 11429846 batches: 0.0326
trigger times: 7
Loss after 11430809 batches: 0.0330
trigger times: 8
Loss after 11431772 batches: 0.0322
trigger times: 9
Loss after 11432735 batches: 0.0321
trigger times: 0
Loss after 11433698 batches: 0.0313
trigger times: 1
Loss after 11434661 batches: 0.0319
trigger times: 2
Loss after 11435624 batches: 0.0322
trigger times: 3
Loss after 11436587 batches: 0.0310
trigger times: 4
Loss after 11437550 batches: 0.0301
trigger times: 5
Loss after 11438513 batches: 0.0312
trigger times: 6
Loss after 11439476 batches: 0.0307
trigger times: 7
Loss after 11440439 batches: 0.0306
trigger times: 8
Loss after 11441402 batches: 0.0300
trigger times: 9
Loss after 11442365 batches: 0.0300
trigger times: 10
Loss after 11443328 batches: 0.0294
trigger times: 11
Loss after 11444291 batches: 0.0300
trigger times: 12
Loss after 11445254 batches: 0.0295
trigger times: 13
Loss after 11446217 batches: 0.0292
trigger times: 14
Loss after 11447180 batches: 0.0301
trigger times: 15
Loss after 11448143 batches: 0.0283
trigger times: 16
Loss after 11449106 batches: 0.0284
trigger times: 17
Loss after 11450069 batches: 0.0281
trigger times: 18
Loss after 11451032 batches: 0.0283
trigger times: 19
Loss after 11451995 batches: 0.0295
trigger times: 20
Loss after 11452958 batches: 0.0287
trigger times: 21
Loss after 11453921 batches: 0.0283
trigger times: 22
Loss after 11454884 batches: 0.0276
trigger times: 23
Loss after 11455847 batches: 0.0274
trigger times: 24
Loss after 11456810 batches: 0.0276
trigger times: 25
Early stopping!
Start to test process.
Loss after 11457773 batches: 0.0262
Time to train on one home:  65.1473937034607
trigger times: 0
Loss after 11458736 batches: 0.0889
trigger times: 1
Loss after 11459699 batches: 0.0497
trigger times: 2
Loss after 11460662 batches: 0.0488
trigger times: 3
Loss after 11461625 batches: 0.0468
trigger times: 4
Loss after 11462588 batches: 0.0448
trigger times: 5
Loss after 11463551 batches: 0.0421
trigger times: 6
Loss after 11464514 batches: 0.0406
trigger times: 7
Loss after 11465477 batches: 0.0392
trigger times: 8
Loss after 11466440 batches: 0.0388
trigger times: 9
Loss after 11467403 batches: 0.0376
trigger times: 10
Loss after 11468366 batches: 0.0380
trigger times: 11
Loss after 11469329 batches: 0.0373
trigger times: 12
Loss after 11470292 batches: 0.0370
trigger times: 13
Loss after 11471255 batches: 0.0364
trigger times: 14
Loss after 11472218 batches: 0.0358
trigger times: 15
Loss after 11473181 batches: 0.0360
trigger times: 16
Loss after 11474144 batches: 0.0362
trigger times: 17
Loss after 11475107 batches: 0.0356
trigger times: 18
Loss after 11476070 batches: 0.0358
trigger times: 19
Loss after 11477033 batches: 0.0357
trigger times: 20
Loss after 11477996 batches: 0.0349
trigger times: 21
Loss after 11478959 batches: 0.0344
trigger times: 22
Loss after 11479922 batches: 0.0346
trigger times: 23
Loss after 11480885 batches: 0.0342
trigger times: 24
Loss after 11481848 batches: 0.0348
trigger times: 25
Early stopping!
Start to test process.
Loss after 11482811 batches: 0.0334
Time to train on one home:  52.25625419616699
trigger times: 0
Loss after 11483774 batches: 0.0953
trigger times: 1
Loss after 11484737 batches: 0.0872
trigger times: 2
Loss after 11485700 batches: 0.0834
trigger times: 3
Loss after 11486663 batches: 0.0799
trigger times: 4
Loss after 11487626 batches: 0.0773
trigger times: 5
Loss after 11488589 batches: 0.0744
trigger times: 6
Loss after 11489552 batches: 0.0740
trigger times: 7
Loss after 11490515 batches: 0.0719
trigger times: 8
Loss after 11491478 batches: 0.0701
trigger times: 9
Loss after 11492441 batches: 0.0697
trigger times: 10
Loss after 11493404 batches: 0.0683
trigger times: 11
Loss after 11494367 batches: 0.0662
trigger times: 12
Loss after 11495330 batches: 0.0656
trigger times: 13
Loss after 11496293 batches: 0.0666
trigger times: 14
Loss after 11497256 batches: 0.0654
trigger times: 15
Loss after 11498219 batches: 0.0640
trigger times: 16
Loss after 11499182 batches: 0.0634
trigger times: 17
Loss after 11500145 batches: 0.0633
trigger times: 18
Loss after 11501108 batches: 0.0627
trigger times: 19
Loss after 11502071 batches: 0.0634
trigger times: 20
Loss after 11503034 batches: 0.0622
trigger times: 21
Loss after 11503997 batches: 0.0622
trigger times: 22
Loss after 11504960 batches: 0.0627
trigger times: 23
Loss after 11505923 batches: 0.0633
trigger times: 24
Loss after 11506886 batches: 0.0616
trigger times: 25
Early stopping!
Start to test process.
Loss after 11507849 batches: 0.0611
Time to train on one home:  51.849363803863525
trigger times: 0
Loss after 11508812 batches: 0.0945
trigger times: 1
Loss after 11509775 batches: 0.0634
trigger times: 2
Loss after 11510738 batches: 0.0588
trigger times: 3
Loss after 11511701 batches: 0.0526
trigger times: 4
Loss after 11512664 batches: 0.0500
trigger times: 5
Loss after 11513627 batches: 0.0476
trigger times: 6
Loss after 11514590 batches: 0.0455
trigger times: 7
Loss after 11515553 batches: 0.0442
trigger times: 8
Loss after 11516516 batches: 0.0435
trigger times: 9
Loss after 11517479 batches: 0.0430
trigger times: 10
Loss after 11518442 batches: 0.0432
trigger times: 11
Loss after 11519405 batches: 0.0423
trigger times: 12
Loss after 11520368 batches: 0.0413
trigger times: 13
Loss after 11521331 batches: 0.0414
trigger times: 14
Loss after 11522294 batches: 0.0414
trigger times: 15
Loss after 11523257 batches: 0.0417
trigger times: 16
Loss after 11524220 batches: 0.0394
trigger times: 17
Loss after 11525183 batches: 0.0402
trigger times: 18
Loss after 11526146 batches: 0.0394
trigger times: 19
Loss after 11527109 batches: 0.0392
trigger times: 20
Loss after 11528072 batches: 0.0395
trigger times: 21
Loss after 11529035 batches: 0.0386
trigger times: 22
Loss after 11529998 batches: 0.0390
trigger times: 23
Loss after 11530961 batches: 0.0374
trigger times: 24
Loss after 11531924 batches: 0.0379
trigger times: 25
Early stopping!
Start to test process.
Loss after 11532887 batches: 0.0376
Time to train on one home:  52.4361789226532
trigger times: 0
Loss after 11533816 batches: 0.0900
trigger times: 1
Loss after 11534745 batches: 0.0641
trigger times: 2
Loss after 11535674 batches: 0.0506
trigger times: 3
Loss after 11536603 batches: 0.0429
trigger times: 4
Loss after 11537532 batches: 0.0400
trigger times: 5
Loss after 11538461 batches: 0.0386
trigger times: 6
Loss after 11539390 batches: 0.0336
trigger times: 7
Loss after 11540319 batches: 0.0331
trigger times: 8
Loss after 11541248 batches: 0.0331
trigger times: 9
Loss after 11542177 batches: 0.0335
trigger times: 10
Loss after 11543106 batches: 0.0314
trigger times: 11
Loss after 11544035 batches: 0.0293
trigger times: 12
Loss after 11544964 batches: 0.0389
trigger times: 13
Loss after 11545893 batches: 0.0482
trigger times: 14
Loss after 11546822 batches: 0.0443
trigger times: 15
Loss after 11547751 batches: 0.0403
trigger times: 16
Loss after 11548680 batches: 0.0382
trigger times: 17
Loss after 11549609 batches: 0.0344
trigger times: 18
Loss after 11550538 batches: 0.0342
trigger times: 19
Loss after 11551467 batches: 0.0382
trigger times: 20
Loss after 11552396 batches: 0.0349
trigger times: 21
Loss after 11553325 batches: 0.0333
trigger times: 22
Loss after 11554254 batches: 0.0327
trigger times: 23
Loss after 11555183 batches: 0.0329
trigger times: 24
Loss after 11556112 batches: 0.0310
trigger times: 25
Early stopping!
Start to test process.
Loss after 11557041 batches: 0.0302
Time to train on one home:  51.743367195129395
trigger times: 0
Loss after 11558004 batches: 0.0724
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 11558967 batches: 0.0326
trigger times: 2
Loss after 11559930 batches: 0.0267
trigger times: 3
Loss after 11560893 batches: 0.0265
trigger times: 4
Loss after 11561856 batches: 0.0253
trigger times: 5
Loss after 11562819 batches: 0.0245
trigger times: 6
Loss after 11563782 batches: 0.0240
trigger times: 7
Loss after 11564745 batches: 0.0234
trigger times: 8
Loss after 11565708 batches: 0.0224
trigger times: 9
Loss after 11566671 batches: 0.0218
trigger times: 10
Loss after 11567634 batches: 0.0211
trigger times: 11
Loss after 11568597 batches: 0.0207
trigger times: 12
Loss after 11569560 batches: 0.0205
trigger times: 13
Loss after 11570523 batches: 0.0200
trigger times: 14
Loss after 11571486 batches: 0.0200
trigger times: 15
Loss after 11572449 batches: 0.0197
trigger times: 16
Loss after 11573412 batches: 0.0190
trigger times: 17
Loss after 11574375 batches: 0.0192
trigger times: 18
Loss after 11575338 batches: 0.0190
trigger times: 19
Loss after 11576301 batches: 0.0185
trigger times: 20
Loss after 11577264 batches: 0.0185
trigger times: 21
Loss after 11578227 batches: 0.0184
trigger times: 22
Loss after 11579190 batches: 0.0181
trigger times: 23
Loss after 11580153 batches: 0.0184
trigger times: 24
Loss after 11581116 batches: 0.0179
trigger times: 25
Early stopping!
Start to test process.
Loss after 11582079 batches: 0.0178
Time to train on one home:  52.21222805976868
trigger times: 0
Loss after 11583042 batches: 0.1791
trigger times: 1
Loss after 11584005 batches: 0.1210
trigger times: 2
Loss after 11584968 batches: 0.0987
trigger times: 3
Loss after 11585931 batches: 0.0908
trigger times: 4
Loss after 11586894 batches: 0.0828
trigger times: 5
Loss after 11587857 batches: 0.0782
trigger times: 6
Loss after 11588820 batches: 0.0748
trigger times: 7
Loss after 11589783 batches: 0.0688
trigger times: 8
Loss after 11590746 batches: 0.0650
trigger times: 9
Loss after 11591709 batches: 0.0607
trigger times: 10
Loss after 11592672 batches: 0.0591
trigger times: 11
Loss after 11593635 batches: 0.0567
trigger times: 12
Loss after 11594598 batches: 0.0531
trigger times: 13
Loss after 11595561 batches: 0.0533
trigger times: 14
Loss after 11596524 batches: 0.0507
trigger times: 15
Loss after 11597487 batches: 0.0511
trigger times: 16
Loss after 11598450 batches: 0.0493
trigger times: 17
Loss after 11599413 batches: 0.0479
trigger times: 18
Loss after 11600376 batches: 0.0494
trigger times: 19
Loss after 11601339 batches: 0.0476
trigger times: 20
Loss after 11602302 batches: 0.0467
trigger times: 21
Loss after 11603265 batches: 0.0459
trigger times: 22
Loss after 11604228 batches: 0.0454
trigger times: 23
Loss after 11605191 batches: 0.0450
trigger times: 24
Loss after 11606154 batches: 0.0467
trigger times: 25
Early stopping!
Start to test process.
Loss after 11607117 batches: 0.0461
Time to train on one home:  52.20422291755676
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11608080 batches: 0.0840
trigger times: 1
Loss after 11609043 batches: 0.0724
trigger times: 2
Loss after 11610006 batches: 0.0703
trigger times: 3
Loss after 11610969 batches: 0.0679
trigger times: 4
Loss after 11611932 batches: 0.0676
trigger times: 5
Loss after 11612895 batches: 0.0644
trigger times: 6
Loss after 11613858 batches: 0.0634
trigger times: 7
Loss after 11614821 batches: 0.0617
trigger times: 8
Loss after 11615784 batches: 0.0604
trigger times: 9
Loss after 11616747 batches: 0.0599
trigger times: 10
Loss after 11617710 batches: 0.0601
trigger times: 11
Loss after 11618673 batches: 0.0593
trigger times: 12
Loss after 11619636 batches: 0.0589
trigger times: 13
Loss after 11620599 batches: 0.0585
trigger times: 14
Loss after 11621562 batches: 0.0573
trigger times: 15
Loss after 11622525 batches: 0.0579
trigger times: 16
Loss after 11623488 batches: 0.0579
trigger times: 17
Loss after 11624451 batches: 0.0582
trigger times: 18
Loss after 11625414 batches: 0.0569
trigger times: 19
Loss after 11626377 batches: 0.0571
trigger times: 20
Loss after 11627340 batches: 0.0572
trigger times: 21
Loss after 11628303 batches: 0.0556
trigger times: 22
Loss after 11629266 batches: 0.0551
trigger times: 23
Loss after 11630229 batches: 0.0558
trigger times: 24
Loss after 11631192 batches: 0.0543
trigger times: 25
Early stopping!
Start to test process.
Loss after 11632155 batches: 0.0538
Time to train on one home:  52.10223984718323
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11633118 batches: 0.0870
trigger times: 1
Loss after 11634081 batches: 0.0752
trigger times: 2
Loss after 11635044 batches: 0.0734
trigger times: 3
Loss after 11636007 batches: 0.0699
trigger times: 4
Loss after 11636970 batches: 0.0674
trigger times: 5
Loss after 11637933 batches: 0.0642
trigger times: 6
Loss after 11638896 batches: 0.0617
trigger times: 7
Loss after 11639859 batches: 0.0597
trigger times: 8
Loss after 11640822 batches: 0.0582
trigger times: 9
Loss after 11641785 batches: 0.0572
trigger times: 10
Loss after 11642748 batches: 0.0571
trigger times: 11
Loss after 11643711 batches: 0.0611
trigger times: 12
Loss after 11644674 batches: 0.0607
trigger times: 13
Loss after 11645637 batches: 0.0582
trigger times: 14
Loss after 11646600 batches: 0.0563
trigger times: 15
Loss after 11647563 batches: 0.0539
trigger times: 16
Loss after 11648526 batches: 0.0547
trigger times: 17
Loss after 11649489 batches: 0.0522
trigger times: 18
Loss after 11650452 batches: 0.0540
trigger times: 19
Loss after 11651415 batches: 0.0521
trigger times: 20
Loss after 11652378 batches: 0.0531
trigger times: 21
Loss after 11653341 batches: 0.0520
trigger times: 22
Loss after 11654304 batches: 0.0515
trigger times: 23
Loss after 11655267 batches: 0.0512
trigger times: 24
Loss after 11656230 batches: 0.0503
trigger times: 25
Early stopping!
Start to test process.
Loss after 11657193 batches: 0.0494
Time to train on one home:  52.16125726699829
trigger times: 0
Loss after 11658156 batches: 0.0245
trigger times: 1
Loss after 11659119 batches: 0.0203
trigger times: 2
Loss after 11660082 batches: 0.0197
trigger times: 3
Loss after 11661045 batches: 0.0181
trigger times: 0
Loss after 11662008 batches: 0.0163
trigger times: 1
Loss after 11662971 batches: 0.0155
trigger times: 2
Loss after 11663934 batches: 0.0151
trigger times: 3
Loss after 11664897 batches: 0.0151
trigger times: 4
Loss after 11665860 batches: 0.0148
trigger times: 5
Loss after 11666823 batches: 0.0142
trigger times: 6
Loss after 11667786 batches: 0.0142
trigger times: 7
Loss after 11668749 batches: 0.0140
trigger times: 8
Loss after 11669712 batches: 0.0134
trigger times: 9
Loss after 11670675 batches: 0.0137
trigger times: 10
Loss after 11671638 batches: 0.0136
trigger times: 11
Loss after 11672601 batches: 0.0136
trigger times: 12
Loss after 11673564 batches: 0.0134
trigger times: 13
Loss after 11674527 batches: 0.0124
trigger times: 14
Loss after 11675490 batches: 0.0127
trigger times: 15
Loss after 11676453 batches: 0.0125
trigger times: 16
Loss after 11677416 batches: 0.0129
trigger times: 17
Loss after 11678379 batches: 0.0123
trigger times: 18
Loss after 11679342 batches: 0.0124
trigger times: 19
Loss after 11680305 batches: 0.0131
trigger times: 20
Loss after 11681268 batches: 0.0129
trigger times: 21
Loss after 11682231 batches: 0.0129
trigger times: 22
Loss after 11683194 batches: 0.0126
trigger times: 23
Loss after 11684157 batches: 0.0125
trigger times: 24
Loss after 11685120 batches: 0.0127
trigger times: 25
Early stopping!
Start to test process.
Loss after 11686083 batches: 0.0121
Time to train on one home:  55.21329736709595
trigger times: 0
Loss after 11687046 batches: 0.0433
trigger times: 1
Loss after 11688009 batches: 0.0354
trigger times: 2
Loss after 11688972 batches: 0.0320
trigger times: 3
Loss after 11689935 batches: 0.0306
trigger times: 4
Loss after 11690898 batches: 0.0286
trigger times: 5
Loss after 11691861 batches: 0.0272
trigger times: 6
Loss after 11692824 batches: 0.0259
trigger times: 7
Loss after 11693787 batches: 0.0254
trigger times: 8
Loss after 11694750 batches: 0.0247
trigger times: 9
Loss after 11695713 batches: 0.0247
trigger times: 10
Loss after 11696676 batches: 0.0242
trigger times: 11
Loss after 11697639 batches: 0.0239
trigger times: 12
Loss after 11698602 batches: 0.0233
trigger times: 13
Loss after 11699565 batches: 0.0223
trigger times: 14
Loss after 11700528 batches: 0.0221
trigger times: 15
Loss after 11701491 batches: 0.0219
trigger times: 16
Loss after 11702454 batches: 0.0216
trigger times: 17
Loss after 11703417 batches: 0.0216
trigger times: 18
Loss after 11704380 batches: 0.0213
trigger times: 19
Loss after 11705343 batches: 0.0205
trigger times: 20
Loss after 11706306 batches: 0.0213
trigger times: 21
Loss after 11707269 batches: 0.0213
trigger times: 22
Loss after 11708232 batches: 0.0212
trigger times: 23
Loss after 11709195 batches: 0.0215
trigger times: 24
Loss after 11710158 batches: 0.0216
trigger times: 25
Early stopping!
Start to test process.
Loss after 11711121 batches: 0.0211
Time to train on one home:  52.06138730049133
trigger times: 0
Loss after 11712084 batches: 0.0909
trigger times: 1
Loss after 11713047 batches: 0.0644
trigger times: 2
Loss after 11714010 batches: 0.0591
trigger times: 3
Loss after 11714973 batches: 0.0525
trigger times: 4
Loss after 11715936 batches: 0.0507
trigger times: 5
Loss after 11716899 batches: 0.0476
trigger times: 6
Loss after 11717862 batches: 0.0463
trigger times: 7
Loss after 11718825 batches: 0.0453
trigger times: 8
Loss after 11719788 batches: 0.0439
trigger times: 9
Loss after 11720751 batches: 0.0434
trigger times: 10
Loss after 11721714 batches: 0.0429
trigger times: 11
Loss after 11722677 batches: 0.0429
trigger times: 12
Loss after 11723640 batches: 0.0416
trigger times: 13
Loss after 11724603 batches: 0.0408
trigger times: 14
Loss after 11725566 batches: 0.0410
trigger times: 15
Loss after 11726529 batches: 0.0403
trigger times: 16
Loss after 11727492 batches: 0.0399
trigger times: 17
Loss after 11728455 batches: 0.0394
trigger times: 18
Loss after 11729418 batches: 0.0387
trigger times: 19
Loss after 11730381 batches: 0.0394
trigger times: 20
Loss after 11731344 batches: 0.0393
trigger times: 21
Loss after 11732307 batches: 0.0380
trigger times: 22
Loss after 11733270 batches: 0.0379
trigger times: 23
Loss after 11734233 batches: 0.0366
trigger times: 24
Loss after 11735196 batches: 0.0360
trigger times: 25
Early stopping!
Start to test process.
Loss after 11736159 batches: 0.0363
Time to train on one home:  52.20409917831421
trigger times: 0
Loss after 11737122 batches: 0.0941
trigger times: 1
Loss after 11738085 batches: 0.0633
trigger times: 2
Loss after 11739048 batches: 0.0588
trigger times: 3
Loss after 11740011 batches: 0.0525
trigger times: 4
Loss after 11740974 batches: 0.0503
trigger times: 5
Loss after 11741937 batches: 0.0475
trigger times: 6
Loss after 11742900 batches: 0.0458
trigger times: 7
Loss after 11743863 batches: 0.0443
trigger times: 8
Loss after 11744826 batches: 0.0432
trigger times: 9
Loss after 11745789 batches: 0.0413
trigger times: 10
Loss after 11746752 batches: 0.0425
trigger times: 11
Loss after 11747715 batches: 0.0413
trigger times: 12
Loss after 11748678 batches: 0.0410
trigger times: 13
Loss after 11749641 batches: 0.0405
trigger times: 14
Loss after 11750604 batches: 0.0406
trigger times: 15
Loss after 11751567 batches: 0.0399
trigger times: 16
Loss after 11752530 batches: 0.0388
trigger times: 17
Loss after 11753493 batches: 0.0375
trigger times: 18
Loss after 11754456 batches: 0.0386
trigger times: 19
Loss after 11755419 batches: 0.0387
trigger times: 20
Loss after 11756382 batches: 0.0393
trigger times: 21
Loss after 11757345 batches: 0.0399
trigger times: 22
Loss after 11758308 batches: 0.0391
trigger times: 23
Loss after 11759271 batches: 0.0384
trigger times: 24
Loss after 11760234 batches: 0.0383
trigger times: 25
Early stopping!
Start to test process.
Loss after 11761197 batches: 0.0373
Time to train on one home:  52.10248804092407
trigger times: 0
Loss after 11762160 batches: 0.0519
trigger times: 1
Loss after 11763123 batches: 0.0431
trigger times: 2
Loss after 11764086 batches: 0.0388
trigger times: 3
Loss after 11765049 batches: 0.0367
trigger times: 4
Loss after 11766012 batches: 0.0340
trigger times: 5
Loss after 11766975 batches: 0.0330
trigger times: 6
Loss after 11767938 batches: 0.0329
trigger times: 7
Loss after 11768901 batches: 0.0330
trigger times: 8
Loss after 11769864 batches: 0.0321
trigger times: 9
Loss after 11770827 batches: 0.0319
trigger times: 10
Loss after 11771790 batches: 0.0338
trigger times: 11
Loss after 11772753 batches: 0.0337
trigger times: 12
Loss after 11773716 batches: 0.0333
trigger times: 13
Loss after 11774679 batches: 0.0321
trigger times: 14
Loss after 11775642 batches: 0.0318
trigger times: 15
Loss after 11776605 batches: 0.0306
trigger times: 16
Loss after 11777568 batches: 0.0299
trigger times: 17
Loss after 11778531 batches: 0.0309
trigger times: 18
Loss after 11779494 batches: 0.0321
trigger times: 19
Loss after 11780457 batches: 0.0317
trigger times: 20
Loss after 11781420 batches: 0.0307
trigger times: 21
Loss after 11782383 batches: 0.0308
trigger times: 22
Loss after 11783346 batches: 0.0298
trigger times: 23
Loss after 11784309 batches: 0.0305
trigger times: 24
Loss after 11785272 batches: 0.0299
trigger times: 25
Early stopping!
Start to test process.
Loss after 11786235 batches: 0.0300
Time to train on one home:  52.170788049697876
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11787198 batches: 0.0839
trigger times: 1
Loss after 11788161 batches: 0.0525
trigger times: 2
Loss after 11789124 batches: 0.0546
trigger times: 0
Loss after 11790087 batches: 0.0463
trigger times: 0
Loss after 11791050 batches: 0.0421
trigger times: 0
Loss after 11792013 batches: 0.0379
trigger times: 1
Loss after 11792976 batches: 0.0371
trigger times: 2
Loss after 11793939 batches: 0.0348
trigger times: 3
Loss after 11794902 batches: 0.0331
trigger times: 0
Loss after 11795865 batches: 0.0325
trigger times: 0
Loss after 11796828 batches: 0.0314
trigger times: 1
Loss after 11797791 batches: 0.0307
trigger times: 2
Loss after 11798754 batches: 0.0304
trigger times: 3
Loss after 11799717 batches: 0.0289
trigger times: 4
Loss after 11800680 batches: 0.0295
trigger times: 5
Loss after 11801643 batches: 0.0282
trigger times: 6
Loss after 11802606 batches: 0.0291
trigger times: 7
Loss after 11803569 batches: 0.0294
trigger times: 8
Loss after 11804532 batches: 0.0277
trigger times: 9
Loss after 11805495 batches: 0.0278
trigger times: 10
Loss after 11806458 batches: 0.0270
trigger times: 11
Loss after 11807421 batches: 0.0278
trigger times: 12
Loss after 11808384 batches: 0.0273
trigger times: 13
Loss after 11809347 batches: 0.0266
trigger times: 14
Loss after 11810310 batches: 0.0270
trigger times: 15
Loss after 11811273 batches: 0.0276
trigger times: 16
Loss after 11812236 batches: 0.0271
trigger times: 17
Loss after 11813199 batches: 0.0259
trigger times: 18
Loss after 11814162 batches: 0.0265
trigger times: 19
Loss after 11815125 batches: 0.0279
trigger times: 20
Loss after 11816088 batches: 0.0249
trigger times: 21
Loss after 11817051 batches: 0.0254
trigger times: 22
Loss after 11818014 batches: 0.0260
trigger times: 23
Loss after 11818977 batches: 0.0260
trigger times: 24
Loss after 11819940 batches: 0.0253
trigger times: 25
Early stopping!
Start to test process.
Loss after 11820903 batches: 0.0251
Time to train on one home:  59.83657765388489
trigger times: 0
Loss after 11821866 batches: 0.1009
trigger times: 1
Loss after 11822829 batches: 0.0788
trigger times: 2
Loss after 11823792 batches: 0.0762
trigger times: 3
Loss after 11824755 batches: 0.0757
trigger times: 4
Loss after 11825718 batches: 0.0726
trigger times: 5
Loss after 11826681 batches: 0.0702
trigger times: 6
Loss after 11827644 batches: 0.0676
trigger times: 7
Loss after 11828607 batches: 0.0662
trigger times: 8
Loss after 11829570 batches: 0.0656
trigger times: 9
Loss after 11830533 batches: 0.0645
trigger times: 10
Loss after 11831496 batches: 0.0622
trigger times: 11
Loss after 11832459 batches: 0.0644
trigger times: 12
Loss after 11833422 batches: 0.0621
trigger times: 13
Loss after 11834385 batches: 0.0624
trigger times: 14
Loss after 11835348 batches: 0.0614
trigger times: 15
Loss after 11836311 batches: 0.0615
trigger times: 16
Loss after 11837274 batches: 0.0616
trigger times: 17
Loss after 11838237 batches: 0.0617
trigger times: 18
Loss after 11839200 batches: 0.0610
trigger times: 19
Loss after 11840163 batches: 0.0620
trigger times: 20
Loss after 11841126 batches: 0.0612
trigger times: 21
Loss after 11842089 batches: 0.0604
trigger times: 22
Loss after 11843052 batches: 0.0587
trigger times: 23
Loss after 11844015 batches: 0.0580
trigger times: 24
Loss after 11844978 batches: 0.0587
trigger times: 25
Early stopping!
Start to test process.
Loss after 11845941 batches: 0.0581
Time to train on one home:  52.11930751800537
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11846904 batches: 0.0843
trigger times: 1
Loss after 11847867 batches: 0.0521
trigger times: 2
Loss after 11848830 batches: 0.0538
trigger times: 0
Loss after 11849793 batches: 0.0438
trigger times: 1
Loss after 11850756 batches: 0.0392
trigger times: 2
Loss after 11851719 batches: 0.0376
trigger times: 3
Loss after 11852682 batches: 0.0358
trigger times: 0
Loss after 11853645 batches: 0.0352
trigger times: 1
Loss after 11854608 batches: 0.0338
trigger times: 0
Loss after 11855571 batches: 0.0321
trigger times: 1
Loss after 11856534 batches: 0.0313
trigger times: 2
Loss after 11857497 batches: 0.0302
trigger times: 3
Loss after 11858460 batches: 0.0302
trigger times: 4
Loss after 11859423 batches: 0.0306
trigger times: 5
Loss after 11860386 batches: 0.0291
trigger times: 6
Loss after 11861349 batches: 0.0295
trigger times: 7
Loss after 11862312 batches: 0.0283
trigger times: 8
Loss after 11863275 batches: 0.0286
trigger times: 9
Loss after 11864238 batches: 0.0277
trigger times: 10
Loss after 11865201 batches: 0.0266
trigger times: 11
Loss after 11866164 batches: 0.0274
trigger times: 12
Loss after 11867127 batches: 0.0272
trigger times: 13
Loss after 11868090 batches: 0.0270
trigger times: 14
Loss after 11869053 batches: 0.0269
trigger times: 15
Loss after 11870016 batches: 0.0271
trigger times: 16
Loss after 11870979 batches: 0.0264
trigger times: 17
Loss after 11871942 batches: 0.0259
trigger times: 18
Loss after 11872905 batches: 0.0252
trigger times: 19
Loss after 11873868 batches: 0.0269
trigger times: 20
Loss after 11874831 batches: 0.0257
trigger times: 0
Loss after 11875794 batches: 0.0247
trigger times: 1
Loss after 11876757 batches: 0.0247
trigger times: 2
Loss after 11877720 batches: 0.0245
trigger times: 3
Loss after 11878683 batches: 0.0246
trigger times: 4
Loss after 11879646 batches: 0.0239
trigger times: 5
Loss after 11880609 batches: 0.0243
trigger times: 6
Loss after 11881572 batches: 0.0231
trigger times: 7
Loss after 11882535 batches: 0.0240
trigger times: 0
Loss after 11883498 batches: 0.0245
trigger times: 1
Loss after 11884461 batches: 0.0234
trigger times: 2
Loss after 11885424 batches: 0.0242
trigger times: 3
Loss after 11886387 batches: 0.0235
trigger times: 4
Loss after 11887350 batches: 0.0236
trigger times: 5
Loss after 11888313 batches: 0.0232
trigger times: 6
Loss after 11889276 batches: 0.0235
trigger times: 7
Loss after 11890239 batches: 0.0245
trigger times: 8
Loss after 11891202 batches: 0.0242
trigger times: 9
Loss after 11892165 batches: 0.0230
trigger times: 10
Loss after 11893128 batches: 0.0224
trigger times: 11
Loss after 11894091 batches: 0.0219
trigger times: 12
Loss after 11895054 batches: 0.0222
trigger times: 13
Loss after 11896017 batches: 0.0230
trigger times: 14
Loss after 11896980 batches: 0.0220
trigger times: 15
Loss after 11897943 batches: 0.0226
trigger times: 16
Loss after 11898906 batches: 0.0235
trigger times: 17
Loss after 11899869 batches: 0.0232
trigger times: 18
Loss after 11900832 batches: 0.0227
trigger times: 19
Loss after 11901795 batches: 0.0227
trigger times: 20
Loss after 11902758 batches: 0.0216
trigger times: 21
Loss after 11903721 batches: 0.0218
trigger times: 22
Loss after 11904684 batches: 0.0214
trigger times: 23
Loss after 11905647 batches: 0.0210
trigger times: 24
Loss after 11906610 batches: 0.0219
trigger times: 25
Early stopping!
Start to test process.
Loss after 11907573 batches: 0.0213
Time to train on one home:  80.8354320526123
trigger times: 0
Loss after 11908468 batches: 0.0622
trigger times: 1
Loss after 11909363 batches: 0.0365
trigger times: 2
Loss after 11910258 batches: 0.0189
trigger times: 3
Loss after 11911153 batches: 0.0116
trigger times: 4
Loss after 11912048 batches: 0.0096
trigger times: 5
Loss after 11912943 batches: 0.0075
trigger times: 6
Loss after 11913838 batches: 0.0063
trigger times: 7
Loss after 11914733 batches: 0.0055
trigger times: 8
Loss after 11915628 batches: 0.0049
trigger times: 9
Loss after 11916523 batches: 0.0044
trigger times: 10
Loss after 11917418 batches: 0.0040
trigger times: 11
Loss after 11918313 batches: 0.0038
trigger times: 12
Loss after 11919208 batches: 0.0035
trigger times: 13
Loss after 11920103 batches: 0.0034
trigger times: 14
Loss after 11920998 batches: 0.0030
trigger times: 15
Loss after 11921893 batches: 0.0030
trigger times: 16
Loss after 11922788 batches: 0.0031
trigger times: 17
Loss after 11923683 batches: 0.0032
trigger times: 18
Loss after 11924578 batches: 0.0031
trigger times: 19
Loss after 11925473 batches: 0.0027
trigger times: 20
Loss after 11926368 batches: 0.0026
trigger times: 21
Loss after 11927263 batches: 0.0023
trigger times: 22
Loss after 11928158 batches: 0.0067
trigger times: 23
Loss after 11929053 batches: 0.0052
trigger times: 24
Loss after 11929948 batches: 0.0040
trigger times: 25
Early stopping!
Start to test process.
Loss after 11930843 batches: 0.0034
Time to train on one home:  50.80108690261841
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 11931780 batches: 0.0810
trigger times: 1
Loss after 11932717 batches: 0.0678
trigger times: 2
Loss after 11933654 batches: 0.0649
trigger times: 3
Loss after 11934591 batches: 0.0621
trigger times: 4
Loss after 11935528 batches: 0.0599
trigger times: 5
Loss after 11936465 batches: 0.0573
trigger times: 6
Loss after 11937402 batches: 0.0559
trigger times: 7
Loss after 11938339 batches: 0.0552
trigger times: 8
Loss after 11939276 batches: 0.0543
trigger times: 9
Loss after 11940213 batches: 0.0532
trigger times: 10
Loss after 11941150 batches: 0.0540
trigger times: 11
Loss after 11942087 batches: 0.0528
trigger times: 12
Loss after 11943024 batches: 0.0521
trigger times: 13
Loss after 11943961 batches: 0.0526
trigger times: 14
Loss after 11944898 batches: 0.0517
trigger times: 15
Loss after 11945835 batches: 0.0506
trigger times: 16
Loss after 11946772 batches: 0.0511
trigger times: 17
Loss after 11947709 batches: 0.0504
trigger times: 18
Loss after 11948646 batches: 0.0506
trigger times: 19
Loss after 11949583 batches: 0.0506
trigger times: 20
Loss after 11950520 batches: 0.0493
trigger times: 21
Loss after 11951457 batches: 0.0486
trigger times: 22
Loss after 11952394 batches: 0.0489
trigger times: 23
Loss after 11953331 batches: 0.0478
trigger times: 24
Loss after 11954268 batches: 0.0499
trigger times: 25
Early stopping!
Start to test process.
Loss after 11955205 batches: 0.0481
Time to train on one home:  52.27873253822327
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066, 0.03401813118363776, 0.033508403208926535]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863], [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861], [0.0757279247045517, 0.10266384027132536, 0.5909740425258612, 0.8803234656976516, 0.8076583271010261, 28.206427011804568, 8296.775]]
Round_9_results:  [0.0757279247045517, 0.10266384027132536, 0.5909740425258612, 0.8803234656976516, 0.8076583271010261, 28.206427011804568, 8296.775]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 12470 < 12471; dropping {'Training_Loss': 0.07177395586456571, 'Validation_Loss': 0.08150625973939896, 'Training_R2': -0.106969158744052, 'Validation_R2': 0.12408845307678495, 'Training_F1': 0.42401772600788995, 'Validation_F1': 0.6059249568075865, 'Training_NEP': 0.8328791754852128, 'Validation_NEP': 0.7730591371483111, 'Training_NDE': 0.6741710591217458, 'Validation_NDE': 0.7913652635417786, 'Training_MAE': 31.649676638884635, 'Validation_MAE': 27.48942146684381, 'Training_MSE': 2489.974, 'Validation_MSE': 10366.8955}.
trigger times: 0
Loss after 11956167 batches: 0.0718
trigger times: 1
Loss after 11957129 batches: 0.0647
trigger times: 2
Loss after 11958091 batches: 0.0629
trigger times: 3
Loss after 11959053 batches: 0.0594
trigger times: 4
Loss after 11960015 batches: 0.0572
trigger times: 5
Loss after 11960977 batches: 0.0557
trigger times: 6
Loss after 11961939 batches: 0.0550
trigger times: 7
Loss after 11962901 batches: 0.0540
trigger times: 8
Loss after 11963863 batches: 0.0539
trigger times: 9
Loss after 11964825 batches: 0.0532
trigger times: 10
Loss after 11965787 batches: 0.0524
trigger times: 11
Loss after 11966749 batches: 0.0518
trigger times: 12
Loss after 11967711 batches: 0.0512
trigger times: 13
Loss after 11968673 batches: 0.0512
trigger times: 14
Loss after 11969635 batches: 0.0518
trigger times: 15
Loss after 11970597 batches: 0.0509
trigger times: 16
Loss after 11971559 batches: 0.0507
trigger times: 17
Loss after 11972521 batches: 0.0505
trigger times: 18
Loss after 11973483 batches: 0.0502
trigger times: 19
Loss after 11974445 batches: 0.0500
trigger times: 20
Loss after 11975407 batches: 0.0499
trigger times: 21
Loss after 11976369 batches: 0.0498
trigger times: 22
Loss after 11977331 batches: 0.0500
trigger times: 23
Loss after 11978293 batches: 0.0494
trigger times: 24
Loss after 11979255 batches: 0.0489
trigger times: 25
Early stopping!
Start to test process.
Loss after 11980217 batches: 0.0489
Time to train on one home:  52.420799016952515
trigger times: 0
Loss after 11981146 batches: 0.1094
trigger times: 1
Loss after 11982075 batches: 0.0666
trigger times: 2
Loss after 11983004 batches: 0.0509
trigger times: 3
Loss after 11983933 batches: 0.0459
trigger times: 4
Loss after 11984862 batches: 0.0402
trigger times: 5
Loss after 11985791 batches: 0.0379
trigger times: 6
Loss after 11986720 batches: 0.0361
trigger times: 7
Loss after 11987649 batches: 0.0351
trigger times: 8
Loss after 11988578 batches: 0.0331
trigger times: 9
Loss after 11989507 batches: 0.0335
trigger times: 10
Loss after 11990436 batches: 0.0340
trigger times: 11
Loss after 11991365 batches: 0.0304
trigger times: 12
Loss after 11992294 batches: 0.0290
trigger times: 13
Loss after 11993223 batches: 0.0315
trigger times: 14
Loss after 11994152 batches: 0.0281
trigger times: 15
Loss after 11995081 batches: 0.0264
trigger times: 16
Loss after 11996010 batches: 0.0292
trigger times: 17
Loss after 11996939 batches: 0.0275
trigger times: 18
Loss after 11997868 batches: 0.0296
trigger times: 19
Loss after 11998797 batches: 0.0290
trigger times: 20
Loss after 11999726 batches: 0.0262
trigger times: 21
Loss after 12000655 batches: 0.0263
trigger times: 22
Loss after 12001584 batches: 0.0283
trigger times: 23
Loss after 12002513 batches: 0.0272
trigger times: 24
Loss after 12003442 batches: 0.0238
trigger times: 25
Early stopping!
Start to test process.
Loss after 12004371 batches: 0.0253
Time to train on one home:  51.744038581848145
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 12005334 batches: 0.0336
trigger times: 1
Loss after 12006297 batches: 0.0162
trigger times: 2
Loss after 12007260 batches: 0.0142
trigger times: 3
Loss after 12008223 batches: 0.0138
trigger times: 4
Loss after 12009186 batches: 0.0130
trigger times: 5
Loss after 12010149 batches: 0.0127
trigger times: 6
Loss after 12011112 batches: 0.0117
trigger times: 7
Loss after 12012075 batches: 0.0106
trigger times: 8
Loss after 12013038 batches: 0.0100
trigger times: 9
Loss after 12014001 batches: 0.0095
trigger times: 10
Loss after 12014964 batches: 0.0090
trigger times: 11
Loss after 12015927 batches: 0.0085
trigger times: 12
Loss after 12016890 batches: 0.0083
trigger times: 13
Loss after 12017853 batches: 0.0083
trigger times: 14
Loss after 12018816 batches: 0.0082
trigger times: 15
Loss after 12019779 batches: 0.0080
trigger times: 16
Loss after 12020742 batches: 0.0076
trigger times: 17
Loss after 12021705 batches: 0.0079
trigger times: 18
Loss after 12022668 batches: 0.0072
trigger times: 19
Loss after 12023631 batches: 0.0073
trigger times: 20
Loss after 12024594 batches: 0.0074
trigger times: 21
Loss after 12025557 batches: 0.0073
trigger times: 22
Loss after 12026520 batches: 0.0071
trigger times: 23
Loss after 12027483 batches: 0.0072
trigger times: 24
Loss after 12028446 batches: 0.0070
trigger times: 25
Early stopping!
Start to test process.
Loss after 12029409 batches: 0.0068
Time to train on one home:  52.3419144153595
trigger times: 0
Loss after 12030372 batches: 0.0225
trigger times: 1
Loss after 12031335 batches: 0.0204
trigger times: 2
Loss after 12032298 batches: 0.0181
trigger times: 3
Loss after 12033261 batches: 0.0169
trigger times: 4
Loss after 12034224 batches: 0.0160
trigger times: 5
Loss after 12035187 batches: 0.0154
trigger times: 6
Loss after 12036150 batches: 0.0148
trigger times: 0
Loss after 12037113 batches: 0.0142
trigger times: 1
Loss after 12038076 batches: 0.0140
trigger times: 2
Loss after 12039039 batches: 0.0138
trigger times: 3
Loss after 12040002 batches: 0.0139
trigger times: 4
Loss after 12040965 batches: 0.0140
trigger times: 5
Loss after 12041928 batches: 0.0137
trigger times: 6
Loss after 12042891 batches: 0.0131
trigger times: 7
Loss after 12043854 batches: 0.0150
trigger times: 8
Loss after 12044817 batches: 0.0151
trigger times: 9
Loss after 12045780 batches: 0.0143
trigger times: 10
Loss after 12046743 batches: 0.0144
trigger times: 11
Loss after 12047706 batches: 0.0135
trigger times: 12
Loss after 12048669 batches: 0.0134
trigger times: 13
Loss after 12049632 batches: 0.0135
trigger times: 14
Loss after 12050595 batches: 0.0127
trigger times: 15
Loss after 12051558 batches: 0.0130
trigger times: 16
Loss after 12052521 batches: 0.0127
trigger times: 17
Loss after 12053484 batches: 0.0124
trigger times: 18
Loss after 12054447 batches: 0.0129
trigger times: 19
Loss after 12055410 batches: 0.0128
trigger times: 20
Loss after 12056373 batches: 0.0129
trigger times: 21
Loss after 12057336 batches: 0.0125
trigger times: 22
Loss after 12058299 batches: 0.0121
trigger times: 23
Loss after 12059262 batches: 0.0126
trigger times: 24
Loss after 12060225 batches: 0.0124
trigger times: 25
Early stopping!
Start to test process.
Loss after 12061188 batches: 0.0126
Time to train on one home:  57.444395542144775
trigger times: 0
Loss after 12062151 batches: 0.0969
trigger times: 1
Loss after 12063114 batches: 0.0889
trigger times: 2
Loss after 12064077 batches: 0.0827
trigger times: 3
Loss after 12065040 batches: 0.0791
trigger times: 4
Loss after 12066003 batches: 0.0768
trigger times: 5
Loss after 12066966 batches: 0.0735
trigger times: 6
Loss after 12067929 batches: 0.0711
trigger times: 7
Loss after 12068892 batches: 0.0703
trigger times: 8
Loss after 12069855 batches: 0.0682
trigger times: 9
Loss after 12070818 batches: 0.0668
trigger times: 10
Loss after 12071781 batches: 0.0672
trigger times: 11
Loss after 12072744 batches: 0.0648
trigger times: 12
Loss after 12073707 batches: 0.0649
trigger times: 13
Loss after 12074670 batches: 0.0627
trigger times: 14
Loss after 12075633 batches: 0.0651
trigger times: 15
Loss after 12076596 batches: 0.0631
trigger times: 16
Loss after 12077559 batches: 0.0660
trigger times: 17
Loss after 12078522 batches: 0.0623
trigger times: 18
Loss after 12079485 batches: 0.0614
trigger times: 19
Loss after 12080448 batches: 0.0622
trigger times: 20
Loss after 12081411 batches: 0.0624
trigger times: 21
Loss after 12082374 batches: 0.0613
trigger times: 22
Loss after 12083337 batches: 0.0613
trigger times: 23
Loss after 12084300 batches: 0.0603
trigger times: 24
Loss after 12085263 batches: 0.0582
trigger times: 25
Early stopping!
Start to test process.
Loss after 12086226 batches: 0.0576
Time to train on one home:  52.18004512786865
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12087189 batches: 0.0798
trigger times: 1
Loss after 12088152 batches: 0.0713
trigger times: 2
Loss after 12089115 batches: 0.0683
trigger times: 3
Loss after 12090078 batches: 0.0667
trigger times: 4
Loss after 12091041 batches: 0.0651
trigger times: 5
Loss after 12092004 batches: 0.0622
trigger times: 6
Loss after 12092967 batches: 0.0625
trigger times: 7
Loss after 12093930 batches: 0.0604
trigger times: 8
Loss after 12094893 batches: 0.0588
trigger times: 9
Loss after 12095856 batches: 0.0592
trigger times: 10
Loss after 12096819 batches: 0.0594
trigger times: 11
Loss after 12097782 batches: 0.0583
trigger times: 12
Loss after 12098745 batches: 0.0565
trigger times: 13
Loss after 12099708 batches: 0.0575
trigger times: 14
Loss after 12100671 batches: 0.0572
trigger times: 15
Loss after 12101634 batches: 0.0561
trigger times: 16
Loss after 12102597 batches: 0.0553
trigger times: 17
Loss after 12103560 batches: 0.0584
trigger times: 18
Loss after 12104523 batches: 0.0562
trigger times: 19
Loss after 12105486 batches: 0.0570
trigger times: 20
Loss after 12106449 batches: 0.0560
trigger times: 21
Loss after 12107412 batches: 0.0551
trigger times: 22
Loss after 12108375 batches: 0.0544
trigger times: 23
Loss after 12109338 batches: 0.0540
trigger times: 24
Loss after 12110301 batches: 0.0542
trigger times: 25
Early stopping!
Start to test process.
Loss after 12111264 batches: 0.0548
Time to train on one home:  52.126091957092285
trigger times: 0
Loss after 12112227 batches: 0.0748
trigger times: 1
Loss after 12113190 batches: 0.0693
trigger times: 2
Loss after 12114153 batches: 0.0659
trigger times: 3
Loss after 12115116 batches: 0.0644
trigger times: 4
Loss after 12116079 batches: 0.0614
trigger times: 5
Loss after 12117042 batches: 0.0606
trigger times: 6
Loss after 12118005 batches: 0.0591
trigger times: 7
Loss after 12118968 batches: 0.0575
trigger times: 8
Loss after 12119931 batches: 0.0567
trigger times: 9
Loss after 12120894 batches: 0.0564
trigger times: 10
Loss after 12121857 batches: 0.0552
trigger times: 11
Loss after 12122820 batches: 0.0548
trigger times: 12
Loss after 12123783 batches: 0.0557
trigger times: 13
Loss after 12124746 batches: 0.0564
trigger times: 14
Loss after 12125709 batches: 0.0552
trigger times: 15
Loss after 12126672 batches: 0.0535
trigger times: 16
Loss after 12127635 batches: 0.0531
trigger times: 17
Loss after 12128598 batches: 0.0526
trigger times: 18
Loss after 12129561 batches: 0.0532
trigger times: 19
Loss after 12130524 batches: 0.0534
trigger times: 20
Loss after 12131487 batches: 0.0543
trigger times: 21
Loss after 12132450 batches: 0.0521
trigger times: 22
Loss after 12133413 batches: 0.0524
trigger times: 23
Loss after 12134376 batches: 0.0519
trigger times: 24
Loss after 12135339 batches: 0.0518
trigger times: 25
Early stopping!
Start to test process.
Loss after 12136302 batches: 0.0516
Time to train on one home:  52.14710760116577
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12137265 batches: 0.0628
trigger times: 1
Loss after 12138228 batches: 0.0505
trigger times: 2
Loss after 12139191 batches: 0.0488
trigger times: 3
Loss after 12140154 batches: 0.0433
trigger times: 4
Loss after 12141117 batches: 0.0371
trigger times: 5
Loss after 12142080 batches: 0.0350
trigger times: 6
Loss after 12143043 batches: 0.0324
trigger times: 7
Loss after 12144006 batches: 0.0306
trigger times: 8
Loss after 12144969 batches: 0.0290
trigger times: 9
Loss after 12145932 batches: 0.0293
trigger times: 10
Loss after 12146895 batches: 0.0286
trigger times: 11
Loss after 12147858 batches: 0.0281
trigger times: 12
Loss after 12148821 batches: 0.0265
trigger times: 13
Loss after 12149784 batches: 0.0265
trigger times: 14
Loss after 12150747 batches: 0.0269
trigger times: 15
Loss after 12151710 batches: 0.0268
trigger times: 16
Loss after 12152673 batches: 0.0254
trigger times: 17
Loss after 12153636 batches: 0.0250
trigger times: 18
Loss after 12154599 batches: 0.0252
trigger times: 19
Loss after 12155562 batches: 0.0247
trigger times: 20
Loss after 12156525 batches: 0.0243
trigger times: 21
Loss after 12157488 batches: 0.0240
trigger times: 22
Loss after 12158451 batches: 0.0239
trigger times: 23
Loss after 12159414 batches: 0.0238
trigger times: 24
Loss after 12160377 batches: 0.0236
trigger times: 25
Early stopping!
Start to test process.
Loss after 12161340 batches: 0.0234
Time to train on one home:  52.43004751205444
trigger times: 0
Loss after 12162298 batches: 0.0767
trigger times: 1
Loss after 12163256 batches: 0.0474
trigger times: 2
Loss after 12164214 batches: 0.0382
trigger times: 3
Loss after 12165172 batches: 0.0327
trigger times: 4
Loss after 12166130 batches: 0.0325
trigger times: 5
Loss after 12167088 batches: 0.0306
trigger times: 6
Loss after 12168046 batches: 0.0285
trigger times: 7
Loss after 12169004 batches: 0.0262
trigger times: 8
Loss after 12169962 batches: 0.0264
trigger times: 9
Loss after 12170920 batches: 0.0236
trigger times: 10
Loss after 12171878 batches: 0.0240
trigger times: 11
Loss after 12172836 batches: 0.0218
trigger times: 12
Loss after 12173794 batches: 0.0214
trigger times: 13
Loss after 12174752 batches: 0.0203
trigger times: 14
Loss after 12175710 batches: 0.0207
trigger times: 15
Loss after 12176668 batches: 0.0207
trigger times: 16
Loss after 12177626 batches: 0.0207
trigger times: 17
Loss after 12178584 batches: 0.0192
trigger times: 18
Loss after 12179542 batches: 0.0188
trigger times: 19
Loss after 12180500 batches: 0.0186
trigger times: 20
Loss after 12181458 batches: 0.0182
trigger times: 21
Loss after 12182416 batches: 0.0171
trigger times: 22
Loss after 12183374 batches: 0.0183
trigger times: 23
Loss after 12184332 batches: 0.0187
trigger times: 24
Loss after 12185290 batches: 0.0193
trigger times: 25
Early stopping!
Start to test process.
Loss after 12186248 batches: 0.0188
Time to train on one home:  52.147143840789795
trigger times: 0
Loss after 12187210 batches: 0.0708
trigger times: 1
Loss after 12188172 batches: 0.0645
trigger times: 2
Loss after 12189134 batches: 0.0628
trigger times: 3
Loss after 12190096 batches: 0.0597
trigger times: 4
Loss after 12191058 batches: 0.0582
trigger times: 5
Loss after 12192020 batches: 0.0567
trigger times: 6
Loss after 12192982 batches: 0.0542
trigger times: 7
Loss after 12193944 batches: 0.0538
trigger times: 8
Loss after 12194906 batches: 0.0538
trigger times: 9
Loss after 12195868 batches: 0.0526
trigger times: 10
Loss after 12196830 batches: 0.0527
trigger times: 11
Loss after 12197792 batches: 0.0525
trigger times: 12
Loss after 12198754 batches: 0.0511
trigger times: 13
Loss after 12199716 batches: 0.0522
trigger times: 14
Loss after 12200678 batches: 0.0511
trigger times: 15
Loss after 12201640 batches: 0.0511
trigger times: 16
Loss after 12202602 batches: 0.0505
trigger times: 17
Loss after 12203564 batches: 0.0505
trigger times: 18
Loss after 12204526 batches: 0.0491
trigger times: 19
Loss after 12205488 batches: 0.0503
trigger times: 20
Loss after 12206450 batches: 0.0501
trigger times: 21
Loss after 12207412 batches: 0.0507
trigger times: 22
Loss after 12208374 batches: 0.0483
trigger times: 23
Loss after 12209336 batches: 0.0488
trigger times: 24
Loss after 12210298 batches: 0.0486
trigger times: 25
Early stopping!
Start to test process.
Loss after 12211260 batches: 0.0480
Time to train on one home:  52.77995538711548
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12212223 batches: 0.0663
trigger times: 1
Loss after 12213186 batches: 0.0244
trigger times: 2
Loss after 12214149 batches: 0.0203
trigger times: 3
Loss after 12215112 batches: 0.0167
trigger times: 4
Loss after 12216075 batches: 0.0155
trigger times: 5
Loss after 12217038 batches: 0.0148
trigger times: 6
Loss after 12218001 batches: 0.0143
trigger times: 7
Loss after 12218964 batches: 0.0139
trigger times: 8
Loss after 12219927 batches: 0.0135
trigger times: 9
Loss after 12220890 batches: 0.0134
trigger times: 10
Loss after 12221853 batches: 0.0136
trigger times: 11
Loss after 12222816 batches: 0.0134
trigger times: 12
Loss after 12223779 batches: 0.0131
trigger times: 13
Loss after 12224742 batches: 0.0131
trigger times: 14
Loss after 12225705 batches: 0.0128
trigger times: 15
Loss after 12226668 batches: 0.0128
trigger times: 16
Loss after 12227631 batches: 0.0128
trigger times: 17
Loss after 12228594 batches: 0.0130
trigger times: 18
Loss after 12229557 batches: 0.0125
trigger times: 19
Loss after 12230520 batches: 0.0125
trigger times: 20
Loss after 12231483 batches: 0.0124
trigger times: 21
Loss after 12232446 batches: 0.0125
trigger times: 22
Loss after 12233409 batches: 0.0123
trigger times: 23
Loss after 12234372 batches: 0.0125
trigger times: 24
Loss after 12235335 batches: 0.0123
trigger times: 25
Early stopping!
Start to test process.
Loss after 12236298 batches: 0.0119
Time to train on one home:  52.10736966133118
trigger times: 0
Loss after 12237261 batches: 0.0559
trigger times: 1
Loss after 12238224 batches: 0.0434
trigger times: 2
Loss after 12239187 batches: 0.0402
trigger times: 3
Loss after 12240150 batches: 0.0368
trigger times: 4
Loss after 12241113 batches: 0.0341
trigger times: 5
Loss after 12242076 batches: 0.0325
trigger times: 6
Loss after 12243039 batches: 0.0316
trigger times: 7
Loss after 12244002 batches: 0.0301
trigger times: 8
Loss after 12244965 batches: 0.0299
trigger times: 9
Loss after 12245928 batches: 0.0335
trigger times: 10
Loss after 12246891 batches: 0.0324
trigger times: 11
Loss after 12247854 batches: 0.0321
trigger times: 12
Loss after 12248817 batches: 0.0321
trigger times: 13
Loss after 12249780 batches: 0.0307
trigger times: 14
Loss after 12250743 batches: 0.0300
trigger times: 15
Loss after 12251706 batches: 0.0305
trigger times: 16
Loss after 12252669 batches: 0.0301
trigger times: 17
Loss after 12253632 batches: 0.0283
trigger times: 18
Loss after 12254595 batches: 0.0281
trigger times: 19
Loss after 12255558 batches: 0.0283
trigger times: 20
Loss after 12256521 batches: 0.0303
trigger times: 21
Loss after 12257484 batches: 0.0291
trigger times: 22
Loss after 12258447 batches: 0.0298
trigger times: 23
Loss after 12259410 batches: 0.0273
trigger times: 24
Loss after 12260373 batches: 0.0277
trigger times: 25
Early stopping!
Start to test process.
Loss after 12261336 batches: 0.0262
Time to train on one home:  52.75327014923096
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12262299 batches: 0.0630
trigger times: 1
Loss after 12263262 batches: 0.0512
trigger times: 2
Loss after 12264225 batches: 0.0491
trigger times: 3
Loss after 12265188 batches: 0.0444
trigger times: 4
Loss after 12266151 batches: 0.0371
trigger times: 5
Loss after 12267114 batches: 0.0340
trigger times: 6
Loss after 12268077 batches: 0.0318
trigger times: 7
Loss after 12269040 batches: 0.0305
trigger times: 8
Loss after 12270003 batches: 0.0291
trigger times: 9
Loss after 12270966 batches: 0.0289
trigger times: 10
Loss after 12271929 batches: 0.0282
trigger times: 11
Loss after 12272892 batches: 0.0274
trigger times: 12
Loss after 12273855 batches: 0.0267
trigger times: 13
Loss after 12274818 batches: 0.0267
trigger times: 14
Loss after 12275781 batches: 0.0263
trigger times: 15
Loss after 12276744 batches: 0.0264
trigger times: 16
Loss after 12277707 batches: 0.0257
trigger times: 17
Loss after 12278670 batches: 0.0255
trigger times: 18
Loss after 12279633 batches: 0.0253
trigger times: 19
Loss after 12280596 batches: 0.0247
trigger times: 20
Loss after 12281559 batches: 0.0242
trigger times: 21
Loss after 12282522 batches: 0.0237
trigger times: 22
Loss after 12283485 batches: 0.0256
trigger times: 23
Loss after 12284448 batches: 0.0246
trigger times: 24
Loss after 12285411 batches: 0.0242
trigger times: 25
Early stopping!
Start to test process.
Loss after 12286374 batches: 0.0240
Time to train on one home:  52.423322439193726
trigger times: 0
Loss after 12287337 batches: 0.0557
trigger times: 1
Loss after 12288300 batches: 0.0437
trigger times: 2
Loss after 12289263 batches: 0.0401
trigger times: 3
Loss after 12290226 batches: 0.0372
trigger times: 4
Loss after 12291189 batches: 0.0343
trigger times: 5
Loss after 12292152 batches: 0.0338
trigger times: 6
Loss after 12293115 batches: 0.0318
trigger times: 7
Loss after 12294078 batches: 0.0310
trigger times: 8
Loss after 12295041 batches: 0.0304
trigger times: 9
Loss after 12296004 batches: 0.0291
trigger times: 10
Loss after 12296967 batches: 0.0297
trigger times: 11
Loss after 12297930 batches: 0.0296
trigger times: 12
Loss after 12298893 batches: 0.0307
trigger times: 13
Loss after 12299856 batches: 0.0290
trigger times: 14
Loss after 12300819 batches: 0.0307
trigger times: 15
Loss after 12301782 batches: 0.0303
trigger times: 16
Loss after 12302745 batches: 0.0288
trigger times: 17
Loss after 12303708 batches: 0.0281
trigger times: 18
Loss after 12304671 batches: 0.0288
trigger times: 19
Loss after 12305634 batches: 0.0277
trigger times: 20
Loss after 12306597 batches: 0.0277
trigger times: 21
Loss after 12307560 batches: 0.0308
trigger times: 22
Loss after 12308523 batches: 0.0309
trigger times: 23
Loss after 12309486 batches: 0.0306
trigger times: 24
Loss after 12310449 batches: 0.0290
trigger times: 25
Early stopping!
Start to test process.
Loss after 12311412 batches: 0.0281
Time to train on one home:  52.13640570640564
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12312375 batches: 0.0507
trigger times: 1
Loss after 12313338 batches: 0.0457
trigger times: 2
Loss after 12314301 batches: 0.0434
trigger times: 3
Loss after 12315264 batches: 0.0412
trigger times: 0
Loss after 12316227 batches: 0.0387
trigger times: 1
Loss after 12317190 batches: 0.0372
trigger times: 2
Loss after 12318153 batches: 0.0363
trigger times: 0
Loss after 12319116 batches: 0.0349
trigger times: 1
Loss after 12320079 batches: 0.0340
trigger times: 0
Loss after 12321042 batches: 0.0325
trigger times: 1
Loss after 12322005 batches: 0.0331
trigger times: 2
Loss after 12322968 batches: 0.0327
trigger times: 3
Loss after 12323931 batches: 0.0336
trigger times: 4
Loss after 12324894 batches: 0.0322
trigger times: 5
Loss after 12325857 batches: 0.0315
trigger times: 6
Loss after 12326820 batches: 0.0326
trigger times: 7
Loss after 12327783 batches: 0.0319
trigger times: 8
Loss after 12328746 batches: 0.0312
trigger times: 9
Loss after 12329709 batches: 0.0301
trigger times: 10
Loss after 12330672 batches: 0.0293
trigger times: 11
Loss after 12331635 batches: 0.0301
trigger times: 12
Loss after 12332598 batches: 0.0288
trigger times: 13
Loss after 12333561 batches: 0.0290
trigger times: 14
Loss after 12334524 batches: 0.0290
trigger times: 15
Loss after 12335487 batches: 0.0294
trigger times: 0
Loss after 12336450 batches: 0.0284
trigger times: 1
Loss after 12337413 batches: 0.0301
trigger times: 2
Loss after 12338376 batches: 0.0300
trigger times: 3
Loss after 12339339 batches: 0.0288
trigger times: 4
Loss after 12340302 batches: 0.0287
trigger times: 5
Loss after 12341265 batches: 0.0286
trigger times: 6
Loss after 12342228 batches: 0.0274
trigger times: 7
Loss after 12343191 batches: 0.0270
trigger times: 8
Loss after 12344154 batches: 0.0273
trigger times: 9
Loss after 12345117 batches: 0.0282
trigger times: 10
Loss after 12346080 batches: 0.0264
trigger times: 11
Loss after 12347043 batches: 0.0262
trigger times: 12
Loss after 12348006 batches: 0.0258
trigger times: 13
Loss after 12348969 batches: 0.0255
trigger times: 14
Loss after 12349932 batches: 0.0257
trigger times: 15
Loss after 12350895 batches: 0.0274
trigger times: 16
Loss after 12351858 batches: 0.0274
trigger times: 17
Loss after 12352821 batches: 0.0266
trigger times: 18
Loss after 12353784 batches: 0.0259
trigger times: 19
Loss after 12354747 batches: 0.0257
trigger times: 20
Loss after 12355710 batches: 0.0269
trigger times: 21
Loss after 12356673 batches: 0.0277
trigger times: 22
Loss after 12357636 batches: 0.0270
trigger times: 23
Loss after 12358599 batches: 0.0266
trigger times: 24
Loss after 12359562 batches: 0.0255
trigger times: 25
Early stopping!
Start to test process.
Loss after 12360525 batches: 0.0269
Time to train on one home:  71.50251173973083
trigger times: 0
Loss after 12361488 batches: 0.0715
trigger times: 1
Loss after 12362451 batches: 0.0472
trigger times: 2
Loss after 12363414 batches: 0.0476
trigger times: 3
Loss after 12364377 batches: 0.0440
trigger times: 4
Loss after 12365340 batches: 0.0420
trigger times: 5
Loss after 12366303 batches: 0.0406
trigger times: 6
Loss after 12367266 batches: 0.0391
trigger times: 7
Loss after 12368229 batches: 0.0389
trigger times: 8
Loss after 12369192 batches: 0.0378
trigger times: 9
Loss after 12370155 batches: 0.0370
trigger times: 10
Loss after 12371118 batches: 0.0369
trigger times: 11
Loss after 12372081 batches: 0.0360
trigger times: 12
Loss after 12373044 batches: 0.0368
trigger times: 13
Loss after 12374007 batches: 0.0366
trigger times: 14
Loss after 12374970 batches: 0.0360
trigger times: 15
Loss after 12375933 batches: 0.0369
trigger times: 16
Loss after 12376896 batches: 0.0354
trigger times: 17
Loss after 12377859 batches: 0.0349
trigger times: 18
Loss after 12378822 batches: 0.0345
trigger times: 19
Loss after 12379785 batches: 0.0350
trigger times: 20
Loss after 12380748 batches: 0.0344
trigger times: 21
Loss after 12381711 batches: 0.0342
trigger times: 22
Loss after 12382674 batches: 0.0345
trigger times: 23
Loss after 12383637 batches: 0.0343
trigger times: 24
Loss after 12384600 batches: 0.0350
trigger times: 25
Early stopping!
Start to test process.
Loss after 12385563 batches: 0.0350
Time to train on one home:  52.49620246887207
trigger times: 0
Loss after 12386526 batches: 0.0966
trigger times: 1
Loss after 12387489 batches: 0.0888
trigger times: 2
Loss after 12388452 batches: 0.0820
trigger times: 3
Loss after 12389415 batches: 0.0795
trigger times: 4
Loss after 12390378 batches: 0.0767
trigger times: 5
Loss after 12391341 batches: 0.0747
trigger times: 6
Loss after 12392304 batches: 0.0717
trigger times: 7
Loss after 12393267 batches: 0.0705
trigger times: 8
Loss after 12394230 batches: 0.0680
trigger times: 9
Loss after 12395193 batches: 0.0665
trigger times: 10
Loss after 12396156 batches: 0.0656
trigger times: 11
Loss after 12397119 batches: 0.0683
trigger times: 12
Loss after 12398082 batches: 0.0669
trigger times: 13
Loss after 12399045 batches: 0.0662
trigger times: 14
Loss after 12400008 batches: 0.0653
trigger times: 15
Loss after 12400971 batches: 0.0665
trigger times: 16
Loss after 12401934 batches: 0.0640
trigger times: 17
Loss after 12402897 batches: 0.0632
trigger times: 18
Loss after 12403860 batches: 0.0637
trigger times: 19
Loss after 12404823 batches: 0.0616
trigger times: 20
Loss after 12405786 batches: 0.0607
trigger times: 21
Loss after 12406749 batches: 0.0620
trigger times: 22
Loss after 12407712 batches: 0.0590
trigger times: 23
Loss after 12408675 batches: 0.0629
trigger times: 24
Loss after 12409638 batches: 0.0618
trigger times: 25
Early stopping!
Start to test process.
Loss after 12410601 batches: 0.0601
Time to train on one home:  52.33022856712341
trigger times: 0
Loss after 12411564 batches: 0.1028
trigger times: 1
Loss after 12412527 batches: 0.0650
trigger times: 2
Loss after 12413490 batches: 0.0603
trigger times: 3
Loss after 12414453 batches: 0.0543
trigger times: 4
Loss after 12415416 batches: 0.0507
trigger times: 5
Loss after 12416379 batches: 0.0478
trigger times: 6
Loss after 12417342 batches: 0.0463
trigger times: 7
Loss after 12418305 batches: 0.0443
trigger times: 8
Loss after 12419268 batches: 0.0434
trigger times: 9
Loss after 12420231 batches: 0.0417
trigger times: 10
Loss after 12421194 batches: 0.0416
trigger times: 11
Loss after 12422157 batches: 0.0408
trigger times: 12
Loss after 12423120 batches: 0.0403
trigger times: 13
Loss after 12424083 batches: 0.0411
trigger times: 14
Loss after 12425046 batches: 0.0399
trigger times: 15
Loss after 12426009 batches: 0.0397
trigger times: 16
Loss after 12426972 batches: 0.0390
trigger times: 17
Loss after 12427935 batches: 0.0389
trigger times: 18
Loss after 12428898 batches: 0.0384
trigger times: 19
Loss after 12429861 batches: 0.0381
trigger times: 20
Loss after 12430824 batches: 0.0382
trigger times: 21
Loss after 12431787 batches: 0.0372
trigger times: 22
Loss after 12432750 batches: 0.0371
trigger times: 23
Loss after 12433713 batches: 0.0372
trigger times: 24
Loss after 12434676 batches: 0.0380
trigger times: 25
Early stopping!
Start to test process.
Loss after 12435639 batches: 0.0390
Time to train on one home:  52.18525767326355
trigger times: 0
Loss after 12436568 batches: 0.1120
trigger times: 1
Loss after 12437497 batches: 0.0683
trigger times: 2
Loss after 12438426 batches: 0.0510
trigger times: 3
Loss after 12439355 batches: 0.0465
trigger times: 4
Loss after 12440284 batches: 0.0406
trigger times: 5
Loss after 12441213 batches: 0.0392
trigger times: 6
Loss after 12442142 batches: 0.0338
trigger times: 7
Loss after 12443071 batches: 0.0340
trigger times: 8
Loss after 12444000 batches: 0.0315
trigger times: 9
Loss after 12444929 batches: 0.0302
trigger times: 10
Loss after 12445858 batches: 0.0298
trigger times: 11
Loss after 12446787 batches: 0.0300
trigger times: 12
Loss after 12447716 batches: 0.0292
trigger times: 13
Loss after 12448645 batches: 0.0298
trigger times: 14
Loss after 12449574 batches: 0.0283
trigger times: 15
Loss after 12450503 batches: 0.0278
trigger times: 16
Loss after 12451432 batches: 0.0297
trigger times: 17
Loss after 12452361 batches: 0.0269
trigger times: 18
Loss after 12453290 batches: 0.0273
trigger times: 19
Loss after 12454219 batches: 0.0284
trigger times: 20
Loss after 12455148 batches: 0.0257
trigger times: 21
Loss after 12456077 batches: 0.0264
trigger times: 22
Loss after 12457006 batches: 0.0371
trigger times: 23
Loss after 12457935 batches: 0.0388
trigger times: 24
Loss after 12458864 batches: 0.0342
trigger times: 25
Early stopping!
Start to test process.
Loss after 12459793 batches: 0.0351
Time to train on one home:  52.10726571083069
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12460756 batches: 0.0596
trigger times: 1
Loss after 12461719 batches: 0.0299
trigger times: 2
Loss after 12462682 batches: 0.0273
trigger times: 3
Loss after 12463645 batches: 0.0274
trigger times: 4
Loss after 12464608 batches: 0.0262
trigger times: 5
Loss after 12465571 batches: 0.0248
trigger times: 6
Loss after 12466534 batches: 0.0245
trigger times: 7
Loss after 12467497 batches: 0.0233
trigger times: 8
Loss after 12468460 batches: 0.0230
trigger times: 9
Loss after 12469423 batches: 0.0219
trigger times: 10
Loss after 12470386 batches: 0.0215
trigger times: 11
Loss after 12471349 batches: 0.0214
trigger times: 12
Loss after 12472312 batches: 0.0206
trigger times: 13
Loss after 12473275 batches: 0.0203
trigger times: 14
Loss after 12474238 batches: 0.0199
trigger times: 15
Loss after 12475201 batches: 0.0194
trigger times: 16
Loss after 12476164 batches: 0.0191
trigger times: 17
Loss after 12477127 batches: 0.0188
trigger times: 18
Loss after 12478090 batches: 0.0187
trigger times: 19
Loss after 12479053 batches: 0.0185
trigger times: 20
Loss after 12480016 batches: 0.0184
trigger times: 21
Loss after 12480979 batches: 0.0183
trigger times: 22
Loss after 12481942 batches: 0.0179
trigger times: 23
Loss after 12482905 batches: 0.0180
trigger times: 24
Loss after 12483868 batches: 0.0180
trigger times: 25
Early stopping!
Start to test process.
Loss after 12484831 batches: 0.0179
Time to train on one home:  52.184239625930786
trigger times: 0
Loss after 12485794 batches: 0.1634
trigger times: 1
Loss after 12486757 batches: 0.1168
trigger times: 2
Loss after 12487720 batches: 0.1000
trigger times: 3
Loss after 12488683 batches: 0.0847
trigger times: 4
Loss after 12489646 batches: 0.0785
trigger times: 5
Loss after 12490609 batches: 0.0755
trigger times: 6
Loss after 12491572 batches: 0.0727
trigger times: 7
Loss after 12492535 batches: 0.0703
trigger times: 8
Loss after 12493498 batches: 0.0640
trigger times: 9
Loss after 12494461 batches: 0.0625
trigger times: 10
Loss after 12495424 batches: 0.0574
trigger times: 11
Loss after 12496387 batches: 0.0547
trigger times: 12
Loss after 12497350 batches: 0.0537
trigger times: 13
Loss after 12498313 batches: 0.0510
trigger times: 14
Loss after 12499276 batches: 0.0504
trigger times: 15
Loss after 12500239 batches: 0.0513
trigger times: 16
Loss after 12501202 batches: 0.0518
trigger times: 17
Loss after 12502165 batches: 0.0500
trigger times: 18
Loss after 12503128 batches: 0.0469
trigger times: 19
Loss after 12504091 batches: 0.0471
trigger times: 20
Loss after 12505054 batches: 0.0463
trigger times: 21
Loss after 12506017 batches: 0.0450
trigger times: 22
Loss after 12506980 batches: 0.0447
trigger times: 23
Loss after 12507943 batches: 0.0452
trigger times: 24
Loss after 12508906 batches: 0.0425
trigger times: 25
Early stopping!
Start to test process.
Loss after 12509869 batches: 0.0428
Time to train on one home:  51.93930244445801
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12510832 batches: 0.0800
trigger times: 1
Loss after 12511795 batches: 0.0716
trigger times: 2
Loss after 12512758 batches: 0.0680
trigger times: 3
Loss after 12513721 batches: 0.0654
trigger times: 4
Loss after 12514684 batches: 0.0642
trigger times: 5
Loss after 12515647 batches: 0.0633
trigger times: 6
Loss after 12516610 batches: 0.0609
trigger times: 7
Loss after 12517573 batches: 0.0604
trigger times: 8
Loss after 12518536 batches: 0.0594
trigger times: 9
Loss after 12519499 batches: 0.0600
trigger times: 10
Loss after 12520462 batches: 0.0598
trigger times: 11
Loss after 12521425 batches: 0.0586
trigger times: 12
Loss after 12522388 batches: 0.0595
trigger times: 13
Loss after 12523351 batches: 0.0570
trigger times: 14
Loss after 12524314 batches: 0.0575
trigger times: 15
Loss after 12525277 batches: 0.0564
trigger times: 16
Loss after 12526240 batches: 0.0564
trigger times: 17
Loss after 12527203 batches: 0.0565
trigger times: 18
Loss after 12528166 batches: 0.0571
trigger times: 19
Loss after 12529129 batches: 0.0557
trigger times: 20
Loss after 12530092 batches: 0.0556
trigger times: 21
Loss after 12531055 batches: 0.0547
trigger times: 22
Loss after 12532018 batches: 0.0551
trigger times: 23
Loss after 12532981 batches: 0.0547
trigger times: 24
Loss after 12533944 batches: 0.0550
trigger times: 25
Early stopping!
Start to test process.
Loss after 12534907 batches: 0.0545
Time to train on one home:  52.14926362037659
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12535870 batches: 0.0824
trigger times: 1
Loss after 12536833 batches: 0.0741
trigger times: 2
Loss after 12537796 batches: 0.0729
trigger times: 3
Loss after 12538759 batches: 0.0683
trigger times: 4
Loss after 12539722 batches: 0.0660
trigger times: 5
Loss after 12540685 batches: 0.0628
trigger times: 6
Loss after 12541648 batches: 0.0627
trigger times: 7
Loss after 12542611 batches: 0.0578
trigger times: 8
Loss after 12543574 batches: 0.0568
trigger times: 9
Loss after 12544537 batches: 0.0547
trigger times: 10
Loss after 12545500 batches: 0.0545
trigger times: 11
Loss after 12546463 batches: 0.0535
trigger times: 12
Loss after 12547426 batches: 0.0526
trigger times: 13
Loss after 12548389 batches: 0.0521
trigger times: 14
Loss after 12549352 batches: 0.0525
trigger times: 15
Loss after 12550315 batches: 0.0520
trigger times: 16
Loss after 12551278 batches: 0.0508
trigger times: 17
Loss after 12552241 batches: 0.0490
trigger times: 18
Loss after 12553204 batches: 0.0478
trigger times: 19
Loss after 12554167 batches: 0.0482
trigger times: 20
Loss after 12555130 batches: 0.0504
trigger times: 21
Loss after 12556093 batches: 0.0504
trigger times: 22
Loss after 12557056 batches: 0.0459
trigger times: 23
Loss after 12558019 batches: 0.0481
trigger times: 24
Loss after 12558982 batches: 0.0502
trigger times: 25
Early stopping!
Start to test process.
Loss after 12559945 batches: 0.0490
Time to train on one home:  52.305177211761475
trigger times: 0
Loss after 12560908 batches: 0.0223
trigger times: 1
Loss after 12561871 batches: 0.0195
trigger times: 2
Loss after 12562834 batches: 0.0182
trigger times: 3
Loss after 12563797 batches: 0.0168
trigger times: 0
Loss after 12564760 batches: 0.0157
trigger times: 1
Loss after 12565723 batches: 0.0150
trigger times: 2
Loss after 12566686 batches: 0.0148
trigger times: 3
Loss after 12567649 batches: 0.0147
trigger times: 4
Loss after 12568612 batches: 0.0148
trigger times: 5
Loss after 12569575 batches: 0.0142
trigger times: 6
Loss after 12570538 batches: 0.0142
trigger times: 7
Loss after 12571501 batches: 0.0136
trigger times: 8
Loss after 12572464 batches: 0.0138
trigger times: 9
Loss after 12573427 batches: 0.0128
trigger times: 10
Loss after 12574390 batches: 0.0130
trigger times: 11
Loss after 12575353 batches: 0.0129
trigger times: 12
Loss after 12576316 batches: 0.0127
trigger times: 13
Loss after 12577279 batches: 0.0130
trigger times: 14
Loss after 12578242 batches: 0.0123
trigger times: 15
Loss after 12579205 batches: 0.0125
trigger times: 16
Loss after 12580168 batches: 0.0118
trigger times: 17
Loss after 12581131 batches: 0.0124
trigger times: 18
Loss after 12582094 batches: 0.0125
trigger times: 19
Loss after 12583057 batches: 0.0122
trigger times: 20
Loss after 12584020 batches: 0.0126
trigger times: 21
Loss after 12584983 batches: 0.0115
trigger times: 22
Loss after 12585946 batches: 0.0119
trigger times: 23
Loss after 12586909 batches: 0.0116
trigger times: 24
Loss after 12587872 batches: 0.0113
trigger times: 25
Early stopping!
Start to test process.
Loss after 12588835 batches: 0.0111
Time to train on one home:  55.46621870994568
trigger times: 0
Loss after 12589798 batches: 0.0435
trigger times: 1
Loss after 12590761 batches: 0.0359
trigger times: 2
Loss after 12591724 batches: 0.0331
trigger times: 3
Loss after 12592687 batches: 0.0304
trigger times: 4
Loss after 12593650 batches: 0.0281
trigger times: 5
Loss after 12594613 batches: 0.0275
trigger times: 6
Loss after 12595576 batches: 0.0253
trigger times: 7
Loss after 12596539 batches: 0.0244
trigger times: 8
Loss after 12597502 batches: 0.0233
trigger times: 9
Loss after 12598465 batches: 0.0235
trigger times: 10
Loss after 12599428 batches: 0.0227
trigger times: 11
Loss after 12600391 batches: 0.0220
trigger times: 12
Loss after 12601354 batches: 0.0226
trigger times: 13
Loss after 12602317 batches: 0.0218
trigger times: 14
Loss after 12603280 batches: 0.0213
trigger times: 15
Loss after 12604243 batches: 0.0209
trigger times: 16
Loss after 12605206 batches: 0.0210
trigger times: 17
Loss after 12606169 batches: 0.0216
trigger times: 18
Loss after 12607132 batches: 0.0217
trigger times: 19
Loss after 12608095 batches: 0.0211
trigger times: 20
Loss after 12609058 batches: 0.0210
trigger times: 21
Loss after 12610021 batches: 0.0207
trigger times: 22
Loss after 12610984 batches: 0.0208
trigger times: 23
Loss after 12611947 batches: 0.0205
trigger times: 24
Loss after 12612910 batches: 0.0202
trigger times: 25
Early stopping!
Start to test process.
Loss after 12613873 batches: 0.0201
Time to train on one home:  52.231224060058594
trigger times: 0
Loss after 12614836 batches: 0.0904
trigger times: 1
Loss after 12615799 batches: 0.0653
trigger times: 2
Loss after 12616762 batches: 0.0577
trigger times: 3
Loss after 12617725 batches: 0.0521
trigger times: 4
Loss after 12618688 batches: 0.0496
trigger times: 5
Loss after 12619651 batches: 0.0465
trigger times: 6
Loss after 12620614 batches: 0.0452
trigger times: 7
Loss after 12621577 batches: 0.0453
trigger times: 8
Loss after 12622540 batches: 0.0443
trigger times: 9
Loss after 12623503 batches: 0.0428
trigger times: 10
Loss after 12624466 batches: 0.0410
trigger times: 11
Loss after 12625429 batches: 0.0411
trigger times: 12
Loss after 12626392 batches: 0.0400
trigger times: 13
Loss after 12627355 batches: 0.0394
trigger times: 14
Loss after 12628318 batches: 0.0388
trigger times: 15
Loss after 12629281 batches: 0.0388
trigger times: 16
Loss after 12630244 batches: 0.0382
trigger times: 17
Loss after 12631207 batches: 0.0381
trigger times: 18
Loss after 12632170 batches: 0.0371
trigger times: 19
Loss after 12633133 batches: 0.0376
trigger times: 20
Loss after 12634096 batches: 0.0385
trigger times: 21
Loss after 12635059 batches: 0.0375
trigger times: 22
Loss after 12636022 batches: 0.0373
trigger times: 23
Loss after 12636985 batches: 0.0376
trigger times: 24
Loss after 12637948 batches: 0.0358
trigger times: 25
Early stopping!
Start to test process.
Loss after 12638911 batches: 0.0359
Time to train on one home:  52.23219108581543
trigger times: 0
Loss after 12639874 batches: 0.1032
trigger times: 1
Loss after 12640837 batches: 0.0661
trigger times: 2
Loss after 12641800 batches: 0.0603
trigger times: 3
Loss after 12642763 batches: 0.0526
trigger times: 4
Loss after 12643726 batches: 0.0503
trigger times: 5
Loss after 12644689 batches: 0.0476
trigger times: 6
Loss after 12645652 batches: 0.0461
trigger times: 7
Loss after 12646615 batches: 0.0445
trigger times: 8
Loss after 12647578 batches: 0.0434
trigger times: 9
Loss after 12648541 batches: 0.0421
trigger times: 10
Loss after 12649504 batches: 0.0420
trigger times: 11
Loss after 12650467 batches: 0.0411
trigger times: 12
Loss after 12651430 batches: 0.0404
trigger times: 13
Loss after 12652393 batches: 0.0400
trigger times: 14
Loss after 12653356 batches: 0.0399
trigger times: 15
Loss after 12654319 batches: 0.0394
trigger times: 16
Loss after 12655282 batches: 0.0394
trigger times: 17
Loss after 12656245 batches: 0.0392
trigger times: 18
Loss after 12657208 batches: 0.0392
trigger times: 19
Loss after 12658171 batches: 0.0385
trigger times: 20
Loss after 12659134 batches: 0.0380
trigger times: 21
Loss after 12660097 batches: 0.0373
trigger times: 22
Loss after 12661060 batches: 0.0383
trigger times: 23
Loss after 12662023 batches: 0.0369
trigger times: 24
Loss after 12662986 batches: 0.0371
trigger times: 25
Early stopping!
Start to test process.
Loss after 12663949 batches: 0.0374
Time to train on one home:  52.3681583404541
trigger times: 0
Loss after 12664912 batches: 0.0565
trigger times: 1
Loss after 12665875 batches: 0.0439
trigger times: 2
Loss after 12666838 batches: 0.0403
trigger times: 3
Loss after 12667801 batches: 0.0368
trigger times: 4
Loss after 12668764 batches: 0.0339
trigger times: 5
Loss after 12669727 batches: 0.0331
trigger times: 6
Loss after 12670690 batches: 0.0324
trigger times: 7
Loss after 12671653 batches: 0.0317
trigger times: 8
Loss after 12672616 batches: 0.0302
trigger times: 9
Loss after 12673579 batches: 0.0300
trigger times: 10
Loss after 12674542 batches: 0.0297
trigger times: 11
Loss after 12675505 batches: 0.0291
trigger times: 12
Loss after 12676468 batches: 0.0305
trigger times: 13
Loss after 12677431 batches: 0.0300
trigger times: 14
Loss after 12678394 batches: 0.0308
trigger times: 15
Loss after 12679357 batches: 0.0290
trigger times: 16
Loss after 12680320 batches: 0.0296
trigger times: 17
Loss after 12681283 batches: 0.0294
trigger times: 18
Loss after 12682246 batches: 0.0286
trigger times: 19
Loss after 12683209 batches: 0.0294
trigger times: 20
Loss after 12684172 batches: 0.0284
trigger times: 21
Loss after 12685135 batches: 0.0267
trigger times: 22
Loss after 12686098 batches: 0.0269
trigger times: 23
Loss after 12687061 batches: 0.0280
trigger times: 24
Loss after 12688024 batches: 0.0268
trigger times: 25
Early stopping!
Start to test process.
Loss after 12688987 batches: 0.0266
Time to train on one home:  52.23318409919739
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12689950 batches: 0.0901
trigger times: 1
Loss after 12690913 batches: 0.0534
trigger times: 2
Loss after 12691876 batches: 0.0542
trigger times: 0
Loss after 12692839 batches: 0.0440
trigger times: 0
Loss after 12693802 batches: 0.0413
trigger times: 1
Loss after 12694765 batches: 0.0380
trigger times: 2
Loss after 12695728 batches: 0.0354
trigger times: 3
Loss after 12696691 batches: 0.0343
trigger times: 0
Loss after 12697654 batches: 0.0336
trigger times: 1
Loss after 12698617 batches: 0.0318
trigger times: 2
Loss after 12699580 batches: 0.0304
trigger times: 3
Loss after 12700543 batches: 0.0299
trigger times: 4
Loss after 12701506 batches: 0.0291
trigger times: 5
Loss after 12702469 batches: 0.0290
trigger times: 6
Loss after 12703432 batches: 0.0277
trigger times: 7
Loss after 12704395 batches: 0.0288
trigger times: 8
Loss after 12705358 batches: 0.0271
trigger times: 9
Loss after 12706321 batches: 0.0268
trigger times: 10
Loss after 12707284 batches: 0.0272
trigger times: 11
Loss after 12708247 batches: 0.0271
trigger times: 12
Loss after 12709210 batches: 0.0264
trigger times: 13
Loss after 12710173 batches: 0.0274
trigger times: 14
Loss after 12711136 batches: 0.0267
trigger times: 15
Loss after 12712099 batches: 0.0260
trigger times: 16
Loss after 12713062 batches: 0.0262
trigger times: 17
Loss after 12714025 batches: 0.0251
trigger times: 18
Loss after 12714988 batches: 0.0249
trigger times: 0
Loss after 12715951 batches: 0.0244
trigger times: 1
Loss after 12716914 batches: 0.0250
trigger times: 2
Loss after 12717877 batches: 0.0244
trigger times: 3
Loss after 12718840 batches: 0.0229
trigger times: 4
Loss after 12719803 batches: 0.0241
trigger times: 5
Loss after 12720766 batches: 0.0236
trigger times: 6
Loss after 12721729 batches: 0.0254
trigger times: 7
Loss after 12722692 batches: 0.0242
trigger times: 8
Loss after 12723655 batches: 0.0238
trigger times: 9
Loss after 12724618 batches: 0.0233
trigger times: 10
Loss after 12725581 batches: 0.0238
trigger times: 11
Loss after 12726544 batches: 0.0234
trigger times: 12
Loss after 12727507 batches: 0.0234
trigger times: 13
Loss after 12728470 batches: 0.0224
trigger times: 14
Loss after 12729433 batches: 0.0231
trigger times: 15
Loss after 12730396 batches: 0.0234
trigger times: 16
Loss after 12731359 batches: 0.0231
trigger times: 17
Loss after 12732322 batches: 0.0228
trigger times: 18
Loss after 12733285 batches: 0.0222
trigger times: 19
Loss after 12734248 batches: 0.0214
trigger times: 20
Loss after 12735211 batches: 0.0222
trigger times: 21
Loss after 12736174 batches: 0.0212
trigger times: 22
Loss after 12737137 batches: 0.0235
trigger times: 23
Loss after 12738100 batches: 0.0233
trigger times: 24
Loss after 12739063 batches: 0.0214
trigger times: 25
Early stopping!
Start to test process.
Loss after 12740026 batches: 0.0213
Time to train on one home:  72.89794492721558
trigger times: 0
Loss after 12740989 batches: 0.0935
trigger times: 1
Loss after 12741952 batches: 0.0778
trigger times: 2
Loss after 12742915 batches: 0.0758
trigger times: 3
Loss after 12743878 batches: 0.0749
trigger times: 4
Loss after 12744841 batches: 0.0715
trigger times: 5
Loss after 12745804 batches: 0.0686
trigger times: 6
Loss after 12746767 batches: 0.0666
trigger times: 7
Loss after 12747730 batches: 0.0652
trigger times: 8
Loss after 12748693 batches: 0.0645
trigger times: 9
Loss after 12749656 batches: 0.0624
trigger times: 10
Loss after 12750619 batches: 0.0625
trigger times: 11
Loss after 12751582 batches: 0.0611
trigger times: 12
Loss after 12752545 batches: 0.0630
trigger times: 13
Loss after 12753508 batches: 0.0607
trigger times: 14
Loss after 12754471 batches: 0.0626
trigger times: 15
Loss after 12755434 batches: 0.0613
trigger times: 16
Loss after 12756397 batches: 0.0624
trigger times: 17
Loss after 12757360 batches: 0.0615
trigger times: 18
Loss after 12758323 batches: 0.0605
trigger times: 19
Loss after 12759286 batches: 0.0587
trigger times: 20
Loss after 12760249 batches: 0.0587
trigger times: 21
Loss after 12761212 batches: 0.0584
trigger times: 22
Loss after 12762175 batches: 0.0576
trigger times: 23
Loss after 12763138 batches: 0.0584
trigger times: 24
Loss after 12764101 batches: 0.0560
trigger times: 25
Early stopping!
Start to test process.
Loss after 12765064 batches: 0.0568
Time to train on one home:  52.31982111930847
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12766027 batches: 0.0891
trigger times: 1
Loss after 12766990 batches: 0.0538
trigger times: 2
Loss after 12767953 batches: 0.0544
trigger times: 0
Loss after 12768916 batches: 0.0460
trigger times: 0
Loss after 12769879 batches: 0.0419
trigger times: 0
Loss after 12770842 batches: 0.0379
trigger times: 1
Loss after 12771805 batches: 0.0359
trigger times: 2
Loss after 12772768 batches: 0.0336
trigger times: 3
Loss after 12773731 batches: 0.0316
trigger times: 4
Loss after 12774694 batches: 0.0311
trigger times: 5
Loss after 12775657 batches: 0.0305
trigger times: 6
Loss after 12776620 batches: 0.0302
trigger times: 7
Loss after 12777583 batches: 0.0288
trigger times: 8
Loss after 12778546 batches: 0.0326
trigger times: 9
Loss after 12779509 batches: 0.0312
trigger times: 10
Loss after 12780472 batches: 0.0299
trigger times: 11
Loss after 12781435 batches: 0.0296
trigger times: 12
Loss after 12782398 batches: 0.0277
trigger times: 13
Loss after 12783361 batches: 0.0268
trigger times: 14
Loss after 12784324 batches: 0.0263
trigger times: 15
Loss after 12785287 batches: 0.0265
trigger times: 16
Loss after 12786250 batches: 0.0263
trigger times: 17
Loss after 12787213 batches: 0.0256
trigger times: 18
Loss after 12788176 batches: 0.0253
trigger times: 19
Loss after 12789139 batches: 0.0252
trigger times: 20
Loss after 12790102 batches: 0.0245
trigger times: 21
Loss after 12791065 batches: 0.0254
trigger times: 22
Loss after 12792028 batches: 0.0252
trigger times: 23
Loss after 12792991 batches: 0.0243
trigger times: 24
Loss after 12793954 batches: 0.0244
trigger times: 25
Early stopping!
Start to test process.
Loss after 12794917 batches: 0.0246
Time to train on one home:  56.05988264083862
trigger times: 0
Loss after 12795812 batches: 0.0735
trigger times: 1
Loss after 12796707 batches: 0.0397
trigger times: 2
Loss after 12797602 batches: 0.0193
trigger times: 3
Loss after 12798497 batches: 0.0106
trigger times: 4
Loss after 12799392 batches: 0.0094
trigger times: 5
Loss after 12800287 batches: 0.0072
trigger times: 6
Loss after 12801182 batches: 0.0062
trigger times: 7
Loss after 12802077 batches: 0.0051
trigger times: 8
Loss after 12802972 batches: 0.0049
trigger times: 9
Loss after 12803867 batches: 0.0046
trigger times: 10
Loss after 12804762 batches: 0.0041
trigger times: 11
Loss after 12805657 batches: 0.0035
trigger times: 12
Loss after 12806552 batches: 0.0035
trigger times: 13
Loss after 12807447 batches: 0.0038
trigger times: 14
Loss after 12808342 batches: 0.0032
trigger times: 15
Loss after 12809237 batches: 0.0032
trigger times: 16
Loss after 12810132 batches: 0.0028
trigger times: 17
Loss after 12811027 batches: 0.0028
trigger times: 18
Loss after 12811922 batches: 0.0038
trigger times: 19
Loss after 12812817 batches: 0.0036
trigger times: 20
Loss after 12813712 batches: 0.0040
trigger times: 21
Loss after 12814607 batches: 0.0031
trigger times: 22
Loss after 12815502 batches: 0.0026
trigger times: 23
Loss after 12816397 batches: 0.0026
trigger times: 24
Loss after 12817292 batches: 0.0027
trigger times: 25
Early stopping!
Start to test process.
Loss after 12818187 batches: 0.0027
Time to train on one home:  51.54500985145569
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 12819124 batches: 0.0800
trigger times: 1
Loss after 12820061 batches: 0.0686
trigger times: 2
Loss after 12820998 batches: 0.0660
trigger times: 3
Loss after 12821935 batches: 0.0636
trigger times: 4
Loss after 12822872 batches: 0.0604
trigger times: 5
Loss after 12823809 batches: 0.0582
trigger times: 6
Loss after 12824746 batches: 0.0562
trigger times: 7
Loss after 12825683 batches: 0.0564
trigger times: 8
Loss after 12826620 batches: 0.0532
trigger times: 9
Loss after 12827557 batches: 0.0526
trigger times: 10
Loss after 12828494 batches: 0.0531
trigger times: 11
Loss after 12829431 batches: 0.0531
trigger times: 12
Loss after 12830368 batches: 0.0521
trigger times: 13
Loss after 12831305 batches: 0.0505
trigger times: 14
Loss after 12832242 batches: 0.0507
trigger times: 15
Loss after 12833179 batches: 0.0505
trigger times: 16
Loss after 12834116 batches: 0.0501
trigger times: 17
Loss after 12835053 batches: 0.0499
trigger times: 18
Loss after 12835990 batches: 0.0498
trigger times: 19
Loss after 12836927 batches: 0.0489
trigger times: 20
Loss after 12837864 batches: 0.0494
trigger times: 21
Loss after 12838801 batches: 0.0484
trigger times: 22
Loss after 12839738 batches: 0.0477
trigger times: 23
Loss after 12840675 batches: 0.0472
trigger times: 24
Loss after 12841612 batches: 0.0474
trigger times: 25
Early stopping!
Start to test process.
Loss after 12842549 batches: 0.0456
Time to train on one home:  52.53259587287903
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066, 0.03401813118363776, 0.033508403208926535, 0.03273922320832277]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863], [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861], [0.0757279247045517, 0.10266384027132536, 0.5909740425258612, 0.8803234656976516, 0.8076583271010261, 28.206427011804568, 8296.775], [0.06181953847408295, 0.10445801892372542, 0.5961459732150404, 0.831618823847048, 0.806043429565136, 26.645882531252276, 8280.187]]
Round_10_results:  [0.06181953847408295, 0.10445801892372542, 0.5961459732150404, 0.831618823847048, 0.806043429565136, 26.645882531252276, 8280.187]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 13396 < 13397; dropping {'Training_Loss': 0.07671595471245903, 'Validation_Loss': 0.07843837141990662, 'Training_R2': -0.14557288703699434, 'Validation_R2': 0.11163906519727929, 'Training_F1': 0.39098980704124414, 'Validation_F1': 0.6000794046912323, 'Training_NEP': 0.8385355052418042, 'Validation_NEP': 0.8120392070232114, 'Training_NDE': 0.6976816657565771, 'Validation_NDE': 0.8026129697035866, 'Training_MAE': 31.864618989501967, 'Validation_MAE': 28.875524441515186, 'Training_MSE': 2576.808, 'Validation_MSE': 10514.24}.
trigger times: 0
Loss after 12843511 batches: 0.0767
trigger times: 1
Loss after 12844473 batches: 0.0648
trigger times: 2
Loss after 12845435 batches: 0.0633
trigger times: 3
Loss after 12846397 batches: 0.0593
trigger times: 4
Loss after 12847359 batches: 0.0574
trigger times: 5
Loss after 12848321 batches: 0.0559
trigger times: 6
Loss after 12849283 batches: 0.0548
trigger times: 7
Loss after 12850245 batches: 0.0541
trigger times: 8
Loss after 12851207 batches: 0.0536
trigger times: 9
Loss after 12852169 batches: 0.0538
trigger times: 10
Loss after 12853131 batches: 0.0527
trigger times: 11
Loss after 12854093 batches: 0.0522
trigger times: 12
Loss after 12855055 batches: 0.0513
trigger times: 13
Loss after 12856017 batches: 0.0511
trigger times: 14
Loss after 12856979 batches: 0.0509
trigger times: 15
Loss after 12857941 batches: 0.0516
trigger times: 16
Loss after 12858903 batches: 0.0512
trigger times: 17
Loss after 12859865 batches: 0.0504
trigger times: 18
Loss after 12860827 batches: 0.0497
trigger times: 19
Loss after 12861789 batches: 0.0498
trigger times: 20
Loss after 12862751 batches: 0.0482
trigger times: 21
Loss after 12863713 batches: 0.0488
trigger times: 22
Loss after 12864675 batches: 0.0483
trigger times: 23
Loss after 12865637 batches: 0.0477
trigger times: 24
Loss after 12866599 batches: 0.0488
trigger times: 25
Early stopping!
Start to test process.
Loss after 12867561 batches: 0.0471
Time to train on one home:  52.33659267425537
trigger times: 0
Loss after 12868490 batches: 0.0881
trigger times: 1
Loss after 12869419 batches: 0.0623
trigger times: 2
Loss after 12870348 batches: 0.0488
trigger times: 3
Loss after 12871277 batches: 0.0418
trigger times: 4
Loss after 12872206 batches: 0.0357
trigger times: 5
Loss after 12873135 batches: 0.0361
trigger times: 6
Loss after 12874064 batches: 0.0342
trigger times: 7
Loss after 12874993 batches: 0.0332
trigger times: 8
Loss after 12875922 batches: 0.0307
trigger times: 9
Loss after 12876851 batches: 0.0313
trigger times: 10
Loss after 12877780 batches: 0.0287
trigger times: 11
Loss after 12878709 batches: 0.0283
trigger times: 12
Loss after 12879638 batches: 0.0303
trigger times: 13
Loss after 12880567 batches: 0.0293
trigger times: 14
Loss after 12881496 batches: 0.0285
trigger times: 15
Loss after 12882425 batches: 0.0259
trigger times: 16
Loss after 12883354 batches: 0.0262
trigger times: 17
Loss after 12884283 batches: 0.0254
trigger times: 18
Loss after 12885212 batches: 0.0251
trigger times: 19
Loss after 12886141 batches: 0.0252
trigger times: 20
Loss after 12887070 batches: 0.0259
trigger times: 21
Loss after 12887999 batches: 0.0239
trigger times: 22
Loss after 12888928 batches: 0.0272
trigger times: 23
Loss after 12889857 batches: 0.0307
trigger times: 24
Loss after 12890786 batches: 0.0315
trigger times: 25
Early stopping!
Start to test process.
Loss after 12891715 batches: 0.0319
Time to train on one home:  52.04957842826843
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 12892678 batches: 0.0562
trigger times: 1
Loss after 12893641 batches: 0.0197
trigger times: 2
Loss after 12894604 batches: 0.0148
trigger times: 3
Loss after 12895567 batches: 0.0141
trigger times: 4
Loss after 12896530 batches: 0.0139
trigger times: 5
Loss after 12897493 batches: 0.0137
trigger times: 6
Loss after 12898456 batches: 0.0137
trigger times: 7
Loss after 12899419 batches: 0.0135
trigger times: 8
Loss after 12900382 batches: 0.0131
trigger times: 9
Loss after 12901345 batches: 0.0126
trigger times: 10
Loss after 12902308 batches: 0.0120
trigger times: 11
Loss after 12903271 batches: 0.0115
trigger times: 12
Loss after 12904234 batches: 0.0111
trigger times: 13
Loss after 12905197 batches: 0.0102
trigger times: 14
Loss after 12906160 batches: 0.0100
trigger times: 15
Loss after 12907123 batches: 0.0097
trigger times: 16
Loss after 12908086 batches: 0.0095
trigger times: 17
Loss after 12909049 batches: 0.0093
trigger times: 18
Loss after 12910012 batches: 0.0090
trigger times: 19
Loss after 12910975 batches: 0.0088
trigger times: 20
Loss after 12911938 batches: 0.0087
trigger times: 21
Loss after 12912901 batches: 0.0083
trigger times: 22
Loss after 12913864 batches: 0.0082
trigger times: 23
Loss after 12914827 batches: 0.0079
trigger times: 24
Loss after 12915790 batches: 0.0079
trigger times: 25
Early stopping!
Start to test process.
Loss after 12916753 batches: 0.0076
Time to train on one home:  52.38441205024719
trigger times: 0
Loss after 12917716 batches: 0.0233
trigger times: 1
Loss after 12918679 batches: 0.0197
trigger times: 2
Loss after 12919642 batches: 0.0180
trigger times: 3
Loss after 12920605 batches: 0.0162
trigger times: 4
Loss after 12921568 batches: 0.0158
trigger times: 5
Loss after 12922531 batches: 0.0147
trigger times: 6
Loss after 12923494 batches: 0.0147
trigger times: 7
Loss after 12924457 batches: 0.0143
trigger times: 8
Loss after 12925420 batches: 0.0137
trigger times: 9
Loss after 12926383 batches: 0.0133
trigger times: 10
Loss after 12927346 batches: 0.0136
trigger times: 11
Loss after 12928309 batches: 0.0132
trigger times: 12
Loss after 12929272 batches: 0.0135
trigger times: 13
Loss after 12930235 batches: 0.0132
trigger times: 14
Loss after 12931198 batches: 0.0124
trigger times: 15
Loss after 12932161 batches: 0.0128
trigger times: 16
Loss after 12933124 batches: 0.0124
trigger times: 17
Loss after 12934087 batches: 0.0120
trigger times: 18
Loss after 12935050 batches: 0.0121
trigger times: 19
Loss after 12936013 batches: 0.0113
trigger times: 20
Loss after 12936976 batches: 0.0115
trigger times: 21
Loss after 12937939 batches: 0.0118
trigger times: 22
Loss after 12938902 batches: 0.0117
trigger times: 23
Loss after 12939865 batches: 0.0122
trigger times: 24
Loss after 12940828 batches: 0.0123
trigger times: 25
Early stopping!
Start to test process.
Loss after 12941791 batches: 0.0119
Time to train on one home:  51.94049525260925
trigger times: 0
Loss after 12942754 batches: 0.0928
trigger times: 1
Loss after 12943717 batches: 0.0867
trigger times: 2
Loss after 12944680 batches: 0.0806
trigger times: 3
Loss after 12945643 batches: 0.0787
trigger times: 4
Loss after 12946606 batches: 0.0749
trigger times: 5
Loss after 12947569 batches: 0.0731
trigger times: 6
Loss after 12948532 batches: 0.0690
trigger times: 7
Loss after 12949495 batches: 0.0686
trigger times: 8
Loss after 12950458 batches: 0.0694
trigger times: 9
Loss after 12951421 batches: 0.0672
trigger times: 10
Loss after 12952384 batches: 0.0638
trigger times: 11
Loss after 12953347 batches: 0.0637
trigger times: 12
Loss after 12954310 batches: 0.0630
trigger times: 13
Loss after 12955273 batches: 0.0622
trigger times: 14
Loss after 12956236 batches: 0.0626
trigger times: 15
Loss after 12957199 batches: 0.0628
trigger times: 16
Loss after 12958162 batches: 0.0649
trigger times: 17
Loss after 12959125 batches: 0.0627
trigger times: 18
Loss after 12960088 batches: 0.0612
trigger times: 19
Loss after 12961051 batches: 0.0615
trigger times: 20
Loss after 12962014 batches: 0.0619
trigger times: 21
Loss after 12962977 batches: 0.0606
trigger times: 22
Loss after 12963940 batches: 0.0594
trigger times: 23
Loss after 12964903 batches: 0.0583
trigger times: 24
Loss after 12965866 batches: 0.0584
trigger times: 25
Early stopping!
Start to test process.
Loss after 12966829 batches: 0.0580
Time to train on one home:  52.41230797767639
trigger times: 0
Loss after 12967792 batches: 0.0860
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 12968755 batches: 0.0721
trigger times: 2
Loss after 12969718 batches: 0.0692
trigger times: 3
Loss after 12970681 batches: 0.0667
trigger times: 4
Loss after 12971644 batches: 0.0642
trigger times: 5
Loss after 12972607 batches: 0.0632
trigger times: 6
Loss after 12973570 batches: 0.0606
trigger times: 7
Loss after 12974533 batches: 0.0605
trigger times: 8
Loss after 12975496 batches: 0.0589
trigger times: 9
Loss after 12976459 batches: 0.0589
trigger times: 10
Loss after 12977422 batches: 0.0576
trigger times: 11
Loss after 12978385 batches: 0.0582
trigger times: 12
Loss after 12979348 batches: 0.0565
trigger times: 13
Loss after 12980311 batches: 0.0554
trigger times: 14
Loss after 12981274 batches: 0.0543
trigger times: 15
Loss after 12982237 batches: 0.0547
trigger times: 16
Loss after 12983200 batches: 0.0554
trigger times: 17
Loss after 12984163 batches: 0.0555
trigger times: 18
Loss after 12985126 batches: 0.0554
trigger times: 19
Loss after 12986089 batches: 0.0540
trigger times: 20
Loss after 12987052 batches: 0.0547
trigger times: 21
Loss after 12988015 batches: 0.0535
trigger times: 22
Loss after 12988978 batches: 0.0527
trigger times: 23
Loss after 12989941 batches: 0.0527
trigger times: 24
Loss after 12990904 batches: 0.0524
trigger times: 25
Early stopping!
Start to test process.
Loss after 12991867 batches: 0.0518
Time to train on one home:  52.061389684677124
trigger times: 0
Loss after 12992830 batches: 0.0733
trigger times: 1
Loss after 12993793 batches: 0.0687
trigger times: 2
Loss after 12994756 batches: 0.0654
trigger times: 3
Loss after 12995719 batches: 0.0637
trigger times: 4
Loss after 12996682 batches: 0.0610
trigger times: 5
Loss after 12997645 batches: 0.0599
trigger times: 6
Loss after 12998608 batches: 0.0584
trigger times: 7
Loss after 12999571 batches: 0.0562
trigger times: 8
Loss after 13000534 batches: 0.0568
trigger times: 9
Loss after 13001497 batches: 0.0558
trigger times: 10
Loss after 13002460 batches: 0.0550
trigger times: 11
Loss after 13003423 batches: 0.0552
trigger times: 12
Loss after 13004386 batches: 0.0544
trigger times: 13
Loss after 13005349 batches: 0.0530
trigger times: 14
Loss after 13006312 batches: 0.0520
trigger times: 15
Loss after 13007275 batches: 0.0527
trigger times: 16
Loss after 13008238 batches: 0.0531
trigger times: 17
Loss after 13009201 batches: 0.0521
trigger times: 18
Loss after 13010164 batches: 0.0519
trigger times: 19
Loss after 13011127 batches: 0.0510
trigger times: 20
Loss after 13012090 batches: 0.0514
trigger times: 21
Loss after 13013053 batches: 0.0500
trigger times: 22
Loss after 13014016 batches: 0.0501
trigger times: 23
Loss after 13014979 batches: 0.0508
trigger times: 24
Loss after 13015942 batches: 0.0506
trigger times: 25
Early stopping!
Start to test process.
Loss after 13016905 batches: 0.0510
Time to train on one home:  52.171321392059326
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13017868 batches: 0.0642
trigger times: 1
Loss after 13018831 batches: 0.0501
trigger times: 2
Loss after 13019794 batches: 0.0486
trigger times: 3
Loss after 13020757 batches: 0.0438
trigger times: 4
Loss after 13021720 batches: 0.0383
trigger times: 5
Loss after 13022683 batches: 0.0347
trigger times: 6
Loss after 13023646 batches: 0.0324
trigger times: 7
Loss after 13024609 batches: 0.0305
trigger times: 8
Loss after 13025572 batches: 0.0296
trigger times: 9
Loss after 13026535 batches: 0.0285
trigger times: 10
Loss after 13027498 batches: 0.0281
trigger times: 11
Loss after 13028461 batches: 0.0270
trigger times: 12
Loss after 13029424 batches: 0.0270
trigger times: 13
Loss after 13030387 batches: 0.0261
trigger times: 14
Loss after 13031350 batches: 0.0254
trigger times: 15
Loss after 13032313 batches: 0.0257
trigger times: 16
Loss after 13033276 batches: 0.0258
trigger times: 17
Loss after 13034239 batches: 0.0251
trigger times: 18
Loss after 13035202 batches: 0.0249
trigger times: 19
Loss after 13036165 batches: 0.0249
trigger times: 20
Loss after 13037128 batches: 0.0243
trigger times: 21
Loss after 13038091 batches: 0.0242
trigger times: 22
Loss after 13039054 batches: 0.0249
trigger times: 23
Loss after 13040017 batches: 0.0238
trigger times: 24
Loss after 13040980 batches: 0.0237
trigger times: 25
Early stopping!
Start to test process.
Loss after 13041943 batches: 0.0235
Time to train on one home:  52.28027129173279
trigger times: 0
Loss after 13042901 batches: 0.0623
trigger times: 1
Loss after 13043859 batches: 0.0432
trigger times: 2
Loss after 13044817 batches: 0.0364
trigger times: 3
Loss after 13045775 batches: 0.0319
trigger times: 4
Loss after 13046733 batches: 0.0282
trigger times: 5
Loss after 13047691 batches: 0.0261
trigger times: 6
Loss after 13048649 batches: 0.0251
trigger times: 7
Loss after 13049607 batches: 0.0227
trigger times: 8
Loss after 13050565 batches: 0.0239
trigger times: 9
Loss after 13051523 batches: 0.0223
trigger times: 10
Loss after 13052481 batches: 0.0202
trigger times: 11
Loss after 13053439 batches: 0.0205
trigger times: 12
Loss after 13054397 batches: 0.0195
trigger times: 13
Loss after 13055355 batches: 0.0192
trigger times: 14
Loss after 13056313 batches: 0.0183
trigger times: 15
Loss after 13057271 batches: 0.0179
trigger times: 16
Loss after 13058229 batches: 0.0176
trigger times: 17
Loss after 13059187 batches: 0.0165
trigger times: 18
Loss after 13060145 batches: 0.0178
trigger times: 19
Loss after 13061103 batches: 0.0185
trigger times: 20
Loss after 13062061 batches: 0.0187
trigger times: 21
Loss after 13063019 batches: 0.0176
trigger times: 22
Loss after 13063977 batches: 0.0174
trigger times: 23
Loss after 13064935 batches: 0.0168
trigger times: 24
Loss after 13065893 batches: 0.0154
trigger times: 25
Early stopping!
Start to test process.
Loss after 13066851 batches: 0.0164
Time to train on one home:  52.352251052856445
trigger times: 0
Loss after 13067813 batches: 0.0773
trigger times: 1
Loss after 13068775 batches: 0.0652
trigger times: 2
Loss after 13069737 batches: 0.0635
trigger times: 3
Loss after 13070699 batches: 0.0600
trigger times: 4
Loss after 13071661 batches: 0.0577
trigger times: 5
Loss after 13072623 batches: 0.0566
trigger times: 6
Loss after 13073585 batches: 0.0541
trigger times: 7
Loss after 13074547 batches: 0.0535
trigger times: 8
Loss after 13075509 batches: 0.0536
trigger times: 9
Loss after 13076471 batches: 0.0525
trigger times: 10
Loss after 13077433 batches: 0.0529
trigger times: 11
Loss after 13078395 batches: 0.0521
trigger times: 12
Loss after 13079357 batches: 0.0518
trigger times: 13
Loss after 13080319 batches: 0.0530
trigger times: 14
Loss after 13081281 batches: 0.0524
trigger times: 15
Loss after 13082243 batches: 0.0522
trigger times: 16
Loss after 13083205 batches: 0.0521
trigger times: 17
Loss after 13084167 batches: 0.0513
trigger times: 18
Loss after 13085129 batches: 0.0501
trigger times: 19
Loss after 13086091 batches: 0.0503
trigger times: 20
Loss after 13087053 batches: 0.0496
trigger times: 21
Loss after 13088015 batches: 0.0488
trigger times: 22
Loss after 13088977 batches: 0.0484
trigger times: 23
Loss after 13089939 batches: 0.0485
trigger times: 24
Loss after 13090901 batches: 0.0477
trigger times: 25
Early stopping!
Start to test process.
Loss after 13091863 batches: 0.0486
Time to train on one home:  52.32122468948364
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13092826 batches: 0.1010
trigger times: 1
Loss after 13093789 batches: 0.0279
trigger times: 2
Loss after 13094752 batches: 0.0212
trigger times: 3
Loss after 13095715 batches: 0.0168
trigger times: 4
Loss after 13096678 batches: 0.0160
trigger times: 5
Loss after 13097641 batches: 0.0155
trigger times: 6
Loss after 13098604 batches: 0.0143
trigger times: 7
Loss after 13099567 batches: 0.0139
trigger times: 8
Loss after 13100530 batches: 0.0137
trigger times: 9
Loss after 13101493 batches: 0.0136
trigger times: 10
Loss after 13102456 batches: 0.0134
trigger times: 11
Loss after 13103419 batches: 0.0132
trigger times: 12
Loss after 13104382 batches: 0.0131
trigger times: 13
Loss after 13105345 batches: 0.0129
trigger times: 14
Loss after 13106308 batches: 0.0129
trigger times: 15
Loss after 13107271 batches: 0.0129
trigger times: 16
Loss after 13108234 batches: 0.0127
trigger times: 17
Loss after 13109197 batches: 0.0127
trigger times: 18
Loss after 13110160 batches: 0.0126
trigger times: 19
Loss after 13111123 batches: 0.0125
trigger times: 20
Loss after 13112086 batches: 0.0126
trigger times: 21
Loss after 13113049 batches: 0.0126
trigger times: 22
Loss after 13114012 batches: 0.0129
trigger times: 23
Loss after 13114975 batches: 0.0123
trigger times: 24
Loss after 13115938 batches: 0.0124
trigger times: 25
Early stopping!
Start to test process.
Loss after 13116901 batches: 0.0121
Time to train on one home:  52.72208905220032
trigger times: 0
Loss after 13117864 batches: 0.0492
trigger times: 1
Loss after 13118827 batches: 0.0405
trigger times: 2
Loss after 13119790 batches: 0.0376
trigger times: 3
Loss after 13120753 batches: 0.0333
trigger times: 4
Loss after 13121716 batches: 0.0317
trigger times: 5
Loss after 13122679 batches: 0.0308
trigger times: 6
Loss after 13123642 batches: 0.0312
trigger times: 7
Loss after 13124605 batches: 0.0307
trigger times: 8
Loss after 13125568 batches: 0.0294
trigger times: 9
Loss after 13126531 batches: 0.0289
trigger times: 10
Loss after 13127494 batches: 0.0272
trigger times: 11
Loss after 13128457 batches: 0.0281
trigger times: 12
Loss after 13129420 batches: 0.0281
trigger times: 13
Loss after 13130383 batches: 0.0281
trigger times: 14
Loss after 13131346 batches: 0.0283
trigger times: 15
Loss after 13132309 batches: 0.0282
trigger times: 16
Loss after 13133272 batches: 0.0270
trigger times: 17
Loss after 13134235 batches: 0.0265
trigger times: 18
Loss after 13135198 batches: 0.0269
trigger times: 19
Loss after 13136161 batches: 0.0283
trigger times: 20
Loss after 13137124 batches: 0.0262
trigger times: 21
Loss after 13138087 batches: 0.0282
trigger times: 22
Loss after 13139050 batches: 0.0274
trigger times: 23
Loss after 13140013 batches: 0.0261
trigger times: 24
Loss after 13140976 batches: 0.0249
trigger times: 25
Early stopping!
Start to test process.
Loss after 13141939 batches: 0.0259
Time to train on one home:  52.40120315551758
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13142902 batches: 0.0640
trigger times: 1
Loss after 13143865 batches: 0.0500
trigger times: 2
Loss after 13144828 batches: 0.0484
trigger times: 3
Loss after 13145791 batches: 0.0435
trigger times: 4
Loss after 13146754 batches: 0.0371
trigger times: 5
Loss after 13147717 batches: 0.0351
trigger times: 6
Loss after 13148680 batches: 0.0329
trigger times: 7
Loss after 13149643 batches: 0.0312
trigger times: 8
Loss after 13150606 batches: 0.0299
trigger times: 9
Loss after 13151569 batches: 0.0286
trigger times: 10
Loss after 13152532 batches: 0.0281
trigger times: 11
Loss after 13153495 batches: 0.0278
trigger times: 12
Loss after 13154458 batches: 0.0275
trigger times: 13
Loss after 13155421 batches: 0.0265
trigger times: 14
Loss after 13156384 batches: 0.0256
trigger times: 15
Loss after 13157347 batches: 0.0255
trigger times: 16
Loss after 13158310 batches: 0.0250
trigger times: 17
Loss after 13159273 batches: 0.0251
trigger times: 18
Loss after 13160236 batches: 0.0247
trigger times: 19
Loss after 13161199 batches: 0.0251
trigger times: 20
Loss after 13162162 batches: 0.0245
trigger times: 21
Loss after 13163125 batches: 0.0236
trigger times: 22
Loss after 13164088 batches: 0.0246
trigger times: 23
Loss after 13165051 batches: 0.0241
trigger times: 24
Loss after 13166014 batches: 0.0229
trigger times: 25
Early stopping!
Start to test process.
Loss after 13166977 batches: 0.0229
Time to train on one home:  52.3031952381134
trigger times: 0
Loss after 13167940 batches: 0.0495
trigger times: 1
Loss after 13168903 batches: 0.0407
trigger times: 2
Loss after 13169866 batches: 0.0362
trigger times: 3
Loss after 13170829 batches: 0.0339
trigger times: 4
Loss after 13171792 batches: 0.0324
trigger times: 5
Loss after 13172755 batches: 0.0305
trigger times: 6
Loss after 13173718 batches: 0.0303
trigger times: 7
Loss after 13174681 batches: 0.0294
trigger times: 8
Loss after 13175644 batches: 0.0294
trigger times: 9
Loss after 13176607 batches: 0.0313
trigger times: 10
Loss after 13177570 batches: 0.0302
trigger times: 11
Loss after 13178533 batches: 0.0300
trigger times: 12
Loss after 13179496 batches: 0.0297
trigger times: 13
Loss after 13180459 batches: 0.0282
trigger times: 14
Loss after 13181422 batches: 0.0287
trigger times: 15
Loss after 13182385 batches: 0.0299
trigger times: 16
Loss after 13183348 batches: 0.0288
trigger times: 17
Loss after 13184311 batches: 0.0274
trigger times: 18
Loss after 13185274 batches: 0.0293
trigger times: 19
Loss after 13186237 batches: 0.0296
trigger times: 20
Loss after 13187200 batches: 0.0293
trigger times: 21
Loss after 13188163 batches: 0.0293
trigger times: 22
Loss after 13189126 batches: 0.0271
trigger times: 23
Loss after 13190089 batches: 0.0274
trigger times: 24
Loss after 13191052 batches: 0.0293
trigger times: 25
Early stopping!
Start to test process.
Loss after 13192015 batches: 0.0281
Time to train on one home:  52.2932071685791
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13192978 batches: 0.0477
trigger times: 1
Loss after 13193941 batches: 0.0437
trigger times: 2
Loss after 13194904 batches: 0.0401
trigger times: 3
Loss after 13195867 batches: 0.0391
trigger times: 4
Loss after 13196830 batches: 0.0373
trigger times: 5
Loss after 13197793 batches: 0.0359
trigger times: 6
Loss after 13198756 batches: 0.0348
trigger times: 7
Loss after 13199719 batches: 0.0336
trigger times: 8
Loss after 13200682 batches: 0.0330
trigger times: 9
Loss after 13201645 batches: 0.0333
trigger times: 10
Loss after 13202608 batches: 0.0321
trigger times: 11
Loss after 13203571 batches: 0.0313
trigger times: 0
Loss after 13204534 batches: 0.0303
trigger times: 1
Loss after 13205497 batches: 0.0322
trigger times: 2
Loss after 13206460 batches: 0.0308
trigger times: 3
Loss after 13207423 batches: 0.0303
trigger times: 4
Loss after 13208386 batches: 0.0312
trigger times: 5
Loss after 13209349 batches: 0.0307
trigger times: 6
Loss after 13210312 batches: 0.0309
trigger times: 7
Loss after 13211275 batches: 0.0294
trigger times: 8
Loss after 13212238 batches: 0.0292
trigger times: 9
Loss after 13213201 batches: 0.0283
trigger times: 10
Loss after 13214164 batches: 0.0272
trigger times: 11
Loss after 13215127 batches: 0.0283
trigger times: 0
Loss after 13216090 batches: 0.0284
trigger times: 1
Loss after 13217053 batches: 0.0288
trigger times: 2
Loss after 13218016 batches: 0.0283
trigger times: 3
Loss after 13218979 batches: 0.0274
trigger times: 4
Loss after 13219942 batches: 0.0276
trigger times: 5
Loss after 13220905 batches: 0.0263
trigger times: 6
Loss after 13221868 batches: 0.0259
trigger times: 7
Loss after 13222831 batches: 0.0273
trigger times: 8
Loss after 13223794 batches: 0.0266
trigger times: 9
Loss after 13224757 batches: 0.0260
trigger times: 0
Loss after 13225720 batches: 0.0255
trigger times: 1
Loss after 13226683 batches: 0.0260
trigger times: 2
Loss after 13227646 batches: 0.0263
trigger times: 3
Loss after 13228609 batches: 0.0268
trigger times: 4
Loss after 13229572 batches: 0.0263
trigger times: 5
Loss after 13230535 batches: 0.0258
trigger times: 6
Loss after 13231498 batches: 0.0252
trigger times: 7
Loss after 13232461 batches: 0.0250
trigger times: 8
Loss after 13233424 batches: 0.0236
trigger times: 9
Loss after 13234387 batches: 0.0251
trigger times: 10
Loss after 13235350 batches: 0.0280
trigger times: 11
Loss after 13236313 batches: 0.0264
trigger times: 12
Loss after 13237276 batches: 0.0258
trigger times: 13
Loss after 13238239 batches: 0.0255
trigger times: 14
Loss after 13239202 batches: 0.0251
trigger times: 15
Loss after 13240165 batches: 0.0243
trigger times: 16
Loss after 13241128 batches: 0.0245
trigger times: 17
Loss after 13242091 batches: 0.0235
trigger times: 18
Loss after 13243054 batches: 0.0244
trigger times: 19
Loss after 13244017 batches: 0.0239
trigger times: 20
Loss after 13244980 batches: 0.0233
trigger times: 21
Loss after 13245943 batches: 0.0238
trigger times: 22
Loss after 13246906 batches: 0.0276
trigger times: 23
Loss after 13247869 batches: 0.0314
trigger times: 24
Loss after 13248832 batches: 0.0308
trigger times: 25
Early stopping!
Start to test process.
Loss after 13249795 batches: 0.0280
Time to train on one home:  78.25131559371948
trigger times: 0
Loss after 13250758 batches: 0.0898
trigger times: 1
Loss after 13251721 batches: 0.0494
trigger times: 2
Loss after 13252684 batches: 0.0479
trigger times: 3
Loss after 13253647 batches: 0.0458
trigger times: 4
Loss after 13254610 batches: 0.0432
trigger times: 5
Loss after 13255573 batches: 0.0419
trigger times: 6
Loss after 13256536 batches: 0.0415
trigger times: 7
Loss after 13257499 batches: 0.0397
trigger times: 8
Loss after 13258462 batches: 0.0389
trigger times: 9
Loss after 13259425 batches: 0.0374
trigger times: 10
Loss after 13260388 batches: 0.0371
trigger times: 11
Loss after 13261351 batches: 0.0366
trigger times: 12
Loss after 13262314 batches: 0.0361
trigger times: 13
Loss after 13263277 batches: 0.0352
trigger times: 14
Loss after 13264240 batches: 0.0363
trigger times: 15
Loss after 13265203 batches: 0.0350
trigger times: 16
Loss after 13266166 batches: 0.0357
trigger times: 17
Loss after 13267129 batches: 0.0354
trigger times: 18
Loss after 13268092 batches: 0.0349
trigger times: 19
Loss after 13269055 batches: 0.0348
trigger times: 20
Loss after 13270018 batches: 0.0343
trigger times: 21
Loss after 13270981 batches: 0.0334
trigger times: 22
Loss after 13271944 batches: 0.0335
trigger times: 23
Loss after 13272907 batches: 0.0341
trigger times: 24
Loss after 13273870 batches: 0.0336
trigger times: 25
Early stopping!
Start to test process.
Loss after 13274833 batches: 0.0347
Time to train on one home:  52.46260070800781
trigger times: 0
Loss after 13275796 batches: 0.0934
trigger times: 1
Loss after 13276759 batches: 0.0868
trigger times: 2
Loss after 13277722 batches: 0.0809
trigger times: 3
Loss after 13278685 batches: 0.0796
trigger times: 4
Loss after 13279648 batches: 0.0756
trigger times: 5
Loss after 13280611 batches: 0.0733
trigger times: 6
Loss after 13281574 batches: 0.0706
trigger times: 7
Loss after 13282537 batches: 0.0697
trigger times: 8
Loss after 13283500 batches: 0.0683
trigger times: 9
Loss after 13284463 batches: 0.0644
trigger times: 10
Loss after 13285426 batches: 0.0656
trigger times: 11
Loss after 13286389 batches: 0.0652
trigger times: 12
Loss after 13287352 batches: 0.0649
trigger times: 13
Loss after 13288315 batches: 0.0618
trigger times: 14
Loss after 13289278 batches: 0.0625
trigger times: 15
Loss after 13290241 batches: 0.0633
trigger times: 16
Loss after 13291204 batches: 0.0606
trigger times: 17
Loss after 13292167 batches: 0.0616
trigger times: 18
Loss after 13293130 batches: 0.0604
trigger times: 19
Loss after 13294093 batches: 0.0607
trigger times: 20
Loss after 13295056 batches: 0.0583
trigger times: 21
Loss after 13296019 batches: 0.0581
trigger times: 22
Loss after 13296982 batches: 0.0571
trigger times: 23
Loss after 13297945 batches: 0.0598
trigger times: 24
Loss after 13298908 batches: 0.0586
trigger times: 25
Early stopping!
Start to test process.
Loss after 13299871 batches: 0.0576
Time to train on one home:  52.36168670654297
trigger times: 0
Loss after 13300834 batches: 0.0913
trigger times: 1
Loss after 13301797 batches: 0.0597
trigger times: 2
Loss after 13302760 batches: 0.0566
trigger times: 3
Loss after 13303723 batches: 0.0514
trigger times: 4
Loss after 13304686 batches: 0.0479
trigger times: 5
Loss after 13305649 batches: 0.0462
trigger times: 6
Loss after 13306612 batches: 0.0453
trigger times: 7
Loss after 13307575 batches: 0.0434
trigger times: 8
Loss after 13308538 batches: 0.0415
trigger times: 9
Loss after 13309501 batches: 0.0409
trigger times: 10
Loss after 13310464 batches: 0.0398
trigger times: 11
Loss after 13311427 batches: 0.0390
trigger times: 12
Loss after 13312390 batches: 0.0405
trigger times: 13
Loss after 13313353 batches: 0.0403
trigger times: 14
Loss after 13314316 batches: 0.0387
trigger times: 15
Loss after 13315279 batches: 0.0381
trigger times: 16
Loss after 13316242 batches: 0.0389
trigger times: 17
Loss after 13317205 batches: 0.0380
trigger times: 18
Loss after 13318168 batches: 0.0378
trigger times: 19
Loss after 13319131 batches: 0.0369
trigger times: 20
Loss after 13320094 batches: 0.0368
trigger times: 21
Loss after 13321057 batches: 0.0361
trigger times: 22
Loss after 13322020 batches: 0.0369
trigger times: 23
Loss after 13322983 batches: 0.0372
trigger times: 24
Loss after 13323946 batches: 0.0371
trigger times: 25
Early stopping!
Start to test process.
Loss after 13324909 batches: 0.0363
Time to train on one home:  52.17663860321045
trigger times: 0
Loss after 13325838 batches: 0.0894
trigger times: 1
Loss after 13326767 batches: 0.0663
trigger times: 2
Loss after 13327696 batches: 0.0489
trigger times: 3
Loss after 13328625 batches: 0.0449
trigger times: 4
Loss after 13329554 batches: 0.0388
trigger times: 5
Loss after 13330483 batches: 0.0365
trigger times: 6
Loss after 13331412 batches: 0.0360
trigger times: 7
Loss after 13332341 batches: 0.0327
trigger times: 8
Loss after 13333270 batches: 0.0301
trigger times: 9
Loss after 13334199 batches: 0.0290
trigger times: 10
Loss after 13335128 batches: 0.0279
trigger times: 11
Loss after 13336057 batches: 0.0273
trigger times: 12
Loss after 13336986 batches: 0.0302
trigger times: 13
Loss after 13337915 batches: 0.0279
trigger times: 14
Loss after 13338844 batches: 0.0263
trigger times: 15
Loss after 13339773 batches: 0.0261
trigger times: 16
Loss after 13340702 batches: 0.0258
trigger times: 17
Loss after 13341631 batches: 0.0257
trigger times: 18
Loss after 13342560 batches: 0.0249
trigger times: 19
Loss after 13343489 batches: 0.0255
trigger times: 20
Loss after 13344418 batches: 0.0273
trigger times: 21
Loss after 13345347 batches: 0.0250
trigger times: 22
Loss after 13346276 batches: 0.0263
trigger times: 23
Loss after 13347205 batches: 0.0270
trigger times: 24
Loss after 13348134 batches: 0.0256
trigger times: 25
Early stopping!
Start to test process.
Loss after 13349063 batches: 0.0268
Time to train on one home:  51.88765287399292
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13350026 batches: 0.0733
trigger times: 1
Loss after 13350989 batches: 0.0328
trigger times: 2
Loss after 13351952 batches: 0.0275
trigger times: 3
Loss after 13352915 batches: 0.0271
trigger times: 4
Loss after 13353878 batches: 0.0263
trigger times: 5
Loss after 13354841 batches: 0.0251
trigger times: 6
Loss after 13355804 batches: 0.0242
trigger times: 7
Loss after 13356767 batches: 0.0234
trigger times: 8
Loss after 13357730 batches: 0.0225
trigger times: 9
Loss after 13358693 batches: 0.0218
trigger times: 10
Loss after 13359656 batches: 0.0209
trigger times: 11
Loss after 13360619 batches: 0.0210
trigger times: 12
Loss after 13361582 batches: 0.0201
trigger times: 13
Loss after 13362545 batches: 0.0202
trigger times: 14
Loss after 13363508 batches: 0.0194
trigger times: 15
Loss after 13364471 batches: 0.0197
trigger times: 16
Loss after 13365434 batches: 0.0189
trigger times: 17
Loss after 13366397 batches: 0.0188
trigger times: 18
Loss after 13367360 batches: 0.0187
trigger times: 19
Loss after 13368323 batches: 0.0185
trigger times: 20
Loss after 13369286 batches: 0.0186
trigger times: 21
Loss after 13370249 batches: 0.0184
trigger times: 22
Loss after 13371212 batches: 0.0180
trigger times: 23
Loss after 13372175 batches: 0.0180
trigger times: 24
Loss after 13373138 batches: 0.0180
trigger times: 25
Early stopping!
Start to test process.
Loss after 13374101 batches: 0.0178
Time to train on one home:  52.1924946308136
trigger times: 0
Loss after 13375064 batches: 0.1811
trigger times: 1
Loss after 13376027 batches: 0.1232
trigger times: 2
Loss after 13376990 batches: 0.0996
trigger times: 3
Loss after 13377953 batches: 0.0896
trigger times: 4
Loss after 13378916 batches: 0.0809
trigger times: 5
Loss after 13379879 batches: 0.0771
trigger times: 6
Loss after 13380842 batches: 0.0712
trigger times: 7
Loss after 13381805 batches: 0.0645
trigger times: 8
Loss after 13382768 batches: 0.0613
trigger times: 9
Loss after 13383731 batches: 0.0573
trigger times: 10
Loss after 13384694 batches: 0.0560
trigger times: 11
Loss after 13385657 batches: 0.0533
trigger times: 12
Loss after 13386620 batches: 0.0518
trigger times: 13
Loss after 13387583 batches: 0.0532
trigger times: 14
Loss after 13388546 batches: 0.0509
trigger times: 15
Loss after 13389509 batches: 0.0492
trigger times: 16
Loss after 13390472 batches: 0.0498
trigger times: 17
Loss after 13391435 batches: 0.0496
trigger times: 18
Loss after 13392398 batches: 0.0471
trigger times: 19
Loss after 13393361 batches: 0.0500
trigger times: 20
Loss after 13394324 batches: 0.0506
trigger times: 21
Loss after 13395287 batches: 0.0466
trigger times: 22
Loss after 13396250 batches: 0.0462
trigger times: 23
Loss after 13397213 batches: 0.0432
trigger times: 24
Loss after 13398176 batches: 0.0434
trigger times: 25
Early stopping!
Start to test process.
Loss after 13399139 batches: 0.0452
Time to train on one home:  52.27641034126282
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13400102 batches: 0.0857
trigger times: 1
Loss after 13401065 batches: 0.0728
trigger times: 2
Loss after 13402028 batches: 0.0692
trigger times: 3
Loss after 13402991 batches: 0.0661
trigger times: 4
Loss after 13403954 batches: 0.0653
trigger times: 5
Loss after 13404917 batches: 0.0628
trigger times: 6
Loss after 13405880 batches: 0.0602
trigger times: 7
Loss after 13406843 batches: 0.0600
trigger times: 8
Loss after 13407806 batches: 0.0576
trigger times: 9
Loss after 13408769 batches: 0.0582
trigger times: 10
Loss after 13409732 batches: 0.0582
trigger times: 11
Loss after 13410695 batches: 0.0575
trigger times: 12
Loss after 13411658 batches: 0.0564
trigger times: 13
Loss after 13412621 batches: 0.0566
trigger times: 14
Loss after 13413584 batches: 0.0559
trigger times: 15
Loss after 13414547 batches: 0.0556
trigger times: 16
Loss after 13415510 batches: 0.0558
trigger times: 17
Loss after 13416473 batches: 0.0542
trigger times: 18
Loss after 13417436 batches: 0.0553
trigger times: 19
Loss after 13418399 batches: 0.0539
trigger times: 20
Loss after 13419362 batches: 0.0531
trigger times: 21
Loss after 13420325 batches: 0.0529
trigger times: 22
Loss after 13421288 batches: 0.0531
trigger times: 23
Loss after 13422251 batches: 0.0536
trigger times: 24
Loss after 13423214 batches: 0.0538
trigger times: 25
Early stopping!
Start to test process.
Loss after 13424177 batches: 0.0527
Time to train on one home:  52.47833490371704
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13425140 batches: 0.0865
trigger times: 1
Loss after 13426103 batches: 0.0743
trigger times: 2
Loss after 13427066 batches: 0.0719
trigger times: 3
Loss after 13428029 batches: 0.0675
trigger times: 4
Loss after 13428992 batches: 0.0654
trigger times: 5
Loss after 13429955 batches: 0.0631
trigger times: 6
Loss after 13430918 batches: 0.0630
trigger times: 7
Loss after 13431881 batches: 0.0617
trigger times: 8
Loss after 13432844 batches: 0.0576
trigger times: 9
Loss after 13433807 batches: 0.0569
trigger times: 10
Loss after 13434770 batches: 0.0533
trigger times: 11
Loss after 13435733 batches: 0.0533
trigger times: 12
Loss after 13436696 batches: 0.0520
trigger times: 13
Loss after 13437659 batches: 0.0518
trigger times: 14
Loss after 13438622 batches: 0.0504
trigger times: 15
Loss after 13439585 batches: 0.0499
trigger times: 16
Loss after 13440548 batches: 0.0515
trigger times: 17
Loss after 13441511 batches: 0.0490
trigger times: 18
Loss after 13442474 batches: 0.0484
trigger times: 19
Loss after 13443437 batches: 0.0491
trigger times: 20
Loss after 13444400 batches: 0.0493
trigger times: 21
Loss after 13445363 batches: 0.0479
trigger times: 22
Loss after 13446326 batches: 0.0467
trigger times: 23
Loss after 13447289 batches: 0.0512
trigger times: 24
Loss after 13448252 batches: 0.0513
trigger times: 25
Early stopping!
Start to test process.
Loss after 13449215 batches: 0.0501
Time to train on one home:  52.31932258605957
trigger times: 0
Loss after 13450178 batches: 0.0226
trigger times: 1
Loss after 13451141 batches: 0.0188
trigger times: 0
Loss after 13452104 batches: 0.0179
trigger times: 1
Loss after 13453067 batches: 0.0162
trigger times: 2
Loss after 13454030 batches: 0.0151
trigger times: 3
Loss after 13454993 batches: 0.0147
trigger times: 4
Loss after 13455956 batches: 0.0146
trigger times: 5
Loss after 13456919 batches: 0.0138
trigger times: 6
Loss after 13457882 batches: 0.0136
trigger times: 7
Loss after 13458845 batches: 0.0131
trigger times: 8
Loss after 13459808 batches: 0.0130
trigger times: 9
Loss after 13460771 batches: 0.0130
trigger times: 10
Loss after 13461734 batches: 0.0128
trigger times: 11
Loss after 13462697 batches: 0.0133
trigger times: 12
Loss after 13463660 batches: 0.0131
trigger times: 13
Loss after 13464623 batches: 0.0128
trigger times: 14
Loss after 13465586 batches: 0.0128
trigger times: 15
Loss after 13466549 batches: 0.0126
trigger times: 16
Loss after 13467512 batches: 0.0123
trigger times: 17
Loss after 13468475 batches: 0.0117
trigger times: 18
Loss after 13469438 batches: 0.0116
trigger times: 19
Loss after 13470401 batches: 0.0123
trigger times: 20
Loss after 13471364 batches: 0.0115
trigger times: 21
Loss after 13472327 batches: 0.0116
trigger times: 22
Loss after 13473290 batches: 0.0117
trigger times: 23
Loss after 13474253 batches: 0.0122
trigger times: 24
Loss after 13475216 batches: 0.0120
trigger times: 25
Early stopping!
Start to test process.
Loss after 13476179 batches: 0.0117
Time to train on one home:  54.04976963996887
trigger times: 0
Loss after 13477142 batches: 0.0399
trigger times: 1
Loss after 13478105 batches: 0.0337
trigger times: 2
Loss after 13479068 batches: 0.0303
trigger times: 3
Loss after 13480031 batches: 0.0294
trigger times: 4
Loss after 13480994 batches: 0.0271
trigger times: 5
Loss after 13481957 batches: 0.0261
trigger times: 6
Loss after 13482920 batches: 0.0259
trigger times: 7
Loss after 13483883 batches: 0.0240
trigger times: 8
Loss after 13484846 batches: 0.0234
trigger times: 9
Loss after 13485809 batches: 0.0235
trigger times: 10
Loss after 13486772 batches: 0.0227
trigger times: 11
Loss after 13487735 batches: 0.0220
trigger times: 12
Loss after 13488698 batches: 0.0220
trigger times: 13
Loss after 13489661 batches: 0.0218
trigger times: 14
Loss after 13490624 batches: 0.0209
trigger times: 15
Loss after 13491587 batches: 0.0215
trigger times: 16
Loss after 13492550 batches: 0.0217
trigger times: 17
Loss after 13493513 batches: 0.0222
trigger times: 18
Loss after 13494476 batches: 0.0216
trigger times: 19
Loss after 13495439 batches: 0.0210
trigger times: 20
Loss after 13496402 batches: 0.0209
trigger times: 21
Loss after 13497365 batches: 0.0207
trigger times: 22
Loss after 13498328 batches: 0.0209
trigger times: 23
Loss after 13499291 batches: 0.0205
trigger times: 24
Loss after 13500254 batches: 0.0207
trigger times: 25
Early stopping!
Start to test process.
Loss after 13501217 batches: 0.0207
Time to train on one home:  52.119325160980225
trigger times: 0
Loss after 13502180 batches: 0.0882
trigger times: 1
Loss after 13503143 batches: 0.0648
trigger times: 2
Loss after 13504106 batches: 0.0566
trigger times: 3
Loss after 13505069 batches: 0.0518
trigger times: 4
Loss after 13506032 batches: 0.0476
trigger times: 5
Loss after 13506995 batches: 0.0464
trigger times: 6
Loss after 13507958 batches: 0.0453
trigger times: 7
Loss after 13508921 batches: 0.0439
trigger times: 8
Loss after 13509884 batches: 0.0422
trigger times: 9
Loss after 13510847 batches: 0.0413
trigger times: 10
Loss after 13511810 batches: 0.0414
trigger times: 11
Loss after 13512773 batches: 0.0411
trigger times: 12
Loss after 13513736 batches: 0.0396
trigger times: 13
Loss after 13514699 batches: 0.0390
trigger times: 14
Loss after 13515662 batches: 0.0376
trigger times: 15
Loss after 13516625 batches: 0.0382
trigger times: 16
Loss after 13517588 batches: 0.0369
trigger times: 17
Loss after 13518551 batches: 0.0372
trigger times: 18
Loss after 13519514 batches: 0.0390
trigger times: 19
Loss after 13520477 batches: 0.0373
trigger times: 20
Loss after 13521440 batches: 0.0359
trigger times: 21
Loss after 13522403 batches: 0.0369
trigger times: 22
Loss after 13523366 batches: 0.0363
trigger times: 23
Loss after 13524329 batches: 0.0360
trigger times: 24
Loss after 13525292 batches: 0.0351
trigger times: 25
Early stopping!
Start to test process.
Loss after 13526255 batches: 0.0351
Time to train on one home:  52.33024621009827
trigger times: 0
Loss after 13527218 batches: 0.0917
trigger times: 1
Loss after 13528181 batches: 0.0611
trigger times: 2
Loss after 13529144 batches: 0.0559
trigger times: 3
Loss after 13530107 batches: 0.0506
trigger times: 4
Loss after 13531070 batches: 0.0468
trigger times: 5
Loss after 13532033 batches: 0.0450
trigger times: 6
Loss after 13532996 batches: 0.0445
trigger times: 7
Loss after 13533959 batches: 0.0425
trigger times: 8
Loss after 13534922 batches: 0.0421
trigger times: 9
Loss after 13535885 batches: 0.0412
trigger times: 10
Loss after 13536848 batches: 0.0417
trigger times: 11
Loss after 13537811 batches: 0.0412
trigger times: 12
Loss after 13538774 batches: 0.0407
trigger times: 13
Loss after 13539737 batches: 0.0400
trigger times: 14
Loss after 13540700 batches: 0.0386
trigger times: 15
Loss after 13541663 batches: 0.0392
trigger times: 16
Loss after 13542626 batches: 0.0377
trigger times: 17
Loss after 13543589 batches: 0.0382
trigger times: 18
Loss after 13544552 batches: 0.0372
trigger times: 19
Loss after 13545515 batches: 0.0372
trigger times: 20
Loss after 13546478 batches: 0.0378
trigger times: 21
Loss after 13547441 batches: 0.0371
trigger times: 22
Loss after 13548404 batches: 0.0364
trigger times: 23
Loss after 13549367 batches: 0.0360
trigger times: 24
Loss after 13550330 batches: 0.0360
trigger times: 25
Early stopping!
Start to test process.
Loss after 13551293 batches: 0.0352
Time to train on one home:  52.94302153587341
trigger times: 0
Loss after 13552256 batches: 0.0490
trigger times: 1
Loss after 13553219 batches: 0.0407
trigger times: 2
Loss after 13554182 batches: 0.0361
trigger times: 3
Loss after 13555145 batches: 0.0337
trigger times: 4
Loss after 13556108 batches: 0.0335
trigger times: 5
Loss after 13557071 batches: 0.0324
trigger times: 6
Loss after 13558034 batches: 0.0312
trigger times: 7
Loss after 13558997 batches: 0.0303
trigger times: 8
Loss after 13559960 batches: 0.0307
trigger times: 9
Loss after 13560923 batches: 0.0297
trigger times: 10
Loss after 13561886 batches: 0.0298
trigger times: 11
Loss after 13562849 batches: 0.0285
trigger times: 12
Loss after 13563812 batches: 0.0279
trigger times: 13
Loss after 13564775 batches: 0.0294
trigger times: 14
Loss after 13565738 batches: 0.0280
trigger times: 15
Loss after 13566701 batches: 0.0273
trigger times: 16
Loss after 13567664 batches: 0.0280
trigger times: 17
Loss after 13568627 batches: 0.0269
trigger times: 18
Loss after 13569590 batches: 0.0275
trigger times: 19
Loss after 13570553 batches: 0.0272
trigger times: 20
Loss after 13571516 batches: 0.0250
trigger times: 21
Loss after 13572479 batches: 0.0262
trigger times: 22
Loss after 13573442 batches: 0.0262
trigger times: 23
Loss after 13574405 batches: 0.0251
trigger times: 24
Loss after 13575368 batches: 0.0245
trigger times: 25
Early stopping!
Start to test process.
Loss after 13576331 batches: 0.0253
Time to train on one home:  52.75608801841736
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13577294 batches: 0.0811
trigger times: 1
Loss after 13578257 batches: 0.0509
trigger times: 2
Loss after 13579220 batches: 0.0512
trigger times: 0
Loss after 13580183 batches: 0.0430
trigger times: 1
Loss after 13581146 batches: 0.0395
trigger times: 0
Loss after 13582109 batches: 0.0368
trigger times: 1
Loss after 13583072 batches: 0.0348
trigger times: 0
Loss after 13584035 batches: 0.0334
trigger times: 0
Loss after 13584998 batches: 0.0313
trigger times: 0
Loss after 13585961 batches: 0.0306
trigger times: 1
Loss after 13586924 batches: 0.0299
trigger times: 2
Loss after 13587887 batches: 0.0294
trigger times: 3
Loss after 13588850 batches: 0.0289
trigger times: 4
Loss after 13589813 batches: 0.0291
trigger times: 5
Loss after 13590776 batches: 0.0293
trigger times: 6
Loss after 13591739 batches: 0.0283
trigger times: 7
Loss after 13592702 batches: 0.0282
trigger times: 8
Loss after 13593665 batches: 0.0271
trigger times: 9
Loss after 13594628 batches: 0.0267
trigger times: 10
Loss after 13595591 batches: 0.0278
trigger times: 11
Loss after 13596554 batches: 0.0269
trigger times: 12
Loss after 13597517 batches: 0.0263
trigger times: 13
Loss after 13598480 batches: 0.0262
trigger times: 14
Loss after 13599443 batches: 0.0258
trigger times: 15
Loss after 13600406 batches: 0.0265
trigger times: 16
Loss after 13601369 batches: 0.0251
trigger times: 17
Loss after 13602332 batches: 0.0241
trigger times: 18
Loss after 13603295 batches: 0.0248
trigger times: 19
Loss after 13604258 batches: 0.0244
trigger times: 20
Loss after 13605221 batches: 0.0245
trigger times: 21
Loss after 13606184 batches: 0.0246
trigger times: 22
Loss after 13607147 batches: 0.0238
trigger times: 23
Loss after 13608110 batches: 0.0240
trigger times: 24
Loss after 13609073 batches: 0.0234
trigger times: 0
Loss after 13610036 batches: 0.0240
trigger times: 1
Loss after 13610999 batches: 0.0235
trigger times: 2
Loss after 13611962 batches: 0.0231
trigger times: 3
Loss after 13612925 batches: 0.0228
trigger times: 4
Loss after 13613888 batches: 0.0226
trigger times: 5
Loss after 13614851 batches: 0.0223
trigger times: 6
Loss after 13615814 batches: 0.0220
trigger times: 7
Loss after 13616777 batches: 0.0220
trigger times: 8
Loss after 13617740 batches: 0.0227
trigger times: 9
Loss after 13618703 batches: 0.0218
trigger times: 10
Loss after 13619666 batches: 0.0223
trigger times: 11
Loss after 13620629 batches: 0.0213
trigger times: 12
Loss after 13621592 batches: 0.0226
trigger times: 13
Loss after 13622555 batches: 0.0216
trigger times: 14
Loss after 13623518 batches: 0.0212
trigger times: 15
Loss after 13624481 batches: 0.0214
trigger times: 16
Loss after 13625444 batches: 0.0210
trigger times: 17
Loss after 13626407 batches: 0.0213
trigger times: 18
Loss after 13627370 batches: 0.0205
trigger times: 19
Loss after 13628333 batches: 0.0209
trigger times: 20
Loss after 13629296 batches: 0.0210
trigger times: 21
Loss after 13630259 batches: 0.0204
trigger times: 22
Loss after 13631222 batches: 0.0195
trigger times: 23
Loss after 13632185 batches: 0.0203
trigger times: 24
Loss after 13633148 batches: 0.0196
trigger times: 0
Loss after 13634111 batches: 0.0197
trigger times: 1
Loss after 13635074 batches: 0.0193
trigger times: 2
Loss after 13636037 batches: 0.0194
trigger times: 3
Loss after 13637000 batches: 0.0200
trigger times: 4
Loss after 13637963 batches: 0.0239
trigger times: 5
Loss after 13638926 batches: 0.0222
trigger times: 6
Loss after 13639889 batches: 0.0217
trigger times: 7
Loss after 13640852 batches: 0.0215
trigger times: 8
Loss after 13641815 batches: 0.0200
trigger times: 9
Loss after 13642778 batches: 0.0193
trigger times: 10
Loss after 13643741 batches: 0.0187
trigger times: 11
Loss after 13644704 batches: 0.0197
trigger times: 12
Loss after 13645667 batches: 0.0192
trigger times: 13
Loss after 13646630 batches: 0.0192
trigger times: 14
Loss after 13647593 batches: 0.0180
trigger times: 15
Loss after 13648556 batches: 0.0184
trigger times: 16
Loss after 13649519 batches: 0.0194
trigger times: 17
Loss after 13650482 batches: 0.0188
trigger times: 18
Loss after 13651445 batches: 0.0180
trigger times: 19
Loss after 13652408 batches: 0.0184
trigger times: 20
Loss after 13653371 batches: 0.0181
trigger times: 21
Loss after 13654334 batches: 0.0186
trigger times: 22
Loss after 13655297 batches: 0.0173
trigger times: 23
Loss after 13656260 batches: 0.0178
trigger times: 24
Loss after 13657223 batches: 0.0168
trigger times: 25
Early stopping!
Start to test process.
Loss after 13658186 batches: 0.0177
Time to train on one home:  97.59655404090881
trigger times: 0
Loss after 13659149 batches: 0.1023
trigger times: 1
Loss after 13660112 batches: 0.0789
trigger times: 2
Loss after 13661075 batches: 0.0762
trigger times: 3
Loss after 13662038 batches: 0.0757
trigger times: 4
Loss after 13663001 batches: 0.0724
trigger times: 5
Loss after 13663964 batches: 0.0697
trigger times: 6
Loss after 13664927 batches: 0.0672
trigger times: 7
Loss after 13665890 batches: 0.0654
trigger times: 8
Loss after 13666853 batches: 0.0646
trigger times: 9
Loss after 13667816 batches: 0.0641
trigger times: 10
Loss after 13668779 batches: 0.0615
trigger times: 11
Loss after 13669742 batches: 0.0614
trigger times: 12
Loss after 13670705 batches: 0.0607
trigger times: 13
Loss after 13671668 batches: 0.0594
trigger times: 14
Loss after 13672631 batches: 0.0619
trigger times: 15
Loss after 13673594 batches: 0.0615
trigger times: 16
Loss after 13674557 batches: 0.0595
trigger times: 17
Loss after 13675520 batches: 0.0580
trigger times: 18
Loss after 13676483 batches: 0.0578
trigger times: 19
Loss after 13677446 batches: 0.0575
trigger times: 20
Loss after 13678409 batches: 0.0561
trigger times: 21
Loss after 13679372 batches: 0.0576
trigger times: 22
Loss after 13680335 batches: 0.0560
trigger times: 23
Loss after 13681298 batches: 0.0559
trigger times: 24
Loss after 13682261 batches: 0.0565
trigger times: 25
Early stopping!
Start to test process.
Loss after 13683224 batches: 0.0565
Time to train on one home:  52.28118705749512
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13684187 batches: 0.0807
trigger times: 1
Loss after 13685150 batches: 0.0490
trigger times: 2
Loss after 13686113 batches: 0.0505
trigger times: 0
Loss after 13687076 batches: 0.0430
trigger times: 0
Loss after 13688039 batches: 0.0383
trigger times: 0
Loss after 13689002 batches: 0.0365
trigger times: 0
Loss after 13689965 batches: 0.0347
trigger times: 0
Loss after 13690928 batches: 0.0329
trigger times: 1
Loss after 13691891 batches: 0.0312
trigger times: 2
Loss after 13692854 batches: 0.0302
trigger times: 3
Loss after 13693817 batches: 0.0294
trigger times: 4
Loss after 13694780 batches: 0.0292
trigger times: 5
Loss after 13695743 batches: 0.0287
trigger times: 6
Loss after 13696706 batches: 0.0285
trigger times: 7
Loss after 13697669 batches: 0.0275
trigger times: 8
Loss after 13698632 batches: 0.0272
trigger times: 9
Loss after 13699595 batches: 0.0266
trigger times: 10
Loss after 13700558 batches: 0.0259
trigger times: 11
Loss after 13701521 batches: 0.0262
trigger times: 12
Loss after 13702484 batches: 0.0262
trigger times: 13
Loss after 13703447 batches: 0.0256
trigger times: 14
Loss after 13704410 batches: 0.0255
trigger times: 15
Loss after 13705373 batches: 0.0257
trigger times: 16
Loss after 13706336 batches: 0.0259
trigger times: 17
Loss after 13707299 batches: 0.0254
trigger times: 18
Loss after 13708262 batches: 0.0258
trigger times: 19
Loss after 13709225 batches: 0.0259
trigger times: 20
Loss after 13710188 batches: 0.0257
trigger times: 21
Loss after 13711151 batches: 0.0256
trigger times: 22
Loss after 13712114 batches: 0.0254
trigger times: 23
Loss after 13713077 batches: 0.0250
trigger times: 24
Loss after 13714040 batches: 0.0250
trigger times: 25
Early stopping!
Start to test process.
Loss after 13715003 batches: 0.0247
Time to train on one home:  57.241694927215576
trigger times: 0
Loss after 13715898 batches: 0.0676
trigger times: 1
Loss after 13716793 batches: 0.0417
trigger times: 2
Loss after 13717688 batches: 0.0201
trigger times: 3
Loss after 13718583 batches: 0.0117
trigger times: 4
Loss after 13719478 batches: 0.0088
trigger times: 5
Loss after 13720373 batches: 0.0069
trigger times: 6
Loss after 13721268 batches: 0.0054
trigger times: 7
Loss after 13722163 batches: 0.0051
trigger times: 8
Loss after 13723058 batches: 0.0043
trigger times: 9
Loss after 13723953 batches: 0.0039
trigger times: 10
Loss after 13724848 batches: 0.0036
trigger times: 11
Loss after 13725743 batches: 0.0032
trigger times: 12
Loss after 13726638 batches: 0.0036
trigger times: 13
Loss after 13727533 batches: 0.0043
trigger times: 14
Loss after 13728428 batches: 0.0033
trigger times: 15
Loss after 13729323 batches: 0.0038
trigger times: 16
Loss after 13730218 batches: 0.0037
trigger times: 17
Loss after 13731113 batches: 0.0031
trigger times: 18
Loss after 13732008 batches: 0.0031
trigger times: 19
Loss after 13732903 batches: 0.0027
trigger times: 20
Loss after 13733798 batches: 0.0030
trigger times: 21
Loss after 13734693 batches: 0.0031
trigger times: 22
Loss after 13735588 batches: 0.0027
trigger times: 23
Loss after 13736483 batches: 0.0028
trigger times: 24
Loss after 13737378 batches: 0.0027
trigger times: 25
Early stopping!
Start to test process.
Loss after 13738273 batches: 0.0023
Time to train on one home:  51.11054301261902
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13739210 batches: 0.0774
trigger times: 1
Loss after 13740147 batches: 0.0659
trigger times: 2
Loss after 13741084 batches: 0.0632
trigger times: 3
Loss after 13742021 batches: 0.0611
trigger times: 4
Loss after 13742958 batches: 0.0601
trigger times: 5
Loss after 13743895 batches: 0.0559
trigger times: 6
Loss after 13744832 batches: 0.0551
trigger times: 7
Loss after 13745769 batches: 0.0532
trigger times: 8
Loss after 13746706 batches: 0.0530
trigger times: 9
Loss after 13747643 batches: 0.0525
trigger times: 10
Loss after 13748580 batches: 0.0514
trigger times: 11
Loss after 13749517 batches: 0.0510
trigger times: 12
Loss after 13750454 batches: 0.0506
trigger times: 13
Loss after 13751391 batches: 0.0513
trigger times: 14
Loss after 13752328 batches: 0.0493
trigger times: 15
Loss after 13753265 batches: 0.0484
trigger times: 16
Loss after 13754202 batches: 0.0492
trigger times: 17
Loss after 13755139 batches: 0.0504
trigger times: 18
Loss after 13756076 batches: 0.0478
trigger times: 19
Loss after 13757013 batches: 0.0488
trigger times: 20
Loss after 13757950 batches: 0.0474
trigger times: 21
Loss after 13758887 batches: 0.0480
trigger times: 22
Loss after 13759824 batches: 0.0469
trigger times: 23
Loss after 13760761 batches: 0.0475
trigger times: 24
Loss after 13761698 batches: 0.0470
trigger times: 25
Early stopping!
Start to test process.
Loss after 13762635 batches: 0.0461
Time to train on one home:  52.29219055175781
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066, 0.03401813118363776, 0.033508403208926535, 0.03273922320832277, 0.032162070075287874]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863], [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861], [0.0757279247045517, 0.10266384027132536, 0.5909740425258612, 0.8803234656976516, 0.8076583271010261, 28.206427011804568, 8296.775], [0.06181953847408295, 0.10445801892372542, 0.5961459732150404, 0.831618823847048, 0.806043429565136, 26.645882531252276, 8280.187], [0.07333928346633911, 0.09887557787656209, 0.5904951093552152, 0.8805827319948272, 0.8110679704670578, 28.214734158180708, 8331.802]]
Round_11_results:  [0.07333928346633911, 0.09887557787656209, 0.5904951093552152, 0.8805827319948272, 0.8110679704670578, 28.214734158180708, 8331.802]
trigger times: 0
Loss after 13763597 batches: 0.0706
trigger times: 1
Loss after 13764559 batches: 0.0640
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 14356 < 14357; dropping {'Training_Loss': 0.07056484797171184, 'Validation_Loss': 0.0814746543765068, 'Training_R2': -0.09501689613954811, 'Validation_R2': 0.12445331656659198, 'Training_F1': 0.4316206283447299, 'Validation_F1': 0.6055544340263072, 'Training_NEP': 0.8265531112407339, 'Validation_NEP': 0.7852252402252505, 'Training_NDE': 0.666891842732417, 'Validation_NDE': 0.7910356124298057, 'Training_MAE': 31.409284162247285, 'Validation_MAE': 27.92203925637638, 'Training_MSE': 2463.089, 'Validation_MSE': 10362.575}.
trigger times: 2
Loss after 13765521 batches: 0.0622
trigger times: 3
Loss after 13766483 batches: 0.0589
trigger times: 4
Loss after 13767445 batches: 0.0567
trigger times: 5
Loss after 13768407 batches: 0.0553
trigger times: 6
Loss after 13769369 batches: 0.0540
trigger times: 7
Loss after 13770331 batches: 0.0528
trigger times: 8
Loss after 13771293 batches: 0.0521
trigger times: 9
Loss after 13772255 batches: 0.0517
trigger times: 10
Loss after 13773217 batches: 0.0526
trigger times: 11
Loss after 13774179 batches: 0.0510
trigger times: 12
Loss after 13775141 batches: 0.0512
trigger times: 13
Loss after 13776103 batches: 0.0518
trigger times: 14
Loss after 13777065 batches: 0.0518
trigger times: 15
Loss after 13778027 batches: 0.0510
trigger times: 16
Loss after 13778989 batches: 0.0514
trigger times: 17
Loss after 13779951 batches: 0.0502
trigger times: 18
Loss after 13780913 batches: 0.0490
trigger times: 19
Loss after 13781875 batches: 0.0496
trigger times: 20
Loss after 13782837 batches: 0.0506
trigger times: 21
Loss after 13783799 batches: 0.0503
trigger times: 22
Loss after 13784761 batches: 0.0486
trigger times: 23
Loss after 13785723 batches: 0.0491
trigger times: 24
Loss after 13786685 batches: 0.0490
trigger times: 25
Early stopping!
Start to test process.
Loss after 13787647 batches: 0.0482
Time to train on one home:  52.22528672218323
trigger times: 0
Loss after 13788576 batches: 0.1119
trigger times: 1
Loss after 13789505 batches: 0.0652
trigger times: 2
Loss after 13790434 batches: 0.0531
trigger times: 3
Loss after 13791363 batches: 0.0430
trigger times: 4
Loss after 13792292 batches: 0.0386
trigger times: 5
Loss after 13793221 batches: 0.0377
trigger times: 6
Loss after 13794150 batches: 0.0362
trigger times: 7
Loss after 13795079 batches: 0.0339
trigger times: 8
Loss after 13796008 batches: 0.0332
trigger times: 9
Loss after 13796937 batches: 0.0297
trigger times: 10
Loss after 13797866 batches: 0.0294
trigger times: 11
Loss after 13798795 batches: 0.0276
trigger times: 12
Loss after 13799724 batches: 0.0274
trigger times: 13
Loss after 13800653 batches: 0.0268
trigger times: 14
Loss after 13801582 batches: 0.0273
trigger times: 15
Loss after 13802511 batches: 0.0270
trigger times: 16
Loss after 13803440 batches: 0.0263
trigger times: 17
Loss after 13804369 batches: 0.0245
trigger times: 18
Loss after 13805298 batches: 0.0278
trigger times: 19
Loss after 13806227 batches: 0.0242
trigger times: 20
Loss after 13807156 batches: 0.0242
trigger times: 21
Loss after 13808085 batches: 0.0244
trigger times: 22
Loss after 13809014 batches: 0.0254
trigger times: 23
Loss after 13809943 batches: 0.0227
trigger times: 24
Loss after 13810872 batches: 0.0230
trigger times: 25
Early stopping!
Start to test process.
Loss after 13811801 batches: 0.0248
Time to train on one home:  52.19673418998718
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 13812764 batches: 0.0305
trigger times: 0
Loss after 13813727 batches: 0.0163
trigger times: 1
Loss after 13814690 batches: 0.0145
trigger times: 2
Loss after 13815653 batches: 0.0139
trigger times: 3
Loss after 13816616 batches: 0.0135
trigger times: 4
Loss after 13817579 batches: 0.0133
trigger times: 5
Loss after 13818542 batches: 0.0128
trigger times: 6
Loss after 13819505 batches: 0.0124
trigger times: 7
Loss after 13820468 batches: 0.0114
trigger times: 8
Loss after 13821431 batches: 0.0107
trigger times: 9
Loss after 13822394 batches: 0.0098
trigger times: 10
Loss after 13823357 batches: 0.0092
trigger times: 11
Loss after 13824320 batches: 0.0090
trigger times: 12
Loss after 13825283 batches: 0.0084
trigger times: 13
Loss after 13826246 batches: 0.0082
trigger times: 14
Loss after 13827209 batches: 0.0078
trigger times: 15
Loss after 13828172 batches: 0.0077
trigger times: 16
Loss after 13829135 batches: 0.0078
trigger times: 17
Loss after 13830098 batches: 0.0075
trigger times: 18
Loss after 13831061 batches: 0.0076
trigger times: 19
Loss after 13832024 batches: 0.0073
trigger times: 20
Loss after 13832987 batches: 0.0072
trigger times: 21
Loss after 13833950 batches: 0.0070
trigger times: 22
Loss after 13834913 batches: 0.0069
trigger times: 23
Loss after 13835876 batches: 0.0068
trigger times: 24
Loss after 13836839 batches: 0.0067
trigger times: 25
Early stopping!
Start to test process.
Loss after 13837802 batches: 0.0065
Time to train on one home:  53.331275939941406
trigger times: 0
Loss after 13838765 batches: 0.0220
trigger times: 1
Loss after 13839728 batches: 0.0185
trigger times: 2
Loss after 13840691 batches: 0.0174
trigger times: 3
Loss after 13841654 batches: 0.0159
trigger times: 4
Loss after 13842617 batches: 0.0150
trigger times: 0
Loss after 13843580 batches: 0.0139
trigger times: 1
Loss after 13844543 batches: 0.0135
trigger times: 2
Loss after 13845506 batches: 0.0141
trigger times: 3
Loss after 13846469 batches: 0.0137
trigger times: 4
Loss after 13847432 batches: 0.0128
trigger times: 5
Loss after 13848395 batches: 0.0127
trigger times: 6
Loss after 13849358 batches: 0.0128
trigger times: 7
Loss after 13850321 batches: 0.0125
trigger times: 8
Loss after 13851284 batches: 0.0121
trigger times: 9
Loss after 13852247 batches: 0.0120
trigger times: 10
Loss after 13853210 batches: 0.0118
trigger times: 11
Loss after 13854173 batches: 0.0120
trigger times: 12
Loss after 13855136 batches: 0.0125
trigger times: 13
Loss after 13856099 batches: 0.0118
trigger times: 14
Loss after 13857062 batches: 0.0116
trigger times: 15
Loss after 13858025 batches: 0.0114
trigger times: 16
Loss after 13858988 batches: 0.0114
trigger times: 17
Loss after 13859951 batches: 0.0113
trigger times: 18
Loss after 13860914 batches: 0.0118
trigger times: 19
Loss after 13861877 batches: 0.0119
trigger times: 20
Loss after 13862840 batches: 0.0117
trigger times: 21
Loss after 13863803 batches: 0.0117
trigger times: 22
Loss after 13864766 batches: 0.0112
trigger times: 23
Loss after 13865729 batches: 0.0107
trigger times: 24
Loss after 13866692 batches: 0.0105
trigger times: 25
Early stopping!
Start to test process.
Loss after 13867655 batches: 0.0107
Time to train on one home:  56.007455348968506
trigger times: 0
Loss after 13868618 batches: 0.0962
trigger times: 1
Loss after 13869581 batches: 0.0887
trigger times: 2
Loss after 13870544 batches: 0.0828
trigger times: 3
Loss after 13871507 batches: 0.0786
trigger times: 4
Loss after 13872470 batches: 0.0757
trigger times: 5
Loss after 13873433 batches: 0.0722
trigger times: 6
Loss after 13874396 batches: 0.0694
trigger times: 7
Loss after 13875359 batches: 0.0665
trigger times: 8
Loss after 13876322 batches: 0.0662
trigger times: 9
Loss after 13877285 batches: 0.0654
trigger times: 10
Loss after 13878248 batches: 0.0639
trigger times: 11
Loss after 13879211 batches: 0.0629
trigger times: 12
Loss after 13880174 batches: 0.0640
trigger times: 13
Loss after 13881137 batches: 0.0606
trigger times: 14
Loss after 13882100 batches: 0.0587
trigger times: 15
Loss after 13883063 batches: 0.0598
trigger times: 16
Loss after 13884026 batches: 0.0595
trigger times: 17
Loss after 13884989 batches: 0.0628
trigger times: 18
Loss after 13885952 batches: 0.0609
trigger times: 19
Loss after 13886915 batches: 0.0616
trigger times: 20
Loss after 13887878 batches: 0.0607
trigger times: 21
Loss after 13888841 batches: 0.0599
trigger times: 22
Loss after 13889804 batches: 0.0578
trigger times: 23
Loss after 13890767 batches: 0.0584
trigger times: 24
Loss after 13891730 batches: 0.0570
trigger times: 25
Early stopping!
Start to test process.
Loss after 13892693 batches: 0.0585
Time to train on one home:  52.14052700996399
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13893656 batches: 0.0794
trigger times: 1
Loss after 13894619 batches: 0.0714
trigger times: 2
Loss after 13895582 batches: 0.0667
trigger times: 3
Loss after 13896545 batches: 0.0638
trigger times: 4
Loss after 13897508 batches: 0.0641
trigger times: 5
Loss after 13898471 batches: 0.0615
trigger times: 6
Loss after 13899434 batches: 0.0598
trigger times: 7
Loss after 13900397 batches: 0.0583
trigger times: 8
Loss after 13901360 batches: 0.0582
trigger times: 9
Loss after 13902323 batches: 0.0573
trigger times: 10
Loss after 13903286 batches: 0.0555
trigger times: 11
Loss after 13904249 batches: 0.0570
trigger times: 12
Loss after 13905212 batches: 0.0560
trigger times: 13
Loss after 13906175 batches: 0.0528
trigger times: 14
Loss after 13907138 batches: 0.0550
trigger times: 15
Loss after 13908101 batches: 0.0556
trigger times: 16
Loss after 13909064 batches: 0.0548
trigger times: 17
Loss after 13910027 batches: 0.0537
trigger times: 18
Loss after 13910990 batches: 0.0524
trigger times: 19
Loss after 13911953 batches: 0.0532
trigger times: 20
Loss after 13912916 batches: 0.0532
trigger times: 21
Loss after 13913879 batches: 0.0530
trigger times: 22
Loss after 13914842 batches: 0.0530
trigger times: 23
Loss after 13915805 batches: 0.0510
trigger times: 24
Loss after 13916768 batches: 0.0519
trigger times: 25
Early stopping!
Start to test process.
Loss after 13917731 batches: 0.0513
Time to train on one home:  52.486371755599976
trigger times: 0
Loss after 13918694 batches: 0.0744
trigger times: 1
Loss after 13919657 batches: 0.0699
trigger times: 2
Loss after 13920620 batches: 0.0658
trigger times: 3
Loss after 13921583 batches: 0.0638
trigger times: 4
Loss after 13922546 batches: 0.0606
trigger times: 5
Loss after 13923509 batches: 0.0592
trigger times: 6
Loss after 13924472 batches: 0.0572
trigger times: 7
Loss after 13925435 batches: 0.0565
trigger times: 8
Loss after 13926398 batches: 0.0549
trigger times: 9
Loss after 13927361 batches: 0.0551
trigger times: 10
Loss after 13928324 batches: 0.0544
trigger times: 11
Loss after 13929287 batches: 0.0542
trigger times: 12
Loss after 13930250 batches: 0.0531
trigger times: 13
Loss after 13931213 batches: 0.0522
trigger times: 14
Loss after 13932176 batches: 0.0525
trigger times: 15
Loss after 13933139 batches: 0.0521
trigger times: 16
Loss after 13934102 batches: 0.0502
trigger times: 17
Loss after 13935065 batches: 0.0519
trigger times: 18
Loss after 13936028 batches: 0.0508
trigger times: 19
Loss after 13936991 batches: 0.0514
trigger times: 20
Loss after 13937954 batches: 0.0501
trigger times: 21
Loss after 13938917 batches: 0.0513
trigger times: 22
Loss after 13939880 batches: 0.0504
trigger times: 23
Loss after 13940843 batches: 0.0500
trigger times: 24
Loss after 13941806 batches: 0.0513
trigger times: 25
Early stopping!
Start to test process.
Loss after 13942769 batches: 0.0504
Time to train on one home:  52.447317123413086
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 13943732 batches: 0.0619
trigger times: 1
Loss after 13944695 batches: 0.0494
trigger times: 2
Loss after 13945658 batches: 0.0477
trigger times: 3
Loss after 13946621 batches: 0.0414
trigger times: 4
Loss after 13947584 batches: 0.0361
trigger times: 5
Loss after 13948547 batches: 0.0333
trigger times: 6
Loss after 13949510 batches: 0.0311
trigger times: 7
Loss after 13950473 batches: 0.0295
trigger times: 8
Loss after 13951436 batches: 0.0288
trigger times: 9
Loss after 13952399 batches: 0.0280
trigger times: 10
Loss after 13953362 batches: 0.0274
trigger times: 11
Loss after 13954325 batches: 0.0262
trigger times: 12
Loss after 13955288 batches: 0.0261
trigger times: 13
Loss after 13956251 batches: 0.0256
trigger times: 14
Loss after 13957214 batches: 0.0252
trigger times: 15
Loss after 13958177 batches: 0.0242
trigger times: 16
Loss after 13959140 batches: 0.0239
trigger times: 17
Loss after 13960103 batches: 0.0247
trigger times: 18
Loss after 13961066 batches: 0.0250
trigger times: 19
Loss after 13962029 batches: 0.0238
trigger times: 20
Loss after 13962992 batches: 0.0235
trigger times: 21
Loss after 13963955 batches: 0.0232
trigger times: 22
Loss after 13964918 batches: 0.0236
trigger times: 23
Loss after 13965881 batches: 0.0235
trigger times: 24
Loss after 13966844 batches: 0.0227
trigger times: 25
Early stopping!
Start to test process.
Loss after 13967807 batches: 0.0219
Time to train on one home:  52.01443386077881
trigger times: 0
Loss after 13968765 batches: 0.0763
trigger times: 1
Loss after 13969723 batches: 0.0458
trigger times: 2
Loss after 13970681 batches: 0.0374
trigger times: 3
Loss after 13971639 batches: 0.0334
trigger times: 4
Loss after 13972597 batches: 0.0291
trigger times: 5
Loss after 13973555 batches: 0.0271
trigger times: 6
Loss after 13974513 batches: 0.0238
trigger times: 7
Loss after 13975471 batches: 0.0230
trigger times: 8
Loss after 13976429 batches: 0.0249
trigger times: 9
Loss after 13977387 batches: 0.0220
trigger times: 10
Loss after 13978345 batches: 0.0219
trigger times: 11
Loss after 13979303 batches: 0.0227
trigger times: 12
Loss after 13980261 batches: 0.0216
trigger times: 13
Loss after 13981219 batches: 0.0193
trigger times: 14
Loss after 13982177 batches: 0.0189
trigger times: 15
Loss after 13983135 batches: 0.0181
trigger times: 16
Loss after 13984093 batches: 0.0176
trigger times: 17
Loss after 13985051 batches: 0.0176
trigger times: 18
Loss after 13986009 batches: 0.0169
trigger times: 19
Loss after 13986967 batches: 0.0170
trigger times: 20
Loss after 13987925 batches: 0.0169
trigger times: 21
Loss after 13988883 batches: 0.0182
trigger times: 22
Loss after 13989841 batches: 0.0167
trigger times: 23
Loss after 13990799 batches: 0.0172
trigger times: 24
Loss after 13991757 batches: 0.0160
trigger times: 25
Early stopping!
Start to test process.
Loss after 13992715 batches: 0.0158
Time to train on one home:  52.11437487602234
trigger times: 0
Loss after 13993677 batches: 0.0708
trigger times: 1
Loss after 13994639 batches: 0.0638
trigger times: 2
Loss after 13995601 batches: 0.0618
trigger times: 3
Loss after 13996563 batches: 0.0582
trigger times: 4
Loss after 13997525 batches: 0.0564
trigger times: 5
Loss after 13998487 batches: 0.0553
trigger times: 6
Loss after 13999449 batches: 0.0542
trigger times: 7
Loss after 14000411 batches: 0.0551
trigger times: 8
Loss after 14001373 batches: 0.0528
trigger times: 9
Loss after 14002335 batches: 0.0520
trigger times: 10
Loss after 14003297 batches: 0.0517
trigger times: 11
Loss after 14004259 batches: 0.0518
trigger times: 12
Loss after 14005221 batches: 0.0509
trigger times: 13
Loss after 14006183 batches: 0.0508
trigger times: 14
Loss after 14007145 batches: 0.0504
trigger times: 15
Loss after 14008107 batches: 0.0493
trigger times: 16
Loss after 14009069 batches: 0.0499
trigger times: 17
Loss after 14010031 batches: 0.0482
trigger times: 18
Loss after 14010993 batches: 0.0490
trigger times: 19
Loss after 14011955 batches: 0.0494
trigger times: 20
Loss after 14012917 batches: 0.0494
trigger times: 21
Loss after 14013879 batches: 0.0487
trigger times: 22
Loss after 14014841 batches: 0.0480
trigger times: 23
Loss after 14015803 batches: 0.0477
trigger times: 24
Loss after 14016765 batches: 0.0472
trigger times: 25
Early stopping!
Start to test process.
Loss after 14017727 batches: 0.0471
Time to train on one home:  52.24431347846985
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14018690 batches: 0.0640
trigger times: 1
Loss after 14019653 batches: 0.0247
trigger times: 2
Loss after 14020616 batches: 0.0190
trigger times: 3
Loss after 14021579 batches: 0.0161
trigger times: 4
Loss after 14022542 batches: 0.0152
trigger times: 5
Loss after 14023505 batches: 0.0147
trigger times: 6
Loss after 14024468 batches: 0.0139
trigger times: 7
Loss after 14025431 batches: 0.0135
trigger times: 8
Loss after 14026394 batches: 0.0133
trigger times: 9
Loss after 14027357 batches: 0.0130
trigger times: 10
Loss after 14028320 batches: 0.0130
trigger times: 11
Loss after 14029283 batches: 0.0130
trigger times: 12
Loss after 14030246 batches: 0.0129
trigger times: 13
Loss after 14031209 batches: 0.0127
trigger times: 14
Loss after 14032172 batches: 0.0129
trigger times: 15
Loss after 14033135 batches: 0.0123
trigger times: 16
Loss after 14034098 batches: 0.0124
trigger times: 17
Loss after 14035061 batches: 0.0125
trigger times: 18
Loss after 14036024 batches: 0.0122
trigger times: 19
Loss after 14036987 batches: 0.0124
trigger times: 20
Loss after 14037950 batches: 0.0121
trigger times: 21
Loss after 14038913 batches: 0.0125
trigger times: 22
Loss after 14039876 batches: 0.0125
trigger times: 23
Loss after 14040839 batches: 0.0120
trigger times: 24
Loss after 14041802 batches: 0.0118
trigger times: 25
Early stopping!
Start to test process.
Loss after 14042765 batches: 0.0117
Time to train on one home:  52.72911238670349
trigger times: 0
Loss after 14043728 batches: 0.0543
trigger times: 1
Loss after 14044691 batches: 0.0412
trigger times: 2
Loss after 14045654 batches: 0.0378
trigger times: 3
Loss after 14046617 batches: 0.0335
trigger times: 4
Loss after 14047580 batches: 0.0315
trigger times: 5
Loss after 14048543 batches: 0.0298
trigger times: 6
Loss after 14049506 batches: 0.0313
trigger times: 7
Loss after 14050469 batches: 0.0294
trigger times: 8
Loss after 14051432 batches: 0.0292
trigger times: 9
Loss after 14052395 batches: 0.0278
trigger times: 10
Loss after 14053358 batches: 0.0288
trigger times: 11
Loss after 14054321 batches: 0.0280
trigger times: 12
Loss after 14055284 batches: 0.0278
trigger times: 13
Loss after 14056247 batches: 0.0287
trigger times: 14
Loss after 14057210 batches: 0.0271
trigger times: 15
Loss after 14058173 batches: 0.0284
trigger times: 16
Loss after 14059136 batches: 0.0269
trigger times: 17
Loss after 14060099 batches: 0.0283
trigger times: 18
Loss after 14061062 batches: 0.0277
trigger times: 19
Loss after 14062025 batches: 0.0277
trigger times: 20
Loss after 14062988 batches: 0.0261
trigger times: 21
Loss after 14063951 batches: 0.0276
trigger times: 22
Loss after 14064914 batches: 0.0266
trigger times: 23
Loss after 14065877 batches: 0.0270
trigger times: 24
Loss after 14066840 batches: 0.0270
trigger times: 25
Early stopping!
Start to test process.
Loss after 14067803 batches: 0.0257
Time to train on one home:  52.07933020591736
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14068766 batches: 0.0613
trigger times: 1
Loss after 14069729 batches: 0.0502
trigger times: 2
Loss after 14070692 batches: 0.0477
trigger times: 3
Loss after 14071655 batches: 0.0414
trigger times: 4
Loss after 14072618 batches: 0.0358
trigger times: 5
Loss after 14073581 batches: 0.0330
trigger times: 6
Loss after 14074544 batches: 0.0307
trigger times: 7
Loss after 14075507 batches: 0.0299
trigger times: 8
Loss after 14076470 batches: 0.0291
trigger times: 9
Loss after 14077433 batches: 0.0280
trigger times: 10
Loss after 14078396 batches: 0.0277
trigger times: 11
Loss after 14079359 batches: 0.0263
trigger times: 12
Loss after 14080322 batches: 0.0262
trigger times: 13
Loss after 14081285 batches: 0.0259
trigger times: 14
Loss after 14082248 batches: 0.0253
trigger times: 15
Loss after 14083211 batches: 0.0246
trigger times: 16
Loss after 14084174 batches: 0.0242
trigger times: 17
Loss after 14085137 batches: 0.0242
trigger times: 18
Loss after 14086100 batches: 0.0231
trigger times: 19
Loss after 14087063 batches: 0.0229
trigger times: 20
Loss after 14088026 batches: 0.0230
trigger times: 21
Loss after 14088989 batches: 0.0233
trigger times: 22
Loss after 14089952 batches: 0.0239
trigger times: 23
Loss after 14090915 batches: 0.0235
trigger times: 24
Loss after 14091878 batches: 0.0231
trigger times: 25
Early stopping!
Start to test process.
Loss after 14092841 batches: 0.0231
Time to train on one home:  52.22522974014282
trigger times: 0
Loss after 14093804 batches: 0.0540
trigger times: 1
Loss after 14094767 batches: 0.0413
trigger times: 2
Loss after 14095730 batches: 0.0381
trigger times: 3
Loss after 14096693 batches: 0.0349
trigger times: 4
Loss after 14097656 batches: 0.0322
trigger times: 5
Loss after 14098619 batches: 0.0312
trigger times: 6
Loss after 14099582 batches: 0.0296
trigger times: 7
Loss after 14100545 batches: 0.0280
trigger times: 8
Loss after 14101508 batches: 0.0286
trigger times: 9
Loss after 14102471 batches: 0.0281
trigger times: 10
Loss after 14103434 batches: 0.0290
trigger times: 11
Loss after 14104397 batches: 0.0290
trigger times: 12
Loss after 14105360 batches: 0.0292
trigger times: 13
Loss after 14106323 batches: 0.0283
trigger times: 14
Loss after 14107286 batches: 0.0278
trigger times: 15
Loss after 14108249 batches: 0.0296
trigger times: 16
Loss after 14109212 batches: 0.0279
trigger times: 17
Loss after 14110175 batches: 0.0262
trigger times: 18
Loss after 14111138 batches: 0.0276
trigger times: 19
Loss after 14112101 batches: 0.0270
trigger times: 20
Loss after 14113064 batches: 0.0277
trigger times: 21
Loss after 14114027 batches: 0.0263
trigger times: 22
Loss after 14114990 batches: 0.0262
trigger times: 23
Loss after 14115953 batches: 0.0262
trigger times: 24
Loss after 14116916 batches: 0.0250
trigger times: 25
Early stopping!
Start to test process.
Loss after 14117879 batches: 0.0250
Time to train on one home:  52.26622200012207
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14118842 batches: 0.0504
trigger times: 1
Loss after 14119805 batches: 0.0457
trigger times: 2
Loss after 14120768 batches: 0.0417
trigger times: 3
Loss after 14121731 batches: 0.0399
trigger times: 0
Loss after 14122694 batches: 0.0382
trigger times: 1
Loss after 14123657 batches: 0.0354
trigger times: 2
Loss after 14124620 batches: 0.0345
trigger times: 3
Loss after 14125583 batches: 0.0343
trigger times: 4
Loss after 14126546 batches: 0.0341
trigger times: 5
Loss after 14127509 batches: 0.0338
trigger times: 6
Loss after 14128472 batches: 0.0344
trigger times: 7
Loss after 14129435 batches: 0.0316
trigger times: 8
Loss after 14130398 batches: 0.0309
trigger times: 9
Loss after 14131361 batches: 0.0301
trigger times: 10
Loss after 14132324 batches: 0.0300
trigger times: 11
Loss after 14133287 batches: 0.0293
trigger times: 12
Loss after 14134250 batches: 0.0291
trigger times: 13
Loss after 14135213 batches: 0.0297
trigger times: 14
Loss after 14136176 batches: 0.0306
trigger times: 15
Loss after 14137139 batches: 0.0293
trigger times: 16
Loss after 14138102 batches: 0.0286
trigger times: 0
Loss after 14139065 batches: 0.0281
trigger times: 1
Loss after 14140028 batches: 0.0273
trigger times: 2
Loss after 14140991 batches: 0.0267
trigger times: 3
Loss after 14141954 batches: 0.0263
trigger times: 4
Loss after 14142917 batches: 0.0269
trigger times: 5
Loss after 14143880 batches: 0.0272
trigger times: 6
Loss after 14144843 batches: 0.0258
trigger times: 7
Loss after 14145806 batches: 0.0256
trigger times: 8
Loss after 14146769 batches: 0.0267
trigger times: 9
Loss after 14147732 batches: 0.0265
trigger times: 10
Loss after 14148695 batches: 0.0264
trigger times: 11
Loss after 14149658 batches: 0.0269
trigger times: 12
Loss after 14150621 batches: 0.0262
trigger times: 13
Loss after 14151584 batches: 0.0270
trigger times: 14
Loss after 14152547 batches: 0.0253
trigger times: 15
Loss after 14153510 batches: 0.0256
trigger times: 16
Loss after 14154473 batches: 0.0259
trigger times: 17
Loss after 14155436 batches: 0.0243
trigger times: 18
Loss after 14156399 batches: 0.0243
trigger times: 19
Loss after 14157362 batches: 0.0247
trigger times: 20
Loss after 14158325 batches: 0.0245
trigger times: 21
Loss after 14159288 batches: 0.0246
trigger times: 22
Loss after 14160251 batches: 0.0236
trigger times: 23
Loss after 14161214 batches: 0.0235
trigger times: 24
Loss after 14162177 batches: 0.0236
trigger times: 25
Early stopping!
Start to test process.
Loss after 14163140 batches: 0.0235
Time to train on one home:  67.9804904460907
trigger times: 0
Loss after 14164103 batches: 0.0703
trigger times: 1
Loss after 14165066 batches: 0.0462
trigger times: 2
Loss after 14166029 batches: 0.0461
trigger times: 3
Loss after 14166992 batches: 0.0438
trigger times: 4
Loss after 14167955 batches: 0.0416
trigger times: 5
Loss after 14168918 batches: 0.0396
trigger times: 6
Loss after 14169881 batches: 0.0386
trigger times: 7
Loss after 14170844 batches: 0.0375
trigger times: 8
Loss after 14171807 batches: 0.0372
trigger times: 9
Loss after 14172770 batches: 0.0368
trigger times: 10
Loss after 14173733 batches: 0.0361
trigger times: 11
Loss after 14174696 batches: 0.0361
trigger times: 12
Loss after 14175659 batches: 0.0357
trigger times: 13
Loss after 14176622 batches: 0.0353
trigger times: 14
Loss after 14177585 batches: 0.0346
trigger times: 15
Loss after 14178548 batches: 0.0349
trigger times: 16
Loss after 14179511 batches: 0.0352
trigger times: 17
Loss after 14180474 batches: 0.0343
trigger times: 18
Loss after 14181437 batches: 0.0341
trigger times: 19
Loss after 14182400 batches: 0.0347
trigger times: 20
Loss after 14183363 batches: 0.0344
trigger times: 21
Loss after 14184326 batches: 0.0337
trigger times: 22
Loss after 14185289 batches: 0.0337
trigger times: 23
Loss after 14186252 batches: 0.0333
trigger times: 24
Loss after 14187215 batches: 0.0332
trigger times: 25
Early stopping!
Start to test process.
Loss after 14188178 batches: 0.0339
Time to train on one home:  52.513150215148926
trigger times: 0
Loss after 14189141 batches: 0.0968
trigger times: 1
Loss after 14190104 batches: 0.0892
trigger times: 2
Loss after 14191067 batches: 0.0814
trigger times: 3
Loss after 14192030 batches: 0.0774
trigger times: 4
Loss after 14192993 batches: 0.0743
trigger times: 5
Loss after 14193956 batches: 0.0728
trigger times: 6
Loss after 14194919 batches: 0.0698
trigger times: 7
Loss after 14195882 batches: 0.0670
trigger times: 8
Loss after 14196845 batches: 0.0659
trigger times: 9
Loss after 14197808 batches: 0.0657
trigger times: 10
Loss after 14198771 batches: 0.0656
trigger times: 11
Loss after 14199734 batches: 0.0664
trigger times: 12
Loss after 14200697 batches: 0.0637
trigger times: 13
Loss after 14201660 batches: 0.0638
trigger times: 14
Loss after 14202623 batches: 0.0624
trigger times: 15
Loss after 14203586 batches: 0.0624
trigger times: 16
Loss after 14204549 batches: 0.0595
trigger times: 17
Loss after 14205512 batches: 0.0595
trigger times: 18
Loss after 14206475 batches: 0.0582
trigger times: 19
Loss after 14207438 batches: 0.0588
trigger times: 20
Loss after 14208401 batches: 0.0586
trigger times: 21
Loss after 14209364 batches: 0.0565
trigger times: 22
Loss after 14210327 batches: 0.0576
trigger times: 23
Loss after 14211290 batches: 0.0562
trigger times: 24
Loss after 14212253 batches: 0.0569
trigger times: 25
Early stopping!
Start to test process.
Loss after 14213216 batches: 0.0568
Time to train on one home:  52.27419304847717
trigger times: 0
Loss after 14214179 batches: 0.1020
trigger times: 1
Loss after 14215142 batches: 0.0640
trigger times: 2
Loss after 14216105 batches: 0.0579
trigger times: 3
Loss after 14217068 batches: 0.0506
trigger times: 4
Loss after 14218031 batches: 0.0487
trigger times: 5
Loss after 14218994 batches: 0.0456
trigger times: 6
Loss after 14219957 batches: 0.0443
trigger times: 7
Loss after 14220920 batches: 0.0428
trigger times: 8
Loss after 14221883 batches: 0.0413
trigger times: 9
Loss after 14222846 batches: 0.0407
trigger times: 10
Loss after 14223809 batches: 0.0407
trigger times: 11
Loss after 14224772 batches: 0.0401
trigger times: 12
Loss after 14225735 batches: 0.0389
trigger times: 13
Loss after 14226698 batches: 0.0390
trigger times: 14
Loss after 14227661 batches: 0.0382
trigger times: 15
Loss after 14228624 batches: 0.0380
trigger times: 16
Loss after 14229587 batches: 0.0379
trigger times: 17
Loss after 14230550 batches: 0.0377
trigger times: 18
Loss after 14231513 batches: 0.0389
trigger times: 19
Loss after 14232476 batches: 0.0380
trigger times: 20
Loss after 14233439 batches: 0.0373
trigger times: 21
Loss after 14234402 batches: 0.0373
trigger times: 22
Loss after 14235365 batches: 0.0363
trigger times: 23
Loss after 14236328 batches: 0.0369
trigger times: 24
Loss after 14237291 batches: 0.0365
trigger times: 25
Early stopping!
Start to test process.
Loss after 14238254 batches: 0.0352
Time to train on one home:  52.35716724395752
trigger times: 0
Loss after 14239183 batches: 0.1156
trigger times: 1
Loss after 14240112 batches: 0.0665
trigger times: 2
Loss after 14241041 batches: 0.0505
trigger times: 3
Loss after 14241970 batches: 0.0453
trigger times: 4
Loss after 14242899 batches: 0.0399
trigger times: 5
Loss after 14243828 batches: 0.0370
trigger times: 6
Loss after 14244757 batches: 0.0365
trigger times: 7
Loss after 14245686 batches: 0.0317
trigger times: 8
Loss after 14246615 batches: 0.0298
trigger times: 9
Loss after 14247544 batches: 0.0303
trigger times: 10
Loss after 14248473 batches: 0.0314
trigger times: 11
Loss after 14249402 batches: 0.0293
trigger times: 12
Loss after 14250331 batches: 0.0293
trigger times: 13
Loss after 14251260 batches: 0.0273
trigger times: 14
Loss after 14252189 batches: 0.0273
trigger times: 15
Loss after 14253118 batches: 0.0271
trigger times: 16
Loss after 14254047 batches: 0.0251
trigger times: 17
Loss after 14254976 batches: 0.0279
trigger times: 18
Loss after 14255905 batches: 0.0246
trigger times: 19
Loss after 14256834 batches: 0.0237
trigger times: 20
Loss after 14257763 batches: 0.0259
trigger times: 21
Loss after 14258692 batches: 0.0257
trigger times: 22
Loss after 14259621 batches: 0.0250
trigger times: 23
Loss after 14260550 batches: 0.0255
trigger times: 24
Loss after 14261479 batches: 0.0252
trigger times: 25
Early stopping!
Start to test process.
Loss after 14262408 batches: 0.0244
Time to train on one home:  51.906301975250244
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14263371 batches: 0.0592
trigger times: 1
Loss after 14264334 batches: 0.0293
trigger times: 2
Loss after 14265297 batches: 0.0273
trigger times: 3
Loss after 14266260 batches: 0.0273
trigger times: 4
Loss after 14267223 batches: 0.0266
trigger times: 5
Loss after 14268186 batches: 0.0259
trigger times: 6
Loss after 14269149 batches: 0.0243
trigger times: 7
Loss after 14270112 batches: 0.0232
trigger times: 8
Loss after 14271075 batches: 0.0223
trigger times: 9
Loss after 14272038 batches: 0.0216
trigger times: 10
Loss after 14273001 batches: 0.0210
trigger times: 11
Loss after 14273964 batches: 0.0203
trigger times: 12
Loss after 14274927 batches: 0.0197
trigger times: 13
Loss after 14275890 batches: 0.0195
trigger times: 14
Loss after 14276853 batches: 0.0190
trigger times: 15
Loss after 14277816 batches: 0.0187
trigger times: 16
Loss after 14278779 batches: 0.0183
trigger times: 17
Loss after 14279742 batches: 0.0186
trigger times: 18
Loss after 14280705 batches: 0.0183
trigger times: 19
Loss after 14281668 batches: 0.0181
trigger times: 20
Loss after 14282631 batches: 0.0181
trigger times: 21
Loss after 14283594 batches: 0.0179
trigger times: 22
Loss after 14284557 batches: 0.0179
trigger times: 23
Loss after 14285520 batches: 0.0177
trigger times: 24
Loss after 14286483 batches: 0.0173
trigger times: 25
Early stopping!
Start to test process.
Loss after 14287446 batches: 0.0174
Time to train on one home:  52.18719959259033
trigger times: 0
Loss after 14288409 batches: 0.1657
trigger times: 1
Loss after 14289372 batches: 0.1185
trigger times: 2
Loss after 14290335 batches: 0.1015
trigger times: 3
Loss after 14291298 batches: 0.0854
trigger times: 4
Loss after 14292261 batches: 0.0785
trigger times: 5
Loss after 14293224 batches: 0.0717
trigger times: 6
Loss after 14294187 batches: 0.0670
trigger times: 7
Loss after 14295150 batches: 0.0638
trigger times: 8
Loss after 14296113 batches: 0.0608
trigger times: 9
Loss after 14297076 batches: 0.0566
trigger times: 10
Loss after 14298039 batches: 0.0558
trigger times: 11
Loss after 14299002 batches: 0.0528
trigger times: 12
Loss after 14299965 batches: 0.0523
trigger times: 13
Loss after 14300928 batches: 0.0502
trigger times: 14
Loss after 14301891 batches: 0.0481
trigger times: 15
Loss after 14302854 batches: 0.0482
trigger times: 16
Loss after 14303817 batches: 0.0465
trigger times: 17
Loss after 14304780 batches: 0.0469
trigger times: 18
Loss after 14305743 batches: 0.0465
trigger times: 19
Loss after 14306706 batches: 0.0462
trigger times: 20
Loss after 14307669 batches: 0.0442
trigger times: 21
Loss after 14308632 batches: 0.0438
trigger times: 22
Loss after 14309595 batches: 0.0449
trigger times: 23
Loss after 14310558 batches: 0.0440
trigger times: 24
Loss after 14311521 batches: 0.0431
trigger times: 25
Early stopping!
Start to test process.
Loss after 14312484 batches: 0.0434
Time to train on one home:  52.20093774795532
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14313447 batches: 0.0786
trigger times: 1
Loss after 14314410 batches: 0.0718
trigger times: 2
Loss after 14315373 batches: 0.0675
trigger times: 3
Loss after 14316336 batches: 0.0648
trigger times: 4
Loss after 14317299 batches: 0.0627
trigger times: 5
Loss after 14318262 batches: 0.0621
trigger times: 6
Loss after 14319225 batches: 0.0619
trigger times: 7
Loss after 14320188 batches: 0.0598
trigger times: 8
Loss after 14321151 batches: 0.0576
trigger times: 9
Loss after 14322114 batches: 0.0573
trigger times: 10
Loss after 14323077 batches: 0.0566
trigger times: 11
Loss after 14324040 batches: 0.0563
trigger times: 12
Loss after 14325003 batches: 0.0556
trigger times: 13
Loss after 14325966 batches: 0.0560
trigger times: 14
Loss after 14326929 batches: 0.0552
trigger times: 15
Loss after 14327892 batches: 0.0556
trigger times: 16
Loss after 14328855 batches: 0.0530
trigger times: 17
Loss after 14329818 batches: 0.0549
trigger times: 18
Loss after 14330781 batches: 0.0533
trigger times: 19
Loss after 14331744 batches: 0.0523
trigger times: 20
Loss after 14332707 batches: 0.0532
trigger times: 21
Loss after 14333670 batches: 0.0536
trigger times: 22
Loss after 14334633 batches: 0.0528
trigger times: 23
Loss after 14335596 batches: 0.0519
trigger times: 24
Loss after 14336559 batches: 0.0519
trigger times: 25
Early stopping!
Start to test process.
Loss after 14337522 batches: 0.0525
Time to train on one home:  52.34793448448181
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14338485 batches: 0.0825
trigger times: 1
Loss after 14339448 batches: 0.0744
trigger times: 2
Loss after 14340411 batches: 0.0724
trigger times: 3
Loss after 14341374 batches: 0.0690
trigger times: 4
Loss after 14342337 batches: 0.0634
trigger times: 5
Loss after 14343300 batches: 0.0619
trigger times: 6
Loss after 14344263 batches: 0.0599
trigger times: 7
Loss after 14345226 batches: 0.0584
trigger times: 8
Loss after 14346189 batches: 0.0541
trigger times: 9
Loss after 14347152 batches: 0.0530
trigger times: 10
Loss after 14348115 batches: 0.0519
trigger times: 11
Loss after 14349078 batches: 0.0532
trigger times: 12
Loss after 14350041 batches: 0.0523
trigger times: 13
Loss after 14351004 batches: 0.0509
trigger times: 14
Loss after 14351967 batches: 0.0519
trigger times: 15
Loss after 14352930 batches: 0.0491
trigger times: 16
Loss after 14353893 batches: 0.0504
trigger times: 17
Loss after 14354856 batches: 0.0516
trigger times: 18
Loss after 14355819 batches: 0.0496
trigger times: 19
Loss after 14356782 batches: 0.0492
trigger times: 20
Loss after 14357745 batches: 0.0473
trigger times: 21
Loss after 14358708 batches: 0.0470
trigger times: 22
Loss after 14359671 batches: 0.0452
trigger times: 23
Loss after 14360634 batches: 0.0459
trigger times: 24
Loss after 14361597 batches: 0.0470
trigger times: 25
Early stopping!
Start to test process.
Loss after 14362560 batches: 0.0485
Time to train on one home:  52.0290641784668
trigger times: 0
Loss after 14363523 batches: 0.0219
trigger times: 1
Loss after 14364486 batches: 0.0185
trigger times: 2
Loss after 14365449 batches: 0.0172
trigger times: 3
Loss after 14366412 batches: 0.0159
trigger times: 4
Loss after 14367375 batches: 0.0149
trigger times: 5
Loss after 14368338 batches: 0.0145
trigger times: 6
Loss after 14369301 batches: 0.0140
trigger times: 7
Loss after 14370264 batches: 0.0137
trigger times: 8
Loss after 14371227 batches: 0.0138
trigger times: 9
Loss after 14372190 batches: 0.0135
trigger times: 10
Loss after 14373153 batches: 0.0126
trigger times: 11
Loss after 14374116 batches: 0.0132
trigger times: 12
Loss after 14375079 batches: 0.0126
trigger times: 13
Loss after 14376042 batches: 0.0121
trigger times: 14
Loss after 14377005 batches: 0.0122
trigger times: 15
Loss after 14377968 batches: 0.0127
trigger times: 16
Loss after 14378931 batches: 0.0120
trigger times: 17
Loss after 14379894 batches: 0.0122
trigger times: 18
Loss after 14380857 batches: 0.0116
trigger times: 19
Loss after 14381820 batches: 0.0113
trigger times: 20
Loss after 14382783 batches: 0.0114
trigger times: 21
Loss after 14383746 batches: 0.0114
trigger times: 22
Loss after 14384709 batches: 0.0111
trigger times: 23
Loss after 14385672 batches: 0.0117
trigger times: 24
Loss after 14386635 batches: 0.0112
trigger times: 25
Early stopping!
Start to test process.
Loss after 14387598 batches: 0.0148
Time to train on one home:  52.173051595687866
trigger times: 0
Loss after 14388561 batches: 0.0423
trigger times: 1
Loss after 14389524 batches: 0.0341
trigger times: 2
Loss after 14390487 batches: 0.0307
trigger times: 3
Loss after 14391450 batches: 0.0289
trigger times: 4
Loss after 14392413 batches: 0.0267
trigger times: 5
Loss after 14393376 batches: 0.0251
trigger times: 6
Loss after 14394339 batches: 0.0236
trigger times: 7
Loss after 14395302 batches: 0.0230
trigger times: 8
Loss after 14396265 batches: 0.0228
trigger times: 9
Loss after 14397228 batches: 0.0222
trigger times: 10
Loss after 14398191 batches: 0.0218
trigger times: 11
Loss after 14399154 batches: 0.0231
trigger times: 12
Loss after 14400117 batches: 0.0225
trigger times: 13
Loss after 14401080 batches: 0.0214
trigger times: 14
Loss after 14402043 batches: 0.0214
trigger times: 15
Loss after 14403006 batches: 0.0209
trigger times: 16
Loss after 14403969 batches: 0.0208
trigger times: 17
Loss after 14404932 batches: 0.0213
trigger times: 18
Loss after 14405895 batches: 0.0207
trigger times: 19
Loss after 14406858 batches: 0.0205
trigger times: 20
Loss after 14407821 batches: 0.0199
trigger times: 21
Loss after 14408784 batches: 0.0191
trigger times: 22
Loss after 14409747 batches: 0.0191
trigger times: 23
Loss after 14410710 batches: 0.0187
trigger times: 24
Loss after 14411673 batches: 0.0192
trigger times: 25
Early stopping!
Start to test process.
Loss after 14412636 batches: 0.0191
Time to train on one home:  52.07112193107605
trigger times: 0
Loss after 14413599 batches: 0.0896
trigger times: 1
Loss after 14414562 batches: 0.0637
trigger times: 2
Loss after 14415525 batches: 0.0561
trigger times: 3
Loss after 14416488 batches: 0.0504
trigger times: 4
Loss after 14417451 batches: 0.0475
trigger times: 5
Loss after 14418414 batches: 0.0458
trigger times: 6
Loss after 14419377 batches: 0.0435
trigger times: 7
Loss after 14420340 batches: 0.0435
trigger times: 8
Loss after 14421303 batches: 0.0422
trigger times: 9
Loss after 14422266 batches: 0.0409
trigger times: 10
Loss after 14423229 batches: 0.0405
trigger times: 11
Loss after 14424192 batches: 0.0406
trigger times: 12
Loss after 14425155 batches: 0.0403
trigger times: 13
Loss after 14426118 batches: 0.0398
trigger times: 14
Loss after 14427081 batches: 0.0389
trigger times: 15
Loss after 14428044 batches: 0.0388
trigger times: 16
Loss after 14429007 batches: 0.0382
trigger times: 17
Loss after 14429970 batches: 0.0372
trigger times: 18
Loss after 14430933 batches: 0.0360
trigger times: 19
Loss after 14431896 batches: 0.0369
trigger times: 20
Loss after 14432859 batches: 0.0397
trigger times: 21
Loss after 14433822 batches: 0.0385
trigger times: 22
Loss after 14434785 batches: 0.0370
trigger times: 23
Loss after 14435748 batches: 0.0357
trigger times: 24
Loss after 14436711 batches: 0.0339
trigger times: 25
Early stopping!
Start to test process.
Loss after 14437674 batches: 0.0339
Time to train on one home:  52.81591773033142
trigger times: 0
Loss after 14438637 batches: 0.1015
trigger times: 1
Loss after 14439600 batches: 0.0630
trigger times: 2
Loss after 14440563 batches: 0.0576
trigger times: 3
Loss after 14441526 batches: 0.0512
trigger times: 4
Loss after 14442489 batches: 0.0489
trigger times: 5
Loss after 14443452 batches: 0.0452
trigger times: 6
Loss after 14444415 batches: 0.0440
trigger times: 7
Loss after 14445378 batches: 0.0427
trigger times: 8
Loss after 14446341 batches: 0.0406
trigger times: 9
Loss after 14447304 batches: 0.0405
trigger times: 10
Loss after 14448267 batches: 0.0393
trigger times: 11
Loss after 14449230 batches: 0.0386
trigger times: 12
Loss after 14450193 batches: 0.0383
trigger times: 13
Loss after 14451156 batches: 0.0381
trigger times: 14
Loss after 14452119 batches: 0.0383
trigger times: 15
Loss after 14453082 batches: 0.0378
trigger times: 16
Loss after 14454045 batches: 0.0374
trigger times: 17
Loss after 14455008 batches: 0.0374
trigger times: 18
Loss after 14455971 batches: 0.0376
trigger times: 19
Loss after 14456934 batches: 0.0372
trigger times: 20
Loss after 14457897 batches: 0.0367
trigger times: 21
Loss after 14458860 batches: 0.0361
trigger times: 22
Loss after 14459823 batches: 0.0359
trigger times: 23
Loss after 14460786 batches: 0.0358
trigger times: 24
Loss after 14461749 batches: 0.0367
trigger times: 25
Early stopping!
Start to test process.
Loss after 14462712 batches: 0.0351
Time to train on one home:  52.1720986366272
trigger times: 0
Loss after 14463675 batches: 0.0540
trigger times: 1
Loss after 14464638 batches: 0.0416
trigger times: 2
Loss after 14465601 batches: 0.0373
trigger times: 3
Loss after 14466564 batches: 0.0343
trigger times: 4
Loss after 14467527 batches: 0.0317
trigger times: 5
Loss after 14468490 batches: 0.0314
trigger times: 6
Loss after 14469453 batches: 0.0297
trigger times: 7
Loss after 14470416 batches: 0.0295
trigger times: 8
Loss after 14471379 batches: 0.0299
trigger times: 9
Loss after 14472342 batches: 0.0308
trigger times: 10
Loss after 14473305 batches: 0.0296
trigger times: 11
Loss after 14474268 batches: 0.0291
trigger times: 12
Loss after 14475231 batches: 0.0285
trigger times: 13
Loss after 14476194 batches: 0.0289
trigger times: 14
Loss after 14477157 batches: 0.0288
trigger times: 15
Loss after 14478120 batches: 0.0277
trigger times: 16
Loss after 14479083 batches: 0.0285
trigger times: 17
Loss after 14480046 batches: 0.0270
trigger times: 18
Loss after 14481009 batches: 0.0269
trigger times: 19
Loss after 14481972 batches: 0.0260
trigger times: 20
Loss after 14482935 batches: 0.0264
trigger times: 21
Loss after 14483898 batches: 0.0254
trigger times: 22
Loss after 14484861 batches: 0.0257
trigger times: 23
Loss after 14485824 batches: 0.0249
trigger times: 24
Loss after 14486787 batches: 0.0252
trigger times: 25
Early stopping!
Start to test process.
Loss after 14487750 batches: 0.0240
Time to train on one home:  52.229154109954834
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14488713 batches: 0.0879
trigger times: 1
Loss after 14489676 batches: 0.0508
trigger times: 2
Loss after 14490639 batches: 0.0524
trigger times: 3
Loss after 14491602 batches: 0.0435
trigger times: 0
Loss after 14492565 batches: 0.0399
trigger times: 0
Loss after 14493528 batches: 0.0356
trigger times: 1
Loss after 14494491 batches: 0.0340
trigger times: 2
Loss after 14495454 batches: 0.0331
trigger times: 3
Loss after 14496417 batches: 0.0313
trigger times: 4
Loss after 14497380 batches: 0.0295
trigger times: 5
Loss after 14498343 batches: 0.0304
trigger times: 6
Loss after 14499306 batches: 0.0287
trigger times: 7
Loss after 14500269 batches: 0.0294
trigger times: 8
Loss after 14501232 batches: 0.0273
trigger times: 9
Loss after 14502195 batches: 0.0269
trigger times: 10
Loss after 14503158 batches: 0.0287
trigger times: 11
Loss after 14504121 batches: 0.0280
trigger times: 12
Loss after 14505084 batches: 0.0259
trigger times: 13
Loss after 14506047 batches: 0.0260
trigger times: 14
Loss after 14507010 batches: 0.0262
trigger times: 15
Loss after 14507973 batches: 0.0260
trigger times: 16
Loss after 14508936 batches: 0.0243
trigger times: 17
Loss after 14509899 batches: 0.0249
trigger times: 0
Loss after 14510862 batches: 0.0249
trigger times: 1
Loss after 14511825 batches: 0.0248
trigger times: 2
Loss after 14512788 batches: 0.0233
trigger times: 3
Loss after 14513751 batches: 0.0235
trigger times: 4
Loss after 14514714 batches: 0.0241
trigger times: 5
Loss after 14515677 batches: 0.0239
trigger times: 6
Loss after 14516640 batches: 0.0237
trigger times: 7
Loss after 14517603 batches: 0.0238
trigger times: 8
Loss after 14518566 batches: 0.0230
trigger times: 9
Loss after 14519529 batches: 0.0229
trigger times: 10
Loss after 14520492 batches: 0.0218
trigger times: 11
Loss after 14521455 batches: 0.0218
trigger times: 12
Loss after 14522418 batches: 0.0223
trigger times: 13
Loss after 14523381 batches: 0.0220
trigger times: 14
Loss after 14524344 batches: 0.0234
trigger times: 15
Loss after 14525307 batches: 0.0227
trigger times: 16
Loss after 14526270 batches: 0.0247
trigger times: 17
Loss after 14527233 batches: 0.0248
trigger times: 18
Loss after 14528196 batches: 0.0220
trigger times: 19
Loss after 14529159 batches: 0.0217
trigger times: 0
Loss after 14530122 batches: 0.0213
trigger times: 1
Loss after 14531085 batches: 0.0207
trigger times: 2
Loss after 14532048 batches: 0.0214
trigger times: 3
Loss after 14533011 batches: 0.0238
trigger times: 4
Loss after 14533974 batches: 0.0246
trigger times: 5
Loss after 14534937 batches: 0.0235
trigger times: 6
Loss after 14535900 batches: 0.0233
trigger times: 7
Loss after 14536863 batches: 0.0223
trigger times: 8
Loss after 14537826 batches: 0.0221
trigger times: 9
Loss after 14538789 batches: 0.0214
trigger times: 10
Loss after 14539752 batches: 0.0204
trigger times: 11
Loss after 14540715 batches: 0.0199
trigger times: 12
Loss after 14541678 batches: 0.0203
trigger times: 13
Loss after 14542641 batches: 0.0190
trigger times: 14
Loss after 14543604 batches: 0.0187
trigger times: 15
Loss after 14544567 batches: 0.0190
trigger times: 16
Loss after 14545530 batches: 0.0195
trigger times: 17
Loss after 14546493 batches: 0.0201
trigger times: 18
Loss after 14547456 batches: 0.0198
trigger times: 19
Loss after 14548419 batches: 0.0194
trigger times: 20
Loss after 14549382 batches: 0.0188
trigger times: 21
Loss after 14550345 batches: 0.0185
trigger times: 22
Loss after 14551308 batches: 0.0190
trigger times: 23
Loss after 14552271 batches: 0.0190
trigger times: 24
Loss after 14553234 batches: 0.0191
trigger times: 25
Early stopping!
Start to test process.
Loss after 14554197 batches: 0.0199
Time to train on one home:  84.8112564086914
trigger times: 0
Loss after 14555160 batches: 0.0940
trigger times: 1
Loss after 14556123 batches: 0.0782
trigger times: 2
Loss after 14557086 batches: 0.0757
trigger times: 3
Loss after 14558049 batches: 0.0739
trigger times: 4
Loss after 14559012 batches: 0.0705
trigger times: 5
Loss after 14559975 batches: 0.0677
trigger times: 6
Loss after 14560938 batches: 0.0658
trigger times: 7
Loss after 14561901 batches: 0.0640
trigger times: 8
Loss after 14562864 batches: 0.0633
trigger times: 9
Loss after 14563827 batches: 0.0620
trigger times: 10
Loss after 14564790 batches: 0.0618
trigger times: 11
Loss after 14565753 batches: 0.0606
trigger times: 12
Loss after 14566716 batches: 0.0601
trigger times: 13
Loss after 14567679 batches: 0.0604
trigger times: 14
Loss after 14568642 batches: 0.0592
trigger times: 15
Loss after 14569605 batches: 0.0596
trigger times: 16
Loss after 14570568 batches: 0.0578
trigger times: 17
Loss after 14571531 batches: 0.0565
trigger times: 18
Loss after 14572494 batches: 0.0573
trigger times: 19
Loss after 14573457 batches: 0.0575
trigger times: 20
Loss after 14574420 batches: 0.0560
trigger times: 21
Loss after 14575383 batches: 0.0558
trigger times: 22
Loss after 14576346 batches: 0.0558
trigger times: 23
Loss after 14577309 batches: 0.0554
trigger times: 24
Loss after 14578272 batches: 0.0547
trigger times: 25
Early stopping!
Start to test process.
Loss after 14579235 batches: 0.0551
Time to train on one home:  52.24615144729614
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14580198 batches: 0.0885
trigger times: 1
Loss after 14581161 batches: 0.0511
trigger times: 2
Loss after 14582124 batches: 0.0522
trigger times: 3
Loss after 14583087 batches: 0.0427
trigger times: 0
Loss after 14584050 batches: 0.0380
trigger times: 0
Loss after 14585013 batches: 0.0352
trigger times: 1
Loss after 14585976 batches: 0.0345
trigger times: 2
Loss after 14586939 batches: 0.0328
trigger times: 3
Loss after 14587902 batches: 0.0317
trigger times: 4
Loss after 14588865 batches: 0.0305
trigger times: 5
Loss after 14589828 batches: 0.0286
trigger times: 6
Loss after 14590791 batches: 0.0280
trigger times: 7
Loss after 14591754 batches: 0.0290
trigger times: 8
Loss after 14592717 batches: 0.0269
trigger times: 9
Loss after 14593680 batches: 0.0270
trigger times: 10
Loss after 14594643 batches: 0.0262
trigger times: 11
Loss after 14595606 batches: 0.0270
trigger times: 12
Loss after 14596569 batches: 0.0250
trigger times: 13
Loss after 14597532 batches: 0.0257
trigger times: 14
Loss after 14598495 batches: 0.0253
trigger times: 15
Loss after 14599458 batches: 0.0247
trigger times: 16
Loss after 14600421 batches: 0.0246
trigger times: 17
Loss after 14601384 batches: 0.0248
trigger times: 18
Loss after 14602347 batches: 0.0257
trigger times: 19
Loss after 14603310 batches: 0.0255
trigger times: 20
Loss after 14604273 batches: 0.0249
trigger times: 21
Loss after 14605236 batches: 0.0241
trigger times: 22
Loss after 14606199 batches: 0.0244
trigger times: 23
Loss after 14607162 batches: 0.0232
trigger times: 0
Loss after 14608125 batches: 0.0235
trigger times: 1
Loss after 14609088 batches: 0.0241
trigger times: 2
Loss after 14610051 batches: 0.0238
trigger times: 3
Loss after 14611014 batches: 0.0228
trigger times: 4
Loss after 14611977 batches: 0.0238
trigger times: 5
Loss after 14612940 batches: 0.0220
trigger times: 6
Loss after 14613903 batches: 0.0221
trigger times: 7
Loss after 14614866 batches: 0.0215
trigger times: 8
Loss after 14615829 batches: 0.0216
trigger times: 9
Loss after 14616792 batches: 0.0227
trigger times: 10
Loss after 14617755 batches: 0.0217
trigger times: 11
Loss after 14618718 batches: 0.0216
trigger times: 12
Loss after 14619681 batches: 0.0211
trigger times: 13
Loss after 14620644 batches: 0.0218
trigger times: 14
Loss after 14621607 batches: 0.0229
trigger times: 15
Loss after 14622570 batches: 0.0222
trigger times: 16
Loss after 14623533 batches: 0.0219
trigger times: 17
Loss after 14624496 batches: 0.0212
trigger times: 18
Loss after 14625459 batches: 0.0208
trigger times: 19
Loss after 14626422 batches: 0.0211
trigger times: 20
Loss after 14627385 batches: 0.0208
trigger times: 21
Loss after 14628348 batches: 0.0195
trigger times: 22
Loss after 14629311 batches: 0.0201
trigger times: 23
Loss after 14630274 batches: 0.0226
trigger times: 24
Loss after 14631237 batches: 0.0215
trigger times: 25
Early stopping!
Start to test process.
Loss after 14632200 batches: 0.0220
Time to train on one home:  74.47841739654541
trigger times: 0
Loss after 14633095 batches: 0.0763
trigger times: 1
Loss after 14633990 batches: 0.0451
trigger times: 2
Loss after 14634885 batches: 0.0219
trigger times: 3
Loss after 14635780 batches: 0.0119
trigger times: 4
Loss after 14636675 batches: 0.0111
trigger times: 5
Loss after 14637570 batches: 0.0078
trigger times: 6
Loss after 14638465 batches: 0.0064
trigger times: 7
Loss after 14639360 batches: 0.0052
trigger times: 8
Loss after 14640255 batches: 0.0060
trigger times: 9
Loss after 14641150 batches: 0.0057
trigger times: 10
Loss after 14642045 batches: 0.0046
trigger times: 11
Loss after 14642940 batches: 0.0042
trigger times: 12
Loss after 14643835 batches: 0.0043
trigger times: 13
Loss after 14644730 batches: 0.0039
trigger times: 14
Loss after 14645625 batches: 0.0035
trigger times: 15
Loss after 14646520 batches: 0.0033
trigger times: 16
Loss after 14647415 batches: 0.0038
trigger times: 17
Loss after 14648310 batches: 0.0067
trigger times: 18
Loss after 14649205 batches: 0.0054
trigger times: 19
Loss after 14650100 batches: 0.0043
trigger times: 20
Loss after 14650995 batches: 0.0040
trigger times: 21
Loss after 14651890 batches: 0.0035
trigger times: 22
Loss after 14652785 batches: 0.0035
trigger times: 23
Loss after 14653680 batches: 0.0033
trigger times: 24
Loss after 14654575 batches: 0.0028
trigger times: 25
Early stopping!
Start to test process.
Loss after 14655470 batches: 0.0028
Time to train on one home:  51.482417583465576
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14656407 batches: 0.0807
trigger times: 1
Loss after 14657344 batches: 0.0674
trigger times: 2
Loss after 14658281 batches: 0.0643
trigger times: 3
Loss after 14659218 batches: 0.0619
trigger times: 4
Loss after 14660155 batches: 0.0580
trigger times: 5
Loss after 14661092 batches: 0.0568
trigger times: 6
Loss after 14662029 batches: 0.0549
trigger times: 7
Loss after 14662966 batches: 0.0537
trigger times: 8
Loss after 14663903 batches: 0.0533
trigger times: 9
Loss after 14664840 batches: 0.0516
trigger times: 10
Loss after 14665777 batches: 0.0509
trigger times: 11
Loss after 14666714 batches: 0.0513
trigger times: 12
Loss after 14667651 batches: 0.0516
trigger times: 13
Loss after 14668588 batches: 0.0494
trigger times: 14
Loss after 14669525 batches: 0.0491
trigger times: 15
Loss after 14670462 batches: 0.0497
trigger times: 16
Loss after 14671399 batches: 0.0497
trigger times: 17
Loss after 14672336 batches: 0.0481
trigger times: 18
Loss after 14673273 batches: 0.0485
trigger times: 19
Loss after 14674210 batches: 0.0472
trigger times: 20
Loss after 14675147 batches: 0.0495
trigger times: 21
Loss after 14676084 batches: 0.0471
trigger times: 22
Loss after 14677021 batches: 0.0476
trigger times: 23
Loss after 14677958 batches: 0.0480
trigger times: 24
Loss after 14678895 batches: 0.0479
trigger times: 25
Early stopping!
Start to test process.
Loss after 14679832 batches: 0.0468
Time to train on one home:  52.30015730857849
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066, 0.03401813118363776, 0.033508403208926535, 0.03273922320832277, 0.032162070075287874, 0.031213877056718613]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863], [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861], [0.0757279247045517, 0.10266384027132536, 0.5909740425258612, 0.8803234656976516, 0.8076583271010261, 28.206427011804568, 8296.775], [0.06181953847408295, 0.10445801892372542, 0.5961459732150404, 0.831618823847048, 0.806043429565136, 26.645882531252276, 8280.187], [0.07333928346633911, 0.09887557787656209, 0.5904951093552152, 0.8805827319948272, 0.8110679704670578, 28.214734158180708, 8331.802], [0.06249981373548508, 0.1056225345966233, 0.6023961404422763, 0.8395811685763604, 0.8049953091236983, 26.90100386358231, 8269.419]]
Round_12_results:  [0.06249981373548508, 0.1056225345966233, 0.6023961404422763, 0.8395811685763604, 0.8049953091236983, 26.90100386358231, 8269.419]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 15313 < 15314; dropping {'Training_Loss': 0.07754940965345927, 'Validation_Loss': 0.07761882245540619, 'Training_R2': -0.1273492626467687, 'Validation_R2': 0.12076878166416949, 'Training_F1': 0.40158559707428154, 'Validation_F1': 0.6106707872235313, 'Training_NEP': 0.8343960257374526, 'Validation_NEP': 0.8075470394885126, 'Training_NDE': 0.6865830393604987, 'Validation_NDE': 0.7943645299342486, 'Training_MAE': 31.70731743650097, 'Validation_MAE': 28.71578622651066, 'Training_MSE': 2535.8164, 'Validation_MSE': 10406.186}.
trigger times: 0
Loss after 14680794 batches: 0.0775
trigger times: 1
Loss after 14681756 batches: 0.0638
trigger times: 2
Loss after 14682718 batches: 0.0636
trigger times: 3
Loss after 14683680 batches: 0.0601
trigger times: 4
Loss after 14684642 batches: 0.0572
trigger times: 5
Loss after 14685604 batches: 0.0547
trigger times: 6
Loss after 14686566 batches: 0.0539
trigger times: 7
Loss after 14687528 batches: 0.0526
trigger times: 8
Loss after 14688490 batches: 0.0526
trigger times: 9
Loss after 14689452 batches: 0.0526
trigger times: 10
Loss after 14690414 batches: 0.0506
trigger times: 11
Loss after 14691376 batches: 0.0501
trigger times: 12
Loss after 14692338 batches: 0.0503
trigger times: 13
Loss after 14693300 batches: 0.0506
trigger times: 14
Loss after 14694262 batches: 0.0500
trigger times: 15
Loss after 14695224 batches: 0.0497
trigger times: 16
Loss after 14696186 batches: 0.0489
trigger times: 17
Loss after 14697148 batches: 0.0492
trigger times: 18
Loss after 14698110 batches: 0.0489
trigger times: 19
Loss after 14699072 batches: 0.0482
trigger times: 20
Loss after 14700034 batches: 0.0479
trigger times: 21
Loss after 14700996 batches: 0.0471
trigger times: 22
Loss after 14701958 batches: 0.0477
trigger times: 23
Loss after 14702920 batches: 0.0471
trigger times: 24
Loss after 14703882 batches: 0.0470
trigger times: 25
Early stopping!
Start to test process.
Loss after 14704844 batches: 0.0474
Time to train on one home:  52.24617290496826
trigger times: 0
Loss after 14705773 batches: 0.0900
trigger times: 1
Loss after 14706702 batches: 0.0638
trigger times: 2
Loss after 14707631 batches: 0.0464
trigger times: 3
Loss after 14708560 batches: 0.0409
trigger times: 4
Loss after 14709489 batches: 0.0378
trigger times: 5
Loss after 14710418 batches: 0.0344
trigger times: 6
Loss after 14711347 batches: 0.0322
trigger times: 7
Loss after 14712276 batches: 0.0295
trigger times: 8
Loss after 14713205 batches: 0.0286
trigger times: 9
Loss after 14714134 batches: 0.0265
trigger times: 10
Loss after 14715063 batches: 0.0261
trigger times: 11
Loss after 14715992 batches: 0.0276
trigger times: 12
Loss after 14716921 batches: 0.0270
trigger times: 13
Loss after 14717850 batches: 0.0265
trigger times: 14
Loss after 14718779 batches: 0.0305
trigger times: 15
Loss after 14719708 batches: 0.0304
trigger times: 16
Loss after 14720637 batches: 0.0289
trigger times: 17
Loss after 14721566 batches: 0.0259
trigger times: 18
Loss after 14722495 batches: 0.0261
trigger times: 19
Loss after 14723424 batches: 0.0256
trigger times: 20
Loss after 14724353 batches: 0.0242
trigger times: 21
Loss after 14725282 batches: 0.0300
trigger times: 22
Loss after 14726211 batches: 0.0326
trigger times: 23
Loss after 14727140 batches: 0.0300
trigger times: 24
Loss after 14728069 batches: 0.0291
trigger times: 25
Early stopping!
Start to test process.
Loss after 14728998 batches: 0.0278
Time to train on one home:  52.73502850532532
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 14729961 batches: 0.0577
trigger times: 1
Loss after 14730924 batches: 0.0193
trigger times: 2
Loss after 14731887 batches: 0.0144
trigger times: 3
Loss after 14732850 batches: 0.0141
trigger times: 4
Loss after 14733813 batches: 0.0139
trigger times: 5
Loss after 14734776 batches: 0.0133
trigger times: 6
Loss after 14735739 batches: 0.0128
trigger times: 7
Loss after 14736702 batches: 0.0121
trigger times: 8
Loss after 14737665 batches: 0.0112
trigger times: 9
Loss after 14738628 batches: 0.0105
trigger times: 10
Loss after 14739591 batches: 0.0103
trigger times: 11
Loss after 14740554 batches: 0.0097
trigger times: 12
Loss after 14741517 batches: 0.0093
trigger times: 13
Loss after 14742480 batches: 0.0089
trigger times: 14
Loss after 14743443 batches: 0.0087
trigger times: 15
Loss after 14744406 batches: 0.0084
trigger times: 16
Loss after 14745369 batches: 0.0082
trigger times: 17
Loss after 14746332 batches: 0.0081
trigger times: 18
Loss after 14747295 batches: 0.0079
trigger times: 19
Loss after 14748258 batches: 0.0077
trigger times: 20
Loss after 14749221 batches: 0.0075
trigger times: 21
Loss after 14750184 batches: 0.0072
trigger times: 22
Loss after 14751147 batches: 0.0070
trigger times: 23
Loss after 14752110 batches: 0.0071
trigger times: 24
Loss after 14753073 batches: 0.0069
trigger times: 25
Early stopping!
Start to test process.
Loss after 14754036 batches: 0.0071
Time to train on one home:  52.362173557281494
trigger times: 0
Loss after 14754999 batches: 0.0223
trigger times: 1
Loss after 14755962 batches: 0.0184
trigger times: 2
Loss after 14756925 batches: 0.0170
trigger times: 3
Loss after 14757888 batches: 0.0154
trigger times: 4
Loss after 14758851 batches: 0.0142
trigger times: 5
Loss after 14759814 batches: 0.0135
trigger times: 6
Loss after 14760777 batches: 0.0136
trigger times: 7
Loss after 14761740 batches: 0.0134
trigger times: 8
Loss after 14762703 batches: 0.0141
trigger times: 9
Loss after 14763666 batches: 0.0132
trigger times: 10
Loss after 14764629 batches: 0.0132
trigger times: 11
Loss after 14765592 batches: 0.0126
trigger times: 12
Loss after 14766555 batches: 0.0120
trigger times: 13
Loss after 14767518 batches: 0.0128
trigger times: 14
Loss after 14768481 batches: 0.0119
trigger times: 15
Loss after 14769444 batches: 0.0119
trigger times: 16
Loss after 14770407 batches: 0.0119
trigger times: 17
Loss after 14771370 batches: 0.0118
trigger times: 18
Loss after 14772333 batches: 0.0112
trigger times: 19
Loss after 14773296 batches: 0.0114
trigger times: 20
Loss after 14774259 batches: 0.0116
trigger times: 21
Loss after 14775222 batches: 0.0114
trigger times: 22
Loss after 14776185 batches: 0.0114
trigger times: 23
Loss after 14777148 batches: 0.0117
trigger times: 24
Loss after 14778111 batches: 0.0119
trigger times: 25
Early stopping!
Start to test process.
Loss after 14779074 batches: 0.0110
Time to train on one home:  52.570114850997925
trigger times: 0
Loss after 14780037 batches: 0.0942
trigger times: 1
Loss after 14781000 batches: 0.0854
trigger times: 2
Loss after 14781963 batches: 0.0821
trigger times: 3
Loss after 14782926 batches: 0.0774
trigger times: 4
Loss after 14783889 batches: 0.0741
trigger times: 5
Loss after 14784852 batches: 0.0704
trigger times: 6
Loss after 14785815 batches: 0.0676
trigger times: 7
Loss after 14786778 batches: 0.0669
trigger times: 8
Loss after 14787741 batches: 0.0647
trigger times: 9
Loss after 14788704 batches: 0.0640
trigger times: 10
Loss after 14789667 batches: 0.0634
trigger times: 11
Loss after 14790630 batches: 0.0618
trigger times: 12
Loss after 14791593 batches: 0.0612
trigger times: 13
Loss after 14792556 batches: 0.0610
trigger times: 14
Loss after 14793519 batches: 0.0594
trigger times: 15
Loss after 14794482 batches: 0.0585
trigger times: 16
Loss after 14795445 batches: 0.0581
trigger times: 17
Loss after 14796408 batches: 0.0578
trigger times: 18
Loss after 14797371 batches: 0.0584
trigger times: 19
Loss after 14798334 batches: 0.0588
trigger times: 20
Loss after 14799297 batches: 0.0599
trigger times: 21
Loss after 14800260 batches: 0.0574
trigger times: 22
Loss after 14801223 batches: 0.0562
trigger times: 23
Loss after 14802186 batches: 0.0562
trigger times: 24
Loss after 14803149 batches: 0.0571
trigger times: 25
Early stopping!
Start to test process.
Loss after 14804112 batches: 0.0579
Time to train on one home:  52.48110914230347
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14805075 batches: 0.0851
trigger times: 1
Loss after 14806038 batches: 0.0715
trigger times: 2
Loss after 14807001 batches: 0.0687
trigger times: 3
Loss after 14807964 batches: 0.0644
trigger times: 4
Loss after 14808927 batches: 0.0628
trigger times: 5
Loss after 14809890 batches: 0.0609
trigger times: 6
Loss after 14810853 batches: 0.0582
trigger times: 7
Loss after 14811816 batches: 0.0585
trigger times: 8
Loss after 14812779 batches: 0.0576
trigger times: 9
Loss after 14813742 batches: 0.0565
trigger times: 10
Loss after 14814705 batches: 0.0555
trigger times: 11
Loss after 14815668 batches: 0.0552
trigger times: 12
Loss after 14816631 batches: 0.0557
trigger times: 13
Loss after 14817594 batches: 0.0542
trigger times: 14
Loss after 14818557 batches: 0.0537
trigger times: 15
Loss after 14819520 batches: 0.0543
trigger times: 16
Loss after 14820483 batches: 0.0541
trigger times: 17
Loss after 14821446 batches: 0.0531
trigger times: 18
Loss after 14822409 batches: 0.0531
trigger times: 19
Loss after 14823372 batches: 0.0521
trigger times: 20
Loss after 14824335 batches: 0.0531
trigger times: 21
Loss after 14825298 batches: 0.0527
trigger times: 22
Loss after 14826261 batches: 0.0523
trigger times: 23
Loss after 14827224 batches: 0.0519
trigger times: 24
Loss after 14828187 batches: 0.0508
trigger times: 25
Early stopping!
Start to test process.
Loss after 14829150 batches: 0.0506
Time to train on one home:  52.54439616203308
trigger times: 0
Loss after 14830113 batches: 0.0747
trigger times: 1
Loss after 14831076 batches: 0.0676
trigger times: 2
Loss after 14832039 batches: 0.0650
trigger times: 3
Loss after 14833002 batches: 0.0624
trigger times: 4
Loss after 14833965 batches: 0.0604
trigger times: 5
Loss after 14834928 batches: 0.0588
trigger times: 6
Loss after 14835891 batches: 0.0577
trigger times: 7
Loss after 14836854 batches: 0.0564
trigger times: 8
Loss after 14837817 batches: 0.0561
trigger times: 9
Loss after 14838780 batches: 0.0561
trigger times: 10
Loss after 14839743 batches: 0.0546
trigger times: 11
Loss after 14840706 batches: 0.0534
trigger times: 12
Loss after 14841669 batches: 0.0520
trigger times: 13
Loss after 14842632 batches: 0.0531
trigger times: 14
Loss after 14843595 batches: 0.0526
trigger times: 15
Loss after 14844558 batches: 0.0514
trigger times: 16
Loss after 14845521 batches: 0.0511
trigger times: 17
Loss after 14846484 batches: 0.0501
trigger times: 18
Loss after 14847447 batches: 0.0495
trigger times: 19
Loss after 14848410 batches: 0.0488
trigger times: 20
Loss after 14849373 batches: 0.0498
trigger times: 21
Loss after 14850336 batches: 0.0483
trigger times: 22
Loss after 14851299 batches: 0.0493
trigger times: 23
Loss after 14852262 batches: 0.0494
trigger times: 24
Loss after 14853225 batches: 0.0501
trigger times: 25
Early stopping!
Start to test process.
Loss after 14854188 batches: 0.0488
Time to train on one home:  52.15688753128052
trigger times: 0
Loss after 14855151 batches: 0.0651
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 14856114 batches: 0.0497
trigger times: 2
Loss after 14857077 batches: 0.0482
trigger times: 3
Loss after 14858040 batches: 0.0432
trigger times: 4
Loss after 14859003 batches: 0.0372
trigger times: 5
Loss after 14859966 batches: 0.0338
trigger times: 6
Loss after 14860929 batches: 0.0326
trigger times: 7
Loss after 14861892 batches: 0.0306
trigger times: 8
Loss after 14862855 batches: 0.0299
trigger times: 9
Loss after 14863818 batches: 0.0286
trigger times: 10
Loss after 14864781 batches: 0.0278
trigger times: 11
Loss after 14865744 batches: 0.0268
trigger times: 12
Loss after 14866707 batches: 0.0260
trigger times: 13
Loss after 14867670 batches: 0.0252
trigger times: 14
Loss after 14868633 batches: 0.0249
trigger times: 15
Loss after 14869596 batches: 0.0252
trigger times: 16
Loss after 14870559 batches: 0.0251
trigger times: 17
Loss after 14871522 batches: 0.0245
trigger times: 18
Loss after 14872485 batches: 0.0231
trigger times: 19
Loss after 14873448 batches: 0.0230
trigger times: 20
Loss after 14874411 batches: 0.0245
trigger times: 21
Loss after 14875374 batches: 0.0240
trigger times: 22
Loss after 14876337 batches: 0.0237
trigger times: 23
Loss after 14877300 batches: 0.0228
trigger times: 24
Loss after 14878263 batches: 0.0228
trigger times: 25
Early stopping!
Start to test process.
Loss after 14879226 batches: 0.0226
Time to train on one home:  52.32068729400635
trigger times: 0
Loss after 14880184 batches: 0.0597
trigger times: 1
Loss after 14881142 batches: 0.0418
trigger times: 2
Loss after 14882100 batches: 0.0337
trigger times: 3
Loss after 14883058 batches: 0.0289
trigger times: 4
Loss after 14884016 batches: 0.0270
trigger times: 5
Loss after 14884974 batches: 0.0258
trigger times: 6
Loss after 14885932 batches: 0.0249
trigger times: 7
Loss after 14886890 batches: 0.0217
trigger times: 8
Loss after 14887848 batches: 0.0217
trigger times: 9
Loss after 14888806 batches: 0.0191
trigger times: 10
Loss after 14889764 batches: 0.0180
trigger times: 11
Loss after 14890722 batches: 0.0176
trigger times: 12
Loss after 14891680 batches: 0.0177
trigger times: 13
Loss after 14892638 batches: 0.0176
trigger times: 14
Loss after 14893596 batches: 0.0178
trigger times: 15
Loss after 14894554 batches: 0.0169
trigger times: 16
Loss after 14895512 batches: 0.0170
trigger times: 17
Loss after 14896470 batches: 0.0160
trigger times: 18
Loss after 14897428 batches: 0.0163
trigger times: 19
Loss after 14898386 batches: 0.0162
trigger times: 20
Loss after 14899344 batches: 0.0158
trigger times: 21
Loss after 14900302 batches: 0.0161
trigger times: 22
Loss after 14901260 batches: 0.0148
trigger times: 23
Loss after 14902218 batches: 0.0150
trigger times: 24
Loss after 14903176 batches: 0.0175
trigger times: 25
Early stopping!
Start to test process.
Loss after 14904134 batches: 0.0165
Time to train on one home:  52.23061394691467
trigger times: 0
Loss after 14905096 batches: 0.0782
trigger times: 1
Loss after 14906058 batches: 0.0647
trigger times: 2
Loss after 14907020 batches: 0.0631
trigger times: 3
Loss after 14907982 batches: 0.0594
trigger times: 4
Loss after 14908944 batches: 0.0571
trigger times: 5
Loss after 14909906 batches: 0.0552
trigger times: 6
Loss after 14910868 batches: 0.0544
trigger times: 7
Loss after 14911830 batches: 0.0528
trigger times: 8
Loss after 14912792 batches: 0.0528
trigger times: 9
Loss after 14913754 batches: 0.0513
trigger times: 10
Loss after 14914716 batches: 0.0509
trigger times: 11
Loss after 14915678 batches: 0.0508
trigger times: 12
Loss after 14916640 batches: 0.0500
trigger times: 13
Loss after 14917602 batches: 0.0505
trigger times: 14
Loss after 14918564 batches: 0.0497
trigger times: 15
Loss after 14919526 batches: 0.0486
trigger times: 16
Loss after 14920488 batches: 0.0488
trigger times: 17
Loss after 14921450 batches: 0.0485
trigger times: 18
Loss after 14922412 batches: 0.0484
trigger times: 19
Loss after 14923374 batches: 0.0489
trigger times: 20
Loss after 14924336 batches: 0.0488
trigger times: 21
Loss after 14925298 batches: 0.0477
trigger times: 22
Loss after 14926260 batches: 0.0472
trigger times: 23
Loss after 14927222 batches: 0.0473
trigger times: 24
Loss after 14928184 batches: 0.0471
trigger times: 25
Early stopping!
Start to test process.
Loss after 14929146 batches: 0.0471
Time to train on one home:  52.3695068359375
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 14930109 batches: 0.0990
trigger times: 1
Loss after 14931072 batches: 0.0285
trigger times: 2
Loss after 14932035 batches: 0.0206
trigger times: 3
Loss after 14932998 batches: 0.0181
trigger times: 4
Loss after 14933961 batches: 0.0166
trigger times: 5
Loss after 14934924 batches: 0.0153
trigger times: 6
Loss after 14935887 batches: 0.0145
trigger times: 7
Loss after 14936850 batches: 0.0145
trigger times: 8
Loss after 14937813 batches: 0.0136
trigger times: 9
Loss after 14938776 batches: 0.0137
trigger times: 10
Loss after 14939739 batches: 0.0133
trigger times: 11
Loss after 14940702 batches: 0.0134
trigger times: 12
Loss after 14941665 batches: 0.0130
trigger times: 13
Loss after 14942628 batches: 0.0129
trigger times: 14
Loss after 14943591 batches: 0.0126
trigger times: 15
Loss after 14944554 batches: 0.0128
trigger times: 16
Loss after 14945517 batches: 0.0130
trigger times: 17
Loss after 14946480 batches: 0.0124
trigger times: 18
Loss after 14947443 batches: 0.0124
trigger times: 19
Loss after 14948406 batches: 0.0123
trigger times: 20
Loss after 14949369 batches: 0.0123
trigger times: 21
Loss after 14950332 batches: 0.0123
trigger times: 22
Loss after 14951295 batches: 0.0126
trigger times: 23
Loss after 14952258 batches: 0.0122
trigger times: 24
Loss after 14953221 batches: 0.0121
trigger times: 25
Early stopping!
Start to test process.
Loss after 14954184 batches: 0.0119
Time to train on one home:  52.15351414680481
trigger times: 0
Loss after 14955147 batches: 0.0487
trigger times: 1
Loss after 14956110 batches: 0.0404
trigger times: 2
Loss after 14957073 batches: 0.0362
trigger times: 3
Loss after 14958036 batches: 0.0335
trigger times: 4
Loss after 14958999 batches: 0.0320
trigger times: 5
Loss after 14959962 batches: 0.0288
trigger times: 6
Loss after 14960925 batches: 0.0300
trigger times: 7
Loss after 14961888 batches: 0.0292
trigger times: 8
Loss after 14962851 batches: 0.0284
trigger times: 9
Loss after 14963814 batches: 0.0275
trigger times: 10
Loss after 14964777 batches: 0.0263
trigger times: 11
Loss after 14965740 batches: 0.0276
trigger times: 12
Loss after 14966703 batches: 0.0280
trigger times: 13
Loss after 14967666 batches: 0.0278
trigger times: 14
Loss after 14968629 batches: 0.0272
trigger times: 15
Loss after 14969592 batches: 0.0264
trigger times: 16
Loss after 14970555 batches: 0.0252
trigger times: 17
Loss after 14971518 batches: 0.0266
trigger times: 18
Loss after 14972481 batches: 0.0284
trigger times: 19
Loss after 14973444 batches: 0.0276
trigger times: 20
Loss after 14974407 batches: 0.0275
trigger times: 21
Loss after 14975370 batches: 0.0261
trigger times: 22
Loss after 14976333 batches: 0.0252
trigger times: 23
Loss after 14977296 batches: 0.0243
trigger times: 24
Loss after 14978259 batches: 0.0245
trigger times: 25
Early stopping!
Start to test process.
Loss after 14979222 batches: 0.0248
Time to train on one home:  52.28140616416931
trigger times: 0
Loss after 14980185 batches: 0.0653
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 14981148 batches: 0.0498
trigger times: 2
Loss after 14982111 batches: 0.0489
trigger times: 3
Loss after 14983074 batches: 0.0426
trigger times: 4
Loss after 14984037 batches: 0.0373
trigger times: 5
Loss after 14985000 batches: 0.0354
trigger times: 6
Loss after 14985963 batches: 0.0333
trigger times: 7
Loss after 14986926 batches: 0.0305
trigger times: 8
Loss after 14987889 batches: 0.0296
trigger times: 9
Loss after 14988852 batches: 0.0285
trigger times: 10
Loss after 14989815 batches: 0.0280
trigger times: 11
Loss after 14990778 batches: 0.0270
trigger times: 12
Loss after 14991741 batches: 0.0269
trigger times: 13
Loss after 14992704 batches: 0.0262
trigger times: 14
Loss after 14993667 batches: 0.0258
trigger times: 15
Loss after 14994630 batches: 0.0248
trigger times: 16
Loss after 14995593 batches: 0.0246
trigger times: 17
Loss after 14996556 batches: 0.0245
trigger times: 18
Loss after 14997519 batches: 0.0241
trigger times: 19
Loss after 14998482 batches: 0.0238
trigger times: 20
Loss after 14999445 batches: 0.0234
trigger times: 21
Loss after 15000408 batches: 0.0228
trigger times: 22
Loss after 15001371 batches: 0.0232
trigger times: 23
Loss after 15002334 batches: 0.0231
trigger times: 24
Loss after 15003297 batches: 0.0226
trigger times: 25
Early stopping!
Start to test process.
Loss after 15004260 batches: 0.0230
Time to train on one home:  52.38733124732971
trigger times: 0
Loss after 15005223 batches: 0.0491
trigger times: 1
Loss after 15006186 batches: 0.0394
trigger times: 2
Loss after 15007149 batches: 0.0359
trigger times: 3
Loss after 15008112 batches: 0.0327
trigger times: 4
Loss after 15009075 batches: 0.0308
trigger times: 5
Loss after 15010038 batches: 0.0303
trigger times: 6
Loss after 15011001 batches: 0.0299
trigger times: 7
Loss after 15011964 batches: 0.0294
trigger times: 8
Loss after 15012927 batches: 0.0292
trigger times: 9
Loss after 15013890 batches: 0.0302
trigger times: 10
Loss after 15014853 batches: 0.0284
trigger times: 11
Loss after 15015816 batches: 0.0272
trigger times: 12
Loss after 15016779 batches: 0.0287
trigger times: 13
Loss after 15017742 batches: 0.0279
trigger times: 14
Loss after 15018705 batches: 0.0272
trigger times: 15
Loss after 15019668 batches: 0.0272
trigger times: 16
Loss after 15020631 batches: 0.0274
trigger times: 17
Loss after 15021594 batches: 0.0275
trigger times: 18
Loss after 15022557 batches: 0.0262
trigger times: 19
Loss after 15023520 batches: 0.0260
trigger times: 20
Loss after 15024483 batches: 0.0250
trigger times: 21
Loss after 15025446 batches: 0.0249
trigger times: 22
Loss after 15026409 batches: 0.0237
trigger times: 23
Loss after 15027372 batches: 0.0248
trigger times: 24
Loss after 15028335 batches: 0.0241
trigger times: 25
Early stopping!
Start to test process.
Loss after 15029298 batches: 0.0253
Time to train on one home:  52.60722875595093
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15030261 batches: 0.0463
trigger times: 1
Loss after 15031224 batches: 0.0423
trigger times: 2
Loss after 15032187 batches: 0.0401
trigger times: 3
Loss after 15033150 batches: 0.0384
trigger times: 0
Loss after 15034113 batches: 0.0353
trigger times: 1
Loss after 15035076 batches: 0.0347
trigger times: 2
Loss after 15036039 batches: 0.0339
trigger times: 3
Loss after 15037002 batches: 0.0327
trigger times: 4
Loss after 15037965 batches: 0.0320
trigger times: 5
Loss after 15038928 batches: 0.0319
trigger times: 6
Loss after 15039891 batches: 0.0313
trigger times: 7
Loss after 15040854 batches: 0.0301
trigger times: 8
Loss after 15041817 batches: 0.0298
trigger times: 0
Loss after 15042780 batches: 0.0293
trigger times: 1
Loss after 15043743 batches: 0.0350
trigger times: 2
Loss after 15044706 batches: 0.0336
trigger times: 3
Loss after 15045669 batches: 0.0325
trigger times: 4
Loss after 15046632 batches: 0.0320
trigger times: 5
Loss after 15047595 batches: 0.0318
trigger times: 6
Loss after 15048558 batches: 0.0304
trigger times: 7
Loss after 15049521 batches: 0.0301
trigger times: 8
Loss after 15050484 batches: 0.0297
trigger times: 9
Loss after 15051447 batches: 0.0298
trigger times: 10
Loss after 15052410 batches: 0.0281
trigger times: 11
Loss after 15053373 batches: 0.0278
trigger times: 12
Loss after 15054336 batches: 0.0285
trigger times: 13
Loss after 15055299 batches: 0.0273
trigger times: 14
Loss after 15056262 batches: 0.0282
trigger times: 15
Loss after 15057225 batches: 0.0282
trigger times: 16
Loss after 15058188 batches: 0.0286
trigger times: 17
Loss after 15059151 batches: 0.0267
trigger times: 18
Loss after 15060114 batches: 0.0269
trigger times: 19
Loss after 15061077 batches: 0.0265
trigger times: 20
Loss after 15062040 batches: 0.0280
trigger times: 21
Loss after 15063003 batches: 0.0271
trigger times: 22
Loss after 15063966 batches: 0.0268
trigger times: 23
Loss after 15064929 batches: 0.0258
trigger times: 24
Loss after 15065892 batches: 0.0252
trigger times: 25
Early stopping!
Start to test process.
Loss after 15066855 batches: 0.0257
Time to train on one home:  62.089353799819946
trigger times: 0
Loss after 15067818 batches: 0.0898
trigger times: 1
Loss after 15068781 batches: 0.0496
trigger times: 2
Loss after 15069744 batches: 0.0478
trigger times: 3
Loss after 15070707 batches: 0.0469
trigger times: 4
Loss after 15071670 batches: 0.0426
trigger times: 5
Loss after 15072633 batches: 0.0407
trigger times: 6
Loss after 15073596 batches: 0.0394
trigger times: 7
Loss after 15074559 batches: 0.0386
trigger times: 8
Loss after 15075522 batches: 0.0374
trigger times: 9
Loss after 15076485 batches: 0.0372
trigger times: 10
Loss after 15077448 batches: 0.0366
trigger times: 11
Loss after 15078411 batches: 0.0358
trigger times: 12
Loss after 15079374 batches: 0.0356
trigger times: 13
Loss after 15080337 batches: 0.0352
trigger times: 14
Loss after 15081300 batches: 0.0360
trigger times: 15
Loss after 15082263 batches: 0.0347
trigger times: 16
Loss after 15083226 batches: 0.0349
trigger times: 17
Loss after 15084189 batches: 0.0342
trigger times: 18
Loss after 15085152 batches: 0.0347
trigger times: 19
Loss after 15086115 batches: 0.0335
trigger times: 20
Loss after 15087078 batches: 0.0336
trigger times: 21
Loss after 15088041 batches: 0.0335
trigger times: 22
Loss after 15089004 batches: 0.0332
trigger times: 23
Loss after 15089967 batches: 0.0330
trigger times: 24
Loss after 15090930 batches: 0.0323
trigger times: 25
Early stopping!
Start to test process.
Loss after 15091893 batches: 0.0330
Time to train on one home:  52.44622588157654
trigger times: 0
Loss after 15092856 batches: 0.0941
trigger times: 1
Loss after 15093819 batches: 0.0869
trigger times: 2
Loss after 15094782 batches: 0.0804
trigger times: 3
Loss after 15095745 batches: 0.0761
trigger times: 4
Loss after 15096708 batches: 0.0734
trigger times: 5
Loss after 15097671 batches: 0.0703
trigger times: 6
Loss after 15098634 batches: 0.0674
trigger times: 7
Loss after 15099597 batches: 0.0657
trigger times: 8
Loss after 15100560 batches: 0.0652
trigger times: 9
Loss after 15101523 batches: 0.0652
trigger times: 10
Loss after 15102486 batches: 0.0626
trigger times: 11
Loss after 15103449 batches: 0.0623
trigger times: 12
Loss after 15104412 batches: 0.0636
trigger times: 13
Loss after 15105375 batches: 0.0614
trigger times: 14
Loss after 15106338 batches: 0.0603
trigger times: 15
Loss after 15107301 batches: 0.0592
trigger times: 16
Loss after 15108264 batches: 0.0586
trigger times: 17
Loss after 15109227 batches: 0.0594
trigger times: 18
Loss after 15110190 batches: 0.0588
trigger times: 19
Loss after 15111153 batches: 0.0560
trigger times: 20
Loss after 15112116 batches: 0.0567
trigger times: 21
Loss after 15113079 batches: 0.0569
trigger times: 22
Loss after 15114042 batches: 0.0565
trigger times: 23
Loss after 15115005 batches: 0.0556
trigger times: 24
Loss after 15115968 batches: 0.0551
trigger times: 25
Early stopping!
Start to test process.
Loss after 15116931 batches: 0.0545
Time to train on one home:  52.50318765640259
trigger times: 0
Loss after 15117894 batches: 0.0921
trigger times: 1
Loss after 15118857 batches: 0.0600
trigger times: 2
Loss after 15119820 batches: 0.0547
trigger times: 3
Loss after 15120783 batches: 0.0491
trigger times: 4
Loss after 15121746 batches: 0.0466
trigger times: 5
Loss after 15122709 batches: 0.0434
trigger times: 6
Loss after 15123672 batches: 0.0426
trigger times: 7
Loss after 15124635 batches: 0.0415
trigger times: 8
Loss after 15125598 batches: 0.0404
trigger times: 9
Loss after 15126561 batches: 0.0405
trigger times: 10
Loss after 15127524 batches: 0.0396
trigger times: 11
Loss after 15128487 batches: 0.0397
trigger times: 12
Loss after 15129450 batches: 0.0383
trigger times: 13
Loss after 15130413 batches: 0.0374
trigger times: 14
Loss after 15131376 batches: 0.0375
trigger times: 15
Loss after 15132339 batches: 0.0372
trigger times: 16
Loss after 15133302 batches: 0.0369
trigger times: 17
Loss after 15134265 batches: 0.0370
trigger times: 18
Loss after 15135228 batches: 0.0370
trigger times: 19
Loss after 15136191 batches: 0.0364
trigger times: 20
Loss after 15137154 batches: 0.0354
trigger times: 21
Loss after 15138117 batches: 0.0354
trigger times: 22
Loss after 15139080 batches: 0.0361
trigger times: 23
Loss after 15140043 batches: 0.0362
trigger times: 24
Loss after 15141006 batches: 0.0355
trigger times: 25
Early stopping!
Start to test process.
Loss after 15141969 batches: 0.0355
Time to train on one home:  52.349260330200195
trigger times: 0
Loss after 15142898 batches: 0.0894
trigger times: 1
Loss after 15143827 batches: 0.0613
trigger times: 2
Loss after 15144756 batches: 0.0456
trigger times: 3
Loss after 15145685 batches: 0.0409
trigger times: 4
Loss after 15146614 batches: 0.0392
trigger times: 5
Loss after 15147543 batches: 0.0341
trigger times: 6
Loss after 15148472 batches: 0.0324
trigger times: 7
Loss after 15149401 batches: 0.0298
trigger times: 8
Loss after 15150330 batches: 0.0289
trigger times: 9
Loss after 15151259 batches: 0.0284
trigger times: 10
Loss after 15152188 batches: 0.0260
trigger times: 11
Loss after 15153117 batches: 0.0281
trigger times: 12
Loss after 15154046 batches: 0.0268
trigger times: 13
Loss after 15154975 batches: 0.0265
trigger times: 14
Loss after 15155904 batches: 0.0281
trigger times: 15
Loss after 15156833 batches: 0.0256
trigger times: 16
Loss after 15157762 batches: 0.0247
trigger times: 17
Loss after 15158691 batches: 0.0251
trigger times: 18
Loss after 15159620 batches: 0.0254
trigger times: 19
Loss after 15160549 batches: 0.0248
trigger times: 20
Loss after 15161478 batches: 0.0271
trigger times: 21
Loss after 15162407 batches: 0.0289
trigger times: 22
Loss after 15163336 batches: 0.0252
trigger times: 23
Loss after 15164265 batches: 0.0257
trigger times: 24
Loss after 15165194 batches: 0.0256
trigger times: 25
Early stopping!
Start to test process.
Loss after 15166123 batches: 0.0251
Time to train on one home:  52.49912142753601
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15167086 batches: 0.0746
trigger times: 1
Loss after 15168049 batches: 0.0336
trigger times: 2
Loss after 15169012 batches: 0.0270
trigger times: 3
Loss after 15169975 batches: 0.0267
trigger times: 4
Loss after 15170938 batches: 0.0253
trigger times: 5
Loss after 15171901 batches: 0.0243
trigger times: 6
Loss after 15172864 batches: 0.0238
trigger times: 7
Loss after 15173827 batches: 0.0229
trigger times: 8
Loss after 15174790 batches: 0.0217
trigger times: 9
Loss after 15175753 batches: 0.0210
trigger times: 10
Loss after 15176716 batches: 0.0205
trigger times: 11
Loss after 15177679 batches: 0.0200
trigger times: 12
Loss after 15178642 batches: 0.0194
trigger times: 13
Loss after 15179605 batches: 0.0191
trigger times: 14
Loss after 15180568 batches: 0.0188
trigger times: 15
Loss after 15181531 batches: 0.0187
trigger times: 16
Loss after 15182494 batches: 0.0184
trigger times: 17
Loss after 15183457 batches: 0.0182
trigger times: 18
Loss after 15184420 batches: 0.0183
trigger times: 19
Loss after 15185383 batches: 0.0179
trigger times: 20
Loss after 15186346 batches: 0.0180
trigger times: 21
Loss after 15187309 batches: 0.0175
trigger times: 22
Loss after 15188272 batches: 0.0175
trigger times: 23
Loss after 15189235 batches: 0.0173
trigger times: 24
Loss after 15190198 batches: 0.0171
trigger times: 25
Early stopping!
Start to test process.
Loss after 15191161 batches: 0.0171
Time to train on one home:  52.17027735710144
trigger times: 0
Loss after 15192124 batches: 0.1835
trigger times: 1
Loss after 15193087 batches: 0.1238
trigger times: 2
Loss after 15194050 batches: 0.0973
trigger times: 3
Loss after 15195013 batches: 0.0888
trigger times: 4
Loss after 15195976 batches: 0.0804
trigger times: 5
Loss after 15196939 batches: 0.0741
trigger times: 6
Loss after 15197902 batches: 0.0680
trigger times: 7
Loss after 15198865 batches: 0.0635
trigger times: 8
Loss after 15199828 batches: 0.0598
trigger times: 9
Loss after 15200791 batches: 0.0559
trigger times: 10
Loss after 15201754 batches: 0.0541
trigger times: 11
Loss after 15202717 batches: 0.0516
trigger times: 12
Loss after 15203680 batches: 0.0511
trigger times: 13
Loss after 15204643 batches: 0.0498
trigger times: 14
Loss after 15205606 batches: 0.0511
trigger times: 15
Loss after 15206569 batches: 0.0509
trigger times: 16
Loss after 15207532 batches: 0.0475
trigger times: 17
Loss after 15208495 batches: 0.0466
trigger times: 18
Loss after 15209458 batches: 0.0459
trigger times: 19
Loss after 15210421 batches: 0.0446
trigger times: 20
Loss after 15211384 batches: 0.0436
trigger times: 21
Loss after 15212347 batches: 0.0435
trigger times: 22
Loss after 15213310 batches: 0.0431
trigger times: 23
Loss after 15214273 batches: 0.0429
trigger times: 24
Loss after 15215236 batches: 0.0417
trigger times: 25
Early stopping!
Start to test process.
Loss after 15216199 batches: 0.0422
Time to train on one home:  52.2982017993927
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15217162 batches: 0.0861
trigger times: 1
Loss after 15218125 batches: 0.0724
trigger times: 2
Loss after 15219088 batches: 0.0679
trigger times: 3
Loss after 15220051 batches: 0.0653
trigger times: 4
Loss after 15221014 batches: 0.0631
trigger times: 5
Loss after 15221977 batches: 0.0609
trigger times: 6
Loss after 15222940 batches: 0.0595
trigger times: 7
Loss after 15223903 batches: 0.0584
trigger times: 8
Loss after 15224866 batches: 0.0554
trigger times: 9
Loss after 15225829 batches: 0.0555
trigger times: 10
Loss after 15226792 batches: 0.0570
trigger times: 11
Loss after 15227755 batches: 0.0560
trigger times: 12
Loss after 15228718 batches: 0.0539
trigger times: 13
Loss after 15229681 batches: 0.0545
trigger times: 14
Loss after 15230644 batches: 0.0559
trigger times: 15
Loss after 15231607 batches: 0.0551
trigger times: 16
Loss after 15232570 batches: 0.0533
trigger times: 17
Loss after 15233533 batches: 0.0546
trigger times: 18
Loss after 15234496 batches: 0.0545
trigger times: 19
Loss after 15235459 batches: 0.0536
trigger times: 20
Loss after 15236422 batches: 0.0535
trigger times: 21
Loss after 15237385 batches: 0.0518
trigger times: 22
Loss after 15238348 batches: 0.0509
trigger times: 23
Loss after 15239311 batches: 0.0523
trigger times: 24
Loss after 15240274 batches: 0.0518
trigger times: 25
Early stopping!
Start to test process.
Loss after 15241237 batches: 0.0503
Time to train on one home:  52.79804825782776
trigger times: 0
Loss after 15242200 batches: 0.0872
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 15243163 batches: 0.0746
trigger times: 2
Loss after 15244126 batches: 0.0722
trigger times: 3
Loss after 15245089 batches: 0.0678
trigger times: 4
Loss after 15246052 batches: 0.0663
trigger times: 5
Loss after 15247015 batches: 0.0613
trigger times: 6
Loss after 15247978 batches: 0.0590
trigger times: 7
Loss after 15248941 batches: 0.0577
trigger times: 8
Loss after 15249904 batches: 0.0547
trigger times: 9
Loss after 15250867 batches: 0.0529
trigger times: 10
Loss after 15251830 batches: 0.0514
trigger times: 11
Loss after 15252793 batches: 0.0509
trigger times: 12
Loss after 15253756 batches: 0.0501
trigger times: 13
Loss after 15254719 batches: 0.0500
trigger times: 14
Loss after 15255682 batches: 0.0488
trigger times: 15
Loss after 15256645 batches: 0.0489
trigger times: 16
Loss after 15257608 batches: 0.0488
trigger times: 17
Loss after 15258571 batches: 0.0482
trigger times: 18
Loss after 15259534 batches: 0.0481
trigger times: 19
Loss after 15260497 batches: 0.0463
trigger times: 20
Loss after 15261460 batches: 0.0454
trigger times: 21
Loss after 15262423 batches: 0.0500
trigger times: 22
Loss after 15263386 batches: 0.0497
trigger times: 23
Loss after 15264349 batches: 0.0489
trigger times: 24
Loss after 15265312 batches: 0.0483
trigger times: 25
Early stopping!
Start to test process.
Loss after 15266275 batches: 0.0454
Time to train on one home:  52.63810110092163
trigger times: 0
Loss after 15267238 batches: 0.0220
trigger times: 1
Loss after 15268201 batches: 0.0188
trigger times: 2
Loss after 15269164 batches: 0.0172
trigger times: 3
Loss after 15270127 batches: 0.0163
trigger times: 4
Loss after 15271090 batches: 0.0149
trigger times: 5
Loss after 15272053 batches: 0.0145
trigger times: 6
Loss after 15273016 batches: 0.0139
trigger times: 7
Loss after 15273979 batches: 0.0134
trigger times: 8
Loss after 15274942 batches: 0.0131
trigger times: 9
Loss after 15275905 batches: 0.0128
trigger times: 10
Loss after 15276868 batches: 0.0123
trigger times: 11
Loss after 15277831 batches: 0.0123
trigger times: 12
Loss after 15278794 batches: 0.0124
trigger times: 13
Loss after 15279757 batches: 0.0121
trigger times: 14
Loss after 15280720 batches: 0.0117
trigger times: 15
Loss after 15281683 batches: 0.0116
trigger times: 16
Loss after 15282646 batches: 0.0114
trigger times: 17
Loss after 15283609 batches: 0.0123
trigger times: 18
Loss after 15284572 batches: 0.0123
trigger times: 19
Loss after 15285535 batches: 0.0120
trigger times: 20
Loss after 15286498 batches: 0.0118
trigger times: 21
Loss after 15287461 batches: 0.0117
trigger times: 22
Loss after 15288424 batches: 0.0121
trigger times: 23
Loss after 15289387 batches: 0.0115
trigger times: 24
Loss after 15290350 batches: 0.0112
trigger times: 25
Early stopping!
Start to test process.
Loss after 15291313 batches: 0.0115
Time to train on one home:  52.27315664291382
trigger times: 0
Loss after 15292276 batches: 0.0406
trigger times: 1
Loss after 15293239 batches: 0.0331
trigger times: 2
Loss after 15294202 batches: 0.0306
trigger times: 3
Loss after 15295165 batches: 0.0287
trigger times: 4
Loss after 15296128 batches: 0.0264
trigger times: 5
Loss after 15297091 batches: 0.0244
trigger times: 6
Loss after 15298054 batches: 0.0233
trigger times: 7
Loss after 15299017 batches: 0.0225
trigger times: 8
Loss after 15299980 batches: 0.0224
trigger times: 9
Loss after 15300943 batches: 0.0224
trigger times: 10
Loss after 15301906 batches: 0.0222
trigger times: 11
Loss after 15302869 batches: 0.0224
trigger times: 12
Loss after 15303832 batches: 0.0228
trigger times: 13
Loss after 15304795 batches: 0.0217
trigger times: 14
Loss after 15305758 batches: 0.0210
trigger times: 15
Loss after 15306721 batches: 0.0200
trigger times: 16
Loss after 15307684 batches: 0.0195
trigger times: 17
Loss after 15308647 batches: 0.0198
trigger times: 18
Loss after 15309610 batches: 0.0200
trigger times: 19
Loss after 15310573 batches: 0.0194
trigger times: 20
Loss after 15311536 batches: 0.0194
trigger times: 21
Loss after 15312499 batches: 0.0195
trigger times: 22
Loss after 15313462 batches: 0.0197
trigger times: 23
Loss after 15314425 batches: 0.0189
trigger times: 24
Loss after 15315388 batches: 0.0193
trigger times: 25
Early stopping!
Start to test process.
Loss after 15316351 batches: 0.0189
Time to train on one home:  52.51518416404724
trigger times: 0
Loss after 15317314 batches: 0.0883
trigger times: 1
Loss after 15318277 batches: 0.0634
trigger times: 2
Loss after 15319240 batches: 0.0555
trigger times: 3
Loss after 15320203 batches: 0.0504
trigger times: 4
Loss after 15321166 batches: 0.0480
trigger times: 5
Loss after 15322129 batches: 0.0461
trigger times: 6
Loss after 15323092 batches: 0.0439
trigger times: 7
Loss after 15324055 batches: 0.0426
trigger times: 8
Loss after 15325018 batches: 0.0411
trigger times: 9
Loss after 15325981 batches: 0.0410
trigger times: 10
Loss after 15326944 batches: 0.0411
trigger times: 11
Loss after 15327907 batches: 0.0400
trigger times: 12
Loss after 15328870 batches: 0.0385
trigger times: 13
Loss after 15329833 batches: 0.0375
trigger times: 14
Loss after 15330796 batches: 0.0370
trigger times: 15
Loss after 15331759 batches: 0.0371
trigger times: 16
Loss after 15332722 batches: 0.0366
trigger times: 17
Loss after 15333685 batches: 0.0375
trigger times: 18
Loss after 15334648 batches: 0.0355
trigger times: 19
Loss after 15335611 batches: 0.0346
trigger times: 20
Loss after 15336574 batches: 0.0357
trigger times: 21
Loss after 15337537 batches: 0.0348
trigger times: 22
Loss after 15338500 batches: 0.0331
trigger times: 23
Loss after 15339463 batches: 0.0335
trigger times: 24
Loss after 15340426 batches: 0.0324
trigger times: 25
Early stopping!
Start to test process.
Loss after 15341389 batches: 0.0336
Time to train on one home:  52.747788190841675
trigger times: 0
Loss after 15342352 batches: 0.0924
trigger times: 1
Loss after 15343315 batches: 0.0604
trigger times: 2
Loss after 15344278 batches: 0.0548
trigger times: 3
Loss after 15345241 batches: 0.0500
trigger times: 4
Loss after 15346204 batches: 0.0469
trigger times: 5
Loss after 15347167 batches: 0.0447
trigger times: 6
Loss after 15348130 batches: 0.0428
trigger times: 7
Loss after 15349093 batches: 0.0406
trigger times: 8
Loss after 15350056 batches: 0.0403
trigger times: 9
Loss after 15351019 batches: 0.0403
trigger times: 10
Loss after 15351982 batches: 0.0389
trigger times: 11
Loss after 15352945 batches: 0.0379
trigger times: 12
Loss after 15353908 batches: 0.0380
trigger times: 13
Loss after 15354871 batches: 0.0369
trigger times: 14
Loss after 15355834 batches: 0.0369
trigger times: 15
Loss after 15356797 batches: 0.0366
trigger times: 16
Loss after 15357760 batches: 0.0367
trigger times: 17
Loss after 15358723 batches: 0.0376
trigger times: 18
Loss after 15359686 batches: 0.0366
trigger times: 19
Loss after 15360649 batches: 0.0360
trigger times: 20
Loss after 15361612 batches: 0.0368
trigger times: 21
Loss after 15362575 batches: 0.0364
trigger times: 22
Loss after 15363538 batches: 0.0346
trigger times: 23
Loss after 15364501 batches: 0.0353
trigger times: 24
Loss after 15365464 batches: 0.0359
trigger times: 25
Early stopping!
Start to test process.
Loss after 15366427 batches: 0.0353
Time to train on one home:  52.418785095214844
trigger times: 0
Loss after 15367390 batches: 0.0485
trigger times: 1
Loss after 15368353 batches: 0.0408
trigger times: 2
Loss after 15369316 batches: 0.0364
trigger times: 3
Loss after 15370279 batches: 0.0332
trigger times: 4
Loss after 15371242 batches: 0.0309
trigger times: 5
Loss after 15372205 batches: 0.0291
trigger times: 6
Loss after 15373168 batches: 0.0276
trigger times: 7
Loss after 15374131 batches: 0.0277
trigger times: 8
Loss after 15375094 batches: 0.0280
trigger times: 9
Loss after 15376057 batches: 0.0278
trigger times: 10
Loss after 15377020 batches: 0.0270
trigger times: 11
Loss after 15377983 batches: 0.0284
trigger times: 12
Loss after 15378946 batches: 0.0300
trigger times: 13
Loss after 15379909 batches: 0.0299
trigger times: 14
Loss after 15380872 batches: 0.0287
trigger times: 15
Loss after 15381835 batches: 0.0282
trigger times: 16
Loss after 15382798 batches: 0.0277
trigger times: 17
Loss after 15383761 batches: 0.0271
trigger times: 18
Loss after 15384724 batches: 0.0258
trigger times: 19
Loss after 15385687 batches: 0.0251
trigger times: 20
Loss after 15386650 batches: 0.0255
trigger times: 21
Loss after 15387613 batches: 0.0254
trigger times: 22
Loss after 15388576 batches: 0.0255
trigger times: 23
Loss after 15389539 batches: 0.0256
trigger times: 24
Loss after 15390502 batches: 0.0241
trigger times: 25
Early stopping!
Start to test process.
Loss after 15391465 batches: 0.0238
Time to train on one home:  52.343663454055786
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15392428 batches: 0.0801
trigger times: 1
Loss after 15393391 batches: 0.0492
trigger times: 2
Loss after 15394354 batches: 0.0483
trigger times: 0
Loss after 15395317 batches: 0.0431
trigger times: 1
Loss after 15396280 batches: 0.0379
trigger times: 0
Loss after 15397243 batches: 0.0358
trigger times: 1
Loss after 15398206 batches: 0.0329
trigger times: 2
Loss after 15399169 batches: 0.0320
trigger times: 0
Loss after 15400132 batches: 0.0298
trigger times: 1
Loss after 15401095 batches: 0.0297
trigger times: 0
Loss after 15402058 batches: 0.0286
trigger times: 1
Loss after 15403021 batches: 0.0280
trigger times: 2
Loss after 15403984 batches: 0.0272
trigger times: 3
Loss after 15404947 batches: 0.0260
trigger times: 4
Loss after 15405910 batches: 0.0258
trigger times: 5
Loss after 15406873 batches: 0.0254
trigger times: 6
Loss after 15407836 batches: 0.0256
trigger times: 7
Loss after 15408799 batches: 0.0246
trigger times: 8
Loss after 15409762 batches: 0.0256
trigger times: 9
Loss after 15410725 batches: 0.0253
trigger times: 10
Loss after 15411688 batches: 0.0262
trigger times: 11
Loss after 15412651 batches: 0.0266
trigger times: 12
Loss after 15413614 batches: 0.0247
trigger times: 13
Loss after 15414577 batches: 0.0251
trigger times: 14
Loss after 15415540 batches: 0.0236
trigger times: 15
Loss after 15416503 batches: 0.0241
trigger times: 16
Loss after 15417466 batches: 0.0225
trigger times: 17
Loss after 15418429 batches: 0.0217
trigger times: 18
Loss after 15419392 batches: 0.0226
trigger times: 19
Loss after 15420355 batches: 0.0217
trigger times: 20
Loss after 15421318 batches: 0.0222
trigger times: 21
Loss after 15422281 batches: 0.0221
trigger times: 22
Loss after 15423244 batches: 0.0221
trigger times: 23
Loss after 15424207 batches: 0.0224
trigger times: 24
Loss after 15425170 batches: 0.0214
trigger times: 25
Early stopping!
Start to test process.
Loss after 15426133 batches: 0.0220
Time to train on one home:  60.249231815338135
trigger times: 0
Loss after 15427096 batches: 0.1036
trigger times: 1
Loss after 15428059 batches: 0.0783
trigger times: 2
Loss after 15429022 batches: 0.0755
trigger times: 3
Loss after 15429985 batches: 0.0736
trigger times: 4
Loss after 15430948 batches: 0.0700
trigger times: 5
Loss after 15431911 batches: 0.0680
trigger times: 6
Loss after 15432874 batches: 0.0670
trigger times: 7
Loss after 15433837 batches: 0.0646
trigger times: 8
Loss after 15434800 batches: 0.0629
trigger times: 9
Loss after 15435763 batches: 0.0621
trigger times: 10
Loss after 15436726 batches: 0.0618
trigger times: 11
Loss after 15437689 batches: 0.0601
trigger times: 12
Loss after 15438652 batches: 0.0590
trigger times: 13
Loss after 15439615 batches: 0.0574
trigger times: 14
Loss after 15440578 batches: 0.0584
trigger times: 15
Loss after 15441541 batches: 0.0574
trigger times: 16
Loss after 15442504 batches: 0.0579
trigger times: 17
Loss after 15443467 batches: 0.0565
trigger times: 18
Loss after 15444430 batches: 0.0567
trigger times: 19
Loss after 15445393 batches: 0.0582
trigger times: 20
Loss after 15446356 batches: 0.0555
trigger times: 21
Loss after 15447319 batches: 0.0557
trigger times: 22
Loss after 15448282 batches: 0.0552
trigger times: 23
Loss after 15449245 batches: 0.0563
trigger times: 24
Loss after 15450208 batches: 0.0546
trigger times: 25
Early stopping!
Start to test process.
Loss after 15451171 batches: 0.0543
Time to train on one home:  52.51143670082092
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15452134 batches: 0.0795
trigger times: 1
Loss after 15453097 batches: 0.0489
trigger times: 2
Loss after 15454060 batches: 0.0496
trigger times: 0
Loss after 15455023 batches: 0.0412
trigger times: 1
Loss after 15455986 batches: 0.0367
trigger times: 0
Loss after 15456949 batches: 0.0344
trigger times: 0
Loss after 15457912 batches: 0.0324
trigger times: 1
Loss after 15458875 batches: 0.0318
trigger times: 2
Loss after 15459838 batches: 0.0297
trigger times: 0
Loss after 15460801 batches: 0.0294
trigger times: 1
Loss after 15461764 batches: 0.0282
trigger times: 2
Loss after 15462727 batches: 0.0282
trigger times: 3
Loss after 15463690 batches: 0.0277
trigger times: 4
Loss after 15464653 batches: 0.0282
trigger times: 5
Loss after 15465616 batches: 0.0275
trigger times: 6
Loss after 15466579 batches: 0.0279
trigger times: 7
Loss after 15467542 batches: 0.0254
trigger times: 8
Loss after 15468505 batches: 0.0262
trigger times: 9
Loss after 15469468 batches: 0.0268
trigger times: 10
Loss after 15470431 batches: 0.0254
trigger times: 11
Loss after 15471394 batches: 0.0242
trigger times: 12
Loss after 15472357 batches: 0.0238
trigger times: 13
Loss after 15473320 batches: 0.0243
trigger times: 14
Loss after 15474283 batches: 0.0237
trigger times: 15
Loss after 15475246 batches: 0.0241
trigger times: 16
Loss after 15476209 batches: 0.0234
trigger times: 17
Loss after 15477172 batches: 0.0223
trigger times: 18
Loss after 15478135 batches: 0.0229
trigger times: 19
Loss after 15479098 batches: 0.0228
trigger times: 20
Loss after 15480061 batches: 0.0229
trigger times: 21
Loss after 15481024 batches: 0.0231
trigger times: 22
Loss after 15481987 batches: 0.0229
trigger times: 23
Loss after 15482950 batches: 0.0221
trigger times: 24
Loss after 15483913 batches: 0.0223
trigger times: 25
Early stopping!
Start to test process.
Loss after 15484876 batches: 0.0219
Time to train on one home:  59.54928112030029
trigger times: 0
Loss after 15485771 batches: 0.0674
trigger times: 1
Loss after 15486666 batches: 0.0404
trigger times: 2
Loss after 15487561 batches: 0.0197
trigger times: 3
Loss after 15488456 batches: 0.0105
trigger times: 4
Loss after 15489351 batches: 0.0087
trigger times: 5
Loss after 15490246 batches: 0.0065
trigger times: 6
Loss after 15491141 batches: 0.0054
trigger times: 7
Loss after 15492036 batches: 0.0049
trigger times: 8
Loss after 15492931 batches: 0.0086
trigger times: 9
Loss after 15493826 batches: 0.0049
trigger times: 10
Loss after 15494721 batches: 0.0045
trigger times: 11
Loss after 15495616 batches: 0.0047
trigger times: 12
Loss after 15496511 batches: 0.0038
trigger times: 13
Loss after 15497406 batches: 0.0038
trigger times: 14
Loss after 15498301 batches: 0.0034
trigger times: 15
Loss after 15499196 batches: 0.0032
trigger times: 16
Loss after 15500091 batches: 0.0036
trigger times: 17
Loss after 15500986 batches: 0.0033
trigger times: 18
Loss after 15501881 batches: 0.0033
trigger times: 19
Loss after 15502776 batches: 0.0028
trigger times: 20
Loss after 15503671 batches: 0.0025
trigger times: 21
Loss after 15504566 batches: 0.0023
trigger times: 22
Loss after 15505461 batches: 0.0023
trigger times: 23
Loss after 15506356 batches: 0.0022
trigger times: 24
Loss after 15507251 batches: 0.0019
trigger times: 25
Early stopping!
Start to test process.
Loss after 15508146 batches: 0.0021
Time to train on one home:  51.61261439323425
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15509083 batches: 0.0810
trigger times: 1
Loss after 15510020 batches: 0.0671
trigger times: 2
Loss after 15510957 batches: 0.0650
trigger times: 3
Loss after 15511894 batches: 0.0606
trigger times: 4
Loss after 15512831 batches: 0.0585
trigger times: 5
Loss after 15513768 batches: 0.0553
trigger times: 6
Loss after 15514705 batches: 0.0553
trigger times: 7
Loss after 15515642 batches: 0.0553
trigger times: 8
Loss after 15516579 batches: 0.0526
trigger times: 9
Loss after 15517516 batches: 0.0522
trigger times: 10
Loss after 15518453 batches: 0.0520
trigger times: 11
Loss after 15519390 batches: 0.0502
trigger times: 12
Loss after 15520327 batches: 0.0504
trigger times: 13
Loss after 15521264 batches: 0.0487
trigger times: 14
Loss after 15522201 batches: 0.0480
trigger times: 15
Loss after 15523138 batches: 0.0498
trigger times: 16
Loss after 15524075 batches: 0.0485
trigger times: 17
Loss after 15525012 batches: 0.0496
trigger times: 18
Loss after 15525949 batches: 0.0489
trigger times: 19
Loss after 15526886 batches: 0.0492
trigger times: 20
Loss after 15527823 batches: 0.0494
trigger times: 21
Loss after 15528760 batches: 0.0487
trigger times: 22
Loss after 15529697 batches: 0.0478
trigger times: 23
Loss after 15530634 batches: 0.0474
trigger times: 24
Loss after 15531571 batches: 0.0464
trigger times: 25
Early stopping!
Start to test process.
Loss after 15532508 batches: 0.0469
Time to train on one home:  52.47929811477661
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066, 0.03401813118363776, 0.033508403208926535, 0.03273922320832277, 0.032162070075287874, 0.031213877056718613, 0.03093800142524674]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863], [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861], [0.0757279247045517, 0.10266384027132536, 0.5909740425258612, 0.8803234656976516, 0.8076583271010261, 28.206427011804568, 8296.775], [0.06181953847408295, 0.10445801892372542, 0.5961459732150404, 0.831618823847048, 0.806043429565136, 26.645882531252276, 8280.187], [0.07333928346633911, 0.09887557787656209, 0.5904951093552152, 0.8805827319948272, 0.8110679704670578, 28.214734158180708, 8331.802], [0.06249981373548508, 0.1056225345966233, 0.6023961404422763, 0.8395811685763604, 0.8049953091236983, 26.90100386358231, 8269.419], [0.07111044973134995, 0.11364462231541128, 0.6015437535926366, 0.8360909977276793, 0.7977749276509928, 26.78917536742376, 8195.247]]
Round_13_results:  [0.07111044973134995, 0.11364462231541128, 0.6015437535926366, 0.8360909977276793, 0.7977749276509928, 26.78917536742376, 8195.247]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 16203 < 16204; dropping {'Training_Loss': 0.07138629044805254, 'Validation_Loss': 0.08104954659938812, 'Training_R2': -0.10696155316625999, 'Validation_R2': 0.1298101533657502, 'Training_F1': 0.4149491753055886, 'Validation_F1': 0.6104252131993013, 'Training_NEP': 0.8343701066686314, 'Validation_NEP': 0.7787229346853973, 'Training_NDE': 0.6741664275047308, 'Validation_NDE': 0.7861958532617297, 'Training_MAE': 31.706332503546562, 'Validation_MAE': 27.69082199381277, 'Training_MSE': 2489.957, 'Validation_MSE': 10299.175}.
trigger times: 0
Loss after 15533470 batches: 0.0714
trigger times: 1
Loss after 15534432 batches: 0.0637
trigger times: 2
Loss after 15535394 batches: 0.0611
trigger times: 3
Loss after 15536356 batches: 0.0584
trigger times: 4
Loss after 15537318 batches: 0.0566
trigger times: 5
Loss after 15538280 batches: 0.0556
trigger times: 6
Loss after 15539242 batches: 0.0536
trigger times: 7
Loss after 15540204 batches: 0.0518
trigger times: 8
Loss after 15541166 batches: 0.0515
trigger times: 9
Loss after 15542128 batches: 0.0511
trigger times: 10
Loss after 15543090 batches: 0.0516
trigger times: 11
Loss after 15544052 batches: 0.0511
trigger times: 12
Loss after 15545014 batches: 0.0506
trigger times: 13
Loss after 15545976 batches: 0.0484
trigger times: 14
Loss after 15546938 batches: 0.0498
trigger times: 15
Loss after 15547900 batches: 0.0513
trigger times: 16
Loss after 15548862 batches: 0.0502
trigger times: 17
Loss after 15549824 batches: 0.0487
trigger times: 18
Loss after 15550786 batches: 0.0490
trigger times: 19
Loss after 15551748 batches: 0.0480
trigger times: 20
Loss after 15552710 batches: 0.0485
trigger times: 21
Loss after 15553672 batches: 0.0472
trigger times: 22
Loss after 15554634 batches: 0.0480
trigger times: 23
Loss after 15555596 batches: 0.0483
trigger times: 24
Loss after 15556558 batches: 0.0473
trigger times: 25
Early stopping!
Start to test process.
Loss after 15557520 batches: 0.0477
Time to train on one home:  52.46528768539429
trigger times: 0
Loss after 15558449 batches: 0.1090
trigger times: 1
Loss after 15559378 batches: 0.0640
trigger times: 2
Loss after 15560307 batches: 0.0499
trigger times: 3
Loss after 15561236 batches: 0.0414
trigger times: 4
Loss after 15562165 batches: 0.0388
trigger times: 5
Loss after 15563094 batches: 0.0341
trigger times: 6
Loss after 15564023 batches: 0.0314
trigger times: 7
Loss after 15564952 batches: 0.0303
trigger times: 8
Loss after 15565881 batches: 0.0287
trigger times: 9
Loss after 15566810 batches: 0.0297
trigger times: 10
Loss after 15567739 batches: 0.0276
trigger times: 11
Loss after 15568668 batches: 0.0273
trigger times: 12
Loss after 15569597 batches: 0.0279
trigger times: 13
Loss after 15570526 batches: 0.0259
trigger times: 14
Loss after 15571455 batches: 0.0254
trigger times: 15
Loss after 15572384 batches: 0.0250
trigger times: 16
Loss after 15573313 batches: 0.0263
trigger times: 17
Loss after 15574242 batches: 0.0241
trigger times: 18
Loss after 15575171 batches: 0.0246
trigger times: 19
Loss after 15576100 batches: 0.0258
trigger times: 20
Loss after 15577029 batches: 0.0262
trigger times: 21
Loss after 15577958 batches: 0.0247
trigger times: 22
Loss after 15578887 batches: 0.0277
trigger times: 23
Loss after 15579816 batches: 0.0305
trigger times: 24
Loss after 15580745 batches: 0.0269
trigger times: 25
Early stopping!
Start to test process.
Loss after 15581674 batches: 0.0259
Time to train on one home:  52.12333965301514
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 15582637 batches: 0.0305
trigger times: 0
Loss after 15583600 batches: 0.0175
trigger times: 1
Loss after 15584563 batches: 0.0147
trigger times: 2
Loss after 15585526 batches: 0.0141
trigger times: 3
Loss after 15586489 batches: 0.0137
trigger times: 4
Loss after 15587452 batches: 0.0130
trigger times: 5
Loss after 15588415 batches: 0.0123
trigger times: 6
Loss after 15589378 batches: 0.0116
trigger times: 7
Loss after 15590341 batches: 0.0105
trigger times: 8
Loss after 15591304 batches: 0.0097
trigger times: 9
Loss after 15592267 batches: 0.0092
trigger times: 10
Loss after 15593230 batches: 0.0088
trigger times: 11
Loss after 15594193 batches: 0.0084
trigger times: 12
Loss after 15595156 batches: 0.0079
trigger times: 13
Loss after 15596119 batches: 0.0075
trigger times: 14
Loss after 15597082 batches: 0.0076
trigger times: 15
Loss after 15598045 batches: 0.0078
trigger times: 16
Loss after 15599008 batches: 0.0072
trigger times: 17
Loss after 15599971 batches: 0.0072
trigger times: 18
Loss after 15600934 batches: 0.0071
trigger times: 19
Loss after 15601897 batches: 0.0069
trigger times: 20
Loss after 15602860 batches: 0.0068
trigger times: 21
Loss after 15603823 batches: 0.0067
trigger times: 22
Loss after 15604786 batches: 0.0065
trigger times: 23
Loss after 15605749 batches: 0.0065
trigger times: 24
Loss after 15606712 batches: 0.0062
trigger times: 25
Early stopping!
Start to test process.
Loss after 15607675 batches: 0.0060
Time to train on one home:  53.67684745788574
trigger times: 0
Loss after 15608638 batches: 0.0210
trigger times: 1
Loss after 15609601 batches: 0.0177
trigger times: 2
Loss after 15610564 batches: 0.0165
trigger times: 3
Loss after 15611527 batches: 0.0154
trigger times: 4
Loss after 15612490 batches: 0.0142
trigger times: 5
Loss after 15613453 batches: 0.0136
trigger times: 6
Loss after 15614416 batches: 0.0132
trigger times: 7
Loss after 15615379 batches: 0.0136
trigger times: 8
Loss after 15616342 batches: 0.0130
trigger times: 9
Loss after 15617305 batches: 0.0123
trigger times: 10
Loss after 15618268 batches: 0.0118
trigger times: 11
Loss after 15619231 batches: 0.0121
trigger times: 12
Loss after 15620194 batches: 0.0121
trigger times: 13
Loss after 15621157 batches: 0.0118
trigger times: 14
Loss after 15622120 batches: 0.0122
trigger times: 15
Loss after 15623083 batches: 0.0119
trigger times: 16
Loss after 15624046 batches: 0.0120
trigger times: 17
Loss after 15625009 batches: 0.0117
trigger times: 18
Loss after 15625972 batches: 0.0112
trigger times: 19
Loss after 15626935 batches: 0.0121
trigger times: 20
Loss after 15627898 batches: 0.0120
trigger times: 21
Loss after 15628861 batches: 0.0112
trigger times: 22
Loss after 15629824 batches: 0.0112
trigger times: 23
Loss after 15630787 batches: 0.0110
trigger times: 24
Loss after 15631750 batches: 0.0109
trigger times: 25
Early stopping!
Start to test process.
Loss after 15632713 batches: 0.0110
Time to train on one home:  52.383248805999756
trigger times: 0
Loss after 15633676 batches: 0.0958
trigger times: 1
Loss after 15634639 batches: 0.0876
trigger times: 2
Loss after 15635602 batches: 0.0804
trigger times: 3
Loss after 15636565 batches: 0.0774
trigger times: 4
Loss after 15637528 batches: 0.0738
trigger times: 5
Loss after 15638491 batches: 0.0701
trigger times: 6
Loss after 15639454 batches: 0.0681
trigger times: 7
Loss after 15640417 batches: 0.0666
trigger times: 8
Loss after 15641380 batches: 0.0642
trigger times: 9
Loss after 15642343 batches: 0.0631
trigger times: 10
Loss after 15643306 batches: 0.0613
trigger times: 11
Loss after 15644269 batches: 0.0593
trigger times: 12
Loss after 15645232 batches: 0.0598
trigger times: 13
Loss after 15646195 batches: 0.0604
trigger times: 14
Loss after 15647158 batches: 0.0601
trigger times: 15
Loss after 15648121 batches: 0.0586
trigger times: 16
Loss after 15649084 batches: 0.0591
trigger times: 17
Loss after 15650047 batches: 0.0561
trigger times: 18
Loss after 15651010 batches: 0.0554
trigger times: 19
Loss after 15651973 batches: 0.0544
trigger times: 20
Loss after 15652936 batches: 0.0561
trigger times: 21
Loss after 15653899 batches: 0.0558
trigger times: 22
Loss after 15654862 batches: 0.0562
trigger times: 23
Loss after 15655825 batches: 0.0552
trigger times: 24
Loss after 15656788 batches: 0.0556
trigger times: 25
Early stopping!
Start to test process.
Loss after 15657751 batches: 0.0559
Time to train on one home:  52.19326877593994
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15658714 batches: 0.0793
trigger times: 1
Loss after 15659677 batches: 0.0708
trigger times: 2
Loss after 15660640 batches: 0.0669
trigger times: 3
Loss after 15661603 batches: 0.0639
trigger times: 4
Loss after 15662566 batches: 0.0617
trigger times: 5
Loss after 15663529 batches: 0.0607
trigger times: 6
Loss after 15664492 batches: 0.0581
trigger times: 7
Loss after 15665455 batches: 0.0577
trigger times: 8
Loss after 15666418 batches: 0.0554
trigger times: 9
Loss after 15667381 batches: 0.0547
trigger times: 10
Loss after 15668344 batches: 0.0548
trigger times: 11
Loss after 15669307 batches: 0.0537
trigger times: 12
Loss after 15670270 batches: 0.0548
trigger times: 13
Loss after 15671233 batches: 0.0551
trigger times: 14
Loss after 15672196 batches: 0.0544
trigger times: 15
Loss after 15673159 batches: 0.0550
trigger times: 16
Loss after 15674122 batches: 0.0518
trigger times: 17
Loss after 15675085 batches: 0.0521
trigger times: 18
Loss after 15676048 batches: 0.0515
trigger times: 19
Loss after 15677011 batches: 0.0524
trigger times: 20
Loss after 15677974 batches: 0.0510
trigger times: 21
Loss after 15678937 batches: 0.0510
trigger times: 22
Loss after 15679900 batches: 0.0516
trigger times: 23
Loss after 15680863 batches: 0.0514
trigger times: 24
Loss after 15681826 batches: 0.0516
trigger times: 25
Early stopping!
Start to test process.
Loss after 15682789 batches: 0.0509
Time to train on one home:  52.535149335861206
trigger times: 0
Loss after 15683752 batches: 0.0741
trigger times: 1
Loss after 15684715 batches: 0.0692
trigger times: 2
Loss after 15685678 batches: 0.0644
trigger times: 3
Loss after 15686641 batches: 0.0626
trigger times: 4
Loss after 15687604 batches: 0.0592
trigger times: 5
Loss after 15688567 batches: 0.0575
trigger times: 6
Loss after 15689530 batches: 0.0561
trigger times: 7
Loss after 15690493 batches: 0.0558
trigger times: 8
Loss after 15691456 batches: 0.0551
trigger times: 9
Loss after 15692419 batches: 0.0544
trigger times: 10
Loss after 15693382 batches: 0.0558
trigger times: 11
Loss after 15694345 batches: 0.0534
trigger times: 12
Loss after 15695308 batches: 0.0531
trigger times: 13
Loss after 15696271 batches: 0.0526
trigger times: 14
Loss after 15697234 batches: 0.0513
trigger times: 15
Loss after 15698197 batches: 0.0520
trigger times: 16
Loss after 15699160 batches: 0.0504
trigger times: 17
Loss after 15700123 batches: 0.0506
trigger times: 18
Loss after 15701086 batches: 0.0498
trigger times: 19
Loss after 15702049 batches: 0.0504
trigger times: 20
Loss after 15703012 batches: 0.0507
trigger times: 21
Loss after 15703975 batches: 0.0492
trigger times: 22
Loss after 15704938 batches: 0.0485
trigger times: 23
Loss after 15705901 batches: 0.0498
trigger times: 24
Loss after 15706864 batches: 0.0497
trigger times: 25
Early stopping!
Start to test process.
Loss after 15707827 batches: 0.0493
Time to train on one home:  52.440171003341675
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15708790 batches: 0.0605
trigger times: 1
Loss after 15709753 batches: 0.0488
trigger times: 2
Loss after 15710716 batches: 0.0456
trigger times: 3
Loss after 15711679 batches: 0.0392
trigger times: 4
Loss after 15712642 batches: 0.0344
trigger times: 5
Loss after 15713605 batches: 0.0321
trigger times: 6
Loss after 15714568 batches: 0.0303
trigger times: 7
Loss after 15715531 batches: 0.0283
trigger times: 8
Loss after 15716494 batches: 0.0281
trigger times: 9
Loss after 15717457 batches: 0.0264
trigger times: 10
Loss after 15718420 batches: 0.0265
trigger times: 11
Loss after 15719383 batches: 0.0263
trigger times: 12
Loss after 15720346 batches: 0.0247
trigger times: 13
Loss after 15721309 batches: 0.0252
trigger times: 14
Loss after 15722272 batches: 0.0244
trigger times: 15
Loss after 15723235 batches: 0.0243
trigger times: 16
Loss after 15724198 batches: 0.0240
trigger times: 17
Loss after 15725161 batches: 0.0237
trigger times: 18
Loss after 15726124 batches: 0.0233
trigger times: 19
Loss after 15727087 batches: 0.0230
trigger times: 20
Loss after 15728050 batches: 0.0227
trigger times: 21
Loss after 15729013 batches: 0.0231
trigger times: 22
Loss after 15729976 batches: 0.0233
trigger times: 23
Loss after 15730939 batches: 0.0223
trigger times: 24
Loss after 15731902 batches: 0.0222
trigger times: 25
Early stopping!
Start to test process.
Loss after 15732865 batches: 0.0216
Time to train on one home:  52.243223667144775
trigger times: 0
Loss after 15733823 batches: 0.0756
trigger times: 1
Loss after 15734781 batches: 0.0446
trigger times: 2
Loss after 15735739 batches: 0.0368
trigger times: 3
Loss after 15736697 batches: 0.0308
trigger times: 4
Loss after 15737655 batches: 0.0296
trigger times: 5
Loss after 15738613 batches: 0.0289
trigger times: 6
Loss after 15739571 batches: 0.0262
trigger times: 7
Loss after 15740529 batches: 0.0242
trigger times: 8
Loss after 15741487 batches: 0.0225
trigger times: 9
Loss after 15742445 batches: 0.0220
trigger times: 10
Loss after 15743403 batches: 0.0212
trigger times: 11
Loss after 15744361 batches: 0.0196
trigger times: 12
Loss after 15745319 batches: 0.0196
trigger times: 13
Loss after 15746277 batches: 0.0214
trigger times: 14
Loss after 15747235 batches: 0.0194
trigger times: 15
Loss after 15748193 batches: 0.0190
trigger times: 16
Loss after 15749151 batches: 0.0166
trigger times: 17
Loss after 15750109 batches: 0.0167
trigger times: 18
Loss after 15751067 batches: 0.0171
trigger times: 19
Loss after 15752025 batches: 0.0172
trigger times: 20
Loss after 15752983 batches: 0.0174
trigger times: 21
Loss after 15753941 batches: 0.0168
trigger times: 22
Loss after 15754899 batches: 0.0167
trigger times: 23
Loss after 15755857 batches: 0.0162
trigger times: 24
Loss after 15756815 batches: 0.0161
trigger times: 25
Early stopping!
Start to test process.
Loss after 15757773 batches: 0.0152
Time to train on one home:  52.22722244262695
trigger times: 0
Loss after 15758735 batches: 0.0705
trigger times: 1
Loss after 15759697 batches: 0.0636
trigger times: 2
Loss after 15760659 batches: 0.0611
trigger times: 3
Loss after 15761621 batches: 0.0583
trigger times: 4
Loss after 15762583 batches: 0.0554
trigger times: 5
Loss after 15763545 batches: 0.0538
trigger times: 6
Loss after 15764507 batches: 0.0531
trigger times: 7
Loss after 15765469 batches: 0.0526
trigger times: 8
Loss after 15766431 batches: 0.0512
trigger times: 9
Loss after 15767393 batches: 0.0518
trigger times: 10
Loss after 15768355 batches: 0.0514
trigger times: 11
Loss after 15769317 batches: 0.0507
trigger times: 12
Loss after 15770279 batches: 0.0503
trigger times: 13
Loss after 15771241 batches: 0.0501
trigger times: 14
Loss after 15772203 batches: 0.0500
trigger times: 15
Loss after 15773165 batches: 0.0477
trigger times: 16
Loss after 15774127 batches: 0.0485
trigger times: 17
Loss after 15775089 batches: 0.0475
trigger times: 18
Loss after 15776051 batches: 0.0473
trigger times: 19
Loss after 15777013 batches: 0.0471
trigger times: 20
Loss after 15777975 batches: 0.0470
trigger times: 21
Loss after 15778937 batches: 0.0470
trigger times: 22
Loss after 15779899 batches: 0.0467
trigger times: 23
Loss after 15780861 batches: 0.0470
trigger times: 24
Loss after 15781823 batches: 0.0464
trigger times: 25
Early stopping!
Start to test process.
Loss after 15782785 batches: 0.0469
Time to train on one home:  52.58910632133484
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15783748 batches: 0.0657
trigger times: 1
Loss after 15784711 batches: 0.0256
trigger times: 2
Loss after 15785674 batches: 0.0194
trigger times: 3
Loss after 15786637 batches: 0.0162
trigger times: 4
Loss after 15787600 batches: 0.0151
trigger times: 5
Loss after 15788563 batches: 0.0145
trigger times: 6
Loss after 15789526 batches: 0.0138
trigger times: 7
Loss after 15790489 batches: 0.0134
trigger times: 8
Loss after 15791452 batches: 0.0131
trigger times: 9
Loss after 15792415 batches: 0.0130
trigger times: 10
Loss after 15793378 batches: 0.0129
trigger times: 11
Loss after 15794341 batches: 0.0125
trigger times: 12
Loss after 15795304 batches: 0.0126
trigger times: 13
Loss after 15796267 batches: 0.0125
trigger times: 14
Loss after 15797230 batches: 0.0123
trigger times: 15
Loss after 15798193 batches: 0.0123
trigger times: 16
Loss after 15799156 batches: 0.0122
trigger times: 17
Loss after 15800119 batches: 0.0121
trigger times: 18
Loss after 15801082 batches: 0.0121
trigger times: 19
Loss after 15802045 batches: 0.0120
trigger times: 20
Loss after 15803008 batches: 0.0120
trigger times: 21
Loss after 15803971 batches: 0.0118
trigger times: 22
Loss after 15804934 batches: 0.0118
trigger times: 23
Loss after 15805897 batches: 0.0119
trigger times: 24
Loss after 15806860 batches: 0.0118
trigger times: 25
Early stopping!
Start to test process.
Loss after 15807823 batches: 0.0118
Time to train on one home:  52.34816360473633
trigger times: 0
Loss after 15808786 batches: 0.0528
trigger times: 1
Loss after 15809749 batches: 0.0409
trigger times: 2
Loss after 15810712 batches: 0.0361
trigger times: 3
Loss after 15811675 batches: 0.0331
trigger times: 4
Loss after 15812638 batches: 0.0323
trigger times: 5
Loss after 15813601 batches: 0.0287
trigger times: 6
Loss after 15814564 batches: 0.0295
trigger times: 7
Loss after 15815527 batches: 0.0276
trigger times: 8
Loss after 15816490 batches: 0.0281
trigger times: 9
Loss after 15817453 batches: 0.0277
trigger times: 10
Loss after 15818416 batches: 0.0260
trigger times: 11
Loss after 15819379 batches: 0.0259
trigger times: 12
Loss after 15820342 batches: 0.0258
trigger times: 13
Loss after 15821305 batches: 0.0253
trigger times: 14
Loss after 15822268 batches: 0.0246
trigger times: 15
Loss after 15823231 batches: 0.0249
trigger times: 16
Loss after 15824194 batches: 0.0259
trigger times: 17
Loss after 15825157 batches: 0.0252
trigger times: 18
Loss after 15826120 batches: 0.0264
trigger times: 19
Loss after 15827083 batches: 0.0282
trigger times: 20
Loss after 15828046 batches: 0.0271
trigger times: 21
Loss after 15829009 batches: 0.0260
trigger times: 22
Loss after 15829972 batches: 0.0259
trigger times: 23
Loss after 15830935 batches: 0.0274
trigger times: 24
Loss after 15831898 batches: 0.0256
trigger times: 25
Early stopping!
Start to test process.
Loss after 15832861 batches: 0.0266
Time to train on one home:  52.29925465583801
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15833824 batches: 0.0605
trigger times: 1
Loss after 15834787 batches: 0.0490
trigger times: 2
Loss after 15835750 batches: 0.0457
trigger times: 3
Loss after 15836713 batches: 0.0398
trigger times: 4
Loss after 15837676 batches: 0.0342
trigger times: 5
Loss after 15838639 batches: 0.0323
trigger times: 6
Loss after 15839602 batches: 0.0307
trigger times: 7
Loss after 15840565 batches: 0.0297
trigger times: 8
Loss after 15841528 batches: 0.0282
trigger times: 9
Loss after 15842491 batches: 0.0271
trigger times: 10
Loss after 15843454 batches: 0.0259
trigger times: 11
Loss after 15844417 batches: 0.0257
trigger times: 12
Loss after 15845380 batches: 0.0252
trigger times: 13
Loss after 15846343 batches: 0.0246
trigger times: 14
Loss after 15847306 batches: 0.0250
trigger times: 15
Loss after 15848269 batches: 0.0241
trigger times: 16
Loss after 15849232 batches: 0.0239
trigger times: 17
Loss after 15850195 batches: 0.0231
trigger times: 18
Loss after 15851158 batches: 0.0235
trigger times: 19
Loss after 15852121 batches: 0.0233
trigger times: 20
Loss after 15853084 batches: 0.0233
trigger times: 21
Loss after 15854047 batches: 0.0227
trigger times: 22
Loss after 15855010 batches: 0.0222
trigger times: 23
Loss after 15855973 batches: 0.0218
trigger times: 24
Loss after 15856936 batches: 0.0218
trigger times: 25
Early stopping!
Start to test process.
Loss after 15857899 batches: 0.0220
Time to train on one home:  52.6580069065094
trigger times: 0
Loss after 15858862 batches: 0.0532
trigger times: 1
Loss after 15859825 batches: 0.0409
trigger times: 2
Loss after 15860788 batches: 0.0362
trigger times: 3
Loss after 15861751 batches: 0.0328
trigger times: 4
Loss after 15862714 batches: 0.0299
trigger times: 5
Loss after 15863677 batches: 0.0278
trigger times: 6
Loss after 15864640 batches: 0.0280
trigger times: 7
Loss after 15865603 batches: 0.0265
trigger times: 8
Loss after 15866566 batches: 0.0262
trigger times: 9
Loss after 15867529 batches: 0.0253
trigger times: 10
Loss after 15868492 batches: 0.0260
trigger times: 11
Loss after 15869455 batches: 0.0257
trigger times: 12
Loss after 15870418 batches: 0.0254
trigger times: 13
Loss after 15871381 batches: 0.0257
trigger times: 14
Loss after 15872344 batches: 0.0265
trigger times: 15
Loss after 15873307 batches: 0.0259
trigger times: 16
Loss after 15874270 batches: 0.0265
trigger times: 17
Loss after 15875233 batches: 0.0269
trigger times: 18
Loss after 15876196 batches: 0.0290
trigger times: 19
Loss after 15877159 batches: 0.0267
trigger times: 20
Loss after 15878122 batches: 0.0257
trigger times: 21
Loss after 15879085 batches: 0.0259
trigger times: 22
Loss after 15880048 batches: 0.0277
trigger times: 23
Loss after 15881011 batches: 0.0251
trigger times: 24
Loss after 15881974 batches: 0.0241
trigger times: 25
Early stopping!
Start to test process.
Loss after 15882937 batches: 0.0239
Time to train on one home:  52.59464979171753
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 15883900 batches: 0.0491
trigger times: 1
Loss after 15884863 batches: 0.0452
trigger times: 2
Loss after 15885826 batches: 0.0408
trigger times: 3
Loss after 15886789 batches: 0.0383
trigger times: 4
Loss after 15887752 batches: 0.0352
trigger times: 5
Loss after 15888715 batches: 0.0342
trigger times: 6
Loss after 15889678 batches: 0.0331
trigger times: 7
Loss after 15890641 batches: 0.0321
trigger times: 8
Loss after 15891604 batches: 0.0323
trigger times: 9
Loss after 15892567 batches: 0.0308
trigger times: 10
Loss after 15893530 batches: 0.0306
trigger times: 11
Loss after 15894493 batches: 0.0294
trigger times: 12
Loss after 15895456 batches: 0.0289
trigger times: 13
Loss after 15896419 batches: 0.0288
trigger times: 14
Loss after 15897382 batches: 0.0281
trigger times: 15
Loss after 15898345 batches: 0.0277
trigger times: 16
Loss after 15899308 batches: 0.0281
trigger times: 17
Loss after 15900271 batches: 0.0278
trigger times: 18
Loss after 15901234 batches: 0.0273
trigger times: 19
Loss after 15902197 batches: 0.0281
trigger times: 20
Loss after 15903160 batches: 0.0266
trigger times: 21
Loss after 15904123 batches: 0.0265
trigger times: 0
Loss after 15905086 batches: 0.0264
trigger times: 1
Loss after 15906049 batches: 0.0285
trigger times: 2
Loss after 15907012 batches: 0.0272
trigger times: 3
Loss after 15907975 batches: 0.0267
trigger times: 4
Loss after 15908938 batches: 0.0267
trigger times: 5
Loss after 15909901 batches: 0.0257
trigger times: 6
Loss after 15910864 batches: 0.0367
trigger times: 7
Loss after 15911827 batches: 0.0341
trigger times: 8
Loss after 15912790 batches: 0.0312
trigger times: 9
Loss after 15913753 batches: 0.0292
trigger times: 10
Loss after 15914716 batches: 0.0277
trigger times: 11
Loss after 15915679 batches: 0.0267
trigger times: 12
Loss after 15916642 batches: 0.0265
trigger times: 13
Loss after 15917605 batches: 0.0255
trigger times: 14
Loss after 15918568 batches: 0.0253
trigger times: 15
Loss after 15919531 batches: 0.0251
trigger times: 16
Loss after 15920494 batches: 0.0242
trigger times: 17
Loss after 15921457 batches: 0.0237
trigger times: 18
Loss after 15922420 batches: 0.0240
trigger times: 19
Loss after 15923383 batches: 0.0240
trigger times: 20
Loss after 15924346 batches: 0.0237
trigger times: 21
Loss after 15925309 batches: 0.0244
trigger times: 22
Loss after 15926272 batches: 0.0249
trigger times: 23
Loss after 15927235 batches: 0.0240
trigger times: 24
Loss after 15928198 batches: 0.0225
trigger times: 25
Early stopping!
Start to test process.
Loss after 15929161 batches: 0.0223
Time to train on one home:  69.0445191860199
trigger times: 0
Loss after 15930124 batches: 0.0707
trigger times: 1
Loss after 15931087 batches: 0.0477
trigger times: 2
Loss after 15932050 batches: 0.0460
trigger times: 3
Loss after 15933013 batches: 0.0434
trigger times: 4
Loss after 15933976 batches: 0.0405
trigger times: 5
Loss after 15934939 batches: 0.0393
trigger times: 6
Loss after 15935902 batches: 0.0383
trigger times: 7
Loss after 15936865 batches: 0.0372
trigger times: 8
Loss after 15937828 batches: 0.0368
trigger times: 9
Loss after 15938791 batches: 0.0365
trigger times: 10
Loss after 15939754 batches: 0.0360
trigger times: 11
Loss after 15940717 batches: 0.0348
trigger times: 12
Loss after 15941680 batches: 0.0348
trigger times: 13
Loss after 15942643 batches: 0.0347
trigger times: 14
Loss after 15943606 batches: 0.0343
trigger times: 15
Loss after 15944569 batches: 0.0339
trigger times: 16
Loss after 15945532 batches: 0.0339
trigger times: 17
Loss after 15946495 batches: 0.0339
trigger times: 18
Loss after 15947458 batches: 0.0331
trigger times: 19
Loss after 15948421 batches: 0.0324
trigger times: 20
Loss after 15949384 batches: 0.0323
trigger times: 21
Loss after 15950347 batches: 0.0328
trigger times: 22
Loss after 15951310 batches: 0.0329
trigger times: 23
Loss after 15952273 batches: 0.0330
trigger times: 24
Loss after 15953236 batches: 0.0332
trigger times: 25
Early stopping!
Start to test process.
Loss after 15954199 batches: 0.0335
Time to train on one home:  52.425639390945435
trigger times: 0
Loss after 15955162 batches: 0.0953
trigger times: 1
Loss after 15956125 batches: 0.0867
trigger times: 2
Loss after 15957088 batches: 0.0811
trigger times: 3
Loss after 15958051 batches: 0.0776
trigger times: 4
Loss after 15959014 batches: 0.0729
trigger times: 5
Loss after 15959977 batches: 0.0698
trigger times: 6
Loss after 15960940 batches: 0.0674
trigger times: 7
Loss after 15961903 batches: 0.0654
trigger times: 8
Loss after 15962866 batches: 0.0652
trigger times: 9
Loss after 15963829 batches: 0.0634
trigger times: 10
Loss after 15964792 batches: 0.0623
trigger times: 11
Loss after 15965755 batches: 0.0620
trigger times: 12
Loss after 15966718 batches: 0.0602
trigger times: 13
Loss after 15967681 batches: 0.0594
trigger times: 14
Loss after 15968644 batches: 0.0582
trigger times: 15
Loss after 15969607 batches: 0.0571
trigger times: 16
Loss after 15970570 batches: 0.0573
trigger times: 17
Loss after 15971533 batches: 0.0575
trigger times: 18
Loss after 15972496 batches: 0.0568
trigger times: 19
Loss after 15973459 batches: 0.0554
trigger times: 20
Loss after 15974422 batches: 0.0548
trigger times: 21
Loss after 15975385 batches: 0.0554
trigger times: 22
Loss after 15976348 batches: 0.0552
trigger times: 23
Loss after 15977311 batches: 0.0558
trigger times: 24
Loss after 15978274 batches: 0.0559
trigger times: 25
Early stopping!
Start to test process.
Loss after 15979237 batches: 0.0564
Time to train on one home:  52.77677273750305
trigger times: 0
Loss after 15980200 batches: 0.0997
trigger times: 1
Loss after 15981163 batches: 0.0616
trigger times: 2
Loss after 15982126 batches: 0.0549
trigger times: 3
Loss after 15983089 batches: 0.0501
trigger times: 4
Loss after 15984052 batches: 0.0473
trigger times: 5
Loss after 15985015 batches: 0.0445
trigger times: 6
Loss after 15985978 batches: 0.0427
trigger times: 7
Loss after 15986941 batches: 0.0412
trigger times: 8
Loss after 15987904 batches: 0.0402
trigger times: 9
Loss after 15988867 batches: 0.0398
trigger times: 10
Loss after 15989830 batches: 0.0392
trigger times: 11
Loss after 15990793 batches: 0.0376
trigger times: 12
Loss after 15991756 batches: 0.0392
trigger times: 13
Loss after 15992719 batches: 0.0382
trigger times: 14
Loss after 15993682 batches: 0.0372
trigger times: 15
Loss after 15994645 batches: 0.0361
trigger times: 16
Loss after 15995608 batches: 0.0364
trigger times: 17
Loss after 15996571 batches: 0.0346
trigger times: 18
Loss after 15997534 batches: 0.0352
trigger times: 19
Loss after 15998497 batches: 0.0353
trigger times: 20
Loss after 15999460 batches: 0.0344
trigger times: 21
Loss after 16000423 batches: 0.0342
trigger times: 22
Loss after 16001386 batches: 0.0347
trigger times: 23
Loss after 16002349 batches: 0.0349
trigger times: 24
Loss after 16003312 batches: 0.0342
trigger times: 25
Early stopping!
Start to test process.
Loss after 16004275 batches: 0.0347
Time to train on one home:  52.52208995819092
trigger times: 0
Loss after 16005204 batches: 0.1106
trigger times: 1
Loss after 16006133 batches: 0.0656
trigger times: 2
Loss after 16007062 batches: 0.0501
trigger times: 3
Loss after 16007991 batches: 0.0424
trigger times: 4
Loss after 16008920 batches: 0.0372
trigger times: 5
Loss after 16009849 batches: 0.0343
trigger times: 6
Loss after 16010778 batches: 0.0327
trigger times: 7
Loss after 16011707 batches: 0.0312
trigger times: 8
Loss after 16012636 batches: 0.0304
trigger times: 9
Loss after 16013565 batches: 0.0311
trigger times: 10
Loss after 16014494 batches: 0.0285
trigger times: 11
Loss after 16015423 batches: 0.0278
trigger times: 12
Loss after 16016352 batches: 0.0260
trigger times: 13
Loss after 16017281 batches: 0.0290
trigger times: 14
Loss after 16018210 batches: 0.0256
trigger times: 15
Loss after 16019139 batches: 0.0252
trigger times: 16
Loss after 16020068 batches: 0.0265
trigger times: 17
Loss after 16020997 batches: 0.0245
trigger times: 18
Loss after 16021926 batches: 0.0244
trigger times: 19
Loss after 16022855 batches: 0.0234
trigger times: 20
Loss after 16023784 batches: 0.0255
trigger times: 21
Loss after 16024713 batches: 0.0220
trigger times: 22
Loss after 16025642 batches: 0.0245
trigger times: 23
Loss after 16026571 batches: 0.0248
trigger times: 24
Loss after 16027500 batches: 0.0233
trigger times: 25
Early stopping!
Start to test process.
Loss after 16028429 batches: 0.0229
Time to train on one home:  52.146395206451416
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16029392 batches: 0.0602
trigger times: 1
Loss after 16030355 batches: 0.0306
trigger times: 2
Loss after 16031318 batches: 0.0278
trigger times: 3
Loss after 16032281 batches: 0.0272
trigger times: 4
Loss after 16033244 batches: 0.0270
trigger times: 5
Loss after 16034207 batches: 0.0255
trigger times: 6
Loss after 16035170 batches: 0.0245
trigger times: 7
Loss after 16036133 batches: 0.0234
trigger times: 8
Loss after 16037096 batches: 0.0222
trigger times: 9
Loss after 16038059 batches: 0.0212
trigger times: 10
Loss after 16039022 batches: 0.0205
trigger times: 11
Loss after 16039985 batches: 0.0197
trigger times: 12
Loss after 16040948 batches: 0.0194
trigger times: 13
Loss after 16041911 batches: 0.0191
trigger times: 14
Loss after 16042874 batches: 0.0189
trigger times: 15
Loss after 16043837 batches: 0.0184
trigger times: 16
Loss after 16044800 batches: 0.0183
trigger times: 17
Loss after 16045763 batches: 0.0180
trigger times: 18
Loss after 16046726 batches: 0.0176
trigger times: 19
Loss after 16047689 batches: 0.0179
trigger times: 20
Loss after 16048652 batches: 0.0175
trigger times: 21
Loss after 16049615 batches: 0.0172
trigger times: 22
Loss after 16050578 batches: 0.0173
trigger times: 23
Loss after 16051541 batches: 0.0170
trigger times: 24
Loss after 16052504 batches: 0.0170
trigger times: 25
Early stopping!
Start to test process.
Loss after 16053467 batches: 0.0172
Time to train on one home:  52.39046669006348
trigger times: 0
Loss after 16054430 batches: 0.1656
trigger times: 1
Loss after 16055393 batches: 0.1186
trigger times: 2
Loss after 16056356 batches: 0.0978
trigger times: 3
Loss after 16057319 batches: 0.0844
trigger times: 4
Loss after 16058282 batches: 0.0767
trigger times: 5
Loss after 16059245 batches: 0.0737
trigger times: 6
Loss after 16060208 batches: 0.0679
trigger times: 7
Loss after 16061171 batches: 0.0621
trigger times: 8
Loss after 16062134 batches: 0.0598
trigger times: 9
Loss after 16063097 batches: 0.0563
trigger times: 10
Loss after 16064060 batches: 0.0554
trigger times: 11
Loss after 16065023 batches: 0.0541
trigger times: 12
Loss after 16065986 batches: 0.0503
trigger times: 13
Loss after 16066949 batches: 0.0501
trigger times: 14
Loss after 16067912 batches: 0.0475
trigger times: 15
Loss after 16068875 batches: 0.0482
trigger times: 16
Loss after 16069838 batches: 0.0470
trigger times: 17
Loss after 16070801 batches: 0.0469
trigger times: 18
Loss after 16071764 batches: 0.0452
trigger times: 19
Loss after 16072727 batches: 0.0465
trigger times: 20
Loss after 16073690 batches: 0.0446
trigger times: 21
Loss after 16074653 batches: 0.0423
trigger times: 22
Loss after 16075616 batches: 0.0413
trigger times: 23
Loss after 16076579 batches: 0.0430
trigger times: 24
Loss after 16077542 batches: 0.0414
trigger times: 25
Early stopping!
Start to test process.
Loss after 16078505 batches: 0.0423
Time to train on one home:  55.213714361190796
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16079468 batches: 0.0794
trigger times: 1
Loss after 16080431 batches: 0.0717
trigger times: 2
Loss after 16081394 batches: 0.0674
trigger times: 3
Loss after 16082357 batches: 0.0650
trigger times: 4
Loss after 16083320 batches: 0.0613
trigger times: 5
Loss after 16084283 batches: 0.0603
trigger times: 6
Loss after 16085246 batches: 0.0583
trigger times: 7
Loss after 16086209 batches: 0.0565
trigger times: 8
Loss after 16087172 batches: 0.0569
trigger times: 9
Loss after 16088135 batches: 0.0569
trigger times: 10
Loss after 16089098 batches: 0.0543
trigger times: 11
Loss after 16090061 batches: 0.0558
trigger times: 12
Loss after 16091024 batches: 0.0544
trigger times: 13
Loss after 16091987 batches: 0.0540
trigger times: 14
Loss after 16092950 batches: 0.0518
trigger times: 15
Loss after 16093913 batches: 0.0535
trigger times: 16
Loss after 16094876 batches: 0.0537
trigger times: 17
Loss after 16095839 batches: 0.0533
trigger times: 18
Loss after 16096802 batches: 0.0536
trigger times: 19
Loss after 16097765 batches: 0.0523
trigger times: 20
Loss after 16098728 batches: 0.0521
trigger times: 21
Loss after 16099691 batches: 0.0510
trigger times: 22
Loss after 16100654 batches: 0.0519
trigger times: 23
Loss after 16101617 batches: 0.0512
trigger times: 24
Loss after 16102580 batches: 0.0505
trigger times: 25
Early stopping!
Start to test process.
Loss after 16103543 batches: 0.0497
Time to train on one home:  53.64431071281433
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16104506 batches: 0.0817
trigger times: 1
Loss after 16105469 batches: 0.0741
trigger times: 2
Loss after 16106432 batches: 0.0713
trigger times: 3
Loss after 16107395 batches: 0.0658
trigger times: 4
Loss after 16108358 batches: 0.0632
trigger times: 5
Loss after 16109321 batches: 0.0609
trigger times: 6
Loss after 16110284 batches: 0.0580
trigger times: 7
Loss after 16111247 batches: 0.0559
trigger times: 8
Loss after 16112210 batches: 0.0546
trigger times: 9
Loss after 16113173 batches: 0.0537
trigger times: 10
Loss after 16114136 batches: 0.0508
trigger times: 11
Loss after 16115099 batches: 0.0500
trigger times: 12
Loss after 16116062 batches: 0.0502
trigger times: 13
Loss after 16117025 batches: 0.0494
trigger times: 14
Loss after 16117988 batches: 0.0496
trigger times: 15
Loss after 16118951 batches: 0.0503
trigger times: 16
Loss after 16119914 batches: 0.0504
trigger times: 17
Loss after 16120877 batches: 0.0492
trigger times: 18
Loss after 16121840 batches: 0.0478
trigger times: 19
Loss after 16122803 batches: 0.0453
trigger times: 20
Loss after 16123766 batches: 0.0459
trigger times: 21
Loss after 16124729 batches: 0.0454
trigger times: 22
Loss after 16125692 batches: 0.0445
trigger times: 23
Loss after 16126655 batches: 0.0460
trigger times: 24
Loss after 16127618 batches: 0.0463
trigger times: 25
Early stopping!
Start to test process.
Loss after 16128581 batches: 0.0444
Time to train on one home:  52.6976854801178
trigger times: 0
Loss after 16129544 batches: 0.0209
trigger times: 1
Loss after 16130507 batches: 0.0181
trigger times: 2
Loss after 16131470 batches: 0.0168
trigger times: 3
Loss after 16132433 batches: 0.0155
trigger times: 4
Loss after 16133396 batches: 0.0141
trigger times: 5
Loss after 16134359 batches: 0.0136
trigger times: 6
Loss after 16135322 batches: 0.0137
trigger times: 7
Loss after 16136285 batches: 0.0128
trigger times: 8
Loss after 16137248 batches: 0.0132
trigger times: 9
Loss after 16138211 batches: 0.0124
trigger times: 10
Loss after 16139174 batches: 0.0120
trigger times: 11
Loss after 16140137 batches: 0.0121
trigger times: 12
Loss after 16141100 batches: 0.0120
trigger times: 13
Loss after 16142063 batches: 0.0123
trigger times: 14
Loss after 16143026 batches: 0.0118
trigger times: 15
Loss after 16143989 batches: 0.0114
trigger times: 16
Loss after 16144952 batches: 0.0115
trigger times: 17
Loss after 16145915 batches: 0.0117
trigger times: 18
Loss after 16146878 batches: 0.0122
trigger times: 19
Loss after 16147841 batches: 0.0114
trigger times: 20
Loss after 16148804 batches: 0.0111
trigger times: 21
Loss after 16149767 batches: 0.0113
trigger times: 22
Loss after 16150730 batches: 0.0111
trigger times: 23
Loss after 16151693 batches: 0.0111
trigger times: 24
Loss after 16152656 batches: 0.0115
trigger times: 25
Early stopping!
Start to test process.
Loss after 16153619 batches: 0.0111
Time to train on one home:  52.195913791656494
trigger times: 0
Loss after 16154582 batches: 0.0420
trigger times: 1
Loss after 16155545 batches: 0.0324
trigger times: 2
Loss after 16156508 batches: 0.0298
trigger times: 3
Loss after 16157471 batches: 0.0280
trigger times: 4
Loss after 16158434 batches: 0.0259
trigger times: 5
Loss after 16159397 batches: 0.0244
trigger times: 6
Loss after 16160360 batches: 0.0226
trigger times: 7
Loss after 16161323 batches: 0.0226
trigger times: 8
Loss after 16162286 batches: 0.0222
trigger times: 9
Loss after 16163249 batches: 0.0209
trigger times: 10
Loss after 16164212 batches: 0.0221
trigger times: 11
Loss after 16165175 batches: 0.0219
trigger times: 12
Loss after 16166138 batches: 0.0214
trigger times: 13
Loss after 16167101 batches: 0.0205
trigger times: 14
Loss after 16168064 batches: 0.0208
trigger times: 15
Loss after 16169027 batches: 0.0201
trigger times: 16
Loss after 16169990 batches: 0.0205
trigger times: 17
Loss after 16170953 batches: 0.0200
trigger times: 18
Loss after 16171916 batches: 0.0195
trigger times: 19
Loss after 16172879 batches: 0.0194
trigger times: 20
Loss after 16173842 batches: 0.0197
trigger times: 21
Loss after 16174805 batches: 0.0199
trigger times: 22
Loss after 16175768 batches: 0.0190
trigger times: 23
Loss after 16176731 batches: 0.0184
trigger times: 24
Loss after 16177694 batches: 0.0180
trigger times: 25
Early stopping!
Start to test process.
Loss after 16178657 batches: 0.0182
Time to train on one home:  52.47087836265564
trigger times: 0
Loss after 16179620 batches: 0.0876
trigger times: 1
Loss after 16180583 batches: 0.0622
trigger times: 2
Loss after 16181546 batches: 0.0558
trigger times: 3
Loss after 16182509 batches: 0.0486
trigger times: 4
Loss after 16183472 batches: 0.0478
trigger times: 5
Loss after 16184435 batches: 0.0440
trigger times: 6
Loss after 16185398 batches: 0.0427
trigger times: 7
Loss after 16186361 batches: 0.0416
trigger times: 8
Loss after 16187324 batches: 0.0403
trigger times: 9
Loss after 16188287 batches: 0.0385
trigger times: 10
Loss after 16189250 batches: 0.0395
trigger times: 11
Loss after 16190213 batches: 0.0391
trigger times: 12
Loss after 16191176 batches: 0.0372
trigger times: 13
Loss after 16192139 batches: 0.0359
trigger times: 14
Loss after 16193102 batches: 0.0367
trigger times: 15
Loss after 16194065 batches: 0.0371
trigger times: 16
Loss after 16195028 batches: 0.0372
trigger times: 17
Loss after 16195991 batches: 0.0361
trigger times: 18
Loss after 16196954 batches: 0.0357
trigger times: 19
Loss after 16197917 batches: 0.0350
trigger times: 20
Loss after 16198880 batches: 0.0332
trigger times: 21
Loss after 16199843 batches: 0.0355
trigger times: 22
Loss after 16200806 batches: 0.0345
trigger times: 23
Loss after 16201769 batches: 0.0344
trigger times: 24
Loss after 16202732 batches: 0.0342
trigger times: 25
Early stopping!
Start to test process.
Loss after 16203695 batches: 0.0330
Time to train on one home:  52.40194082260132
trigger times: 0
Loss after 16204658 batches: 0.0997
trigger times: 1
Loss after 16205621 batches: 0.0618
trigger times: 2
Loss after 16206584 batches: 0.0546
trigger times: 3
Loss after 16207547 batches: 0.0501
trigger times: 4
Loss after 16208510 batches: 0.0473
trigger times: 5
Loss after 16209473 batches: 0.0442
trigger times: 6
Loss after 16210436 batches: 0.0434
trigger times: 7
Loss after 16211399 batches: 0.0410
trigger times: 8
Loss after 16212362 batches: 0.0397
trigger times: 9
Loss after 16213325 batches: 0.0392
trigger times: 10
Loss after 16214288 batches: 0.0390
trigger times: 11
Loss after 16215251 batches: 0.0381
trigger times: 12
Loss after 16216214 batches: 0.0375
trigger times: 13
Loss after 16217177 batches: 0.0384
trigger times: 14
Loss after 16218140 batches: 0.0380
trigger times: 15
Loss after 16219103 batches: 0.0369
trigger times: 16
Loss after 16220066 batches: 0.0366
trigger times: 17
Loss after 16221029 batches: 0.0357
trigger times: 18
Loss after 16221992 batches: 0.0356
trigger times: 19
Loss after 16222955 batches: 0.0354
trigger times: 20
Loss after 16223918 batches: 0.0365
trigger times: 21
Loss after 16224881 batches: 0.0352
trigger times: 22
Loss after 16225844 batches: 0.0356
trigger times: 23
Loss after 16226807 batches: 0.0365
trigger times: 24
Loss after 16227770 batches: 0.0354
trigger times: 25
Early stopping!
Start to test process.
Loss after 16228733 batches: 0.0340
Time to train on one home:  52.530937910079956
trigger times: 0
Loss after 16229696 batches: 0.0536
trigger times: 1
Loss after 16230659 batches: 0.0398
trigger times: 2
Loss after 16231622 batches: 0.0371
trigger times: 3
Loss after 16232585 batches: 0.0332
trigger times: 4
Loss after 16233548 batches: 0.0311
trigger times: 5
Loss after 16234511 batches: 0.0310
trigger times: 6
Loss after 16235474 batches: 0.0293
trigger times: 7
Loss after 16236437 batches: 0.0285
trigger times: 8
Loss after 16237400 batches: 0.0306
trigger times: 9
Loss after 16238363 batches: 0.0278
trigger times: 10
Loss after 16239326 batches: 0.0292
trigger times: 11
Loss after 16240289 batches: 0.0276
trigger times: 12
Loss after 16241252 batches: 0.0256
trigger times: 13
Loss after 16242215 batches: 0.0258
trigger times: 14
Loss after 16243178 batches: 0.0249
trigger times: 15
Loss after 16244141 batches: 0.0241
trigger times: 16
Loss after 16245104 batches: 0.0244
trigger times: 17
Loss after 16246067 batches: 0.0260
trigger times: 18
Loss after 16247030 batches: 0.0258
trigger times: 19
Loss after 16247993 batches: 0.0261
trigger times: 20
Loss after 16248956 batches: 0.0252
trigger times: 21
Loss after 16249919 batches: 0.0253
trigger times: 22
Loss after 16250882 batches: 0.0248
trigger times: 23
Loss after 16251845 batches: 0.0240
trigger times: 24
Loss after 16252808 batches: 0.0246
trigger times: 25
Early stopping!
Start to test process.
Loss after 16253771 batches: 0.0254
Time to train on one home:  52.80291032791138
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16254734 batches: 0.0846
trigger times: 1
Loss after 16255697 batches: 0.0496
trigger times: 2
Loss after 16256660 batches: 0.0493
trigger times: 3
Loss after 16257623 batches: 0.0418
trigger times: 0
Loss after 16258586 batches: 0.0378
trigger times: 0
Loss after 16259549 batches: 0.0344
trigger times: 0
Loss after 16260512 batches: 0.0327
trigger times: 1
Loss after 16261475 batches: 0.0317
trigger times: 2
Loss after 16262438 batches: 0.0291
trigger times: 3
Loss after 16263401 batches: 0.0297
trigger times: 4
Loss after 16264364 batches: 0.0281
trigger times: 5
Loss after 16265327 batches: 0.0268
trigger times: 0
Loss after 16266290 batches: 0.0254
trigger times: 1
Loss after 16267253 batches: 0.0257
trigger times: 2
Loss after 16268216 batches: 0.0259
trigger times: 3
Loss after 16269179 batches: 0.0260
trigger times: 4
Loss after 16270142 batches: 0.0257
trigger times: 5
Loss after 16271105 batches: 0.0250
trigger times: 6
Loss after 16272068 batches: 0.0254
trigger times: 7
Loss after 16273031 batches: 0.0250
trigger times: 8
Loss after 16273994 batches: 0.0246
trigger times: 9
Loss after 16274957 batches: 0.0245
trigger times: 10
Loss after 16275920 batches: 0.0231
trigger times: 11
Loss after 16276883 batches: 0.0223
trigger times: 12
Loss after 16277846 batches: 0.0235
trigger times: 0
Loss after 16278809 batches: 0.0214
trigger times: 1
Loss after 16279772 batches: 0.0226
trigger times: 2
Loss after 16280735 batches: 0.0229
trigger times: 3
Loss after 16281698 batches: 0.0224
trigger times: 4
Loss after 16282661 batches: 0.0221
trigger times: 5
Loss after 16283624 batches: 0.0238
trigger times: 6
Loss after 16284587 batches: 0.0233
trigger times: 7
Loss after 16285550 batches: 0.0231
trigger times: 8
Loss after 16286513 batches: 0.0230
trigger times: 9
Loss after 16287476 batches: 0.0226
trigger times: 10
Loss after 16288439 batches: 0.0212
trigger times: 11
Loss after 16289402 batches: 0.0208
trigger times: 12
Loss after 16290365 batches: 0.0208
trigger times: 13
Loss after 16291328 batches: 0.0210
trigger times: 0
Loss after 16292291 batches: 0.0212
trigger times: 1
Loss after 16293254 batches: 0.0212
trigger times: 2
Loss after 16294217 batches: 0.0211
trigger times: 3
Loss after 16295180 batches: 0.0208
trigger times: 4
Loss after 16296143 batches: 0.0218
trigger times: 5
Loss after 16297106 batches: 0.0206
trigger times: 6
Loss after 16298069 batches: 0.0203
trigger times: 7
Loss after 16299032 batches: 0.0194
trigger times: 8
Loss after 16299995 batches: 0.0199
trigger times: 9
Loss after 16300958 batches: 0.0196
trigger times: 10
Loss after 16301921 batches: 0.0193
trigger times: 11
Loss after 16302884 batches: 0.0198
trigger times: 12
Loss after 16303847 batches: 0.0197
trigger times: 13
Loss after 16304810 batches: 0.0187
trigger times: 14
Loss after 16305773 batches: 0.0188
trigger times: 15
Loss after 16306736 batches: 0.0187
trigger times: 16
Loss after 16307699 batches: 0.0184
trigger times: 17
Loss after 16308662 batches: 0.0183
trigger times: 18
Loss after 16309625 batches: 0.0184
trigger times: 0
Loss after 16310588 batches: 0.0181
trigger times: 1
Loss after 16311551 batches: 0.0179
trigger times: 2
Loss after 16312514 batches: 0.0172
trigger times: 3
Loss after 16313477 batches: 0.0184
trigger times: 4
Loss after 16314440 batches: 0.0175
trigger times: 5
Loss after 16315403 batches: 0.0180
trigger times: 6
Loss after 16316366 batches: 0.0182
trigger times: 7
Loss after 16317329 batches: 0.0180
trigger times: 8
Loss after 16318292 batches: 0.0172
trigger times: 9
Loss after 16319255 batches: 0.0170
trigger times: 10
Loss after 16320218 batches: 0.0189
trigger times: 11
Loss after 16321181 batches: 0.0178
trigger times: 12
Loss after 16322144 batches: 0.0174
trigger times: 13
Loss after 16323107 batches: 0.0175
trigger times: 14
Loss after 16324070 batches: 0.0166
trigger times: 15
Loss after 16325033 batches: 0.0173
trigger times: 16
Loss after 16325996 batches: 0.0170
trigger times: 17
Loss after 16326959 batches: 0.0181
trigger times: 18
Loss after 16327922 batches: 0.0177
trigger times: 19
Loss after 16328885 batches: 0.0168
trigger times: 20
Loss after 16329848 batches: 0.0170
trigger times: 21
Loss after 16330811 batches: 0.0160
trigger times: 22
Loss after 16331774 batches: 0.0161
trigger times: 0
Loss after 16332737 batches: 0.0154
trigger times: 1
Loss after 16333700 batches: 0.0159
trigger times: 2
Loss after 16334663 batches: 0.0165
trigger times: 3
Loss after 16335626 batches: 0.0163
trigger times: 4
Loss after 16336589 batches: 0.0164
trigger times: 5
Loss after 16337552 batches: 0.0160
trigger times: 6
Loss after 16338515 batches: 0.0166
trigger times: 7
Loss after 16339478 batches: 0.0160
trigger times: 8
Loss after 16340441 batches: 0.0156
trigger times: 9
Loss after 16341404 batches: 0.0162
trigger times: 10
Loss after 16342367 batches: 0.0161
trigger times: 11
Loss after 16343330 batches: 0.0160
trigger times: 12
Loss after 16344293 batches: 0.0156
trigger times: 0
Loss after 16345256 batches: 0.0157
trigger times: 1
Loss after 16346219 batches: 0.0151
trigger times: 2
Loss after 16347182 batches: 0.0164
trigger times: 3
Loss after 16348145 batches: 0.0158
trigger times: 4
Loss after 16349108 batches: 0.0151
trigger times: 5
Loss after 16350071 batches: 0.0158
trigger times: 6
Loss after 16351034 batches: 0.0157
trigger times: 7
Loss after 16351997 batches: 0.0163
trigger times: 8
Loss after 16352960 batches: 0.0161
trigger times: 9
Loss after 16353923 batches: 0.0152
trigger times: 10
Loss after 16354886 batches: 0.0167
trigger times: 11
Loss after 16355849 batches: 0.0161
trigger times: 12
Loss after 16356812 batches: 0.0155
trigger times: 13
Loss after 16357775 batches: 0.0163
trigger times: 14
Loss after 16358738 batches: 0.0159
trigger times: 15
Loss after 16359701 batches: 0.0153
trigger times: 16
Loss after 16360664 batches: 0.0156
trigger times: 17
Loss after 16361627 batches: 0.0153
trigger times: 18
Loss after 16362590 batches: 0.0147
trigger times: 19
Loss after 16363553 batches: 0.0154
trigger times: 20
Loss after 16364516 batches: 0.0150
trigger times: 21
Loss after 16365479 batches: 0.0148
trigger times: 22
Loss after 16366442 batches: 0.0137
trigger times: 23
Loss after 16367405 batches: 0.0146
trigger times: 24
Loss after 16368368 batches: 0.0136
trigger times: 25
Early stopping!
Start to test process.
Loss after 16369331 batches: 0.0131
Time to train on one home:  123.7974591255188
trigger times: 0
Loss after 16370294 batches: 0.0934
trigger times: 1
Loss after 16371257 batches: 0.0779
trigger times: 2
Loss after 16372220 batches: 0.0755
trigger times: 3
Loss after 16373183 batches: 0.0733
trigger times: 4
Loss after 16374146 batches: 0.0700
trigger times: 5
Loss after 16375109 batches: 0.0679
trigger times: 6
Loss after 16376072 batches: 0.0657
trigger times: 7
Loss after 16377035 batches: 0.0652
trigger times: 8
Loss after 16377998 batches: 0.0622
trigger times: 9
Loss after 16378961 batches: 0.0623
trigger times: 10
Loss after 16379924 batches: 0.0608
trigger times: 11
Loss after 16380887 batches: 0.0598
trigger times: 12
Loss after 16381850 batches: 0.0590
trigger times: 13
Loss after 16382813 batches: 0.0588
trigger times: 14
Loss after 16383776 batches: 0.0577
trigger times: 15
Loss after 16384739 batches: 0.0581
trigger times: 16
Loss after 16385702 batches: 0.0563
trigger times: 17
Loss after 16386665 batches: 0.0574
trigger times: 18
Loss after 16387628 batches: 0.0577
trigger times: 19
Loss after 16388591 batches: 0.0559
trigger times: 20
Loss after 16389554 batches: 0.0555
trigger times: 21
Loss after 16390517 batches: 0.0541
trigger times: 22
Loss after 16391480 batches: 0.0542
trigger times: 23
Loss after 16392443 batches: 0.0547
trigger times: 24
Loss after 16393406 batches: 0.0553
trigger times: 25
Early stopping!
Start to test process.
Loss after 16394369 batches: 0.0526
Time to train on one home:  52.457807779312134
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16395332 batches: 0.0843
trigger times: 1
Loss after 16396295 batches: 0.0496
trigger times: 2
Loss after 16397258 batches: 0.0493
trigger times: 3
Loss after 16398221 batches: 0.0403
trigger times: 0
Loss after 16399184 batches: 0.0369
trigger times: 1
Loss after 16400147 batches: 0.0347
trigger times: 2
Loss after 16401110 batches: 0.0322
trigger times: 3
Loss after 16402073 batches: 0.0309
trigger times: 4
Loss after 16403036 batches: 0.0302
trigger times: 0
Loss after 16403999 batches: 0.0285
trigger times: 1
Loss after 16404962 batches: 0.0283
trigger times: 2
Loss after 16405925 batches: 0.0268
trigger times: 3
Loss after 16406888 batches: 0.0257
trigger times: 4
Loss after 16407851 batches: 0.0253
trigger times: 5
Loss after 16408814 batches: 0.0246
trigger times: 6
Loss after 16409777 batches: 0.0262
trigger times: 0
Loss after 16410740 batches: 0.0245
trigger times: 1
Loss after 16411703 batches: 0.0239
trigger times: 2
Loss after 16412666 batches: 0.0238
trigger times: 3
Loss after 16413629 batches: 0.0247
trigger times: 4
Loss after 16414592 batches: 0.0250
trigger times: 5
Loss after 16415555 batches: 0.0244
trigger times: 6
Loss after 16416518 batches: 0.0231
trigger times: 7
Loss after 16417481 batches: 0.0229
trigger times: 8
Loss after 16418444 batches: 0.0229
trigger times: 9
Loss after 16419407 batches: 0.0221
trigger times: 10
Loss after 16420370 batches: 0.0228
trigger times: 11
Loss after 16421333 batches: 0.0222
trigger times: 12
Loss after 16422296 batches: 0.0222
trigger times: 13
Loss after 16423259 batches: 0.0213
trigger times: 14
Loss after 16424222 batches: 0.0224
trigger times: 15
Loss after 16425185 batches: 0.0219
trigger times: 16
Loss after 16426148 batches: 0.0201
trigger times: 17
Loss after 16427111 batches: 0.0212
trigger times: 18
Loss after 16428074 batches: 0.0215
trigger times: 19
Loss after 16429037 batches: 0.0211
trigger times: 20
Loss after 16430000 batches: 0.0219
trigger times: 21
Loss after 16430963 batches: 0.0213
trigger times: 22
Loss after 16431926 batches: 0.0213
trigger times: 23
Loss after 16432889 batches: 0.0223
trigger times: 24
Loss after 16433852 batches: 0.0208
trigger times: 25
Early stopping!
Start to test process.
Loss after 16434815 batches: 0.0197
Time to train on one home:  64.52910161018372
trigger times: 0
Loss after 16435710 batches: 0.0739
trigger times: 1
Loss after 16436605 batches: 0.0442
trigger times: 2
Loss after 16437500 batches: 0.0194
trigger times: 3
Loss after 16438395 batches: 0.0120
trigger times: 4
Loss after 16439290 batches: 0.0123
trigger times: 5
Loss after 16440185 batches: 0.0087
trigger times: 6
Loss after 16441080 batches: 0.0068
trigger times: 7
Loss after 16441975 batches: 0.0057
trigger times: 8
Loss after 16442870 batches: 0.0056
trigger times: 9
Loss after 16443765 batches: 0.0041
trigger times: 10
Loss after 16444660 batches: 0.0040
trigger times: 11
Loss after 16445555 batches: 0.0035
trigger times: 12
Loss after 16446450 batches: 0.0033
trigger times: 13
Loss after 16447345 batches: 0.0030
trigger times: 14
Loss after 16448240 batches: 0.0033
trigger times: 15
Loss after 16449135 batches: 0.0036
trigger times: 16
Loss after 16450030 batches: 0.0030
trigger times: 17
Loss after 16450925 batches: 0.0024
trigger times: 18
Loss after 16451820 batches: 0.0024
trigger times: 19
Loss after 16452715 batches: 0.0022
trigger times: 20
Loss after 16453610 batches: 0.0020
trigger times: 21
Loss after 16454505 batches: 0.0021
trigger times: 22
Loss after 16455400 batches: 0.0023
trigger times: 23
Loss after 16456295 batches: 0.0024
trigger times: 24
Loss after 16457190 batches: 0.0039
trigger times: 25
Early stopping!
Start to test process.
Loss after 16458085 batches: 0.0033
Time to train on one home:  51.66604208946228
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16459022 batches: 0.0802
trigger times: 1
Loss after 16459959 batches: 0.0677
trigger times: 2
Loss after 16460896 batches: 0.0634
trigger times: 3
Loss after 16461833 batches: 0.0609
trigger times: 4
Loss after 16462770 batches: 0.0582
trigger times: 5
Loss after 16463707 batches: 0.0547
trigger times: 6
Loss after 16464644 batches: 0.0542
trigger times: 7
Loss after 16465581 batches: 0.0532
trigger times: 8
Loss after 16466518 batches: 0.0522
trigger times: 9
Loss after 16467455 batches: 0.0513
trigger times: 10
Loss after 16468392 batches: 0.0503
trigger times: 11
Loss after 16469329 batches: 0.0513
trigger times: 12
Loss after 16470266 batches: 0.0510
trigger times: 13
Loss after 16471203 batches: 0.0496
trigger times: 14
Loss after 16472140 batches: 0.0476
trigger times: 15
Loss after 16473077 batches: 0.0485
trigger times: 16
Loss after 16474014 batches: 0.0478
trigger times: 17
Loss after 16474951 batches: 0.0479
trigger times: 18
Loss after 16475888 batches: 0.0459
trigger times: 19
Loss after 16476825 batches: 0.0473
trigger times: 20
Loss after 16477762 batches: 0.0462
trigger times: 21
Loss after 16478699 batches: 0.0464
trigger times: 22
Loss after 16479636 batches: 0.0458
trigger times: 23
Loss after 16480573 batches: 0.0466
trigger times: 24
Loss after 16481510 batches: 0.0452
trigger times: 25
Early stopping!
Start to test process.
Loss after 16482447 batches: 0.0451
Time to train on one home:  52.77139139175415
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066, 0.03401813118363776, 0.033508403208926535, 0.03273922320832277, 0.032162070075287874, 0.031213877056718613, 0.03093800142524674, 0.030100551508447693]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863], [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861], [0.0757279247045517, 0.10266384027132536, 0.5909740425258612, 0.8803234656976516, 0.8076583271010261, 28.206427011804568, 8296.775], [0.06181953847408295, 0.10445801892372542, 0.5961459732150404, 0.831618823847048, 0.806043429565136, 26.645882531252276, 8280.187], [0.07333928346633911, 0.09887557787656209, 0.5904951093552152, 0.8805827319948272, 0.8110679704670578, 28.214734158180708, 8331.802], [0.06249981373548508, 0.1056225345966233, 0.6023961404422763, 0.8395811685763604, 0.8049953091236983, 26.90100386358231, 8269.419], [0.07111044973134995, 0.11364462231541128, 0.6015437535926366, 0.8360909977276793, 0.7977749276509928, 26.78917536742376, 8195.247], [0.06206701323390007, 0.09726051170449412, 0.5975866108942892, 0.8595424626232535, 0.8125216526577462, 27.540583297204083, 8346.734]]
Round_14_results:  [0.06206701323390007, 0.09726051170449412, 0.5975866108942892, 0.8595424626232535, 0.8125216526577462, 27.540583297204083, 8346.734]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 17194 < 17195; dropping {'Training_Loss': 0.07883656663554055, 'Validation_Loss': 0.07712003588676453, 'Training_R2': -0.12266947145524121, 'Validation_R2': 0.12141771595441131, 'Training_F1': 0.4095898846474024, 'Validation_F1': 0.609785366800524, 'Training_NEP': 0.8326216785408521, 'Validation_NEP': 0.8071833742858924, 'Training_NDE': 0.683732931871838, 'Validation_NDE': 0.7937782444255287, 'Training_MAE': 31.639891672151894, 'Validation_MAE': 28.702854556024842, 'Training_MSE': 2525.29, 'Validation_MSE': 10398.505}.
trigger times: 0
Loss after 16483409 batches: 0.0788
trigger times: 1
Loss after 16484371 batches: 0.0636
trigger times: 2
Loss after 16485333 batches: 0.0629
trigger times: 3
Loss after 16486295 batches: 0.0596
trigger times: 4
Loss after 16487257 batches: 0.0568
trigger times: 5
Loss after 16488219 batches: 0.0551
trigger times: 6
Loss after 16489181 batches: 0.0529
trigger times: 7
Loss after 16490143 batches: 0.0515
trigger times: 8
Loss after 16491105 batches: 0.0511
trigger times: 9
Loss after 16492067 batches: 0.0500
trigger times: 10
Loss after 16493029 batches: 0.0499
trigger times: 11
Loss after 16493991 batches: 0.0494
trigger times: 12
Loss after 16494953 batches: 0.0496
trigger times: 13
Loss after 16495915 batches: 0.0489
trigger times: 14
Loss after 16496877 batches: 0.0485
trigger times: 15
Loss after 16497839 batches: 0.0476
trigger times: 16
Loss after 16498801 batches: 0.0484
trigger times: 17
Loss after 16499763 batches: 0.0479
trigger times: 18
Loss after 16500725 batches: 0.0468
trigger times: 19
Loss after 16501687 batches: 0.0475
trigger times: 20
Loss after 16502649 batches: 0.0467
trigger times: 21
Loss after 16503611 batches: 0.0465
trigger times: 22
Loss after 16504573 batches: 0.0470
trigger times: 23
Loss after 16505535 batches: 0.0474
trigger times: 24
Loss after 16506497 batches: 0.0474
trigger times: 25
Early stopping!
Start to test process.
Loss after 16507459 batches: 0.0461
Time to train on one home:  52.58125925064087
trigger times: 0
Loss after 16508388 batches: 0.0908
trigger times: 1
Loss after 16509317 batches: 0.0617
trigger times: 2
Loss after 16510246 batches: 0.0455
trigger times: 3
Loss after 16511175 batches: 0.0379
trigger times: 4
Loss after 16512104 batches: 0.0354
trigger times: 5
Loss after 16513033 batches: 0.0328
trigger times: 6
Loss after 16513962 batches: 0.0309
trigger times: 7
Loss after 16514891 batches: 0.0306
trigger times: 8
Loss after 16515820 batches: 0.0290
trigger times: 9
Loss after 16516749 batches: 0.0295
trigger times: 10
Loss after 16517678 batches: 0.0268
trigger times: 11
Loss after 16518607 batches: 0.0258
trigger times: 12
Loss after 16519536 batches: 0.0255
trigger times: 13
Loss after 16520465 batches: 0.0276
trigger times: 14
Loss after 16521394 batches: 0.0259
trigger times: 15
Loss after 16522323 batches: 0.0258
trigger times: 16
Loss after 16523252 batches: 0.0246
trigger times: 17
Loss after 16524181 batches: 0.0245
trigger times: 18
Loss after 16525110 batches: 0.0234
trigger times: 19
Loss after 16526039 batches: 0.0252
trigger times: 20
Loss after 16526968 batches: 0.0244
trigger times: 21
Loss after 16527897 batches: 0.0244
trigger times: 22
Loss after 16528826 batches: 0.0241
trigger times: 23
Loss after 16529755 batches: 0.0222
trigger times: 24
Loss after 16530684 batches: 0.0243
trigger times: 25
Early stopping!
Start to test process.
Loss after 16531613 batches: 0.0218
Time to train on one home:  52.059181928634644
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 16532576 batches: 0.0559
trigger times: 1
Loss after 16533539 batches: 0.0188
trigger times: 2
Loss after 16534502 batches: 0.0145
trigger times: 3
Loss after 16535465 batches: 0.0141
trigger times: 4
Loss after 16536428 batches: 0.0138
trigger times: 5
Loss after 16537391 batches: 0.0135
trigger times: 6
Loss after 16538354 batches: 0.0131
trigger times: 7
Loss after 16539317 batches: 0.0128
trigger times: 8
Loss after 16540280 batches: 0.0120
trigger times: 9
Loss after 16541243 batches: 0.0113
trigger times: 10
Loss after 16542206 batches: 0.0105
trigger times: 11
Loss after 16543169 batches: 0.0100
trigger times: 12
Loss after 16544132 batches: 0.0094
trigger times: 13
Loss after 16545095 batches: 0.0089
trigger times: 14
Loss after 16546058 batches: 0.0089
trigger times: 15
Loss after 16547021 batches: 0.0082
trigger times: 16
Loss after 16547984 batches: 0.0078
trigger times: 17
Loss after 16548947 batches: 0.0075
trigger times: 18
Loss after 16549910 batches: 0.0076
trigger times: 19
Loss after 16550873 batches: 0.0074
trigger times: 20
Loss after 16551836 batches: 0.0072
trigger times: 21
Loss after 16552799 batches: 0.0073
trigger times: 22
Loss after 16553762 batches: 0.0071
trigger times: 23
Loss after 16554725 batches: 0.0068
trigger times: 24
Loss after 16555688 batches: 0.0066
trigger times: 25
Early stopping!
Start to test process.
Loss after 16556651 batches: 0.0070
Time to train on one home:  52.58184552192688
trigger times: 0
Loss after 16557614 batches: 0.0228
trigger times: 1
Loss after 16558577 batches: 0.0179
trigger times: 2
Loss after 16559540 batches: 0.0170
trigger times: 3
Loss after 16560503 batches: 0.0159
trigger times: 4
Loss after 16561466 batches: 0.0141
trigger times: 5
Loss after 16562429 batches: 0.0140
trigger times: 6
Loss after 16563392 batches: 0.0131
trigger times: 7
Loss after 16564355 batches: 0.0129
trigger times: 8
Loss after 16565318 batches: 0.0125
trigger times: 9
Loss after 16566281 batches: 0.0124
trigger times: 10
Loss after 16567244 batches: 0.0120
trigger times: 11
Loss after 16568207 batches: 0.0125
trigger times: 12
Loss after 16569170 batches: 0.0121
trigger times: 13
Loss after 16570133 batches: 0.0122
trigger times: 14
Loss after 16571096 batches: 0.0120
trigger times: 15
Loss after 16572059 batches: 0.0119
trigger times: 16
Loss after 16573022 batches: 0.0114
trigger times: 17
Loss after 16573985 batches: 0.0110
trigger times: 18
Loss after 16574948 batches: 0.0111
trigger times: 19
Loss after 16575911 batches: 0.0114
trigger times: 20
Loss after 16576874 batches: 0.0115
trigger times: 21
Loss after 16577837 batches: 0.0111
trigger times: 22
Loss after 16578800 batches: 0.0106
trigger times: 23
Loss after 16579763 batches: 0.0109
trigger times: 24
Loss after 16580726 batches: 0.0107
trigger times: 25
Early stopping!
Start to test process.
Loss after 16581689 batches: 0.0112
Time to train on one home:  52.62667155265808
trigger times: 0
Loss after 16582652 batches: 0.0933
trigger times: 1
Loss after 16583615 batches: 0.0859
trigger times: 2
Loss after 16584578 batches: 0.0785
trigger times: 3
Loss after 16585541 batches: 0.0769
trigger times: 4
Loss after 16586504 batches: 0.0718
trigger times: 5
Loss after 16587467 batches: 0.0685
trigger times: 6
Loss after 16588430 batches: 0.0667
trigger times: 7
Loss after 16589393 batches: 0.0648
trigger times: 8
Loss after 16590356 batches: 0.0623
trigger times: 9
Loss after 16591319 batches: 0.0610
trigger times: 10
Loss after 16592282 batches: 0.0616
trigger times: 11
Loss after 16593245 batches: 0.0593
trigger times: 12
Loss after 16594208 batches: 0.0591
trigger times: 13
Loss after 16595171 batches: 0.0591
trigger times: 14
Loss after 16596134 batches: 0.0577
trigger times: 15
Loss after 16597097 batches: 0.0571
trigger times: 16
Loss after 16598060 batches: 0.0571
trigger times: 17
Loss after 16599023 batches: 0.0564
trigger times: 18
Loss after 16599986 batches: 0.0569
trigger times: 19
Loss after 16600949 batches: 0.0568
trigger times: 20
Loss after 16601912 batches: 0.0562
trigger times: 21
Loss after 16602875 batches: 0.0541
trigger times: 22
Loss after 16603838 batches: 0.0542
trigger times: 23
Loss after 16604801 batches: 0.0538
trigger times: 24
Loss after 16605764 batches: 0.0536
trigger times: 25
Early stopping!
Start to test process.
Loss after 16606727 batches: 0.0528
Time to train on one home:  52.79352068901062
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16607690 batches: 0.0854
trigger times: 1
Loss after 16608653 batches: 0.0710
trigger times: 2
Loss after 16609616 batches: 0.0671
trigger times: 3
Loss after 16610579 batches: 0.0654
trigger times: 4
Loss after 16611542 batches: 0.0618
trigger times: 5
Loss after 16612505 batches: 0.0605
trigger times: 6
Loss after 16613468 batches: 0.0574
trigger times: 7
Loss after 16614431 batches: 0.0572
trigger times: 8
Loss after 16615394 batches: 0.0558
trigger times: 9
Loss after 16616357 batches: 0.0557
trigger times: 10
Loss after 16617320 batches: 0.0547
trigger times: 11
Loss after 16618283 batches: 0.0548
trigger times: 12
Loss after 16619246 batches: 0.0532
trigger times: 13
Loss after 16620209 batches: 0.0530
trigger times: 14
Loss after 16621172 batches: 0.0532
trigger times: 15
Loss after 16622135 batches: 0.0530
trigger times: 16
Loss after 16623098 batches: 0.0523
trigger times: 17
Loss after 16624061 batches: 0.0534
trigger times: 18
Loss after 16625024 batches: 0.0522
trigger times: 19
Loss after 16625987 batches: 0.0499
trigger times: 20
Loss after 16626950 batches: 0.0519
trigger times: 21
Loss after 16627913 batches: 0.0503
trigger times: 22
Loss after 16628876 batches: 0.0504
trigger times: 23
Loss after 16629839 batches: 0.0507
trigger times: 24
Loss after 16630802 batches: 0.0500
trigger times: 25
Early stopping!
Start to test process.
Loss after 16631765 batches: 0.0491
Time to train on one home:  52.858434200286865
trigger times: 0
Loss after 16632728 batches: 0.0744
trigger times: 1
Loss after 16633691 batches: 0.0675
trigger times: 2
Loss after 16634654 batches: 0.0647
trigger times: 3
Loss after 16635617 batches: 0.0622
trigger times: 4
Loss after 16636580 batches: 0.0590
trigger times: 5
Loss after 16637543 batches: 0.0585
trigger times: 6
Loss after 16638506 batches: 0.0571
trigger times: 7
Loss after 16639469 batches: 0.0558
trigger times: 8
Loss after 16640432 batches: 0.0539
trigger times: 9
Loss after 16641395 batches: 0.0535
trigger times: 10
Loss after 16642358 batches: 0.0538
trigger times: 11
Loss after 16643321 batches: 0.0538
trigger times: 12
Loss after 16644284 batches: 0.0517
trigger times: 13
Loss after 16645247 batches: 0.0506
trigger times: 14
Loss after 16646210 batches: 0.0516
trigger times: 15
Loss after 16647173 batches: 0.0506
trigger times: 16
Loss after 16648136 batches: 0.0509
trigger times: 17
Loss after 16649099 batches: 0.0498
trigger times: 18
Loss after 16650062 batches: 0.0520
trigger times: 19
Loss after 16651025 batches: 0.0500
trigger times: 20
Loss after 16651988 batches: 0.0488
trigger times: 21
Loss after 16652951 batches: 0.0497
trigger times: 22
Loss after 16653914 batches: 0.0488
trigger times: 23
Loss after 16654877 batches: 0.0480
trigger times: 24
Loss after 16655840 batches: 0.0469
trigger times: 25
Early stopping!
Start to test process.
Loss after 16656803 batches: 0.0479
Time to train on one home:  52.588409423828125
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16657766 batches: 0.0646
trigger times: 1
Loss after 16658729 batches: 0.0493
trigger times: 2
Loss after 16659692 batches: 0.0471
trigger times: 3
Loss after 16660655 batches: 0.0412
trigger times: 4
Loss after 16661618 batches: 0.0360
trigger times: 5
Loss after 16662581 batches: 0.0335
trigger times: 6
Loss after 16663544 batches: 0.0311
trigger times: 7
Loss after 16664507 batches: 0.0297
trigger times: 8
Loss after 16665470 batches: 0.0283
trigger times: 9
Loss after 16666433 batches: 0.0286
trigger times: 10
Loss after 16667396 batches: 0.0269
trigger times: 11
Loss after 16668359 batches: 0.0257
trigger times: 12
Loss after 16669322 batches: 0.0257
trigger times: 13
Loss after 16670285 batches: 0.0248
trigger times: 14
Loss after 16671248 batches: 0.0246
trigger times: 15
Loss after 16672211 batches: 0.0241
trigger times: 16
Loss after 16673174 batches: 0.0238
trigger times: 17
Loss after 16674137 batches: 0.0234
trigger times: 18
Loss after 16675100 batches: 0.0232
trigger times: 19
Loss after 16676063 batches: 0.0228
trigger times: 20
Loss after 16677026 batches: 0.0232
trigger times: 21
Loss after 16677989 batches: 0.0227
trigger times: 22
Loss after 16678952 batches: 0.0219
trigger times: 23
Loss after 16679915 batches: 0.0222
trigger times: 24
Loss after 16680878 batches: 0.0214
trigger times: 25
Early stopping!
Start to test process.
Loss after 16681841 batches: 0.0213
Time to train on one home:  52.71131229400635
trigger times: 0
Loss after 16682799 batches: 0.0594
trigger times: 1
Loss after 16683757 batches: 0.0398
trigger times: 2
Loss after 16684715 batches: 0.0351
trigger times: 3
Loss after 16685673 batches: 0.0285
trigger times: 4
Loss after 16686631 batches: 0.0254
trigger times: 5
Loss after 16687589 batches: 0.0235
trigger times: 6
Loss after 16688547 batches: 0.0223
trigger times: 7
Loss after 16689505 batches: 0.0203
trigger times: 8
Loss after 16690463 batches: 0.0218
trigger times: 9
Loss after 16691421 batches: 0.0191
trigger times: 10
Loss after 16692379 batches: 0.0196
trigger times: 11
Loss after 16693337 batches: 0.0182
trigger times: 12
Loss after 16694295 batches: 0.0191
trigger times: 13
Loss after 16695253 batches: 0.0185
trigger times: 14
Loss after 16696211 batches: 0.0172
trigger times: 15
Loss after 16697169 batches: 0.0163
trigger times: 16
Loss after 16698127 batches: 0.0162
trigger times: 17
Loss after 16699085 batches: 0.0162
trigger times: 18
Loss after 16700043 batches: 0.0159
trigger times: 19
Loss after 16701001 batches: 0.0160
trigger times: 20
Loss after 16701959 batches: 0.0155
trigger times: 21
Loss after 16702917 batches: 0.0158
trigger times: 22
Loss after 16703875 batches: 0.0163
trigger times: 23
Loss after 16704833 batches: 0.0150
trigger times: 24
Loss after 16705791 batches: 0.0150
trigger times: 25
Early stopping!
Start to test process.
Loss after 16706749 batches: 0.0150
Time to train on one home:  52.983182430267334
trigger times: 0
Loss after 16707711 batches: 0.0788
trigger times: 1
Loss after 16708673 batches: 0.0630
trigger times: 2
Loss after 16709635 batches: 0.0634
trigger times: 3
Loss after 16710597 batches: 0.0592
trigger times: 4
Loss after 16711559 batches: 0.0576
trigger times: 5
Loss after 16712521 batches: 0.0552
trigger times: 6
Loss after 16713483 batches: 0.0532
trigger times: 7
Loss after 16714445 batches: 0.0525
trigger times: 8
Loss after 16715407 batches: 0.0513
trigger times: 9
Loss after 16716369 batches: 0.0508
trigger times: 10
Loss after 16717331 batches: 0.0502
trigger times: 11
Loss after 16718293 batches: 0.0488
trigger times: 12
Loss after 16719255 batches: 0.0492
trigger times: 13
Loss after 16720217 batches: 0.0498
trigger times: 14
Loss after 16721179 batches: 0.0496
trigger times: 15
Loss after 16722141 batches: 0.0490
trigger times: 16
Loss after 16723103 batches: 0.0495
trigger times: 17
Loss after 16724065 batches: 0.0487
trigger times: 18
Loss after 16725027 batches: 0.0480
trigger times: 19
Loss after 16725989 batches: 0.0482
trigger times: 20
Loss after 16726951 batches: 0.0473
trigger times: 21
Loss after 16727913 batches: 0.0473
trigger times: 22
Loss after 16728875 batches: 0.0487
trigger times: 23
Loss after 16729837 batches: 0.0477
trigger times: 24
Loss after 16730799 batches: 0.0461
trigger times: 25
Early stopping!
Start to test process.
Loss after 16731761 batches: 0.0453
Time to train on one home:  52.54724192619324
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16732724 batches: 0.1005
trigger times: 1
Loss after 16733687 batches: 0.0284
trigger times: 2
Loss after 16734650 batches: 0.0199
trigger times: 3
Loss after 16735613 batches: 0.0173
trigger times: 4
Loss after 16736576 batches: 0.0163
trigger times: 5
Loss after 16737539 batches: 0.0149
trigger times: 6
Loss after 16738502 batches: 0.0145
trigger times: 7
Loss after 16739465 batches: 0.0137
trigger times: 8
Loss after 16740428 batches: 0.0134
trigger times: 9
Loss after 16741391 batches: 0.0133
trigger times: 10
Loss after 16742354 batches: 0.0131
trigger times: 11
Loss after 16743317 batches: 0.0126
trigger times: 12
Loss after 16744280 batches: 0.0128
trigger times: 13
Loss after 16745243 batches: 0.0128
trigger times: 14
Loss after 16746206 batches: 0.0125
trigger times: 15
Loss after 16747169 batches: 0.0125
trigger times: 16
Loss after 16748132 batches: 0.0125
trigger times: 17
Loss after 16749095 batches: 0.0123
trigger times: 18
Loss after 16750058 batches: 0.0122
trigger times: 19
Loss after 16751021 batches: 0.0122
trigger times: 20
Loss after 16751984 batches: 0.0120
trigger times: 21
Loss after 16752947 batches: 0.0118
trigger times: 22
Loss after 16753910 batches: 0.0122
trigger times: 23
Loss after 16754873 batches: 0.0123
trigger times: 24
Loss after 16755836 batches: 0.0120
trigger times: 25
Early stopping!
Start to test process.
Loss after 16756799 batches: 0.0118
Time to train on one home:  52.33033514022827
trigger times: 0
Loss after 16757762 batches: 0.0489
trigger times: 1
Loss after 16758725 batches: 0.0402
trigger times: 2
Loss after 16759688 batches: 0.0362
trigger times: 3
Loss after 16760651 batches: 0.0316
trigger times: 4
Loss after 16761614 batches: 0.0292
trigger times: 5
Loss after 16762577 batches: 0.0284
trigger times: 6
Loss after 16763540 batches: 0.0270
trigger times: 7
Loss after 16764503 batches: 0.0284
trigger times: 8
Loss after 16765466 batches: 0.0284
trigger times: 9
Loss after 16766429 batches: 0.0269
trigger times: 10
Loss after 16767392 batches: 0.0262
trigger times: 11
Loss after 16768355 batches: 0.0265
trigger times: 12
Loss after 16769318 batches: 0.0266
trigger times: 13
Loss after 16770281 batches: 0.0259
trigger times: 14
Loss after 16771244 batches: 0.0246
trigger times: 15
Loss after 16772207 batches: 0.0237
trigger times: 16
Loss after 16773170 batches: 0.0238
trigger times: 17
Loss after 16774133 batches: 0.0247
trigger times: 18
Loss after 16775096 batches: 0.0242
trigger times: 19
Loss after 16776059 batches: 0.0230
trigger times: 20
Loss after 16777022 batches: 0.0236
trigger times: 21
Loss after 16777985 batches: 0.0242
trigger times: 22
Loss after 16778948 batches: 0.0244
trigger times: 23
Loss after 16779911 batches: 0.0236
trigger times: 24
Loss after 16780874 batches: 0.0257
trigger times: 25
Early stopping!
Start to test process.
Loss after 16781837 batches: 0.0326
Time to train on one home:  52.80910897254944
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16782800 batches: 0.0648
trigger times: 1
Loss after 16783763 batches: 0.0488
trigger times: 2
Loss after 16784726 batches: 0.0471
trigger times: 3
Loss after 16785689 batches: 0.0406
trigger times: 4
Loss after 16786652 batches: 0.0358
trigger times: 5
Loss after 16787615 batches: 0.0338
trigger times: 6
Loss after 16788578 batches: 0.0317
trigger times: 7
Loss after 16789541 batches: 0.0294
trigger times: 8
Loss after 16790504 batches: 0.0287
trigger times: 9
Loss after 16791467 batches: 0.0276
trigger times: 10
Loss after 16792430 batches: 0.0266
trigger times: 11
Loss after 16793393 batches: 0.0262
trigger times: 12
Loss after 16794356 batches: 0.0251
trigger times: 13
Loss after 16795319 batches: 0.0253
trigger times: 14
Loss after 16796282 batches: 0.0244
trigger times: 15
Loss after 16797245 batches: 0.0244
trigger times: 16
Loss after 16798208 batches: 0.0243
trigger times: 17
Loss after 16799171 batches: 0.0249
trigger times: 18
Loss after 16800134 batches: 0.0234
trigger times: 19
Loss after 16801097 batches: 0.0235
trigger times: 20
Loss after 16802060 batches: 0.0235
trigger times: 21
Loss after 16803023 batches: 0.0226
trigger times: 22
Loss after 16803986 batches: 0.0223
trigger times: 23
Loss after 16804949 batches: 0.0216
trigger times: 24
Loss after 16805912 batches: 0.0210
trigger times: 25
Early stopping!
Start to test process.
Loss after 16806875 batches: 0.0218
Time to train on one home:  52.6081907749176
trigger times: 0
Loss after 16807838 batches: 0.0486
trigger times: 1
Loss after 16808801 batches: 0.0392
trigger times: 2
Loss after 16809764 batches: 0.0344
trigger times: 3
Loss after 16810727 batches: 0.0311
trigger times: 4
Loss after 16811690 batches: 0.0289
trigger times: 5
Loss after 16812653 batches: 0.0272
trigger times: 6
Loss after 16813616 batches: 0.0272
trigger times: 7
Loss after 16814579 batches: 0.0263
trigger times: 8
Loss after 16815542 batches: 0.0273
trigger times: 9
Loss after 16816505 batches: 0.0254
trigger times: 10
Loss after 16817468 batches: 0.0257
trigger times: 11
Loss after 16818431 batches: 0.0253
trigger times: 12
Loss after 16819394 batches: 0.0253
trigger times: 13
Loss after 16820357 batches: 0.0252
trigger times: 14
Loss after 16821320 batches: 0.0242
trigger times: 15
Loss after 16822283 batches: 0.0256
trigger times: 16
Loss after 16823246 batches: 0.0259
trigger times: 17
Loss after 16824209 batches: 0.0253
trigger times: 18
Loss after 16825172 batches: 0.0258
trigger times: 19
Loss after 16826135 batches: 0.0269
trigger times: 20
Loss after 16827098 batches: 0.0308
trigger times: 21
Loss after 16828061 batches: 0.0307
trigger times: 22
Loss after 16829024 batches: 0.0284
trigger times: 23
Loss after 16829987 batches: 0.0268
trigger times: 24
Loss after 16830950 batches: 0.0250
trigger times: 25
Early stopping!
Start to test process.
Loss after 16831913 batches: 0.0251
Time to train on one home:  52.92405366897583
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16832876 batches: 0.0464
trigger times: 1
Loss after 16833839 batches: 0.0428
trigger times: 2
Loss after 16834802 batches: 0.0389
trigger times: 3
Loss after 16835765 batches: 0.0372
trigger times: 4
Loss after 16836728 batches: 0.0353
trigger times: 5
Loss after 16837691 batches: 0.0337
trigger times: 6
Loss after 16838654 batches: 0.0318
trigger times: 7
Loss after 16839617 batches: 0.0309
trigger times: 8
Loss after 16840580 batches: 0.0308
trigger times: 9
Loss after 16841543 batches: 0.0301
trigger times: 10
Loss after 16842506 batches: 0.0295
trigger times: 11
Loss after 16843469 batches: 0.0304
trigger times: 0
Loss after 16844432 batches: 0.0368
trigger times: 1
Loss after 16845395 batches: 0.0348
trigger times: 0
Loss after 16846358 batches: 0.0331
trigger times: 1
Loss after 16847321 batches: 0.0324
trigger times: 2
Loss after 16848284 batches: 0.0307
trigger times: 3
Loss after 16849247 batches: 0.0304
trigger times: 4
Loss after 16850210 batches: 0.0313
trigger times: 5
Loss after 16851173 batches: 0.0299
trigger times: 6
Loss after 16852136 batches: 0.0305
trigger times: 7
Loss after 16853099 batches: 0.0294
trigger times: 8
Loss after 16854062 batches: 0.0291
trigger times: 9
Loss after 16855025 batches: 0.0281
trigger times: 10
Loss after 16855988 batches: 0.0274
trigger times: 11
Loss after 16856951 batches: 0.0275
trigger times: 12
Loss after 16857914 batches: 0.0295
trigger times: 13
Loss after 16858877 batches: 0.0298
trigger times: 14
Loss after 16859840 batches: 0.0285
trigger times: 15
Loss after 16860803 batches: 0.0280
trigger times: 16
Loss after 16861766 batches: 0.0269
trigger times: 17
Loss after 16862729 batches: 0.0259
trigger times: 18
Loss after 16863692 batches: 0.0251
trigger times: 19
Loss after 16864655 batches: 0.0253
trigger times: 21
Loss after 16866581 batches: 0.0266
trigger times: 22
Loss after 16867544 batches: 0.0272
trigger times: 23
Loss after 16868507 batches: 0.0251
trigger times: 24
Loss after 16869470 batches: 0.0260
trigger times: 25
Early stopping!
Start to test process.
Loss after 16870433 batches: 0.0247
Time to train on one home:  63.23995780944824
trigger times: 0
Loss after 16871396 batches: 0.0901
trigger times: 1
Loss after 16872359 batches: 0.0490
trigger times: 2
Loss after 16873322 batches: 0.0475
trigger times: 3
Loss after 16874285 batches: 0.0454
trigger times: 4
Loss after 16875248 batches: 0.0436
trigger times: 5
Loss after 16876211 batches: 0.0415
trigger times: 6
Loss after 16877174 batches: 0.0400
trigger times: 7
Loss after 16878137 batches: 0.0391
trigger times: 8
Loss after 16879100 batches: 0.0383
trigger times: 9
Loss after 16880063 batches: 0.0373
trigger times: 10
Loss after 16881026 batches: 0.0368
trigger times: 11
Loss after 16881989 batches: 0.0365
trigger times: 12
Loss after 16882952 batches: 0.0358
trigger times: 13
Loss after 16883915 batches: 0.0351
trigger times: 14
Loss after 16884878 batches: 0.0351
trigger times: 15
Loss after 16885841 batches: 0.0348
trigger times: 16
Loss after 16886804 batches: 0.0344
trigger times: 17
Loss after 16887767 batches: 0.0339
trigger times: 18
Loss after 16888730 batches: 0.0331
trigger times: 19
Loss after 16889693 batches: 0.0331
trigger times: 20
Loss after 16890656 batches: 0.0321
trigger times: 21
Loss after 16891619 batches: 0.0326
trigger times: 22
Loss after 16892582 batches: 0.0330
trigger times: 23
Loss after 16893545 batches: 0.0321
trigger times: 24
Loss after 16894508 batches: 0.0319
trigger times: 25
Early stopping!
Start to test process.
Loss after 16895471 batches: 0.0323
Time to train on one home:  52.53556489944458
trigger times: 0
Loss after 16896434 batches: 0.0950
trigger times: 1
Loss after 16897397 batches: 0.0859
trigger times: 2
Loss after 16898360 batches: 0.0785
trigger times: 3
Loss after 16899323 batches: 0.0757
trigger times: 4
Loss after 16900286 batches: 0.0707
trigger times: 5
Loss after 16901249 batches: 0.0688
trigger times: 6
Loss after 16902212 batches: 0.0651
trigger times: 7
Loss after 16903175 batches: 0.0643
trigger times: 8
Loss after 16904138 batches: 0.0619
trigger times: 9
Loss after 16905101 batches: 0.0633
trigger times: 10
Loss after 16906064 batches: 0.0634
trigger times: 11
Loss after 16907027 batches: 0.0611
trigger times: 12
Loss after 16907990 batches: 0.0585
trigger times: 13
Loss after 16908953 batches: 0.0584
trigger times: 14
Loss after 16909916 batches: 0.0591
trigger times: 15
Loss after 16910879 batches: 0.0572
trigger times: 16
Loss after 16911842 batches: 0.0564
trigger times: 17
Loss after 16912805 batches: 0.0593
trigger times: 18
Loss after 16913768 batches: 0.0624
trigger times: 19
Loss after 16914731 batches: 0.0591
trigger times: 20
Loss after 16915694 batches: 0.0589
trigger times: 21
Loss after 16916657 batches: 0.0571
trigger times: 22
Loss after 16917620 batches: 0.0536
trigger times: 23
Loss after 16918583 batches: 0.0530
trigger times: 24
Loss after 16919546 batches: 0.0546
trigger times: 25
Early stopping!
Start to test process.
Loss after 16920509 batches: 0.0541
Time to train on one home:  52.61042881011963
trigger times: 0
Loss after 16921472 batches: 0.0891
trigger times: 1
Loss after 16922435 batches: 0.0582
trigger times: 2
Loss after 16923398 batches: 0.0535
trigger times: 3
Loss after 16924361 batches: 0.0471
trigger times: 4
Loss after 16925324 batches: 0.0457
trigger times: 5
Loss after 16926287 batches: 0.0429
trigger times: 6
Loss after 16927250 batches: 0.0414
trigger times: 7
Loss after 16928213 batches: 0.0414
trigger times: 8
Loss after 16929176 batches: 0.0399
trigger times: 9
Loss after 16930139 batches: 0.0392
trigger times: 10
Loss after 16931102 batches: 0.0382
trigger times: 11
Loss after 16932065 batches: 0.0375
trigger times: 12
Loss after 16933028 batches: 0.0372
trigger times: 13
Loss after 16933991 batches: 0.0367
trigger times: 14
Loss after 16934954 batches: 0.0368
trigger times: 15
Loss after 16935917 batches: 0.0366
trigger times: 16
Loss after 16936880 batches: 0.0368
trigger times: 17
Loss after 16937843 batches: 0.0360
trigger times: 18
Loss after 16938806 batches: 0.0360
trigger times: 19
Loss after 16939769 batches: 0.0353
trigger times: 20
Loss after 16940732 batches: 0.0347
trigger times: 21
Loss after 16941695 batches: 0.0359
trigger times: 22
Loss after 16942658 batches: 0.0350
trigger times: 23
Loss after 16943621 batches: 0.0337
trigger times: 24
Loss after 16944584 batches: 0.0340
trigger times: 25
Early stopping!
Start to test process.
Loss after 16945547 batches: 0.0352
Time to train on one home:  52.411428451538086
trigger times: 0
Loss after 16946476 batches: 0.0909
trigger times: 1
Loss after 16947405 batches: 0.0644
trigger times: 2
Loss after 16948334 batches: 0.0458
trigger times: 3
Loss after 16949263 batches: 0.0380
trigger times: 4
Loss after 16950192 batches: 0.0336
trigger times: 5
Loss after 16951121 batches: 0.0312
trigger times: 6
Loss after 16952050 batches: 0.0292
trigger times: 7
Loss after 16952979 batches: 0.0300
trigger times: 8
Loss after 16953908 batches: 0.0283
trigger times: 9
Loss after 16954837 batches: 0.0261
trigger times: 10
Loss after 16955766 batches: 0.0264
trigger times: 11
Loss after 16956695 batches: 0.0260
trigger times: 12
Loss after 16957624 batches: 0.0244
trigger times: 13
Loss after 16958553 batches: 0.0244
trigger times: 14
Loss after 16959482 batches: 0.0243
trigger times: 15
Loss after 16960411 batches: 0.0247
trigger times: 16
Loss after 16961340 batches: 0.0252
trigger times: 17
Loss after 16962269 batches: 0.0236
trigger times: 18
Loss after 16963198 batches: 0.0242
trigger times: 19
Loss after 16964127 batches: 0.0218
trigger times: 20
Loss after 16965056 batches: 0.0244
trigger times: 21
Loss after 16965985 batches: 0.0247
trigger times: 22
Loss after 16966914 batches: 0.0240
trigger times: 23
Loss after 16967843 batches: 0.0216
trigger times: 24
Loss after 16968772 batches: 0.0219
trigger times: 25
Early stopping!
Start to test process.
Loss after 16969701 batches: 0.0219
Time to train on one home:  52.26941275596619
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 16970664 batches: 0.0774
trigger times: 1
Loss after 16971627 batches: 0.0343
trigger times: 2
Loss after 16972590 batches: 0.0270
trigger times: 3
Loss after 16973553 batches: 0.0271
trigger times: 4
Loss after 16974516 batches: 0.0264
trigger times: 5
Loss after 16975479 batches: 0.0249
trigger times: 6
Loss after 16976442 batches: 0.0243
trigger times: 7
Loss after 16977405 batches: 0.0232
trigger times: 8
Loss after 16978368 batches: 0.0221
trigger times: 9
Loss after 16979331 batches: 0.0210
trigger times: 10
Loss after 16980294 batches: 0.0206
trigger times: 11
Loss after 16981257 batches: 0.0200
trigger times: 12
Loss after 16982220 batches: 0.0199
trigger times: 13
Loss after 16983183 batches: 0.0195
trigger times: 14
Loss after 16984146 batches: 0.0191
trigger times: 15
Loss after 16985109 batches: 0.0189
trigger times: 16
Loss after 16986072 batches: 0.0184
trigger times: 17
Loss after 16987035 batches: 0.0185
trigger times: 18
Loss after 16987998 batches: 0.0180
trigger times: 19
Loss after 16988961 batches: 0.0178
trigger times: 20
Loss after 16989924 batches: 0.0177
trigger times: 21
Loss after 16990887 batches: 0.0176
trigger times: 22
Loss after 16991850 batches: 0.0175
trigger times: 23
Loss after 16992813 batches: 0.0175
trigger times: 24
Loss after 16993776 batches: 0.0170
trigger times: 25
Early stopping!
Start to test process.
Loss after 16994739 batches: 0.0170
Time to train on one home:  52.59027409553528
trigger times: 0
Loss after 16995702 batches: 0.1844
trigger times: 1
Loss after 16996665 batches: 0.1253
trigger times: 2
Loss after 16997628 batches: 0.0957
trigger times: 3
Loss after 16998591 batches: 0.0878
trigger times: 4
Loss after 16999554 batches: 0.0806
trigger times: 5
Loss after 17000517 batches: 0.0714
trigger times: 6
Loss after 17001480 batches: 0.0654
trigger times: 7
Loss after 17002443 batches: 0.0638
trigger times: 8
Loss after 17003406 batches: 0.0565
trigger times: 9
Loss after 17004369 batches: 0.0555
trigger times: 10
Loss after 17005332 batches: 0.0525
trigger times: 11
Loss after 17006295 batches: 0.0509
trigger times: 12
Loss after 17007258 batches: 0.0488
trigger times: 13
Loss after 17008221 batches: 0.0469
trigger times: 14
Loss after 17009184 batches: 0.0475
trigger times: 15
Loss after 17010147 batches: 0.0455
trigger times: 16
Loss after 17011110 batches: 0.0467
trigger times: 17
Loss after 17012073 batches: 0.0466
trigger times: 18
Loss after 17013036 batches: 0.0448
trigger times: 19
Loss after 17013999 batches: 0.0433
trigger times: 20
Loss after 17014962 batches: 0.0429
trigger times: 21
Loss after 17015925 batches: 0.0441
trigger times: 22
Loss after 17016888 batches: 0.0436
trigger times: 23
Loss after 17017851 batches: 0.0421
trigger times: 24
Loss after 17018814 batches: 0.0433
trigger times: 25
Early stopping!
Start to test process.
Loss after 17019777 batches: 0.0430
Time to train on one home:  52.46827745437622
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17020740 batches: 0.0858
trigger times: 1
Loss after 17021703 batches: 0.0713
trigger times: 0
Loss after 17022666 batches: 0.0682
trigger times: 1
Loss after 17023629 batches: 0.0636
trigger times: 2
Loss after 17024592 batches: 0.0628
trigger times: 3
Loss after 17025555 batches: 0.0612
trigger times: 4
Loss after 17026518 batches: 0.0594
trigger times: 5
Loss after 17027481 batches: 0.0583
trigger times: 6
Loss after 17028444 batches: 0.0561
trigger times: 7
Loss after 17029407 batches: 0.0548
trigger times: 8
Loss after 17030370 batches: 0.0551
trigger times: 9
Loss after 17031333 batches: 0.0555
trigger times: 10
Loss after 17032296 batches: 0.0550
trigger times: 11
Loss after 17033259 batches: 0.0529
trigger times: 12
Loss after 17034222 batches: 0.0529
trigger times: 13
Loss after 17035185 batches: 0.0531
trigger times: 14
Loss after 17036148 batches: 0.0531
trigger times: 15
Loss after 17037111 batches: 0.0533
trigger times: 16
Loss after 17038074 batches: 0.0529
trigger times: 17
Loss after 17039037 batches: 0.0514
trigger times: 18
Loss after 17040000 batches: 0.0509
trigger times: 19
Loss after 17040963 batches: 0.0521
trigger times: 20
Loss after 17041926 batches: 0.0504
trigger times: 21
Loss after 17042889 batches: 0.0503
trigger times: 22
Loss after 17043852 batches: 0.0500
trigger times: 23
Loss after 17044815 batches: 0.0492
trigger times: 24
Loss after 17045778 batches: 0.0498
trigger times: 25
Early stopping!
Start to test process.
Loss after 17046741 batches: 0.0496
Time to train on one home:  54.11875033378601
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17047704 batches: 0.0888
trigger times: 1
Loss after 17048667 batches: 0.0746
trigger times: 2
Loss after 17049630 batches: 0.0720
trigger times: 3
Loss after 17050593 batches: 0.0675
trigger times: 4
Loss after 17051556 batches: 0.0636
trigger times: 5
Loss after 17052519 batches: 0.0597
trigger times: 6
Loss after 17053482 batches: 0.0582
trigger times: 7
Loss after 17054445 batches: 0.0545
trigger times: 8
Loss after 17055408 batches: 0.0541
trigger times: 9
Loss after 17056371 batches: 0.0533
trigger times: 10
Loss after 17057334 batches: 0.0526
trigger times: 11
Loss after 17058297 batches: 0.0511
trigger times: 12
Loss after 17059260 batches: 0.0485
trigger times: 13
Loss after 17060223 batches: 0.0491
trigger times: 14
Loss after 17061186 batches: 0.0473
trigger times: 15
Loss after 17062149 batches: 0.0499
trigger times: 16
Loss after 17063112 batches: 0.0493
trigger times: 17
Loss after 17064075 batches: 0.0493
trigger times: 18
Loss after 17065038 batches: 0.0465
trigger times: 19
Loss after 17066001 batches: 0.0484
trigger times: 20
Loss after 17066964 batches: 0.0467
trigger times: 21
Loss after 17067927 batches: 0.0462
trigger times: 22
Loss after 17068890 batches: 0.0456
trigger times: 23
Loss after 17069853 batches: 0.0443
trigger times: 24
Loss after 17070816 batches: 0.0425
trigger times: 25
Early stopping!
Start to test process.
Loss after 17071779 batches: 0.0426
Time to train on one home:  52.567190408706665
trigger times: 0
Loss after 17072742 batches: 0.0229
trigger times: 1
Loss after 17073705 batches: 0.0183
trigger times: 2
Loss after 17074668 batches: 0.0169
trigger times: 3
Loss after 17075631 batches: 0.0153
trigger times: 4
Loss after 17076594 batches: 0.0141
trigger times: 5
Loss after 17077557 batches: 0.0136
trigger times: 6
Loss after 17078520 batches: 0.0127
trigger times: 7
Loss after 17079483 batches: 0.0122
trigger times: 8
Loss after 17080446 batches: 0.0125
trigger times: 9
Loss after 17081409 batches: 0.0122
trigger times: 10
Loss after 17082372 batches: 0.0121
trigger times: 11
Loss after 17083335 batches: 0.0118
trigger times: 12
Loss after 17084298 batches: 0.0113
trigger times: 13
Loss after 17085261 batches: 0.0114
trigger times: 14
Loss after 17086224 batches: 0.0113
trigger times: 15
Loss after 17087187 batches: 0.0112
trigger times: 16
Loss after 17088150 batches: 0.0116
trigger times: 17
Loss after 17089113 batches: 0.0115
trigger times: 18
Loss after 17090076 batches: 0.0114
trigger times: 19
Loss after 17091039 batches: 0.0111
trigger times: 20
Loss after 17092002 batches: 0.0108
trigger times: 21
Loss after 17092965 batches: 0.0109
trigger times: 22
Loss after 17093928 batches: 0.0113
trigger times: 23
Loss after 17094891 batches: 0.0108
trigger times: 24
Loss after 17095854 batches: 0.0107
trigger times: 25
Early stopping!
Start to test process.
Loss after 17096817 batches: 0.0102
Time to train on one home:  52.943058252334595
trigger times: 0
Loss after 17097780 batches: 0.0394
trigger times: 1
Loss after 17098743 batches: 0.0317
trigger times: 2
Loss after 17099706 batches: 0.0291
trigger times: 3
Loss after 17100669 batches: 0.0278
trigger times: 4
Loss after 17101632 batches: 0.0251
trigger times: 5
Loss after 17102595 batches: 0.0237
trigger times: 6
Loss after 17103558 batches: 0.0230
trigger times: 7
Loss after 17104521 batches: 0.0218
trigger times: 8
Loss after 17105484 batches: 0.0213
trigger times: 9
Loss after 17106447 batches: 0.0216
trigger times: 10
Loss after 17107410 batches: 0.0213
trigger times: 11
Loss after 17108373 batches: 0.0216
trigger times: 12
Loss after 17109336 batches: 0.0204
trigger times: 13
Loss after 17110299 batches: 0.0193
trigger times: 14
Loss after 17111262 batches: 0.0198
trigger times: 15
Loss after 17112225 batches: 0.0196
trigger times: 16
Loss after 17113188 batches: 0.0204
trigger times: 17
Loss after 17114151 batches: 0.0212
trigger times: 18
Loss after 17115114 batches: 0.0202
trigger times: 19
Loss after 17116077 batches: 0.0193
trigger times: 20
Loss after 17117040 batches: 0.0201
trigger times: 21
Loss after 17118003 batches: 0.0191
trigger times: 22
Loss after 17118966 batches: 0.0189
trigger times: 23
Loss after 17119929 batches: 0.0189
trigger times: 24
Loss after 17120892 batches: 0.0189
trigger times: 25
Early stopping!
Start to test process.
Loss after 17121855 batches: 0.0184
Time to train on one home:  52.437169313430786
trigger times: 0
Loss after 17122818 batches: 0.0838
trigger times: 1
Loss after 17123781 batches: 0.0616
trigger times: 2
Loss after 17124744 batches: 0.0543
trigger times: 3
Loss after 17125707 batches: 0.0482
trigger times: 4
Loss after 17126670 batches: 0.0454
trigger times: 5
Loss after 17127633 batches: 0.0442
trigger times: 6
Loss after 17128596 batches: 0.0415
trigger times: 7
Loss after 17129559 batches: 0.0411
trigger times: 8
Loss after 17130522 batches: 0.0402
trigger times: 9
Loss after 17131485 batches: 0.0384
trigger times: 10
Loss after 17132448 batches: 0.0380
trigger times: 11
Loss after 17133411 batches: 0.0373
trigger times: 12
Loss after 17134374 batches: 0.0364
trigger times: 13
Loss after 17135337 batches: 0.0367
trigger times: 14
Loss after 17136300 batches: 0.0355
trigger times: 15
Loss after 17137263 batches: 0.0343
trigger times: 16
Loss after 17138226 batches: 0.0338
trigger times: 17
Loss after 17139189 batches: 0.0354
trigger times: 18
Loss after 17140152 batches: 0.0344
trigger times: 19
Loss after 17141115 batches: 0.0340
trigger times: 20
Loss after 17142078 batches: 0.0330
trigger times: 21
Loss after 17143041 batches: 0.0333
trigger times: 22
Loss after 17144004 batches: 0.0335
trigger times: 23
Loss after 17144967 batches: 0.0332
trigger times: 24
Loss after 17145930 batches: 0.0323
trigger times: 25
Early stopping!
Start to test process.
Loss after 17146893 batches: 0.0321
Time to train on one home:  52.293227195739746
trigger times: 0
Loss after 17147856 batches: 0.0873
trigger times: 1
Loss after 17148819 batches: 0.0583
trigger times: 2
Loss after 17149782 batches: 0.0527
trigger times: 3
Loss after 17150745 batches: 0.0477
trigger times: 4
Loss after 17151708 batches: 0.0463
trigger times: 5
Loss after 17152671 batches: 0.0433
trigger times: 6
Loss after 17153634 batches: 0.0422
trigger times: 7
Loss after 17154597 batches: 0.0398
trigger times: 8
Loss after 17155560 batches: 0.0403
trigger times: 9
Loss after 17156523 batches: 0.0385
trigger times: 10
Loss after 17157486 batches: 0.0385
trigger times: 11
Loss after 17158449 batches: 0.0391
trigger times: 12
Loss after 17159412 batches: 0.0394
trigger times: 13
Loss after 17160375 batches: 0.0374
trigger times: 14
Loss after 17161338 batches: 0.0370
trigger times: 15
Loss after 17162301 batches: 0.0363
trigger times: 16
Loss after 17163264 batches: 0.0358
trigger times: 17
Loss after 17164227 batches: 0.0363
trigger times: 18
Loss after 17165190 batches: 0.0360
trigger times: 19
Loss after 17166153 batches: 0.0353
trigger times: 20
Loss after 17167116 batches: 0.0353
trigger times: 21
Loss after 17168079 batches: 0.0350
trigger times: 22
Loss after 17169042 batches: 0.0340
trigger times: 23
Loss after 17170005 batches: 0.0347
trigger times: 24
Loss after 17170968 batches: 0.0333
trigger times: 25
Early stopping!
Start to test process.
Loss after 17171931 batches: 0.0338
Time to train on one home:  52.29524612426758
trigger times: 0
Loss after 17172894 batches: 0.0482
trigger times: 1
Loss after 17173857 batches: 0.0391
trigger times: 2
Loss after 17174820 batches: 0.0350
trigger times: 3
Loss after 17175783 batches: 0.0317
trigger times: 4
Loss after 17176746 batches: 0.0293
trigger times: 5
Loss after 17177709 batches: 0.0284
trigger times: 6
Loss after 17178672 batches: 0.0288
trigger times: 7
Loss after 17179635 batches: 0.0265
trigger times: 8
Loss after 17180598 batches: 0.0275
trigger times: 9
Loss after 17181561 batches: 0.0271
trigger times: 10
Loss after 17182524 batches: 0.0262
trigger times: 11
Loss after 17183487 batches: 0.0267
trigger times: 12
Loss after 17184450 batches: 0.0263
trigger times: 13
Loss after 17185413 batches: 0.0265
trigger times: 14
Loss after 17186376 batches: 0.0276
trigger times: 15
Loss after 17187339 batches: 0.0255
trigger times: 16
Loss after 17188302 batches: 0.0254
trigger times: 17
Loss after 17189265 batches: 0.0244
trigger times: 18
Loss after 17190228 batches: 0.0240
trigger times: 19
Loss after 17191191 batches: 0.0241
trigger times: 20
Loss after 17192154 batches: 0.0240
trigger times: 21
Loss after 17193117 batches: 0.0238
trigger times: 22
Loss after 17194080 batches: 0.0233
trigger times: 23
Loss after 17195043 batches: 0.0237
trigger times: 24
Loss after 17196006 batches: 0.0264
trigger times: 25
Early stopping!
Start to test process.
Loss after 17196969 batches: 0.0237
Time to train on one home:  52.34616303443909
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17197932 batches: 0.0799
trigger times: 1
Loss after 17198895 batches: 0.0489
trigger times: 2
Loss after 17199858 batches: 0.0488
trigger times: 0
Loss after 17200821 batches: 0.0405
trigger times: 1
Loss after 17201784 batches: 0.0370
trigger times: 2
Loss after 17202747 batches: 0.0344
trigger times: 3
Loss after 17203710 batches: 0.0332
trigger times: 4
Loss after 17204673 batches: 0.0311
trigger times: 0
Loss after 17205636 batches: 0.0306
trigger times: 1
Loss after 17206599 batches: 0.0294
trigger times: 2
Loss after 17207562 batches: 0.0293
trigger times: 3
Loss after 17208525 batches: 0.0272
trigger times: 4
Loss after 17209488 batches: 0.0281
trigger times: 0
Loss after 17210451 batches: 0.0259
trigger times: 0
Loss after 17211414 batches: 0.0262
trigger times: 1
Loss after 17212377 batches: 0.0259
trigger times: 2
Loss after 17213340 batches: 0.0257
trigger times: 3
Loss after 17214303 batches: 0.0250
trigger times: 4
Loss after 17215266 batches: 0.0243
trigger times: 5
Loss after 17216229 batches: 0.0248
trigger times: 6
Loss after 17217192 batches: 0.0236
trigger times: 7
Loss after 17218155 batches: 0.0238
trigger times: 8
Loss after 17219118 batches: 0.0236
trigger times: 0
Loss after 17220081 batches: 0.0230
trigger times: 1
Loss after 17221044 batches: 0.0231
trigger times: 2
Loss after 17222007 batches: 0.0233
trigger times: 3
Loss after 17222970 batches: 0.0231
trigger times: 4
Loss after 17223933 batches: 0.0215
trigger times: 5
Loss after 17224896 batches: 0.0212
trigger times: 6
Loss after 17225859 batches: 0.0227
trigger times: 7
Loss after 17226822 batches: 0.0231
trigger times: 8
Loss after 17227785 batches: 0.0229
trigger times: 9
Loss after 17228748 batches: 0.0229
trigger times: 10
Loss after 17229711 batches: 0.0220
trigger times: 11
Loss after 17230674 batches: 0.0216
trigger times: 12
Loss after 17231637 batches: 0.0221
trigger times: 13
Loss after 17232600 batches: 0.0213
trigger times: 14
Loss after 17233563 batches: 0.0205
trigger times: 15
Loss after 17234526 batches: 0.0194
trigger times: 16
Loss after 17235489 batches: 0.0189
trigger times: 17
Loss after 17236452 batches: 0.0195
trigger times: 18
Loss after 17237415 batches: 0.0205
trigger times: 19
Loss after 17238378 batches: 0.0210
trigger times: 20
Loss after 17239341 batches: 0.0194
trigger times: 21
Loss after 17240304 batches: 0.0198
trigger times: 22
Loss after 17241267 batches: 0.0194
trigger times: 23
Loss after 17242230 batches: 0.0192
trigger times: 24
Loss after 17243193 batches: 0.0193
trigger times: 25
Early stopping!
Start to test process.
Loss after 17244156 batches: 0.0186
Time to train on one home:  70.14783501625061
trigger times: 0
Loss after 17245119 batches: 0.1056
trigger times: 1
Loss after 17246082 batches: 0.0784
trigger times: 2
Loss after 17247045 batches: 0.0751
trigger times: 3
Loss after 17248008 batches: 0.0741
trigger times: 4
Loss after 17248971 batches: 0.0708
trigger times: 5
Loss after 17249934 batches: 0.0684
trigger times: 6
Loss after 17250897 batches: 0.0653
trigger times: 7
Loss after 17251860 batches: 0.0643
trigger times: 8
Loss after 17252823 batches: 0.0631
trigger times: 9
Loss after 17253786 batches: 0.0621
trigger times: 10
Loss after 17254749 batches: 0.0592
trigger times: 11
Loss after 17255712 batches: 0.0595
trigger times: 12
Loss after 17256675 batches: 0.0592
trigger times: 13
Loss after 17257638 batches: 0.0588
trigger times: 14
Loss after 17258601 batches: 0.0588
trigger times: 15
Loss after 17259564 batches: 0.0572
trigger times: 16
Loss after 17260527 batches: 0.0569
trigger times: 17
Loss after 17261490 batches: 0.0560
trigger times: 18
Loss after 17262453 batches: 0.0566
trigger times: 19
Loss after 17263416 batches: 0.0563
trigger times: 20
Loss after 17264379 batches: 0.0561
trigger times: 21
Loss after 17265342 batches: 0.0551
trigger times: 22
Loss after 17266305 batches: 0.0539
trigger times: 23
Loss after 17267268 batches: 0.0541
trigger times: 24
Loss after 17268231 batches: 0.0523
trigger times: 25
Early stopping!
Start to test process.
Loss after 17269194 batches: 0.0531
Time to train on one home:  52.85902547836304
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17270157 batches: 0.0801
trigger times: 1
Loss after 17271120 batches: 0.0482
trigger times: 2
Loss after 17272083 batches: 0.0487
trigger times: 3
Loss after 17273046 batches: 0.0410
trigger times: 0
Loss after 17274009 batches: 0.0368
trigger times: 0
Loss after 17274972 batches: 0.0348
trigger times: 0
Loss after 17275935 batches: 0.0322
trigger times: 1
Loss after 17276898 batches: 0.0310
trigger times: 2
Loss after 17277861 batches: 0.0295
trigger times: 3
Loss after 17278824 batches: 0.0286
trigger times: 4
Loss after 17279787 batches: 0.0282
trigger times: 5
Loss after 17280750 batches: 0.0282
trigger times: 6
Loss after 17281713 batches: 0.0266
trigger times: 0
Loss after 17282676 batches: 0.0267
trigger times: 1
Loss after 17283639 batches: 0.0262
trigger times: 2
Loss after 17284602 batches: 0.0249
trigger times: 3
Loss after 17285565 batches: 0.0239
trigger times: 4
Loss after 17286528 batches: 0.0247
trigger times: 5
Loss after 17287491 batches: 0.0271
trigger times: 6
Loss after 17288454 batches: 0.0246
trigger times: 7
Loss after 17289417 batches: 0.0249
trigger times: 8
Loss after 17290380 batches: 0.0239
trigger times: 9
Loss after 17291343 batches: 0.0228
trigger times: 10
Loss after 17292306 batches: 0.0229
trigger times: 11
Loss after 17293269 batches: 0.0229
trigger times: 12
Loss after 17294232 batches: 0.0231
trigger times: 13
Loss after 17295195 batches: 0.0229
trigger times: 14
Loss after 17296158 batches: 0.0219
trigger times: 15
Loss after 17297121 batches: 0.0221
trigger times: 16
Loss after 17298084 batches: 0.0214
trigger times: 17
Loss after 17299047 batches: 0.0216
trigger times: 18
Loss after 17300010 batches: 0.0229
trigger times: 19
Loss after 17300973 batches: 0.0231
trigger times: 20
Loss after 17301936 batches: 0.0237
trigger times: 21
Loss after 17302899 batches: 0.0224
trigger times: 22
Loss after 17303862 batches: 0.0217
trigger times: 23
Loss after 17304825 batches: 0.0222
trigger times: 24
Loss after 17305788 batches: 0.0225
trigger times: 25
Early stopping!
Start to test process.
Loss after 17306751 batches: 0.0219
Time to train on one home:  62.283170223236084
trigger times: 0
Loss after 17307646 batches: 0.0677
trigger times: 1
Loss after 17308541 batches: 0.0388
trigger times: 2
Loss after 17309436 batches: 0.0177
trigger times: 3
Loss after 17310331 batches: 0.0098
trigger times: 4
Loss after 17311226 batches: 0.0082
trigger times: 5
Loss after 17312121 batches: 0.0067
trigger times: 6
Loss after 17313016 batches: 0.0060
trigger times: 7
Loss after 17313911 batches: 0.0049
trigger times: 8
Loss after 17314806 batches: 0.0045
trigger times: 9
Loss after 17315701 batches: 0.0041
trigger times: 10
Loss after 17316596 batches: 0.0036
trigger times: 11
Loss after 17317491 batches: 0.0030
trigger times: 12
Loss after 17318386 batches: 0.0028
trigger times: 13
Loss after 17319281 batches: 0.0023
trigger times: 14
Loss after 17320176 batches: 0.0024
trigger times: 15
Loss after 17321071 batches: 0.0025
trigger times: 16
Loss after 17321966 batches: 0.0027
trigger times: 17
Loss after 17322861 batches: 0.0027
trigger times: 18
Loss after 17323756 batches: 0.0026
trigger times: 19
Loss after 17324651 batches: 0.0023
trigger times: 20
Loss after 17325546 batches: 0.0021
trigger times: 21
Loss after 17326441 batches: 0.0019
trigger times: 22
Loss after 17327336 batches: 0.0018
trigger times: 23
Loss after 17328231 batches: 0.0021
trigger times: 24
Loss after 17329126 batches: 0.0019
trigger times: 25
Early stopping!
Start to test process.
Loss after 17330021 batches: 0.0022
Time to train on one home:  51.07154893875122
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17330958 batches: 0.0806
trigger times: 1
Loss after 17331895 batches: 0.0662
trigger times: 2
Loss after 17332832 batches: 0.0639
trigger times: 3
Loss after 17333769 batches: 0.0599
trigger times: 4
Loss after 17334706 batches: 0.0577
trigger times: 5
Loss after 17335643 batches: 0.0550
trigger times: 6
Loss after 17336580 batches: 0.0532
trigger times: 7
Loss after 17337517 batches: 0.0515
trigger times: 8
Loss after 17338454 batches: 0.0526
trigger times: 9
Loss after 17339391 batches: 0.0511
trigger times: 10
Loss after 17340328 batches: 0.0493
trigger times: 11
Loss after 17341265 batches: 0.0493
trigger times: 12
Loss after 17342202 batches: 0.0491
trigger times: 13
Loss after 17343139 batches: 0.0507
trigger times: 14
Loss after 17344076 batches: 0.0479
trigger times: 15
Loss after 17345013 batches: 0.0478
trigger times: 16
Loss after 17345950 batches: 0.0472
trigger times: 17
Loss after 17346887 batches: 0.0479
trigger times: 18
Loss after 17347824 batches: 0.0470
trigger times: 19
Loss after 17348761 batches: 0.0459
trigger times: 20
Loss after 17349698 batches: 0.0463
trigger times: 21
Loss after 17350635 batches: 0.0453
trigger times: 22
Loss after 17351572 batches: 0.0461
trigger times: 23
Loss after 17352509 batches: 0.0450
trigger times: 24
Loss after 17353446 batches: 0.0449
trigger times: 25
Early stopping!
Start to test process.
Loss after 17354383 batches: 0.0446
Time to train on one home:  52.03626585006714
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066, 0.03401813118363776, 0.033508403208926535, 0.03273922320832277, 0.032162070075287874, 0.031213877056718613, 0.03093800142524674, 0.030100551508447693, 0.029937661720089038]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863], [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861], [0.0757279247045517, 0.10266384027132536, 0.5909740425258612, 0.8803234656976516, 0.8076583271010261, 28.206427011804568, 8296.775], [0.06181953847408295, 0.10445801892372542, 0.5961459732150404, 0.831618823847048, 0.806043429565136, 26.645882531252276, 8280.187], [0.07333928346633911, 0.09887557787656209, 0.5904951093552152, 0.8805827319948272, 0.8110679704670578, 28.214734158180708, 8331.802], [0.06249981373548508, 0.1056225345966233, 0.6023961404422763, 0.8395811685763604, 0.8049953091236983, 26.90100386358231, 8269.419], [0.07111044973134995, 0.11364462231541128, 0.6015437535926366, 0.8360909977276793, 0.7977749276509928, 26.78917536742376, 8195.247], [0.06206701323390007, 0.09726051170449412, 0.5975866108942892, 0.8595424626232535, 0.8125216526577462, 27.540583297204083, 8346.734], [0.07088565826416016, 0.11302232012409985, 0.6007153971593981, 0.8537924311344753, 0.7983350331303676, 27.35634665030831, 8201.001]]
Round_15_results:  [0.07088565826416016, 0.11302232012409985, 0.6007153971593981, 0.8537924311344753, 0.7983350331303676, 27.35634665030831, 8201.001]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 18104 < 18105; dropping {'Training_Loss': 0.07149060922009605, 'Validation_Loss': 0.07992614060640335, 'Training_R2': -0.10145526911032055, 'Validation_R2': 0.13154933622534004, 'Training_F1': 0.42204016655004667, 'Validation_F1': 0.6128667921431162, 'Training_NEP': 0.8286605070688567, 'Validation_NEP': 0.7935130231170794, 'Training_NDE': 0.6708129695843726, 'Validation_NDE': 0.7846245440341993, 'Training_MAE': 31.489365881749233, 'Validation_MAE': 28.21674679683647, 'Training_MSE': 2477.5718, 'Validation_MSE': 10278.592}.
trigger times: 0
Loss after 17355345 batches: 0.0715
trigger times: 1
Loss after 17356307 batches: 0.0639
trigger times: 2
Loss after 17357269 batches: 0.0612
trigger times: 3
Loss after 17358231 batches: 0.0575
trigger times: 4
Loss after 17359193 batches: 0.0555
trigger times: 5
Loss after 17360155 batches: 0.0539
trigger times: 6
Loss after 17361117 batches: 0.0529
trigger times: 7
Loss after 17362079 batches: 0.0532
trigger times: 8
Loss after 17363041 batches: 0.0525
trigger times: 9
Loss after 17364003 batches: 0.0511
trigger times: 10
Loss after 17364965 batches: 0.0501
trigger times: 11
Loss after 17365927 batches: 0.0502
trigger times: 12
Loss after 17366889 batches: 0.0491
trigger times: 13
Loss after 17367851 batches: 0.0483
trigger times: 14
Loss after 17368813 batches: 0.0485
trigger times: 15
Loss after 17369775 batches: 0.0486
trigger times: 16
Loss after 17370737 batches: 0.0477
trigger times: 17
Loss after 17371699 batches: 0.0486
trigger times: 18
Loss after 17372661 batches: 0.0476
trigger times: 19
Loss after 17373623 batches: 0.0464
trigger times: 20
Loss after 17374585 batches: 0.0465
trigger times: 21
Loss after 17375547 batches: 0.0453
trigger times: 22
Loss after 17376509 batches: 0.0457
trigger times: 23
Loss after 17377471 batches: 0.0461
trigger times: 24
Loss after 17378433 batches: 0.0459
trigger times: 25
Early stopping!
Start to test process.
Loss after 17379395 batches: 0.0459
Time to train on one home:  52.817957162857056
trigger times: 0
Loss after 17380324 batches: 0.1038
trigger times: 1
Loss after 17381253 batches: 0.0616
trigger times: 2
Loss after 17382182 batches: 0.0487
trigger times: 3
Loss after 17383111 batches: 0.0400
trigger times: 4
Loss after 17384040 batches: 0.0345
trigger times: 5
Loss after 17384969 batches: 0.0338
trigger times: 6
Loss after 17385898 batches: 0.0304
trigger times: 7
Loss after 17386827 batches: 0.0301
trigger times: 8
Loss after 17387756 batches: 0.0282
trigger times: 9
Loss after 17388685 batches: 0.0278
trigger times: 10
Loss after 17389614 batches: 0.0302
trigger times: 11
Loss after 17390543 batches: 0.0278
trigger times: 12
Loss after 17391472 batches: 0.0255
trigger times: 13
Loss after 17392401 batches: 0.0236
trigger times: 14
Loss after 17393330 batches: 0.0249
trigger times: 15
Loss after 17394259 batches: 0.0262
trigger times: 16
Loss after 17395188 batches: 0.0273
trigger times: 17
Loss after 17396117 batches: 0.0246
trigger times: 18
Loss after 17397046 batches: 0.0246
trigger times: 19
Loss after 17397975 batches: 0.0236
trigger times: 20
Loss after 17398904 batches: 0.0237
trigger times: 21
Loss after 17399833 batches: 0.0225
trigger times: 22
Loss after 17400762 batches: 0.0227
trigger times: 23
Loss after 17401691 batches: 0.0227
trigger times: 24
Loss after 17402620 batches: 0.0213
trigger times: 25
Early stopping!
Start to test process.
Loss after 17403549 batches: 0.0208
Time to train on one home:  52.347487449645996
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 17404512 batches: 0.0320
trigger times: 1
Loss after 17405475 batches: 0.0175
trigger times: 2
Loss after 17406438 batches: 0.0147
trigger times: 3
Loss after 17407401 batches: 0.0138
trigger times: 4
Loss after 17408364 batches: 0.0135
trigger times: 5
Loss after 17409327 batches: 0.0130
trigger times: 6
Loss after 17410290 batches: 0.0125
trigger times: 7
Loss after 17411253 batches: 0.0115
trigger times: 8
Loss after 17412216 batches: 0.0107
trigger times: 9
Loss after 17413179 batches: 0.0096
trigger times: 10
Loss after 17414142 batches: 0.0092
trigger times: 11
Loss after 17415105 batches: 0.0088
trigger times: 12
Loss after 17416068 batches: 0.0085
trigger times: 13
Loss after 17417031 batches: 0.0080
trigger times: 14
Loss after 17417994 batches: 0.0079
trigger times: 15
Loss after 17418957 batches: 0.0076
trigger times: 16
Loss after 17419920 batches: 0.0075
trigger times: 17
Loss after 17420883 batches: 0.0072
trigger times: 18
Loss after 17421846 batches: 0.0073
trigger times: 19
Loss after 17422809 batches: 0.0071
trigger times: 20
Loss after 17423772 batches: 0.0070
trigger times: 21
Loss after 17424735 batches: 0.0067
trigger times: 22
Loss after 17425698 batches: 0.0067
trigger times: 23
Loss after 17426661 batches: 0.0065
trigger times: 24
Loss after 17427624 batches: 0.0064
trigger times: 25
Early stopping!
Start to test process.
Loss after 17428587 batches: 0.0064
Time to train on one home:  52.70468330383301
trigger times: 0
Loss after 17429550 batches: 0.0205
trigger times: 1
Loss after 17430513 batches: 0.0178
trigger times: 2
Loss after 17431476 batches: 0.0166
trigger times: 3
Loss after 17432439 batches: 0.0150
trigger times: 4
Loss after 17433402 batches: 0.0140
trigger times: 5
Loss after 17434365 batches: 0.0135
trigger times: 6
Loss after 17435328 batches: 0.0127
trigger times: 7
Loss after 17436291 batches: 0.0125
trigger times: 8
Loss after 17437254 batches: 0.0125
trigger times: 9
Loss after 17438217 batches: 0.0123
trigger times: 10
Loss after 17439180 batches: 0.0125
trigger times: 11
Loss after 17440143 batches: 0.0123
trigger times: 12
Loss after 17441106 batches: 0.0118
trigger times: 13
Loss after 17442069 batches: 0.0118
trigger times: 14
Loss after 17443032 batches: 0.0117
trigger times: 15
Loss after 17443995 batches: 0.0113
trigger times: 16
Loss after 17444958 batches: 0.0111
trigger times: 17
Loss after 17445921 batches: 0.0109
trigger times: 18
Loss after 17446884 batches: 0.0108
trigger times: 19
Loss after 17447847 batches: 0.0111
trigger times: 20
Loss after 17448810 batches: 0.0111
trigger times: 21
Loss after 17449773 batches: 0.0108
trigger times: 22
Loss after 17450736 batches: 0.0109
trigger times: 23
Loss after 17451699 batches: 0.0110
trigger times: 24
Loss after 17452662 batches: 0.0104
trigger times: 25
Early stopping!
Start to test process.
Loss after 17453625 batches: 0.0104
Time to train on one home:  52.67894506454468
trigger times: 0
Loss after 17454588 batches: 0.0952
trigger times: 1
Loss after 17455551 batches: 0.0865
trigger times: 2
Loss after 17456514 batches: 0.0809
trigger times: 3
Loss after 17457477 batches: 0.0756
trigger times: 4
Loss after 17458440 batches: 0.0724
trigger times: 5
Loss after 17459403 batches: 0.0699
trigger times: 6
Loss after 17460366 batches: 0.0670
trigger times: 7
Loss after 17461329 batches: 0.0635
trigger times: 8
Loss after 17462292 batches: 0.0618
trigger times: 9
Loss after 17463255 batches: 0.0600
trigger times: 10
Loss after 17464218 batches: 0.0608
trigger times: 11
Loss after 17465181 batches: 0.0606
trigger times: 12
Loss after 17466144 batches: 0.0601
trigger times: 13
Loss after 17467107 batches: 0.0608
trigger times: 14
Loss after 17468070 batches: 0.0583
trigger times: 15
Loss after 17469033 batches: 0.0579
trigger times: 16
Loss after 17469996 batches: 0.0567
trigger times: 17
Loss after 17470959 batches: 0.0568
trigger times: 18
Loss after 17471922 batches: 0.0555
trigger times: 19
Loss after 17472885 batches: 0.0568
trigger times: 20
Loss after 17473848 batches: 0.0568
trigger times: 21
Loss after 17474811 batches: 0.0545
trigger times: 22
Loss after 17475774 batches: 0.0540
trigger times: 23
Loss after 17476737 batches: 0.0533
trigger times: 24
Loss after 17477700 batches: 0.0524
trigger times: 25
Early stopping!
Start to test process.
Loss after 17478663 batches: 0.0513
Time to train on one home:  52.40424203872681
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17479626 batches: 0.0786
trigger times: 1
Loss after 17480589 batches: 0.0717
trigger times: 2
Loss after 17481552 batches: 0.0655
trigger times: 3
Loss after 17482515 batches: 0.0635
trigger times: 4
Loss after 17483478 batches: 0.0603
trigger times: 5
Loss after 17484441 batches: 0.0598
trigger times: 6
Loss after 17485404 batches: 0.0576
trigger times: 7
Loss after 17486367 batches: 0.0569
trigger times: 8
Loss after 17487330 batches: 0.0561
trigger times: 9
Loss after 17488293 batches: 0.0551
trigger times: 10
Loss after 17489256 batches: 0.0536
trigger times: 11
Loss after 17490219 batches: 0.0540
trigger times: 12
Loss after 17491182 batches: 0.0519
trigger times: 13
Loss after 17492145 batches: 0.0533
trigger times: 14
Loss after 17493108 batches: 0.0518
trigger times: 15
Loss after 17494071 batches: 0.0519
trigger times: 16
Loss after 17495034 batches: 0.0524
trigger times: 17
Loss after 17495997 batches: 0.0510
trigger times: 18
Loss after 17496960 batches: 0.0511
trigger times: 19
Loss after 17497923 batches: 0.0515
trigger times: 20
Loss after 17498886 batches: 0.0512
trigger times: 21
Loss after 17499849 batches: 0.0501
trigger times: 22
Loss after 17500812 batches: 0.0503
trigger times: 23
Loss after 17501775 batches: 0.0507
trigger times: 24
Loss after 17502738 batches: 0.0513
trigger times: 25
Early stopping!
Start to test process.
Loss after 17503701 batches: 0.0496
Time to train on one home:  52.403377532958984
trigger times: 0
Loss after 17504664 batches: 0.0734
trigger times: 1
Loss after 17505627 batches: 0.0680
trigger times: 2
Loss after 17506590 batches: 0.0629
trigger times: 3
Loss after 17507553 batches: 0.0607
trigger times: 4
Loss after 17508516 batches: 0.0587
trigger times: 5
Loss after 17509479 batches: 0.0566
trigger times: 6
Loss after 17510442 batches: 0.0557
trigger times: 7
Loss after 17511405 batches: 0.0558
trigger times: 8
Loss after 17512368 batches: 0.0542
trigger times: 9
Loss after 17513331 batches: 0.0539
trigger times: 10
Loss after 17514294 batches: 0.0532
trigger times: 11
Loss after 17515257 batches: 0.0511
trigger times: 12
Loss after 17516220 batches: 0.0507
trigger times: 13
Loss after 17517183 batches: 0.0513
trigger times: 14
Loss after 17518146 batches: 0.0500
trigger times: 15
Loss after 17519109 batches: 0.0499
trigger times: 16
Loss after 17520072 batches: 0.0491
trigger times: 17
Loss after 17521035 batches: 0.0502
trigger times: 18
Loss after 17521998 batches: 0.0492
trigger times: 19
Loss after 17522961 batches: 0.0487
trigger times: 20
Loss after 17523924 batches: 0.0487
trigger times: 21
Loss after 17524887 batches: 0.0487
trigger times: 22
Loss after 17525850 batches: 0.0476
trigger times: 23
Loss after 17526813 batches: 0.0472
trigger times: 24
Loss after 17527776 batches: 0.0475
trigger times: 25
Early stopping!
Start to test process.
Loss after 17528739 batches: 0.0479
Time to train on one home:  52.72647142410278
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17529702 batches: 0.0605
trigger times: 1
Loss after 17530665 batches: 0.0483
trigger times: 2
Loss after 17531628 batches: 0.0450
trigger times: 3
Loss after 17532591 batches: 0.0382
trigger times: 4
Loss after 17533554 batches: 0.0337
trigger times: 5
Loss after 17534517 batches: 0.0314
trigger times: 6
Loss after 17535480 batches: 0.0292
trigger times: 7
Loss after 17536443 batches: 0.0282
trigger times: 8
Loss after 17537406 batches: 0.0284
trigger times: 9
Loss after 17538369 batches: 0.0279
trigger times: 10
Loss after 17539332 batches: 0.0258
trigger times: 11
Loss after 17540295 batches: 0.0254
trigger times: 12
Loss after 17541258 batches: 0.0252
trigger times: 13
Loss after 17542221 batches: 0.0243
trigger times: 14
Loss after 17543184 batches: 0.0242
trigger times: 15
Loss after 17544147 batches: 0.0238
trigger times: 16
Loss after 17545110 batches: 0.0235
trigger times: 17
Loss after 17546073 batches: 0.0234
trigger times: 18
Loss after 17547036 batches: 0.0222
trigger times: 19
Loss after 17547999 batches: 0.0220
trigger times: 20
Loss after 17548962 batches: 0.0216
trigger times: 21
Loss after 17549925 batches: 0.0214
trigger times: 22
Loss after 17550888 batches: 0.0210
trigger times: 23
Loss after 17551851 batches: 0.0219
trigger times: 24
Loss after 17552814 batches: 0.0212
trigger times: 25
Early stopping!
Start to test process.
Loss after 17553777 batches: 0.0209
Time to train on one home:  52.084752798080444
trigger times: 0
Loss after 17554735 batches: 0.0716
trigger times: 1
Loss after 17555693 batches: 0.0426
trigger times: 2
Loss after 17556651 batches: 0.0377
trigger times: 3
Loss after 17557609 batches: 0.0301
trigger times: 4
Loss after 17558567 batches: 0.0279
trigger times: 5
Loss after 17559525 batches: 0.0315
trigger times: 6
Loss after 17560483 batches: 0.0347
trigger times: 7
Loss after 17561441 batches: 0.0308
trigger times: 8
Loss after 17562399 batches: 0.0272
trigger times: 9
Loss after 17563357 batches: 0.0260
trigger times: 10
Loss after 17564315 batches: 0.0252
trigger times: 11
Loss after 17565273 batches: 0.0238
trigger times: 12
Loss after 17566231 batches: 0.0221
trigger times: 13
Loss after 17567189 batches: 0.0220
trigger times: 14
Loss after 17568147 batches: 0.0226
trigger times: 15
Loss after 17569105 batches: 0.0227
trigger times: 16
Loss after 17570063 batches: 0.0204
trigger times: 17
Loss after 17571021 batches: 0.0194
trigger times: 18
Loss after 17571979 batches: 0.0193
trigger times: 19
Loss after 17572937 batches: 0.0182
trigger times: 20
Loss after 17573895 batches: 0.0180
trigger times: 21
Loss after 17574853 batches: 0.0180
trigger times: 22
Loss after 17575811 batches: 0.0176
trigger times: 23
Loss after 17576769 batches: 0.0173
trigger times: 24
Loss after 17577727 batches: 0.0168
trigger times: 25
Early stopping!
Start to test process.
Loss after 17578685 batches: 0.0182
Time to train on one home:  52.378750801086426
trigger times: 0
Loss after 17579647 batches: 0.0708
trigger times: 1
Loss after 17580609 batches: 0.0633
trigger times: 2
Loss after 17581571 batches: 0.0604
trigger times: 3
Loss after 17582533 batches: 0.0578
trigger times: 4
Loss after 17583495 batches: 0.0557
trigger times: 5
Loss after 17584457 batches: 0.0536
trigger times: 6
Loss after 17585419 batches: 0.0526
trigger times: 7
Loss after 17586381 batches: 0.0515
trigger times: 8
Loss after 17587343 batches: 0.0513
trigger times: 9
Loss after 17588305 batches: 0.0500
trigger times: 10
Loss after 17589267 batches: 0.0505
trigger times: 11
Loss after 17590229 batches: 0.0483
trigger times: 12
Loss after 17591191 batches: 0.0502
trigger times: 13
Loss after 17592153 batches: 0.0488
trigger times: 14
Loss after 17593115 batches: 0.0486
trigger times: 15
Loss after 17594077 batches: 0.0480
trigger times: 16
Loss after 17595039 batches: 0.0476
trigger times: 17
Loss after 17596001 batches: 0.0470
trigger times: 18
Loss after 17596963 batches: 0.0466
trigger times: 19
Loss after 17597925 batches: 0.0465
trigger times: 20
Loss after 17598887 batches: 0.0472
trigger times: 21
Loss after 17599849 batches: 0.0459
trigger times: 22
Loss after 17600811 batches: 0.0463
trigger times: 23
Loss after 17601773 batches: 0.0453
trigger times: 24
Loss after 17602735 batches: 0.0455
trigger times: 25
Early stopping!
Start to test process.
Loss after 17603697 batches: 0.0452
Time to train on one home:  52.58974814414978
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17604660 batches: 0.0696
trigger times: 1
Loss after 17605623 batches: 0.0232
trigger times: 2
Loss after 17606586 batches: 0.0189
trigger times: 3
Loss after 17607549 batches: 0.0162
trigger times: 4
Loss after 17608512 batches: 0.0149
trigger times: 5
Loss after 17609475 batches: 0.0144
trigger times: 6
Loss after 17610438 batches: 0.0137
trigger times: 7
Loss after 17611401 batches: 0.0131
trigger times: 8
Loss after 17612364 batches: 0.0132
trigger times: 9
Loss after 17613327 batches: 0.0128
trigger times: 10
Loss after 17614290 batches: 0.0125
trigger times: 11
Loss after 17615253 batches: 0.0126
trigger times: 12
Loss after 17616216 batches: 0.0126
trigger times: 13
Loss after 17617179 batches: 0.0124
trigger times: 14
Loss after 17618142 batches: 0.0123
trigger times: 15
Loss after 17619105 batches: 0.0121
trigger times: 16
Loss after 17620068 batches: 0.0121
trigger times: 17
Loss after 17621031 batches: 0.0119
trigger times: 18
Loss after 17621994 batches: 0.0120
trigger times: 19
Loss after 17622957 batches: 0.0118
trigger times: 20
Loss after 17623920 batches: 0.0119
trigger times: 21
Loss after 17624883 batches: 0.0117
trigger times: 22
Loss after 17625846 batches: 0.0117
trigger times: 23
Loss after 17626809 batches: 0.0118
trigger times: 24
Loss after 17627772 batches: 0.0116
trigger times: 25
Early stopping!
Start to test process.
Loss after 17628735 batches: 0.0115
Time to train on one home:  52.414878606796265
trigger times: 0
Loss after 17629698 batches: 0.0520
trigger times: 1
Loss after 17630661 batches: 0.0400
trigger times: 2
Loss after 17631624 batches: 0.0352
trigger times: 3
Loss after 17632587 batches: 0.0319
trigger times: 4
Loss after 17633550 batches: 0.0309
trigger times: 5
Loss after 17634513 batches: 0.0287
trigger times: 6
Loss after 17635476 batches: 0.0270
trigger times: 7
Loss after 17636439 batches: 0.0270
trigger times: 8
Loss after 17637402 batches: 0.0266
trigger times: 9
Loss after 17638365 batches: 0.0260
trigger times: 10
Loss after 17639328 batches: 0.0246
trigger times: 11
Loss after 17640291 batches: 0.0250
trigger times: 12
Loss after 17641254 batches: 0.0243
trigger times: 13
Loss after 17642217 batches: 0.0251
trigger times: 14
Loss after 17643180 batches: 0.0246
trigger times: 15
Loss after 17644143 batches: 0.0237
trigger times: 16
Loss after 17645106 batches: 0.0231
trigger times: 17
Loss after 17646069 batches: 0.0228
trigger times: 18
Loss after 17647032 batches: 0.0234
trigger times: 19
Loss after 17647995 batches: 0.0233
trigger times: 20
Loss after 17648958 batches: 0.0237
trigger times: 21
Loss after 17649921 batches: 0.0266
trigger times: 22
Loss after 17650884 batches: 0.0242
trigger times: 23
Loss after 17651847 batches: 0.0244
trigger times: 24
Loss after 17652810 batches: 0.0241
trigger times: 25
Early stopping!
Start to test process.
Loss after 17653773 batches: 0.0235
Time to train on one home:  52.52388954162598
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17654736 batches: 0.0597
trigger times: 1
Loss after 17655699 batches: 0.0480
trigger times: 2
Loss after 17656662 batches: 0.0445
trigger times: 3
Loss after 17657625 batches: 0.0379
trigger times: 4
Loss after 17658588 batches: 0.0336
trigger times: 5
Loss after 17659551 batches: 0.0314
trigger times: 6
Loss after 17660514 batches: 0.0296
trigger times: 7
Loss after 17661477 batches: 0.0288
trigger times: 8
Loss after 17662440 batches: 0.0276
trigger times: 9
Loss after 17663403 batches: 0.0275
trigger times: 10
Loss after 17664366 batches: 0.0257
trigger times: 11
Loss after 17665329 batches: 0.0251
trigger times: 12
Loss after 17666292 batches: 0.0248
trigger times: 13
Loss after 17667255 batches: 0.0247
trigger times: 14
Loss after 17668218 batches: 0.0237
trigger times: 15
Loss after 17669181 batches: 0.0233
trigger times: 16
Loss after 17670144 batches: 0.0234
trigger times: 17
Loss after 17671107 batches: 0.0222
trigger times: 18
Loss after 17672070 batches: 0.0225
trigger times: 19
Loss after 17673033 batches: 0.0220
trigger times: 20
Loss after 17673996 batches: 0.0210
trigger times: 21
Loss after 17674959 batches: 0.0210
trigger times: 22
Loss after 17675922 batches: 0.0212
trigger times: 23
Loss after 17676885 batches: 0.0220
trigger times: 24
Loss after 17677848 batches: 0.0213
trigger times: 25
Early stopping!
Start to test process.
Loss after 17678811 batches: 0.0205
Time to train on one home:  53.052775621414185
trigger times: 0
Loss after 17679774 batches: 0.0519
trigger times: 1
Loss after 17680737 batches: 0.0405
trigger times: 2
Loss after 17681700 batches: 0.0356
trigger times: 3
Loss after 17682663 batches: 0.0326
trigger times: 4
Loss after 17683626 batches: 0.0301
trigger times: 5
Loss after 17684589 batches: 0.0290
trigger times: 6
Loss after 17685552 batches: 0.0272
trigger times: 7
Loss after 17686515 batches: 0.0279
trigger times: 8
Loss after 17687478 batches: 0.0273
trigger times: 9
Loss after 17688441 batches: 0.0263
trigger times: 10
Loss after 17689404 batches: 0.0246
trigger times: 11
Loss after 17690367 batches: 0.0249
trigger times: 12
Loss after 17691330 batches: 0.0257
trigger times: 13
Loss after 17692293 batches: 0.0253
trigger times: 14
Loss after 17693256 batches: 0.0262
trigger times: 15
Loss after 17694219 batches: 0.0274
trigger times: 16
Loss after 17695182 batches: 0.0272
trigger times: 17
Loss after 17696145 batches: 0.0251
trigger times: 18
Loss after 17697108 batches: 0.0259
trigger times: 19
Loss after 17698071 batches: 0.0242
trigger times: 20
Loss after 17699034 batches: 0.0237
trigger times: 21
Loss after 17699997 batches: 0.0232
trigger times: 22
Loss after 17700960 batches: 0.0235
trigger times: 23
Loss after 17701923 batches: 0.0231
trigger times: 24
Loss after 17702886 batches: 0.0230
trigger times: 25
Early stopping!
Start to test process.
Loss after 17703849 batches: 0.0233
Time to train on one home:  52.81786561012268
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17704812 batches: 0.0493
trigger times: 1
Loss after 17705775 batches: 0.0434
trigger times: 2
Loss after 17706738 batches: 0.0400
trigger times: 3
Loss after 17707701 batches: 0.0374
trigger times: 4
Loss after 17708664 batches: 0.0345
trigger times: 5
Loss after 17709627 batches: 0.0329
trigger times: 6
Loss after 17710590 batches: 0.0319
trigger times: 7
Loss after 17711553 batches: 0.0313
trigger times: 8
Loss after 17712516 batches: 0.0305
trigger times: 9
Loss after 17713479 batches: 0.0290
trigger times: 10
Loss after 17714442 batches: 0.0294
trigger times: 11
Loss after 17715405 batches: 0.0284
trigger times: 12
Loss after 17716368 batches: 0.0281
trigger times: 13
Loss after 17717331 batches: 0.0272
trigger times: 14
Loss after 17718294 batches: 0.0264
trigger times: 15
Loss after 17719257 batches: 0.0267
trigger times: 16
Loss after 17720220 batches: 0.0271
trigger times: 17
Loss after 17721183 batches: 0.0265
trigger times: 18
Loss after 17722146 batches: 0.0265
trigger times: 19
Loss after 17723109 batches: 0.0256
trigger times: 20
Loss after 17724072 batches: 0.0265
trigger times: 0
Loss after 17725035 batches: 0.0272
trigger times: 0
Loss after 17725998 batches: 0.0264
trigger times: 1
Loss after 17726961 batches: 0.0261
trigger times: 2
Loss after 17727924 batches: 0.0256
trigger times: 3
Loss after 17728887 batches: 0.0248
trigger times: 4
Loss after 17729850 batches: 0.0245
trigger times: 5
Loss after 17730813 batches: 0.0239
trigger times: 6
Loss after 17731776 batches: 0.0235
trigger times: 7
Loss after 17732739 batches: 0.0237
trigger times: 8
Loss after 17733702 batches: 0.0235
trigger times: 9
Loss after 17734665 batches: 0.0235
trigger times: 10
Loss after 17735628 batches: 0.0229
trigger times: 11
Loss after 17736591 batches: 0.0227
trigger times: 12
Loss after 17737554 batches: 0.0232
trigger times: 13
Loss after 17738517 batches: 0.0230
trigger times: 14
Loss after 17739480 batches: 0.0228
trigger times: 15
Loss after 17740443 batches: 0.0251
trigger times: 16
Loss after 17741406 batches: 0.0252
trigger times: 17
Loss after 17742369 batches: 0.0253
trigger times: 18
Loss after 17743332 batches: 0.0252
trigger times: 19
Loss after 17744295 batches: 0.0241
trigger times: 20
Loss after 17745258 batches: 0.0234
trigger times: 21
Loss after 17746221 batches: 0.0236
trigger times: 22
Loss after 17747184 batches: 0.0227
trigger times: 23
Loss after 17748147 batches: 0.0305
trigger times: 24
Loss after 17749110 batches: 0.0288
trigger times: 25
Early stopping!
Start to test process.
Loss after 17750073 batches: 0.0262
Time to train on one home:  69.40781474113464
trigger times: 0
Loss after 17751036 batches: 0.0738
trigger times: 1
Loss after 17751999 batches: 0.0469
trigger times: 2
Loss after 17752962 batches: 0.0461
trigger times: 3
Loss after 17753925 batches: 0.0431
trigger times: 4
Loss after 17754888 batches: 0.0408
trigger times: 5
Loss after 17755851 batches: 0.0393
trigger times: 6
Loss after 17756814 batches: 0.0376
trigger times: 7
Loss after 17757777 batches: 0.0371
trigger times: 8
Loss after 17758740 batches: 0.0367
trigger times: 9
Loss after 17759703 batches: 0.0367
trigger times: 10
Loss after 17760666 batches: 0.0361
trigger times: 11
Loss after 17761629 batches: 0.0355
trigger times: 12
Loss after 17762592 batches: 0.0352
trigger times: 13
Loss after 17763555 batches: 0.0346
trigger times: 14
Loss after 17764518 batches: 0.0347
trigger times: 15
Loss after 17765481 batches: 0.0347
trigger times: 16
Loss after 17766444 batches: 0.0336
trigger times: 17
Loss after 17767407 batches: 0.0336
trigger times: 18
Loss after 17768370 batches: 0.0339
trigger times: 19
Loss after 17769333 batches: 0.0327
trigger times: 20
Loss after 17770296 batches: 0.0317
trigger times: 21
Loss after 17771259 batches: 0.0333
trigger times: 22
Loss after 17772222 batches: 0.0322
trigger times: 23
Loss after 17773185 batches: 0.0335
trigger times: 24
Loss after 17774148 batches: 0.0327
trigger times: 25
Early stopping!
Start to test process.
Loss after 17775111 batches: 0.0321
Time to train on one home:  52.525047302246094
trigger times: 0
Loss after 17776074 batches: 0.0950
trigger times: 1
Loss after 17777037 batches: 0.0868
trigger times: 2
Loss after 17778000 batches: 0.0802
trigger times: 3
Loss after 17778963 batches: 0.0759
trigger times: 4
Loss after 17779926 batches: 0.0717
trigger times: 5
Loss after 17780889 batches: 0.0693
trigger times: 6
Loss after 17781852 batches: 0.0670
trigger times: 7
Loss after 17782815 batches: 0.0651
trigger times: 8
Loss after 17783778 batches: 0.0638
trigger times: 9
Loss after 17784741 batches: 0.0619
trigger times: 10
Loss after 17785704 batches: 0.0607
trigger times: 11
Loss after 17786667 batches: 0.0593
trigger times: 12
Loss after 17787630 batches: 0.0587
trigger times: 13
Loss after 17788593 batches: 0.0560
trigger times: 14
Loss after 17789556 batches: 0.0593
trigger times: 15
Loss after 17790519 batches: 0.0558
trigger times: 16
Loss after 17791482 batches: 0.0566
trigger times: 17
Loss after 17792445 batches: 0.0545
trigger times: 18
Loss after 17793408 batches: 0.0545
trigger times: 19
Loss after 17794371 batches: 0.0559
trigger times: 20
Loss after 17795334 batches: 0.0566
trigger times: 21
Loss after 17796297 batches: 0.0556
trigger times: 22
Loss after 17797260 batches: 0.0533
trigger times: 23
Loss after 17798223 batches: 0.0533
trigger times: 24
Loss after 17799186 batches: 0.0531
trigger times: 25
Early stopping!
Start to test process.
Loss after 17800149 batches: 0.0554
Time to train on one home:  52.76298522949219
trigger times: 0
Loss after 17801112 batches: 0.0966
trigger times: 1
Loss after 17802075 batches: 0.0611
trigger times: 2
Loss after 17803038 batches: 0.0540
trigger times: 3
Loss after 17804001 batches: 0.0490
trigger times: 4
Loss after 17804964 batches: 0.0465
trigger times: 5
Loss after 17805927 batches: 0.0433
trigger times: 6
Loss after 17806890 batches: 0.0419
trigger times: 7
Loss after 17807853 batches: 0.0401
trigger times: 8
Loss after 17808816 batches: 0.0394
trigger times: 9
Loss after 17809779 batches: 0.0386
trigger times: 10
Loss after 17810742 batches: 0.0378
trigger times: 11
Loss after 17811705 batches: 0.0378
trigger times: 12
Loss after 17812668 batches: 0.0374
trigger times: 13
Loss after 17813631 batches: 0.0375
trigger times: 14
Loss after 17814594 batches: 0.0358
trigger times: 15
Loss after 17815557 batches: 0.0366
trigger times: 16
Loss after 17816520 batches: 0.0356
trigger times: 17
Loss after 17817483 batches: 0.0353
trigger times: 18
Loss after 17818446 batches: 0.0344
trigger times: 19
Loss after 17819409 batches: 0.0343
trigger times: 20
Loss after 17820372 batches: 0.0349
trigger times: 21
Loss after 17821335 batches: 0.0345
trigger times: 22
Loss after 17822298 batches: 0.0344
trigger times: 23
Loss after 17823261 batches: 0.0331
trigger times: 24
Loss after 17824224 batches: 0.0332
trigger times: 25
Early stopping!
Start to test process.
Loss after 17825187 batches: 0.0342
Time to train on one home:  52.47106432914734
trigger times: 0
Loss after 17826116 batches: 0.1033
trigger times: 1
Loss after 17827045 batches: 0.0650
trigger times: 2
Loss after 17827974 batches: 0.0484
trigger times: 3
Loss after 17828903 batches: 0.0435
trigger times: 4
Loss after 17829832 batches: 0.0361
trigger times: 5
Loss after 17830761 batches: 0.0326
trigger times: 6
Loss after 17831690 batches: 0.0322
trigger times: 7
Loss after 17832619 batches: 0.0327
trigger times: 8
Loss after 17833548 batches: 0.0293
trigger times: 9
Loss after 17834477 batches: 0.0282
trigger times: 10
Loss after 17835406 batches: 0.0264
trigger times: 11
Loss after 17836335 batches: 0.0271
trigger times: 12
Loss after 17837264 batches: 0.0249
trigger times: 13
Loss after 17838193 batches: 0.0239
trigger times: 14
Loss after 17839122 batches: 0.0269
trigger times: 15
Loss after 17840051 batches: 0.0239
trigger times: 16
Loss after 17840980 batches: 0.0228
trigger times: 17
Loss after 17841909 batches: 0.0227
trigger times: 18
Loss after 17842838 batches: 0.0216
trigger times: 19
Loss after 17843767 batches: 0.0223
trigger times: 20
Loss after 17844696 batches: 0.0231
trigger times: 21
Loss after 17845625 batches: 0.0245
trigger times: 22
Loss after 17846554 batches: 0.0232
trigger times: 23
Loss after 17847483 batches: 0.0263
trigger times: 24
Loss after 17848412 batches: 0.0250
trigger times: 25
Early stopping!
Start to test process.
Loss after 17849341 batches: 0.0225
Time to train on one home:  52.61305904388428
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17850304 batches: 0.0619
trigger times: 1
Loss after 17851267 batches: 0.0315
trigger times: 2
Loss after 17852230 batches: 0.0276
trigger times: 3
Loss after 17853193 batches: 0.0270
trigger times: 4
Loss after 17854156 batches: 0.0262
trigger times: 5
Loss after 17855119 batches: 0.0252
trigger times: 6
Loss after 17856082 batches: 0.0241
trigger times: 7
Loss after 17857045 batches: 0.0227
trigger times: 8
Loss after 17858008 batches: 0.0218
trigger times: 9
Loss after 17858971 batches: 0.0209
trigger times: 10
Loss after 17859934 batches: 0.0204
trigger times: 11
Loss after 17860897 batches: 0.0199
trigger times: 12
Loss after 17861860 batches: 0.0191
trigger times: 13
Loss after 17862823 batches: 0.0185
trigger times: 14
Loss after 17863786 batches: 0.0184
trigger times: 15
Loss after 17864749 batches: 0.0181
trigger times: 16
Loss after 17865712 batches: 0.0179
trigger times: 17
Loss after 17866675 batches: 0.0177
trigger times: 18
Loss after 17867638 batches: 0.0180
trigger times: 19
Loss after 17868601 batches: 0.0175
trigger times: 20
Loss after 17869564 batches: 0.0176
trigger times: 21
Loss after 17870527 batches: 0.0175
trigger times: 22
Loss after 17871490 batches: 0.0173
trigger times: 23
Loss after 17872453 batches: 0.0170
trigger times: 24
Loss after 17873416 batches: 0.0167
trigger times: 25
Early stopping!
Start to test process.
Loss after 17874379 batches: 0.0167
Time to train on one home:  52.69604444503784
trigger times: 0
Loss after 17875342 batches: 0.1678
trigger times: 1
Loss after 17876305 batches: 0.1203
trigger times: 2
Loss after 17877268 batches: 0.0954
trigger times: 3
Loss after 17878231 batches: 0.0834
trigger times: 4
Loss after 17879194 batches: 0.0775
trigger times: 5
Loss after 17880157 batches: 0.0700
trigger times: 6
Loss after 17881120 batches: 0.0663
trigger times: 7
Loss after 17882083 batches: 0.0620
trigger times: 8
Loss after 17883046 batches: 0.0574
trigger times: 9
Loss after 17884009 batches: 0.0562
trigger times: 10
Loss after 17884972 batches: 0.0524
trigger times: 11
Loss after 17885935 batches: 0.0513
trigger times: 12
Loss after 17886898 batches: 0.0477
trigger times: 13
Loss after 17887861 batches: 0.0486
trigger times: 14
Loss after 17888824 batches: 0.0461
trigger times: 15
Loss after 17889787 batches: 0.0453
trigger times: 16
Loss after 17890750 batches: 0.0457
trigger times: 17
Loss after 17891713 batches: 0.0441
trigger times: 18
Loss after 17892676 batches: 0.0430
trigger times: 19
Loss after 17893639 batches: 0.0424
trigger times: 20
Loss after 17894602 batches: 0.0420
trigger times: 21
Loss after 17895565 batches: 0.0410
trigger times: 22
Loss after 17896528 batches: 0.0401
trigger times: 23
Loss after 17897491 batches: 0.0400
trigger times: 24
Loss after 17898454 batches: 0.0377
trigger times: 25
Early stopping!
Start to test process.
Loss after 17899417 batches: 0.0406
Time to train on one home:  52.545331716537476
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17900380 batches: 0.0794
trigger times: 1
Loss after 17901343 batches: 0.0711
trigger times: 2
Loss after 17902306 batches: 0.0654
trigger times: 3
Loss after 17903269 batches: 0.0626
trigger times: 4
Loss after 17904232 batches: 0.0605
trigger times: 5
Loss after 17905195 batches: 0.0591
trigger times: 6
Loss after 17906158 batches: 0.0570
trigger times: 7
Loss after 17907121 batches: 0.0570
trigger times: 8
Loss after 17908084 batches: 0.0553
trigger times: 9
Loss after 17909047 batches: 0.0556
trigger times: 10
Loss after 17910010 batches: 0.0533
trigger times: 11
Loss after 17910973 batches: 0.0533
trigger times: 12
Loss after 17911936 batches: 0.0526
trigger times: 13
Loss after 17912899 batches: 0.0519
trigger times: 14
Loss after 17913862 batches: 0.0527
trigger times: 15
Loss after 17914825 batches: 0.0530
trigger times: 16
Loss after 17915788 batches: 0.0525
trigger times: 17
Loss after 17916751 batches: 0.0521
trigger times: 18
Loss after 17917714 batches: 0.0544
trigger times: 19
Loss after 17918677 batches: 0.0516
trigger times: 20
Loss after 17919640 batches: 0.0523
trigger times: 21
Loss after 17920603 batches: 0.0516
trigger times: 22
Loss after 17921566 batches: 0.0522
trigger times: 23
Loss after 17922529 batches: 0.0515
trigger times: 24
Loss after 17923492 batches: 0.0507
trigger times: 25
Early stopping!
Start to test process.
Loss after 17924455 batches: 0.0503
Time to train on one home:  52.68224906921387
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 17925418 batches: 0.0814
trigger times: 1
Loss after 17926381 batches: 0.0736
trigger times: 2
Loss after 17927344 batches: 0.0702
trigger times: 3
Loss after 17928307 batches: 0.0650
trigger times: 4
Loss after 17929270 batches: 0.0613
trigger times: 5
Loss after 17930233 batches: 0.0595
trigger times: 6
Loss after 17931196 batches: 0.0597
trigger times: 7
Loss after 17932159 batches: 0.0568
trigger times: 8
Loss after 17933122 batches: 0.0528
trigger times: 9
Loss after 17934085 batches: 0.0513
trigger times: 10
Loss after 17935048 batches: 0.0519
trigger times: 11
Loss after 17936011 batches: 0.0500
trigger times: 12
Loss after 17936974 batches: 0.0486
trigger times: 13
Loss after 17937937 batches: 0.0473
trigger times: 14
Loss after 17938900 batches: 0.0459
trigger times: 15
Loss after 17939863 batches: 0.0471
trigger times: 16
Loss after 17940826 batches: 0.0459
trigger times: 17
Loss after 17941789 batches: 0.0477
trigger times: 18
Loss after 17942752 batches: 0.0471
trigger times: 19
Loss after 17943715 batches: 0.0471
trigger times: 20
Loss after 17944678 batches: 0.0486
trigger times: 21
Loss after 17945641 batches: 0.0463
trigger times: 22
Loss after 17946604 batches: 0.0473
trigger times: 23
Loss after 17947567 batches: 0.0451
trigger times: 24
Loss after 17948530 batches: 0.0443
trigger times: 25
Early stopping!
Start to test process.
Loss after 17949493 batches: 0.0460
Time to train on one home:  52.67699599266052
trigger times: 0
Loss after 17950456 batches: 0.0209
trigger times: 1
Loss after 17951419 batches: 0.0175
trigger times: 2
Loss after 17952382 batches: 0.0165
trigger times: 3
Loss after 17953345 batches: 0.0151
trigger times: 4
Loss after 17954308 batches: 0.0142
trigger times: 5
Loss after 17955271 batches: 0.0134
trigger times: 6
Loss after 17956234 batches: 0.0128
trigger times: 7
Loss after 17957197 batches: 0.0124
trigger times: 8
Loss after 17958160 batches: 0.0128
trigger times: 9
Loss after 17959123 batches: 0.0124
trigger times: 10
Loss after 17960086 batches: 0.0120
trigger times: 11
Loss after 17961049 batches: 0.0112
trigger times: 12
Loss after 17962012 batches: 0.0117
trigger times: 13
Loss after 17962975 batches: 0.0113
trigger times: 14
Loss after 17963938 batches: 0.0111
trigger times: 15
Loss after 17964901 batches: 0.0108
trigger times: 16
Loss after 17965864 batches: 0.0114
trigger times: 17
Loss after 17966827 batches: 0.0112
trigger times: 18
Loss after 17967790 batches: 0.0111
trigger times: 19
Loss after 17968753 batches: 0.0107
trigger times: 20
Loss after 17969716 batches: 0.0105
trigger times: 21
Loss after 17970679 batches: 0.0107
trigger times: 22
Loss after 17971642 batches: 0.0107
trigger times: 23
Loss after 17972605 batches: 0.0104
trigger times: 24
Loss after 17973568 batches: 0.0107
trigger times: 25
Early stopping!
Start to test process.
Loss after 17974531 batches: 0.0101
Time to train on one home:  52.656829833984375
trigger times: 0
Loss after 17975494 batches: 0.0400
trigger times: 1
Loss after 17976457 batches: 0.0311
trigger times: 2
Loss after 17977420 batches: 0.0287
trigger times: 3
Loss after 17978383 batches: 0.0274
trigger times: 4
Loss after 17979346 batches: 0.0246
trigger times: 5
Loss after 17980309 batches: 0.0235
trigger times: 6
Loss after 17981272 batches: 0.0229
trigger times: 7
Loss after 17982235 batches: 0.0224
trigger times: 8
Loss after 17983198 batches: 0.0215
trigger times: 9
Loss after 17984161 batches: 0.0211
trigger times: 10
Loss after 17985124 batches: 0.0212
trigger times: 11
Loss after 17986087 batches: 0.0211
trigger times: 12
Loss after 17987050 batches: 0.0202
trigger times: 13
Loss after 17988013 batches: 0.0198
trigger times: 14
Loss after 17988976 batches: 0.0202
trigger times: 15
Loss after 17989939 batches: 0.0198
trigger times: 16
Loss after 17990902 batches: 0.0193
trigger times: 17
Loss after 17991865 batches: 0.0194
trigger times: 18
Loss after 17992828 batches: 0.0197
trigger times: 19
Loss after 17993791 batches: 0.0189
trigger times: 20
Loss after 17994754 batches: 0.0191
trigger times: 21
Loss after 17995717 batches: 0.0182
trigger times: 22
Loss after 17996680 batches: 0.0184
trigger times: 23
Loss after 17997643 batches: 0.0182
trigger times: 24
Loss after 17998606 batches: 0.0177
trigger times: 25
Early stopping!
Start to test process.
Loss after 17999569 batches: 0.0179
Time to train on one home:  52.64869284629822
trigger times: 0
Loss after 18000532 batches: 0.0855
trigger times: 1
Loss after 18001495 batches: 0.0614
trigger times: 2
Loss after 18002458 batches: 0.0535
trigger times: 3
Loss after 18003421 batches: 0.0485
trigger times: 4
Loss after 18004384 batches: 0.0452
trigger times: 5
Loss after 18005347 batches: 0.0434
trigger times: 6
Loss after 18006310 batches: 0.0419
trigger times: 7
Loss after 18007273 batches: 0.0393
trigger times: 8
Loss after 18008236 batches: 0.0392
trigger times: 9
Loss after 18009199 batches: 0.0383
trigger times: 10
Loss after 18010162 batches: 0.0373
trigger times: 11
Loss after 18011125 batches: 0.0359
trigger times: 12
Loss after 18012088 batches: 0.0360
trigger times: 13
Loss after 18013051 batches: 0.0353
trigger times: 14
Loss after 18014014 batches: 0.0352
trigger times: 15
Loss after 18014977 batches: 0.0353
trigger times: 16
Loss after 18015940 batches: 0.0341
trigger times: 17
Loss after 18016903 batches: 0.0328
trigger times: 18
Loss after 18017866 batches: 0.0332
trigger times: 19
Loss after 18018829 batches: 0.0325
trigger times: 20
Loss after 18019792 batches: 0.0317
trigger times: 21
Loss after 18020755 batches: 0.0327
trigger times: 22
Loss after 18021718 batches: 0.0327
trigger times: 23
Loss after 18022681 batches: 0.0328
trigger times: 24
Loss after 18023644 batches: 0.0325
trigger times: 25
Early stopping!
Start to test process.
Loss after 18024607 batches: 0.0311
Time to train on one home:  52.653568506240845
trigger times: 0
Loss after 18025570 batches: 0.0976
trigger times: 1
Loss after 18026533 batches: 0.0612
trigger times: 2
Loss after 18027496 batches: 0.0533
trigger times: 3
Loss after 18028459 batches: 0.0502
trigger times: 4
Loss after 18029422 batches: 0.0469
trigger times: 5
Loss after 18030385 batches: 0.0435
trigger times: 6
Loss after 18031348 batches: 0.0418
trigger times: 7
Loss after 18032311 batches: 0.0404
trigger times: 8
Loss after 18033274 batches: 0.0398
trigger times: 9
Loss after 18034237 batches: 0.0377
trigger times: 10
Loss after 18035200 batches: 0.0378
trigger times: 11
Loss after 18036163 batches: 0.0364
trigger times: 12
Loss after 18037126 batches: 0.0368
trigger times: 13
Loss after 18038089 batches: 0.0359
trigger times: 14
Loss after 18039052 batches: 0.0356
trigger times: 15
Loss after 18040015 batches: 0.0351
trigger times: 16
Loss after 18040978 batches: 0.0354
trigger times: 17
Loss after 18041941 batches: 0.0351
trigger times: 18
Loss after 18042904 batches: 0.0353
trigger times: 19
Loss after 18043867 batches: 0.0349
trigger times: 20
Loss after 18044830 batches: 0.0342
trigger times: 21
Loss after 18045793 batches: 0.0355
trigger times: 22
Loss after 18046756 batches: 0.0349
trigger times: 23
Loss after 18047719 batches: 0.0337
trigger times: 24
Loss after 18048682 batches: 0.0348
trigger times: 25
Early stopping!
Start to test process.
Loss after 18049645 batches: 0.0336
Time to train on one home:  52.85544490814209
trigger times: 0
Loss after 18050608 batches: 0.0519
trigger times: 1
Loss after 18051571 batches: 0.0408
trigger times: 2
Loss after 18052534 batches: 0.0354
trigger times: 3
Loss after 18053497 batches: 0.0324
trigger times: 4
Loss after 18054460 batches: 0.0298
trigger times: 5
Loss after 18055423 batches: 0.0288
trigger times: 6
Loss after 18056386 batches: 0.0279
trigger times: 7
Loss after 18057349 batches: 0.0266
trigger times: 8
Loss after 18058312 batches: 0.0267
trigger times: 9
Loss after 18059275 batches: 0.0264
trigger times: 10
Loss after 18060238 batches: 0.0253
trigger times: 11
Loss after 18061201 batches: 0.0251
trigger times: 12
Loss after 18062164 batches: 0.0243
trigger times: 13
Loss after 18063127 batches: 0.0248
trigger times: 14
Loss after 18064090 batches: 0.0257
trigger times: 15
Loss after 18065053 batches: 0.0252
trigger times: 16
Loss after 18066016 batches: 0.0235
trigger times: 17
Loss after 18066979 batches: 0.0236
trigger times: 18
Loss after 18067942 batches: 0.0236
trigger times: 19
Loss after 18068905 batches: 0.0241
trigger times: 20
Loss after 18069868 batches: 0.0226
trigger times: 21
Loss after 18070831 batches: 0.0229
trigger times: 22
Loss after 18071794 batches: 0.0238
trigger times: 23
Loss after 18072757 batches: 0.0236
trigger times: 24
Loss after 18073720 batches: 0.0239
trigger times: 25
Early stopping!
Start to test process.
Loss after 18074683 batches: 0.0240
Time to train on one home:  52.48845195770264
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18075646 batches: 0.0835
trigger times: 1
Loss after 18076609 batches: 0.0489
trigger times: 2
Loss after 18077572 batches: 0.0496
trigger times: 0
Loss after 18078535 batches: 0.0409
trigger times: 1
Loss after 18079498 batches: 0.0360
trigger times: 0
Loss after 18080461 batches: 0.0338
trigger times: 1
Loss after 18081424 batches: 0.0318
trigger times: 2
Loss after 18082387 batches: 0.0294
trigger times: 3
Loss after 18083350 batches: 0.0292
trigger times: 4
Loss after 18084313 batches: 0.0272
trigger times: 0
Loss after 18085276 batches: 0.0260
trigger times: 1
Loss after 18086239 batches: 0.0254
trigger times: 2
Loss after 18087202 batches: 0.0253
trigger times: 3
Loss after 18088165 batches: 0.0246
trigger times: 4
Loss after 18089128 batches: 0.0245
trigger times: 0
Loss after 18090091 batches: 0.0241
trigger times: 0
Loss after 18091054 batches: 0.0235
trigger times: 1
Loss after 18092017 batches: 0.0237
trigger times: 2
Loss after 18092980 batches: 0.0229
trigger times: 3
Loss after 18093943 batches: 0.0240
trigger times: 4
Loss after 18094906 batches: 0.0246
trigger times: 5
Loss after 18095869 batches: 0.0238
trigger times: 6
Loss after 18096832 batches: 0.0226
trigger times: 7
Loss after 18097795 batches: 0.0223
trigger times: 8
Loss after 18098758 batches: 0.0216
trigger times: 0
Loss after 18099721 batches: 0.0209
trigger times: 1
Loss after 18100684 batches: 0.0215
trigger times: 2
Loss after 18101647 batches: 0.0212
trigger times: 3
Loss after 18102610 batches: 0.0228
trigger times: 4
Loss after 18103573 batches: 0.0209
trigger times: 5
Loss after 18104536 batches: 0.0213
trigger times: 6
Loss after 18105499 batches: 0.0208
trigger times: 7
Loss after 18106462 batches: 0.0193
trigger times: 8
Loss after 18107425 batches: 0.0188
trigger times: 9
Loss after 18108388 batches: 0.0183
trigger times: 10
Loss after 18109351 batches: 0.0184
trigger times: 11
Loss after 18110314 batches: 0.0182
trigger times: 12
Loss after 18111277 batches: 0.0188
trigger times: 13
Loss after 18112240 batches: 0.0193
trigger times: 14
Loss after 18113203 batches: 0.0196
trigger times: 15
Loss after 18114166 batches: 0.0194
trigger times: 0
Loss after 18115129 batches: 0.0202
trigger times: 1
Loss after 18116092 batches: 0.0180
trigger times: 2
Loss after 18117055 batches: 0.0191
trigger times: 3
Loss after 18118018 batches: 0.0184
trigger times: 4
Loss after 18118981 batches: 0.0180
trigger times: 5
Loss after 18119944 batches: 0.0175
trigger times: 6
Loss after 18120907 batches: 0.0180
trigger times: 7
Loss after 18121870 batches: 0.0188
trigger times: 0
Loss after 18122833 batches: 0.0189
trigger times: 1
Loss after 18123796 batches: 0.0180
trigger times: 2
Loss after 18124759 batches: 0.0182
trigger times: 3
Loss after 18125722 batches: 0.0175
trigger times: 4
Loss after 18126685 batches: 0.0171
trigger times: 5
Loss after 18127648 batches: 0.0171
trigger times: 6
Loss after 18128611 batches: 0.0172
trigger times: 7
Loss after 18129574 batches: 0.0161
trigger times: 8
Loss after 18130537 batches: 0.0159
trigger times: 9
Loss after 18131500 batches: 0.0160
trigger times: 10
Loss after 18132463 batches: 0.0159
trigger times: 11
Loss after 18133426 batches: 0.0168
trigger times: 12
Loss after 18134389 batches: 0.0161
trigger times: 13
Loss after 18135352 batches: 0.0171
trigger times: 14
Loss after 18136315 batches: 0.0171
trigger times: 15
Loss after 18137278 batches: 0.0168
trigger times: 16
Loss after 18138241 batches: 0.0169
trigger times: 17
Loss after 18139204 batches: 0.0167
trigger times: 18
Loss after 18140167 batches: 0.0164
trigger times: 19
Loss after 18141130 batches: 0.0161
trigger times: 20
Loss after 18142093 batches: 0.0171
trigger times: 21
Loss after 18143056 batches: 0.0163
trigger times: 22
Loss after 18144019 batches: 0.0156
trigger times: 23
Loss after 18144982 batches: 0.0173
trigger times: 24
Loss after 18145945 batches: 0.0164
trigger times: 25
Early stopping!
Start to test process.
Loss after 18146908 batches: 0.0246
Time to train on one home:  89.93222546577454
trigger times: 0
Loss after 18147871 batches: 0.0955
trigger times: 1
Loss after 18148834 batches: 0.0785
trigger times: 2
Loss after 18149797 batches: 0.0753
trigger times: 3
Loss after 18150760 batches: 0.0742
trigger times: 4
Loss after 18151723 batches: 0.0696
trigger times: 5
Loss after 18152686 batches: 0.0686
trigger times: 6
Loss after 18153649 batches: 0.0666
trigger times: 7
Loss after 18154612 batches: 0.0639
trigger times: 8
Loss after 18155575 batches: 0.0617
trigger times: 9
Loss after 18156538 batches: 0.0602
trigger times: 10
Loss after 18157501 batches: 0.0607
trigger times: 11
Loss after 18158464 batches: 0.0595
trigger times: 12
Loss after 18159427 batches: 0.0578
trigger times: 13
Loss after 18160390 batches: 0.0573
trigger times: 14
Loss after 18161353 batches: 0.0571
trigger times: 15
Loss after 18162316 batches: 0.0562
trigger times: 16
Loss after 18163279 batches: 0.0574
trigger times: 17
Loss after 18164242 batches: 0.0575
trigger times: 18
Loss after 18165205 batches: 0.0555
trigger times: 19
Loss after 18166168 batches: 0.0551
trigger times: 20
Loss after 18167131 batches: 0.0555
trigger times: 21
Loss after 18168094 batches: 0.0537
trigger times: 22
Loss after 18169057 batches: 0.0543
trigger times: 23
Loss after 18170020 batches: 0.0554
trigger times: 24
Loss after 18170983 batches: 0.0538
trigger times: 25
Early stopping!
Start to test process.
Loss after 18171946 batches: 0.0533
Time to train on one home:  52.34433579444885
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18172909 batches: 0.0841
trigger times: 1
Loss after 18173872 batches: 0.0483
trigger times: 2
Loss after 18174835 batches: 0.0491
trigger times: 3
Loss after 18175798 batches: 0.0397
trigger times: 0
Loss after 18176761 batches: 0.0369
trigger times: 1
Loss after 18177724 batches: 0.0335
trigger times: 2
Loss after 18178687 batches: 0.0319
trigger times: 3
Loss after 18179650 batches: 0.0286
trigger times: 4
Loss after 18180613 batches: 0.0286
trigger times: 5
Loss after 18181576 batches: 0.0283
trigger times: 0
Loss after 18182539 batches: 0.0276
trigger times: 1
Loss after 18183502 batches: 0.0261
trigger times: 2
Loss after 18184465 batches: 0.0249
trigger times: 3
Loss after 18185428 batches: 0.0247
trigger times: 4
Loss after 18186391 batches: 0.0245
trigger times: 5
Loss after 18187354 batches: 0.0234
trigger times: 0
Loss after 18188317 batches: 0.0242
trigger times: 1
Loss after 18189280 batches: 0.0242
trigger times: 2
Loss after 18190243 batches: 0.0234
trigger times: 3
Loss after 18191206 batches: 0.0232
trigger times: 4
Loss after 18192169 batches: 0.0235
trigger times: 5
Loss after 18193132 batches: 0.0235
trigger times: 6
Loss after 18194095 batches: 0.0219
trigger times: 7
Loss after 18195058 batches: 0.0224
trigger times: 8
Loss after 18196021 batches: 0.0230
trigger times: 9
Loss after 18196984 batches: 0.0219
trigger times: 10
Loss after 18197947 batches: 0.0223
trigger times: 11
Loss after 18198910 batches: 0.0215
trigger times: 12
Loss after 18199873 batches: 0.0211
trigger times: 13
Loss after 18200836 batches: 0.0214
trigger times: 14
Loss after 18201799 batches: 0.0225
trigger times: 15
Loss after 18202762 batches: 0.0215
trigger times: 16
Loss after 18203725 batches: 0.0208
trigger times: 17
Loss after 18204688 batches: 0.0198
trigger times: 18
Loss after 18205651 batches: 0.0202
trigger times: 19
Loss after 18206614 batches: 0.0211
trigger times: 20
Loss after 18207577 batches: 0.0202
trigger times: 21
Loss after 18208540 batches: 0.0203
trigger times: 22
Loss after 18209503 batches: 0.0199
trigger times: 23
Loss after 18210466 batches: 0.0198
trigger times: 24
Loss after 18211429 batches: 0.0200
trigger times: 25
Early stopping!
Start to test process.
Loss after 18212392 batches: 0.0192
Time to train on one home:  64.54165267944336
trigger times: 0
Loss after 18213287 batches: 0.0728
trigger times: 1
Loss after 18214182 batches: 0.0435
trigger times: 2
Loss after 18215077 batches: 0.0197
trigger times: 3
Loss after 18215972 batches: 0.0102
trigger times: 4
Loss after 18216867 batches: 0.0084
trigger times: 5
Loss after 18217762 batches: 0.0061
trigger times: 6
Loss after 18218657 batches: 0.0054
trigger times: 7
Loss after 18219552 batches: 0.0043
trigger times: 8
Loss after 18220447 batches: 0.0039
trigger times: 9
Loss after 18221342 batches: 0.0059
trigger times: 10
Loss after 18222237 batches: 0.0043
trigger times: 11
Loss after 18223132 batches: 0.0041
trigger times: 12
Loss after 18224027 batches: 0.0032
trigger times: 13
Loss after 18224922 batches: 0.0029
trigger times: 14
Loss after 18225817 batches: 0.0026
trigger times: 15
Loss after 18226712 batches: 0.0028
trigger times: 16
Loss after 18227607 batches: 0.0046
trigger times: 17
Loss after 18228502 batches: 0.0038
trigger times: 18
Loss after 18229397 batches: 0.0031
trigger times: 19
Loss after 18230292 batches: 0.0029
trigger times: 20
Loss after 18231187 batches: 0.0039
trigger times: 21
Loss after 18232082 batches: 0.0037
trigger times: 22
Loss after 18232977 batches: 0.0039
trigger times: 23
Loss after 18233872 batches: 0.0032
trigger times: 24
Loss after 18234767 batches: 0.0025
trigger times: 25
Early stopping!
Start to test process.
Loss after 18235662 batches: 0.0021
Time to train on one home:  51.732449531555176
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18236599 batches: 0.0796
trigger times: 1
Loss after 18237536 batches: 0.0663
trigger times: 2
Loss after 18238473 batches: 0.0631
trigger times: 3
Loss after 18239410 batches: 0.0596
trigger times: 4
Loss after 18240347 batches: 0.0574
trigger times: 5
Loss after 18241284 batches: 0.0535
trigger times: 6
Loss after 18242221 batches: 0.0515
trigger times: 7
Loss after 18243158 batches: 0.0520
trigger times: 8
Loss after 18244095 batches: 0.0504
trigger times: 9
Loss after 18245032 batches: 0.0506
trigger times: 10
Loss after 18245969 batches: 0.0505
trigger times: 11
Loss after 18246906 batches: 0.0492
trigger times: 12
Loss after 18247843 batches: 0.0489
trigger times: 13
Loss after 18248780 batches: 0.0475
trigger times: 14
Loss after 18249717 batches: 0.0485
trigger times: 15
Loss after 18250654 batches: 0.0494
trigger times: 16
Loss after 18251591 batches: 0.0473
trigger times: 17
Loss after 18252528 batches: 0.0481
trigger times: 18
Loss after 18253465 batches: 0.0469
trigger times: 19
Loss after 18254402 batches: 0.0463
trigger times: 20
Loss after 18255339 batches: 0.0472
trigger times: 21
Loss after 18256276 batches: 0.0487
trigger times: 22
Loss after 18257213 batches: 0.0467
trigger times: 23
Loss after 18258150 batches: 0.0450
trigger times: 24
Loss after 18259087 batches: 0.0448
trigger times: 25
Early stopping!
Start to test process.
Loss after 18260024 batches: 0.0473
Time to train on one home:  52.540165424346924
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066, 0.03401813118363776, 0.033508403208926535, 0.03273922320832277, 0.032162070075287874, 0.031213877056718613, 0.03093800142524674, 0.030100551508447693, 0.029937661720089038, 0.02977997595640868]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863], [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861], [0.0757279247045517, 0.10266384027132536, 0.5909740425258612, 0.8803234656976516, 0.8076583271010261, 28.206427011804568, 8296.775], [0.06181953847408295, 0.10445801892372542, 0.5961459732150404, 0.831618823847048, 0.806043429565136, 26.645882531252276, 8280.187], [0.07333928346633911, 0.09887557787656209, 0.5904951093552152, 0.8805827319948272, 0.8110679704670578, 28.214734158180708, 8331.802], [0.06249981373548508, 0.1056225345966233, 0.6023961404422763, 0.8395811685763604, 0.8049953091236983, 26.90100386358231, 8269.419], [0.07111044973134995, 0.11364462231541128, 0.6015437535926366, 0.8360909977276793, 0.7977749276509928, 26.78917536742376, 8195.247], [0.06206701323390007, 0.09726051170449412, 0.5975866108942892, 0.8595424626232535, 0.8125216526577462, 27.540583297204083, 8346.734], [0.07088565826416016, 0.11302232012409985, 0.6007153971593981, 0.8537924311344753, 0.7983350331303676, 27.35634665030831, 8201.001], [0.06324874609708786, 0.09630308423125122, 0.5977169726517698, 0.8716301579538709, 0.8133833873815131, 27.927884907770363, 8355.587]]
Round_16_results:  [0.06324874609708786, 0.09630308423125122, 0.5977169726517698, 0.8716301579538709, 0.8133833873815131, 27.927884907770363, 8355.587]
trigger times: 0
Loss after 18260986 batches: 0.0792
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 19049 < 19050; dropping {'Training_Loss': 0.07924682434116091, 'Validation_Loss': 0.07613438367843628, 'Training_R2': -0.1219986635570609, 'Validation_R2': 0.12720934079934143, 'Training_F1': 0.41012083919169057, 'Validation_F1': 0.6139531747002098, 'Training_NEP': 0.8342603051948705, 'Validation_NEP': 0.7896888574184099, 'Training_NDE': 0.6833243909979391, 'Validation_NDE': 0.788545623554948, 'Training_MAE': 31.702160012216137, 'Validation_MAE': 28.08076224197103, 'Training_MSE': 2523.7808, 'Validation_MSE': 10329.957}.
trigger times: 1
Loss after 18261948 batches: 0.0640
trigger times: 2
Loss after 18262910 batches: 0.0625
trigger times: 3
Loss after 18263872 batches: 0.0590
trigger times: 4
Loss after 18264834 batches: 0.0565
trigger times: 5
Loss after 18265796 batches: 0.0545
trigger times: 6
Loss after 18266758 batches: 0.0530
trigger times: 7
Loss after 18267720 batches: 0.0512
trigger times: 8
Loss after 18268682 batches: 0.0500
trigger times: 9
Loss after 18269644 batches: 0.0507
trigger times: 10
Loss after 18270606 batches: 0.0497
trigger times: 11
Loss after 18271568 batches: 0.0495
trigger times: 12
Loss after 18272530 batches: 0.0483
trigger times: 13
Loss after 18273492 batches: 0.0485
trigger times: 14
Loss after 18274454 batches: 0.0486
trigger times: 15
Loss after 18275416 batches: 0.0480
trigger times: 16
Loss after 18276378 batches: 0.0476
trigger times: 17
Loss after 18277340 batches: 0.0476
trigger times: 18
Loss after 18278302 batches: 0.0478
trigger times: 19
Loss after 18279264 batches: 0.0483
trigger times: 20
Loss after 18280226 batches: 0.0480
trigger times: 21
Loss after 18281188 batches: 0.0458
trigger times: 22
Loss after 18282150 batches: 0.0464
trigger times: 23
Loss after 18283112 batches: 0.0449
trigger times: 24
Loss after 18284074 batches: 0.0448
trigger times: 25
Early stopping!
Start to test process.
Loss after 18285036 batches: 0.0456
Time to train on one home:  52.90609669685364
trigger times: 0
Loss after 18285965 batches: 0.0915
trigger times: 1
Loss after 18286894 batches: 0.0620
trigger times: 2
Loss after 18287823 batches: 0.0458
trigger times: 3
Loss after 18288752 batches: 0.0381
trigger times: 4
Loss after 18289681 batches: 0.0330
trigger times: 5
Loss after 18290610 batches: 0.0311
trigger times: 6
Loss after 18291539 batches: 0.0281
trigger times: 7
Loss after 18292468 batches: 0.0295
trigger times: 8
Loss after 18293397 batches: 0.0298
trigger times: 9
Loss after 18294326 batches: 0.0273
trigger times: 10
Loss after 18295255 batches: 0.0266
trigger times: 11
Loss after 18296184 batches: 0.0247
trigger times: 12
Loss after 18297113 batches: 0.0263
trigger times: 13
Loss after 18298042 batches: 0.0253
trigger times: 14
Loss after 18298971 batches: 0.0254
trigger times: 15
Loss after 18299900 batches: 0.0261
trigger times: 16
Loss after 18300829 batches: 0.0238
trigger times: 17
Loss after 18301758 batches: 0.0246
trigger times: 18
Loss after 18302687 batches: 0.0221
trigger times: 19
Loss after 18303616 batches: 0.0213
trigger times: 20
Loss after 18304545 batches: 0.0223
trigger times: 21
Loss after 18305474 batches: 0.0203
trigger times: 22
Loss after 18306403 batches: 0.0243
trigger times: 23
Loss after 18307332 batches: 0.0216
trigger times: 24
Loss after 18308261 batches: 0.0199
trigger times: 25
Early stopping!
Start to test process.
Loss after 18309190 batches: 0.0211
Time to train on one home:  52.50717306137085
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 18310153 batches: 0.0522
trigger times: 1
Loss after 18311116 batches: 0.0180
trigger times: 2
Loss after 18312079 batches: 0.0138
trigger times: 3
Loss after 18313042 batches: 0.0136
trigger times: 4
Loss after 18314005 batches: 0.0135
trigger times: 5
Loss after 18314968 batches: 0.0130
trigger times: 6
Loss after 18315931 batches: 0.0127
trigger times: 7
Loss after 18316894 batches: 0.0123
trigger times: 8
Loss after 18317857 batches: 0.0113
trigger times: 9
Loss after 18318820 batches: 0.0105
trigger times: 10
Loss after 18319783 batches: 0.0098
trigger times: 11
Loss after 18320746 batches: 0.0090
trigger times: 12
Loss after 18321709 batches: 0.0086
trigger times: 13
Loss after 18322672 batches: 0.0082
trigger times: 14
Loss after 18323635 batches: 0.0081
trigger times: 15
Loss after 18324598 batches: 0.0078
trigger times: 16
Loss after 18325561 batches: 0.0076
trigger times: 17
Loss after 18326524 batches: 0.0071
trigger times: 18
Loss after 18327487 batches: 0.0070
trigger times: 19
Loss after 18328450 batches: 0.0071
trigger times: 20
Loss after 18329413 batches: 0.0069
trigger times: 21
Loss after 18330376 batches: 0.0068
trigger times: 22
Loss after 18331339 batches: 0.0067
trigger times: 23
Loss after 18332302 batches: 0.0068
trigger times: 24
Loss after 18333265 batches: 0.0066
trigger times: 25
Early stopping!
Start to test process.
Loss after 18334228 batches: 0.0063
Time to train on one home:  52.85405158996582
trigger times: 0
Loss after 18335191 batches: 0.0223
trigger times: 1
Loss after 18336154 batches: 0.0180
trigger times: 0
Loss after 18337117 batches: 0.0170
trigger times: 1
Loss after 18338080 batches: 0.0153
trigger times: 2
Loss after 18339043 batches: 0.0142
trigger times: 3
Loss after 18340006 batches: 0.0134
trigger times: 4
Loss after 18340969 batches: 0.0129
trigger times: 5
Loss after 18341932 batches: 0.0125
trigger times: 6
Loss after 18342895 batches: 0.0116
trigger times: 7
Loss after 18343858 batches: 0.0117
trigger times: 8
Loss after 18344821 batches: 0.0112
trigger times: 9
Loss after 18345784 batches: 0.0113
trigger times: 10
Loss after 18346747 batches: 0.0115
trigger times: 11
Loss after 18347710 batches: 0.0112
trigger times: 12
Loss after 18348673 batches: 0.0105
trigger times: 13
Loss after 18349636 batches: 0.0110
trigger times: 14
Loss after 18350599 batches: 0.0112
trigger times: 15
Loss after 18351562 batches: 0.0109
trigger times: 16
Loss after 18352525 batches: 0.0107
trigger times: 17
Loss after 18353488 batches: 0.0108
trigger times: 18
Loss after 18354451 batches: 0.0106
trigger times: 19
Loss after 18355414 batches: 0.0103
trigger times: 20
Loss after 18356377 batches: 0.0102
trigger times: 21
Loss after 18357340 batches: 0.0102
trigger times: 22
Loss after 18358303 batches: 0.0100
trigger times: 23
Loss after 18359266 batches: 0.0097
trigger times: 24
Loss after 18360229 batches: 0.0094
trigger times: 25
Early stopping!
Start to test process.
Loss after 18361192 batches: 0.0093
Time to train on one home:  53.727821350097656
trigger times: 0
Loss after 18362155 batches: 0.0936
trigger times: 1
Loss after 18363118 batches: 0.0852
trigger times: 2
Loss after 18364081 batches: 0.0779
trigger times: 3
Loss after 18365044 batches: 0.0765
trigger times: 4
Loss after 18366007 batches: 0.0709
trigger times: 5
Loss after 18366970 batches: 0.0672
trigger times: 6
Loss after 18367933 batches: 0.0661
trigger times: 7
Loss after 18368896 batches: 0.0635
trigger times: 8
Loss after 18369859 batches: 0.0628
trigger times: 9
Loss after 18370822 batches: 0.0624
trigger times: 10
Loss after 18371785 batches: 0.0611
trigger times: 11
Loss after 18372748 batches: 0.0595
trigger times: 12
Loss after 18373711 batches: 0.0583
trigger times: 13
Loss after 18374674 batches: 0.0587
trigger times: 14
Loss after 18375637 batches: 0.0561
trigger times: 15
Loss after 18376600 batches: 0.0552
trigger times: 16
Loss after 18377563 batches: 0.0542
trigger times: 17
Loss after 18378526 batches: 0.0558
trigger times: 18
Loss after 18379489 batches: 0.0546
trigger times: 19
Loss after 18380452 batches: 0.0540
trigger times: 20
Loss after 18381415 batches: 0.0553
trigger times: 21
Loss after 18382378 batches: 0.0557
trigger times: 22
Loss after 18383341 batches: 0.0550
trigger times: 23
Loss after 18384304 batches: 0.0530
trigger times: 24
Loss after 18385267 batches: 0.0529
trigger times: 25
Early stopping!
Start to test process.
Loss after 18386230 batches: 0.0524
Time to train on one home:  52.27622437477112
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18387193 batches: 0.0835
trigger times: 1
Loss after 18388156 batches: 0.0695
trigger times: 2
Loss after 18389119 batches: 0.0663
trigger times: 3
Loss after 18390082 batches: 0.0633
trigger times: 4
Loss after 18391045 batches: 0.0609
trigger times: 5
Loss after 18392008 batches: 0.0593
trigger times: 6
Loss after 18392971 batches: 0.0577
trigger times: 7
Loss after 18393934 batches: 0.0558
trigger times: 8
Loss after 18394897 batches: 0.0552
trigger times: 9
Loss after 18395860 batches: 0.0535
trigger times: 10
Loss after 18396823 batches: 0.0537
trigger times: 11
Loss after 18397786 batches: 0.0532
trigger times: 12
Loss after 18398749 batches: 0.0532
trigger times: 13
Loss after 18399712 batches: 0.0522
trigger times: 14
Loss after 18400675 batches: 0.0521
trigger times: 15
Loss after 18401638 batches: 0.0520
trigger times: 16
Loss after 18402601 batches: 0.0505
trigger times: 17
Loss after 18403564 batches: 0.0498
trigger times: 18
Loss after 18404527 batches: 0.0512
trigger times: 19
Loss after 18405490 batches: 0.0509
trigger times: 20
Loss after 18406453 batches: 0.0505
trigger times: 21
Loss after 18407416 batches: 0.0491
trigger times: 22
Loss after 18408379 batches: 0.0488
trigger times: 23
Loss after 18409342 batches: 0.0492
trigger times: 24
Loss after 18410305 batches: 0.0488
trigger times: 25
Early stopping!
Start to test process.
Loss after 18411268 batches: 0.0489
Time to train on one home:  52.544697284698486
trigger times: 0
Loss after 18412231 batches: 0.0732
trigger times: 1
Loss after 18413194 batches: 0.0674
trigger times: 2
Loss after 18414157 batches: 0.0640
trigger times: 3
Loss after 18415120 batches: 0.0605
trigger times: 4
Loss after 18416083 batches: 0.0583
trigger times: 5
Loss after 18417046 batches: 0.0566
trigger times: 6
Loss after 18418009 batches: 0.0550
trigger times: 7
Loss after 18418972 batches: 0.0555
trigger times: 8
Loss after 18419935 batches: 0.0543
trigger times: 9
Loss after 18420898 batches: 0.0518
trigger times: 10
Loss after 18421861 batches: 0.0528
trigger times: 11
Loss after 18422824 batches: 0.0518
trigger times: 12
Loss after 18423787 batches: 0.0522
trigger times: 13
Loss after 18424750 batches: 0.0521
trigger times: 14
Loss after 18425713 batches: 0.0502
trigger times: 15
Loss after 18426676 batches: 0.0490
trigger times: 16
Loss after 18427639 batches: 0.0488
trigger times: 17
Loss after 18428602 batches: 0.0477
trigger times: 18
Loss after 18429565 batches: 0.0487
trigger times: 19
Loss after 18430528 batches: 0.0487
trigger times: 20
Loss after 18431491 batches: 0.0481
trigger times: 21
Loss after 18432454 batches: 0.0478
trigger times: 22
Loss after 18433417 batches: 0.0483
trigger times: 23
Loss after 18434380 batches: 0.0472
trigger times: 24
Loss after 18435343 batches: 0.0475
trigger times: 25
Early stopping!
Start to test process.
Loss after 18436306 batches: 0.0463
Time to train on one home:  52.63106656074524
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18437269 batches: 0.0638
trigger times: 1
Loss after 18438232 batches: 0.0477
trigger times: 2
Loss after 18439195 batches: 0.0459
trigger times: 3
Loss after 18440158 batches: 0.0391
trigger times: 4
Loss after 18441121 batches: 0.0341
trigger times: 5
Loss after 18442084 batches: 0.0324
trigger times: 6
Loss after 18443047 batches: 0.0305
trigger times: 7
Loss after 18444010 batches: 0.0283
trigger times: 8
Loss after 18444973 batches: 0.0273
trigger times: 9
Loss after 18445936 batches: 0.0257
trigger times: 10
Loss after 18446899 batches: 0.0256
trigger times: 11
Loss after 18447862 batches: 0.0253
trigger times: 12
Loss after 18448825 batches: 0.0248
trigger times: 13
Loss after 18449788 batches: 0.0237
trigger times: 14
Loss after 18450751 batches: 0.0231
trigger times: 15
Loss after 18451714 batches: 0.0228
trigger times: 16
Loss after 18452677 batches: 0.0230
trigger times: 17
Loss after 18453640 batches: 0.0223
trigger times: 18
Loss after 18454603 batches: 0.0220
trigger times: 19
Loss after 18455566 batches: 0.0224
trigger times: 20
Loss after 18456529 batches: 0.0211
trigger times: 21
Loss after 18457492 batches: 0.0212
trigger times: 22
Loss after 18458455 batches: 0.0207
trigger times: 23
Loss after 18459418 batches: 0.0208
trigger times: 24
Loss after 18460381 batches: 0.0203
trigger times: 25
Early stopping!
Start to test process.
Loss after 18461344 batches: 0.0217
Time to train on one home:  52.44430994987488
trigger times: 0
Loss after 18462302 batches: 0.0606
trigger times: 1
Loss after 18463260 batches: 0.0408
trigger times: 2
Loss after 18464218 batches: 0.0334
trigger times: 3
Loss after 18465176 batches: 0.0297
trigger times: 4
Loss after 18466134 batches: 0.0255
trigger times: 5
Loss after 18467092 batches: 0.0238
trigger times: 6
Loss after 18468050 batches: 0.0227
trigger times: 7
Loss after 18469008 batches: 0.0205
trigger times: 8
Loss after 18469966 batches: 0.0195
trigger times: 9
Loss after 18470924 batches: 0.0197
trigger times: 10
Loss after 18471882 batches: 0.0188
trigger times: 11
Loss after 18472840 batches: 0.0180
trigger times: 12
Loss after 18473798 batches: 0.0182
trigger times: 13
Loss after 18474756 batches: 0.0182
trigger times: 14
Loss after 18475714 batches: 0.0182
trigger times: 15
Loss after 18476672 batches: 0.0168
trigger times: 16
Loss after 18477630 batches: 0.0169
trigger times: 17
Loss after 18478588 batches: 0.0172
trigger times: 18
Loss after 18479546 batches: 0.0158
trigger times: 19
Loss after 18480504 batches: 0.0154
trigger times: 20
Loss after 18481462 batches: 0.0152
trigger times: 21
Loss after 18482420 batches: 0.0148
trigger times: 22
Loss after 18483378 batches: 0.0141
trigger times: 23
Loss after 18484336 batches: 0.0138
trigger times: 24
Loss after 18485294 batches: 0.0142
trigger times: 25
Early stopping!
Start to test process.
Loss after 18486252 batches: 0.0142
Time to train on one home:  52.74636936187744
trigger times: 0
Loss after 18487214 batches: 0.0782
trigger times: 1
Loss after 18488176 batches: 0.0640
trigger times: 2
Loss after 18489138 batches: 0.0614
trigger times: 3
Loss after 18490100 batches: 0.0592
trigger times: 4
Loss after 18491062 batches: 0.0564
trigger times: 5
Loss after 18492024 batches: 0.0548
trigger times: 6
Loss after 18492986 batches: 0.0530
trigger times: 7
Loss after 18493948 batches: 0.0522
trigger times: 8
Loss after 18494910 batches: 0.0504
trigger times: 9
Loss after 18495872 batches: 0.0507
trigger times: 10
Loss after 18496834 batches: 0.0502
trigger times: 11
Loss after 18497796 batches: 0.0490
trigger times: 12
Loss after 18498758 batches: 0.0485
trigger times: 13
Loss after 18499720 batches: 0.0488
trigger times: 14
Loss after 18500682 batches: 0.0476
trigger times: 15
Loss after 18501644 batches: 0.0471
trigger times: 16
Loss after 18502606 batches: 0.0467
trigger times: 17
Loss after 18503568 batches: 0.0467
trigger times: 18
Loss after 18504530 batches: 0.0465
trigger times: 19
Loss after 18505492 batches: 0.0470
trigger times: 20
Loss after 18506454 batches: 0.0457
trigger times: 21
Loss after 18507416 batches: 0.0465
trigger times: 22
Loss after 18508378 batches: 0.0463
trigger times: 23
Loss after 18509340 batches: 0.0458
trigger times: 24
Loss after 18510302 batches: 0.0447
trigger times: 25
Early stopping!
Start to test process.
Loss after 18511264 batches: 0.0459
Time to train on one home:  52.781482458114624
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18512227 batches: 0.0944
trigger times: 1
Loss after 18513190 batches: 0.0276
trigger times: 2
Loss after 18514153 batches: 0.0195
trigger times: 3
Loss after 18515116 batches: 0.0182
trigger times: 4
Loss after 18516079 batches: 0.0165
trigger times: 5
Loss after 18517042 batches: 0.0152
trigger times: 6
Loss after 18518005 batches: 0.0143
trigger times: 7
Loss after 18518968 batches: 0.0137
trigger times: 8
Loss after 18519931 batches: 0.0135
trigger times: 9
Loss after 18520894 batches: 0.0131
trigger times: 10
Loss after 18521857 batches: 0.0130
trigger times: 11
Loss after 18522820 batches: 0.0128
trigger times: 12
Loss after 18523783 batches: 0.0131
trigger times: 13
Loss after 18524746 batches: 0.0127
trigger times: 14
Loss after 18525709 batches: 0.0126
trigger times: 15
Loss after 18526672 batches: 0.0124
trigger times: 16
Loss after 18527635 batches: 0.0122
trigger times: 17
Loss after 18528598 batches: 0.0123
trigger times: 18
Loss after 18529561 batches: 0.0121
trigger times: 19
Loss after 18530524 batches: 0.0121
trigger times: 20
Loss after 18531487 batches: 0.0121
trigger times: 21
Loss after 18532450 batches: 0.0118
trigger times: 22
Loss after 18533413 batches: 0.0118
trigger times: 23
Loss after 18534376 batches: 0.0117
trigger times: 24
Loss after 18535339 batches: 0.0117
trigger times: 25
Early stopping!
Start to test process.
Loss after 18536302 batches: 0.0118
Time to train on one home:  52.56166219711304
trigger times: 0
Loss after 18537265 batches: 0.0486
trigger times: 1
Loss after 18538228 batches: 0.0411
trigger times: 2
Loss after 18539191 batches: 0.0349
trigger times: 3
Loss after 18540154 batches: 0.0313
trigger times: 4
Loss after 18541117 batches: 0.0291
trigger times: 5
Loss after 18542080 batches: 0.0283
trigger times: 6
Loss after 18543043 batches: 0.0275
trigger times: 7
Loss after 18544006 batches: 0.0249
trigger times: 8
Loss after 18544969 batches: 0.0252
trigger times: 9
Loss after 18545932 batches: 0.0249
trigger times: 10
Loss after 18546895 batches: 0.0253
trigger times: 11
Loss after 18547858 batches: 0.0249
trigger times: 12
Loss after 18548821 batches: 0.0243
trigger times: 13
Loss after 18549784 batches: 0.0242
trigger times: 14
Loss after 18550747 batches: 0.0244
trigger times: 15
Loss after 18551710 batches: 0.0235
trigger times: 16
Loss after 18552673 batches: 0.0236
trigger times: 17
Loss after 18553636 batches: 0.0234
trigger times: 18
Loss after 18554599 batches: 0.0238
trigger times: 19
Loss after 18555562 batches: 0.0235
trigger times: 20
Loss after 18556525 batches: 0.0229
trigger times: 21
Loss after 18557488 batches: 0.0234
trigger times: 22
Loss after 18558451 batches: 0.0251
trigger times: 23
Loss after 18559414 batches: 0.0259
trigger times: 24
Loss after 18560377 batches: 0.0248
trigger times: 25
Early stopping!
Start to test process.
Loss after 18561340 batches: 0.0240
Time to train on one home:  52.62070441246033
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18562303 batches: 0.0635
trigger times: 1
Loss after 18563266 batches: 0.0476
trigger times: 2
Loss after 18564229 batches: 0.0454
trigger times: 3
Loss after 18565192 batches: 0.0386
trigger times: 4
Loss after 18566155 batches: 0.0347
trigger times: 5
Loss after 18567118 batches: 0.0315
trigger times: 6
Loss after 18568081 batches: 0.0298
trigger times: 7
Loss after 18569044 batches: 0.0288
trigger times: 8
Loss after 18570007 batches: 0.0274
trigger times: 9
Loss after 18570970 batches: 0.0259
trigger times: 10
Loss after 18571933 batches: 0.0255
trigger times: 11
Loss after 18572896 batches: 0.0252
trigger times: 12
Loss after 18573859 batches: 0.0245
trigger times: 13
Loss after 18574822 batches: 0.0251
trigger times: 14
Loss after 18575785 batches: 0.0246
trigger times: 15
Loss after 18576748 batches: 0.0237
trigger times: 16
Loss after 18577711 batches: 0.0226
trigger times: 17
Loss after 18578674 batches: 0.0226
trigger times: 18
Loss after 18579637 batches: 0.0229
trigger times: 19
Loss after 18580600 batches: 0.0227
trigger times: 20
Loss after 18581563 batches: 0.0228
trigger times: 21
Loss after 18582526 batches: 0.0217
trigger times: 22
Loss after 18583489 batches: 0.0220
trigger times: 23
Loss after 18584452 batches: 0.0222
trigger times: 24
Loss after 18585415 batches: 0.0210
trigger times: 25
Early stopping!
Start to test process.
Loss after 18586378 batches: 0.0213
Time to train on one home:  53.17061519622803
trigger times: 0
Loss after 18587341 batches: 0.0496
trigger times: 1
Loss after 18588304 batches: 0.0397
trigger times: 2
Loss after 18589267 batches: 0.0356
trigger times: 3
Loss after 18590230 batches: 0.0309
trigger times: 4
Loss after 18591193 batches: 0.0293
trigger times: 5
Loss after 18592156 batches: 0.0288
trigger times: 6
Loss after 18593119 batches: 0.0271
trigger times: 7
Loss after 18594082 batches: 0.0264
trigger times: 8
Loss after 18595045 batches: 0.0273
trigger times: 9
Loss after 18596008 batches: 0.0272
trigger times: 10
Loss after 18596971 batches: 0.0261
trigger times: 11
Loss after 18597934 batches: 0.0253
trigger times: 12
Loss after 18598897 batches: 0.0250
trigger times: 13
Loss after 18599860 batches: 0.0240
trigger times: 14
Loss after 18600823 batches: 0.0240
trigger times: 15
Loss after 18601786 batches: 0.0244
trigger times: 16
Loss after 18602749 batches: 0.0255
trigger times: 17
Loss after 18603712 batches: 0.0246
trigger times: 18
Loss after 18604675 batches: 0.0223
trigger times: 19
Loss after 18605638 batches: 0.0235
trigger times: 20
Loss after 18606601 batches: 0.0223
trigger times: 21
Loss after 18607564 batches: 0.0222
trigger times: 22
Loss after 18608527 batches: 0.0225
trigger times: 23
Loss after 18609490 batches: 0.0214
trigger times: 24
Loss after 18610453 batches: 0.0206
trigger times: 25
Early stopping!
Start to test process.
Loss after 18611416 batches: 0.0222
Time to train on one home:  52.60987329483032
trigger times: 0
Loss after 18612379 batches: 0.0461
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 18613342 batches: 0.0423
trigger times: 2
Loss after 18614305 batches: 0.0388
trigger times: 3
Loss after 18615268 batches: 0.0366
trigger times: 4
Loss after 18616231 batches: 0.0344
trigger times: 5
Loss after 18617194 batches: 0.0322
trigger times: 6
Loss after 18618157 batches: 0.0312
trigger times: 7
Loss after 18619120 batches: 0.0310
trigger times: 8
Loss after 18620083 batches: 0.0313
trigger times: 9
Loss after 18621046 batches: 0.0301
trigger times: 10
Loss after 18622009 batches: 0.0286
trigger times: 11
Loss after 18622972 batches: 0.0276
trigger times: 12
Loss after 18623935 batches: 0.0281
trigger times: 13
Loss after 18624898 batches: 0.0272
trigger times: 14
Loss after 18625861 batches: 0.0285
trigger times: 15
Loss after 18626824 batches: 0.0276
trigger times: 16
Loss after 18627787 batches: 0.0283
trigger times: 17
Loss after 18628750 batches: 0.0270
trigger times: 18
Loss after 18629713 batches: 0.0257
trigger times: 19
Loss after 18630676 batches: 0.0255
trigger times: 20
Loss after 18631639 batches: 0.0249
trigger times: 21
Loss after 18632602 batches: 0.0253
trigger times: 22
Loss after 18633565 batches: 0.0247
trigger times: 23
Loss after 18634528 batches: 0.0248
trigger times: 0
Loss after 18635491 batches: 0.0243
trigger times: 1
Loss after 18636454 batches: 0.0241
trigger times: 2
Loss after 18637417 batches: 0.0241
trigger times: 3
Loss after 18638380 batches: 0.0253
trigger times: 4
Loss after 18639343 batches: 0.0237
trigger times: 5
Loss after 18640306 batches: 0.0242
trigger times: 6
Loss after 18641269 batches: 0.0232
trigger times: 7
Loss after 18642232 batches: 0.0230
trigger times: 8
Loss after 18643195 batches: 0.0222
trigger times: 9
Loss after 18644158 batches: 0.0228
trigger times: 10
Loss after 18645121 batches: 0.0226
trigger times: 11
Loss after 18646084 batches: 0.0227
trigger times: 12
Loss after 18647047 batches: 0.0237
trigger times: 13
Loss after 18648010 batches: 0.0232
trigger times: 14
Loss after 18648973 batches: 0.0226
trigger times: 15
Loss after 18649936 batches: 0.0216
trigger times: 16
Loss after 18650899 batches: 0.0213
trigger times: 17
Loss after 18651862 batches: 0.0212
trigger times: 18
Loss after 18652825 batches: 0.0219
trigger times: 19
Loss after 18653788 batches: 0.0221
trigger times: 20
Loss after 18654751 batches: 0.0221
trigger times: 21
Loss after 18655714 batches: 0.0217
trigger times: 22
Loss after 18656677 batches: 0.0226
trigger times: 23
Loss after 18657640 batches: 0.0215
trigger times: 24
Loss after 18658603 batches: 0.0223
trigger times: 25
Early stopping!
Start to test process.
Loss after 18659566 batches: 0.0216
Time to train on one home:  71.28017663955688
trigger times: 0
Loss after 18660529 batches: 0.0881
trigger times: 1
Loss after 18661492 batches: 0.0494
trigger times: 2
Loss after 18662455 batches: 0.0478
trigger times: 3
Loss after 18663418 batches: 0.0450
trigger times: 4
Loss after 18664381 batches: 0.0428
trigger times: 5
Loss after 18665344 batches: 0.0403
trigger times: 6
Loss after 18666307 batches: 0.0393
trigger times: 7
Loss after 18667270 batches: 0.0379
trigger times: 8
Loss after 18668233 batches: 0.0373
trigger times: 9
Loss after 18669196 batches: 0.0358
trigger times: 10
Loss after 18670159 batches: 0.0357
trigger times: 11
Loss after 18671122 batches: 0.0352
trigger times: 12
Loss after 18672085 batches: 0.0352
trigger times: 13
Loss after 18673048 batches: 0.0347
trigger times: 14
Loss after 18674011 batches: 0.0341
trigger times: 15
Loss after 18674974 batches: 0.0347
trigger times: 16
Loss after 18675937 batches: 0.0334
trigger times: 17
Loss after 18676900 batches: 0.0326
trigger times: 18
Loss after 18677863 batches: 0.0337
trigger times: 19
Loss after 18678826 batches: 0.0333
trigger times: 20
Loss after 18679789 batches: 0.0325
trigger times: 21
Loss after 18680752 batches: 0.0320
trigger times: 22
Loss after 18681715 batches: 0.0319
trigger times: 23
Loss after 18682678 batches: 0.0319
trigger times: 24
Loss after 18683641 batches: 0.0323
trigger times: 25
Early stopping!
Start to test process.
Loss after 18684604 batches: 0.0307
Time to train on one home:  52.48696851730347
trigger times: 0
Loss after 18685567 batches: 0.0942
trigger times: 1
Loss after 18686530 batches: 0.0851
trigger times: 2
Loss after 18687493 batches: 0.0790
trigger times: 3
Loss after 18688456 batches: 0.0755
trigger times: 4
Loss after 18689419 batches: 0.0711
trigger times: 5
Loss after 18690382 batches: 0.0669
trigger times: 6
Loss after 18691345 batches: 0.0646
trigger times: 7
Loss after 18692308 batches: 0.0615
trigger times: 8
Loss after 18693271 batches: 0.0611
trigger times: 9
Loss after 18694234 batches: 0.0606
trigger times: 10
Loss after 18695197 batches: 0.0593
trigger times: 11
Loss after 18696160 batches: 0.0587
trigger times: 12
Loss after 18697123 batches: 0.0574
trigger times: 13
Loss after 18698086 batches: 0.0584
trigger times: 14
Loss after 18699049 batches: 0.0576
trigger times: 15
Loss after 18700012 batches: 0.0568
trigger times: 16
Loss after 18700975 batches: 0.0572
trigger times: 17
Loss after 18701938 batches: 0.0563
trigger times: 18
Loss after 18702901 batches: 0.0546
trigger times: 19
Loss after 18703864 batches: 0.0539
trigger times: 20
Loss after 18704827 batches: 0.0535
trigger times: 21
Loss after 18705790 batches: 0.0536
trigger times: 22
Loss after 18706753 batches: 0.0543
trigger times: 23
Loss after 18707716 batches: 0.0526
trigger times: 24
Loss after 18708679 batches: 0.0539
trigger times: 25
Early stopping!
Start to test process.
Loss after 18709642 batches: 0.0524
Time to train on one home:  52.19405961036682
trigger times: 0
Loss after 18710605 batches: 0.0912
trigger times: 1
Loss after 18711568 batches: 0.0583
trigger times: 2
Loss after 18712531 batches: 0.0536
trigger times: 3
Loss after 18713494 batches: 0.0467
trigger times: 4
Loss after 18714457 batches: 0.0447
trigger times: 5
Loss after 18715420 batches: 0.0420
trigger times: 6
Loss after 18716383 batches: 0.0400
trigger times: 7
Loss after 18717346 batches: 0.0398
trigger times: 8
Loss after 18718309 batches: 0.0389
trigger times: 9
Loss after 18719272 batches: 0.0384
trigger times: 10
Loss after 18720235 batches: 0.0376
trigger times: 11
Loss after 18721198 batches: 0.0369
trigger times: 12
Loss after 18722161 batches: 0.0362
trigger times: 13
Loss after 18723124 batches: 0.0375
trigger times: 14
Loss after 18724087 batches: 0.0366
trigger times: 15
Loss after 18725050 batches: 0.0352
trigger times: 16
Loss after 18726013 batches: 0.0351
trigger times: 17
Loss after 18726976 batches: 0.0346
trigger times: 18
Loss after 18727939 batches: 0.0354
trigger times: 19
Loss after 18728902 batches: 0.0347
trigger times: 20
Loss after 18729865 batches: 0.0346
trigger times: 21
Loss after 18730828 batches: 0.0346
trigger times: 22
Loss after 18731791 batches: 0.0337
trigger times: 23
Loss after 18732754 batches: 0.0328
trigger times: 24
Loss after 18733717 batches: 0.0332
trigger times: 25
Early stopping!
Start to test process.
Loss after 18734680 batches: 0.0334
Time to train on one home:  52.72597670555115
trigger times: 0
Loss after 18735609 batches: 0.0908
trigger times: 1
Loss after 18736538 batches: 0.0638
trigger times: 2
Loss after 18737467 batches: 0.0479
trigger times: 3
Loss after 18738396 batches: 0.0400
trigger times: 4
Loss after 18739325 batches: 0.0354
trigger times: 5
Loss after 18740254 batches: 0.0329
trigger times: 6
Loss after 18741183 batches: 0.0300
trigger times: 7
Loss after 18742112 batches: 0.0282
trigger times: 8
Loss after 18743041 batches: 0.0247
trigger times: 9
Loss after 18743970 batches: 0.0285
trigger times: 10
Loss after 18744899 batches: 0.0293
trigger times: 11
Loss after 18745828 batches: 0.0352
trigger times: 12
Loss after 18746757 batches: 0.0302
trigger times: 13
Loss after 18747686 batches: 0.0274
trigger times: 14
Loss after 18748615 batches: 0.0279
trigger times: 15
Loss after 18749544 batches: 0.0275
trigger times: 16
Loss after 18750473 batches: 0.0273
trigger times: 17
Loss after 18751402 batches: 0.0323
trigger times: 18
Loss after 18752331 batches: 0.0348
trigger times: 19
Loss after 18753260 batches: 0.0327
trigger times: 20
Loss after 18754189 batches: 0.0299
trigger times: 21
Loss after 18755118 batches: 0.0272
trigger times: 22
Loss after 18756047 batches: 0.0264
trigger times: 23
Loss after 18756976 batches: 0.0257
trigger times: 24
Loss after 18757905 batches: 0.0245
trigger times: 25
Early stopping!
Start to test process.
Loss after 18758834 batches: 0.0244
Time to train on one home:  52.00418281555176
trigger times: 0
Loss after 18759797 batches: 0.0761
trigger times: 1
Loss after 18760760 batches: 0.0339
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 2
Loss after 18761723 batches: 0.0273
trigger times: 3
Loss after 18762686 batches: 0.0269
trigger times: 4
Loss after 18763649 batches: 0.0256
trigger times: 5
Loss after 18764612 batches: 0.0250
trigger times: 6
Loss after 18765575 batches: 0.0236
trigger times: 7
Loss after 18766538 batches: 0.0222
trigger times: 8
Loss after 18767501 batches: 0.0211
trigger times: 9
Loss after 18768464 batches: 0.0203
trigger times: 10
Loss after 18769427 batches: 0.0199
trigger times: 11
Loss after 18770390 batches: 0.0194
trigger times: 12
Loss after 18771353 batches: 0.0193
trigger times: 13
Loss after 18772316 batches: 0.0184
trigger times: 14
Loss after 18773279 batches: 0.0186
trigger times: 15
Loss after 18774242 batches: 0.0183
trigger times: 16
Loss after 18775205 batches: 0.0179
trigger times: 17
Loss after 18776168 batches: 0.0179
trigger times: 18
Loss after 18777131 batches: 0.0173
trigger times: 19
Loss after 18778094 batches: 0.0176
trigger times: 20
Loss after 18779057 batches: 0.0172
trigger times: 21
Loss after 18780020 batches: 0.0171
trigger times: 22
Loss after 18780983 batches: 0.0171
trigger times: 23
Loss after 18781946 batches: 0.0166
trigger times: 24
Loss after 18782909 batches: 0.0167
trigger times: 25
Early stopping!
Start to test process.
Loss after 18783872 batches: 0.0166
Time to train on one home:  52.72598338127136
trigger times: 0
Loss after 18784835 batches: 0.1845
trigger times: 1
Loss after 18785798 batches: 0.1244
trigger times: 2
Loss after 18786761 batches: 0.0942
trigger times: 3
Loss after 18787724 batches: 0.0879
trigger times: 4
Loss after 18788687 batches: 0.0789
trigger times: 5
Loss after 18789650 batches: 0.0733
trigger times: 6
Loss after 18790613 batches: 0.0681
trigger times: 7
Loss after 18791576 batches: 0.0632
trigger times: 8
Loss after 18792539 batches: 0.0596
trigger times: 9
Loss after 18793502 batches: 0.0552
trigger times: 10
Loss after 18794465 batches: 0.0518
trigger times: 11
Loss after 18795428 batches: 0.0512
trigger times: 12
Loss after 18796391 batches: 0.0481
trigger times: 13
Loss after 18797354 batches: 0.0484
trigger times: 14
Loss after 18798317 batches: 0.0458
trigger times: 15
Loss after 18799280 batches: 0.0449
trigger times: 16
Loss after 18800243 batches: 0.0436
trigger times: 17
Loss after 18801206 batches: 0.0434
trigger times: 18
Loss after 18802169 batches: 0.0426
trigger times: 19
Loss after 18803132 batches: 0.0434
trigger times: 20
Loss after 18804095 batches: 0.0410
trigger times: 21
Loss after 18805058 batches: 0.0410
trigger times: 22
Loss after 18806021 batches: 0.0430
trigger times: 23
Loss after 18806984 batches: 0.0419
trigger times: 24
Loss after 18807947 batches: 0.0411
trigger times: 25
Early stopping!
Start to test process.
Loss after 18808910 batches: 0.0406
Time to train on one home:  52.25313639640808
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18809873 batches: 0.0841
trigger times: 1
Loss after 18810836 batches: 0.0692
trigger times: 2
Loss after 18811799 batches: 0.0663
trigger times: 3
Loss after 18812762 batches: 0.0643
trigger times: 4
Loss after 18813725 batches: 0.0608
trigger times: 5
Loss after 18814688 batches: 0.0596
trigger times: 6
Loss after 18815651 batches: 0.0568
trigger times: 7
Loss after 18816614 batches: 0.0557
trigger times: 8
Loss after 18817577 batches: 0.0542
trigger times: 9
Loss after 18818540 batches: 0.0534
trigger times: 10
Loss after 18819503 batches: 0.0540
trigger times: 11
Loss after 18820466 batches: 0.0540
trigger times: 12
Loss after 18821429 batches: 0.0530
trigger times: 13
Loss after 18822392 batches: 0.0505
trigger times: 14
Loss after 18823355 batches: 0.0509
trigger times: 15
Loss after 18824318 batches: 0.0515
trigger times: 16
Loss after 18825281 batches: 0.0521
trigger times: 17
Loss after 18826244 batches: 0.0517
trigger times: 18
Loss after 18827207 batches: 0.0528
trigger times: 19
Loss after 18828170 batches: 0.0520
trigger times: 20
Loss after 18829133 batches: 0.0509
trigger times: 21
Loss after 18830096 batches: 0.0495
trigger times: 22
Loss after 18831059 batches: 0.0506
trigger times: 23
Loss after 18832022 batches: 0.0492
trigger times: 24
Loss after 18832985 batches: 0.0501
trigger times: 25
Early stopping!
Start to test process.
Loss after 18833948 batches: 0.0491
Time to train on one home:  52.75999069213867
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18834911 batches: 0.0876
trigger times: 1
Loss after 18835874 batches: 0.0738
trigger times: 2
Loss after 18836837 batches: 0.0713
trigger times: 3
Loss after 18837800 batches: 0.0670
trigger times: 4
Loss after 18838763 batches: 0.0635
trigger times: 5
Loss after 18839726 batches: 0.0585
trigger times: 6
Loss after 18840689 batches: 0.0571
trigger times: 7
Loss after 18841652 batches: 0.0546
trigger times: 8
Loss after 18842615 batches: 0.0528
trigger times: 9
Loss after 18843578 batches: 0.0496
trigger times: 10
Loss after 18844541 batches: 0.0499
trigger times: 11
Loss after 18845504 batches: 0.0476
trigger times: 12
Loss after 18846467 batches: 0.0487
trigger times: 13
Loss after 18847430 batches: 0.0474
trigger times: 14
Loss after 18848393 batches: 0.0501
trigger times: 15
Loss after 18849356 batches: 0.0485
trigger times: 16
Loss after 18850319 batches: 0.0473
trigger times: 17
Loss after 18851282 batches: 0.0457
trigger times: 18
Loss after 18852245 batches: 0.0457
trigger times: 19
Loss after 18853208 batches: 0.0451
trigger times: 20
Loss after 18854171 batches: 0.0445
trigger times: 21
Loss after 18855134 batches: 0.0439
trigger times: 22
Loss after 18856097 batches: 0.0448
trigger times: 23
Loss after 18857060 batches: 0.0439
trigger times: 24
Loss after 18858023 batches: 0.0427
trigger times: 25
Early stopping!
Start to test process.
Loss after 18858986 batches: 0.0436
Time to train on one home:  52.60204887390137
trigger times: 0
Loss after 18859949 batches: 0.0224
trigger times: 1
Loss after 18860912 batches: 0.0180
trigger times: 2
Loss after 18861875 batches: 0.0171
trigger times: 3
Loss after 18862838 batches: 0.0153
trigger times: 4
Loss after 18863801 batches: 0.0138
trigger times: 5
Loss after 18864764 batches: 0.0132
trigger times: 6
Loss after 18865727 batches: 0.0134
trigger times: 7
Loss after 18866690 batches: 0.0130
trigger times: 8
Loss after 18867653 batches: 0.0123
trigger times: 9
Loss after 18868616 batches: 0.0127
trigger times: 10
Loss after 18869579 batches: 0.0121
trigger times: 11
Loss after 18870542 batches: 0.0121
trigger times: 12
Loss after 18871505 batches: 0.0120
trigger times: 13
Loss after 18872468 batches: 0.0121
trigger times: 14
Loss after 18873431 batches: 0.0131
trigger times: 15
Loss after 18874394 batches: 0.0136
trigger times: 16
Loss after 18875357 batches: 0.0124
trigger times: 17
Loss after 18876320 batches: 0.0118
trigger times: 18
Loss after 18877283 batches: 0.0113
trigger times: 19
Loss after 18878246 batches: 0.0109
trigger times: 20
Loss after 18879209 batches: 0.0109
trigger times: 21
Loss after 18880172 batches: 0.0102
trigger times: 22
Loss after 18881135 batches: 0.0103
trigger times: 23
Loss after 18882098 batches: 0.0102
trigger times: 24
Loss after 18883061 batches: 0.0098
trigger times: 25
Early stopping!
Start to test process.
Loss after 18884024 batches: 0.0102
Time to train on one home:  52.80999159812927
trigger times: 0
Loss after 18884987 batches: 0.0392
trigger times: 1
Loss after 18885950 batches: 0.0313
trigger times: 2
Loss after 18886913 batches: 0.0292
trigger times: 3
Loss after 18887876 batches: 0.0279
trigger times: 4
Loss after 18888839 batches: 0.0250
trigger times: 5
Loss after 18889802 batches: 0.0240
trigger times: 6
Loss after 18890765 batches: 0.0228
trigger times: 7
Loss after 18891728 batches: 0.0217
trigger times: 8
Loss after 18892691 batches: 0.0205
trigger times: 9
Loss after 18893654 batches: 0.0207
trigger times: 10
Loss after 18894617 batches: 0.0208
trigger times: 11
Loss after 18895580 batches: 0.0208
trigger times: 12
Loss after 18896543 batches: 0.0203
trigger times: 13
Loss after 18897506 batches: 0.0197
trigger times: 14
Loss after 18898469 batches: 0.0185
trigger times: 15
Loss after 18899432 batches: 0.0190
trigger times: 16
Loss after 18900395 batches: 0.0192
trigger times: 17
Loss after 18901358 batches: 0.0191
trigger times: 18
Loss after 18902321 batches: 0.0188
trigger times: 19
Loss after 18903284 batches: 0.0186
trigger times: 20
Loss after 18904247 batches: 0.0182
trigger times: 21
Loss after 18905210 batches: 0.0176
trigger times: 22
Loss after 18906173 batches: 0.0179
trigger times: 23
Loss after 18907136 batches: 0.0180
trigger times: 24
Loss after 18908099 batches: 0.0192
trigger times: 25
Early stopping!
Start to test process.
Loss after 18909062 batches: 0.0203
Time to train on one home:  52.40927457809448
trigger times: 0
Loss after 18910025 batches: 0.0818
trigger times: 1
Loss after 18910988 batches: 0.0604
trigger times: 2
Loss after 18911951 batches: 0.0522
trigger times: 3
Loss after 18912914 batches: 0.0477
trigger times: 4
Loss after 18913877 batches: 0.0447
trigger times: 5
Loss after 18914840 batches: 0.0414
trigger times: 6
Loss after 18915803 batches: 0.0404
trigger times: 7
Loss after 18916766 batches: 0.0395
trigger times: 8
Loss after 18917729 batches: 0.0385
trigger times: 9
Loss after 18918692 batches: 0.0376
trigger times: 10
Loss after 18919655 batches: 0.0367
trigger times: 11
Loss after 18920618 batches: 0.0369
trigger times: 12
Loss after 18921581 batches: 0.0349
trigger times: 13
Loss after 18922544 batches: 0.0359
trigger times: 14
Loss after 18923507 batches: 0.0350
trigger times: 15
Loss after 18924470 batches: 0.0348
trigger times: 16
Loss after 18925433 batches: 0.0330
trigger times: 17
Loss after 18926396 batches: 0.0330
trigger times: 18
Loss after 18927359 batches: 0.0321
trigger times: 19
Loss after 18928322 batches: 0.0324
trigger times: 20
Loss after 18929285 batches: 0.0329
trigger times: 21
Loss after 18930248 batches: 0.0330
trigger times: 22
Loss after 18931211 batches: 0.0319
trigger times: 23
Loss after 18932174 batches: 0.0321
trigger times: 24
Loss after 18933137 batches: 0.0329
trigger times: 25
Early stopping!
Start to test process.
Loss after 18934100 batches: 0.0324
Time to train on one home:  52.47662973403931
trigger times: 0
Loss after 18935063 batches: 0.0909
trigger times: 1
Loss after 18936026 batches: 0.0580
trigger times: 2
Loss after 18936989 batches: 0.0529
trigger times: 3
Loss after 18937952 batches: 0.0469
trigger times: 4
Loss after 18938915 batches: 0.0453
trigger times: 5
Loss after 18939878 batches: 0.0425
trigger times: 6
Loss after 18940841 batches: 0.0420
trigger times: 7
Loss after 18941804 batches: 0.0400
trigger times: 8
Loss after 18942767 batches: 0.0385
trigger times: 9
Loss after 18943730 batches: 0.0376
trigger times: 10
Loss after 18944693 batches: 0.0372
trigger times: 11
Loss after 18945656 batches: 0.0367
trigger times: 12
Loss after 18946619 batches: 0.0363
trigger times: 13
Loss after 18947582 batches: 0.0362
trigger times: 14
Loss after 18948545 batches: 0.0355
trigger times: 15
Loss after 18949508 batches: 0.0356
trigger times: 16
Loss after 18950471 batches: 0.0348
trigger times: 17
Loss after 18951434 batches: 0.0348
trigger times: 18
Loss after 18952397 batches: 0.0354
trigger times: 19
Loss after 18953360 batches: 0.0343
trigger times: 20
Loss after 18954323 batches: 0.0363
trigger times: 21
Loss after 18955286 batches: 0.0345
trigger times: 22
Loss after 18956249 batches: 0.0340
trigger times: 23
Loss after 18957212 batches: 0.0340
trigger times: 24
Loss after 18958175 batches: 0.0336
trigger times: 25
Early stopping!
Start to test process.
Loss after 18959138 batches: 0.0339
Time to train on one home:  52.503517150878906
trigger times: 0
Loss after 18960101 batches: 0.0488
trigger times: 1
Loss after 18961064 batches: 0.0396
trigger times: 2
Loss after 18962027 batches: 0.0344
trigger times: 3
Loss after 18962990 batches: 0.0310
trigger times: 4
Loss after 18963953 batches: 0.0296
trigger times: 5
Loss after 18964916 batches: 0.0277
trigger times: 6
Loss after 18965879 batches: 0.0268
trigger times: 7
Loss after 18966842 batches: 0.0266
trigger times: 8
Loss after 18967805 batches: 0.0266
trigger times: 9
Loss after 18968768 batches: 0.0257
trigger times: 10
Loss after 18969731 batches: 0.0251
trigger times: 11
Loss after 18970694 batches: 0.0238
trigger times: 12
Loss after 18971657 batches: 0.0243
trigger times: 13
Loss after 18972620 batches: 0.0260
trigger times: 14
Loss after 18973583 batches: 0.0258
trigger times: 15
Loss after 18974546 batches: 0.0255
trigger times: 16
Loss after 18975509 batches: 0.0246
trigger times: 17
Loss after 18976472 batches: 0.0241
trigger times: 18
Loss after 18977435 batches: 0.0242
trigger times: 19
Loss after 18978398 batches: 0.0223
trigger times: 20
Loss after 18979361 batches: 0.0224
trigger times: 21
Loss after 18980324 batches: 0.0220
trigger times: 22
Loss after 18981287 batches: 0.0219
trigger times: 23
Loss after 18982250 batches: 0.0225
trigger times: 24
Loss after 18983213 batches: 0.0227
trigger times: 25
Early stopping!
Start to test process.
Loss after 18984176 batches: 0.0247
Time to train on one home:  52.72339177131653
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 18985139 batches: 0.0763
trigger times: 1
Loss after 18986102 batches: 0.0480
trigger times: 2
Loss after 18987065 batches: 0.0476
trigger times: 0
Loss after 18988028 batches: 0.0407
trigger times: 0
Loss after 18988991 batches: 0.0359
trigger times: 0
Loss after 18989954 batches: 0.0334
trigger times: 0
Loss after 18990917 batches: 0.0315
trigger times: 1
Loss after 18991880 batches: 0.0290
trigger times: 0
Loss after 18992843 batches: 0.0291
trigger times: 0
Loss after 18993806 batches: 0.0272
trigger times: 1
Loss after 18994769 batches: 0.0263
trigger times: 2
Loss after 18995732 batches: 0.0261
trigger times: 3
Loss after 18996695 batches: 0.0261
trigger times: 4
Loss after 18997658 batches: 0.0256
trigger times: 0
Loss after 18998621 batches: 0.0247
trigger times: 1
Loss after 18999584 batches: 0.0246
trigger times: 2
Loss after 19000547 batches: 0.0228
trigger times: 3
Loss after 19001510 batches: 0.0231
trigger times: 4
Loss after 19002473 batches: 0.0223
trigger times: 0
Loss after 19003436 batches: 0.0224
trigger times: 1
Loss after 19004399 batches: 0.0233
trigger times: 2
Loss after 19005362 batches: 0.0237
trigger times: 3
Loss after 19006325 batches: 0.0246
trigger times: 4
Loss after 19007288 batches: 0.0231
trigger times: 5
Loss after 19008251 batches: 0.0230
trigger times: 6
Loss after 19009214 batches: 0.0228
trigger times: 7
Loss after 19010177 batches: 0.0218
trigger times: 8
Loss after 19011140 batches: 0.0215
trigger times: 9
Loss after 19012103 batches: 0.0219
trigger times: 10
Loss after 19013066 batches: 0.0206
trigger times: 11
Loss after 19014029 batches: 0.0198
trigger times: 12
Loss after 19014992 batches: 0.0192
trigger times: 13
Loss after 19015955 batches: 0.0192
trigger times: 14
Loss after 19016918 batches: 0.0195
trigger times: 15
Loss after 19017881 batches: 0.0209
trigger times: 16
Loss after 19018844 batches: 0.0211
trigger times: 17
Loss after 19019807 batches: 0.0200
trigger times: 18
Loss after 19020770 batches: 0.0195
trigger times: 19
Loss after 19021733 batches: 0.0191
trigger times: 20
Loss after 19022696 batches: 0.0189
trigger times: 21
Loss after 19023659 batches: 0.0181
trigger times: 22
Loss after 19024622 batches: 0.0176
trigger times: 23
Loss after 19025585 batches: 0.0173
trigger times: 24
Loss after 19026548 batches: 0.0185
trigger times: 25
Early stopping!
Start to test process.
Loss after 19027511 batches: 0.0184
Time to train on one home:  67.53690671920776
trigger times: 0
Loss after 19028474 batches: 0.1056
trigger times: 1
Loss after 19029437 batches: 0.0797
trigger times: 2
Loss after 19030400 batches: 0.0753
trigger times: 3
Loss after 19031363 batches: 0.0736
trigger times: 4
Loss after 19032326 batches: 0.0701
trigger times: 5
Loss after 19033289 batches: 0.0691
trigger times: 6
Loss after 19034252 batches: 0.0669
trigger times: 7
Loss after 19035215 batches: 0.0649
trigger times: 8
Loss after 19036178 batches: 0.0640
trigger times: 9
Loss after 19037141 batches: 0.0625
trigger times: 10
Loss after 19038104 batches: 0.0623
trigger times: 11
Loss after 19039067 batches: 0.0602
trigger times: 12
Loss after 19040030 batches: 0.0586
trigger times: 13
Loss after 19040993 batches: 0.0588
trigger times: 14
Loss after 19041956 batches: 0.0574
trigger times: 15
Loss after 19042919 batches: 0.0577
trigger times: 16
Loss after 19043882 batches: 0.0585
trigger times: 17
Loss after 19044845 batches: 0.0570
trigger times: 18
Loss after 19045808 batches: 0.0562
trigger times: 19
Loss after 19046771 batches: 0.0553
trigger times: 20
Loss after 19047734 batches: 0.0555
trigger times: 21
Loss after 19048697 batches: 0.0549
trigger times: 22
Loss after 19049660 batches: 0.0550
trigger times: 23
Loss after 19050623 batches: 0.0542
trigger times: 24
Loss after 19051586 batches: 0.0552
trigger times: 25
Early stopping!
Start to test process.
Loss after 19052549 batches: 0.0520
Time to train on one home:  52.62828040122986
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19053512 batches: 0.0773
trigger times: 1
Loss after 19054475 batches: 0.0454
trigger times: 2
Loss after 19055438 batches: 0.0474
trigger times: 3
Loss after 19056401 batches: 0.0396
trigger times: 0
Loss after 19057364 batches: 0.0362
trigger times: 0
Loss after 19058327 batches: 0.0352
trigger times: 0
Loss after 19059290 batches: 0.0310
trigger times: 1
Loss after 19060253 batches: 0.0306
trigger times: 0
Loss after 19061216 batches: 0.0277
trigger times: 1
Loss after 19062179 batches: 0.0269
trigger times: 2
Loss after 19063142 batches: 0.0267
trigger times: 3
Loss after 19064105 batches: 0.0253
trigger times: 4
Loss after 19065068 batches: 0.0251
trigger times: 5
Loss after 19066031 batches: 0.0241
trigger times: 6
Loss after 19066994 batches: 0.0239
trigger times: 7
Loss after 19067957 batches: 0.0240
trigger times: 8
Loss after 19068920 batches: 0.0230
trigger times: 9
Loss after 19069883 batches: 0.0233
trigger times: 10
Loss after 19070846 batches: 0.0223
trigger times: 11
Loss after 19071809 batches: 0.0222
trigger times: 12
Loss after 19072772 batches: 0.0226
trigger times: 13
Loss after 19073735 batches: 0.0211
trigger times: 14
Loss after 19074698 batches: 0.0227
trigger times: 15
Loss after 19075661 batches: 0.0228
trigger times: 16
Loss after 19076624 batches: 0.0226
trigger times: 17
Loss after 19077587 batches: 0.0220
trigger times: 18
Loss after 19078550 batches: 0.0218
trigger times: 19
Loss after 19079513 batches: 0.0203
trigger times: 20
Loss after 19080476 batches: 0.0209
trigger times: 21
Loss after 19081439 batches: 0.0203
trigger times: 22
Loss after 19082402 batches: 0.0203
trigger times: 23
Loss after 19083365 batches: 0.0198
trigger times: 24
Loss after 19084328 batches: 0.0207
trigger times: 25
Early stopping!
Start to test process.
Loss after 19085291 batches: 0.0194
Time to train on one home:  59.0793399810791
trigger times: 0
Loss after 19086186 batches: 0.0688
trigger times: 1
Loss after 19087081 batches: 0.0402
trigger times: 2
Loss after 19087976 batches: 0.0182
trigger times: 3
Loss after 19088871 batches: 0.0098
trigger times: 4
Loss after 19089766 batches: 0.0073
trigger times: 5
Loss after 19090661 batches: 0.0056
trigger times: 6
Loss after 19091556 batches: 0.0048
trigger times: 7
Loss after 19092451 batches: 0.0044
trigger times: 8
Loss after 19093346 batches: 0.0042
trigger times: 9
Loss after 19094241 batches: 0.0036
trigger times: 10
Loss after 19095136 batches: 0.0031
trigger times: 11
Loss after 19096031 batches: 0.0027
trigger times: 12
Loss after 19096926 batches: 0.0024
trigger times: 13
Loss after 19097821 batches: 0.0027
trigger times: 14
Loss after 19098716 batches: 0.0029
trigger times: 15
Loss after 19099611 batches: 0.0026
trigger times: 16
Loss after 19100506 batches: 0.0029
trigger times: 17
Loss after 19101401 batches: 0.0030
trigger times: 18
Loss after 19102296 batches: 0.0025
trigger times: 19
Loss after 19103191 batches: 0.0027
trigger times: 20
Loss after 19104086 batches: 0.0020
trigger times: 21
Loss after 19104981 batches: 0.0020
trigger times: 22
Loss after 19105876 batches: 0.0021
trigger times: 23
Loss after 19106771 batches: 0.0022
trigger times: 24
Loss after 19107666 batches: 0.0021
trigger times: 25
Early stopping!
Start to test process.
Loss after 19108561 batches: 0.0020
Time to train on one home:  51.66748809814453
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19109498 batches: 0.0800
trigger times: 1
Loss after 19110435 batches: 0.0668
trigger times: 2
Loss after 19111372 batches: 0.0625
trigger times: 3
Loss after 19112309 batches: 0.0606
trigger times: 4
Loss after 19113246 batches: 0.0572
trigger times: 5
Loss after 19114183 batches: 0.0542
trigger times: 6
Loss after 19115120 batches: 0.0516
trigger times: 7
Loss after 19116057 batches: 0.0518
trigger times: 8
Loss after 19116994 batches: 0.0500
trigger times: 9
Loss after 19117931 batches: 0.0512
trigger times: 10
Loss after 19118868 batches: 0.0494
trigger times: 11
Loss after 19119805 batches: 0.0488
trigger times: 12
Loss after 19120742 batches: 0.0504
trigger times: 13
Loss after 19121679 batches: 0.0497
trigger times: 14
Loss after 19122616 batches: 0.0491
trigger times: 15
Loss after 19123553 batches: 0.0485
trigger times: 16
Loss after 19124490 batches: 0.0477
trigger times: 17
Loss after 19125427 batches: 0.0465
trigger times: 18
Loss after 19126364 batches: 0.0479
trigger times: 19
Loss after 19127301 batches: 0.0457
trigger times: 20
Loss after 19128238 batches: 0.0457
trigger times: 21
Loss after 19129175 batches: 0.0463
trigger times: 22
Loss after 19130112 batches: 0.0466
trigger times: 23
Loss after 19131049 batches: 0.0474
trigger times: 24
Loss after 19131986 batches: 0.0470
trigger times: 25
Early stopping!
Start to test process.
Loss after 19132923 batches: 0.0467
Time to train on one home:  52.76711416244507
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066, 0.03401813118363776, 0.033508403208926535, 0.03273922320832277, 0.032162070075287874, 0.031213877056718613, 0.03093800142524674, 0.030100551508447693, 0.029937661720089038, 0.02977997595640868, 0.02919253174856931]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863], [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861], [0.0757279247045517, 0.10266384027132536, 0.5909740425258612, 0.8803234656976516, 0.8076583271010261, 28.206427011804568, 8296.775], [0.06181953847408295, 0.10445801892372542, 0.5961459732150404, 0.831618823847048, 0.806043429565136, 26.645882531252276, 8280.187], [0.07333928346633911, 0.09887557787656209, 0.5904951093552152, 0.8805827319948272, 0.8110679704670578, 28.214734158180708, 8331.802], [0.06249981373548508, 0.1056225345966233, 0.6023961404422763, 0.8395811685763604, 0.8049953091236983, 26.90100386358231, 8269.419], [0.07111044973134995, 0.11364462231541128, 0.6015437535926366, 0.8360909977276793, 0.7977749276509928, 26.78917536742376, 8195.247], [0.06206701323390007, 0.09726051170449412, 0.5975866108942892, 0.8595424626232535, 0.8125216526577462, 27.540583297204083, 8346.734], [0.07088565826416016, 0.11302232012409985, 0.6007153971593981, 0.8537924311344753, 0.7983350331303676, 27.35634665030831, 8201.001], [0.06324874609708786, 0.09630308423125122, 0.5977169726517698, 0.8716301579538709, 0.8133833873815131, 27.927884907770363, 8355.587], [0.07241006940603256, 0.1018741345383698, 0.5940447527818379, 0.8890966213663465, 0.8083691035743891, 28.487527521645152, 8304.077]]
Round_17_results:  [0.07241006940603256, 0.1018741345383698, 0.5940447527818379, 0.8890966213663465, 0.8083691035743891, 28.487527521645152, 8304.077]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 19960 < 19961; dropping {'Training_Loss': 0.07133875467947551, 'Validation_Loss': 0.0803423598408699, 'Training_R2': -0.09637578407070135, 'Validation_R2': 0.12827165919255268, 'Training_F1': 0.42579942423489164, 'Validation_F1': 0.6113997217264643, 'Training_NEP': 0.8269012688371624, 'Validation_NEP': 0.8057384590727851, 'Training_NDE': 0.6677194378470376, 'Validation_NDE': 0.7875858299725254, 'Training_MAE': 31.42251426292775, 'Validation_MAE': 28.651474420446235, 'Training_MSE': 2466.146, 'Validation_MSE': 10317.384}.
trigger times: 0
Loss after 19133885 batches: 0.0713
trigger times: 1
Loss after 19134847 batches: 0.0636
trigger times: 2
Loss after 19135809 batches: 0.0603
trigger times: 3
Loss after 19136771 batches: 0.0574
trigger times: 4
Loss after 19137733 batches: 0.0553
trigger times: 5
Loss after 19138695 batches: 0.0525
trigger times: 6
Loss after 19139657 batches: 0.0517
trigger times: 7
Loss after 19140619 batches: 0.0507
trigger times: 8
Loss after 19141581 batches: 0.0503
trigger times: 9
Loss after 19142543 batches: 0.0499
trigger times: 10
Loss after 19143505 batches: 0.0485
trigger times: 11
Loss after 19144467 batches: 0.0471
trigger times: 12
Loss after 19145429 batches: 0.0477
trigger times: 13
Loss after 19146391 batches: 0.0475
trigger times: 14
Loss after 19147353 batches: 0.0460
trigger times: 15
Loss after 19148315 batches: 0.0478
trigger times: 16
Loss after 19149277 batches: 0.0460
trigger times: 17
Loss after 19150239 batches: 0.0465
trigger times: 18
Loss after 19151201 batches: 0.0460
trigger times: 19
Loss after 19152163 batches: 0.0467
trigger times: 20
Loss after 19153125 batches: 0.0470
trigger times: 21
Loss after 19154087 batches: 0.0470
trigger times: 22
Loss after 19155049 batches: 0.0457
trigger times: 23
Loss after 19156011 batches: 0.0470
trigger times: 24
Loss after 19156973 batches: 0.0458
trigger times: 25
Early stopping!
Start to test process.
Loss after 19157935 batches: 0.0449
Time to train on one home:  52.516218423843384
trigger times: 0
Loss after 19158864 batches: 0.1009
trigger times: 1
Loss after 19159793 batches: 0.0620
trigger times: 2
Loss after 19160722 batches: 0.0471
trigger times: 3
Loss after 19161651 batches: 0.0395
trigger times: 4
Loss after 19162580 batches: 0.0364
trigger times: 5
Loss after 19163509 batches: 0.0319
trigger times: 6
Loss after 19164438 batches: 0.0290
trigger times: 7
Loss after 19165367 batches: 0.0275
trigger times: 8
Loss after 19166296 batches: 0.0274
trigger times: 9
Loss after 19167225 batches: 0.0270
trigger times: 10
Loss after 19168154 batches: 0.0278
trigger times: 11
Loss after 19169083 batches: 0.0268
trigger times: 12
Loss after 19170012 batches: 0.0245
trigger times: 13
Loss after 19170941 batches: 0.0238
trigger times: 14
Loss after 19171870 batches: 0.0238
trigger times: 15
Loss after 19172799 batches: 0.0236
trigger times: 16
Loss after 19173728 batches: 0.0250
trigger times: 17
Loss after 19174657 batches: 0.0240
trigger times: 18
Loss after 19175586 batches: 0.0268
trigger times: 19
Loss after 19176515 batches: 0.0260
trigger times: 20
Loss after 19177444 batches: 0.0216
trigger times: 21
Loss after 19178373 batches: 0.0229
trigger times: 22
Loss after 19179302 batches: 0.0221
trigger times: 23
Loss after 19180231 batches: 0.0207
trigger times: 24
Loss after 19181160 batches: 0.0278
trigger times: 25
Early stopping!
Start to test process.
Loss after 19182089 batches: 0.0339
Time to train on one home:  52.36921429634094
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 0
Loss after 19183052 batches: 0.0300
trigger times: 1
Loss after 19184015 batches: 0.0172
trigger times: 2
Loss after 19184978 batches: 0.0146
trigger times: 3
Loss after 19185941 batches: 0.0139
trigger times: 4
Loss after 19186904 batches: 0.0137
trigger times: 5
Loss after 19187867 batches: 0.0137
trigger times: 6
Loss after 19188830 batches: 0.0131
trigger times: 7
Loss after 19189793 batches: 0.0126
trigger times: 8
Loss after 19190756 batches: 0.0111
trigger times: 9
Loss after 19191719 batches: 0.0102
trigger times: 10
Loss after 19192682 batches: 0.0093
trigger times: 11
Loss after 19193645 batches: 0.0089
trigger times: 12
Loss after 19194608 batches: 0.0082
trigger times: 13
Loss after 19195571 batches: 0.0080
trigger times: 14
Loss after 19196534 batches: 0.0080
trigger times: 15
Loss after 19197497 batches: 0.0076
trigger times: 16
Loss after 19198460 batches: 0.0073
trigger times: 17
Loss after 19199423 batches: 0.0069
trigger times: 18
Loss after 19200386 batches: 0.0070
trigger times: 19
Loss after 19201349 batches: 0.0067
trigger times: 20
Loss after 19202312 batches: 0.0066
trigger times: 21
Loss after 19203275 batches: 0.0067
trigger times: 22
Loss after 19204238 batches: 0.0066
trigger times: 23
Loss after 19205201 batches: 0.0063
trigger times: 24
Loss after 19206164 batches: 0.0064
trigger times: 25
Early stopping!
Start to test process.
Loss after 19207127 batches: 0.0061
Time to train on one home:  52.606141805648804
trigger times: 0
Loss after 19208090 batches: 0.0205
trigger times: 1
Loss after 19209053 batches: 0.0171
trigger times: 2
Loss after 19210016 batches: 0.0160
trigger times: 3
Loss after 19210979 batches: 0.0138
trigger times: 4
Loss after 19211942 batches: 0.0136
trigger times: 5
Loss after 19212905 batches: 0.0128
trigger times: 6
Loss after 19213868 batches: 0.0119
trigger times: 7
Loss after 19214831 batches: 0.0118
trigger times: 8
Loss after 19215794 batches: 0.0120
trigger times: 9
Loss after 19216757 batches: 0.0117
trigger times: 10
Loss after 19217720 batches: 0.0111
trigger times: 11
Loss after 19218683 batches: 0.0113
trigger times: 12
Loss after 19219646 batches: 0.0115
trigger times: 13
Loss after 19220609 batches: 0.0118
trigger times: 14
Loss after 19221572 batches: 0.0116
trigger times: 15
Loss after 19222535 batches: 0.0119
trigger times: 16
Loss after 19223498 batches: 0.0117
trigger times: 17
Loss after 19224461 batches: 0.0114
trigger times: 18
Loss after 19225424 batches: 0.0119
trigger times: 19
Loss after 19226387 batches: 0.0110
trigger times: 20
Loss after 19227350 batches: 0.0100
trigger times: 21
Loss after 19228313 batches: 0.0101
trigger times: 22
Loss after 19229276 batches: 0.0098
trigger times: 23
Loss after 19230239 batches: 0.0097
trigger times: 24
Loss after 19231202 batches: 0.0100
trigger times: 25
Early stopping!
Start to test process.
Loss after 19232165 batches: 0.0097
Time to train on one home:  52.798091411590576
trigger times: 0
Loss after 19233128 batches: 0.0955
trigger times: 1
Loss after 19234091 batches: 0.0844
trigger times: 2
Loss after 19235054 batches: 0.0779
trigger times: 3
Loss after 19236017 batches: 0.0748
trigger times: 4
Loss after 19236980 batches: 0.0707
trigger times: 5
Loss after 19237943 batches: 0.0675
trigger times: 6
Loss after 19238906 batches: 0.0640
trigger times: 7
Loss after 19239869 batches: 0.0635
trigger times: 8
Loss after 19240832 batches: 0.0618
trigger times: 9
Loss after 19241795 batches: 0.0587
trigger times: 10
Loss after 19242758 batches: 0.0587
trigger times: 11
Loss after 19243721 batches: 0.0571
trigger times: 12
Loss after 19244684 batches: 0.0570
trigger times: 13
Loss after 19245647 batches: 0.0566
trigger times: 14
Loss after 19246610 batches: 0.0558
trigger times: 15
Loss after 19247573 batches: 0.0561
trigger times: 16
Loss after 19248536 batches: 0.0544
trigger times: 17
Loss after 19249499 batches: 0.0550
trigger times: 18
Loss after 19250462 batches: 0.0530
trigger times: 19
Loss after 19251425 batches: 0.0552
trigger times: 20
Loss after 19252388 batches: 0.0535
trigger times: 21
Loss after 19253351 batches: 0.0528
trigger times: 22
Loss after 19254314 batches: 0.0539
trigger times: 23
Loss after 19255277 batches: 0.0522
trigger times: 24
Loss after 19256240 batches: 0.0508
trigger times: 25
Early stopping!
Start to test process.
Loss after 19257203 batches: 0.0507
Time to train on one home:  52.77406311035156
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19258166 batches: 0.0786
trigger times: 1
Loss after 19259129 batches: 0.0703
trigger times: 2
Loss after 19260092 batches: 0.0655
trigger times: 3
Loss after 19261055 batches: 0.0615
trigger times: 4
Loss after 19262018 batches: 0.0601
trigger times: 5
Loss after 19262981 batches: 0.0574
trigger times: 6
Loss after 19263944 batches: 0.0557
trigger times: 7
Loss after 19264907 batches: 0.0556
trigger times: 8
Loss after 19265870 batches: 0.0546
trigger times: 9
Loss after 19266833 batches: 0.0527
trigger times: 10
Loss after 19267796 batches: 0.0525
trigger times: 11
Loss after 19268759 batches: 0.0521
trigger times: 12
Loss after 19269722 batches: 0.0515
trigger times: 13
Loss after 19270685 batches: 0.0510
trigger times: 14
Loss after 19271648 batches: 0.0514
trigger times: 15
Loss after 19272611 batches: 0.0515
trigger times: 16
Loss after 19273574 batches: 0.0503
trigger times: 17
Loss after 19274537 batches: 0.0488
trigger times: 18
Loss after 19275500 batches: 0.0490
trigger times: 19
Loss after 19276463 batches: 0.0502
trigger times: 20
Loss after 19277426 batches: 0.0487
trigger times: 21
Loss after 19278389 batches: 0.0481
trigger times: 22
Loss after 19279352 batches: 0.0478
trigger times: 23
Loss after 19280315 batches: 0.0487
trigger times: 24
Loss after 19281278 batches: 0.0492
trigger times: 25
Early stopping!
Start to test process.
Loss after 19282241 batches: 0.0473
Time to train on one home:  52.86000323295593
trigger times: 0
Loss after 19283204 batches: 0.0732
trigger times: 1
Loss after 19284167 batches: 0.0673
trigger times: 2
Loss after 19285130 batches: 0.0635
trigger times: 3
Loss after 19286093 batches: 0.0611
trigger times: 4
Loss after 19287056 batches: 0.0580
trigger times: 5
Loss after 19288019 batches: 0.0572
trigger times: 6
Loss after 19288982 batches: 0.0551
trigger times: 7
Loss after 19289945 batches: 0.0549
trigger times: 8
Loss after 19290908 batches: 0.0546
trigger times: 9
Loss after 19291871 batches: 0.0542
trigger times: 10
Loss after 19292834 batches: 0.0518
trigger times: 11
Loss after 19293797 batches: 0.0507
trigger times: 12
Loss after 19294760 batches: 0.0509
trigger times: 13
Loss after 19295723 batches: 0.0519
trigger times: 14
Loss after 19296686 batches: 0.0507
trigger times: 15
Loss after 19297649 batches: 0.0491
trigger times: 16
Loss after 19298612 batches: 0.0481
trigger times: 17
Loss after 19299575 batches: 0.0499
trigger times: 18
Loss after 19300538 batches: 0.0478
trigger times: 19
Loss after 19301501 batches: 0.0467
trigger times: 20
Loss after 19302464 batches: 0.0470
trigger times: 21
Loss after 19303427 batches: 0.0469
trigger times: 22
Loss after 19304390 batches: 0.0470
trigger times: 23
Loss after 19305353 batches: 0.0467
trigger times: 24
Loss after 19306316 batches: 0.0484
trigger times: 25
Early stopping!
Start to test process.
Loss after 19307279 batches: 0.0463
Time to train on one home:  52.81502938270569
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19308242 batches: 0.0597
trigger times: 1
Loss after 19309205 batches: 0.0473
trigger times: 2
Loss after 19310168 batches: 0.0430
trigger times: 3
Loss after 19311131 batches: 0.0375
trigger times: 4
Loss after 19312094 batches: 0.0338
trigger times: 5
Loss after 19313057 batches: 0.0312
trigger times: 6
Loss after 19314020 batches: 0.0292
trigger times: 7
Loss after 19314983 batches: 0.0280
trigger times: 8
Loss after 19315946 batches: 0.0268
trigger times: 9
Loss after 19316909 batches: 0.0259
trigger times: 10
Loss after 19317872 batches: 0.0245
trigger times: 11
Loss after 19318835 batches: 0.0239
trigger times: 12
Loss after 19319798 batches: 0.0231
trigger times: 13
Loss after 19320761 batches: 0.0234
trigger times: 14
Loss after 19321724 batches: 0.0226
trigger times: 15
Loss after 19322687 batches: 0.0221
trigger times: 16
Loss after 19323650 batches: 0.0220
trigger times: 17
Loss after 19324613 batches: 0.0218
trigger times: 18
Loss after 19325576 batches: 0.0220
trigger times: 19
Loss after 19326539 batches: 0.0220
trigger times: 20
Loss after 19327502 batches: 0.0215
trigger times: 21
Loss after 19328465 batches: 0.0216
trigger times: 22
Loss after 19329428 batches: 0.0208
trigger times: 23
Loss after 19330391 batches: 0.0210
trigger times: 24
Loss after 19331354 batches: 0.0217
trigger times: 25
Early stopping!
Start to test process.
Loss after 19332317 batches: 0.0206
Time to train on one home:  52.974979877471924
trigger times: 0
Loss after 19333275 batches: 0.0692
trigger times: 1
Loss after 19334233 batches: 0.0424
trigger times: 2
Loss after 19335191 batches: 0.0353
trigger times: 3
Loss after 19336149 batches: 0.0301
trigger times: 4
Loss after 19337107 batches: 0.0297
trigger times: 5
Loss after 19338065 batches: 0.0270
trigger times: 6
Loss after 19339023 batches: 0.0243
trigger times: 7
Loss after 19339981 batches: 0.0215
trigger times: 8
Loss after 19340939 batches: 0.0210
trigger times: 9
Loss after 19341897 batches: 0.0189
trigger times: 10
Loss after 19342855 batches: 0.0180
trigger times: 11
Loss after 19343813 batches: 0.0175
trigger times: 12
Loss after 19344771 batches: 0.0167
trigger times: 13
Loss after 19345729 batches: 0.0163
trigger times: 14
Loss after 19346687 batches: 0.0166
trigger times: 15
Loss after 19347645 batches: 0.0163
trigger times: 16
Loss after 19348603 batches: 0.0161
trigger times: 17
Loss after 19349561 batches: 0.0157
trigger times: 18
Loss after 19350519 batches: 0.0154
trigger times: 19
Loss after 19351477 batches: 0.0160
trigger times: 20
Loss after 19352435 batches: 0.0153
trigger times: 21
Loss after 19353393 batches: 0.0148
trigger times: 22
Loss after 19354351 batches: 0.0149
trigger times: 23
Loss after 19355309 batches: 0.0145
trigger times: 24
Loss after 19356267 batches: 0.0142
trigger times: 25
Early stopping!
Start to test process.
Loss after 19357225 batches: 0.0144
Time to train on one home:  52.373162508010864
trigger times: 0
Loss after 19358187 batches: 0.0717
trigger times: 1
Loss after 19359149 batches: 0.0637
trigger times: 2
Loss after 19360111 batches: 0.0604
trigger times: 3
Loss after 19361073 batches: 0.0578
trigger times: 4
Loss after 19362035 batches: 0.0556
trigger times: 5
Loss after 19362997 batches: 0.0537
trigger times: 6
Loss after 19363959 batches: 0.0531
trigger times: 7
Loss after 19364921 batches: 0.0515
trigger times: 8
Loss after 19365883 batches: 0.0506
trigger times: 9
Loss after 19366845 batches: 0.0504
trigger times: 10
Loss after 19367807 batches: 0.0494
trigger times: 11
Loss after 19368769 batches: 0.0490
trigger times: 12
Loss after 19369731 batches: 0.0492
trigger times: 13
Loss after 19370693 batches: 0.0488
trigger times: 14
Loss after 19371655 batches: 0.0483
trigger times: 15
Loss after 19372617 batches: 0.0479
trigger times: 16
Loss after 19373579 batches: 0.0462
trigger times: 17
Loss after 19374541 batches: 0.0464
trigger times: 18
Loss after 19375503 batches: 0.0469
trigger times: 19
Loss after 19376465 batches: 0.0463
trigger times: 20
Loss after 19377427 batches: 0.0471
trigger times: 21
Loss after 19378389 batches: 0.0466
trigger times: 22
Loss after 19379351 batches: 0.0456
trigger times: 23
Loss after 19380313 batches: 0.0446
trigger times: 24
Loss after 19381275 batches: 0.0436
trigger times: 25
Early stopping!
Start to test process.
Loss after 19382237 batches: 0.0448
Time to train on one home:  52.749022245407104
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19383200 batches: 0.0677
trigger times: 1
Loss after 19384163 batches: 0.0224
trigger times: 2
Loss after 19385126 batches: 0.0175
trigger times: 3
Loss after 19386089 batches: 0.0156
trigger times: 4
Loss after 19387052 batches: 0.0146
trigger times: 5
Loss after 19388015 batches: 0.0139
trigger times: 6
Loss after 19388978 batches: 0.0136
trigger times: 7
Loss after 19389941 batches: 0.0134
trigger times: 8
Loss after 19390904 batches: 0.0131
trigger times: 9
Loss after 19391867 batches: 0.0128
trigger times: 10
Loss after 19392830 batches: 0.0126
trigger times: 11
Loss after 19393793 batches: 0.0125
trigger times: 12
Loss after 19394756 batches: 0.0124
trigger times: 13
Loss after 19395719 batches: 0.0122
trigger times: 14
Loss after 19396682 batches: 0.0122
trigger times: 15
Loss after 19397645 batches: 0.0123
trigger times: 16
Loss after 19398608 batches: 0.0120
trigger times: 17
Loss after 19399571 batches: 0.0122
trigger times: 18
Loss after 19400534 batches: 0.0121
trigger times: 19
Loss after 19401497 batches: 0.0118
trigger times: 20
Loss after 19402460 batches: 0.0119
trigger times: 21
Loss after 19403423 batches: 0.0117
trigger times: 22
Loss after 19404386 batches: 0.0118
trigger times: 23
Loss after 19405349 batches: 0.0115
trigger times: 24
Loss after 19406312 batches: 0.0114
trigger times: 25
Early stopping!
Start to test process.
Loss after 19407275 batches: 0.0115
Time to train on one home:  52.33479690551758
trigger times: 0
Loss after 19408238 batches: 0.0510
trigger times: 1
Loss after 19409201 batches: 0.0381
trigger times: 2
Loss after 19410164 batches: 0.0363
trigger times: 3
Loss after 19411127 batches: 0.0318
trigger times: 4
Loss after 19412090 batches: 0.0287
trigger times: 5
Loss after 19413053 batches: 0.0267
trigger times: 6
Loss after 19414016 batches: 0.0250
trigger times: 7
Loss after 19414979 batches: 0.0253
trigger times: 8
Loss after 19415942 batches: 0.0253
trigger times: 9
Loss after 19416905 batches: 0.0247
trigger times: 10
Loss after 19417868 batches: 0.0258
trigger times: 11
Loss after 19418831 batches: 0.0242
trigger times: 12
Loss after 19419794 batches: 0.0232
trigger times: 13
Loss after 19420757 batches: 0.0244
trigger times: 14
Loss after 19421720 batches: 0.0237
trigger times: 15
Loss after 19422683 batches: 0.0231
trigger times: 16
Loss after 19423646 batches: 0.0222
trigger times: 17
Loss after 19424609 batches: 0.0235
trigger times: 18
Loss after 19425572 batches: 0.0224
trigger times: 19
Loss after 19426535 batches: 0.0227
trigger times: 20
Loss after 19427498 batches: 0.0212
trigger times: 21
Loss after 19428461 batches: 0.0223
trigger times: 22
Loss after 19429424 batches: 0.0220
trigger times: 23
Loss after 19430387 batches: 0.0218
trigger times: 24
Loss after 19431350 batches: 0.0208
trigger times: 25
Early stopping!
Start to test process.
Loss after 19432313 batches: 0.0218
Time to train on one home:  52.65895080566406
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19433276 batches: 0.0600
trigger times: 1
Loss after 19434239 batches: 0.0474
trigger times: 2
Loss after 19435202 batches: 0.0441
trigger times: 3
Loss after 19436165 batches: 0.0368
trigger times: 4
Loss after 19437128 batches: 0.0332
trigger times: 5
Loss after 19438091 batches: 0.0313
trigger times: 6
Loss after 19439054 batches: 0.0295
trigger times: 7
Loss after 19440017 batches: 0.0277
trigger times: 8
Loss after 19440980 batches: 0.0262
trigger times: 9
Loss after 19441943 batches: 0.0256
trigger times: 10
Loss after 19442906 batches: 0.0247
trigger times: 11
Loss after 19443869 batches: 0.0238
trigger times: 12
Loss after 19444832 batches: 0.0238
trigger times: 13
Loss after 19445795 batches: 0.0227
trigger times: 14
Loss after 19446758 batches: 0.0233
trigger times: 15
Loss after 19447721 batches: 0.0230
trigger times: 16
Loss after 19448684 batches: 0.0226
trigger times: 17
Loss after 19449647 batches: 0.0222
trigger times: 18
Loss after 19450610 batches: 0.0231
trigger times: 19
Loss after 19451573 batches: 0.0219
trigger times: 20
Loss after 19452536 batches: 0.0212
trigger times: 21
Loss after 19453499 batches: 0.0218
trigger times: 22
Loss after 19454462 batches: 0.0204
trigger times: 23
Loss after 19455425 batches: 0.0204
trigger times: 24
Loss after 19456388 batches: 0.0205
trigger times: 25
Early stopping!
Start to test process.
Loss after 19457351 batches: 0.0195
Time to train on one home:  52.88236355781555
trigger times: 0
Loss after 19458314 batches: 0.0509
trigger times: 1
Loss after 19459277 batches: 0.0389
trigger times: 2
Loss after 19460240 batches: 0.0351
trigger times: 3
Loss after 19461203 batches: 0.0314
trigger times: 4
Loss after 19462166 batches: 0.0300
trigger times: 5
Loss after 19463129 batches: 0.0277
trigger times: 6
Loss after 19464092 batches: 0.0275
trigger times: 7
Loss after 19465055 batches: 0.0266
trigger times: 8
Loss after 19466018 batches: 0.0251
trigger times: 9
Loss after 19466981 batches: 0.0248
trigger times: 10
Loss after 19467944 batches: 0.0254
trigger times: 11
Loss after 19468907 batches: 0.0257
trigger times: 12
Loss after 19469870 batches: 0.0239
trigger times: 13
Loss after 19470833 batches: 0.0235
trigger times: 14
Loss after 19471796 batches: 0.0242
trigger times: 15
Loss after 19472759 batches: 0.0249
trigger times: 16
Loss after 19473722 batches: 0.0240
trigger times: 17
Loss after 19474685 batches: 0.0235
trigger times: 18
Loss after 19475648 batches: 0.0229
trigger times: 19
Loss after 19476611 batches: 0.0221
trigger times: 20
Loss after 19477574 batches: 0.0226
trigger times: 21
Loss after 19478537 batches: 0.0211
trigger times: 22
Loss after 19479500 batches: 0.0222
trigger times: 23
Loss after 19480463 batches: 0.0223
trigger times: 24
Loss after 19481426 batches: 0.0226
trigger times: 25
Early stopping!
Start to test process.
Loss after 19482389 batches: 0.0214
Time to train on one home:  52.239094495773315
trigger times: 0
Loss after 19483352 batches: 0.0483
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 19484315 batches: 0.0428
trigger times: 2
Loss after 19485278 batches: 0.0386
trigger times: 3
Loss after 19486241 batches: 0.0359
trigger times: 4
Loss after 19487204 batches: 0.0348
trigger times: 5
Loss after 19488167 batches: 0.0329
trigger times: 6
Loss after 19489130 batches: 0.0313
trigger times: 7
Loss after 19490093 batches: 0.0396
trigger times: 8
Loss after 19491056 batches: 0.0379
trigger times: 9
Loss after 19492019 batches: 0.0364
trigger times: 10
Loss after 19492982 batches: 0.0346
trigger times: 11
Loss after 19493945 batches: 0.0324
trigger times: 12
Loss after 19494908 batches: 0.0312
trigger times: 13
Loss after 19495871 batches: 0.0304
trigger times: 14
Loss after 19496834 batches: 0.0301
trigger times: 15
Loss after 19497797 batches: 0.0299
trigger times: 16
Loss after 19498760 batches: 0.0292
trigger times: 17
Loss after 19499723 batches: 0.0287
trigger times: 18
Loss after 19500686 batches: 0.0295
trigger times: 19
Loss after 19501649 batches: 0.0277
trigger times: 20
Loss after 19502612 batches: 0.0275
trigger times: 21
Loss after 19503575 batches: 0.0275
trigger times: 22
Loss after 19504538 batches: 0.0275
trigger times: 23
Loss after 19505501 batches: 0.0264
trigger times: 24
Loss after 19506464 batches: 0.0260
trigger times: 0
Loss after 19507427 batches: 0.0258
trigger times: 1
Loss after 19508390 batches: 0.0256
trigger times: 2
Loss after 19509353 batches: 0.0252
trigger times: 3
Loss after 19510316 batches: 0.0253
trigger times: 4
Loss after 19511279 batches: 0.0250
trigger times: 5
Loss after 19512242 batches: 0.0253
trigger times: 6
Loss after 19513205 batches: 0.0245
trigger times: 7
Loss after 19514168 batches: 0.0264
trigger times: 8
Loss after 19515131 batches: 0.0258
trigger times: 9
Loss after 19516094 batches: 0.0254
trigger times: 10
Loss after 19517057 batches: 0.0246
trigger times: 11
Loss after 19518020 batches: 0.0237
trigger times: 12
Loss after 19518983 batches: 0.0240
trigger times: 13
Loss after 19519946 batches: 0.0238
trigger times: 14
Loss after 19520909 batches: 0.0239
trigger times: 15
Loss after 19521872 batches: 0.0230
trigger times: 16
Loss after 19522835 batches: 0.0230
trigger times: 17
Loss after 19523798 batches: 0.0231
trigger times: 18
Loss after 19524761 batches: 0.0226
trigger times: 19
Loss after 19525724 batches: 0.0215
trigger times: 20
Loss after 19526687 batches: 0.0217
trigger times: 21
Loss after 19527650 batches: 0.0212
trigger times: 22
Loss after 19528613 batches: 0.0211
trigger times: 23
Loss after 19529576 batches: 0.0218
trigger times: 24
Loss after 19530539 batches: 0.0213
trigger times: 25
Early stopping!
Start to test process.
Loss after 19531502 batches: 0.0214
Time to train on one home:  71.61437392234802
trigger times: 0
Loss after 19532465 batches: 0.0735
trigger times: 1
Loss after 19533428 batches: 0.0461
trigger times: 2
Loss after 19534391 batches: 0.0457
trigger times: 3
Loss after 19535354 batches: 0.0430
trigger times: 4
Loss after 19536317 batches: 0.0415
trigger times: 5
Loss after 19537280 batches: 0.0393
trigger times: 6
Loss after 19538243 batches: 0.0383
trigger times: 7
Loss after 19539206 batches: 0.0377
trigger times: 8
Loss after 19540169 batches: 0.0369
trigger times: 9
Loss after 19541132 batches: 0.0360
trigger times: 10
Loss after 19542095 batches: 0.0347
trigger times: 11
Loss after 19543058 batches: 0.0354
trigger times: 12
Loss after 19544021 batches: 0.0349
trigger times: 13
Loss after 19544984 batches: 0.0350
trigger times: 14
Loss after 19545947 batches: 0.0343
trigger times: 15
Loss after 19546910 batches: 0.0344
trigger times: 16
Loss after 19547873 batches: 0.0334
trigger times: 17
Loss after 19548836 batches: 0.0335
trigger times: 18
Loss after 19549799 batches: 0.0334
trigger times: 19
Loss after 19550762 batches: 0.0328
trigger times: 20
Loss after 19551725 batches: 0.0329
trigger times: 21
Loss after 19552688 batches: 0.0323
trigger times: 22
Loss after 19553651 batches: 0.0325
trigger times: 23
Loss after 19554614 batches: 0.0317
trigger times: 24
Loss after 19555577 batches: 0.0321
trigger times: 25
Early stopping!
Start to test process.
Loss after 19556540 batches: 0.0316
Time to train on one home:  52.58626580238342
trigger times: 0
Loss after 19557503 batches: 0.0954
trigger times: 1
Loss after 19558466 batches: 0.0863
trigger times: 2
Loss after 19559429 batches: 0.0798
trigger times: 3
Loss after 19560392 batches: 0.0764
trigger times: 4
Loss after 19561355 batches: 0.0715
trigger times: 5
Loss after 19562318 batches: 0.0692
trigger times: 6
Loss after 19563281 batches: 0.0640
trigger times: 7
Loss after 19564244 batches: 0.0614
trigger times: 8
Loss after 19565207 batches: 0.0594
trigger times: 9
Loss after 19566170 batches: 0.0606
trigger times: 10
Loss after 19567133 batches: 0.0580
trigger times: 11
Loss after 19568096 batches: 0.0572
trigger times: 12
Loss after 19569059 batches: 0.0579
trigger times: 13
Loss after 19570022 batches: 0.0554
trigger times: 14
Loss after 19570985 batches: 0.0584
trigger times: 15
Loss after 19571948 batches: 0.0555
trigger times: 16
Loss after 19572911 batches: 0.0564
trigger times: 17
Loss after 19573874 batches: 0.0544
trigger times: 18
Loss after 19574837 batches: 0.0547
trigger times: 19
Loss after 19575800 batches: 0.0552
trigger times: 20
Loss after 19576763 batches: 0.0537
trigger times: 21
Loss after 19577726 batches: 0.0521
trigger times: 22
Loss after 19578689 batches: 0.0517
trigger times: 23
Loss after 19579652 batches: 0.0509
trigger times: 24
Loss after 19580615 batches: 0.0517
trigger times: 25
Early stopping!
Start to test process.
Loss after 19581578 batches: 0.0504
Time to train on one home:  52.32215476036072
trigger times: 0
Loss after 19582541 batches: 0.0953
trigger times: 1
Loss after 19583504 batches: 0.0594
trigger times: 2
Loss after 19584467 batches: 0.0526
trigger times: 3
Loss after 19585430 batches: 0.0480
trigger times: 4
Loss after 19586393 batches: 0.0457
trigger times: 5
Loss after 19587356 batches: 0.0425
trigger times: 6
Loss after 19588319 batches: 0.0407
trigger times: 7
Loss after 19589282 batches: 0.0396
trigger times: 8
Loss after 19590245 batches: 0.0385
trigger times: 9
Loss after 19591208 batches: 0.0375
trigger times: 10
Loss after 19592171 batches: 0.0360
trigger times: 11
Loss after 19593134 batches: 0.0361
trigger times: 12
Loss after 19594097 batches: 0.0361
trigger times: 13
Loss after 19595060 batches: 0.0356
trigger times: 14
Loss after 19596023 batches: 0.0347
trigger times: 15
Loss after 19596986 batches: 0.0353
trigger times: 16
Loss after 19597949 batches: 0.0342
trigger times: 17
Loss after 19598912 batches: 0.0350
trigger times: 18
Loss after 19599875 batches: 0.0363
trigger times: 19
Loss after 19600838 batches: 0.0345
trigger times: 20
Loss after 19601801 batches: 0.0332
trigger times: 21
Loss after 19602764 batches: 0.0340
trigger times: 22
Loss after 19603727 batches: 0.0327
trigger times: 23
Loss after 19604690 batches: 0.0333
trigger times: 24
Loss after 19605653 batches: 0.0333
trigger times: 25
Early stopping!
Start to test process.
Loss after 19606616 batches: 0.0318
Time to train on one home:  52.080015420913696
trigger times: 0
Loss after 19607545 batches: 0.1006
trigger times: 1
Loss after 19608474 batches: 0.0624
trigger times: 2
Loss after 19609403 batches: 0.0468
trigger times: 3
Loss after 19610332 batches: 0.0375
trigger times: 4
Loss after 19611261 batches: 0.0360
trigger times: 5
Loss after 19612190 batches: 0.0325
trigger times: 6
Loss after 19613119 batches: 0.0302
trigger times: 7
Loss after 19614048 batches: 0.0282
trigger times: 8
Loss after 19614977 batches: 0.0267
trigger times: 9
Loss after 19615906 batches: 0.0289
trigger times: 10
Loss after 19616835 batches: 0.0264
trigger times: 11
Loss after 19617764 batches: 0.0365
trigger times: 12
Loss after 19618693 batches: 0.0338
trigger times: 13
Loss after 19619622 batches: 0.0315
trigger times: 14
Loss after 19620551 batches: 0.0315
trigger times: 15
Loss after 19621480 batches: 0.0307
trigger times: 16
Loss after 19622409 batches: 0.0275
trigger times: 17
Loss after 19623338 batches: 0.0252
trigger times: 18
Loss after 19624267 batches: 0.0321
trigger times: 19
Loss after 19625196 batches: 0.0329
trigger times: 20
Loss after 19626125 batches: 0.0282
trigger times: 21
Loss after 19627054 batches: 0.0289
trigger times: 22
Loss after 19627983 batches: 0.0289
trigger times: 23
Loss after 19628912 batches: 0.0263
trigger times: 24
Loss after 19629841 batches: 0.0246
trigger times: 25
Early stopping!
Start to test process.
Loss after 19630770 batches: 0.0247
Time to train on one home:  52.116889238357544
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19631733 batches: 0.0629
trigger times: 1
Loss after 19632696 batches: 0.0303
trigger times: 2
Loss after 19633659 batches: 0.0275
trigger times: 3
Loss after 19634622 batches: 0.0269
trigger times: 4
Loss after 19635585 batches: 0.0257
trigger times: 5
Loss after 19636548 batches: 0.0247
trigger times: 6
Loss after 19637511 batches: 0.0234
trigger times: 7
Loss after 19638474 batches: 0.0221
trigger times: 8
Loss after 19639437 batches: 0.0213
trigger times: 9
Loss after 19640400 batches: 0.0204
trigger times: 10
Loss after 19641363 batches: 0.0197
trigger times: 11
Loss after 19642326 batches: 0.0191
trigger times: 12
Loss after 19643289 batches: 0.0185
trigger times: 13
Loss after 19644252 batches: 0.0183
trigger times: 14
Loss after 19645215 batches: 0.0179
trigger times: 15
Loss after 19646178 batches: 0.0178
trigger times: 16
Loss after 19647141 batches: 0.0177
trigger times: 17
Loss after 19648104 batches: 0.0173
trigger times: 18
Loss after 19649067 batches: 0.0168
trigger times: 19
Loss after 19650030 batches: 0.0169
trigger times: 20
Loss after 19650993 batches: 0.0167
trigger times: 21
Loss after 19651956 batches: 0.0167
trigger times: 22
Loss after 19652919 batches: 0.0166
trigger times: 23
Loss after 19653882 batches: 0.0164
trigger times: 24
Loss after 19654845 batches: 0.0162
trigger times: 25
Early stopping!
Start to test process.
Loss after 19655808 batches: 0.0162
Time to train on one home:  52.88952422142029
trigger times: 0
Loss after 19656771 batches: 0.1682
trigger times: 1
Loss after 19657734 batches: 0.1171
trigger times: 2
Loss after 19658697 batches: 0.0941
trigger times: 3
Loss after 19659660 batches: 0.0843
trigger times: 4
Loss after 19660623 batches: 0.0752
trigger times: 5
Loss after 19661586 batches: 0.0685
trigger times: 6
Loss after 19662549 batches: 0.0659
trigger times: 7
Loss after 19663512 batches: 0.0616
trigger times: 8
Loss after 19664475 batches: 0.0586
trigger times: 9
Loss after 19665438 batches: 0.0547
trigger times: 10
Loss after 19666401 batches: 0.0526
trigger times: 11
Loss after 19667364 batches: 0.0506
trigger times: 12
Loss after 19668327 batches: 0.0486
trigger times: 13
Loss after 19669290 batches: 0.0482
trigger times: 14
Loss after 19670253 batches: 0.0449
trigger times: 15
Loss after 19671216 batches: 0.0448
trigger times: 16
Loss after 19672179 batches: 0.0443
trigger times: 17
Loss after 19673142 batches: 0.0440
trigger times: 18
Loss after 19674105 batches: 0.0435
trigger times: 19
Loss after 19675068 batches: 0.0425
trigger times: 20
Loss after 19676031 batches: 0.0406
trigger times: 21
Loss after 19676994 batches: 0.0411
trigger times: 22
Loss after 19677957 batches: 0.0402
trigger times: 23
Loss after 19678920 batches: 0.0396
trigger times: 24
Loss after 19679883 batches: 0.0397
trigger times: 25
Early stopping!
Start to test process.
Loss after 19680846 batches: 0.0399
Time to train on one home:  52.50953555107117
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19681809 batches: 0.0797
trigger times: 1
Loss after 19682772 batches: 0.0692
trigger times: 2
Loss after 19683735 batches: 0.0646
trigger times: 3
Loss after 19684698 batches: 0.0626
trigger times: 4
Loss after 19685661 batches: 0.0594
trigger times: 5
Loss after 19686624 batches: 0.0581
trigger times: 6
Loss after 19687587 batches: 0.0554
trigger times: 7
Loss after 19688550 batches: 0.0558
trigger times: 8
Loss after 19689513 batches: 0.0539
trigger times: 9
Loss after 19690476 batches: 0.0543
trigger times: 10
Loss after 19691439 batches: 0.0529
trigger times: 11
Loss after 19692402 batches: 0.0524
trigger times: 12
Loss after 19693365 batches: 0.0528
trigger times: 13
Loss after 19694328 batches: 0.0526
trigger times: 14
Loss after 19695291 batches: 0.0537
trigger times: 15
Loss after 19696254 batches: 0.0529
trigger times: 16
Loss after 19697217 batches: 0.0508
trigger times: 17
Loss after 19698180 batches: 0.0516
trigger times: 18
Loss after 19699143 batches: 0.0509
trigger times: 19
Loss after 19700106 batches: 0.0505
trigger times: 20
Loss after 19701069 batches: 0.0493
trigger times: 21
Loss after 19702032 batches: 0.0490
trigger times: 22
Loss after 19702995 batches: 0.0485
trigger times: 23
Loss after 19703958 batches: 0.0498
trigger times: 24
Loss after 19704921 batches: 0.0486
trigger times: 25
Early stopping!
Start to test process.
Loss after 19705884 batches: 0.0485
Time to train on one home:  52.635424852371216
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19706847 batches: 0.0824
trigger times: 1
Loss after 19707810 batches: 0.0738
trigger times: 2
Loss after 19708773 batches: 0.0704
trigger times: 3
Loss after 19709736 batches: 0.0657
trigger times: 4
Loss after 19710699 batches: 0.0613
trigger times: 5
Loss after 19711662 batches: 0.0579
trigger times: 6
Loss after 19712625 batches: 0.0551
trigger times: 7
Loss after 19713588 batches: 0.0526
trigger times: 8
Loss after 19714551 batches: 0.0506
trigger times: 9
Loss after 19715514 batches: 0.0493
trigger times: 10
Loss after 19716477 batches: 0.0491
trigger times: 11
Loss after 19717440 batches: 0.0489
trigger times: 12
Loss after 19718403 batches: 0.0463
trigger times: 13
Loss after 19719366 batches: 0.0470
trigger times: 14
Loss after 19720329 batches: 0.0466
trigger times: 15
Loss after 19721292 batches: 0.0458
trigger times: 16
Loss after 19722255 batches: 0.0454
trigger times: 17
Loss after 19723218 batches: 0.0450
trigger times: 18
Loss after 19724181 batches: 0.0435
trigger times: 19
Loss after 19725144 batches: 0.0440
trigger times: 20
Loss after 19726107 batches: 0.0440
trigger times: 21
Loss after 19727070 batches: 0.0438
trigger times: 22
Loss after 19728033 batches: 0.0443
trigger times: 23
Loss after 19728996 batches: 0.0460
trigger times: 24
Loss after 19729959 batches: 0.0439
trigger times: 25
Early stopping!
Start to test process.
Loss after 19730922 batches: 0.0448
Time to train on one home:  52.76131510734558
trigger times: 0
Loss after 19731885 batches: 0.0205
trigger times: 1
Loss after 19732848 batches: 0.0172
trigger times: 2
Loss after 19733811 batches: 0.0158
trigger times: 3
Loss after 19734774 batches: 0.0147
trigger times: 4
Loss after 19735737 batches: 0.0134
trigger times: 5
Loss after 19736700 batches: 0.0129
trigger times: 6
Loss after 19737663 batches: 0.0121
trigger times: 7
Loss after 19738626 batches: 0.0121
trigger times: 8
Loss after 19739589 batches: 0.0122
trigger times: 9
Loss after 19740552 batches: 0.0120
trigger times: 10
Loss after 19741515 batches: 0.0119
trigger times: 11
Loss after 19742478 batches: 0.0114
trigger times: 12
Loss after 19743441 batches: 0.0114
trigger times: 13
Loss after 19744404 batches: 0.0114
trigger times: 14
Loss after 19745367 batches: 0.0110
trigger times: 15
Loss after 19746330 batches: 0.0107
trigger times: 16
Loss after 19747293 batches: 0.0109
trigger times: 17
Loss after 19748256 batches: 0.0111
trigger times: 18
Loss after 19749219 batches: 0.0101
trigger times: 19
Loss after 19750182 batches: 0.0100
trigger times: 20
Loss after 19751145 batches: 0.0099
trigger times: 21
Loss after 19752108 batches: 0.0103
trigger times: 22
Loss after 19753071 batches: 0.0102
trigger times: 23
Loss after 19754034 batches: 0.0099
trigger times: 24
Loss after 19754997 batches: 0.0102
trigger times: 25
Early stopping!
Start to test process.
Loss after 19755960 batches: 0.0098
Time to train on one home:  53.29310083389282
trigger times: 0
Loss after 19756923 batches: 0.0399
trigger times: 1
Loss after 19757886 batches: 0.0318
trigger times: 2
Loss after 19758849 batches: 0.0288
trigger times: 3
Loss after 19759812 batches: 0.0268
trigger times: 4
Loss after 19760775 batches: 0.0246
trigger times: 5
Loss after 19761738 batches: 0.0234
trigger times: 6
Loss after 19762701 batches: 0.0257
trigger times: 7
Loss after 19763664 batches: 0.0236
trigger times: 8
Loss after 19764627 batches: 0.0228
trigger times: 9
Loss after 19765590 batches: 0.0221
trigger times: 10
Loss after 19766553 batches: 0.0208
trigger times: 11
Loss after 19767516 batches: 0.0206
trigger times: 12
Loss after 19768479 batches: 0.0200
trigger times: 13
Loss after 19769442 batches: 0.0199
trigger times: 14
Loss after 19770405 batches: 0.0201
trigger times: 15
Loss after 19771368 batches: 0.0207
trigger times: 16
Loss after 19772331 batches: 0.0198
trigger times: 17
Loss after 19773294 batches: 0.0190
trigger times: 18
Loss after 19774257 batches: 0.0196
trigger times: 19
Loss after 19775220 batches: 0.0185
trigger times: 20
Loss after 19776183 batches: 0.0177
trigger times: 21
Loss after 19777146 batches: 0.0178
trigger times: 22
Loss after 19778109 batches: 0.0181
trigger times: 23
Loss after 19779072 batches: 0.0175
trigger times: 24
Loss after 19780035 batches: 0.0177
trigger times: 25
Early stopping!
Start to test process.
Loss after 19780998 batches: 0.0174
Time to train on one home:  52.57128071784973
trigger times: 0
Loss after 19781961 batches: 0.0834
trigger times: 1
Loss after 19782924 batches: 0.0608
trigger times: 2
Loss after 19783887 batches: 0.0516
trigger times: 3
Loss after 19784850 batches: 0.0456
trigger times: 4
Loss after 19785813 batches: 0.0434
trigger times: 5
Loss after 19786776 batches: 0.0418
trigger times: 6
Loss after 19787739 batches: 0.0403
trigger times: 7
Loss after 19788702 batches: 0.0399
trigger times: 8
Loss after 19789665 batches: 0.0387
trigger times: 9
Loss after 19790628 batches: 0.0371
trigger times: 10
Loss after 19791591 batches: 0.0357
trigger times: 11
Loss after 19792554 batches: 0.0357
trigger times: 12
Loss after 19793517 batches: 0.0336
trigger times: 13
Loss after 19794480 batches: 0.0330
trigger times: 14
Loss after 19795443 batches: 0.0333
trigger times: 15
Loss after 19796406 batches: 0.0340
trigger times: 16
Loss after 19797369 batches: 0.0329
trigger times: 17
Loss after 19798332 batches: 0.0331
trigger times: 18
Loss after 19799295 batches: 0.0329
trigger times: 19
Loss after 19800258 batches: 0.0323
trigger times: 20
Loss after 19801221 batches: 0.0318
trigger times: 21
Loss after 19802184 batches: 0.0314
trigger times: 22
Loss after 19803147 batches: 0.0304
trigger times: 23
Loss after 19804110 batches: 0.0296
trigger times: 24
Loss after 19805073 batches: 0.0296
trigger times: 25
Early stopping!
Start to test process.
Loss after 19806036 batches: 0.0311
Time to train on one home:  52.313323736190796
trigger times: 0
Loss after 19806999 batches: 0.0949
trigger times: 1
Loss after 19807962 batches: 0.0600
trigger times: 2
Loss after 19808925 batches: 0.0534
trigger times: 3
Loss after 19809888 batches: 0.0480
trigger times: 4
Loss after 19810851 batches: 0.0453
trigger times: 5
Loss after 19811814 batches: 0.0423
trigger times: 6
Loss after 19812777 batches: 0.0407
trigger times: 7
Loss after 19813740 batches: 0.0398
trigger times: 8
Loss after 19814703 batches: 0.0383
trigger times: 9
Loss after 19815666 batches: 0.0370
trigger times: 10
Loss after 19816629 batches: 0.0377
trigger times: 11
Loss after 19817592 batches: 0.0361
trigger times: 12
Loss after 19818555 batches: 0.0365
trigger times: 13
Loss after 19819518 batches: 0.0355
trigger times: 14
Loss after 19820481 batches: 0.0348
trigger times: 15
Loss after 19821444 batches: 0.0351
trigger times: 16
Loss after 19822407 batches: 0.0336
trigger times: 17
Loss after 19823370 batches: 0.0339
trigger times: 18
Loss after 19824333 batches: 0.0336
trigger times: 19
Loss after 19825296 batches: 0.0332
trigger times: 20
Loss after 19826259 batches: 0.0338
trigger times: 21
Loss after 19827222 batches: 0.0332
trigger times: 22
Loss after 19828185 batches: 0.0331
trigger times: 23
Loss after 19829148 batches: 0.0325
trigger times: 24
Loss after 19830111 batches: 0.0325
trigger times: 25
Early stopping!
Start to test process.
Loss after 19831074 batches: 0.0335
Time to train on one home:  52.41726803779602
trigger times: 0
Loss after 19832037 batches: 0.0510
trigger times: 1
Loss after 19833000 batches: 0.0376
trigger times: 2
Loss after 19833963 batches: 0.0358
trigger times: 3
Loss after 19834926 batches: 0.0320
trigger times: 4
Loss after 19835889 batches: 0.0289
trigger times: 5
Loss after 19836852 batches: 0.0275
trigger times: 6
Loss after 19837815 batches: 0.0271
trigger times: 7
Loss after 19838778 batches: 0.0249
trigger times: 8
Loss after 19839741 batches: 0.0245
trigger times: 9
Loss after 19840704 batches: 0.0240
trigger times: 10
Loss after 19841667 batches: 0.0249
trigger times: 11
Loss after 19842630 batches: 0.0244
trigger times: 12
Loss after 19843593 batches: 0.0239
trigger times: 13
Loss after 19844556 batches: 0.0246
trigger times: 14
Loss after 19845519 batches: 0.0247
trigger times: 15
Loss after 19846482 batches: 0.0257
trigger times: 16
Loss after 19847445 batches: 0.0252
trigger times: 17
Loss after 19848408 batches: 0.0229
trigger times: 18
Loss after 19849371 batches: 0.0218
trigger times: 19
Loss after 19850334 batches: 0.0234
trigger times: 20
Loss after 19851297 batches: 0.0235
trigger times: 21
Loss after 19852260 batches: 0.0237
trigger times: 22
Loss after 19853223 batches: 0.0220
trigger times: 23
Loss after 19854186 batches: 0.0223
trigger times: 24
Loss after 19855149 batches: 0.0214
trigger times: 25
Early stopping!
Start to test process.
Loss after 19856112 batches: 0.0256
Time to train on one home:  52.65716624259949
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19857075 batches: 0.0823
trigger times: 1
Loss after 19858038 batches: 0.0466
trigger times: 2
Loss after 19859001 batches: 0.0486
trigger times: 3
Loss after 19859964 batches: 0.0392
trigger times: 0
Loss after 19860927 batches: 0.0352
trigger times: 0
Loss after 19861890 batches: 0.0319
trigger times: 0
Loss after 19862853 batches: 0.0310
trigger times: 0
Loss after 19863816 batches: 0.0284
trigger times: 1
Loss after 19864779 batches: 0.0282
trigger times: 2
Loss after 19865742 batches: 0.0260
trigger times: 3
Loss after 19866705 batches: 0.0244
trigger times: 4
Loss after 19867668 batches: 0.0253
trigger times: 5
Loss after 19868631 batches: 0.0250
trigger times: 0
Loss after 19869594 batches: 0.0238
trigger times: 1
Loss after 19870557 batches: 0.0231
trigger times: 2
Loss after 19871520 batches: 0.0219
trigger times: 3
Loss after 19872483 batches: 0.0226
trigger times: 4
Loss after 19873446 batches: 0.0238
trigger times: 5
Loss after 19874409 batches: 0.0232
trigger times: 6
Loss after 19875372 batches: 0.0220
trigger times: 7
Loss after 19876335 batches: 0.0216
trigger times: 8
Loss after 19877298 batches: 0.0216
trigger times: 9
Loss after 19878261 batches: 0.0203
trigger times: 10
Loss after 19879224 batches: 0.0205
trigger times: 11
Loss after 19880187 batches: 0.0204
trigger times: 12
Loss after 19881150 batches: 0.0208
trigger times: 13
Loss after 19882113 batches: 0.0214
trigger times: 14
Loss after 19883076 batches: 0.0205
trigger times: 15
Loss after 19884039 batches: 0.0196
trigger times: 16
Loss after 19885002 batches: 0.0207
trigger times: 17
Loss after 19885965 batches: 0.0212
trigger times: 18
Loss after 19886928 batches: 0.0198
trigger times: 19
Loss after 19887891 batches: 0.0223
trigger times: 20
Loss after 19888854 batches: 0.0258
trigger times: 21
Loss after 19889817 batches: 0.0222
trigger times: 22
Loss after 19890780 batches: 0.0214
trigger times: 23
Loss after 19891743 batches: 0.0217
trigger times: 24
Loss after 19892706 batches: 0.0203
trigger times: 25
Early stopping!
Start to test process.
Loss after 19893669 batches: 0.0198
Time to train on one home:  62.683162689208984
trigger times: 0
Loss after 19894632 batches: 0.0960
trigger times: 1
Loss after 19895595 batches: 0.0784
trigger times: 2
Loss after 19896558 batches: 0.0745
trigger times: 3
Loss after 19897521 batches: 0.0725
trigger times: 4
Loss after 19898484 batches: 0.0696
trigger times: 5
Loss after 19899447 batches: 0.0671
trigger times: 6
Loss after 19900410 batches: 0.0652
trigger times: 7
Loss after 19901373 batches: 0.0623
trigger times: 8
Loss after 19902336 batches: 0.0604
trigger times: 9
Loss after 19903299 batches: 0.0597
trigger times: 10
Loss after 19904262 batches: 0.0591
trigger times: 11
Loss after 19905225 batches: 0.0593
trigger times: 12
Loss after 19906188 batches: 0.0579
trigger times: 13
Loss after 19907151 batches: 0.0569
trigger times: 14
Loss after 19908114 batches: 0.0571
trigger times: 15
Loss after 19909077 batches: 0.0554
trigger times: 16
Loss after 19910040 batches: 0.0563
trigger times: 17
Loss after 19911003 batches: 0.0540
trigger times: 18
Loss after 19911966 batches: 0.0552
trigger times: 19
Loss after 19912929 batches: 0.0536
trigger times: 20
Loss after 19913892 batches: 0.0537
trigger times: 21
Loss after 19914855 batches: 0.0536
trigger times: 22
Loss after 19915818 batches: 0.0518
trigger times: 23
Loss after 19916781 batches: 0.0519
trigger times: 24
Loss after 19917744 batches: 0.0510
trigger times: 25
Early stopping!
Start to test process.
Loss after 19918707 batches: 0.0515
Time to train on one home:  53.08937215805054
trigger times: 0
Loss after 19919670 batches: 0.0832
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 19920633 batches: 0.0468
trigger times: 2
Loss after 19921596 batches: 0.0461
trigger times: 3
Loss after 19922559 batches: 0.0382
trigger times: 0
Loss after 19923522 batches: 0.0348
trigger times: 1
Loss after 19924485 batches: 0.0328
trigger times: 2
Loss after 19925448 batches: 0.0311
trigger times: 3
Loss after 19926411 batches: 0.0293
trigger times: 4
Loss after 19927374 batches: 0.0283
trigger times: 5
Loss after 19928337 batches: 0.0276
trigger times: 6
Loss after 19929300 batches: 0.0259
trigger times: 7
Loss after 19930263 batches: 0.0255
trigger times: 8
Loss after 19931226 batches: 0.0249
trigger times: 9
Loss after 19932189 batches: 0.0238
trigger times: 10
Loss after 19933152 batches: 0.0238
trigger times: 11
Loss after 19934115 batches: 0.0229
trigger times: 12
Loss after 19935078 batches: 0.0230
trigger times: 13
Loss after 19936041 batches: 0.0220
trigger times: 14
Loss after 19937004 batches: 0.0224
trigger times: 15
Loss after 19937967 batches: 0.0240
trigger times: 0
Loss after 19938930 batches: 0.0231
trigger times: 0
Loss after 19939893 batches: 0.0230
trigger times: 1
Loss after 19940856 batches: 0.0240
trigger times: 2
Loss after 19941819 batches: 0.0229
trigger times: 3
Loss after 19942782 batches: 0.0212
trigger times: 4
Loss after 19943745 batches: 0.0209
trigger times: 5
Loss after 19944708 batches: 0.0201
trigger times: 6
Loss after 19945671 batches: 0.0205
trigger times: 7
Loss after 19946634 batches: 0.0201
trigger times: 8
Loss after 19947597 batches: 0.0199
trigger times: 9
Loss after 19948560 batches: 0.0193
trigger times: 10
Loss after 19949523 batches: 0.0195
trigger times: 11
Loss after 19950486 batches: 0.0196
trigger times: 12
Loss after 19951449 batches: 0.0194
trigger times: 13
Loss after 19952412 batches: 0.0187
trigger times: 14
Loss after 19953375 batches: 0.0190
trigger times: 15
Loss after 19954338 batches: 0.0184
trigger times: 16
Loss after 19955301 batches: 0.0188
trigger times: 17
Loss after 19956264 batches: 0.0190
trigger times: 18
Loss after 19957227 batches: 0.0182
trigger times: 19
Loss after 19958190 batches: 0.0181
trigger times: 20
Loss after 19959153 batches: 0.0178
trigger times: 21
Loss after 19960116 batches: 0.0182
trigger times: 22
Loss after 19961079 batches: 0.0177
trigger times: 23
Loss after 19962042 batches: 0.0183
trigger times: 24
Loss after 19963005 batches: 0.0185
trigger times: 25
Early stopping!
Start to test process.
Loss after 19963968 batches: 0.0178
Time to train on one home:  68.36376881599426
trigger times: 0
Loss after 19964863 batches: 0.0714
trigger times: 1
Loss after 19965758 batches: 0.0416
trigger times: 2
Loss after 19966653 batches: 0.0170
trigger times: 3
Loss after 19967548 batches: 0.0095
trigger times: 4
Loss after 19968443 batches: 0.0078
trigger times: 5
Loss after 19969338 batches: 0.0057
trigger times: 6
Loss after 19970233 batches: 0.0050
trigger times: 7
Loss after 19971128 batches: 0.0045
trigger times: 8
Loss after 19972023 batches: 0.0040
trigger times: 9
Loss after 19972918 batches: 0.0031
trigger times: 10
Loss after 19973813 batches: 0.0030
trigger times: 11
Loss after 19974708 batches: 0.0027
trigger times: 12
Loss after 19975603 batches: 0.0024
trigger times: 13
Loss after 19976498 batches: 0.0020
trigger times: 14
Loss after 19977393 batches: 0.0022
trigger times: 15
Loss after 19978288 batches: 0.0021
trigger times: 16
Loss after 19979183 batches: 0.0027
trigger times: 17
Loss after 19980078 batches: 0.0023
trigger times: 18
Loss after 19980973 batches: 0.0022
trigger times: 19
Loss after 19981868 batches: 0.0024
trigger times: 20
Loss after 19982763 batches: 0.0020
trigger times: 21
Loss after 19983658 batches: 0.0022
trigger times: 22
Loss after 19984553 batches: 0.0023
trigger times: 23
Loss after 19985448 batches: 0.0027
trigger times: 24
Loss after 19986343 batches: 0.0038
trigger times: 25
Early stopping!
Start to test process.
Loss after 19987238 batches: 0.0037
Time to train on one home:  51.747849225997925
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 19988175 batches: 0.0796
trigger times: 1
Loss after 19989112 batches: 0.0662
trigger times: 2
Loss after 19990049 batches: 0.0626
trigger times: 3
Loss after 19990986 batches: 0.0587
trigger times: 4
Loss after 19991923 batches: 0.0549
trigger times: 5
Loss after 19992860 batches: 0.0544
trigger times: 6
Loss after 19993797 batches: 0.0506
trigger times: 7
Loss after 19994734 batches: 0.0508
trigger times: 8
Loss after 19995671 batches: 0.0511
trigger times: 9
Loss after 19996608 batches: 0.0499
trigger times: 10
Loss after 19997545 batches: 0.0492
trigger times: 11
Loss after 19998482 batches: 0.0472
trigger times: 12
Loss after 19999419 batches: 0.0472
trigger times: 13
Loss after 20000356 batches: 0.0484
trigger times: 14
Loss after 20001293 batches: 0.0477
trigger times: 15
Loss after 20002230 batches: 0.0462
trigger times: 16
Loss after 20003167 batches: 0.0473
trigger times: 17
Loss after 20004104 batches: 0.0473
trigger times: 18
Loss after 20005041 batches: 0.0456
trigger times: 19
Loss after 20005978 batches: 0.0467
trigger times: 20
Loss after 20006915 batches: 0.0462
trigger times: 21
Loss after 20007852 batches: 0.0451
trigger times: 22
Loss after 20008789 batches: 0.0448
trigger times: 23
Loss after 20009726 batches: 0.0446
trigger times: 24
Loss after 20010663 batches: 0.0448
trigger times: 25
Early stopping!
Start to test process.
Loss after 20011600 batches: 0.0451
Time to train on one home:  52.73178505897522
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066, 0.03401813118363776, 0.033508403208926535, 0.03273922320832277, 0.032162070075287874, 0.031213877056718613, 0.03093800142524674, 0.030100551508447693, 0.029937661720089038, 0.02977997595640868, 0.02919253174856931, 0.029015750784499503]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863], [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861], [0.0757279247045517, 0.10266384027132536, 0.5909740425258612, 0.8803234656976516, 0.8076583271010261, 28.206427011804568, 8296.775], [0.06181953847408295, 0.10445801892372542, 0.5961459732150404, 0.831618823847048, 0.806043429565136, 26.645882531252276, 8280.187], [0.07333928346633911, 0.09887557787656209, 0.5904951093552152, 0.8805827319948272, 0.8110679704670578, 28.214734158180708, 8331.802], [0.06249981373548508, 0.1056225345966233, 0.6023961404422763, 0.8395811685763604, 0.8049953091236983, 26.90100386358231, 8269.419], [0.07111044973134995, 0.11364462231541128, 0.6015437535926366, 0.8360909977276793, 0.7977749276509928, 26.78917536742376, 8195.247], [0.06206701323390007, 0.09726051170449412, 0.5975866108942892, 0.8595424626232535, 0.8125216526577462, 27.540583297204083, 8346.734], [0.07088565826416016, 0.11302232012409985, 0.6007153971593981, 0.8537924311344753, 0.7983350331303676, 27.35634665030831, 8201.001], [0.06324874609708786, 0.09630308423125122, 0.5977169726517698, 0.8716301579538709, 0.8133833873815131, 27.927884907770363, 8355.587], [0.07241006940603256, 0.1018741345383698, 0.5940447527818379, 0.8890966213663465, 0.8083691035743891, 28.487527521645152, 8304.077], [0.06157456710934639, 0.09768669531039154, 0.5995392137772617, 0.860373560964417, 0.8121380691527804, 27.567212502959812, 8342.794]]
Round_18_results:  [0.06157456710934639, 0.09768669531039154, 0.5995392137772617, 0.860373560964417, 0.8121380691527804, 27.567212502959812, 8342.794]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 20877 < 20878; dropping {'Training_Loss': 0.0781792476773262, 'Validation_Loss': 0.07478809356689453, 'Training_R2': -0.12141588333895048, 'Validation_R2': 0.12789771792644922, 'Training_F1': 0.4080457980464329, 'Validation_F1': 0.6145170689492729, 'Training_NEP': 0.8333812638719965, 'Validation_NEP': 0.7911912163859738, 'Training_NDE': 0.682969467825667, 'Validation_NDE': 0.7879236881520061, 'Training_MAE': 31.66875615912427, 'Validation_MAE': 28.134185035738405, 'Training_MSE': 2522.47, 'Validation_MSE': 10321.81}.
trigger times: 0
Loss after 20012562 batches: 0.0782
trigger times: 1
Loss after 20013524 batches: 0.0636
trigger times: 2
Loss after 20014486 batches: 0.0605
trigger times: 3
Loss after 20015448 batches: 0.0594
trigger times: 4
Loss after 20016410 batches: 0.0560
trigger times: 5
Loss after 20017372 batches: 0.0537
trigger times: 6
Loss after 20018334 batches: 0.0525
trigger times: 7
Loss after 20019296 batches: 0.0525
trigger times: 8
Loss after 20020258 batches: 0.0502
trigger times: 9
Loss after 20021220 batches: 0.0495
trigger times: 10
Loss after 20022182 batches: 0.0478
trigger times: 11
Loss after 20023144 batches: 0.0485
trigger times: 12
Loss after 20024106 batches: 0.0501
trigger times: 13
Loss after 20025068 batches: 0.0495
trigger times: 14
Loss after 20026030 batches: 0.0486
trigger times: 15
Loss after 20026992 batches: 0.0467
trigger times: 16
Loss after 20027954 batches: 0.0469
trigger times: 17
Loss after 20028916 batches: 0.0469
trigger times: 18
Loss after 20029878 batches: 0.0455
trigger times: 19
Loss after 20030840 batches: 0.0463
trigger times: 20
Loss after 20031802 batches: 0.0452
trigger times: 21
Loss after 20032764 batches: 0.0455
trigger times: 22
Loss after 20033726 batches: 0.0448
trigger times: 23
Loss after 20034688 batches: 0.0456
trigger times: 24
Loss after 20035650 batches: 0.0449
trigger times: 25
Early stopping!
Start to test process.
Loss after 20036612 batches: 0.0454
Time to train on one home:  52.911991119384766
trigger times: 0
Loss after 20037541 batches: 0.0883
trigger times: 1
Loss after 20038470 batches: 0.0607
trigger times: 2
Loss after 20039399 batches: 0.0451
trigger times: 3
Loss after 20040328 batches: 0.0390
trigger times: 4
Loss after 20041257 batches: 0.0337
trigger times: 5
Loss after 20042186 batches: 0.0326
trigger times: 6
Loss after 20043115 batches: 0.0291
trigger times: 7
Loss after 20044044 batches: 0.0269
trigger times: 8
Loss after 20044973 batches: 0.0264
trigger times: 9
Loss after 20045902 batches: 0.0271
trigger times: 10
Loss after 20046831 batches: 0.0243
trigger times: 11
Loss after 20047760 batches: 0.0241
trigger times: 12
Loss after 20048689 batches: 0.0224
trigger times: 13
Loss after 20049618 batches: 0.0238
trigger times: 14
Loss after 20050547 batches: 0.0250
trigger times: 15
Loss after 20051476 batches: 0.0232
trigger times: 16
Loss after 20052405 batches: 0.0235
trigger times: 17
Loss after 20053334 batches: 0.0236
trigger times: 18
Loss after 20054263 batches: 0.0225
trigger times: 19
Loss after 20055192 batches: 0.0251
trigger times: 20
Loss after 20056121 batches: 0.0237
trigger times: 21
Loss after 20057050 batches: 0.0225
trigger times: 22
Loss after 20057979 batches: 0.0240
trigger times: 23
Loss after 20058908 batches: 0.0238
trigger times: 24
Loss after 20059837 batches: 0.0224
trigger times: 25
Early stopping!
Start to test process.
Loss after 20060766 batches: 0.0220
Time to train on one home:  52.73320722579956
trigger times: 0
Loss after 20061729 batches: 0.0487
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\stats\morestats.py:914: RuntimeWarning: divide by zero encountered in log
  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2621: RuntimeWarning: invalid value encountered in double_scalars
  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2214: RuntimeWarning: invalid value encountered in double_scalars
  tmp1 = (x - w) * (fx - fv)
C:\Users\aar245\AppData\Roaming\Python\Python39\site-packages\scipy\optimize\optimize.py:2215: RuntimeWarning: invalid value encountered in double_scalars
  tmp2 = (x - v) * (fx - fw)
trigger times: 1
Loss after 20062692 batches: 0.0174
trigger times: 2
Loss after 20063655 batches: 0.0143
trigger times: 3
Loss after 20064618 batches: 0.0139
trigger times: 4
Loss after 20065581 batches: 0.0137
trigger times: 5
Loss after 20066544 batches: 0.0134
trigger times: 6
Loss after 20067507 batches: 0.0128
trigger times: 7
Loss after 20068470 batches: 0.0124
trigger times: 8
Loss after 20069433 batches: 0.0113
trigger times: 9
Loss after 20070396 batches: 0.0106
trigger times: 10
Loss after 20071359 batches: 0.0096
trigger times: 11
Loss after 20072322 batches: 0.0089
trigger times: 12
Loss after 20073285 batches: 0.0086
trigger times: 13
Loss after 20074248 batches: 0.0082
trigger times: 14
Loss after 20075211 batches: 0.0081
trigger times: 15
Loss after 20076174 batches: 0.0078
trigger times: 16
Loss after 20077137 batches: 0.0077
trigger times: 17
Loss after 20078100 batches: 0.0072
trigger times: 18
Loss after 20079063 batches: 0.0069
trigger times: 19
Loss after 20080026 batches: 0.0068
trigger times: 20
Loss after 20080989 batches: 0.0066
trigger times: 21
Loss after 20081952 batches: 0.0065
trigger times: 22
Loss after 20082915 batches: 0.0066
trigger times: 23
Loss after 20083878 batches: 0.0066
trigger times: 24
Loss after 20084841 batches: 0.0064
trigger times: 25
Early stopping!
Start to test process.
Loss after 20085804 batches: 0.0062
Time to train on one home:  52.714378356933594
trigger times: 0
Loss after 20086767 batches: 0.0223
trigger times: 1
Loss after 20087730 batches: 0.0178
trigger times: 2
Loss after 20088693 batches: 0.0163
trigger times: 3
Loss after 20089656 batches: 0.0147
trigger times: 4
Loss after 20090619 batches: 0.0135
trigger times: 5
Loss after 20091582 batches: 0.0128
trigger times: 6
Loss after 20092545 batches: 0.0124
trigger times: 7
Loss after 20093508 batches: 0.0119
trigger times: 8
Loss after 20094471 batches: 0.0120
trigger times: 9
Loss after 20095434 batches: 0.0118
trigger times: 10
Loss after 20096397 batches: 0.0117
trigger times: 11
Loss after 20097360 batches: 0.0119
trigger times: 12
Loss after 20098323 batches: 0.0113
trigger times: 13
Loss after 20099286 batches: 0.0108
trigger times: 14
Loss after 20100249 batches: 0.0107
trigger times: 15
Loss after 20101212 batches: 0.0114
trigger times: 16
Loss after 20102175 batches: 0.0109
trigger times: 17
Loss after 20103138 batches: 0.0111
trigger times: 18
Loss after 20104101 batches: 0.0108
trigger times: 19
Loss after 20105064 batches: 0.0108
trigger times: 20
Loss after 20106027 batches: 0.0102
trigger times: 21
Loss after 20106990 batches: 0.0100
trigger times: 22
Loss after 20107953 batches: 0.0102
trigger times: 23
Loss after 20108916 batches: 0.0104
trigger times: 24
Loss after 20109879 batches: 0.0100
trigger times: 25
Early stopping!
Start to test process.
Loss after 20110842 batches: 0.0106
Time to train on one home:  52.8444561958313
trigger times: 0
Loss after 20111805 batches: 0.0933
trigger times: 1
Loss after 20112768 batches: 0.0839
trigger times: 2
Loss after 20113731 batches: 0.0768
trigger times: 3
Loss after 20114694 batches: 0.0730
trigger times: 4
Loss after 20115657 batches: 0.0682
trigger times: 5
Loss after 20116620 batches: 0.0653
trigger times: 6
Loss after 20117583 batches: 0.0638
trigger times: 7
Loss after 20118546 batches: 0.0631
trigger times: 8
Loss after 20119509 batches: 0.0615
trigger times: 9
Loss after 20120472 batches: 0.0596
trigger times: 10
Loss after 20121435 batches: 0.0588
trigger times: 11
Loss after 20122398 batches: 0.0607
trigger times: 12
Loss after 20123361 batches: 0.0577
trigger times: 13
Loss after 20124324 batches: 0.0574
trigger times: 14
Loss after 20125287 batches: 0.0552
trigger times: 15
Loss after 20126250 batches: 0.0548
trigger times: 16
Loss after 20127213 batches: 0.0549
trigger times: 17
Loss after 20128176 batches: 0.0543
trigger times: 18
Loss after 20129139 batches: 0.0526
trigger times: 19
Loss after 20130102 batches: 0.0523
trigger times: 20
Loss after 20131065 batches: 0.0510
trigger times: 21
Loss after 20132028 batches: 0.0515
trigger times: 22
Loss after 20132991 batches: 0.0512
trigger times: 23
Loss after 20133954 batches: 0.0511
trigger times: 24
Loss after 20134917 batches: 0.0520
trigger times: 25
Early stopping!
Start to test process.
Loss after 20135880 batches: 0.0520
Time to train on one home:  52.64862847328186
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20136843 batches: 0.0853
trigger times: 1
Loss after 20137806 batches: 0.0703
trigger times: 2
Loss after 20138769 batches: 0.0650
trigger times: 3
Loss after 20139732 batches: 0.0636
trigger times: 4
Loss after 20140695 batches: 0.0605
trigger times: 5
Loss after 20141658 batches: 0.0587
trigger times: 6
Loss after 20142621 batches: 0.0561
trigger times: 7
Loss after 20143584 batches: 0.0536
trigger times: 8
Loss after 20144547 batches: 0.0532
trigger times: 9
Loss after 20145510 batches: 0.0531
trigger times: 10
Loss after 20146473 batches: 0.0524
trigger times: 11
Loss after 20147436 batches: 0.0522
trigger times: 12
Loss after 20148399 batches: 0.0527
trigger times: 13
Loss after 20149362 batches: 0.0527
trigger times: 14
Loss after 20150325 batches: 0.0505
trigger times: 15
Loss after 20151288 batches: 0.0497
trigger times: 16
Loss after 20152251 batches: 0.0507
trigger times: 17
Loss after 20153214 batches: 0.0504
trigger times: 18
Loss after 20154177 batches: 0.0524
trigger times: 19
Loss after 20155140 batches: 0.0522
trigger times: 20
Loss after 20156103 batches: 0.0488
trigger times: 21
Loss after 20157066 batches: 0.0495
trigger times: 22
Loss after 20158029 batches: 0.0480
trigger times: 23
Loss after 20158992 batches: 0.0474
trigger times: 24
Loss after 20159955 batches: 0.0481
trigger times: 25
Early stopping!
Start to test process.
Loss after 20160918 batches: 0.0482
Time to train on one home:  52.53973865509033
trigger times: 0
Loss after 20161881 batches: 0.0734
trigger times: 1
Loss after 20162844 batches: 0.0672
trigger times: 2
Loss after 20163807 batches: 0.0645
trigger times: 3
Loss after 20164770 batches: 0.0592
trigger times: 4
Loss after 20165733 batches: 0.0568
trigger times: 5
Loss after 20166696 batches: 0.0552
trigger times: 6
Loss after 20167659 batches: 0.0536
trigger times: 7
Loss after 20168622 batches: 0.0534
trigger times: 8
Loss after 20169585 batches: 0.0531
trigger times: 9
Loss after 20170548 batches: 0.0526
trigger times: 10
Loss after 20171511 batches: 0.0511
trigger times: 11
Loss after 20172474 batches: 0.0509
trigger times: 12
Loss after 20173437 batches: 0.0501
trigger times: 13
Loss after 20174400 batches: 0.0507
trigger times: 14
Loss after 20175363 batches: 0.0497
trigger times: 15
Loss after 20176326 batches: 0.0483
trigger times: 16
Loss after 20177289 batches: 0.0496
trigger times: 17
Loss after 20178252 batches: 0.0499
trigger times: 18
Loss after 20179215 batches: 0.0501
trigger times: 19
Loss after 20180178 batches: 0.0484
trigger times: 20
Loss after 20181141 batches: 0.0474
trigger times: 21
Loss after 20182104 batches: 0.0485
trigger times: 22
Loss after 20183067 batches: 0.0478
trigger times: 23
Loss after 20184030 batches: 0.0475
trigger times: 24
Loss after 20184993 batches: 0.0476
trigger times: 25
Early stopping!
Start to test process.
Loss after 20185956 batches: 0.0459
Time to train on one home:  52.28789043426514
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20186919 batches: 0.0627
trigger times: 1
Loss after 20187882 batches: 0.0472
trigger times: 2
Loss after 20188845 batches: 0.0441
trigger times: 3
Loss after 20189808 batches: 0.0380
trigger times: 4
Loss after 20190771 batches: 0.0337
trigger times: 5
Loss after 20191734 batches: 0.0316
trigger times: 6
Loss after 20192697 batches: 0.0297
trigger times: 7
Loss after 20193660 batches: 0.0284
trigger times: 8
Loss after 20194623 batches: 0.0267
trigger times: 9
Loss after 20195586 batches: 0.0256
trigger times: 10
Loss after 20196549 batches: 0.0251
trigger times: 11
Loss after 20197512 batches: 0.0255
trigger times: 12
Loss after 20198475 batches: 0.0237
trigger times: 13
Loss after 20199438 batches: 0.0227
trigger times: 14
Loss after 20200401 batches: 0.0237
trigger times: 15
Loss after 20201364 batches: 0.0233
trigger times: 16
Loss after 20202327 batches: 0.0235
trigger times: 17
Loss after 20203290 batches: 0.0221
trigger times: 18
Loss after 20204253 batches: 0.0222
trigger times: 19
Loss after 20205216 batches: 0.0215
trigger times: 20
Loss after 20206179 batches: 0.0210
trigger times: 21
Loss after 20207142 batches: 0.0203
trigger times: 22
Loss after 20208105 batches: 0.0206
trigger times: 23
Loss after 20209068 batches: 0.0202
trigger times: 24
Loss after 20210031 batches: 0.0207
trigger times: 25
Early stopping!
Start to test process.
Loss after 20210994 batches: 0.0215
Time to train on one home:  52.39090943336487
trigger times: 0
Loss after 20211952 batches: 0.0596
trigger times: 1
Loss after 20212910 batches: 0.0414
trigger times: 2
Loss after 20213868 batches: 0.0334
trigger times: 3
Loss after 20214826 batches: 0.0269
trigger times: 4
Loss after 20215784 batches: 0.0245
trigger times: 5
Loss after 20216742 batches: 0.0222
trigger times: 6
Loss after 20217700 batches: 0.0205
trigger times: 7
Loss after 20218658 batches: 0.0202
trigger times: 8
Loss after 20219616 batches: 0.0185
trigger times: 9
Loss after 20220574 batches: 0.0184
trigger times: 10
Loss after 20221532 batches: 0.0173
trigger times: 11
Loss after 20222490 batches: 0.0157
trigger times: 12
Loss after 20223448 batches: 0.0160
trigger times: 13
Loss after 20224406 batches: 0.0158
trigger times: 14
Loss after 20225364 batches: 0.0152
trigger times: 15
Loss after 20226322 batches: 0.0150
trigger times: 16
Loss after 20227280 batches: 0.0149
trigger times: 17
Loss after 20228238 batches: 0.0147
trigger times: 18
Loss after 20229196 batches: 0.0152
trigger times: 19
Loss after 20230154 batches: 0.0149
trigger times: 20
Loss after 20231112 batches: 0.0153
trigger times: 21
Loss after 20232070 batches: 0.0173
trigger times: 22
Loss after 20233028 batches: 0.0158
trigger times: 23
Loss after 20233986 batches: 0.0142
trigger times: 24
Loss after 20234944 batches: 0.0145
trigger times: 25
Early stopping!
Start to test process.
Loss after 20235902 batches: 0.0139
Time to train on one home:  52.3839590549469
trigger times: 0
Loss after 20236864 batches: 0.0779
trigger times: 1
Loss after 20237826 batches: 0.0640
trigger times: 2
Loss after 20238788 batches: 0.0621
trigger times: 3
Loss after 20239750 batches: 0.0584
trigger times: 4
Loss after 20240712 batches: 0.0554
trigger times: 5
Loss after 20241674 batches: 0.0535
trigger times: 6
Loss after 20242636 batches: 0.0510
trigger times: 7
Loss after 20243598 batches: 0.0513
trigger times: 8
Loss after 20244560 batches: 0.0503
trigger times: 9
Loss after 20245522 batches: 0.0502
trigger times: 10
Loss after 20246484 batches: 0.0484
trigger times: 11
Loss after 20247446 batches: 0.0479
trigger times: 12
Loss after 20248408 batches: 0.0482
trigger times: 13
Loss after 20249370 batches: 0.0472
trigger times: 14
Loss after 20250332 batches: 0.0473
trigger times: 15
Loss after 20251294 batches: 0.0477
trigger times: 16
Loss after 20252256 batches: 0.0479
trigger times: 17
Loss after 20253218 batches: 0.0463
trigger times: 18
Loss after 20254180 batches: 0.0449
trigger times: 19
Loss after 20255142 batches: 0.0466
trigger times: 20
Loss after 20256104 batches: 0.0450
trigger times: 21
Loss after 20257066 batches: 0.0447
trigger times: 22
Loss after 20258028 batches: 0.0444
trigger times: 23
Loss after 20258990 batches: 0.0448
trigger times: 24
Loss after 20259952 batches: 0.0454
trigger times: 25
Early stopping!
Start to test process.
Loss after 20260914 batches: 0.0430
Time to train on one home:  52.56291937828064
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20261877 batches: 0.0927
trigger times: 1
Loss after 20262840 batches: 0.0253
trigger times: 2
Loss after 20263803 batches: 0.0197
trigger times: 3
Loss after 20264766 batches: 0.0167
trigger times: 4
Loss after 20265729 batches: 0.0154
trigger times: 5
Loss after 20266692 batches: 0.0143
trigger times: 6
Loss after 20267655 batches: 0.0138
trigger times: 7
Loss after 20268618 batches: 0.0134
trigger times: 8
Loss after 20269581 batches: 0.0132
trigger times: 9
Loss after 20270544 batches: 0.0131
trigger times: 10
Loss after 20271507 batches: 0.0129
trigger times: 11
Loss after 20272470 batches: 0.0126
trigger times: 12
Loss after 20273433 batches: 0.0126
trigger times: 13
Loss after 20274396 batches: 0.0124
trigger times: 14
Loss after 20275359 batches: 0.0123
trigger times: 15
Loss after 20276322 batches: 0.0122
trigger times: 16
Loss after 20277285 batches: 0.0121
trigger times: 17
Loss after 20278248 batches: 0.0119
trigger times: 18
Loss after 20279211 batches: 0.0120
trigger times: 19
Loss after 20280174 batches: 0.0119
trigger times: 20
Loss after 20281137 batches: 0.0117
trigger times: 21
Loss after 20282100 batches: 0.0118
trigger times: 22
Loss after 20283063 batches: 0.0117
trigger times: 23
Loss after 20284026 batches: 0.0118
trigger times: 24
Loss after 20284989 batches: 0.0116
trigger times: 25
Early stopping!
Start to test process.
Loss after 20285952 batches: 0.0118
Time to train on one home:  52.72690534591675
trigger times: 0
Loss after 20286915 batches: 0.0476
trigger times: 1
Loss after 20287878 batches: 0.0391
trigger times: 2
Loss after 20288841 batches: 0.0345
trigger times: 3
Loss after 20289804 batches: 0.0301
trigger times: 4
Loss after 20290767 batches: 0.0293
trigger times: 5
Loss after 20291730 batches: 0.0278
trigger times: 6
Loss after 20292693 batches: 0.0273
trigger times: 7
Loss after 20293656 batches: 0.0257
trigger times: 8
Loss after 20294619 batches: 0.0249
trigger times: 9
Loss after 20295582 batches: 0.0245
trigger times: 10
Loss after 20296545 batches: 0.0247
trigger times: 11
Loss after 20297508 batches: 0.0236
trigger times: 12
Loss after 20298471 batches: 0.0239
trigger times: 13
Loss after 20299434 batches: 0.0235
trigger times: 14
Loss after 20300397 batches: 0.0228
trigger times: 15
Loss after 20301360 batches: 0.0224
trigger times: 16
Loss after 20302323 batches: 0.0231
trigger times: 17
Loss after 20303286 batches: 0.0219
trigger times: 18
Loss after 20304249 batches: 0.0237
trigger times: 19
Loss after 20305212 batches: 0.0227
trigger times: 20
Loss after 20306175 batches: 0.0242
trigger times: 21
Loss after 20307138 batches: 0.0230
trigger times: 22
Loss after 20308101 batches: 0.0242
trigger times: 23
Loss after 20309064 batches: 0.0226
trigger times: 24
Loss after 20310027 batches: 0.0223
trigger times: 25
Early stopping!
Start to test process.
Loss after 20310990 batches: 0.0217
Time to train on one home:  52.800917625427246
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20311953 batches: 0.0625
trigger times: 1
Loss after 20312916 batches: 0.0470
trigger times: 2
Loss after 20313879 batches: 0.0436
trigger times: 3
Loss after 20314842 batches: 0.0374
trigger times: 4
Loss after 20315805 batches: 0.0333
trigger times: 5
Loss after 20316768 batches: 0.0316
trigger times: 6
Loss after 20317731 batches: 0.0291
trigger times: 7
Loss after 20318694 batches: 0.0284
trigger times: 8
Loss after 20319657 batches: 0.0265
trigger times: 9
Loss after 20320620 batches: 0.0259
trigger times: 10
Loss after 20321583 batches: 0.0260
trigger times: 11
Loss after 20322546 batches: 0.0253
trigger times: 12
Loss after 20323509 batches: 0.0238
trigger times: 13
Loss after 20324472 batches: 0.0231
trigger times: 14
Loss after 20325435 batches: 0.0230
trigger times: 15
Loss after 20326398 batches: 0.0233
trigger times: 16
Loss after 20327361 batches: 0.0226
trigger times: 17
Loss after 20328324 batches: 0.0232
trigger times: 18
Loss after 20329287 batches: 0.0231
trigger times: 19
Loss after 20330250 batches: 0.0218
trigger times: 20
Loss after 20331213 batches: 0.0217
trigger times: 21
Loss after 20332176 batches: 0.0208
trigger times: 22
Loss after 20333139 batches: 0.0202
trigger times: 23
Loss after 20334102 batches: 0.0205
trigger times: 24
Loss after 20335065 batches: 0.0206
trigger times: 25
Early stopping!
Start to test process.
Loss after 20336028 batches: 0.0196
Time to train on one home:  52.45203971862793
trigger times: 0
Loss after 20336991 batches: 0.0475
trigger times: 1
Loss after 20337954 batches: 0.0382
trigger times: 2
Loss after 20338917 batches: 0.0341
trigger times: 3
Loss after 20339880 batches: 0.0302
trigger times: 4
Loss after 20340843 batches: 0.0291
trigger times: 5
Loss after 20341806 batches: 0.0266
trigger times: 6
Loss after 20342769 batches: 0.0258
trigger times: 7
Loss after 20343732 batches: 0.0243
trigger times: 8
Loss after 20344695 batches: 0.0246
trigger times: 9
Loss after 20345658 batches: 0.0243
trigger times: 10
Loss after 20346621 batches: 0.0239
trigger times: 11
Loss after 20347584 batches: 0.0241
trigger times: 12
Loss after 20348547 batches: 0.0240
trigger times: 13
Loss after 20349510 batches: 0.0234
trigger times: 14
Loss after 20350473 batches: 0.0233
trigger times: 15
Loss after 20351436 batches: 0.0244
trigger times: 16
Loss after 20352399 batches: 0.0231
trigger times: 17
Loss after 20353362 batches: 0.0218
trigger times: 18
Loss after 20354325 batches: 0.0222
trigger times: 19
Loss after 20355288 batches: 0.0214
trigger times: 20
Loss after 20356251 batches: 0.0225
trigger times: 21
Loss after 20357214 batches: 0.0226
trigger times: 22
Loss after 20358177 batches: 0.0217
trigger times: 23
Loss after 20359140 batches: 0.0228
trigger times: 24
Loss after 20360103 batches: 0.0206
trigger times: 25
Early stopping!
Start to test process.
Loss after 20361066 batches: 0.0220
Time to train on one home:  52.69300317764282
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20362029 batches: 0.0460
trigger times: 1
Loss after 20362992 batches: 0.0410
trigger times: 2
Loss after 20363955 batches: 0.0377
trigger times: 3
Loss after 20364918 batches: 0.0354
trigger times: 4
Loss after 20365881 batches: 0.0340
trigger times: 5
Loss after 20366844 batches: 0.0309
trigger times: 6
Loss after 20367807 batches: 0.0307
trigger times: 7
Loss after 20368770 batches: 0.0289
trigger times: 8
Loss after 20369733 batches: 0.0287
trigger times: 9
Loss after 20370696 batches: 0.0288
trigger times: 10
Loss after 20371659 batches: 0.0282
trigger times: 11
Loss after 20372622 batches: 0.0280
trigger times: 12
Loss after 20373585 batches: 0.0269
trigger times: 13
Loss after 20374548 batches: 0.0257
trigger times: 14
Loss after 20375511 batches: 0.0252
trigger times: 15
Loss after 20376474 batches: 0.0256
trigger times: 16
Loss after 20377437 batches: 0.0247
trigger times: 17
Loss after 20378400 batches: 0.0239
trigger times: 18
Loss after 20379363 batches: 0.0246
trigger times: 19
Loss after 20380326 batches: 0.0240
trigger times: 20
Loss after 20381289 batches: 0.0246
trigger times: 21
Loss after 20382252 batches: 0.0271
trigger times: 22
Loss after 20383215 batches: 0.0255
trigger times: 23
Loss after 20384178 batches: 0.0250
trigger times: 24
Loss after 20385141 batches: 0.0255
trigger times: 25
Early stopping!
Start to test process.
Loss after 20386104 batches: 0.0244
Time to train on one home:  52.474079608917236
trigger times: 0
Loss after 20387067 batches: 0.0867
trigger times: 1
Loss after 20388030 batches: 0.0486
trigger times: 2
Loss after 20388993 batches: 0.0464
trigger times: 3
Loss after 20389956 batches: 0.0448
trigger times: 4
Loss after 20390919 batches: 0.0414
trigger times: 5
Loss after 20391882 batches: 0.0403
trigger times: 6
Loss after 20392845 batches: 0.0389
trigger times: 7
Loss after 20393808 batches: 0.0385
trigger times: 8
Loss after 20394771 batches: 0.0366
trigger times: 9
Loss after 20395734 batches: 0.0356
trigger times: 10
Loss after 20396697 batches: 0.0360
trigger times: 11
Loss after 20397660 batches: 0.0354
trigger times: 12
Loss after 20398623 batches: 0.0348
trigger times: 13
Loss after 20399586 batches: 0.0346
trigger times: 14
Loss after 20400549 batches: 0.0343
trigger times: 15
Loss after 20401512 batches: 0.0345
trigger times: 16
Loss after 20402475 batches: 0.0341
trigger times: 17
Loss after 20403438 batches: 0.0348
trigger times: 18
Loss after 20404401 batches: 0.0339
trigger times: 19
Loss after 20405364 batches: 0.0328
trigger times: 20
Loss after 20406327 batches: 0.0325
trigger times: 21
Loss after 20407290 batches: 0.0321
trigger times: 22
Loss after 20408253 batches: 0.0318
trigger times: 23
Loss after 20409216 batches: 0.0316
trigger times: 24
Loss after 20410179 batches: 0.0314
trigger times: 25
Early stopping!
Start to test process.
Loss after 20411142 batches: 0.0315
Time to train on one home:  52.95695614814758
trigger times: 0
Loss after 20412105 batches: 0.0934
trigger times: 1
Loss after 20413068 batches: 0.0853
trigger times: 2
Loss after 20414031 batches: 0.0771
trigger times: 3
Loss after 20414994 batches: 0.0758
trigger times: 4
Loss after 20415957 batches: 0.0707
trigger times: 5
Loss after 20416920 batches: 0.0662
trigger times: 6
Loss after 20417883 batches: 0.0631
trigger times: 7
Loss after 20418846 batches: 0.0605
trigger times: 8
Loss after 20419809 batches: 0.0594
trigger times: 9
Loss after 20420772 batches: 0.0590
trigger times: 10
Loss after 20421735 batches: 0.0573
trigger times: 11
Loss after 20422698 batches: 0.0572
trigger times: 12
Loss after 20423661 batches: 0.0567
trigger times: 13
Loss after 20424624 batches: 0.0598
trigger times: 14
Loss after 20425587 batches: 0.0552
trigger times: 15
Loss after 20426550 batches: 0.0551
trigger times: 16
Loss after 20427513 batches: 0.0547
trigger times: 17
Loss after 20428476 batches: 0.0543
trigger times: 18
Loss after 20429439 batches: 0.0544
trigger times: 19
Loss after 20430402 batches: 0.0539
trigger times: 20
Loss after 20431365 batches: 0.0540
trigger times: 21
Loss after 20432328 batches: 0.0532
trigger times: 22
Loss after 20433291 batches: 0.0518
trigger times: 23
Loss after 20434254 batches: 0.0517
trigger times: 24
Loss after 20435217 batches: 0.0516
trigger times: 25
Early stopping!
Start to test process.
Loss after 20436180 batches: 0.0503
Time to train on one home:  52.71784162521362
trigger times: 0
Loss after 20437143 batches: 0.0892
trigger times: 1
Loss after 20438106 batches: 0.0562
trigger times: 2
Loss after 20439069 batches: 0.0517
trigger times: 3
Loss after 20440032 batches: 0.0461
trigger times: 4
Loss after 20440995 batches: 0.0432
trigger times: 5
Loss after 20441958 batches: 0.0418
trigger times: 6
Loss after 20442921 batches: 0.0408
trigger times: 7
Loss after 20443884 batches: 0.0385
trigger times: 8
Loss after 20444847 batches: 0.0380
trigger times: 9
Loss after 20445810 batches: 0.0371
trigger times: 10
Loss after 20446773 batches: 0.0361
trigger times: 11
Loss after 20447736 batches: 0.0344
trigger times: 12
Loss after 20448699 batches: 0.0350
trigger times: 13
Loss after 20449662 batches: 0.0342
trigger times: 14
Loss after 20450625 batches: 0.0356
trigger times: 15
Loss after 20451588 batches: 0.0347
trigger times: 16
Loss after 20452551 batches: 0.0352
trigger times: 17
Loss after 20453514 batches: 0.0338
trigger times: 18
Loss after 20454477 batches: 0.0344
trigger times: 19
Loss after 20455440 batches: 0.0329
trigger times: 20
Loss after 20456403 batches: 0.0337
trigger times: 21
Loss after 20457366 batches: 0.0340
trigger times: 22
Loss after 20458329 batches: 0.0331
trigger times: 23
Loss after 20459292 batches: 0.0329
trigger times: 24
Loss after 20460255 batches: 0.0321
trigger times: 25
Early stopping!
Start to test process.
Loss after 20461218 batches: 0.0321
Time to train on one home:  52.846848011016846
trigger times: 0
Loss after 20462147 batches: 0.0885
trigger times: 1
Loss after 20463076 batches: 0.0589
trigger times: 2
Loss after 20464005 batches: 0.0456
trigger times: 3
Loss after 20464934 batches: 0.0368
trigger times: 4
Loss after 20465863 batches: 0.0359
trigger times: 5
Loss after 20466792 batches: 0.0320
trigger times: 6
Loss after 20467721 batches: 0.0280
trigger times: 7
Loss after 20468650 batches: 0.0277
trigger times: 8
Loss after 20469579 batches: 0.0279
trigger times: 9
Loss after 20470508 batches: 0.0268
trigger times: 10
Loss after 20471437 batches: 0.0262
trigger times: 11
Loss after 20472366 batches: 0.0356
trigger times: 12
Loss after 20473295 batches: 0.0288
trigger times: 13
Loss after 20474224 batches: 0.0271
trigger times: 14
Loss after 20475153 batches: 0.0268
trigger times: 15
Loss after 20476082 batches: 0.0266
trigger times: 16
Loss after 20477011 batches: 0.0259
trigger times: 17
Loss after 20477940 batches: 0.0260
trigger times: 18
Loss after 20478869 batches: 0.0227
trigger times: 19
Loss after 20479798 batches: 0.0245
trigger times: 20
Loss after 20480727 batches: 0.0304
trigger times: 21
Loss after 20481656 batches: 0.0401
trigger times: 22
Loss after 20482585 batches: 0.0360
trigger times: 23
Loss after 20483514 batches: 0.0307
trigger times: 24
Loss after 20484443 batches: 0.0295
trigger times: 25
Early stopping!
Start to test process.
Loss after 20485372 batches: 0.0299
Time to train on one home:  52.2760226726532
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20486335 batches: 0.0766
trigger times: 1
Loss after 20487298 batches: 0.0337
trigger times: 2
Loss after 20488261 batches: 0.0276
trigger times: 3
Loss after 20489224 batches: 0.0275
trigger times: 4
Loss after 20490187 batches: 0.0271
trigger times: 5
Loss after 20491150 batches: 0.0259
trigger times: 6
Loss after 20492113 batches: 0.0247
trigger times: 7
Loss after 20493076 batches: 0.0238
trigger times: 8
Loss after 20494039 batches: 0.0225
trigger times: 9
Loss after 20495002 batches: 0.0214
trigger times: 10
Loss after 20495965 batches: 0.0203
trigger times: 11
Loss after 20496928 batches: 0.0198
trigger times: 12
Loss after 20497891 batches: 0.0195
trigger times: 13
Loss after 20498854 batches: 0.0191
trigger times: 14
Loss after 20499817 batches: 0.0190
trigger times: 15
Loss after 20500780 batches: 0.0185
trigger times: 16
Loss after 20501743 batches: 0.0183
trigger times: 17
Loss after 20502706 batches: 0.0181
trigger times: 18
Loss after 20503669 batches: 0.0176
trigger times: 19
Loss after 20504632 batches: 0.0173
trigger times: 20
Loss after 20505595 batches: 0.0170
trigger times: 21
Loss after 20506558 batches: 0.0170
trigger times: 22
Loss after 20507521 batches: 0.0171
trigger times: 23
Loss after 20508484 batches: 0.0169
trigger times: 24
Loss after 20509447 batches: 0.0165
trigger times: 25
Early stopping!
Start to test process.
Loss after 20510410 batches: 0.0166
Time to train on one home:  52.7629611492157
trigger times: 0
Loss after 20511373 batches: 0.1845
trigger times: 1
Loss after 20512336 batches: 0.1226
trigger times: 2
Loss after 20513299 batches: 0.0917
trigger times: 3
Loss after 20514262 batches: 0.0834
trigger times: 4
Loss after 20515225 batches: 0.0755
trigger times: 5
Loss after 20516188 batches: 0.0704
trigger times: 6
Loss after 20517151 batches: 0.0642
trigger times: 7
Loss after 20518114 batches: 0.0608
trigger times: 8
Loss after 20519077 batches: 0.0554
trigger times: 9
Loss after 20520040 batches: 0.0533
trigger times: 10
Loss after 20521003 batches: 0.0501
trigger times: 11
Loss after 20521966 batches: 0.0481
trigger times: 12
Loss after 20522929 batches: 0.0475
trigger times: 13
Loss after 20523892 batches: 0.0472
trigger times: 14
Loss after 20524855 batches: 0.0443
trigger times: 15
Loss after 20525818 batches: 0.0454
trigger times: 16
Loss after 20526781 batches: 0.0420
trigger times: 17
Loss after 20527744 batches: 0.0410
trigger times: 18
Loss after 20528707 batches: 0.0398
trigger times: 19
Loss after 20529670 batches: 0.0415
trigger times: 20
Loss after 20530633 batches: 0.0406
trigger times: 21
Loss after 20531596 batches: 0.0400
trigger times: 22
Loss after 20532559 batches: 0.0406
trigger times: 23
Loss after 20533522 batches: 0.0410
trigger times: 24
Loss after 20534485 batches: 0.0401
trigger times: 25
Early stopping!
Start to test process.
Loss after 20535448 batches: 0.0382
Time to train on one home:  52.486053466796875
trigger times: 0
Loss after 20536411 batches: 0.0841
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 1
Loss after 20537374 batches: 0.0706
trigger times: 2
Loss after 20538337 batches: 0.0661
trigger times: 3
Loss after 20539300 batches: 0.0628
trigger times: 4
Loss after 20540263 batches: 0.0610
trigger times: 5
Loss after 20541226 batches: 0.0581
trigger times: 6
Loss after 20542189 batches: 0.0558
trigger times: 7
Loss after 20543152 batches: 0.0550
trigger times: 8
Loss after 20544115 batches: 0.0537
trigger times: 9
Loss after 20545078 batches: 0.0532
trigger times: 10
Loss after 20546041 batches: 0.0523
trigger times: 11
Loss after 20547004 batches: 0.0529
trigger times: 12
Loss after 20547967 batches: 0.0510
trigger times: 13
Loss after 20548930 batches: 0.0524
trigger times: 14
Loss after 20549893 batches: 0.0514
trigger times: 15
Loss after 20550856 batches: 0.0507
trigger times: 16
Loss after 20551819 batches: 0.0517
trigger times: 17
Loss after 20552782 batches: 0.0515
trigger times: 18
Loss after 20553745 batches: 0.0495
trigger times: 19
Loss after 20554708 batches: 0.0497
trigger times: 20
Loss after 20555671 batches: 0.0498
trigger times: 21
Loss after 20556634 batches: 0.0481
trigger times: 22
Loss after 20557597 batches: 0.0476
trigger times: 23
Loss after 20558560 batches: 0.0480
trigger times: 24
Loss after 20559523 batches: 0.0493
trigger times: 25
Early stopping!
Start to test process.
Loss after 20560486 batches: 0.0477
Time to train on one home:  52.37209153175354
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20561449 batches: 0.0893
trigger times: 1
Loss after 20562412 batches: 0.0739
trigger times: 2
Loss after 20563375 batches: 0.0712
trigger times: 3
Loss after 20564338 batches: 0.0664
trigger times: 4
Loss after 20565301 batches: 0.0641
trigger times: 5
Loss after 20566264 batches: 0.0592
trigger times: 6
Loss after 20567227 batches: 0.0563
trigger times: 7
Loss after 20568190 batches: 0.0532
trigger times: 8
Loss after 20569153 batches: 0.0516
trigger times: 9
Loss after 20570116 batches: 0.0478
trigger times: 10
Loss after 20571079 batches: 0.0487
trigger times: 11
Loss after 20572042 batches: 0.0494
trigger times: 12
Loss after 20573005 batches: 0.0480
trigger times: 13
Loss after 20573968 batches: 0.0478
trigger times: 14
Loss after 20574931 batches: 0.0471
trigger times: 15
Loss after 20575894 batches: 0.0461
trigger times: 16
Loss after 20576857 batches: 0.0482
trigger times: 17
Loss after 20577820 batches: 0.0442
trigger times: 18
Loss after 20578783 batches: 0.0427
trigger times: 19
Loss after 20579746 batches: 0.0429
trigger times: 20
Loss after 20580709 batches: 0.0422
trigger times: 21
Loss after 20581672 batches: 0.0434
trigger times: 22
Loss after 20582635 batches: 0.0442
trigger times: 23
Loss after 20583598 batches: 0.0415
trigger times: 24
Loss after 20584561 batches: 0.0410
trigger times: 25
Early stopping!
Start to test process.
Loss after 20585524 batches: 0.0428
Time to train on one home:  52.422112226486206
trigger times: 0
Loss after 20586487 batches: 0.0223
trigger times: 1
Loss after 20587450 batches: 0.0174
trigger times: 2
Loss after 20588413 batches: 0.0164
trigger times: 3
Loss after 20589376 batches: 0.0148
trigger times: 4
Loss after 20590339 batches: 0.0138
trigger times: 5
Loss after 20591302 batches: 0.0130
trigger times: 6
Loss after 20592265 batches: 0.0122
trigger times: 7
Loss after 20593228 batches: 0.0119
trigger times: 8
Loss after 20594191 batches: 0.0116
trigger times: 9
Loss after 20595154 batches: 0.0120
trigger times: 10
Loss after 20596117 batches: 0.0113
trigger times: 11
Loss after 20597080 batches: 0.0116
trigger times: 12
Loss after 20598043 batches: 0.0114
trigger times: 13
Loss after 20599006 batches: 0.0109
trigger times: 14
Loss after 20599969 batches: 0.0107
trigger times: 15
Loss after 20600932 batches: 0.0108
trigger times: 16
Loss after 20601895 batches: 0.0111
trigger times: 17
Loss after 20602858 batches: 0.0107
trigger times: 18
Loss after 20603821 batches: 0.0107
trigger times: 19
Loss after 20604784 batches: 0.0107
trigger times: 20
Loss after 20605747 batches: 0.0096
trigger times: 21
Loss after 20606710 batches: 0.0095
trigger times: 22
Loss after 20607673 batches: 0.0097
trigger times: 23
Loss after 20608636 batches: 0.0096
trigger times: 24
Loss after 20609599 batches: 0.0097
trigger times: 25
Early stopping!
Start to test process.
Loss after 20610562 batches: 0.0096
Time to train on one home:  52.84896373748779
trigger times: 0
Loss after 20611525 batches: 0.0396
trigger times: 1
Loss after 20612488 batches: 0.0308
trigger times: 2
Loss after 20613451 batches: 0.0278
trigger times: 3
Loss after 20614414 batches: 0.0266
trigger times: 4
Loss after 20615377 batches: 0.0245
trigger times: 5
Loss after 20616340 batches: 0.0227
trigger times: 6
Loss after 20617303 batches: 0.0222
trigger times: 7
Loss after 20618266 batches: 0.0433
trigger times: 8
Loss after 20619229 batches: 0.0428
trigger times: 9
Loss after 20620192 batches: 0.0390
trigger times: 10
Loss after 20621155 batches: 0.0362
trigger times: 11
Loss after 20622118 batches: 0.0340
trigger times: 12
Loss after 20623081 batches: 0.0319
trigger times: 13
Loss after 20624044 batches: 0.0312
trigger times: 14
Loss after 20625007 batches: 0.0297
trigger times: 15
Loss after 20625970 batches: 0.0295
trigger times: 16
Loss after 20626933 batches: 0.0282
trigger times: 17
Loss after 20627896 batches: 0.0280
trigger times: 18
Loss after 20628859 batches: 0.0273
trigger times: 19
Loss after 20629822 batches: 0.0282
trigger times: 20
Loss after 20630785 batches: 0.0268
trigger times: 21
Loss after 20631748 batches: 0.0262
trigger times: 22
Loss after 20632711 batches: 0.0261
trigger times: 23
Loss after 20633674 batches: 0.0255
trigger times: 24
Loss after 20634637 batches: 0.0261
trigger times: 25
Early stopping!
Start to test process.
Loss after 20635600 batches: 0.0248
Time to train on one home:  52.565056562423706
trigger times: 0
Loss after 20636563 batches: 0.0811
trigger times: 1
Loss after 20637526 batches: 0.0584
trigger times: 2
Loss after 20638489 batches: 0.0518
trigger times: 3
Loss after 20639452 batches: 0.0466
trigger times: 4
Loss after 20640415 batches: 0.0425
trigger times: 5
Loss after 20641378 batches: 0.0411
trigger times: 6
Loss after 20642341 batches: 0.0395
trigger times: 7
Loss after 20643304 batches: 0.0379
trigger times: 8
Loss after 20644267 batches: 0.0361
trigger times: 9
Loss after 20645230 batches: 0.0361
trigger times: 10
Loss after 20646193 batches: 0.0361
trigger times: 11
Loss after 20647156 batches: 0.0352
trigger times: 12
Loss after 20648119 batches: 0.0353
trigger times: 13
Loss after 20649082 batches: 0.0351
trigger times: 14
Loss after 20650045 batches: 0.0345
trigger times: 15
Loss after 20651008 batches: 0.0348
trigger times: 16
Loss after 20651971 batches: 0.0338
trigger times: 17
Loss after 20652934 batches: 0.0324
trigger times: 18
Loss after 20653897 batches: 0.0324
trigger times: 19
Loss after 20654860 batches: 0.0311
trigger times: 20
Loss after 20655823 batches: 0.0312
trigger times: 21
Loss after 20656786 batches: 0.0313
trigger times: 22
Loss after 20657749 batches: 0.0318
trigger times: 23
Loss after 20658712 batches: 0.0306
trigger times: 24
Loss after 20659675 batches: 0.0317
trigger times: 25
Early stopping!
Start to test process.
Loss after 20660638 batches: 0.0312
Time to train on one home:  52.79797124862671
trigger times: 0
Loss after 20661601 batches: 0.0882
trigger times: 1
Loss after 20662564 batches: 0.0563
trigger times: 2
Loss after 20663527 batches: 0.0505
trigger times: 3
Loss after 20664490 batches: 0.0465
trigger times: 4
Loss after 20665453 batches: 0.0436
trigger times: 5
Loss after 20666416 batches: 0.0411
trigger times: 6
Loss after 20667379 batches: 0.0394
trigger times: 7
Loss after 20668342 batches: 0.0391
trigger times: 8
Loss after 20669305 batches: 0.0379
trigger times: 9
Loss after 20670268 batches: 0.0371
trigger times: 10
Loss after 20671231 batches: 0.0357
trigger times: 11
Loss after 20672194 batches: 0.0359
trigger times: 12
Loss after 20673157 batches: 0.0352
trigger times: 13
Loss after 20674120 batches: 0.0348
trigger times: 14
Loss after 20675083 batches: 0.0346
trigger times: 15
Loss after 20676046 batches: 0.0343
trigger times: 16
Loss after 20677009 batches: 0.0341
trigger times: 17
Loss after 20677972 batches: 0.0350
trigger times: 18
Loss after 20678935 batches: 0.0341
trigger times: 19
Loss after 20679898 batches: 0.0332
trigger times: 20
Loss after 20680861 batches: 0.0328
trigger times: 21
Loss after 20681824 batches: 0.0334
trigger times: 22
Loss after 20682787 batches: 0.0328
trigger times: 23
Loss after 20683750 batches: 0.0329
trigger times: 24
Loss after 20684713 batches: 0.0331
trigger times: 25
Early stopping!
Start to test process.
Loss after 20685676 batches: 0.0330
Time to train on one home:  52.95698547363281
trigger times: 0
Loss after 20686639 batches: 0.0474
trigger times: 1
Loss after 20687602 batches: 0.0386
trigger times: 2
Loss after 20688565 batches: 0.0335
trigger times: 3
Loss after 20689528 batches: 0.0311
trigger times: 4
Loss after 20690491 batches: 0.0291
trigger times: 5
Loss after 20691454 batches: 0.0266
trigger times: 6
Loss after 20692417 batches: 0.0260
trigger times: 7
Loss after 20693380 batches: 0.0252
trigger times: 8
Loss after 20694343 batches: 0.0244
trigger times: 9
Loss after 20695306 batches: 0.0241
trigger times: 10
Loss after 20696269 batches: 0.0241
trigger times: 11
Loss after 20697232 batches: 0.0235
trigger times: 12
Loss after 20698195 batches: 0.0236
trigger times: 13
Loss after 20699158 batches: 0.0223
trigger times: 14
Loss after 20700121 batches: 0.0218
trigger times: 15
Loss after 20701084 batches: 0.0219
trigger times: 16
Loss after 20702047 batches: 0.0224
trigger times: 17
Loss after 20703010 batches: 0.0213
trigger times: 18
Loss after 20703973 batches: 0.0220
trigger times: 19
Loss after 20704936 batches: 0.0231
trigger times: 20
Loss after 20705899 batches: 0.0221
trigger times: 21
Loss after 20706862 batches: 0.0252
trigger times: 22
Loss after 20707825 batches: 0.0230
trigger times: 23
Loss after 20708788 batches: 0.0217
trigger times: 24
Loss after 20709751 batches: 0.0230
trigger times: 25
Early stopping!
Start to test process.
Loss after 20710714 batches: 0.0214
Time to train on one home:  52.61206245422363
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20711677 batches: 0.0743
trigger times: 1
Loss after 20712640 batches: 0.0449
trigger times: 2
Loss after 20713603 batches: 0.0438
trigger times: 3
Loss after 20714566 batches: 0.0383
trigger times: 0
Loss after 20715529 batches: 0.0350
trigger times: 1
Loss after 20716492 batches: 0.0314
trigger times: 2
Loss after 20717455 batches: 0.0304
trigger times: 3
Loss after 20718418 batches: 0.0274
trigger times: 4
Loss after 20719381 batches: 0.0275
trigger times: 5
Loss after 20720344 batches: 0.0269
trigger times: 6
Loss after 20721307 batches: 0.0252
trigger times: 7
Loss after 20722270 batches: 0.0254
trigger times: 0
Loss after 20723233 batches: 0.0246
trigger times: 1
Loss after 20724196 batches: 0.0237
trigger times: 2
Loss after 20725159 batches: 0.0230
trigger times: 3
Loss after 20726122 batches: 0.0223
trigger times: 4
Loss after 20727085 batches: 0.0232
trigger times: 5
Loss after 20728048 batches: 0.0222
trigger times: 6
Loss after 20729011 batches: 0.0214
trigger times: 7
Loss after 20729974 batches: 0.0222
trigger times: 8
Loss after 20730937 batches: 0.0201
trigger times: 9
Loss after 20731900 batches: 0.0203
trigger times: 10
Loss after 20732863 batches: 0.0208
trigger times: 11
Loss after 20733826 batches: 0.0217
trigger times: 12
Loss after 20734789 batches: 0.0213
trigger times: 13
Loss after 20735752 batches: 0.0208
trigger times: 14
Loss after 20736715 batches: 0.0205
trigger times: 15
Loss after 20737678 batches: 0.0202
trigger times: 16
Loss after 20738641 batches: 0.0198
trigger times: 17
Loss after 20739604 batches: 0.0191
trigger times: 18
Loss after 20740567 batches: 0.0194
trigger times: 19
Loss after 20741530 batches: 0.0195
trigger times: 20
Loss after 20742493 batches: 0.0197
trigger times: 21
Loss after 20743456 batches: 0.0188
trigger times: 22
Loss after 20744419 batches: 0.0183
trigger times: 23
Loss after 20745382 batches: 0.0185
trigger times: 24
Loss after 20746345 batches: 0.0182
trigger times: 25
Early stopping!
Start to test process.
Loss after 20747308 batches: 0.0175
Time to train on one home:  62.01322102546692
trigger times: 0
Loss after 20748271 batches: 0.1049
trigger times: 1
Loss after 20749234 batches: 0.0795
trigger times: 2
Loss after 20750197 batches: 0.0755
trigger times: 3
Loss after 20751160 batches: 0.0735
trigger times: 4
Loss after 20752123 batches: 0.0703
trigger times: 5
Loss after 20753086 batches: 0.0674
trigger times: 6
Loss after 20754049 batches: 0.0654
trigger times: 7
Loss after 20755012 batches: 0.0631
trigger times: 8
Loss after 20755975 batches: 0.0613
trigger times: 9
Loss after 20756938 batches: 0.0605
trigger times: 10
Loss after 20757901 batches: 0.0597
trigger times: 11
Loss after 20758864 batches: 0.0583
trigger times: 12
Loss after 20759827 batches: 0.0579
trigger times: 13
Loss after 20760790 batches: 0.0561
trigger times: 14
Loss after 20761753 batches: 0.0571
trigger times: 15
Loss after 20762716 batches: 0.0552
trigger times: 16
Loss after 20763679 batches: 0.0536
trigger times: 17
Loss after 20764642 batches: 0.0552
trigger times: 18
Loss after 20765605 batches: 0.0540
trigger times: 19
Loss after 20766568 batches: 0.0533
trigger times: 20
Loss after 20767531 batches: 0.0534
trigger times: 21
Loss after 20768494 batches: 0.0538
trigger times: 22
Loss after 20769457 batches: 0.0526
trigger times: 23
Loss after 20770420 batches: 0.0518
trigger times: 24
Loss after 20771383 batches: 0.0526
trigger times: 25
Early stopping!
Start to test process.
Loss after 20772346 batches: 0.0525
Time to train on one home:  52.260175943374634
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20773309 batches: 0.0756
trigger times: 1
Loss after 20774272 batches: 0.0449
trigger times: 2
Loss after 20775235 batches: 0.0453
trigger times: 3
Loss after 20776198 batches: 0.0392
trigger times: 0
Loss after 20777161 batches: 0.0352
trigger times: 1
Loss after 20778124 batches: 0.0319
trigger times: 0
Loss after 20779087 batches: 0.0297
trigger times: 1
Loss after 20780050 batches: 0.0287
trigger times: 2
Loss after 20781013 batches: 0.0283
trigger times: 3
Loss after 20781976 batches: 0.0266
trigger times: 4
Loss after 20782939 batches: 0.0256
trigger times: 5
Loss after 20783902 batches: 0.0249
trigger times: 6
Loss after 20784865 batches: 0.0242
trigger times: 7
Loss after 20785828 batches: 0.0236
trigger times: 8
Loss after 20786791 batches: 0.0230
trigger times: 9
Loss after 20787754 batches: 0.0246
trigger times: 10
Loss after 20788717 batches: 0.0236
trigger times: 11
Loss after 20789680 batches: 0.0221
trigger times: 12
Loss after 20790643 batches: 0.0230
trigger times: 13
Loss after 20791606 batches: 0.0231
trigger times: 14
Loss after 20792569 batches: 0.0222
trigger times: 15
Loss after 20793532 batches: 0.0217
trigger times: 16
Loss after 20794495 batches: 0.0214
trigger times: 17
Loss after 20795458 batches: 0.0215
trigger times: 18
Loss after 20796421 batches: 0.0220
trigger times: 19
Loss after 20797384 batches: 0.0203
trigger times: 20
Loss after 20798347 batches: 0.0210
trigger times: 21
Loss after 20799310 batches: 0.0212
trigger times: 22
Loss after 20800273 batches: 0.0206
trigger times: 23
Loss after 20801236 batches: 0.0205
trigger times: 24
Loss after 20802199 batches: 0.0198
trigger times: 25
Early stopping!
Start to test process.
Loss after 20803162 batches: 0.0201
Time to train on one home:  57.013745069503784
trigger times: 0
Loss after 20804057 batches: 0.0674
trigger times: 1
Loss after 20804952 batches: 0.0389
trigger times: 2
Loss after 20805847 batches: 0.0160
trigger times: 3
Loss after 20806742 batches: 0.0094
trigger times: 4
Loss after 20807637 batches: 0.0091
trigger times: 5
Loss after 20808532 batches: 0.0061
trigger times: 6
Loss after 20809427 batches: 0.0056
trigger times: 7
Loss after 20810322 batches: 0.0041
trigger times: 8
Loss after 20811217 batches: 0.0041
trigger times: 9
Loss after 20812112 batches: 0.0038
trigger times: 10
Loss after 20813007 batches: 0.0036
trigger times: 11
Loss after 20813902 batches: 0.0045
trigger times: 12
Loss after 20814797 batches: 0.0031
trigger times: 13
Loss after 20815692 batches: 0.0026
trigger times: 14
Loss after 20816587 batches: 0.0024
trigger times: 15
Loss after 20817482 batches: 0.0022
trigger times: 16
Loss after 20818377 batches: 0.0019
trigger times: 17
Loss after 20819272 batches: 0.0018
trigger times: 18
Loss after 20820167 batches: 0.0018
trigger times: 19
Loss after 20821062 batches: 0.0019
trigger times: 20
Loss after 20821957 batches: 0.0019
trigger times: 21
Loss after 20822852 batches: 0.0017
trigger times: 22
Loss after 20823747 batches: 0.0019
trigger times: 23
Loss after 20824642 batches: 0.0017
trigger times: 24
Loss after 20825537 batches: 0.0020
trigger times: 25
Early stopping!
Start to test process.
Loss after 20826432 batches: 0.0019
Time to train on one home:  51.54639720916748
C:\Users\aar245\Anaconda3\envs\privacy_ml_env\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
trigger times: 0
Loss after 20827369 batches: 0.0791
trigger times: 1
Loss after 20828306 batches: 0.0647
trigger times: 2
Loss after 20829243 batches: 0.0634
trigger times: 3
Loss after 20830180 batches: 0.0580
trigger times: 4
Loss after 20831117 batches: 0.0551
trigger times: 5
Loss after 20832054 batches: 0.0534
trigger times: 6
Loss after 20832991 batches: 0.0519
trigger times: 7
Loss after 20833928 batches: 0.0517
trigger times: 8
Loss after 20834865 batches: 0.0502
trigger times: 9
Loss after 20835802 batches: 0.0502
trigger times: 10
Loss after 20836739 batches: 0.0480
trigger times: 11
Loss after 20837676 batches: 0.0476
trigger times: 12
Loss after 20838613 batches: 0.0487
trigger times: 13
Loss after 20839550 batches: 0.0476
trigger times: 14
Loss after 20840487 batches: 0.0476
trigger times: 15
Loss after 20841424 batches: 0.0463
trigger times: 16
Loss after 20842361 batches: 0.0459
trigger times: 17
Loss after 20843298 batches: 0.0455
trigger times: 18
Loss after 20844235 batches: 0.0466
trigger times: 19
Loss after 20845172 batches: 0.0483
trigger times: 20
Loss after 20846109 batches: 0.0457
trigger times: 21
Loss after 20847046 batches: 0.0457
trigger times: 22
Loss after 20847983 batches: 0.0451
trigger times: 23
Loss after 20848920 batches: 0.0451
trigger times: 24
Loss after 20849857 batches: 0.0446
trigger times: 25
Early stopping!
Start to test process.
Loss after 20850794 batches: 0.0430
Time to train on one home:  52.55909538269043
train_results:  [0.08213193090377217, 0.05804704250025322, 0.04255319350922526, 0.04011089927103628, 0.038012115396848165, 0.03689554216616356, 0.03601429709364374, 0.03523272839573066, 0.03401813118363776, 0.033508403208926535, 0.03273922320832277, 0.032162070075287874, 0.031213877056718613, 0.03093800142524674, 0.030100551508447693, 0.029937661720089038, 0.02977997595640868, 0.02919253174856931, 0.029015750784499503, 0.028856189975002316]
test_results:  [[0.10082405805587769, -0.054764222263741, 0.1231860661470977, 1.1374510211962554, 0.9493533524329186, 36.445046007547695, 9752.355], [0.11666058003902435, 0.0033553729248713138, 0.22448578730774937, 1.1967830488395308, 0.8970421050311045, 38.34610234921426, 9214.982], [0.09502508491277695, 0.06306210793407208, 0.4091371338812714, 1.0743021801538923, 0.8433023433560337, 34.421695222129436, 8662.934], [0.09201239049434662, 0.0888807369859389, 0.4750409455392341, 0.9362492796710058, 0.8200639469571572, 29.998344927641377, 8424.214], [0.06690056622028351, 0.11779342491118161, 0.575479130561629, 0.7734026148743284, 0.7940407462593525, 24.780578113867872, 8156.887], [0.08562850952148438, 0.09116170711364147, 0.5687260656187799, 0.9169960265281519, 0.8180109409907265, 29.381451818830143, 8403.124], [0.0629526749253273, 0.11004705945059767, 0.5942846983018615, 0.8012983318796003, 0.8010129671558082, 25.674384238901233, 8228.51], [0.0780257135629654, 0.1035195067392124, 0.5885601973830207, 0.8781840137793461, 0.806888160052532, 28.13787687457617, 8288.863], [0.06482576578855515, 0.11552502542995946, 0.6041126568254397, 0.7994763119207765, 0.796082468981858, 25.61600493290161, 8177.861], [0.0757279247045517, 0.10266384027132536, 0.5909740425258612, 0.8803234656976516, 0.8076583271010261, 28.206427011804568, 8296.775], [0.06181953847408295, 0.10445801892372542, 0.5961459732150404, 0.831618823847048, 0.806043429565136, 26.645882531252276, 8280.187], [0.07333928346633911, 0.09887557787656209, 0.5904951093552152, 0.8805827319948272, 0.8110679704670578, 28.214734158180708, 8331.802], [0.06249981373548508, 0.1056225345966233, 0.6023961404422763, 0.8395811685763604, 0.8049953091236983, 26.90100386358231, 8269.419], [0.07111044973134995, 0.11364462231541128, 0.6015437535926366, 0.8360909977276793, 0.7977749276509928, 26.78917536742376, 8195.247], [0.06206701323390007, 0.09726051170449412, 0.5975866108942892, 0.8595424626232535, 0.8125216526577462, 27.540583297204083, 8346.734], [0.07088565826416016, 0.11302232012409985, 0.6007153971593981, 0.8537924311344753, 0.7983350331303676, 27.35634665030831, 8201.001], [0.06324874609708786, 0.09630308423125122, 0.5977169726517698, 0.8716301579538709, 0.8133833873815131, 27.927884907770363, 8355.587], [0.07241006940603256, 0.1018741345383698, 0.5940447527818379, 0.8890966213663465, 0.8083691035743891, 28.487527521645152, 8304.077], [0.06157456710934639, 0.09768669531039154, 0.5995392137772617, 0.860373560964417, 0.8121380691527804, 27.567212502959812, 8342.794], [0.06892590969800949, 0.10954055344572677, 0.5985059514769252, 0.8515056738163342, 0.8014688307189015, 27.28307670363395, 8233.193]]
Round_19_results:  [0.06892590969800949, 0.10954055344572677, 0.5985059514769252, 0.8515056738163342, 0.8014688307189015, 27.28307670363395, 8233.193]