{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbfc162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from data_loaders import make_train_data, make_test_val_data, make_model\n",
    "from train_fed import train, test\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "import lstm\n",
    "import config_file\n",
    "from clean_data import load_all_houses_with_device\n",
    "import random\n",
    "import optuna\n",
    "from data_loaders import PecanStreetDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lstm import LSTM\n",
    "from clean_data import normalize_y\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc13e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "\n",
    "config_ = config_file.load_hyperparameters(\"refrigerator1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d641b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679eb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "homes = load_all_houses_with_device(config_file.path, config_['appliance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41efa54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_update(client_model, optimizer, train_loader, epoch=config_['epochs']):\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = client_model(data)\n",
    "            loss = nn.MSELoss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return float(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d4473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_aggregate(global_model, client_models):\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k] for i in range(len(client_models))], 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f24effbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters, train_months, test_month, appliance, window_length, train_buildings,\n",
    "                   test_buildings, patience):\n",
    "    with wandb.init(project=\"jan27_FL_trials\", config=hyperparameters):\n",
    "        wandb.run.name = str(config_['appliance'])+\"_Test:\"+str(test_buildings)+\"_Train:\" + str(train_buildings)\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        # lengths = [85320, 132480, 132480, 132480, 132480, 132480, 132480]\n",
    "\n",
    "        global_model, criterion, optimizer = make_model(config)\n",
    "        \n",
    "        client_models = [lstm.LSTM(\n",
    "            config.in_channels,\n",
    "            config.out_channels,\n",
    "            config.kernel_size,\n",
    "            config.hidden_size_1,\n",
    "            config.hidden_size_2,\n",
    "            config.fc1\n",
    "        ).to(device) for _ in range(len(train_buildings))]\n",
    "        \n",
    "        for model in client_models:\n",
    "            model.load_state_dict(global_model.state_dict())\n",
    "            \n",
    "        optimizers = [torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr = config.learning_rate,\n",
    "            weight_decay = config.weight_decay\n",
    "        ) for model in client_models]\n",
    "        \n",
    "        print(global_model)\n",
    "        print(\"Window Length: \", window_length)\n",
    "        \n",
    "        wandb.watch(global_model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "        validation_loader, test_loader, test_val_seq_min, test_val_seq_max  = make_test_val_data(\n",
    "            config,\n",
    "            test_month,\n",
    "            appliance,\n",
    "            window_length,\n",
    "            test_buildings\n",
    "        )\n",
    "        \n",
    "        test_results = []\n",
    "        train_results = []\n",
    "        example_ct = 0\n",
    "        batch_ct = 0\n",
    "        all_epochs = 0\n",
    "        for r in range(20):\n",
    "            client_losses = 0.0\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            for i in range(len(train_buildings)):\n",
    "                #wandb.watch(client_models[i], criterion, log=\"all\", log_freq=10)\n",
    "                time_log = time.time()\n",
    "                #scheduler = torch.optim.lr_scheduler.StepLR(optimizers[i], step_size=50, gamma=0.9, verbose=True)\n",
    "                scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "                    optimizers[i],\n",
    "                    base_lr = 0.1*config_['learning_rate'],\n",
    "                    max_lr = 5*config_['learning_rate'],\n",
    "                    step_size_up = 50,\n",
    "                    step_size_down = 6000,\n",
    "                    gamma = 1,\n",
    "                    cycle_momentum=False,\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                train_loader, train_seq_min, train_seq_max = make_train_data(\n",
    "                    config,\n",
    "                    train_months,\n",
    "                    appliance,\n",
    "                    window_length,\n",
    "                    [train_buildings[i]]\n",
    "                )\n",
    "                \n",
    "                #loss += client_update(client_models[i], optimizer[i], train_loader, epochs = config.epochs)\n",
    "                example_ct, batch_ct, all_epochs, _, client_train_loss = train(\n",
    "                    client_models[i].to(device),\n",
    "                    train_loader,\n",
    "                    validation_loader,\n",
    "                    criterion,\n",
    "                    optimizers[i],\n",
    "                    config,\n",
    "                    example_ct,\n",
    "                    batch_ct,\n",
    "                    all_epochs,\n",
    "                    scheduler,\n",
    "                    test_val_seq_min,\n",
    "                    test_val_seq_max,\n",
    "                    train_seq_min,\n",
    "                    train_seq_max,\n",
    "                    patience\n",
    "                )\n",
    "                \n",
    "                client_losses += client_train_loss\n",
    "                \n",
    "                print(\"Time to train on one home: \", time.time() - time_log)\n",
    "            \n",
    "            client_losses = client_losses/len(train_buildings)\n",
    "            server_aggregate(global_model, client_models)\n",
    "            test_results.append(test(global_model, test_loader, criterion, test_val_seq_min, test_val_seq_max))\n",
    "            train_results.append(client_losses)\n",
    "            \n",
    "            print(\"train_results: \", train_results)\n",
    "            print(\"test_results: \", test_results)\n",
    "            print(\"Round_\"+str(r)+\"_results: \", test(global_model, test_loader, criterion, test_val_seq_min, test_val_seq_max))\n",
    "\n",
    "    return train_results, test_results, global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "412bd4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_ids = homes.dataid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af8d8a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([  142,   183,   335,   387,   526,   690,  1417,  2126,  2358,\n",
      "        2561,  3383,  3488,  3700,  3976,  3996,  5058,  6178,  6240,\n",
      "        6526,  6672,  7021,  7069,  7365,  8825,  9004,  9053,  9290,\n",
      "        9973, 10164, 10182, 10811, 10983, 11878], dtype=int64)"
     ]
    }
   ],
   "source": [
    "home_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1acbcafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33"
     ]
    }
   ],
   "source": [
    "len(home_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee412bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_select = [5,10,15,20,25,30,33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84f80f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_select = [5,10,15,20,25,30,33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ce4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_select = [5,10,15,20,25,30,33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0107af8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nilm/jan27_FL_trials/runs/227z8i41\" target=\"_blank\">earthy-violet-32</a></strong> to <a href=\"https://wandb.ai/nilm/jan27_FL_trials\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_results = {}\n",
    "random.seed(3)\n",
    "global_models = []\n",
    "\n",
    "for i in range(1):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    #train_homes = home_ids\n",
    "    train_homes = random.choices(home_ids, k=random_select[-1])\n",
    "    test_homes = [random.choice(train_homes)]\n",
    "    patience = 25\n",
    "    print(\"patience: \", patience)\n",
    "    print(\"training_home: \", train_homes)\n",
    "    print(\"test_home: \", test_homes)\n",
    "    train_results, test_results, global_model = model_pipeline(\n",
    "    config_,\n",
    "    'sept_oct_nov',\n",
    "    'dec',\n",
    "    config_['appliance'],\n",
    "    config_['window_size'],\n",
    "    train_homes,\n",
    "    test_homes,\n",
    "    patience)\n",
    "    global_models.append(global_model)\n",
    "    result = {\"Train_home_\"+str(train_homes)+\"_Test_home_\"+str(test_homes): test_results}\n",
    "    final_results.update(result)\n",
    "    print(final_results)\n",
    "    #torch.save(model.state_dict(), r\"C:\\Users\\aar245\\Desktop\\FL_models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
