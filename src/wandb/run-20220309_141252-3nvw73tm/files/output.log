LSTM(
  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))
  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))
  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))
  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))
  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))
  (linear1): Linear(in_features=23500, out_features=1024, bias=True)
  (linear2): Linear(in_features=1024, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (dropout): Dropout(p=0.2, inplace=False)
)
Window Length:  499
trigger times: 0
Loss after 131100 batches: 0.8572
trigger times: 0
Loss after 262200 batches: 0.4001
trigger times: 0
Loss after 393300 batches: 0.2811
trigger times: 0
Loss after 524400 batches: 0.2235
trigger times: 1
Loss after 655500 batches: 0.1861
trigger times: 2
Loss after 786600 batches: 0.1600
trigger times: 3
Loss after 917700 batches: 0.1387
trigger times: 0
Loss after 1048800 batches: 0.1182
trigger times: 1
Loss after 1179900 batches: 0.1016
trigger times: 0
Loss after 1311000 batches: 0.0904
trigger times: 1
Loss after 1442100 batches: 0.0797
trigger times: 0
Loss after 1573200 batches: 0.0709
trigger times: 1
Loss after 1704300 batches: 0.0644
trigger times: 2
Loss after 1835400 batches: 0.0586
trigger times: 3
Loss after 1966500 batches: 0.0541
trigger times: 4
Loss after 2097600 batches: 0.0500
trigger times: 5
Loss after 2228700 batches: 0.0477
trigger times: 6
Loss after 2359800 batches: 0.0439
trigger times: 0
Loss after 2490900 batches: 0.0423
trigger times: 1
Loss after 2622000 batches: 0.0407
trigger times: 2
Loss after 2753100 batches: 0.0383
trigger times: 3
Loss after 2884200 batches: 0.0355
trigger times: 4
Loss after 3015300 batches: 0.0345
trigger times: 0
Loss after 3146400 batches: 0.0334
trigger times: 1
Loss after 3277500 batches: 0.0320
trigger times: 0
Loss after 3408600 batches: 0.0303
trigger times: 1
Loss after 3539700 batches: 0.0298
trigger times: 2
Loss after 3670800 batches: 0.0292
trigger times: 3
Loss after 3801900 batches: 0.0272
trigger times: 0
Loss after 3933000 batches: 0.0267
trigger times: 0
Loss after 4064100 batches: 0.0259
trigger times: 1
Loss after 4195200 batches: 0.0253
trigger times: 0
Loss after 4326300 batches: 0.0245
trigger times: 1
Loss after 4457400 batches: 0.0240
trigger times: 0
Loss after 4588500 batches: 0.0234
trigger times: 1
Loss after 4719600 batches: 0.0230
trigger times: 0
Loss after 4850700 batches: 0.0225
trigger times: 1
Loss after 4981800 batches: 0.0217
trigger times: 2
Loss after 5112900 batches: 0.0208
trigger times: 3
Loss after 5244000 batches: 0.0207
trigger times: 4
Loss after 5375100 batches: 0.0208
trigger times: 0
Loss after 5506200 batches: 0.0196
trigger times: 1
Loss after 5637300 batches: 0.0195
trigger times: 0
Loss after 5768400 batches: 0.0196
trigger times: 0
Loss after 5899500 batches: 0.0185
trigger times: 1
Loss after 6030600 batches: 0.0182
trigger times: 2
Loss after 6161700 batches: 0.0175
trigger times: 0
Loss after 6292800 batches: 0.0176
trigger times: 1
Loss after 6423900 batches: 0.0176
trigger times: 0
Loss after 6555000 batches: 0.0180
trigger times: 1
Loss after 6686100 batches: 0.0169
trigger times: 2
Loss after 6817200 batches: 0.0173
trigger times: 3
Loss after 6948300 batches: 0.0168
trigger times: 4
Loss after 7079400 batches: 0.0160
trigger times: 0
Loss after 7210500 batches: 0.0164
trigger times: 1
Loss after 7341600 batches: 0.0156
trigger times: 0
Loss after 7472700 batches: 0.0158
trigger times: 1
Loss after 7603800 batches: 0.0157
trigger times: 2
Loss after 7734900 batches: 0.0153
trigger times: 3
Loss after 7866000 batches: 0.0150
trigger times: 4
Loss after 7997100 batches: 0.0151
trigger times: 5
Loss after 8128200 batches: 0.0146
trigger times: 0
Loss after 8259300 batches: 0.0145
trigger times: 1
Loss after 8390400 batches: 0.0148
trigger times: 2
Loss after 8521500 batches: 0.0143
trigger times: 0
Loss after 8652600 batches: 0.0143
trigger times: 1
Loss after 8783700 batches: 0.0136
trigger times: 2
Loss after 8914800 batches: 0.0137
trigger times: 3
Loss after 9045900 batches: 0.0134
trigger times: 4
Loss after 9177000 batches: 0.0135
trigger times: 5
Loss after 9308100 batches: 0.0138
trigger times: 0
Loss after 9439200 batches: 0.0137
trigger times: 1
Loss after 9570300 batches: 0.0132
trigger times: 2
Loss after 9701400 batches: 0.0132
trigger times: 3
Loss after 9832500 batches: 0.0126
trigger times: 4
Loss after 9963600 batches: 0.0125
trigger times: 5
Loss after 10094700 batches: 0.0126
trigger times: 6
Loss after 10225800 batches: 0.0123
trigger times: 7
Loss after 10356900 batches: 0.0124
trigger times: 8
Loss after 10488000 batches: 0.0120
trigger times: 9
Loss after 10619100 batches: 0.0123
trigger times: 10
Loss after 10750200 batches: 0.0124
trigger times: 11
Loss after 10881300 batches: 0.0119
trigger times: 12
Loss after 11012400 batches: 0.0119
trigger times: 13
Loss after 11143500 batches: 0.0119
trigger times: 14
Loss after 11274600 batches: 0.0118
trigger times: 15
Loss after 11405700 batches: 0.0116
trigger times: 16
Loss after 11536800 batches: 0.0116
trigger times: 0
Loss after 11667900 batches: 0.0114
trigger times: 1
Loss after 11799000 batches: 0.0112
trigger times: 2
Loss after 11930100 batches: 0.0113
trigger times: 3
Loss after 12061200 batches: 0.0117
trigger times: 4
Loss after 12192300 batches: 0.0114
trigger times: 5
Loss after 12323400 batches: 0.0110
trigger times: 6
Loss after 12454500 batches: 0.0112
trigger times: 7
Loss after 12585600 batches: 0.0111
trigger times: 8
Loss after 12716700 batches: 0.0107
trigger times: 9
Loss after 12847800 batches: 0.0114
trigger times: 10
Loss after 12978900 batches: 0.0113
trigger times: 11
Loss after 13110000 batches: 0.0111
trigger times: 12
Loss after 13241100 batches: 0.0107
trigger times: 13
Loss after 13372200 batches: 0.0108
trigger times: 14
Loss after 13503300 batches: 0.0108
trigger times: 15
Loss after 13634400 batches: 0.0106
trigger times: 16
Loss after 13765500 batches: 0.0103
trigger times: 17
Loss after 13896600 batches: 0.0102
trigger times: 18
Loss after 14027700 batches: 0.0102
trigger times: 19
Loss after 14158800 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 14289900 batches: 0.0102
Time to train on one home:  782.5062253475189
trigger times: 0
Loss after 14392500 batches: 0.9064
trigger times: 0
Loss after 14495100 batches: 0.6998
trigger times: 0
Loss after 14597700 batches: 0.5976
trigger times: 1
Loss after 14700300 batches: 0.5080
trigger times: 2
Loss after 14802900 batches: 0.4539
trigger times: 0
Loss after 14905500 batches: 0.4054
trigger times: 1
Loss after 15008100 batches: 0.3526
trigger times: 2
Loss after 15110700 batches: 0.3212
trigger times: 3
Loss after 15213300 batches: 0.2897
trigger times: 4
Loss after 15315900 batches: 0.2598
trigger times: 5
Loss after 15418500 batches: 0.2245
trigger times: 6
Loss after 15521100 batches: 0.2004
trigger times: 7
Loss after 15623700 batches: 0.1985
trigger times: 8
Loss after 15726300 batches: 0.1912
trigger times: 9
Loss after 15828900 batches: 0.1711
trigger times: 10
Loss after 15931500 batches: 0.1555
trigger times: 11
Loss after 16034100 batches: 0.1440
trigger times: 12
Loss after 16136700 batches: 0.1406
trigger times: 13
Loss after 16239300 batches: 0.1263
trigger times: 14
Loss after 16341900 batches: 0.1182
trigger times: 15
Loss after 16444500 batches: 0.1142
trigger times: 16
Loss after 16547100 batches: 0.1147
trigger times: 17
Loss after 16649700 batches: 0.1082
trigger times: 18
Loss after 16752300 batches: 0.0973
trigger times: 19
Loss after 16854900 batches: 0.0953
trigger times: 20
Early stopping!
Start to test process.
Loss after 16957500 batches: 0.0886
Time to train on one home:  159.01223230361938
trigger times: 0
Loss after 17088600 batches: 0.7791
trigger times: 0
Loss after 17219700 batches: 0.4977
trigger times: 1
Loss after 17350800 batches: 0.3996
trigger times: 2
Loss after 17481900 batches: 0.3283
trigger times: 3
Loss after 17613000 batches: 0.2729
trigger times: 4
Loss after 17744100 batches: 0.2334
trigger times: 5
Loss after 17875200 batches: 0.2026
trigger times: 6
Loss after 18006300 batches: 0.1767
trigger times: 7
Loss after 18137400 batches: 0.1534
trigger times: 8
Loss after 18268500 batches: 0.1331
trigger times: 9
Loss after 18399600 batches: 0.1170
trigger times: 10
Loss after 18530700 batches: 0.1015
trigger times: 11
Loss after 18661800 batches: 0.0927
trigger times: 12
Loss after 18792900 batches: 0.0816
trigger times: 13
Loss after 18924000 batches: 0.0754
trigger times: 14
Loss after 19055100 batches: 0.0683
trigger times: 15
Loss after 19186200 batches: 0.0656
trigger times: 16
Loss after 19317300 batches: 0.0602
trigger times: 17
Loss after 19448400 batches: 0.0560
trigger times: 18
Loss after 19579500 batches: 0.0520
trigger times: 19
Loss after 19710600 batches: 0.0500
trigger times: 20
Early stopping!
Start to test process.
Loss after 19841700 batches: 0.0466
Time to train on one home:  167.2746615409851
trigger times: 0
Loss after 19972800 batches: 0.9469
trigger times: 0
Loss after 20103900 batches: 0.7685
trigger times: 1
Loss after 20235000 batches: 0.6669
trigger times: 2
Loss after 20366100 batches: 0.5995
trigger times: 3
Loss after 20497200 batches: 0.5211
trigger times: 4
Loss after 20628300 batches: 0.4194
trigger times: 5
Loss after 20759400 batches: 0.3178
trigger times: 6
Loss after 20890500 batches: 0.2394
trigger times: 7
Loss after 21021600 batches: 0.1912
trigger times: 8
Loss after 21152700 batches: 0.1559
trigger times: 9
Loss after 21283800 batches: 0.1336
trigger times: 10
Loss after 21414900 batches: 0.1192
trigger times: 11
Loss after 21546000 batches: 0.1100
trigger times: 12
Loss after 21677100 batches: 0.1022
trigger times: 13
Loss after 21808200 batches: 0.0954
trigger times: 14
Loss after 21939300 batches: 0.0889
trigger times: 15
Loss after 22070400 batches: 0.0851
trigger times: 16
Loss after 22201500 batches: 0.0811
trigger times: 17
Loss after 22332600 batches: 0.0787
trigger times: 18
Loss after 22463700 batches: 0.0749
trigger times: 19
Loss after 22594800 batches: 0.0712
trigger times: 20
Early stopping!
Start to test process.
Loss after 22725900 batches: 0.0688
Time to train on one home:  167.34506750106812
trigger times: 0
Loss after 22854540 batches: 0.7733
trigger times: 0
Loss after 22983180 batches: 0.4748
trigger times: 0
Loss after 23111820 batches: 0.4124
trigger times: 0
Loss after 23240460 batches: 0.3504
trigger times: 0
Loss after 23369100 batches: 0.2810
trigger times: 1
Loss after 23497740 batches: 0.2271
trigger times: 0
Loss after 23626380 batches: 0.1819
trigger times: 1
Loss after 23755020 batches: 0.1498
trigger times: 2
Loss after 23883660 batches: 0.1310
trigger times: 3
Loss after 24012300 batches: 0.1119
trigger times: 4
Loss after 24140940 batches: 0.1003
trigger times: 5
Loss after 24269580 batches: 0.0904
trigger times: 6
Loss after 24398220 batches: 0.0825
trigger times: 7
Loss after 24526860 batches: 0.0744
trigger times: 8
Loss after 24655500 batches: 0.0706
trigger times: 9
Loss after 24784140 batches: 0.0658
trigger times: 10
Loss after 24912780 batches: 0.0609
trigger times: 11
Loss after 25041420 batches: 0.0592
trigger times: 12
Loss after 25170060 batches: 0.0565
trigger times: 13
Loss after 25298700 batches: 0.0540
trigger times: 14
Loss after 25427340 batches: 0.0511
trigger times: 15
Loss after 25555980 batches: 0.0494
trigger times: 16
Loss after 25684620 batches: 0.0472
trigger times: 17
Loss after 25813260 batches: 0.0453
trigger times: 18
Loss after 25941900 batches: 0.0431
trigger times: 19
Loss after 26070540 batches: 0.0419
trigger times: 20
Early stopping!
Start to test process.
Loss after 26199180 batches: 0.0420
Time to train on one home:  199.7084095478058
trigger times: 0
Loss after 26330280 batches: 0.9325
trigger times: 1
Loss after 26461380 batches: 0.7964
trigger times: 0
Loss after 26592480 batches: 0.7277
trigger times: 1
Loss after 26723580 batches: 0.6653
trigger times: 2
Loss after 26854680 batches: 0.5795
trigger times: 3
Loss after 26985780 batches: 0.4814
trigger times: 4
Loss after 27116880 batches: 0.3669
trigger times: 5
Loss after 27247980 batches: 0.2823
trigger times: 6
Loss after 27379080 batches: 0.2242
trigger times: 7
Loss after 27510180 batches: 0.1834
trigger times: 8
Loss after 27641280 batches: 0.1594
trigger times: 9
Loss after 27772380 batches: 0.1410
trigger times: 10
Loss after 27903480 batches: 0.1309
trigger times: 11
Loss after 28034580 batches: 0.1162
trigger times: 12
Loss after 28165680 batches: 0.1082
trigger times: 13
Loss after 28296780 batches: 0.1021
trigger times: 14
Loss after 28427880 batches: 0.0974
trigger times: 15
Loss after 28558980 batches: 0.0910
trigger times: 16
Loss after 28690080 batches: 0.0870
trigger times: 17
Loss after 28821180 batches: 0.0845
trigger times: 18
Loss after 28952280 batches: 0.0811
trigger times: 19
Loss after 29083380 batches: 0.0761
trigger times: 20
Early stopping!
Start to test process.
Loss after 29214480 batches: 0.0742
Time to train on one home:  174.34371638298035
trigger times: 0
Loss after 29345580 batches: 0.9829
trigger times: 0
Loss after 29476680 batches: 0.8521
trigger times: 0
Loss after 29607780 batches: 0.7896
trigger times: 1
Loss after 29738880 batches: 0.7356
trigger times: 2
Loss after 29869980 batches: 0.6662
trigger times: 3
Loss after 30001080 batches: 0.5825
trigger times: 4
Loss after 30132180 batches: 0.5206
trigger times: 5
Loss after 30263280 batches: 0.4398
trigger times: 6
Loss after 30394380 batches: 0.3962
trigger times: 7
Loss after 30525480 batches: 0.3541
trigger times: 8
Loss after 30656580 batches: 0.3161
trigger times: 9
Loss after 30787680 batches: 0.2820
trigger times: 10
Loss after 30918780 batches: 0.2569
trigger times: 11
Loss after 31049880 batches: 0.2409
trigger times: 12
Loss after 31180980 batches: 0.2237
trigger times: 13
Loss after 31312080 batches: 0.2089
trigger times: 14
Loss after 31443180 batches: 0.1954
trigger times: 15
Loss after 31574280 batches: 0.1743
trigger times: 16
Loss after 31705380 batches: 0.1676
trigger times: 17
Loss after 31836480 batches: 0.1605
trigger times: 18
Loss after 31967580 batches: 0.1508
trigger times: 19
Loss after 32098680 batches: 0.1398
trigger times: 20
Early stopping!
Start to test process.
Loss after 32229780 batches: 0.1364
Time to train on one home:  174.05368542671204
trigger times: 0
Loss after 32360880 batches: 0.8451
trigger times: 0
Loss after 32491980 batches: 0.3883
trigger times: 0
Loss after 32623080 batches: 0.2627
trigger times: 0
Loss after 32754180 batches: 0.2044
trigger times: 1
Loss after 32885280 batches: 0.1637
trigger times: 2
Loss after 33016380 batches: 0.1333
trigger times: 3
Loss after 33147480 batches: 0.1067
trigger times: 4
Loss after 33278580 batches: 0.0899
trigger times: 5
Loss after 33409680 batches: 0.0746
trigger times: 6
Loss after 33540780 batches: 0.0650
trigger times: 7
Loss after 33671880 batches: 0.0572
trigger times: 8
Loss after 33802980 batches: 0.0514
trigger times: 9
Loss after 33934080 batches: 0.0481
trigger times: 10
Loss after 34065180 batches: 0.0439
trigger times: 11
Loss after 34196280 batches: 0.0408
trigger times: 12
Loss after 34327380 batches: 0.0389
trigger times: 13
Loss after 34458480 batches: 0.0358
trigger times: 14
Loss after 34589580 batches: 0.0339
trigger times: 15
Loss after 34720680 batches: 0.0318
trigger times: 16
Loss after 34851780 batches: 0.0306
trigger times: 17
Loss after 34982880 batches: 0.0298
trigger times: 18
Loss after 35113980 batches: 0.0283
trigger times: 19
Loss after 35245080 batches: 0.0280
trigger times: 20
Early stopping!
Start to test process.
Loss after 35376180 batches: 0.0263
Time to train on one home:  180.88333106040955
train_results:  [0.061632601527815266]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225]]
Round_0_results:  [0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 277 < 278; dropping {'Training_Loss': 0.5067578517603424, 'Validation_Loss': 0.4045330252912309, 'Training_R2': 0.4907998675217561, 'Validation_R2': 0.6237003655628319, 'Training_F1': 0.5946277324043776, 'Validation_F1': 0.6710392383876996, 'Training_NEP': 0.824057922124858, 'Validation_NEP': 0.6883989567034843, 'Training_NDE': 0.38226557776777714, 'Validation_NDE': 0.2996651925656077, 'Training_MAE': 27.291164097687744, 'Validation_MAE': 18.878755385132248, 'Training_MSE': 1681.9086, 'Validation_MSE': 1106.6553}.
trigger times: 0
Loss after 35507280 batches: 0.5068
trigger times: 0
Loss after 35638380 batches: 0.2185
trigger times: 1
Loss after 35769480 batches: 0.1746
trigger times: 2
Loss after 35900580 batches: 0.1479
trigger times: 0
Loss after 36031680 batches: 0.1268
trigger times: 1
Loss after 36162780 batches: 0.1102
trigger times: 2
Loss after 36293880 batches: 0.0967
trigger times: 0
Loss after 36424980 batches: 0.0857
trigger times: 1
Loss after 36556080 batches: 0.0785
trigger times: 2
Loss after 36687180 batches: 0.0727
trigger times: 0
Loss after 36818280 batches: 0.0666
trigger times: 0
Loss after 36949380 batches: 0.0631
trigger times: 0
Loss after 37080480 batches: 0.0598
trigger times: 0
Loss after 37211580 batches: 0.0559
trigger times: 1
Loss after 37342680 batches: 0.0543
trigger times: 2
Loss after 37473780 batches: 0.0515
trigger times: 3
Loss after 37604880 batches: 0.0488
trigger times: 4
Loss after 37735980 batches: 0.0471
trigger times: 5
Loss after 37867080 batches: 0.0464
trigger times: 6
Loss after 37998180 batches: 0.0441
trigger times: 0
Loss after 38129280 batches: 0.0428
trigger times: 1
Loss after 38260380 batches: 0.0418
trigger times: 2
Loss after 38391480 batches: 0.0410
trigger times: 3
Loss after 38522580 batches: 0.0390
trigger times: 4
Loss after 38653680 batches: 0.0382
trigger times: 0
Loss after 38784780 batches: 0.0373
trigger times: 1
Loss after 38915880 batches: 0.0362
trigger times: 2
Loss after 39046980 batches: 0.0358
trigger times: 3
Loss after 39178080 batches: 0.0354
trigger times: 0
Loss after 39309180 batches: 0.0339
trigger times: 0
Loss after 39440280 batches: 0.0329
trigger times: 1
Loss after 39571380 batches: 0.0327
trigger times: 0
Loss after 39702480 batches: 0.0317
trigger times: 1
Loss after 39833580 batches: 0.0313
trigger times: 0
Loss after 39964680 batches: 0.0311
trigger times: 0
Loss after 40095780 batches: 0.0307
trigger times: 0
Loss after 40226880 batches: 0.0298
trigger times: 0
Loss after 40357980 batches: 0.0291
trigger times: 1
Loss after 40489080 batches: 0.0286
trigger times: 2
Loss after 40620180 batches: 0.0283
trigger times: 3
Loss after 40751280 batches: 0.0282
trigger times: 4
Loss after 40882380 batches: 0.0277
trigger times: 0
Loss after 41013480 batches: 0.0270
trigger times: 1
Loss after 41144580 batches: 0.0264
trigger times: 2
Loss after 41275680 batches: 0.0259
trigger times: 3
Loss after 41406780 batches: 0.0254
trigger times: 4
Loss after 41537880 batches: 0.0246
trigger times: 5
Loss after 41668980 batches: 0.0245
trigger times: 6
Loss after 41800080 batches: 0.0244
trigger times: 7
Loss after 41931180 batches: 0.0241
trigger times: 8
Loss after 42062280 batches: 0.0235
trigger times: 9
Loss after 42193380 batches: 0.0239
trigger times: 10
Loss after 42324480 batches: 0.0231
trigger times: 11
Loss after 42455580 batches: 0.0223
trigger times: 0
Loss after 42586680 batches: 0.0225
trigger times: 1
Loss after 42717780 batches: 0.0223
trigger times: 2
Loss after 42848880 batches: 0.0215
trigger times: 3
Loss after 42979980 batches: 0.0209
trigger times: 4
Loss after 43111080 batches: 0.0209
trigger times: 5
Loss after 43242180 batches: 0.0208
trigger times: 6
Loss after 43373280 batches: 0.0207
trigger times: 7
Loss after 43504380 batches: 0.0202
trigger times: 8
Loss after 43635480 batches: 0.0200
trigger times: 0
Loss after 43766580 batches: 0.0199
trigger times: 1
Loss after 43897680 batches: 0.0206
trigger times: 2
Loss after 44028780 batches: 0.0194
trigger times: 3
Loss after 44159880 batches: 0.0191
trigger times: 0
Loss after 44290980 batches: 0.0191
trigger times: 1
Loss after 44422080 batches: 0.0190
trigger times: 2
Loss after 44553180 batches: 0.0187
trigger times: 3
Loss after 44684280 batches: 0.0183
trigger times: 4
Loss after 44815380 batches: 0.0182
trigger times: 5
Loss after 44946480 batches: 0.0180
trigger times: 6
Loss after 45077580 batches: 0.0175
trigger times: 0
Loss after 45208680 batches: 0.0176
trigger times: 1
Loss after 45339780 batches: 0.0177
trigger times: 2
Loss after 45470880 batches: 0.0171
trigger times: 3
Loss after 45601980 batches: 0.0171
trigger times: 4
Loss after 45733080 batches: 0.0177
trigger times: 5
Loss after 45864180 batches: 0.0170
trigger times: 6
Loss after 45995280 batches: 0.0168
trigger times: 7
Loss after 46126380 batches: 0.0165
trigger times: 8
Loss after 46257480 batches: 0.0163
trigger times: 9
Loss after 46388580 batches: 0.0164
trigger times: 10
Loss after 46519680 batches: 0.0161
trigger times: 11
Loss after 46650780 batches: 0.0160
trigger times: 12
Loss after 46781880 batches: 0.0158
trigger times: 13
Loss after 46912980 batches: 0.0158
trigger times: 14
Loss after 47044080 batches: 0.0158
trigger times: 15
Loss after 47175180 batches: 0.0156
trigger times: 16
Loss after 47306280 batches: 0.0154
trigger times: 0
Loss after 47437380 batches: 0.0153
trigger times: 1
Loss after 47568480 batches: 0.0153
trigger times: 2
Loss after 47699580 batches: 0.0153
trigger times: 3
Loss after 47830680 batches: 0.0151
trigger times: 4
Loss after 47961780 batches: 0.0146
trigger times: 0
Loss after 48092880 batches: 0.0150
trigger times: 1
Loss after 48223980 batches: 0.0143
trigger times: 2
Loss after 48355080 batches: 0.0146
trigger times: 3
Loss after 48486180 batches: 0.0145
trigger times: 4
Loss after 48617280 batches: 0.0142
trigger times: 5
Loss after 48748380 batches: 0.0142
trigger times: 6
Loss after 48879480 batches: 0.0142
trigger times: 7
Loss after 49010580 batches: 0.0139
trigger times: 8
Loss after 49141680 batches: 0.0140
trigger times: 9
Loss after 49272780 batches: 0.0138
trigger times: 10
Loss after 49403880 batches: 0.0139
trigger times: 11
Loss after 49534980 batches: 0.0141
trigger times: 12
Loss after 49666080 batches: 0.0138
trigger times: 13
Loss after 49797180 batches: 0.0136
trigger times: 14
Loss after 49928280 batches: 0.0137
trigger times: 15
Loss after 50059380 batches: 0.0132
trigger times: 16
Loss after 50190480 batches: 0.0131
trigger times: 17
Loss after 50321580 batches: 0.0129
trigger times: 18
Loss after 50452680 batches: 0.0133
trigger times: 0
Loss after 50583780 batches: 0.0130
trigger times: 1
Loss after 50714880 batches: 0.0129
trigger times: 2
Loss after 50845980 batches: 0.0129
trigger times: 3
Loss after 50977080 batches: 0.0130
trigger times: 4
Loss after 51108180 batches: 0.0127
trigger times: 5
Loss after 51239280 batches: 0.0126
trigger times: 6
Loss after 51370380 batches: 0.0126
trigger times: 7
Loss after 51501480 batches: 0.0124
trigger times: 8
Loss after 51632580 batches: 0.0123
trigger times: 9
Loss after 51763680 batches: 0.0123
trigger times: 10
Loss after 51894780 batches: 0.0123
trigger times: 11
Loss after 52025880 batches: 0.0121
trigger times: 12
Loss after 52156980 batches: 0.0120
trigger times: 13
Loss after 52288080 batches: 0.0118
trigger times: 14
Loss after 52419180 batches: 0.0118
trigger times: 15
Loss after 52550280 batches: 0.0118
trigger times: 16
Loss after 52681380 batches: 0.0120
trigger times: 17
Loss after 52812480 batches: 0.0117
trigger times: 18
Loss after 52943580 batches: 0.0118
trigger times: 19
Loss after 53074680 batches: 0.0115
trigger times: 20
Early stopping!
Start to test process.
Loss after 53205780 batches: 0.0113
Time to train on one home:  971.0823476314545
trigger times: 0
Loss after 53308380 batches: 0.7667
trigger times: 0
Loss after 53410980 batches: 0.5569
trigger times: 0
Loss after 53513580 batches: 0.4682
trigger times: 1
Loss after 53616180 batches: 0.3995
trigger times: 2
Loss after 53718780 batches: 0.3347
trigger times: 3
Loss after 53821380 batches: 0.2906
trigger times: 4
Loss after 53923980 batches: 0.2562
trigger times: 5
Loss after 54026580 batches: 0.2434
trigger times: 6
Loss after 54129180 batches: 0.2323
trigger times: 7
Loss after 54231780 batches: 0.2016
trigger times: 8
Loss after 54334380 batches: 0.1864
trigger times: 9
Loss after 54436980 batches: 0.1794
trigger times: 10
Loss after 54539580 batches: 0.1616
trigger times: 11
Loss after 54642180 batches: 0.1527
trigger times: 12
Loss after 54744780 batches: 0.1478
trigger times: 13
Loss after 54847380 batches: 0.1430
trigger times: 14
Loss after 54949980 batches: 0.1412
trigger times: 15
Loss after 55052580 batches: 0.1344
trigger times: 16
Loss after 55155180 batches: 0.1254
trigger times: 17
Loss after 55257780 batches: 0.1239
trigger times: 18
Loss after 55360380 batches: 0.1252
trigger times: 19
Loss after 55462980 batches: 0.1224
trigger times: 20
Early stopping!
Start to test process.
Loss after 55565580 batches: 0.1443
Time to train on one home:  142.20769548416138
trigger times: 0
Loss after 55696680 batches: 0.5570
trigger times: 1
Loss after 55827780 batches: 0.3541
trigger times: 2
Loss after 55958880 batches: 0.2744
trigger times: 3
Loss after 56089980 batches: 0.2154
trigger times: 4
Loss after 56221080 batches: 0.1742
trigger times: 5
Loss after 56352180 batches: 0.1476
trigger times: 6
Loss after 56483280 batches: 0.1299
trigger times: 7
Loss after 56614380 batches: 0.1157
trigger times: 8
Loss after 56745480 batches: 0.1054
trigger times: 9
Loss after 56876580 batches: 0.0976
trigger times: 10
Loss after 57007680 batches: 0.0910
trigger times: 11
Loss after 57138780 batches: 0.0849
trigger times: 12
Loss after 57269880 batches: 0.0802
trigger times: 13
Loss after 57400980 batches: 0.0760
trigger times: 14
Loss after 57532080 batches: 0.0724
trigger times: 15
Loss after 57663180 batches: 0.0689
trigger times: 16
Loss after 57794280 batches: 0.0668
trigger times: 17
Loss after 57925380 batches: 0.0642
trigger times: 18
Loss after 58056480 batches: 0.0612
trigger times: 19
Loss after 58187580 batches: 0.0598
trigger times: 20
Early stopping!
Start to test process.
Loss after 58318680 batches: 0.0580
Time to train on one home:  160.26044178009033
trigger times: 0
Loss after 58449780 batches: 0.7889
trigger times: 1
Loss after 58580880 batches: 0.6156
trigger times: 2
Loss after 58711980 batches: 0.4984
trigger times: 3
Loss after 58843080 batches: 0.3869
trigger times: 4
Loss after 58974180 batches: 0.3015
trigger times: 5
Loss after 59105280 batches: 0.2481
trigger times: 6
Loss after 59236380 batches: 0.2099
trigger times: 7
Loss after 59367480 batches: 0.1857
trigger times: 8
Loss after 59498580 batches: 0.1689
trigger times: 9
Loss after 59629680 batches: 0.1541
trigger times: 10
Loss after 59760780 batches: 0.1418
trigger times: 11
Loss after 59891880 batches: 0.1346
trigger times: 12
Loss after 60022980 batches: 0.1263
trigger times: 13
Loss after 60154080 batches: 0.1185
trigger times: 14
Loss after 60285180 batches: 0.1125
trigger times: 15
Loss after 60416280 batches: 0.1084
trigger times: 16
Loss after 60547380 batches: 0.1052
trigger times: 17
Loss after 60678480 batches: 0.1005
trigger times: 18
Loss after 60809580 batches: 0.0979
trigger times: 19
Loss after 60940680 batches: 0.0942
trigger times: 20
Early stopping!
Start to test process.
Loss after 61071780 batches: 0.0913
Time to train on one home:  159.91728830337524
trigger times: 0
Loss after 61200420 batches: 0.4978
trigger times: 0
Loss after 61329060 batches: 0.3180
trigger times: 1
Loss after 61457700 batches: 0.2325
trigger times: 2
Loss after 61586340 batches: 0.1841
trigger times: 3
Loss after 61714980 batches: 0.1516
trigger times: 4
Loss after 61843620 batches: 0.1284
trigger times: 5
Loss after 61972260 batches: 0.1111
trigger times: 6
Loss after 62100900 batches: 0.0998
trigger times: 7
Loss after 62229540 batches: 0.0920
trigger times: 8
Loss after 62358180 batches: 0.0837
trigger times: 9
Loss after 62486820 batches: 0.0770
trigger times: 10
Loss after 62615460 batches: 0.0725
trigger times: 11
Loss after 62744100 batches: 0.0680
trigger times: 12
Loss after 62872740 batches: 0.0645
trigger times: 13
Loss after 63001380 batches: 0.0628
trigger times: 14
Loss after 63130020 batches: 0.0577
trigger times: 15
Loss after 63258660 batches: 0.0570
trigger times: 16
Loss after 63387300 batches: 0.0541
trigger times: 17
Loss after 63515940 batches: 0.0527
trigger times: 18
Loss after 63644580 batches: 0.0508
trigger times: 19
Loss after 63773220 batches: 0.0487
trigger times: 20
Early stopping!
Start to test process.
Loss after 63901860 batches: 0.0476
Time to train on one home:  164.2001793384552
trigger times: 0
Loss after 64032960 batches: 0.7818
trigger times: 1
Loss after 64164060 batches: 0.5461
trigger times: 2
Loss after 64295160 batches: 0.4082
trigger times: 3
Loss after 64426260 batches: 0.2842
trigger times: 4
Loss after 64557360 batches: 0.2115
trigger times: 5
Loss after 64688460 batches: 0.1722
trigger times: 6
Loss after 64819560 batches: 0.1448
trigger times: 7
Loss after 64950660 batches: 0.1294
trigger times: 8
Loss after 65081760 batches: 0.1161
trigger times: 9
Loss after 65212860 batches: 0.1071
trigger times: 10
Loss after 65343960 batches: 0.0998
trigger times: 11
Loss after 65475060 batches: 0.0948
trigger times: 12
Loss after 65606160 batches: 0.0899
trigger times: 13
Loss after 65737260 batches: 0.0857
trigger times: 14
Loss after 65868360 batches: 0.0820
trigger times: 15
Loss after 65999460 batches: 0.0787
trigger times: 16
Loss after 66130560 batches: 0.0759
trigger times: 17
Loss after 66261660 batches: 0.0738
trigger times: 18
Loss after 66392760 batches: 0.0726
trigger times: 19
Loss after 66523860 batches: 0.0701
trigger times: 20
Early stopping!
Start to test process.
Loss after 66654960 batches: 0.0674
Time to train on one home:  160.01312637329102
trigger times: 0
Loss after 66786060 batches: 0.8525
trigger times: 1
Loss after 66917160 batches: 0.7167
trigger times: 2
Loss after 67048260 batches: 0.6182
trigger times: 3
Loss after 67179360 batches: 0.5230
trigger times: 4
Loss after 67310460 batches: 0.4493
trigger times: 5
Loss after 67441560 batches: 0.4006
trigger times: 6
Loss after 67572660 batches: 0.3535
trigger times: 7
Loss after 67703760 batches: 0.3259
trigger times: 8
Loss after 67834860 batches: 0.2953
trigger times: 9
Loss after 67965960 batches: 0.2652
trigger times: 10
Loss after 68097060 batches: 0.2556
trigger times: 11
Loss after 68228160 batches: 0.2319
trigger times: 12
Loss after 68359260 batches: 0.2160
trigger times: 13
Loss after 68490360 batches: 0.1987
trigger times: 14
Loss after 68621460 batches: 0.1921
trigger times: 15
Loss after 68752560 batches: 0.1891
trigger times: 16
Loss after 68883660 batches: 0.1736
trigger times: 17
Loss after 69014760 batches: 0.1627
trigger times: 18
Loss after 69145860 batches: 0.1550
trigger times: 19
Loss after 69276960 batches: 0.1476
trigger times: 20
Early stopping!
Start to test process.
Loss after 69408060 batches: 0.1414
Time to train on one home:  160.12565803527832
trigger times: 0
Loss after 69539160 batches: 0.4195
trigger times: 1
Loss after 69670260 batches: 0.2020
trigger times: 2
Loss after 69801360 batches: 0.1445
trigger times: 3
Loss after 69932460 batches: 0.1068
trigger times: 4
Loss after 70063560 batches: 0.0840
trigger times: 5
Loss after 70194660 batches: 0.0688
trigger times: 6
Loss after 70325760 batches: 0.0594
trigger times: 7
Loss after 70456860 batches: 0.0524
trigger times: 8
Loss after 70587960 batches: 0.0489
trigger times: 9
Loss after 70719060 batches: 0.0446
trigger times: 10
Loss after 70850160 batches: 0.0418
trigger times: 11
Loss after 70981260 batches: 0.0393
trigger times: 12
Loss after 71112360 batches: 0.0374
trigger times: 13
Loss after 71243460 batches: 0.0351
trigger times: 14
Loss after 71374560 batches: 0.0339
trigger times: 15
Loss after 71505660 batches: 0.0328
trigger times: 16
Loss after 71636760 batches: 0.0317
trigger times: 17
Loss after 71767860 batches: 0.0307
trigger times: 18
Loss after 71898960 batches: 0.0291
trigger times: 19
Loss after 72030060 batches: 0.0291
trigger times: 20
Early stopping!
Start to test process.
Loss after 72161160 batches: 0.0283
Time to train on one home:  159.69117379188538
train_results:  [0.061632601527815266, 0.0737033663785427]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503]]
Round_1_results:  [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 563 < 564; dropping {'Training_Loss': 0.30634610129977174, 'Validation_Loss': 0.3008953299787309, 'Training_R2': 0.6922585870649136, 'Validation_R2': 0.7200697038665651, 'Training_F1': 0.6953825115487783, 'Validation_F1': 0.6990803205718374, 'Training_NEP': 0.608476756959234, 'Validation_NEP': 0.6437088087568256, 'Training_NDE': 0.23102694110891522, 'Validation_NDE': 0.2229217315643712, 'Training_MAE': 20.151543450955668, 'Validation_MAE': 17.65316612036852, 'Training_MSE': 1016.4823, 'Validation_MSE': 823.24384}.
trigger times: 0
Loss after 72292260 batches: 0.3063
trigger times: 0
Loss after 72423360 batches: 0.1330
trigger times: 0
Loss after 72554460 batches: 0.0910
trigger times: 0
Loss after 72685560 batches: 0.0698
trigger times: 1
Loss after 72816660 batches: 0.0592
trigger times: 2
Loss after 72947760 batches: 0.0515
trigger times: 0
Loss after 73078860 batches: 0.0465
trigger times: 1
Loss after 73209960 batches: 0.0436
trigger times: 2
Loss after 73341060 batches: 0.0398
trigger times: 3
Loss after 73472160 batches: 0.0379
trigger times: 0
Loss after 73603260 batches: 0.0362
trigger times: 0
Loss after 73734360 batches: 0.0346
trigger times: 0
Loss after 73865460 batches: 0.0329
trigger times: 0
Loss after 73996560 batches: 0.0321
trigger times: 1
Loss after 74127660 batches: 0.0308
trigger times: 2
Loss after 74258760 batches: 0.0299
trigger times: 0
Loss after 74389860 batches: 0.0284
trigger times: 1
Loss after 74520960 batches: 0.0278
trigger times: 2
Loss after 74652060 batches: 0.0272
trigger times: 0
Loss after 74783160 batches: 0.0259
trigger times: 1
Loss after 74914260 batches: 0.0251
trigger times: 2
Loss after 75045360 batches: 0.0246
trigger times: 0
Loss after 75176460 batches: 0.0243
trigger times: 0
Loss after 75307560 batches: 0.0241
trigger times: 1
Loss after 75438660 batches: 0.0235
trigger times: 2
Loss after 75569760 batches: 0.0226
trigger times: 3
Loss after 75700860 batches: 0.0222
trigger times: 4
Loss after 75831960 batches: 0.0219
trigger times: 5
Loss after 75963060 batches: 0.0212
trigger times: 6
Loss after 76094160 batches: 0.0211
trigger times: 7
Loss after 76225260 batches: 0.0203
trigger times: 0
Loss after 76356360 batches: 0.0202
trigger times: 1
Loss after 76487460 batches: 0.0202
trigger times: 2
Loss after 76618560 batches: 0.0205
trigger times: 3
Loss after 76749660 batches: 0.0199
trigger times: 4
Loss after 76880760 batches: 0.0190
trigger times: 5
Loss after 77011860 batches: 0.0189
trigger times: 6
Loss after 77142960 batches: 0.0188
trigger times: 7
Loss after 77274060 batches: 0.0186
trigger times: 8
Loss after 77405160 batches: 0.0179
trigger times: 9
Loss after 77536260 batches: 0.0177
trigger times: 0
Loss after 77667360 batches: 0.0174
trigger times: 1
Loss after 77798460 batches: 0.0174
trigger times: 2
Loss after 77929560 batches: 0.0174
trigger times: 3
Loss after 78060660 batches: 0.0171
trigger times: 4
Loss after 78191760 batches: 0.0165
trigger times: 5
Loss after 78322860 batches: 0.0165
trigger times: 0
Loss after 78453960 batches: 0.0165
trigger times: 0
Loss after 78585060 batches: 0.0165
trigger times: 1
Loss after 78716160 batches: 0.0161
trigger times: 2
Loss after 78847260 batches: 0.0159
trigger times: 3
Loss after 78978360 batches: 0.0156
trigger times: 0
Loss after 79109460 batches: 0.0158
trigger times: 1
Loss after 79240560 batches: 0.0151
trigger times: 2
Loss after 79371660 batches: 0.0151
trigger times: 3
Loss after 79502760 batches: 0.0150
trigger times: 4
Loss after 79633860 batches: 0.0149
trigger times: 5
Loss after 79764960 batches: 0.0148
trigger times: 6
Loss after 79896060 batches: 0.0149
trigger times: 7
Loss after 80027160 batches: 0.0142
trigger times: 8
Loss after 80158260 batches: 0.0143
trigger times: 9
Loss after 80289360 batches: 0.0142
trigger times: 10
Loss after 80420460 batches: 0.0140
trigger times: 11
Loss after 80551560 batches: 0.0138
trigger times: 12
Loss after 80682660 batches: 0.0139
trigger times: 13
Loss after 80813760 batches: 0.0135
trigger times: 14
Loss after 80944860 batches: 0.0135
trigger times: 15
Loss after 81075960 batches: 0.0132
trigger times: 16
Loss after 81207060 batches: 0.0133
trigger times: 17
Loss after 81338160 batches: 0.0134
trigger times: 18
Loss after 81469260 batches: 0.0132
trigger times: 19
Loss after 81600360 batches: 0.0131
trigger times: 20
Early stopping!
Start to test process.
Loss after 81731460 batches: 0.0132
Time to train on one home:  526.8539679050446
trigger times: 0
Loss after 81834060 batches: 0.6043
trigger times: 1
Loss after 81936660 batches: 0.3726
trigger times: 2
Loss after 82039260 batches: 0.2616
trigger times: 3
Loss after 82141860 batches: 0.2015
trigger times: 4
Loss after 82244460 batches: 0.1697
trigger times: 5
Loss after 82347060 batches: 0.1514
trigger times: 6
Loss after 82449660 batches: 0.1317
trigger times: 7
Loss after 82552260 batches: 0.1180
trigger times: 8
Loss after 82654860 batches: 0.1087
trigger times: 9
Loss after 82757460 batches: 0.1023
trigger times: 10
Loss after 82860060 batches: 0.0946
trigger times: 11
Loss after 82962660 batches: 0.0894
trigger times: 12
Loss after 83065260 batches: 0.0860
trigger times: 13
Loss after 83167860 batches: 0.0852
trigger times: 14
Loss after 83270460 batches: 0.0910
trigger times: 15
Loss after 83373060 batches: 0.0768
trigger times: 16
Loss after 83475660 batches: 0.0748
trigger times: 17
Loss after 83578260 batches: 0.0699
trigger times: 18
Loss after 83680860 batches: 0.0703
trigger times: 19
Loss after 83783460 batches: 0.0660
trigger times: 20
Early stopping!
Start to test process.
Loss after 83886060 batches: 0.0641
Time to train on one home:  130.3774275779724
trigger times: 0
Loss after 84017160 batches: 0.4048
trigger times: 1
Loss after 84148260 batches: 0.2087
trigger times: 0
Loss after 84279360 batches: 0.1362
trigger times: 1
Loss after 84410460 batches: 0.1073
trigger times: 2
Loss after 84541560 batches: 0.0891
trigger times: 3
Loss after 84672660 batches: 0.0777
trigger times: 4
Loss after 84803760 batches: 0.0718
trigger times: 5
Loss after 84934860 batches: 0.0659
trigger times: 6
Loss after 85065960 batches: 0.0613
trigger times: 7
Loss after 85197060 batches: 0.0572
trigger times: 8
Loss after 85328160 batches: 0.0544
trigger times: 9
Loss after 85459260 batches: 0.0526
trigger times: 0
Loss after 85590360 batches: 0.0493
trigger times: 1
Loss after 85721460 batches: 0.0480
trigger times: 0
Loss after 85852560 batches: 0.0462
trigger times: 1
Loss after 85983660 batches: 0.0450
trigger times: 2
Loss after 86114760 batches: 0.0434
trigger times: 0
Loss after 86245860 batches: 0.0422
trigger times: 0
Loss after 86376960 batches: 0.0409
trigger times: 1
Loss after 86508060 batches: 0.0405
trigger times: 0
Loss after 86639160 batches: 0.0390
trigger times: 1
Loss after 86770260 batches: 0.0386
trigger times: 0
Loss after 86901360 batches: 0.0376
trigger times: 1
Loss after 87032460 batches: 0.0368
trigger times: 2
Loss after 87163560 batches: 0.0358
trigger times: 3
Loss after 87294660 batches: 0.0353
trigger times: 4
Loss after 87425760 batches: 0.0348
trigger times: 5
Loss after 87556860 batches: 0.0340
trigger times: 0
Loss after 87687960 batches: 0.0337
trigger times: 1
Loss after 87819060 batches: 0.0332
trigger times: 2
Loss after 87950160 batches: 0.0325
trigger times: 3
Loss after 88081260 batches: 0.0315
trigger times: 4
Loss after 88212360 batches: 0.0316
trigger times: 5
Loss after 88343460 batches: 0.0309
trigger times: 6
Loss after 88474560 batches: 0.0304
trigger times: 7
Loss after 88605660 batches: 0.0303
trigger times: 8
Loss after 88736760 batches: 0.0299
trigger times: 9
Loss after 88867860 batches: 0.0296
trigger times: 10
Loss after 88998960 batches: 0.0290
trigger times: 11
Loss after 89130060 batches: 0.0285
trigger times: 0
Loss after 89261160 batches: 0.0289
trigger times: 1
Loss after 89392260 batches: 0.0280
trigger times: 0
Loss after 89523360 batches: 0.0278
trigger times: 1
Loss after 89654460 batches: 0.0282
trigger times: 0
Loss after 89785560 batches: 0.0273
trigger times: 0
Loss after 89916660 batches: 0.0270
trigger times: 1
Loss after 90047760 batches: 0.0267
trigger times: 2
Loss after 90178860 batches: 0.0262
trigger times: 3
Loss after 90309960 batches: 0.0263
trigger times: 4
Loss after 90441060 batches: 0.0261
trigger times: 5
Loss after 90572160 batches: 0.0255
trigger times: 6
Loss after 90703260 batches: 0.0252
trigger times: 7
Loss after 90834360 batches: 0.0253
trigger times: 8
Loss after 90965460 batches: 0.0250
trigger times: 9
Loss after 91096560 batches: 0.0246
trigger times: 10
Loss after 91227660 batches: 0.0249
trigger times: 11
Loss after 91358760 batches: 0.0247
trigger times: 12
Loss after 91489860 batches: 0.0241
trigger times: 13
Loss after 91620960 batches: 0.0243
trigger times: 14
Loss after 91752060 batches: 0.0240
trigger times: 15
Loss after 91883160 batches: 0.0235
trigger times: 0
Loss after 92014260 batches: 0.0236
trigger times: 1
Loss after 92145360 batches: 0.0230
trigger times: 2
Loss after 92276460 batches: 0.0229
trigger times: 3
Loss after 92407560 batches: 0.0227
trigger times: 4
Loss after 92538660 batches: 0.0230
trigger times: 0
Loss after 92669760 batches: 0.0226
trigger times: 1
Loss after 92800860 batches: 0.0226
trigger times: 2
Loss after 92931960 batches: 0.0224
trigger times: 3
Loss after 93063060 batches: 0.0220
trigger times: 4
Loss after 93194160 batches: 0.0216
trigger times: 5
Loss after 93325260 batches: 0.0217
trigger times: 6
Loss after 93456360 batches: 0.0214
trigger times: 7
Loss after 93587460 batches: 0.0215
trigger times: 8
Loss after 93718560 batches: 0.0213
trigger times: 9
Loss after 93849660 batches: 0.0211
trigger times: 0
Loss after 93980760 batches: 0.0210
trigger times: 1
Loss after 94111860 batches: 0.0211
trigger times: 2
Loss after 94242960 batches: 0.0208
trigger times: 3
Loss after 94374060 batches: 0.0212
trigger times: 4
Loss after 94505160 batches: 0.0205
trigger times: 5
Loss after 94636260 batches: 0.0209
trigger times: 6
Loss after 94767360 batches: 0.0205
trigger times: 7
Loss after 94898460 batches: 0.0205
trigger times: 8
Loss after 95029560 batches: 0.0201
trigger times: 9
Loss after 95160660 batches: 0.0202
trigger times: 10
Loss after 95291760 batches: 0.0203
trigger times: 11
Loss after 95422860 batches: 0.0203
trigger times: 12
Loss after 95553960 batches: 0.0199
trigger times: 13
Loss after 95685060 batches: 0.0196
trigger times: 14
Loss after 95816160 batches: 0.0195
trigger times: 15
Loss after 95947260 batches: 0.0196
trigger times: 16
Loss after 96078360 batches: 0.0196
trigger times: 17
Loss after 96209460 batches: 0.0193
trigger times: 18
Loss after 96340560 batches: 0.0192
trigger times: 19
Loss after 96471660 batches: 0.0192
trigger times: 0
Loss after 96602760 batches: 0.0191
trigger times: 1
Loss after 96733860 batches: 0.0192
trigger times: 0
Loss after 96864960 batches: 0.0189
trigger times: 1
Loss after 96996060 batches: 0.0187
trigger times: 2
Loss after 97127160 batches: 0.0187
trigger times: 3
Loss after 97258260 batches: 0.0185
trigger times: 4
Loss after 97389360 batches: 0.0184
trigger times: 5
Loss after 97520460 batches: 0.0185
trigger times: 6
Loss after 97651560 batches: 0.0182
trigger times: 7
Loss after 97782660 batches: 0.0184
trigger times: 8
Loss after 97913760 batches: 0.0185
trigger times: 9
Loss after 98044860 batches: 0.0184
trigger times: 10
Loss after 98175960 batches: 0.0180
trigger times: 11
Loss after 98307060 batches: 0.0184
trigger times: 12
Loss after 98438160 batches: 0.0181
trigger times: 13
Loss after 98569260 batches: 0.0181
trigger times: 14
Loss after 98700360 batches: 0.0179
trigger times: 15
Loss after 98831460 batches: 0.0179
trigger times: 16
Loss after 98962560 batches: 0.0179
trigger times: 17
Loss after 99093660 batches: 0.0174
trigger times: 18
Loss after 99224760 batches: 0.0175
trigger times: 19
Loss after 99355860 batches: 0.0176
trigger times: 20
Early stopping!
Start to test process.
Loss after 99486960 batches: 0.0174
Time to train on one home:  853.6093912124634
trigger times: 0
Loss after 99618060 batches: 0.6718
trigger times: 1
Loss after 99749160 batches: 0.3993
trigger times: 2
Loss after 99880260 batches: 0.2559
trigger times: 3
Loss after 100011360 batches: 0.1865
trigger times: 4
Loss after 100142460 batches: 0.1530
trigger times: 5
Loss after 100273560 batches: 0.1313
trigger times: 6
Loss after 100404660 batches: 0.1187
trigger times: 7
Loss after 100535760 batches: 0.1080
trigger times: 8
Loss after 100666860 batches: 0.1005
trigger times: 9
Loss after 100797960 batches: 0.0949
trigger times: 10
Loss after 100929060 batches: 0.0889
trigger times: 11
Loss after 101060160 batches: 0.0841
trigger times: 12
Loss after 101191260 batches: 0.0816
trigger times: 13
Loss after 101322360 batches: 0.0776
trigger times: 14
Loss after 101453460 batches: 0.0753
trigger times: 15
Loss after 101584560 batches: 0.0720
trigger times: 16
Loss after 101715660 batches: 0.0708
trigger times: 17
Loss after 101846760 batches: 0.0688
trigger times: 18
Loss after 101977860 batches: 0.0665
trigger times: 19
Loss after 102108960 batches: 0.0655
trigger times: 20
Early stopping!
Start to test process.
Loss after 102240060 batches: 0.0633
Time to train on one home:  160.62248349189758
trigger times: 0
Loss after 102368700 batches: 0.3688
trigger times: 1
Loss after 102497340 batches: 0.1630
trigger times: 2
Loss after 102625980 batches: 0.1056
trigger times: 3
Loss after 102754620 batches: 0.0839
trigger times: 4
Loss after 102883260 batches: 0.0699
trigger times: 5
Loss after 103011900 batches: 0.0616
trigger times: 6
Loss after 103140540 batches: 0.0551
trigger times: 7
Loss after 103269180 batches: 0.0508
trigger times: 8
Loss after 103397820 batches: 0.0469
trigger times: 9
Loss after 103526460 batches: 0.0446
trigger times: 0
Loss after 103655100 batches: 0.0433
trigger times: 1
Loss after 103783740 batches: 0.0418
trigger times: 2
Loss after 103912380 batches: 0.0396
trigger times: 3
Loss after 104041020 batches: 0.0372
trigger times: 4
Loss after 104169660 batches: 0.0366
trigger times: 0
Loss after 104298300 batches: 0.0351
trigger times: 1
Loss after 104426940 batches: 0.0339
trigger times: 2
Loss after 104555580 batches: 0.0335
trigger times: 3
Loss after 104684220 batches: 0.0321
trigger times: 4
Loss after 104812860 batches: 0.0319
trigger times: 0
Loss after 104941500 batches: 0.0312
trigger times: 0
Loss after 105070140 batches: 0.0302
trigger times: 0
Loss after 105198780 batches: 0.0295
trigger times: 1
Loss after 105327420 batches: 0.0294
trigger times: 2
Loss after 105456060 batches: 0.0281
trigger times: 3
Loss after 105584700 batches: 0.0279
trigger times: 4
Loss after 105713340 batches: 0.0274
trigger times: 5
Loss after 105841980 batches: 0.0271
trigger times: 6
Loss after 105970620 batches: 0.0264
trigger times: 7
Loss after 106099260 batches: 0.0258
trigger times: 8
Loss after 106227900 batches: 0.0258
trigger times: 9
Loss after 106356540 batches: 0.0253
trigger times: 10
Loss after 106485180 batches: 0.0247
trigger times: 11
Loss after 106613820 batches: 0.0244
trigger times: 12
Loss after 106742460 batches: 0.0248
trigger times: 13
Loss after 106871100 batches: 0.0238
trigger times: 14
Loss after 106999740 batches: 0.0239
trigger times: 15
Loss after 107128380 batches: 0.0234
trigger times: 16
Loss after 107257020 batches: 0.0232
trigger times: 17
Loss after 107385660 batches: 0.0229
trigger times: 18
Loss after 107514300 batches: 0.0227
trigger times: 19
Loss after 107642940 batches: 0.0222
trigger times: 20
Early stopping!
Start to test process.
Loss after 107771580 batches: 0.0221
Time to train on one home:  310.1208417415619
trigger times: 0
Loss after 107902680 batches: 0.6842
trigger times: 1
Loss after 108033780 batches: 0.3846
trigger times: 2
Loss after 108164880 batches: 0.2188
trigger times: 3
Loss after 108295980 batches: 0.1487
trigger times: 4
Loss after 108427080 batches: 0.1196
trigger times: 5
Loss after 108558180 batches: 0.1034
trigger times: 6
Loss after 108689280 batches: 0.0917
trigger times: 7
Loss after 108820380 batches: 0.0847
trigger times: 8
Loss after 108951480 batches: 0.0781
trigger times: 9
Loss after 109082580 batches: 0.0721
trigger times: 10
Loss after 109213680 batches: 0.0691
trigger times: 11
Loss after 109344780 batches: 0.0661
trigger times: 12
Loss after 109475880 batches: 0.0635
trigger times: 13
Loss after 109606980 batches: 0.0611
trigger times: 14
Loss after 109738080 batches: 0.0597
trigger times: 15
Loss after 109869180 batches: 0.0573
trigger times: 16
Loss after 110000280 batches: 0.0557
trigger times: 17
Loss after 110131380 batches: 0.0544
trigger times: 18
Loss after 110262480 batches: 0.0537
trigger times: 19
Loss after 110393580 batches: 0.0517
trigger times: 20
Early stopping!
Start to test process.
Loss after 110524680 batches: 0.0501
Time to train on one home:  159.96536016464233
trigger times: 0
Loss after 110655780 batches: 0.7440
trigger times: 1
Loss after 110786880 batches: 0.4954
trigger times: 2
Loss after 110917980 batches: 0.3704
trigger times: 3
Loss after 111049080 batches: 0.2963
trigger times: 4
Loss after 111180180 batches: 0.2478
trigger times: 5
Loss after 111311280 batches: 0.2184
trigger times: 6
Loss after 111442380 batches: 0.1874
trigger times: 7
Loss after 111573480 batches: 0.1727
trigger times: 8
Loss after 111704580 batches: 0.1559
trigger times: 9
Loss after 111835680 batches: 0.1438
trigger times: 10
Loss after 111966780 batches: 0.1362
trigger times: 11
Loss after 112097880 batches: 0.1241
trigger times: 12
Loss after 112228980 batches: 0.1155
trigger times: 13
Loss after 112360080 batches: 0.1057
trigger times: 14
Loss after 112491180 batches: 0.1003
trigger times: 15
Loss after 112622280 batches: 0.0939
trigger times: 16
Loss after 112753380 batches: 0.0901
trigger times: 17
Loss after 112884480 batches: 0.0896
trigger times: 18
Loss after 113015580 batches: 0.0833
trigger times: 19
Loss after 113146680 batches: 0.0825
trigger times: 20
Early stopping!
Start to test process.
Loss after 113277780 batches: 0.0811
Time to train on one home:  159.99496412277222
trigger times: 0
Loss after 113408880 batches: 0.2554
trigger times: 1
Loss after 113539980 batches: 0.0903
trigger times: 2
Loss after 113671080 batches: 0.0560
trigger times: 3
Loss after 113802180 batches: 0.0437
trigger times: 4
Loss after 113933280 batches: 0.0386
trigger times: 5
Loss after 114064380 batches: 0.0345
trigger times: 6
Loss after 114195480 batches: 0.0313
trigger times: 7
Loss after 114326580 batches: 0.0288
trigger times: 8
Loss after 114457680 batches: 0.0276
trigger times: 9
Loss after 114588780 batches: 0.0260
trigger times: 10
Loss after 114719880 batches: 0.0245
trigger times: 11
Loss after 114850980 batches: 0.0238
trigger times: 12
Loss after 114982080 batches: 0.0229
trigger times: 0
Loss after 115113180 batches: 0.0222
trigger times: 0
Loss after 115244280 batches: 0.0215
trigger times: 1
Loss after 115375380 batches: 0.0210
trigger times: 2
Loss after 115506480 batches: 0.0203
trigger times: 3
Loss after 115637580 batches: 0.0197
trigger times: 4
Loss after 115768680 batches: 0.0191
trigger times: 5
Loss after 115899780 batches: 0.0188
trigger times: 6
Loss after 116030880 batches: 0.0180
trigger times: 0
Loss after 116161980 batches: 0.0178
trigger times: 1
Loss after 116293080 batches: 0.0174
trigger times: 2
Loss after 116424180 batches: 0.0176
trigger times: 3
Loss after 116555280 batches: 0.0171
trigger times: 4
Loss after 116686380 batches: 0.0166
trigger times: 5
Loss after 116817480 batches: 0.0161
trigger times: 0
Loss after 116948580 batches: 0.0157
trigger times: 1
Loss after 117079680 batches: 0.0158
trigger times: 2
Loss after 117210780 batches: 0.0153
trigger times: 0
Loss after 117341880 batches: 0.0155
trigger times: 1
Loss after 117472980 batches: 0.0150
trigger times: 2
Loss after 117604080 batches: 0.0148
trigger times: 0
Loss after 117735180 batches: 0.0145
trigger times: 1
Loss after 117866280 batches: 0.0144
trigger times: 2
Loss after 117997380 batches: 0.0142
trigger times: 0
Loss after 118128480 batches: 0.0139
trigger times: 1
Loss after 118259580 batches: 0.0139
trigger times: 0
Loss after 118390680 batches: 0.0135
trigger times: 0
Loss after 118521780 batches: 0.0133
trigger times: 1
Loss after 118652880 batches: 0.0135
trigger times: 2
Loss after 118783980 batches: 0.0133
trigger times: 0
Loss after 118915080 batches: 0.0131
trigger times: 1
Loss after 119046180 batches: 0.0129
trigger times: 2
Loss after 119177280 batches: 0.0129
trigger times: 0
Loss after 119308380 batches: 0.0127
trigger times: 1
Loss after 119439480 batches: 0.0124
trigger times: 2
Loss after 119570580 batches: 0.0124
trigger times: 3
Loss after 119701680 batches: 0.0124
trigger times: 4
Loss after 119832780 batches: 0.0124
trigger times: 5
Loss after 119963880 batches: 0.0121
trigger times: 6
Loss after 120094980 batches: 0.0121
trigger times: 7
Loss after 120226080 batches: 0.0117
trigger times: 8
Loss after 120357180 batches: 0.0115
trigger times: 9
Loss after 120488280 batches: 0.0116
trigger times: 10
Loss after 120619380 batches: 0.0112
trigger times: 11
Loss after 120750480 batches: 0.0115
trigger times: 12
Loss after 120881580 batches: 0.0115
trigger times: 13
Loss after 121012680 batches: 0.0114
trigger times: 14
Loss after 121143780 batches: 0.0112
trigger times: 15
Loss after 121274880 batches: 0.0110
trigger times: 16
Loss after 121405980 batches: 0.0111
trigger times: 17
Loss after 121537080 batches: 0.0111
trigger times: 18
Loss after 121668180 batches: 0.0111
trigger times: 19
Loss after 121799280 batches: 0.0106
trigger times: 0
Loss after 121930380 batches: 0.0107
trigger times: 1
Loss after 122061480 batches: 0.0105
trigger times: 2
Loss after 122192580 batches: 0.0105
trigger times: 3
Loss after 122323680 batches: 0.0106
trigger times: 4
Loss after 122454780 batches: 0.0104
trigger times: 0
Loss after 122585880 batches: 0.0106
trigger times: 1
Loss after 122716980 batches: 0.0102
trigger times: 2
Loss after 122848080 batches: 0.0102
trigger times: 3
Loss after 122979180 batches: 0.0101
trigger times: 4
Loss after 123110280 batches: 0.0100
trigger times: 5
Loss after 123241380 batches: 0.0098
trigger times: 6
Loss after 123372480 batches: 0.0098
trigger times: 7
Loss after 123503580 batches: 0.0098
trigger times: 0
Loss after 123634680 batches: 0.0098
trigger times: 1
Loss after 123765780 batches: 0.0098
trigger times: 2
Loss after 123896880 batches: 0.0098
trigger times: 3
Loss after 124027980 batches: 0.0099
trigger times: 0
Loss after 124159080 batches: 0.0098
trigger times: 1
Loss after 124290180 batches: 0.0096
trigger times: 0
Loss after 124421280 batches: 0.0097
trigger times: 1
Loss after 124552380 batches: 0.0094
trigger times: 2
Loss after 124683480 batches: 0.0092
trigger times: 3
Loss after 124814580 batches: 0.0094
trigger times: 4
Loss after 124945680 batches: 0.0094
trigger times: 5
Loss after 125076780 batches: 0.0093
trigger times: 6
Loss after 125207880 batches: 0.0091
trigger times: 7
Loss after 125338980 batches: 0.0092
trigger times: 8
Loss after 125470080 batches: 0.0090
trigger times: 9
Loss after 125601180 batches: 0.0091
trigger times: 10
Loss after 125732280 batches: 0.0093
trigger times: 11
Loss after 125863380 batches: 0.0092
trigger times: 12
Loss after 125994480 batches: 0.0091
trigger times: 13
Loss after 126125580 batches: 0.0091
trigger times: 14
Loss after 126256680 batches: 0.0088
trigger times: 15
Loss after 126387780 batches: 0.0089
trigger times: 16
Loss after 126518880 batches: 0.0090
trigger times: 17
Loss after 126649980 batches: 0.0090
trigger times: 18
Loss after 126781080 batches: 0.0088
trigger times: 19
Loss after 126912180 batches: 0.0086
trigger times: 20
Early stopping!
Start to test process.
Loss after 127043280 batches: 0.0085
Time to train on one home:  753.7845451831818
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918]]
Round_2_results:  [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 987 < 988; dropping {'Training_Loss': 0.31827639043331146, 'Validation_Loss': 0.27752378914091325, 'Training_R2': 0.6803044618663918, 'Validation_R2': 0.74140846825037, 'Training_F1': 0.6950919364747035, 'Validation_F1': 0.7140528550267126, 'Training_NEP': 0.6086995927759584, 'Validation_NEP': 0.5938456020899398, 'Training_NDE': 0.24000111508149663, 'Validation_NDE': 0.20592866446307231, 'Training_MAE': 20.1589233312745, 'Validation_MAE': 16.285710123790206, 'Training_MSE': 1055.9673, 'Validation_MSE': 760.4889}.
trigger times: 0
Loss after 127174380 batches: 0.3183
trigger times: 0
Loss after 127305480 batches: 0.1325
trigger times: 0
Loss after 127436580 batches: 0.0884
trigger times: 0
Loss after 127567680 batches: 0.0686
trigger times: 0
Loss after 127698780 batches: 0.0575
trigger times: 1
Loss after 127829880 batches: 0.0495
trigger times: 2
Loss after 127960980 batches: 0.0440
trigger times: 0
Loss after 128092080 batches: 0.0402
trigger times: 1
Loss after 128223180 batches: 0.0376
trigger times: 0
Loss after 128354280 batches: 0.0347
trigger times: 0
Loss after 128485380 batches: 0.0329
trigger times: 0
Loss after 128616480 batches: 0.0319
trigger times: 1
Loss after 128747580 batches: 0.0305
trigger times: 2
Loss after 128878680 batches: 0.0291
trigger times: 3
Loss after 129009780 batches: 0.0273
trigger times: 4
Loss after 129140880 batches: 0.0266
trigger times: 5
Loss after 129271980 batches: 0.0261
trigger times: 6
Loss after 129403080 batches: 0.0251
trigger times: 7
Loss after 129534180 batches: 0.0241
trigger times: 0
Loss after 129665280 batches: 0.0237
trigger times: 1
Loss after 129796380 batches: 0.0221
trigger times: 2
Loss after 129927480 batches: 0.0223
trigger times: 3
Loss after 130058580 batches: 0.0218
trigger times: 4
Loss after 130189680 batches: 0.0216
trigger times: 5
Loss after 130320780 batches: 0.0212
trigger times: 6
Loss after 130451880 batches: 0.0204
trigger times: 7
Loss after 130582980 batches: 0.0200
trigger times: 8
Loss after 130714080 batches: 0.0197
trigger times: 9
Loss after 130845180 batches: 0.0196
trigger times: 10
Loss after 130976280 batches: 0.0194
trigger times: 11
Loss after 131107380 batches: 0.0188
trigger times: 12
Loss after 131238480 batches: 0.0185
trigger times: 13
Loss after 131369580 batches: 0.0181
trigger times: 14
Loss after 131500680 batches: 0.0184
trigger times: 15
Loss after 131631780 batches: 0.0176
trigger times: 16
Loss after 131762880 batches: 0.0169
trigger times: 17
Loss after 131893980 batches: 0.0173
trigger times: 18
Loss after 132025080 batches: 0.0170
trigger times: 19
Loss after 132156180 batches: 0.0169
trigger times: 20
Early stopping!
Start to test process.
Loss after 132287280 batches: 0.0164
Time to train on one home:  294.0086259841919
trigger times: 0
Loss after 132389880 batches: 0.5115
trigger times: 1
Loss after 132492480 batches: 0.2559
trigger times: 2
Loss after 132595080 batches: 0.1646
trigger times: 3
Loss after 132697680 batches: 0.1285
trigger times: 4
Loss after 132800280 batches: 0.1120
trigger times: 5
Loss after 132902880 batches: 0.0968
trigger times: 6
Loss after 133005480 batches: 0.0887
trigger times: 7
Loss after 133108080 batches: 0.0838
trigger times: 8
Loss after 133210680 batches: 0.0754
trigger times: 9
Loss after 133313280 batches: 0.0707
trigger times: 10
Loss after 133415880 batches: 0.0664
trigger times: 11
Loss after 133518480 batches: 0.0617
trigger times: 12
Loss after 133621080 batches: 0.0603
trigger times: 13
Loss after 133723680 batches: 0.0570
trigger times: 14
Loss after 133826280 batches: 0.0542
trigger times: 15
Loss after 133928880 batches: 0.0548
trigger times: 16
Loss after 134031480 batches: 0.0498
trigger times: 17
Loss after 134134080 batches: 0.0482
trigger times: 18
Loss after 134236680 batches: 0.0489
trigger times: 19
Loss after 134339280 batches: 0.0463
trigger times: 20
Early stopping!
Start to test process.
Loss after 134441880 batches: 0.0445
Time to train on one home:  130.40287113189697
trigger times: 0
Loss after 134572980 batches: 0.2777
trigger times: 1
Loss after 134704080 batches: 0.1059
trigger times: 0
Loss after 134835180 batches: 0.0694
trigger times: 0
Loss after 134966280 batches: 0.0554
trigger times: 0
Loss after 135097380 batches: 0.0487
trigger times: 1
Loss after 135228480 batches: 0.0430
trigger times: 0
Loss after 135359580 batches: 0.0395
trigger times: 1
Loss after 135490680 batches: 0.0376
trigger times: 2
Loss after 135621780 batches: 0.0353
trigger times: 0
Loss after 135752880 batches: 0.0331
trigger times: 1
Loss after 135883980 batches: 0.0316
trigger times: 2
Loss after 136015080 batches: 0.0304
trigger times: 3
Loss after 136146180 batches: 0.0298
trigger times: 4
Loss after 136277280 batches: 0.0286
trigger times: 0
Loss after 136408380 batches: 0.0282
trigger times: 1
Loss after 136539480 batches: 0.0273
trigger times: 0
Loss after 136670580 batches: 0.0267
trigger times: 1
Loss after 136801680 batches: 0.0262
trigger times: 2
Loss after 136932780 batches: 0.0254
trigger times: 3
Loss after 137063880 batches: 0.0246
trigger times: 0
Loss after 137194980 batches: 0.0240
trigger times: 1
Loss after 137326080 batches: 0.0240
trigger times: 2
Loss after 137457180 batches: 0.0233
trigger times: 3
Loss after 137588280 batches: 0.0231
trigger times: 4
Loss after 137719380 batches: 0.0230
trigger times: 5
Loss after 137850480 batches: 0.0226
trigger times: 6
Loss after 137981580 batches: 0.0225
trigger times: 7
Loss after 138112680 batches: 0.0222
trigger times: 8
Loss after 138243780 batches: 0.0218
trigger times: 9
Loss after 138374880 batches: 0.0216
trigger times: 10
Loss after 138505980 batches: 0.0214
trigger times: 11
Loss after 138637080 batches: 0.0216
trigger times: 12
Loss after 138768180 batches: 0.0205
trigger times: 13
Loss after 138899280 batches: 0.0204
trigger times: 14
Loss after 139030380 batches: 0.0206
trigger times: 15
Loss after 139161480 batches: 0.0205
trigger times: 16
Loss after 139292580 batches: 0.0198
trigger times: 17
Loss after 139423680 batches: 0.0195
trigger times: 18
Loss after 139554780 batches: 0.0197
trigger times: 19
Loss after 139685880 batches: 0.0194
trigger times: 20
Early stopping!
Start to test process.
Loss after 139816980 batches: 0.0193
Time to train on one home:  300.77743554115295
trigger times: 0
Loss after 139948080 batches: 0.5898
trigger times: 1
Loss after 140079180 batches: 0.2490
trigger times: 2
Loss after 140210280 batches: 0.1444
trigger times: 3
Loss after 140341380 batches: 0.1108
trigger times: 4
Loss after 140472480 batches: 0.0947
trigger times: 5
Loss after 140603580 batches: 0.0846
trigger times: 6
Loss after 140734680 batches: 0.0765
trigger times: 7
Loss after 140865780 batches: 0.0708
trigger times: 8
Loss after 140996880 batches: 0.0672
trigger times: 9
Loss after 141127980 batches: 0.0634
trigger times: 10
Loss after 141259080 batches: 0.0608
trigger times: 11
Loss after 141390180 batches: 0.0589
trigger times: 12
Loss after 141521280 batches: 0.0566
trigger times: 13
Loss after 141652380 batches: 0.0542
trigger times: 14
Loss after 141783480 batches: 0.0524
trigger times: 15
Loss after 141914580 batches: 0.0513
trigger times: 16
Loss after 142045680 batches: 0.0499
trigger times: 17
Loss after 142176780 batches: 0.0482
trigger times: 18
Loss after 142307880 batches: 0.0477
trigger times: 19
Loss after 142438980 batches: 0.0459
trigger times: 20
Early stopping!
Start to test process.
Loss after 142570080 batches: 0.0456
Time to train on one home:  159.51213574409485
trigger times: 0
Loss after 142698720 batches: 0.2596
trigger times: 1
Loss after 142827360 batches: 0.0901
trigger times: 0
Loss after 142956000 batches: 0.0599
trigger times: 0
Loss after 143084640 batches: 0.0487
trigger times: 1
Loss after 143213280 batches: 0.0422
trigger times: 0
Loss after 143341920 batches: 0.0379
trigger times: 1
Loss after 143470560 batches: 0.0350
trigger times: 2
Loss after 143599200 batches: 0.0334
trigger times: 3
Loss after 143727840 batches: 0.0310
trigger times: 0
Loss after 143856480 batches: 0.0295
trigger times: 1
Loss after 143985120 batches: 0.0282
trigger times: 2
Loss after 144113760 batches: 0.0274
trigger times: 3
Loss after 144242400 batches: 0.0261
trigger times: 4
Loss after 144371040 batches: 0.0255
trigger times: 5
Loss after 144499680 batches: 0.0249
trigger times: 0
Loss after 144628320 batches: 0.0241
trigger times: 1
Loss after 144756960 batches: 0.0231
trigger times: 2
Loss after 144885600 batches: 0.0228
trigger times: 0
Loss after 145014240 batches: 0.0227
trigger times: 1
Loss after 145142880 batches: 0.0223
trigger times: 2
Loss after 145271520 batches: 0.0215
trigger times: 3
Loss after 145400160 batches: 0.0213
trigger times: 4
Loss after 145528800 batches: 0.0210
trigger times: 5
Loss after 145657440 batches: 0.0204
trigger times: 6
Loss after 145786080 batches: 0.0204
trigger times: 7
Loss after 145914720 batches: 0.0202
trigger times: 8
Loss after 146043360 batches: 0.0200
trigger times: 9
Loss after 146172000 batches: 0.0195
trigger times: 10
Loss after 146300640 batches: 0.0192
trigger times: 11
Loss after 146429280 batches: 0.0189
trigger times: 12
Loss after 146557920 batches: 0.0186
trigger times: 13
Loss after 146686560 batches: 0.0185
trigger times: 14
Loss after 146815200 batches: 0.0184
trigger times: 15
Loss after 146943840 batches: 0.0179
trigger times: 16
Loss after 147072480 batches: 0.0184
trigger times: 17
Loss after 147201120 batches: 0.0180
trigger times: 18
Loss after 147329760 batches: 0.0177
trigger times: 19
Loss after 147458400 batches: 0.0174
trigger times: 20
Early stopping!
Start to test process.
Loss after 147587040 batches: 0.0178
Time to train on one home:  282.24814653396606
trigger times: 0
Loss after 147718140 batches: 0.5761
trigger times: 1
Loss after 147849240 batches: 0.2110
trigger times: 2
Loss after 147980340 batches: 0.1162
trigger times: 3
Loss after 148111440 batches: 0.0884
trigger times: 4
Loss after 148242540 batches: 0.0753
trigger times: 5
Loss after 148373640 batches: 0.0665
trigger times: 6
Loss after 148504740 batches: 0.0615
trigger times: 7
Loss after 148635840 batches: 0.0584
trigger times: 8
Loss after 148766940 batches: 0.0545
trigger times: 9
Loss after 148898040 batches: 0.0517
trigger times: 10
Loss after 149029140 batches: 0.0497
trigger times: 11
Loss after 149160240 batches: 0.0474
trigger times: 12
Loss after 149291340 batches: 0.0456
trigger times: 13
Loss after 149422440 batches: 0.0438
trigger times: 14
Loss after 149553540 batches: 0.0432
trigger times: 15
Loss after 149684640 batches: 0.0419
trigger times: 16
Loss after 149815740 batches: 0.0412
trigger times: 17
Loss after 149946840 batches: 0.0400
trigger times: 18
Loss after 150077940 batches: 0.0388
trigger times: 19
Loss after 150209040 batches: 0.0379
trigger times: 20
Early stopping!
Start to test process.
Loss after 150340140 batches: 0.0376
Time to train on one home:  159.93009305000305
trigger times: 0
Loss after 150471240 batches: 0.6546
trigger times: 1
Loss after 150602340 batches: 0.3724
trigger times: 2
Loss after 150733440 batches: 0.2518
trigger times: 3
Loss after 150864540 batches: 0.2006
trigger times: 4
Loss after 150995640 batches: 0.1529
trigger times: 5
Loss after 151126740 batches: 0.1346
trigger times: 6
Loss after 151257840 batches: 0.1182
trigger times: 7
Loss after 151388940 batches: 0.1078
trigger times: 8
Loss after 151520040 batches: 0.0952
trigger times: 9
Loss after 151651140 batches: 0.0859
trigger times: 10
Loss after 151782240 batches: 0.0820
trigger times: 11
Loss after 151913340 batches: 0.0747
trigger times: 12
Loss after 152044440 batches: 0.0707
trigger times: 13
Loss after 152175540 batches: 0.0672
trigger times: 14
Loss after 152306640 batches: 0.0641
trigger times: 15
Loss after 152437740 batches: 0.0608
trigger times: 16
Loss after 152568840 batches: 0.0602
trigger times: 17
Loss after 152699940 batches: 0.0582
trigger times: 18
Loss after 152831040 batches: 0.0569
trigger times: 19
Loss after 152962140 batches: 0.0552
trigger times: 20
Early stopping!
Start to test process.
Loss after 153093240 batches: 0.0525
Time to train on one home:  160.07693552970886
trigger times: 0
Loss after 153224340 batches: 0.2218
trigger times: 1
Loss after 153355440 batches: 0.0814
trigger times: 2
Loss after 153486540 batches: 0.0556
trigger times: 3
Loss after 153617640 batches: 0.0446
trigger times: 4
Loss after 153748740 batches: 0.0391
trigger times: 5
Loss after 153879840 batches: 0.0354
trigger times: 6
Loss after 154010940 batches: 0.0323
trigger times: 7
Loss after 154142040 batches: 0.0299
trigger times: 8
Loss after 154273140 batches: 0.0286
trigger times: 9
Loss after 154404240 batches: 0.0268
trigger times: 10
Loss after 154535340 batches: 0.0254
trigger times: 11
Loss after 154666440 batches: 0.0243
trigger times: 12
Loss after 154797540 batches: 0.0235
trigger times: 13
Loss after 154928640 batches: 0.0228
trigger times: 14
Loss after 155059740 batches: 0.0217
trigger times: 15
Loss after 155190840 batches: 0.0216
trigger times: 16
Loss after 155321940 batches: 0.0212
trigger times: 17
Loss after 155453040 batches: 0.0201
trigger times: 18
Loss after 155584140 batches: 0.0196
trigger times: 19
Loss after 155715240 batches: 0.0196
trigger times: 20
Early stopping!
Start to test process.
Loss after 155846340 batches: 0.0190
Time to train on one home:  159.97822642326355
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899]]
Round_3_results:  [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899]
trigger times: 0
Loss after 155977440 batches: 0.1776
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1212 < 1213; dropping {'Training_Loss': 0.1776287818573556, 'Validation_Loss': 0.2514292134179009, 'Training_R2': 0.8213457107163432, 'Validation_R2': 0.7659657847068916, 'Training_F1': 0.7726937279050614, 'Validation_F1': 0.698682643887375, 'Training_NEP': 0.45499285461018096, 'Validation_NEP': 0.5661180340474236, 'Training_NDE': 0.13411894608379088, 'Validation_NDE': 0.18637251215416853, 'Training_MAE': 15.068460996556519, 'Validation_MAE': 15.525305173431246, 'Training_MSE': 590.10236, 'Validation_MSE': 688.26855}.
trigger times: 0
Loss after 156108540 batches: 0.0583
trigger times: 1
Loss after 156239640 batches: 0.0403
trigger times: 0
Loss after 156370740 batches: 0.0341
trigger times: 0
Loss after 156501840 batches: 0.0294
trigger times: 0
Loss after 156632940 batches: 0.0273
trigger times: 1
Loss after 156764040 batches: 0.0255
trigger times: 0
Loss after 156895140 batches: 0.0241
trigger times: 0
Loss after 157026240 batches: 0.0227
trigger times: 1
Loss after 157157340 batches: 0.0218
trigger times: 0
Loss after 157288440 batches: 0.0212
trigger times: 0
Loss after 157419540 batches: 0.0208
trigger times: 1
Loss after 157550640 batches: 0.0202
trigger times: 2
Loss after 157681740 batches: 0.0189
trigger times: 3
Loss after 157812840 batches: 0.0188
trigger times: 4
Loss after 157943940 batches: 0.0186
trigger times: 5
Loss after 158075040 batches: 0.0177
trigger times: 6
Loss after 158206140 batches: 0.0173
trigger times: 7
Loss after 158337240 batches: 0.0171
trigger times: 8
Loss after 158468340 batches: 0.0172
trigger times: 9
Loss after 158599440 batches: 0.0163
trigger times: 0
Loss after 158730540 batches: 0.0163
trigger times: 1
Loss after 158861640 batches: 0.0162
trigger times: 2
Loss after 158992740 batches: 0.0154
trigger times: 3
Loss after 159123840 batches: 0.0152
trigger times: 4
Loss after 159254940 batches: 0.0153
trigger times: 5
Loss after 159386040 batches: 0.0151
trigger times: 6
Loss after 159517140 batches: 0.0148
trigger times: 0
Loss after 159648240 batches: 0.0143
trigger times: 1
Loss after 159779340 batches: 0.0147
trigger times: 2
Loss after 159910440 batches: 0.0142
trigger times: 3
Loss after 160041540 batches: 0.0145
trigger times: 4
Loss after 160172640 batches: 0.0140
trigger times: 5
Loss after 160303740 batches: 0.0137
trigger times: 6
Loss after 160434840 batches: 0.0139
trigger times: 7
Loss after 160565940 batches: 0.0140
trigger times: 8
Loss after 160697040 batches: 0.0135
trigger times: 0
Loss after 160828140 batches: 0.0135
trigger times: 1
Loss after 160959240 batches: 0.0134
trigger times: 2
Loss after 161090340 batches: 0.0132
trigger times: 3
Loss after 161221440 batches: 0.0130
trigger times: 4
Loss after 161352540 batches: 0.0127
trigger times: 5
Loss after 161483640 batches: 0.0129
trigger times: 6
Loss after 161614740 batches: 0.0125
trigger times: 7
Loss after 161745840 batches: 0.0125
trigger times: 8
Loss after 161876940 batches: 0.0126
trigger times: 9
Loss after 162008040 batches: 0.0125
trigger times: 10
Loss after 162139140 batches: 0.0122
trigger times: 11
Loss after 162270240 batches: 0.0126
trigger times: 12
Loss after 162401340 batches: 0.0121
trigger times: 13
Loss after 162532440 batches: 0.0121
trigger times: 14
Loss after 162663540 batches: 0.0118
trigger times: 15
Loss after 162794640 batches: 0.0121
trigger times: 16
Loss after 162925740 batches: 0.0121
trigger times: 17
Loss after 163056840 batches: 0.0117
trigger times: 18
Loss after 163187940 batches: 0.0114
trigger times: 19
Loss after 163319040 batches: 0.0116
trigger times: 20
Early stopping!
Start to test process.
Loss after 163450140 batches: 0.0112
Time to train on one home:  420.8310315608978
trigger times: 0
Loss after 163552740 batches: 0.4605
trigger times: 1
Loss after 163655340 batches: 0.1927
trigger times: 2
Loss after 163757940 batches: 0.1249
trigger times: 3
Loss after 163860540 batches: 0.0986
trigger times: 4
Loss after 163963140 batches: 0.0886
trigger times: 5
Loss after 164065740 batches: 0.0861
trigger times: 6
Loss after 164168340 batches: 0.0709
trigger times: 7
Loss after 164270940 batches: 0.0658
trigger times: 8
Loss after 164373540 batches: 0.0571
trigger times: 9
Loss after 164476140 batches: 0.0526
trigger times: 10
Loss after 164578740 batches: 0.0529
trigger times: 11
Loss after 164681340 batches: 0.0487
trigger times: 12
Loss after 164783940 batches: 0.0475
trigger times: 13
Loss after 164886540 batches: 0.0447
trigger times: 14
Loss after 164989140 batches: 0.0430
trigger times: 15
Loss after 165091740 batches: 0.0424
trigger times: 16
Loss after 165194340 batches: 0.0407
trigger times: 17
Loss after 165296940 batches: 0.0408
trigger times: 18
Loss after 165399540 batches: 0.0397
trigger times: 19
Loss after 165502140 batches: 0.0360
trigger times: 20
Early stopping!
Start to test process.
Loss after 165604740 batches: 0.0365
Time to train on one home:  130.33724641799927
trigger times: 0
Loss after 165735840 batches: 0.2095
trigger times: 0
Loss after 165866940 batches: 0.0736
trigger times: 0
Loss after 165998040 batches: 0.0511
trigger times: 0
Loss after 166129140 batches: 0.0427
trigger times: 0
Loss after 166260240 batches: 0.0378
trigger times: 0
Loss after 166391340 batches: 0.0353
trigger times: 1
Loss after 166522440 batches: 0.0324
trigger times: 2
Loss after 166653540 batches: 0.0309
trigger times: 3
Loss after 166784640 batches: 0.0288
trigger times: 4
Loss after 166915740 batches: 0.0276
trigger times: 5
Loss after 167046840 batches: 0.0274
trigger times: 6
Loss after 167177940 batches: 0.0259
trigger times: 7
Loss after 167309040 batches: 0.0254
trigger times: 8
Loss after 167440140 batches: 0.0249
trigger times: 9
Loss after 167571240 batches: 0.0243
trigger times: 10
Loss after 167702340 batches: 0.0234
trigger times: 11
Loss after 167833440 batches: 0.0231
trigger times: 12
Loss after 167964540 batches: 0.0228
trigger times: 13
Loss after 168095640 batches: 0.0223
trigger times: 14
Loss after 168226740 batches: 0.0222
trigger times: 15
Loss after 168357840 batches: 0.0216
trigger times: 16
Loss after 168488940 batches: 0.0215
trigger times: 0
Loss after 168620040 batches: 0.0212
trigger times: 1
Loss after 168751140 batches: 0.0209
trigger times: 2
Loss after 168882240 batches: 0.0203
trigger times: 3
Loss after 169013340 batches: 0.0204
trigger times: 4
Loss after 169144440 batches: 0.0200
trigger times: 5
Loss after 169275540 batches: 0.0197
trigger times: 6
Loss after 169406640 batches: 0.0196
trigger times: 7
Loss after 169537740 batches: 0.0196
trigger times: 8
Loss after 169668840 batches: 0.0195
trigger times: 9
Loss after 169799940 batches: 0.0192
trigger times: 10
Loss after 169931040 batches: 0.0190
trigger times: 11
Loss after 170062140 batches: 0.0186
trigger times: 12
Loss after 170193240 batches: 0.0186
trigger times: 13
Loss after 170324340 batches: 0.0183
trigger times: 14
Loss after 170455440 batches: 0.0185
trigger times: 15
Loss after 170586540 batches: 0.0182
trigger times: 16
Loss after 170717640 batches: 0.0182
trigger times: 17
Loss after 170848740 batches: 0.0179
trigger times: 18
Loss after 170979840 batches: 0.0176
trigger times: 0
Loss after 171110940 batches: 0.0177
trigger times: 1
Loss after 171242040 batches: 0.0177
trigger times: 2
Loss after 171373140 batches: 0.0174
trigger times: 3
Loss after 171504240 batches: 0.0174
trigger times: 4
Loss after 171635340 batches: 0.0174
trigger times: 5
Loss after 171766440 batches: 0.0170
trigger times: 6
Loss after 171897540 batches: 0.0173
trigger times: 7
Loss after 172028640 batches: 0.0167
trigger times: 8
Loss after 172159740 batches: 0.0167
trigger times: 9
Loss after 172290840 batches: 0.0167
trigger times: 10
Loss after 172421940 batches: 0.0165
trigger times: 11
Loss after 172553040 batches: 0.0166
trigger times: 12
Loss after 172684140 batches: 0.0162
trigger times: 13
Loss after 172815240 batches: 0.0162
trigger times: 14
Loss after 172946340 batches: 0.0164
trigger times: 15
Loss after 173077440 batches: 0.0161
trigger times: 16
Loss after 173208540 batches: 0.0160
trigger times: 17
Loss after 173339640 batches: 0.0159
trigger times: 18
Loss after 173470740 batches: 0.0159
trigger times: 19
Loss after 173601840 batches: 0.0158
trigger times: 20
Early stopping!
Start to test process.
Loss after 173732940 batches: 0.0155
Time to train on one home:  450.11348938941956
trigger times: 0
Loss after 173864040 batches: 0.4719
trigger times: 1
Loss after 173995140 batches: 0.1568
trigger times: 2
Loss after 174126240 batches: 0.1032
trigger times: 3
Loss after 174257340 batches: 0.0854
trigger times: 4
Loss after 174388440 batches: 0.0755
trigger times: 5
Loss after 174519540 batches: 0.0681
trigger times: 6
Loss after 174650640 batches: 0.0635
trigger times: 7
Loss after 174781740 batches: 0.0590
trigger times: 8
Loss after 174912840 batches: 0.0560
trigger times: 9
Loss after 175043940 batches: 0.0534
trigger times: 10
Loss after 175175040 batches: 0.0516
trigger times: 11
Loss after 175306140 batches: 0.0491
trigger times: 12
Loss after 175437240 batches: 0.0479
trigger times: 13
Loss after 175568340 batches: 0.0461
trigger times: 14
Loss after 175699440 batches: 0.0445
trigger times: 15
Loss after 175830540 batches: 0.0434
trigger times: 16
Loss after 175961640 batches: 0.0419
trigger times: 17
Loss after 176092740 batches: 0.0414
trigger times: 18
Loss after 176223840 batches: 0.0405
trigger times: 19
Loss after 176354940 batches: 0.0396
trigger times: 20
Early stopping!
Start to test process.
Loss after 176486040 batches: 0.0388
Time to train on one home:  160.23642826080322
trigger times: 0
Loss after 176614680 batches: 0.1951
trigger times: 0
Loss after 176743320 batches: 0.0607
trigger times: 0
Loss after 176871960 batches: 0.0438
trigger times: 0
Loss after 177000600 batches: 0.0365
trigger times: 0
Loss after 177129240 batches: 0.0324
trigger times: 1
Loss after 177257880 batches: 0.0302
trigger times: 2
Loss after 177386520 batches: 0.0285
trigger times: 0
Loss after 177515160 batches: 0.0263
trigger times: 0
Loss after 177643800 batches: 0.0251
trigger times: 1
Loss after 177772440 batches: 0.0240
trigger times: 2
Loss after 177901080 batches: 0.0234
trigger times: 3
Loss after 178029720 batches: 0.0229
trigger times: 0
Loss after 178158360 batches: 0.0226
trigger times: 1
Loss after 178287000 batches: 0.0215
trigger times: 2
Loss after 178415640 batches: 0.0207
trigger times: 3
Loss after 178544280 batches: 0.0201
trigger times: 4
Loss after 178672920 batches: 0.0199
trigger times: 5
Loss after 178801560 batches: 0.0194
trigger times: 6
Loss after 178930200 batches: 0.0193
trigger times: 7
Loss after 179058840 batches: 0.0188
trigger times: 0
Loss after 179187480 batches: 0.0187
trigger times: 1
Loss after 179316120 batches: 0.0186
trigger times: 2
Loss after 179444760 batches: 0.0184
trigger times: 3
Loss after 179573400 batches: 0.0176
trigger times: 4
Loss after 179702040 batches: 0.0176
trigger times: 5
Loss after 179830680 batches: 0.0171
trigger times: 6
Loss after 179959320 batches: 0.0173
trigger times: 7
Loss after 180087960 batches: 0.0172
trigger times: 0
Loss after 180216600 batches: 0.0168
trigger times: 1
Loss after 180345240 batches: 0.0165
trigger times: 2
Loss after 180473880 batches: 0.0166
trigger times: 3
Loss after 180602520 batches: 0.0165
trigger times: 4
Loss after 180731160 batches: 0.0163
trigger times: 5
Loss after 180859800 batches: 0.0163
trigger times: 6
Loss after 180988440 batches: 0.0159
trigger times: 7
Loss after 181117080 batches: 0.0157
trigger times: 8
Loss after 181245720 batches: 0.0159
trigger times: 9
Loss after 181374360 batches: 0.0156
trigger times: 10
Loss after 181503000 batches: 0.0153
trigger times: 11
Loss after 181631640 batches: 0.0155
trigger times: 12
Loss after 181760280 batches: 0.0153
trigger times: 13
Loss after 181888920 batches: 0.0153
trigger times: 14
Loss after 182017560 batches: 0.0148
trigger times: 15
Loss after 182146200 batches: 0.0148
trigger times: 16
Loss after 182274840 batches: 0.0148
trigger times: 17
Loss after 182403480 batches: 0.0149
trigger times: 18
Loss after 182532120 batches: 0.0144
trigger times: 19
Loss after 182660760 batches: 0.0144
trigger times: 20
Early stopping!
Start to test process.
Loss after 182789400 batches: 0.0145
Time to train on one home:  352.1100296974182
trigger times: 0
Loss after 182920500 batches: 0.4642
trigger times: 1
Loss after 183051600 batches: 0.1379
trigger times: 2
Loss after 183182700 batches: 0.0879
trigger times: 3
Loss after 183313800 batches: 0.0704
trigger times: 4
Loss after 183444900 batches: 0.0621
trigger times: 5
Loss after 183576000 batches: 0.0569
trigger times: 6
Loss after 183707100 batches: 0.0526
trigger times: 7
Loss after 183838200 batches: 0.0495
trigger times: 0
Loss after 183969300 batches: 0.0468
trigger times: 1
Loss after 184100400 batches: 0.0455
trigger times: 0
Loss after 184231500 batches: 0.0436
trigger times: 1
Loss after 184362600 batches: 0.0419
trigger times: 2
Loss after 184493700 batches: 0.0408
trigger times: 3
Loss after 184624800 batches: 0.0387
trigger times: 0
Loss after 184755900 batches: 0.0386
trigger times: 1
Loss after 184887000 batches: 0.0376
trigger times: 2
Loss after 185018100 batches: 0.0366
trigger times: 3
Loss after 185149200 batches: 0.0361
trigger times: 4
Loss after 185280300 batches: 0.0354
trigger times: 5
Loss after 185411400 batches: 0.0349
trigger times: 6
Loss after 185542500 batches: 0.0337
trigger times: 7
Loss after 185673600 batches: 0.0334
trigger times: 8
Loss after 185804700 batches: 0.0323
trigger times: 9
Loss after 185935800 batches: 0.0324
trigger times: 10
Loss after 186066900 batches: 0.0315
trigger times: 11
Loss after 186198000 batches: 0.0317
trigger times: 0
Loss after 186329100 batches: 0.0314
trigger times: 1
Loss after 186460200 batches: 0.0302
trigger times: 0
Loss after 186591300 batches: 0.0302
trigger times: 1
Loss after 186722400 batches: 0.0298
trigger times: 2
Loss after 186853500 batches: 0.0297
trigger times: 3
Loss after 186984600 batches: 0.0291
trigger times: 4
Loss after 187115700 batches: 0.0292
trigger times: 5
Loss after 187246800 batches: 0.0290
trigger times: 6
Loss after 187377900 batches: 0.0290
trigger times: 7
Loss after 187509000 batches: 0.0285
trigger times: 8
Loss after 187640100 batches: 0.0277
trigger times: 9
Loss after 187771200 batches: 0.0278
trigger times: 10
Loss after 187902300 batches: 0.0275
trigger times: 11
Loss after 188033400 batches: 0.0274
trigger times: 12
Loss after 188164500 batches: 0.0273
trigger times: 13
Loss after 188295600 batches: 0.0267
trigger times: 0
Loss after 188426700 batches: 0.0269
trigger times: 1
Loss after 188557800 batches: 0.0262
trigger times: 2
Loss after 188688900 batches: 0.0261
trigger times: 3
Loss after 188820000 batches: 0.0261
trigger times: 4
Loss after 188951100 batches: 0.0258
trigger times: 5
Loss after 189082200 batches: 0.0257
trigger times: 6
Loss after 189213300 batches: 0.0254
trigger times: 7
Loss after 189344400 batches: 0.0254
trigger times: 0
Loss after 189475500 batches: 0.0248
trigger times: 1
Loss after 189606600 batches: 0.0249
trigger times: 2
Loss after 189737700 batches: 0.0247
trigger times: 3
Loss after 189868800 batches: 0.0248
trigger times: 4
Loss after 189999900 batches: 0.0245
trigger times: 5
Loss after 190131000 batches: 0.0242
trigger times: 6
Loss after 190262100 batches: 0.0239
trigger times: 7
Loss after 190393200 batches: 0.0239
trigger times: 8
Loss after 190524300 batches: 0.0235
trigger times: 9
Loss after 190655400 batches: 0.0235
trigger times: 10
Loss after 190786500 batches: 0.0235
trigger times: 11
Loss after 190917600 batches: 0.0235
trigger times: 12
Loss after 191048700 batches: 0.0236
trigger times: 13
Loss after 191179800 batches: 0.0230
trigger times: 14
Loss after 191310900 batches: 0.0233
trigger times: 0
Loss after 191442000 batches: 0.0232
trigger times: 1
Loss after 191573100 batches: 0.0229
trigger times: 2
Loss after 191704200 batches: 0.0224
trigger times: 3
Loss after 191835300 batches: 0.0228
trigger times: 4
Loss after 191966400 batches: 0.0224
trigger times: 5
Loss after 192097500 batches: 0.0225
trigger times: 6
Loss after 192228600 batches: 0.0222
trigger times: 7
Loss after 192359700 batches: 0.0220
trigger times: 8
Loss after 192490800 batches: 0.0220
trigger times: 9
Loss after 192621900 batches: 0.0219
trigger times: 10
Loss after 192753000 batches: 0.0217
trigger times: 0
Loss after 192884100 batches: 0.0218
trigger times: 1
Loss after 193015200 batches: 0.0218
trigger times: 0
Loss after 193146300 batches: 0.0218
trigger times: 1
Loss after 193277400 batches: 0.0215
trigger times: 2
Loss after 193408500 batches: 0.0216
trigger times: 3
Loss after 193539600 batches: 0.0213
trigger times: 4
Loss after 193670700 batches: 0.0216
trigger times: 0
Loss after 193801800 batches: 0.0214
trigger times: 1
Loss after 193932900 batches: 0.0210
trigger times: 2
Loss after 194064000 batches: 0.0209
trigger times: 3
Loss after 194195100 batches: 0.0209
trigger times: 0
Loss after 194326200 batches: 0.0210
trigger times: 1
Loss after 194457300 batches: 0.0209
trigger times: 2
Loss after 194588400 batches: 0.0204
trigger times: 3
Loss after 194719500 batches: 0.0208
trigger times: 4
Loss after 194850600 batches: 0.0205
trigger times: 0
Loss after 194981700 batches: 0.0203
trigger times: 1
Loss after 195112800 batches: 0.0205
trigger times: 2
Loss after 195243900 batches: 0.0204
trigger times: 0
Loss after 195375000 batches: 0.0203
trigger times: 1
Loss after 195506100 batches: 0.0202
trigger times: 2
Loss after 195637200 batches: 0.0201
trigger times: 3
Loss after 195768300 batches: 0.0203
trigger times: 4
Loss after 195899400 batches: 0.0201
trigger times: 5
Loss after 196030500 batches: 0.0201
trigger times: 6
Loss after 196161600 batches: 0.0200
trigger times: 7
Loss after 196292700 batches: 0.0199
trigger times: 8
Loss after 196423800 batches: 0.0198
trigger times: 9
Loss after 196554900 batches: 0.0198
trigger times: 10
Loss after 196686000 batches: 0.0197
trigger times: 11
Loss after 196817100 batches: 0.0197
trigger times: 12
Loss after 196948200 batches: 0.0197
trigger times: 13
Loss after 197079300 batches: 0.0195
trigger times: 14
Loss after 197210400 batches: 0.0195
trigger times: 15
Loss after 197341500 batches: 0.0195
trigger times: 16
Loss after 197472600 batches: 0.0195
trigger times: 17
Loss after 197603700 batches: 0.0192
trigger times: 18
Loss after 197734800 batches: 0.0192
trigger times: 19
Loss after 197865900 batches: 0.0191
trigger times: 20
Early stopping!
Start to test process.
Loss after 197997000 batches: 0.0188
Time to train on one home:  831.506582736969
trigger times: 0
Loss after 198128100 batches: 0.5675
trigger times: 0
Loss after 198259200 batches: 0.2610
trigger times: 1
Loss after 198390300 batches: 0.1697
trigger times: 0
Loss after 198521400 batches: 0.1360
trigger times: 1
Loss after 198652500 batches: 0.1081
trigger times: 0
Loss after 198783600 batches: 0.0929
trigger times: 1
Loss after 198914700 batches: 0.0818
trigger times: 2
Loss after 199045800 batches: 0.0752
trigger times: 3
Loss after 199176900 batches: 0.0672
trigger times: 4
Loss after 199308000 batches: 0.0606
trigger times: 5
Loss after 199439100 batches: 0.0579
trigger times: 6
Loss after 199570200 batches: 0.0563
trigger times: 7
Loss after 199701300 batches: 0.0576
trigger times: 8
Loss after 199832400 batches: 0.0524
trigger times: 0
Loss after 199963500 batches: 0.0503
trigger times: 1
Loss after 200094600 batches: 0.0489
trigger times: 2
Loss after 200225700 batches: 0.0483
trigger times: 3
Loss after 200356800 batches: 0.0469
trigger times: 0
Loss after 200487900 batches: 0.0447
trigger times: 1
Loss after 200619000 batches: 0.0459
trigger times: 2
Loss after 200750100 batches: 0.0425
trigger times: 3
Loss after 200881200 batches: 0.0411
trigger times: 4
Loss after 201012300 batches: 0.0420
trigger times: 5
Loss after 201143400 batches: 0.0416
trigger times: 0
Loss after 201274500 batches: 0.0398
trigger times: 1
Loss after 201405600 batches: 0.0391
trigger times: 2
Loss after 201536700 batches: 0.0383
trigger times: 3
Loss after 201667800 batches: 0.0391
trigger times: 4
Loss after 201798900 batches: 0.0392
trigger times: 5
Loss after 201930000 batches: 0.0380
trigger times: 6
Loss after 202061100 batches: 0.0387
trigger times: 7
Loss after 202192200 batches: 0.0365
trigger times: 8
Loss after 202323300 batches: 0.0355
trigger times: 9
Loss after 202454400 batches: 0.0363
trigger times: 10
Loss after 202585500 batches: 0.0359
trigger times: 11
Loss after 202716600 batches: 0.0353
trigger times: 0
Loss after 202847700 batches: 0.0352
trigger times: 1
Loss after 202978800 batches: 0.0364
trigger times: 2
Loss after 203109900 batches: 0.0363
trigger times: 3
Loss after 203241000 batches: 0.0345
trigger times: 0
Loss after 203372100 batches: 0.0365
trigger times: 1
Loss after 203503200 batches: 0.0335
trigger times: 2
Loss after 203634300 batches: 0.0337
trigger times: 3
Loss after 203765400 batches: 0.0333
trigger times: 4
Loss after 203896500 batches: 0.0339
trigger times: 5
Loss after 204027600 batches: 0.0342
trigger times: 6
Loss after 204158700 batches: 0.0339
trigger times: 0
Loss after 204289800 batches: 0.0312
trigger times: 1
Loss after 204420900 batches: 0.0324
trigger times: 2
Loss after 204552000 batches: 0.0316
trigger times: 3
Loss after 204683100 batches: 0.0306
trigger times: 4
Loss after 204814200 batches: 0.0301
trigger times: 5
Loss after 204945300 batches: 0.0303
trigger times: 6
Loss after 205076400 batches: 0.0311
trigger times: 7
Loss after 205207500 batches: 0.0296
trigger times: 8
Loss after 205338600 batches: 0.0294
trigger times: 9
Loss after 205469700 batches: 0.0310
trigger times: 10
Loss after 205600800 batches: 0.0309
trigger times: 11
Loss after 205731900 batches: 0.0319
trigger times: 12
Loss after 205863000 batches: 0.0303
trigger times: 13
Loss after 205994100 batches: 0.0302
trigger times: 14
Loss after 206125200 batches: 0.0297
trigger times: 15
Loss after 206256300 batches: 0.0289
trigger times: 16
Loss after 206387400 batches: 0.0295
trigger times: 17
Loss after 206518500 batches: 0.0301
trigger times: 18
Loss after 206649600 batches: 0.0291
trigger times: 19
Loss after 206780700 batches: 0.0305
trigger times: 20
Early stopping!
Start to test process.
Loss after 206911800 batches: 0.0290
Time to train on one home:  492.2715151309967
trigger times: 0
Loss after 207042900 batches: 0.1320
trigger times: 1
Loss after 207174000 batches: 0.0433
trigger times: 2
Loss after 207305100 batches: 0.0313
trigger times: 3
Loss after 207436200 batches: 0.0264
trigger times: 4
Loss after 207567300 batches: 0.0240
trigger times: 5
Loss after 207698400 batches: 0.0224
trigger times: 6
Loss after 207829500 batches: 0.0210
trigger times: 7
Loss after 207960600 batches: 0.0195
trigger times: 8
Loss after 208091700 batches: 0.0191
trigger times: 9
Loss after 208222800 batches: 0.0183
trigger times: 10
Loss after 208353900 batches: 0.0176
trigger times: 11
Loss after 208485000 batches: 0.0170
trigger times: 12
Loss after 208616100 batches: 0.0162
trigger times: 13
Loss after 208747200 batches: 0.0159
trigger times: 14
Loss after 208878300 batches: 0.0154
trigger times: 15
Loss after 209009400 batches: 0.0150
trigger times: 16
Loss after 209140500 batches: 0.0147
trigger times: 17
Loss after 209271600 batches: 0.0145
trigger times: 18
Loss after 209402700 batches: 0.0143
trigger times: 19
Loss after 209533800 batches: 0.0143
trigger times: 20
Early stopping!
Start to test process.
Loss after 209664900 batches: 0.0136
Time to train on one home:  160.07506465911865
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274]]
Round_4_results:  [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1628 < 1629; dropping {'Training_Loss': 0.14268130317049207, 'Validation_Loss': 0.24374577071931627, 'Training_R2': 0.85660277324387, 'Validation_R2': 0.7732720886054708, 'Training_F1': 0.7934679026054442, 'Validation_F1': 0.75538704421898, 'Training_NEP': 0.41255158067888004, 'Validation_NEP': 0.5015760360784335, 'Training_NDE': 0.10765084343054698, 'Validation_NDE': 0.1805541568746442, 'Training_MAE': 13.662890174074267, 'Validation_MAE': 13.75529582077457, 'Training_MSE': 473.64682, 'Validation_MSE': 666.78156}.
trigger times: 0
Loss after 209796000 batches: 0.1427
trigger times: 0
Loss after 209927100 batches: 0.0460
trigger times: 1
Loss after 210058200 batches: 0.0322
trigger times: 0
Loss after 210189300 batches: 0.0270
trigger times: 0
Loss after 210320400 batches: 0.0244
trigger times: 1
Loss after 210451500 batches: 0.0222
trigger times: 2
Loss after 210582600 batches: 0.0211
trigger times: 3
Loss after 210713700 batches: 0.0198
trigger times: 4
Loss after 210844800 batches: 0.0190
trigger times: 5
Loss after 210975900 batches: 0.0182
trigger times: 6
Loss after 211107000 batches: 0.0176
trigger times: 7
Loss after 211238100 batches: 0.0168
trigger times: 8
Loss after 211369200 batches: 0.0164
trigger times: 9
Loss after 211500300 batches: 0.0165
trigger times: 10
Loss after 211631400 batches: 0.0160
trigger times: 11
Loss after 211762500 batches: 0.0154
trigger times: 12
Loss after 211893600 batches: 0.0153
trigger times: 13
Loss after 212024700 batches: 0.0151
trigger times: 14
Loss after 212155800 batches: 0.0149
trigger times: 15
Loss after 212286900 batches: 0.0143
trigger times: 16
Loss after 212418000 batches: 0.0141
trigger times: 17
Loss after 212549100 batches: 0.0140
trigger times: 18
Loss after 212680200 batches: 0.0137
trigger times: 19
Loss after 212811300 batches: 0.0140
trigger times: 0
Loss after 212942400 batches: 0.0134
trigger times: 1
Loss after 213073500 batches: 0.0134
trigger times: 2
Loss after 213204600 batches: 0.0129
trigger times: 3
Loss after 213335700 batches: 0.0127
trigger times: 4
Loss after 213466800 batches: 0.0129
trigger times: 5
Loss after 213597900 batches: 0.0123
trigger times: 6
Loss after 213729000 batches: 0.0125
trigger times: 7
Loss after 213860100 batches: 0.0127
trigger times: 8
Loss after 213991200 batches: 0.0124
trigger times: 9
Loss after 214122300 batches: 0.0122
trigger times: 10
Loss after 214253400 batches: 0.0121
trigger times: 11
Loss after 214384500 batches: 0.0120
trigger times: 12
Loss after 214515600 batches: 0.0118
trigger times: 13
Loss after 214646700 batches: 0.0117
trigger times: 14
Loss after 214777800 batches: 0.0114
trigger times: 15
Loss after 214908900 batches: 0.0117
trigger times: 16
Loss after 215040000 batches: 0.0118
trigger times: 17
Loss after 215171100 batches: 0.0113
trigger times: 18
Loss after 215302200 batches: 0.0114
trigger times: 19
Loss after 215433300 batches: 0.0115
trigger times: 20
Early stopping!
Start to test process.
Loss after 215564400 batches: 0.0110
Time to train on one home:  329.8210232257843
trigger times: 0
Loss after 215667000 batches: 0.4103
trigger times: 0
Loss after 215769600 batches: 0.1650
trigger times: 1
Loss after 215872200 batches: 0.1032
trigger times: 2
Loss after 215974800 batches: 0.0950
trigger times: 0
Loss after 216077400 batches: 0.0710
trigger times: 1
Loss after 216180000 batches: 0.0599
trigger times: 0
Loss after 216282600 batches: 0.0570
trigger times: 1
Loss after 216385200 batches: 0.0501
trigger times: 2
Loss after 216487800 batches: 0.0452
trigger times: 3
Loss after 216590400 batches: 0.0431
trigger times: 4
Loss after 216693000 batches: 0.0416
trigger times: 5
Loss after 216795600 batches: 0.0412
trigger times: 0
Loss after 216898200 batches: 0.0401
trigger times: 1
Loss after 217000800 batches: 0.0384
trigger times: 0
Loss after 217103400 batches: 0.0358
trigger times: 1
Loss after 217206000 batches: 0.0353
trigger times: 0
Loss after 217308600 batches: 0.0337
trigger times: 1
Loss after 217411200 batches: 0.0331
trigger times: 0
Loss after 217513800 batches: 0.0333
trigger times: 1
Loss after 217616400 batches: 0.0308
trigger times: 2
Loss after 217719000 batches: 0.0303
trigger times: 3
Loss after 217821600 batches: 0.0305
trigger times: 4
Loss after 217924200 batches: 0.0306
trigger times: 5
Loss after 218026800 batches: 0.0290
trigger times: 6
Loss after 218129400 batches: 0.0311
trigger times: 7
Loss after 218232000 batches: 0.0290
trigger times: 8
Loss after 218334600 batches: 0.0286
trigger times: 9
Loss after 218437200 batches: 0.0279
trigger times: 0
Loss after 218539800 batches: 0.0275
trigger times: 1
Loss after 218642400 batches: 0.0268
trigger times: 2
Loss after 218745000 batches: 0.0266
trigger times: 0
Loss after 218847600 batches: 0.0277
trigger times: 1
Loss after 218950200 batches: 0.0266
trigger times: 2
Loss after 219052800 batches: 0.0271
trigger times: 0
Loss after 219155400 batches: 0.0261
trigger times: 1
Loss after 219258000 batches: 0.0261
trigger times: 2
Loss after 219360600 batches: 0.0267
trigger times: 3
Loss after 219463200 batches: 0.0271
trigger times: 0
Loss after 219565800 batches: 0.0272
trigger times: 1
Loss after 219668400 batches: 0.0304
trigger times: 2
Loss after 219771000 batches: 0.0258
trigger times: 3
Loss after 219873600 batches: 0.0276
trigger times: 0
Loss after 219976200 batches: 0.0242
trigger times: 1
Loss after 220078800 batches: 0.0230
trigger times: 2
Loss after 220181400 batches: 0.0237
trigger times: 3
Loss after 220284000 batches: 0.0234
trigger times: 4
Loss after 220386600 batches: 0.0231
trigger times: 0
Loss after 220489200 batches: 0.0232
trigger times: 1
Loss after 220591800 batches: 0.0237
trigger times: 2
Loss after 220694400 batches: 0.0221
trigger times: 3
Loss after 220797000 batches: 0.0233
trigger times: 4
Loss after 220899600 batches: 0.0253
trigger times: 5
Loss after 221002200 batches: 0.0238
trigger times: 6
Loss after 221104800 batches: 0.0234
trigger times: 7
Loss after 221207400 batches: 0.0223
trigger times: 8
Loss after 221310000 batches: 0.0240
trigger times: 9
Loss after 221412600 batches: 0.0228
trigger times: 0
Loss after 221515200 batches: 0.0231
trigger times: 0
Loss after 221617800 batches: 0.0235
trigger times: 1
Loss after 221720400 batches: 0.0218
trigger times: 2
Loss after 221823000 batches: 0.0241
trigger times: 3
Loss after 221925600 batches: 0.0219
trigger times: 4
Loss after 222028200 batches: 0.0234
trigger times: 0
Loss after 222130800 batches: 0.0227
trigger times: 1
Loss after 222233400 batches: 0.0215
trigger times: 2
Loss after 222336000 batches: 0.0225
trigger times: 3
Loss after 222438600 batches: 0.0247
trigger times: 4
Loss after 222541200 batches: 0.0225
trigger times: 5
Loss after 222643800 batches: 0.0211
trigger times: 6
Loss after 222746400 batches: 0.0214
trigger times: 7
Loss after 222849000 batches: 0.0206
trigger times: 8
Loss after 222951600 batches: 0.0203
trigger times: 9
Loss after 223054200 batches: 0.0212
trigger times: 10
Loss after 223156800 batches: 0.0203
trigger times: 11
Loss after 223259400 batches: 0.0199
trigger times: 12
Loss after 223362000 batches: 0.0199
trigger times: 13
Loss after 223464600 batches: 0.0210
trigger times: 14
Loss after 223567200 batches: 0.0233
trigger times: 15
Loss after 223669800 batches: 0.0215
trigger times: 16
Loss after 223772400 batches: 0.0201
trigger times: 17
Loss after 223875000 batches: 0.0209
trigger times: 18
Loss after 223977600 batches: 0.0212
trigger times: 19
Loss after 224080200 batches: 0.0200
trigger times: 20
Early stopping!
Start to test process.
Loss after 224182800 batches: 0.0200
Time to train on one home:  488.3725600242615
trigger times: 0
Loss after 224313900 batches: 0.2069
trigger times: 1
Loss after 224445000 batches: 0.0722
trigger times: 2
Loss after 224576100 batches: 0.0495
trigger times: 0
Loss after 224707200 batches: 0.0401
trigger times: 1
Loss after 224838300 batches: 0.0361
trigger times: 0
Loss after 224969400 batches: 0.0326
trigger times: 1
Loss after 225100500 batches: 0.0308
trigger times: 2
Loss after 225231600 batches: 0.0291
trigger times: 3
Loss after 225362700 batches: 0.0277
trigger times: 4
Loss after 225493800 batches: 0.0267
trigger times: 5
Loss after 225624900 batches: 0.0255
trigger times: 6
Loss after 225756000 batches: 0.0253
trigger times: 7
Loss after 225887100 batches: 0.0240
trigger times: 8
Loss after 226018200 batches: 0.0232
trigger times: 9
Loss after 226149300 batches: 0.0229
trigger times: 10
Loss after 226280400 batches: 0.0226
trigger times: 11
Loss after 226411500 batches: 0.0220
trigger times: 12
Loss after 226542600 batches: 0.0215
trigger times: 13
Loss after 226673700 batches: 0.0207
trigger times: 14
Loss after 226804800 batches: 0.0208
trigger times: 15
Loss after 226935900 batches: 0.0202
trigger times: 16
Loss after 227067000 batches: 0.0201
trigger times: 17
Loss after 227198100 batches: 0.0201
trigger times: 18
Loss after 227329200 batches: 0.0198
trigger times: 19
Loss after 227460300 batches: 0.0194
trigger times: 20
Early stopping!
Start to test process.
Loss after 227591400 batches: 0.0193
Time to train on one home:  195.61195468902588
trigger times: 0
Loss after 227722500 batches: 0.3942
trigger times: 1
Loss after 227853600 batches: 0.1264
trigger times: 2
Loss after 227984700 batches: 0.0870
trigger times: 3
Loss after 228115800 batches: 0.0735
trigger times: 4
Loss after 228246900 batches: 0.0640
trigger times: 5
Loss after 228378000 batches: 0.0579
trigger times: 6
Loss after 228509100 batches: 0.0532
trigger times: 7
Loss after 228640200 batches: 0.0509
trigger times: 8
Loss after 228771300 batches: 0.0484
trigger times: 9
Loss after 228902400 batches: 0.0467
trigger times: 10
Loss after 229033500 batches: 0.0445
trigger times: 11
Loss after 229164600 batches: 0.0437
trigger times: 12
Loss after 229295700 batches: 0.0423
trigger times: 13
Loss after 229426800 batches: 0.0404
trigger times: 14
Loss after 229557900 batches: 0.0395
trigger times: 15
Loss after 229689000 batches: 0.0383
trigger times: 16
Loss after 229820100 batches: 0.0375
trigger times: 17
Loss after 229951200 batches: 0.0367
trigger times: 18
Loss after 230082300 batches: 0.0362
trigger times: 19
Loss after 230213400 batches: 0.0350
trigger times: 20
Early stopping!
Start to test process.
Loss after 230344500 batches: 0.0348
Time to train on one home:  160.2120270729065
trigger times: 0
Loss after 230473140 batches: 0.1661
trigger times: 0
Loss after 230601780 batches: 0.0534
trigger times: 0
Loss after 230730420 batches: 0.0376
trigger times: 0
Loss after 230859060 batches: 0.0320
trigger times: 1
Loss after 230987700 batches: 0.0282
trigger times: 2
Loss after 231116340 batches: 0.0260
trigger times: 3
Loss after 231244980 batches: 0.0244
trigger times: 4
Loss after 231373620 batches: 0.0231
trigger times: 5
Loss after 231502260 batches: 0.0222
trigger times: 6
Loss after 231630900 batches: 0.0210
trigger times: 7
Loss after 231759540 batches: 0.0207
trigger times: 8
Loss after 231888180 batches: 0.0199
trigger times: 9
Loss after 232016820 batches: 0.0189
trigger times: 10
Loss after 232145460 batches: 0.0192
trigger times: 11
Loss after 232274100 batches: 0.0186
trigger times: 12
Loss after 232402740 batches: 0.0181
trigger times: 13
Loss after 232531380 batches: 0.0174
trigger times: 14
Loss after 232660020 batches: 0.0177
trigger times: 15
Loss after 232788660 batches: 0.0171
trigger times: 16
Loss after 232917300 batches: 0.0171
trigger times: 17
Loss after 233045940 batches: 0.0167
trigger times: 18
Loss after 233174580 batches: 0.0163
trigger times: 19
Loss after 233303220 batches: 0.0163
trigger times: 20
Early stopping!
Start to test process.
Loss after 233431860 batches: 0.0160
Time to train on one home:  178.72210001945496
trigger times: 0
Loss after 233562960 batches: 0.4286
trigger times: 0
Loss after 233694060 batches: 0.1292
trigger times: 0
Loss after 233825160 batches: 0.0818
trigger times: 1
Loss after 233956260 batches: 0.0658
trigger times: 2
Loss after 234087360 batches: 0.0580
trigger times: 0
Loss after 234218460 batches: 0.0527
trigger times: 1
Loss after 234349560 batches: 0.0483
trigger times: 2
Loss after 234480660 batches: 0.0460
trigger times: 3
Loss after 234611760 batches: 0.0435
trigger times: 4
Loss after 234742860 batches: 0.0414
trigger times: 0
Loss after 234873960 batches: 0.0400
trigger times: 1
Loss after 235005060 batches: 0.0388
trigger times: 0
Loss after 235136160 batches: 0.0373
trigger times: 0
Loss after 235267260 batches: 0.0365
trigger times: 0
Loss after 235398360 batches: 0.0351
trigger times: 1
Loss after 235529460 batches: 0.0343
trigger times: 2
Loss after 235660560 batches: 0.0339
trigger times: 3
Loss after 235791660 batches: 0.0328
trigger times: 4
Loss after 235922760 batches: 0.0325
trigger times: 5
Loss after 236053860 batches: 0.0321
trigger times: 6
Loss after 236184960 batches: 0.0312
trigger times: 7
Loss after 236316060 batches: 0.0304
trigger times: 8
Loss after 236447160 batches: 0.0305
trigger times: 0
Loss after 236578260 batches: 0.0299
trigger times: 1
Loss after 236709360 batches: 0.0295
trigger times: 2
Loss after 236840460 batches: 0.0293
trigger times: 3
Loss after 236971560 batches: 0.0285
trigger times: 4
Loss after 237102660 batches: 0.0280
trigger times: 5
Loss after 237233760 batches: 0.0280
trigger times: 6
Loss after 237364860 batches: 0.0276
trigger times: 7
Loss after 237495960 batches: 0.0270
trigger times: 8
Loss after 237627060 batches: 0.0272
trigger times: 0
Loss after 237758160 batches: 0.0269
trigger times: 1
Loss after 237889260 batches: 0.0266
trigger times: 2
Loss after 238020360 batches: 0.0259
trigger times: 0
Loss after 238151460 batches: 0.0262
trigger times: 1
Loss after 238282560 batches: 0.0259
trigger times: 2
Loss after 238413660 batches: 0.0256
trigger times: 3
Loss after 238544760 batches: 0.0254
trigger times: 4
Loss after 238675860 batches: 0.0255
trigger times: 5
Loss after 238806960 batches: 0.0248
trigger times: 6
Loss after 238938060 batches: 0.0252
trigger times: 7
Loss after 239069160 batches: 0.0246
trigger times: 8
Loss after 239200260 batches: 0.0248
trigger times: 9
Loss after 239331360 batches: 0.0247
trigger times: 10
Loss after 239462460 batches: 0.0239
trigger times: 0
Loss after 239593560 batches: 0.0240
trigger times: 1
Loss after 239724660 batches: 0.0235
trigger times: 2
Loss after 239855760 batches: 0.0236
trigger times: 3
Loss after 239986860 batches: 0.0235
trigger times: 4
Loss after 240117960 batches: 0.0234
trigger times: 0
Loss after 240249060 batches: 0.0233
trigger times: 1
Loss after 240380160 batches: 0.0226
trigger times: 2
Loss after 240511260 batches: 0.0228
trigger times: 3
Loss after 240642360 batches: 0.0228
trigger times: 4
Loss after 240773460 batches: 0.0228
trigger times: 5
Loss after 240904560 batches: 0.0225
trigger times: 6
Loss after 241035660 batches: 0.0222
trigger times: 7
Loss after 241166760 batches: 0.0225
trigger times: 8
Loss after 241297860 batches: 0.0222
trigger times: 0
Loss after 241428960 batches: 0.0222
trigger times: 1
Loss after 241560060 batches: 0.0219
trigger times: 2
Loss after 241691160 batches: 0.0216
trigger times: 3
Loss after 241822260 batches: 0.0217
trigger times: 4
Loss after 241953360 batches: 0.0217
trigger times: 5
Loss after 242084460 batches: 0.0216
trigger times: 6
Loss after 242215560 batches: 0.0210
trigger times: 7
Loss after 242346660 batches: 0.0210
trigger times: 8
Loss after 242477760 batches: 0.0211
trigger times: 9
Loss after 242608860 batches: 0.0212
trigger times: 10
Loss after 242739960 batches: 0.0208
trigger times: 11
Loss after 242871060 batches: 0.0208
trigger times: 12
Loss after 243002160 batches: 0.0210
trigger times: 13
Loss after 243133260 batches: 0.0208
trigger times: 14
Loss after 243264360 batches: 0.0207
trigger times: 15
Loss after 243395460 batches: 0.0201
trigger times: 0
Loss after 243526560 batches: 0.0202
trigger times: 1
Loss after 243657660 batches: 0.0201
trigger times: 2
Loss after 243788760 batches: 0.0202
trigger times: 3
Loss after 243919860 batches: 0.0201
trigger times: 4
Loss after 244050960 batches: 0.0199
trigger times: 0
Loss after 244182060 batches: 0.0200
trigger times: 1
Loss after 244313160 batches: 0.0202
trigger times: 2
Loss after 244444260 batches: 0.0198
trigger times: 0
Loss after 244575360 batches: 0.0194
trigger times: 1
Loss after 244706460 batches: 0.0196
trigger times: 2
Loss after 244837560 batches: 0.0196
trigger times: 3
Loss after 244968660 batches: 0.0196
trigger times: 4
Loss after 245099760 batches: 0.0194
trigger times: 5
Loss after 245230860 batches: 0.0196
trigger times: 6
Loss after 245361960 batches: 0.0195
trigger times: 7
Loss after 245493060 batches: 0.0195
trigger times: 8
Loss after 245624160 batches: 0.0192
trigger times: 9
Loss after 245755260 batches: 0.0189
trigger times: 10
Loss after 245886360 batches: 0.0190
trigger times: 11
Loss after 246017460 batches: 0.0191
trigger times: 12
Loss after 246148560 batches: 0.0190
trigger times: 13
Loss after 246279660 batches: 0.0190
trigger times: 14
Loss after 246410760 batches: 0.0192
trigger times: 15
Loss after 246541860 batches: 0.0190
trigger times: 16
Loss after 246672960 batches: 0.0190
trigger times: 0
Loss after 246804060 batches: 0.0188
trigger times: 0
Loss after 246935160 batches: 0.0189
trigger times: 1
Loss after 247066260 batches: 0.0187
trigger times: 2
Loss after 247197360 batches: 0.0187
trigger times: 3
Loss after 247328460 batches: 0.0185
trigger times: 4
Loss after 247459560 batches: 0.0187
trigger times: 0
Loss after 247590660 batches: 0.0184
trigger times: 1
Loss after 247721760 batches: 0.0183
trigger times: 2
Loss after 247852860 batches: 0.0183
trigger times: 3
Loss after 247983960 batches: 0.0183
trigger times: 4
Loss after 248115060 batches: 0.0181
trigger times: 5
Loss after 248246160 batches: 0.0187
trigger times: 6
Loss after 248377260 batches: 0.0184
trigger times: 7
Loss after 248508360 batches: 0.0180
trigger times: 8
Loss after 248639460 batches: 0.0181
trigger times: 9
Loss after 248770560 batches: 0.0181
trigger times: 10
Loss after 248901660 batches: 0.0182
trigger times: 11
Loss after 249032760 batches: 0.0179
trigger times: 12
Loss after 249163860 batches: 0.0179
trigger times: 13
Loss after 249294960 batches: 0.0176
trigger times: 14
Loss after 249426060 batches: 0.0173
trigger times: 15
Loss after 249557160 batches: 0.0175
trigger times: 16
Loss after 249688260 batches: 0.0178
trigger times: 0
Loss after 249819360 batches: 0.0177
trigger times: 1
Loss after 249950460 batches: 0.0177
trigger times: 2
Loss after 250081560 batches: 0.0176
trigger times: 3
Loss after 250212660 batches: 0.0174
trigger times: 4
Loss after 250343760 batches: 0.0171
trigger times: 5
Loss after 250474860 batches: 0.0170
trigger times: 6
Loss after 250605960 batches: 0.0172
trigger times: 7
Loss after 250737060 batches: 0.0175
trigger times: 8
Loss after 250868160 batches: 0.0171
trigger times: 9
Loss after 250999260 batches: 0.0170
trigger times: 10
Loss after 251130360 batches: 0.0171
trigger times: 11
Loss after 251261460 batches: 0.0171
trigger times: 12
Loss after 251392560 batches: 0.0167
trigger times: 13
Loss after 251523660 batches: 0.0167
trigger times: 14
Loss after 251654760 batches: 0.0171
trigger times: 15
Loss after 251785860 batches: 0.0170
trigger times: 16
Loss after 251916960 batches: 0.0167
trigger times: 17
Loss after 252048060 batches: 0.0167
trigger times: 18
Loss after 252179160 batches: 0.0167
trigger times: 19
Loss after 252310260 batches: 0.0164
trigger times: 20
Early stopping!
Start to test process.
Loss after 252441360 batches: 0.0164
Time to train on one home:  1035.440733909607
trigger times: 0
Loss after 252572460 batches: 0.4481
trigger times: 0
Loss after 252703560 batches: 0.1827
trigger times: 0
Loss after 252834660 batches: 0.1157
trigger times: 0
Loss after 252965760 batches: 0.0891
trigger times: 0
Loss after 253096860 batches: 0.0718
trigger times: 0
Loss after 253227960 batches: 0.0615
trigger times: 1
Loss after 253359060 batches: 0.0571
trigger times: 2
Loss after 253490160 batches: 0.0542
trigger times: 3
Loss after 253621260 batches: 0.0520
trigger times: 0
Loss after 253752360 batches: 0.0476
trigger times: 1
Loss after 253883460 batches: 0.0460
trigger times: 2
Loss after 254014560 batches: 0.0453
trigger times: 0
Loss after 254145660 batches: 0.0420
trigger times: 0
Loss after 254276760 batches: 0.0407
trigger times: 1
Loss after 254407860 batches: 0.0424
trigger times: 2
Loss after 254538960 batches: 0.0393
trigger times: 3
Loss after 254670060 batches: 0.0370
trigger times: 4
Loss after 254801160 batches: 0.0364
trigger times: 5
Loss after 254932260 batches: 0.0368
trigger times: 0
Loss after 255063360 batches: 0.0353
trigger times: 0
Loss after 255194460 batches: 0.0339
trigger times: 1
Loss after 255325560 batches: 0.0345
trigger times: 2
Loss after 255456660 batches: 0.0338
trigger times: 0
Loss after 255587760 batches: 0.0343
trigger times: 1
Loss after 255718860 batches: 0.0336
trigger times: 2
Loss after 255849960 batches: 0.0325
trigger times: 3
Loss after 255981060 batches: 0.0320
trigger times: 4
Loss after 256112160 batches: 0.0334
trigger times: 0
Loss after 256243260 batches: 0.0321
trigger times: 1
Loss after 256374360 batches: 0.0309
trigger times: 2
Loss after 256505460 batches: 0.0321
trigger times: 3
Loss after 256636560 batches: 0.0307
trigger times: 4
Loss after 256767660 batches: 0.0307
trigger times: 5
Loss after 256898760 batches: 0.0327
trigger times: 6
Loss after 257029860 batches: 0.0317
trigger times: 7
Loss after 257160960 batches: 0.0307
trigger times: 0
Loss after 257292060 batches: 0.0297
trigger times: 1
Loss after 257423160 batches: 0.0302
trigger times: 2
Loss after 257554260 batches: 0.0302
trigger times: 3
Loss after 257685360 batches: 0.0301
trigger times: 4
Loss after 257816460 batches: 0.0294
trigger times: 5
Loss after 257947560 batches: 0.0294
trigger times: 6
Loss after 258078660 batches: 0.0298
trigger times: 7
Loss after 258209760 batches: 0.0287
trigger times: 8
Loss after 258340860 batches: 0.0284
trigger times: 9
Loss after 258471960 batches: 0.0280
trigger times: 10
Loss after 258603060 batches: 0.0280
trigger times: 0
Loss after 258734160 batches: 0.0283
trigger times: 1
Loss after 258865260 batches: 0.0269
trigger times: 2
Loss after 258996360 batches: 0.0271
trigger times: 3
Loss after 259127460 batches: 0.0274
trigger times: 4
Loss after 259258560 batches: 0.0269
trigger times: 5
Loss after 259389660 batches: 0.0266
trigger times: 6
Loss after 259520760 batches: 0.0272
trigger times: 7
Loss after 259651860 batches: 0.0262
trigger times: 8
Loss after 259782960 batches: 0.0271
trigger times: 9
Loss after 259914060 batches: 0.0264
trigger times: 10
Loss after 260045160 batches: 0.0259
trigger times: 11
Loss after 260176260 batches: 0.0252
trigger times: 12
Loss after 260307360 batches: 0.0256
trigger times: 13
Loss after 260438460 batches: 0.0258
trigger times: 14
Loss after 260569560 batches: 0.0260
trigger times: 15
Loss after 260700660 batches: 0.0269
trigger times: 16
Loss after 260831760 batches: 0.0252
trigger times: 17
Loss after 260962860 batches: 0.0246
trigger times: 18
Loss after 261093960 batches: 0.0244
trigger times: 19
Loss after 261225060 batches: 0.0250
trigger times: 20
Early stopping!
Start to test process.
Loss after 261356160 batches: 0.0262
Time to train on one home:  491.54197931289673
trigger times: 0
Loss after 261487260 batches: 0.1157
trigger times: 1
Loss after 261618360 batches: 0.0353
trigger times: 2
Loss after 261749460 batches: 0.0260
trigger times: 3
Loss after 261880560 batches: 0.0220
trigger times: 4
Loss after 262011660 batches: 0.0203
trigger times: 5
Loss after 262142760 batches: 0.0186
trigger times: 6
Loss after 262273860 batches: 0.0174
trigger times: 7
Loss after 262404960 batches: 0.0171
trigger times: 8
Loss after 262536060 batches: 0.0167
trigger times: 9
Loss after 262667160 batches: 0.0157
trigger times: 10
Loss after 262798260 batches: 0.0150
trigger times: 11
Loss after 262929360 batches: 0.0146
trigger times: 12
Loss after 263060460 batches: 0.0142
trigger times: 13
Loss after 263191560 batches: 0.0138
trigger times: 14
Loss after 263322660 batches: 0.0138
trigger times: 15
Loss after 263453760 batches: 0.0134
trigger times: 16
Loss after 263584860 batches: 0.0129
trigger times: 17
Loss after 263715960 batches: 0.0127
trigger times: 18
Loss after 263847060 batches: 0.0127
trigger times: 19
Loss after 263978160 batches: 0.0123
trigger times: 20
Early stopping!
Start to test process.
Loss after 264109260 batches: 0.0123
Time to train on one home:  160.06520700454712
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564]]
Round_5_results:  [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2062 < 2063; dropping {'Training_Loss': 0.13161581649251705, 'Validation_Loss': 0.24326569007502663, 'Training_R2': 0.8676177521876802, 'Validation_R2': 0.7739071454166542, 'Training_F1': 0.8049637402519856, 'Validation_F1': 0.7182939496213328, 'Training_NEP': 0.39034475941697794, 'Validation_NEP': 0.5226067195069918, 'Training_NDE': 0.09938170322124903, 'Validation_NDE': 0.18004843110667204, 'Training_MAE': 12.927444294755647, 'Validation_MAE': 14.332044411346477, 'Training_MSE': 437.2639, 'Validation_MSE': 664.9139}.
trigger times: 0
Loss after 264240360 batches: 0.1316
trigger times: 0
Loss after 264371460 batches: 0.0399
trigger times: 0
Loss after 264502560 batches: 0.0291
trigger times: 0
Loss after 264633660 batches: 0.0238
trigger times: 0
Loss after 264764760 batches: 0.0213
trigger times: 0
Loss after 264895860 batches: 0.0199
trigger times: 0
Loss after 265026960 batches: 0.0187
trigger times: 1
Loss after 265158060 batches: 0.0183
trigger times: 0
Loss after 265289160 batches: 0.0175
trigger times: 0
Loss after 265420260 batches: 0.0164
trigger times: 0
Loss after 265551360 batches: 0.0158
trigger times: 1
Loss after 265682460 batches: 0.0154
trigger times: 2
Loss after 265813560 batches: 0.0150
trigger times: 3
Loss after 265944660 batches: 0.0152
trigger times: 4
Loss after 266075760 batches: 0.0149
trigger times: 5
Loss after 266206860 batches: 0.0146
trigger times: 6
Loss after 266337960 batches: 0.0141
trigger times: 7
Loss after 266469060 batches: 0.0135
trigger times: 8
Loss after 266600160 batches: 0.0136
trigger times: 9
Loss after 266731260 batches: 0.0132
trigger times: 10
Loss after 266862360 batches: 0.0130
trigger times: 11
Loss after 266993460 batches: 0.0130
trigger times: 12
Loss after 267124560 batches: 0.0131
trigger times: 13
Loss after 267255660 batches: 0.0127
trigger times: 14
Loss after 267386760 batches: 0.0125
trigger times: 15
Loss after 267517860 batches: 0.0125
trigger times: 16
Loss after 267648960 batches: 0.0122
trigger times: 17
Loss after 267780060 batches: 0.0122
trigger times: 18
Loss after 267911160 batches: 0.0120
trigger times: 19
Loss after 268042260 batches: 0.0119
trigger times: 20
Early stopping!
Start to test process.
Loss after 268173360 batches: 0.0117
Time to train on one home:  230.63249611854553
trigger times: 0
Loss after 268275960 batches: 0.3598
trigger times: 0
Loss after 268378560 batches: 0.1286
trigger times: 0
Loss after 268481160 batches: 0.0827
trigger times: 0
Loss after 268583760 batches: 0.0644
trigger times: 1
Loss after 268686360 batches: 0.0545
trigger times: 2
Loss after 268788960 batches: 0.0462
trigger times: 3
Loss after 268891560 batches: 0.0427
trigger times: 0
Loss after 268994160 batches: 0.0395
trigger times: 1
Loss after 269096760 batches: 0.0373
trigger times: 2
Loss after 269199360 batches: 0.0364
trigger times: 3
Loss after 269301960 batches: 0.0347
trigger times: 4
Loss after 269404560 batches: 0.0323
trigger times: 5
Loss after 269507160 batches: 0.0316
trigger times: 6
Loss after 269609760 batches: 0.0366
trigger times: 0
Loss after 269712360 batches: 0.0308
trigger times: 0
Loss after 269814960 batches: 0.0289
trigger times: 0
Loss after 269917560 batches: 0.0300
trigger times: 0
Loss after 270020160 batches: 0.0335
trigger times: 0
Loss after 270122760 batches: 0.0291
trigger times: 0
Loss after 270225360 batches: 0.0313
trigger times: 1
Loss after 270327960 batches: 0.0297
trigger times: 2
Loss after 270430560 batches: 0.0268
trigger times: 0
Loss after 270533160 batches: 0.0275
trigger times: 0
Loss after 270635760 batches: 0.0278
trigger times: 1
Loss after 270738360 batches: 0.0261
trigger times: 2
Loss after 270840960 batches: 0.0249
trigger times: 3
Loss after 270943560 batches: 0.0242
trigger times: 4
Loss after 271046160 batches: 0.0236
trigger times: 5
Loss after 271148760 batches: 0.0245
trigger times: 6
Loss after 271251360 batches: 0.0270
trigger times: 7
Loss after 271353960 batches: 0.0250
trigger times: 8
Loss after 271456560 batches: 0.0236
trigger times: 9
Loss after 271559160 batches: 0.0236
trigger times: 10
Loss after 271661760 batches: 0.0241
trigger times: 11
Loss after 271764360 batches: 0.0227
trigger times: 0
Loss after 271866960 batches: 0.0218
trigger times: 1
Loss after 271969560 batches: 0.0221
trigger times: 2
Loss after 272072160 batches: 0.0232
trigger times: 3
Loss after 272174760 batches: 0.0220
trigger times: 4
Loss after 272277360 batches: 0.0223
trigger times: 5
Loss after 272379960 batches: 0.0229
trigger times: 6
Loss after 272482560 batches: 0.0217
trigger times: 0
Loss after 272585160 batches: 0.0210
trigger times: 1
Loss after 272687760 batches: 0.0218
trigger times: 2
Loss after 272790360 batches: 0.0236
trigger times: 3
Loss after 272892960 batches: 0.0207
trigger times: 4
Loss after 272995560 batches: 0.0207
trigger times: 5
Loss after 273098160 batches: 0.0215
trigger times: 6
Loss after 273200760 batches: 0.0209
trigger times: 7
Loss after 273303360 batches: 0.0205
trigger times: 8
Loss after 273405960 batches: 0.0203
trigger times: 9
Loss after 273508560 batches: 0.0212
trigger times: 10
Loss after 273611160 batches: 0.0221
trigger times: 11
Loss after 273713760 batches: 0.0224
trigger times: 12
Loss after 273816360 batches: 0.0207
trigger times: 13
Loss after 273918960 batches: 0.0196
trigger times: 14
Loss after 274021560 batches: 0.0196
trigger times: 15
Loss after 274124160 batches: 0.0205
trigger times: 16
Loss after 274226760 batches: 0.0213
trigger times: 17
Loss after 274329360 batches: 0.0204
trigger times: 18
Loss after 274431960 batches: 0.0205
trigger times: 19
Loss after 274534560 batches: 0.0194
trigger times: 20
Early stopping!
Start to test process.
Loss after 274637160 batches: 0.0195
Time to train on one home:  368.63168478012085
trigger times: 0
Loss after 274768260 batches: 0.1789
trigger times: 0
Loss after 274899360 batches: 0.0554
trigger times: 1
Loss after 275030460 batches: 0.0401
trigger times: 2
Loss after 275161560 batches: 0.0344
trigger times: 3
Loss after 275292660 batches: 0.0309
trigger times: 4
Loss after 275423760 batches: 0.0286
trigger times: 0
Loss after 275554860 batches: 0.0269
trigger times: 1
Loss after 275685960 batches: 0.0254
trigger times: 2
Loss after 275817060 batches: 0.0245
trigger times: 3
Loss after 275948160 batches: 0.0238
trigger times: 4
Loss after 276079260 batches: 0.0233
trigger times: 5
Loss after 276210360 batches: 0.0222
trigger times: 6
Loss after 276341460 batches: 0.0216
trigger times: 7
Loss after 276472560 batches: 0.0214
trigger times: 8
Loss after 276603660 batches: 0.0211
trigger times: 9
Loss after 276734760 batches: 0.0205
trigger times: 10
Loss after 276865860 batches: 0.0203
trigger times: 11
Loss after 276996960 batches: 0.0200
trigger times: 12
Loss after 277128060 batches: 0.0194
trigger times: 13
Loss after 277259160 batches: 0.0192
trigger times: 14
Loss after 277390260 batches: 0.0191
trigger times: 15
Loss after 277521360 batches: 0.0188
trigger times: 16
Loss after 277652460 batches: 0.0186
trigger times: 17
Loss after 277783560 batches: 0.0184
trigger times: 18
Loss after 277914660 batches: 0.0183
trigger times: 19
Loss after 278045760 batches: 0.0181
trigger times: 20
Early stopping!
Start to test process.
Loss after 278176860 batches: 0.0176
Time to train on one home:  202.30436897277832
trigger times: 0
Loss after 278307960 batches: 0.3700
trigger times: 0
Loss after 278439060 batches: 0.1130
trigger times: 1
Loss after 278570160 batches: 0.0785
trigger times: 2
Loss after 278701260 batches: 0.0660
trigger times: 3
Loss after 278832360 batches: 0.0586
trigger times: 4
Loss after 278963460 batches: 0.0532
trigger times: 5
Loss after 279094560 batches: 0.0492
trigger times: 6
Loss after 279225660 batches: 0.0468
trigger times: 7
Loss after 279356760 batches: 0.0451
trigger times: 8
Loss after 279487860 batches: 0.0431
trigger times: 9
Loss after 279618960 batches: 0.0411
trigger times: 0
Loss after 279750060 batches: 0.0401
trigger times: 1
Loss after 279881160 batches: 0.0385
trigger times: 2
Loss after 280012260 batches: 0.0369
trigger times: 3
Loss after 280143360 batches: 0.0368
trigger times: 4
Loss after 280274460 batches: 0.0354
trigger times: 5
Loss after 280405560 batches: 0.0349
trigger times: 6
Loss after 280536660 batches: 0.0345
trigger times: 7
Loss after 280667760 batches: 0.0334
trigger times: 8
Loss after 280798860 batches: 0.0331
trigger times: 9
Loss after 280929960 batches: 0.0333
trigger times: 10
Loss after 281061060 batches: 0.0325
trigger times: 0
Loss after 281192160 batches: 0.0318
trigger times: 1
Loss after 281323260 batches: 0.0311
trigger times: 2
Loss after 281454360 batches: 0.0306
trigger times: 0
Loss after 281585460 batches: 0.0303
trigger times: 1
Loss after 281716560 batches: 0.0298
trigger times: 2
Loss after 281847660 batches: 0.0294
trigger times: 3
Loss after 281978760 batches: 0.0290
trigger times: 4
Loss after 282109860 batches: 0.0286
trigger times: 5
Loss after 282240960 batches: 0.0287
trigger times: 6
Loss after 282372060 batches: 0.0287
trigger times: 7
Loss after 282503160 batches: 0.0285
trigger times: 0
Loss after 282634260 batches: 0.0276
trigger times: 1
Loss after 282765360 batches: 0.0274
trigger times: 2
Loss after 282896460 batches: 0.0271
trigger times: 0
Loss after 283027560 batches: 0.0273
trigger times: 1
Loss after 283158660 batches: 0.0268
trigger times: 2
Loss after 283289760 batches: 0.0265
trigger times: 3
Loss after 283420860 batches: 0.0261
trigger times: 4
Loss after 283551960 batches: 0.0258
trigger times: 5
Loss after 283683060 batches: 0.0258
trigger times: 6
Loss after 283814160 batches: 0.0256
trigger times: 7
Loss after 283945260 batches: 0.0253
trigger times: 8
Loss after 284076360 batches: 0.0250
trigger times: 9
Loss after 284207460 batches: 0.0253
trigger times: 10
Loss after 284338560 batches: 0.0251
trigger times: 11
Loss after 284469660 batches: 0.0250
trigger times: 12
Loss after 284600760 batches: 0.0245
trigger times: 13
Loss after 284731860 batches: 0.0246
trigger times: 14
Loss after 284862960 batches: 0.0240
trigger times: 15
Loss after 284994060 batches: 0.0247
trigger times: 16
Loss after 285125160 batches: 0.0247
trigger times: 17
Loss after 285256260 batches: 0.0238
trigger times: 18
Loss after 285387360 batches: 0.0235
trigger times: 0
Loss after 285518460 batches: 0.0237
trigger times: 1
Loss after 285649560 batches: 0.0234
trigger times: 2
Loss after 285780660 batches: 0.0231
trigger times: 3
Loss after 285911760 batches: 0.0232
trigger times: 0
Loss after 286042860 batches: 0.0232
trigger times: 1
Loss after 286173960 batches: 0.0226
trigger times: 2
Loss after 286305060 batches: 0.0229
trigger times: 3
Loss after 286436160 batches: 0.0228
trigger times: 4
Loss after 286567260 batches: 0.0228
trigger times: 5
Loss after 286698360 batches: 0.0229
trigger times: 6
Loss after 286829460 batches: 0.0226
trigger times: 7
Loss after 286960560 batches: 0.0222
trigger times: 0
Loss after 287091660 batches: 0.0222
trigger times: 1
Loss after 287222760 batches: 0.0220
trigger times: 2
Loss after 287353860 batches: 0.0219
trigger times: 3
Loss after 287484960 batches: 0.0217
trigger times: 4
Loss after 287616060 batches: 0.0213
trigger times: 5
Loss after 287747160 batches: 0.0214
trigger times: 6
Loss after 287878260 batches: 0.0216
trigger times: 7
Loss after 288009360 batches: 0.0216
trigger times: 8
Loss after 288140460 batches: 0.0215
trigger times: 9
Loss after 288271560 batches: 0.0214
trigger times: 10
Loss after 288402660 batches: 0.0211
trigger times: 11
Loss after 288533760 batches: 0.0211
trigger times: 12
Loss after 288664860 batches: 0.0214
trigger times: 13
Loss after 288795960 batches: 0.0216
trigger times: 14
Loss after 288927060 batches: 0.0209
trigger times: 15
Loss after 289058160 batches: 0.0208
trigger times: 16
Loss after 289189260 batches: 0.0207
trigger times: 17
Loss after 289320360 batches: 0.0208
trigger times: 18
Loss after 289451460 batches: 0.0209
trigger times: 19
Loss after 289582560 batches: 0.0209
trigger times: 0
Loss after 289713660 batches: 0.0206
trigger times: 1
Loss after 289844760 batches: 0.0207
trigger times: 2
Loss after 289975860 batches: 0.0208
trigger times: 3
Loss after 290106960 batches: 0.0204
trigger times: 4
Loss after 290238060 batches: 0.0200
trigger times: 5
Loss after 290369160 batches: 0.0202
trigger times: 6
Loss after 290500260 batches: 0.0201
trigger times: 7
Loss after 290631360 batches: 0.0201
trigger times: 8
Loss after 290762460 batches: 0.0200
trigger times: 9
Loss after 290893560 batches: 0.0200
trigger times: 10
Loss after 291024660 batches: 0.0202
trigger times: 11
Loss after 291155760 batches: 0.0199
trigger times: 12
Loss after 291286860 batches: 0.0195
trigger times: 13
Loss after 291417960 batches: 0.0197
trigger times: 14
Loss after 291549060 batches: 0.0198
trigger times: 15
Loss after 291680160 batches: 0.0196
trigger times: 16
Loss after 291811260 batches: 0.0197
trigger times: 17
Loss after 291942360 batches: 0.0197
trigger times: 18
Loss after 292073460 batches: 0.0192
trigger times: 19
Loss after 292204560 batches: 0.0193
trigger times: 20
Early stopping!
Start to test process.
Loss after 292335660 batches: 0.0194
Time to train on one home:  774.9198861122131
trigger times: 0
Loss after 292464300 batches: 0.2197
trigger times: 0
Loss after 292592940 batches: 0.0645
trigger times: 0
Loss after 292721580 batches: 0.0422
trigger times: 1
Loss after 292850220 batches: 0.0345
trigger times: 2
Loss after 292978860 batches: 0.0312
trigger times: 3
Loss after 293107500 batches: 0.0279
trigger times: 4
Loss after 293236140 batches: 0.0265
trigger times: 0
Loss after 293364780 batches: 0.0251
trigger times: 1
Loss after 293493420 batches: 0.0239
trigger times: 2
Loss after 293622060 batches: 0.0230
trigger times: 0
Loss after 293750700 batches: 0.0222
trigger times: 1
Loss after 293879340 batches: 0.0210
trigger times: 0
Loss after 294007980 batches: 0.0204
trigger times: 1
Loss after 294136620 batches: 0.0204
trigger times: 2
Loss after 294265260 batches: 0.0195
trigger times: 3
Loss after 294393900 batches: 0.0190
trigger times: 4
Loss after 294522540 batches: 0.0191
trigger times: 5
Loss after 294651180 batches: 0.0183
trigger times: 6
Loss after 294779820 batches: 0.0182
trigger times: 7
Loss after 294908460 batches: 0.0176
trigger times: 8
Loss after 295037100 batches: 0.0176
trigger times: 9
Loss after 295165740 batches: 0.0171
trigger times: 10
Loss after 295294380 batches: 0.0169
trigger times: 11
Loss after 295423020 batches: 0.0166
trigger times: 12
Loss after 295551660 batches: 0.0164
trigger times: 0
Loss after 295680300 batches: 0.0165
trigger times: 1
Loss after 295808940 batches: 0.0161
trigger times: 2
Loss after 295937580 batches: 0.0159
trigger times: 3
Loss after 296066220 batches: 0.0159
trigger times: 4
Loss after 296194860 batches: 0.0155
trigger times: 5
Loss after 296323500 batches: 0.0155
trigger times: 6
Loss after 296452140 batches: 0.0153
trigger times: 0
Loss after 296580780 batches: 0.0151
trigger times: 1
Loss after 296709420 batches: 0.0152
trigger times: 2
Loss after 296838060 batches: 0.0149
trigger times: 3
Loss after 296966700 batches: 0.0148
trigger times: 4
Loss after 297095340 batches: 0.0145
trigger times: 5
Loss after 297223980 batches: 0.0143
trigger times: 6
Loss after 297352620 batches: 0.0144
trigger times: 7
Loss after 297481260 batches: 0.0144
trigger times: 8
Loss after 297609900 batches: 0.0144
trigger times: 9
Loss after 297738540 batches: 0.0140
trigger times: 10
Loss after 297867180 batches: 0.0141
trigger times: 11
Loss after 297995820 batches: 0.0141
trigger times: 12
Loss after 298124460 batches: 0.0140
trigger times: 13
Loss after 298253100 batches: 0.0137
trigger times: 14
Loss after 298381740 batches: 0.0137
trigger times: 15
Loss after 298510380 batches: 0.0134
trigger times: 16
Loss after 298639020 batches: 0.0133
trigger times: 17
Loss after 298767660 batches: 0.0133
trigger times: 18
Loss after 298896300 batches: 0.0131
trigger times: 19
Loss after 299024940 batches: 0.0131
trigger times: 20
Early stopping!
Start to test process.
Loss after 299153580 batches: 0.0131
Time to train on one home:  379.858371257782
trigger times: 0
Loss after 299284680 batches: 0.3252
trigger times: 0
Loss after 299415780 batches: 0.0888
trigger times: 0
Loss after 299546880 batches: 0.0587
trigger times: 1
Loss after 299677980 batches: 0.0493
trigger times: 0
Loss after 299809080 batches: 0.0438
trigger times: 0
Loss after 299940180 batches: 0.0401
trigger times: 0
Loss after 300071280 batches: 0.0372
trigger times: 1
Loss after 300202380 batches: 0.0355
trigger times: 2
Loss after 300333480 batches: 0.0345
trigger times: 3
Loss after 300464580 batches: 0.0328
trigger times: 4
Loss after 300595680 batches: 0.0317
trigger times: 0
Loss after 300726780 batches: 0.0309
trigger times: 1
Loss after 300857880 batches: 0.0298
trigger times: 2
Loss after 300988980 batches: 0.0293
trigger times: 0
Loss after 301120080 batches: 0.0285
trigger times: 1
Loss after 301251180 batches: 0.0285
trigger times: 0
Loss after 301382280 batches: 0.0276
trigger times: 1
Loss after 301513380 batches: 0.0271
trigger times: 2
Loss after 301644480 batches: 0.0268
trigger times: 3
Loss after 301775580 batches: 0.0262
trigger times: 4
Loss after 301906680 batches: 0.0259
trigger times: 5
Loss after 302037780 batches: 0.0252
trigger times: 6
Loss after 302168880 batches: 0.0254
trigger times: 0
Loss after 302299980 batches: 0.0248
trigger times: 1
Loss after 302431080 batches: 0.0249
trigger times: 2
Loss after 302562180 batches: 0.0244
trigger times: 3
Loss after 302693280 batches: 0.0241
trigger times: 4
Loss after 302824380 batches: 0.0239
trigger times: 5
Loss after 302955480 batches: 0.0238
trigger times: 6
Loss after 303086580 batches: 0.0237
trigger times: 7
Loss after 303217680 batches: 0.0231
trigger times: 8
Loss after 303348780 batches: 0.0227
trigger times: 9
Loss after 303479880 batches: 0.0227
trigger times: 10
Loss after 303610980 batches: 0.0227
trigger times: 11
Loss after 303742080 batches: 0.0226
trigger times: 12
Loss after 303873180 batches: 0.0223
trigger times: 13
Loss after 304004280 batches: 0.0218
trigger times: 14
Loss after 304135380 batches: 0.0219
trigger times: 15
Loss after 304266480 batches: 0.0215
trigger times: 16
Loss after 304397580 batches: 0.0218
trigger times: 17
Loss after 304528680 batches: 0.0215
trigger times: 18
Loss after 304659780 batches: 0.0212
trigger times: 19
Loss after 304790880 batches: 0.0212
trigger times: 20
Early stopping!
Start to test process.
Loss after 304921980 batches: 0.0210
Time to train on one home:  322.3156690597534
trigger times: 0
Loss after 305053080 batches: 0.3713
trigger times: 1
Loss after 305184180 batches: 0.1490
trigger times: 2
Loss after 305315280 batches: 0.0849
trigger times: 0
Loss after 305446380 batches: 0.0644
trigger times: 1
Loss after 305577480 batches: 0.0564
trigger times: 0
Loss after 305708580 batches: 0.0541
trigger times: 1
Loss after 305839680 batches: 0.0499
trigger times: 2
Loss after 305970780 batches: 0.0464
trigger times: 0
Loss after 306101880 batches: 0.0408
trigger times: 1
Loss after 306232980 batches: 0.0391
trigger times: 0
Loss after 306364080 batches: 0.0369
trigger times: 0
Loss after 306495180 batches: 0.0372
trigger times: 1
Loss after 306626280 batches: 0.0379
trigger times: 2
Loss after 306757380 batches: 0.0345
trigger times: 0
Loss after 306888480 batches: 0.0347
trigger times: 1
Loss after 307019580 batches: 0.0353
trigger times: 2
Loss after 307150680 batches: 0.0351
trigger times: 3
Loss after 307281780 batches: 0.0324
trigger times: 4
Loss after 307412880 batches: 0.0326
trigger times: 5
Loss after 307543980 batches: 0.0308
trigger times: 6
Loss after 307675080 batches: 0.0309
trigger times: 7
Loss after 307806180 batches: 0.0322
trigger times: 8
Loss after 307937280 batches: 0.0306
trigger times: 0
Loss after 308068380 batches: 0.0299
trigger times: 0
Loss after 308199480 batches: 0.0300
trigger times: 1
Loss after 308330580 batches: 0.0288
trigger times: 0
Loss after 308461680 batches: 0.0292
trigger times: 1
Loss after 308592780 batches: 0.0290
trigger times: 2
Loss after 308723880 batches: 0.0287
trigger times: 3
Loss after 308854980 batches: 0.0285
trigger times: 4
Loss after 308986080 batches: 0.0277
trigger times: 5
Loss after 309117180 batches: 0.0278
trigger times: 6
Loss after 309248280 batches: 0.0278
trigger times: 7
Loss after 309379380 batches: 0.0277
trigger times: 8
Loss after 309510480 batches: 0.0273
trigger times: 9
Loss after 309641580 batches: 0.0272
trigger times: 10
Loss after 309772680 batches: 0.0278
trigger times: 0
Loss after 309903780 batches: 0.0270
trigger times: 0
Loss after 310034880 batches: 0.0257
trigger times: 0
Loss after 310165980 batches: 0.0268
trigger times: 1
Loss after 310297080 batches: 0.0260
trigger times: 2
Loss after 310428180 batches: 0.0267
trigger times: 0
Loss after 310559280 batches: 0.0255
trigger times: 0
Loss after 310690380 batches: 0.0255
trigger times: 1
Loss after 310821480 batches: 0.0262
trigger times: 2
Loss after 310952580 batches: 0.0250
trigger times: 3
Loss after 311083680 batches: 0.0248
trigger times: 4
Loss after 311214780 batches: 0.0252
trigger times: 5
Loss after 311345880 batches: 0.0251
trigger times: 6
Loss after 311476980 batches: 0.0256
trigger times: 7
Loss after 311608080 batches: 0.0248
trigger times: 8
Loss after 311739180 batches: 0.0250
trigger times: 9
Loss after 311870280 batches: 0.0261
trigger times: 10
Loss after 312001380 batches: 0.0240
trigger times: 11
Loss after 312132480 batches: 0.0248
trigger times: 12
Loss after 312263580 batches: 0.0249
trigger times: 13
Loss after 312394680 batches: 0.0249
trigger times: 0
Loss after 312525780 batches: 0.0244
trigger times: 1
Loss after 312656880 batches: 0.0238
trigger times: 2
Loss after 312787980 batches: 0.0244
trigger times: 3
Loss after 312919080 batches: 0.0236
trigger times: 4
Loss after 313050180 batches: 0.0235
trigger times: 5
Loss after 313181280 batches: 0.0234
trigger times: 6
Loss after 313312380 batches: 0.0233
trigger times: 7
Loss after 313443480 batches: 0.0219
trigger times: 8
Loss after 313574580 batches: 0.0219
trigger times: 9
Loss after 313705680 batches: 0.0230
trigger times: 10
Loss after 313836780 batches: 0.0234
trigger times: 11
Loss after 313967880 batches: 0.0232
trigger times: 12
Loss after 314098980 batches: 0.0232
trigger times: 13
Loss after 314230080 batches: 0.0234
trigger times: 14
Loss after 314361180 batches: 0.0217
trigger times: 15
Loss after 314492280 batches: 0.0222
trigger times: 16
Loss after 314623380 batches: 0.0226
trigger times: 17
Loss after 314754480 batches: 0.0226
trigger times: 18
Loss after 314885580 batches: 0.0217
trigger times: 19
Loss after 315016680 batches: 0.0216
trigger times: 20
Early stopping!
Start to test process.
Loss after 315147780 batches: 0.0220
Time to train on one home:  562.4771957397461
trigger times: 0
Loss after 315278880 batches: 0.1099
trigger times: 0
Loss after 315409980 batches: 0.0336
trigger times: 0
Loss after 315541080 batches: 0.0242
trigger times: 1
Loss after 315672180 batches: 0.0208
trigger times: 2
Loss after 315803280 batches: 0.0191
trigger times: 3
Loss after 315934380 batches: 0.0173
trigger times: 4
Loss after 316065480 batches: 0.0164
trigger times: 5
Loss after 316196580 batches: 0.0158
trigger times: 6
Loss after 316327680 batches: 0.0153
trigger times: 7
Loss after 316458780 batches: 0.0145
trigger times: 8
Loss after 316589880 batches: 0.0143
trigger times: 9
Loss after 316720980 batches: 0.0135
trigger times: 10
Loss after 316852080 batches: 0.0134
trigger times: 11
Loss after 316983180 batches: 0.0132
trigger times: 12
Loss after 317114280 batches: 0.0131
trigger times: 13
Loss after 317245380 batches: 0.0126
trigger times: 14
Loss after 317376480 batches: 0.0123
trigger times: 15
Loss after 317507580 batches: 0.0123
trigger times: 16
Loss after 317638680 batches: 0.0119
trigger times: 17
Loss after 317769780 batches: 0.0118
trigger times: 18
Loss after 317900880 batches: 0.0114
trigger times: 19
Loss after 318031980 batches: 0.0113
trigger times: 20
Early stopping!
Start to test process.
Loss after 318163080 batches: 0.0114
Time to train on one home:  174.14919567108154
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459]]
Round_6_results:  [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459]
trigger times: 0
Loss after 318294180 batches: 0.1297
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2489 < 2490; dropping {'Training_Loss': 0.1297476017531359, 'Validation_Loss': 0.22662651538848877, 'Training_R2': 0.8692947459432812, 'Validation_R2': 0.7893553643995339, 'Training_F1': 0.8054846197846792, 'Validation_F1': 0.7350566530834866, 'Training_NEP': 0.38895162651693155, 'Validation_NEP': 0.5104672919419029, 'Training_NDE': 0.09812275424223399, 'Validation_NDE': 0.16774628384781431, 'Training_MAE': 12.881306495986578, 'Validation_MAE': 13.999130944111869, 'Training_MSE': 431.72476, 'Validation_MSE': 619.4824}.
trigger times: 0
Loss after 318425280 batches: 0.0354
trigger times: 0
Loss after 318556380 batches: 0.0263
trigger times: 1
Loss after 318687480 batches: 0.0223
trigger times: 2
Loss after 318818580 batches: 0.0205
trigger times: 3
Loss after 318949680 batches: 0.0188
trigger times: 0
Loss after 319080780 batches: 0.0180
trigger times: 1
Loss after 319211880 batches: 0.0170
trigger times: 2
Loss after 319342980 batches: 0.0160
trigger times: 0
Loss after 319474080 batches: 0.0154
trigger times: 1
Loss after 319605180 batches: 0.0151
trigger times: 2
Loss after 319736280 batches: 0.0150
trigger times: 3
Loss after 319867380 batches: 0.0143
trigger times: 4
Loss after 319998480 batches: 0.0140
trigger times: 5
Loss after 320129580 batches: 0.0139
trigger times: 6
Loss after 320260680 batches: 0.0137
trigger times: 7
Loss after 320391780 batches: 0.0131
trigger times: 8
Loss after 320522880 batches: 0.0130
trigger times: 9
Loss after 320653980 batches: 0.0129
trigger times: 10
Loss after 320785080 batches: 0.0125
trigger times: 11
Loss after 320916180 batches: 0.0127
trigger times: 12
Loss after 321047280 batches: 0.0124
trigger times: 13
Loss after 321178380 batches: 0.0124
trigger times: 14
Loss after 321309480 batches: 0.0120
trigger times: 15
Loss after 321440580 batches: 0.0118
trigger times: 16
Loss after 321571680 batches: 0.0118
trigger times: 17
Loss after 321702780 batches: 0.0115
trigger times: 18
Loss after 321833880 batches: 0.0118
trigger times: 19
Loss after 321964980 batches: 0.0113
trigger times: 20
Early stopping!
Start to test process.
Loss after 322096080 batches: 0.0116
Time to train on one home:  224.09034705162048
trigger times: 0
Loss after 322198680 batches: 0.3320
trigger times: 0
Loss after 322301280 batches: 0.1078
trigger times: 0
Loss after 322403880 batches: 0.0807
trigger times: 0
Loss after 322506480 batches: 0.0569
trigger times: 1
Loss after 322609080 batches: 0.0470
trigger times: 2
Loss after 322711680 batches: 0.0419
trigger times: 3
Loss after 322814280 batches: 0.0388
trigger times: 0
Loss after 322916880 batches: 0.0365
trigger times: 1
Loss after 323019480 batches: 0.0362
trigger times: 2
Loss after 323122080 batches: 0.0353
trigger times: 3
Loss after 323224680 batches: 0.0318
trigger times: 0
Loss after 323327280 batches: 0.0319
trigger times: 1
Loss after 323429880 batches: 0.0294
trigger times: 2
Loss after 323532480 batches: 0.0288
trigger times: 3
Loss after 323635080 batches: 0.0282
trigger times: 0
Loss after 323737680 batches: 0.0335
trigger times: 0
Loss after 323840280 batches: 0.0274
trigger times: 1
Loss after 323942880 batches: 0.0272
trigger times: 2
Loss after 324045480 batches: 0.0270
trigger times: 0
Loss after 324148080 batches: 0.0275
trigger times: 1
Loss after 324250680 batches: 0.0250
trigger times: 2
Loss after 324353280 batches: 0.0241
trigger times: 3
Loss after 324455880 batches: 0.0242
trigger times: 4
Loss after 324558480 batches: 0.0249
trigger times: 0
Loss after 324661080 batches: 0.0240
trigger times: 1
Loss after 324763680 batches: 0.0235
trigger times: 0
Loss after 324866280 batches: 0.0228
trigger times: 1
Loss after 324968880 batches: 0.0221
trigger times: 2
Loss after 325071480 batches: 0.0225
trigger times: 3
Loss after 325174080 batches: 0.0223
trigger times: 4
Loss after 325276680 batches: 0.0224
trigger times: 5
Loss after 325379280 batches: 0.0218
trigger times: 6
Loss after 325481880 batches: 0.0222
trigger times: 7
Loss after 325584480 batches: 0.0214
trigger times: 8
Loss after 325687080 batches: 0.0209
trigger times: 0
Loss after 325789680 batches: 0.0229
trigger times: 1
Loss after 325892280 batches: 0.0212
trigger times: 2
Loss after 325994880 batches: 0.0209
trigger times: 3
Loss after 326097480 batches: 0.0201
trigger times: 4
Loss after 326200080 batches: 0.0201
trigger times: 0
Loss after 326302680 batches: 0.0202
trigger times: 1
Loss after 326405280 batches: 0.0204
trigger times: 0
Loss after 326507880 batches: 0.0206
trigger times: 1
Loss after 326610480 batches: 0.0203
trigger times: 2
Loss after 326713080 batches: 0.0218
trigger times: 3
Loss after 326815680 batches: 0.0207
trigger times: 4
Loss after 326918280 batches: 0.0206
trigger times: 5
Loss after 327020880 batches: 0.0207
trigger times: 6
Loss after 327123480 batches: 0.0206
trigger times: 0
Loss after 327226080 batches: 0.0205
trigger times: 1
Loss after 327328680 batches: 0.0193
trigger times: 2
Loss after 327431280 batches: 0.0197
trigger times: 3
Loss after 327533880 batches: 0.0190
trigger times: 4
Loss after 327636480 batches: 0.0193
trigger times: 5
Loss after 327739080 batches: 0.0187
trigger times: 6
Loss after 327841680 batches: 0.0189
trigger times: 7
Loss after 327944280 batches: 0.0183
trigger times: 8
Loss after 328046880 batches: 0.0188
trigger times: 9
Loss after 328149480 batches: 0.0184
trigger times: 10
Loss after 328252080 batches: 0.0185
trigger times: 11
Loss after 328354680 batches: 0.0186
trigger times: 12
Loss after 328457280 batches: 0.0188
trigger times: 13
Loss after 328559880 batches: 0.0192
trigger times: 0
Loss after 328662480 batches: 0.0223
trigger times: 1
Loss after 328765080 batches: 0.0199
trigger times: 2
Loss after 328867680 batches: 0.0211
trigger times: 3
Loss after 328970280 batches: 0.0182
trigger times: 4
Loss after 329072880 batches: 0.0190
trigger times: 5
Loss after 329175480 batches: 0.0202
trigger times: 6
Loss after 329278080 batches: 0.0175
trigger times: 7
Loss after 329380680 batches: 0.0179
trigger times: 8
Loss after 329483280 batches: 0.0186
trigger times: 9
Loss after 329585880 batches: 0.0199
trigger times: 10
Loss after 329688480 batches: 0.0193
trigger times: 11
Loss after 329791080 batches: 0.0176
trigger times: 12
Loss after 329893680 batches: 0.0184
trigger times: 0
Loss after 329996280 batches: 0.0179
trigger times: 1
Loss after 330098880 batches: 0.0173
trigger times: 2
Loss after 330201480 batches: 0.0163
trigger times: 3
Loss after 330304080 batches: 0.0177
trigger times: 4
Loss after 330406680 batches: 0.0171
trigger times: 5
Loss after 330509280 batches: 0.0167
trigger times: 6
Loss after 330611880 batches: 0.0165
trigger times: 7
Loss after 330714480 batches: 0.0163
trigger times: 8
Loss after 330817080 batches: 0.0167
trigger times: 9
Loss after 330919680 batches: 0.0172
trigger times: 10
Loss after 331022280 batches: 0.0181
trigger times: 0
Loss after 331124880 batches: 0.0171
trigger times: 1
Loss after 331227480 batches: 0.0171
trigger times: 0
Loss after 331330080 batches: 0.0174
trigger times: 0
Loss after 331432680 batches: 0.0170
trigger times: 1
Loss after 331535280 batches: 0.0164
trigger times: 2
Loss after 331637880 batches: 0.0167
trigger times: 3
Loss after 331740480 batches: 0.0174
trigger times: 4
Loss after 331843080 batches: 0.0172
trigger times: 5
Loss after 331945680 batches: 0.0167
trigger times: 6
Loss after 332048280 batches: 0.0166
trigger times: 7
Loss after 332150880 batches: 0.0164
trigger times: 8
Loss after 332253480 batches: 0.0163
trigger times: 9
Loss after 332356080 batches: 0.0160
trigger times: 10
Loss after 332458680 batches: 0.0161
trigger times: 11
Loss after 332561280 batches: 0.0163
trigger times: 12
Loss after 332663880 batches: 0.0163
trigger times: 13
Loss after 332766480 batches: 0.0193
trigger times: 14
Loss after 332869080 batches: 0.0169
trigger times: 15
Loss after 332971680 batches: 0.0167
trigger times: 16
Loss after 333074280 batches: 0.0168
trigger times: 17
Loss after 333176880 batches: 0.0160
trigger times: 18
Loss after 333279480 batches: 0.0162
trigger times: 19
Loss after 333382080 batches: 0.0164
trigger times: 20
Early stopping!
Start to test process.
Loss after 333484680 batches: 0.0162
Time to train on one home:  641.2945775985718
trigger times: 0
Loss after 333615780 batches: 0.1628
trigger times: 1
Loss after 333746880 batches: 0.0523
trigger times: 0
Loss after 333877980 batches: 0.0377
trigger times: 0
Loss after 334009080 batches: 0.0323
trigger times: 1
Loss after 334140180 batches: 0.0295
trigger times: 2
Loss after 334271280 batches: 0.0274
trigger times: 3
Loss after 334402380 batches: 0.0261
trigger times: 4
Loss after 334533480 batches: 0.0245
trigger times: 5
Loss after 334664580 batches: 0.0238
trigger times: 6
Loss after 334795680 batches: 0.0231
trigger times: 7
Loss after 334926780 batches: 0.0222
trigger times: 8
Loss after 335057880 batches: 0.0217
trigger times: 9
Loss after 335188980 batches: 0.0209
trigger times: 10
Loss after 335320080 batches: 0.0208
trigger times: 11
Loss after 335451180 batches: 0.0204
trigger times: 12
Loss after 335582280 batches: 0.0197
trigger times: 13
Loss after 335713380 batches: 0.0194
trigger times: 14
Loss after 335844480 batches: 0.0191
trigger times: 15
Loss after 335975580 batches: 0.0194
trigger times: 16
Loss after 336106680 batches: 0.0190
trigger times: 17
Loss after 336237780 batches: 0.0184
trigger times: 18
Loss after 336368880 batches: 0.0179
trigger times: 19
Loss after 336499980 batches: 0.0178
trigger times: 20
Early stopping!
Start to test process.
Loss after 336631080 batches: 0.0179
Time to train on one home:  180.91044807434082
trigger times: 0
Loss after 336762180 batches: 0.2820
trigger times: 0
Loss after 336893280 batches: 0.0896
trigger times: 1
Loss after 337024380 batches: 0.0619
trigger times: 2
Loss after 337155480 batches: 0.0520
trigger times: 3
Loss after 337286580 batches: 0.0461
trigger times: 4
Loss after 337417680 batches: 0.0428
trigger times: 5
Loss after 337548780 batches: 0.0400
trigger times: 6
Loss after 337679880 batches: 0.0382
trigger times: 7
Loss after 337810980 batches: 0.0362
trigger times: 0
Loss after 337942080 batches: 0.0347
trigger times: 1
Loss after 338073180 batches: 0.0337
trigger times: 2
Loss after 338204280 batches: 0.0328
trigger times: 3
Loss after 338335380 batches: 0.0320
trigger times: 4
Loss after 338466480 batches: 0.0310
trigger times: 0
Loss after 338597580 batches: 0.0304
trigger times: 1
Loss after 338728680 batches: 0.0293
trigger times: 2
Loss after 338859780 batches: 0.0294
trigger times: 3
Loss after 338990880 batches: 0.0286
trigger times: 4
Loss after 339121980 batches: 0.0282
trigger times: 5
Loss after 339253080 batches: 0.0277
trigger times: 6
Loss after 339384180 batches: 0.0273
trigger times: 7
Loss after 339515280 batches: 0.0268
trigger times: 8
Loss after 339646380 batches: 0.0264
trigger times: 9
Loss after 339777480 batches: 0.0258
trigger times: 10
Loss after 339908580 batches: 0.0259
trigger times: 11
Loss after 340039680 batches: 0.0255
trigger times: 12
Loss after 340170780 batches: 0.0252
trigger times: 13
Loss after 340301880 batches: 0.0249
trigger times: 14
Loss after 340432980 batches: 0.0244
trigger times: 15
Loss after 340564080 batches: 0.0243
trigger times: 16
Loss after 340695180 batches: 0.0245
trigger times: 17
Loss after 340826280 batches: 0.0239
trigger times: 18
Loss after 340957380 batches: 0.0239
trigger times: 0
Loss after 341088480 batches: 0.0237
trigger times: 1
Loss after 341219580 batches: 0.0236
trigger times: 2
Loss after 341350680 batches: 0.0235
trigger times: 0
Loss after 341481780 batches: 0.0233
trigger times: 1
Loss after 341612880 batches: 0.0231
trigger times: 2
Loss after 341743980 batches: 0.0225
trigger times: 0
Loss after 341875080 batches: 0.0229
trigger times: 0
Loss after 342006180 batches: 0.0225
trigger times: 1
Loss after 342137280 batches: 0.0224
trigger times: 2
Loss after 342268380 batches: 0.0223
trigger times: 3
Loss after 342399480 batches: 0.0219
trigger times: 4
Loss after 342530580 batches: 0.0220
trigger times: 5
Loss after 342661680 batches: 0.0218
trigger times: 6
Loss after 342792780 batches: 0.0216
trigger times: 7
Loss after 342923880 batches: 0.0215
trigger times: 8
Loss after 343054980 batches: 0.0213
trigger times: 9
Loss after 343186080 batches: 0.0211
trigger times: 10
Loss after 343317180 batches: 0.0210
trigger times: 11
Loss after 343448280 batches: 0.0208
trigger times: 12
Loss after 343579380 batches: 0.0208
trigger times: 13
Loss after 343710480 batches: 0.0209
trigger times: 14
Loss after 343841580 batches: 0.0207
trigger times: 15
Loss after 343972680 batches: 0.0209
trigger times: 16
Loss after 344103780 batches: 0.0206
trigger times: 17
Loss after 344234880 batches: 0.0208
trigger times: 18
Loss after 344365980 batches: 0.0201
trigger times: 19
Loss after 344497080 batches: 0.0199
trigger times: 0
Loss after 344628180 batches: 0.0198
trigger times: 1
Loss after 344759280 batches: 0.0202
trigger times: 2
Loss after 344890380 batches: 0.0201
trigger times: 3
Loss after 345021480 batches: 0.0197
trigger times: 4
Loss after 345152580 batches: 0.0195
trigger times: 0
Loss after 345283680 batches: 0.0199
trigger times: 1
Loss after 345414780 batches: 0.0198
trigger times: 2
Loss after 345545880 batches: 0.0194
trigger times: 3
Loss after 345676980 batches: 0.0196
trigger times: 4
Loss after 345808080 batches: 0.0191
trigger times: 5
Loss after 345939180 batches: 0.0189
trigger times: 6
Loss after 346070280 batches: 0.0189
trigger times: 7
Loss after 346201380 batches: 0.0191
trigger times: 8
Loss after 346332480 batches: 0.0195
trigger times: 9
Loss after 346463580 batches: 0.0191
trigger times: 10
Loss after 346594680 batches: 0.0192
trigger times: 11
Loss after 346725780 batches: 0.0189
trigger times: 12
Loss after 346856880 batches: 0.0190
trigger times: 13
Loss after 346987980 batches: 0.0185
trigger times: 14
Loss after 347119080 batches: 0.0188
trigger times: 15
Loss after 347250180 batches: 0.0188
trigger times: 16
Loss after 347381280 batches: 0.0188
trigger times: 17
Loss after 347512380 batches: 0.0184
trigger times: 18
Loss after 347643480 batches: 0.0186
trigger times: 19
Loss after 347774580 batches: 0.0182
trigger times: 20
Early stopping!
Start to test process.
Loss after 347905680 batches: 0.0186
Time to train on one home:  619.7776737213135
trigger times: 0
Loss after 348034320 batches: 0.1516
trigger times: 0
Loss after 348162960 batches: 0.0470
trigger times: 0
Loss after 348291600 batches: 0.0333
trigger times: 0
Loss after 348420240 batches: 0.0280
trigger times: 1
Loss after 348548880 batches: 0.0249
trigger times: 2
Loss after 348677520 batches: 0.0236
trigger times: 0
Loss after 348806160 batches: 0.0224
trigger times: 0
Loss after 348934800 batches: 0.0212
trigger times: 1
Loss after 349063440 batches: 0.0199
trigger times: 2
Loss after 349192080 batches: 0.0195
trigger times: 3
Loss after 349320720 batches: 0.0186
trigger times: 4
Loss after 349449360 batches: 0.0183
trigger times: 5
Loss after 349578000 batches: 0.0177
trigger times: 6
Loss after 349706640 batches: 0.0173
trigger times: 7
Loss after 349835280 batches: 0.0169
trigger times: 8
Loss after 349963920 batches: 0.0170
trigger times: 9
Loss after 350092560 batches: 0.0164
trigger times: 10
Loss after 350221200 batches: 0.0162
trigger times: 11
Loss after 350349840 batches: 0.0157
trigger times: 12
Loss after 350478480 batches: 0.0155
trigger times: 13
Loss after 350607120 batches: 0.0152
trigger times: 14
Loss after 350735760 batches: 0.0150
trigger times: 15
Loss after 350864400 batches: 0.0152
trigger times: 16
Loss after 350993040 batches: 0.0149
trigger times: 17
Loss after 351121680 batches: 0.0144
trigger times: 18
Loss after 351250320 batches: 0.0143
trigger times: 19
Loss after 351378960 batches: 0.0139
trigger times: 20
Early stopping!
Start to test process.
Loss after 351507600 batches: 0.0142
Time to train on one home:  206.1322729587555
trigger times: 0
Loss after 351638700 batches: 0.2499
trigger times: 0
Loss after 351769800 batches: 0.0672
trigger times: 1
Loss after 351900900 batches: 0.0483
trigger times: 0
Loss after 352032000 batches: 0.0409
trigger times: 0
Loss after 352163100 batches: 0.0374
trigger times: 1
Loss after 352294200 batches: 0.0350
trigger times: 2
Loss after 352425300 batches: 0.0332
trigger times: 0
Loss after 352556400 batches: 0.0315
trigger times: 0
Loss after 352687500 batches: 0.0304
trigger times: 1
Loss after 352818600 batches: 0.0295
trigger times: 0
Loss after 352949700 batches: 0.0287
trigger times: 1
Loss after 353080800 batches: 0.0280
trigger times: 2
Loss after 353211900 batches: 0.0272
trigger times: 3
Loss after 353343000 batches: 0.0266
trigger times: 4
Loss after 353474100 batches: 0.0263
trigger times: 5
Loss after 353605200 batches: 0.0256
trigger times: 6
Loss after 353736300 batches: 0.0249
trigger times: 7
Loss after 353867400 batches: 0.0249
trigger times: 8
Loss after 353998500 batches: 0.0245
trigger times: 9
Loss after 354129600 batches: 0.0241
trigger times: 10
Loss after 354260700 batches: 0.0237
trigger times: 11
Loss after 354391800 batches: 0.0235
trigger times: 0
Loss after 354522900 batches: 0.0234
trigger times: 1
Loss after 354654000 batches: 0.0233
trigger times: 2
Loss after 354785100 batches: 0.0227
trigger times: 3
Loss after 354916200 batches: 0.0225
trigger times: 4
Loss after 355047300 batches: 0.0224
trigger times: 5
Loss after 355178400 batches: 0.0223
trigger times: 0
Loss after 355309500 batches: 0.0221
trigger times: 1
Loss after 355440600 batches: 0.0218
trigger times: 2
Loss after 355571700 batches: 0.0218
trigger times: 3
Loss after 355702800 batches: 0.0215
trigger times: 4
Loss after 355833900 batches: 0.0215
trigger times: 5
Loss after 355965000 batches: 0.0210
trigger times: 6
Loss after 356096100 batches: 0.0210
trigger times: 7
Loss after 356227200 batches: 0.0209
trigger times: 8
Loss after 356358300 batches: 0.0207
trigger times: 9
Loss after 356489400 batches: 0.0207
trigger times: 10
Loss after 356620500 batches: 0.0207
trigger times: 11
Loss after 356751600 batches: 0.0206
trigger times: 12
Loss after 356882700 batches: 0.0205
trigger times: 13
Loss after 357013800 batches: 0.0204
trigger times: 14
Loss after 357144900 batches: 0.0202
trigger times: 15
Loss after 357276000 batches: 0.0199
trigger times: 16
Loss after 357407100 batches: 0.0198
trigger times: 17
Loss after 357538200 batches: 0.0195
trigger times: 18
Loss after 357669300 batches: 0.0197
trigger times: 19
Loss after 357800400 batches: 0.0196
trigger times: 20
Early stopping!
Start to test process.
Loss after 357931500 batches: 0.0196
Time to train on one home:  358.80722188949585
trigger times: 0
Loss after 358062600 batches: 0.4290
trigger times: 0
Loss after 358193700 batches: 0.1510
trigger times: 0
Loss after 358324800 batches: 0.0837
trigger times: 1
Loss after 358455900 batches: 0.0657
trigger times: 0
Loss after 358587000 batches: 0.0530
trigger times: 0
Loss after 358718100 batches: 0.0489
trigger times: 0
Loss after 358849200 batches: 0.0453
trigger times: 1
Loss after 358980300 batches: 0.0419
trigger times: 0
Loss after 359111400 batches: 0.0414
trigger times: 1
Loss after 359242500 batches: 0.0372
trigger times: 2
Loss after 359373600 batches: 0.0370
trigger times: 0
Loss after 359504700 batches: 0.0368
trigger times: 1
Loss after 359635800 batches: 0.0345
trigger times: 2
Loss after 359766900 batches: 0.0339
trigger times: 3
Loss after 359898000 batches: 0.0326
trigger times: 4
Loss after 360029100 batches: 0.0340
trigger times: 5
Loss after 360160200 batches: 0.0324
trigger times: 6
Loss after 360291300 batches: 0.0308
trigger times: 7
Loss after 360422400 batches: 0.0301
trigger times: 8
Loss after 360553500 batches: 0.0299
trigger times: 0
Loss after 360684600 batches: 0.0297
trigger times: 1
Loss after 360815700 batches: 0.0291
trigger times: 2
Loss after 360946800 batches: 0.0299
trigger times: 0
Loss after 361077900 batches: 0.0287
trigger times: 1
Loss after 361209000 batches: 0.0288
trigger times: 2
Loss after 361340100 batches: 0.0288
trigger times: 3
Loss after 361471200 batches: 0.0273
trigger times: 4
Loss after 361602300 batches: 0.0270
trigger times: 5
Loss after 361733400 batches: 0.0287
trigger times: 6
Loss after 361864500 batches: 0.0276
trigger times: 0
Loss after 361995600 batches: 0.0281
trigger times: 1
Loss after 362126700 batches: 0.0271
trigger times: 2
Loss after 362257800 batches: 0.0269
trigger times: 3
Loss after 362388900 batches: 0.0272
trigger times: 4
Loss after 362520000 batches: 0.0268
trigger times: 5
Loss after 362651100 batches: 0.0263
trigger times: 6
Loss after 362782200 batches: 0.0262
trigger times: 7
Loss after 362913300 batches: 0.0257
trigger times: 8
Loss after 363044400 batches: 0.0243
trigger times: 9
Loss after 363175500 batches: 0.0248
trigger times: 10
Loss after 363306600 batches: 0.0249
trigger times: 11
Loss after 363437700 batches: 0.0259
trigger times: 12
Loss after 363568800 batches: 0.0253
trigger times: 13
Loss after 363699900 batches: 0.0246
trigger times: 14
Loss after 363831000 batches: 0.0246
trigger times: 15
Loss after 363962100 batches: 0.0242
trigger times: 16
Loss after 364093200 batches: 0.0242
trigger times: 0
Loss after 364224300 batches: 0.0227
trigger times: 1
Loss after 364355400 batches: 0.0235
trigger times: 2
Loss after 364486500 batches: 0.0231
trigger times: 3
Loss after 364617600 batches: 0.0234
trigger times: 4
Loss after 364748700 batches: 0.0235
trigger times: 5
Loss after 364879800 batches: 0.0232
trigger times: 6
Loss after 365010900 batches: 0.0233
trigger times: 7
Loss after 365142000 batches: 0.0228
trigger times: 8
Loss after 365273100 batches: 0.0230
trigger times: 9
Loss after 365404200 batches: 0.0227
trigger times: 10
Loss after 365535300 batches: 0.0228
trigger times: 11
Loss after 365666400 batches: 0.0229
trigger times: 12
Loss after 365797500 batches: 0.0222
trigger times: 0
Loss after 365928600 batches: 0.0231
trigger times: 1
Loss after 366059700 batches: 0.0229
trigger times: 2
Loss after 366190800 batches: 0.0222
trigger times: 3
Loss after 366321900 batches: 0.0221
trigger times: 0
Loss after 366453000 batches: 0.0231
trigger times: 1
Loss after 366584100 batches: 0.0225
trigger times: 2
Loss after 366715200 batches: 0.0214
trigger times: 3
Loss after 366846300 batches: 0.0217
trigger times: 4
Loss after 366977400 batches: 0.0223
trigger times: 5
Loss after 367108500 batches: 0.0220
trigger times: 0
Loss after 367239600 batches: 0.0221
trigger times: 1
Loss after 367370700 batches: 0.0219
trigger times: 2
Loss after 367501800 batches: 0.0219
trigger times: 3
Loss after 367632900 batches: 0.0221
trigger times: 4
Loss after 367764000 batches: 0.0217
trigger times: 5
Loss after 367895100 batches: 0.0208
trigger times: 6
Loss after 368026200 batches: 0.0218
trigger times: 7
Loss after 368157300 batches: 0.0217
trigger times: 8
Loss after 368288400 batches: 0.0206
trigger times: 9
Loss after 368419500 batches: 0.0210
trigger times: 10
Loss after 368550600 batches: 0.0208
trigger times: 11
Loss after 368681700 batches: 0.0212
trigger times: 12
Loss after 368812800 batches: 0.0217
trigger times: 13
Loss after 368943900 batches: 0.0206
trigger times: 14
Loss after 369075000 batches: 0.0211
trigger times: 15
Loss after 369206100 batches: 0.0220
trigger times: 16
Loss after 369337200 batches: 0.0203
trigger times: 17
Loss after 369468300 batches: 0.0215
trigger times: 18
Loss after 369599400 batches: 0.0198
trigger times: 19
Loss after 369730500 batches: 0.0210
trigger times: 20
Early stopping!
Start to test process.
Loss after 369861600 batches: 0.0206
Time to train on one home:  656.4682457447052
trigger times: 0
Loss after 369992700 batches: 0.1035
trigger times: 1
Loss after 370123800 batches: 0.0302
trigger times: 2
Loss after 370254900 batches: 0.0226
trigger times: 3
Loss after 370386000 batches: 0.0192
trigger times: 4
Loss after 370517100 batches: 0.0176
trigger times: 5
Loss after 370648200 batches: 0.0168
trigger times: 6
Loss after 370779300 batches: 0.0155
trigger times: 7
Loss after 370910400 batches: 0.0148
trigger times: 8
Loss after 371041500 batches: 0.0140
trigger times: 9
Loss after 371172600 batches: 0.0138
trigger times: 10
Loss after 371303700 batches: 0.0134
trigger times: 11
Loss after 371434800 batches: 0.0129
trigger times: 12
Loss after 371565900 batches: 0.0126
trigger times: 13
Loss after 371697000 batches: 0.0121
trigger times: 14
Loss after 371828100 batches: 0.0121
trigger times: 15
Loss after 371959200 batches: 0.0118
trigger times: 16
Loss after 372090300 batches: 0.0115
trigger times: 17
Loss after 372221400 batches: 0.0114
trigger times: 18
Loss after 372352500 batches: 0.0114
trigger times: 19
Loss after 372483600 batches: 0.0112
trigger times: 20
Early stopping!
Start to test process.
Loss after 372614700 batches: 0.0109
Time to train on one home:  160.02749037742615
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61]]
Round_7_results:  [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2929 < 2930; dropping {'Training_Loss': 0.13077042413488874, 'Validation_Loss': 0.21208394981092876, 'Training_R2': 0.868299740759327, 'Validation_R2': 0.8028155026142538, 'Training_F1': 0.8034301344537628, 'Validation_F1': 0.7522227910193611, 'Training_NEP': 0.39251305413695825, 'Validation_NEP': 0.47551247140460207, 'Training_NDE': 0.09886972229519769, 'Validation_NDE': 0.15702733931281188, 'Training_MAE': 12.999253915689266, 'Validation_MAE': 13.040524746312036, 'Training_MSE': 435.01126, 'Validation_MSE': 579.89764}.
trigger times: 0
Loss after 372745800 batches: 0.1308
trigger times: 1
Loss after 372876900 batches: 0.0344
trigger times: 0
Loss after 373008000 batches: 0.0245
trigger times: 0
Loss after 373139100 batches: 0.0215
trigger times: 0
Loss after 373270200 batches: 0.0194
trigger times: 1
Loss after 373401300 batches: 0.0180
trigger times: 2
Loss after 373532400 batches: 0.0172
trigger times: 3
Loss after 373663500 batches: 0.0161
trigger times: 4
Loss after 373794600 batches: 0.0155
trigger times: 5
Loss after 373925700 batches: 0.0150
trigger times: 0
Loss after 374056800 batches: 0.0144
trigger times: 1
Loss after 374187900 batches: 0.0143
trigger times: 2
Loss after 374319000 batches: 0.0136
trigger times: 3
Loss after 374450100 batches: 0.0135
trigger times: 4
Loss after 374581200 batches: 0.0132
trigger times: 5
Loss after 374712300 batches: 0.0129
trigger times: 6
Loss after 374843400 batches: 0.0130
trigger times: 7
Loss after 374974500 batches: 0.0129
trigger times: 8
Loss after 375105600 batches: 0.0128
trigger times: 9
Loss after 375236700 batches: 0.0123
trigger times: 10
Loss after 375367800 batches: 0.0121
trigger times: 11
Loss after 375498900 batches: 0.0125
trigger times: 12
Loss after 375630000 batches: 0.0116
trigger times: 13
Loss after 375761100 batches: 0.0116
trigger times: 14
Loss after 375892200 batches: 0.0115
trigger times: 15
Loss after 376023300 batches: 0.0116
trigger times: 16
Loss after 376154400 batches: 0.0112
trigger times: 17
Loss after 376285500 batches: 0.0113
trigger times: 18
Loss after 376416600 batches: 0.0110
trigger times: 19
Loss after 376547700 batches: 0.0110
trigger times: 20
Early stopping!
Start to test process.
Loss after 376678800 batches: 0.0110
Time to train on one home:  230.52885365486145
trigger times: 0
Loss after 376781400 batches: 0.2504
trigger times: 0
Loss after 376884000 batches: 0.0950
trigger times: 0
Loss after 376986600 batches: 0.0556
trigger times: 0
Loss after 377089200 batches: 0.0434
trigger times: 0
Loss after 377191800 batches: 0.0383
trigger times: 1
Loss after 377294400 batches: 0.0334
trigger times: 2
Loss after 377397000 batches: 0.0331
trigger times: 0
Loss after 377499600 batches: 0.0309
trigger times: 1
Loss after 377602200 batches: 0.0285
trigger times: 2
Loss after 377704800 batches: 0.0295
trigger times: 3
Loss after 377807400 batches: 0.0297
trigger times: 4
Loss after 377910000 batches: 0.0260
trigger times: 5
Loss after 378012600 batches: 0.0257
trigger times: 6
Loss after 378115200 batches: 0.0253
trigger times: 7
Loss after 378217800 batches: 0.0247
trigger times: 8
Loss after 378320400 batches: 0.0254
trigger times: 0
Loss after 378423000 batches: 0.0240
trigger times: 1
Loss after 378525600 batches: 0.0233
trigger times: 2
Loss after 378628200 batches: 0.0228
trigger times: 3
Loss after 378730800 batches: 0.0219
trigger times: 4
Loss after 378833400 batches: 0.0231
trigger times: 5
Loss after 378936000 batches: 0.0211
trigger times: 6
Loss after 379038600 batches: 0.0218
trigger times: 7
Loss after 379141200 batches: 0.0230
trigger times: 8
Loss after 379243800 batches: 0.0210
trigger times: 9
Loss after 379346400 batches: 0.0201
trigger times: 10
Loss after 379449000 batches: 0.0200
trigger times: 0
Loss after 379551600 batches: 0.0195
trigger times: 1
Loss after 379654200 batches: 0.0199
trigger times: 2
Loss after 379756800 batches: 0.0209
trigger times: 3
Loss after 379859400 batches: 0.0207
trigger times: 4
Loss after 379962000 batches: 0.0196
trigger times: 5
Loss after 380064600 batches: 0.0198
trigger times: 0
Loss after 380167200 batches: 0.0197
trigger times: 1
Loss after 380269800 batches: 0.0194
trigger times: 2
Loss after 380372400 batches: 0.0191
trigger times: 3
Loss after 380475000 batches: 0.0198
trigger times: 4
Loss after 380577600 batches: 0.0188
trigger times: 5
Loss after 380680200 batches: 0.0188
trigger times: 6
Loss after 380782800 batches: 0.0201
trigger times: 7
Loss after 380885400 batches: 0.0187
trigger times: 8
Loss after 380988000 batches: 0.0184
trigger times: 0
Loss after 381090600 batches: 0.0187
trigger times: 1
Loss after 381193200 batches: 0.0196
trigger times: 2
Loss after 381295800 batches: 0.0187
trigger times: 3
Loss after 381398400 batches: 0.0172
trigger times: 4
Loss after 381501000 batches: 0.0172
trigger times: 5
Loss after 381603600 batches: 0.0179
trigger times: 6
Loss after 381706200 batches: 0.0179
trigger times: 7
Loss after 381808800 batches: 0.0180
trigger times: 0
Loss after 381911400 batches: 0.0191
trigger times: 1
Loss after 382014000 batches: 0.0182
trigger times: 2
Loss after 382116600 batches: 0.0169
trigger times: 3
Loss after 382219200 batches: 0.0174
trigger times: 4
Loss after 382321800 batches: 0.0189
trigger times: 5
Loss after 382424400 batches: 0.0176
trigger times: 6
Loss after 382527000 batches: 0.0171
trigger times: 7
Loss after 382629600 batches: 0.0171
trigger times: 8
Loss after 382732200 batches: 0.0171
trigger times: 9
Loss after 382834800 batches: 0.0164
trigger times: 10
Loss after 382937400 batches: 0.0176
trigger times: 11
Loss after 383040000 batches: 0.0191
trigger times: 12
Loss after 383142600 batches: 0.0176
trigger times: 13
Loss after 383245200 batches: 0.0173
trigger times: 14
Loss after 383347800 batches: 0.0164
trigger times: 15
Loss after 383450400 batches: 0.0164
trigger times: 16
Loss after 383553000 batches: 0.0169
trigger times: 17
Loss after 383655600 batches: 0.0169
trigger times: 18
Loss after 383758200 batches: 0.0167
trigger times: 19
Loss after 383860800 batches: 0.0167
trigger times: 20
Early stopping!
Start to test process.
Loss after 383963400 batches: 0.0157
Time to train on one home:  413.85721254348755
trigger times: 0
Loss after 384094500 batches: 0.1909
trigger times: 0
Loss after 384225600 batches: 0.0559
trigger times: 0
Loss after 384356700 batches: 0.0389
trigger times: 1
Loss after 384487800 batches: 0.0332
trigger times: 2
Loss after 384618900 batches: 0.0301
trigger times: 3
Loss after 384750000 batches: 0.0281
trigger times: 4
Loss after 384881100 batches: 0.0261
trigger times: 5
Loss after 385012200 batches: 0.0252
trigger times: 6
Loss after 385143300 batches: 0.0242
trigger times: 0
Loss after 385274400 batches: 0.0230
trigger times: 1
Loss after 385405500 batches: 0.0228
trigger times: 2
Loss after 385536600 batches: 0.0218
trigger times: 3
Loss after 385667700 batches: 0.0211
trigger times: 4
Loss after 385798800 batches: 0.0208
trigger times: 5
Loss after 385929900 batches: 0.0205
trigger times: 0
Loss after 386061000 batches: 0.0200
trigger times: 1
Loss after 386192100 batches: 0.0195
trigger times: 2
Loss after 386323200 batches: 0.0194
trigger times: 3
Loss after 386454300 batches: 0.0186
trigger times: 4
Loss after 386585400 batches: 0.0185
trigger times: 5
Loss after 386716500 batches: 0.0184
trigger times: 6
Loss after 386847600 batches: 0.0182
trigger times: 7
Loss after 386978700 batches: 0.0180
trigger times: 8
Loss after 387109800 batches: 0.0177
trigger times: 9
Loss after 387240900 batches: 0.0176
trigger times: 10
Loss after 387372000 batches: 0.0175
trigger times: 0
Loss after 387503100 batches: 0.0171
trigger times: 1
Loss after 387634200 batches: 0.0170
trigger times: 2
Loss after 387765300 batches: 0.0169
trigger times: 3
Loss after 387896400 batches: 0.0168
trigger times: 4
Loss after 388027500 batches: 0.0170
trigger times: 5
Loss after 388158600 batches: 0.0165
trigger times: 6
Loss after 388289700 batches: 0.0167
trigger times: 7
Loss after 388420800 batches: 0.0164
trigger times: 8
Loss after 388551900 batches: 0.0158
trigger times: 9
Loss after 388683000 batches: 0.0159
trigger times: 10
Loss after 388814100 batches: 0.0157
trigger times: 11
Loss after 388945200 batches: 0.0159
trigger times: 12
Loss after 389076300 batches: 0.0158
trigger times: 13
Loss after 389207400 batches: 0.0156
trigger times: 14
Loss after 389338500 batches: 0.0156
trigger times: 0
Loss after 389469600 batches: 0.0154
trigger times: 1
Loss after 389600700 batches: 0.0152
trigger times: 2
Loss after 389731800 batches: 0.0153
trigger times: 3
Loss after 389862900 batches: 0.0152
trigger times: 4
Loss after 389994000 batches: 0.0149
trigger times: 5
Loss after 390125100 batches: 0.0149
trigger times: 6
Loss after 390256200 batches: 0.0150
trigger times: 7
Loss after 390387300 batches: 0.0146
trigger times: 8
Loss after 390518400 batches: 0.0146
trigger times: 9
Loss after 390649500 batches: 0.0147
trigger times: 10
Loss after 390780600 batches: 0.0146
trigger times: 11
Loss after 390911700 batches: 0.0143
trigger times: 12
Loss after 391042800 batches: 0.0145
trigger times: 13
Loss after 391173900 batches: 0.0142
trigger times: 14
Loss after 391305000 batches: 0.0140
trigger times: 15
Loss after 391436100 batches: 0.0140
trigger times: 16
Loss after 391567200 batches: 0.0141
trigger times: 17
Loss after 391698300 batches: 0.0138
trigger times: 0
Loss after 391829400 batches: 0.0139
trigger times: 1
Loss after 391960500 batches: 0.0139
trigger times: 2
Loss after 392091600 batches: 0.0136
trigger times: 3
Loss after 392222700 batches: 0.0138
trigger times: 4
Loss after 392353800 batches: 0.0138
trigger times: 5
Loss after 392484900 batches: 0.0136
trigger times: 6
Loss after 392616000 batches: 0.0138
trigger times: 7
Loss after 392747100 batches: 0.0134
trigger times: 8
Loss after 392878200 batches: 0.0134
trigger times: 9
Loss after 393009300 batches: 0.0133
trigger times: 10
Loss after 393140400 batches: 0.0133
trigger times: 11
Loss after 393271500 batches: 0.0129
trigger times: 12
Loss after 393402600 batches: 0.0133
trigger times: 13
Loss after 393533700 batches: 0.0133
trigger times: 14
Loss after 393664800 batches: 0.0131
trigger times: 15
Loss after 393795900 batches: 0.0131
trigger times: 16
Loss after 393927000 batches: 0.0129
trigger times: 17
Loss after 394058100 batches: 0.0128
trigger times: 18
Loss after 394189200 batches: 0.0129
trigger times: 19
Loss after 394320300 batches: 0.0130
trigger times: 20
Early stopping!
Start to test process.
Loss after 394451400 batches: 0.0129
Time to train on one home:  577.5957319736481
trigger times: 0
Loss after 394582500 batches: 0.2801
trigger times: 0
Loss after 394713600 batches: 0.0791
trigger times: 0
Loss after 394844700 batches: 0.0553
trigger times: 0
Loss after 394975800 batches: 0.0471
trigger times: 1
Loss after 395106900 batches: 0.0419
trigger times: 2
Loss after 395238000 batches: 0.0387
trigger times: 3
Loss after 395369100 batches: 0.0364
trigger times: 4
Loss after 395500200 batches: 0.0345
trigger times: 5
Loss after 395631300 batches: 0.0326
trigger times: 0
Loss after 395762400 batches: 0.0318
trigger times: 0
Loss after 395893500 batches: 0.0306
trigger times: 1
Loss after 396024600 batches: 0.0299
trigger times: 2
Loss after 396155700 batches: 0.0291
trigger times: 3
Loss after 396286800 batches: 0.0285
trigger times: 4
Loss after 396417900 batches: 0.0281
trigger times: 5
Loss after 396549000 batches: 0.0274
trigger times: 6
Loss after 396680100 batches: 0.0268
trigger times: 7
Loss after 396811200 batches: 0.0267
trigger times: 8
Loss after 396942300 batches: 0.0261
trigger times: 0
Loss after 397073400 batches: 0.0257
trigger times: 1
Loss after 397204500 batches: 0.0253
trigger times: 2
Loss after 397335600 batches: 0.0249
trigger times: 0
Loss after 397466700 batches: 0.0245
trigger times: 0
Loss after 397597800 batches: 0.0245
trigger times: 1
Loss after 397728900 batches: 0.0242
trigger times: 2
Loss after 397860000 batches: 0.0239
trigger times: 3
Loss after 397991100 batches: 0.0236
trigger times: 0
Loss after 398122200 batches: 0.0232
trigger times: 1
Loss after 398253300 batches: 0.0230
trigger times: 2
Loss after 398384400 batches: 0.0232
trigger times: 3
Loss after 398515500 batches: 0.0227
trigger times: 4
Loss after 398646600 batches: 0.0227
trigger times: 5
Loss after 398777700 batches: 0.0225
trigger times: 6
Loss after 398908800 batches: 0.0220
trigger times: 7
Loss after 399039900 batches: 0.0225
trigger times: 8
Loss after 399171000 batches: 0.0221
trigger times: 9
Loss after 399302100 batches: 0.0218
trigger times: 10
Loss after 399433200 batches: 0.0219
trigger times: 11
Loss after 399564300 batches: 0.0218
trigger times: 12
Loss after 399695400 batches: 0.0216
trigger times: 13
Loss after 399826500 batches: 0.0212
trigger times: 14
Loss after 399957600 batches: 0.0211
trigger times: 15
Loss after 400088700 batches: 0.0210
trigger times: 16
Loss after 400219800 batches: 0.0208
trigger times: 17
Loss after 400350900 batches: 0.0206
trigger times: 18
Loss after 400482000 batches: 0.0206
trigger times: 19
Loss after 400613100 batches: 0.0203
trigger times: 20
Early stopping!
Start to test process.
Loss after 400744200 batches: 0.0205
Time to train on one home:  351.46990990638733
trigger times: 0
Loss after 400872840 batches: 0.1371
trigger times: 0
Loss after 401001480 batches: 0.0383
trigger times: 0
Loss after 401130120 batches: 0.0282
trigger times: 1
Loss after 401258760 batches: 0.0247
trigger times: 2
Loss after 401387400 batches: 0.0227
trigger times: 3
Loss after 401516040 batches: 0.0215
trigger times: 4
Loss after 401644680 batches: 0.0200
trigger times: 0
Loss after 401773320 batches: 0.0192
trigger times: 1
Loss after 401901960 batches: 0.0182
trigger times: 0
Loss after 402030600 batches: 0.0179
trigger times: 0
Loss after 402159240 batches: 0.0172
trigger times: 0
Loss after 402287880 batches: 0.0171
trigger times: 0
Loss after 402416520 batches: 0.0166
trigger times: 1
Loss after 402545160 batches: 0.0161
trigger times: 2
Loss after 402673800 batches: 0.0156
trigger times: 3
Loss after 402802440 batches: 0.0155
trigger times: 4
Loss after 402931080 batches: 0.0152
trigger times: 5
Loss after 403059720 batches: 0.0152
trigger times: 6
Loss after 403188360 batches: 0.0149
trigger times: 7
Loss after 403317000 batches: 0.0145
trigger times: 8
Loss after 403445640 batches: 0.0145
trigger times: 9
Loss after 403574280 batches: 0.0142
trigger times: 10
Loss after 403702920 batches: 0.0142
trigger times: 11
Loss after 403831560 batches: 0.0137
trigger times: 12
Loss after 403960200 batches: 0.0140
trigger times: 13
Loss after 404088840 batches: 0.0137
trigger times: 14
Loss after 404217480 batches: 0.0133
trigger times: 15
Loss after 404346120 batches: 0.0132
trigger times: 16
Loss after 404474760 batches: 0.0132
trigger times: 17
Loss after 404603400 batches: 0.0130
trigger times: 18
Loss after 404732040 batches: 0.0130
trigger times: 19
Loss after 404860680 batches: 0.0131
trigger times: 20
Early stopping!
Start to test process.
Loss after 404989320 batches: 0.0131
Time to train on one home:  241.32252287864685
trigger times: 0
Loss after 405120420 batches: 0.2677
trigger times: 0
Loss after 405251520 batches: 0.0659
trigger times: 0
Loss after 405382620 batches: 0.0467
trigger times: 0
Loss after 405513720 batches: 0.0401
trigger times: 0
Loss after 405644820 batches: 0.0372
trigger times: 1
Loss after 405775920 batches: 0.0342
trigger times: 0
Loss after 405907020 batches: 0.0321
trigger times: 1
Loss after 406038120 batches: 0.0308
trigger times: 2
Loss after 406169220 batches: 0.0302
trigger times: 3
Loss after 406300320 batches: 0.0293
trigger times: 4
Loss after 406431420 batches: 0.0281
trigger times: 5
Loss after 406562520 batches: 0.0275
trigger times: 0
Loss after 406693620 batches: 0.0266
trigger times: 0
Loss after 406824720 batches: 0.0264
trigger times: 1
Loss after 406955820 batches: 0.0256
trigger times: 2
Loss after 407086920 batches: 0.0255
trigger times: 0
Loss after 407218020 batches: 0.0249
trigger times: 1
Loss after 407349120 batches: 0.0242
trigger times: 2
Loss after 407480220 batches: 0.0239
trigger times: 0
Loss after 407611320 batches: 0.0238
trigger times: 1
Loss after 407742420 batches: 0.0236
trigger times: 2
Loss after 407873520 batches: 0.0232
trigger times: 3
Loss after 408004620 batches: 0.0231
trigger times: 4
Loss after 408135720 batches: 0.0225
trigger times: 5
Loss after 408266820 batches: 0.0223
trigger times: 6
Loss after 408397920 batches: 0.0223
trigger times: 7
Loss after 408529020 batches: 0.0221
trigger times: 0
Loss after 408660120 batches: 0.0217
trigger times: 1
Loss after 408791220 batches: 0.0216
trigger times: 2
Loss after 408922320 batches: 0.0215
trigger times: 3
Loss after 409053420 batches: 0.0213
trigger times: 4
Loss after 409184520 batches: 0.0211
trigger times: 5
Loss after 409315620 batches: 0.0210
trigger times: 6
Loss after 409446720 batches: 0.0208
trigger times: 7
Loss after 409577820 batches: 0.0207
trigger times: 8
Loss after 409708920 batches: 0.0204
trigger times: 9
Loss after 409840020 batches: 0.0205
trigger times: 10
Loss after 409971120 batches: 0.0207
trigger times: 11
Loss after 410102220 batches: 0.0204
trigger times: 12
Loss after 410233320 batches: 0.0203
trigger times: 13
Loss after 410364420 batches: 0.0199
trigger times: 14
Loss after 410495520 batches: 0.0200
trigger times: 15
Loss after 410626620 batches: 0.0198
trigger times: 16
Loss after 410757720 batches: 0.0198
trigger times: 17
Loss after 410888820 batches: 0.0197
trigger times: 18
Loss after 411019920 batches: 0.0193
trigger times: 19
Loss after 411151020 batches: 0.0194
trigger times: 20
Early stopping!
Start to test process.
Loss after 411282120 batches: 0.0193
Time to train on one home:  351.3401210308075
trigger times: 0
Loss after 411413220 batches: 0.5217
trigger times: 0
Loss after 411544320 batches: 0.1884
trigger times: 0
Loss after 411675420 batches: 0.1063
trigger times: 0
Loss after 411806520 batches: 0.0757
trigger times: 0
Loss after 411937620 batches: 0.0609
trigger times: 0
Loss after 412068720 batches: 0.0530
trigger times: 1
Loss after 412199820 batches: 0.0506
trigger times: 0
Loss after 412330920 batches: 0.0451
trigger times: 1
Loss after 412462020 batches: 0.0446
trigger times: 0
Loss after 412593120 batches: 0.0425
trigger times: 1
Loss after 412724220 batches: 0.0390
trigger times: 2
Loss after 412855320 batches: 0.0375
trigger times: 0
Loss after 412986420 batches: 0.0372
trigger times: 1
Loss after 413117520 batches: 0.0365
trigger times: 0
Loss after 413248620 batches: 0.0355
trigger times: 1
Loss after 413379720 batches: 0.0341
trigger times: 2
Loss after 413510820 batches: 0.0345
trigger times: 3
Loss after 413641920 batches: 0.0344
trigger times: 4
Loss after 413773020 batches: 0.0321
trigger times: 0
Loss after 413904120 batches: 0.0313
trigger times: 1
Loss after 414035220 batches: 0.0302
trigger times: 2
Loss after 414166320 batches: 0.0312
trigger times: 0
Loss after 414297420 batches: 0.0301
trigger times: 1
Loss after 414428520 batches: 0.0297
trigger times: 2
Loss after 414559620 batches: 0.0292
trigger times: 3
Loss after 414690720 batches: 0.0293
trigger times: 4
Loss after 414821820 batches: 0.0280
trigger times: 5
Loss after 414952920 batches: 0.0283
trigger times: 6
Loss after 415084020 batches: 0.0278
trigger times: 7
Loss after 415215120 batches: 0.0270
trigger times: 8
Loss after 415346220 batches: 0.0275
trigger times: 9
Loss after 415477320 batches: 0.0283
trigger times: 0
Loss after 415608420 batches: 0.0272
trigger times: 1
Loss after 415739520 batches: 0.0270
trigger times: 2
Loss after 415870620 batches: 0.0277
trigger times: 3
Loss after 416001720 batches: 0.0268
trigger times: 4
Loss after 416132820 batches: 0.0270
trigger times: 5
Loss after 416263920 batches: 0.0262
trigger times: 6
Loss after 416395020 batches: 0.0255
trigger times: 7
Loss after 416526120 batches: 0.0256
trigger times: 8
Loss after 416657220 batches: 0.0262
trigger times: 9
Loss after 416788320 batches: 0.0269
trigger times: 10
Loss after 416919420 batches: 0.0258
trigger times: 11
Loss after 417050520 batches: 0.0247
trigger times: 12
Loss after 417181620 batches: 0.0248
trigger times: 13
Loss after 417312720 batches: 0.0255
trigger times: 14
Loss after 417443820 batches: 0.0249
trigger times: 15
Loss after 417574920 batches: 0.0263
trigger times: 16
Loss after 417706020 batches: 0.0270
trigger times: 17
Loss after 417837120 batches: 0.0240
trigger times: 18
Loss after 417968220 batches: 0.0230
trigger times: 19
Loss after 418099320 batches: 0.0235
trigger times: 20
Early stopping!
Start to test process.
Loss after 418230420 batches: 0.0247
Time to train on one home:  386.23465037345886
trigger times: 0
Loss after 418361520 batches: 0.1015
trigger times: 0
Loss after 418492620 batches: 0.0289
trigger times: 0
Loss after 418623720 batches: 0.0211
trigger times: 0
Loss after 418754820 batches: 0.0182
trigger times: 0
Loss after 418885920 batches: 0.0167
trigger times: 1
Loss after 419017020 batches: 0.0154
trigger times: 2
Loss after 419148120 batches: 0.0150
trigger times: 3
Loss after 419279220 batches: 0.0142
trigger times: 4
Loss after 419410320 batches: 0.0137
trigger times: 5
Loss after 419541420 batches: 0.0133
trigger times: 6
Loss after 419672520 batches: 0.0131
trigger times: 7
Loss after 419803620 batches: 0.0124
trigger times: 8
Loss after 419934720 batches: 0.0122
trigger times: 9
Loss after 420065820 batches: 0.0120
trigger times: 10
Loss after 420196920 batches: 0.0117
trigger times: 11
Loss after 420328020 batches: 0.0116
trigger times: 12
Loss after 420459120 batches: 0.0114
trigger times: 13
Loss after 420590220 batches: 0.0111
trigger times: 14
Loss after 420721320 batches: 0.0108
trigger times: 15
Loss after 420852420 batches: 0.0108
trigger times: 16
Loss after 420983520 batches: 0.0106
trigger times: 17
Loss after 421114620 batches: 0.0105
trigger times: 18
Loss after 421245720 batches: 0.0103
trigger times: 19
Loss after 421376820 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 421507920 batches: 0.0100
Time to train on one home:  188.8013346195221
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378, 0.015884290067137628]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61], [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846]]
Round_8_results:  [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846]
trigger times: 0
Loss after 421639020 batches: 0.1283
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3318 < 3319; dropping {'Training_Loss': 0.12827691941891076, 'Validation_Loss': 0.20452738304932913, 'Training_R2': 0.8708204185692509, 'Validation_R2': 0.8098263850216233, 'Training_F1': 0.8058864203900686, 'Validation_F1': 0.747519682855143, 'Training_NEP': 0.38785864373178497, 'Validation_NEP': 0.4780225188335095, 'Training_NDE': 0.09697740472118721, 'Validation_NDE': 0.1514442421360059, 'Training_MAE': 12.845109073761126, 'Validation_MAE': 13.10936066036156, 'Training_MSE': 426.6854, 'Validation_MSE': 559.2794}.
trigger times: 0
Loss after 421770120 batches: 0.0321
trigger times: 0
Loss after 421901220 batches: 0.0234
trigger times: 1
Loss after 422032320 batches: 0.0202
trigger times: 2
Loss after 422163420 batches: 0.0182
trigger times: 3
Loss after 422294520 batches: 0.0171
trigger times: 4
Loss after 422425620 batches: 0.0165
trigger times: 0
Loss after 422556720 batches: 0.0156
trigger times: 0
Loss after 422687820 batches: 0.0154
trigger times: 1
Loss after 422818920 batches: 0.0144
trigger times: 2
Loss after 422950020 batches: 0.0142
trigger times: 3
Loss after 423081120 batches: 0.0141
trigger times: 4
Loss after 423212220 batches: 0.0137
trigger times: 5
Loss after 423343320 batches: 0.0133
trigger times: 6
Loss after 423474420 batches: 0.0128
trigger times: 7
Loss after 423605520 batches: 0.0129
trigger times: 8
Loss after 423736620 batches: 0.0124
trigger times: 9
Loss after 423867720 batches: 0.0125
trigger times: 10
Loss after 423998820 batches: 0.0121
trigger times: 11
Loss after 424129920 batches: 0.0121
trigger times: 12
Loss after 424261020 batches: 0.0117
trigger times: 13
Loss after 424392120 batches: 0.0115
trigger times: 14
Loss after 424523220 batches: 0.0114
trigger times: 15
Loss after 424654320 batches: 0.0112
trigger times: 16
Loss after 424785420 batches: 0.0112
trigger times: 17
Loss after 424916520 batches: 0.0112
trigger times: 18
Loss after 425047620 batches: 0.0112
trigger times: 19
Loss after 425178720 batches: 0.0108
trigger times: 20
Early stopping!
Start to test process.
Loss after 425309820 batches: 0.0108
Time to train on one home:  216.53862142562866
trigger times: 0
Loss after 425412420 batches: 0.2234
trigger times: 0
Loss after 425515020 batches: 0.0810
trigger times: 0
Loss after 425617620 batches: 0.0576
trigger times: 1
Loss after 425720220 batches: 0.0420
trigger times: 2
Loss after 425822820 batches: 0.0353
trigger times: 3
Loss after 425925420 batches: 0.0324
trigger times: 4
Loss after 426028020 batches: 0.0287
trigger times: 5
Loss after 426130620 batches: 0.0275
trigger times: 0
Loss after 426233220 batches: 0.0281
trigger times: 0
Loss after 426335820 batches: 0.0272
trigger times: 0
Loss after 426438420 batches: 0.0263
trigger times: 0
Loss after 426541020 batches: 0.0247
trigger times: 1
Loss after 426643620 batches: 0.0246
trigger times: 2
Loss after 426746220 batches: 0.0248
trigger times: 0
Loss after 426848820 batches: 0.0231
trigger times: 1
Loss after 426951420 batches: 0.0225
trigger times: 2
Loss after 427054020 batches: 0.0250
trigger times: 3
Loss after 427156620 batches: 0.0220
trigger times: 4
Loss after 427259220 batches: 0.0221
trigger times: 5
Loss after 427361820 batches: 0.0211
trigger times: 6
Loss after 427464420 batches: 0.0213
trigger times: 7
Loss after 427567020 batches: 0.0231
trigger times: 8
Loss after 427669620 batches: 0.0223
trigger times: 9
Loss after 427772220 batches: 0.0205
trigger times: 10
Loss after 427874820 batches: 0.0195
trigger times: 11
Loss after 427977420 batches: 0.0197
trigger times: 12
Loss after 428080020 batches: 0.0189
trigger times: 13
Loss after 428182620 batches: 0.0193
trigger times: 14
Loss after 428285220 batches: 0.0190
trigger times: 0
Loss after 428387820 batches: 0.0187
trigger times: 1
Loss after 428490420 batches: 0.0186
trigger times: 0
Loss after 428593020 batches: 0.0197
trigger times: 1
Loss after 428695620 batches: 0.0213
trigger times: 2
Loss after 428798220 batches: 0.0195
trigger times: 0
Loss after 428900820 batches: 0.0191
trigger times: 1
Loss after 429003420 batches: 0.0184
trigger times: 2
Loss after 429106020 batches: 0.0180
trigger times: 3
Loss after 429208620 batches: 0.0179
trigger times: 4
Loss after 429311220 batches: 0.0200
trigger times: 5
Loss after 429413820 batches: 0.0201
trigger times: 6
Loss after 429516420 batches: 0.0176
trigger times: 7
Loss after 429619020 batches: 0.0172
trigger times: 8
Loss after 429721620 batches: 0.0180
trigger times: 9
Loss after 429824220 batches: 0.0192
trigger times: 10
Loss after 429926820 batches: 0.0199
trigger times: 11
Loss after 430029420 batches: 0.0174
trigger times: 12
Loss after 430132020 batches: 0.0180
trigger times: 0
Loss after 430234620 batches: 0.0194
trigger times: 1
Loss after 430337220 batches: 0.0175
trigger times: 2
Loss after 430439820 batches: 0.0172
trigger times: 3
Loss after 430542420 batches: 0.0170
trigger times: 4
Loss after 430645020 batches: 0.0167
trigger times: 5
Loss after 430747620 batches: 0.0167
trigger times: 0
Loss after 430850220 batches: 0.0175
trigger times: 1
Loss after 430952820 batches: 0.0169
trigger times: 2
Loss after 431055420 batches: 0.0188
trigger times: 3
Loss after 431158020 batches: 0.0184
trigger times: 4
Loss after 431260620 batches: 0.0172
trigger times: 5
Loss after 431363220 batches: 0.0169
trigger times: 6
Loss after 431465820 batches: 0.0176
trigger times: 7
Loss after 431568420 batches: 0.0168
trigger times: 8
Loss after 431671020 batches: 0.0167
trigger times: 9
Loss after 431773620 batches: 0.0182
trigger times: 10
Loss after 431876220 batches: 0.0166
trigger times: 11
Loss after 431978820 batches: 0.0165
trigger times: 12
Loss after 432081420 batches: 0.0168
trigger times: 13
Loss after 432184020 batches: 0.0165
trigger times: 14
Loss after 432286620 batches: 0.0182
trigger times: 15
Loss after 432389220 batches: 0.0162
trigger times: 16
Loss after 432491820 batches: 0.0158
trigger times: 17
Loss after 432594420 batches: 0.0165
trigger times: 18
Loss after 432697020 batches: 0.0161
trigger times: 19
Loss after 432799620 batches: 0.0170
trigger times: 20
Early stopping!
Start to test process.
Loss after 432902220 batches: 0.0178
Time to train on one home:  431.53394508361816
trigger times: 0
Loss after 433033320 batches: 0.1453
trigger times: 1
Loss after 433164420 batches: 0.0479
trigger times: 2
Loss after 433295520 batches: 0.0338
trigger times: 0
Loss after 433426620 batches: 0.0289
trigger times: 1
Loss after 433557720 batches: 0.0264
trigger times: 0
Loss after 433688820 batches: 0.0247
trigger times: 1
Loss after 433819920 batches: 0.0235
trigger times: 2
Loss after 433951020 batches: 0.0224
trigger times: 3
Loss after 434082120 batches: 0.0213
trigger times: 4
Loss after 434213220 batches: 0.0209
trigger times: 5
Loss after 434344320 batches: 0.0200
trigger times: 6
Loss after 434475420 batches: 0.0198
trigger times: 7
Loss after 434606520 batches: 0.0192
trigger times: 8
Loss after 434737620 batches: 0.0187
trigger times: 9
Loss after 434868720 batches: 0.0183
trigger times: 10
Loss after 434999820 batches: 0.0183
trigger times: 11
Loss after 435130920 batches: 0.0178
trigger times: 12
Loss after 435262020 batches: 0.0177
trigger times: 13
Loss after 435393120 batches: 0.0170
trigger times: 14
Loss after 435524220 batches: 0.0169
trigger times: 15
Loss after 435655320 batches: 0.0167
trigger times: 0
Loss after 435786420 batches: 0.0165
trigger times: 1
Loss after 435917520 batches: 0.0166
trigger times: 2
Loss after 436048620 batches: 0.0163
trigger times: 3
Loss after 436179720 batches: 0.0161
trigger times: 4
Loss after 436310820 batches: 0.0160
trigger times: 5
Loss after 436441920 batches: 0.0158
trigger times: 6
Loss after 436573020 batches: 0.0154
trigger times: 7
Loss after 436704120 batches: 0.0155
trigger times: 8
Loss after 436835220 batches: 0.0153
trigger times: 9
Loss after 436966320 batches: 0.0154
trigger times: 10
Loss after 437097420 batches: 0.0151
trigger times: 11
Loss after 437228520 batches: 0.0150
trigger times: 12
Loss after 437359620 batches: 0.0148
trigger times: 13
Loss after 437490720 batches: 0.0149
trigger times: 14
Loss after 437621820 batches: 0.0146
trigger times: 15
Loss after 437752920 batches: 0.0146
trigger times: 16
Loss after 437884020 batches: 0.0147
trigger times: 17
Loss after 438015120 batches: 0.0145
trigger times: 18
Loss after 438146220 batches: 0.0145
trigger times: 19
Loss after 438277320 batches: 0.0142
trigger times: 20
Early stopping!
Start to test process.
Loss after 438408420 batches: 0.0143
Time to train on one home:  308.0817017555237
trigger times: 0
Loss after 438539520 batches: 0.2341
trigger times: 0
Loss after 438670620 batches: 0.0689
trigger times: 0
Loss after 438801720 batches: 0.0492
trigger times: 1
Loss after 438932820 batches: 0.0428
trigger times: 2
Loss after 439063920 batches: 0.0382
trigger times: 0
Loss after 439195020 batches: 0.0353
trigger times: 1
Loss after 439326120 batches: 0.0335
trigger times: 2
Loss after 439457220 batches: 0.0317
trigger times: 0
Loss after 439588320 batches: 0.0310
trigger times: 1
Loss after 439719420 batches: 0.0297
trigger times: 2
Loss after 439850520 batches: 0.0292
trigger times: 3
Loss after 439981620 batches: 0.0280
trigger times: 4
Loss after 440112720 batches: 0.0277
trigger times: 5
Loss after 440243820 batches: 0.0270
trigger times: 6
Loss after 440374920 batches: 0.0267
trigger times: 7
Loss after 440506020 batches: 0.0260
trigger times: 8
Loss after 440637120 batches: 0.0255
trigger times: 9
Loss after 440768220 batches: 0.0253
trigger times: 0
Loss after 440899320 batches: 0.0247
trigger times: 0
Loss after 441030420 batches: 0.0243
trigger times: 1
Loss after 441161520 batches: 0.0242
trigger times: 2
Loss after 441292620 batches: 0.0238
trigger times: 3
Loss after 441423720 batches: 0.0236
trigger times: 4
Loss after 441554820 batches: 0.0232
trigger times: 5
Loss after 441685920 batches: 0.0230
trigger times: 6
Loss after 441817020 batches: 0.0228
trigger times: 7
Loss after 441948120 batches: 0.0225
trigger times: 8
Loss after 442079220 batches: 0.0226
trigger times: 9
Loss after 442210320 batches: 0.0222
trigger times: 0
Loss after 442341420 batches: 0.0219
trigger times: 1
Loss after 442472520 batches: 0.0217
trigger times: 2
Loss after 442603620 batches: 0.0217
trigger times: 3
Loss after 442734720 batches: 0.0214
trigger times: 4
Loss after 442865820 batches: 0.0214
trigger times: 5
Loss after 442996920 batches: 0.0209
trigger times: 6
Loss after 443128020 batches: 0.0213
trigger times: 7
Loss after 443259120 batches: 0.0207
trigger times: 8
Loss after 443390220 batches: 0.0208
trigger times: 9
Loss after 443521320 batches: 0.0209
trigger times: 10
Loss after 443652420 batches: 0.0206
trigger times: 11
Loss after 443783520 batches: 0.0204
trigger times: 12
Loss after 443914620 batches: 0.0205
trigger times: 0
Loss after 444045720 batches: 0.0203
trigger times: 1
Loss after 444176820 batches: 0.0198
trigger times: 2
Loss after 444307920 batches: 0.0199
trigger times: 3
Loss after 444439020 batches: 0.0197
trigger times: 4
Loss after 444570120 batches: 0.0199
trigger times: 5
Loss after 444701220 batches: 0.0195
trigger times: 6
Loss after 444832320 batches: 0.0195
trigger times: 7
Loss after 444963420 batches: 0.0191
trigger times: 0
Loss after 445094520 batches: 0.0194
trigger times: 1
Loss after 445225620 batches: 0.0193
trigger times: 2
Loss after 445356720 batches: 0.0193
trigger times: 3
Loss after 445487820 batches: 0.0191
trigger times: 4
Loss after 445618920 batches: 0.0193
trigger times: 0
Loss after 445750020 batches: 0.0188
trigger times: 1
Loss after 445881120 batches: 0.0189
trigger times: 2
Loss after 446012220 batches: 0.0188
trigger times: 3
Loss after 446143320 batches: 0.0187
trigger times: 4
Loss after 446274420 batches: 0.0186
trigger times: 5
Loss after 446405520 batches: 0.0186
trigger times: 6
Loss after 446536620 batches: 0.0185
trigger times: 7
Loss after 446667720 batches: 0.0182
trigger times: 8
Loss after 446798820 batches: 0.0183
trigger times: 9
Loss after 446929920 batches: 0.0185
trigger times: 10
Loss after 447061020 batches: 0.0181
trigger times: 11
Loss after 447192120 batches: 0.0181
trigger times: 0
Loss after 447323220 batches: 0.0178
trigger times: 1
Loss after 447454320 batches: 0.0179
trigger times: 2
Loss after 447585420 batches: 0.0180
trigger times: 0
Loss after 447716520 batches: 0.0179
trigger times: 1
Loss after 447847620 batches: 0.0176
trigger times: 2
Loss after 447978720 batches: 0.0176
trigger times: 3
Loss after 448109820 batches: 0.0175
trigger times: 4
Loss after 448240920 batches: 0.0177
trigger times: 5
Loss after 448372020 batches: 0.0174
trigger times: 0
Loss after 448503120 batches: 0.0175
trigger times: 1
Loss after 448634220 batches: 0.0174
trigger times: 2
Loss after 448765320 batches: 0.0171
trigger times: 3
Loss after 448896420 batches: 0.0172
trigger times: 4
Loss after 449027520 batches: 0.0175
trigger times: 5
Loss after 449158620 batches: 0.0174
trigger times: 6
Loss after 449289720 batches: 0.0173
trigger times: 7
Loss after 449420820 batches: 0.0173
trigger times: 8
Loss after 449551920 batches: 0.0172
trigger times: 9
Loss after 449683020 batches: 0.0174
trigger times: 10
Loss after 449814120 batches: 0.0172
trigger times: 11
Loss after 449945220 batches: 0.0172
trigger times: 12
Loss after 450076320 batches: 0.0172
trigger times: 13
Loss after 450207420 batches: 0.0170
trigger times: 14
Loss after 450338520 batches: 0.0170
trigger times: 15
Loss after 450469620 batches: 0.0168
trigger times: 16
Loss after 450600720 batches: 0.0168
trigger times: 17
Loss after 450731820 batches: 0.0168
trigger times: 18
Loss after 450862920 batches: 0.0168
trigger times: 19
Loss after 450994020 batches: 0.0164
trigger times: 20
Early stopping!
Start to test process.
Loss after 451125120 batches: 0.0167
Time to train on one home:  697.7004723548889
trigger times: 0
Loss after 451253760 batches: 0.1291
trigger times: 0
Loss after 451382400 batches: 0.0368
trigger times: 1
Loss after 451511040 batches: 0.0275
trigger times: 2
Loss after 451639680 batches: 0.0237
trigger times: 3
Loss after 451768320 batches: 0.0220
trigger times: 4
Loss after 451896960 batches: 0.0205
trigger times: 5
Loss after 452025600 batches: 0.0195
trigger times: 0
Loss after 452154240 batches: 0.0184
trigger times: 1
Loss after 452282880 batches: 0.0181
trigger times: 2
Loss after 452411520 batches: 0.0172
trigger times: 3
Loss after 452540160 batches: 0.0170
trigger times: 4
Loss after 452668800 batches: 0.0165
trigger times: 0
Loss after 452797440 batches: 0.0159
trigger times: 1
Loss after 452926080 batches: 0.0157
trigger times: 2
Loss after 453054720 batches: 0.0154
trigger times: 3
Loss after 453183360 batches: 0.0152
trigger times: 4
Loss after 453312000 batches: 0.0149
trigger times: 5
Loss after 453440640 batches: 0.0147
trigger times: 6
Loss after 453569280 batches: 0.0145
trigger times: 7
Loss after 453697920 batches: 0.0143
trigger times: 8
Loss after 453826560 batches: 0.0142
trigger times: 9
Loss after 453955200 batches: 0.0140
trigger times: 10
Loss after 454083840 batches: 0.0135
trigger times: 11
Loss after 454212480 batches: 0.0135
trigger times: 12
Loss after 454341120 batches: 0.0132
trigger times: 13
Loss after 454469760 batches: 0.0131
trigger times: 14
Loss after 454598400 batches: 0.0132
trigger times: 15
Loss after 454727040 batches: 0.0131
trigger times: 16
Loss after 454855680 batches: 0.0130
trigger times: 17
Loss after 454984320 batches: 0.0129
trigger times: 18
Loss after 455112960 batches: 0.0130
trigger times: 19
Loss after 455241600 batches: 0.0126
trigger times: 20
Early stopping!
Start to test process.
Loss after 455370240 batches: 0.0124
Time to train on one home:  241.09910464286804
trigger times: 0
Loss after 455501340 batches: 0.2294
trigger times: 0
Loss after 455632440 batches: 0.0587
trigger times: 0
Loss after 455763540 batches: 0.0426
trigger times: 0
Loss after 455894640 batches: 0.0374
trigger times: 0
Loss after 456025740 batches: 0.0341
trigger times: 0
Loss after 456156840 batches: 0.0320
trigger times: 0
Loss after 456287940 batches: 0.0305
trigger times: 0
Loss after 456419040 batches: 0.0293
trigger times: 1
Loss after 456550140 batches: 0.0281
trigger times: 2
Loss after 456681240 batches: 0.0276
trigger times: 3
Loss after 456812340 batches: 0.0263
trigger times: 4
Loss after 456943440 batches: 0.0260
trigger times: 5
Loss after 457074540 batches: 0.0253
trigger times: 6
Loss after 457205640 batches: 0.0247
trigger times: 0
Loss after 457336740 batches: 0.0243
trigger times: 1
Loss after 457467840 batches: 0.0241
trigger times: 2
Loss after 457598940 batches: 0.0235
trigger times: 3
Loss after 457730040 batches: 0.0236
trigger times: 4
Loss after 457861140 batches: 0.0230
trigger times: 5
Loss after 457992240 batches: 0.0226
trigger times: 6
Loss after 458123340 batches: 0.0224
trigger times: 7
Loss after 458254440 batches: 0.0224
trigger times: 8
Loss after 458385540 batches: 0.0219
trigger times: 9
Loss after 458516640 batches: 0.0216
trigger times: 10
Loss after 458647740 batches: 0.0214
trigger times: 11
Loss after 458778840 batches: 0.0213
trigger times: 12
Loss after 458909940 batches: 0.0208
trigger times: 13
Loss after 459041040 batches: 0.0211
trigger times: 14
Loss after 459172140 batches: 0.0208
trigger times: 15
Loss after 459303240 batches: 0.0208
trigger times: 16
Loss after 459434340 batches: 0.0208
trigger times: 17
Loss after 459565440 batches: 0.0204
trigger times: 18
Loss after 459696540 batches: 0.0203
trigger times: 19
Loss after 459827640 batches: 0.0202
trigger times: 20
Early stopping!
Start to test process.
Loss after 459958740 batches: 0.0200
Time to train on one home:  259.16942286491394
trigger times: 0
Loss after 460089840 batches: 0.2831
trigger times: 0
Loss after 460220940 batches: 0.0923
trigger times: 1
Loss after 460352040 batches: 0.0613
trigger times: 2
Loss after 460483140 batches: 0.0471
trigger times: 3
Loss after 460614240 batches: 0.0415
trigger times: 0
Loss after 460745340 batches: 0.0379
trigger times: 1
Loss after 460876440 batches: 0.0357
trigger times: 0
Loss after 461007540 batches: 0.0346
trigger times: 1
Loss after 461138640 batches: 0.0336
trigger times: 2
Loss after 461269740 batches: 0.0317
trigger times: 3
Loss after 461400840 batches: 0.0310
trigger times: 4
Loss after 461531940 batches: 0.0294
trigger times: 5
Loss after 461663040 batches: 0.0305
trigger times: 6
Loss after 461794140 batches: 0.0294
trigger times: 0
Loss after 461925240 batches: 0.0289
trigger times: 1
Loss after 462056340 batches: 0.0289
trigger times: 2
Loss after 462187440 batches: 0.0292
trigger times: 3
Loss after 462318540 batches: 0.0281
trigger times: 4
Loss after 462449640 batches: 0.0277
trigger times: 5
Loss after 462580740 batches: 0.0262
trigger times: 0
Loss after 462711840 batches: 0.0263
trigger times: 1
Loss after 462842940 batches: 0.0255
trigger times: 2
Loss after 462974040 batches: 0.0252
trigger times: 3
Loss after 463105140 batches: 0.0275
trigger times: 4
Loss after 463236240 batches: 0.0278
trigger times: 5
Loss after 463367340 batches: 0.0257
trigger times: 6
Loss after 463498440 batches: 0.0253
trigger times: 7
Loss after 463629540 batches: 0.0247
trigger times: 8
Loss after 463760640 batches: 0.0246
trigger times: 9
Loss after 463891740 batches: 0.0247
trigger times: 10
Loss after 464022840 batches: 0.0249
trigger times: 11
Loss after 464153940 batches: 0.0239
trigger times: 12
Loss after 464285040 batches: 0.0239
trigger times: 13
Loss after 464416140 batches: 0.0229
trigger times: 14
Loss after 464547240 batches: 0.0237
trigger times: 15
Loss after 464678340 batches: 0.0238
trigger times: 0
Loss after 464809440 batches: 0.0232
trigger times: 1
Loss after 464940540 batches: 0.0227
trigger times: 2
Loss after 465071640 batches: 0.0230
trigger times: 3
Loss after 465202740 batches: 0.0225
trigger times: 4
Loss after 465333840 batches: 0.0229
trigger times: 5
Loss after 465464940 batches: 0.0234
trigger times: 6
Loss after 465596040 batches: 0.0225
trigger times: 7
Loss after 465727140 batches: 0.0223
trigger times: 8
Loss after 465858240 batches: 0.0219
trigger times: 9
Loss after 465989340 batches: 0.0214
trigger times: 10
Loss after 466120440 batches: 0.0215
trigger times: 11
Loss after 466251540 batches: 0.0218
trigger times: 12
Loss after 466382640 batches: 0.0213
trigger times: 13
Loss after 466513740 batches: 0.0211
trigger times: 14
Loss after 466644840 batches: 0.0217
trigger times: 15
Loss after 466775940 batches: 0.0211
trigger times: 16
Loss after 466907040 batches: 0.0215
trigger times: 17
Loss after 467038140 batches: 0.0207
trigger times: 18
Loss after 467169240 batches: 0.0208
trigger times: 19
Loss after 467300340 batches: 0.0216
trigger times: 20
Early stopping!
Start to test process.
Loss after 467431440 batches: 0.0211
Time to train on one home:  414.5108997821808
trigger times: 0
Loss after 467562540 batches: 0.1020
trigger times: 1
Loss after 467693640 batches: 0.0291
trigger times: 2
Loss after 467824740 batches: 0.0212
trigger times: 3
Loss after 467955840 batches: 0.0185
trigger times: 4
Loss after 468086940 batches: 0.0165
trigger times: 5
Loss after 468218040 batches: 0.0152
trigger times: 6
Loss after 468349140 batches: 0.0146
trigger times: 7
Loss after 468480240 batches: 0.0139
trigger times: 8
Loss after 468611340 batches: 0.0132
trigger times: 9
Loss after 468742440 batches: 0.0129
trigger times: 10
Loss after 468873540 batches: 0.0124
trigger times: 11
Loss after 469004640 batches: 0.0124
trigger times: 12
Loss after 469135740 batches: 0.0119
trigger times: 13
Loss after 469266840 batches: 0.0117
trigger times: 14
Loss after 469397940 batches: 0.0115
trigger times: 15
Loss after 469529040 batches: 0.0114
trigger times: 16
Loss after 469660140 batches: 0.0111
trigger times: 17
Loss after 469791240 batches: 0.0109
trigger times: 18
Loss after 469922340 batches: 0.0105
trigger times: 19
Loss after 470053440 batches: 0.0105
trigger times: 20
Early stopping!
Start to test process.
Loss after 470184540 batches: 0.0104
Time to train on one home:  160.00737309455872
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378, 0.015884290067137628, 0.015422517538360537]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61], [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846], [0.5624003973272111, 0.39199562780285213, 0.46338177904139005, 1.0500325960715429, 0.49807389089844895, 24.807396438856706, 1537.5851]]
Round_9_results:  [0.5624003973272111, 0.39199562780285213, 0.46338177904139005, 1.0500325960715429, 0.49807389089844895, 24.807396438856706, 1537.5851]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3706 < 3707; dropping {'Training_Loss': 0.12187439421156668, 'Validation_Loss': 0.1942493493358294, 'Training_R2': 0.8772882445122296, 'Validation_R2': 0.8193297577618193, 'Training_F1': 0.8111220994351718, 'Validation_F1': 0.7561563464081092, 'Training_NEP': 0.3773817043098161, 'Validation_NEP': 0.4689869059300325, 'Training_NDE': 0.09212189298170456, 'Validation_NDE': 0.14387625704754575, 'Training_MAE': 12.498133618116924, 'Validation_MAE': 12.861566668086581, 'Training_MSE': 405.32187, 'Validation_MSE': 531.33105}.
trigger times: 0
Loss after 470315640 batches: 0.1219
trigger times: 0
Loss after 470446740 batches: 0.0303
trigger times: 0
Loss after 470577840 batches: 0.0220
trigger times: 0
Loss after 470708940 batches: 0.0195
trigger times: 1
Loss after 470840040 batches: 0.0177
trigger times: 0
Loss after 470971140 batches: 0.0170
trigger times: 1
Loss after 471102240 batches: 0.0158
trigger times: 2
Loss after 471233340 batches: 0.0151
trigger times: 3
Loss after 471364440 batches: 0.0144
trigger times: 4
Loss after 471495540 batches: 0.0141
trigger times: 5
Loss after 471626640 batches: 0.0136
trigger times: 6
Loss after 471757740 batches: 0.0133
trigger times: 7
Loss after 471888840 batches: 0.0129
trigger times: 8
Loss after 472019940 batches: 0.0125
trigger times: 9
Loss after 472151040 batches: 0.0123
trigger times: 10
Loss after 472282140 batches: 0.0122
trigger times: 11
Loss after 472413240 batches: 0.0120
trigger times: 12
Loss after 472544340 batches: 0.0119
trigger times: 13
Loss after 472675440 batches: 0.0119
trigger times: 14
Loss after 472806540 batches: 0.0115
trigger times: 15
Loss after 472937640 batches: 0.0114
trigger times: 16
Loss after 473068740 batches: 0.0115
trigger times: 17
Loss after 473199840 batches: 0.0113
trigger times: 18
Loss after 473330940 batches: 0.0111
trigger times: 19
Loss after 473462040 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 473593140 batches: 0.0107
Time to train on one home:  195.37640190124512
trigger times: 0
Loss after 473695740 batches: 0.1941
trigger times: 0
Loss after 473798340 batches: 0.0696
trigger times: 1
Loss after 473900940 batches: 0.0510
trigger times: 2
Loss after 474003540 batches: 0.0364
trigger times: 3
Loss after 474106140 batches: 0.0323
trigger times: 4
Loss after 474208740 batches: 0.0308
trigger times: 5
Loss after 474311340 batches: 0.0291
trigger times: 6
Loss after 474413940 batches: 0.0304
trigger times: 7
Loss after 474516540 batches: 0.0270
trigger times: 0
Loss after 474619140 batches: 0.0288
trigger times: 1
Loss after 474721740 batches: 0.0253
trigger times: 0
Loss after 474824340 batches: 0.0241
trigger times: 1
Loss after 474926940 batches: 0.0227
trigger times: 0
Loss after 475029540 batches: 0.0225
trigger times: 1
Loss after 475132140 batches: 0.0222
trigger times: 2
Loss after 475234740 batches: 0.0221
trigger times: 0
Loss after 475337340 batches: 0.0229
trigger times: 1
Loss after 475439940 batches: 0.0231
trigger times: 0
Loss after 475542540 batches: 0.0246
trigger times: 1
Loss after 475645140 batches: 0.0214
trigger times: 2
Loss after 475747740 batches: 0.0226
trigger times: 3
Loss after 475850340 batches: 0.0202
trigger times: 4
Loss after 475952940 batches: 0.0192
trigger times: 5
Loss after 476055540 batches: 0.0192
trigger times: 6
Loss after 476158140 batches: 0.0194
trigger times: 0
Loss after 476260740 batches: 0.0206
trigger times: 1
Loss after 476363340 batches: 0.0200
trigger times: 2
Loss after 476465940 batches: 0.0190
trigger times: 3
Loss after 476568540 batches: 0.0190
trigger times: 4
Loss after 476671140 batches: 0.0181
trigger times: 5
Loss after 476773740 batches: 0.0180
trigger times: 0
Loss after 476876340 batches: 0.0185
trigger times: 0
Loss after 476978940 batches: 0.0184
trigger times: 1
Loss after 477081540 batches: 0.0193
trigger times: 2
Loss after 477184140 batches: 0.0182
trigger times: 3
Loss after 477286740 batches: 0.0174
trigger times: 4
Loss after 477389340 batches: 0.0178
trigger times: 5
Loss after 477491940 batches: 0.0164
trigger times: 6
Loss after 477594540 batches: 0.0171
trigger times: 7
Loss after 477697140 batches: 0.0171
trigger times: 8
Loss after 477799740 batches: 0.0182
trigger times: 9
Loss after 477902340 batches: 0.0180
trigger times: 10
Loss after 478004940 batches: 0.0171
trigger times: 11
Loss after 478107540 batches: 0.0171
trigger times: 12
Loss after 478210140 batches: 0.0177
trigger times: 13
Loss after 478312740 batches: 0.0172
trigger times: 14
Loss after 478415340 batches: 0.0175
trigger times: 15
Loss after 478517940 batches: 0.0202
trigger times: 16
Loss after 478620540 batches: 0.0183
trigger times: 17
Loss after 478723140 batches: 0.0187
trigger times: 18
Loss after 478825740 batches: 0.0175
trigger times: 19
Loss after 478928340 batches: 0.0164
trigger times: 20
Early stopping!
Start to test process.
Loss after 479030940 batches: 0.0160
Time to train on one home:  312.74274015426636
trigger times: 0
Loss after 479162040 batches: 0.1256
trigger times: 0
Loss after 479293140 batches: 0.0428
trigger times: 0
Loss after 479424240 batches: 0.0310
trigger times: 0
Loss after 479555340 batches: 0.0268
trigger times: 1
Loss after 479686440 batches: 0.0245
trigger times: 2
Loss after 479817540 batches: 0.0230
trigger times: 3
Loss after 479948640 batches: 0.0223
trigger times: 4
Loss after 480079740 batches: 0.0210
trigger times: 5
Loss after 480210840 batches: 0.0203
trigger times: 0
Loss after 480341940 batches: 0.0195
trigger times: 1
Loss after 480473040 batches: 0.0193
trigger times: 2
Loss after 480604140 batches: 0.0185
trigger times: 3
Loss after 480735240 batches: 0.0183
trigger times: 4
Loss after 480866340 batches: 0.0182
trigger times: 5
Loss after 480997440 batches: 0.0176
trigger times: 6
Loss after 481128540 batches: 0.0173
trigger times: 7
Loss after 481259640 batches: 0.0169
trigger times: 8
Loss after 481390740 batches: 0.0168
trigger times: 9
Loss after 481521840 batches: 0.0164
trigger times: 10
Loss after 481652940 batches: 0.0163
trigger times: 11
Loss after 481784040 batches: 0.0163
trigger times: 12
Loss after 481915140 batches: 0.0159
trigger times: 13
Loss after 482046240 batches: 0.0156
trigger times: 14
Loss after 482177340 batches: 0.0156
trigger times: 15
Loss after 482308440 batches: 0.0154
trigger times: 16
Loss after 482439540 batches: 0.0153
trigger times: 17
Loss after 482570640 batches: 0.0151
trigger times: 18
Loss after 482701740 batches: 0.0150
trigger times: 19
Loss after 482832840 batches: 0.0151
trigger times: 20
Early stopping!
Start to test process.
Loss after 482963940 batches: 0.0149
Time to train on one home:  223.84212183952332
trigger times: 0
Loss after 483095040 batches: 0.2107
trigger times: 0
Loss after 483226140 batches: 0.0629
trigger times: 0
Loss after 483357240 batches: 0.0451
trigger times: 1
Loss after 483488340 batches: 0.0384
trigger times: 2
Loss after 483619440 batches: 0.0348
trigger times: 0
Loss after 483750540 batches: 0.0324
trigger times: 0
Loss after 483881640 batches: 0.0310
trigger times: 1
Loss after 484012740 batches: 0.0295
trigger times: 2
Loss after 484143840 batches: 0.0289
trigger times: 3
Loss after 484274940 batches: 0.0277
trigger times: 4
Loss after 484406040 batches: 0.0271
trigger times: 0
Loss after 484537140 batches: 0.0264
trigger times: 1
Loss after 484668240 batches: 0.0256
trigger times: 2
Loss after 484799340 batches: 0.0250
trigger times: 3
Loss after 484930440 batches: 0.0243
trigger times: 4
Loss after 485061540 batches: 0.0241
trigger times: 5
Loss after 485192640 batches: 0.0237
trigger times: 6
Loss after 485323740 batches: 0.0235
trigger times: 7
Loss after 485454840 batches: 0.0230
trigger times: 8
Loss after 485585940 batches: 0.0229
trigger times: 9
Loss after 485717040 batches: 0.0230
trigger times: 10
Loss after 485848140 batches: 0.0225
trigger times: 11
Loss after 485979240 batches: 0.0222
trigger times: 0
Loss after 486110340 batches: 0.0217
trigger times: 1
Loss after 486241440 batches: 0.0215
trigger times: 2
Loss after 486372540 batches: 0.0211
trigger times: 3
Loss after 486503640 batches: 0.0211
trigger times: 4
Loss after 486634740 batches: 0.0212
trigger times: 5
Loss after 486765840 batches: 0.0209
trigger times: 6
Loss after 486896940 batches: 0.0206
trigger times: 7
Loss after 487028040 batches: 0.0203
trigger times: 8
Loss after 487159140 batches: 0.0206
trigger times: 9
Loss after 487290240 batches: 0.0202
trigger times: 0
Loss after 487421340 batches: 0.0202
trigger times: 1
Loss after 487552440 batches: 0.0200
trigger times: 2
Loss after 487683540 batches: 0.0198
trigger times: 3
Loss after 487814640 batches: 0.0198
trigger times: 0
Loss after 487945740 batches: 0.0198
trigger times: 1
Loss after 488076840 batches: 0.0194
trigger times: 2
Loss after 488207940 batches: 0.0192
trigger times: 3
Loss after 488339040 batches: 0.0194
trigger times: 4
Loss after 488470140 batches: 0.0192
trigger times: 5
Loss after 488601240 batches: 0.0193
trigger times: 6
Loss after 488732340 batches: 0.0189
trigger times: 7
Loss after 488863440 batches: 0.0187
trigger times: 8
Loss after 488994540 batches: 0.0187
trigger times: 0
Loss after 489125640 batches: 0.0190
trigger times: 0
Loss after 489256740 batches: 0.0186
trigger times: 1
Loss after 489387840 batches: 0.0184
trigger times: 2
Loss after 489518940 batches: 0.0187
trigger times: 3
Loss after 489650040 batches: 0.0186
trigger times: 4
Loss after 489781140 batches: 0.0183
trigger times: 0
Loss after 489912240 batches: 0.0184
trigger times: 1
Loss after 490043340 batches: 0.0181
trigger times: 2
Loss after 490174440 batches: 0.0178
trigger times: 3
Loss after 490305540 batches: 0.0181
trigger times: 4
Loss after 490436640 batches: 0.0184
trigger times: 5
Loss after 490567740 batches: 0.0177
trigger times: 6
Loss after 490698840 batches: 0.0177
trigger times: 7
Loss after 490829940 batches: 0.0175
trigger times: 8
Loss after 490961040 batches: 0.0177
trigger times: 9
Loss after 491092140 batches: 0.0176
trigger times: 10
Loss after 491223240 batches: 0.0177
trigger times: 11
Loss after 491354340 batches: 0.0177
trigger times: 12
Loss after 491485440 batches: 0.0175
trigger times: 13
Loss after 491616540 batches: 0.0173
trigger times: 14
Loss after 491747640 batches: 0.0171
trigger times: 15
Loss after 491878740 batches: 0.0172
trigger times: 16
Loss after 492009840 batches: 0.0171
trigger times: 17
Loss after 492140940 batches: 0.0171
trigger times: 18
Loss after 492272040 batches: 0.0170
trigger times: 19
Loss after 492403140 batches: 0.0169
trigger times: 20
Early stopping!
Start to test process.
Loss after 492534240 batches: 0.0170
Time to train on one home:  528.1638903617859
trigger times: 0
Loss after 492662880 batches: 0.1295
trigger times: 0
Loss after 492791520 batches: 0.0345
trigger times: 0
Loss after 492920160 batches: 0.0256
trigger times: 1
Loss after 493048800 batches: 0.0228
trigger times: 2
Loss after 493177440 batches: 0.0214
trigger times: 3
Loss after 493306080 batches: 0.0198
trigger times: 4
Loss after 493434720 batches: 0.0189
trigger times: 5
Loss after 493563360 batches: 0.0177
trigger times: 6
Loss after 493692000 batches: 0.0173
trigger times: 7
Loss after 493820640 batches: 0.0167
trigger times: 8
Loss after 493949280 batches: 0.0166
trigger times: 9
Loss after 494077920 batches: 0.0159
trigger times: 10
Loss after 494206560 batches: 0.0154
trigger times: 11
Loss after 494335200 batches: 0.0153
trigger times: 12
Loss after 494463840 batches: 0.0148
trigger times: 13
Loss after 494592480 batches: 0.0144
trigger times: 14
Loss after 494721120 batches: 0.0143
trigger times: 15
Loss after 494849760 batches: 0.0143
trigger times: 16
Loss after 494978400 batches: 0.0143
trigger times: 17
Loss after 495107040 batches: 0.0136
trigger times: 18
Loss after 495235680 batches: 0.0133
trigger times: 19
Loss after 495364320 batches: 0.0134
trigger times: 20
Early stopping!
Start to test process.
Loss after 495492960 batches: 0.0134
Time to train on one home:  171.61531114578247
trigger times: 0
Loss after 495624060 batches: 0.2118
trigger times: 0
Loss after 495755160 batches: 0.0567
trigger times: 0
Loss after 495886260 batches: 0.0415
trigger times: 1
Loss after 496017360 batches: 0.0368
trigger times: 2
Loss after 496148460 batches: 0.0337
trigger times: 0
Loss after 496279560 batches: 0.0312
trigger times: 1
Loss after 496410660 batches: 0.0299
trigger times: 2
Loss after 496541760 batches: 0.0291
trigger times: 0
Loss after 496672860 batches: 0.0278
trigger times: 0
Loss after 496803960 batches: 0.0268
trigger times: 1
Loss after 496935060 batches: 0.0263
trigger times: 2
Loss after 497066160 batches: 0.0256
trigger times: 0
Loss after 497197260 batches: 0.0249
trigger times: 0
Loss after 497328360 batches: 0.0245
trigger times: 0
Loss after 497459460 batches: 0.0241
trigger times: 1
Loss after 497590560 batches: 0.0240
trigger times: 2
Loss after 497721660 batches: 0.0235
trigger times: 3
Loss after 497852760 batches: 0.0230
trigger times: 4
Loss after 497983860 batches: 0.0230
trigger times: 5
Loss after 498114960 batches: 0.0227
trigger times: 6
Loss after 498246060 batches: 0.0220
trigger times: 7
Loss after 498377160 batches: 0.0221
trigger times: 8
Loss after 498508260 batches: 0.0215
trigger times: 9
Loss after 498639360 batches: 0.0212
trigger times: 10
Loss after 498770460 batches: 0.0215
trigger times: 11
Loss after 498901560 batches: 0.0212
trigger times: 12
Loss after 499032660 batches: 0.0207
trigger times: 13
Loss after 499163760 batches: 0.0210
trigger times: 14
Loss after 499294860 batches: 0.0205
trigger times: 15
Loss after 499425960 batches: 0.0204
trigger times: 16
Loss after 499557060 batches: 0.0203
trigger times: 17
Loss after 499688160 batches: 0.0201
trigger times: 18
Loss after 499819260 batches: 0.0199
trigger times: 19
Loss after 499950360 batches: 0.0199
trigger times: 20
Early stopping!
Start to test process.
Loss after 500081460 batches: 0.0194
Time to train on one home:  259.2527482509613
trigger times: 0
Loss after 500212560 batches: 0.2651
trigger times: 0
Loss after 500343660 batches: 0.0825
trigger times: 0
Loss after 500474760 batches: 0.0535
trigger times: 0
Loss after 500605860 batches: 0.0446
trigger times: 1
Loss after 500736960 batches: 0.0397
trigger times: 2
Loss after 500868060 batches: 0.0369
trigger times: 0
Loss after 500999160 batches: 0.0353
trigger times: 1
Loss after 501130260 batches: 0.0347
trigger times: 2
Loss after 501261360 batches: 0.0329
trigger times: 3
Loss after 501392460 batches: 0.0316
trigger times: 4
Loss after 501523560 batches: 0.0299
trigger times: 5
Loss after 501654660 batches: 0.0295
trigger times: 6
Loss after 501785760 batches: 0.0294
trigger times: 7
Loss after 501916860 batches: 0.0294
trigger times: 8
Loss after 502047960 batches: 0.0284
trigger times: 0
Loss after 502179060 batches: 0.0285
trigger times: 1
Loss after 502310160 batches: 0.0272
trigger times: 0
Loss after 502441260 batches: 0.0272
trigger times: 1
Loss after 502572360 batches: 0.0266
trigger times: 2
Loss after 502703460 batches: 0.0251
trigger times: 3
Loss after 502834560 batches: 0.0256
trigger times: 4
Loss after 502965660 batches: 0.0251
trigger times: 5
Loss after 503096760 batches: 0.0256
trigger times: 6
Loss after 503227860 batches: 0.0257
trigger times: 7
Loss after 503358960 batches: 0.0252
trigger times: 8
Loss after 503490060 batches: 0.0249
trigger times: 9
Loss after 503621160 batches: 0.0242
trigger times: 10
Loss after 503752260 batches: 0.0239
trigger times: 11
Loss after 503883360 batches: 0.0234
trigger times: 12
Loss after 504014460 batches: 0.0234
trigger times: 13
Loss after 504145560 batches: 0.0228
trigger times: 14
Loss after 504276660 batches: 0.0238
trigger times: 15
Loss after 504407760 batches: 0.0238
trigger times: 16
Loss after 504538860 batches: 0.0226
trigger times: 17
Loss after 504669960 batches: 0.0235
trigger times: 18
Loss after 504801060 batches: 0.0228
trigger times: 19
Loss after 504932160 batches: 0.0242
trigger times: 20
Early stopping!
Start to test process.
Loss after 505063260 batches: 0.0239
Time to train on one home:  279.4802083969116
trigger times: 0
Loss after 505194360 batches: 0.0854
trigger times: 0
Loss after 505325460 batches: 0.0249
trigger times: 0
Loss after 505456560 batches: 0.0194
trigger times: 1
Loss after 505587660 batches: 0.0170
trigger times: 2
Loss after 505718760 batches: 0.0157
trigger times: 3
Loss after 505849860 batches: 0.0145
trigger times: 4
Loss after 505980960 batches: 0.0132
trigger times: 5
Loss after 506112060 batches: 0.0131
trigger times: 6
Loss after 506243160 batches: 0.0126
trigger times: 7
Loss after 506374260 batches: 0.0122
trigger times: 8
Loss after 506505360 batches: 0.0117
trigger times: 9
Loss after 506636460 batches: 0.0117
trigger times: 10
Loss after 506767560 batches: 0.0115
trigger times: 11
Loss after 506898660 batches: 0.0111
trigger times: 12
Loss after 507029760 batches: 0.0110
trigger times: 13
Loss after 507160860 batches: 0.0109
trigger times: 14
Loss after 507291960 batches: 0.0105
trigger times: 15
Loss after 507423060 batches: 0.0104
trigger times: 16
Loss after 507554160 batches: 0.0101
trigger times: 17
Loss after 507685260 batches: 0.0100
trigger times: 18
Loss after 507816360 batches: 0.0098
trigger times: 19
Loss after 507947460 batches: 0.0096
trigger times: 20
Early stopping!
Start to test process.
Loss after 508078560 batches: 0.0096
Time to train on one home:  174.02641654014587
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378, 0.015884290067137628, 0.015422517538360537, 0.015618174097677863]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61], [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846], [0.5624003973272111, 0.39199562780285213, 0.46338177904139005, 1.0500325960715429, 0.49807389089844895, 24.807396438856706, 1537.5851], [0.5526623295413123, 0.4025342703450545, 0.4773154352778254, 1.041540162760643, 0.4894406919679644, 24.60675965799653, 1510.9338]]
Round_10_results:  [0.5526623295413123, 0.4025342703450545, 0.4773154352778254, 1.041540162760643, 0.4894406919679644, 24.60675965799653, 1510.9338]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4007 < 4008; dropping {'Training_Loss': 0.11205454993079293, 'Validation_Loss': 0.19818886038329867, 'Training_R2': 0.8871667241104514, 'Validation_R2': 0.8158250446474553, 'Training_F1': 0.8225953193116147, 'Validation_F1': 0.768024976707115, 'Training_NEP': 0.3544318392360303, 'Validation_NEP': 0.45610981425313385, 'Training_NDE': 0.08470594300403479, 'Validation_NDE': 0.14666722582399402, 'Training_MAE': 11.738079601363495, 'Validation_MAE': 12.508423390525216, 'Training_MSE': 372.6929, 'Validation_MSE': 541.638}.
trigger times: 0
Loss after 508209660 batches: 0.1121
trigger times: 0
Loss after 508340760 batches: 0.0290
trigger times: 0
Loss after 508471860 batches: 0.0219
trigger times: 1
Loss after 508602960 batches: 0.0189
trigger times: 0
Loss after 508734060 batches: 0.0172
trigger times: 0
Loss after 508865160 batches: 0.0163
trigger times: 1
Loss after 508996260 batches: 0.0155
trigger times: 2
Loss after 509127360 batches: 0.0144
trigger times: 3
Loss after 509258460 batches: 0.0146
trigger times: 4
Loss after 509389560 batches: 0.0138
trigger times: 5
Loss after 509520660 batches: 0.0132
trigger times: 6
Loss after 509651760 batches: 0.0130
trigger times: 7
Loss after 509782860 batches: 0.0127
trigger times: 8
Loss after 509913960 batches: 0.0123
trigger times: 9
Loss after 510045060 batches: 0.0121
trigger times: 10
Loss after 510176160 batches: 0.0121
trigger times: 11
Loss after 510307260 batches: 0.0118
trigger times: 12
Loss after 510438360 batches: 0.0117
trigger times: 13
Loss after 510569460 batches: 0.0113
trigger times: 14
Loss after 510700560 batches: 0.0115
trigger times: 15
Loss after 510831660 batches: 0.0111
trigger times: 16
Loss after 510962760 batches: 0.0111
trigger times: 17
Loss after 511093860 batches: 0.0109
trigger times: 18
Loss after 511224960 batches: 0.0106
trigger times: 19
Loss after 511356060 batches: 0.0104
trigger times: 20
Early stopping!
Start to test process.
Loss after 511487160 batches: 0.0107
Time to train on one home:  195.79409837722778
trigger times: 0
Loss after 511589760 batches: 0.1739
trigger times: 0
Loss after 511692360 batches: 0.0722
trigger times: 0
Loss after 511794960 batches: 0.0524
trigger times: 1
Loss after 511897560 batches: 0.0396
trigger times: 0
Loss after 512000160 batches: 0.0364
trigger times: 0
Loss after 512102760 batches: 0.0313
trigger times: 1
Loss after 512205360 batches: 0.0273
trigger times: 0
Loss after 512307960 batches: 0.0276
trigger times: 1
Loss after 512410560 batches: 0.0256
trigger times: 2
Loss after 512513160 batches: 0.0246
trigger times: 3
Loss after 512615760 batches: 0.0233
trigger times: 4
Loss after 512718360 batches: 0.0231
trigger times: 5
Loss after 512820960 batches: 0.0220
trigger times: 6
Loss after 512923560 batches: 0.0217
trigger times: 7
Loss after 513026160 batches: 0.0224
trigger times: 8
Loss after 513128760 batches: 0.0255
trigger times: 0
Loss after 513231360 batches: 0.0238
trigger times: 1
Loss after 513333960 batches: 0.0252
trigger times: 2
Loss after 513436560 batches: 0.0238
trigger times: 3
Loss after 513539160 batches: 0.0210
trigger times: 4
Loss after 513641760 batches: 0.0200
trigger times: 5
Loss after 513744360 batches: 0.0202
trigger times: 6
Loss after 513846960 batches: 0.0189
trigger times: 7
Loss after 513949560 batches: 0.0190
trigger times: 8
Loss after 514052160 batches: 0.0182
trigger times: 9
Loss after 514154760 batches: 0.0183
trigger times: 10
Loss after 514257360 batches: 0.0179
trigger times: 11
Loss after 514359960 batches: 0.0181
trigger times: 12
Loss after 514462560 batches: 0.0179
trigger times: 13
Loss after 514565160 batches: 0.0182
trigger times: 14
Loss after 514667760 batches: 0.0181
trigger times: 15
Loss after 514770360 batches: 0.0178
trigger times: 16
Loss after 514872960 batches: 0.0177
trigger times: 17
Loss after 514975560 batches: 0.0174
trigger times: 18
Loss after 515078160 batches: 0.0172
trigger times: 19
Loss after 515180760 batches: 0.0174
trigger times: 20
Early stopping!
Start to test process.
Loss after 515283360 batches: 0.0175
Time to train on one home:  221.74787735939026
trigger times: 0
Loss after 515414460 batches: 0.1097
trigger times: 1
Loss after 515545560 batches: 0.0395
trigger times: 0
Loss after 515676660 batches: 0.0297
trigger times: 1
Loss after 515807760 batches: 0.0262
trigger times: 2
Loss after 515938860 batches: 0.0240
trigger times: 3
Loss after 516069960 batches: 0.0224
trigger times: 4
Loss after 516201060 batches: 0.0213
trigger times: 5
Loss after 516332160 batches: 0.0209
trigger times: 6
Loss after 516463260 batches: 0.0199
trigger times: 7
Loss after 516594360 batches: 0.0189
trigger times: 8
Loss after 516725460 batches: 0.0189
trigger times: 9
Loss after 516856560 batches: 0.0184
trigger times: 10
Loss after 516987660 batches: 0.0184
trigger times: 11
Loss after 517118760 batches: 0.0177
trigger times: 12
Loss after 517249860 batches: 0.0174
trigger times: 13
Loss after 517380960 batches: 0.0173
trigger times: 14
Loss after 517512060 batches: 0.0170
trigger times: 15
Loss after 517643160 batches: 0.0166
trigger times: 16
Loss after 517774260 batches: 0.0165
trigger times: 17
Loss after 517905360 batches: 0.0160
trigger times: 18
Loss after 518036460 batches: 0.0158
trigger times: 19
Loss after 518167560 batches: 0.0158
trigger times: 20
Early stopping!
Start to test process.
Loss after 518298660 batches: 0.0157
Time to train on one home:  174.45812511444092
trigger times: 0
Loss after 518429760 batches: 0.1794
trigger times: 0
Loss after 518560860 batches: 0.0572
trigger times: 1
Loss after 518691960 batches: 0.0414
trigger times: 0
Loss after 518823060 batches: 0.0355
trigger times: 0
Loss after 518954160 batches: 0.0326
trigger times: 1
Loss after 519085260 batches: 0.0305
trigger times: 2
Loss after 519216360 batches: 0.0290
trigger times: 3
Loss after 519347460 batches: 0.0274
trigger times: 4
Loss after 519478560 batches: 0.0267
trigger times: 5
Loss after 519609660 batches: 0.0258
trigger times: 6
Loss after 519740760 batches: 0.0253
trigger times: 7
Loss after 519871860 batches: 0.0250
trigger times: 8
Loss after 520002960 batches: 0.0242
trigger times: 9
Loss after 520134060 batches: 0.0238
trigger times: 10
Loss after 520265160 batches: 0.0234
trigger times: 11
Loss after 520396260 batches: 0.0228
trigger times: 0
Loss after 520527360 batches: 0.0227
trigger times: 1
Loss after 520658460 batches: 0.0224
trigger times: 2
Loss after 520789560 batches: 0.0222
trigger times: 3
Loss after 520920660 batches: 0.0219
trigger times: 4
Loss after 521051760 batches: 0.0214
trigger times: 5
Loss after 521182860 batches: 0.0212
trigger times: 6
Loss after 521313960 batches: 0.0209
trigger times: 0
Loss after 521445060 batches: 0.0210
trigger times: 1
Loss after 521576160 batches: 0.0208
trigger times: 2
Loss after 521707260 batches: 0.0205
trigger times: 3
Loss after 521838360 batches: 0.0204
trigger times: 4
Loss after 521969460 batches: 0.0204
trigger times: 5
Loss after 522100560 batches: 0.0202
trigger times: 6
Loss after 522231660 batches: 0.0199
trigger times: 7
Loss after 522362760 batches: 0.0197
trigger times: 8
Loss after 522493860 batches: 0.0196
trigger times: 9
Loss after 522624960 batches: 0.0195
trigger times: 10
Loss after 522756060 batches: 0.0194
trigger times: 11
Loss after 522887160 batches: 0.0194
trigger times: 12
Loss after 523018260 batches: 0.0193
trigger times: 13
Loss after 523149360 batches: 0.0190
trigger times: 14
Loss after 523280460 batches: 0.0188
trigger times: 15
Loss after 523411560 batches: 0.0189
trigger times: 16
Loss after 523542660 batches: 0.0190
trigger times: 17
Loss after 523673760 batches: 0.0186
trigger times: 18
Loss after 523804860 batches: 0.0187
trigger times: 19
Loss after 523935960 batches: 0.0185
trigger times: 20
Early stopping!
Start to test process.
Loss after 524067060 batches: 0.0184
Time to train on one home:  322.43816351890564
trigger times: 0
Loss after 524195700 batches: 0.1094
trigger times: 0
Loss after 524324340 batches: 0.0327
trigger times: 0
Loss after 524452980 batches: 0.0251
trigger times: 1
Loss after 524581620 batches: 0.0217
trigger times: 2
Loss after 524710260 batches: 0.0199
trigger times: 0
Loss after 524838900 batches: 0.0191
trigger times: 1
Loss after 524967540 batches: 0.0183
trigger times: 2
Loss after 525096180 batches: 0.0176
trigger times: 3
Loss after 525224820 batches: 0.0168
trigger times: 4
Loss after 525353460 batches: 0.0163
trigger times: 5
Loss after 525482100 batches: 0.0158
trigger times: 6
Loss after 525610740 batches: 0.0156
trigger times: 7
Loss after 525739380 batches: 0.0151
trigger times: 8
Loss after 525868020 batches: 0.0150
trigger times: 9
Loss after 525996660 batches: 0.0146
trigger times: 10
Loss after 526125300 batches: 0.0141
trigger times: 11
Loss after 526253940 batches: 0.0140
trigger times: 12
Loss after 526382580 batches: 0.0139
trigger times: 13
Loss after 526511220 batches: 0.0137
trigger times: 14
Loss after 526639860 batches: 0.0137
trigger times: 15
Loss after 526768500 batches: 0.0136
trigger times: 16
Loss after 526897140 batches: 0.0132
trigger times: 17
Loss after 527025780 batches: 0.0130
trigger times: 18
Loss after 527154420 batches: 0.0128
trigger times: 19
Loss after 527283060 batches: 0.0129
trigger times: 20
Early stopping!
Start to test process.
Loss after 527411700 batches: 0.0128
Time to train on one home:  191.9817500114441
trigger times: 0
Loss after 527542800 batches: 0.2112
trigger times: 0
Loss after 527673900 batches: 0.0554
trigger times: 1
Loss after 527805000 batches: 0.0407
trigger times: 0
Loss after 527936100 batches: 0.0356
trigger times: 0
Loss after 528067200 batches: 0.0326
trigger times: 1
Loss after 528198300 batches: 0.0306
trigger times: 2
Loss after 528329400 batches: 0.0296
trigger times: 0
Loss after 528460500 batches: 0.0284
trigger times: 1
Loss after 528591600 batches: 0.0272
trigger times: 0
Loss after 528722700 batches: 0.0266
trigger times: 0
Loss after 528853800 batches: 0.0258
trigger times: 1
Loss after 528984900 batches: 0.0250
trigger times: 2
Loss after 529116000 batches: 0.0247
trigger times: 3
Loss after 529247100 batches: 0.0242
trigger times: 4
Loss after 529378200 batches: 0.0239
trigger times: 5
Loss after 529509300 batches: 0.0235
trigger times: 6
Loss after 529640400 batches: 0.0230
trigger times: 7
Loss after 529771500 batches: 0.0227
trigger times: 8
Loss after 529902600 batches: 0.0225
trigger times: 9
Loss after 530033700 batches: 0.0222
trigger times: 10
Loss after 530164800 batches: 0.0221
trigger times: 0
Loss after 530295900 batches: 0.0216
trigger times: 1
Loss after 530427000 batches: 0.0215
trigger times: 2
Loss after 530558100 batches: 0.0211
trigger times: 3
Loss after 530689200 batches: 0.0211
trigger times: 4
Loss after 530820300 batches: 0.0212
trigger times: 5
Loss after 530951400 batches: 0.0209
trigger times: 6
Loss after 531082500 batches: 0.0204
trigger times: 7
Loss after 531213600 batches: 0.0205
trigger times: 8
Loss after 531344700 batches: 0.0203
trigger times: 9
Loss after 531475800 batches: 0.0200
trigger times: 10
Loss after 531606900 batches: 0.0199
trigger times: 11
Loss after 531738000 batches: 0.0200
trigger times: 12
Loss after 531869100 batches: 0.0198
trigger times: 13
Loss after 532000200 batches: 0.0195
trigger times: 14
Loss after 532131300 batches: 0.0197
trigger times: 15
Loss after 532262400 batches: 0.0193
trigger times: 16
Loss after 532393500 batches: 0.0191
trigger times: 17
Loss after 532524600 batches: 0.0189
trigger times: 18
Loss after 532655700 batches: 0.0191
trigger times: 19
Loss after 532786800 batches: 0.0191
trigger times: 0
Loss after 532917900 batches: 0.0186
trigger times: 1
Loss after 533049000 batches: 0.0187
trigger times: 2
Loss after 533180100 batches: 0.0184
trigger times: 3
Loss after 533311200 batches: 0.0184
trigger times: 4
Loss after 533442300 batches: 0.0183
trigger times: 5
Loss after 533573400 batches: 0.0180
trigger times: 6
Loss after 533704500 batches: 0.0183
trigger times: 7
Loss after 533835600 batches: 0.0179
trigger times: 0
Loss after 533966700 batches: 0.0182
trigger times: 1
Loss after 534097800 batches: 0.0178
trigger times: 2
Loss after 534228900 batches: 0.0179
trigger times: 3
Loss after 534360000 batches: 0.0176
trigger times: 4
Loss after 534491100 batches: 0.0177
trigger times: 5
Loss after 534622200 batches: 0.0178
trigger times: 6
Loss after 534753300 batches: 0.0174
trigger times: 7
Loss after 534884400 batches: 0.0174
trigger times: 8
Loss after 535015500 batches: 0.0173
trigger times: 9
Loss after 535146600 batches: 0.0172
trigger times: 10
Loss after 535277700 batches: 0.0173
trigger times: 11
Loss after 535408800 batches: 0.0172
trigger times: 12
Loss after 535539900 batches: 0.0171
trigger times: 13
Loss after 535671000 batches: 0.0171
trigger times: 14
Loss after 535802100 batches: 0.0169
trigger times: 15
Loss after 535933200 batches: 0.0169
trigger times: 16
Loss after 536064300 batches: 0.0169
trigger times: 17
Loss after 536195400 batches: 0.0169
trigger times: 18
Loss after 536326500 batches: 0.0169
trigger times: 19
Loss after 536457600 batches: 0.0165
trigger times: 20
Early stopping!
Start to test process.
Loss after 536588700 batches: 0.0165
Time to train on one home:  506.21679067611694
trigger times: 0
Loss after 536719800 batches: 0.2450
trigger times: 0
Loss after 536850900 batches: 0.0786
trigger times: 1
Loss after 536982000 batches: 0.0547
trigger times: 0
Loss after 537113100 batches: 0.0450
trigger times: 0
Loss after 537244200 batches: 0.0383
trigger times: 0
Loss after 537375300 batches: 0.0356
trigger times: 1
Loss after 537506400 batches: 0.0340
trigger times: 0
Loss after 537637500 batches: 0.0339
trigger times: 1
Loss after 537768600 batches: 0.0317
trigger times: 2
Loss after 537899700 batches: 0.0307
trigger times: 3
Loss after 538030800 batches: 0.0295
trigger times: 4
Loss after 538161900 batches: 0.0283
trigger times: 5
Loss after 538293000 batches: 0.0284
trigger times: 6
Loss after 538424100 batches: 0.0290
trigger times: 7
Loss after 538555200 batches: 0.0270
trigger times: 8
Loss after 538686300 batches: 0.0268
trigger times: 9
Loss after 538817400 batches: 0.0254
trigger times: 10
Loss after 538948500 batches: 0.0268
trigger times: 11
Loss after 539079600 batches: 0.0258
trigger times: 12
Loss after 539210700 batches: 0.0254
trigger times: 13
Loss after 539341800 batches: 0.0254
trigger times: 14
Loss after 539472900 batches: 0.0248
trigger times: 15
Loss after 539604000 batches: 0.0247
trigger times: 16
Loss after 539735100 batches: 0.0242
trigger times: 17
Loss after 539866200 batches: 0.0244
trigger times: 18
Loss after 539997300 batches: 0.0240
trigger times: 19
Loss after 540128400 batches: 0.0234
trigger times: 20
Early stopping!
Start to test process.
Loss after 540259500 batches: 0.0246
Time to train on one home:  209.351322889328
trigger times: 0
Loss after 540390600 batches: 0.0752
trigger times: 0
Loss after 540521700 batches: 0.0244
trigger times: 0
Loss after 540652800 batches: 0.0185
trigger times: 1
Loss after 540783900 batches: 0.0162
trigger times: 2
Loss after 540915000 batches: 0.0145
trigger times: 3
Loss after 541046100 batches: 0.0138
trigger times: 4
Loss after 541177200 batches: 0.0130
trigger times: 5
Loss after 541308300 batches: 0.0127
trigger times: 6
Loss after 541439400 batches: 0.0124
trigger times: 7
Loss after 541570500 batches: 0.0121
trigger times: 8
Loss after 541701600 batches: 0.0115
trigger times: 9
Loss after 541832700 batches: 0.0111
trigger times: 10
Loss after 541963800 batches: 0.0111
trigger times: 11
Loss after 542094900 batches: 0.0108
trigger times: 12
Loss after 542226000 batches: 0.0107
trigger times: 13
Loss after 542357100 batches: 0.0103
trigger times: 14
Loss after 542488200 batches: 0.0103
trigger times: 15
Loss after 542619300 batches: 0.0100
trigger times: 16
Loss after 542750400 batches: 0.0099
trigger times: 17
Loss after 542881500 batches: 0.0096
trigger times: 18
Loss after 543012600 batches: 0.0097
trigger times: 19
Loss after 543143700 batches: 0.0096
trigger times: 20
Early stopping!
Start to test process.
Loss after 543274800 batches: 0.0093
Time to train on one home:  173.96862816810608
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378, 0.015884290067137628, 0.015422517538360537, 0.015618174097677863, 0.015689947067699456]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61], [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846], [0.5624003973272111, 0.39199562780285213, 0.46338177904139005, 1.0500325960715429, 0.49807389089844895, 24.807396438856706, 1537.5851], [0.5526623295413123, 0.4025342703450545, 0.4773154352778254, 1.041540162760643, 0.4894406919679644, 24.60675965799653, 1510.9338], [0.5434306197696261, 0.41257883262882467, 0.4890240237330659, 1.0321400196734463, 0.48121224091099224, 24.384677908326562, 1485.5321]]
Round_11_results:  [0.5434306197696261, 0.41257883262882467, 0.4890240237330659, 1.0321400196734463, 0.48121224091099224, 24.384677908326562, 1485.5321]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4284 < 4285; dropping {'Training_Loss': 0.10830521133710753, 'Validation_Loss': 0.1885351778732406, 'Training_R2': 0.8909001795590032, 'Validation_R2': 0.8247792142663103, 'Training_F1': 0.8242113406061662, 'Validation_F1': 0.7751580845529951, 'Training_NEP': 0.35116305369879125, 'Validation_NEP': 0.45215624479908145, 'Training_NDE': 0.0819031717298702, 'Validation_NDE': 0.13953659715061587, 'Training_MAE': 11.629823907070902, 'Validation_MAE': 12.40000011374019, 'Training_MSE': 360.3611, 'Validation_MSE': 515.3048}.
trigger times: 0
Loss after 543405900 batches: 0.1083
trigger times: 0
Loss after 543537000 batches: 0.0287
trigger times: 0
Loss after 543668100 batches: 0.0210
trigger times: 0
Loss after 543799200 batches: 0.0180
trigger times: 0
Loss after 543930300 batches: 0.0168
trigger times: 1
Loss after 544061400 batches: 0.0160
trigger times: 2
Loss after 544192500 batches: 0.0150
trigger times: 0
Loss after 544323600 batches: 0.0152
trigger times: 1
Loss after 544454700 batches: 0.0143
trigger times: 2
Loss after 544585800 batches: 0.0133
trigger times: 3
Loss after 544716900 batches: 0.0131
trigger times: 4
Loss after 544848000 batches: 0.0130
trigger times: 5
Loss after 544979100 batches: 0.0127
trigger times: 6
Loss after 545110200 batches: 0.0125
trigger times: 7
Loss after 545241300 batches: 0.0125
trigger times: 8
Loss after 545372400 batches: 0.0119
trigger times: 9
Loss after 545503500 batches: 0.0120
trigger times: 10
Loss after 545634600 batches: 0.0117
trigger times: 11
Loss after 545765700 batches: 0.0116
trigger times: 12
Loss after 545896800 batches: 0.0116
trigger times: 13
Loss after 546027900 batches: 0.0111
trigger times: 14
Loss after 546159000 batches: 0.0110
trigger times: 15
Loss after 546290100 batches: 0.0110
trigger times: 16
Loss after 546421200 batches: 0.0106
trigger times: 17
Loss after 546552300 batches: 0.0107
trigger times: 18
Loss after 546683400 batches: 0.0105
trigger times: 19
Loss after 546814500 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 546945600 batches: 0.0103
Time to train on one home:  209.37475275993347
trigger times: 0
Loss after 547048200 batches: 0.1686
trigger times: 0
Loss after 547150800 batches: 0.0562
trigger times: 1
Loss after 547253400 batches: 0.0417
trigger times: 2
Loss after 547356000 batches: 0.0340
trigger times: 0
Loss after 547458600 batches: 0.0324
trigger times: 1
Loss after 547561200 batches: 0.0313
trigger times: 0
Loss after 547663800 batches: 0.0275
trigger times: 1
Loss after 547766400 batches: 0.0263
trigger times: 2
Loss after 547869000 batches: 0.0263
trigger times: 3
Loss after 547971600 batches: 0.0259
trigger times: 4
Loss after 548074200 batches: 0.0234
trigger times: 5
Loss after 548176800 batches: 0.0230
trigger times: 6
Loss after 548279400 batches: 0.0250
trigger times: 7
Loss after 548382000 batches: 0.0233
trigger times: 0
Loss after 548484600 batches: 0.0213
trigger times: 1
Loss after 548587200 batches: 0.0223
trigger times: 2
Loss after 548689800 batches: 0.0249
trigger times: 3
Loss after 548792400 batches: 0.0247
trigger times: 4
Loss after 548895000 batches: 0.0220
trigger times: 5
Loss after 548997600 batches: 0.0198
trigger times: 6
Loss after 549100200 batches: 0.0196
trigger times: 7
Loss after 549202800 batches: 0.0209
trigger times: 8
Loss after 549305400 batches: 0.0201
trigger times: 9
Loss after 549408000 batches: 0.0204
trigger times: 10
Loss after 549510600 batches: 0.0193
trigger times: 11
Loss after 549613200 batches: 0.0188
trigger times: 12
Loss after 549715800 batches: 0.0190
trigger times: 13
Loss after 549818400 batches: 0.0181
trigger times: 14
Loss after 549921000 batches: 0.0182
trigger times: 15
Loss after 550023600 batches: 0.0183
trigger times: 16
Loss after 550126200 batches: 0.0186
trigger times: 17
Loss after 550228800 batches: 0.0179
trigger times: 18
Loss after 550331400 batches: 0.0181
trigger times: 19
Loss after 550434000 batches: 0.0193
trigger times: 20
Early stopping!
Start to test process.
Loss after 550536600 batches: 0.0186
Time to train on one home:  210.33035898208618
trigger times: 0
Loss after 550667700 batches: 0.1044
trigger times: 0
Loss after 550798800 batches: 0.0383
trigger times: 1
Loss after 550929900 batches: 0.0296
trigger times: 2
Loss after 551061000 batches: 0.0259
trigger times: 3
Loss after 551192100 batches: 0.0236
trigger times: 4
Loss after 551323200 batches: 0.0224
trigger times: 5
Loss after 551454300 batches: 0.0213
trigger times: 6
Loss after 551585400 batches: 0.0205
trigger times: 7
Loss after 551716500 batches: 0.0197
trigger times: 8
Loss after 551847600 batches: 0.0194
trigger times: 9
Loss after 551978700 batches: 0.0191
trigger times: 10
Loss after 552109800 batches: 0.0184
trigger times: 11
Loss after 552240900 batches: 0.0178
trigger times: 12
Loss after 552372000 batches: 0.0176
trigger times: 13
Loss after 552503100 batches: 0.0174
trigger times: 14
Loss after 552634200 batches: 0.0172
trigger times: 15
Loss after 552765300 batches: 0.0167
trigger times: 16
Loss after 552896400 batches: 0.0167
trigger times: 17
Loss after 553027500 batches: 0.0166
trigger times: 18
Loss after 553158600 batches: 0.0162
trigger times: 19
Loss after 553289700 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 553420800 batches: 0.0159
Time to train on one home:  167.23365688323975
trigger times: 0
Loss after 553551900 batches: 0.1840
trigger times: 0
Loss after 553683000 batches: 0.0534
trigger times: 1
Loss after 553814100 batches: 0.0393
trigger times: 2
Loss after 553945200 batches: 0.0342
trigger times: 3
Loss after 554076300 batches: 0.0318
trigger times: 0
Loss after 554207400 batches: 0.0295
trigger times: 0
Loss after 554338500 batches: 0.0284
trigger times: 0
Loss after 554469600 batches: 0.0273
trigger times: 0
Loss after 554600700 batches: 0.0266
trigger times: 1
Loss after 554731800 batches: 0.0257
trigger times: 2
Loss after 554862900 batches: 0.0250
trigger times: 3
Loss after 554994000 batches: 0.0245
trigger times: 0
Loss after 555125100 batches: 0.0238
trigger times: 1
Loss after 555256200 batches: 0.0231
trigger times: 2
Loss after 555387300 batches: 0.0229
trigger times: 3
Loss after 555518400 batches: 0.0225
trigger times: 4
Loss after 555649500 batches: 0.0224
trigger times: 5
Loss after 555780600 batches: 0.0219
trigger times: 6
Loss after 555911700 batches: 0.0219
trigger times: 7
Loss after 556042800 batches: 0.0215
trigger times: 8
Loss after 556173900 batches: 0.0214
trigger times: 9
Loss after 556305000 batches: 0.0210
trigger times: 10
Loss after 556436100 batches: 0.0209
trigger times: 11
Loss after 556567200 batches: 0.0207
trigger times: 12
Loss after 556698300 batches: 0.0205
trigger times: 13
Loss after 556829400 batches: 0.0203
trigger times: 14
Loss after 556960500 batches: 0.0203
trigger times: 15
Loss after 557091600 batches: 0.0199
trigger times: 16
Loss after 557222700 batches: 0.0200
trigger times: 17
Loss after 557353800 batches: 0.0195
trigger times: 18
Loss after 557484900 batches: 0.0195
trigger times: 19
Loss after 557616000 batches: 0.0199
trigger times: 20
Early stopping!
Start to test process.
Loss after 557747100 batches: 0.0197
Time to train on one home:  245.1152365207672
trigger times: 0
Loss after 557875740 batches: 0.0989
trigger times: 1
Loss after 558004380 batches: 0.0310
trigger times: 2
Loss after 558133020 batches: 0.0239
trigger times: 0
Loss after 558261660 batches: 0.0214
trigger times: 1
Loss after 558390300 batches: 0.0198
trigger times: 2
Loss after 558518940 batches: 0.0187
trigger times: 3
Loss after 558647580 batches: 0.0177
trigger times: 4
Loss after 558776220 batches: 0.0168
trigger times: 5
Loss after 558904860 batches: 0.0167
trigger times: 6
Loss after 559033500 batches: 0.0158
trigger times: 7
Loss after 559162140 batches: 0.0152
trigger times: 8
Loss after 559290780 batches: 0.0149
trigger times: 9
Loss after 559419420 batches: 0.0148
trigger times: 10
Loss after 559548060 batches: 0.0144
trigger times: 11
Loss after 559676700 batches: 0.0143
trigger times: 12
Loss after 559805340 batches: 0.0140
trigger times: 13
Loss after 559933980 batches: 0.0137
trigger times: 14
Loss after 560062620 batches: 0.0137
trigger times: 15
Loss after 560191260 batches: 0.0136
trigger times: 16
Loss after 560319900 batches: 0.0134
trigger times: 17
Loss after 560448540 batches: 0.0132
trigger times: 18
Loss after 560577180 batches: 0.0128
trigger times: 19
Loss after 560705820 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 560834460 batches: 0.0129
Time to train on one home:  178.45976901054382
trigger times: 0
Loss after 560965560 batches: 0.2058
trigger times: 0
Loss after 561096660 batches: 0.0531
trigger times: 0
Loss after 561227760 batches: 0.0401
trigger times: 0
Loss after 561358860 batches: 0.0348
trigger times: 0
Loss after 561489960 batches: 0.0314
trigger times: 1
Loss after 561621060 batches: 0.0298
trigger times: 2
Loss after 561752160 batches: 0.0286
trigger times: 3
Loss after 561883260 batches: 0.0272
trigger times: 4
Loss after 562014360 batches: 0.0264
trigger times: 5
Loss after 562145460 batches: 0.0255
trigger times: 0
Loss after 562276560 batches: 0.0252
trigger times: 1
Loss after 562407660 batches: 0.0244
trigger times: 2
Loss after 562538760 batches: 0.0236
trigger times: 3
Loss after 562669860 batches: 0.0235
trigger times: 4
Loss after 562800960 batches: 0.0229
trigger times: 5
Loss after 562932060 batches: 0.0226
trigger times: 6
Loss after 563063160 batches: 0.0223
trigger times: 0
Loss after 563194260 batches: 0.0221
trigger times: 1
Loss after 563325360 batches: 0.0219
trigger times: 2
Loss after 563456460 batches: 0.0214
trigger times: 3
Loss after 563587560 batches: 0.0214
trigger times: 4
Loss after 563718660 batches: 0.0211
trigger times: 5
Loss after 563849760 batches: 0.0207
trigger times: 6
Loss after 563980860 batches: 0.0207
trigger times: 7
Loss after 564111960 batches: 0.0203
trigger times: 8
Loss after 564243060 batches: 0.0204
trigger times: 9
Loss after 564374160 batches: 0.0203
trigger times: 10
Loss after 564505260 batches: 0.0197
trigger times: 11
Loss after 564636360 batches: 0.0197
trigger times: 12
Loss after 564767460 batches: 0.0194
trigger times: 13
Loss after 564898560 batches: 0.0191
trigger times: 14
Loss after 565029660 batches: 0.0197
trigger times: 15
Loss after 565160760 batches: 0.0192
trigger times: 16
Loss after 565291860 batches: 0.0189
trigger times: 17
Loss after 565422960 batches: 0.0189
trigger times: 18
Loss after 565554060 batches: 0.0190
trigger times: 19
Loss after 565685160 batches: 0.0187
trigger times: 20
Early stopping!
Start to test process.
Loss after 565816260 batches: 0.0187
Time to train on one home:  280.19139194488525
trigger times: 0
Loss after 565947360 batches: 0.2102
trigger times: 0
Loss after 566078460 batches: 0.0745
trigger times: 1
Loss after 566209560 batches: 0.0536
trigger times: 0
Loss after 566340660 batches: 0.0413
trigger times: 1
Loss after 566471760 batches: 0.0387
trigger times: 0
Loss after 566602860 batches: 0.0378
trigger times: 1
Loss after 566733960 batches: 0.0345
trigger times: 0
Loss after 566865060 batches: 0.0330
trigger times: 1
Loss after 566996160 batches: 0.0313
trigger times: 2
Loss after 567127260 batches: 0.0294
trigger times: 3
Loss after 567258360 batches: 0.0289
trigger times: 4
Loss after 567389460 batches: 0.0289
trigger times: 5
Loss after 567520560 batches: 0.0284
trigger times: 6
Loss after 567651660 batches: 0.0277
trigger times: 7
Loss after 567782760 batches: 0.0279
trigger times: 8
Loss after 567913860 batches: 0.0266
trigger times: 0
Loss after 568044960 batches: 0.0264
trigger times: 1
Loss after 568176060 batches: 0.0260
trigger times: 2
Loss after 568307160 batches: 0.0255
trigger times: 3
Loss after 568438260 batches: 0.0254
trigger times: 4
Loss after 568569360 batches: 0.0250
trigger times: 0
Loss after 568700460 batches: 0.0251
trigger times: 1
Loss after 568831560 batches: 0.0257
trigger times: 2
Loss after 568962660 batches: 0.0249
trigger times: 3
Loss after 569093760 batches: 0.0245
trigger times: 4
Loss after 569224860 batches: 0.0243
trigger times: 5
Loss after 569355960 batches: 0.0238
trigger times: 6
Loss after 569487060 batches: 0.0250
trigger times: 7
Loss after 569618160 batches: 0.0240
trigger times: 8
Loss after 569749260 batches: 0.0249
trigger times: 9
Loss after 569880360 batches: 0.0229
trigger times: 10
Loss after 570011460 batches: 0.0227
trigger times: 11
Loss after 570142560 batches: 0.0218
trigger times: 12
Loss after 570273660 batches: 0.0227
trigger times: 13
Loss after 570404760 batches: 0.0224
trigger times: 14
Loss after 570535860 batches: 0.0228
trigger times: 15
Loss after 570666960 batches: 0.0219
trigger times: 16
Loss after 570798060 batches: 0.0218
trigger times: 17
Loss after 570929160 batches: 0.0225
trigger times: 18
Loss after 571060260 batches: 0.0228
trigger times: 19
Loss after 571191360 batches: 0.0219
trigger times: 20
Early stopping!
Start to test process.
Loss after 571322460 batches: 0.0221
Time to train on one home:  308.42801427841187
trigger times: 0
Loss after 571453560 batches: 0.0691
trigger times: 0
Loss after 571584660 batches: 0.0234
trigger times: 1
Loss after 571715760 batches: 0.0178
trigger times: 2
Loss after 571846860 batches: 0.0157
trigger times: 3
Loss after 571977960 batches: 0.0144
trigger times: 4
Loss after 572109060 batches: 0.0134
trigger times: 5
Loss after 572240160 batches: 0.0128
trigger times: 6
Loss after 572371260 batches: 0.0124
trigger times: 7
Loss after 572502360 batches: 0.0120
trigger times: 8
Loss after 572633460 batches: 0.0116
trigger times: 9
Loss after 572764560 batches: 0.0113
trigger times: 10
Loss after 572895660 batches: 0.0109
trigger times: 11
Loss after 573026760 batches: 0.0106
trigger times: 12
Loss after 573157860 batches: 0.0106
trigger times: 13
Loss after 573288960 batches: 0.0104
trigger times: 14
Loss after 573420060 batches: 0.0101
trigger times: 15
Loss after 573551160 batches: 0.0100
trigger times: 16
Loss after 573682260 batches: 0.0099
trigger times: 17
Loss after 573813360 batches: 0.0099
trigger times: 18
Loss after 573944460 batches: 0.0096
trigger times: 19
Loss after 574075560 batches: 0.0094
trigger times: 20
Early stopping!
Start to test process.
Loss after 574206660 batches: 0.0107
Time to train on one home:  166.9401559829712
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378, 0.015884290067137628, 0.015422517538360537, 0.015618174097677863, 0.015689947067699456, 0.016115858282786194]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61], [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846], [0.5624003973272111, 0.39199562780285213, 0.46338177904139005, 1.0500325960715429, 0.49807389089844895, 24.807396438856706, 1537.5851], [0.5526623295413123, 0.4025342703450545, 0.4773154352778254, 1.041540162760643, 0.4894406919679644, 24.60675965799653, 1510.9338], [0.5434306197696261, 0.41257883262882467, 0.4890240237330659, 1.0321400196734463, 0.48121224091099224, 24.384677908326562, 1485.5321], [0.5378838711314731, 0.41862279133969893, 0.4914352831936747, 1.0233749172087403, 0.4762610626477895, 24.177599220976607, 1470.2474]]
Round_12_results:  [0.5378838711314731, 0.41862279133969893, 0.4914352831936747, 1.0233749172087403, 0.4762610626477895, 24.177599220976607, 1470.2474]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4528 < 4529; dropping {'Training_Loss': 0.09605764738231334, 'Validation_Loss': 0.1936262837714619, 'Training_R2': 0.9033127289710662, 'Validation_R2': 0.8200456414607552, 'Training_F1': 0.8346144600563984, 'Validation_F1': 0.7600688029390307, 'Training_NEP': 0.330460607685547, 'Validation_NEP': 0.4582494841733704, 'Training_NDE': 0.07258485056314101, 'Validation_NDE': 0.14330616500688473, 'Training_MAE': 10.944199952489997, 'Validation_MAE': 12.56710201668483, 'Training_MSE': 319.36197, 'Validation_MSE': 529.2257}.
trigger times: 0
Loss after 574337760 batches: 0.0961
trigger times: 0
Loss after 574468860 batches: 0.0265
trigger times: 0
Loss after 574599960 batches: 0.0205
trigger times: 1
Loss after 574731060 batches: 0.0186
trigger times: 2
Loss after 574862160 batches: 0.0161
trigger times: 0
Loss after 574993260 batches: 0.0156
trigger times: 0
Loss after 575124360 batches: 0.0148
trigger times: 0
Loss after 575255460 batches: 0.0139
trigger times: 1
Loss after 575386560 batches: 0.0137
trigger times: 0
Loss after 575517660 batches: 0.0132
trigger times: 1
Loss after 575648760 batches: 0.0129
trigger times: 2
Loss after 575779860 batches: 0.0124
trigger times: 3
Loss after 575910960 batches: 0.0123
trigger times: 4
Loss after 576042060 batches: 0.0121
trigger times: 5
Loss after 576173160 batches: 0.0119
trigger times: 6
Loss after 576304260 batches: 0.0115
trigger times: 7
Loss after 576435360 batches: 0.0114
trigger times: 8
Loss after 576566460 batches: 0.0110
trigger times: 9
Loss after 576697560 batches: 0.0109
trigger times: 10
Loss after 576828660 batches: 0.0110
trigger times: 11
Loss after 576959760 batches: 0.0108
trigger times: 12
Loss after 577090860 batches: 0.0109
trigger times: 13
Loss after 577221960 batches: 0.0107
trigger times: 14
Loss after 577353060 batches: 0.0104
trigger times: 15
Loss after 577484160 batches: 0.0105
trigger times: 16
Loss after 577615260 batches: 0.0102
trigger times: 17
Loss after 577746360 batches: 0.0102
trigger times: 18
Loss after 577877460 batches: 0.0099
trigger times: 19
Loss after 578008560 batches: 0.0098
trigger times: 20
Early stopping!
Start to test process.
Loss after 578139660 batches: 0.0096
Time to train on one home:  223.5186059474945
trigger times: 0
Loss after 578242260 batches: 0.1687
trigger times: 1
Loss after 578344860 batches: 0.0574
trigger times: 0
Loss after 578447460 batches: 0.0412
trigger times: 1
Loss after 578550060 batches: 0.0342
trigger times: 0
Loss after 578652660 batches: 0.0334
trigger times: 1
Loss after 578755260 batches: 0.0287
trigger times: 2
Loss after 578857860 batches: 0.0299
trigger times: 3
Loss after 578960460 batches: 0.0257
trigger times: 0
Loss after 579063060 batches: 0.0256
trigger times: 1
Loss after 579165660 batches: 0.0250
trigger times: 2
Loss after 579268260 batches: 0.0235
trigger times: 3
Loss after 579370860 batches: 0.0226
trigger times: 4
Loss after 579473460 batches: 0.0219
trigger times: 5
Loss after 579576060 batches: 0.0215
trigger times: 6
Loss after 579678660 batches: 0.0228
trigger times: 7
Loss after 579781260 batches: 0.0253
trigger times: 8
Loss after 579883860 batches: 0.0228
trigger times: 9
Loss after 579986460 batches: 0.0239
trigger times: 10
Loss after 580089060 batches: 0.0231
trigger times: 11
Loss after 580191660 batches: 0.0210
trigger times: 12
Loss after 580294260 batches: 0.0211
trigger times: 13
Loss after 580396860 batches: 0.0204
trigger times: 14
Loss after 580499460 batches: 0.0191
trigger times: 0
Loss after 580602060 batches: 0.0207
trigger times: 0
Loss after 580704660 batches: 0.0185
trigger times: 0
Loss after 580807260 batches: 0.0188
trigger times: 0
Loss after 580909860 batches: 0.0214
trigger times: 1
Loss after 581012460 batches: 0.0215
trigger times: 2
Loss after 581115060 batches: 0.0185
trigger times: 0
Loss after 581217660 batches: 0.0196
trigger times: 1
Loss after 581320260 batches: 0.0206
trigger times: 2
Loss after 581422860 batches: 0.0196
trigger times: 3
Loss after 581525460 batches: 0.0189
trigger times: 4
Loss after 581628060 batches: 0.0197
trigger times: 5
Loss after 581730660 batches: 0.0185
trigger times: 6
Loss after 581833260 batches: 0.0168
trigger times: 7
Loss after 581935860 batches: 0.0167
trigger times: 8
Loss after 582038460 batches: 0.0166
trigger times: 9
Loss after 582141060 batches: 0.0165
trigger times: 10
Loss after 582243660 batches: 0.0177
trigger times: 11
Loss after 582346260 batches: 0.0179
trigger times: 12
Loss after 582448860 batches: 0.0176
trigger times: 13
Loss after 582551460 batches: 0.0168
trigger times: 14
Loss after 582654060 batches: 0.0168
trigger times: 15
Loss after 582756660 batches: 0.0172
trigger times: 16
Loss after 582859260 batches: 0.0175
trigger times: 17
Loss after 582961860 batches: 0.0166
trigger times: 18
Loss after 583064460 batches: 0.0156
trigger times: 19
Loss after 583167060 batches: 0.0162
trigger times: 20
Early stopping!
Start to test process.
Loss after 583269660 batches: 0.0160
Time to train on one home:  294.9601125717163
trigger times: 0
Loss after 583400760 batches: 0.1036
trigger times: 0
Loss after 583531860 batches: 0.0371
trigger times: 1
Loss after 583662960 batches: 0.0284
trigger times: 2
Loss after 583794060 batches: 0.0257
trigger times: 3
Loss after 583925160 batches: 0.0237
trigger times: 4
Loss after 584056260 batches: 0.0223
trigger times: 5
Loss after 584187360 batches: 0.0213
trigger times: 6
Loss after 584318460 batches: 0.0205
trigger times: 7
Loss after 584449560 batches: 0.0198
trigger times: 8
Loss after 584580660 batches: 0.0194
trigger times: 9
Loss after 584711760 batches: 0.0185
trigger times: 10
Loss after 584842860 batches: 0.0183
trigger times: 11
Loss after 584973960 batches: 0.0181
trigger times: 12
Loss after 585105060 batches: 0.0174
trigger times: 13
Loss after 585236160 batches: 0.0173
trigger times: 14
Loss after 585367260 batches: 0.0169
trigger times: 15
Loss after 585498360 batches: 0.0167
trigger times: 16
Loss after 585629460 batches: 0.0166
trigger times: 17
Loss after 585760560 batches: 0.0165
trigger times: 18
Loss after 585891660 batches: 0.0162
trigger times: 19
Loss after 586022760 batches: 0.0157
trigger times: 20
Early stopping!
Start to test process.
Loss after 586153860 batches: 0.0158
Time to train on one home:  167.3617250919342
trigger times: 0
Loss after 586284960 batches: 0.1666
trigger times: 0
Loss after 586416060 batches: 0.0501
trigger times: 0
Loss after 586547160 batches: 0.0386
trigger times: 0
Loss after 586678260 batches: 0.0334
trigger times: 0
Loss after 586809360 batches: 0.0308
trigger times: 1
Loss after 586940460 batches: 0.0293
trigger times: 2
Loss after 587071560 batches: 0.0280
trigger times: 0
Loss after 587202660 batches: 0.0270
trigger times: 1
Loss after 587333760 batches: 0.0259
trigger times: 2
Loss after 587464860 batches: 0.0252
trigger times: 0
Loss after 587595960 batches: 0.0248
trigger times: 1
Loss after 587727060 batches: 0.0241
trigger times: 2
Loss after 587858160 batches: 0.0240
trigger times: 0
Loss after 587989260 batches: 0.0233
trigger times: 1
Loss after 588120360 batches: 0.0229
trigger times: 2
Loss after 588251460 batches: 0.0226
trigger times: 3
Loss after 588382560 batches: 0.0224
trigger times: 4
Loss after 588513660 batches: 0.0218
trigger times: 5
Loss after 588644760 batches: 0.0217
trigger times: 6
Loss after 588775860 batches: 0.0217
trigger times: 7
Loss after 588906960 batches: 0.0214
trigger times: 8
Loss after 589038060 batches: 0.0212
trigger times: 9
Loss after 589169160 batches: 0.0210
trigger times: 10
Loss after 589300260 batches: 0.0205
trigger times: 11
Loss after 589431360 batches: 0.0203
trigger times: 0
Loss after 589562460 batches: 0.0203
trigger times: 1
Loss after 589693560 batches: 0.0200
trigger times: 2
Loss after 589824660 batches: 0.0200
trigger times: 3
Loss after 589955760 batches: 0.0196
trigger times: 4
Loss after 590086860 batches: 0.0196
trigger times: 5
Loss after 590217960 batches: 0.0194
trigger times: 6
Loss after 590349060 batches: 0.0192
trigger times: 7
Loss after 590480160 batches: 0.0193
trigger times: 0
Loss after 590611260 batches: 0.0193
trigger times: 1
Loss after 590742360 batches: 0.0189
trigger times: 2
Loss after 590873460 batches: 0.0190
trigger times: 0
Loss after 591004560 batches: 0.0190
trigger times: 1
Loss after 591135660 batches: 0.0186
trigger times: 2
Loss after 591266760 batches: 0.0184
trigger times: 3
Loss after 591397860 batches: 0.0184
trigger times: 4
Loss after 591528960 batches: 0.0189
trigger times: 5
Loss after 591660060 batches: 0.0183
trigger times: 6
Loss after 591791160 batches: 0.0182
trigger times: 7
Loss after 591922260 batches: 0.0180
trigger times: 8
Loss after 592053360 batches: 0.0183
trigger times: 9
Loss after 592184460 batches: 0.0180
trigger times: 10
Loss after 592315560 batches: 0.0178
trigger times: 11
Loss after 592446660 batches: 0.0181
trigger times: 12
Loss after 592577760 batches: 0.0178
trigger times: 13
Loss after 592708860 batches: 0.0177
trigger times: 14
Loss after 592839960 batches: 0.0178
trigger times: 0
Loss after 592971060 batches: 0.0180
trigger times: 1
Loss after 593102160 batches: 0.0173
trigger times: 2
Loss after 593233260 batches: 0.0175
trigger times: 3
Loss after 593364360 batches: 0.0174
trigger times: 4
Loss after 593495460 batches: 0.0173
trigger times: 5
Loss after 593626560 batches: 0.0171
trigger times: 6
Loss after 593757660 batches: 0.0170
trigger times: 7
Loss after 593888760 batches: 0.0170
trigger times: 8
Loss after 594019860 batches: 0.0167
trigger times: 9
Loss after 594150960 batches: 0.0167
trigger times: 10
Loss after 594282060 batches: 0.0167
trigger times: 11
Loss after 594413160 batches: 0.0167
trigger times: 12
Loss after 594544260 batches: 0.0166
trigger times: 13
Loss after 594675360 batches: 0.0167
trigger times: 14
Loss after 594806460 batches: 0.0167
trigger times: 15
Loss after 594937560 batches: 0.0168
trigger times: 16
Loss after 595068660 batches: 0.0167
trigger times: 17
Loss after 595199760 batches: 0.0164
trigger times: 18
Loss after 595330860 batches: 0.0163
trigger times: 19
Loss after 595461960 batches: 0.0164
trigger times: 20
Early stopping!
Start to test process.
Loss after 595593060 batches: 0.0164
Time to train on one home:  520.3947207927704
trigger times: 0
Loss after 595721700 batches: 0.0964
trigger times: 0
Loss after 595850340 batches: 0.0295
trigger times: 0
Loss after 595978980 batches: 0.0229
trigger times: 1
Loss after 596107620 batches: 0.0205
trigger times: 2
Loss after 596236260 batches: 0.0190
trigger times: 0
Loss after 596364900 batches: 0.0180
trigger times: 0
Loss after 596493540 batches: 0.0174
trigger times: 1
Loss after 596622180 batches: 0.0166
trigger times: 2
Loss after 596750820 batches: 0.0160
trigger times: 3
Loss after 596879460 batches: 0.0155
trigger times: 4
Loss after 597008100 batches: 0.0151
trigger times: 5
Loss after 597136740 batches: 0.0148
trigger times: 6
Loss after 597265380 batches: 0.0144
trigger times: 7
Loss after 597394020 batches: 0.0143
trigger times: 8
Loss after 597522660 batches: 0.0138
trigger times: 9
Loss after 597651300 batches: 0.0138
trigger times: 10
Loss after 597779940 batches: 0.0135
trigger times: 11
Loss after 597908580 batches: 0.0133
trigger times: 12
Loss after 598037220 batches: 0.0130
trigger times: 13
Loss after 598165860 batches: 0.0130
trigger times: 14
Loss after 598294500 batches: 0.0131
trigger times: 15
Loss after 598423140 batches: 0.0128
trigger times: 16
Loss after 598551780 batches: 0.0128
trigger times: 17
Loss after 598680420 batches: 0.0123
trigger times: 18
Loss after 598809060 batches: 0.0123
trigger times: 19
Loss after 598937700 batches: 0.0122
trigger times: 20
Early stopping!
Start to test process.
Loss after 599066340 batches: 0.0124
Time to train on one home:  199.40300011634827
trigger times: 0
Loss after 599197440 batches: 0.1753
trigger times: 0
Loss after 599328540 batches: 0.0480
trigger times: 1
Loss after 599459640 batches: 0.0367
trigger times: 2
Loss after 599590740 batches: 0.0327
trigger times: 0
Loss after 599721840 batches: 0.0298
trigger times: 1
Loss after 599852940 batches: 0.0284
trigger times: 0
Loss after 599984040 batches: 0.0269
trigger times: 1
Loss after 600115140 batches: 0.0261
trigger times: 2
Loss after 600246240 batches: 0.0251
trigger times: 3
Loss after 600377340 batches: 0.0248
trigger times: 4
Loss after 600508440 batches: 0.0243
trigger times: 5
Loss after 600639540 batches: 0.0236
trigger times: 6
Loss after 600770640 batches: 0.0231
trigger times: 7
Loss after 600901740 batches: 0.0230
trigger times: 8
Loss after 601032840 batches: 0.0220
trigger times: 9
Loss after 601163940 batches: 0.0221
trigger times: 10
Loss after 601295040 batches: 0.0220
trigger times: 11
Loss after 601426140 batches: 0.0214
trigger times: 12
Loss after 601557240 batches: 0.0212
trigger times: 13
Loss after 601688340 batches: 0.0210
trigger times: 14
Loss after 601819440 batches: 0.0211
trigger times: 15
Loss after 601950540 batches: 0.0203
trigger times: 16
Loss after 602081640 batches: 0.0203
trigger times: 17
Loss after 602212740 batches: 0.0201
trigger times: 18
Loss after 602343840 batches: 0.0200
trigger times: 19
Loss after 602474940 batches: 0.0201
trigger times: 20
Early stopping!
Start to test process.
Loss after 602606040 batches: 0.0196
Time to train on one home:  202.5556890964508
trigger times: 0
Loss after 602737140 batches: 0.1985
trigger times: 0
Loss after 602868240 batches: 0.0713
trigger times: 0
Loss after 602999340 batches: 0.0499
trigger times: 0
Loss after 603130440 batches: 0.0397
trigger times: 0
Loss after 603261540 batches: 0.0365
trigger times: 1
Loss after 603392640 batches: 0.0342
trigger times: 2
Loss after 603523740 batches: 0.0327
trigger times: 3
Loss after 603654840 batches: 0.0317
trigger times: 4
Loss after 603785940 batches: 0.0301
trigger times: 0
Loss after 603917040 batches: 0.0302
trigger times: 1
Loss after 604048140 batches: 0.0297
trigger times: 2
Loss after 604179240 batches: 0.0280
trigger times: 3
Loss after 604310340 batches: 0.0275
trigger times: 0
Loss after 604441440 batches: 0.0271
trigger times: 1
Loss after 604572540 batches: 0.0263
trigger times: 2
Loss after 604703640 batches: 0.0267
trigger times: 3
Loss after 604834740 batches: 0.0269
trigger times: 4
Loss after 604965840 batches: 0.0256
trigger times: 5
Loss after 605096940 batches: 0.0261
trigger times: 0
Loss after 605228040 batches: 0.0245
trigger times: 0
Loss after 605359140 batches: 0.0249
trigger times: 1
Loss after 605490240 batches: 0.0255
trigger times: 2
Loss after 605621340 batches: 0.0246
trigger times: 0
Loss after 605752440 batches: 0.0240
trigger times: 1
Loss after 605883540 batches: 0.0239
trigger times: 2
Loss after 606014640 batches: 0.0232
trigger times: 3
Loss after 606145740 batches: 0.0233
trigger times: 4
Loss after 606276840 batches: 0.0239
trigger times: 5
Loss after 606407940 batches: 0.0227
trigger times: 6
Loss after 606539040 batches: 0.0225
trigger times: 7
Loss after 606670140 batches: 0.0230
trigger times: 8
Loss after 606801240 batches: 0.0233
trigger times: 9
Loss after 606932340 batches: 0.0224
trigger times: 10
Loss after 607063440 batches: 0.0226
trigger times: 11
Loss after 607194540 batches: 0.0226
trigger times: 12
Loss after 607325640 batches: 0.0226
trigger times: 13
Loss after 607456740 batches: 0.0223
trigger times: 14
Loss after 607587840 batches: 0.0225
trigger times: 15
Loss after 607718940 batches: 0.0208
trigger times: 16
Loss after 607850040 batches: 0.0218
trigger times: 17
Loss after 607981140 batches: 0.0210
trigger times: 18
Loss after 608112240 batches: 0.0206
trigger times: 19
Loss after 608243340 batches: 0.0212
trigger times: 20
Early stopping!
Start to test process.
Loss after 608374440 batches: 0.0211
Time to train on one home:  322.7274706363678
trigger times: 0
Loss after 608505540 batches: 0.0647
trigger times: 0
Loss after 608636640 batches: 0.0224
trigger times: 0
Loss after 608767740 batches: 0.0175
trigger times: 1
Loss after 608898840 batches: 0.0154
trigger times: 2
Loss after 609029940 batches: 0.0139
trigger times: 3
Loss after 609161040 batches: 0.0132
trigger times: 4
Loss after 609292140 batches: 0.0127
trigger times: 5
Loss after 609423240 batches: 0.0122
trigger times: 6
Loss after 609554340 batches: 0.0117
trigger times: 7
Loss after 609685440 batches: 0.0112
trigger times: 8
Loss after 609816540 batches: 0.0112
trigger times: 9
Loss after 609947640 batches: 0.0108
trigger times: 10
Loss after 610078740 batches: 0.0106
trigger times: 11
Loss after 610209840 batches: 0.0104
trigger times: 12
Loss after 610340940 batches: 0.0103
trigger times: 13
Loss after 610472040 batches: 0.0097
trigger times: 14
Loss after 610603140 batches: 0.0097
trigger times: 15
Loss after 610734240 batches: 0.0098
trigger times: 16
Loss after 610865340 batches: 0.0094
trigger times: 17
Loss after 610996440 batches: 0.0092
trigger times: 18
Loss after 611127540 batches: 0.0091
trigger times: 19
Loss after 611258640 batches: 0.0091
trigger times: 20
Early stopping!
Start to test process.
Loss after 611389740 batches: 0.0092
Time to train on one home:  174.37602019309998
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378, 0.015884290067137628, 0.015422517538360537, 0.015618174097677863, 0.015689947067699456, 0.016115858282786194, 0.015032807630775873]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61], [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846], [0.5624003973272111, 0.39199562780285213, 0.46338177904139005, 1.0500325960715429, 0.49807389089844895, 24.807396438856706, 1537.5851], [0.5526623295413123, 0.4025342703450545, 0.4773154352778254, 1.041540162760643, 0.4894406919679644, 24.60675965799653, 1510.9338], [0.5434306197696261, 0.41257883262882467, 0.4890240237330659, 1.0321400196734463, 0.48121224091099224, 24.384677908326562, 1485.5321], [0.5378838711314731, 0.41862279133969893, 0.4914352831936747, 1.0233749172087403, 0.4762610626477895, 24.177599220976607, 1470.2474], [0.5388939446873136, 0.4175359601598919, 0.4899945807831285, 1.0201219390637546, 0.47715138886784625, 24.100746446356553, 1472.9958]]
Round_13_results:  [0.5388939446873136, 0.4175359601598919, 0.4899945807831285, 1.0201219390637546, 0.47715138886784625, 24.100746446356553, 1472.9958]
trigger times: 0
Loss after 611520840 batches: 0.1071
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4823 < 4824; dropping {'Training_Loss': 0.10712848547494637, 'Validation_Loss': 0.1973495160539945, 'Training_R2': 0.8920856859192653, 'Validation_R2': 0.8165170983581616, 'Training_F1': 0.8246744489103245, 'Validation_F1': 0.766680796875374, 'Training_NEP': 0.35014529177930126, 'Validation_NEP': 0.45930070730843525, 'Training_NDE': 0.08101319106245078, 'Validation_NDE': 0.14611611072978273, 'Training_MAE': 11.596117650736932, 'Validation_MAE': 12.595930916305939, 'Training_MSE': 356.4454, 'Validation_MSE': 539.6028}.
trigger times: 0
Loss after 611651940 batches: 0.0263
trigger times: 0
Loss after 611783040 batches: 0.0196
trigger times: 0
Loss after 611914140 batches: 0.0176
trigger times: 0
Loss after 612045240 batches: 0.0163
trigger times: 0
Loss after 612176340 batches: 0.0150
trigger times: 1
Loss after 612307440 batches: 0.0143
trigger times: 2
Loss after 612438540 batches: 0.0136
trigger times: 3
Loss after 612569640 batches: 0.0133
trigger times: 4
Loss after 612700740 batches: 0.0131
trigger times: 5
Loss after 612831840 batches: 0.0125
trigger times: 6
Loss after 612962940 batches: 0.0124
trigger times: 7
Loss after 613094040 batches: 0.0121
trigger times: 8
Loss after 613225140 batches: 0.0119
trigger times: 9
Loss after 613356240 batches: 0.0115
trigger times: 10
Loss after 613487340 batches: 0.0114
trigger times: 11
Loss after 613618440 batches: 0.0115
trigger times: 12
Loss after 613749540 batches: 0.0110
trigger times: 13
Loss after 613880640 batches: 0.0109
trigger times: 14
Loss after 614011740 batches: 0.0108
trigger times: 15
Loss after 614142840 batches: 0.0107
trigger times: 16
Loss after 614273940 batches: 0.0105
trigger times: 17
Loss after 614405040 batches: 0.0102
trigger times: 18
Loss after 614536140 batches: 0.0104
trigger times: 19
Loss after 614667240 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 614798340 batches: 0.0100
Time to train on one home:  195.3110795021057
trigger times: 0
Loss after 614900940 batches: 0.1556
trigger times: 0
Loss after 615003540 batches: 0.0598
trigger times: 1
Loss after 615106140 batches: 0.0397
trigger times: 2
Loss after 615208740 batches: 0.0317
trigger times: 3
Loss after 615311340 batches: 0.0290
trigger times: 0
Loss after 615413940 batches: 0.0292
trigger times: 1
Loss after 615516540 batches: 0.0292
trigger times: 0
Loss after 615619140 batches: 0.0270
trigger times: 0
Loss after 615721740 batches: 0.0255
trigger times: 1
Loss after 615824340 batches: 0.0261
trigger times: 2
Loss after 615926940 batches: 0.0255
trigger times: 3
Loss after 616029540 batches: 0.0247
trigger times: 4
Loss after 616132140 batches: 0.0230
trigger times: 5
Loss after 616234740 batches: 0.0214
trigger times: 6
Loss after 616337340 batches: 0.0210
trigger times: 7
Loss after 616439940 batches: 0.0203
trigger times: 8
Loss after 616542540 batches: 0.0198
trigger times: 9
Loss after 616645140 batches: 0.0209
trigger times: 10
Loss after 616747740 batches: 0.0192
trigger times: 11
Loss after 616850340 batches: 0.0197
trigger times: 0
Loss after 616952940 batches: 0.0197
trigger times: 0
Loss after 617055540 batches: 0.0198
trigger times: 1
Loss after 617158140 batches: 0.0215
trigger times: 2
Loss after 617260740 batches: 0.0200
trigger times: 3
Loss after 617363340 batches: 0.0196
trigger times: 4
Loss after 617465940 batches: 0.0189
trigger times: 5
Loss after 617568540 batches: 0.0177
trigger times: 6
Loss after 617671140 batches: 0.0174
trigger times: 7
Loss after 617773740 batches: 0.0179
trigger times: 8
Loss after 617876340 batches: 0.0172
trigger times: 9
Loss after 617978940 batches: 0.0178
trigger times: 10
Loss after 618081540 batches: 0.0178
trigger times: 11
Loss after 618184140 batches: 0.0177
trigger times: 12
Loss after 618286740 batches: 0.0168
trigger times: 13
Loss after 618389340 batches: 0.0167
trigger times: 14
Loss after 618491940 batches: 0.0167
trigger times: 15
Loss after 618594540 batches: 0.0167
trigger times: 16
Loss after 618697140 batches: 0.0175
trigger times: 17
Loss after 618799740 batches: 0.0173
trigger times: 18
Loss after 618902340 batches: 0.0180
trigger times: 19
Loss after 619004940 batches: 0.0171
trigger times: 0
Loss after 619107540 batches: 0.0180
trigger times: 1
Loss after 619210140 batches: 0.0170
trigger times: 2
Loss after 619312740 batches: 0.0167
trigger times: 3
Loss after 619415340 batches: 0.0159
trigger times: 4
Loss after 619517940 batches: 0.0158
trigger times: 0
Loss after 619620540 batches: 0.0172
trigger times: 1
Loss after 619723140 batches: 0.0165
trigger times: 2
Loss after 619825740 batches: 0.0162
trigger times: 3
Loss after 619928340 batches: 0.0167
trigger times: 4
Loss after 620030940 batches: 0.0167
trigger times: 5
Loss after 620133540 batches: 0.0168
trigger times: 6
Loss after 620236140 batches: 0.0168
trigger times: 7
Loss after 620338740 batches: 0.0168
trigger times: 8
Loss after 620441340 batches: 0.0164
trigger times: 9
Loss after 620543940 batches: 0.0154
trigger times: 10
Loss after 620646540 batches: 0.0154
trigger times: 11
Loss after 620749140 batches: 0.0155
trigger times: 12
Loss after 620851740 batches: 0.0151
trigger times: 13
Loss after 620954340 batches: 0.0161
trigger times: 14
Loss after 621056940 batches: 0.0156
trigger times: 15
Loss after 621159540 batches: 0.0168
trigger times: 16
Loss after 621262140 batches: 0.0154
trigger times: 17
Loss after 621364740 batches: 0.0146
trigger times: 18
Loss after 621467340 batches: 0.0146
trigger times: 19
Loss after 621569940 batches: 0.0147
trigger times: 20
Early stopping!
Start to test process.
Loss after 621672540 batches: 0.0151
Time to train on one home:  391.88566732406616
trigger times: 0
Loss after 621803640 batches: 0.1056
trigger times: 0
Loss after 621934740 batches: 0.0368
trigger times: 0
Loss after 622065840 batches: 0.0285
trigger times: 1
Loss after 622196940 batches: 0.0252
trigger times: 2
Loss after 622328040 batches: 0.0231
trigger times: 3
Loss after 622459140 batches: 0.0220
trigger times: 4
Loss after 622590240 batches: 0.0212
trigger times: 5
Loss after 622721340 batches: 0.0203
trigger times: 6
Loss after 622852440 batches: 0.0197
trigger times: 7
Loss after 622983540 batches: 0.0189
trigger times: 8
Loss after 623114640 batches: 0.0189
trigger times: 0
Loss after 623245740 batches: 0.0180
trigger times: 1
Loss after 623376840 batches: 0.0176
trigger times: 2
Loss after 623507940 batches: 0.0175
trigger times: 3
Loss after 623639040 batches: 0.0171
trigger times: 4
Loss after 623770140 batches: 0.0168
trigger times: 5
Loss after 623901240 batches: 0.0168
trigger times: 6
Loss after 624032340 batches: 0.0166
trigger times: 7
Loss after 624163440 batches: 0.0162
trigger times: 8
Loss after 624294540 batches: 0.0159
trigger times: 9
Loss after 624425640 batches: 0.0157
trigger times: 10
Loss after 624556740 batches: 0.0157
trigger times: 11
Loss after 624687840 batches: 0.0155
trigger times: 12
Loss after 624818940 batches: 0.0156
trigger times: 13
Loss after 624950040 batches: 0.0150
trigger times: 14
Loss after 625081140 batches: 0.0148
trigger times: 15
Loss after 625212240 batches: 0.0148
trigger times: 16
Loss after 625343340 batches: 0.0147
trigger times: 17
Loss after 625474440 batches: 0.0147
trigger times: 18
Loss after 625605540 batches: 0.0146
trigger times: 19
Loss after 625736640 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 625867740 batches: 0.0141
Time to train on one home:  237.51400136947632
trigger times: 0
Loss after 625998840 batches: 0.1787
trigger times: 0
Loss after 626129940 batches: 0.0492
trigger times: 0
Loss after 626261040 batches: 0.0365
trigger times: 0
Loss after 626392140 batches: 0.0326
trigger times: 0
Loss after 626523240 batches: 0.0299
trigger times: 0
Loss after 626654340 batches: 0.0285
trigger times: 1
Loss after 626785440 batches: 0.0270
trigger times: 2
Loss after 626916540 batches: 0.0257
trigger times: 0
Loss after 627047640 batches: 0.0254
trigger times: 1
Loss after 627178740 batches: 0.0244
trigger times: 2
Loss after 627309840 batches: 0.0234
trigger times: 0
Loss after 627440940 batches: 0.0236
trigger times: 1
Loss after 627572040 batches: 0.0230
trigger times: 2
Loss after 627703140 batches: 0.0225
trigger times: 3
Loss after 627834240 batches: 0.0219
trigger times: 4
Loss after 627965340 batches: 0.0216
trigger times: 5
Loss after 628096440 batches: 0.0214
trigger times: 6
Loss after 628227540 batches: 0.0209
trigger times: 7
Loss after 628358640 batches: 0.0209
trigger times: 8
Loss after 628489740 batches: 0.0203
trigger times: 9
Loss after 628620840 batches: 0.0203
trigger times: 10
Loss after 628751940 batches: 0.0202
trigger times: 11
Loss after 628883040 batches: 0.0204
trigger times: 0
Loss after 629014140 batches: 0.0201
trigger times: 1
Loss after 629145240 batches: 0.0198
trigger times: 2
Loss after 629276340 batches: 0.0195
trigger times: 3
Loss after 629407440 batches: 0.0196
trigger times: 4
Loss after 629538540 batches: 0.0193
trigger times: 5
Loss after 629669640 batches: 0.0191
trigger times: 6
Loss after 629800740 batches: 0.0189
trigger times: 7
Loss after 629931840 batches: 0.0190
trigger times: 8
Loss after 630062940 batches: 0.0190
trigger times: 9
Loss after 630194040 batches: 0.0188
trigger times: 0
Loss after 630325140 batches: 0.0185
trigger times: 1
Loss after 630456240 batches: 0.0187
trigger times: 2
Loss after 630587340 batches: 0.0181
trigger times: 3
Loss after 630718440 batches: 0.0182
trigger times: 4
Loss after 630849540 batches: 0.0183
trigger times: 5
Loss after 630980640 batches: 0.0185
trigger times: 6
Loss after 631111740 batches: 0.0180
trigger times: 7
Loss after 631242840 batches: 0.0180
trigger times: 0
Loss after 631373940 batches: 0.0179
trigger times: 1
Loss after 631505040 batches: 0.0177
trigger times: 2
Loss after 631636140 batches: 0.0178
trigger times: 0
Loss after 631767240 batches: 0.0173
trigger times: 1
Loss after 631898340 batches: 0.0176
trigger times: 0
Loss after 632029440 batches: 0.0174
trigger times: 1
Loss after 632160540 batches: 0.0173
trigger times: 2
Loss after 632291640 batches: 0.0171
trigger times: 3
Loss after 632422740 batches: 0.0171
trigger times: 4
Loss after 632553840 batches: 0.0172
trigger times: 5
Loss after 632684940 batches: 0.0170
trigger times: 6
Loss after 632816040 batches: 0.0171
trigger times: 7
Loss after 632947140 batches: 0.0168
trigger times: 8
Loss after 633078240 batches: 0.0167
trigger times: 9
Loss after 633209340 batches: 0.0168
trigger times: 10
Loss after 633340440 batches: 0.0170
trigger times: 11
Loss after 633471540 batches: 0.0169
trigger times: 12
Loss after 633602640 batches: 0.0168
trigger times: 13
Loss after 633733740 batches: 0.0167
trigger times: 14
Loss after 633864840 batches: 0.0167
trigger times: 15
Loss after 633995940 batches: 0.0167
trigger times: 16
Loss after 634127040 batches: 0.0164
trigger times: 17
Loss after 634258140 batches: 0.0164
trigger times: 18
Loss after 634389240 batches: 0.0165
trigger times: 19
Loss after 634520340 batches: 0.0162
trigger times: 20
Early stopping!
Start to test process.
Loss after 634651440 batches: 0.0162
Time to train on one home:  485.12742710113525
trigger times: 0
Loss after 634780080 batches: 0.0948
trigger times: 0
Loss after 634908720 batches: 0.0289
trigger times: 0
Loss after 635037360 batches: 0.0224
trigger times: 1
Loss after 635166000 batches: 0.0200
trigger times: 0
Loss after 635294640 batches: 0.0185
trigger times: 1
Loss after 635423280 batches: 0.0178
trigger times: 2
Loss after 635551920 batches: 0.0168
trigger times: 3
Loss after 635680560 batches: 0.0163
trigger times: 4
Loss after 635809200 batches: 0.0159
trigger times: 5
Loss after 635937840 batches: 0.0153
trigger times: 6
Loss after 636066480 batches: 0.0149
trigger times: 7
Loss after 636195120 batches: 0.0147
trigger times: 8
Loss after 636323760 batches: 0.0144
trigger times: 9
Loss after 636452400 batches: 0.0140
trigger times: 10
Loss after 636581040 batches: 0.0138
trigger times: 11
Loss after 636709680 batches: 0.0135
trigger times: 12
Loss after 636838320 batches: 0.0135
trigger times: 13
Loss after 636966960 batches: 0.0132
trigger times: 14
Loss after 637095600 batches: 0.0129
trigger times: 15
Loss after 637224240 batches: 0.0128
trigger times: 16
Loss after 637352880 batches: 0.0128
trigger times: 17
Loss after 637481520 batches: 0.0127
trigger times: 18
Loss after 637610160 batches: 0.0125
trigger times: 19
Loss after 637738800 batches: 0.0122
trigger times: 20
Early stopping!
Start to test process.
Loss after 637867440 batches: 0.0121
Time to train on one home:  185.41464114189148
trigger times: 0
Loss after 637998540 batches: 0.1806
trigger times: 0
Loss after 638129640 batches: 0.0474
trigger times: 1
Loss after 638260740 batches: 0.0368
trigger times: 0
Loss after 638391840 batches: 0.0325
trigger times: 1
Loss after 638522940 batches: 0.0301
trigger times: 2
Loss after 638654040 batches: 0.0285
trigger times: 3
Loss after 638785140 batches: 0.0271
trigger times: 0
Loss after 638916240 batches: 0.0262
trigger times: 0
Loss after 639047340 batches: 0.0253
trigger times: 1
Loss after 639178440 batches: 0.0245
trigger times: 2
Loss after 639309540 batches: 0.0242
trigger times: 3
Loss after 639440640 batches: 0.0236
trigger times: 4
Loss after 639571740 batches: 0.0233
trigger times: 5
Loss after 639702840 batches: 0.0232
trigger times: 6
Loss after 639833940 batches: 0.0226
trigger times: 7
Loss after 639965040 batches: 0.0220
trigger times: 8
Loss after 640096140 batches: 0.0217
trigger times: 9
Loss after 640227240 batches: 0.0217
trigger times: 10
Loss after 640358340 batches: 0.0212
trigger times: 0
Loss after 640489440 batches: 0.0210
trigger times: 1
Loss after 640620540 batches: 0.0208
trigger times: 2
Loss after 640751640 batches: 0.0207
trigger times: 3
Loss after 640882740 batches: 0.0201
trigger times: 4
Loss after 641013840 batches: 0.0200
trigger times: 5
Loss after 641144940 batches: 0.0197
trigger times: 0
Loss after 641276040 batches: 0.0200
trigger times: 0
Loss after 641407140 batches: 0.0196
trigger times: 1
Loss after 641538240 batches: 0.0196
trigger times: 2
Loss after 641669340 batches: 0.0193
trigger times: 3
Loss after 641800440 batches: 0.0193
trigger times: 4
Loss after 641931540 batches: 0.0193
trigger times: 5
Loss after 642062640 batches: 0.0191
trigger times: 6
Loss after 642193740 batches: 0.0189
trigger times: 7
Loss after 642324840 batches: 0.0188
trigger times: 8
Loss after 642455940 batches: 0.0188
trigger times: 0
Loss after 642587040 batches: 0.0185
trigger times: 1
Loss after 642718140 batches: 0.0183
trigger times: 0
Loss after 642849240 batches: 0.0184
trigger times: 1
Loss after 642980340 batches: 0.0180
trigger times: 2
Loss after 643111440 batches: 0.0179
trigger times: 3
Loss after 643242540 batches: 0.0179
trigger times: 4
Loss after 643373640 batches: 0.0179
trigger times: 5
Loss after 643504740 batches: 0.0179
trigger times: 6
Loss after 643635840 batches: 0.0176
trigger times: 7
Loss after 643766940 batches: 0.0175
trigger times: 8
Loss after 643898040 batches: 0.0175
trigger times: 9
Loss after 644029140 batches: 0.0177
trigger times: 10
Loss after 644160240 batches: 0.0176
trigger times: 0
Loss after 644291340 batches: 0.0175
trigger times: 1
Loss after 644422440 batches: 0.0173
trigger times: 2
Loss after 644553540 batches: 0.0173
trigger times: 3
Loss after 644684640 batches: 0.0173
trigger times: 4
Loss after 644815740 batches: 0.0171
trigger times: 5
Loss after 644946840 batches: 0.0170
trigger times: 6
Loss after 645077940 batches: 0.0166
trigger times: 7
Loss after 645209040 batches: 0.0166
trigger times: 8
Loss after 645340140 batches: 0.0166
trigger times: 9
Loss after 645471240 batches: 0.0167
trigger times: 10
Loss after 645602340 batches: 0.0167
trigger times: 11
Loss after 645733440 batches: 0.0164
trigger times: 12
Loss after 645864540 batches: 0.0166
trigger times: 13
Loss after 645995640 batches: 0.0165
trigger times: 14
Loss after 646126740 batches: 0.0163
trigger times: 15
Loss after 646257840 batches: 0.0164
trigger times: 16
Loss after 646388940 batches: 0.0162
trigger times: 17
Loss after 646520040 batches: 0.0161
trigger times: 18
Loss after 646651140 batches: 0.0161
trigger times: 19
Loss after 646782240 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 646913340 batches: 0.0162
Time to train on one home:  500.25055623054504
trigger times: 0
Loss after 647044440 batches: 0.1999
trigger times: 0
Loss after 647175540 batches: 0.0682
trigger times: 0
Loss after 647306640 batches: 0.0486
trigger times: 1
Loss after 647437740 batches: 0.0393
trigger times: 2
Loss after 647568840 batches: 0.0351
trigger times: 0
Loss after 647699940 batches: 0.0337
trigger times: 1
Loss after 647831040 batches: 0.0314
trigger times: 0
Loss after 647962140 batches: 0.0316
trigger times: 1
Loss after 648093240 batches: 0.0289
trigger times: 2
Loss after 648224340 batches: 0.0284
trigger times: 3
Loss after 648355440 batches: 0.0281
trigger times: 4
Loss after 648486540 batches: 0.0268
trigger times: 5
Loss after 648617640 batches: 0.0290
trigger times: 6
Loss after 648748740 batches: 0.0273
trigger times: 7
Loss after 648879840 batches: 0.0271
trigger times: 8
Loss after 649010940 batches: 0.0260
trigger times: 9
Loss after 649142040 batches: 0.0260
trigger times: 10
Loss after 649273140 batches: 0.0246
trigger times: 11
Loss after 649404240 batches: 0.0256
trigger times: 12
Loss after 649535340 batches: 0.0256
trigger times: 0
Loss after 649666440 batches: 0.0243
trigger times: 1
Loss after 649797540 batches: 0.0240
trigger times: 2
Loss after 649928640 batches: 0.0234
trigger times: 0
Loss after 650059740 batches: 0.0235
trigger times: 1
Loss after 650190840 batches: 0.0230
trigger times: 2
Loss after 650321940 batches: 0.0233
trigger times: 3
Loss after 650453040 batches: 0.0236
trigger times: 0
Loss after 650584140 batches: 0.0223
trigger times: 1
Loss after 650715240 batches: 0.0227
trigger times: 2
Loss after 650846340 batches: 0.0224
trigger times: 3
Loss after 650977440 batches: 0.0223
trigger times: 4
Loss after 651108540 batches: 0.0219
trigger times: 5
Loss after 651239640 batches: 0.0228
trigger times: 6
Loss after 651370740 batches: 0.0220
trigger times: 7
Loss after 651501840 batches: 0.0215
trigger times: 8
Loss after 651632940 batches: 0.0226
trigger times: 9
Loss after 651764040 batches: 0.0213
trigger times: 0
Loss after 651895140 batches: 0.0214
trigger times: 1
Loss after 652026240 batches: 0.0211
trigger times: 2
Loss after 652157340 batches: 0.0206
trigger times: 3
Loss after 652288440 batches: 0.0215
trigger times: 0
Loss after 652419540 batches: 0.0208
trigger times: 1
Loss after 652550640 batches: 0.0210
trigger times: 2
Loss after 652681740 batches: 0.0209
trigger times: 3
Loss after 652812840 batches: 0.0204
trigger times: 4
Loss after 652943940 batches: 0.0200
trigger times: 5
Loss after 653075040 batches: 0.0205
trigger times: 6
Loss after 653206140 batches: 0.0206
trigger times: 7
Loss after 653337240 batches: 0.0199
trigger times: 8
Loss after 653468340 batches: 0.0197
trigger times: 9
Loss after 653599440 batches: 0.0210
trigger times: 10
Loss after 653730540 batches: 0.0203
trigger times: 11
Loss after 653861640 batches: 0.0202
trigger times: 12
Loss after 653992740 batches: 0.0197
trigger times: 13
Loss after 654123840 batches: 0.0192
trigger times: 14
Loss after 654254940 batches: 0.0203
trigger times: 0
Loss after 654386040 batches: 0.0198
trigger times: 1
Loss after 654517140 batches: 0.0201
trigger times: 2
Loss after 654648240 batches: 0.0196
trigger times: 3
Loss after 654779340 batches: 0.0190
trigger times: 0
Loss after 654910440 batches: 0.0189
trigger times: 1
Loss after 655041540 batches: 0.0209
trigger times: 2
Loss after 655172640 batches: 0.0195
trigger times: 3
Loss after 655303740 batches: 0.0180
trigger times: 4
Loss after 655434840 batches: 0.0190
trigger times: 5
Loss after 655565940 batches: 0.0186
trigger times: 6
Loss after 655697040 batches: 0.0190
trigger times: 7
Loss after 655828140 batches: 0.0189
trigger times: 8
Loss after 655959240 batches: 0.0188
trigger times: 9
Loss after 656090340 batches: 0.0188
trigger times: 10
Loss after 656221440 batches: 0.0184
trigger times: 11
Loss after 656352540 batches: 0.0183
trigger times: 12
Loss after 656483640 batches: 0.0181
trigger times: 13
Loss after 656614740 batches: 0.0180
trigger times: 14
Loss after 656745840 batches: 0.0178
trigger times: 15
Loss after 656876940 batches: 0.0187
trigger times: 16
Loss after 657008040 batches: 0.0186
trigger times: 17
Loss after 657139140 batches: 0.0193
trigger times: 18
Loss after 657270240 batches: 0.0187
trigger times: 19
Loss after 657401340 batches: 0.0183
trigger times: 20
Early stopping!
Start to test process.
Loss after 657532440 batches: 0.0186
Time to train on one home:  584.7706782817841
trigger times: 0
Loss after 657663540 batches: 0.0630
trigger times: 0
Loss after 657794640 batches: 0.0215
trigger times: 0
Loss after 657925740 batches: 0.0169
trigger times: 1
Loss after 658056840 batches: 0.0146
trigger times: 2
Loss after 658187940 batches: 0.0135
trigger times: 3
Loss after 658319040 batches: 0.0129
trigger times: 4
Loss after 658450140 batches: 0.0120
trigger times: 5
Loss after 658581240 batches: 0.0115
trigger times: 6
Loss after 658712340 batches: 0.0112
trigger times: 7
Loss after 658843440 batches: 0.0110
trigger times: 8
Loss after 658974540 batches: 0.0107
trigger times: 9
Loss after 659105640 batches: 0.0106
trigger times: 10
Loss after 659236740 batches: 0.0104
trigger times: 11
Loss after 659367840 batches: 0.0101
trigger times: 12
Loss after 659498940 batches: 0.0098
trigger times: 13
Loss after 659630040 batches: 0.0097
trigger times: 14
Loss after 659761140 batches: 0.0096
trigger times: 15
Loss after 659892240 batches: 0.0094
trigger times: 16
Loss after 660023340 batches: 0.0093
trigger times: 17
Loss after 660154440 batches: 0.0093
trigger times: 18
Loss after 660285540 batches: 0.0092
trigger times: 19
Loss after 660416640 batches: 0.0087
trigger times: 20
Early stopping!
Start to test process.
Loss after 660547740 batches: 0.0088
Time to train on one home:  174.04480171203613
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378, 0.015884290067137628, 0.015422517538360537, 0.015618174097677863, 0.015689947067699456, 0.016115858282786194, 0.015032807630775873, 0.013889531067539576]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61], [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846], [0.5624003973272111, 0.39199562780285213, 0.46338177904139005, 1.0500325960715429, 0.49807389089844895, 24.807396438856706, 1537.5851], [0.5526623295413123, 0.4025342703450545, 0.4773154352778254, 1.041540162760643, 0.4894406919679644, 24.60675965799653, 1510.9338], [0.5434306197696261, 0.41257883262882467, 0.4890240237330659, 1.0321400196734463, 0.48121224091099224, 24.384677908326562, 1485.5321], [0.5378838711314731, 0.41862279133969893, 0.4914352831936747, 1.0233749172087403, 0.4762610626477895, 24.177599220976607, 1470.2474], [0.5388939446873136, 0.4175359601598919, 0.4899945807831285, 1.0201219390637546, 0.47715138886784625, 24.100746446356553, 1472.9958], [0.5538486010498471, 0.40130114966802066, 0.4733353493101595, 1.0370228287424743, 0.49045085775236186, 24.500036022697298, 1514.0524]]
Round_14_results:  [0.5538486010498471, 0.40130114966802066, 0.4733353493101595, 1.0370228287424743, 0.49045085775236186, 24.500036022697298, 1514.0524]
trigger times: 0
Loss after 660678840 batches: 0.1030
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5213 < 5214; dropping {'Training_Loss': 0.10296618495628519, 'Validation_Loss': 0.20729091266791025, 'Training_R2': 0.8963024712808035, 'Validation_R2': 0.8073231611695301, 'Training_F1': 0.8302260184521298, 'Validation_F1': 0.7634631413658329, 'Training_NEP': 0.339081435866753, 'Validation_NEP': 0.4643781879294882, 'Training_NDE': 0.07784757544348789, 'Validation_NDE': 0.1534376776565966, 'Training_MAE': 11.229704684905657, 'Validation_MAE': 12.735176500111924, 'Training_MSE': 342.5171, 'Validation_MSE': 566.6411}.
trigger times: 0
Loss after 660809940 batches: 0.0254
trigger times: 1
Loss after 660941040 batches: 0.0199
trigger times: 0
Loss after 661072140 batches: 0.0171
trigger times: 1
Loss after 661203240 batches: 0.0156
trigger times: 2
Loss after 661334340 batches: 0.0147
trigger times: 3
Loss after 661465440 batches: 0.0140
trigger times: 4
Loss after 661596540 batches: 0.0136
trigger times: 5
Loss after 661727640 batches: 0.0133
trigger times: 6
Loss after 661858740 batches: 0.0130
trigger times: 7
Loss after 661989840 batches: 0.0126
trigger times: 8
Loss after 662120940 batches: 0.0123
trigger times: 9
Loss after 662252040 batches: 0.0117
trigger times: 10
Loss after 662383140 batches: 0.0118
trigger times: 11
Loss after 662514240 batches: 0.0116
trigger times: 12
Loss after 662645340 batches: 0.0113
trigger times: 13
Loss after 662776440 batches: 0.0117
trigger times: 14
Loss after 662907540 batches: 0.0108
trigger times: 15
Loss after 663038640 batches: 0.0110
trigger times: 16
Loss after 663169740 batches: 0.0105
trigger times: 17
Loss after 663300840 batches: 0.0106
trigger times: 18
Loss after 663431940 batches: 0.0106
trigger times: 19
Loss after 663563040 batches: 0.0104
trigger times: 20
Early stopping!
Start to test process.
Loss after 663694140 batches: 0.0103
Time to train on one home:  181.4904191493988
trigger times: 0
Loss after 663796740 batches: 0.1579
trigger times: 0
Loss after 663899340 batches: 0.0556
trigger times: 0
Loss after 664001940 batches: 0.0425
trigger times: 0
Loss after 664104540 batches: 0.0324
trigger times: 1
Loss after 664207140 batches: 0.0312
trigger times: 2
Loss after 664309740 batches: 0.0293
trigger times: 3
Loss after 664412340 batches: 0.0253
trigger times: 4
Loss after 664514940 batches: 0.0243
trigger times: 5
Loss after 664617540 batches: 0.0240
trigger times: 6
Loss after 664720140 batches: 0.0239
trigger times: 7
Loss after 664822740 batches: 0.0222
trigger times: 8
Loss after 664925340 batches: 0.0211
trigger times: 9
Loss after 665027940 batches: 0.0212
trigger times: 0
Loss after 665130540 batches: 0.0208
trigger times: 1
Loss after 665233140 batches: 0.0201
trigger times: 2
Loss after 665335740 batches: 0.0202
trigger times: 3
Loss after 665438340 batches: 0.0195
trigger times: 0
Loss after 665540940 batches: 0.0187
trigger times: 0
Loss after 665643540 batches: 0.0189
trigger times: 1
Loss after 665746140 batches: 0.0204
trigger times: 0
Loss after 665848740 batches: 0.0262
trigger times: 1
Loss after 665951340 batches: 0.0199
trigger times: 2
Loss after 666053940 batches: 0.0185
trigger times: 3
Loss after 666156540 batches: 0.0194
trigger times: 4
Loss after 666259140 batches: 0.0187
trigger times: 5
Loss after 666361740 batches: 0.0190
trigger times: 6
Loss after 666464340 batches: 0.0184
trigger times: 7
Loss after 666566940 batches: 0.0172
trigger times: 8
Loss after 666669540 batches: 0.0169
trigger times: 9
Loss after 666772140 batches: 0.0175
trigger times: 10
Loss after 666874740 batches: 0.0177
trigger times: 11
Loss after 666977340 batches: 0.0188
trigger times: 12
Loss after 667079940 batches: 0.0173
trigger times: 13
Loss after 667182540 batches: 0.0182
trigger times: 14
Loss after 667285140 batches: 0.0181
trigger times: 15
Loss after 667387740 batches: 0.0179
trigger times: 16
Loss after 667490340 batches: 0.0187
trigger times: 17
Loss after 667592940 batches: 0.0187
trigger times: 0
Loss after 667695540 batches: 0.0171
trigger times: 1
Loss after 667798140 batches: 0.0166
trigger times: 0
Loss after 667900740 batches: 0.0194
trigger times: 1
Loss after 668003340 batches: 0.0175
trigger times: 2
Loss after 668105940 batches: 0.0165
trigger times: 3
Loss after 668208540 batches: 0.0178
trigger times: 4
Loss after 668311140 batches: 0.0171
trigger times: 5
Loss after 668413740 batches: 0.0169
trigger times: 6
Loss after 668516340 batches: 0.0163
trigger times: 7
Loss after 668618940 batches: 0.0158
trigger times: 8
Loss after 668721540 batches: 0.0154
trigger times: 9
Loss after 668824140 batches: 0.0152
trigger times: 10
Loss after 668926740 batches: 0.0155
trigger times: 11
Loss after 669029340 batches: 0.0158
trigger times: 12
Loss after 669131940 batches: 0.0163
trigger times: 13
Loss after 669234540 batches: 0.0153
trigger times: 14
Loss after 669337140 batches: 0.0149
trigger times: 15
Loss after 669439740 batches: 0.0153
trigger times: 16
Loss after 669542340 batches: 0.0155
trigger times: 17
Loss after 669644940 batches: 0.0151
trigger times: 18
Loss after 669747540 batches: 0.0146
trigger times: 19
Loss after 669850140 batches: 0.0157
trigger times: 20
Early stopping!
Start to test process.
Loss after 669952740 batches: 0.0171
Time to train on one home:  357.63879799842834
trigger times: 0
Loss after 670083840 batches: 0.1017
trigger times: 0
Loss after 670214940 batches: 0.0362
trigger times: 0
Loss after 670346040 batches: 0.0279
trigger times: 1
Loss after 670477140 batches: 0.0252
trigger times: 2
Loss after 670608240 batches: 0.0231
trigger times: 3
Loss after 670739340 batches: 0.0219
trigger times: 4
Loss after 670870440 batches: 0.0208
trigger times: 0
Loss after 671001540 batches: 0.0199
trigger times: 1
Loss after 671132640 batches: 0.0191
trigger times: 0
Loss after 671263740 batches: 0.0186
trigger times: 1
Loss after 671394840 batches: 0.0181
trigger times: 2
Loss after 671525940 batches: 0.0179
trigger times: 3
Loss after 671657040 batches: 0.0174
trigger times: 4
Loss after 671788140 batches: 0.0172
trigger times: 5
Loss after 671919240 batches: 0.0170
trigger times: 6
Loss after 672050340 batches: 0.0164
trigger times: 7
Loss after 672181440 batches: 0.0161
trigger times: 8
Loss after 672312540 batches: 0.0162
trigger times: 9
Loss after 672443640 batches: 0.0158
trigger times: 10
Loss after 672574740 batches: 0.0157
trigger times: 11
Loss after 672705840 batches: 0.0154
trigger times: 12
Loss after 672836940 batches: 0.0154
trigger times: 13
Loss after 672968040 batches: 0.0149
trigger times: 14
Loss after 673099140 batches: 0.0149
trigger times: 15
Loss after 673230240 batches: 0.0148
trigger times: 16
Loss after 673361340 batches: 0.0148
trigger times: 17
Loss after 673492440 batches: 0.0145
trigger times: 18
Loss after 673623540 batches: 0.0144
trigger times: 19
Loss after 673754640 batches: 0.0142
trigger times: 20
Early stopping!
Start to test process.
Loss after 673885740 batches: 0.0141
Time to train on one home:  223.86661791801453
trigger times: 0
Loss after 674016840 batches: 0.1683
trigger times: 0
Loss after 674147940 batches: 0.0462
trigger times: 0
Loss after 674279040 batches: 0.0354
trigger times: 1
Loss after 674410140 batches: 0.0310
trigger times: 2
Loss after 674541240 batches: 0.0287
trigger times: 0
Loss after 674672340 batches: 0.0271
trigger times: 1
Loss after 674803440 batches: 0.0260
trigger times: 2
Loss after 674934540 batches: 0.0247
trigger times: 3
Loss after 675065640 batches: 0.0240
trigger times: 4
Loss after 675196740 batches: 0.0234
trigger times: 5
Loss after 675327840 batches: 0.0227
trigger times: 0
Loss after 675458940 batches: 0.0225
trigger times: 1
Loss after 675590040 batches: 0.0224
trigger times: 2
Loss after 675721140 batches: 0.0219
trigger times: 3
Loss after 675852240 batches: 0.0213
trigger times: 4
Loss after 675983340 batches: 0.0212
trigger times: 5
Loss after 676114440 batches: 0.0208
trigger times: 6
Loss after 676245540 batches: 0.0203
trigger times: 7
Loss after 676376640 batches: 0.0203
trigger times: 8
Loss after 676507740 batches: 0.0202
trigger times: 9
Loss after 676638840 batches: 0.0197
trigger times: 0
Loss after 676769940 batches: 0.0197
trigger times: 1
Loss after 676901040 batches: 0.0197
trigger times: 2
Loss after 677032140 batches: 0.0191
trigger times: 3
Loss after 677163240 batches: 0.0191
trigger times: 4
Loss after 677294340 batches: 0.0190
trigger times: 5
Loss after 677425440 batches: 0.0187
trigger times: 6
Loss after 677556540 batches: 0.0189
trigger times: 7
Loss after 677687640 batches: 0.0186
trigger times: 8
Loss after 677818740 batches: 0.0183
trigger times: 9
Loss after 677949840 batches: 0.0184
trigger times: 10
Loss after 678080940 batches: 0.0184
trigger times: 11
Loss after 678212040 batches: 0.0183
trigger times: 12
Loss after 678343140 batches: 0.0184
trigger times: 13
Loss after 678474240 batches: 0.0182
trigger times: 14
Loss after 678605340 batches: 0.0181
trigger times: 15
Loss after 678736440 batches: 0.0181
trigger times: 16
Loss after 678867540 batches: 0.0179
trigger times: 17
Loss after 678998640 batches: 0.0177
trigger times: 18
Loss after 679129740 batches: 0.0178
trigger times: 19
Loss after 679260840 batches: 0.0176
trigger times: 20
Early stopping!
Start to test process.
Loss after 679391940 batches: 0.0172
Time to train on one home:  308.21205496788025
trigger times: 0
Loss after 679520580 batches: 0.0952
trigger times: 0
Loss after 679649220 batches: 0.0286
trigger times: 0
Loss after 679777860 batches: 0.0225
trigger times: 1
Loss after 679906500 batches: 0.0201
trigger times: 2
Loss after 680035140 batches: 0.0185
trigger times: 3
Loss after 680163780 batches: 0.0174
trigger times: 4
Loss after 680292420 batches: 0.0168
trigger times: 5
Loss after 680421060 batches: 0.0160
trigger times: 6
Loss after 680549700 batches: 0.0155
trigger times: 7
Loss after 680678340 batches: 0.0148
trigger times: 8
Loss after 680806980 batches: 0.0146
trigger times: 9
Loss after 680935620 batches: 0.0144
trigger times: 10
Loss after 681064260 batches: 0.0139
trigger times: 11
Loss after 681192900 batches: 0.0138
trigger times: 12
Loss after 681321540 batches: 0.0137
trigger times: 13
Loss after 681450180 batches: 0.0136
trigger times: 14
Loss after 681578820 batches: 0.0134
trigger times: 15
Loss after 681707460 batches: 0.0132
trigger times: 16
Loss after 681836100 batches: 0.0127
trigger times: 17
Loss after 681964740 batches: 0.0128
trigger times: 18
Loss after 682093380 batches: 0.0125
trigger times: 19
Loss after 682222020 batches: 0.0126
trigger times: 20
Early stopping!
Start to test process.
Loss after 682350660 batches: 0.0122
Time to train on one home:  171.84682250022888
trigger times: 0
Loss after 682481760 batches: 0.1807
trigger times: 0
Loss after 682612860 batches: 0.0455
trigger times: 0
Loss after 682743960 batches: 0.0348
trigger times: 1
Loss after 682875060 batches: 0.0312
trigger times: 0
Loss after 683006160 batches: 0.0289
trigger times: 0
Loss after 683137260 batches: 0.0273
trigger times: 0
Loss after 683268360 batches: 0.0263
trigger times: 1
Loss after 683399460 batches: 0.0251
trigger times: 2
Loss after 683530560 batches: 0.0242
trigger times: 3
Loss after 683661660 batches: 0.0238
trigger times: 4
Loss after 683792760 batches: 0.0232
trigger times: 5
Loss after 683923860 batches: 0.0227
trigger times: 6
Loss after 684054960 batches: 0.0222
trigger times: 7
Loss after 684186060 batches: 0.0220
trigger times: 8
Loss after 684317160 batches: 0.0214
trigger times: 9
Loss after 684448260 batches: 0.0211
trigger times: 10
Loss after 684579360 batches: 0.0209
trigger times: 11
Loss after 684710460 batches: 0.0205
trigger times: 12
Loss after 684841560 batches: 0.0201
trigger times: 13
Loss after 684972660 batches: 0.0202
trigger times: 14
Loss after 685103760 batches: 0.0200
trigger times: 15
Loss after 685234860 batches: 0.0199
trigger times: 16
Loss after 685365960 batches: 0.0195
trigger times: 17
Loss after 685497060 batches: 0.0196
trigger times: 18
Loss after 685628160 batches: 0.0193
trigger times: 19
Loss after 685759260 batches: 0.0193
trigger times: 20
Early stopping!
Start to test process.
Loss after 685890360 batches: 0.0190
Time to train on one home:  202.64209461212158
trigger times: 0
Loss after 686021460 batches: 0.1767
trigger times: 1
Loss after 686152560 batches: 0.0682
trigger times: 0
Loss after 686283660 batches: 0.0477
trigger times: 1
Loss after 686414760 batches: 0.0385
trigger times: 0
Loss after 686545860 batches: 0.0357
trigger times: 1
Loss after 686676960 batches: 0.0325
trigger times: 2
Loss after 686808060 batches: 0.0308
trigger times: 3
Loss after 686939160 batches: 0.0288
trigger times: 4
Loss after 687070260 batches: 0.0288
trigger times: 5
Loss after 687201360 batches: 0.0272
trigger times: 0
Loss after 687332460 batches: 0.0267
trigger times: 0
Loss after 687463560 batches: 0.0263
trigger times: 1
Loss after 687594660 batches: 0.0263
trigger times: 0
Loss after 687725760 batches: 0.0257
trigger times: 0
Loss after 687856860 batches: 0.0254
trigger times: 1
Loss after 687987960 batches: 0.0244
trigger times: 2
Loss after 688119060 batches: 0.0240
trigger times: 3
Loss after 688250160 batches: 0.0242
trigger times: 4
Loss after 688381260 batches: 0.0245
trigger times: 5
Loss after 688512360 batches: 0.0234
trigger times: 6
Loss after 688643460 batches: 0.0234
trigger times: 7
Loss after 688774560 batches: 0.0218
trigger times: 8
Loss after 688905660 batches: 0.0220
trigger times: 0
Loss after 689036760 batches: 0.0227
trigger times: 1
Loss after 689167860 batches: 0.0228
trigger times: 2
Loss after 689298960 batches: 0.0224
trigger times: 3
Loss after 689430060 batches: 0.0228
trigger times: 4
Loss after 689561160 batches: 0.0220
trigger times: 5
Loss after 689692260 batches: 0.0208
trigger times: 6
Loss after 689823360 batches: 0.0210
trigger times: 7
Loss after 689954460 batches: 0.0213
trigger times: 8
Loss after 690085560 batches: 0.0211
trigger times: 9
Loss after 690216660 batches: 0.0211
trigger times: 10
Loss after 690347760 batches: 0.0217
trigger times: 11
Loss after 690478860 batches: 0.0212
trigger times: 12
Loss after 690609960 batches: 0.0206
trigger times: 0
Loss after 690741060 batches: 0.0198
trigger times: 1
Loss after 690872160 batches: 0.0199
trigger times: 2
Loss after 691003260 batches: 0.0201
trigger times: 0
Loss after 691134360 batches: 0.0199
trigger times: 1
Loss after 691265460 batches: 0.0203
trigger times: 2
Loss after 691396560 batches: 0.0210
trigger times: 3
Loss after 691527660 batches: 0.0199
trigger times: 4
Loss after 691658760 batches: 0.0205
trigger times: 5
Loss after 691789860 batches: 0.0216
trigger times: 6
Loss after 691920960 batches: 0.0208
trigger times: 7
Loss after 692052060 batches: 0.0194
trigger times: 8
Loss after 692183160 batches: 0.0202
trigger times: 9
Loss after 692314260 batches: 0.0191
trigger times: 0
Loss after 692445360 batches: 0.0189
trigger times: 1
Loss after 692576460 batches: 0.0192
trigger times: 2
Loss after 692707560 batches: 0.0186
trigger times: 3
Loss after 692838660 batches: 0.0198
trigger times: 4
Loss after 692969760 batches: 0.0197
trigger times: 5
Loss after 693100860 batches: 0.0187
trigger times: 6
Loss after 693231960 batches: 0.0185
trigger times: 7
Loss after 693363060 batches: 0.0183
trigger times: 8
Loss after 693494160 batches: 0.0182
trigger times: 9
Loss after 693625260 batches: 0.0184
trigger times: 10
Loss after 693756360 batches: 0.0194
trigger times: 11
Loss after 693887460 batches: 0.0187
trigger times: 12
Loss after 694018560 batches: 0.0188
trigger times: 13
Loss after 694149660 batches: 0.0184
trigger times: 14
Loss after 694280760 batches: 0.0179
trigger times: 15
Loss after 694411860 batches: 0.0176
trigger times: 16
Loss after 694542960 batches: 0.0181
trigger times: 17
Loss after 694674060 batches: 0.0184
trigger times: 18
Loss after 694805160 batches: 0.0177
trigger times: 19
Loss after 694936260 batches: 0.0177
trigger times: 20
Early stopping!
Start to test process.
Loss after 695067360 batches: 0.0172
Time to train on one home:  506.8570137023926
trigger times: 0
Loss after 695198460 batches: 0.0661
trigger times: 1
Loss after 695329560 batches: 0.0215
trigger times: 2
Loss after 695460660 batches: 0.0161
trigger times: 3
Loss after 695591760 batches: 0.0144
trigger times: 4
Loss after 695722860 batches: 0.0136
trigger times: 5
Loss after 695853960 batches: 0.0125
trigger times: 6
Loss after 695985060 batches: 0.0122
trigger times: 7
Loss after 696116160 batches: 0.0114
trigger times: 8
Loss after 696247260 batches: 0.0112
trigger times: 9
Loss after 696378360 batches: 0.0108
trigger times: 10
Loss after 696509460 batches: 0.0106
trigger times: 11
Loss after 696640560 batches: 0.0103
trigger times: 12
Loss after 696771660 batches: 0.0100
trigger times: 13
Loss after 696902760 batches: 0.0097
trigger times: 14
Loss after 697033860 batches: 0.0096
trigger times: 15
Loss after 697164960 batches: 0.0097
trigger times: 16
Loss after 697296060 batches: 0.0094
trigger times: 17
Loss after 697427160 batches: 0.0093
trigger times: 18
Loss after 697558260 batches: 0.0094
trigger times: 19
Loss after 697689360 batches: 0.0090
trigger times: 20
Early stopping!
Start to test process.
Loss after 697820460 batches: 0.0089
Time to train on one home:  160.21673893928528
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378, 0.015884290067137628, 0.015422517538360537, 0.015618174097677863, 0.015689947067699456, 0.016115858282786194, 0.015032807630775873, 0.013889531067539576, 0.014509777593618559]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61], [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846], [0.5624003973272111, 0.39199562780285213, 0.46338177904139005, 1.0500325960715429, 0.49807389089844895, 24.807396438856706, 1537.5851], [0.5526623295413123, 0.4025342703450545, 0.4773154352778254, 1.041540162760643, 0.4894406919679644, 24.60675965799653, 1510.9338], [0.5434306197696261, 0.41257883262882467, 0.4890240237330659, 1.0321400196734463, 0.48121224091099224, 24.384677908326562, 1485.5321], [0.5378838711314731, 0.41862279133969893, 0.4914352831936747, 1.0233749172087403, 0.4762610626477895, 24.177599220976607, 1470.2474], [0.5388939446873136, 0.4175359601598919, 0.4899945807831285, 1.0201219390637546, 0.47715138886784625, 24.100746446356553, 1472.9958], [0.5538486010498471, 0.40130114966802066, 0.4733353493101595, 1.0370228287424743, 0.49045085775236186, 24.500036022697298, 1514.0524], [0.5457354452874925, 0.4101143038770134, 0.47950876644385965, 1.028278416589283, 0.4832311695252878, 24.29344615137278, 1491.7646]]
Round_15_results:  [0.5457354452874925, 0.4101143038770134, 0.47950876644385965, 1.028278416589283, 0.4832311695252878, 24.29344615137278, 1491.7646]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5511 < 5512; dropping {'Training_Loss': 0.10598296960288624, 'Validation_Loss': 0.20196845879157385, 'Training_R2': 0.8931906270049277, 'Validation_R2': 0.8122711488816456, 'Training_F1': 0.8274594755170405, 'Validation_F1': 0.7768783398431717, 'Training_NEP': 0.3448739142613691, 'Validation_NEP': 0.4403806666914865, 'Training_NDE': 0.08018369217670959, 'Validation_NDE': 0.1494973610714343, 'Training_MAE': 11.421540081611942, 'Validation_MAE': 12.077064908148994, 'Training_MSE': 352.7957, 'Validation_MSE': 552.0896}.
trigger times: 0
Loss after 697951560 batches: 0.1060
trigger times: 0
Loss after 698082660 batches: 0.0251
trigger times: 1
Loss after 698213760 batches: 0.0193
trigger times: 2
Loss after 698344860 batches: 0.0169
trigger times: 0
Loss after 698475960 batches: 0.0159
trigger times: 1
Loss after 698607060 batches: 0.0146
trigger times: 2
Loss after 698738160 batches: 0.0139
trigger times: 3
Loss after 698869260 batches: 0.0134
trigger times: 4
Loss after 699000360 batches: 0.0130
trigger times: 5
Loss after 699131460 batches: 0.0130
trigger times: 6
Loss after 699262560 batches: 0.0125
trigger times: 7
Loss after 699393660 batches: 0.0124
trigger times: 8
Loss after 699524760 batches: 0.0120
trigger times: 9
Loss after 699655860 batches: 0.0116
trigger times: 10
Loss after 699786960 batches: 0.0116
trigger times: 11
Loss after 699918060 batches: 0.0110
trigger times: 12
Loss after 700049160 batches: 0.0110
trigger times: 13
Loss after 700180260 batches: 0.0109
trigger times: 14
Loss after 700311360 batches: 0.0109
trigger times: 15
Loss after 700442460 batches: 0.0105
trigger times: 16
Loss after 700573560 batches: 0.0105
trigger times: 17
Loss after 700704660 batches: 0.0104
trigger times: 18
Loss after 700835760 batches: 0.0104
trigger times: 19
Loss after 700966860 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 701097960 batches: 0.0101
Time to train on one home:  188.44726276397705
trigger times: 0
Loss after 701200560 batches: 0.1492
trigger times: 1
Loss after 701303160 batches: 0.0556
trigger times: 2
Loss after 701405760 batches: 0.0409
trigger times: 3
Loss after 701508360 batches: 0.0316
trigger times: 4
Loss after 701610960 batches: 0.0280
trigger times: 5
Loss after 701713560 batches: 0.0274
trigger times: 6
Loss after 701816160 batches: 0.0266
trigger times: 7
Loss after 701918760 batches: 0.0245
trigger times: 0
Loss after 702021360 batches: 0.0236
trigger times: 1
Loss after 702123960 batches: 0.0253
trigger times: 2
Loss after 702226560 batches: 0.0227
trigger times: 3
Loss after 702329160 batches: 0.0210
trigger times: 4
Loss after 702431760 batches: 0.0223
trigger times: 5
Loss after 702534360 batches: 0.0209
trigger times: 6
Loss after 702636960 batches: 0.0197
trigger times: 7
Loss after 702739560 batches: 0.0192
trigger times: 8
Loss after 702842160 batches: 0.0185
trigger times: 0
Loss after 702944760 batches: 0.0185
trigger times: 1
Loss after 703047360 batches: 0.0187
trigger times: 0
Loss after 703149960 batches: 0.0197
trigger times: 1
Loss after 703252560 batches: 0.0245
trigger times: 2
Loss after 703355160 batches: 0.0214
trigger times: 3
Loss after 703457760 batches: 0.0196
trigger times: 4
Loss after 703560360 batches: 0.0202
trigger times: 5
Loss after 703662960 batches: 0.0230
trigger times: 0
Loss after 703765560 batches: 0.0193
trigger times: 1
Loss after 703868160 batches: 0.0213
trigger times: 0
Loss after 703970760 batches: 0.0202
trigger times: 1
Loss after 704073360 batches: 0.0183
trigger times: 2
Loss after 704175960 batches: 0.0182
trigger times: 3
Loss after 704278560 batches: 0.0173
trigger times: 4
Loss after 704381160 batches: 0.0186
trigger times: 5
Loss after 704483760 batches: 0.0182
trigger times: 6
Loss after 704586360 batches: 0.0181
trigger times: 7
Loss after 704688960 batches: 0.0187
trigger times: 8
Loss after 704791560 batches: 0.0170
trigger times: 9
Loss after 704894160 batches: 0.0173
trigger times: 10
Loss after 704996760 batches: 0.0163
trigger times: 11
Loss after 705099360 batches: 0.0156
trigger times: 12
Loss after 705201960 batches: 0.0150
trigger times: 13
Loss after 705304560 batches: 0.0155
trigger times: 14
Loss after 705407160 batches: 0.0155
trigger times: 15
Loss after 705509760 batches: 0.0153
trigger times: 16
Loss after 705612360 batches: 0.0158
trigger times: 17
Loss after 705714960 batches: 0.0157
trigger times: 18
Loss after 705817560 batches: 0.0151
trigger times: 19
Loss after 705920160 batches: 0.0149
trigger times: 20
Early stopping!
Start to test process.
Loss after 706022760 batches: 0.0147
Time to train on one home:  283.98310112953186
trigger times: 0
Loss after 706153860 batches: 0.1083
trigger times: 1
Loss after 706284960 batches: 0.0365
trigger times: 0
Loss after 706416060 batches: 0.0280
trigger times: 1
Loss after 706547160 batches: 0.0245
trigger times: 2
Loss after 706678260 batches: 0.0228
trigger times: 3
Loss after 706809360 batches: 0.0216
trigger times: 0
Loss after 706940460 batches: 0.0205
trigger times: 1
Loss after 707071560 batches: 0.0198
trigger times: 2
Loss after 707202660 batches: 0.0192
trigger times: 3
Loss after 707333760 batches: 0.0186
trigger times: 4
Loss after 707464860 batches: 0.0185
trigger times: 5
Loss after 707595960 batches: 0.0177
trigger times: 0
Loss after 707727060 batches: 0.0176
trigger times: 0
Loss after 707858160 batches: 0.0169
trigger times: 1
Loss after 707989260 batches: 0.0168
trigger times: 2
Loss after 708120360 batches: 0.0162
trigger times: 3
Loss after 708251460 batches: 0.0165
trigger times: 4
Loss after 708382560 batches: 0.0163
trigger times: 5
Loss after 708513660 batches: 0.0159
trigger times: 0
Loss after 708644760 batches: 0.0156
trigger times: 1
Loss after 708775860 batches: 0.0154
trigger times: 2
Loss after 708906960 batches: 0.0153
trigger times: 3
Loss after 709038060 batches: 0.0149
trigger times: 4
Loss after 709169160 batches: 0.0149
trigger times: 5
Loss after 709300260 batches: 0.0150
trigger times: 6
Loss after 709431360 batches: 0.0146
trigger times: 7
Loss after 709562460 batches: 0.0146
trigger times: 8
Loss after 709693560 batches: 0.0145
trigger times: 9
Loss after 709824660 batches: 0.0143
trigger times: 0
Loss after 709955760 batches: 0.0141
trigger times: 1
Loss after 710086860 batches: 0.0140
trigger times: 2
Loss after 710217960 batches: 0.0141
trigger times: 3
Loss after 710349060 batches: 0.0138
trigger times: 4
Loss after 710480160 batches: 0.0138
trigger times: 5
Loss after 710611260 batches: 0.0138
trigger times: 6
Loss after 710742360 batches: 0.0138
trigger times: 7
Loss after 710873460 batches: 0.0134
trigger times: 8
Loss after 711004560 batches: 0.0134
trigger times: 9
Loss after 711135660 batches: 0.0134
trigger times: 10
Loss after 711266760 batches: 0.0135
trigger times: 11
Loss after 711397860 batches: 0.0132
trigger times: 12
Loss after 711528960 batches: 0.0132
trigger times: 13
Loss after 711660060 batches: 0.0132
trigger times: 14
Loss after 711791160 batches: 0.0129
trigger times: 15
Loss after 711922260 batches: 0.0128
trigger times: 16
Loss after 712053360 batches: 0.0129
trigger times: 17
Loss after 712184460 batches: 0.0128
trigger times: 18
Loss after 712315560 batches: 0.0128
trigger times: 19
Loss after 712446660 batches: 0.0126
trigger times: 20
Early stopping!
Start to test process.
Loss after 712577760 batches: 0.0126
Time to train on one home:  365.3407950401306
trigger times: 0
Loss after 712708860 batches: 0.1621
trigger times: 0
Loss after 712839960 batches: 0.0453
trigger times: 1
Loss after 712971060 batches: 0.0350
trigger times: 0
Loss after 713102160 batches: 0.0308
trigger times: 1
Loss after 713233260 batches: 0.0286
trigger times: 0
Loss after 713364360 batches: 0.0268
trigger times: 1
Loss after 713495460 batches: 0.0261
trigger times: 2
Loss after 713626560 batches: 0.0246
trigger times: 3
Loss after 713757660 batches: 0.0241
trigger times: 4
Loss after 713888760 batches: 0.0234
trigger times: 5
Loss after 714019860 batches: 0.0231
trigger times: 6
Loss after 714150960 batches: 0.0226
trigger times: 7
Loss after 714282060 batches: 0.0222
trigger times: 8
Loss after 714413160 batches: 0.0220
trigger times: 9
Loss after 714544260 batches: 0.0212
trigger times: 10
Loss after 714675360 batches: 0.0210
trigger times: 11
Loss after 714806460 batches: 0.0209
trigger times: 0
Loss after 714937560 batches: 0.0206
trigger times: 1
Loss after 715068660 batches: 0.0205
trigger times: 2
Loss after 715199760 batches: 0.0202
trigger times: 3
Loss after 715330860 batches: 0.0200
trigger times: 4
Loss after 715461960 batches: 0.0195
trigger times: 5
Loss after 715593060 batches: 0.0197
trigger times: 6
Loss after 715724160 batches: 0.0198
trigger times: 0
Loss after 715855260 batches: 0.0192
trigger times: 1
Loss after 715986360 batches: 0.0195
trigger times: 2
Loss after 716117460 batches: 0.0187
trigger times: 3
Loss after 716248560 batches: 0.0188
trigger times: 4
Loss after 716379660 batches: 0.0188
trigger times: 5
Loss after 716510760 batches: 0.0187
trigger times: 6
Loss after 716641860 batches: 0.0185
trigger times: 7
Loss after 716772960 batches: 0.0187
trigger times: 8
Loss after 716904060 batches: 0.0179
trigger times: 9
Loss after 717035160 batches: 0.0179
trigger times: 10
Loss after 717166260 batches: 0.0181
trigger times: 11
Loss after 717297360 batches: 0.0180
trigger times: 12
Loss after 717428460 batches: 0.0180
trigger times: 13
Loss after 717559560 batches: 0.0179
trigger times: 14
Loss after 717690660 batches: 0.0178
trigger times: 15
Loss after 717821760 batches: 0.0180
trigger times: 16
Loss after 717952860 batches: 0.0178
trigger times: 17
Loss after 718083960 batches: 0.0176
trigger times: 18
Loss after 718215060 batches: 0.0174
trigger times: 19
Loss after 718346160 batches: 0.0170
trigger times: 20
Early stopping!
Start to test process.
Loss after 718477260 batches: 0.0172
Time to train on one home:  330.2550251483917
trigger times: 0
Loss after 718605900 batches: 0.0929
trigger times: 0
Loss after 718734540 batches: 0.0284
trigger times: 1
Loss after 718863180 batches: 0.0217
trigger times: 2
Loss after 718991820 batches: 0.0193
trigger times: 3
Loss after 719120460 batches: 0.0179
trigger times: 4
Loss after 719249100 batches: 0.0173
trigger times: 5
Loss after 719377740 batches: 0.0163
trigger times: 6
Loss after 719506380 batches: 0.0158
trigger times: 7
Loss after 719635020 batches: 0.0154
trigger times: 8
Loss after 719763660 batches: 0.0151
trigger times: 9
Loss after 719892300 batches: 0.0147
trigger times: 10
Loss after 720020940 batches: 0.0140
trigger times: 11
Loss after 720149580 batches: 0.0139
trigger times: 12
Loss after 720278220 batches: 0.0135
trigger times: 13
Loss after 720406860 batches: 0.0134
trigger times: 14
Loss after 720535500 batches: 0.0131
trigger times: 15
Loss after 720664140 batches: 0.0132
trigger times: 16
Loss after 720792780 batches: 0.0130
trigger times: 17
Loss after 720921420 batches: 0.0127
trigger times: 18
Loss after 721050060 batches: 0.0127
trigger times: 19
Loss after 721178700 batches: 0.0123
trigger times: 20
Early stopping!
Start to test process.
Loss after 721307340 batches: 0.0124
Time to train on one home:  164.19737839698792
trigger times: 0
Loss after 721438440 batches: 0.1707
trigger times: 0
Loss after 721569540 batches: 0.0444
trigger times: 0
Loss after 721700640 batches: 0.0342
trigger times: 1
Loss after 721831740 batches: 0.0311
trigger times: 2
Loss after 721962840 batches: 0.0290
trigger times: 3
Loss after 722093940 batches: 0.0278
trigger times: 4
Loss after 722225040 batches: 0.0261
trigger times: 5
Loss after 722356140 batches: 0.0253
trigger times: 6
Loss after 722487240 batches: 0.0247
trigger times: 7
Loss after 722618340 batches: 0.0240
trigger times: 8
Loss after 722749440 batches: 0.0233
trigger times: 9
Loss after 722880540 batches: 0.0228
trigger times: 10
Loss after 723011640 batches: 0.0224
trigger times: 11
Loss after 723142740 batches: 0.0223
trigger times: 12
Loss after 723273840 batches: 0.0217
trigger times: 13
Loss after 723404940 batches: 0.0212
trigger times: 14
Loss after 723536040 batches: 0.0210
trigger times: 15
Loss after 723667140 batches: 0.0208
trigger times: 16
Loss after 723798240 batches: 0.0205
trigger times: 17
Loss after 723929340 batches: 0.0202
trigger times: 18
Loss after 724060440 batches: 0.0199
trigger times: 19
Loss after 724191540 batches: 0.0196
trigger times: 20
Early stopping!
Start to test process.
Loss after 724322640 batches: 0.0196
Time to train on one home:  173.74841618537903
trigger times: 0
Loss after 724453740 batches: 0.1692
trigger times: 0
Loss after 724584840 batches: 0.0625
trigger times: 0
Loss after 724715940 batches: 0.0431
trigger times: 0
Loss after 724847040 batches: 0.0363
trigger times: 1
Loss after 724978140 batches: 0.0331
trigger times: 2
Loss after 725109240 batches: 0.0301
trigger times: 3
Loss after 725240340 batches: 0.0291
trigger times: 0
Loss after 725371440 batches: 0.0280
trigger times: 0
Loss after 725502540 batches: 0.0275
trigger times: 1
Loss after 725633640 batches: 0.0275
trigger times: 2
Loss after 725764740 batches: 0.0262
trigger times: 0
Loss after 725895840 batches: 0.0259
trigger times: 1
Loss after 726026940 batches: 0.0248
trigger times: 2
Loss after 726158040 batches: 0.0251
trigger times: 0
Loss after 726289140 batches: 0.0245
trigger times: 1
Loss after 726420240 batches: 0.0230
trigger times: 2
Loss after 726551340 batches: 0.0237
trigger times: 3
Loss after 726682440 batches: 0.0227
trigger times: 4
Loss after 726813540 batches: 0.0228
trigger times: 5
Loss after 726944640 batches: 0.0228
trigger times: 6
Loss after 727075740 batches: 0.0224
trigger times: 7
Loss after 727206840 batches: 0.0219
trigger times: 8
Loss after 727337940 batches: 0.0218
trigger times: 9
Loss after 727469040 batches: 0.0221
trigger times: 10
Loss after 727600140 batches: 0.0221
trigger times: 11
Loss after 727731240 batches: 0.0213
trigger times: 12
Loss after 727862340 batches: 0.0216
trigger times: 13
Loss after 727993440 batches: 0.0213
trigger times: 14
Loss after 728124540 batches: 0.0204
trigger times: 15
Loss after 728255640 batches: 0.0208
trigger times: 16
Loss after 728386740 batches: 0.0211
trigger times: 0
Loss after 728517840 batches: 0.0215
trigger times: 1
Loss after 728648940 batches: 0.0210
trigger times: 2
Loss after 728780040 batches: 0.0194
trigger times: 3
Loss after 728911140 batches: 0.0201
trigger times: 4
Loss after 729042240 batches: 0.0205
trigger times: 5
Loss after 729173340 batches: 0.0204
trigger times: 6
Loss after 729304440 batches: 0.0199
trigger times: 0
Loss after 729435540 batches: 0.0193
trigger times: 1
Loss after 729566640 batches: 0.0206
trigger times: 2
Loss after 729697740 batches: 0.0207
trigger times: 3
Loss after 729828840 batches: 0.0203
trigger times: 4
Loss after 729959940 batches: 0.0197
trigger times: 5
Loss after 730091040 batches: 0.0192
trigger times: 6
Loss after 730222140 batches: 0.0189
trigger times: 7
Loss after 730353240 batches: 0.0195
trigger times: 0
Loss after 730484340 batches: 0.0187
trigger times: 1
Loss after 730615440 batches: 0.0193
trigger times: 2
Loss after 730746540 batches: 0.0199
trigger times: 3
Loss after 730877640 batches: 0.0194
trigger times: 4
Loss after 731008740 batches: 0.0191
trigger times: 5
Loss after 731139840 batches: 0.0186
trigger times: 6
Loss after 731270940 batches: 0.0190
trigger times: 0
Loss after 731402040 batches: 0.0182
trigger times: 1
Loss after 731533140 batches: 0.0187
trigger times: 0
Loss after 731664240 batches: 0.0183
trigger times: 1
Loss after 731795340 batches: 0.0181
trigger times: 2
Loss after 731926440 batches: 0.0177
trigger times: 3
Loss after 732057540 batches: 0.0180
trigger times: 4
Loss after 732188640 batches: 0.0180
trigger times: 5
Loss after 732319740 batches: 0.0206
trigger times: 6
Loss after 732450840 batches: 0.0194
trigger times: 7
Loss after 732581940 batches: 0.0189
trigger times: 8
Loss after 732713040 batches: 0.0181
trigger times: 9
Loss after 732844140 batches: 0.0186
trigger times: 10
Loss after 732975240 batches: 0.0180
trigger times: 11
Loss after 733106340 batches: 0.0176
trigger times: 12
Loss after 733237440 batches: 0.0179
trigger times: 13
Loss after 733368540 batches: 0.0185
trigger times: 14
Loss after 733499640 batches: 0.0174
trigger times: 15
Loss after 733630740 batches: 0.0175
trigger times: 16
Loss after 733761840 batches: 0.0172
trigger times: 17
Loss after 733892940 batches: 0.0173
trigger times: 18
Loss after 734024040 batches: 0.0173
trigger times: 19
Loss after 734155140 batches: 0.0173
trigger times: 20
Early stopping!
Start to test process.
Loss after 734286240 batches: 0.0168
Time to train on one home:  548.6622471809387
trigger times: 0
Loss after 734417340 batches: 0.0618
trigger times: 0
Loss after 734548440 batches: 0.0204
trigger times: 0
Loss after 734679540 batches: 0.0160
trigger times: 0
Loss after 734810640 batches: 0.0142
trigger times: 1
Loss after 734941740 batches: 0.0131
trigger times: 2
Loss after 735072840 batches: 0.0124
trigger times: 3
Loss after 735203940 batches: 0.0120
trigger times: 4
Loss after 735335040 batches: 0.0114
trigger times: 5
Loss after 735466140 batches: 0.0110
trigger times: 6
Loss after 735597240 batches: 0.0106
trigger times: 7
Loss after 735728340 batches: 0.0103
trigger times: 8
Loss after 735859440 batches: 0.0103
trigger times: 9
Loss after 735990540 batches: 0.0100
trigger times: 10
Loss after 736121640 batches: 0.0098
trigger times: 11
Loss after 736252740 batches: 0.0095
trigger times: 12
Loss after 736383840 batches: 0.0093
trigger times: 13
Loss after 736514940 batches: 0.0092
trigger times: 14
Loss after 736646040 batches: 0.0093
trigger times: 15
Loss after 736777140 batches: 0.0090
trigger times: 16
Loss after 736908240 batches: 0.0090
trigger times: 17
Loss after 737039340 batches: 0.0087
trigger times: 18
Loss after 737170440 batches: 0.0087
trigger times: 19
Loss after 737301540 batches: 0.0086
trigger times: 20
Early stopping!
Start to test process.
Loss after 737432640 batches: 0.0084
Time to train on one home:  181.55833649635315
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378, 0.015884290067137628, 0.015422517538360537, 0.015618174097677863, 0.015689947067699456, 0.016115858282786194, 0.015032807630775873, 0.013889531067539576, 0.014509777593618559, 0.013978963742659806]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61], [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846], [0.5624003973272111, 0.39199562780285213, 0.46338177904139005, 1.0500325960715429, 0.49807389089844895, 24.807396438856706, 1537.5851], [0.5526623295413123, 0.4025342703450545, 0.4773154352778254, 1.041540162760643, 0.4894406919679644, 24.60675965799653, 1510.9338], [0.5434306197696261, 0.41257883262882467, 0.4890240237330659, 1.0321400196734463, 0.48121224091099224, 24.384677908326562, 1485.5321], [0.5378838711314731, 0.41862279133969893, 0.4914352831936747, 1.0233749172087403, 0.4762610626477895, 24.177599220976607, 1470.2474], [0.5388939446873136, 0.4175359601598919, 0.4899945807831285, 1.0201219390637546, 0.47715138886784625, 24.100746446356553, 1472.9958], [0.5538486010498471, 0.40130114966802066, 0.4733353493101595, 1.0370228287424743, 0.49045085775236186, 24.500036022697298, 1514.0524], [0.5457354452874925, 0.4101143038770134, 0.47950876644385965, 1.028278416589283, 0.4832311695252878, 24.29344615137278, 1491.7646], [0.5428701837857565, 0.4132375522040934, 0.4832618067565206, 1.022986243672678, 0.48067262140023154, 24.16841666937729, 1483.8663]]
Round_16_results:  [0.5428701837857565, 0.4132375522040934, 0.4832618067565206, 1.022986243672678, 0.48067262140023154, 24.16841666937729, 1483.8663]
trigger times: 0
Loss after 737563740 batches: 0.1014
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5824 < 5825; dropping {'Training_Loss': 0.10135849171651984, 'Validation_Loss': 0.20148815545770857, 'Training_R2': 0.8979819927728079, 'Validation_R2': 0.8126821974995277, 'Training_F1': 0.8305684726076128, 'Validation_F1': 0.7566963553323737, 'Training_NEP': 0.33855315213668175, 'Validation_NEP': 0.4710534645357401, 'Training_NDE': 0.07658672884788789, 'Validation_NDE': 0.14917002362021464, 'Training_MAE': 11.21220897546531, 'Validation_MAE': 12.918240278681546, 'Training_MSE': 336.96957, 'Validation_MSE': 550.8808}.
trigger times: 0
Loss after 737694840 batches: 0.0256
trigger times: 1
Loss after 737825940 batches: 0.0192
trigger times: 0
Loss after 737957040 batches: 0.0166
trigger times: 1
Loss after 738088140 batches: 0.0152
trigger times: 2
Loss after 738219240 batches: 0.0145
trigger times: 3
Loss after 738350340 batches: 0.0142
trigger times: 4
Loss after 738481440 batches: 0.0133
trigger times: 5
Loss after 738612540 batches: 0.0130
trigger times: 6
Loss after 738743640 batches: 0.0127
trigger times: 7
Loss after 738874740 batches: 0.0123
trigger times: 8
Loss after 739005840 batches: 0.0118
trigger times: 9
Loss after 739136940 batches: 0.0119
trigger times: 10
Loss after 739268040 batches: 0.0114
trigger times: 11
Loss after 739399140 batches: 0.0115
trigger times: 12
Loss after 739530240 batches: 0.0108
trigger times: 13
Loss after 739661340 batches: 0.0110
trigger times: 14
Loss after 739792440 batches: 0.0107
trigger times: 15
Loss after 739923540 batches: 0.0104
trigger times: 16
Loss after 740054640 batches: 0.0104
trigger times: 17
Loss after 740185740 batches: 0.0103
trigger times: 18
Loss after 740316840 batches: 0.0101
trigger times: 19
Loss after 740447940 batches: 0.0103
trigger times: 20
Early stopping!
Start to test process.
Loss after 740579040 batches: 0.0100
Time to train on one home:  181.46164345741272
trigger times: 0
Loss after 740681640 batches: 0.1383
trigger times: 0
Loss after 740784240 batches: 0.0471
trigger times: 1
Loss after 740886840 batches: 0.0363
trigger times: 2
Loss after 740989440 batches: 0.0351
trigger times: 3
Loss after 741092040 batches: 0.0278
trigger times: 0
Loss after 741194640 batches: 0.0257
trigger times: 1
Loss after 741297240 batches: 0.0241
trigger times: 2
Loss after 741399840 batches: 0.0235
trigger times: 0
Loss after 741502440 batches: 0.0225
trigger times: 1
Loss after 741605040 batches: 0.0215
trigger times: 0
Loss after 741707640 batches: 0.0219
trigger times: 1
Loss after 741810240 batches: 0.0214
trigger times: 2
Loss after 741912840 batches: 0.0218
trigger times: 0
Loss after 742015440 batches: 0.0209
trigger times: 1
Loss after 742118040 batches: 0.0222
trigger times: 0
Loss after 742220640 batches: 0.0192
trigger times: 1
Loss after 742323240 batches: 0.0206
trigger times: 2
Loss after 742425840 batches: 0.0225
trigger times: 3
Loss after 742528440 batches: 0.0209
trigger times: 4
Loss after 742631040 batches: 0.0188
trigger times: 5
Loss after 742733640 batches: 0.0181
trigger times: 0
Loss after 742836240 batches: 0.0187
trigger times: 1
Loss after 742938840 batches: 0.0175
trigger times: 2
Loss after 743041440 batches: 0.0174
trigger times: 3
Loss after 743144040 batches: 0.0174
trigger times: 4
Loss after 743246640 batches: 0.0177
trigger times: 5
Loss after 743349240 batches: 0.0171
trigger times: 6
Loss after 743451840 batches: 0.0171
trigger times: 7
Loss after 743554440 batches: 0.0169
trigger times: 0
Loss after 743657040 batches: 0.0175
trigger times: 1
Loss after 743759640 batches: 0.0172
trigger times: 2
Loss after 743862240 batches: 0.0171
trigger times: 3
Loss after 743964840 batches: 0.0166
trigger times: 4
Loss after 744067440 batches: 0.0180
trigger times: 0
Loss after 744170040 batches: 0.0176
trigger times: 1
Loss after 744272640 batches: 0.0168
trigger times: 2
Loss after 744375240 batches: 0.0163
trigger times: 3
Loss after 744477840 batches: 0.0156
trigger times: 0
Loss after 744580440 batches: 0.0164
trigger times: 1
Loss after 744683040 batches: 0.0170
trigger times: 2
Loss after 744785640 batches: 0.0173
trigger times: 0
Loss after 744888240 batches: 0.0170
trigger times: 1
Loss after 744990840 batches: 0.0166
trigger times: 2
Loss after 745093440 batches: 0.0198
trigger times: 3
Loss after 745196040 batches: 0.0161
trigger times: 4
Loss after 745298640 batches: 0.0161
trigger times: 5
Loss after 745401240 batches: 0.0156
trigger times: 6
Loss after 745503840 batches: 0.0157
trigger times: 7
Loss after 745606440 batches: 0.0153
trigger times: 8
Loss after 745709040 batches: 0.0153
trigger times: 9
Loss after 745811640 batches: 0.0151
trigger times: 10
Loss after 745914240 batches: 0.0152
trigger times: 0
Loss after 746016840 batches: 0.0151
trigger times: 1
Loss after 746119440 batches: 0.0145
trigger times: 2
Loss after 746222040 batches: 0.0146
trigger times: 3
Loss after 746324640 batches: 0.0149
trigger times: 0
Loss after 746427240 batches: 0.0160
trigger times: 1
Loss after 746529840 batches: 0.0155
trigger times: 2
Loss after 746632440 batches: 0.0146
trigger times: 3
Loss after 746735040 batches: 0.0156
trigger times: 4
Loss after 746837640 batches: 0.0155
trigger times: 5
Loss after 746940240 batches: 0.0144
trigger times: 6
Loss after 747042840 batches: 0.0141
trigger times: 7
Loss after 747145440 batches: 0.0142
trigger times: 8
Loss after 747248040 batches: 0.0153
trigger times: 9
Loss after 747350640 batches: 0.0138
trigger times: 10
Loss after 747453240 batches: 0.0135
trigger times: 11
Loss after 747555840 batches: 0.0146
trigger times: 12
Loss after 747658440 batches: 0.0164
trigger times: 13
Loss after 747761040 batches: 0.0151
trigger times: 14
Loss after 747863640 batches: 0.0153
trigger times: 15
Loss after 747966240 batches: 0.0137
trigger times: 16
Loss after 748068840 batches: 0.0140
trigger times: 17
Loss after 748171440 batches: 0.0136
trigger times: 18
Loss after 748274040 batches: 0.0152
trigger times: 19
Loss after 748376640 batches: 0.0143
trigger times: 20
Early stopping!
Start to test process.
Loss after 748479240 batches: 0.0139
Time to train on one home:  448.8048493862152
trigger times: 0
Loss after 748610340 batches: 0.0956
trigger times: 0
Loss after 748741440 batches: 0.0345
trigger times: 1
Loss after 748872540 batches: 0.0265
trigger times: 2
Loss after 749003640 batches: 0.0232
trigger times: 3
Loss after 749134740 batches: 0.0215
trigger times: 4
Loss after 749265840 batches: 0.0204
trigger times: 5
Loss after 749396940 batches: 0.0195
trigger times: 6
Loss after 749528040 batches: 0.0189
trigger times: 7
Loss after 749659140 batches: 0.0183
trigger times: 8
Loss after 749790240 batches: 0.0178
trigger times: 9
Loss after 749921340 batches: 0.0172
trigger times: 10
Loss after 750052440 batches: 0.0169
trigger times: 11
Loss after 750183540 batches: 0.0167
trigger times: 12
Loss after 750314640 batches: 0.0165
trigger times: 13
Loss after 750445740 batches: 0.0161
trigger times: 14
Loss after 750576840 batches: 0.0157
trigger times: 15
Loss after 750707940 batches: 0.0156
trigger times: 16
Loss after 750839040 batches: 0.0154
trigger times: 17
Loss after 750970140 batches: 0.0150
trigger times: 18
Loss after 751101240 batches: 0.0149
trigger times: 19
Loss after 751232340 batches: 0.0148
trigger times: 20
Early stopping!
Start to test process.
Loss after 751363440 batches: 0.0146
Time to train on one home:  167.48039650917053
trigger times: 0
Loss after 751494540 batches: 0.1532
trigger times: 0
Loss after 751625640 batches: 0.0442
trigger times: 0
Loss after 751756740 batches: 0.0343
trigger times: 1
Loss after 751887840 batches: 0.0303
trigger times: 0
Loss after 752018940 batches: 0.0280
trigger times: 1
Loss after 752150040 batches: 0.0264
trigger times: 2
Loss after 752281140 batches: 0.0253
trigger times: 0
Loss after 752412240 batches: 0.0249
trigger times: 0
Loss after 752543340 batches: 0.0242
trigger times: 1
Loss after 752674440 batches: 0.0234
trigger times: 0
Loss after 752805540 batches: 0.0229
trigger times: 0
Loss after 752936640 batches: 0.0223
trigger times: 1
Loss after 753067740 batches: 0.0219
trigger times: 2
Loss after 753198840 batches: 0.0219
trigger times: 3
Loss after 753329940 batches: 0.0214
trigger times: 4
Loss after 753461040 batches: 0.0210
trigger times: 5
Loss after 753592140 batches: 0.0208
trigger times: 6
Loss after 753723240 batches: 0.0203
trigger times: 7
Loss after 753854340 batches: 0.0201
trigger times: 0
Loss after 753985440 batches: 0.0202
trigger times: 1
Loss after 754116540 batches: 0.0197
trigger times: 2
Loss after 754247640 batches: 0.0195
trigger times: 3
Loss after 754378740 batches: 0.0196
trigger times: 4
Loss after 754509840 batches: 0.0193
trigger times: 5
Loss after 754640940 batches: 0.0191
trigger times: 6
Loss after 754772040 batches: 0.0188
trigger times: 7
Loss after 754903140 batches: 0.0191
trigger times: 8
Loss after 755034240 batches: 0.0187
trigger times: 9
Loss after 755165340 batches: 0.0188
trigger times: 10
Loss after 755296440 batches: 0.0185
trigger times: 11
Loss after 755427540 batches: 0.0183
trigger times: 12
Loss after 755558640 batches: 0.0181
trigger times: 13
Loss after 755689740 batches: 0.0178
trigger times: 0
Loss after 755820840 batches: 0.0179
trigger times: 1
Loss after 755951940 batches: 0.0179
trigger times: 2
Loss after 756083040 batches: 0.0176
trigger times: 3
Loss after 756214140 batches: 0.0178
trigger times: 4
Loss after 756345240 batches: 0.0176
trigger times: 5
Loss after 756476340 batches: 0.0177
trigger times: 6
Loss after 756607440 batches: 0.0175
trigger times: 0
Loss after 756738540 batches: 0.0172
trigger times: 1
Loss after 756869640 batches: 0.0173
trigger times: 2
Loss after 757000740 batches: 0.0172
trigger times: 3
Loss after 757131840 batches: 0.0170
trigger times: 4
Loss after 757262940 batches: 0.0170
trigger times: 5
Loss after 757394040 batches: 0.0169
trigger times: 6
Loss after 757525140 batches: 0.0173
trigger times: 7
Loss after 757656240 batches: 0.0169
trigger times: 8
Loss after 757787340 batches: 0.0169
trigger times: 9
Loss after 757918440 batches: 0.0168
trigger times: 10
Loss after 758049540 batches: 0.0168
trigger times: 11
Loss after 758180640 batches: 0.0167
trigger times: 12
Loss after 758311740 batches: 0.0164
trigger times: 13
Loss after 758442840 batches: 0.0164
trigger times: 14
Loss after 758573940 batches: 0.0164
trigger times: 15
Loss after 758705040 batches: 0.0161
trigger times: 0
Loss after 758836140 batches: 0.0162
trigger times: 1
Loss after 758967240 batches: 0.0161
trigger times: 2
Loss after 759098340 batches: 0.0160
trigger times: 3
Loss after 759229440 batches: 0.0161
trigger times: 4
Loss after 759360540 batches: 0.0160
trigger times: 5
Loss after 759491640 batches: 0.0160
trigger times: 6
Loss after 759622740 batches: 0.0159
trigger times: 7
Loss after 759753840 batches: 0.0158
trigger times: 8
Loss after 759884940 batches: 0.0156
trigger times: 9
Loss after 760016040 batches: 0.0157
trigger times: 0
Loss after 760147140 batches: 0.0157
trigger times: 1
Loss after 760278240 batches: 0.0157
trigger times: 2
Loss after 760409340 batches: 0.0155
trigger times: 3
Loss after 760540440 batches: 0.0157
trigger times: 4
Loss after 760671540 batches: 0.0156
trigger times: 0
Loss after 760802640 batches: 0.0155
trigger times: 0
Loss after 760933740 batches: 0.0154
trigger times: 1
Loss after 761064840 batches: 0.0152
trigger times: 2
Loss after 761195940 batches: 0.0153
trigger times: 3
Loss after 761327040 batches: 0.0153
trigger times: 4
Loss after 761458140 batches: 0.0154
trigger times: 5
Loss after 761589240 batches: 0.0152
trigger times: 6
Loss after 761720340 batches: 0.0152
trigger times: 7
Loss after 761851440 batches: 0.0152
trigger times: 8
Loss after 761982540 batches: 0.0150
trigger times: 9
Loss after 762113640 batches: 0.0150
trigger times: 10
Loss after 762244740 batches: 0.0148
trigger times: 11
Loss after 762375840 batches: 0.0148
trigger times: 12
Loss after 762506940 batches: 0.0148
trigger times: 13
Loss after 762638040 batches: 0.0148
trigger times: 14
Loss after 762769140 batches: 0.0148
trigger times: 15
Loss after 762900240 batches: 0.0149
trigger times: 16
Loss after 763031340 batches: 0.0152
trigger times: 17
Loss after 763162440 batches: 0.0148
trigger times: 18
Loss after 763293540 batches: 0.0147
trigger times: 0
Loss after 763424640 batches: 0.0146
trigger times: 1
Loss after 763555740 batches: 0.0149
trigger times: 2
Loss after 763686840 batches: 0.0147
trigger times: 3
Loss after 763817940 batches: 0.0146
trigger times: 4
Loss after 763949040 batches: 0.0145
trigger times: 5
Loss after 764080140 batches: 0.0145
trigger times: 6
Loss after 764211240 batches: 0.0143
trigger times: 7
Loss after 764342340 batches: 0.0144
trigger times: 8
Loss after 764473440 batches: 0.0145
trigger times: 9
Loss after 764604540 batches: 0.0147
trigger times: 10
Loss after 764735640 batches: 0.0146
trigger times: 11
Loss after 764866740 batches: 0.0145
trigger times: 12
Loss after 764997840 batches: 0.0143
trigger times: 13
Loss after 765128940 batches: 0.0143
trigger times: 14
Loss after 765260040 batches: 0.0143
trigger times: 15
Loss after 765391140 batches: 0.0145
trigger times: 16
Loss after 765522240 batches: 0.0144
trigger times: 17
Loss after 765653340 batches: 0.0144
trigger times: 18
Loss after 765784440 batches: 0.0143
trigger times: 19
Loss after 765915540 batches: 0.0144
trigger times: 20
Early stopping!
Start to test process.
Loss after 766046640 batches: 0.0142
Time to train on one home:  804.768824338913
trigger times: 0
Loss after 766175280 batches: 0.0920
trigger times: 0
Loss after 766303920 batches: 0.0278
trigger times: 0
Loss after 766432560 batches: 0.0221
trigger times: 1
Loss after 766561200 batches: 0.0189
trigger times: 2
Loss after 766689840 batches: 0.0180
trigger times: 3
Loss after 766818480 batches: 0.0169
trigger times: 4
Loss after 766947120 batches: 0.0161
trigger times: 5
Loss after 767075760 batches: 0.0156
trigger times: 6
Loss after 767204400 batches: 0.0152
trigger times: 7
Loss after 767333040 batches: 0.0150
trigger times: 8
Loss after 767461680 batches: 0.0145
trigger times: 9
Loss after 767590320 batches: 0.0143
trigger times: 10
Loss after 767718960 batches: 0.0138
trigger times: 11
Loss after 767847600 batches: 0.0137
trigger times: 12
Loss after 767976240 batches: 0.0135
trigger times: 13
Loss after 768104880 batches: 0.0134
trigger times: 14
Loss after 768233520 batches: 0.0131
trigger times: 15
Loss after 768362160 batches: 0.0129
trigger times: 16
Loss after 768490800 batches: 0.0126
trigger times: 17
Loss after 768619440 batches: 0.0126
trigger times: 18
Loss after 768748080 batches: 0.0125
trigger times: 19
Loss after 768876720 batches: 0.0121
trigger times: 20
Early stopping!
Start to test process.
Loss after 769005360 batches: 0.0122
Time to train on one home:  171.79747557640076
trigger times: 0
Loss after 769136460 batches: 0.1796
trigger times: 0
Loss after 769267560 batches: 0.0456
trigger times: 0
Loss after 769398660 batches: 0.0351
trigger times: 1
Loss after 769529760 batches: 0.0314
trigger times: 2
Loss after 769660860 batches: 0.0294
trigger times: 3
Loss after 769791960 batches: 0.0279
trigger times: 4
Loss after 769923060 batches: 0.0270
trigger times: 5
Loss after 770054160 batches: 0.0257
trigger times: 6
Loss after 770185260 batches: 0.0247
trigger times: 7
Loss after 770316360 batches: 0.0242
trigger times: 8
Loss after 770447460 batches: 0.0239
trigger times: 0
Loss after 770578560 batches: 0.0233
trigger times: 1
Loss after 770709660 batches: 0.0229
trigger times: 2
Loss after 770840760 batches: 0.0223
trigger times: 3
Loss after 770971860 batches: 0.0220
trigger times: 0
Loss after 771102960 batches: 0.0214
trigger times: 1
Loss after 771234060 batches: 0.0213
trigger times: 2
Loss after 771365160 batches: 0.0210
trigger times: 3
Loss after 771496260 batches: 0.0207
trigger times: 4
Loss after 771627360 batches: 0.0208
trigger times: 5
Loss after 771758460 batches: 0.0205
trigger times: 6
Loss after 771889560 batches: 0.0202
trigger times: 0
Loss after 772020660 batches: 0.0197
trigger times: 1
Loss after 772151760 batches: 0.0198
trigger times: 2
Loss after 772282860 batches: 0.0196
trigger times: 3
Loss after 772413960 batches: 0.0195
trigger times: 4
Loss after 772545060 batches: 0.0192
trigger times: 5
Loss after 772676160 batches: 0.0190
trigger times: 6
Loss after 772807260 batches: 0.0191
trigger times: 7
Loss after 772938360 batches: 0.0189
trigger times: 8
Loss after 773069460 batches: 0.0188
trigger times: 9
Loss after 773200560 batches: 0.0184
trigger times: 10
Loss after 773331660 batches: 0.0183
trigger times: 11
Loss after 773462760 batches: 0.0182
trigger times: 12
Loss after 773593860 batches: 0.0181
trigger times: 13
Loss after 773724960 batches: 0.0182
trigger times: 14
Loss after 773856060 batches: 0.0181
trigger times: 15
Loss after 773987160 batches: 0.0180
trigger times: 16
Loss after 774118260 batches: 0.0180
trigger times: 17
Loss after 774249360 batches: 0.0176
trigger times: 18
Loss after 774380460 batches: 0.0176
trigger times: 19
Loss after 774511560 batches: 0.0176
trigger times: 20
Early stopping!
Start to test process.
Loss after 774642660 batches: 0.0174
Time to train on one home:  315.4633758068085
trigger times: 0
Loss after 774773760 batches: 0.1507
trigger times: 0
Loss after 774904860 batches: 0.0591
trigger times: 0
Loss after 775035960 batches: 0.0447
trigger times: 1
Loss after 775167060 batches: 0.0372
trigger times: 2
Loss after 775298160 batches: 0.0326
trigger times: 0
Loss after 775429260 batches: 0.0296
trigger times: 1
Loss after 775560360 batches: 0.0282
trigger times: 0
Loss after 775691460 batches: 0.0263
trigger times: 1
Loss after 775822560 batches: 0.0255
trigger times: 2
Loss after 775953660 batches: 0.0257
trigger times: 3
Loss after 776084760 batches: 0.0245
trigger times: 4
Loss after 776215860 batches: 0.0243
trigger times: 0
Loss after 776346960 batches: 0.0238
trigger times: 0
Loss after 776478060 batches: 0.0244
trigger times: 1
Loss after 776609160 batches: 0.0240
trigger times: 2
Loss after 776740260 batches: 0.0236
trigger times: 3
Loss after 776871360 batches: 0.0223
trigger times: 4
Loss after 777002460 batches: 0.0220
trigger times: 5
Loss after 777133560 batches: 0.0217
trigger times: 0
Loss after 777264660 batches: 0.0214
trigger times: 1
Loss after 777395760 batches: 0.0230
trigger times: 2
Loss after 777526860 batches: 0.0211
trigger times: 3
Loss after 777657960 batches: 0.0211
trigger times: 4
Loss after 777789060 batches: 0.0222
trigger times: 5
Loss after 777920160 batches: 0.0222
trigger times: 6
Loss after 778051260 batches: 0.0201
trigger times: 7
Loss after 778182360 batches: 0.0206
trigger times: 8
Loss after 778313460 batches: 0.0206
trigger times: 9
Loss after 778444560 batches: 0.0203
trigger times: 0
Loss after 778575660 batches: 0.0210
trigger times: 1
Loss after 778706760 batches: 0.0199
trigger times: 2
Loss after 778837860 batches: 0.0205
trigger times: 0
Loss after 778968960 batches: 0.0199
trigger times: 1
Loss after 779100060 batches: 0.0193
trigger times: 2
Loss after 779231160 batches: 0.0194
trigger times: 3
Loss after 779362260 batches: 0.0205
trigger times: 4
Loss after 779493360 batches: 0.0194
trigger times: 5
Loss after 779624460 batches: 0.0191
trigger times: 6
Loss after 779755560 batches: 0.0191
trigger times: 7
Loss after 779886660 batches: 0.0195
trigger times: 8
Loss after 780017760 batches: 0.0200
trigger times: 9
Loss after 780148860 batches: 0.0189
trigger times: 0
Loss after 780279960 batches: 0.0185
trigger times: 1
Loss after 780411060 batches: 0.0201
trigger times: 2
Loss after 780542160 batches: 0.0199
trigger times: 0
Loss after 780673260 batches: 0.0190
trigger times: 1
Loss after 780804360 batches: 0.0186
trigger times: 2
Loss after 780935460 batches: 0.0201
trigger times: 3
Loss after 781066560 batches: 0.0189
trigger times: 4
Loss after 781197660 batches: 0.0184
trigger times: 5
Loss after 781328760 batches: 0.0183
trigger times: 6
Loss after 781459860 batches: 0.0183
trigger times: 7
Loss after 781590960 batches: 0.0181
trigger times: 8
Loss after 781722060 batches: 0.0184
trigger times: 0
Loss after 781853160 batches: 0.0181
trigger times: 1
Loss after 781984260 batches: 0.0174
trigger times: 2
Loss after 782115360 batches: 0.0175
trigger times: 3
Loss after 782246460 batches: 0.0176
trigger times: 4
Loss after 782377560 batches: 0.0179
trigger times: 5
Loss after 782508660 batches: 0.0176
trigger times: 6
Loss after 782639760 batches: 0.0175
trigger times: 7
Loss after 782770860 batches: 0.0183
trigger times: 8
Loss after 782901960 batches: 0.0173
trigger times: 9
Loss after 783033060 batches: 0.0175
trigger times: 10
Loss after 783164160 batches: 0.0171
trigger times: 11
Loss after 783295260 batches: 0.0169
trigger times: 12
Loss after 783426360 batches: 0.0173
trigger times: 13
Loss after 783557460 batches: 0.0170
trigger times: 14
Loss after 783688560 batches: 0.0168
trigger times: 15
Loss after 783819660 batches: 0.0171
trigger times: 16
Loss after 783950760 batches: 0.0183
trigger times: 17
Loss after 784081860 batches: 0.0181
trigger times: 18
Loss after 784212960 batches: 0.0169
trigger times: 19
Loss after 784344060 batches: 0.0168
trigger times: 20
Early stopping!
Start to test process.
Loss after 784475160 batches: 0.0171
Time to train on one home:  542.2552974224091
trigger times: 0
Loss after 784606260 batches: 0.0607
trigger times: 0
Loss after 784737360 batches: 0.0206
trigger times: 0
Loss after 784868460 batches: 0.0154
trigger times: 1
Loss after 784999560 batches: 0.0139
trigger times: 2
Loss after 785130660 batches: 0.0129
trigger times: 0
Loss after 785261760 batches: 0.0122
trigger times: 1
Loss after 785392860 batches: 0.0117
trigger times: 2
Loss after 785523960 batches: 0.0111
trigger times: 3
Loss after 785655060 batches: 0.0109
trigger times: 4
Loss after 785786160 batches: 0.0106
trigger times: 5
Loss after 785917260 batches: 0.0101
trigger times: 0
Loss after 786048360 batches: 0.0100
trigger times: 1
Loss after 786179460 batches: 0.0098
trigger times: 2
Loss after 786310560 batches: 0.0096
trigger times: 3
Loss after 786441660 batches: 0.0094
trigger times: 4
Loss after 786572760 batches: 0.0095
trigger times: 5
Loss after 786703860 batches: 0.0092
trigger times: 6
Loss after 786834960 batches: 0.0091
trigger times: 7
Loss after 786966060 batches: 0.0089
trigger times: 8
Loss after 787097160 batches: 0.0088
trigger times: 9
Loss after 787228260 batches: 0.0088
trigger times: 10
Loss after 787359360 batches: 0.0087
trigger times: 11
Loss after 787490460 batches: 0.0087
trigger times: 12
Loss after 787621560 batches: 0.0085
trigger times: 13
Loss after 787752660 batches: 0.0083
trigger times: 14
Loss after 787883760 batches: 0.0082
trigger times: 15
Loss after 788014860 batches: 0.0082
trigger times: 16
Loss after 788145960 batches: 0.0081
trigger times: 17
Loss after 788277060 batches: 0.0080
trigger times: 18
Loss after 788408160 batches: 0.0081
trigger times: 19
Loss after 788539260 batches: 0.0078
trigger times: 20
Early stopping!
Start to test process.
Loss after 788670360 batches: 0.0078
Time to train on one home:  238.19581532478333
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378, 0.015884290067137628, 0.015422517538360537, 0.015618174097677863, 0.015689947067699456, 0.016115858282786194, 0.015032807630775873, 0.013889531067539576, 0.014509777593618559, 0.013978963742659806, 0.013400668321247023]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61], [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846], [0.5624003973272111, 0.39199562780285213, 0.46338177904139005, 1.0500325960715429, 0.49807389089844895, 24.807396438856706, 1537.5851], [0.5526623295413123, 0.4025342703450545, 0.4773154352778254, 1.041540162760643, 0.4894406919679644, 24.60675965799653, 1510.9338], [0.5434306197696261, 0.41257883262882467, 0.4890240237330659, 1.0321400196734463, 0.48121224091099224, 24.384677908326562, 1485.5321], [0.5378838711314731, 0.41862279133969893, 0.4914352831936747, 1.0233749172087403, 0.4762610626477895, 24.177599220976607, 1470.2474], [0.5388939446873136, 0.4175359601598919, 0.4899945807831285, 1.0201219390637546, 0.47715138886784625, 24.100746446356553, 1472.9958], [0.5538486010498471, 0.40130114966802066, 0.4733353493101595, 1.0370228287424743, 0.49045085775236186, 24.500036022697298, 1514.0524], [0.5457354452874925, 0.4101143038770134, 0.47950876644385965, 1.028278416589283, 0.4832311695252878, 24.29344615137278, 1491.7646], [0.5428701837857565, 0.4132375522040934, 0.4832618067565206, 1.022986243672678, 0.48067262140023154, 24.16841666937729, 1483.8663], [0.5393426484531827, 0.41705296588574414, 0.4905813481431651, 1.012340178256715, 0.47754705516303547, 23.91689955812201, 1474.2175]]
Round_17_results:  [0.5393426484531827, 0.41705296588574414, 0.4905813481431651, 1.012340178256715, 0.47754705516303547, 23.91689955812201, 1474.2175]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 6232 < 6233; dropping {'Training_Loss': 0.10516738765082269, 'Validation_Loss': 0.19491190628872979, 'Training_R2': 0.8940179583735217, 'Validation_R2': 0.818759037185077, 'Training_F1': 0.8273149421623788, 'Validation_F1': 0.7666672438489132, 'Training_NEP': 0.3449015833511414, 'Validation_NEP': 0.4507207999571412, 'Training_NDE': 0.07956259983315102, 'Validation_NDE': 0.14433074882983618, 'Training_MAE': 11.422456426991477, 'Validation_MAE': 12.36063426087834, 'Training_MSE': 350.06296, 'Validation_MSE': 533.00946}.
trigger times: 0
Loss after 788801460 batches: 0.1052
trigger times: 0
Loss after 788932560 batches: 0.0247
trigger times: 0
Loss after 789063660 batches: 0.0188
trigger times: 0
Loss after 789194760 batches: 0.0167
trigger times: 1
Loss after 789325860 batches: 0.0156
trigger times: 2
Loss after 789456960 batches: 0.0144
trigger times: 3
Loss after 789588060 batches: 0.0133
trigger times: 0
Loss after 789719160 batches: 0.0132
trigger times: 1
Loss after 789850260 batches: 0.0126
trigger times: 2
Loss after 789981360 batches: 0.0124
trigger times: 3
Loss after 790112460 batches: 0.0123
trigger times: 4
Loss after 790243560 batches: 0.0119
trigger times: 5
Loss after 790374660 batches: 0.0115
trigger times: 6
Loss after 790505760 batches: 0.0112
trigger times: 7
Loss after 790636860 batches: 0.0112
trigger times: 8
Loss after 790767960 batches: 0.0109
trigger times: 9
Loss after 790899060 batches: 0.0110
trigger times: 10
Loss after 791030160 batches: 0.0108
trigger times: 11
Loss after 791161260 batches: 0.0104
trigger times: 12
Loss after 791292360 batches: 0.0104
trigger times: 13
Loss after 791423460 batches: 0.0105
trigger times: 14
Loss after 791554560 batches: 0.0103
trigger times: 0
Loss after 791685660 batches: 0.0100
trigger times: 1
Loss after 791816760 batches: 0.0099
trigger times: 2
Loss after 791947860 batches: 0.0097
trigger times: 3
Loss after 792078960 batches: 0.0097
trigger times: 4
Loss after 792210060 batches: 0.0098
trigger times: 5
Loss after 792341160 batches: 0.0096
trigger times: 6
Loss after 792472260 batches: 0.0095
trigger times: 7
Loss after 792603360 batches: 0.0093
trigger times: 8
Loss after 792734460 batches: 0.0093
trigger times: 9
Loss after 792865560 batches: 0.0091
trigger times: 10
Loss after 792996660 batches: 0.0091
trigger times: 11
Loss after 793127760 batches: 0.0095
trigger times: 12
Loss after 793258860 batches: 0.0091
trigger times: 13
Loss after 793389960 batches: 0.0089
trigger times: 14
Loss after 793521060 batches: 0.0090
trigger times: 15
Loss after 793652160 batches: 0.0088
trigger times: 16
Loss after 793783260 batches: 0.0087
trigger times: 17
Loss after 793914360 batches: 0.0087
trigger times: 18
Loss after 794045460 batches: 0.0085
trigger times: 19
Loss after 794176560 batches: 0.0087
trigger times: 20
Early stopping!
Start to test process.
Loss after 794307660 batches: 0.0086
Time to train on one home:  316.49909019470215
trigger times: 0
Loss after 794410260 batches: 0.1450
trigger times: 0
Loss after 794512860 batches: 0.0479
trigger times: 1
Loss after 794615460 batches: 0.0371
trigger times: 2
Loss after 794718060 batches: 0.0319
trigger times: 0
Loss after 794820660 batches: 0.0275
trigger times: 1
Loss after 794923260 batches: 0.0260
trigger times: 2
Loss after 795025860 batches: 0.0243
trigger times: 3
Loss after 795128460 batches: 0.0230
trigger times: 4
Loss after 795231060 batches: 0.0257
trigger times: 5
Loss after 795333660 batches: 0.0215
trigger times: 6
Loss after 795436260 batches: 0.0209
trigger times: 7
Loss after 795538860 batches: 0.0204
trigger times: 8
Loss after 795641460 batches: 0.0203
trigger times: 9
Loss after 795744060 batches: 0.0197
trigger times: 10
Loss after 795846660 batches: 0.0191
trigger times: 11
Loss after 795949260 batches: 0.0196
trigger times: 12
Loss after 796051860 batches: 0.0191
trigger times: 13
Loss after 796154460 batches: 0.0188
trigger times: 14
Loss after 796257060 batches: 0.0185
trigger times: 15
Loss after 796359660 batches: 0.0186
trigger times: 16
Loss after 796462260 batches: 0.0234
trigger times: 17
Loss after 796564860 batches: 0.0197
trigger times: 18
Loss after 796667460 batches: 0.0184
trigger times: 19
Loss after 796770060 batches: 0.0175
trigger times: 20
Early stopping!
Start to test process.
Loss after 796872660 batches: 0.0170
Time to train on one home:  153.15721011161804
trigger times: 0
Loss after 797003760 batches: 0.0983
trigger times: 1
Loss after 797134860 batches: 0.0332
trigger times: 0
Loss after 797265960 batches: 0.0259
trigger times: 1
Loss after 797397060 batches: 0.0233
trigger times: 2
Loss after 797528160 batches: 0.0218
trigger times: 3
Loss after 797659260 batches: 0.0204
trigger times: 0
Loss after 797790360 batches: 0.0198
trigger times: 0
Loss after 797921460 batches: 0.0189
trigger times: 1
Loss after 798052560 batches: 0.0182
trigger times: 2
Loss after 798183660 batches: 0.0178
trigger times: 3
Loss after 798314760 batches: 0.0175
trigger times: 4
Loss after 798445860 batches: 0.0171
trigger times: 5
Loss after 798576960 batches: 0.0167
trigger times: 6
Loss after 798708060 batches: 0.0163
trigger times: 7
Loss after 798839160 batches: 0.0163
trigger times: 8
Loss after 798970260 batches: 0.0160
trigger times: 9
Loss after 799101360 batches: 0.0159
trigger times: 10
Loss after 799232460 batches: 0.0154
trigger times: 11
Loss after 799363560 batches: 0.0153
trigger times: 12
Loss after 799494660 batches: 0.0151
trigger times: 13
Loss after 799625760 batches: 0.0150
trigger times: 0
Loss after 799756860 batches: 0.0148
trigger times: 1
Loss after 799887960 batches: 0.0145
trigger times: 2
Loss after 800019060 batches: 0.0145
trigger times: 3
Loss after 800150160 batches: 0.0143
trigger times: 4
Loss after 800281260 batches: 0.0143
trigger times: 5
Loss after 800412360 batches: 0.0142
trigger times: 6
Loss after 800543460 batches: 0.0140
trigger times: 7
Loss after 800674560 batches: 0.0137
trigger times: 8
Loss after 800805660 batches: 0.0139
trigger times: 9
Loss after 800936760 batches: 0.0136
trigger times: 10
Loss after 801067860 batches: 0.0137
trigger times: 11
Loss after 801198960 batches: 0.0136
trigger times: 12
Loss after 801330060 batches: 0.0135
trigger times: 13
Loss after 801461160 batches: 0.0133
trigger times: 14
Loss after 801592260 batches: 0.0132
trigger times: 15
Loss after 801723360 batches: 0.0132
trigger times: 16
Loss after 801854460 batches: 0.0129
trigger times: 17
Loss after 801985560 batches: 0.0130
trigger times: 18
Loss after 802116660 batches: 0.0130
trigger times: 19
Loss after 802247760 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 802378860 batches: 0.0126
Time to train on one home:  308.76992774009705
trigger times: 0
Loss after 802509960 batches: 0.1439
trigger times: 1
Loss after 802641060 batches: 0.0430
trigger times: 0
Loss after 802772160 batches: 0.0324
trigger times: 0
Loss after 802903260 batches: 0.0291
trigger times: 1
Loss after 803034360 batches: 0.0265
trigger times: 0
Loss after 803165460 batches: 0.0253
trigger times: 0
Loss after 803296560 batches: 0.0243
trigger times: 1
Loss after 803427660 batches: 0.0234
trigger times: 2
Loss after 803558760 batches: 0.0230
trigger times: 3
Loss after 803689860 batches: 0.0220
trigger times: 4
Loss after 803820960 batches: 0.0216
trigger times: 5
Loss after 803952060 batches: 0.0213
trigger times: 6
Loss after 804083160 batches: 0.0209
trigger times: 7
Loss after 804214260 batches: 0.0206
trigger times: 8
Loss after 804345360 batches: 0.0205
trigger times: 9
Loss after 804476460 batches: 0.0200
trigger times: 10
Loss after 804607560 batches: 0.0199
trigger times: 11
Loss after 804738660 batches: 0.0195
trigger times: 12
Loss after 804869760 batches: 0.0192
trigger times: 13
Loss after 805000860 batches: 0.0192
trigger times: 14
Loss after 805131960 batches: 0.0188
trigger times: 15
Loss after 805263060 batches: 0.0186
trigger times: 16
Loss after 805394160 batches: 0.0184
trigger times: 17
Loss after 805525260 batches: 0.0184
trigger times: 18
Loss after 805656360 batches: 0.0184
trigger times: 19
Loss after 805787460 batches: 0.0180
trigger times: 20
Early stopping!
Start to test process.
Loss after 805918560 batches: 0.0180
Time to train on one home:  202.2978765964508
trigger times: 0
Loss after 806047200 batches: 0.1057
trigger times: 0
Loss after 806175840 batches: 0.0282
trigger times: 1
Loss after 806304480 batches: 0.0214
trigger times: 0
Loss after 806433120 batches: 0.0195
trigger times: 0
Loss after 806561760 batches: 0.0177
trigger times: 1
Loss after 806690400 batches: 0.0169
trigger times: 0
Loss after 806819040 batches: 0.0161
trigger times: 0
Loss after 806947680 batches: 0.0156
trigger times: 1
Loss after 807076320 batches: 0.0152
trigger times: 2
Loss after 807204960 batches: 0.0147
trigger times: 0
Loss after 807333600 batches: 0.0148
trigger times: 1
Loss after 807462240 batches: 0.0142
trigger times: 2
Loss after 807590880 batches: 0.0142
trigger times: 3
Loss after 807719520 batches: 0.0135
trigger times: 4
Loss after 807848160 batches: 0.0135
trigger times: 5
Loss after 807976800 batches: 0.0131
trigger times: 6
Loss after 808105440 batches: 0.0130
trigger times: 7
Loss after 808234080 batches: 0.0128
trigger times: 8
Loss after 808362720 batches: 0.0124
trigger times: 9
Loss after 808491360 batches: 0.0126
trigger times: 10
Loss after 808620000 batches: 0.0121
trigger times: 11
Loss after 808748640 batches: 0.0122
trigger times: 12
Loss after 808877280 batches: 0.0122
trigger times: 13
Loss after 809005920 batches: 0.0122
trigger times: 14
Loss after 809134560 batches: 0.0118
trigger times: 15
Loss after 809263200 batches: 0.0116
trigger times: 16
Loss after 809391840 batches: 0.0116
trigger times: 17
Loss after 809520480 batches: 0.0116
trigger times: 18
Loss after 809649120 batches: 0.0116
trigger times: 19
Loss after 809777760 batches: 0.0115
trigger times: 20
Early stopping!
Start to test process.
Loss after 809906400 batches: 0.0111
Time to train on one home:  226.811785697937
trigger times: 0
Loss after 810037500 batches: 0.1757
trigger times: 0
Loss after 810168600 batches: 0.0448
trigger times: 0
Loss after 810299700 batches: 0.0341
trigger times: 0
Loss after 810430800 batches: 0.0311
trigger times: 0
Loss after 810561900 batches: 0.0290
trigger times: 1
Loss after 810693000 batches: 0.0272
trigger times: 2
Loss after 810824100 batches: 0.0264
trigger times: 0
Loss after 810955200 batches: 0.0253
trigger times: 1
Loss after 811086300 batches: 0.0242
trigger times: 2
Loss after 811217400 batches: 0.0240
trigger times: 3
Loss after 811348500 batches: 0.0231
trigger times: 4
Loss after 811479600 batches: 0.0226
trigger times: 5
Loss after 811610700 batches: 0.0223
trigger times: 6
Loss after 811741800 batches: 0.0221
trigger times: 7
Loss after 811872900 batches: 0.0216
trigger times: 8
Loss after 812004000 batches: 0.0215
trigger times: 9
Loss after 812135100 batches: 0.0210
trigger times: 10
Loss after 812266200 batches: 0.0206
trigger times: 11
Loss after 812397300 batches: 0.0203
trigger times: 12
Loss after 812528400 batches: 0.0202
trigger times: 13
Loss after 812659500 batches: 0.0199
trigger times: 14
Loss after 812790600 batches: 0.0200
trigger times: 15
Loss after 812921700 batches: 0.0199
trigger times: 16
Loss after 813052800 batches: 0.0194
trigger times: 17
Loss after 813183900 batches: 0.0193
trigger times: 18
Loss after 813315000 batches: 0.0192
trigger times: 19
Loss after 813446100 batches: 0.0190
trigger times: 20
Early stopping!
Start to test process.
Loss after 813577200 batches: 0.0189
Time to train on one home:  209.39627528190613
trigger times: 0
Loss after 813708300 batches: 0.1427
trigger times: 1
Loss after 813839400 batches: 0.0558
trigger times: 0
Loss after 813970500 batches: 0.0401
trigger times: 1
Loss after 814101600 batches: 0.0347
trigger times: 0
Loss after 814232700 batches: 0.0329
trigger times: 0
Loss after 814363800 batches: 0.0289
trigger times: 1
Loss after 814494900 batches: 0.0276
trigger times: 2
Loss after 814626000 batches: 0.0265
trigger times: 0
Loss after 814757100 batches: 0.0262
trigger times: 0
Loss after 814888200 batches: 0.0247
trigger times: 0
Loss after 815019300 batches: 0.0249
trigger times: 1
Loss after 815150400 batches: 0.0256
trigger times: 2
Loss after 815281500 batches: 0.0232
trigger times: 0
Loss after 815412600 batches: 0.0226
trigger times: 1
Loss after 815543700 batches: 0.0229
trigger times: 0
Loss after 815674800 batches: 0.0220
trigger times: 1
Loss after 815805900 batches: 0.0221
trigger times: 2
Loss after 815937000 batches: 0.0218
trigger times: 3
Loss after 816068100 batches: 0.0208
trigger times: 4
Loss after 816199200 batches: 0.0214
trigger times: 5
Loss after 816330300 batches: 0.0222
trigger times: 6
Loss after 816461400 batches: 0.0216
trigger times: 7
Loss after 816592500 batches: 0.0213
trigger times: 8
Loss after 816723600 batches: 0.0206
trigger times: 9
Loss after 816854700 batches: 0.0207
trigger times: 10
Loss after 816985800 batches: 0.0200
trigger times: 11
Loss after 817116900 batches: 0.0201
trigger times: 12
Loss after 817248000 batches: 0.0199
trigger times: 13
Loss after 817379100 batches: 0.0206
trigger times: 14
Loss after 817510200 batches: 0.0212
trigger times: 15
Loss after 817641300 batches: 0.0206
trigger times: 16
Loss after 817772400 batches: 0.0189
trigger times: 17
Loss after 817903500 batches: 0.0205
trigger times: 18
Loss after 818034600 batches: 0.0197
trigger times: 19
Loss after 818165700 batches: 0.0198
trigger times: 20
Early stopping!
Start to test process.
Loss after 818296800 batches: 0.0193
Time to train on one home:  266.3831079006195
trigger times: 0
Loss after 818427900 batches: 0.0598
trigger times: 0
Loss after 818559000 batches: 0.0199
trigger times: 1
Loss after 818690100 batches: 0.0155
trigger times: 2
Loss after 818821200 batches: 0.0135
trigger times: 3
Loss after 818952300 batches: 0.0124
trigger times: 4
Loss after 819083400 batches: 0.0117
trigger times: 5
Loss after 819214500 batches: 0.0115
trigger times: 6
Loss after 819345600 batches: 0.0107
trigger times: 7
Loss after 819476700 batches: 0.0109
trigger times: 8
Loss after 819607800 batches: 0.0104
trigger times: 9
Loss after 819738900 batches: 0.0100
trigger times: 10
Loss after 819870000 batches: 0.0098
trigger times: 11
Loss after 820001100 batches: 0.0097
trigger times: 12
Loss after 820132200 batches: 0.0094
trigger times: 13
Loss after 820263300 batches: 0.0091
trigger times: 14
Loss after 820394400 batches: 0.0091
trigger times: 15
Loss after 820525500 batches: 0.0090
trigger times: 16
Loss after 820656600 batches: 0.0089
trigger times: 17
Loss after 820787700 batches: 0.0087
trigger times: 18
Loss after 820918800 batches: 0.0085
trigger times: 19
Loss after 821049900 batches: 0.0086
trigger times: 20
Early stopping!
Start to test process.
Loss after 821181000 batches: 0.0087
Time to train on one home:  167.53722310066223
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378, 0.015884290067137628, 0.015422517538360537, 0.015618174097677863, 0.015689947067699456, 0.016115858282786194, 0.015032807630775873, 0.013889531067539576, 0.014509777593618559, 0.013978963742659806, 0.013400668321247023, 0.014278996777965423]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61], [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846], [0.5624003973272111, 0.39199562780285213, 0.46338177904139005, 1.0500325960715429, 0.49807389089844895, 24.807396438856706, 1537.5851], [0.5526623295413123, 0.4025342703450545, 0.4773154352778254, 1.041540162760643, 0.4894406919679644, 24.60675965799653, 1510.9338], [0.5434306197696261, 0.41257883262882467, 0.4890240237330659, 1.0321400196734463, 0.48121224091099224, 24.384677908326562, 1485.5321], [0.5378838711314731, 0.41862279133969893, 0.4914352831936747, 1.0233749172087403, 0.4762610626477895, 24.177599220976607, 1470.2474], [0.5388939446873136, 0.4175359601598919, 0.4899945807831285, 1.0201219390637546, 0.47715138886784625, 24.100746446356553, 1472.9958], [0.5538486010498471, 0.40130114966802066, 0.4733353493101595, 1.0370228287424743, 0.49045085775236186, 24.500036022697298, 1514.0524], [0.5457354452874925, 0.4101143038770134, 0.47950876644385965, 1.028278416589283, 0.4832311695252878, 24.29344615137278, 1491.7646], [0.5428701837857565, 0.4132375522040934, 0.4832618067565206, 1.022986243672678, 0.48067262140023154, 24.16841666937729, 1483.8663], [0.5393426484531827, 0.41705296588574414, 0.4905813481431651, 1.012340178256715, 0.47754705516303547, 23.91689955812201, 1474.2175], [0.5338054365581937, 0.4230893844820084, 0.49812482537836433, 1.0087689383164333, 0.4726020537209111, 23.832527734515487, 1458.9519]]
Round_18_results:  [0.5338054365581937, 0.4230893844820084, 0.49812482537836433, 1.0087689383164333, 0.4726020537209111, 23.832527734515487, 1458.9519]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 6486 < 6487; dropping {'Training_Loss': 0.0910011388642608, 'Validation_Loss': 0.18837402926550972, 'Training_R2': 0.9083252926094864, 'Validation_R2': 0.8250073830867097, 'Training_F1': 0.8401553629268663, 'Validation_F1': 0.787028177154042, 'Training_NEP': 0.3191272333751962, 'Validation_NEP': 0.430455463049335, 'Training_NDE': 0.06882183006663695, 'Validation_NDE': 0.13935489552976638, 'Training_MAE': 10.568861071836121, 'Validation_MAE': 11.804874647133667, 'Training_MSE': 302.80524, 'Validation_MSE': 514.6337}.
trigger times: 0
Loss after 821312100 batches: 0.0910
trigger times: 0
Loss after 821443200 batches: 0.0242
trigger times: 0
Loss after 821574300 batches: 0.0178
trigger times: 1
Loss after 821705400 batches: 0.0159
trigger times: 2
Loss after 821836500 batches: 0.0145
trigger times: 3
Loss after 821967600 batches: 0.0140
trigger times: 4
Loss after 822098700 batches: 0.0132
trigger times: 5
Loss after 822229800 batches: 0.0129
trigger times: 6
Loss after 822360900 batches: 0.0122
trigger times: 7
Loss after 822492000 batches: 0.0116
trigger times: 8
Loss after 822623100 batches: 0.0118
trigger times: 9
Loss after 822754200 batches: 0.0111
trigger times: 10
Loss after 822885300 batches: 0.0111
trigger times: 11
Loss after 823016400 batches: 0.0110
trigger times: 12
Loss after 823147500 batches: 0.0106
trigger times: 13
Loss after 823278600 batches: 0.0106
trigger times: 14
Loss after 823409700 batches: 0.0104
trigger times: 15
Loss after 823540800 batches: 0.0103
trigger times: 16
Loss after 823671900 batches: 0.0102
trigger times: 17
Loss after 823803000 batches: 0.0102
trigger times: 18
Loss after 823934100 batches: 0.0098
trigger times: 19
Loss after 824065200 batches: 0.0098
trigger times: 20
Early stopping!
Start to test process.
Loss after 824196300 batches: 0.0098
Time to train on one home:  174.62740445137024
trigger times: 0
Loss after 824298900 batches: 0.1357
trigger times: 0
Loss after 824401500 batches: 0.0497
trigger times: 1
Loss after 824504100 batches: 0.0356
trigger times: 2
Loss after 824606700 batches: 0.0289
trigger times: 3
Loss after 824709300 batches: 0.0265
trigger times: 4
Loss after 824811900 batches: 0.0264
trigger times: 5
Loss after 824914500 batches: 0.0247
trigger times: 6
Loss after 825017100 batches: 0.0233
trigger times: 7
Loss after 825119700 batches: 0.0245
trigger times: 0
Loss after 825222300 batches: 0.0245
trigger times: 1
Loss after 825324900 batches: 0.0215
trigger times: 2
Loss after 825427500 batches: 0.0212
trigger times: 3
Loss after 825530100 batches: 0.0206
trigger times: 4
Loss after 825632700 batches: 0.0200
trigger times: 5
Loss after 825735300 batches: 0.0199
trigger times: 6
Loss after 825837900 batches: 0.0190
trigger times: 7
Loss after 825940500 batches: 0.0193
trigger times: 8
Loss after 826043100 batches: 0.0180
trigger times: 9
Loss after 826145700 batches: 0.0180
trigger times: 10
Loss after 826248300 batches: 0.0187
trigger times: 11
Loss after 826350900 batches: 0.0206
trigger times: 0
Loss after 826453500 batches: 0.0184
trigger times: 0
Loss after 826556100 batches: 0.0183
trigger times: 1
Loss after 826658700 batches: 0.0190
trigger times: 2
Loss after 826761300 batches: 0.0182
trigger times: 3
Loss after 826863900 batches: 0.0178
trigger times: 4
Loss after 826966500 batches: 0.0179
trigger times: 5
Loss after 827069100 batches: 0.0177
trigger times: 6
Loss after 827171700 batches: 0.0170
trigger times: 7
Loss after 827274300 batches: 0.0169
trigger times: 8
Loss after 827376900 batches: 0.0159
trigger times: 9
Loss after 827479500 batches: 0.0167
trigger times: 10
Loss after 827582100 batches: 0.0163
trigger times: 0
Loss after 827684700 batches: 0.0163
trigger times: 1
Loss after 827787300 batches: 0.0166
trigger times: 2
Loss after 827889900 batches: 0.0172
trigger times: 3
Loss after 827992500 batches: 0.0188
trigger times: 4
Loss after 828095100 batches: 0.0176
trigger times: 5
Loss after 828197700 batches: 0.0162
trigger times: 6
Loss after 828300300 batches: 0.0157
trigger times: 7
Loss after 828402900 batches: 0.0157
trigger times: 8
Loss after 828505500 batches: 0.0170
trigger times: 9
Loss after 828608100 batches: 0.0175
trigger times: 10
Loss after 828710700 batches: 0.0150
trigger times: 0
Loss after 828813300 batches: 0.0151
trigger times: 1
Loss after 828915900 batches: 0.0148
trigger times: 2
Loss after 829018500 batches: 0.0150
trigger times: 3
Loss after 829121100 batches: 0.0151
trigger times: 4
Loss after 829223700 batches: 0.0156
trigger times: 5
Loss after 829326300 batches: 0.0155
trigger times: 6
Loss after 829428900 batches: 0.0157
trigger times: 7
Loss after 829531500 batches: 0.0157
trigger times: 8
Loss after 829634100 batches: 0.0147
trigger times: 9
Loss after 829736700 batches: 0.0155
trigger times: 10
Loss after 829839300 batches: 0.0153
trigger times: 0
Loss after 829941900 batches: 0.0143
trigger times: 1
Loss after 830044500 batches: 0.0150
trigger times: 2
Loss after 830147100 batches: 0.0148
trigger times: 3
Loss after 830249700 batches: 0.0143
trigger times: 4
Loss after 830352300 batches: 0.0141
trigger times: 0
Loss after 830454900 batches: 0.0145
trigger times: 1
Loss after 830557500 batches: 0.0144
trigger times: 2
Loss after 830660100 batches: 0.0155
trigger times: 3
Loss after 830762700 batches: 0.0147
trigger times: 4
Loss after 830865300 batches: 0.0140
trigger times: 5
Loss after 830967900 batches: 0.0170
trigger times: 6
Loss after 831070500 batches: 0.0146
trigger times: 7
Loss after 831173100 batches: 0.0140
trigger times: 8
Loss after 831275700 batches: 0.0139
trigger times: 9
Loss after 831378300 batches: 0.0138
trigger times: 10
Loss after 831480900 batches: 0.0140
trigger times: 11
Loss after 831583500 batches: 0.0144
trigger times: 12
Loss after 831686100 batches: 0.0145
trigger times: 13
Loss after 831788700 batches: 0.0139
trigger times: 14
Loss after 831891300 batches: 0.0138
trigger times: 15
Loss after 831993900 batches: 0.0140
trigger times: 16
Loss after 832096500 batches: 0.0139
trigger times: 17
Loss after 832199100 batches: 0.0152
trigger times: 18
Loss after 832301700 batches: 0.0165
trigger times: 19
Loss after 832404300 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 832506900 batches: 0.0140
Time to train on one home:  471.7289035320282
trigger times: 0
Loss after 832638000 batches: 0.0918
trigger times: 0
Loss after 832769100 batches: 0.0333
trigger times: 1
Loss after 832900200 batches: 0.0260
trigger times: 0
Loss after 833031300 batches: 0.0229
trigger times: 1
Loss after 833162400 batches: 0.0211
trigger times: 2
Loss after 833293500 batches: 0.0200
trigger times: 3
Loss after 833424600 batches: 0.0194
trigger times: 4
Loss after 833555700 batches: 0.0185
trigger times: 5
Loss after 833686800 batches: 0.0179
trigger times: 6
Loss after 833817900 batches: 0.0176
trigger times: 7
Loss after 833949000 batches: 0.0168
trigger times: 8
Loss after 834080100 batches: 0.0168
trigger times: 9
Loss after 834211200 batches: 0.0161
trigger times: 10
Loss after 834342300 batches: 0.0161
trigger times: 11
Loss after 834473400 batches: 0.0159
trigger times: 12
Loss after 834604500 batches: 0.0159
trigger times: 13
Loss after 834735600 batches: 0.0152
trigger times: 14
Loss after 834866700 batches: 0.0151
trigger times: 15
Loss after 834997800 batches: 0.0149
trigger times: 16
Loss after 835128900 batches: 0.0147
trigger times: 17
Loss after 835260000 batches: 0.0145
trigger times: 18
Loss after 835391100 batches: 0.0145
trigger times: 19
Loss after 835522200 batches: 0.0144
trigger times: 20
Early stopping!
Start to test process.
Loss after 835653300 batches: 0.0143
Time to train on one home:  181.76412391662598
trigger times: 0
Loss after 835784400 batches: 0.1464
trigger times: 0
Loss after 835915500 batches: 0.0411
trigger times: 1
Loss after 836046600 batches: 0.0316
trigger times: 2
Loss after 836177700 batches: 0.0287
trigger times: 3
Loss after 836308800 batches: 0.0266
trigger times: 0
Loss after 836439900 batches: 0.0253
trigger times: 0
Loss after 836571000 batches: 0.0240
trigger times: 1
Loss after 836702100 batches: 0.0233
trigger times: 2
Loss after 836833200 batches: 0.0227
trigger times: 3
Loss after 836964300 batches: 0.0222
trigger times: 0
Loss after 837095400 batches: 0.0222
trigger times: 1
Loss after 837226500 batches: 0.0221
trigger times: 2
Loss after 837357600 batches: 0.0212
trigger times: 3
Loss after 837488700 batches: 0.0209
trigger times: 4
Loss after 837619800 batches: 0.0206
trigger times: 5
Loss after 837750900 batches: 0.0202
trigger times: 6
Loss after 837882000 batches: 0.0200
trigger times: 7
Loss after 838013100 batches: 0.0198
trigger times: 8
Loss after 838144200 batches: 0.0196
trigger times: 9
Loss after 838275300 batches: 0.0193
trigger times: 10
Loss after 838406400 batches: 0.0191
trigger times: 11
Loss after 838537500 batches: 0.0192
trigger times: 12
Loss after 838668600 batches: 0.0189
trigger times: 13
Loss after 838799700 batches: 0.0186
trigger times: 14
Loss after 838930800 batches: 0.0184
trigger times: 15
Loss after 839061900 batches: 0.0182
trigger times: 16
Loss after 839193000 batches: 0.0184
trigger times: 17
Loss after 839324100 batches: 0.0180
trigger times: 18
Loss after 839455200 batches: 0.0179
trigger times: 19
Loss after 839586300 batches: 0.0177
trigger times: 20
Early stopping!
Start to test process.
Loss after 839717400 batches: 0.0177
Time to train on one home:  231.0610430240631
trigger times: 0
Loss after 839846040 batches: 0.0868
trigger times: 0
Loss after 839974680 batches: 0.0258
trigger times: 0
Loss after 840103320 batches: 0.0213
trigger times: 0
Loss after 840231960 batches: 0.0190
trigger times: 0
Loss after 840360600 batches: 0.0177
trigger times: 0
Loss after 840489240 batches: 0.0162
trigger times: 1
Loss after 840617880 batches: 0.0159
trigger times: 2
Loss after 840746520 batches: 0.0151
trigger times: 3
Loss after 840875160 batches: 0.0149
trigger times: 4
Loss after 841003800 batches: 0.0147
trigger times: 5
Loss after 841132440 batches: 0.0142
trigger times: 6
Loss after 841261080 batches: 0.0135
trigger times: 7
Loss after 841389720 batches: 0.0135
trigger times: 8
Loss after 841518360 batches: 0.0135
trigger times: 9
Loss after 841647000 batches: 0.0132
trigger times: 10
Loss after 841775640 batches: 0.0126
trigger times: 11
Loss after 841904280 batches: 0.0130
trigger times: 12
Loss after 842032920 batches: 0.0125
trigger times: 13
Loss after 842161560 batches: 0.0121
trigger times: 14
Loss after 842290200 batches: 0.0122
trigger times: 15
Loss after 842418840 batches: 0.0120
trigger times: 16
Loss after 842547480 batches: 0.0118
trigger times: 17
Loss after 842676120 batches: 0.0116
trigger times: 18
Loss after 842804760 batches: 0.0117
trigger times: 19
Loss after 842933400 batches: 0.0119
trigger times: 20
Early stopping!
Start to test process.
Loss after 843062040 batches: 0.0115
Time to train on one home:  192.63113570213318
trigger times: 0
Loss after 843193140 batches: 0.1711
trigger times: 0
Loss after 843324240 batches: 0.0434
trigger times: 0
Loss after 843455340 batches: 0.0341
trigger times: 1
Loss after 843586440 batches: 0.0308
trigger times: 2
Loss after 843717540 batches: 0.0287
trigger times: 0
Loss after 843848640 batches: 0.0271
trigger times: 1
Loss after 843979740 batches: 0.0260
trigger times: 0
Loss after 844110840 batches: 0.0251
trigger times: 1
Loss after 844241940 batches: 0.0247
trigger times: 2
Loss after 844373040 batches: 0.0240
trigger times: 0
Loss after 844504140 batches: 0.0237
trigger times: 1
Loss after 844635240 batches: 0.0230
trigger times: 2
Loss after 844766340 batches: 0.0220
trigger times: 3
Loss after 844897440 batches: 0.0219
trigger times: 4
Loss after 845028540 batches: 0.0215
trigger times: 5
Loss after 845159640 batches: 0.0211
trigger times: 6
Loss after 845290740 batches: 0.0211
trigger times: 7
Loss after 845421840 batches: 0.0208
trigger times: 0
Loss after 845552940 batches: 0.0208
trigger times: 1
Loss after 845684040 batches: 0.0206
trigger times: 2
Loss after 845815140 batches: 0.0200
trigger times: 0
Loss after 845946240 batches: 0.0199
trigger times: 1
Loss after 846077340 batches: 0.0200
trigger times: 2
Loss after 846208440 batches: 0.0197
trigger times: 3
Loss after 846339540 batches: 0.0194
trigger times: 4
Loss after 846470640 batches: 0.0190
trigger times: 5
Loss after 846601740 batches: 0.0190
trigger times: 6
Loss after 846732840 batches: 0.0189
trigger times: 7
Loss after 846863940 batches: 0.0189
trigger times: 8
Loss after 846995040 batches: 0.0187
trigger times: 9
Loss after 847126140 batches: 0.0185
trigger times: 10
Loss after 847257240 batches: 0.0184
trigger times: 11
Loss after 847388340 batches: 0.0183
trigger times: 12
Loss after 847519440 batches: 0.0180
trigger times: 13
Loss after 847650540 batches: 0.0180
trigger times: 14
Loss after 847781640 batches: 0.0178
trigger times: 15
Loss after 847912740 batches: 0.0176
trigger times: 16
Loss after 848043840 batches: 0.0177
trigger times: 17
Loss after 848174940 batches: 0.0178
trigger times: 18
Loss after 848306040 batches: 0.0175
trigger times: 19
Loss after 848437140 batches: 0.0175
trigger times: 20
Early stopping!
Start to test process.
Loss after 848568240 batches: 0.0172
Time to train on one home:  308.1111605167389
trigger times: 0
Loss after 848699340 batches: 0.1508
trigger times: 1
Loss after 848830440 batches: 0.0499
trigger times: 2
Loss after 848961540 batches: 0.0372
trigger times: 3
Loss after 849092640 batches: 0.0334
trigger times: 4
Loss after 849223740 batches: 0.0300
trigger times: 5
Loss after 849354840 batches: 0.0288
trigger times: 0
Loss after 849485940 batches: 0.0279
trigger times: 1
Loss after 849617040 batches: 0.0279
trigger times: 0
Loss after 849748140 batches: 0.0281
trigger times: 1
Loss after 849879240 batches: 0.0259
trigger times: 2
Loss after 850010340 batches: 0.0243
trigger times: 3
Loss after 850141440 batches: 0.0236
trigger times: 4
Loss after 850272540 batches: 0.0234
trigger times: 5
Loss after 850403640 batches: 0.0224
trigger times: 6
Loss after 850534740 batches: 0.0234
trigger times: 7
Loss after 850665840 batches: 0.0228
trigger times: 8
Loss after 850796940 batches: 0.0222
trigger times: 9
Loss after 850928040 batches: 0.0216
trigger times: 10
Loss after 851059140 batches: 0.0210
trigger times: 11
Loss after 851190240 batches: 0.0210
trigger times: 12
Loss after 851321340 batches: 0.0223
trigger times: 13
Loss after 851452440 batches: 0.0210
trigger times: 0
Loss after 851583540 batches: 0.0217
trigger times: 1
Loss after 851714640 batches: 0.0203
trigger times: 2
Loss after 851845740 batches: 0.0207
trigger times: 3
Loss after 851976840 batches: 0.0210
trigger times: 4
Loss after 852107940 batches: 0.0202
trigger times: 5
Loss after 852239040 batches: 0.0202
trigger times: 6
Loss after 852370140 batches: 0.0198
trigger times: 7
Loss after 852501240 batches: 0.0200
trigger times: 8
Loss after 852632340 batches: 0.0201
trigger times: 9
Loss after 852763440 batches: 0.0197
trigger times: 10
Loss after 852894540 batches: 0.0202
trigger times: 11
Loss after 853025640 batches: 0.0196
trigger times: 12
Loss after 853156740 batches: 0.0203
trigger times: 13
Loss after 853287840 batches: 0.0188
trigger times: 14
Loss after 853418940 batches: 0.0189
trigger times: 15
Loss after 853550040 batches: 0.0189
trigger times: 16
Loss after 853681140 batches: 0.0196
trigger times: 17
Loss after 853812240 batches: 0.0193
trigger times: 18
Loss after 853943340 batches: 0.0185
trigger times: 19
Loss after 854074440 batches: 0.0197
trigger times: 0
Loss after 854205540 batches: 0.0204
trigger times: 1
Loss after 854336640 batches: 0.0183
trigger times: 2
Loss after 854467740 batches: 0.0188
trigger times: 3
Loss after 854598840 batches: 0.0188
trigger times: 4
Loss after 854729940 batches: 0.0186
trigger times: 5
Loss after 854861040 batches: 0.0177
trigger times: 6
Loss after 854992140 batches: 0.0183
trigger times: 7
Loss after 855123240 batches: 0.0185
trigger times: 8
Loss after 855254340 batches: 0.0181
trigger times: 9
Loss after 855385440 batches: 0.0179
trigger times: 10
Loss after 855516540 batches: 0.0178
trigger times: 11
Loss after 855647640 batches: 0.0180
trigger times: 12
Loss after 855778740 batches: 0.0177
trigger times: 13
Loss after 855909840 batches: 0.0174
trigger times: 14
Loss after 856040940 batches: 0.0176
trigger times: 15
Loss after 856172040 batches: 0.0178
trigger times: 16
Loss after 856303140 batches: 0.0171
trigger times: 17
Loss after 856434240 batches: 0.0175
trigger times: 18
Loss after 856565340 batches: 0.0180
trigger times: 19
Loss after 856696440 batches: 0.0178
trigger times: 20
Early stopping!
Start to test process.
Loss after 856827540 batches: 0.0170
Time to train on one home:  456.8716330528259
trigger times: 0
Loss after 856958640 batches: 0.0556
trigger times: 0
Loss after 857089740 batches: 0.0184
trigger times: 1
Loss after 857220840 batches: 0.0150
trigger times: 2
Loss after 857351940 batches: 0.0133
trigger times: 3
Loss after 857483040 batches: 0.0123
trigger times: 4
Loss after 857614140 batches: 0.0115
trigger times: 5
Loss after 857745240 batches: 0.0112
trigger times: 6
Loss after 857876340 batches: 0.0109
trigger times: 7
Loss after 858007440 batches: 0.0104
trigger times: 8
Loss after 858138540 batches: 0.0102
trigger times: 9
Loss after 858269640 batches: 0.0098
trigger times: 10
Loss after 858400740 batches: 0.0096
trigger times: 11
Loss after 858531840 batches: 0.0092
trigger times: 12
Loss after 858662940 batches: 0.0093
trigger times: 13
Loss after 858794040 batches: 0.0092
trigger times: 14
Loss after 858925140 batches: 0.0090
trigger times: 15
Loss after 859056240 batches: 0.0089
trigger times: 16
Loss after 859187340 batches: 0.0087
trigger times: 17
Loss after 859318440 batches: 0.0086
trigger times: 18
Loss after 859449540 batches: 0.0086
trigger times: 19
Loss after 859580640 batches: 0.0085
trigger times: 20
Early stopping!
Start to test process.
Loss after 859711740 batches: 0.0084
Time to train on one home:  167.16403436660767
train_results:  [0.061632601527815266, 0.0737033663785427, 0.03997817480015655, 0.031572172258733905, 0.02221291167074724, 0.01950113010923024, 0.01697008983612915, 0.016192251683436378, 0.015884290067137628, 0.015422517538360537, 0.015618174097677863, 0.015689947067699456, 0.016115858282786194, 0.015032807630775873, 0.013889531067539576, 0.014509777593618559, 0.013978963742659806, 0.013400668321247023, 0.014278996777965423, 0.013729694579487764]
test_results:  [[0.8864443666405148, 0.041077961787462924, 0.22517106610121368, 1.5059967277136097, 0.7855437435668988, 35.57971247729435, 2425.0225], [0.7328704496224722, 0.2073839693314723, 0.2977390768252295, 1.049367666155007, 0.649306762313131, 24.791687231251842, 2004.4503], [0.6414049333996243, 0.3064515221821016, 0.32797787708147025, 1.0670083070035805, 0.5681511592180577, 25.208453694124803, 1753.918], [0.6389666067229377, 0.30907243089758085, 0.3220512097825379, 1.131077292124787, 0.5660041249839277, 26.722106431462006, 1747.2899], [0.602967941098743, 0.3480072737299167, 0.4134239506459629, 1.0848461867674657, 0.5341089124693488, 25.62987999706713, 1648.8274], [0.6014250848028395, 0.34969310386059627, 0.426604770211607, 1.0990123690490494, 0.532727889550801, 25.964561130966338, 1644.564], [0.5869387752479978, 0.3653592725239855, 0.44163535864686004, 1.0781866661213702, 0.5198942489744216, 25.472546434872466, 1604.9459], [0.5780817634529538, 0.37498237688589975, 0.44912218797315967, 1.0595363238192395, 0.5120110539659054, 25.031925413258186, 1580.61], [0.5695491267575158, 0.3842454682456413, 0.46206430265206744, 1.066557814585212, 0.5044227796602101, 25.197810649273645, 1557.1846], [0.5624003973272111, 0.39199562780285213, 0.46338177904139005, 1.0500325960715429, 0.49807389089844895, 24.807396438856706, 1537.5851], [0.5526623295413123, 0.4025342703450545, 0.4773154352778254, 1.041540162760643, 0.4894406919679644, 24.60675965799653, 1510.9338], [0.5434306197696261, 0.41257883262882467, 0.4890240237330659, 1.0321400196734463, 0.48121224091099224, 24.384677908326562, 1485.5321], [0.5378838711314731, 0.41862279133969893, 0.4914352831936747, 1.0233749172087403, 0.4762610626477895, 24.177599220976607, 1470.2474], [0.5388939446873136, 0.4175359601598919, 0.4899945807831285, 1.0201219390637546, 0.47715138886784625, 24.100746446356553, 1472.9958], [0.5538486010498471, 0.40130114966802066, 0.4733353493101595, 1.0370228287424743, 0.49045085775236186, 24.500036022697298, 1514.0524], [0.5457354452874925, 0.4101143038770134, 0.47950876644385965, 1.028278416589283, 0.4832311695252878, 24.29344615137278, 1491.7646], [0.5428701837857565, 0.4132375522040934, 0.4832618067565206, 1.022986243672678, 0.48067262140023154, 24.16841666937729, 1483.8663], [0.5393426484531827, 0.41705296588574414, 0.4905813481431651, 1.012340178256715, 0.47754705516303547, 23.91689955812201, 1474.2175], [0.5338054365581937, 0.4230893844820084, 0.49812482537836433, 1.0087689383164333, 0.4726020537209111, 23.832527734515487, 1458.9519], [0.5354502035511864, 0.4212794521359309, 0.49606151052259445, 1.0091032699448068, 0.47408474050261334, 23.840426439068224, 1463.5292]]
Round_19_results:  [0.5354502035511864, 0.4212794521359309, 0.49606151052259445, 1.0091032699448068, 0.47408474050261334, 23.840426439068224, 1463.5292]