LSTM(
  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))
  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))
  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))
  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))
  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))
  (linear1): Linear(in_features=23500, out_features=1024, bias=True)
  (linear2): Linear(in_features=1024, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (dropout): Dropout(p=0.2, inplace=False)
)
Window Length:  499
trigger times: 0
Loss after 102600 batches: 1.0069
trigger times: 0
Loss after 205200 batches: 0.8489
trigger times: 0
Loss after 307800 batches: 0.6781
trigger times: 0
Loss after 410400 batches: 0.5715
trigger times: 0
Loss after 513000 batches: 0.4992
trigger times: 1
Loss after 615600 batches: 0.4466
trigger times: 2
Loss after 718200 batches: 0.3890
trigger times: 3
Loss after 820800 batches: 0.3438
trigger times: 4
Loss after 923400 batches: 0.2929
trigger times: 5
Loss after 1026000 batches: 0.2647
trigger times: 6
Loss after 1128600 batches: 0.2525
trigger times: 7
Loss after 1231200 batches: 0.2295
trigger times: 8
Loss after 1333800 batches: 0.2049
trigger times: 9
Loss after 1436400 batches: 0.1980
trigger times: 10
Loss after 1539000 batches: 0.1752
trigger times: 11
Loss after 1641600 batches: 0.1582
trigger times: 12
Loss after 1744200 batches: 0.1606
trigger times: 13
Loss after 1846800 batches: 0.1536
trigger times: 14
Loss after 1949400 batches: 0.1334
trigger times: 15
Loss after 2052000 batches: 0.1314
trigger times: 16
Loss after 2154600 batches: 0.1212
trigger times: 17
Loss after 2257200 batches: 0.1146
trigger times: 18
Loss after 2359800 batches: 0.1090
trigger times: 19
Loss after 2462400 batches: 0.1087
trigger times: 20
Early stopping!
Start to test process.
Loss after 2565000 batches: 0.0992
Time to train on one home:  169.1446237564087
trigger times: 0
Loss after 2696100 batches: 1.0064
trigger times: 0
Loss after 2827200 batches: 0.6262
trigger times: 1
Loss after 2958300 batches: 0.4676
trigger times: 0
Loss after 3089400 batches: 0.3782
trigger times: 1
Loss after 3220500 batches: 0.3221
trigger times: 2
Loss after 3351600 batches: 0.2746
trigger times: 3
Loss after 3482700 batches: 0.2291
trigger times: 4
Loss after 3613800 batches: 0.1918
trigger times: 5
Loss after 3744900 batches: 0.1627
trigger times: 6
Loss after 3876000 batches: 0.1400
trigger times: 7
Loss after 4007100 batches: 0.1257
trigger times: 8
Loss after 4138200 batches: 0.1100
trigger times: 9
Loss after 4269300 batches: 0.0977
trigger times: 10
Loss after 4400400 batches: 0.0888
trigger times: 11
Loss after 4531500 batches: 0.0830
trigger times: 12
Loss after 4662600 batches: 0.0778
trigger times: 13
Loss after 4793700 batches: 0.0721
trigger times: 14
Loss after 4924800 batches: 0.0682
trigger times: 15
Loss after 5055900 batches: 0.0636
trigger times: 16
Loss after 5187000 batches: 0.0606
trigger times: 17
Loss after 5318100 batches: 0.0580
trigger times: 18
Loss after 5449200 batches: 0.0552
trigger times: 19
Loss after 5580300 batches: 0.0532
trigger times: 20
Early stopping!
Start to test process.
Loss after 5711400 batches: 0.0512
Time to train on one home:  195.9409875869751
trigger times: 0
Loss after 5842500 batches: 0.9894
trigger times: 0
Loss after 5973600 batches: 0.8298
trigger times: 1
Loss after 6104700 batches: 0.7008
trigger times: 2
Loss after 6235800 batches: 0.6403
trigger times: 3
Loss after 6366900 batches: 0.5760
trigger times: 4
Loss after 6498000 batches: 0.4966
trigger times: 5
Loss after 6629100 batches: 0.3981
trigger times: 6
Loss after 6760200 batches: 0.3081
trigger times: 7
Loss after 6891300 batches: 0.2413
trigger times: 8
Loss after 7022400 batches: 0.1947
trigger times: 9
Loss after 7153500 batches: 0.1650
trigger times: 10
Loss after 7284600 batches: 0.1466
trigger times: 11
Loss after 7415700 batches: 0.1283
trigger times: 12
Loss after 7546800 batches: 0.1183
trigger times: 13
Loss after 7677900 batches: 0.1102
trigger times: 14
Loss after 7809000 batches: 0.1027
trigger times: 15
Loss after 7940100 batches: 0.0972
trigger times: 16
Loss after 8071200 batches: 0.0910
trigger times: 17
Loss after 8202300 batches: 0.0895
trigger times: 18
Loss after 8333400 batches: 0.0834
trigger times: 19
Loss after 8464500 batches: 0.0815
trigger times: 20
Early stopping!
Start to test process.
Loss after 8595600 batches: 0.0780
Time to train on one home:  181.44350814819336
trigger times: 0
Loss after 8724240 batches: 0.9456
trigger times: 0
Loss after 8852880 batches: 0.5082
trigger times: 0
Loss after 8981520 batches: 0.4300
trigger times: 0
Loss after 9110160 batches: 0.3752
trigger times: 0
Loss after 9238800 batches: 0.3139
trigger times: 1
Loss after 9367440 batches: 0.2631
trigger times: 2
Loss after 9496080 batches: 0.2184
trigger times: 3
Loss after 9624720 batches: 0.1802
trigger times: 4
Loss after 9753360 batches: 0.1556
trigger times: 5
Loss after 9882000 batches: 0.1374
trigger times: 6
Loss after 10010640 batches: 0.1212
trigger times: 7
Loss after 10139280 batches: 0.1086
trigger times: 8
Loss after 10267920 batches: 0.1003
trigger times: 9
Loss after 10396560 batches: 0.0929
trigger times: 10
Loss after 10525200 batches: 0.0845
trigger times: 11
Loss after 10653840 batches: 0.0791
trigger times: 12
Loss after 10782480 batches: 0.0739
trigger times: 13
Loss after 10911120 batches: 0.0706
trigger times: 14
Loss after 11039760 batches: 0.0661
trigger times: 15
Loss after 11168400 batches: 0.0636
trigger times: 16
Loss after 11297040 batches: 0.0609
trigger times: 17
Loss after 11425680 batches: 0.0576
trigger times: 18
Loss after 11554320 batches: 0.0554
trigger times: 19
Loss after 11682960 batches: 0.0543
trigger times: 20
Early stopping!
Start to test process.
Loss after 11811600 batches: 0.0520
Time to train on one home:  200.8615996837616
trigger times: 0
Loss after 11942700 batches: 0.9923
trigger times: 1
Loss after 12073800 batches: 0.8578
trigger times: 0
Loss after 12204900 batches: 0.7536
trigger times: 1
Loss after 12336000 batches: 0.6962
trigger times: 2
Loss after 12467100 batches: 0.6347
trigger times: 3
Loss after 12598200 batches: 0.5845
trigger times: 4
Loss after 12729300 batches: 0.5200
trigger times: 5
Loss after 12860400 batches: 0.4540
trigger times: 6
Loss after 12991500 batches: 0.3880
trigger times: 7
Loss after 13122600 batches: 0.3222
trigger times: 8
Loss after 13253700 batches: 0.2774
trigger times: 9
Loss after 13384800 batches: 0.2391
trigger times: 10
Loss after 13515900 batches: 0.2060
trigger times: 11
Loss after 13647000 batches: 0.1843
trigger times: 12
Loss after 13778100 batches: 0.1682
trigger times: 13
Loss after 13909200 batches: 0.1544
trigger times: 14
Loss after 14040300 batches: 0.1436
trigger times: 15
Loss after 14171400 batches: 0.1337
trigger times: 16
Loss after 14302500 batches: 0.1248
trigger times: 17
Loss after 14433600 batches: 0.1184
trigger times: 18
Loss after 14564700 batches: 0.1133
trigger times: 19
Loss after 14695800 batches: 0.1074
trigger times: 20
Early stopping!
Start to test process.
Loss after 14826900 batches: 0.1038
Time to train on one home:  184.91741371154785
trigger times: 0
Loss after 14958000 batches: 1.0178
trigger times: 0
Loss after 15089100 batches: 0.9038
trigger times: 0
Loss after 15220200 batches: 0.8064
trigger times: 0
Loss after 15351300 batches: 0.7510
trigger times: 1
Loss after 15482400 batches: 0.6661
trigger times: 2
Loss after 15613500 batches: 0.5882
trigger times: 3
Loss after 15744600 batches: 0.5154
trigger times: 4
Loss after 15875700 batches: 0.4572
trigger times: 5
Loss after 16006800 batches: 0.3997
trigger times: 6
Loss after 16137900 batches: 0.3601
trigger times: 7
Loss after 16269000 batches: 0.3235
trigger times: 8
Loss after 16400100 batches: 0.2965
trigger times: 9
Loss after 16531200 batches: 0.2733
trigger times: 10
Loss after 16662300 batches: 0.2461
trigger times: 11
Loss after 16793400 batches: 0.2314
trigger times: 12
Loss after 16924500 batches: 0.2108
trigger times: 13
Loss after 17055600 batches: 0.2000
trigger times: 14
Loss after 17186700 batches: 0.1867
trigger times: 15
Loss after 17317800 batches: 0.1702
trigger times: 16
Loss after 17448900 batches: 0.1680
trigger times: 17
Loss after 17580000 batches: 0.1600
trigger times: 18
Loss after 17711100 batches: 0.1434
trigger times: 19
Loss after 17842200 batches: 0.1390
trigger times: 20
Early stopping!
Start to test process.
Loss after 17973300 batches: 0.1351
Time to train on one home:  191.7095718383789
trigger times: 0
Loss after 18104400 batches: 0.9577
trigger times: 1
Loss after 18235500 batches: 0.4970
trigger times: 2
Loss after 18366600 batches: 0.3534
trigger times: 3
Loss after 18497700 batches: 0.2701
trigger times: 4
Loss after 18628800 batches: 0.2117
trigger times: 5
Loss after 18759900 batches: 0.1679
trigger times: 6
Loss after 18891000 batches: 0.1376
trigger times: 7
Loss after 19022100 batches: 0.1118
trigger times: 8
Loss after 19153200 batches: 0.0939
trigger times: 9
Loss after 19284300 batches: 0.0825
trigger times: 10
Loss after 19415400 batches: 0.0725
trigger times: 11
Loss after 19546500 batches: 0.0647
trigger times: 12
Loss after 19677600 batches: 0.0606
trigger times: 13
Loss after 19808700 batches: 0.0558
trigger times: 14
Loss after 19939800 batches: 0.0506
trigger times: 15
Loss after 20070900 batches: 0.0469
trigger times: 16
Loss after 20202000 batches: 0.0448
trigger times: 17
Loss after 20333100 batches: 0.0429
trigger times: 18
Loss after 20464200 batches: 0.0403
trigger times: 19
Loss after 20595300 batches: 0.0383
trigger times: 20
Early stopping!
Start to test process.
Loss after 20726400 batches: 0.0370
Time to train on one home:  172.65169668197632
trigger times: 0
Loss after 20857500 batches: 0.8942
trigger times: 0
Loss after 20988600 batches: 0.4268
trigger times: 1
Loss after 21119700 batches: 0.2927
trigger times: 0
Loss after 21250800 batches: 0.2240
trigger times: 1
Loss after 21381900 batches: 0.1915
trigger times: 2
Loss after 21513000 batches: 0.1647
trigger times: 0
Loss after 21644100 batches: 0.1438
trigger times: 0
Loss after 21775200 batches: 0.1233
trigger times: 0
Loss after 21906300 batches: 0.1104
trigger times: 0
Loss after 22037400 batches: 0.0964
trigger times: 1
Loss after 22168500 batches: 0.0878
trigger times: 2
Loss after 22299600 batches: 0.0782
trigger times: 3
Loss after 22430700 batches: 0.0705
trigger times: 0
Loss after 22561800 batches: 0.0658
trigger times: 0
Loss after 22692900 batches: 0.0599
trigger times: 1
Loss after 22824000 batches: 0.0548
trigger times: 2
Loss after 22955100 batches: 0.0506
trigger times: 3
Loss after 23086200 batches: 0.0501
trigger times: 0
Loss after 23217300 batches: 0.0465
trigger times: 1
Loss after 23348400 batches: 0.0425
trigger times: 2
Loss after 23479500 batches: 0.0393
trigger times: 0
Loss after 23610600 batches: 0.0385
trigger times: 0
Loss after 23741700 batches: 0.0370
trigger times: 1
Loss after 23872800 batches: 0.0345
trigger times: 2
Loss after 24003900 batches: 0.0335
trigger times: 0
Loss after 24135000 batches: 0.0321
trigger times: 0
Loss after 24266100 batches: 0.0309
trigger times: 1
Loss after 24397200 batches: 0.0303
trigger times: 2
Loss after 24528300 batches: 0.0289
trigger times: 0
Loss after 24659400 batches: 0.0292
trigger times: 1
Loss after 24790500 batches: 0.0275
trigger times: 0
Loss after 24921600 batches: 0.0261
trigger times: 1
Loss after 25052700 batches: 0.0251
trigger times: 0
Loss after 25183800 batches: 0.0247
trigger times: 1
Loss after 25314900 batches: 0.0248
trigger times: 2
Loss after 25446000 batches: 0.0237
trigger times: 3
Loss after 25577100 batches: 0.0231
trigger times: 4
Loss after 25708200 batches: 0.0222
trigger times: 5
Loss after 25839300 batches: 0.0218
trigger times: 0
Loss after 25970400 batches: 0.0215
trigger times: 1
Loss after 26101500 batches: 0.0213
trigger times: 2
Loss after 26232600 batches: 0.0208
trigger times: 3
Loss after 26363700 batches: 0.0206
trigger times: 0
Loss after 26494800 batches: 0.0202
trigger times: 1
Loss after 26625900 batches: 0.0201
trigger times: 2
Loss after 26757000 batches: 0.0190
trigger times: 3
Loss after 26888100 batches: 0.0196
trigger times: 4
Loss after 27019200 batches: 0.0191
trigger times: 0
Loss after 27150300 batches: 0.0184
trigger times: 1
Loss after 27281400 batches: 0.0182
trigger times: 2
Loss after 27412500 batches: 0.0179
trigger times: 3
Loss after 27543600 batches: 0.0174
trigger times: 4
Loss after 27674700 batches: 0.0176
trigger times: 0
Loss after 27805800 batches: 0.0172
trigger times: 1
Loss after 27936900 batches: 0.0168
trigger times: 2
Loss after 28068000 batches: 0.0169
trigger times: 3
Loss after 28199100 batches: 0.0164
trigger times: 4
Loss after 28330200 batches: 0.0159
trigger times: 5
Loss after 28461300 batches: 0.0163
trigger times: 6
Loss after 28592400 batches: 0.0159
trigger times: 7
Loss after 28723500 batches: 0.0156
trigger times: 8
Loss after 28854600 batches: 0.0152
trigger times: 0
Loss after 28985700 batches: 0.0151
trigger times: 1
Loss after 29116800 batches: 0.0151
trigger times: 2
Loss after 29247900 batches: 0.0148
trigger times: 0
Loss after 29379000 batches: 0.0146
trigger times: 1
Loss after 29510100 batches: 0.0143
trigger times: 2
Loss after 29641200 batches: 0.0144
trigger times: 3
Loss after 29772300 batches: 0.0139
trigger times: 4
Loss after 29903400 batches: 0.0140
trigger times: 5
Loss after 30034500 batches: 0.0139
trigger times: 6
Loss after 30165600 batches: 0.0138
trigger times: 7
Loss after 30296700 batches: 0.0139
trigger times: 8
Loss after 30427800 batches: 0.0137
trigger times: 9
Loss after 30558900 batches: 0.0133
trigger times: 0
Loss after 30690000 batches: 0.0131
trigger times: 1
Loss after 30821100 batches: 0.0132
trigger times: 2
Loss after 30952200 batches: 0.0133
trigger times: 3
Loss after 31083300 batches: 0.0133
trigger times: 4
Loss after 31214400 batches: 0.0129
trigger times: 5
Loss after 31345500 batches: 0.0126
trigger times: 6
Loss after 31476600 batches: 0.0128
trigger times: 7
Loss after 31607700 batches: 0.0126
trigger times: 8
Loss after 31738800 batches: 0.0125
trigger times: 9
Loss after 31869900 batches: 0.0124
trigger times: 10
Loss after 32001000 batches: 0.0123
trigger times: 11
Loss after 32132100 batches: 0.0121
trigger times: 12
Loss after 32263200 batches: 0.0118
trigger times: 0
Loss after 32394300 batches: 0.0117
trigger times: 1
Loss after 32525400 batches: 0.0117
trigger times: 2
Loss after 32656500 batches: 0.0117
trigger times: 3
Loss after 32787600 batches: 0.0117
trigger times: 4
Loss after 32918700 batches: 0.0117
trigger times: 5
Loss after 33049800 batches: 0.0117
trigger times: 6
Loss after 33180900 batches: 0.0114
trigger times: 7
Loss after 33312000 batches: 0.0113
trigger times: 8
Loss after 33443100 batches: 0.0116
trigger times: 9
Loss after 33574200 batches: 0.0113
trigger times: 10
Loss after 33705300 batches: 0.0110
trigger times: 11
Loss after 33836400 batches: 0.0110
trigger times: 12
Loss after 33967500 batches: 0.0109
trigger times: 13
Loss after 34098600 batches: 0.0108
trigger times: 14
Loss after 34229700 batches: 0.0111
trigger times: 15
Loss after 34360800 batches: 0.0112
trigger times: 16
Loss after 34491900 batches: 0.0106
trigger times: 17
Loss after 34623000 batches: 0.0106
trigger times: 18
Loss after 34754100 batches: 0.0106
trigger times: 19
Loss after 34885200 batches: 0.0105
trigger times: 20
Early stopping!
Start to test process.
Loss after 35016300 batches: 0.0104
Time to train on one home:  843.6171576976776
trigger times: 0
Loss after 35094900 batches: 1.0334
trigger times: 0
Loss after 35173500 batches: 0.9446
trigger times: 0
Loss after 35252100 batches: 0.8515
trigger times: 0
Loss after 35330700 batches: 0.6833
trigger times: 1
Loss after 35409300 batches: 0.5990
trigger times: 0
Loss after 35487900 batches: 0.5349
trigger times: 1
Loss after 35566500 batches: 0.4716
trigger times: 2
Loss after 35645100 batches: 0.4152
trigger times: 3
Loss after 35723700 batches: 0.3602
trigger times: 4
Loss after 35802300 batches: 0.3115
trigger times: 5
Loss after 35880900 batches: 0.2651
trigger times: 6
Loss after 35959500 batches: 0.2311
trigger times: 7
Loss after 36038100 batches: 0.1980
trigger times: 8
Loss after 36116700 batches: 0.1760
trigger times: 9
Loss after 36195300 batches: 0.1554
trigger times: 10
Loss after 36273900 batches: 0.1411
trigger times: 11
Loss after 36352500 batches: 0.1299
trigger times: 12
Loss after 36431100 batches: 0.1195
trigger times: 13
Loss after 36509700 batches: 0.1109
trigger times: 14
Loss after 36588300 batches: 0.1044
trigger times: 15
Loss after 36666900 batches: 0.0993
trigger times: 16
Loss after 36745500 batches: 0.0915
trigger times: 17
Loss after 36824100 batches: 0.0877
trigger times: 18
Loss after 36902700 batches: 0.0857
trigger times: 19
Loss after 36981300 batches: 0.0815
trigger times: 20
Early stopping!
Start to test process.
Loss after 37059900 batches: 0.0788
Time to train on one home:  142.1668152809143
trigger times: 0
Loss after 37191000 batches: 0.8603
trigger times: 0
Loss after 37322100 batches: 0.4354
trigger times: 1
Loss after 37453200 batches: 0.3247
trigger times: 0
Loss after 37584300 batches: 0.2654
trigger times: 1
Loss after 37715400 batches: 0.2195
trigger times: 2
Loss after 37846500 batches: 0.1807
trigger times: 3
Loss after 37977600 batches: 0.1505
trigger times: 4
Loss after 38108700 batches: 0.1246
trigger times: 5
Loss after 38239800 batches: 0.1061
trigger times: 6
Loss after 38370900 batches: 0.0929
trigger times: 7
Loss after 38502000 batches: 0.0800
trigger times: 8
Loss after 38633100 batches: 0.0761
trigger times: 9
Loss after 38764200 batches: 0.0687
trigger times: 10
Loss after 38895300 batches: 0.0612
trigger times: 11
Loss after 39026400 batches: 0.0567
trigger times: 12
Loss after 39157500 batches: 0.0539
trigger times: 13
Loss after 39288600 batches: 0.0512
trigger times: 14
Loss after 39419700 batches: 0.0473
trigger times: 15
Loss after 39550800 batches: 0.0444
trigger times: 16
Loss after 39681900 batches: 0.0434
trigger times: 17
Loss after 39813000 batches: 0.0416
trigger times: 18
Loss after 39944100 batches: 0.0394
trigger times: 19
Loss after 40075200 batches: 0.0383
trigger times: 20
Early stopping!
Start to test process.
Loss after 40206300 batches: 0.0358
Time to train on one home:  197.21474838256836
trigger times: 0
Loss after 40337400 batches: 0.9118
trigger times: 1
Loss after 40468500 batches: 0.5855
trigger times: 2
Loss after 40599600 batches: 0.4701
trigger times: 3
Loss after 40730700 batches: 0.3187
trigger times: 4
Loss after 40861800 batches: 0.2341
trigger times: 5
Loss after 40992900 batches: 0.1908
trigger times: 6
Loss after 41124000 batches: 0.1593
trigger times: 7
Loss after 41255100 batches: 0.1366
trigger times: 8
Loss after 41386200 batches: 0.1163
trigger times: 9
Loss after 41517300 batches: 0.1031
trigger times: 10
Loss after 41648400 batches: 0.0927
trigger times: 11
Loss after 41779500 batches: 0.0850
trigger times: 12
Loss after 41910600 batches: 0.0758
trigger times: 13
Loss after 42041700 batches: 0.0708
trigger times: 14
Loss after 42172800 batches: 0.0682
trigger times: 15
Loss after 42303900 batches: 0.0619
trigger times: 16
Loss after 42435000 batches: 0.0599
trigger times: 17
Loss after 42566100 batches: 0.0562
trigger times: 18
Loss after 42697200 batches: 0.0525
trigger times: 19
Loss after 42828300 batches: 0.0505
trigger times: 20
Early stopping!
Start to test process.
Loss after 42959400 batches: 0.0468
Time to train on one home:  174.93197202682495
trigger times: 0
Loss after 43090500 batches: 1.0234
trigger times: 0
Loss after 43221600 batches: 0.8718
trigger times: 0
Loss after 43352700 batches: 0.6597
trigger times: 0
Loss after 43483800 batches: 0.5491
trigger times: 1
Loss after 43614900 batches: 0.4328
trigger times: 2
Loss after 43746000 batches: 0.3335
trigger times: 3
Loss after 43877100 batches: 0.2533
trigger times: 4
Loss after 44008200 batches: 0.2058
trigger times: 5
Loss after 44139300 batches: 0.1701
trigger times: 6
Loss after 44270400 batches: 0.1482
trigger times: 7
Loss after 44401500 batches: 0.1298
trigger times: 8
Loss after 44532600 batches: 0.1184
trigger times: 9
Loss after 44663700 batches: 0.1096
trigger times: 10
Loss after 44794800 batches: 0.1022
trigger times: 11
Loss after 44925900 batches: 0.0947
trigger times: 12
Loss after 45057000 batches: 0.0914
trigger times: 13
Loss after 45188100 batches: 0.0844
trigger times: 14
Loss after 45319200 batches: 0.0808
trigger times: 15
Loss after 45450300 batches: 0.0766
trigger times: 16
Loss after 45581400 batches: 0.0734
trigger times: 17
Loss after 45712500 batches: 0.0710
trigger times: 18
Loss after 45843600 batches: 0.0670
trigger times: 19
Loss after 45974700 batches: 0.0671
trigger times: 20
Early stopping!
Start to test process.
Loss after 46105800 batches: 0.0652
Time to train on one home:  196.1861057281494
trigger times: 0
Loss after 46236900 batches: 0.9960
trigger times: 0
Loss after 46368000 batches: 0.8190
trigger times: 1
Loss after 46499100 batches: 0.6931
trigger times: 2
Loss after 46630200 batches: 0.5579
trigger times: 3
Loss after 46761300 batches: 0.4451
trigger times: 4
Loss after 46892400 batches: 0.3778
trigger times: 5
Loss after 47023500 batches: 0.3220
trigger times: 6
Loss after 47154600 batches: 0.2739
trigger times: 7
Loss after 47285700 batches: 0.2364
trigger times: 8
Loss after 47416800 batches: 0.2061
trigger times: 9
Loss after 47547900 batches: 0.1788
trigger times: 10
Loss after 47679000 batches: 0.1605
trigger times: 11
Loss after 47810100 batches: 0.1447
trigger times: 12
Loss after 47941200 batches: 0.1302
trigger times: 13
Loss after 48072300 batches: 0.1188
trigger times: 14
Loss after 48203400 batches: 0.1104
trigger times: 15
Loss after 48334500 batches: 0.1039
trigger times: 16
Loss after 48465600 batches: 0.0979
trigger times: 17
Loss after 48596700 batches: 0.0921
trigger times: 18
Loss after 48727800 batches: 0.0873
trigger times: 19
Loss after 48858900 batches: 0.0836
trigger times: 20
Early stopping!
Start to test process.
Loss after 48990000 batches: 0.0802
Time to train on one home:  179.48996210098267
trigger times: 0
Loss after 49121100 batches: 1.0123
trigger times: 0
Loss after 49252200 batches: 0.9005
trigger times: 1
Loss after 49383300 batches: 0.7141
trigger times: 2
Loss after 49514400 batches: 0.5006
trigger times: 3
Loss after 49645500 batches: 0.3433
trigger times: 4
Loss after 49776600 batches: 0.2562
trigger times: 5
Loss after 49907700 batches: 0.2066
trigger times: 6
Loss after 50038800 batches: 0.1765
trigger times: 7
Loss after 50169900 batches: 0.1544
trigger times: 8
Loss after 50301000 batches: 0.1386
trigger times: 9
Loss after 50432100 batches: 0.1292
trigger times: 10
Loss after 50563200 batches: 0.1192
trigger times: 11
Loss after 50694300 batches: 0.1110
trigger times: 12
Loss after 50825400 batches: 0.1024
trigger times: 13
Loss after 50956500 batches: 0.0961
trigger times: 14
Loss after 51087600 batches: 0.0958
trigger times: 15
Loss after 51218700 batches: 0.0895
trigger times: 16
Loss after 51349800 batches: 0.0863
trigger times: 17
Loss after 51480900 batches: 0.0809
trigger times: 0
Loss after 51612000 batches: 0.0802
trigger times: 1
Loss after 51743100 batches: 0.0763
trigger times: 2
Loss after 51874200 batches: 0.0735
trigger times: 0
Loss after 52005300 batches: 0.0717
trigger times: 1
Loss after 52136400 batches: 0.0675
trigger times: 2
Loss after 52267500 batches: 0.0671
trigger times: 0
Loss after 52398600 batches: 0.0655
trigger times: 1
Loss after 52529700 batches: 0.0658
trigger times: 2
Loss after 52660800 batches: 0.0623
trigger times: 0
Loss after 52791900 batches: 0.0620
trigger times: 1
Loss after 52923000 batches: 0.0591
trigger times: 0
Loss after 53054100 batches: 0.0568
trigger times: 0
Loss after 53185200 batches: 0.0571
trigger times: 1
Loss after 53316300 batches: 0.0561
trigger times: 2
Loss after 53447400 batches: 0.0544
trigger times: 3
Loss after 53578500 batches: 0.0521
trigger times: 4
Loss after 53709600 batches: 0.0515
trigger times: 5
Loss after 53840700 batches: 0.0513
trigger times: 6
Loss after 53971800 batches: 0.0513
trigger times: 7
Loss after 54102900 batches: 0.0492
trigger times: 8
Loss after 54234000 batches: 0.0485
trigger times: 9
Loss after 54365100 batches: 0.0472
trigger times: 0
Loss after 54496200 batches: 0.0452
trigger times: 1
Loss after 54627300 batches: 0.0445
trigger times: 2
Loss after 54758400 batches: 0.0451
trigger times: 3
Loss after 54889500 batches: 0.0448
trigger times: 0
Loss after 55020600 batches: 0.0434
trigger times: 1
Loss after 55151700 batches: 0.0430
trigger times: 2
Loss after 55282800 batches: 0.0416
trigger times: 3
Loss after 55413900 batches: 0.0414
trigger times: 4
Loss after 55545000 batches: 0.0406
trigger times: 0
Loss after 55676100 batches: 0.0407
trigger times: 1
Loss after 55807200 batches: 0.0396
trigger times: 2
Loss after 55938300 batches: 0.0384
trigger times: 3
Loss after 56069400 batches: 0.0377
trigger times: 4
Loss after 56200500 batches: 0.0376
trigger times: 5
Loss after 56331600 batches: 0.0378
trigger times: 6
Loss after 56462700 batches: 0.0365
trigger times: 7
Loss after 56593800 batches: 0.0361
trigger times: 8
Loss after 56724900 batches: 0.0356
trigger times: 9
Loss after 56856000 batches: 0.0354
trigger times: 0
Loss after 56987100 batches: 0.0339
trigger times: 1
Loss after 57118200 batches: 0.0344
trigger times: 2
Loss after 57249300 batches: 0.0343
trigger times: 3
Loss after 57380400 batches: 0.0339
trigger times: 0
Loss after 57511500 batches: 0.0341
trigger times: 1
Loss after 57642600 batches: 0.0328
trigger times: 2
Loss after 57773700 batches: 0.0322
trigger times: 0
Loss after 57904800 batches: 0.0323
trigger times: 1
Loss after 58035900 batches: 0.0316
trigger times: 2
Loss after 58167000 batches: 0.0305
trigger times: 3
Loss after 58298100 batches: 0.0305
trigger times: 4
Loss after 58429200 batches: 0.0306
trigger times: 5
Loss after 58560300 batches: 0.0307
trigger times: 6
Loss after 58691400 batches: 0.0297
trigger times: 7
Loss after 58822500 batches: 0.0298
trigger times: 8
Loss after 58953600 batches: 0.0290
trigger times: 9
Loss after 59084700 batches: 0.0287
trigger times: 10
Loss after 59215800 batches: 0.0286
trigger times: 11
Loss after 59346900 batches: 0.0283
trigger times: 0
Loss after 59478000 batches: 0.0281
trigger times: 1
Loss after 59609100 batches: 0.0280
trigger times: 2
Loss after 59740200 batches: 0.0281
trigger times: 3
Loss after 59871300 batches: 0.0267
trigger times: 4
Loss after 60002400 batches: 0.0266
trigger times: 5
Loss after 60133500 batches: 0.0268
trigger times: 6
Loss after 60264600 batches: 0.0267
trigger times: 7
Loss after 60395700 batches: 0.0265
trigger times: 8
Loss after 60526800 batches: 0.0263
trigger times: 9
Loss after 60657900 batches: 0.0268
trigger times: 10
Loss after 60789000 batches: 0.0265
trigger times: 0
Loss after 60920100 batches: 0.0270
trigger times: 1
Loss after 61051200 batches: 0.0262
trigger times: 2
Loss after 61182300 batches: 0.0253
trigger times: 3
Loss after 61313400 batches: 0.0256
trigger times: 4
Loss after 61444500 batches: 0.0258
trigger times: 5
Loss after 61575600 batches: 0.0252
trigger times: 6
Loss after 61706700 batches: 0.0251
trigger times: 0
Loss after 61837800 batches: 0.0249
trigger times: 0
Loss after 61968900 batches: 0.0246
trigger times: 1
Loss after 62100000 batches: 0.0242
trigger times: 2
Loss after 62231100 batches: 0.0248
trigger times: 0
Loss after 62362200 batches: 0.0241
trigger times: 1
Loss after 62493300 batches: 0.0247
trigger times: 2
Loss after 62624400 batches: 0.0240
trigger times: 3
Loss after 62755500 batches: 0.0235
trigger times: 0
Loss after 62886600 batches: 0.0235
trigger times: 0
Loss after 63017700 batches: 0.0239
trigger times: 1
Loss after 63148800 batches: 0.0239
trigger times: 2
Loss after 63279900 batches: 0.0232
trigger times: 3
Loss after 63411000 batches: 0.0234
trigger times: 4
Loss after 63542100 batches: 0.0228
trigger times: 5
Loss after 63673200 batches: 0.0228
trigger times: 6
Loss after 63804300 batches: 0.0229
trigger times: 7
Loss after 63935400 batches: 0.0225
trigger times: 8
Loss after 64066500 batches: 0.0228
trigger times: 9
Loss after 64197600 batches: 0.0231
trigger times: 10
Loss after 64328700 batches: 0.0234
trigger times: 11
Loss after 64459800 batches: 0.0224
trigger times: 12
Loss after 64590900 batches: 0.0218
trigger times: 13
Loss after 64722000 batches: 0.0220
trigger times: 14
Loss after 64853100 batches: 0.0216
trigger times: 0
Loss after 64984200 batches: 0.0217
trigger times: 1
Loss after 65115300 batches: 0.0216
trigger times: 2
Loss after 65246400 batches: 0.0215
trigger times: 3
Loss after 65377500 batches: 0.0211
trigger times: 4
Loss after 65508600 batches: 0.0212
trigger times: 5
Loss after 65639700 batches: 0.0213
trigger times: 6
Loss after 65770800 batches: 0.0212
trigger times: 7
Loss after 65901900 batches: 0.0213
trigger times: 8
Loss after 66033000 batches: 0.0207
trigger times: 9
Loss after 66164100 batches: 0.0205
trigger times: 10
Loss after 66295200 batches: 0.0205
trigger times: 11
Loss after 66426300 batches: 0.0210
trigger times: 12
Loss after 66557400 batches: 0.0212
trigger times: 13
Loss after 66688500 batches: 0.0206
trigger times: 14
Loss after 66819600 batches: 0.0202
trigger times: 15
Loss after 66950700 batches: 0.0202
trigger times: 16
Loss after 67081800 batches: 0.0204
trigger times: 17
Loss after 67212900 batches: 0.0201
trigger times: 18
Loss after 67344000 batches: 0.0196
trigger times: 19
Loss after 67475100 batches: 0.0196
trigger times: 20
Early stopping!
Start to test process.
Loss after 67606200 batches: 0.0194
Time to train on one home:  1095.6784880161285
trigger times: 0
Loss after 67737300 batches: 0.9621
trigger times: 1
Loss after 67868400 batches: 0.5532
trigger times: 2
Loss after 67999500 batches: 0.4726
trigger times: 0
Loss after 68130600 batches: 0.4042
trigger times: 0
Loss after 68261700 batches: 0.3411
trigger times: 1
Loss after 68392800 batches: 0.2834
trigger times: 0
Loss after 68523900 batches: 0.2364
trigger times: 1
Loss after 68655000 batches: 0.1911
trigger times: 2
Loss after 68786100 batches: 0.1606
trigger times: 3
Loss after 68917200 batches: 0.1352
trigger times: 4
Loss after 69048300 batches: 0.1171
trigger times: 5
Loss after 69179400 batches: 0.1047
trigger times: 6
Loss after 69310500 batches: 0.0957
trigger times: 7
Loss after 69441600 batches: 0.0867
trigger times: 8
Loss after 69572700 batches: 0.0801
trigger times: 9
Loss after 69703800 batches: 0.0760
trigger times: 10
Loss after 69834900 batches: 0.0712
trigger times: 11
Loss after 69966000 batches: 0.0670
trigger times: 12
Loss after 70097100 batches: 0.0637
trigger times: 13
Loss after 70228200 batches: 0.0611
trigger times: 14
Loss after 70359300 batches: 0.0586
trigger times: 15
Loss after 70490400 batches: 0.0569
trigger times: 16
Loss after 70621500 batches: 0.0552
trigger times: 17
Loss after 70752600 batches: 0.0543
trigger times: 18
Loss after 70883700 batches: 0.0514
trigger times: 19
Loss after 71014800 batches: 0.0497
trigger times: 20
Early stopping!
Start to test process.
Loss after 71145900 batches: 0.0490
Time to train on one home:  220.09781289100647
trigger times: 0
Loss after 71277000 batches: 0.9980
trigger times: 1
Loss after 71408100 batches: 0.9097
trigger times: 2
Loss after 71539200 batches: 0.7981
trigger times: 3
Loss after 71670300 batches: 0.7346
trigger times: 4
Loss after 71801400 batches: 0.6507
trigger times: 5
Loss after 71932500 batches: 0.5468
trigger times: 6
Loss after 72063600 batches: 0.4366
trigger times: 7
Loss after 72194700 batches: 0.3444
trigger times: 8
Loss after 72325800 batches: 0.2814
trigger times: 9
Loss after 72456900 batches: 0.2322
trigger times: 10
Loss after 72588000 batches: 0.1930
trigger times: 11
Loss after 72719100 batches: 0.1717
trigger times: 12
Loss after 72850200 batches: 0.1530
trigger times: 13
Loss after 72981300 batches: 0.1371
trigger times: 14
Loss after 73112400 batches: 0.1265
trigger times: 15
Loss after 73243500 batches: 0.1198
trigger times: 16
Loss after 73374600 batches: 0.1123
trigger times: 17
Loss after 73505700 batches: 0.1049
trigger times: 18
Loss after 73636800 batches: 0.1020
trigger times: 19
Loss after 73767900 batches: 0.0972
trigger times: 20
Early stopping!
Start to test process.
Loss after 73899000 batches: 0.0910
Time to train on one home:  173.81619310379028
trigger times: 0
Loss after 73992960 batches: 0.9820
trigger times: 1
Loss after 74086920 batches: 0.8911
trigger times: 2
Loss after 74180880 batches: 0.7607
trigger times: 3
Loss after 74274840 batches: 0.6567
trigger times: 4
Loss after 74368800 batches: 0.5633
trigger times: 5
Loss after 74462760 batches: 0.4696
trigger times: 6
Loss after 74556720 batches: 0.3906
trigger times: 7
Loss after 74650680 batches: 0.3189
trigger times: 8
Loss after 74744640 batches: 0.2714
trigger times: 9
Loss after 74838600 batches: 0.2216
trigger times: 10
Loss after 74932560 batches: 0.1930
trigger times: 11
Loss after 75026520 batches: 0.1739
trigger times: 12
Loss after 75120480 batches: 0.1514
trigger times: 13
Loss after 75214440 batches: 0.1362
trigger times: 14
Loss after 75308400 batches: 0.1277
trigger times: 15
Loss after 75402360 batches: 0.1178
trigger times: 16
Loss after 75496320 batches: 0.1126
trigger times: 17
Loss after 75590280 batches: 0.1063
trigger times: 18
Loss after 75684240 batches: 0.1002
trigger times: 19
Loss after 75778200 batches: 0.0932
trigger times: 20
Early stopping!
Start to test process.
Loss after 75872160 batches: 0.0892
Time to train on one home:  134.06803607940674
trigger times: 0
Loss after 76003260 batches: 0.4010
trigger times: 1
Loss after 76134360 batches: 0.1087
trigger times: 0
Loss after 76265460 batches: 0.0656
trigger times: 0
Loss after 76396560 batches: 0.0471
trigger times: 0
Loss after 76527660 batches: 0.0379
trigger times: 1
Loss after 76658760 batches: 0.0318
trigger times: 2
Loss after 76789860 batches: 0.0279
trigger times: 0
Loss after 76920960 batches: 0.0248
trigger times: 0
Loss after 77052060 batches: 0.0227
trigger times: 0
Loss after 77183160 batches: 0.0202
trigger times: 1
Loss after 77314260 batches: 0.0185
trigger times: 2
Loss after 77445360 batches: 0.0170
trigger times: 3
Loss after 77576460 batches: 0.0159
trigger times: 4
Loss after 77707560 batches: 0.0145
trigger times: 5
Loss after 77838660 batches: 0.0137
trigger times: 6
Loss after 77969760 batches: 0.0127
trigger times: 7
Loss after 78100860 batches: 0.0116
trigger times: 8
Loss after 78231960 batches: 0.0111
trigger times: 0
Loss after 78363060 batches: 0.0103
trigger times: 0
Loss after 78494160 batches: 0.0098
trigger times: 1
Loss after 78625260 batches: 0.0095
trigger times: 2
Loss after 78756360 batches: 0.0086
trigger times: 3
Loss after 78887460 batches: 0.0080
trigger times: 4
Loss after 79018560 batches: 0.0081
trigger times: 5
Loss after 79149660 batches: 0.0076
trigger times: 6
Loss after 79280760 batches: 0.0073
trigger times: 7
Loss after 79411860 batches: 0.0070
trigger times: 8
Loss after 79542960 batches: 0.0069
trigger times: 9
Loss after 79674060 batches: 0.0064
trigger times: 10
Loss after 79805160 batches: 0.0064
trigger times: 11
Loss after 79936260 batches: 0.0062
trigger times: 12
Loss after 80067360 batches: 0.0060
trigger times: 13
Loss after 80198460 batches: 0.0057
trigger times: 14
Loss after 80329560 batches: 0.0056
trigger times: 15
Loss after 80460660 batches: 0.0054
trigger times: 16
Loss after 80591760 batches: 0.0053
trigger times: 17
Loss after 80722860 batches: 0.0051
trigger times: 18
Loss after 80853960 batches: 0.0051
trigger times: 19
Loss after 80985060 batches: 0.0050
trigger times: 20
Early stopping!
Start to test process.
Loss after 81116160 batches: 0.0048
Time to train on one home:  318.4658341407776
trigger times: 0
Loss after 81247260 batches: 0.9608
trigger times: 0
Loss after 81378360 batches: 0.4130
trigger times: 0
Loss after 81509460 batches: 0.3287
trigger times: 0
Loss after 81640560 batches: 0.2845
trigger times: 0
Loss after 81771660 batches: 0.2416
trigger times: 1
Loss after 81902760 batches: 0.2030
trigger times: 2
Loss after 82033860 batches: 0.1721
trigger times: 3
Loss after 82164960 batches: 0.1490
trigger times: 4
Loss after 82296060 batches: 0.1306
trigger times: 5
Loss after 82427160 batches: 0.1173
trigger times: 6
Loss after 82558260 batches: 0.1027
trigger times: 7
Loss after 82689360 batches: 0.0936
trigger times: 8
Loss after 82820460 batches: 0.0875
trigger times: 9
Loss after 82951560 batches: 0.0804
trigger times: 10
Loss after 83082660 batches: 0.0743
trigger times: 11
Loss after 83213760 batches: 0.0714
trigger times: 12
Loss after 83344860 batches: 0.0659
trigger times: 13
Loss after 83475960 batches: 0.0617
trigger times: 14
Loss after 83607060 batches: 0.0609
trigger times: 15
Loss after 83738160 batches: 0.0547
trigger times: 16
Loss after 83869260 batches: 0.0527
trigger times: 17
Loss after 84000360 batches: 0.0498
trigger times: 18
Loss after 84131460 batches: 0.0479
trigger times: 19
Loss after 84262560 batches: 0.0459
trigger times: 20
Early stopping!
Start to test process.
Loss after 84393660 batches: 0.0440
Time to train on one home:  204.84896612167358
trigger times: 0
Loss after 84524760 batches: 1.0078
trigger times: 1
Loss after 84655860 batches: 0.9469
trigger times: 0
Loss after 84786960 batches: 0.7526
trigger times: 1
Loss after 84918060 batches: 0.6149
trigger times: 2
Loss after 85049160 batches: 0.4753
trigger times: 3
Loss after 85180260 batches: 0.3886
trigger times: 4
Loss after 85311360 batches: 0.3156
trigger times: 5
Loss after 85442460 batches: 0.2588
trigger times: 6
Loss after 85573560 batches: 0.2155
trigger times: 7
Loss after 85704660 batches: 0.1861
trigger times: 8
Loss after 85835760 batches: 0.1613
trigger times: 9
Loss after 85966860 batches: 0.1435
trigger times: 10
Loss after 86097960 batches: 0.1340
trigger times: 11
Loss after 86229060 batches: 0.1246
trigger times: 12
Loss after 86360160 batches: 0.1144
trigger times: 13
Loss after 86491260 batches: 0.1088
trigger times: 14
Loss after 86622360 batches: 0.1069
trigger times: 15
Loss after 86753460 batches: 0.0995
trigger times: 16
Loss after 86884560 batches: 0.0953
trigger times: 17
Loss after 87015660 batches: 0.0926
trigger times: 18
Loss after 87146760 batches: 0.0900
trigger times: 19
Loss after 87277860 batches: 0.0873
trigger times: 20
Early stopping!
Start to test process.
Loss after 87408960 batches: 0.0850
Time to train on one home:  189.70681834220886
train_results:  [0.06280048316701156]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435]]
Round_0_results:  [0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435]
trigger times: 0
Loss after 87511560 batches: 0.8058
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 690 < 691; dropping {'Training_Loss': 0.8058426635605949, 'Validation_Loss': 0.8122531440522935, 'Training_R2': 0.18320644381173434, 'Validation_R2': 0.2431672538742241, 'Training_F1': 0.5876390132259467, 'Validation_F1': 0.37972357197320167, 'Training_NEP': 0.8254927949164458, 'Validation_NEP': 1.176480154293318, 'Training_NDE': 0.5501841917464807, 'Validation_NDE': 0.6027017032502794, 'Training_MAE': 54.17860470297741, 'Validation_MAE': 32.26396674789408, 'Training_MSE': 7260.626, 'Validation_MSE': 2225.7607}.
trigger times: 0
Loss after 87614160 batches: 0.5790
trigger times: 1
Loss after 87716760 batches: 0.4813
trigger times: 2
Loss after 87819360 batches: 0.4124
trigger times: 3
Loss after 87921960 batches: 0.3706
trigger times: 4
Loss after 88024560 batches: 0.3214
trigger times: 5
Loss after 88127160 batches: 0.2912
trigger times: 6
Loss after 88229760 batches: 0.2755
trigger times: 7
Loss after 88332360 batches: 0.2841
trigger times: 8
Loss after 88434960 batches: 0.2343
trigger times: 9
Loss after 88537560 batches: 0.2177
trigger times: 10
Loss after 88640160 batches: 0.2109
trigger times: 11
Loss after 88742760 batches: 0.1922
trigger times: 12
Loss after 88845360 batches: 0.1822
trigger times: 13
Loss after 88947960 batches: 0.1718
trigger times: 14
Loss after 89050560 batches: 0.1687
trigger times: 15
Loss after 89153160 batches: 0.1691
trigger times: 16
Loss after 89255760 batches: 0.1718
trigger times: 17
Loss after 89358360 batches: 0.1530
trigger times: 18
Loss after 89460960 batches: 0.1448
trigger times: 19
Loss after 89563560 batches: 0.1432
trigger times: 20
Early stopping!
Start to test process.
Loss after 89666160 batches: 0.1348
Time to train on one home:  151.80751609802246
trigger times: 0
Loss after 89797260 batches: 0.6200
trigger times: 1
Loss after 89928360 batches: 0.3833
trigger times: 2
Loss after 90059460 batches: 0.3079
trigger times: 3
Loss after 90190560 batches: 0.2579
trigger times: 4
Loss after 90321660 batches: 0.2180
trigger times: 5
Loss after 90452760 batches: 0.1915
trigger times: 6
Loss after 90583860 batches: 0.1716
trigger times: 7
Loss after 90714960 batches: 0.1582
trigger times: 8
Loss after 90846060 batches: 0.1435
trigger times: 9
Loss after 90977160 batches: 0.1331
trigger times: 10
Loss after 91108260 batches: 0.1240
trigger times: 11
Loss after 91239360 batches: 0.1164
trigger times: 12
Loss after 91370460 batches: 0.1111
trigger times: 13
Loss after 91501560 batches: 0.1059
trigger times: 14
Loss after 91632660 batches: 0.1004
trigger times: 15
Loss after 91763760 batches: 0.0964
trigger times: 16
Loss after 91894860 batches: 0.0925
trigger times: 17
Loss after 92025960 batches: 0.0894
trigger times: 18
Loss after 92157060 batches: 0.0863
trigger times: 19
Loss after 92288160 batches: 0.0837
trigger times: 20
Early stopping!
Start to test process.
Loss after 92419260 batches: 0.0813
Time to train on one home:  174.89976811408997
trigger times: 0
Loss after 92550360 batches: 0.8390
trigger times: 1
Loss after 92681460 batches: 0.6710
trigger times: 2
Loss after 92812560 batches: 0.5853
trigger times: 3
Loss after 92943660 batches: 0.4982
trigger times: 4
Loss after 93074760 batches: 0.4195
trigger times: 5
Loss after 93205860 batches: 0.3611
trigger times: 6
Loss after 93336960 batches: 0.3175
trigger times: 7
Loss after 93468060 batches: 0.2853
trigger times: 8
Loss after 93599160 batches: 0.2622
trigger times: 9
Loss after 93730260 batches: 0.2386
trigger times: 10
Loss after 93861360 batches: 0.2221
trigger times: 11
Loss after 93992460 batches: 0.2065
trigger times: 12
Loss after 94123560 batches: 0.1927
trigger times: 13
Loss after 94254660 batches: 0.1814
trigger times: 14
Loss after 94385760 batches: 0.1707
trigger times: 15
Loss after 94516860 batches: 0.1638
trigger times: 16
Loss after 94647960 batches: 0.1568
trigger times: 17
Loss after 94779060 batches: 0.1502
trigger times: 18
Loss after 94910160 batches: 0.1444
trigger times: 19
Loss after 95041260 batches: 0.1405
trigger times: 20
Early stopping!
Start to test process.
Loss after 95172360 batches: 0.1352
Time to train on one home:  174.53426218032837
trigger times: 0
Loss after 95301000 batches: 0.5882
trigger times: 0
Loss after 95429640 batches: 0.4059
trigger times: 0
Loss after 95558280 batches: 0.3564
trigger times: 1
Loss after 95686920 batches: 0.3073
trigger times: 2
Loss after 95815560 batches: 0.2638
trigger times: 3
Loss after 95944200 batches: 0.2291
trigger times: 4
Loss after 96072840 batches: 0.2052
trigger times: 5
Loss after 96201480 batches: 0.1872
trigger times: 6
Loss after 96330120 batches: 0.1755
trigger times: 7
Loss after 96458760 batches: 0.1651
trigger times: 8
Loss after 96587400 batches: 0.1539
trigger times: 9
Loss after 96716040 batches: 0.1434
trigger times: 10
Loss after 96844680 batches: 0.1356
trigger times: 11
Loss after 96973320 batches: 0.1278
trigger times: 12
Loss after 97101960 batches: 0.1216
trigger times: 13
Loss after 97230600 batches: 0.1142
trigger times: 14
Loss after 97359240 batches: 0.1121
trigger times: 15
Loss after 97487880 batches: 0.1045
trigger times: 16
Loss after 97616520 batches: 0.1016
trigger times: 17
Loss after 97745160 batches: 0.0971
trigger times: 18
Loss after 97873800 batches: 0.0936
trigger times: 19
Loss after 98002440 batches: 0.0910
trigger times: 20
Early stopping!
Start to test process.
Loss after 98131080 batches: 0.0874
Time to train on one home:  186.5601282119751
trigger times: 0
Loss after 98262180 batches: 0.8325
trigger times: 1
Loss after 98393280 batches: 0.6599
trigger times: 2
Loss after 98524380 batches: 0.5494
trigger times: 3
Loss after 98655480 batches: 0.4505
trigger times: 4
Loss after 98786580 batches: 0.3633
trigger times: 5
Loss after 98917680 batches: 0.3026
trigger times: 6
Loss after 99048780 batches: 0.2587
trigger times: 7
Loss after 99179880 batches: 0.2258
trigger times: 8
Loss after 99310980 batches: 0.2050
trigger times: 9
Loss after 99442080 batches: 0.1864
trigger times: 10
Loss after 99573180 batches: 0.1709
trigger times: 11
Loss after 99704280 batches: 0.1598
trigger times: 12
Loss after 99835380 batches: 0.1477
trigger times: 13
Loss after 99966480 batches: 0.1399
trigger times: 14
Loss after 100097580 batches: 0.1329
trigger times: 15
Loss after 100228680 batches: 0.1270
trigger times: 16
Loss after 100359780 batches: 0.1221
trigger times: 17
Loss after 100490880 batches: 0.1165
trigger times: 18
Loss after 100621980 batches: 0.1114
trigger times: 19
Loss after 100753080 batches: 0.1095
trigger times: 20
Early stopping!
Start to test process.
Loss after 100884180 batches: 0.1045
Time to train on one home:  171.65540647506714
trigger times: 0
Loss after 101015280 batches: 0.8758
trigger times: 0
Loss after 101146380 batches: 0.7403
trigger times: 1
Loss after 101277480 batches: 0.6280
trigger times: 2
Loss after 101408580 batches: 0.5665
trigger times: 3
Loss after 101539680 batches: 0.5104
trigger times: 4
Loss after 101670780 batches: 0.4523
trigger times: 5
Loss after 101801880 batches: 0.4161
trigger times: 6
Loss after 101932980 batches: 0.3712
trigger times: 7
Loss after 102064080 batches: 0.3432
trigger times: 8
Loss after 102195180 batches: 0.3224
trigger times: 9
Loss after 102326280 batches: 0.3013
trigger times: 10
Loss after 102457380 batches: 0.2793
trigger times: 11
Loss after 102588480 batches: 0.2624
trigger times: 12
Loss after 102719580 batches: 0.2434
trigger times: 13
Loss after 102850680 batches: 0.2294
trigger times: 14
Loss after 102981780 batches: 0.2173
trigger times: 15
Loss after 103112880 batches: 0.2015
trigger times: 16
Loss after 103243980 batches: 0.1924
trigger times: 17
Loss after 103375080 batches: 0.1859
trigger times: 18
Loss after 103506180 batches: 0.1787
trigger times: 19
Loss after 103637280 batches: 0.1696
trigger times: 20
Early stopping!
Start to test process.
Loss after 103768380 batches: 0.1651
Time to train on one home:  182.70853877067566
trigger times: 0
Loss after 103899480 batches: 0.5406
trigger times: 0
Loss after 104030580 batches: 0.2694
trigger times: 1
Loss after 104161680 batches: 0.2040
trigger times: 2
Loss after 104292780 batches: 0.1662
trigger times: 3
Loss after 104423880 batches: 0.1408
trigger times: 4
Loss after 104554980 batches: 0.1214
trigger times: 5
Loss after 104686080 batches: 0.1082
trigger times: 6
Loss after 104817180 batches: 0.0991
trigger times: 7
Loss after 104948280 batches: 0.0902
trigger times: 8
Loss after 105079380 batches: 0.0844
trigger times: 9
Loss after 105210480 batches: 0.0791
trigger times: 10
Loss after 105341580 batches: 0.0748
trigger times: 11
Loss after 105472680 batches: 0.0711
trigger times: 12
Loss after 105603780 batches: 0.0665
trigger times: 13
Loss after 105734880 batches: 0.0645
trigger times: 14
Loss after 105865980 batches: 0.0613
trigger times: 15
Loss after 105997080 batches: 0.0588
trigger times: 16
Loss after 106128180 batches: 0.0568
trigger times: 17
Loss after 106259280 batches: 0.0548
trigger times: 18
Loss after 106390380 batches: 0.0526
trigger times: 19
Loss after 106521480 batches: 0.0518
trigger times: 20
Early stopping!
Start to test process.
Loss after 106652580 batches: 0.0493
Time to train on one home:  182.64589023590088
trigger times: 0
Loss after 106783680 batches: 0.5064
trigger times: 1
Loss after 106914780 batches: 0.2582
trigger times: 0
Loss after 107045880 batches: 0.2024
trigger times: 0
Loss after 107176980 batches: 0.1797
trigger times: 0
Loss after 107308080 batches: 0.1617
trigger times: 1
Loss after 107439180 batches: 0.1509
trigger times: 2
Loss after 107570280 batches: 0.1389
trigger times: 3
Loss after 107701380 batches: 0.1288
trigger times: 4
Loss after 107832480 batches: 0.1217
trigger times: 0
Loss after 107963580 batches: 0.1164
trigger times: 1
Loss after 108094680 batches: 0.1089
trigger times: 0
Loss after 108225780 batches: 0.1036
trigger times: 0
Loss after 108356880 batches: 0.0989
trigger times: 1
Loss after 108487980 batches: 0.0951
trigger times: 0
Loss after 108619080 batches: 0.0912
trigger times: 1
Loss after 108750180 batches: 0.0871
trigger times: 2
Loss after 108881280 batches: 0.0833
trigger times: 3
Loss after 109012380 batches: 0.0792
trigger times: 4
Loss after 109143480 batches: 0.0765
trigger times: 5
Loss after 109274580 batches: 0.0748
trigger times: 6
Loss after 109405680 batches: 0.0717
trigger times: 7
Loss after 109536780 batches: 0.0690
trigger times: 8
Loss after 109667880 batches: 0.0666
trigger times: 9
Loss after 109798980 batches: 0.0627
trigger times: 10
Loss after 109930080 batches: 0.0614
trigger times: 11
Loss after 110061180 batches: 0.0592
trigger times: 12
Loss after 110192280 batches: 0.0573
trigger times: 13
Loss after 110323380 batches: 0.0546
trigger times: 14
Loss after 110454480 batches: 0.0533
trigger times: 15
Loss after 110585580 batches: 0.0526
trigger times: 0
Loss after 110716680 batches: 0.0505
trigger times: 1
Loss after 110847780 batches: 0.0491
trigger times: 2
Loss after 110978880 batches: 0.0474
trigger times: 3
Loss after 111109980 batches: 0.0464
trigger times: 4
Loss after 111241080 batches: 0.0451
trigger times: 5
Loss after 111372180 batches: 0.0432
trigger times: 6
Loss after 111503280 batches: 0.0420
trigger times: 7
Loss after 111634380 batches: 0.0413
trigger times: 8
Loss after 111765480 batches: 0.0396
trigger times: 0
Loss after 111896580 batches: 0.0397
trigger times: 1
Loss after 112027680 batches: 0.0382
trigger times: 2
Loss after 112158780 batches: 0.0368
trigger times: 3
Loss after 112289880 batches: 0.0355
trigger times: 4
Loss after 112420980 batches: 0.0345
trigger times: 5
Loss after 112552080 batches: 0.0336
trigger times: 6
Loss after 112683180 batches: 0.0334
trigger times: 7
Loss after 112814280 batches: 0.0330
trigger times: 8
Loss after 112945380 batches: 0.0319
trigger times: 9
Loss after 113076480 batches: 0.0305
trigger times: 10
Loss after 113207580 batches: 0.0301
trigger times: 0
Loss after 113338680 batches: 0.0295
trigger times: 1
Loss after 113469780 batches: 0.0292
trigger times: 2
Loss after 113600880 batches: 0.0289
trigger times: 3
Loss after 113731980 batches: 0.0280
trigger times: 4
Loss after 113863080 batches: 0.0276
trigger times: 5
Loss after 113994180 batches: 0.0269
trigger times: 6
Loss after 114125280 batches: 0.0266
trigger times: 7
Loss after 114256380 batches: 0.0272
trigger times: 8
Loss after 114387480 batches: 0.0260
trigger times: 9
Loss after 114518580 batches: 0.0259
trigger times: 10
Loss after 114649680 batches: 0.0252
trigger times: 11
Loss after 114780780 batches: 0.0249
trigger times: 12
Loss after 114911880 batches: 0.0241
trigger times: 13
Loss after 115042980 batches: 0.0237
trigger times: 14
Loss after 115174080 batches: 0.0234
trigger times: 15
Loss after 115305180 batches: 0.0234
trigger times: 16
Loss after 115436280 batches: 0.0236
trigger times: 17
Loss after 115567380 batches: 0.0232
trigger times: 18
Loss after 115698480 batches: 0.0224
trigger times: 19
Loss after 115829580 batches: 0.0218
trigger times: 20
Early stopping!
Start to test process.
Loss after 115960680 batches: 0.0221
Time to train on one home:  555.4945833683014
trigger times: 0
Loss after 116039280 batches: 0.8921
trigger times: 0
Loss after 116117880 batches: 0.7236
trigger times: 0
Loss after 116196480 batches: 0.6332
trigger times: 1
Loss after 116275080 batches: 0.5529
trigger times: 2
Loss after 116353680 batches: 0.4888
trigger times: 3
Loss after 116432280 batches: 0.4250
trigger times: 4
Loss after 116510880 batches: 0.3793
trigger times: 5
Loss after 116589480 batches: 0.3432
trigger times: 6
Loss after 116668080 batches: 0.3194
trigger times: 7
Loss after 116746680 batches: 0.2891
trigger times: 8
Loss after 116825280 batches: 0.2746
trigger times: 9
Loss after 116903880 batches: 0.2548
trigger times: 10
Loss after 116982480 batches: 0.2421
trigger times: 11
Loss after 117061080 batches: 0.2255
trigger times: 12
Loss after 117139680 batches: 0.2152
trigger times: 13
Loss after 117218280 batches: 0.2097
trigger times: 14
Loss after 117296880 batches: 0.2003
trigger times: 15
Loss after 117375480 batches: 0.1912
trigger times: 16
Loss after 117454080 batches: 0.1786
trigger times: 17
Loss after 117532680 batches: 0.1705
trigger times: 18
Loss after 117611280 batches: 0.1621
trigger times: 19
Loss after 117689880 batches: 0.1598
trigger times: 20
Early stopping!
Start to test process.
Loss after 117768480 batches: 0.1514
Time to train on one home:  127.36067581176758
trigger times: 0
Loss after 117899580 batches: 0.5297
trigger times: 1
Loss after 118030680 batches: 0.3019
trigger times: 2
Loss after 118161780 batches: 0.2381
trigger times: 3
Loss after 118292880 batches: 0.1939
trigger times: 4
Loss after 118423980 batches: 0.1632
trigger times: 5
Loss after 118555080 batches: 0.1464
trigger times: 6
Loss after 118686180 batches: 0.1299
trigger times: 7
Loss after 118817280 batches: 0.1197
trigger times: 8
Loss after 118948380 batches: 0.1105
trigger times: 9
Loss after 119079480 batches: 0.1056
trigger times: 10
Loss after 119210580 batches: 0.0990
trigger times: 11
Loss after 119341680 batches: 0.0916
trigger times: 12
Loss after 119472780 batches: 0.0891
trigger times: 13
Loss after 119603880 batches: 0.0836
trigger times: 14
Loss after 119734980 batches: 0.0799
trigger times: 15
Loss after 119866080 batches: 0.0767
trigger times: 16
Loss after 119997180 batches: 0.0713
trigger times: 17
Loss after 120128280 batches: 0.0680
trigger times: 18
Loss after 120259380 batches: 0.0658
trigger times: 19
Loss after 120390480 batches: 0.0630
trigger times: 20
Early stopping!
Start to test process.
Loss after 120521580 batches: 0.0606
Time to train on one home:  174.3786597251892
trigger times: 0
Loss after 120652680 batches: 0.5806
trigger times: 0
Loss after 120783780 batches: 0.2978
trigger times: 1
Loss after 120914880 batches: 0.2176
trigger times: 2
Loss after 121045980 batches: 0.1782
trigger times: 3
Loss after 121177080 batches: 0.1508
trigger times: 4
Loss after 121308180 batches: 0.1322
trigger times: 5
Loss after 121439280 batches: 0.1197
trigger times: 6
Loss after 121570380 batches: 0.1086
trigger times: 7
Loss after 121701480 batches: 0.1025
trigger times: 8
Loss after 121832580 batches: 0.0951
trigger times: 9
Loss after 121963680 batches: 0.0907
trigger times: 0
Loss after 122094780 batches: 0.0862
trigger times: 1
Loss after 122225880 batches: 0.0816
trigger times: 0
Loss after 122356980 batches: 0.0787
trigger times: 1
Loss after 122488080 batches: 0.0765
trigger times: 2
Loss after 122619180 batches: 0.0725
trigger times: 3
Loss after 122750280 batches: 0.0699
trigger times: 0
Loss after 122881380 batches: 0.0678
trigger times: 1
Loss after 123012480 batches: 0.0649
trigger times: 0
Loss after 123143580 batches: 0.0628
trigger times: 1
Loss after 123274680 batches: 0.0610
trigger times: 2
Loss after 123405780 batches: 0.0590
trigger times: 3
Loss after 123536880 batches: 0.0567
trigger times: 4
Loss after 123667980 batches: 0.0552
trigger times: 0
Loss after 123799080 batches: 0.0548
trigger times: 1
Loss after 123930180 batches: 0.0531
trigger times: 2
Loss after 124061280 batches: 0.0518
trigger times: 3
Loss after 124192380 batches: 0.0498
trigger times: 0
Loss after 124323480 batches: 0.0486
trigger times: 0
Loss after 124454580 batches: 0.0471
trigger times: 1
Loss after 124585680 batches: 0.0466
trigger times: 2
Loss after 124716780 batches: 0.0454
trigger times: 3
Loss after 124847880 batches: 0.0440
trigger times: 4
Loss after 124978980 batches: 0.0437
trigger times: 5
Loss after 125110080 batches: 0.0424
trigger times: 6
Loss after 125241180 batches: 0.0416
trigger times: 7
Loss after 125372280 batches: 0.0409
trigger times: 8
Loss after 125503380 batches: 0.0403
trigger times: 9
Loss after 125634480 batches: 0.0394
trigger times: 10
Loss after 125765580 batches: 0.0391
trigger times: 0
Loss after 125896680 batches: 0.0379
trigger times: 1
Loss after 126027780 batches: 0.0373
trigger times: 2
Loss after 126158880 batches: 0.0364
trigger times: 3
Loss after 126289980 batches: 0.0364
trigger times: 4
Loss after 126421080 batches: 0.0355
trigger times: 5
Loss after 126552180 batches: 0.0350
trigger times: 0
Loss after 126683280 batches: 0.0341
trigger times: 1
Loss after 126814380 batches: 0.0340
trigger times: 2
Loss after 126945480 batches: 0.0335
trigger times: 0
Loss after 127076580 batches: 0.0331
trigger times: 1
Loss after 127207680 batches: 0.0330
trigger times: 2
Loss after 127338780 batches: 0.0321
trigger times: 3
Loss after 127469880 batches: 0.0322
trigger times: 4
Loss after 127600980 batches: 0.0310
trigger times: 5
Loss after 127732080 batches: 0.0308
trigger times: 6
Loss after 127863180 batches: 0.0307
trigger times: 7
Loss after 127994280 batches: 0.0299
trigger times: 8
Loss after 128125380 batches: 0.0302
trigger times: 0
Loss after 128256480 batches: 0.0294
trigger times: 1
Loss after 128387580 batches: 0.0292
trigger times: 2
Loss after 128518680 batches: 0.0290
trigger times: 3
Loss after 128649780 batches: 0.0289
trigger times: 4
Loss after 128780880 batches: 0.0284
trigger times: 0
Loss after 128911980 batches: 0.0283
trigger times: 1
Loss after 129043080 batches: 0.0283
trigger times: 2
Loss after 129174180 batches: 0.0279
trigger times: 3
Loss after 129305280 batches: 0.0280
trigger times: 4
Loss after 129436380 batches: 0.0271
trigger times: 5
Loss after 129567480 batches: 0.0271
trigger times: 6
Loss after 129698580 batches: 0.0267
trigger times: 7
Loss after 129829680 batches: 0.0262
trigger times: 8
Loss after 129960780 batches: 0.0261
trigger times: 9
Loss after 130091880 batches: 0.0258
trigger times: 10
Loss after 130222980 batches: 0.0258
trigger times: 11
Loss after 130354080 batches: 0.0256
trigger times: 12
Loss after 130485180 batches: 0.0252
trigger times: 13
Loss after 130616280 batches: 0.0259
trigger times: 14
Loss after 130747380 batches: 0.0255
trigger times: 15
Loss after 130878480 batches: 0.0250
trigger times: 16
Loss after 131009580 batches: 0.0245
trigger times: 17
Loss after 131140680 batches: 0.0245
trigger times: 18
Loss after 131271780 batches: 0.0241
trigger times: 0
Loss after 131402880 batches: 0.0244
trigger times: 0
Loss after 131533980 batches: 0.0238
trigger times: 1
Loss after 131665080 batches: 0.0234
trigger times: 2
Loss after 131796180 batches: 0.0238
trigger times: 3
Loss after 131927280 batches: 0.0234
trigger times: 4
Loss after 132058380 batches: 0.0235
trigger times: 5
Loss after 132189480 batches: 0.0231
trigger times: 6
Loss after 132320580 batches: 0.0238
trigger times: 7
Loss after 132451680 batches: 0.0230
trigger times: 8
Loss after 132582780 batches: 0.0221
trigger times: 9
Loss after 132713880 batches: 0.0224
trigger times: 10
Loss after 132844980 batches: 0.0223
trigger times: 11
Loss after 132976080 batches: 0.0221
trigger times: 0
Loss after 133107180 batches: 0.0223
trigger times: 1
Loss after 133238280 batches: 0.0218
trigger times: 2
Loss after 133369380 batches: 0.0214
trigger times: 3
Loss after 133500480 batches: 0.0212
trigger times: 4
Loss after 133631580 batches: 0.0212
trigger times: 5
Loss after 133762680 batches: 0.0208
trigger times: 6
Loss after 133893780 batches: 0.0210
trigger times: 7
Loss after 134024880 batches: 0.0205
trigger times: 8
Loss after 134155980 batches: 0.0209
trigger times: 9
Loss after 134287080 batches: 0.0204
trigger times: 0
Loss after 134418180 batches: 0.0207
trigger times: 1
Loss after 134549280 batches: 0.0203
trigger times: 2
Loss after 134680380 batches: 0.0202
trigger times: 3
Loss after 134811480 batches: 0.0200
trigger times: 4
Loss after 134942580 batches: 0.0197
trigger times: 5
Loss after 135073680 batches: 0.0197
trigger times: 6
Loss after 135204780 batches: 0.0195
trigger times: 7
Loss after 135335880 batches: 0.0193
trigger times: 8
Loss after 135466980 batches: 0.0191
trigger times: 9
Loss after 135598080 batches: 0.0188
trigger times: 10
Loss after 135729180 batches: 0.0194
trigger times: 11
Loss after 135860280 batches: 0.0190
trigger times: 12
Loss after 135991380 batches: 0.0187
trigger times: 13
Loss after 136122480 batches: 0.0188
trigger times: 14
Loss after 136253580 batches: 0.0189
trigger times: 15
Loss after 136384680 batches: 0.0186
trigger times: 16
Loss after 136515780 batches: 0.0188
trigger times: 17
Loss after 136646880 batches: 0.0185
trigger times: 18
Loss after 136777980 batches: 0.0186
trigger times: 19
Loss after 136909080 batches: 0.0183
trigger times: 20
Early stopping!
Start to test process.
Loss after 137040180 batches: 0.0181
Time to train on one home:  974.7092132568359
trigger times: 0
Loss after 137171280 batches: 0.8262
trigger times: 0
Loss after 137302380 batches: 0.6104
trigger times: 1
Loss after 137433480 batches: 0.4755
trigger times: 2
Loss after 137564580 batches: 0.3870
trigger times: 3
Loss after 137695680 batches: 0.3313
trigger times: 4
Loss after 137826780 batches: 0.2940
trigger times: 5
Loss after 137957880 batches: 0.2661
trigger times: 6
Loss after 138088980 batches: 0.2477
trigger times: 7
Loss after 138220080 batches: 0.2265
trigger times: 8
Loss after 138351180 batches: 0.2167
trigger times: 9
Loss after 138482280 batches: 0.1978
trigger times: 10
Loss after 138613380 batches: 0.1837
trigger times: 11
Loss after 138744480 batches: 0.1745
trigger times: 12
Loss after 138875580 batches: 0.1649
trigger times: 13
Loss after 139006680 batches: 0.1522
trigger times: 14
Loss after 139137780 batches: 0.1459
trigger times: 15
Loss after 139268880 batches: 0.1404
trigger times: 16
Loss after 139399980 batches: 0.1332
trigger times: 17
Loss after 139531080 batches: 0.1273
trigger times: 18
Loss after 139662180 batches: 0.1239
trigger times: 19
Loss after 139793280 batches: 0.1191
trigger times: 20
Early stopping!
Start to test process.
Loss after 139924380 batches: 0.1159
Time to train on one home:  181.40984106063843
trigger times: 0
Loss after 140055480 batches: 0.8136
trigger times: 1
Loss after 140186580 batches: 0.6196
trigger times: 2
Loss after 140317680 batches: 0.4748
trigger times: 3
Loss after 140448780 batches: 0.3980
trigger times: 4
Loss after 140579880 batches: 0.3484
trigger times: 5
Loss after 140710980 batches: 0.3066
trigger times: 6
Loss after 140842080 batches: 0.2702
trigger times: 7
Loss after 140973180 batches: 0.2451
trigger times: 8
Loss after 141104280 batches: 0.2203
trigger times: 9
Loss after 141235380 batches: 0.2010
trigger times: 10
Loss after 141366480 batches: 0.1843
trigger times: 11
Loss after 141497580 batches: 0.1711
trigger times: 12
Loss after 141628680 batches: 0.1614
trigger times: 13
Loss after 141759780 batches: 0.1506
trigger times: 14
Loss after 141890880 batches: 0.1404
trigger times: 15
Loss after 142021980 batches: 0.1340
trigger times: 16
Loss after 142153080 batches: 0.1267
trigger times: 17
Loss after 142284180 batches: 0.1215
trigger times: 18
Loss after 142415280 batches: 0.1146
trigger times: 19
Loss after 142546380 batches: 0.1098
trigger times: 20
Early stopping!
Start to test process.
Loss after 142677480 batches: 0.1058
Time to train on one home:  174.43292474746704
trigger times: 0
Loss after 142808580 batches: 0.7989
trigger times: 1
Loss after 142939680 batches: 0.5150
trigger times: 2
Loss after 143070780 batches: 0.3685
trigger times: 3
Loss after 143201880 batches: 0.2936
trigger times: 4
Loss after 143332980 batches: 0.2509
trigger times: 5
Loss after 143464080 batches: 0.2200
trigger times: 6
Loss after 143595180 batches: 0.1959
trigger times: 7
Loss after 143726280 batches: 0.1763
trigger times: 8
Loss after 143857380 batches: 0.1576
trigger times: 9
Loss after 143988480 batches: 0.1467
trigger times: 10
Loss after 144119580 batches: 0.1322
trigger times: 11
Loss after 144250680 batches: 0.1232
trigger times: 12
Loss after 144381780 batches: 0.1128
trigger times: 13
Loss after 144512880 batches: 0.1071
trigger times: 14
Loss after 144643980 batches: 0.1018
trigger times: 15
Loss after 144775080 batches: 0.0964
trigger times: 16
Loss after 144906180 batches: 0.0938
trigger times: 17
Loss after 145037280 batches: 0.0878
trigger times: 18
Loss after 145168380 batches: 0.0871
trigger times: 19
Loss after 145299480 batches: 0.0837
trigger times: 20
Early stopping!
Start to test process.
Loss after 145430580 batches: 0.0799
Time to train on one home:  173.93087363243103
trigger times: 0
Loss after 145561680 batches: 0.6396
trigger times: 0
Loss after 145692780 batches: 0.4639
trigger times: 0
Loss after 145823880 batches: 0.4019
trigger times: 0
Loss after 145954980 batches: 0.3482
trigger times: 0
Loss after 146086080 batches: 0.3063
trigger times: 0
Loss after 146217180 batches: 0.2751
trigger times: 1
Loss after 146348280 batches: 0.2517
trigger times: 2
Loss after 146479380 batches: 0.2261
trigger times: 3
Loss after 146610480 batches: 0.2065
trigger times: 4
Loss after 146741580 batches: 0.1856
trigger times: 5
Loss after 146872680 batches: 0.1693
trigger times: 6
Loss after 147003780 batches: 0.1529
trigger times: 7
Loss after 147134880 batches: 0.1425
trigger times: 8
Loss after 147265980 batches: 0.1309
trigger times: 9
Loss after 147397080 batches: 0.1242
trigger times: 10
Loss after 147528180 batches: 0.1140
trigger times: 11
Loss after 147659280 batches: 0.1081
trigger times: 12
Loss after 147790380 batches: 0.1015
trigger times: 13
Loss after 147921480 batches: 0.0971
trigger times: 14
Loss after 148052580 batches: 0.0916
trigger times: 15
Loss after 148183680 batches: 0.0867
trigger times: 16
Loss after 148314780 batches: 0.0844
trigger times: 17
Loss after 148445880 batches: 0.0810
trigger times: 18
Loss after 148576980 batches: 0.0787
trigger times: 19
Loss after 148708080 batches: 0.0759
trigger times: 20
Early stopping!
Start to test process.
Loss after 148839180 batches: 0.0736
Time to train on one home:  212.35683059692383
trigger times: 0
Loss after 148970280 batches: 0.8929
trigger times: 1
Loss after 149101380 batches: 0.7856
trigger times: 2
Loss after 149232480 batches: 0.7274
trigger times: 3
Loss after 149363580 batches: 0.6711
trigger times: 4
Loss after 149494680 batches: 0.6117
trigger times: 5
Loss after 149625780 batches: 0.5461
trigger times: 6
Loss after 149756880 batches: 0.4758
trigger times: 7
Loss after 149887980 batches: 0.4138
trigger times: 8
Loss after 150019080 batches: 0.3529
trigger times: 9
Loss after 150150180 batches: 0.3045
trigger times: 10
Loss after 150281280 batches: 0.2649
trigger times: 11
Loss after 150412380 batches: 0.2345
trigger times: 12
Loss after 150543480 batches: 0.2115
trigger times: 13
Loss after 150674580 batches: 0.1947
trigger times: 14
Loss after 150805680 batches: 0.1815
trigger times: 15
Loss after 150936780 batches: 0.1682
trigger times: 16
Loss after 151067880 batches: 0.1570
trigger times: 17
Loss after 151198980 batches: 0.1526
trigger times: 18
Loss after 151330080 batches: 0.1428
trigger times: 19
Loss after 151461180 batches: 0.1350
trigger times: 20
Early stopping!
Start to test process.
Loss after 151592280 batches: 0.1327
Time to train on one home:  175.48547196388245
trigger times: 0
Loss after 151686240 batches: 0.8787
trigger times: 0
Loss after 151780200 batches: 0.6741
trigger times: 1
Loss after 151874160 batches: 0.5587
trigger times: 2
Loss after 151968120 batches: 0.4701
trigger times: 3
Loss after 152062080 batches: 0.3940
trigger times: 4
Loss after 152156040 batches: 0.3331
trigger times: 5
Loss after 152250000 batches: 0.2864
trigger times: 6
Loss after 152343960 batches: 0.2570
trigger times: 7
Loss after 152437920 batches: 0.2212
trigger times: 8
Loss after 152531880 batches: 0.2034
trigger times: 9
Loss after 152625840 batches: 0.1827
trigger times: 10
Loss after 152719800 batches: 0.1707
trigger times: 11
Loss after 152813760 batches: 0.1564
trigger times: 12
Loss after 152907720 batches: 0.1479
trigger times: 13
Loss after 153001680 batches: 0.1393
trigger times: 14
Loss after 153095640 batches: 0.1311
trigger times: 15
Loss after 153189600 batches: 0.1240
trigger times: 16
Loss after 153283560 batches: 0.1206
trigger times: 17
Loss after 153377520 batches: 0.1147
trigger times: 18
Loss after 153471480 batches: 0.1106
trigger times: 19
Loss after 153565440 batches: 0.1072
trigger times: 20
Early stopping!
Start to test process.
Loss after 153659400 batches: 0.1039
Time to train on one home:  140.1172571182251
trigger times: 0
Loss after 153790500 batches: 0.2102
trigger times: 1
Loss after 153921600 batches: 0.0827
trigger times: 2
Loss after 154052700 batches: 0.0613
trigger times: 3
Loss after 154183800 batches: 0.0519
trigger times: 4
Loss after 154314900 batches: 0.0453
trigger times: 5
Loss after 154446000 batches: 0.0425
trigger times: 6
Loss after 154577100 batches: 0.0382
trigger times: 7
Loss after 154708200 batches: 0.0358
trigger times: 8
Loss after 154839300 batches: 0.0336
trigger times: 9
Loss after 154970400 batches: 0.0335
trigger times: 10
Loss after 155101500 batches: 0.0304
trigger times: 11
Loss after 155232600 batches: 0.0291
trigger times: 12
Loss after 155363700 batches: 0.0277
trigger times: 13
Loss after 155494800 batches: 0.0261
trigger times: 14
Loss after 155625900 batches: 0.0264
trigger times: 15
Loss after 155757000 batches: 0.0246
trigger times: 16
Loss after 155888100 batches: 0.0228
trigger times: 17
Loss after 156019200 batches: 0.0213
trigger times: 18
Loss after 156150300 batches: 0.0200
trigger times: 19
Loss after 156281400 batches: 0.0185
trigger times: 20
Early stopping!
Start to test process.
Loss after 156412500 batches: 0.0179
Time to train on one home:  175.32969880104065
trigger times: 0
Loss after 156543600 batches: 0.4559
trigger times: 0
Loss after 156674700 batches: 0.2524
trigger times: 1
Loss after 156805800 batches: 0.1917
trigger times: 2
Loss after 156936900 batches: 0.1600
trigger times: 3
Loss after 157068000 batches: 0.1362
trigger times: 4
Loss after 157199100 batches: 0.1200
trigger times: 5
Loss after 157330200 batches: 0.1084
trigger times: 6
Loss after 157461300 batches: 0.0970
trigger times: 7
Loss after 157592400 batches: 0.0896
trigger times: 8
Loss after 157723500 batches: 0.0819
trigger times: 9
Loss after 157854600 batches: 0.0769
trigger times: 10
Loss after 157985700 batches: 0.0716
trigger times: 11
Loss after 158116800 batches: 0.0679
trigger times: 12
Loss after 158247900 batches: 0.0647
trigger times: 13
Loss after 158379000 batches: 0.0608
trigger times: 14
Loss after 158510100 batches: 0.0584
trigger times: 15
Loss after 158641200 batches: 0.0562
trigger times: 16
Loss after 158772300 batches: 0.0539
trigger times: 17
Loss after 158903400 batches: 0.0514
trigger times: 18
Loss after 159034500 batches: 0.0500
trigger times: 19
Loss after 159165600 batches: 0.0483
trigger times: 20
Early stopping!
Start to test process.
Loss after 159296700 batches: 0.0464
Time to train on one home:  181.54553174972534
trigger times: 0
Loss after 159427800 batches: 0.8624
trigger times: 1
Loss after 159558900 batches: 0.6256
trigger times: 2
Loss after 159690000 batches: 0.4897
trigger times: 3
Loss after 159821100 batches: 0.4048
trigger times: 4
Loss after 159952200 batches: 0.3501
trigger times: 5
Loss after 160083300 batches: 0.3069
trigger times: 6
Loss after 160214400 batches: 0.2720
trigger times: 7
Loss after 160345500 batches: 0.2463
trigger times: 8
Loss after 160476600 batches: 0.2287
trigger times: 9
Loss after 160607700 batches: 0.2106
trigger times: 10
Loss after 160738800 batches: 0.1968
trigger times: 11
Loss after 160869900 batches: 0.1856
trigger times: 12
Loss after 161001000 batches: 0.1792
trigger times: 13
Loss after 161132100 batches: 0.1679
trigger times: 14
Loss after 161263200 batches: 0.1626
trigger times: 15
Loss after 161394300 batches: 0.1557
trigger times: 16
Loss after 161525400 batches: 0.1510
trigger times: 17
Loss after 161656500 batches: 0.1433
trigger times: 18
Loss after 161787600 batches: 0.1394
trigger times: 19
Loss after 161918700 batches: 0.1345
trigger times: 20
Early stopping!
Start to test process.
Loss after 162049800 batches: 0.1309
Time to train on one home:  174.27766132354736
train_results:  [0.06280048316701156, 0.09085162036455097]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323]]
Round_1_results:  [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1280 < 1281; dropping {'Training_Loss': 0.6745540542261941, 'Validation_Loss': 0.7987312740749783, 'Training_R2': 0.31573626699337487, 'Validation_R2': 0.25555960473238537, 'Training_F1': 0.6416949775981772, 'Validation_F1': 0.3014352401501683, 'Training_NEP': 0.7172958506919307, 'Validation_NEP': 1.097217464488967, 'Training_NDE': 0.46091339241528595, 'Validation_NDE': 0.5928330882785804, 'Training_MAE': 47.077441001356696, 'Validation_MAE': 30.09025495270247, 'Training_MSE': 6082.545, 'Validation_MSE': 2189.3164}.
trigger times: 0
Loss after 162152400 batches: 0.6746
trigger times: 0
Loss after 162255000 batches: 0.4747
trigger times: 1
Loss after 162357600 batches: 0.4047
trigger times: 2
Loss after 162460200 batches: 0.3455
trigger times: 3
Loss after 162562800 batches: 0.3144
trigger times: 4
Loss after 162665400 batches: 0.2791
trigger times: 5
Loss after 162768000 batches: 0.2677
trigger times: 6
Loss after 162870600 batches: 0.2440
trigger times: 7
Loss after 162973200 batches: 0.2260
trigger times: 8
Loss after 163075800 batches: 0.2093
trigger times: 9
Loss after 163178400 batches: 0.2066
trigger times: 10
Loss after 163281000 batches: 0.1908
trigger times: 11
Loss after 163383600 batches: 0.1805
trigger times: 12
Loss after 163486200 batches: 0.1758
trigger times: 13
Loss after 163588800 batches: 0.1657
trigger times: 14
Loss after 163691400 batches: 0.1570
trigger times: 15
Loss after 163794000 batches: 0.1600
trigger times: 16
Loss after 163896600 batches: 0.1735
trigger times: 17
Loss after 163999200 batches: 0.1594
trigger times: 18
Loss after 164101800 batches: 0.1432
trigger times: 19
Loss after 164204400 batches: 0.1332
trigger times: 20
Early stopping!
Start to test process.
Loss after 164307000 batches: 0.1321
Time to train on one home:  149.10706496238708
trigger times: 0
Loss after 164438100 batches: 0.4869
trigger times: 0
Loss after 164569200 batches: 0.3177
trigger times: 1
Loss after 164700300 batches: 0.2504
trigger times: 2
Loss after 164831400 batches: 0.2147
trigger times: 3
Loss after 164962500 batches: 0.1891
trigger times: 4
Loss after 165093600 batches: 0.1707
trigger times: 5
Loss after 165224700 batches: 0.1579
trigger times: 6
Loss after 165355800 batches: 0.1470
trigger times: 7
Loss after 165486900 batches: 0.1377
trigger times: 8
Loss after 165618000 batches: 0.1296
trigger times: 9
Loss after 165749100 batches: 0.1223
trigger times: 10
Loss after 165880200 batches: 0.1176
trigger times: 11
Loss after 166011300 batches: 0.1119
trigger times: 12
Loss after 166142400 batches: 0.1065
trigger times: 13
Loss after 166273500 batches: 0.1030
trigger times: 14
Loss after 166404600 batches: 0.0990
trigger times: 15
Loss after 166535700 batches: 0.0953
trigger times: 16
Loss after 166666800 batches: 0.0928
trigger times: 17
Loss after 166797900 batches: 0.0891
trigger times: 18
Loss after 166929000 batches: 0.0855
trigger times: 19
Loss after 167060100 batches: 0.0839
trigger times: 20
Early stopping!
Start to test process.
Loss after 167191200 batches: 0.0811
Time to train on one home:  182.85553574562073
trigger times: 0
Loss after 167322300 batches: 0.7710
trigger times: 1
Loss after 167453400 batches: 0.6072
trigger times: 2
Loss after 167584500 batches: 0.4959
trigger times: 3
Loss after 167715600 batches: 0.4216
trigger times: 4
Loss after 167846700 batches: 0.3732
trigger times: 5
Loss after 167977800 batches: 0.3389
trigger times: 6
Loss after 168108900 batches: 0.3110
trigger times: 7
Loss after 168240000 batches: 0.2882
trigger times: 8
Loss after 168371100 batches: 0.2653
trigger times: 9
Loss after 168502200 batches: 0.2476
trigger times: 10
Loss after 168633300 batches: 0.2293
trigger times: 11
Loss after 168764400 batches: 0.2161
trigger times: 12
Loss after 168895500 batches: 0.2016
trigger times: 13
Loss after 169026600 batches: 0.1855
trigger times: 14
Loss after 169157700 batches: 0.1745
trigger times: 15
Loss after 169288800 batches: 0.1656
trigger times: 16
Loss after 169419900 batches: 0.1567
trigger times: 17
Loss after 169551000 batches: 0.1472
trigger times: 18
Loss after 169682100 batches: 0.1413
trigger times: 19
Loss after 169813200 batches: 0.1359
trigger times: 20
Early stopping!
Start to test process.
Loss after 169944300 batches: 0.1314
Time to train on one home:  174.39134240150452
trigger times: 0
Loss after 170072940 batches: 0.4942
trigger times: 0
Loss after 170201580 batches: 0.3406
trigger times: 1
Loss after 170330220 batches: 0.2695
trigger times: 2
Loss after 170458860 batches: 0.2382
trigger times: 3
Loss after 170587500 batches: 0.2165
trigger times: 4
Loss after 170716140 batches: 0.2011
trigger times: 5
Loss after 170844780 batches: 0.1926
trigger times: 6
Loss after 170973420 batches: 0.1810
trigger times: 7
Loss after 171102060 batches: 0.1704
trigger times: 8
Loss after 171230700 batches: 0.1592
trigger times: 9
Loss after 171359340 batches: 0.1516
trigger times: 10
Loss after 171487980 batches: 0.1433
trigger times: 11
Loss after 171616620 batches: 0.1366
trigger times: 12
Loss after 171745260 batches: 0.1279
trigger times: 13
Loss after 171873900 batches: 0.1234
trigger times: 14
Loss after 172002540 batches: 0.1179
trigger times: 15
Loss after 172131180 batches: 0.1139
trigger times: 16
Loss after 172259820 batches: 0.1089
trigger times: 17
Loss after 172388460 batches: 0.1041
trigger times: 18
Loss after 172517100 batches: 0.0987
trigger times: 19
Loss after 172645740 batches: 0.0954
trigger times: 20
Early stopping!
Start to test process.
Loss after 172774380 batches: 0.0915
Time to train on one home:  179.2068703174591
trigger times: 0
Loss after 172905480 batches: 0.7752
trigger times: 1
Loss after 173036580 batches: 0.5619
trigger times: 2
Loss after 173167680 batches: 0.4414
trigger times: 3
Loss after 173298780 batches: 0.3533
trigger times: 4
Loss after 173429880 batches: 0.2957
trigger times: 5
Loss after 173560980 batches: 0.2559
trigger times: 6
Loss after 173692080 batches: 0.2232
trigger times: 7
Loss after 173823180 batches: 0.1978
trigger times: 8
Loss after 173954280 batches: 0.1806
trigger times: 9
Loss after 174085380 batches: 0.1664
trigger times: 10
Loss after 174216480 batches: 0.1563
trigger times: 11
Loss after 174347580 batches: 0.1451
trigger times: 12
Loss after 174478680 batches: 0.1364
trigger times: 13
Loss after 174609780 batches: 0.1299
trigger times: 14
Loss after 174740880 batches: 0.1225
trigger times: 15
Loss after 174871980 batches: 0.1173
trigger times: 16
Loss after 175003080 batches: 0.1125
trigger times: 17
Loss after 175134180 batches: 0.1080
trigger times: 18
Loss after 175265280 batches: 0.1050
trigger times: 19
Loss after 175396380 batches: 0.1009
trigger times: 20
Early stopping!
Start to test process.
Loss after 175527480 batches: 0.0992
Time to train on one home:  173.75941014289856
trigger times: 0
Loss after 175658580 batches: 0.8263
trigger times: 1
Loss after 175789680 batches: 0.7086
trigger times: 2
Loss after 175920780 batches: 0.6382
trigger times: 3
Loss after 176051880 batches: 0.5605
trigger times: 4
Loss after 176182980 batches: 0.5129
trigger times: 5
Loss after 176314080 batches: 0.4745
trigger times: 6
Loss after 176445180 batches: 0.4336
trigger times: 7
Loss after 176576280 batches: 0.3957
trigger times: 8
Loss after 176707380 batches: 0.3645
trigger times: 9
Loss after 176838480 batches: 0.3550
trigger times: 10
Loss after 176969580 batches: 0.3194
trigger times: 11
Loss after 177100680 batches: 0.2889
trigger times: 12
Loss after 177231780 batches: 0.2768
trigger times: 13
Loss after 177362880 batches: 0.2452
trigger times: 14
Loss after 177493980 batches: 0.2357
trigger times: 15
Loss after 177625080 batches: 0.2121
trigger times: 16
Loss after 177756180 batches: 0.2083
trigger times: 17
Loss after 177887280 batches: 0.1915
trigger times: 18
Loss after 178018380 batches: 0.1821
trigger times: 19
Loss after 178149480 batches: 0.1718
trigger times: 20
Early stopping!
Start to test process.
Loss after 178280580 batches: 0.1596
Time to train on one home:  173.5124990940094
trigger times: 0
Loss after 178411680 batches: 0.4235
trigger times: 1
Loss after 178542780 batches: 0.2143
trigger times: 2
Loss after 178673880 batches: 0.1549
trigger times: 3
Loss after 178804980 batches: 0.1222
trigger times: 4
Loss after 178936080 batches: 0.1023
trigger times: 5
Loss after 179067180 batches: 0.0891
trigger times: 6
Loss after 179198280 batches: 0.0820
trigger times: 7
Loss after 179329380 batches: 0.0757
trigger times: 8
Loss after 179460480 batches: 0.0689
trigger times: 9
Loss after 179591580 batches: 0.0658
trigger times: 10
Loss after 179722680 batches: 0.0628
trigger times: 11
Loss after 179853780 batches: 0.0606
trigger times: 12
Loss after 179984880 batches: 0.0572
trigger times: 13
Loss after 180115980 batches: 0.0546
trigger times: 14
Loss after 180247080 batches: 0.0536
trigger times: 15
Loss after 180378180 batches: 0.0510
trigger times: 16
Loss after 180509280 batches: 0.0509
trigger times: 17
Loss after 180640380 batches: 0.0488
trigger times: 18
Loss after 180771480 batches: 0.0472
trigger times: 19
Loss after 180902580 batches: 0.0460
trigger times: 20
Early stopping!
Start to test process.
Loss after 181033680 batches: 0.0436
Time to train on one home:  174.04817390441895
trigger times: 0
Loss after 181164780 batches: 0.5105
trigger times: 0
Loss after 181295880 batches: 0.2951
trigger times: 0
Loss after 181426980 batches: 0.2098
trigger times: 0
Loss after 181558080 batches: 0.1738
trigger times: 1
Loss after 181689180 batches: 0.1527
trigger times: 0
Loss after 181820280 batches: 0.1386
trigger times: 0
Loss after 181951380 batches: 0.1315
trigger times: 1
Loss after 182082480 batches: 0.1218
trigger times: 2
Loss after 182213580 batches: 0.1145
trigger times: 3
Loss after 182344680 batches: 0.1086
trigger times: 4
Loss after 182475780 batches: 0.1033
trigger times: 5
Loss after 182606880 batches: 0.0986
trigger times: 6
Loss after 182737980 batches: 0.0938
trigger times: 0
Loss after 182869080 batches: 0.0893
trigger times: 0
Loss after 183000180 batches: 0.0867
trigger times: 1
Loss after 183131280 batches: 0.0835
trigger times: 2
Loss after 183262380 batches: 0.0812
trigger times: 0
Loss after 183393480 batches: 0.0765
trigger times: 1
Loss after 183524580 batches: 0.0759
trigger times: 2
Loss after 183655680 batches: 0.0720
trigger times: 3
Loss after 183786780 batches: 0.0700
trigger times: 4
Loss after 183917880 batches: 0.0675
trigger times: 5
Loss after 184048980 batches: 0.0647
trigger times: 6
Loss after 184180080 batches: 0.0631
trigger times: 0
Loss after 184311180 batches: 0.0603
trigger times: 0
Loss after 184442280 batches: 0.0580
trigger times: 1
Loss after 184573380 batches: 0.0565
trigger times: 0
Loss after 184704480 batches: 0.0545
trigger times: 1
Loss after 184835580 batches: 0.0538
trigger times: 2
Loss after 184966680 batches: 0.0514
trigger times: 3
Loss after 185097780 batches: 0.0506
trigger times: 0
Loss after 185228880 batches: 0.0500
trigger times: 1
Loss after 185359980 batches: 0.0488
trigger times: 2
Loss after 185491080 batches: 0.0478
trigger times: 3
Loss after 185622180 batches: 0.0456
trigger times: 4
Loss after 185753280 batches: 0.0446
trigger times: 5
Loss after 185884380 batches: 0.0441
trigger times: 6
Loss after 186015480 batches: 0.0424
trigger times: 0
Loss after 186146580 batches: 0.0424
trigger times: 1
Loss after 186277680 batches: 0.0407
trigger times: 2
Loss after 186408780 batches: 0.0398
trigger times: 3
Loss after 186539880 batches: 0.0384
trigger times: 4
Loss after 186670980 batches: 0.0380
trigger times: 5
Loss after 186802080 batches: 0.0377
trigger times: 0
Loss after 186933180 batches: 0.0365
trigger times: 1
Loss after 187064280 batches: 0.0360
trigger times: 2
Loss after 187195380 batches: 0.0356
trigger times: 3
Loss after 187326480 batches: 0.0344
trigger times: 4
Loss after 187457580 batches: 0.0343
trigger times: 5
Loss after 187588680 batches: 0.0335
trigger times: 6
Loss after 187719780 batches: 0.0326
trigger times: 7
Loss after 187850880 batches: 0.0326
trigger times: 8
Loss after 187981980 batches: 0.0322
trigger times: 9
Loss after 188113080 batches: 0.0312
trigger times: 10
Loss after 188244180 batches: 0.0306
trigger times: 11
Loss after 188375280 batches: 0.0306
trigger times: 12
Loss after 188506380 batches: 0.0298
trigger times: 13
Loss after 188637480 batches: 0.0294
trigger times: 14
Loss after 188768580 batches: 0.0287
trigger times: 15
Loss after 188899680 batches: 0.0286
trigger times: 16
Loss after 189030780 batches: 0.0277
trigger times: 17
Loss after 189161880 batches: 0.0276
trigger times: 18
Loss after 189292980 batches: 0.0278
trigger times: 19
Loss after 189424080 batches: 0.0268
trigger times: 20
Early stopping!
Start to test process.
Loss after 189555180 batches: 0.0270
Time to train on one home:  509.59663486480713
trigger times: 0
Loss after 189633780 batches: 0.8186
trigger times: 0
Loss after 189712380 batches: 0.5964
trigger times: 1
Loss after 189790980 batches: 0.4622
trigger times: 0
Loss after 189869580 batches: 0.3690
trigger times: 1
Loss after 189948180 batches: 0.3074
trigger times: 2
Loss after 190026780 batches: 0.2579
trigger times: 3
Loss after 190105380 batches: 0.2230
trigger times: 4
Loss after 190183980 batches: 0.2019
trigger times: 5
Loss after 190262580 batches: 0.1831
trigger times: 6
Loss after 190341180 batches: 0.1688
trigger times: 7
Loss after 190419780 batches: 0.1527
trigger times: 8
Loss after 190498380 batches: 0.1450
trigger times: 9
Loss after 190576980 batches: 0.1361
trigger times: 10
Loss after 190655580 batches: 0.1332
trigger times: 11
Loss after 190734180 batches: 0.1290
trigger times: 12
Loss after 190812780 batches: 0.1216
trigger times: 13
Loss after 190891380 batches: 0.1180
trigger times: 14
Loss after 190969980 batches: 0.1141
trigger times: 15
Loss after 191048580 batches: 0.1105
trigger times: 16
Loss after 191127180 batches: 0.1070
trigger times: 17
Loss after 191205780 batches: 0.1006
trigger times: 18
Loss after 191284380 batches: 0.1000
trigger times: 19
Loss after 191362980 batches: 0.0961
trigger times: 20
Early stopping!
Start to test process.
Loss after 191441580 batches: 0.0929
Time to train on one home:  133.20838141441345
trigger times: 0
Loss after 191572680 batches: 0.4109
trigger times: 1
Loss after 191703780 batches: 0.2161
trigger times: 2
Loss after 191834880 batches: 0.1549
trigger times: 3
Loss after 191965980 batches: 0.1201
trigger times: 4
Loss after 192097080 batches: 0.1005
trigger times: 5
Loss after 192228180 batches: 0.0881
trigger times: 6
Loss after 192359280 batches: 0.0793
trigger times: 7
Loss after 192490380 batches: 0.0725
trigger times: 8
Loss after 192621480 batches: 0.0677
trigger times: 9
Loss after 192752580 batches: 0.0632
trigger times: 10
Loss after 192883680 batches: 0.0597
trigger times: 11
Loss after 193014780 batches: 0.0576
trigger times: 12
Loss after 193145880 batches: 0.0542
trigger times: 13
Loss after 193276980 batches: 0.0523
trigger times: 14
Loss after 193408080 batches: 0.0503
trigger times: 15
Loss after 193539180 batches: 0.0486
trigger times: 16
Loss after 193670280 batches: 0.0474
trigger times: 17
Loss after 193801380 batches: 0.0460
trigger times: 18
Loss after 193932480 batches: 0.0438
trigger times: 19
Loss after 194063580 batches: 0.0431
trigger times: 20
Early stopping!
Start to test process.
Loss after 194194680 batches: 0.0419
Time to train on one home:  174.15457725524902
trigger times: 0
Loss after 194325780 batches: 0.4055
trigger times: 1
Loss after 194456880 batches: 0.2147
trigger times: 2
Loss after 194587980 batches: 0.1606
trigger times: 3
Loss after 194719080 batches: 0.1343
trigger times: 4
Loss after 194850180 batches: 0.1173
trigger times: 5
Loss after 194981280 batches: 0.1063
trigger times: 6
Loss after 195112380 batches: 0.0971
trigger times: 7
Loss after 195243480 batches: 0.0905
trigger times: 8
Loss after 195374580 batches: 0.0856
trigger times: 9
Loss after 195505680 batches: 0.0798
trigger times: 10
Loss after 195636780 batches: 0.0768
trigger times: 11
Loss after 195767880 batches: 0.0723
trigger times: 12
Loss after 195898980 batches: 0.0692
trigger times: 13
Loss after 196030080 batches: 0.0658
trigger times: 14
Loss after 196161180 batches: 0.0631
trigger times: 15
Loss after 196292280 batches: 0.0611
trigger times: 16
Loss after 196423380 batches: 0.0595
trigger times: 17
Loss after 196554480 batches: 0.0565
trigger times: 18
Loss after 196685580 batches: 0.0552
trigger times: 19
Loss after 196816680 batches: 0.0533
trigger times: 20
Early stopping!
Start to test process.
Loss after 196947780 batches: 0.0515
Time to train on one home:  174.84583854675293
trigger times: 0
Loss after 197078880 batches: 0.7115
trigger times: 1
Loss after 197209980 batches: 0.4444
trigger times: 2
Loss after 197341080 batches: 0.3330
trigger times: 3
Loss after 197472180 batches: 0.2819
trigger times: 4
Loss after 197603280 batches: 0.2414
trigger times: 5
Loss after 197734380 batches: 0.2140
trigger times: 6
Loss after 197865480 batches: 0.1908
trigger times: 7
Loss after 197996580 batches: 0.1722
trigger times: 8
Loss after 198127680 batches: 0.1596
trigger times: 9
Loss after 198258780 batches: 0.1486
trigger times: 10
Loss after 198389880 batches: 0.1374
trigger times: 11
Loss after 198520980 batches: 0.1328
trigger times: 12
Loss after 198652080 batches: 0.1231
trigger times: 13
Loss after 198783180 batches: 0.1207
trigger times: 14
Loss after 198914280 batches: 0.1128
trigger times: 15
Loss after 199045380 batches: 0.1114
trigger times: 16
Loss after 199176480 batches: 0.1014
trigger times: 17
Loss after 199307580 batches: 0.1001
trigger times: 18
Loss after 199438680 batches: 0.0962
trigger times: 19
Loss after 199569780 batches: 0.0932
trigger times: 20
Early stopping!
Start to test process.
Loss after 199700880 batches: 0.0875
Time to train on one home:  174.96839118003845
trigger times: 0
Loss after 199831980 batches: 0.6948
trigger times: 1
Loss after 199963080 batches: 0.4764
trigger times: 2
Loss after 200094180 batches: 0.3988
trigger times: 3
Loss after 200225280 batches: 0.3415
trigger times: 4
Loss after 200356380 batches: 0.2962
trigger times: 5
Loss after 200487480 batches: 0.2642
trigger times: 6
Loss after 200618580 batches: 0.2373
trigger times: 7
Loss after 200749680 batches: 0.2107
trigger times: 8
Loss after 200880780 batches: 0.1947
trigger times: 9
Loss after 201011880 batches: 0.1781
trigger times: 10
Loss after 201142980 batches: 0.1647
trigger times: 11
Loss after 201274080 batches: 0.1528
trigger times: 12
Loss after 201405180 batches: 0.1414
trigger times: 13
Loss after 201536280 batches: 0.1341
trigger times: 14
Loss after 201667380 batches: 0.1281
trigger times: 15
Loss after 201798480 batches: 0.1213
trigger times: 16
Loss after 201929580 batches: 0.1164
trigger times: 17
Loss after 202060680 batches: 0.1118
trigger times: 18
Loss after 202191780 batches: 0.1097
trigger times: 19
Loss after 202322880 batches: 0.1054
trigger times: 20
Early stopping!
Start to test process.
Loss after 202453980 batches: 0.1009
Time to train on one home:  175.79364824295044
trigger times: 0
Loss after 202585080 batches: 0.6956
trigger times: 1
Loss after 202716180 batches: 0.4189
trigger times: 2
Loss after 202847280 batches: 0.3024
trigger times: 3
Loss after 202978380 batches: 0.2458
trigger times: 4
Loss after 203109480 batches: 0.2145
trigger times: 5
Loss after 203240580 batches: 0.1894
trigger times: 6
Loss after 203371680 batches: 0.1682
trigger times: 7
Loss after 203502780 batches: 0.1519
trigger times: 8
Loss after 203633880 batches: 0.1421
trigger times: 9
Loss after 203764980 batches: 0.1300
trigger times: 10
Loss after 203896080 batches: 0.1209
trigger times: 11
Loss after 204027180 batches: 0.1148
trigger times: 12
Loss after 204158280 batches: 0.1076
trigger times: 13
Loss after 204289380 batches: 0.1037
trigger times: 14
Loss after 204420480 batches: 0.0990
trigger times: 15
Loss after 204551580 batches: 0.0940
trigger times: 16
Loss after 204682680 batches: 0.0908
trigger times: 17
Loss after 204813780 batches: 0.0888
trigger times: 18
Loss after 204944880 batches: 0.0840
trigger times: 19
Loss after 205075980 batches: 0.0826
trigger times: 20
Early stopping!
Start to test process.
Loss after 205207080 batches: 0.0803
Time to train on one home:  173.01446914672852
trigger times: 0
Loss after 205338180 batches: 0.5924
trigger times: 0
Loss after 205469280 batches: 0.4800
trigger times: 0
Loss after 205600380 batches: 0.4433
trigger times: 0
Loss after 205731480 batches: 0.4065
trigger times: 0
Loss after 205862580 batches: 0.3686
trigger times: 1
Loss after 205993680 batches: 0.3426
trigger times: 2
Loss after 206124780 batches: 0.3169
trigger times: 3
Loss after 206255880 batches: 0.2958
trigger times: 4
Loss after 206386980 batches: 0.2692
trigger times: 5
Loss after 206518080 batches: 0.2522
trigger times: 6
Loss after 206649180 batches: 0.2369
trigger times: 7
Loss after 206780280 batches: 0.2192
trigger times: 8
Loss after 206911380 batches: 0.2059
trigger times: 9
Loss after 207042480 batches: 0.1932
trigger times: 10
Loss after 207173580 batches: 0.1801
trigger times: 11
Loss after 207304680 batches: 0.1683
trigger times: 12
Loss after 207435780 batches: 0.1587
trigger times: 13
Loss after 207566880 batches: 0.1485
trigger times: 14
Loss after 207697980 batches: 0.1392
trigger times: 15
Loss after 207829080 batches: 0.1341
trigger times: 16
Loss after 207960180 batches: 0.1278
trigger times: 17
Loss after 208091280 batches: 0.1229
trigger times: 18
Loss after 208222380 batches: 0.1177
trigger times: 19
Loss after 208353480 batches: 0.1142
trigger times: 20
Early stopping!
Start to test process.
Loss after 208484580 batches: 0.1082
Time to train on one home:  195.34538650512695
trigger times: 0
Loss after 208615680 batches: 0.8429
trigger times: 1
Loss after 208746780 batches: 0.7318
trigger times: 2
Loss after 208877880 batches: 0.6537
trigger times: 3
Loss after 209008980 batches: 0.5837
trigger times: 4
Loss after 209140080 batches: 0.5188
trigger times: 5
Loss after 209271180 batches: 0.4417
trigger times: 6
Loss after 209402280 batches: 0.3821
trigger times: 7
Loss after 209533380 batches: 0.3237
trigger times: 8
Loss after 209664480 batches: 0.2823
trigger times: 9
Loss after 209795580 batches: 0.2513
trigger times: 10
Loss after 209926680 batches: 0.2240
trigger times: 11
Loss after 210057780 batches: 0.2010
trigger times: 12
Loss after 210188880 batches: 0.1836
trigger times: 13
Loss after 210319980 batches: 0.1708
trigger times: 14
Loss after 210451080 batches: 0.1598
trigger times: 15
Loss after 210582180 batches: 0.1507
trigger times: 16
Loss after 210713280 batches: 0.1412
trigger times: 17
Loss after 210844380 batches: 0.1312
trigger times: 18
Loss after 210975480 batches: 0.1245
trigger times: 19
Loss after 211106580 batches: 0.1205
trigger times: 20
Early stopping!
Start to test process.
Loss after 211237680 batches: 0.1162
Time to train on one home:  165.18089652061462
trigger times: 0
Loss after 211331640 batches: 0.7997
trigger times: 0
Loss after 211425600 batches: 0.6189
trigger times: 1
Loss after 211519560 batches: 0.5074
trigger times: 2
Loss after 211613520 batches: 0.4257
trigger times: 3
Loss after 211707480 batches: 0.3677
trigger times: 4
Loss after 211801440 batches: 0.3205
trigger times: 5
Loss after 211895400 batches: 0.2829
trigger times: 6
Loss after 211989360 batches: 0.2512
trigger times: 7
Loss after 212083320 batches: 0.2288
trigger times: 8
Loss after 212177280 batches: 0.2067
trigger times: 9
Loss after 212271240 batches: 0.1893
trigger times: 10
Loss after 212365200 batches: 0.1790
trigger times: 11
Loss after 212459160 batches: 0.1685
trigger times: 12
Loss after 212553120 batches: 0.1606
trigger times: 13
Loss after 212647080 batches: 0.1520
trigger times: 14
Loss after 212741040 batches: 0.1460
trigger times: 15
Loss after 212835000 batches: 0.1398
trigger times: 16
Loss after 212928960 batches: 0.1305
trigger times: 17
Loss after 213022920 batches: 0.1272
trigger times: 18
Loss after 213116880 batches: 0.1221
trigger times: 19
Loss after 213210840 batches: 0.1172
trigger times: 20
Early stopping!
Start to test process.
Loss after 213304800 batches: 0.1140
Time to train on one home:  131.99245357513428
trigger times: 0
Loss after 213435900 batches: 0.1510
trigger times: 1
Loss after 213567000 batches: 0.0638
trigger times: 2
Loss after 213698100 batches: 0.0493
trigger times: 0
Loss after 213829200 batches: 0.0396
trigger times: 0
Loss after 213960300 batches: 0.0350
trigger times: 0
Loss after 214091400 batches: 0.0318
trigger times: 1
Loss after 214222500 batches: 0.0283
trigger times: 2
Loss after 214353600 batches: 0.0261
trigger times: 0
Loss after 214484700 batches: 0.0245
trigger times: 1
Loss after 214615800 batches: 0.0219
trigger times: 0
Loss after 214746900 batches: 0.0203
trigger times: 1
Loss after 214878000 batches: 0.0199
trigger times: 2
Loss after 215009100 batches: 0.0182
trigger times: 3
Loss after 215140200 batches: 0.0174
trigger times: 4
Loss after 215271300 batches: 0.0167
trigger times: 5
Loss after 215402400 batches: 0.0153
trigger times: 6
Loss after 215533500 batches: 0.0147
trigger times: 7
Loss after 215664600 batches: 0.0138
trigger times: 8
Loss after 215795700 batches: 0.0134
trigger times: 9
Loss after 215926800 batches: 0.0128
trigger times: 10
Loss after 216057900 batches: 0.0120
trigger times: 0
Loss after 216189000 batches: 0.0117
trigger times: 1
Loss after 216320100 batches: 0.0111
trigger times: 2
Loss after 216451200 batches: 0.0111
trigger times: 3
Loss after 216582300 batches: 0.0107
trigger times: 0
Loss after 216713400 batches: 0.0102
trigger times: 1
Loss after 216844500 batches: 0.0097
trigger times: 2
Loss after 216975600 batches: 0.0095
trigger times: 3
Loss after 217106700 batches: 0.0092
trigger times: 4
Loss after 217237800 batches: 0.0091
trigger times: 5
Loss after 217368900 batches: 0.0088
trigger times: 6
Loss after 217500000 batches: 0.0087
trigger times: 7
Loss after 217631100 batches: 0.0086
trigger times: 8
Loss after 217762200 batches: 0.0087
trigger times: 9
Loss after 217893300 batches: 0.0079
trigger times: 10
Loss after 218024400 batches: 0.0079
trigger times: 11
Loss after 218155500 batches: 0.0077
trigger times: 12
Loss after 218286600 batches: 0.0077
trigger times: 13
Loss after 218417700 batches: 0.0075
trigger times: 14
Loss after 218548800 batches: 0.0071
trigger times: 15
Loss after 218679900 batches: 0.0072
trigger times: 16
Loss after 218811000 batches: 0.0072
trigger times: 17
Loss after 218942100 batches: 0.0070
trigger times: 18
Loss after 219073200 batches: 0.0069
trigger times: 19
Loss after 219204300 batches: 0.0068
trigger times: 20
Early stopping!
Start to test process.
Loss after 219335400 batches: 0.0067
Time to train on one home:  345.83726167678833
trigger times: 0
Loss after 219466500 batches: 0.3776
trigger times: 0
Loss after 219597600 batches: 0.2229
trigger times: 1
Loss after 219728700 batches: 0.1762
trigger times: 2
Loss after 219859800 batches: 0.1474
trigger times: 3
Loss after 219990900 batches: 0.1244
trigger times: 4
Loss after 220122000 batches: 0.1107
trigger times: 5
Loss after 220253100 batches: 0.1008
trigger times: 6
Loss after 220384200 batches: 0.0937
trigger times: 7
Loss after 220515300 batches: 0.0877
trigger times: 8
Loss after 220646400 batches: 0.0832
trigger times: 9
Loss after 220777500 batches: 0.0793
trigger times: 10
Loss after 220908600 batches: 0.0759
trigger times: 0
Loss after 221039700 batches: 0.0726
trigger times: 1
Loss after 221170800 batches: 0.0700
trigger times: 2
Loss after 221301900 batches: 0.0671
trigger times: 3
Loss after 221433000 batches: 0.0648
trigger times: 4
Loss after 221564100 batches: 0.0631
trigger times: 5
Loss after 221695200 batches: 0.0612
trigger times: 0
Loss after 221826300 batches: 0.0601
trigger times: 1
Loss after 221957400 batches: 0.0581
trigger times: 0
Loss after 222088500 batches: 0.0568
trigger times: 1
Loss after 222219600 batches: 0.0554
trigger times: 0
Loss after 222350700 batches: 0.0553
trigger times: 1
Loss after 222481800 batches: 0.0533
trigger times: 0
Loss after 222612900 batches: 0.0515
trigger times: 1
Loss after 222744000 batches: 0.0498
trigger times: 2
Loss after 222875100 batches: 0.0489
trigger times: 3
Loss after 223006200 batches: 0.0476
trigger times: 4
Loss after 223137300 batches: 0.0471
trigger times: 5
Loss after 223268400 batches: 0.0461
trigger times: 6
Loss after 223399500 batches: 0.0449
trigger times: 7
Loss after 223530600 batches: 0.0440
trigger times: 0
Loss after 223661700 batches: 0.0431
trigger times: 1
Loss after 223792800 batches: 0.0427
trigger times: 2
Loss after 223923900 batches: 0.0423
trigger times: 3
Loss after 224055000 batches: 0.0414
trigger times: 0
Loss after 224186100 batches: 0.0406
trigger times: 1
Loss after 224317200 batches: 0.0406
trigger times: 2
Loss after 224448300 batches: 0.0397
trigger times: 3
Loss after 224579400 batches: 0.0392
trigger times: 4
Loss after 224710500 batches: 0.0385
trigger times: 5
Loss after 224841600 batches: 0.0377
trigger times: 6
Loss after 224972700 batches: 0.0371
trigger times: 7
Loss after 225103800 batches: 0.0363
trigger times: 0
Loss after 225234900 batches: 0.0358
trigger times: 1
Loss after 225366000 batches: 0.0345
trigger times: 2
Loss after 225497100 batches: 0.0344
trigger times: 3
Loss after 225628200 batches: 0.0341
trigger times: 4
Loss after 225759300 batches: 0.0339
trigger times: 5
Loss after 225890400 batches: 0.0335
trigger times: 6
Loss after 226021500 batches: 0.0331
trigger times: 7
Loss after 226152600 batches: 0.0317
trigger times: 8
Loss after 226283700 batches: 0.0312
trigger times: 9
Loss after 226414800 batches: 0.0312
trigger times: 10
Loss after 226545900 batches: 0.0306
trigger times: 11
Loss after 226677000 batches: 0.0301
trigger times: 12
Loss after 226808100 batches: 0.0295
trigger times: 0
Loss after 226939200 batches: 0.0288
trigger times: 1
Loss after 227070300 batches: 0.0287
trigger times: 2
Loss after 227201400 batches: 0.0284
trigger times: 3
Loss after 227332500 batches: 0.0278
trigger times: 4
Loss after 227463600 batches: 0.0278
trigger times: 5
Loss after 227594700 batches: 0.0274
trigger times: 6
Loss after 227725800 batches: 0.0268
trigger times: 7
Loss after 227856900 batches: 0.0269
trigger times: 8
Loss after 227988000 batches: 0.0265
trigger times: 9
Loss after 228119100 batches: 0.0261
trigger times: 10
Loss after 228250200 batches: 0.0260
trigger times: 11
Loss after 228381300 batches: 0.0254
trigger times: 12
Loss after 228512400 batches: 0.0255
trigger times: 13
Loss after 228643500 batches: 0.0248
trigger times: 14
Loss after 228774600 batches: 0.0245
trigger times: 15
Loss after 228905700 batches: 0.0242
trigger times: 16
Loss after 229036800 batches: 0.0237
trigger times: 17
Loss after 229167900 batches: 0.0237
trigger times: 18
Loss after 229299000 batches: 0.0235
trigger times: 19
Loss after 229430100 batches: 0.0230
trigger times: 20
Early stopping!
Start to test process.
Loss after 229561200 batches: 0.0229
Time to train on one home:  579.040265083313
trigger times: 0
Loss after 229692300 batches: 0.7301
trigger times: 1
Loss after 229823400 batches: 0.4934
trigger times: 2
Loss after 229954500 batches: 0.3896
trigger times: 3
Loss after 230085600 batches: 0.3306
trigger times: 4
Loss after 230216700 batches: 0.2893
trigger times: 5
Loss after 230347800 batches: 0.2569
trigger times: 6
Loss after 230478900 batches: 0.2324
trigger times: 7
Loss after 230610000 batches: 0.2090
trigger times: 8
Loss after 230741100 batches: 0.1913
trigger times: 9
Loss after 230872200 batches: 0.1775
trigger times: 10
Loss after 231003300 batches: 0.1655
trigger times: 11
Loss after 231134400 batches: 0.1574
trigger times: 12
Loss after 231265500 batches: 0.1483
trigger times: 13
Loss after 231396600 batches: 0.1423
trigger times: 14
Loss after 231527700 batches: 0.1366
trigger times: 15
Loss after 231658800 batches: 0.1322
trigger times: 16
Loss after 231789900 batches: 0.1235
trigger times: 17
Loss after 231921000 batches: 0.1217
trigger times: 18
Loss after 232052100 batches: 0.1168
trigger times: 19
Loss after 232183200 batches: 0.1150
trigger times: 20
Early stopping!
Start to test process.
Loss after 232314300 batches: 0.1108
Time to train on one home:  164.01606583595276
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012]]
Round_2_results:  [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012]
trigger times: 0
Loss after 232416900 batches: 0.6192
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1837 < 1838; dropping {'Training_Loss': 0.6192403584718704, 'Validation_Loss': 0.7231808172331916, 'Training_R2': 0.37124474701400967, 'Validation_R2': 0.3263545958678602, 'Training_F1': 0.6608918987875819, 'Validation_F1': 0.46939990539157594, 'Training_NEP': 0.6763866840668932, 'Validation_NEP': 1.0842728387894534, 'Training_NDE': 0.423523420391329, 'Validation_NDE': 0.536455689233206, 'Training_MAE': 44.39249743679114, 'Validation_MAE': 29.735259612059508, 'Training_MSE': 5589.119, 'Validation_MSE': 1981.1161}.
trigger times: 1
Loss after 232519500 batches: 0.4005
trigger times: 2
Loss after 232622100 batches: 0.3143
trigger times: 3
Loss after 232724700 batches: 0.2608
trigger times: 4
Loss after 232827300 batches: 0.2278
trigger times: 5
Loss after 232929900 batches: 0.2049
trigger times: 6
Loss after 233032500 batches: 0.2159
trigger times: 7
Loss after 233135100 batches: 0.1819
trigger times: 8
Loss after 233237700 batches: 0.1634
trigger times: 9
Loss after 233340300 batches: 0.1615
trigger times: 10
Loss after 233442900 batches: 0.1596
trigger times: 11
Loss after 233545500 batches: 0.1424
trigger times: 12
Loss after 233648100 batches: 0.1329
trigger times: 13
Loss after 233750700 batches: 0.1262
trigger times: 14
Loss after 233853300 batches: 0.1219
trigger times: 15
Loss after 233955900 batches: 0.1197
trigger times: 16
Loss after 234058500 batches: 0.1138
trigger times: 17
Loss after 234161100 batches: 0.1108
trigger times: 18
Loss after 234263700 batches: 0.1143
trigger times: 19
Loss after 234366300 batches: 0.1096
trigger times: 20
Early stopping!
Start to test process.
Loss after 234468900 batches: 0.1014
Time to train on one home:  137.1660897731781
trigger times: 0
Loss after 234600000 batches: 0.4399
trigger times: 1
Loss after 234731100 batches: 0.2898
trigger times: 2
Loss after 234862200 batches: 0.2241
trigger times: 3
Loss after 234993300 batches: 0.1851
trigger times: 4
Loss after 235124400 batches: 0.1614
trigger times: 5
Loss after 235255500 batches: 0.1466
trigger times: 6
Loss after 235386600 batches: 0.1312
trigger times: 7
Loss after 235517700 batches: 0.1207
trigger times: 8
Loss after 235648800 batches: 0.1127
trigger times: 9
Loss after 235779900 batches: 0.1055
trigger times: 10
Loss after 235911000 batches: 0.0998
trigger times: 11
Loss after 236042100 batches: 0.0944
trigger times: 12
Loss after 236173200 batches: 0.0917
trigger times: 13
Loss after 236304300 batches: 0.0870
trigger times: 14
Loss after 236435400 batches: 0.0825
trigger times: 15
Loss after 236566500 batches: 0.0802
trigger times: 16
Loss after 236697600 batches: 0.0776
trigger times: 17
Loss after 236828700 batches: 0.0745
trigger times: 18
Loss after 236959800 batches: 0.0720
trigger times: 19
Loss after 237090900 batches: 0.0688
trigger times: 20
Early stopping!
Start to test process.
Loss after 237222000 batches: 0.0676
Time to train on one home:  165.96633625030518
trigger times: 0
Loss after 237353100 batches: 0.7231
trigger times: 1
Loss after 237484200 batches: 0.5281
trigger times: 2
Loss after 237615300 batches: 0.4170
trigger times: 3
Loss after 237746400 batches: 0.3532
trigger times: 4
Loss after 237877500 batches: 0.3159
trigger times: 5
Loss after 238008600 batches: 0.2788
trigger times: 6
Loss after 238139700 batches: 0.2535
trigger times: 7
Loss after 238270800 batches: 0.2267
trigger times: 8
Loss after 238401900 batches: 0.2082
trigger times: 9
Loss after 238533000 batches: 0.1905
trigger times: 10
Loss after 238664100 batches: 0.1744
trigger times: 11
Loss after 238795200 batches: 0.1639
trigger times: 12
Loss after 238926300 batches: 0.1524
trigger times: 13
Loss after 239057400 batches: 0.1417
trigger times: 14
Loss after 239188500 batches: 0.1357
trigger times: 15
Loss after 239319600 batches: 0.1283
trigger times: 16
Loss after 239450700 batches: 0.1234
trigger times: 17
Loss after 239581800 batches: 0.1171
trigger times: 18
Loss after 239712900 batches: 0.1122
trigger times: 19
Loss after 239844000 batches: 0.1087
trigger times: 20
Early stopping!
Start to test process.
Loss after 239975100 batches: 0.1053
Time to train on one home:  163.68703866004944
trigger times: 0
Loss after 240103740 batches: 0.4195
trigger times: 0
Loss after 240232380 batches: 0.2640
trigger times: 1
Loss after 240361020 batches: 0.2214
trigger times: 2
Loss after 240489660 batches: 0.1944
trigger times: 3
Loss after 240618300 batches: 0.1695
trigger times: 4
Loss after 240746940 batches: 0.1513
trigger times: 5
Loss after 240875580 batches: 0.1346
trigger times: 6
Loss after 241004220 batches: 0.1240
trigger times: 7
Loss after 241132860 batches: 0.1136
trigger times: 8
Loss after 241261500 batches: 0.1063
trigger times: 9
Loss after 241390140 batches: 0.1004
trigger times: 10
Loss after 241518780 batches: 0.0946
trigger times: 11
Loss after 241647420 batches: 0.0897
trigger times: 12
Loss after 241776060 batches: 0.0849
trigger times: 13
Loss after 241904700 batches: 0.0806
trigger times: 14
Loss after 242033340 batches: 0.0776
trigger times: 15
Loss after 242161980 batches: 0.0752
trigger times: 16
Loss after 242290620 batches: 0.0722
trigger times: 17
Loss after 242419260 batches: 0.0702
trigger times: 18
Loss after 242547900 batches: 0.0675
trigger times: 19
Loss after 242676540 batches: 0.0644
trigger times: 0
Loss after 242805180 batches: 0.0628
trigger times: 1
Loss after 242933820 batches: 0.0609
trigger times: 0
Loss after 243062460 batches: 0.0587
trigger times: 0
Loss after 243191100 batches: 0.0579
trigger times: 1
Loss after 243319740 batches: 0.0563
trigger times: 2
Loss after 243448380 batches: 0.0535
trigger times: 0
Loss after 243577020 batches: 0.0534
trigger times: 1
Loss after 243705660 batches: 0.0513
trigger times: 2
Loss after 243834300 batches: 0.0504
trigger times: 3
Loss after 243962940 batches: 0.0486
trigger times: 4
Loss after 244091580 batches: 0.0480
trigger times: 0
Loss after 244220220 batches: 0.0464
trigger times: 1
Loss after 244348860 batches: 0.0454
trigger times: 2
Loss after 244477500 batches: 0.0445
trigger times: 3
Loss after 244606140 batches: 0.0432
trigger times: 4
Loss after 244734780 batches: 0.0426
trigger times: 5
Loss after 244863420 batches: 0.0427
trigger times: 6
Loss after 244992060 batches: 0.0413
trigger times: 7
Loss after 245120700 batches: 0.0406
trigger times: 8
Loss after 245249340 batches: 0.0397
trigger times: 9
Loss after 245377980 batches: 0.0386
trigger times: 10
Loss after 245506620 batches: 0.0383
trigger times: 11
Loss after 245635260 batches: 0.0377
trigger times: 12
Loss after 245763900 batches: 0.0369
trigger times: 13
Loss after 245892540 batches: 0.0372
trigger times: 14
Loss after 246021180 batches: 0.0355
trigger times: 15
Loss after 246149820 batches: 0.0357
trigger times: 16
Loss after 246278460 batches: 0.0353
trigger times: 17
Loss after 246407100 batches: 0.0350
trigger times: 18
Loss after 246535740 batches: 0.0337
trigger times: 19
Loss after 246664380 batches: 0.0331
trigger times: 20
Early stopping!
Start to test process.
Loss after 246793020 batches: 0.0336
Time to train on one home:  393.2011067867279
trigger times: 0
Loss after 246924120 batches: 0.7050
trigger times: 1
Loss after 247055220 batches: 0.4881
trigger times: 2
Loss after 247186320 batches: 0.3588
trigger times: 3
Loss after 247317420 batches: 0.2735
trigger times: 4
Loss after 247448520 batches: 0.2213
trigger times: 5
Loss after 247579620 batches: 0.1900
trigger times: 6
Loss after 247710720 batches: 0.1682
trigger times: 7
Loss after 247841820 batches: 0.1517
trigger times: 8
Loss after 247972920 batches: 0.1413
trigger times: 9
Loss after 248104020 batches: 0.1300
trigger times: 10
Loss after 248235120 batches: 0.1228
trigger times: 11
Loss after 248366220 batches: 0.1165
trigger times: 12
Loss after 248497320 batches: 0.1101
trigger times: 13
Loss after 248628420 batches: 0.1068
trigger times: 14
Loss after 248759520 batches: 0.1016
trigger times: 15
Loss after 248890620 batches: 0.0971
trigger times: 16
Loss after 249021720 batches: 0.0931
trigger times: 17
Loss after 249152820 batches: 0.0912
trigger times: 18
Loss after 249283920 batches: 0.0886
trigger times: 19
Loss after 249415020 batches: 0.0854
trigger times: 20
Early stopping!
Start to test process.
Loss after 249546120 batches: 0.0821
Time to train on one home:  165.2068305015564
trigger times: 0
Loss after 249677220 batches: 0.7891
trigger times: 1
Loss after 249808320 batches: 0.6225
trigger times: 2
Loss after 249939420 batches: 0.5164
trigger times: 3
Loss after 250070520 batches: 0.4530
trigger times: 4
Loss after 250201620 batches: 0.3966
trigger times: 5
Loss after 250332720 batches: 0.3561
trigger times: 6
Loss after 250463820 batches: 0.3263
trigger times: 7
Loss after 250594920 batches: 0.2812
trigger times: 8
Loss after 250726020 batches: 0.2525
trigger times: 9
Loss after 250857120 batches: 0.2285
trigger times: 10
Loss after 250988220 batches: 0.2102
trigger times: 11
Loss after 251119320 batches: 0.1953
trigger times: 12
Loss after 251250420 batches: 0.1769
trigger times: 13
Loss after 251381520 batches: 0.1686
trigger times: 14
Loss after 251512620 batches: 0.1583
trigger times: 15
Loss after 251643720 batches: 0.1530
trigger times: 16
Loss after 251774820 batches: 0.1399
trigger times: 17
Loss after 251905920 batches: 0.1307
trigger times: 18
Loss after 252037020 batches: 0.1301
trigger times: 19
Loss after 252168120 batches: 0.1203
trigger times: 20
Early stopping!
Start to test process.
Loss after 252299220 batches: 0.1120
Time to train on one home:  165.62078881263733
trigger times: 0
Loss after 252430320 batches: 0.3174
trigger times: 1
Loss after 252561420 batches: 0.1663
trigger times: 2
Loss after 252692520 batches: 0.1273
trigger times: 3
Loss after 252823620 batches: 0.1029
trigger times: 4
Loss after 252954720 batches: 0.0875
trigger times: 5
Loss after 253085820 batches: 0.0805
trigger times: 6
Loss after 253216920 batches: 0.0723
trigger times: 7
Loss after 253348020 batches: 0.0669
trigger times: 8
Loss after 253479120 batches: 0.0624
trigger times: 9
Loss after 253610220 batches: 0.0576
trigger times: 10
Loss after 253741320 batches: 0.0549
trigger times: 11
Loss after 253872420 batches: 0.0523
trigger times: 12
Loss after 254003520 batches: 0.0510
trigger times: 13
Loss after 254134620 batches: 0.0488
trigger times: 0
Loss after 254265720 batches: 0.0472
trigger times: 1
Loss after 254396820 batches: 0.0451
trigger times: 0
Loss after 254527920 batches: 0.0444
trigger times: 0
Loss after 254659020 batches: 0.0423
trigger times: 1
Loss after 254790120 batches: 0.0406
trigger times: 0
Loss after 254921220 batches: 0.0398
trigger times: 0
Loss after 255052320 batches: 0.0387
trigger times: 0
Loss after 255183420 batches: 0.0390
trigger times: 1
Loss after 255314520 batches: 0.0377
trigger times: 2
Loss after 255445620 batches: 0.0368
trigger times: 3
Loss after 255576720 batches: 0.0359
trigger times: 4
Loss after 255707820 batches: 0.0348
trigger times: 5
Loss after 255838920 batches: 0.0348
trigger times: 6
Loss after 255970020 batches: 0.0337
trigger times: 7
Loss after 256101120 batches: 0.0329
trigger times: 8
Loss after 256232220 batches: 0.0323
trigger times: 9
Loss after 256363320 batches: 0.0319
trigger times: 0
Loss after 256494420 batches: 0.0314
trigger times: 1
Loss after 256625520 batches: 0.0305
trigger times: 2
Loss after 256756620 batches: 0.0305
trigger times: 3
Loss after 256887720 batches: 0.0298
trigger times: 4
Loss after 257018820 batches: 0.0296
trigger times: 0
Loss after 257149920 batches: 0.0294
trigger times: 1
Loss after 257281020 batches: 0.0291
trigger times: 2
Loss after 257412120 batches: 0.0282
trigger times: 3
Loss after 257543220 batches: 0.0276
trigger times: 4
Loss after 257674320 batches: 0.0275
trigger times: 5
Loss after 257805420 batches: 0.0267
trigger times: 6
Loss after 257936520 batches: 0.0267
trigger times: 7
Loss after 258067620 batches: 0.0266
trigger times: 8
Loss after 258198720 batches: 0.0261
trigger times: 9
Loss after 258329820 batches: 0.0257
trigger times: 10
Loss after 258460920 batches: 0.0250
trigger times: 11
Loss after 258592020 batches: 0.0247
trigger times: 12
Loss after 258723120 batches: 0.0240
trigger times: 13
Loss after 258854220 batches: 0.0245
trigger times: 14
Loss after 258985320 batches: 0.0244
trigger times: 15
Loss after 259116420 batches: 0.0238
trigger times: 16
Loss after 259247520 batches: 0.0239
trigger times: 17
Loss after 259378620 batches: 0.0231
trigger times: 18
Loss after 259509720 batches: 0.0226
trigger times: 19
Loss after 259640820 batches: 0.0227
trigger times: 20
Early stopping!
Start to test process.
Loss after 259771920 batches: 0.0220
Time to train on one home:  425.51026821136475
trigger times: 0
Loss after 259903020 batches: 0.4187
trigger times: 0
Loss after 260034120 batches: 0.2276
trigger times: 0
Loss after 260165220 batches: 0.1711
trigger times: 0
Loss after 260296320 batches: 0.1492
trigger times: 1
Loss after 260427420 batches: 0.1320
trigger times: 2
Loss after 260558520 batches: 0.1193
trigger times: 3
Loss after 260689620 batches: 0.1106
trigger times: 4
Loss after 260820720 batches: 0.1011
trigger times: 5
Loss after 260951820 batches: 0.0948
trigger times: 6
Loss after 261082920 batches: 0.0872
trigger times: 7
Loss after 261214020 batches: 0.0827
trigger times: 8
Loss after 261345120 batches: 0.0758
trigger times: 9
Loss after 261476220 batches: 0.0715
trigger times: 10
Loss after 261607320 batches: 0.0659
trigger times: 11
Loss after 261738420 batches: 0.0636
trigger times: 12
Loss after 261869520 batches: 0.0593
trigger times: 13
Loss after 262000620 batches: 0.0559
trigger times: 14
Loss after 262131720 batches: 0.0539
trigger times: 0
Loss after 262262820 batches: 0.0522
trigger times: 1
Loss after 262393920 batches: 0.0497
trigger times: 2
Loss after 262525020 batches: 0.0477
trigger times: 0
Loss after 262656120 batches: 0.0474
trigger times: 0
Loss after 262787220 batches: 0.0451
trigger times: 1
Loss after 262918320 batches: 0.0429
trigger times: 0
Loss after 263049420 batches: 0.0430
trigger times: 0
Loss after 263180520 batches: 0.0405
trigger times: 1
Loss after 263311620 batches: 0.0391
trigger times: 0
Loss after 263442720 batches: 0.0376
trigger times: 1
Loss after 263573820 batches: 0.0370
trigger times: 0
Loss after 263704920 batches: 0.0356
trigger times: 1
Loss after 263836020 batches: 0.0351
trigger times: 2
Loss after 263967120 batches: 0.0341
trigger times: 0
Loss after 264098220 batches: 0.0329
trigger times: 1
Loss after 264229320 batches: 0.0331
trigger times: 0
Loss after 264360420 batches: 0.0318
trigger times: 1
Loss after 264491520 batches: 0.0314
trigger times: 2
Loss after 264622620 batches: 0.0307
trigger times: 3
Loss after 264753720 batches: 0.0299
trigger times: 0
Loss after 264884820 batches: 0.0289
trigger times: 1
Loss after 265015920 batches: 0.0293
trigger times: 2
Loss after 265147020 batches: 0.0280
trigger times: 3
Loss after 265278120 batches: 0.0275
trigger times: 0
Loss after 265409220 batches: 0.0272
trigger times: 1
Loss after 265540320 batches: 0.0266
trigger times: 2
Loss after 265671420 batches: 0.0257
trigger times: 3
Loss after 265802520 batches: 0.0256
trigger times: 4
Loss after 265933620 batches: 0.0251
trigger times: 5
Loss after 266064720 batches: 0.0252
trigger times: 6
Loss after 266195820 batches: 0.0245
trigger times: 7
Loss after 266326920 batches: 0.0241
trigger times: 8
Loss after 266458020 batches: 0.0241
trigger times: 9
Loss after 266589120 batches: 0.0234
trigger times: 10
Loss after 266720220 batches: 0.0235
trigger times: 11
Loss after 266851320 batches: 0.0230
trigger times: 12
Loss after 266982420 batches: 0.0225
trigger times: 13
Loss after 267113520 batches: 0.0221
trigger times: 14
Loss after 267244620 batches: 0.0221
trigger times: 15
Loss after 267375720 batches: 0.0215
trigger times: 0
Loss after 267506820 batches: 0.0220
trigger times: 1
Loss after 267637920 batches: 0.0216
trigger times: 2
Loss after 267769020 batches: 0.0215
trigger times: 3
Loss after 267900120 batches: 0.0208
trigger times: 4
Loss after 268031220 batches: 0.0208
trigger times: 5
Loss after 268162320 batches: 0.0201
trigger times: 6
Loss after 268293420 batches: 0.0200
trigger times: 7
Loss after 268424520 batches: 0.0200
trigger times: 8
Loss after 268555620 batches: 0.0191
trigger times: 9
Loss after 268686720 batches: 0.0195
trigger times: 10
Loss after 268817820 batches: 0.0192
trigger times: 11
Loss after 268948920 batches: 0.0194
trigger times: 12
Loss after 269080020 batches: 0.0189
trigger times: 13
Loss after 269211120 batches: 0.0187
trigger times: 14
Loss after 269342220 batches: 0.0190
trigger times: 0
Loss after 269473320 batches: 0.0185
trigger times: 1
Loss after 269604420 batches: 0.0189
trigger times: 2
Loss after 269735520 batches: 0.0181
trigger times: 3
Loss after 269866620 batches: 0.0180
trigger times: 4
Loss after 269997720 batches: 0.0180
trigger times: 5
Loss after 270128820 batches: 0.0178
trigger times: 6
Loss after 270259920 batches: 0.0175
trigger times: 7
Loss after 270391020 batches: 0.0174
trigger times: 0
Loss after 270522120 batches: 0.0172
trigger times: 1
Loss after 270653220 batches: 0.0173
trigger times: 2
Loss after 270784320 batches: 0.0170
trigger times: 3
Loss after 270915420 batches: 0.0168
trigger times: 4
Loss after 271046520 batches: 0.0167
trigger times: 5
Loss after 271177620 batches: 0.0168
trigger times: 6
Loss after 271308720 batches: 0.0167
trigger times: 7
Loss after 271439820 batches: 0.0161
trigger times: 8
Loss after 271570920 batches: 0.0170
trigger times: 9
Loss after 271702020 batches: 0.0163
trigger times: 10
Loss after 271833120 batches: 0.0161
trigger times: 11
Loss after 271964220 batches: 0.0159
trigger times: 12
Loss after 272095320 batches: 0.0160
trigger times: 13
Loss after 272226420 batches: 0.0157
trigger times: 0
Loss after 272357520 batches: 0.0156
trigger times: 1
Loss after 272488620 batches: 0.0155
trigger times: 2
Loss after 272619720 batches: 0.0158
trigger times: 3
Loss after 272750820 batches: 0.0157
trigger times: 4
Loss after 272881920 batches: 0.0151
trigger times: 5
Loss after 273013020 batches: 0.0148
trigger times: 0
Loss after 273144120 batches: 0.0150
trigger times: 1
Loss after 273275220 batches: 0.0149
trigger times: 2
Loss after 273406320 batches: 0.0151
trigger times: 3
Loss after 273537420 batches: 0.0147
trigger times: 4
Loss after 273668520 batches: 0.0148
trigger times: 5
Loss after 273799620 batches: 0.0148
trigger times: 6
Loss after 273930720 batches: 0.0146
trigger times: 7
Loss after 274061820 batches: 0.0144
trigger times: 8
Loss after 274192920 batches: 0.0145
trigger times: 9
Loss after 274324020 batches: 0.0142
trigger times: 10
Loss after 274455120 batches: 0.0145
trigger times: 0
Loss after 274586220 batches: 0.0141
trigger times: 1
Loss after 274717320 batches: 0.0142
trigger times: 2
Loss after 274848420 batches: 0.0144
trigger times: 3
Loss after 274979520 batches: 0.0139
trigger times: 4
Loss after 275110620 batches: 0.0138
trigger times: 5
Loss after 275241720 batches: 0.0135
trigger times: 6
Loss after 275372820 batches: 0.0137
trigger times: 7
Loss after 275503920 batches: 0.0135
trigger times: 8
Loss after 275635020 batches: 0.0134
trigger times: 9
Loss after 275766120 batches: 0.0135
trigger times: 10
Loss after 275897220 batches: 0.0134
trigger times: 11
Loss after 276028320 batches: 0.0132
trigger times: 12
Loss after 276159420 batches: 0.0131
trigger times: 13
Loss after 276290520 batches: 0.0131
trigger times: 14
Loss after 276421620 batches: 0.0129
trigger times: 15
Loss after 276552720 batches: 0.0126
trigger times: 16
Loss after 276683820 batches: 0.0124
trigger times: 17
Loss after 276814920 batches: 0.0130
trigger times: 18
Loss after 276946020 batches: 0.0129
trigger times: 19
Loss after 277077120 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 277208220 batches: 0.0122
Time to train on one home:  977.4516403675079
trigger times: 0
Loss after 277286820 batches: 0.7333
trigger times: 0
Loss after 277365420 batches: 0.5001
trigger times: 1
Loss after 277444020 batches: 0.3644
trigger times: 2
Loss after 277522620 batches: 0.2827
trigger times: 3
Loss after 277601220 batches: 0.2312
trigger times: 4
Loss after 277679820 batches: 0.1982
trigger times: 5
Loss after 277758420 batches: 0.1792
trigger times: 6
Loss after 277837020 batches: 0.1594
trigger times: 7
Loss after 277915620 batches: 0.1450
trigger times: 8
Loss after 277994220 batches: 0.1360
trigger times: 9
Loss after 278072820 batches: 0.1278
trigger times: 10
Loss after 278151420 batches: 0.1223
trigger times: 11
Loss after 278230020 batches: 0.1168
trigger times: 12
Loss after 278308620 batches: 0.1122
trigger times: 13
Loss after 278387220 batches: 0.1068
trigger times: 14
Loss after 278465820 batches: 0.1045
trigger times: 15
Loss after 278544420 batches: 0.0994
trigger times: 16
Loss after 278623020 batches: 0.0930
trigger times: 17
Loss after 278701620 batches: 0.0924
trigger times: 18
Loss after 278780220 batches: 0.0905
trigger times: 19
Loss after 278858820 batches: 0.0877
trigger times: 20
Early stopping!
Start to test process.
Loss after 278937420 batches: 0.0856
Time to train on one home:  115.31320238113403
trigger times: 0
Loss after 279068520 batches: 0.3327
trigger times: 1
Loss after 279199620 batches: 0.1820
trigger times: 2
Loss after 279330720 batches: 0.1315
trigger times: 3
Loss after 279461820 batches: 0.1050
trigger times: 4
Loss after 279592920 batches: 0.0882
trigger times: 5
Loss after 279724020 batches: 0.0810
trigger times: 6
Loss after 279855120 batches: 0.0718
trigger times: 7
Loss after 279986220 batches: 0.0652
trigger times: 8
Loss after 280117320 batches: 0.0616
trigger times: 9
Loss after 280248420 batches: 0.0580
trigger times: 10
Loss after 280379520 batches: 0.0548
trigger times: 11
Loss after 280510620 batches: 0.0523
trigger times: 12
Loss after 280641720 batches: 0.0500
trigger times: 13
Loss after 280772820 batches: 0.0474
trigger times: 14
Loss after 280903920 batches: 0.0459
trigger times: 15
Loss after 281035020 batches: 0.0448
trigger times: 16
Loss after 281166120 batches: 0.0429
trigger times: 17
Loss after 281297220 batches: 0.0415
trigger times: 0
Loss after 281428320 batches: 0.0397
trigger times: 1
Loss after 281559420 batches: 0.0381
trigger times: 2
Loss after 281690520 batches: 0.0379
trigger times: 3
Loss after 281821620 batches: 0.0368
trigger times: 4
Loss after 281952720 batches: 0.0357
trigger times: 5
Loss after 282083820 batches: 0.0349
trigger times: 0
Loss after 282214920 batches: 0.0345
trigger times: 1
Loss after 282346020 batches: 0.0340
trigger times: 2
Loss after 282477120 batches: 0.0333
trigger times: 0
Loss after 282608220 batches: 0.0325
trigger times: 1
Loss after 282739320 batches: 0.0314
trigger times: 2
Loss after 282870420 batches: 0.0307
trigger times: 0
Loss after 283001520 batches: 0.0307
trigger times: 0
Loss after 283132620 batches: 0.0296
trigger times: 1
Loss after 283263720 batches: 0.0296
trigger times: 2
Loss after 283394820 batches: 0.0286
trigger times: 3
Loss after 283525920 batches: 0.0287
trigger times: 0
Loss after 283657020 batches: 0.0278
trigger times: 1
Loss after 283788120 batches: 0.0279
trigger times: 2
Loss after 283919220 batches: 0.0273
trigger times: 3
Loss after 284050320 batches: 0.0266
trigger times: 4
Loss after 284181420 batches: 0.0263
trigger times: 5
Loss after 284312520 batches: 0.0265
trigger times: 6
Loss after 284443620 batches: 0.0259
trigger times: 7
Loss after 284574720 batches: 0.0254
trigger times: 8
Loss after 284705820 batches: 0.0248
trigger times: 0
Loss after 284836920 batches: 0.0248
trigger times: 1
Loss after 284968020 batches: 0.0247
trigger times: 2
Loss after 285099120 batches: 0.0244
trigger times: 0
Loss after 285230220 batches: 0.0238
trigger times: 0
Loss after 285361320 batches: 0.0232
trigger times: 1
Loss after 285492420 batches: 0.0234
trigger times: 2
Loss after 285623520 batches: 0.0229
trigger times: 3
Loss after 285754620 batches: 0.0227
trigger times: 4
Loss after 285885720 batches: 0.0225
trigger times: 5
Loss after 286016820 batches: 0.0223
trigger times: 6
Loss after 286147920 batches: 0.0218
trigger times: 7
Loss after 286279020 batches: 0.0219
trigger times: 8
Loss after 286410120 batches: 0.0213
trigger times: 9
Loss after 286541220 batches: 0.0210
trigger times: 10
Loss after 286672320 batches: 0.0211
trigger times: 11
Loss after 286803420 batches: 0.0207
trigger times: 12
Loss after 286934520 batches: 0.0213
trigger times: 13
Loss after 287065620 batches: 0.0207
trigger times: 14
Loss after 287196720 batches: 0.0202
trigger times: 15
Loss after 287327820 batches: 0.0195
trigger times: 16
Loss after 287458920 batches: 0.0203
trigger times: 17
Loss after 287590020 batches: 0.0201
trigger times: 18
Loss after 287721120 batches: 0.0192
trigger times: 19
Loss after 287852220 batches: 0.0191
trigger times: 20
Early stopping!
Start to test process.
Loss after 287983320 batches: 0.0192
Time to train on one home:  512.6797230243683
trigger times: 0
Loss after 288114420 batches: 0.3726
trigger times: 1
Loss after 288245520 batches: 0.1801
trigger times: 2
Loss after 288376620 batches: 0.1341
trigger times: 3
Loss after 288507720 batches: 0.1096
trigger times: 4
Loss after 288638820 batches: 0.0941
trigger times: 5
Loss after 288769920 batches: 0.0853
trigger times: 6
Loss after 288901020 batches: 0.0786
trigger times: 7
Loss after 289032120 batches: 0.0731
trigger times: 8
Loss after 289163220 batches: 0.0669
trigger times: 9
Loss after 289294320 batches: 0.0644
trigger times: 10
Loss after 289425420 batches: 0.0604
trigger times: 11
Loss after 289556520 batches: 0.0570
trigger times: 12
Loss after 289687620 batches: 0.0557
trigger times: 13
Loss after 289818720 batches: 0.0524
trigger times: 14
Loss after 289949820 batches: 0.0506
trigger times: 15
Loss after 290080920 batches: 0.0490
trigger times: 16
Loss after 290212020 batches: 0.0472
trigger times: 0
Loss after 290343120 batches: 0.0456
trigger times: 1
Loss after 290474220 batches: 0.0443
trigger times: 2
Loss after 290605320 batches: 0.0421
trigger times: 3
Loss after 290736420 batches: 0.0413
trigger times: 4
Loss after 290867520 batches: 0.0412
trigger times: 0
Loss after 290998620 batches: 0.0397
trigger times: 1
Loss after 291129720 batches: 0.0387
trigger times: 2
Loss after 291260820 batches: 0.0376
trigger times: 3
Loss after 291391920 batches: 0.0363
trigger times: 0
Loss after 291523020 batches: 0.0359
trigger times: 1
Loss after 291654120 batches: 0.0353
trigger times: 2
Loss after 291785220 batches: 0.0348
trigger times: 3
Loss after 291916320 batches: 0.0339
trigger times: 4
Loss after 292047420 batches: 0.0328
trigger times: 5
Loss after 292178520 batches: 0.0321
trigger times: 6
Loss after 292309620 batches: 0.0322
trigger times: 7
Loss after 292440720 batches: 0.0314
trigger times: 8
Loss after 292571820 batches: 0.0309
trigger times: 9
Loss after 292702920 batches: 0.0302
trigger times: 10
Loss after 292834020 batches: 0.0302
trigger times: 0
Loss after 292965120 batches: 0.0292
trigger times: 1
Loss after 293096220 batches: 0.0287
trigger times: 2
Loss after 293227320 batches: 0.0288
trigger times: 3
Loss after 293358420 batches: 0.0284
trigger times: 4
Loss after 293489520 batches: 0.0277
trigger times: 5
Loss after 293620620 batches: 0.0281
trigger times: 6
Loss after 293751720 batches: 0.0277
trigger times: 0
Loss after 293882820 batches: 0.0272
trigger times: 1
Loss after 294013920 batches: 0.0270
trigger times: 0
Loss after 294145020 batches: 0.0267
trigger times: 0
Loss after 294276120 batches: 0.0261
trigger times: 1
Loss after 294407220 batches: 0.0258
trigger times: 2
Loss after 294538320 batches: 0.0258
trigger times: 3
Loss after 294669420 batches: 0.0255
trigger times: 4
Loss after 294800520 batches: 0.0252
trigger times: 5
Loss after 294931620 batches: 0.0254
trigger times: 6
Loss after 295062720 batches: 0.0245
trigger times: 7
Loss after 295193820 batches: 0.0244
trigger times: 8
Loss after 295324920 batches: 0.0244
trigger times: 9
Loss after 295456020 batches: 0.0242
trigger times: 10
Loss after 295587120 batches: 0.0236
trigger times: 11
Loss after 295718220 batches: 0.0237
trigger times: 0
Loss after 295849320 batches: 0.0234
trigger times: 1
Loss after 295980420 batches: 0.0233
trigger times: 2
Loss after 296111520 batches: 0.0232
trigger times: 3
Loss after 296242620 batches: 0.0230
trigger times: 4
Loss after 296373720 batches: 0.0226
trigger times: 5
Loss after 296504820 batches: 0.0223
trigger times: 6
Loss after 296635920 batches: 0.0220
trigger times: 7
Loss after 296767020 batches: 0.0221
trigger times: 8
Loss after 296898120 batches: 0.0221
trigger times: 9
Loss after 297029220 batches: 0.0218
trigger times: 10
Loss after 297160320 batches: 0.0218
trigger times: 11
Loss after 297291420 batches: 0.0214
trigger times: 12
Loss after 297422520 batches: 0.0218
trigger times: 13
Loss after 297553620 batches: 0.0211
trigger times: 14
Loss after 297684720 batches: 0.0213
trigger times: 15
Loss after 297815820 batches: 0.0207
trigger times: 16
Loss after 297946920 batches: 0.0211
trigger times: 0
Loss after 298078020 batches: 0.0205
trigger times: 1
Loss after 298209120 batches: 0.0206
trigger times: 2
Loss after 298340220 batches: 0.0206
trigger times: 0
Loss after 298471320 batches: 0.0200
trigger times: 1
Loss after 298602420 batches: 0.0205
trigger times: 2
Loss after 298733520 batches: 0.0204
trigger times: 3
Loss after 298864620 batches: 0.0198
trigger times: 4
Loss after 298995720 batches: 0.0198
trigger times: 5
Loss after 299126820 batches: 0.0193
trigger times: 6
Loss after 299257920 batches: 0.0195
trigger times: 7
Loss after 299389020 batches: 0.0197
trigger times: 0
Loss after 299520120 batches: 0.0196
trigger times: 1
Loss after 299651220 batches: 0.0193
trigger times: 2
Loss after 299782320 batches: 0.0191
trigger times: 3
Loss after 299913420 batches: 0.0189
trigger times: 0
Loss after 300044520 batches: 0.0186
trigger times: 1
Loss after 300175620 batches: 0.0190
trigger times: 2
Loss after 300306720 batches: 0.0186
trigger times: 3
Loss after 300437820 batches: 0.0188
trigger times: 4
Loss after 300568920 batches: 0.0187
trigger times: 5
Loss after 300700020 batches: 0.0183
trigger times: 6
Loss after 300831120 batches: 0.0182
trigger times: 7
Loss after 300962220 batches: 0.0184
trigger times: 8
Loss after 301093320 batches: 0.0180
trigger times: 9
Loss after 301224420 batches: 0.0180
trigger times: 10
Loss after 301355520 batches: 0.0179
trigger times: 11
Loss after 301486620 batches: 0.0183
trigger times: 12
Loss after 301617720 batches: 0.0179
trigger times: 0
Loss after 301748820 batches: 0.0180
trigger times: 1
Loss after 301879920 batches: 0.0174
trigger times: 2
Loss after 302011020 batches: 0.0174
trigger times: 0
Loss after 302142120 batches: 0.0178
trigger times: 1
Loss after 302273220 batches: 0.0172
trigger times: 2
Loss after 302404320 batches: 0.0175
trigger times: 3
Loss after 302535420 batches: 0.0174
trigger times: 0
Loss after 302666520 batches: 0.0176
trigger times: 0
Loss after 302797620 batches: 0.0171
trigger times: 1
Loss after 302928720 batches: 0.0167
trigger times: 0
Loss after 303059820 batches: 0.0169
trigger times: 1
Loss after 303190920 batches: 0.0169
trigger times: 2
Loss after 303322020 batches: 0.0170
trigger times: 3
Loss after 303453120 batches: 0.0166
trigger times: 4
Loss after 303584220 batches: 0.0167
trigger times: 5
Loss after 303715320 batches: 0.0165
trigger times: 6
Loss after 303846420 batches: 0.0160
trigger times: 0
Loss after 303977520 batches: 0.0163
trigger times: 1
Loss after 304108620 batches: 0.0161
trigger times: 2
Loss after 304239720 batches: 0.0162
trigger times: 3
Loss after 304370820 batches: 0.0163
trigger times: 4
Loss after 304501920 batches: 0.0161
trigger times: 5
Loss after 304633020 batches: 0.0160
trigger times: 6
Loss after 304764120 batches: 0.0160
trigger times: 7
Loss after 304895220 batches: 0.0156
trigger times: 8
Loss after 305026320 batches: 0.0157
trigger times: 9
Loss after 305157420 batches: 0.0158
trigger times: 10
Loss after 305288520 batches: 0.0155
trigger times: 11
Loss after 305419620 batches: 0.0156
trigger times: 12
Loss after 305550720 batches: 0.0154
trigger times: 0
Loss after 305681820 batches: 0.0156
trigger times: 1
Loss after 305812920 batches: 0.0156
trigger times: 2
Loss after 305944020 batches: 0.0151
trigger times: 3
Loss after 306075120 batches: 0.0152
trigger times: 4
Loss after 306206220 batches: 0.0154
trigger times: 5
Loss after 306337320 batches: 0.0148
trigger times: 6
Loss after 306468420 batches: 0.0152
trigger times: 7
Loss after 306599520 batches: 0.0150
trigger times: 8
Loss after 306730620 batches: 0.0148
trigger times: 9
Loss after 306861720 batches: 0.0146
trigger times: 10
Loss after 306992820 batches: 0.0147
trigger times: 11
Loss after 307123920 batches: 0.0148
trigger times: 12
Loss after 307255020 batches: 0.0147
trigger times: 13
Loss after 307386120 batches: 0.0146
trigger times: 14
Loss after 307517220 batches: 0.0149
trigger times: 15
Loss after 307648320 batches: 0.0144
trigger times: 16
Loss after 307779420 batches: 0.0146
trigger times: 17
Loss after 307910520 batches: 0.0144
trigger times: 18
Loss after 308041620 batches: 0.0144
trigger times: 19
Loss after 308172720 batches: 0.0140
trigger times: 20
Early stopping!
Start to test process.
Loss after 308303820 batches: 0.0144
Time to train on one home:  1137.8765540122986
trigger times: 0
Loss after 308434920 batches: 0.6264
trigger times: 1
Loss after 308566020 batches: 0.3370
trigger times: 2
Loss after 308697120 batches: 0.2495
trigger times: 3
Loss after 308828220 batches: 0.1985
trigger times: 4
Loss after 308959320 batches: 0.1665
trigger times: 5
Loss after 309090420 batches: 0.1461
trigger times: 6
Loss after 309221520 batches: 0.1315
trigger times: 7
Loss after 309352620 batches: 0.1213
trigger times: 8
Loss after 309483720 batches: 0.1149
trigger times: 9
Loss after 309614820 batches: 0.1043
trigger times: 10
Loss after 309745920 batches: 0.0989
trigger times: 11
Loss after 309877020 batches: 0.0934
trigger times: 12
Loss after 310008120 batches: 0.0920
trigger times: 13
Loss after 310139220 batches: 0.0879
trigger times: 14
Loss after 310270320 batches: 0.0833
trigger times: 15
Loss after 310401420 batches: 0.0821
trigger times: 16
Loss after 310532520 batches: 0.0794
trigger times: 17
Loss after 310663620 batches: 0.0752
trigger times: 18
Loss after 310794720 batches: 0.0738
trigger times: 19
Loss after 310925820 batches: 0.0722
trigger times: 20
Early stopping!
Start to test process.
Loss after 311056920 batches: 0.0703
Time to train on one home:  164.93891310691833
trigger times: 0
Loss after 311188020 batches: 0.6525
trigger times: 0
Loss after 311319120 batches: 0.4375
trigger times: 1
Loss after 311450220 batches: 0.3637
trigger times: 2
Loss after 311581320 batches: 0.3103
trigger times: 3
Loss after 311712420 batches: 0.2665
trigger times: 4
Loss after 311843520 batches: 0.2335
trigger times: 5
Loss after 311974620 batches: 0.2046
trigger times: 6
Loss after 312105720 batches: 0.1824
trigger times: 7
Loss after 312236820 batches: 0.1653
trigger times: 8
Loss after 312367920 batches: 0.1548
trigger times: 9
Loss after 312499020 batches: 0.1416
trigger times: 10
Loss after 312630120 batches: 0.1341
trigger times: 11
Loss after 312761220 batches: 0.1269
trigger times: 12
Loss after 312892320 batches: 0.1192
trigger times: 13
Loss after 313023420 batches: 0.1135
trigger times: 14
Loss after 313154520 batches: 0.1080
trigger times: 15
Loss after 313285620 batches: 0.1024
trigger times: 16
Loss after 313416720 batches: 0.0988
trigger times: 17
Loss after 313547820 batches: 0.0950
trigger times: 18
Loss after 313678920 batches: 0.0916
trigger times: 19
Loss after 313810020 batches: 0.0886
trigger times: 20
Early stopping!
Start to test process.
Loss after 313941120 batches: 0.0865
Time to train on one home:  171.40771412849426
trigger times: 0
Loss after 314072220 batches: 0.5764
trigger times: 0
Loss after 314203320 batches: 0.3017
trigger times: 1
Loss after 314334420 batches: 0.2100
trigger times: 2
Loss after 314465520 batches: 0.1652
trigger times: 3
Loss after 314596620 batches: 0.1370
trigger times: 4
Loss after 314727720 batches: 0.1176
trigger times: 5
Loss after 314858820 batches: 0.1058
trigger times: 6
Loss after 314989920 batches: 0.0976
trigger times: 7
Loss after 315121020 batches: 0.0911
trigger times: 8
Loss after 315252120 batches: 0.0867
trigger times: 9
Loss after 315383220 batches: 0.0816
trigger times: 10
Loss after 315514320 batches: 0.0785
trigger times: 11
Loss after 315645420 batches: 0.0748
trigger times: 12
Loss after 315776520 batches: 0.0709
trigger times: 13
Loss after 315907620 batches: 0.0706
trigger times: 14
Loss after 316038720 batches: 0.0682
trigger times: 15
Loss after 316169820 batches: 0.0663
trigger times: 16
Loss after 316300920 batches: 0.0644
trigger times: 17
Loss after 316432020 batches: 0.0631
trigger times: 18
Loss after 316563120 batches: 0.0605
trigger times: 19
Loss after 316694220 batches: 0.0587
trigger times: 20
Early stopping!
Start to test process.
Loss after 316825320 batches: 0.0592
Time to train on one home:  171.2147238254547
trigger times: 0
Loss after 316956420 batches: 0.5962
trigger times: 0
Loss after 317087520 batches: 0.4630
trigger times: 0
Loss after 317218620 batches: 0.4256
trigger times: 0
Loss after 317349720 batches: 0.3943
trigger times: 0
Loss after 317480820 batches: 0.3656
trigger times: 1
Loss after 317611920 batches: 0.3379
trigger times: 0
Loss after 317743020 batches: 0.3080
trigger times: 0
Loss after 317874120 batches: 0.2786
trigger times: 1
Loss after 318005220 batches: 0.2543
trigger times: 2
Loss after 318136320 batches: 0.2296
trigger times: 3
Loss after 318267420 batches: 0.2070
trigger times: 4
Loss after 318398520 batches: 0.1883
trigger times: 5
Loss after 318529620 batches: 0.1711
trigger times: 6
Loss after 318660720 batches: 0.1578
trigger times: 7
Loss after 318791820 batches: 0.1445
trigger times: 8
Loss after 318922920 batches: 0.1358
trigger times: 0
Loss after 319054020 batches: 0.1282
trigger times: 1
Loss after 319185120 batches: 0.1205
trigger times: 2
Loss after 319316220 batches: 0.1132
trigger times: 3
Loss after 319447320 batches: 0.1086
trigger times: 4
Loss after 319578420 batches: 0.1040
trigger times: 0
Loss after 319709520 batches: 0.0995
trigger times: 1
Loss after 319840620 batches: 0.0947
trigger times: 0
Loss after 319971720 batches: 0.0911
trigger times: 1
Loss after 320102820 batches: 0.0885
trigger times: 2
Loss after 320233920 batches: 0.0846
trigger times: 3
Loss after 320365020 batches: 0.0823
trigger times: 4
Loss after 320496120 batches: 0.0787
trigger times: 5
Loss after 320627220 batches: 0.0765
trigger times: 6
Loss after 320758320 batches: 0.0740
trigger times: 0
Loss after 320889420 batches: 0.0722
trigger times: 1
Loss after 321020520 batches: 0.0700
trigger times: 2
Loss after 321151620 batches: 0.0674
trigger times: 3
Loss after 321282720 batches: 0.0660
trigger times: 4
Loss after 321413820 batches: 0.0651
trigger times: 5
Loss after 321544920 batches: 0.0636
trigger times: 6
Loss after 321676020 batches: 0.0616
trigger times: 7
Loss after 321807120 batches: 0.0605
trigger times: 8
Loss after 321938220 batches: 0.0593
trigger times: 9
Loss after 322069320 batches: 0.0586
trigger times: 10
Loss after 322200420 batches: 0.0578
trigger times: 11
Loss after 322331520 batches: 0.0566
trigger times: 12
Loss after 322462620 batches: 0.0555
trigger times: 13
Loss after 322593720 batches: 0.0540
trigger times: 14
Loss after 322724820 batches: 0.0536
trigger times: 15
Loss after 322855920 batches: 0.0526
trigger times: 16
Loss after 322987020 batches: 0.0530
trigger times: 17
Loss after 323118120 batches: 0.0518
trigger times: 18
Loss after 323249220 batches: 0.0510
trigger times: 19
Loss after 323380320 batches: 0.0504
trigger times: 20
Early stopping!
Start to test process.
Loss after 323511420 batches: 0.0494
Time to train on one home:  381.6093099117279
trigger times: 0
Loss after 323642520 batches: 0.8280
trigger times: 1
Loss after 323773620 batches: 0.6736
trigger times: 2
Loss after 323904720 batches: 0.5384
trigger times: 3
Loss after 324035820 batches: 0.4258
trigger times: 4
Loss after 324166920 batches: 0.3436
trigger times: 5
Loss after 324298020 batches: 0.2881
trigger times: 6
Loss after 324429120 batches: 0.2453
trigger times: 7
Loss after 324560220 batches: 0.2132
trigger times: 8
Loss after 324691320 batches: 0.1939
trigger times: 9
Loss after 324822420 batches: 0.1738
trigger times: 10
Loss after 324953520 batches: 0.1627
trigger times: 11
Loss after 325084620 batches: 0.1486
trigger times: 12
Loss after 325215720 batches: 0.1426
trigger times: 13
Loss after 325346820 batches: 0.1305
trigger times: 14
Loss after 325477920 batches: 0.1240
trigger times: 15
Loss after 325609020 batches: 0.1185
trigger times: 16
Loss after 325740120 batches: 0.1128
trigger times: 17
Loss after 325871220 batches: 0.1103
trigger times: 18
Loss after 326002320 batches: 0.1044
trigger times: 19
Loss after 326133420 batches: 0.1006
trigger times: 20
Early stopping!
Start to test process.
Loss after 326264520 batches: 0.0949
Time to train on one home:  164.5749328136444
trigger times: 0
Loss after 326358480 batches: 0.7790
trigger times: 1
Loss after 326452440 batches: 0.5510
trigger times: 2
Loss after 326546400 batches: 0.4123
trigger times: 3
Loss after 326640360 batches: 0.3252
trigger times: 4
Loss after 326734320 batches: 0.2710
trigger times: 5
Loss after 326828280 batches: 0.2284
trigger times: 6
Loss after 326922240 batches: 0.1966
trigger times: 7
Loss after 327016200 batches: 0.1760
trigger times: 8
Loss after 327110160 batches: 0.1598
trigger times: 9
Loss after 327204120 batches: 0.1477
trigger times: 10
Loss after 327298080 batches: 0.1399
trigger times: 11
Loss after 327392040 batches: 0.1305
trigger times: 12
Loss after 327486000 batches: 0.1252
trigger times: 13
Loss after 327579960 batches: 0.1220
trigger times: 14
Loss after 327673920 batches: 0.1180
trigger times: 15
Loss after 327767880 batches: 0.1122
trigger times: 16
Loss after 327861840 batches: 0.1098
trigger times: 17
Loss after 327955800 batches: 0.1048
trigger times: 18
Loss after 328049760 batches: 0.1027
trigger times: 19
Loss after 328143720 batches: 0.0989
trigger times: 20
Early stopping!
Start to test process.
Loss after 328237680 batches: 0.0954
Time to train on one home:  126.71951460838318
trigger times: 0
Loss after 328368780 batches: 0.1163
trigger times: 0
Loss after 328499880 batches: 0.0428
trigger times: 1
Loss after 328630980 batches: 0.0327
trigger times: 2
Loss after 328762080 batches: 0.0270
trigger times: 0
Loss after 328893180 batches: 0.0230
trigger times: 0
Loss after 329024280 batches: 0.0208
trigger times: 1
Loss after 329155380 batches: 0.0196
trigger times: 2
Loss after 329286480 batches: 0.0177
trigger times: 0
Loss after 329417580 batches: 0.0164
trigger times: 1
Loss after 329548680 batches: 0.0157
trigger times: 2
Loss after 329679780 batches: 0.0146
trigger times: 3
Loss after 329810880 batches: 0.0141
trigger times: 4
Loss after 329941980 batches: 0.0130
trigger times: 5
Loss after 330073080 batches: 0.0126
trigger times: 6
Loss after 330204180 batches: 0.0117
trigger times: 7
Loss after 330335280 batches: 0.0116
trigger times: 0
Loss after 330466380 batches: 0.0111
trigger times: 1
Loss after 330597480 batches: 0.0109
trigger times: 2
Loss after 330728580 batches: 0.0101
trigger times: 3
Loss after 330859680 batches: 0.0100
trigger times: 4
Loss after 330990780 batches: 0.0098
trigger times: 5
Loss after 331121880 batches: 0.0093
trigger times: 6
Loss after 331252980 batches: 0.0090
trigger times: 7
Loss after 331384080 batches: 0.0088
trigger times: 8
Loss after 331515180 batches: 0.0082
trigger times: 9
Loss after 331646280 batches: 0.0079
trigger times: 10
Loss after 331777380 batches: 0.0076
trigger times: 11
Loss after 331908480 batches: 0.0074
trigger times: 12
Loss after 332039580 batches: 0.0071
trigger times: 13
Loss after 332170680 batches: 0.0070
trigger times: 0
Loss after 332301780 batches: 0.0070
trigger times: 1
Loss after 332432880 batches: 0.0067
trigger times: 2
Loss after 332563980 batches: 0.0067
trigger times: 3
Loss after 332695080 batches: 0.0063
trigger times: 4
Loss after 332826180 batches: 0.0062
trigger times: 5
Loss after 332957280 batches: 0.0061
trigger times: 6
Loss after 333088380 batches: 0.0059
trigger times: 7
Loss after 333219480 batches: 0.0058
trigger times: 8
Loss after 333350580 batches: 0.0057
trigger times: 9
Loss after 333481680 batches: 0.0057
trigger times: 0
Loss after 333612780 batches: 0.0055
trigger times: 0
Loss after 333743880 batches: 0.0054
trigger times: 1
Loss after 333874980 batches: 0.0054
trigger times: 0
Loss after 334006080 batches: 0.0054
trigger times: 1
Loss after 334137180 batches: 0.0053
trigger times: 0
Loss after 334268280 batches: 0.0053
trigger times: 1
Loss after 334399380 batches: 0.0051
trigger times: 2
Loss after 334530480 batches: 0.0051
trigger times: 3
Loss after 334661580 batches: 0.0050
trigger times: 0
Loss after 334792680 batches: 0.0048
trigger times: 1
Loss after 334923780 batches: 0.0049
trigger times: 2
Loss after 335054880 batches: 0.0048
trigger times: 3
Loss after 335185980 batches: 0.0048
trigger times: 4
Loss after 335317080 batches: 0.0047
trigger times: 0
Loss after 335448180 batches: 0.0046
trigger times: 1
Loss after 335579280 batches: 0.0047
trigger times: 2
Loss after 335710380 batches: 0.0046
trigger times: 3
Loss after 335841480 batches: 0.0045
trigger times: 4
Loss after 335972580 batches: 0.0045
trigger times: 5
Loss after 336103680 batches: 0.0043
trigger times: 6
Loss after 336234780 batches: 0.0043
trigger times: 7
Loss after 336365880 batches: 0.0043
trigger times: 8
Loss after 336496980 batches: 0.0043
trigger times: 9
Loss after 336628080 batches: 0.0041
trigger times: 10
Loss after 336759180 batches: 0.0041
trigger times: 11
Loss after 336890280 batches: 0.0041
trigger times: 12
Loss after 337021380 batches: 0.0041
trigger times: 13
Loss after 337152480 batches: 0.0040
trigger times: 14
Loss after 337283580 batches: 0.0040
trigger times: 15
Loss after 337414680 batches: 0.0039
trigger times: 16
Loss after 337545780 batches: 0.0039
trigger times: 17
Loss after 337676880 batches: 0.0038
trigger times: 18
Loss after 337807980 batches: 0.0038
trigger times: 19
Loss after 337939080 batches: 0.0039
trigger times: 20
Early stopping!
Start to test process.
Loss after 338070180 batches: 0.0039
Time to train on one home:  555.9194188117981
trigger times: 0
Loss after 338201280 batches: 0.3525
trigger times: 1
Loss after 338332380 batches: 0.2037
trigger times: 2
Loss after 338463480 batches: 0.1570
trigger times: 3
Loss after 338594580 batches: 0.1304
trigger times: 4
Loss after 338725680 batches: 0.1138
trigger times: 5
Loss after 338856780 batches: 0.1022
trigger times: 0
Loss after 338987880 batches: 0.0947
trigger times: 1
Loss after 339118980 batches: 0.0876
trigger times: 0
Loss after 339250080 batches: 0.0816
trigger times: 1
Loss after 339381180 batches: 0.0777
trigger times: 2
Loss after 339512280 batches: 0.0733
trigger times: 0
Loss after 339643380 batches: 0.0702
trigger times: 1
Loss after 339774480 batches: 0.0671
trigger times: 0
Loss after 339905580 batches: 0.0639
trigger times: 0
Loss after 340036680 batches: 0.0613
trigger times: 1
Loss after 340167780 batches: 0.0591
trigger times: 2
Loss after 340298880 batches: 0.0564
trigger times: 0
Loss after 340429980 batches: 0.0544
trigger times: 1
Loss after 340561080 batches: 0.0531
trigger times: 0
Loss after 340692180 batches: 0.0517
trigger times: 1
Loss after 340823280 batches: 0.0506
trigger times: 2
Loss after 340954380 batches: 0.0493
trigger times: 3
Loss after 341085480 batches: 0.0472
trigger times: 4
Loss after 341216580 batches: 0.0460
trigger times: 5
Loss after 341347680 batches: 0.0450
trigger times: 6
Loss after 341478780 batches: 0.0437
trigger times: 7
Loss after 341609880 batches: 0.0425
trigger times: 8
Loss after 341740980 batches: 0.0413
trigger times: 9
Loss after 341872080 batches: 0.0406
trigger times: 10
Loss after 342003180 batches: 0.0396
trigger times: 11
Loss after 342134280 batches: 0.0393
trigger times: 12
Loss after 342265380 batches: 0.0382
trigger times: 13
Loss after 342396480 batches: 0.0371
trigger times: 14
Loss after 342527580 batches: 0.0375
trigger times: 15
Loss after 342658680 batches: 0.0371
trigger times: 16
Loss after 342789780 batches: 0.0361
trigger times: 17
Loss after 342920880 batches: 0.0353
trigger times: 18
Loss after 343051980 batches: 0.0344
trigger times: 19
Loss after 343183080 batches: 0.0341
trigger times: 20
Early stopping!
Start to test process.
Loss after 343314180 batches: 0.0333
Time to train on one home:  303.48723101615906
trigger times: 0
Loss after 343445280 batches: 0.6471
trigger times: 1
Loss after 343576380 batches: 0.4292
trigger times: 2
Loss after 343707480 batches: 0.3362
trigger times: 3
Loss after 343838580 batches: 0.2673
trigger times: 4
Loss after 343969680 batches: 0.2236
trigger times: 5
Loss after 344100780 batches: 0.1941
trigger times: 6
Loss after 344231880 batches: 0.1737
trigger times: 7
Loss after 344362980 batches: 0.1600
trigger times: 8
Loss after 344494080 batches: 0.1465
trigger times: 9
Loss after 344625180 batches: 0.1390
trigger times: 10
Loss after 344756280 batches: 0.1312
trigger times: 11
Loss after 344887380 batches: 0.1244
trigger times: 12
Loss after 345018480 batches: 0.1208
trigger times: 13
Loss after 345149580 batches: 0.1156
trigger times: 14
Loss after 345280680 batches: 0.1126
trigger times: 15
Loss after 345411780 batches: 0.1092
trigger times: 16
Loss after 345542880 batches: 0.1052
trigger times: 17
Loss after 345673980 batches: 0.1046
trigger times: 18
Loss after 345805080 batches: 0.0998
trigger times: 19
Loss after 345936180 batches: 0.0971
trigger times: 20
Early stopping!
Start to test process.
Loss after 346067280 batches: 0.0946
Time to train on one home:  163.2855875492096
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067]]
Round_3_results:  [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067]
trigger times: 0
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2725 < 2726; dropping {'Training_Loss': 0.6422157628195626, 'Validation_Loss': 0.7583117948638068, 'Training_R2': 0.3696163422243973, 'Validation_R2': 0.29393295721156476, 'Training_F1': 0.676318230201364, 'Validation_F1': 0.5201421624921365, 'Training_NEP': 0.6447936294969616, 'Validation_NEP': 1.170722323418002, 'Training_NDE': 0.4246202980126331, 'Validation_NDE': 0.5622745731812673, 'Training_MAE': 42.31898737656443, 'Validation_MAE': 32.106063137515896, 'Training_MSE': 5603.594, 'Validation_MSE': 2076.4646}.
Loss after 346169880 batches: 0.6422
trigger times: 0
Loss after 346272480 batches: 0.3935
trigger times: 1
Loss after 346375080 batches: 0.2968
trigger times: 2
Loss after 346477680 batches: 0.2390
trigger times: 3
Loss after 346580280 batches: 0.2086
trigger times: 4
Loss after 346682880 batches: 0.1868
trigger times: 5
Loss after 346785480 batches: 0.1728
trigger times: 6
Loss after 346888080 batches: 0.1546
trigger times: 7
Loss after 346990680 batches: 0.1583
trigger times: 8
Loss after 347093280 batches: 0.1376
trigger times: 9
Loss after 347195880 batches: 0.1247
trigger times: 10
Loss after 347298480 batches: 0.1145
trigger times: 11
Loss after 347401080 batches: 0.1197
trigger times: 12
Loss after 347503680 batches: 0.1285
trigger times: 13
Loss after 347606280 batches: 0.1071
trigger times: 14
Loss after 347708880 batches: 0.0985
trigger times: 15
Loss after 347811480 batches: 0.0964
trigger times: 16
Loss after 347914080 batches: 0.1017
trigger times: 17
Loss after 348016680 batches: 0.1017
trigger times: 18
Loss after 348119280 batches: 0.0865
trigger times: 19
Loss after 348221880 batches: 0.0856
trigger times: 20
Early stopping!
Start to test process.
Loss after 348324480 batches: 0.0809
Time to train on one home:  141.06911635398865
trigger times: 0
Loss after 348455580 batches: 0.4181
trigger times: 1
Loss after 348586680 batches: 0.2607
trigger times: 2
Loss after 348717780 batches: 0.1968
trigger times: 3
Loss after 348848880 batches: 0.1591
trigger times: 4
Loss after 348979980 batches: 0.1351
trigger times: 5
Loss after 349111080 batches: 0.1199
trigger times: 6
Loss after 349242180 batches: 0.1083
trigger times: 7
Loss after 349373280 batches: 0.0995
trigger times: 8
Loss after 349504380 batches: 0.0929
trigger times: 9
Loss after 349635480 batches: 0.0880
trigger times: 10
Loss after 349766580 batches: 0.0823
trigger times: 11
Loss after 349897680 batches: 0.0781
trigger times: 12
Loss after 350028780 batches: 0.0742
trigger times: 13
Loss after 350159880 batches: 0.0717
trigger times: 14
Loss after 350290980 batches: 0.0681
trigger times: 15
Loss after 350422080 batches: 0.0668
trigger times: 16
Loss after 350553180 batches: 0.0641
trigger times: 17
Loss after 350684280 batches: 0.0627
trigger times: 18
Loss after 350815380 batches: 0.0606
trigger times: 19
Loss after 350946480 batches: 0.0586
trigger times: 20
Early stopping!
Start to test process.
Loss after 351077580 batches: 0.0570
Time to train on one home:  164.03301095962524
trigger times: 0
Loss after 351208680 batches: 0.6901
trigger times: 1
Loss after 351339780 batches: 0.4595
trigger times: 2
Loss after 351470880 batches: 0.3455
trigger times: 3
Loss after 351601980 batches: 0.2815
trigger times: 4
Loss after 351733080 batches: 0.2404
trigger times: 5
Loss after 351864180 batches: 0.2049
trigger times: 6
Loss after 351995280 batches: 0.1838
trigger times: 7
Loss after 352126380 batches: 0.1655
trigger times: 8
Loss after 352257480 batches: 0.1510
trigger times: 9
Loss after 352388580 batches: 0.1398
trigger times: 10
Loss after 352519680 batches: 0.1306
trigger times: 11
Loss after 352650780 batches: 0.1230
trigger times: 12
Loss after 352781880 batches: 0.1170
trigger times: 13
Loss after 352912980 batches: 0.1122
trigger times: 14
Loss after 353044080 batches: 0.1059
trigger times: 15
Loss after 353175180 batches: 0.1018
trigger times: 16
Loss after 353306280 batches: 0.0987
trigger times: 17
Loss after 353437380 batches: 0.0961
trigger times: 18
Loss after 353568480 batches: 0.0924
trigger times: 19
Loss after 353699580 batches: 0.0882
trigger times: 20
Early stopping!
Start to test process.
Loss after 353830680 batches: 0.0871
Time to train on one home:  163.69841384887695
trigger times: 0
Loss after 353959320 batches: 0.3781
trigger times: 1
Loss after 354087960 batches: 0.2226
trigger times: 2
Loss after 354216600 batches: 0.1762
trigger times: 3
Loss after 354345240 batches: 0.1447
trigger times: 4
Loss after 354473880 batches: 0.1248
trigger times: 5
Loss after 354602520 batches: 0.1106
trigger times: 6
Loss after 354731160 batches: 0.0978
trigger times: 7
Loss after 354859800 batches: 0.0884
trigger times: 8
Loss after 354988440 batches: 0.0792
trigger times: 9
Loss after 355117080 batches: 0.0740
trigger times: 10
Loss after 355245720 batches: 0.0686
trigger times: 11
Loss after 355374360 batches: 0.0644
trigger times: 12
Loss after 355503000 batches: 0.0620
trigger times: 13
Loss after 355631640 batches: 0.0592
trigger times: 14
Loss after 355760280 batches: 0.0558
trigger times: 15
Loss after 355888920 batches: 0.0539
trigger times: 16
Loss after 356017560 batches: 0.0529
trigger times: 17
Loss after 356146200 batches: 0.0509
trigger times: 18
Loss after 356274840 batches: 0.0494
trigger times: 19
Loss after 356403480 batches: 0.0478
trigger times: 20
Early stopping!
Start to test process.
Loss after 356532120 batches: 0.0461
Time to train on one home:  161.1735520362854
trigger times: 0
Loss after 356663220 batches: 0.6918
trigger times: 1
Loss after 356794320 batches: 0.4789
trigger times: 2
Loss after 356925420 batches: 0.3524
trigger times: 3
Loss after 357056520 batches: 0.2654
trigger times: 4
Loss after 357187620 batches: 0.2138
trigger times: 5
Loss after 357318720 batches: 0.1817
trigger times: 6
Loss after 357449820 batches: 0.1588
trigger times: 7
Loss after 357580920 batches: 0.1426
trigger times: 8
Loss after 357712020 batches: 0.1308
trigger times: 9
Loss after 357843120 batches: 0.1229
trigger times: 10
Loss after 357974220 batches: 0.1162
trigger times: 11
Loss after 358105320 batches: 0.1094
trigger times: 12
Loss after 358236420 batches: 0.1039
trigger times: 13
Loss after 358367520 batches: 0.0983
trigger times: 14
Loss after 358498620 batches: 0.0961
trigger times: 15
Loss after 358629720 batches: 0.0912
trigger times: 16
Loss after 358760820 batches: 0.0878
trigger times: 17
Loss after 358891920 batches: 0.0849
trigger times: 18
Loss after 359023020 batches: 0.0824
trigger times: 19
Loss after 359154120 batches: 0.0799
trigger times: 20
Early stopping!
Start to test process.
Loss after 359285220 batches: 0.0779
Time to train on one home:  164.1973433494568
trigger times: 0
Loss after 359416320 batches: 0.7900
trigger times: 1
Loss after 359547420 batches: 0.6288
trigger times: 2
Loss after 359678520 batches: 0.4813
trigger times: 3
Loss after 359809620 batches: 0.4048
trigger times: 4
Loss after 359940720 batches: 0.3473
trigger times: 5
Loss after 360071820 batches: 0.2954
trigger times: 6
Loss after 360202920 batches: 0.2675
trigger times: 7
Loss after 360334020 batches: 0.2347
trigger times: 8
Loss after 360465120 batches: 0.2119
trigger times: 9
Loss after 360596220 batches: 0.1919
trigger times: 10
Loss after 360727320 batches: 0.1728
trigger times: 11
Loss after 360858420 batches: 0.1666
trigger times: 12
Loss after 360989520 batches: 0.1481
trigger times: 13
Loss after 361120620 batches: 0.1387
trigger times: 14
Loss after 361251720 batches: 0.1296
trigger times: 15
Loss after 361382820 batches: 0.1218
trigger times: 16
Loss after 361513920 batches: 0.1151
trigger times: 17
Loss after 361645020 batches: 0.1095
trigger times: 18
Loss after 361776120 batches: 0.1052
trigger times: 19
Loss after 361907220 batches: 0.1015
trigger times: 20
Early stopping!
Start to test process.
Loss after 362038320 batches: 0.1003
Time to train on one home:  164.3832142353058
trigger times: 0
Loss after 362169420 batches: 0.2678
trigger times: 1
Loss after 362300520 batches: 0.1402
trigger times: 2
Loss after 362431620 batches: 0.0985
trigger times: 0
Loss after 362562720 batches: 0.0776
trigger times: 1
Loss after 362693820 batches: 0.0653
trigger times: 0
Loss after 362824920 batches: 0.0582
trigger times: 0
Loss after 362956020 batches: 0.0521
trigger times: 1
Loss after 363087120 batches: 0.0481
trigger times: 2
Loss after 363218220 batches: 0.0455
trigger times: 0
Loss after 363349320 batches: 0.0425
trigger times: 0
Loss after 363480420 batches: 0.0407
trigger times: 1
Loss after 363611520 batches: 0.0391
trigger times: 0
Loss after 363742620 batches: 0.0370
trigger times: 1
Loss after 363873720 batches: 0.0364
trigger times: 2
Loss after 364004820 batches: 0.0348
trigger times: 3
Loss after 364135920 batches: 0.0332
trigger times: 4
Loss after 364267020 batches: 0.0321
trigger times: 5
Loss after 364398120 batches: 0.0320
trigger times: 0
Loss after 364529220 batches: 0.0306
trigger times: 1
Loss after 364660320 batches: 0.0293
trigger times: 0
Loss after 364791420 batches: 0.0293
trigger times: 0
Loss after 364922520 batches: 0.0285
trigger times: 0
Loss after 365053620 batches: 0.0271
trigger times: 1
Loss after 365184720 batches: 0.0271
trigger times: 2
Loss after 365315820 batches: 0.0266
trigger times: 0
Loss after 365446920 batches: 0.0258
trigger times: 0
Loss after 365578020 batches: 0.0255
trigger times: 1
Loss after 365709120 batches: 0.0259
trigger times: 2
Loss after 365840220 batches: 0.0247
trigger times: 0
Loss after 365971320 batches: 0.0247
trigger times: 1
Loss after 366102420 batches: 0.0241
trigger times: 2
Loss after 366233520 batches: 0.0239
trigger times: 0
Loss after 366364620 batches: 0.0231
trigger times: 1
Loss after 366495720 batches: 0.0230
trigger times: 0
Loss after 366626820 batches: 0.0222
trigger times: 0
Loss after 366757920 batches: 0.0221
trigger times: 1
Loss after 366889020 batches: 0.0214
trigger times: 0
Loss after 367020120 batches: 0.0215
trigger times: 1
Loss after 367151220 batches: 0.0214
trigger times: 2
Loss after 367282320 batches: 0.0213
trigger times: 3
Loss after 367413420 batches: 0.0204
trigger times: 4
Loss after 367544520 batches: 0.0202
trigger times: 5
Loss after 367675620 batches: 0.0196
trigger times: 0
Loss after 367806720 batches: 0.0196
trigger times: 1
Loss after 367937820 batches: 0.0193
trigger times: 2
Loss after 368068920 batches: 0.0193
trigger times: 0
Loss after 368200020 batches: 0.0190
trigger times: 0
Loss after 368331120 batches: 0.0188
trigger times: 1
Loss after 368462220 batches: 0.0187
trigger times: 2
Loss after 368593320 batches: 0.0180
trigger times: 0
Loss after 368724420 batches: 0.0178
trigger times: 1
Loss after 368855520 batches: 0.0174
trigger times: 2
Loss after 368986620 batches: 0.0173
trigger times: 3
Loss after 369117720 batches: 0.0175
trigger times: 4
Loss after 369248820 batches: 0.0171
trigger times: 5
Loss after 369379920 batches: 0.0171
trigger times: 6
Loss after 369511020 batches: 0.0169
trigger times: 7
Loss after 369642120 batches: 0.0167
trigger times: 8
Loss after 369773220 batches: 0.0163
trigger times: 9
Loss after 369904320 batches: 0.0161
trigger times: 10
Loss after 370035420 batches: 0.0159
trigger times: 11
Loss after 370166520 batches: 0.0159
trigger times: 12
Loss after 370297620 batches: 0.0159
trigger times: 13
Loss after 370428720 batches: 0.0160
trigger times: 14
Loss after 370559820 batches: 0.0157
trigger times: 15
Loss after 370690920 batches: 0.0154
trigger times: 16
Loss after 370822020 batches: 0.0153
trigger times: 17
Loss after 370953120 batches: 0.0152
trigger times: 18
Loss after 371084220 batches: 0.0151
trigger times: 19
Loss after 371215320 batches: 0.0150
trigger times: 20
Early stopping!
Start to test process.
Loss after 371346420 batches: 0.0148
Time to train on one home:  527.8936030864716
trigger times: 0
Loss after 371477520 batches: 0.2797
trigger times: 0
Loss after 371608620 batches: 0.1501
trigger times: 0
Loss after 371739720 batches: 0.1157
trigger times: 1
Loss after 371870820 batches: 0.0953
trigger times: 2
Loss after 372001920 batches: 0.0827
trigger times: 0
Loss after 372133020 batches: 0.0729
trigger times: 0
Loss after 372264120 batches: 0.0652
trigger times: 1
Loss after 372395220 batches: 0.0592
trigger times: 0
Loss after 372526320 batches: 0.0535
trigger times: 0
Loss after 372657420 batches: 0.0488
trigger times: 1
Loss after 372788520 batches: 0.0456
trigger times: 2
Loss after 372919620 batches: 0.0433
trigger times: 3
Loss after 373050720 batches: 0.0414
trigger times: 0
Loss after 373181820 batches: 0.0387
trigger times: 1
Loss after 373312920 batches: 0.0371
trigger times: 0
Loss after 373444020 batches: 0.0357
trigger times: 0
Loss after 373575120 batches: 0.0343
trigger times: 1
Loss after 373706220 batches: 0.0333
trigger times: 2
Loss after 373837320 batches: 0.0325
trigger times: 3
Loss after 373968420 batches: 0.0314
trigger times: 4
Loss after 374099520 batches: 0.0304
trigger times: 5
Loss after 374230620 batches: 0.0296
trigger times: 6
Loss after 374361720 batches: 0.0288
trigger times: 7
Loss after 374492820 batches: 0.0279
trigger times: 8
Loss after 374623920 batches: 0.0277
trigger times: 9
Loss after 374755020 batches: 0.0274
trigger times: 10
Loss after 374886120 batches: 0.0267
trigger times: 11
Loss after 375017220 batches: 0.0257
trigger times: 0
Loss after 375148320 batches: 0.0258
trigger times: 1
Loss after 375279420 batches: 0.0250
trigger times: 2
Loss after 375410520 batches: 0.0244
trigger times: 3
Loss after 375541620 batches: 0.0238
trigger times: 4
Loss after 375672720 batches: 0.0237
trigger times: 5
Loss after 375803820 batches: 0.0236
trigger times: 6
Loss after 375934920 batches: 0.0232
trigger times: 0
Loss after 376066020 batches: 0.0232
trigger times: 1
Loss after 376197120 batches: 0.0226
trigger times: 2
Loss after 376328220 batches: 0.0222
trigger times: 3
Loss after 376459320 batches: 0.0217
trigger times: 4
Loss after 376590420 batches: 0.0215
trigger times: 5
Loss after 376721520 batches: 0.0208
trigger times: 6
Loss after 376852620 batches: 0.0209
trigger times: 7
Loss after 376983720 batches: 0.0204
trigger times: 8
Loss after 377114820 batches: 0.0200
trigger times: 9
Loss after 377245920 batches: 0.0197
trigger times: 10
Loss after 377377020 batches: 0.0195
trigger times: 11
Loss after 377508120 batches: 0.0192
trigger times: 12
Loss after 377639220 batches: 0.0194
trigger times: 13
Loss after 377770320 batches: 0.0190
trigger times: 14
Loss after 377901420 batches: 0.0182
trigger times: 0
Loss after 378032520 batches: 0.0184
trigger times: 1
Loss after 378163620 batches: 0.0182
trigger times: 2
Loss after 378294720 batches: 0.0181
trigger times: 3
Loss after 378425820 batches: 0.0176
trigger times: 4
Loss after 378556920 batches: 0.0174
trigger times: 5
Loss after 378688020 batches: 0.0181
trigger times: 6
Loss after 378819120 batches: 0.0169
trigger times: 7
Loss after 378950220 batches: 0.0173
trigger times: 8
Loss after 379081320 batches: 0.0175
trigger times: 9
Loss after 379212420 batches: 0.0171
trigger times: 10
Loss after 379343520 batches: 0.0165
trigger times: 11
Loss after 379474620 batches: 0.0170
trigger times: 0
Loss after 379605720 batches: 0.0164
trigger times: 1
Loss after 379736820 batches: 0.0166
trigger times: 0
Loss after 379867920 batches: 0.0163
trigger times: 1
Loss after 379999020 batches: 0.0161
trigger times: 2
Loss after 380130120 batches: 0.0160
trigger times: 3
Loss after 380261220 batches: 0.0158
trigger times: 4
Loss after 380392320 batches: 0.0153
trigger times: 5
Loss after 380523420 batches: 0.0155
trigger times: 6
Loss after 380654520 batches: 0.0152
trigger times: 7
Loss after 380785620 batches: 0.0160
trigger times: 8
Loss after 380916720 batches: 0.0151
trigger times: 9
Loss after 381047820 batches: 0.0149
trigger times: 10
Loss after 381178920 batches: 0.0150
trigger times: 11
Loss after 381310020 batches: 0.0151
trigger times: 12
Loss after 381441120 batches: 0.0150
trigger times: 13
Loss after 381572220 batches: 0.0147
trigger times: 14
Loss after 381703320 batches: 0.0147
trigger times: 15
Loss after 381834420 batches: 0.0142
trigger times: 16
Loss after 381965520 batches: 0.0142
trigger times: 17
Loss after 382096620 batches: 0.0143
trigger times: 18
Loss after 382227720 batches: 0.0142
trigger times: 19
Loss after 382358820 batches: 0.0141
trigger times: 20
Early stopping!
Start to test process.
Loss after 382489920 batches: 0.0139
Time to train on one home:  628.3899583816528
trigger times: 0
Loss after 382568520 batches: 0.6851
trigger times: 0
Loss after 382647120 batches: 0.4636
trigger times: 1
Loss after 382725720 batches: 0.3223
trigger times: 2
Loss after 382804320 batches: 0.2411
trigger times: 3
Loss after 382882920 batches: 0.1982
trigger times: 4
Loss after 382961520 batches: 0.1687
trigger times: 5
Loss after 383040120 batches: 0.1457
trigger times: 6
Loss after 383118720 batches: 0.1315
trigger times: 7
Loss after 383197320 batches: 0.1177
trigger times: 8
Loss after 383275920 batches: 0.1108
trigger times: 9
Loss after 383354520 batches: 0.1046
trigger times: 10
Loss after 383433120 batches: 0.0977
trigger times: 11
Loss after 383511720 batches: 0.0938
trigger times: 12
Loss after 383590320 batches: 0.0888
trigger times: 13
Loss after 383668920 batches: 0.0853
trigger times: 14
Loss after 383747520 batches: 0.0822
trigger times: 15
Loss after 383826120 batches: 0.0811
trigger times: 16
Loss after 383904720 batches: 0.0776
trigger times: 17
Loss after 383983320 batches: 0.0732
trigger times: 18
Loss after 384061920 batches: 0.0706
trigger times: 19
Loss after 384140520 batches: 0.0709
trigger times: 20
Early stopping!
Start to test process.
Loss after 384219120 batches: 0.0661
Time to train on one home:  115.76092982292175
trigger times: 0
Loss after 384350220 batches: 0.2818
trigger times: 1
Loss after 384481320 batches: 0.1424
trigger times: 2
Loss after 384612420 batches: 0.0989
trigger times: 3
Loss after 384743520 batches: 0.0778
trigger times: 4
Loss after 384874620 batches: 0.0661
trigger times: 5
Loss after 385005720 batches: 0.0580
trigger times: 6
Loss after 385136820 batches: 0.0533
trigger times: 7
Loss after 385267920 batches: 0.0487
trigger times: 8
Loss after 385399020 batches: 0.0456
trigger times: 9
Loss after 385530120 batches: 0.0425
trigger times: 10
Loss after 385661220 batches: 0.0415
trigger times: 11
Loss after 385792320 batches: 0.0391
trigger times: 12
Loss after 385923420 batches: 0.0372
trigger times: 13
Loss after 386054520 batches: 0.0358
trigger times: 14
Loss after 386185620 batches: 0.0344
trigger times: 15
Loss after 386316720 batches: 0.0333
trigger times: 16
Loss after 386447820 batches: 0.0329
trigger times: 17
Loss after 386578920 batches: 0.0317
trigger times: 18
Loss after 386710020 batches: 0.0304
trigger times: 19
Loss after 386841120 batches: 0.0291
trigger times: 20
Early stopping!
Start to test process.
Loss after 386972220 batches: 0.0288
Time to train on one home:  164.88004183769226
trigger times: 0
Loss after 387103320 batches: 0.3290
trigger times: 1
Loss after 387234420 batches: 0.1775
trigger times: 2
Loss after 387365520 batches: 0.1370
trigger times: 3
Loss after 387496620 batches: 0.1151
trigger times: 4
Loss after 387627720 batches: 0.1010
trigger times: 5
Loss after 387758820 batches: 0.0889
trigger times: 6
Loss after 387889920 batches: 0.0801
trigger times: 7
Loss after 388021020 batches: 0.0720
trigger times: 8
Loss after 388152120 batches: 0.0669
trigger times: 9
Loss after 388283220 batches: 0.0619
trigger times: 10
Loss after 388414320 batches: 0.0581
trigger times: 11
Loss after 388545420 batches: 0.0568
trigger times: 12
Loss after 388676520 batches: 0.0534
trigger times: 13
Loss after 388807620 batches: 0.0507
trigger times: 0
Loss after 388938720 batches: 0.0487
trigger times: 1
Loss after 389069820 batches: 0.0473
trigger times: 0
Loss after 389200920 batches: 0.0460
trigger times: 1
Loss after 389332020 batches: 0.0433
trigger times: 0
Loss after 389463120 batches: 0.0419
trigger times: 0
Loss after 389594220 batches: 0.0406
trigger times: 0
Loss after 389725320 batches: 0.0396
trigger times: 1
Loss after 389856420 batches: 0.0387
trigger times: 0
Loss after 389987520 batches: 0.0373
trigger times: 1
Loss after 390118620 batches: 0.0368
trigger times: 2
Loss after 390249720 batches: 0.0355
trigger times: 3
Loss after 390380820 batches: 0.0345
trigger times: 4
Loss after 390511920 batches: 0.0334
trigger times: 0
Loss after 390643020 batches: 0.0330
trigger times: 0
Loss after 390774120 batches: 0.0327
trigger times: 1
Loss after 390905220 batches: 0.0320
trigger times: 0
Loss after 391036320 batches: 0.0315
trigger times: 1
Loss after 391167420 batches: 0.0305
trigger times: 0
Loss after 391298520 batches: 0.0303
trigger times: 0
Loss after 391429620 batches: 0.0297
trigger times: 1
Loss after 391560720 batches: 0.0290
trigger times: 0
Loss after 391691820 batches: 0.0287
trigger times: 1
Loss after 391822920 batches: 0.0281
trigger times: 2
Loss after 391954020 batches: 0.0276
trigger times: 0
Loss after 392085120 batches: 0.0269
trigger times: 1
Loss after 392216220 batches: 0.0274
trigger times: 2
Loss after 392347320 batches: 0.0264
trigger times: 0
Loss after 392478420 batches: 0.0263
trigger times: 1
Loss after 392609520 batches: 0.0267
trigger times: 2
Loss after 392740620 batches: 0.0258
trigger times: 3
Loss after 392871720 batches: 0.0257
trigger times: 4
Loss after 393002820 batches: 0.0252
trigger times: 5
Loss after 393133920 batches: 0.0244
trigger times: 6
Loss after 393265020 batches: 0.0248
trigger times: 7
Loss after 393396120 batches: 0.0242
trigger times: 8
Loss after 393527220 batches: 0.0240
trigger times: 9
Loss after 393658320 batches: 0.0238
trigger times: 10
Loss after 393789420 batches: 0.0237
trigger times: 0
Loss after 393920520 batches: 0.0232
trigger times: 0
Loss after 394051620 batches: 0.0230
trigger times: 1
Loss after 394182720 batches: 0.0227
trigger times: 0
Loss after 394313820 batches: 0.0225
trigger times: 1
Loss after 394444920 batches: 0.0226
trigger times: 2
Loss after 394576020 batches: 0.0221
trigger times: 3
Loss after 394707120 batches: 0.0220
trigger times: 4
Loss after 394838220 batches: 0.0219
trigger times: 5
Loss after 394969320 batches: 0.0216
trigger times: 6
Loss after 395100420 batches: 0.0216
trigger times: 7
Loss after 395231520 batches: 0.0212
trigger times: 0
Loss after 395362620 batches: 0.0215
trigger times: 1
Loss after 395493720 batches: 0.0208
trigger times: 2
Loss after 395624820 batches: 0.0207
trigger times: 3
Loss after 395755920 batches: 0.0207
trigger times: 4
Loss after 395887020 batches: 0.0205
trigger times: 5
Loss after 396018120 batches: 0.0199
trigger times: 6
Loss after 396149220 batches: 0.0199
trigger times: 7
Loss after 396280320 batches: 0.0199
trigger times: 0
Loss after 396411420 batches: 0.0200
trigger times: 1
Loss after 396542520 batches: 0.0198
trigger times: 2
Loss after 396673620 batches: 0.0194
trigger times: 3
Loss after 396804720 batches: 0.0195
trigger times: 4
Loss after 396935820 batches: 0.0198
trigger times: 5
Loss after 397066920 batches: 0.0195
trigger times: 6
Loss after 397198020 batches: 0.0193
trigger times: 7
Loss after 397329120 batches: 0.0194
trigger times: 8
Loss after 397460220 batches: 0.0188
trigger times: 0
Loss after 397591320 batches: 0.0189
trigger times: 1
Loss after 397722420 batches: 0.0186
trigger times: 2
Loss after 397853520 batches: 0.0187
trigger times: 3
Loss after 397984620 batches: 0.0182
trigger times: 4
Loss after 398115720 batches: 0.0183
trigger times: 5
Loss after 398246820 batches: 0.0184
trigger times: 6
Loss after 398377920 batches: 0.0185
trigger times: 7
Loss after 398509020 batches: 0.0180
trigger times: 8
Loss after 398640120 batches: 0.0182
trigger times: 9
Loss after 398771220 batches: 0.0177
trigger times: 10
Loss after 398902320 batches: 0.0181
trigger times: 11
Loss after 399033420 batches: 0.0180
trigger times: 12
Loss after 399164520 batches: 0.0175
trigger times: 13
Loss after 399295620 batches: 0.0178
trigger times: 14
Loss after 399426720 batches: 0.0174
trigger times: 15
Loss after 399557820 batches: 0.0172
trigger times: 16
Loss after 399688920 batches: 0.0175
trigger times: 17
Loss after 399820020 batches: 0.0170
trigger times: 18
Loss after 399951120 batches: 0.0170
trigger times: 19
Loss after 400082220 batches: 0.0170
trigger times: 20
Early stopping!
Start to test process.
Loss after 400213320 batches: 0.0172
Time to train on one home:  744.5968599319458
trigger times: 0
Loss after 400344420 batches: 0.6131
trigger times: 1
Loss after 400475520 batches: 0.3224
trigger times: 2
Loss after 400606620 batches: 0.2299
trigger times: 3
Loss after 400737720 batches: 0.1801
trigger times: 4
Loss after 400868820 batches: 0.1517
trigger times: 5
Loss after 400999920 batches: 0.1312
trigger times: 6
Loss after 401131020 batches: 0.1189
trigger times: 7
Loss after 401262120 batches: 0.1050
trigger times: 8
Loss after 401393220 batches: 0.0983
trigger times: 9
Loss after 401524320 batches: 0.0908
trigger times: 10
Loss after 401655420 batches: 0.0858
trigger times: 11
Loss after 401786520 batches: 0.0820
trigger times: 12
Loss after 401917620 batches: 0.0771
trigger times: 13
Loss after 402048720 batches: 0.0737
trigger times: 14
Loss after 402179820 batches: 0.0723
trigger times: 15
Loss after 402310920 batches: 0.0694
trigger times: 16
Loss after 402442020 batches: 0.0677
trigger times: 17
Loss after 402573120 batches: 0.0647
trigger times: 18
Loss after 402704220 batches: 0.0622
trigger times: 19
Loss after 402835320 batches: 0.0612
trigger times: 20
Early stopping!
Start to test process.
Loss after 402966420 batches: 0.0608
Time to train on one home:  164.15218925476074
trigger times: 0
Loss after 403097520 batches: 0.6218
trigger times: 1
Loss after 403228620 batches: 0.3968
trigger times: 2
Loss after 403359720 batches: 0.3074
trigger times: 3
Loss after 403490820 batches: 0.2521
trigger times: 4
Loss after 403621920 batches: 0.2104
trigger times: 5
Loss after 403753020 batches: 0.1793
trigger times: 6
Loss after 403884120 batches: 0.1557
trigger times: 7
Loss after 404015220 batches: 0.1411
trigger times: 8
Loss after 404146320 batches: 0.1296
trigger times: 9
Loss after 404277420 batches: 0.1183
trigger times: 10
Loss after 404408520 batches: 0.1112
trigger times: 11
Loss after 404539620 batches: 0.1051
trigger times: 12
Loss after 404670720 batches: 0.1006
trigger times: 13
Loss after 404801820 batches: 0.0955
trigger times: 14
Loss after 404932920 batches: 0.0923
trigger times: 15
Loss after 405064020 batches: 0.0883
trigger times: 16
Loss after 405195120 batches: 0.0854
trigger times: 17
Loss after 405326220 batches: 0.0831
trigger times: 18
Loss after 405457320 batches: 0.0801
trigger times: 19
Loss after 405588420 batches: 0.0786
trigger times: 20
Early stopping!
Start to test process.
Loss after 405719520 batches: 0.0759
Time to train on one home:  163.5192837715149
trigger times: 0
Loss after 405850620 batches: 0.5518
trigger times: 1
Loss after 405981720 batches: 0.2633
trigger times: 2
Loss after 406112820 batches: 0.1844
trigger times: 3
Loss after 406243920 batches: 0.1454
trigger times: 4
Loss after 406375020 batches: 0.1208
trigger times: 5
Loss after 406506120 batches: 0.1036
trigger times: 6
Loss after 406637220 batches: 0.0932
trigger times: 7
Loss after 406768320 batches: 0.0839
trigger times: 8
Loss after 406899420 batches: 0.0778
trigger times: 9
Loss after 407030520 batches: 0.0728
trigger times: 10
Loss after 407161620 batches: 0.0693
trigger times: 11
Loss after 407292720 batches: 0.0664
trigger times: 12
Loss after 407423820 batches: 0.0615
trigger times: 13
Loss after 407554920 batches: 0.0603
trigger times: 0
Loss after 407686020 batches: 0.0590
trigger times: 1
Loss after 407817120 batches: 0.0560
trigger times: 0
Loss after 407948220 batches: 0.0547
trigger times: 1
Loss after 408079320 batches: 0.0544
trigger times: 0
Loss after 408210420 batches: 0.0521
trigger times: 1
Loss after 408341520 batches: 0.0515
trigger times: 2
Loss after 408472620 batches: 0.0493
trigger times: 3
Loss after 408603720 batches: 0.0488
trigger times: 4
Loss after 408734820 batches: 0.0465
trigger times: 0
Loss after 408865920 batches: 0.0465
trigger times: 1
Loss after 408997020 batches: 0.0462
trigger times: 0
Loss after 409128120 batches: 0.0448
trigger times: 1
Loss after 409259220 batches: 0.0439
trigger times: 2
Loss after 409390320 batches: 0.0439
trigger times: 3
Loss after 409521420 batches: 0.0432
trigger times: 0
Loss after 409652520 batches: 0.0430
trigger times: 1
Loss after 409783620 batches: 0.0420
trigger times: 2
Loss after 409914720 batches: 0.0417
trigger times: 3
Loss after 410045820 batches: 0.0401
trigger times: 4
Loss after 410176920 batches: 0.0400
trigger times: 5
Loss after 410308020 batches: 0.0387
trigger times: 0
Loss after 410439120 batches: 0.0406
trigger times: 0
Loss after 410570220 batches: 0.0382
trigger times: 1
Loss after 410701320 batches: 0.0389
trigger times: 2
Loss after 410832420 batches: 0.0376
trigger times: 3
Loss after 410963520 batches: 0.0373
trigger times: 4
Loss after 411094620 batches: 0.0362
trigger times: 5
Loss after 411225720 batches: 0.0358
trigger times: 6
Loss after 411356820 batches: 0.0353
trigger times: 7
Loss after 411487920 batches: 0.0355
trigger times: 8
Loss after 411619020 batches: 0.0356
trigger times: 0
Loss after 411750120 batches: 0.0346
trigger times: 1
Loss after 411881220 batches: 0.0344
trigger times: 0
Loss after 412012320 batches: 0.0338
trigger times: 0
Loss after 412143420 batches: 0.0339
trigger times: 1
Loss after 412274520 batches: 0.0339
trigger times: 2
Loss after 412405620 batches: 0.0332
trigger times: 3
Loss after 412536720 batches: 0.0326
trigger times: 4
Loss after 412667820 batches: 0.0316
trigger times: 5
Loss after 412798920 batches: 0.0323
trigger times: 6
Loss after 412930020 batches: 0.0311
trigger times: 7
Loss after 413061120 batches: 0.0323
trigger times: 8
Loss after 413192220 batches: 0.0314
trigger times: 9
Loss after 413323320 batches: 0.0314
trigger times: 10
Loss after 413454420 batches: 0.0321
trigger times: 11
Loss after 413585520 batches: 0.0312
trigger times: 12
Loss after 413716620 batches: 0.0298
trigger times: 13
Loss after 413847720 batches: 0.0302
trigger times: 14
Loss after 413978820 batches: 0.0297
trigger times: 15
Loss after 414109920 batches: 0.0293
trigger times: 16
Loss after 414241020 batches: 0.0297
trigger times: 17
Loss after 414372120 batches: 0.0295
trigger times: 18
Loss after 414503220 batches: 0.0284
trigger times: 19
Loss after 414634320 batches: 0.0285
trigger times: 20
Early stopping!
Start to test process.
Loss after 414765420 batches: 0.0294
Time to train on one home:  512.792295217514
trigger times: 0
Loss after 414896520 batches: 0.5672
trigger times: 0
Loss after 415027620 batches: 0.4129
trigger times: 0
Loss after 415158720 batches: 0.3541
trigger times: 0
Loss after 415289820 batches: 0.3111
trigger times: 0
Loss after 415420920 batches: 0.2806
trigger times: 0
Loss after 415552020 batches: 0.2508
trigger times: 1
Loss after 415683120 batches: 0.2257
trigger times: 2
Loss after 415814220 batches: 0.2015
trigger times: 3
Loss after 415945320 batches: 0.1815
trigger times: 4
Loss after 416076420 batches: 0.1619
trigger times: 5
Loss after 416207520 batches: 0.1467
trigger times: 6
Loss after 416338620 batches: 0.1331
trigger times: 7
Loss after 416469720 batches: 0.1228
trigger times: 8
Loss after 416600820 batches: 0.1133
trigger times: 9
Loss after 416731920 batches: 0.1062
trigger times: 10
Loss after 416863020 batches: 0.0994
trigger times: 11
Loss after 416994120 batches: 0.0943
trigger times: 12
Loss after 417125220 batches: 0.0898
trigger times: 13
Loss after 417256320 batches: 0.0847
trigger times: 14
Loss after 417387420 batches: 0.0813
trigger times: 15
Loss after 417518520 batches: 0.0790
trigger times: 16
Loss after 417649620 batches: 0.0758
trigger times: 17
Loss after 417780720 batches: 0.0730
trigger times: 18
Loss after 417911820 batches: 0.0712
trigger times: 19
Loss after 418042920 batches: 0.0693
trigger times: 20
Early stopping!
Start to test process.
Loss after 418174020 batches: 0.0667
Time to train on one home:  202.22880339622498
trigger times: 0
Loss after 418305120 batches: 0.8231
trigger times: 0
Loss after 418436220 batches: 0.6771
trigger times: 1
Loss after 418567320 batches: 0.5329
trigger times: 2
Loss after 418698420 batches: 0.4152
trigger times: 3
Loss after 418829520 batches: 0.3400
trigger times: 4
Loss after 418960620 batches: 0.2921
trigger times: 5
Loss after 419091720 batches: 0.2599
trigger times: 6
Loss after 419222820 batches: 0.2336
trigger times: 7
Loss after 419353920 batches: 0.2070
trigger times: 8
Loss after 419485020 batches: 0.1864
trigger times: 9
Loss after 419616120 batches: 0.1694
trigger times: 10
Loss after 419747220 batches: 0.1558
trigger times: 11
Loss after 419878320 batches: 0.1450
trigger times: 12
Loss after 420009420 batches: 0.1366
trigger times: 13
Loss after 420140520 batches: 0.1303
trigger times: 14
Loss after 420271620 batches: 0.1227
trigger times: 15
Loss after 420402720 batches: 0.1164
trigger times: 16
Loss after 420533820 batches: 0.1127
trigger times: 17
Loss after 420664920 batches: 0.1058
trigger times: 18
Loss after 420796020 batches: 0.1012
trigger times: 19
Loss after 420927120 batches: 0.1002
trigger times: 20
Early stopping!
Start to test process.
Loss after 421058220 batches: 0.0939
Time to train on one home:  171.2150468826294
trigger times: 0
Loss after 421152180 batches: 0.7606
trigger times: 1
Loss after 421246140 batches: 0.5454
trigger times: 2
Loss after 421340100 batches: 0.3867
trigger times: 3
Loss after 421434060 batches: 0.2914
trigger times: 4
Loss after 421528020 batches: 0.2337
trigger times: 5
Loss after 421621980 batches: 0.2007
trigger times: 6
Loss after 421715940 batches: 0.1748
trigger times: 7
Loss after 421809900 batches: 0.1581
trigger times: 8
Loss after 421903860 batches: 0.1436
trigger times: 9
Loss after 421997820 batches: 0.1325
trigger times: 10
Loss after 422091780 batches: 0.1242
trigger times: 11
Loss after 422185740 batches: 0.1185
trigger times: 12
Loss after 422279700 batches: 0.1140
trigger times: 13
Loss after 422373660 batches: 0.1086
trigger times: 14
Loss after 422467620 batches: 0.1053
trigger times: 15
Loss after 422561580 batches: 0.1013
trigger times: 16
Loss after 422655540 batches: 0.0996
trigger times: 17
Loss after 422749500 batches: 0.0979
trigger times: 18
Loss after 422843460 batches: 0.0931
trigger times: 19
Loss after 422937420 batches: 0.0911
trigger times: 20
Early stopping!
Start to test process.
Loss after 423031380 batches: 0.0905
Time to train on one home:  126.55683946609497
trigger times: 0
Loss after 423162480 batches: 0.1095
trigger times: 1
Loss after 423293580 batches: 0.0331
trigger times: 2
Loss after 423424680 batches: 0.0251
trigger times: 3
Loss after 423555780 batches: 0.0215
trigger times: 4
Loss after 423686880 batches: 0.0182
trigger times: 5
Loss after 423817980 batches: 0.0161
trigger times: 6
Loss after 423949080 batches: 0.0147
trigger times: 7
Loss after 424080180 batches: 0.0134
trigger times: 0
Loss after 424211280 batches: 0.0127
trigger times: 0
Loss after 424342380 batches: 0.0114
trigger times: 0
Loss after 424473480 batches: 0.0105
trigger times: 0
Loss after 424604580 batches: 0.0099
trigger times: 1
Loss after 424735680 batches: 0.0094
trigger times: 0
Loss after 424866780 batches: 0.0089
trigger times: 1
Loss after 424997880 batches: 0.0087
trigger times: 0
Loss after 425128980 batches: 0.0081
trigger times: 0
Loss after 425260080 batches: 0.0077
trigger times: 0
Loss after 425391180 batches: 0.0077
trigger times: 1
Loss after 425522280 batches: 0.0074
trigger times: 0
Loss after 425653380 batches: 0.0070
trigger times: 1
Loss after 425784480 batches: 0.0069
trigger times: 0
Loss after 425915580 batches: 0.0067
trigger times: 1
Loss after 426046680 batches: 0.0065
trigger times: 0
Loss after 426177780 batches: 0.0064
trigger times: 1
Loss after 426308880 batches: 0.0063
trigger times: 2
Loss after 426439980 batches: 0.0061
trigger times: 3
Loss after 426571080 batches: 0.0060
trigger times: 4
Loss after 426702180 batches: 0.0058
trigger times: 5
Loss after 426833280 batches: 0.0057
trigger times: 6
Loss after 426964380 batches: 0.0057
trigger times: 0
Loss after 427095480 batches: 0.0055
trigger times: 1
Loss after 427226580 batches: 0.0055
trigger times: 2
Loss after 427357680 batches: 0.0053
trigger times: 3
Loss after 427488780 batches: 0.0052
trigger times: 4
Loss after 427619880 batches: 0.0052
trigger times: 0
Loss after 427750980 batches: 0.0052
trigger times: 1
Loss after 427882080 batches: 0.0052
trigger times: 2
Loss after 428013180 batches: 0.0052
trigger times: 3
Loss after 428144280 batches: 0.0051
trigger times: 0
Loss after 428275380 batches: 0.0049
trigger times: 1
Loss after 428406480 batches: 0.0048
trigger times: 2
Loss after 428537580 batches: 0.0048
trigger times: 3
Loss after 428668680 batches: 0.0047
trigger times: 4
Loss after 428799780 batches: 0.0045
trigger times: 5
Loss after 428930880 batches: 0.0047
trigger times: 6
Loss after 429061980 batches: 0.0046
trigger times: 7
Loss after 429193080 batches: 0.0046
trigger times: 8
Loss after 429324180 batches: 0.0044
trigger times: 9
Loss after 429455280 batches: 0.0043
trigger times: 10
Loss after 429586380 batches: 0.0042
trigger times: 11
Loss after 429717480 batches: 0.0043
trigger times: 12
Loss after 429848580 batches: 0.0042
trigger times: 13
Loss after 429979680 batches: 0.0042
trigger times: 14
Loss after 430110780 batches: 0.0040
trigger times: 15
Loss after 430241880 batches: 0.0041
trigger times: 16
Loss after 430372980 batches: 0.0041
trigger times: 17
Loss after 430504080 batches: 0.0040
trigger times: 18
Loss after 430635180 batches: 0.0039
trigger times: 19
Loss after 430766280 batches: 0.0040
trigger times: 20
Early stopping!
Start to test process.
Loss after 430897380 batches: 0.0040
Time to train on one home:  449.45467925071716
trigger times: 0
Loss after 431028480 batches: 0.2767
trigger times: 0
Loss after 431159580 batches: 0.1499
trigger times: 0
Loss after 431290680 batches: 0.1117
trigger times: 0
Loss after 431421780 batches: 0.0915
trigger times: 0
Loss after 431552880 batches: 0.0782
trigger times: 0
Loss after 431683980 batches: 0.0685
trigger times: 0
Loss after 431815080 batches: 0.0622
trigger times: 1
Loss after 431946180 batches: 0.0574
trigger times: 0
Loss after 432077280 batches: 0.0538
trigger times: 0
Loss after 432208380 batches: 0.0503
trigger times: 1
Loss after 432339480 batches: 0.0474
trigger times: 2
Loss after 432470580 batches: 0.0443
trigger times: 0
Loss after 432601680 batches: 0.0431
trigger times: 0
Loss after 432732780 batches: 0.0421
trigger times: 1
Loss after 432863880 batches: 0.0398
trigger times: 2
Loss after 432994980 batches: 0.0385
trigger times: 3
Loss after 433126080 batches: 0.0376
trigger times: 0
Loss after 433257180 batches: 0.0359
trigger times: 1
Loss after 433388280 batches: 0.0358
trigger times: 2
Loss after 433519380 batches: 0.0345
trigger times: 3
Loss after 433650480 batches: 0.0331
trigger times: 0
Loss after 433781580 batches: 0.0325
trigger times: 1
Loss after 433912680 batches: 0.0324
trigger times: 2
Loss after 434043780 batches: 0.0311
trigger times: 3
Loss after 434174880 batches: 0.0307
trigger times: 4
Loss after 434305980 batches: 0.0302
trigger times: 5
Loss after 434437080 batches: 0.0295
trigger times: 6
Loss after 434568180 batches: 0.0292
trigger times: 7
Loss after 434699280 batches: 0.0280
trigger times: 8
Loss after 434830380 batches: 0.0279
trigger times: 9
Loss after 434961480 batches: 0.0272
trigger times: 10
Loss after 435092580 batches: 0.0274
trigger times: 11
Loss after 435223680 batches: 0.0269
trigger times: 12
Loss after 435354780 batches: 0.0265
trigger times: 13
Loss after 435485880 batches: 0.0266
trigger times: 0
Loss after 435616980 batches: 0.0256
trigger times: 1
Loss after 435748080 batches: 0.0251
trigger times: 2
Loss after 435879180 batches: 0.0245
trigger times: 3
Loss after 436010280 batches: 0.0245
trigger times: 4
Loss after 436141380 batches: 0.0247
trigger times: 5
Loss after 436272480 batches: 0.0240
trigger times: 0
Loss after 436403580 batches: 0.0240
trigger times: 1
Loss after 436534680 batches: 0.0235
trigger times: 2
Loss after 436665780 batches: 0.0237
trigger times: 3
Loss after 436796880 batches: 0.0231
trigger times: 4
Loss after 436927980 batches: 0.0229
trigger times: 0
Loss after 437059080 batches: 0.0222
trigger times: 1
Loss after 437190180 batches: 0.0225
trigger times: 2
Loss after 437321280 batches: 0.0218
trigger times: 3
Loss after 437452380 batches: 0.0214
trigger times: 4
Loss after 437583480 batches: 0.0216
trigger times: 5
Loss after 437714580 batches: 0.0215
trigger times: 6
Loss after 437845680 batches: 0.0211
trigger times: 7
Loss after 437976780 batches: 0.0204
trigger times: 8
Loss after 438107880 batches: 0.0210
trigger times: 9
Loss after 438238980 batches: 0.0206
trigger times: 10
Loss after 438370080 batches: 0.0204
trigger times: 11
Loss after 438501180 batches: 0.0205
trigger times: 12
Loss after 438632280 batches: 0.0201
trigger times: 13
Loss after 438763380 batches: 0.0195
trigger times: 14
Loss after 438894480 batches: 0.0193
trigger times: 15
Loss after 439025580 batches: 0.0191
trigger times: 16
Loss after 439156680 batches: 0.0195
trigger times: 17
Loss after 439287780 batches: 0.0190
trigger times: 18
Loss after 439418880 batches: 0.0191
trigger times: 19
Loss after 439549980 batches: 0.0185
trigger times: 20
Early stopping!
Start to test process.
Loss after 439681080 batches: 0.0188
Time to train on one home:  499.9543297290802
trigger times: 0
Loss after 439812180 batches: 0.6412
trigger times: 0
Loss after 439943280 batches: 0.4154
trigger times: 0
Loss after 440074380 batches: 0.3117
trigger times: 1
Loss after 440205480 batches: 0.2476
trigger times: 2
Loss after 440336580 batches: 0.2018
trigger times: 3
Loss after 440467680 batches: 0.1730
trigger times: 4
Loss after 440598780 batches: 0.1559
trigger times: 5
Loss after 440729880 batches: 0.1418
trigger times: 6
Loss after 440860980 batches: 0.1309
trigger times: 7
Loss after 440992080 batches: 0.1231
trigger times: 8
Loss after 441123180 batches: 0.1190
trigger times: 9
Loss after 441254280 batches: 0.1133
trigger times: 10
Loss after 441385380 batches: 0.1095
trigger times: 11
Loss after 441516480 batches: 0.1064
trigger times: 12
Loss after 441647580 batches: 0.1043
trigger times: 13
Loss after 441778680 batches: 0.0998
trigger times: 14
Loss after 441909780 batches: 0.0982
trigger times: 15
Loss after 442040880 batches: 0.0947
trigger times: 16
Loss after 442171980 batches: 0.0941
trigger times: 17
Loss after 442303080 batches: 0.0918
trigger times: 18
Loss after 442434180 batches: 0.0894
trigger times: 19
Loss after 442565280 batches: 0.0868
trigger times: 20
Early stopping!
Start to test process.
Loss after 442696380 batches: 0.0856
Time to train on one home:  178.206458568573
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345]]
Round_4_results:  [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345]
trigger times: 0
Loss after 442798980 batches: 0.5761
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3482 < 3483; dropping {'Training_Loss': 0.576082285671007, 'Validation_Loss': 0.7826241321033902, 'Training_R2': 0.42128673592612764, 'Validation_R2': 0.2710271656571881, 'Training_F1': 0.6987858715127744, 'Validation_F1': 0.45158621541699406, 'Training_NEP': 0.603483269799814, 'Validation_NEP': 1.199977879365103, 'Training_NDE': 0.38981562358709687, 'Validation_NDE': 0.5805155381167679, 'Training_MAE': 39.607712775559456, 'Validation_MAE': 32.90837185545207, 'Training_MSE': 5144.287, 'Validation_MSE': 2143.828}.
trigger times: 1
Loss after 442901580 batches: 0.3820
trigger times: 0
Loss after 443004180 batches: 0.2953
trigger times: 1
Loss after 443106780 batches: 0.2251
trigger times: 2
Loss after 443209380 batches: 0.1686
trigger times: 3
Loss after 443311980 batches: 0.1514
trigger times: 4
Loss after 443414580 batches: 0.1447
trigger times: 5
Loss after 443517180 batches: 0.1419
trigger times: 6
Loss after 443619780 batches: 0.1271
trigger times: 7
Loss after 443722380 batches: 0.1156
trigger times: 8
Loss after 443824980 batches: 0.1021
trigger times: 9
Loss after 443927580 batches: 0.1000
trigger times: 10
Loss after 444030180 batches: 0.0918
trigger times: 11
Loss after 444132780 batches: 0.0882
trigger times: 12
Loss after 444235380 batches: 0.0846
trigger times: 13
Loss after 444337980 batches: 0.0798
trigger times: 14
Loss after 444440580 batches: 0.0800
trigger times: 15
Loss after 444543180 batches: 0.0772
trigger times: 16
Loss after 444645780 batches: 0.0730
trigger times: 17
Loss after 444748380 batches: 0.0717
trigger times: 18
Loss after 444850980 batches: 0.0708
trigger times: 19
Loss after 444953580 batches: 0.0668
trigger times: 20
Early stopping!
Start to test process.
Loss after 445056180 batches: 0.0648
Time to train on one home:  147.8188533782959
trigger times: 0
Loss after 445187280 batches: 0.3678
trigger times: 1
Loss after 445318380 batches: 0.2147
trigger times: 2
Loss after 445449480 batches: 0.1539
trigger times: 3
Loss after 445580580 batches: 0.1221
trigger times: 4
Loss after 445711680 batches: 0.1041
trigger times: 5
Loss after 445842780 batches: 0.0918
trigger times: 6
Loss after 445973880 batches: 0.0830
trigger times: 7
Loss after 446104980 batches: 0.0769
trigger times: 8
Loss after 446236080 batches: 0.0719
trigger times: 9
Loss after 446367180 batches: 0.0677
trigger times: 10
Loss after 446498280 batches: 0.0646
trigger times: 11
Loss after 446629380 batches: 0.0619
trigger times: 12
Loss after 446760480 batches: 0.0593
trigger times: 13
Loss after 446891580 batches: 0.0575
trigger times: 14
Loss after 447022680 batches: 0.0550
trigger times: 15
Loss after 447153780 batches: 0.0536
trigger times: 16
Loss after 447284880 batches: 0.0520
trigger times: 17
Loss after 447415980 batches: 0.0504
trigger times: 18
Loss after 447547080 batches: 0.0491
trigger times: 19
Loss after 447678180 batches: 0.0473
trigger times: 20
Early stopping!
Start to test process.
Loss after 447809280 batches: 0.0466
Time to train on one home:  164.46254801750183
trigger times: 0
Loss after 447940380 batches: 0.6411
trigger times: 1
Loss after 448071480 batches: 0.4088
trigger times: 2
Loss after 448202580 batches: 0.2954
trigger times: 3
Loss after 448333680 batches: 0.2337
trigger times: 4
Loss after 448464780 batches: 0.1943
trigger times: 5
Loss after 448595880 batches: 0.1704
trigger times: 6
Loss after 448726980 batches: 0.1494
trigger times: 7
Loss after 448858080 batches: 0.1371
trigger times: 8
Loss after 448989180 batches: 0.1261
trigger times: 9
Loss after 449120280 batches: 0.1174
trigger times: 10
Loss after 449251380 batches: 0.1117
trigger times: 11
Loss after 449382480 batches: 0.1054
trigger times: 12
Loss after 449513580 batches: 0.1020
trigger times: 13
Loss after 449644680 batches: 0.0973
trigger times: 14
Loss after 449775780 batches: 0.0928
trigger times: 15
Loss after 449906880 batches: 0.0894
trigger times: 16
Loss after 450037980 batches: 0.0875
trigger times: 17
Loss after 450169080 batches: 0.0842
trigger times: 18
Loss after 450300180 batches: 0.0810
trigger times: 19
Loss after 450431280 batches: 0.0796
trigger times: 20
Early stopping!
Start to test process.
Loss after 450562380 batches: 0.0777
Time to train on one home:  164.32227182388306
trigger times: 0
Loss after 450691020 batches: 0.3208
trigger times: 1
Loss after 450819660 batches: 0.1740
trigger times: 2
Loss after 450948300 batches: 0.1251
trigger times: 3
Loss after 451076940 batches: 0.1000
trigger times: 4
Loss after 451205580 batches: 0.0841
trigger times: 5
Loss after 451334220 batches: 0.0738
trigger times: 6
Loss after 451462860 batches: 0.0664
trigger times: 7
Loss after 451591500 batches: 0.0625
trigger times: 8
Loss after 451720140 batches: 0.0571
trigger times: 9
Loss after 451848780 batches: 0.0536
trigger times: 10
Loss after 451977420 batches: 0.0503
trigger times: 11
Loss after 452106060 batches: 0.0486
trigger times: 12
Loss after 452234700 batches: 0.0465
trigger times: 13
Loss after 452363340 batches: 0.0455
trigger times: 14
Loss after 452491980 batches: 0.0426
trigger times: 15
Loss after 452620620 batches: 0.0424
trigger times: 16
Loss after 452749260 batches: 0.0403
trigger times: 17
Loss after 452877900 batches: 0.0396
trigger times: 18
Loss after 453006540 batches: 0.0385
trigger times: 19
Loss after 453135180 batches: 0.0373
trigger times: 20
Early stopping!
Start to test process.
Loss after 453263820 batches: 0.0370
Time to train on one home:  162.50035762786865
trigger times: 0
Loss after 453394920 batches: 0.6812
trigger times: 1
Loss after 453526020 batches: 0.4415
trigger times: 2
Loss after 453657120 batches: 0.2956
trigger times: 3
Loss after 453788220 batches: 0.2177
trigger times: 4
Loss after 453919320 batches: 0.1752
trigger times: 5
Loss after 454050420 batches: 0.1480
trigger times: 6
Loss after 454181520 batches: 0.1311
trigger times: 7
Loss after 454312620 batches: 0.1188
trigger times: 8
Loss after 454443720 batches: 0.1099
trigger times: 9
Loss after 454574820 batches: 0.1015
trigger times: 10
Loss after 454705920 batches: 0.0961
trigger times: 11
Loss after 454837020 batches: 0.0912
trigger times: 12
Loss after 454968120 batches: 0.0877
trigger times: 13
Loss after 455099220 batches: 0.0845
trigger times: 14
Loss after 455230320 batches: 0.0803
trigger times: 15
Loss after 455361420 batches: 0.0777
trigger times: 16
Loss after 455492520 batches: 0.0752
trigger times: 17
Loss after 455623620 batches: 0.0728
trigger times: 18
Loss after 455754720 batches: 0.0727
trigger times: 19
Loss after 455885820 batches: 0.0700
trigger times: 20
Early stopping!
Start to test process.
Loss after 456016920 batches: 0.0683
Time to train on one home:  165.37686562538147
trigger times: 0
Loss after 456148020 batches: 0.7197
trigger times: 1
Loss after 456279120 batches: 0.5107
trigger times: 2
Loss after 456410220 batches: 0.3884
trigger times: 3
Loss after 456541320 batches: 0.3085
trigger times: 4
Loss after 456672420 batches: 0.2541
trigger times: 5
Loss after 456803520 batches: 0.2154
trigger times: 6
Loss after 456934620 batches: 0.1905
trigger times: 7
Loss after 457065720 batches: 0.1762
trigger times: 8
Loss after 457196820 batches: 0.1576
trigger times: 9
Loss after 457327920 batches: 0.1432
trigger times: 10
Loss after 457459020 batches: 0.1363
trigger times: 11
Loss after 457590120 batches: 0.1269
trigger times: 12
Loss after 457721220 batches: 0.1182
trigger times: 13
Loss after 457852320 batches: 0.1082
trigger times: 14
Loss after 457983420 batches: 0.1050
trigger times: 15
Loss after 458114520 batches: 0.1020
trigger times: 16
Loss after 458245620 batches: 0.0981
trigger times: 17
Loss after 458376720 batches: 0.0946
trigger times: 18
Loss after 458507820 batches: 0.0899
trigger times: 19
Loss after 458638920 batches: 0.0869
trigger times: 20
Early stopping!
Start to test process.
Loss after 458770020 batches: 0.0824
Time to train on one home:  164.57716298103333
trigger times: 0
Loss after 458901120 batches: 0.2185
trigger times: 0
Loss after 459032220 batches: 0.1044
trigger times: 0
Loss after 459163320 batches: 0.0716
trigger times: 0
Loss after 459294420 batches: 0.0573
trigger times: 0
Loss after 459425520 batches: 0.0493
trigger times: 1
Loss after 459556620 batches: 0.0441
trigger times: 0
Loss after 459687720 batches: 0.0412
trigger times: 0
Loss after 459818820 batches: 0.0385
trigger times: 1
Loss after 459949920 batches: 0.0359
trigger times: 0
Loss after 460081020 batches: 0.0344
trigger times: 0
Loss after 460212120 batches: 0.0315
trigger times: 1
Loss after 460343220 batches: 0.0311
trigger times: 2
Loss after 460474320 batches: 0.0298
trigger times: 0
Loss after 460605420 batches: 0.0287
trigger times: 1
Loss after 460736520 batches: 0.0279
trigger times: 2
Loss after 460867620 batches: 0.0277
trigger times: 0
Loss after 460998720 batches: 0.0259
trigger times: 1
Loss after 461129820 batches: 0.0255
trigger times: 2
Loss after 461260920 batches: 0.0252
trigger times: 3
Loss after 461392020 batches: 0.0237
trigger times: 0
Loss after 461523120 batches: 0.0235
trigger times: 1
Loss after 461654220 batches: 0.0226
trigger times: 2
Loss after 461785320 batches: 0.0231
trigger times: 3
Loss after 461916420 batches: 0.0222
trigger times: 4
Loss after 462047520 batches: 0.0220
trigger times: 5
Loss after 462178620 batches: 0.0218
trigger times: 6
Loss after 462309720 batches: 0.0208
trigger times: 7
Loss after 462440820 batches: 0.0204
trigger times: 8
Loss after 462571920 batches: 0.0203
trigger times: 9
Loss after 462703020 batches: 0.0200
trigger times: 10
Loss after 462834120 batches: 0.0196
trigger times: 11
Loss after 462965220 batches: 0.0193
trigger times: 12
Loss after 463096320 batches: 0.0188
trigger times: 13
Loss after 463227420 batches: 0.0189
trigger times: 14
Loss after 463358520 batches: 0.0186
trigger times: 15
Loss after 463489620 batches: 0.0182
trigger times: 16
Loss after 463620720 batches: 0.0187
trigger times: 17
Loss after 463751820 batches: 0.0179
trigger times: 18
Loss after 463882920 batches: 0.0173
trigger times: 19
Loss after 464014020 batches: 0.0175
trigger times: 20
Early stopping!
Start to test process.
Loss after 464145120 batches: 0.0169
Time to train on one home:  309.23361682891846
trigger times: 0
Loss after 464276220 batches: 0.2486
trigger times: 0
Loss after 464407320 batches: 0.1309
trigger times: 1
Loss after 464538420 batches: 0.0980
trigger times: 0
Loss after 464669520 batches: 0.0765
trigger times: 0
Loss after 464800620 batches: 0.0649
trigger times: 1
Loss after 464931720 batches: 0.0574
trigger times: 2
Loss after 465062820 batches: 0.0515
trigger times: 0
Loss after 465193920 batches: 0.0465
trigger times: 1
Loss after 465325020 batches: 0.0431
trigger times: 0
Loss after 465456120 batches: 0.0400
trigger times: 1
Loss after 465587220 batches: 0.0380
trigger times: 0
Loss after 465718320 batches: 0.0365
trigger times: 1
Loss after 465849420 batches: 0.0342
trigger times: 2
Loss after 465980520 batches: 0.0329
trigger times: 3
Loss after 466111620 batches: 0.0322
trigger times: 0
Loss after 466242720 batches: 0.0308
trigger times: 1
Loss after 466373820 batches: 0.0299
trigger times: 2
Loss after 466504920 batches: 0.0286
trigger times: 3
Loss after 466636020 batches: 0.0283
trigger times: 4
Loss after 466767120 batches: 0.0272
trigger times: 5
Loss after 466898220 batches: 0.0266
trigger times: 6
Loss after 467029320 batches: 0.0259
trigger times: 7
Loss after 467160420 batches: 0.0252
trigger times: 8
Loss after 467291520 batches: 0.0252
trigger times: 9
Loss after 467422620 batches: 0.0246
trigger times: 10
Loss after 467553720 batches: 0.0242
trigger times: 11
Loss after 467684820 batches: 0.0235
trigger times: 12
Loss after 467815920 batches: 0.0231
trigger times: 0
Loss after 467947020 batches: 0.0228
trigger times: 1
Loss after 468078120 batches: 0.0225
trigger times: 2
Loss after 468209220 batches: 0.0223
trigger times: 3
Loss after 468340320 batches: 0.0218
trigger times: 0
Loss after 468471420 batches: 0.0211
trigger times: 1
Loss after 468602520 batches: 0.0210
trigger times: 0
Loss after 468733620 batches: 0.0204
trigger times: 1
Loss after 468864720 batches: 0.0207
trigger times: 2
Loss after 468995820 batches: 0.0202
trigger times: 3
Loss after 469126920 batches: 0.0200
trigger times: 4
Loss after 469258020 batches: 0.0198
trigger times: 5
Loss after 469389120 batches: 0.0193
trigger times: 6
Loss after 469520220 batches: 0.0192
trigger times: 7
Loss after 469651320 batches: 0.0184
trigger times: 8
Loss after 469782420 batches: 0.0184
trigger times: 9
Loss after 469913520 batches: 0.0185
trigger times: 10
Loss after 470044620 batches: 0.0181
trigger times: 11
Loss after 470175720 batches: 0.0174
trigger times: 12
Loss after 470306820 batches: 0.0176
trigger times: 13
Loss after 470437920 batches: 0.0176
trigger times: 14
Loss after 470569020 batches: 0.0167
trigger times: 15
Loss after 470700120 batches: 0.0168
trigger times: 16
Loss after 470831220 batches: 0.0168
trigger times: 17
Loss after 470962320 batches: 0.0166
trigger times: 18
Loss after 471093420 batches: 0.0165
trigger times: 19
Loss after 471224520 batches: 0.0168
trigger times: 20
Early stopping!
Start to test process.
Loss after 471355620 batches: 0.0162
Time to train on one home:  410.1830585002899
trigger times: 0
Loss after 471434220 batches: 0.6351
trigger times: 1
Loss after 471512820 batches: 0.3650
trigger times: 2
Loss after 471591420 batches: 0.2360
trigger times: 3
Loss after 471670020 batches: 0.1751
trigger times: 4
Loss after 471748620 batches: 0.1408
trigger times: 5
Loss after 471827220 batches: 0.1178
trigger times: 6
Loss after 471905820 batches: 0.1066
trigger times: 7
Loss after 471984420 batches: 0.0965
trigger times: 8
Loss after 472063020 batches: 0.0884
trigger times: 0
Loss after 472141620 batches: 0.0822
trigger times: 1
Loss after 472220220 batches: 0.0776
trigger times: 2
Loss after 472298820 batches: 0.0747
trigger times: 3
Loss after 472377420 batches: 0.0703
trigger times: 0
Loss after 472456020 batches: 0.0684
trigger times: 1
Loss after 472534620 batches: 0.0662
trigger times: 2
Loss after 472613220 batches: 0.0640
trigger times: 3
Loss after 472691820 batches: 0.0618
trigger times: 0
Loss after 472770420 batches: 0.0586
trigger times: 1
Loss after 472849020 batches: 0.0585
trigger times: 2
Loss after 472927620 batches: 0.0572
trigger times: 0
Loss after 473006220 batches: 0.0546
trigger times: 1
Loss after 473084820 batches: 0.0540
trigger times: 2
Loss after 473163420 batches: 0.0514
trigger times: 3
Loss after 473242020 batches: 0.0516
trigger times: 4
Loss after 473320620 batches: 0.0502
trigger times: 5
Loss after 473399220 batches: 0.0491
trigger times: 6
Loss after 473477820 batches: 0.0491
trigger times: 7
Loss after 473556420 batches: 0.0478
trigger times: 8
Loss after 473635020 batches: 0.0473
trigger times: 9
Loss after 473713620 batches: 0.0470
trigger times: 10
Loss after 473792220 batches: 0.0449
trigger times: 11
Loss after 473870820 batches: 0.0458
trigger times: 12
Loss after 473949420 batches: 0.0450
trigger times: 13
Loss after 474028020 batches: 0.0433
trigger times: 14
Loss after 474106620 batches: 0.0425
trigger times: 15
Loss after 474185220 batches: 0.0424
trigger times: 16
Loss after 474263820 batches: 0.0414
trigger times: 17
Loss after 474342420 batches: 0.0407
trigger times: 18
Loss after 474421020 batches: 0.0403
trigger times: 19
Loss after 474499620 batches: 0.0408
trigger times: 20
Early stopping!
Start to test process.
Loss after 474578220 batches: 0.0403
Time to train on one home:  204.67091059684753
trigger times: 0
Loss after 474709320 batches: 0.2561
trigger times: 1
Loss after 474840420 batches: 0.1221
trigger times: 2
Loss after 474971520 batches: 0.0804
trigger times: 3
Loss after 475102620 batches: 0.0644
trigger times: 4
Loss after 475233720 batches: 0.0559
trigger times: 5
Loss after 475364820 batches: 0.0496
trigger times: 6
Loss after 475495920 batches: 0.0457
trigger times: 7
Loss after 475627020 batches: 0.0432
trigger times: 8
Loss after 475758120 batches: 0.0401
trigger times: 9
Loss after 475889220 batches: 0.0383
trigger times: 10
Loss after 476020320 batches: 0.0366
trigger times: 11
Loss after 476151420 batches: 0.0348
trigger times: 12
Loss after 476282520 batches: 0.0337
trigger times: 13
Loss after 476413620 batches: 0.0327
trigger times: 14
Loss after 476544720 batches: 0.0320
trigger times: 15
Loss after 476675820 batches: 0.0303
trigger times: 16
Loss after 476806920 batches: 0.0295
trigger times: 17
Loss after 476938020 batches: 0.0284
trigger times: 18
Loss after 477069120 batches: 0.0278
trigger times: 19
Loss after 477200220 batches: 0.0274
trigger times: 20
Early stopping!
Start to test process.
Loss after 477331320 batches: 0.0268
Time to train on one home:  165.70938897132874
trigger times: 0
Loss after 477462420 batches: 0.2732
trigger times: 1
Loss after 477593520 batches: 0.1303
trigger times: 2
Loss after 477724620 batches: 0.0954
trigger times: 3
Loss after 477855720 batches: 0.0775
trigger times: 4
Loss after 477986820 batches: 0.0664
trigger times: 5
Loss after 478117920 batches: 0.0600
trigger times: 6
Loss after 478249020 batches: 0.0557
trigger times: 7
Loss after 478380120 batches: 0.0513
trigger times: 8
Loss after 478511220 batches: 0.0496
trigger times: 0
Loss after 478642320 batches: 0.0469
trigger times: 0
Loss after 478773420 batches: 0.0442
trigger times: 0
Loss after 478904520 batches: 0.0436
trigger times: 0
Loss after 479035620 batches: 0.0414
trigger times: 1
Loss after 479166720 batches: 0.0395
trigger times: 0
Loss after 479297820 batches: 0.0385
trigger times: 1
Loss after 479428920 batches: 0.0371
trigger times: 2
Loss after 479560020 batches: 0.0360
trigger times: 3
Loss after 479691120 batches: 0.0354
trigger times: 4
Loss after 479822220 batches: 0.0344
trigger times: 0
Loss after 479953320 batches: 0.0332
trigger times: 1
Loss after 480084420 batches: 0.0328
trigger times: 0
Loss after 480215520 batches: 0.0320
trigger times: 1
Loss after 480346620 batches: 0.0315
trigger times: 2
Loss after 480477720 batches: 0.0311
trigger times: 3
Loss after 480608820 batches: 0.0303
trigger times: 0
Loss after 480739920 batches: 0.0299
trigger times: 1
Loss after 480871020 batches: 0.0291
trigger times: 2
Loss after 481002120 batches: 0.0286
trigger times: 3
Loss after 481133220 batches: 0.0282
trigger times: 0
Loss after 481264320 batches: 0.0276
trigger times: 1
Loss after 481395420 batches: 0.0276
trigger times: 2
Loss after 481526520 batches: 0.0270
trigger times: 3
Loss after 481657620 batches: 0.0264
trigger times: 4
Loss after 481788720 batches: 0.0267
trigger times: 0
Loss after 481919820 batches: 0.0260
trigger times: 1
Loss after 482050920 batches: 0.0256
trigger times: 0
Loss after 482182020 batches: 0.0251
trigger times: 1
Loss after 482313120 batches: 0.0249
trigger times: 2
Loss after 482444220 batches: 0.0251
trigger times: 3
Loss after 482575320 batches: 0.0248
trigger times: 4
Loss after 482706420 batches: 0.0240
trigger times: 0
Loss after 482837520 batches: 0.0235
trigger times: 1
Loss after 482968620 batches: 0.0237
trigger times: 2
Loss after 483099720 batches: 0.0230
trigger times: 3
Loss after 483230820 batches: 0.0233
trigger times: 4
Loss after 483361920 batches: 0.0229
trigger times: 0
Loss after 483493020 batches: 0.0228
trigger times: 1
Loss after 483624120 batches: 0.0225
trigger times: 2
Loss after 483755220 batches: 0.0225
trigger times: 3
Loss after 483886320 batches: 0.0220
trigger times: 4
Loss after 484017420 batches: 0.0219
trigger times: 5
Loss after 484148520 batches: 0.0217
trigger times: 6
Loss after 484279620 batches: 0.0217
trigger times: 0
Loss after 484410720 batches: 0.0211
trigger times: 1
Loss after 484541820 batches: 0.0216
trigger times: 2
Loss after 484672920 batches: 0.0209
trigger times: 3
Loss after 484804020 batches: 0.0210
trigger times: 4
Loss after 484935120 batches: 0.0210
trigger times: 5
Loss after 485066220 batches: 0.0202
trigger times: 6
Loss after 485197320 batches: 0.0202
trigger times: 7
Loss after 485328420 batches: 0.0200
trigger times: 8
Loss after 485459520 batches: 0.0201
trigger times: 9
Loss after 485590620 batches: 0.0196
trigger times: 10
Loss after 485721720 batches: 0.0195
trigger times: 11
Loss after 485852820 batches: 0.0197
trigger times: 12
Loss after 485983920 batches: 0.0191
trigger times: 13
Loss after 486115020 batches: 0.0196
trigger times: 14
Loss after 486246120 batches: 0.0194
trigger times: 15
Loss after 486377220 batches: 0.0191
trigger times: 16
Loss after 486508320 batches: 0.0188
trigger times: 17
Loss after 486639420 batches: 0.0187
trigger times: 18
Loss after 486770520 batches: 0.0187
trigger times: 19
Loss after 486901620 batches: 0.0187
trigger times: 20
Early stopping!
Start to test process.
Loss after 487032720 batches: 0.0185
Time to train on one home:  548.3528661727905
trigger times: 0
Loss after 487163820 batches: 0.5527
trigger times: 1
Loss after 487294920 batches: 0.2842
trigger times: 2
Loss after 487426020 batches: 0.1936
trigger times: 3
Loss after 487557120 batches: 0.1436
trigger times: 4
Loss after 487688220 batches: 0.1206
trigger times: 5
Loss after 487819320 batches: 0.1085
trigger times: 6
Loss after 487950420 batches: 0.0971
trigger times: 7
Loss after 488081520 batches: 0.0900
trigger times: 8
Loss after 488212620 batches: 0.0829
trigger times: 9
Loss after 488343720 batches: 0.0783
trigger times: 10
Loss after 488474820 batches: 0.0747
trigger times: 11
Loss after 488605920 batches: 0.0719
trigger times: 12
Loss after 488737020 batches: 0.0688
trigger times: 13
Loss after 488868120 batches: 0.0653
trigger times: 14
Loss after 488999220 batches: 0.0658
trigger times: 15
Loss after 489130320 batches: 0.0630
trigger times: 16
Loss after 489261420 batches: 0.0593
trigger times: 17
Loss after 489392520 batches: 0.0574
trigger times: 18
Loss after 489523620 batches: 0.0564
trigger times: 19
Loss after 489654720 batches: 0.0573
trigger times: 20
Early stopping!
Start to test process.
Loss after 489785820 batches: 0.0549
Time to train on one home:  163.95088601112366
trigger times: 0
Loss after 489916920 batches: 0.5598
trigger times: 1
Loss after 490048020 batches: 0.3307
trigger times: 2
Loss after 490179120 batches: 0.2343
trigger times: 3
Loss after 490310220 batches: 0.1797
trigger times: 4
Loss after 490441320 batches: 0.1469
trigger times: 5
Loss after 490572420 batches: 0.1275
trigger times: 6
Loss after 490703520 batches: 0.1144
trigger times: 7
Loss after 490834620 batches: 0.1037
trigger times: 8
Loss after 490965720 batches: 0.0956
trigger times: 9
Loss after 491096820 batches: 0.0903
trigger times: 10
Loss after 491227920 batches: 0.0858
trigger times: 11
Loss after 491359020 batches: 0.0806
trigger times: 12
Loss after 491490120 batches: 0.0770
trigger times: 13
Loss after 491621220 batches: 0.0742
trigger times: 14
Loss after 491752320 batches: 0.0713
trigger times: 15
Loss after 491883420 batches: 0.0698
trigger times: 16
Loss after 492014520 batches: 0.0669
trigger times: 17
Loss after 492145620 batches: 0.0668
trigger times: 18
Loss after 492276720 batches: 0.0643
trigger times: 19
Loss after 492407820 batches: 0.0624
trigger times: 20
Early stopping!
Start to test process.
Loss after 492538920 batches: 0.0608
Time to train on one home:  163.8225908279419
trigger times: 0
Loss after 492670020 batches: 0.4607
trigger times: 0
Loss after 492801120 batches: 0.2094
trigger times: 1
Loss after 492932220 batches: 0.1446
trigger times: 2
Loss after 493063320 batches: 0.1141
trigger times: 3
Loss after 493194420 batches: 0.0958
trigger times: 4
Loss after 493325520 batches: 0.0841
trigger times: 5
Loss after 493456620 batches: 0.0738
trigger times: 0
Loss after 493587720 batches: 0.0681
trigger times: 1
Loss after 493718820 batches: 0.0647
trigger times: 0
Loss after 493849920 batches: 0.0613
trigger times: 1
Loss after 493981020 batches: 0.0566
trigger times: 0
Loss after 494112120 batches: 0.0540
trigger times: 1
Loss after 494243220 batches: 0.0532
trigger times: 2
Loss after 494374320 batches: 0.0511
trigger times: 3
Loss after 494505420 batches: 0.0492
trigger times: 4
Loss after 494636520 batches: 0.0470
trigger times: 5
Loss after 494767620 batches: 0.0463
trigger times: 6
Loss after 494898720 batches: 0.0442
trigger times: 7
Loss after 495029820 batches: 0.0431
trigger times: 8
Loss after 495160920 batches: 0.0418
trigger times: 0
Loss after 495292020 batches: 0.0414
trigger times: 0
Loss after 495423120 batches: 0.0413
trigger times: 1
Loss after 495554220 batches: 0.0409
trigger times: 2
Loss after 495685320 batches: 0.0398
trigger times: 3
Loss after 495816420 batches: 0.0382
trigger times: 4
Loss after 495947520 batches: 0.0392
trigger times: 5
Loss after 496078620 batches: 0.0371
trigger times: 6
Loss after 496209720 batches: 0.0364
trigger times: 7
Loss after 496340820 batches: 0.0371
trigger times: 8
Loss after 496471920 batches: 0.0359
trigger times: 9
Loss after 496603020 batches: 0.0358
trigger times: 0
Loss after 496734120 batches: 0.0360
trigger times: 1
Loss after 496865220 batches: 0.0358
trigger times: 2
Loss after 496996320 batches: 0.0340
trigger times: 3
Loss after 497127420 batches: 0.0338
trigger times: 4
Loss after 497258520 batches: 0.0337
trigger times: 5
Loss after 497389620 batches: 0.0329
trigger times: 6
Loss after 497520720 batches: 0.0316
trigger times: 7
Loss after 497651820 batches: 0.0320
trigger times: 0
Loss after 497782920 batches: 0.0326
trigger times: 1
Loss after 497914020 batches: 0.0311
trigger times: 2
Loss after 498045120 batches: 0.0311
trigger times: 3
Loss after 498176220 batches: 0.0305
trigger times: 0
Loss after 498307320 batches: 0.0305
trigger times: 1
Loss after 498438420 batches: 0.0302
trigger times: 2
Loss after 498569520 batches: 0.0295
trigger times: 3
Loss after 498700620 batches: 0.0295
trigger times: 4
Loss after 498831720 batches: 0.0295
trigger times: 5
Loss after 498962820 batches: 0.0293
trigger times: 6
Loss after 499093920 batches: 0.0297
trigger times: 7
Loss after 499225020 batches: 0.0289
trigger times: 8
Loss after 499356120 batches: 0.0294
trigger times: 9
Loss after 499487220 batches: 0.0286
trigger times: 10
Loss after 499618320 batches: 0.0285
trigger times: 11
Loss after 499749420 batches: 0.0283
trigger times: 12
Loss after 499880520 batches: 0.0280
trigger times: 13
Loss after 500011620 batches: 0.0277
trigger times: 14
Loss after 500142720 batches: 0.0277
trigger times: 15
Loss after 500273820 batches: 0.0281
trigger times: 16
Loss after 500404920 batches: 0.0275
trigger times: 17
Loss after 500536020 batches: 0.0272
trigger times: 18
Loss after 500667120 batches: 0.0264
trigger times: 19
Loss after 500798220 batches: 0.0263
trigger times: 20
Early stopping!
Start to test process.
Loss after 500929320 batches: 0.0262
Time to train on one home:  474.33100938796997
trigger times: 0
Loss after 501060420 batches: 0.5563
trigger times: 0
Loss after 501191520 batches: 0.3978
trigger times: 0
Loss after 501322620 batches: 0.3390
trigger times: 1
Loss after 501453720 batches: 0.2875
trigger times: 2
Loss after 501584820 batches: 0.2506
trigger times: 3
Loss after 501715920 batches: 0.2204
trigger times: 4
Loss after 501847020 batches: 0.1934
trigger times: 5
Loss after 501978120 batches: 0.1705
trigger times: 6
Loss after 502109220 batches: 0.1512
trigger times: 7
Loss after 502240320 batches: 0.1373
trigger times: 8
Loss after 502371420 batches: 0.1278
trigger times: 9
Loss after 502502520 batches: 0.1191
trigger times: 10
Loss after 502633620 batches: 0.1106
trigger times: 11
Loss after 502764720 batches: 0.1047
trigger times: 12
Loss after 502895820 batches: 0.1005
trigger times: 13
Loss after 503026920 batches: 0.0945
trigger times: 14
Loss after 503158020 batches: 0.0901
trigger times: 15
Loss after 503289120 batches: 0.0862
trigger times: 16
Loss after 503420220 batches: 0.0830
trigger times: 17
Loss after 503551320 batches: 0.0795
trigger times: 18
Loss after 503682420 batches: 0.0770
trigger times: 19
Loss after 503813520 batches: 0.0736
trigger times: 20
Early stopping!
Start to test process.
Loss after 503944620 batches: 0.0713
Time to train on one home:  178.82842254638672
trigger times: 0
Loss after 504075720 batches: 0.8093
trigger times: 1
Loss after 504206820 batches: 0.6319
trigger times: 2
Loss after 504337920 batches: 0.4437
trigger times: 3
Loss after 504469020 batches: 0.3191
trigger times: 4
Loss after 504600120 batches: 0.2522
trigger times: 5
Loss after 504731220 batches: 0.2137
trigger times: 6
Loss after 504862320 batches: 0.1860
trigger times: 7
Loss after 504993420 batches: 0.1651
trigger times: 8
Loss after 505124520 batches: 0.1512
trigger times: 9
Loss after 505255620 batches: 0.1412
trigger times: 10
Loss after 505386720 batches: 0.1334
trigger times: 11
Loss after 505517820 batches: 0.1230
trigger times: 12
Loss after 505648920 batches: 0.1181
trigger times: 13
Loss after 505780020 batches: 0.1120
trigger times: 14
Loss after 505911120 batches: 0.1081
trigger times: 15
Loss after 506042220 batches: 0.1032
trigger times: 16
Loss after 506173320 batches: 0.0989
trigger times: 17
Loss after 506304420 batches: 0.0945
trigger times: 18
Loss after 506435520 batches: 0.0933
trigger times: 19
Loss after 506566620 batches: 0.0890
trigger times: 20
Early stopping!
Start to test process.
Loss after 506697720 batches: 0.0861
Time to train on one home:  165.17605662345886
trigger times: 0
Loss after 506791680 batches: 0.7309
trigger times: 1
Loss after 506885640 batches: 0.4847
trigger times: 2
Loss after 506979600 batches: 0.3305
trigger times: 3
Loss after 507073560 batches: 0.2464
trigger times: 4
Loss after 507167520 batches: 0.1955
trigger times: 5
Loss after 507261480 batches: 0.1631
trigger times: 6
Loss after 507355440 batches: 0.1420
trigger times: 7
Loss after 507449400 batches: 0.1282
trigger times: 8
Loss after 507543360 batches: 0.1183
trigger times: 9
Loss after 507637320 batches: 0.1078
trigger times: 10
Loss after 507731280 batches: 0.1027
trigger times: 11
Loss after 507825240 batches: 0.0967
trigger times: 12
Loss after 507919200 batches: 0.0950
trigger times: 13
Loss after 508013160 batches: 0.0915
trigger times: 14
Loss after 508107120 batches: 0.0875
trigger times: 15
Loss after 508201080 batches: 0.0842
trigger times: 16
Loss after 508295040 batches: 0.0829
trigger times: 17
Loss after 508389000 batches: 0.0796
trigger times: 18
Loss after 508482960 batches: 0.0780
trigger times: 19
Loss after 508576920 batches: 0.0765
trigger times: 20
Early stopping!
Start to test process.
Loss after 508670880 batches: 0.0753
Time to train on one home:  126.18292427062988
trigger times: 0
Loss after 508801980 batches: 0.0919
trigger times: 0
Loss after 508933080 batches: 0.0271
trigger times: 1
Loss after 509064180 batches: 0.0195
trigger times: 2
Loss after 509195280 batches: 0.0160
trigger times: 3
Loss after 509326380 batches: 0.0144
trigger times: 0
Loss after 509457480 batches: 0.0128
trigger times: 1
Loss after 509588580 batches: 0.0117
trigger times: 2
Loss after 509719680 batches: 0.0110
trigger times: 0
Loss after 509850780 batches: 0.0101
trigger times: 1
Loss after 509981880 batches: 0.0094
trigger times: 2
Loss after 510112980 batches: 0.0089
trigger times: 3
Loss after 510244080 batches: 0.0083
trigger times: 4
Loss after 510375180 batches: 0.0081
trigger times: 5
Loss after 510506280 batches: 0.0076
trigger times: 6
Loss after 510637380 batches: 0.0072
trigger times: 7
Loss after 510768480 batches: 0.0071
trigger times: 8
Loss after 510899580 batches: 0.0068
trigger times: 9
Loss after 511030680 batches: 0.0065
trigger times: 10
Loss after 511161780 batches: 0.0063
trigger times: 11
Loss after 511292880 batches: 0.0060
trigger times: 12
Loss after 511423980 batches: 0.0060
trigger times: 0
Loss after 511555080 batches: 0.0060
trigger times: 1
Loss after 511686180 batches: 0.0056
trigger times: 2
Loss after 511817280 batches: 0.0055
trigger times: 3
Loss after 511948380 batches: 0.0053
trigger times: 4
Loss after 512079480 batches: 0.0052
trigger times: 5
Loss after 512210580 batches: 0.0053
trigger times: 6
Loss after 512341680 batches: 0.0051
trigger times: 7
Loss after 512472780 batches: 0.0051
trigger times: 8
Loss after 512603880 batches: 0.0050
trigger times: 9
Loss after 512734980 batches: 0.0049
trigger times: 10
Loss after 512866080 batches: 0.0048
trigger times: 11
Loss after 512997180 batches: 0.0048
trigger times: 12
Loss after 513128280 batches: 0.0047
trigger times: 13
Loss after 513259380 batches: 0.0046
trigger times: 14
Loss after 513390480 batches: 0.0045
trigger times: 15
Loss after 513521580 batches: 0.0044
trigger times: 16
Loss after 513652680 batches: 0.0044
trigger times: 17
Loss after 513783780 batches: 0.0044
trigger times: 18
Loss after 513914880 batches: 0.0043
trigger times: 19
Loss after 514045980 batches: 0.0042
trigger times: 0
Loss after 514177080 batches: 0.0041
trigger times: 1
Loss after 514308180 batches: 0.0040
trigger times: 2
Loss after 514439280 batches: 0.0041
trigger times: 3
Loss after 514570380 batches: 0.0039
trigger times: 0
Loss after 514701480 batches: 0.0040
trigger times: 1
Loss after 514832580 batches: 0.0040
trigger times: 2
Loss after 514963680 batches: 0.0040
trigger times: 3
Loss after 515094780 batches: 0.0038
trigger times: 4
Loss after 515225880 batches: 0.0037
trigger times: 5
Loss after 515356980 batches: 0.0038
trigger times: 6
Loss after 515488080 batches: 0.0038
trigger times: 7
Loss after 515619180 batches: 0.0035
trigger times: 8
Loss after 515750280 batches: 0.0037
trigger times: 9
Loss after 515881380 batches: 0.0036
trigger times: 10
Loss after 516012480 batches: 0.0036
trigger times: 0
Loss after 516143580 batches: 0.0035
trigger times: 1
Loss after 516274680 batches: 0.0035
trigger times: 2
Loss after 516405780 batches: 0.0034
trigger times: 3
Loss after 516536880 batches: 0.0035
trigger times: 4
Loss after 516667980 batches: 0.0034
trigger times: 5
Loss after 516799080 batches: 0.0035
trigger times: 6
Loss after 516930180 batches: 0.0034
trigger times: 7
Loss after 517061280 batches: 0.0034
trigger times: 8
Loss after 517192380 batches: 0.0033
trigger times: 9
Loss after 517323480 batches: 0.0033
trigger times: 10
Loss after 517454580 batches: 0.0034
trigger times: 11
Loss after 517585680 batches: 0.0034
trigger times: 12
Loss after 517716780 batches: 0.0033
trigger times: 13
Loss after 517847880 batches: 0.0035
trigger times: 0
Loss after 517978980 batches: 0.0033
trigger times: 1
Loss after 518110080 batches: 0.0031
trigger times: 2
Loss after 518241180 batches: 0.0031
trigger times: 3
Loss after 518372280 batches: 0.0031
trigger times: 4
Loss after 518503380 batches: 0.0030
trigger times: 5
Loss after 518634480 batches: 0.0030
trigger times: 6
Loss after 518765580 batches: 0.0031
trigger times: 7
Loss after 518896680 batches: 0.0031
trigger times: 8
Loss after 519027780 batches: 0.0030
trigger times: 9
Loss after 519158880 batches: 0.0029
trigger times: 10
Loss after 519289980 batches: 0.0030
trigger times: 11
Loss after 519421080 batches: 0.0031
trigger times: 12
Loss after 519552180 batches: 0.0031
trigger times: 13
Loss after 519683280 batches: 0.0029
trigger times: 14
Loss after 519814380 batches: 0.0031
trigger times: 15
Loss after 519945480 batches: 0.0031
trigger times: 16
Loss after 520076580 batches: 0.0030
trigger times: 17
Loss after 520207680 batches: 0.0030
trigger times: 18
Loss after 520338780 batches: 0.0029
trigger times: 19
Loss after 520469880 batches: 0.0028
trigger times: 20
Early stopping!
Start to test process.
Loss after 520600980 batches: 0.0029
Time to train on one home:  677.5412979125977
trigger times: 0
Loss after 520732080 batches: 0.2583
trigger times: 0
Loss after 520863180 batches: 0.1424
trigger times: 0
Loss after 520994280 batches: 0.1017
trigger times: 0
Loss after 521125380 batches: 0.0829
trigger times: 1
Loss after 521256480 batches: 0.0705
trigger times: 0
Loss after 521387580 batches: 0.0622
trigger times: 0
Loss after 521518680 batches: 0.0567
trigger times: 0
Loss after 521649780 batches: 0.0516
trigger times: 0
Loss after 521780880 batches: 0.0482
trigger times: 0
Loss after 521911980 batches: 0.0457
trigger times: 1
Loss after 522043080 batches: 0.0422
trigger times: 0
Loss after 522174180 batches: 0.0407
trigger times: 1
Loss after 522305280 batches: 0.0388
trigger times: 2
Loss after 522436380 batches: 0.0364
trigger times: 3
Loss after 522567480 batches: 0.0361
trigger times: 4
Loss after 522698580 batches: 0.0347
trigger times: 5
Loss after 522829680 batches: 0.0336
trigger times: 0
Loss after 522960780 batches: 0.0321
trigger times: 1
Loss after 523091880 batches: 0.0313
trigger times: 2
Loss after 523222980 batches: 0.0305
trigger times: 3
Loss after 523354080 batches: 0.0295
trigger times: 4
Loss after 523485180 batches: 0.0286
trigger times: 5
Loss after 523616280 batches: 0.0284
trigger times: 6
Loss after 523747380 batches: 0.0281
trigger times: 0
Loss after 523878480 batches: 0.0279
trigger times: 0
Loss after 524009580 batches: 0.0271
trigger times: 0
Loss after 524140680 batches: 0.0268
trigger times: 0
Loss after 524271780 batches: 0.0261
trigger times: 1
Loss after 524402880 batches: 0.0258
trigger times: 2
Loss after 524533980 batches: 0.0252
trigger times: 3
Loss after 524665080 batches: 0.0250
trigger times: 4
Loss after 524796180 batches: 0.0250
trigger times: 5
Loss after 524927280 batches: 0.0241
trigger times: 6
Loss after 525058380 batches: 0.0238
trigger times: 7
Loss after 525189480 batches: 0.0232
trigger times: 8
Loss after 525320580 batches: 0.0233
trigger times: 9
Loss after 525451680 batches: 0.0227
trigger times: 10
Loss after 525582780 batches: 0.0226
trigger times: 11
Loss after 525713880 batches: 0.0224
trigger times: 12
Loss after 525844980 batches: 0.0218
trigger times: 13
Loss after 525976080 batches: 0.0216
trigger times: 0
Loss after 526107180 batches: 0.0211
trigger times: 1
Loss after 526238280 batches: 0.0203
trigger times: 0
Loss after 526369380 batches: 0.0211
trigger times: 0
Loss after 526500480 batches: 0.0208
trigger times: 0
Loss after 526631580 batches: 0.0204
trigger times: 1
Loss after 526762680 batches: 0.0200
trigger times: 2
Loss after 526893780 batches: 0.0199
trigger times: 3
Loss after 527024880 batches: 0.0195
trigger times: 4
Loss after 527155980 batches: 0.0194
trigger times: 5
Loss after 527287080 batches: 0.0190
trigger times: 6
Loss after 527418180 batches: 0.0190
trigger times: 7
Loss after 527549280 batches: 0.0189
trigger times: 8
Loss after 527680380 batches: 0.0187
trigger times: 9
Loss after 527811480 batches: 0.0184
trigger times: 10
Loss after 527942580 batches: 0.0183
trigger times: 11
Loss after 528073680 batches: 0.0185
trigger times: 12
Loss after 528204780 batches: 0.0176
trigger times: 13
Loss after 528335880 batches: 0.0180
trigger times: 14
Loss after 528466980 batches: 0.0177
trigger times: 15
Loss after 528598080 batches: 0.0173
trigger times: 16
Loss after 528729180 batches: 0.0179
trigger times: 17
Loss after 528860280 batches: 0.0172
trigger times: 18
Loss after 528991380 batches: 0.0173
trigger times: 0
Loss after 529122480 batches: 0.0168
trigger times: 1
Loss after 529253580 batches: 0.0168
trigger times: 2
Loss after 529384680 batches: 0.0164
trigger times: 3
Loss after 529515780 batches: 0.0166
trigger times: 4
Loss after 529646880 batches: 0.0162
trigger times: 5
Loss after 529777980 batches: 0.0162
trigger times: 6
Loss after 529909080 batches: 0.0161
trigger times: 7
Loss after 530040180 batches: 0.0161
trigger times: 8
Loss after 530171280 batches: 0.0161
trigger times: 9
Loss after 530302380 batches: 0.0156
trigger times: 10
Loss after 530433480 batches: 0.0160
trigger times: 11
Loss after 530564580 batches: 0.0156
trigger times: 12
Loss after 530695680 batches: 0.0157
trigger times: 13
Loss after 530826780 batches: 0.0158
trigger times: 14
Loss after 530957880 batches: 0.0154
trigger times: 15
Loss after 531088980 batches: 0.0150
trigger times: 16
Loss after 531220080 batches: 0.0155
trigger times: 17
Loss after 531351180 batches: 0.0153
trigger times: 18
Loss after 531482280 batches: 0.0150
trigger times: 19
Loss after 531613380 batches: 0.0147
trigger times: 20
Early stopping!
Start to test process.
Loss after 531744480 batches: 0.0150
Time to train on one home:  630.2959201335907
trigger times: 0
Loss after 531875580 batches: 0.5848
trigger times: 1
Loss after 532006680 batches: 0.3522
trigger times: 2
Loss after 532137780 batches: 0.2405
trigger times: 3
Loss after 532268880 batches: 0.1860
trigger times: 4
Loss after 532399980 batches: 0.1548
trigger times: 5
Loss after 532531080 batches: 0.1383
trigger times: 0
Loss after 532662180 batches: 0.1260
trigger times: 0
Loss after 532793280 batches: 0.1166
trigger times: 1
Loss after 532924380 batches: 0.1098
trigger times: 2
Loss after 533055480 batches: 0.1037
trigger times: 0
Loss after 533186580 batches: 0.0989
trigger times: 0
Loss after 533317680 batches: 0.0956
trigger times: 1
Loss after 533448780 batches: 0.0916
trigger times: 2
Loss after 533579880 batches: 0.0896
trigger times: 3
Loss after 533710980 batches: 0.0870
trigger times: 4
Loss after 533842080 batches: 0.0842
trigger times: 0
Loss after 533973180 batches: 0.0841
trigger times: 0
Loss after 534104280 batches: 0.0808
trigger times: 0
Loss after 534235380 batches: 0.0807
trigger times: 1
Loss after 534366480 batches: 0.0780
trigger times: 0
Loss after 534497580 batches: 0.0768
trigger times: 1
Loss after 534628680 batches: 0.0760
trigger times: 0
Loss after 534759780 batches: 0.0745
trigger times: 0
Loss after 534890880 batches: 0.0728
trigger times: 1
Loss after 535021980 batches: 0.0722
trigger times: 2
Loss after 535153080 batches: 0.0705
trigger times: 3
Loss after 535284180 batches: 0.0709
trigger times: 0
Loss after 535415280 batches: 0.0698
trigger times: 1
Loss after 535546380 batches: 0.0691
trigger times: 2
Loss after 535677480 batches: 0.0667
trigger times: 3
Loss after 535808580 batches: 0.0676
trigger times: 0
Loss after 535939680 batches: 0.0657
trigger times: 1
Loss after 536070780 batches: 0.0651
trigger times: 2
Loss after 536201880 batches: 0.0646
trigger times: 3
Loss after 536332980 batches: 0.0631
trigger times: 4
Loss after 536464080 batches: 0.0643
trigger times: 5
Loss after 536595180 batches: 0.0635
trigger times: 6
Loss after 536726280 batches: 0.0620
trigger times: 7
Loss after 536857380 batches: 0.0632
trigger times: 8
Loss after 536988480 batches: 0.0621
trigger times: 9
Loss after 537119580 batches: 0.0607
trigger times: 0
Loss after 537250680 batches: 0.0606
trigger times: 1
Loss after 537381780 batches: 0.0594
trigger times: 2
Loss after 537512880 batches: 0.0592
trigger times: 0
Loss after 537643980 batches: 0.0590
trigger times: 1
Loss after 537775080 batches: 0.0578
trigger times: 2
Loss after 537906180 batches: 0.0587
trigger times: 0
Loss after 538037280 batches: 0.0584
trigger times: 1
Loss after 538168380 batches: 0.0581
trigger times: 2
Loss after 538299480 batches: 0.0562
trigger times: 3
Loss after 538430580 batches: 0.0562
trigger times: 0
Loss after 538561680 batches: 0.0565
trigger times: 1
Loss after 538692780 batches: 0.0551
trigger times: 2
Loss after 538823880 batches: 0.0558
trigger times: 3
Loss after 538954980 batches: 0.0551
trigger times: 4
Loss after 539086080 batches: 0.0557
trigger times: 5
Loss after 539217180 batches: 0.0541
trigger times: 6
Loss after 539348280 batches: 0.0543
trigger times: 7
Loss after 539479380 batches: 0.0544
trigger times: 8
Loss after 539610480 batches: 0.0536
trigger times: 9
Loss after 539741580 batches: 0.0529
trigger times: 10
Loss after 539872680 batches: 0.0529
trigger times: 11
Loss after 540003780 batches: 0.0532
trigger times: 12
Loss after 540134880 batches: 0.0524
trigger times: 13
Loss after 540265980 batches: 0.0524
trigger times: 14
Loss after 540397080 batches: 0.0518
trigger times: 0
Loss after 540528180 batches: 0.0523
trigger times: 1
Loss after 540659280 batches: 0.0511
trigger times: 2
Loss after 540790380 batches: 0.0516
trigger times: 3
Loss after 540921480 batches: 0.0508
trigger times: 4
Loss after 541052580 batches: 0.0514
trigger times: 5
Loss after 541183680 batches: 0.0502
trigger times: 6
Loss after 541314780 batches: 0.0500
trigger times: 0
Loss after 541445880 batches: 0.0508
trigger times: 0
Loss after 541576980 batches: 0.0501
trigger times: 1
Loss after 541708080 batches: 0.0501
trigger times: 2
Loss after 541839180 batches: 0.0492
trigger times: 3
Loss after 541970280 batches: 0.0492
trigger times: 4
Loss after 542101380 batches: 0.0494
trigger times: 5
Loss after 542232480 batches: 0.0488
trigger times: 6
Loss after 542363580 batches: 0.0485
trigger times: 0
Loss after 542494680 batches: 0.0492
trigger times: 1
Loss after 542625780 batches: 0.0485
trigger times: 2
Loss after 542756880 batches: 0.0485
trigger times: 0
Loss after 542887980 batches: 0.0476
trigger times: 1
Loss after 543019080 batches: 0.0476
trigger times: 2
Loss after 543150180 batches: 0.0479
trigger times: 0
Loss after 543281280 batches: 0.0477
trigger times: 1
Loss after 543412380 batches: 0.0479
trigger times: 0
Loss after 543543480 batches: 0.0476
trigger times: 1
Loss after 543674580 batches: 0.0473
trigger times: 2
Loss after 543805680 batches: 0.0471
trigger times: 3
Loss after 543936780 batches: 0.0468
trigger times: 4
Loss after 544067880 batches: 0.0463
trigger times: 5
Loss after 544198980 batches: 0.0462
trigger times: 6
Loss after 544330080 batches: 0.0465
trigger times: 0
Loss after 544461180 batches: 0.0467
trigger times: 1
Loss after 544592280 batches: 0.0463
trigger times: 2
Loss after 544723380 batches: 0.0466
trigger times: 3
Loss after 544854480 batches: 0.0457
trigger times: 4
Loss after 544985580 batches: 0.0459
trigger times: 5
Loss after 545116680 batches: 0.0451
trigger times: 6
Loss after 545247780 batches: 0.0453
trigger times: 7
Loss after 545378880 batches: 0.0452
trigger times: 8
Loss after 545509980 batches: 0.0453
trigger times: 9
Loss after 545641080 batches: 0.0450
trigger times: 10
Loss after 545772180 batches: 0.0451
trigger times: 11
Loss after 545903280 batches: 0.0453
trigger times: 12
Loss after 546034380 batches: 0.0450
trigger times: 13
Loss after 546165480 batches: 0.0447
trigger times: 14
Loss after 546296580 batches: 0.0446
trigger times: 15
Loss after 546427680 batches: 0.0441
trigger times: 0
Loss after 546558780 batches: 0.0444
trigger times: 1
Loss after 546689880 batches: 0.0450
trigger times: 2
Loss after 546820980 batches: 0.0437
trigger times: 3
Loss after 546952080 batches: 0.0439
trigger times: 4
Loss after 547083180 batches: 0.0441
trigger times: 5
Loss after 547214280 batches: 0.0434
trigger times: 0
Loss after 547345380 batches: 0.0437
trigger times: 1
Loss after 547476480 batches: 0.0439
trigger times: 2
Loss after 547607580 batches: 0.0429
trigger times: 3
Loss after 547738680 batches: 0.0433
trigger times: 4
Loss after 547869780 batches: 0.0427
trigger times: 5
Loss after 548000880 batches: 0.0427
trigger times: 6
Loss after 548131980 batches: 0.0423
trigger times: 7
Loss after 548263080 batches: 0.0424
trigger times: 0
Loss after 548394180 batches: 0.0425
trigger times: 0
Loss after 548525280 batches: 0.0423
trigger times: 1
Loss after 548656380 batches: 0.0418
trigger times: 2
Loss after 548787480 batches: 0.0424
trigger times: 3
Loss after 548918580 batches: 0.0423
trigger times: 4
Loss after 549049680 batches: 0.0423
trigger times: 5
Loss after 549180780 batches: 0.0419
trigger times: 6
Loss after 549311880 batches: 0.0423
trigger times: 7
Loss after 549442980 batches: 0.0420
trigger times: 8
Loss after 549574080 batches: 0.0418
trigger times: 9
Loss after 549705180 batches: 0.0414
trigger times: 10
Loss after 549836280 batches: 0.0409
trigger times: 11
Loss after 549967380 batches: 0.0415
trigger times: 12
Loss after 550098480 batches: 0.0409
trigger times: 13
Loss after 550229580 batches: 0.0409
trigger times: 14
Loss after 550360680 batches: 0.0410
trigger times: 15
Loss after 550491780 batches: 0.0411
trigger times: 16
Loss after 550622880 batches: 0.0413
trigger times: 17
Loss after 550753980 batches: 0.0408
trigger times: 18
Loss after 550885080 batches: 0.0410
trigger times: 19
Loss after 551016180 batches: 0.0409
trigger times: 20
Early stopping!
Start to test process.
Loss after 551147280 batches: 0.0412
Time to train on one home:  1085.3862643241882
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219]]
Round_5_results:  [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4337 < 4338; dropping {'Training_Loss': 0.5696804001927376, 'Validation_Loss': 0.8568994998931885, 'Training_R2': 0.4203940673021038, 'Validation_R2': 0.20195912095208768, 'Training_F1': 0.7014377671717746, 'Validation_F1': 0.43822048209400477, 'Training_NEP': 0.5974818049592991, 'Validation_NEP': 1.3448094768126058, 'Training_NDE': 0.3904169164862449, 'Validation_NDE': 0.6355176880594351, 'Training_MAE': 39.213825641431306, 'Validation_MAE': 36.880255126953124, 'Training_MSE': 5152.222, 'Validation_MSE': 2346.9492}.
trigger times: 0
Loss after 551249880 batches: 0.5697
trigger times: 0
Loss after 551352480 batches: 0.3584
trigger times: 0
Loss after 551455080 batches: 0.2444
trigger times: 0
Loss after 551557680 batches: 0.1917
trigger times: 0
Loss after 551660280 batches: 0.1563
trigger times: 1
Loss after 551762880 batches: 0.1484
trigger times: 2
Loss after 551865480 batches: 0.1292
trigger times: 3
Loss after 551968080 batches: 0.1132
trigger times: 0
Loss after 552070680 batches: 0.1013
trigger times: 1
Loss after 552173280 batches: 0.0976
trigger times: 2
Loss after 552275880 batches: 0.0892
trigger times: 0
Loss after 552378480 batches: 0.0873
trigger times: 1
Loss after 552481080 batches: 0.0833
trigger times: 2
Loss after 552583680 batches: 0.0794
trigger times: 3
Loss after 552686280 batches: 0.0741
trigger times: 0
Loss after 552788880 batches: 0.0729
trigger times: 1
Loss after 552891480 batches: 0.0724
trigger times: 2
Loss after 552994080 batches: 0.0790
trigger times: 0
Loss after 553096680 batches: 0.0710
trigger times: 0
Loss after 553199280 batches: 0.0665
trigger times: 0
Loss after 553301880 batches: 0.0637
trigger times: 1
Loss after 553404480 batches: 0.0634
trigger times: 2
Loss after 553507080 batches: 0.0612
trigger times: 3
Loss after 553609680 batches: 0.0603
trigger times: 0
Loss after 553712280 batches: 0.0609
trigger times: 1
Loss after 553814880 batches: 0.0608
trigger times: 2
Loss after 553917480 batches: 0.0576
trigger times: 3
Loss after 554020080 batches: 0.0560
trigger times: 4
Loss after 554122680 batches: 0.0547
trigger times: 5
Loss after 554225280 batches: 0.0528
trigger times: 0
Loss after 554327880 batches: 0.0525
trigger times: 1
Loss after 554430480 batches: 0.0532
trigger times: 0
Loss after 554533080 batches: 0.0515
trigger times: 0
Loss after 554635680 batches: 0.0513
trigger times: 1
Loss after 554738280 batches: 0.0534
trigger times: 0
Loss after 554840880 batches: 0.0497
trigger times: 1
Loss after 554943480 batches: 0.0490
trigger times: 2
Loss after 555046080 batches: 0.0473
trigger times: 3
Loss after 555148680 batches: 0.0466
trigger times: 4
Loss after 555251280 batches: 0.0479
trigger times: 5
Loss after 555353880 batches: 0.0467
trigger times: 6
Loss after 555456480 batches: 0.0468
trigger times: 0
Loss after 555559080 batches: 0.0456
trigger times: 0
Loss after 555661680 batches: 0.0449
trigger times: 1
Loss after 555764280 batches: 0.0437
trigger times: 2
Loss after 555866880 batches: 0.0444
trigger times: 3
Loss after 555969480 batches: 0.0447
trigger times: 4
Loss after 556072080 batches: 0.0427
trigger times: 5
Loss after 556174680 batches: 0.0419
trigger times: 0
Loss after 556277280 batches: 0.0420
trigger times: 1
Loss after 556379880 batches: 0.0423
trigger times: 2
Loss after 556482480 batches: 0.0454
trigger times: 0
Loss after 556585080 batches: 0.0429
trigger times: 0
Loss after 556687680 batches: 0.0437
trigger times: 1
Loss after 556790280 batches: 0.0424
trigger times: 0
Loss after 556892880 batches: 0.0415
trigger times: 1
Loss after 556995480 batches: 0.0418
trigger times: 0
Loss after 557098080 batches: 0.0408
trigger times: 1
Loss after 557200680 batches: 0.0422
trigger times: 2
Loss after 557303280 batches: 0.0418
trigger times: 3
Loss after 557405880 batches: 0.0414
trigger times: 4
Loss after 557508480 batches: 0.0399
trigger times: 5
Loss after 557611080 batches: 0.0404
trigger times: 6
Loss after 557713680 batches: 0.0410
trigger times: 7
Loss after 557816280 batches: 0.0414
trigger times: 8
Loss after 557918880 batches: 0.0399
trigger times: 9
Loss after 558021480 batches: 0.0385
trigger times: 0
Loss after 558124080 batches: 0.0382
trigger times: 1
Loss after 558226680 batches: 0.0380
trigger times: 2
Loss after 558329280 batches: 0.0397
trigger times: 3
Loss after 558431880 batches: 0.0513
trigger times: 4
Loss after 558534480 batches: 0.0385
trigger times: 5
Loss after 558637080 batches: 0.0366
trigger times: 6
Loss after 558739680 batches: 0.0356
trigger times: 7
Loss after 558842280 batches: 0.0345
trigger times: 8
Loss after 558944880 batches: 0.0341
trigger times: 9
Loss after 559047480 batches: 0.0353
trigger times: 10
Loss after 559150080 batches: 0.0375
trigger times: 11
Loss after 559252680 batches: 0.0350
trigger times: 12
Loss after 559355280 batches: 0.0334
trigger times: 13
Loss after 559457880 batches: 0.0353
trigger times: 14
Loss after 559560480 batches: 0.0364
trigger times: 15
Loss after 559663080 batches: 0.0347
trigger times: 0
Loss after 559765680 batches: 0.0332
trigger times: 1
Loss after 559868280 batches: 0.0338
trigger times: 2
Loss after 559970880 batches: 0.0369
trigger times: 3
Loss after 560073480 batches: 0.0348
trigger times: 4
Loss after 560176080 batches: 0.0317
trigger times: 5
Loss after 560278680 batches: 0.0325
trigger times: 6
Loss after 560381280 batches: 0.0319
trigger times: 7
Loss after 560483880 batches: 0.0317
trigger times: 8
Loss after 560586480 batches: 0.0323
trigger times: 9
Loss after 560689080 batches: 0.0314
trigger times: 10
Loss after 560791680 batches: 0.0323
trigger times: 11
Loss after 560894280 batches: 0.0318
trigger times: 12
Loss after 560996880 batches: 0.0326
trigger times: 13
Loss after 561099480 batches: 0.0321
trigger times: 14
Loss after 561202080 batches: 0.0322
trigger times: 15
Loss after 561304680 batches: 0.0311
trigger times: 16
Loss after 561407280 batches: 0.0314
trigger times: 17
Loss after 561509880 batches: 0.0298
trigger times: 0
Loss after 561612480 batches: 0.0296
trigger times: 1
Loss after 561715080 batches: 0.0308
trigger times: 2
Loss after 561817680 batches: 0.0303
trigger times: 3
Loss after 561920280 batches: 0.0295
trigger times: 4
Loss after 562022880 batches: 0.0331
trigger times: 5
Loss after 562125480 batches: 0.0352
trigger times: 6
Loss after 562228080 batches: 0.0353
trigger times: 7
Loss after 562330680 batches: 0.0351
trigger times: 8
Loss after 562433280 batches: 0.0307
trigger times: 9
Loss after 562535880 batches: 0.0295
trigger times: 10
Loss after 562638480 batches: 0.0286
trigger times: 11
Loss after 562741080 batches: 0.0282
trigger times: 12
Loss after 562843680 batches: 0.0292
trigger times: 13
Loss after 562946280 batches: 0.0288
trigger times: 14
Loss after 563048880 batches: 0.0296
trigger times: 15
Loss after 563151480 batches: 0.0295
trigger times: 0
Loss after 563254080 batches: 0.0306
trigger times: 1
Loss after 563356680 batches: 0.0304
trigger times: 2
Loss after 563459280 batches: 0.0299
trigger times: 3
Loss after 563561880 batches: 0.0294
trigger times: 4
Loss after 563664480 batches: 0.0277
trigger times: 5
Loss after 563767080 batches: 0.0291
trigger times: 6
Loss after 563869680 batches: 0.0314
trigger times: 7
Loss after 563972280 batches: 0.0278
trigger times: 8
Loss after 564074880 batches: 0.0272
trigger times: 9
Loss after 564177480 batches: 0.0293
trigger times: 10
Loss after 564280080 batches: 0.0295
trigger times: 11
Loss after 564382680 batches: 0.0275
trigger times: 12
Loss after 564485280 batches: 0.0288
trigger times: 13
Loss after 564587880 batches: 0.0268
trigger times: 14
Loss after 564690480 batches: 0.0268
trigger times: 0
Loss after 564793080 batches: 0.0264
trigger times: 1
Loss after 564895680 batches: 0.0266
trigger times: 2
Loss after 564998280 batches: 0.0267
trigger times: 3
Loss after 565100880 batches: 0.0266
trigger times: 4
Loss after 565203480 batches: 0.0260
trigger times: 5
Loss after 565306080 batches: 0.0270
trigger times: 6
Loss after 565408680 batches: 0.0269
trigger times: 7
Loss after 565511280 batches: 0.0270
trigger times: 0
Loss after 565613880 batches: 0.0290
trigger times: 0
Loss after 565716480 batches: 0.0274
trigger times: 0
Loss after 565819080 batches: 0.0265
trigger times: 1
Loss after 565921680 batches: 0.0278
trigger times: 2
Loss after 566024280 batches: 0.0266
trigger times: 3
Loss after 566126880 batches: 0.0262
trigger times: 4
Loss after 566229480 batches: 0.0258
trigger times: 5
Loss after 566332080 batches: 0.0272
trigger times: 6
Loss after 566434680 batches: 0.0264
trigger times: 7
Loss after 566537280 batches: 0.0263
trigger times: 8
Loss after 566639880 batches: 0.0266
trigger times: 9
Loss after 566742480 batches: 0.0259
trigger times: 10
Loss after 566845080 batches: 0.0278
trigger times: 11
Loss after 566947680 batches: 0.0256
trigger times: 0
Loss after 567050280 batches: 0.0259
trigger times: 1
Loss after 567152880 batches: 0.0286
trigger times: 2
Loss after 567255480 batches: 0.0319
trigger times: 3
Loss after 567358080 batches: 0.0257
trigger times: 4
Loss after 567460680 batches: 0.0256
trigger times: 5
Loss after 567563280 batches: 0.0262
trigger times: 6
Loss after 567665880 batches: 0.0250
trigger times: 7
Loss after 567768480 batches: 0.0239
trigger times: 8
Loss after 567871080 batches: 0.0236
trigger times: 9
Loss after 567973680 batches: 0.0230
trigger times: 10
Loss after 568076280 batches: 0.0242
trigger times: 11
Loss after 568178880 batches: 0.0238
trigger times: 12
Loss after 568281480 batches: 0.0244
trigger times: 13
Loss after 568384080 batches: 0.0238
trigger times: 14
Loss after 568486680 batches: 0.0255
trigger times: 15
Loss after 568589280 batches: 0.0245
trigger times: 0
Loss after 568691880 batches: 0.0279
trigger times: 1
Loss after 568794480 batches: 0.0272
trigger times: 2
Loss after 568897080 batches: 0.0238
trigger times: 3
Loss after 568999680 batches: 0.0245
trigger times: 4
Loss after 569102280 batches: 0.0267
trigger times: 5
Loss after 569204880 batches: 0.0235
trigger times: 6
Loss after 569307480 batches: 0.0239
trigger times: 7
Loss after 569410080 batches: 0.0266
trigger times: 8
Loss after 569512680 batches: 0.0247
trigger times: 9
Loss after 569615280 batches: 0.0235
trigger times: 10
Loss after 569717880 batches: 0.0252
trigger times: 11
Loss after 569820480 batches: 0.0243
trigger times: 12
Loss after 569923080 batches: 0.0325
trigger times: 13
Loss after 570025680 batches: 0.0376
trigger times: 14
Loss after 570128280 batches: 0.0249
trigger times: 15
Loss after 570230880 batches: 0.0237
trigger times: 16
Loss after 570333480 batches: 0.0234
trigger times: 17
Loss after 570436080 batches: 0.0223
trigger times: 18
Loss after 570538680 batches: 0.0219
trigger times: 19
Loss after 570641280 batches: 0.0216
trigger times: 20
Early stopping!
Start to test process.
Loss after 570743880 batches: 0.0220
Time to train on one home:  1140.0809326171875
trigger times: 0
Loss after 570874980 batches: 0.3422
trigger times: 1
Loss after 571006080 batches: 0.1880
trigger times: 2
Loss after 571137180 batches: 0.1339
trigger times: 3
Loss after 571268280 batches: 0.1053
trigger times: 4
Loss after 571399380 batches: 0.0914
trigger times: 5
Loss after 571530480 batches: 0.0818
trigger times: 6
Loss after 571661580 batches: 0.0742
trigger times: 7
Loss after 571792680 batches: 0.0690
trigger times: 8
Loss after 571923780 batches: 0.0652
trigger times: 9
Loss after 572054880 batches: 0.0613
trigger times: 10
Loss after 572185980 batches: 0.0585
trigger times: 11
Loss after 572317080 batches: 0.0556
trigger times: 12
Loss after 572448180 batches: 0.0531
trigger times: 13
Loss after 572579280 batches: 0.0512
trigger times: 14
Loss after 572710380 batches: 0.0498
trigger times: 15
Loss after 572841480 batches: 0.0482
trigger times: 16
Loss after 572972580 batches: 0.0465
trigger times: 17
Loss after 573103680 batches: 0.0457
trigger times: 18
Loss after 573234780 batches: 0.0441
trigger times: 19
Loss after 573365880 batches: 0.0434
trigger times: 20
Early stopping!
Start to test process.
Loss after 573496980 batches: 0.0429
Time to train on one home:  165.22316026687622
trigger times: 0
Loss after 573628080 batches: 0.6118
trigger times: 1
Loss after 573759180 batches: 0.3573
trigger times: 2
Loss after 573890280 batches: 0.2336
trigger times: 3
Loss after 574021380 batches: 0.1757
trigger times: 4
Loss after 574152480 batches: 0.1462
trigger times: 5
Loss after 574283580 batches: 0.1262
trigger times: 6
Loss after 574414680 batches: 0.1153
trigger times: 7
Loss after 574545780 batches: 0.1061
trigger times: 8
Loss after 574676880 batches: 0.0997
trigger times: 9
Loss after 574807980 batches: 0.0938
trigger times: 10
Loss after 574939080 batches: 0.0876
trigger times: 11
Loss after 575070180 batches: 0.0852
trigger times: 12
Loss after 575201280 batches: 0.0814
trigger times: 13
Loss after 575332380 batches: 0.0787
trigger times: 14
Loss after 575463480 batches: 0.0760
trigger times: 15
Loss after 575594580 batches: 0.0748
trigger times: 16
Loss after 575725680 batches: 0.0719
trigger times: 17
Loss after 575856780 batches: 0.0694
trigger times: 18
Loss after 575987880 batches: 0.0684
trigger times: 19
Loss after 576118980 batches: 0.0670
trigger times: 20
Early stopping!
Start to test process.
Loss after 576250080 batches: 0.0660
Time to train on one home:  164.89510703086853
trigger times: 0
Loss after 576378720 batches: 0.2819
trigger times: 0
Loss after 576507360 batches: 0.1275
trigger times: 1
Loss after 576636000 batches: 0.0875
trigger times: 2
Loss after 576764640 batches: 0.0707
trigger times: 3
Loss after 576893280 batches: 0.0618
trigger times: 4
Loss after 577021920 batches: 0.0557
trigger times: 0
Loss after 577150560 batches: 0.0516
trigger times: 1
Loss after 577279200 batches: 0.0484
trigger times: 2
Loss after 577407840 batches: 0.0451
trigger times: 0
Loss after 577536480 batches: 0.0427
trigger times: 0
Loss after 577665120 batches: 0.0407
trigger times: 1
Loss after 577793760 batches: 0.0400
trigger times: 2
Loss after 577922400 batches: 0.0387
trigger times: 3
Loss after 578051040 batches: 0.0371
trigger times: 4
Loss after 578179680 batches: 0.0360
trigger times: 0
Loss after 578308320 batches: 0.0349
trigger times: 0
Loss after 578436960 batches: 0.0341
trigger times: 0
Loss after 578565600 batches: 0.0331
trigger times: 0
Loss after 578694240 batches: 0.0321
trigger times: 1
Loss after 578822880 batches: 0.0319
trigger times: 0
Loss after 578951520 batches: 0.0306
trigger times: 1
Loss after 579080160 batches: 0.0299
trigger times: 0
Loss after 579208800 batches: 0.0302
trigger times: 1
Loss after 579337440 batches: 0.0290
trigger times: 2
Loss after 579466080 batches: 0.0285
trigger times: 3
Loss after 579594720 batches: 0.0286
trigger times: 4
Loss after 579723360 batches: 0.0279
trigger times: 5
Loss after 579852000 batches: 0.0274
trigger times: 6
Loss after 579980640 batches: 0.0278
trigger times: 7
Loss after 580109280 batches: 0.0261
trigger times: 8
Loss after 580237920 batches: 0.0265
trigger times: 0
Loss after 580366560 batches: 0.0263
trigger times: 1
Loss after 580495200 batches: 0.0261
trigger times: 2
Loss after 580623840 batches: 0.0254
trigger times: 3
Loss after 580752480 batches: 0.0244
trigger times: 4
Loss after 580881120 batches: 0.0249
trigger times: 0
Loss after 581009760 batches: 0.0249
trigger times: 1
Loss after 581138400 batches: 0.0246
trigger times: 2
Loss after 581267040 batches: 0.0247
trigger times: 3
Loss after 581395680 batches: 0.0238
trigger times: 4
Loss after 581524320 batches: 0.0233
trigger times: 5
Loss after 581652960 batches: 0.0233
trigger times: 0
Loss after 581781600 batches: 0.0230
trigger times: 0
Loss after 581910240 batches: 0.0227
trigger times: 1
Loss after 582038880 batches: 0.0226
trigger times: 2
Loss after 582167520 batches: 0.0227
trigger times: 3
Loss after 582296160 batches: 0.0226
trigger times: 4
Loss after 582424800 batches: 0.0218
trigger times: 5
Loss after 582553440 batches: 0.0221
trigger times: 6
Loss after 582682080 batches: 0.0216
trigger times: 7
Loss after 582810720 batches: 0.0216
trigger times: 8
Loss after 582939360 batches: 0.0213
trigger times: 9
Loss after 583068000 batches: 0.0213
trigger times: 10
Loss after 583196640 batches: 0.0210
trigger times: 11
Loss after 583325280 batches: 0.0212
trigger times: 12
Loss after 583453920 batches: 0.0209
trigger times: 13
Loss after 583582560 batches: 0.0210
trigger times: 14
Loss after 583711200 batches: 0.0209
trigger times: 0
Loss after 583839840 batches: 0.0205
trigger times: 1
Loss after 583968480 batches: 0.0204
trigger times: 2
Loss after 584097120 batches: 0.0203
trigger times: 0
Loss after 584225760 batches: 0.0204
trigger times: 1
Loss after 584354400 batches: 0.0198
trigger times: 2
Loss after 584483040 batches: 0.0199
trigger times: 3
Loss after 584611680 batches: 0.0200
trigger times: 0
Loss after 584740320 batches: 0.0195
trigger times: 1
Loss after 584868960 batches: 0.0198
trigger times: 0
Loss after 584997600 batches: 0.0194
trigger times: 1
Loss after 585126240 batches: 0.0192
trigger times: 2
Loss after 585254880 batches: 0.0189
trigger times: 3
Loss after 585383520 batches: 0.0190
trigger times: 4
Loss after 585512160 batches: 0.0185
trigger times: 5
Loss after 585640800 batches: 0.0189
trigger times: 6
Loss after 585769440 batches: 0.0188
trigger times: 7
Loss after 585898080 batches: 0.0187
trigger times: 8
Loss after 586026720 batches: 0.0182
trigger times: 9
Loss after 586155360 batches: 0.0184
trigger times: 0
Loss after 586284000 batches: 0.0185
trigger times: 1
Loss after 586412640 batches: 0.0184
trigger times: 2
Loss after 586541280 batches: 0.0182
trigger times: 3
Loss after 586669920 batches: 0.0181
trigger times: 4
Loss after 586798560 batches: 0.0179
trigger times: 5
Loss after 586927200 batches: 0.0180
trigger times: 0
Loss after 587055840 batches: 0.0178
trigger times: 0
Loss after 587184480 batches: 0.0183
trigger times: 1
Loss after 587313120 batches: 0.0185
trigger times: 0
Loss after 587441760 batches: 0.0182
trigger times: 0
Loss after 587570400 batches: 0.0174
trigger times: 0
Loss after 587699040 batches: 0.0178
trigger times: 1
Loss after 587827680 batches: 0.0176
trigger times: 2
Loss after 587956320 batches: 0.0170
trigger times: 3
Loss after 588084960 batches: 0.0168
trigger times: 4
Loss after 588213600 batches: 0.0170
trigger times: 5
Loss after 588342240 batches: 0.0169
trigger times: 0
Loss after 588470880 batches: 0.0171
trigger times: 1
Loss after 588599520 batches: 0.0168
trigger times: 2
Loss after 588728160 batches: 0.0172
trigger times: 3
Loss after 588856800 batches: 0.0166
trigger times: 4
Loss after 588985440 batches: 0.0165
trigger times: 5
Loss after 589114080 batches: 0.0169
trigger times: 6
Loss after 589242720 batches: 0.0163
trigger times: 7
Loss after 589371360 batches: 0.0166
trigger times: 8
Loss after 589500000 batches: 0.0166
trigger times: 9
Loss after 589628640 batches: 0.0164
trigger times: 10
Loss after 589757280 batches: 0.0163
trigger times: 11
Loss after 589885920 batches: 0.0162
trigger times: 12
Loss after 590014560 batches: 0.0162
trigger times: 13
Loss after 590143200 batches: 0.0161
trigger times: 14
Loss after 590271840 batches: 0.0167
trigger times: 15
Loss after 590400480 batches: 0.0163
trigger times: 16
Loss after 590529120 batches: 0.0159
trigger times: 17
Loss after 590657760 batches: 0.0161
trigger times: 18
Loss after 590786400 batches: 0.0158
trigger times: 19
Loss after 590915040 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 591043680 batches: 0.0158
Time to train on one home:  835.8732666969299
trigger times: 0
Loss after 591174780 batches: 0.6849
trigger times: 1
Loss after 591305880 batches: 0.4270
trigger times: 2
Loss after 591436980 batches: 0.2819
trigger times: 3
Loss after 591568080 batches: 0.2064
trigger times: 4
Loss after 591699180 batches: 0.1650
trigger times: 5
Loss after 591830280 batches: 0.1427
trigger times: 6
Loss after 591961380 batches: 0.1272
trigger times: 7
Loss after 592092480 batches: 0.1156
trigger times: 8
Loss after 592223580 batches: 0.1065
trigger times: 9
Loss after 592354680 batches: 0.0995
trigger times: 10
Loss after 592485780 batches: 0.0962
trigger times: 11
Loss after 592616880 batches: 0.0899
trigger times: 12
Loss after 592747980 batches: 0.0866
trigger times: 13
Loss after 592879080 batches: 0.0830
trigger times: 14
Loss after 593010180 batches: 0.0805
trigger times: 15
Loss after 593141280 batches: 0.0775
trigger times: 16
Loss after 593272380 batches: 0.0749
trigger times: 17
Loss after 593403480 batches: 0.0728
trigger times: 18
Loss after 593534580 batches: 0.0712
trigger times: 19
Loss after 593665680 batches: 0.0697
trigger times: 20
Early stopping!
Start to test process.
Loss after 593796780 batches: 0.0677
Time to train on one home:  165.00545001029968
trigger times: 0
Loss after 593927880 batches: 0.6801
trigger times: 1
Loss after 594058980 batches: 0.4414
trigger times: 2
Loss after 594190080 batches: 0.3186
trigger times: 3
Loss after 594321180 batches: 0.2429
trigger times: 4
Loss after 594452280 batches: 0.2044
trigger times: 5
Loss after 594583380 batches: 0.1743
trigger times: 6
Loss after 594714480 batches: 0.1497
trigger times: 7
Loss after 594845580 batches: 0.1405
trigger times: 8
Loss after 594976680 batches: 0.1287
trigger times: 9
Loss after 595107780 batches: 0.1170
trigger times: 10
Loss after 595238880 batches: 0.1099
trigger times: 11
Loss after 595369980 batches: 0.1031
trigger times: 12
Loss after 595501080 batches: 0.0971
trigger times: 13
Loss after 595632180 batches: 0.0919
trigger times: 14
Loss after 595763280 batches: 0.0899
trigger times: 15
Loss after 595894380 batches: 0.0861
trigger times: 16
Loss after 596025480 batches: 0.0830
trigger times: 17
Loss after 596156580 batches: 0.0806
trigger times: 18
Loss after 596287680 batches: 0.0790
trigger times: 19
Loss after 596418780 batches: 0.0794
trigger times: 20
Early stopping!
Start to test process.
Loss after 596549880 batches: 0.0742
Time to train on one home:  165.3232705593109
trigger times: 0
Loss after 596680980 batches: 0.1927
trigger times: 0
Loss after 596812080 batches: 0.0772
trigger times: 0
Loss after 596943180 batches: 0.0506
trigger times: 0
Loss after 597074280 batches: 0.0414
trigger times: 1
Loss after 597205380 batches: 0.0366
trigger times: 0
Loss after 597336480 batches: 0.0339
trigger times: 1
Loss after 597467580 batches: 0.0313
trigger times: 2
Loss after 597598680 batches: 0.0290
trigger times: 0
Loss after 597729780 batches: 0.0281
trigger times: 0
Loss after 597860880 batches: 0.0271
trigger times: 1
Loss after 597991980 batches: 0.0262
trigger times: 2
Loss after 598123080 batches: 0.0251
trigger times: 3
Loss after 598254180 batches: 0.0243
trigger times: 4
Loss after 598385280 batches: 0.0238
trigger times: 0
Loss after 598516380 batches: 0.0228
trigger times: 1
Loss after 598647480 batches: 0.0225
trigger times: 2
Loss after 598778580 batches: 0.0215
trigger times: 0
Loss after 598909680 batches: 0.0214
trigger times: 1
Loss after 599040780 batches: 0.0209
trigger times: 2
Loss after 599171880 batches: 0.0207
trigger times: 3
Loss after 599302980 batches: 0.0201
trigger times: 4
Loss after 599434080 batches: 0.0196
trigger times: 5
Loss after 599565180 batches: 0.0196
trigger times: 6
Loss after 599696280 batches: 0.0189
trigger times: 7
Loss after 599827380 batches: 0.0190
trigger times: 8
Loss after 599958480 batches: 0.0182
trigger times: 9
Loss after 600089580 batches: 0.0187
trigger times: 10
Loss after 600220680 batches: 0.0182
trigger times: 11
Loss after 600351780 batches: 0.0179
trigger times: 12
Loss after 600482880 batches: 0.0178
trigger times: 13
Loss after 600613980 batches: 0.0172
trigger times: 14
Loss after 600745080 batches: 0.0172
trigger times: 15
Loss after 600876180 batches: 0.0169
trigger times: 16
Loss after 601007280 batches: 0.0170
trigger times: 17
Loss after 601138380 batches: 0.0166
trigger times: 18
Loss after 601269480 batches: 0.0166
trigger times: 19
Loss after 601400580 batches: 0.0161
trigger times: 20
Early stopping!
Start to test process.
Loss after 601531680 batches: 0.0159
Time to train on one home:  288.857679605484
trigger times: 0
Loss after 601662780 batches: 0.2299
trigger times: 0
Loss after 601793880 batches: 0.1013
trigger times: 0
Loss after 601924980 batches: 0.0710
trigger times: 1
Loss after 602056080 batches: 0.0558
trigger times: 2
Loss after 602187180 batches: 0.0491
trigger times: 0
Loss after 602318280 batches: 0.0434
trigger times: 0
Loss after 602449380 batches: 0.0396
trigger times: 1
Loss after 602580480 batches: 0.0365
trigger times: 2
Loss after 602711580 batches: 0.0337
trigger times: 3
Loss after 602842680 batches: 0.0319
trigger times: 0
Loss after 602973780 batches: 0.0300
trigger times: 0
Loss after 603104880 batches: 0.0293
trigger times: 0
Loss after 603235980 batches: 0.0280
trigger times: 1
Loss after 603367080 batches: 0.0272
trigger times: 2
Loss after 603498180 batches: 0.0260
trigger times: 3
Loss after 603629280 batches: 0.0254
trigger times: 4
Loss after 603760380 batches: 0.0249
trigger times: 5
Loss after 603891480 batches: 0.0242
trigger times: 6
Loss after 604022580 batches: 0.0237
trigger times: 7
Loss after 604153680 batches: 0.0228
trigger times: 0
Loss after 604284780 batches: 0.0222
trigger times: 1
Loss after 604415880 batches: 0.0223
trigger times: 2
Loss after 604546980 batches: 0.0214
trigger times: 3
Loss after 604678080 batches: 0.0214
trigger times: 4
Loss after 604809180 batches: 0.0209
trigger times: 5
Loss after 604940280 batches: 0.0207
trigger times: 6
Loss after 605071380 batches: 0.0204
trigger times: 7
Loss after 605202480 batches: 0.0199
trigger times: 8
Loss after 605333580 batches: 0.0194
trigger times: 9
Loss after 605464680 batches: 0.0198
trigger times: 10
Loss after 605595780 batches: 0.0193
trigger times: 11
Loss after 605726880 batches: 0.0188
trigger times: 12
Loss after 605857980 batches: 0.0189
trigger times: 13
Loss after 605989080 batches: 0.0181
trigger times: 14
Loss after 606120180 batches: 0.0181
trigger times: 15
Loss after 606251280 batches: 0.0177
trigger times: 16
Loss after 606382380 batches: 0.0179
trigger times: 17
Loss after 606513480 batches: 0.0177
trigger times: 18
Loss after 606644580 batches: 0.0175
trigger times: 19
Loss after 606775680 batches: 0.0173
trigger times: 20
Early stopping!
Start to test process.
Loss after 606906780 batches: 0.0168
Time to train on one home:  311.67549204826355
trigger times: 0
Loss after 606985380 batches: 0.5878
trigger times: 1
Loss after 607063980 batches: 0.3053
trigger times: 2
Loss after 607142580 batches: 0.1816
trigger times: 3
Loss after 607221180 batches: 0.1302
trigger times: 4
Loss after 607299780 batches: 0.1050
trigger times: 5
Loss after 607378380 batches: 0.0921
trigger times: 6
Loss after 607456980 batches: 0.0824
trigger times: 7
Loss after 607535580 batches: 0.0746
trigger times: 8
Loss after 607614180 batches: 0.0694
trigger times: 9
Loss after 607692780 batches: 0.0634
trigger times: 10
Loss after 607771380 batches: 0.0618
trigger times: 11
Loss after 607849980 batches: 0.0602
trigger times: 12
Loss after 607928580 batches: 0.0558
trigger times: 0
Loss after 608007180 batches: 0.0540
trigger times: 1
Loss after 608085780 batches: 0.0518
trigger times: 2
Loss after 608164380 batches: 0.0521
trigger times: 0
Loss after 608242980 batches: 0.0492
trigger times: 1
Loss after 608321580 batches: 0.0480
trigger times: 2
Loss after 608400180 batches: 0.0473
trigger times: 3
Loss after 608478780 batches: 0.0453
trigger times: 4
Loss after 608557380 batches: 0.0450
trigger times: 5
Loss after 608635980 batches: 0.0448
trigger times: 6
Loss after 608714580 batches: 0.0433
trigger times: 7
Loss after 608793180 batches: 0.0448
trigger times: 8
Loss after 608871780 batches: 0.0420
trigger times: 9
Loss after 608950380 batches: 0.0411
trigger times: 10
Loss after 609028980 batches: 0.0410
trigger times: 11
Loss after 609107580 batches: 0.0402
trigger times: 12
Loss after 609186180 batches: 0.0389
trigger times: 13
Loss after 609264780 batches: 0.0404
trigger times: 14
Loss after 609343380 batches: 0.0390
trigger times: 15
Loss after 609421980 batches: 0.0383
trigger times: 16
Loss after 609500580 batches: 0.0389
trigger times: 17
Loss after 609579180 batches: 0.0377
trigger times: 18
Loss after 609657780 batches: 0.0367
trigger times: 19
Loss after 609736380 batches: 0.0355
trigger times: 20
Early stopping!
Start to test process.
Loss after 609814980 batches: 0.0350
Time to train on one home:  186.53734922409058
trigger times: 0
Loss after 609946080 batches: 0.2282
trigger times: 1
Loss after 610077180 batches: 0.0943
trigger times: 2
Loss after 610208280 batches: 0.0622
trigger times: 3
Loss after 610339380 batches: 0.0501
trigger times: 4
Loss after 610470480 batches: 0.0438
trigger times: 5
Loss after 610601580 batches: 0.0397
trigger times: 6
Loss after 610732680 batches: 0.0366
trigger times: 7
Loss after 610863780 batches: 0.0345
trigger times: 8
Loss after 610994880 batches: 0.0327
trigger times: 9
Loss after 611125980 batches: 0.0308
trigger times: 10
Loss after 611257080 batches: 0.0298
trigger times: 11
Loss after 611388180 batches: 0.0294
trigger times: 12
Loss after 611519280 batches: 0.0281
trigger times: 13
Loss after 611650380 batches: 0.0269
trigger times: 14
Loss after 611781480 batches: 0.0267
trigger times: 15
Loss after 611912580 batches: 0.0255
trigger times: 16
Loss after 612043680 batches: 0.0249
trigger times: 17
Loss after 612174780 batches: 0.0250
trigger times: 18
Loss after 612305880 batches: 0.0243
trigger times: 19
Loss after 612436980 batches: 0.0232
trigger times: 20
Early stopping!
Start to test process.
Loss after 612568080 batches: 0.0234
Time to train on one home:  164.65284371376038
trigger times: 0
Loss after 612699180 batches: 0.2893
trigger times: 1
Loss after 612830280 batches: 0.1331
trigger times: 2
Loss after 612961380 batches: 0.0951
trigger times: 3
Loss after 613092480 batches: 0.0762
trigger times: 4
Loss after 613223580 batches: 0.0655
trigger times: 5
Loss after 613354680 batches: 0.0590
trigger times: 6
Loss after 613485780 batches: 0.0548
trigger times: 7
Loss after 613616880 batches: 0.0506
trigger times: 8
Loss after 613747980 batches: 0.0475
trigger times: 9
Loss after 613879080 batches: 0.0449
trigger times: 0
Loss after 614010180 batches: 0.0427
trigger times: 0
Loss after 614141280 batches: 0.0415
trigger times: 1
Loss after 614272380 batches: 0.0395
trigger times: 2
Loss after 614403480 batches: 0.0387
trigger times: 3
Loss after 614534580 batches: 0.0372
trigger times: 4
Loss after 614665680 batches: 0.0365
trigger times: 5
Loss after 614796780 batches: 0.0354
trigger times: 0
Loss after 614927880 batches: 0.0343
trigger times: 0
Loss after 615058980 batches: 0.0334
trigger times: 1
Loss after 615190080 batches: 0.0321
trigger times: 2
Loss after 615321180 batches: 0.0321
trigger times: 3
Loss after 615452280 batches: 0.0313
trigger times: 4
Loss after 615583380 batches: 0.0306
trigger times: 5
Loss after 615714480 batches: 0.0295
trigger times: 0
Loss after 615845580 batches: 0.0299
trigger times: 1
Loss after 615976680 batches: 0.0292
trigger times: 0
Loss after 616107780 batches: 0.0288
trigger times: 1
Loss after 616238880 batches: 0.0280
trigger times: 2
Loss after 616369980 batches: 0.0271
trigger times: 3
Loss after 616501080 batches: 0.0273
trigger times: 4
Loss after 616632180 batches: 0.0270
trigger times: 5
Loss after 616763280 batches: 0.0264
trigger times: 6
Loss after 616894380 batches: 0.0258
trigger times: 7
Loss after 617025480 batches: 0.0251
trigger times: 8
Loss after 617156580 batches: 0.0253
trigger times: 9
Loss after 617287680 batches: 0.0251
trigger times: 10
Loss after 617418780 batches: 0.0247
trigger times: 11
Loss after 617549880 batches: 0.0243
trigger times: 12
Loss after 617680980 batches: 0.0238
trigger times: 13
Loss after 617812080 batches: 0.0237
trigger times: 14
Loss after 617943180 batches: 0.0233
trigger times: 15
Loss after 618074280 batches: 0.0232
trigger times: 16
Loss after 618205380 batches: 0.0230
trigger times: 17
Loss after 618336480 batches: 0.0228
trigger times: 0
Loss after 618467580 batches: 0.0221
trigger times: 1
Loss after 618598680 batches: 0.0223
trigger times: 2
Loss after 618729780 batches: 0.0219
trigger times: 3
Loss after 618860880 batches: 0.0216
trigger times: 0
Loss after 618991980 batches: 0.0218
trigger times: 1
Loss after 619123080 batches: 0.0211
trigger times: 2
Loss after 619254180 batches: 0.0216
trigger times: 3
Loss after 619385280 batches: 0.0209
trigger times: 4
Loss after 619516380 batches: 0.0208
trigger times: 5
Loss after 619647480 batches: 0.0204
trigger times: 6
Loss after 619778580 batches: 0.0201
trigger times: 7
Loss after 619909680 batches: 0.0204
trigger times: 8
Loss after 620040780 batches: 0.0201
trigger times: 9
Loss after 620171880 batches: 0.0200
trigger times: 10
Loss after 620302980 batches: 0.0198
trigger times: 11
Loss after 620434080 batches: 0.0193
trigger times: 12
Loss after 620565180 batches: 0.0196
trigger times: 13
Loss after 620696280 batches: 0.0194
trigger times: 14
Loss after 620827380 batches: 0.0192
trigger times: 15
Loss after 620958480 batches: 0.0193
trigger times: 16
Loss after 621089580 batches: 0.0187
trigger times: 17
Loss after 621220680 batches: 0.0187
trigger times: 18
Loss after 621351780 batches: 0.0185
trigger times: 19
Loss after 621482880 batches: 0.0185
trigger times: 20
Early stopping!
Start to test process.
Loss after 621613980 batches: 0.0181
Time to train on one home:  513.1430366039276
trigger times: 0
Loss after 621745080 batches: 0.5019
trigger times: 0
Loss after 621876180 batches: 0.2256
trigger times: 0
Loss after 622007280 batches: 0.1453
trigger times: 1
Loss after 622138380 batches: 0.1129
trigger times: 2
Loss after 622269480 batches: 0.0958
trigger times: 0
Loss after 622400580 batches: 0.0859
trigger times: 1
Loss after 622531680 batches: 0.0783
trigger times: 2
Loss after 622662780 batches: 0.0710
trigger times: 3
Loss after 622793880 batches: 0.0672
trigger times: 4
Loss after 622924980 batches: 0.0640
trigger times: 5
Loss after 623056080 batches: 0.0627
trigger times: 0
Loss after 623187180 batches: 0.0588
trigger times: 1
Loss after 623318280 batches: 0.0572
trigger times: 2
Loss after 623449380 batches: 0.0545
trigger times: 0
Loss after 623580480 batches: 0.0528
trigger times: 0
Loss after 623711580 batches: 0.0510
trigger times: 1
Loss after 623842680 batches: 0.0495
trigger times: 0
Loss after 623973780 batches: 0.0480
trigger times: 1
Loss after 624104880 batches: 0.0476
trigger times: 2
Loss after 624235980 batches: 0.0463
trigger times: 3
Loss after 624367080 batches: 0.0450
trigger times: 4
Loss after 624498180 batches: 0.0452
trigger times: 5
Loss after 624629280 batches: 0.0448
trigger times: 6
Loss after 624760380 batches: 0.0437
trigger times: 7
Loss after 624891480 batches: 0.0431
trigger times: 0
Loss after 625022580 batches: 0.0412
trigger times: 1
Loss after 625153680 batches: 0.0416
trigger times: 0
Loss after 625284780 batches: 0.0408
trigger times: 1
Loss after 625415880 batches: 0.0416
trigger times: 2
Loss after 625546980 batches: 0.0395
trigger times: 3
Loss after 625678080 batches: 0.0400
trigger times: 4
Loss after 625809180 batches: 0.0383
trigger times: 5
Loss after 625940280 batches: 0.0381
trigger times: 6
Loss after 626071380 batches: 0.0371
trigger times: 0
Loss after 626202480 batches: 0.0379
trigger times: 1
Loss after 626333580 batches: 0.0376
trigger times: 0
Loss after 626464680 batches: 0.0380
trigger times: 1
Loss after 626595780 batches: 0.0365
trigger times: 2
Loss after 626726880 batches: 0.0357
trigger times: 3
Loss after 626857980 batches: 0.0363
trigger times: 4
Loss after 626989080 batches: 0.0358
trigger times: 5
Loss after 627120180 batches: 0.0345
trigger times: 0
Loss after 627251280 batches: 0.0343
trigger times: 1
Loss after 627382380 batches: 0.0343
trigger times: 2
Loss after 627513480 batches: 0.0340
trigger times: 3
Loss after 627644580 batches: 0.0341
trigger times: 4
Loss after 627775680 batches: 0.0338
trigger times: 5
Loss after 627906780 batches: 0.0333
trigger times: 6
Loss after 628037880 batches: 0.0328
trigger times: 7
Loss after 628168980 batches: 0.0324
trigger times: 8
Loss after 628300080 batches: 0.0321
trigger times: 9
Loss after 628431180 batches: 0.0325
trigger times: 10
Loss after 628562280 batches: 0.0314
trigger times: 11
Loss after 628693380 batches: 0.0312
trigger times: 12
Loss after 628824480 batches: 0.0316
trigger times: 13
Loss after 628955580 batches: 0.0306
trigger times: 14
Loss after 629086680 batches: 0.0310
trigger times: 0
Loss after 629217780 batches: 0.0309
trigger times: 1
Loss after 629348880 batches: 0.0304
trigger times: 2
Loss after 629479980 batches: 0.0303
trigger times: 3
Loss after 629611080 batches: 0.0300
trigger times: 4
Loss after 629742180 batches: 0.0299
trigger times: 5
Loss after 629873280 batches: 0.0294
trigger times: 6
Loss after 630004380 batches: 0.0296
trigger times: 7
Loss after 630135480 batches: 0.0285
trigger times: 8
Loss after 630266580 batches: 0.0287
trigger times: 0
Loss after 630397680 batches: 0.0283
trigger times: 1
Loss after 630528780 batches: 0.0286
trigger times: 2
Loss after 630659880 batches: 0.0276
trigger times: 3
Loss after 630790980 batches: 0.0282
trigger times: 4
Loss after 630922080 batches: 0.0284
trigger times: 5
Loss after 631053180 batches: 0.0282
trigger times: 6
Loss after 631184280 batches: 0.0278
trigger times: 7
Loss after 631315380 batches: 0.0272
trigger times: 8
Loss after 631446480 batches: 0.0272
trigger times: 9
Loss after 631577580 batches: 0.0276
trigger times: 10
Loss after 631708680 batches: 0.0273
trigger times: 11
Loss after 631839780 batches: 0.0273
trigger times: 12
Loss after 631970880 batches: 0.0265
trigger times: 13
Loss after 632101980 batches: 0.0274
trigger times: 14
Loss after 632233080 batches: 0.0279
trigger times: 15
Loss after 632364180 batches: 0.0264
trigger times: 16
Loss after 632495280 batches: 0.0265
trigger times: 17
Loss after 632626380 batches: 0.0265
trigger times: 18
Loss after 632757480 batches: 0.0264
trigger times: 0
Loss after 632888580 batches: 0.0262
trigger times: 1
Loss after 633019680 batches: 0.0257
trigger times: 2
Loss after 633150780 batches: 0.0260
trigger times: 3
Loss after 633281880 batches: 0.0261
trigger times: 4
Loss after 633412980 batches: 0.0249
trigger times: 5
Loss after 633544080 batches: 0.0256
trigger times: 6
Loss after 633675180 batches: 0.0256
trigger times: 7
Loss after 633806280 batches: 0.0252
trigger times: 8
Loss after 633937380 batches: 0.0252
trigger times: 9
Loss after 634068480 batches: 0.0252
trigger times: 10
Loss after 634199580 batches: 0.0249
trigger times: 11
Loss after 634330680 batches: 0.0254
trigger times: 12
Loss after 634461780 batches: 0.0256
trigger times: 13
Loss after 634592880 batches: 0.0247
trigger times: 14
Loss after 634723980 batches: 0.0244
trigger times: 15
Loss after 634855080 batches: 0.0245
trigger times: 16
Loss after 634986180 batches: 0.0248
trigger times: 0
Loss after 635117280 batches: 0.0244
trigger times: 1
Loss after 635248380 batches: 0.0245
trigger times: 2
Loss after 635379480 batches: 0.0241
trigger times: 3
Loss after 635510580 batches: 0.0238
trigger times: 4
Loss after 635641680 batches: 0.0238
trigger times: 5
Loss after 635772780 batches: 0.0234
trigger times: 6
Loss after 635903880 batches: 0.0241
trigger times: 7
Loss after 636034980 batches: 0.0242
trigger times: 8
Loss after 636166080 batches: 0.0234
trigger times: 9
Loss after 636297180 batches: 0.0233
trigger times: 10
Loss after 636428280 batches: 0.0239
trigger times: 11
Loss after 636559380 batches: 0.0234
trigger times: 12
Loss after 636690480 batches: 0.0236
trigger times: 13
Loss after 636821580 batches: 0.0236
trigger times: 14
Loss after 636952680 batches: 0.0236
trigger times: 15
Loss after 637083780 batches: 0.0240
trigger times: 16
Loss after 637214880 batches: 0.0224
trigger times: 17
Loss after 637345980 batches: 0.0225
trigger times: 18
Loss after 637477080 batches: 0.0225
trigger times: 19
Loss after 637608180 batches: 0.0223
trigger times: 20
Early stopping!
Start to test process.
Loss after 637739280 batches: 0.0229
Time to train on one home:  905.9228353500366
trigger times: 0
Loss after 637870380 batches: 0.5231
trigger times: 1
Loss after 638001480 batches: 0.2928
trigger times: 2
Loss after 638132580 batches: 0.2027
trigger times: 3
Loss after 638263680 batches: 0.1537
trigger times: 4
Loss after 638394780 batches: 0.1275
trigger times: 5
Loss after 638525880 batches: 0.1109
trigger times: 6
Loss after 638656980 batches: 0.1007
trigger times: 7
Loss after 638788080 batches: 0.0918
trigger times: 8
Loss after 638919180 batches: 0.0870
trigger times: 9
Loss after 639050280 batches: 0.0816
trigger times: 10
Loss after 639181380 batches: 0.0772
trigger times: 11
Loss after 639312480 batches: 0.0731
trigger times: 12
Loss after 639443580 batches: 0.0712
trigger times: 13
Loss after 639574680 batches: 0.0690
trigger times: 14
Loss after 639705780 batches: 0.0661
trigger times: 15
Loss after 639836880 batches: 0.0657
trigger times: 16
Loss after 639967980 batches: 0.0628
trigger times: 17
Loss after 640099080 batches: 0.0621
trigger times: 18
Loss after 640230180 batches: 0.0601
trigger times: 19
Loss after 640361280 batches: 0.0588
trigger times: 20
Early stopping!
Start to test process.
Loss after 640492380 batches: 0.0584
Time to train on one home:  163.80132293701172
trigger times: 0
Loss after 640623480 batches: 0.4125
trigger times: 1
Loss after 640754580 batches: 0.1688
trigger times: 0
Loss after 640885680 batches: 0.1137
trigger times: 1
Loss after 641016780 batches: 0.0887
trigger times: 0
Loss after 641147880 batches: 0.0760
trigger times: 1
Loss after 641278980 batches: 0.0666
trigger times: 2
Loss after 641410080 batches: 0.0610
trigger times: 3
Loss after 641541180 batches: 0.0571
trigger times: 0
Loss after 641672280 batches: 0.0545
trigger times: 1
Loss after 641803380 batches: 0.0512
trigger times: 2
Loss after 641934480 batches: 0.0474
trigger times: 3
Loss after 642065580 batches: 0.0471
trigger times: 4
Loss after 642196680 batches: 0.0455
trigger times: 5
Loss after 642327780 batches: 0.0433
trigger times: 0
Loss after 642458880 batches: 0.0423
trigger times: 1
Loss after 642589980 batches: 0.0404
trigger times: 2
Loss after 642721080 batches: 0.0404
trigger times: 0
Loss after 642852180 batches: 0.0386
trigger times: 1
Loss after 642983280 batches: 0.0374
trigger times: 2
Loss after 643114380 batches: 0.0374
trigger times: 3
Loss after 643245480 batches: 0.0359
trigger times: 4
Loss after 643376580 batches: 0.0361
trigger times: 0
Loss after 643507680 batches: 0.0358
trigger times: 1
Loss after 643638780 batches: 0.0353
trigger times: 2
Loss after 643769880 batches: 0.0343
trigger times: 3
Loss after 643900980 batches: 0.0344
trigger times: 0
Loss after 644032080 batches: 0.0331
trigger times: 1
Loss after 644163180 batches: 0.0339
trigger times: 2
Loss after 644294280 batches: 0.0320
trigger times: 3
Loss after 644425380 batches: 0.0321
trigger times: 4
Loss after 644556480 batches: 0.0315
trigger times: 5
Loss after 644687580 batches: 0.0315
trigger times: 6
Loss after 644818680 batches: 0.0311
trigger times: 7
Loss after 644949780 batches: 0.0312
trigger times: 8
Loss after 645080880 batches: 0.0305
trigger times: 9
Loss after 645211980 batches: 0.0304
trigger times: 0
Loss after 645343080 batches: 0.0302
trigger times: 1
Loss after 645474180 batches: 0.0289
trigger times: 2
Loss after 645605280 batches: 0.0289
trigger times: 3
Loss after 645736380 batches: 0.0291
trigger times: 4
Loss after 645867480 batches: 0.0291
trigger times: 5
Loss after 645998580 batches: 0.0288
trigger times: 6
Loss after 646129680 batches: 0.0287
trigger times: 7
Loss after 646260780 batches: 0.0278
trigger times: 8
Loss after 646391880 batches: 0.0273
trigger times: 9
Loss after 646522980 batches: 0.0276
trigger times: 10
Loss after 646654080 batches: 0.0274
trigger times: 11
Loss after 646785180 batches: 0.0268
trigger times: 12
Loss after 646916280 batches: 0.0268
trigger times: 13
Loss after 647047380 batches: 0.0265
trigger times: 14
Loss after 647178480 batches: 0.0268
trigger times: 15
Loss after 647309580 batches: 0.0260
trigger times: 16
Loss after 647440680 batches: 0.0256
trigger times: 17
Loss after 647571780 batches: 0.0258
trigger times: 18
Loss after 647702880 batches: 0.0259
trigger times: 19
Loss after 647833980 batches: 0.0248
trigger times: 20
Early stopping!
Start to test process.
Loss after 647965080 batches: 0.0248
Time to train on one home:  425.2865216732025
trigger times: 0
Loss after 648096180 batches: 0.5278
trigger times: 0
Loss after 648227280 batches: 0.3195
trigger times: 0
Loss after 648358380 batches: 0.2333
trigger times: 0
Loss after 648489480 batches: 0.1840
trigger times: 1
Loss after 648620580 batches: 0.1551
trigger times: 0
Loss after 648751680 batches: 0.1371
trigger times: 1
Loss after 648882780 batches: 0.1228
trigger times: 2
Loss after 649013880 batches: 0.1124
trigger times: 3
Loss after 649144980 batches: 0.1036
trigger times: 4
Loss after 649276080 batches: 0.0965
trigger times: 5
Loss after 649407180 batches: 0.0895
trigger times: 6
Loss after 649538280 batches: 0.0849
trigger times: 7
Loss after 649669380 batches: 0.0820
trigger times: 8
Loss after 649800480 batches: 0.0773
trigger times: 9
Loss after 649931580 batches: 0.0753
trigger times: 10
Loss after 650062680 batches: 0.0725
trigger times: 11
Loss after 650193780 batches: 0.0696
trigger times: 12
Loss after 650324880 batches: 0.0679
trigger times: 13
Loss after 650455980 batches: 0.0655
trigger times: 14
Loss after 650587080 batches: 0.0640
trigger times: 15
Loss after 650718180 batches: 0.0618
trigger times: 16
Loss after 650849280 batches: 0.0602
trigger times: 17
Loss after 650980380 batches: 0.0588
trigger times: 18
Loss after 651111480 batches: 0.0580
trigger times: 19
Loss after 651242580 batches: 0.0568
trigger times: 20
Early stopping!
Start to test process.
Loss after 651373680 batches: 0.0554
Time to train on one home:  201.5216143131256
trigger times: 0
Loss after 651504780 batches: 0.7759
trigger times: 1
Loss after 651635880 batches: 0.4920
trigger times: 2
Loss after 651766980 batches: 0.2823
trigger times: 3
Loss after 651898080 batches: 0.2048
trigger times: 4
Loss after 652029180 batches: 0.1633
trigger times: 5
Loss after 652160280 batches: 0.1438
trigger times: 6
Loss after 652291380 batches: 0.1299
trigger times: 7
Loss after 652422480 batches: 0.1204
trigger times: 8
Loss after 652553580 batches: 0.1104
trigger times: 9
Loss after 652684680 batches: 0.1041
trigger times: 10
Loss after 652815780 batches: 0.1006
trigger times: 11
Loss after 652946880 batches: 0.0951
trigger times: 12
Loss after 653077980 batches: 0.0903
trigger times: 13
Loss after 653209080 batches: 0.0874
trigger times: 14
Loss after 653340180 batches: 0.0838
trigger times: 15
Loss after 653471280 batches: 0.0833
trigger times: 16
Loss after 653602380 batches: 0.0814
trigger times: 17
Loss after 653733480 batches: 0.0789
trigger times: 18
Loss after 653864580 batches: 0.0753
trigger times: 19
Loss after 653995680 batches: 0.0763
trigger times: 20
Early stopping!
Start to test process.
Loss after 654126780 batches: 0.0728
Time to train on one home:  164.28885912895203
trigger times: 0
Loss after 654220740 batches: 0.6910
trigger times: 1
Loss after 654314700 batches: 0.4157
trigger times: 2
Loss after 654408660 batches: 0.2636
trigger times: 3
Loss after 654502620 batches: 0.1916
trigger times: 4
Loss after 654596580 batches: 0.1528
trigger times: 5
Loss after 654690540 batches: 0.1292
trigger times: 6
Loss after 654784500 batches: 0.1159
trigger times: 7
Loss after 654878460 batches: 0.1068
trigger times: 8
Loss after 654972420 batches: 0.0977
trigger times: 9
Loss after 655066380 batches: 0.0940
trigger times: 10
Loss after 655160340 batches: 0.0897
trigger times: 11
Loss after 655254300 batches: 0.0845
trigger times: 12
Loss after 655348260 batches: 0.0813
trigger times: 13
Loss after 655442220 batches: 0.0786
trigger times: 14
Loss after 655536180 batches: 0.0760
trigger times: 15
Loss after 655630140 batches: 0.0747
trigger times: 0
Loss after 655724100 batches: 0.0732
trigger times: 1
Loss after 655818060 batches: 0.0701
trigger times: 2
Loss after 655912020 batches: 0.0674
trigger times: 3
Loss after 656005980 batches: 0.0667
trigger times: 4
Loss after 656099940 batches: 0.0654
trigger times: 5
Loss after 656193900 batches: 0.0646
trigger times: 6
Loss after 656287860 batches: 0.0627
trigger times: 7
Loss after 656381820 batches: 0.0615
trigger times: 8
Loss after 656475780 batches: 0.0605
trigger times: 9
Loss after 656569740 batches: 0.0603
trigger times: 10
Loss after 656663700 batches: 0.0579
trigger times: 11
Loss after 656757660 batches: 0.0584
trigger times: 0
Loss after 656851620 batches: 0.0573
trigger times: 1
Loss after 656945580 batches: 0.0568
trigger times: 2
Loss after 657039540 batches: 0.0555
trigger times: 3
Loss after 657133500 batches: 0.0553
trigger times: 4
Loss after 657227460 batches: 0.0531
trigger times: 5
Loss after 657321420 batches: 0.0540
trigger times: 6
Loss after 657415380 batches: 0.0535
trigger times: 7
Loss after 657509340 batches: 0.0524
trigger times: 8
Loss after 657603300 batches: 0.0528
trigger times: 9
Loss after 657697260 batches: 0.0519
trigger times: 10
Loss after 657791220 batches: 0.0507
trigger times: 11
Loss after 657885180 batches: 0.0506
trigger times: 12
Loss after 657979140 batches: 0.0492
trigger times: 13
Loss after 658073100 batches: 0.0485
trigger times: 14
Loss after 658167060 batches: 0.0498
trigger times: 15
Loss after 658261020 batches: 0.0489
trigger times: 0
Loss after 658354980 batches: 0.0483
trigger times: 1
Loss after 658448940 batches: 0.0477
trigger times: 2
Loss after 658542900 batches: 0.0467
trigger times: 3
Loss after 658636860 batches: 0.0474
trigger times: 4
Loss after 658730820 batches: 0.0461
trigger times: 0
Loss after 658824780 batches: 0.0478
trigger times: 1
Loss after 658918740 batches: 0.0460
trigger times: 2
Loss after 659012700 batches: 0.0455
trigger times: 3
Loss after 659106660 batches: 0.0446
trigger times: 4
Loss after 659200620 batches: 0.0444
trigger times: 5
Loss after 659294580 batches: 0.0438
trigger times: 6
Loss after 659388540 batches: 0.0433
trigger times: 7
Loss after 659482500 batches: 0.0430
trigger times: 8
Loss after 659576460 batches: 0.0425
trigger times: 9
Loss after 659670420 batches: 0.0427
trigger times: 10
Loss after 659764380 batches: 0.0429
trigger times: 11
Loss after 659858340 batches: 0.0411
trigger times: 12
Loss after 659952300 batches: 0.0413
trigger times: 13
Loss after 660046260 batches: 0.0409
trigger times: 14
Loss after 660140220 batches: 0.0407
trigger times: 15
Loss after 660234180 batches: 0.0404
trigger times: 16
Loss after 660328140 batches: 0.0409
trigger times: 17
Loss after 660422100 batches: 0.0403
trigger times: 18
Loss after 660516060 batches: 0.0396
trigger times: 19
Loss after 660610020 batches: 0.0394
trigger times: 20
Early stopping!
Start to test process.
Loss after 660703980 batches: 0.0387
Time to train on one home:  392.1865043640137
trigger times: 0
Loss after 660835080 batches: 0.0848
trigger times: 1
Loss after 660966180 batches: 0.0229
trigger times: 2
Loss after 661097280 batches: 0.0164
trigger times: 3
Loss after 661228380 batches: 0.0132
trigger times: 4
Loss after 661359480 batches: 0.0115
trigger times: 5
Loss after 661490580 batches: 0.0104
trigger times: 6
Loss after 661621680 batches: 0.0093
trigger times: 0
Loss after 661752780 batches: 0.0087
trigger times: 1
Loss after 661883880 batches: 0.0081
trigger times: 2
Loss after 662014980 batches: 0.0078
trigger times: 3
Loss after 662146080 batches: 0.0074
trigger times: 4
Loss after 662277180 batches: 0.0072
trigger times: 5
Loss after 662408280 batches: 0.0072
trigger times: 6
Loss after 662539380 batches: 0.0069
trigger times: 0
Loss after 662670480 batches: 0.0066
trigger times: 1
Loss after 662801580 batches: 0.0063
trigger times: 2
Loss after 662932680 batches: 0.0062
trigger times: 3
Loss after 663063780 batches: 0.0061
trigger times: 4
Loss after 663194880 batches: 0.0059
trigger times: 5
Loss after 663325980 batches: 0.0057
trigger times: 0
Loss after 663457080 batches: 0.0056
trigger times: 0
Loss after 663588180 batches: 0.0056
trigger times: 1
Loss after 663719280 batches: 0.0054
trigger times: 2
Loss after 663850380 batches: 0.0052
trigger times: 3
Loss after 663981480 batches: 0.0052
trigger times: 4
Loss after 664112580 batches: 0.0051
trigger times: 5
Loss after 664243680 batches: 0.0050
trigger times: 6
Loss after 664374780 batches: 0.0048
trigger times: 7
Loss after 664505880 batches: 0.0047
trigger times: 8
Loss after 664636980 batches: 0.0049
trigger times: 9
Loss after 664768080 batches: 0.0047
trigger times: 10
Loss after 664899180 batches: 0.0045
trigger times: 11
Loss after 665030280 batches: 0.0045
trigger times: 12
Loss after 665161380 batches: 0.0045
trigger times: 13
Loss after 665292480 batches: 0.0044
trigger times: 0
Loss after 665423580 batches: 0.0043
trigger times: 1
Loss after 665554680 batches: 0.0043
trigger times: 2
Loss after 665685780 batches: 0.0042
trigger times: 3
Loss after 665816880 batches: 0.0041
trigger times: 0
Loss after 665947980 batches: 0.0042
trigger times: 1
Loss after 666079080 batches: 0.0041
trigger times: 2
Loss after 666210180 batches: 0.0040
trigger times: 3
Loss after 666341280 batches: 0.0040
trigger times: 4
Loss after 666472380 batches: 0.0039
trigger times: 5
Loss after 666603480 batches: 0.0039
trigger times: 0
Loss after 666734580 batches: 0.0037
trigger times: 0
Loss after 666865680 batches: 0.0039
trigger times: 1
Loss after 666996780 batches: 0.0037
trigger times: 2
Loss after 667127880 batches: 0.0037
trigger times: 3
Loss after 667258980 batches: 0.0037
trigger times: 4
Loss after 667390080 batches: 0.0038
trigger times: 0
Loss after 667521180 batches: 0.0036
trigger times: 1
Loss after 667652280 batches: 0.0035
trigger times: 2
Loss after 667783380 batches: 0.0036
trigger times: 0
Loss after 667914480 batches: 0.0036
trigger times: 1
Loss after 668045580 batches: 0.0035
trigger times: 2
Loss after 668176680 batches: 0.0034
trigger times: 3
Loss after 668307780 batches: 0.0036
trigger times: 4
Loss after 668438880 batches: 0.0035
trigger times: 0
Loss after 668569980 batches: 0.0034
trigger times: 1
Loss after 668701080 batches: 0.0033
trigger times: 2
Loss after 668832180 batches: 0.0033
trigger times: 3
Loss after 668963280 batches: 0.0033
trigger times: 0
Loss after 669094380 batches: 0.0034
trigger times: 1
Loss after 669225480 batches: 0.0032
trigger times: 2
Loss after 669356580 batches: 0.0032
trigger times: 3
Loss after 669487680 batches: 0.0031
trigger times: 4
Loss after 669618780 batches: 0.0031
trigger times: 5
Loss after 669749880 batches: 0.0032
trigger times: 6
Loss after 669880980 batches: 0.0031
trigger times: 7
Loss after 670012080 batches: 0.0031
trigger times: 8
Loss after 670143180 batches: 0.0031
trigger times: 9
Loss after 670274280 batches: 0.0030
trigger times: 10
Loss after 670405380 batches: 0.0032
trigger times: 11
Loss after 670536480 batches: 0.0030
trigger times: 12
Loss after 670667580 batches: 0.0030
trigger times: 13
Loss after 670798680 batches: 0.0030
trigger times: 14
Loss after 670929780 batches: 0.0030
trigger times: 15
Loss after 671060880 batches: 0.0030
trigger times: 16
Loss after 671191980 batches: 0.0029
trigger times: 17
Loss after 671323080 batches: 0.0028
trigger times: 18
Loss after 671454180 batches: 0.0029
trigger times: 19
Loss after 671585280 batches: 0.0029
trigger times: 20
Early stopping!
Start to test process.
Loss after 671716380 batches: 0.0029
Time to train on one home:  620.6981310844421
trigger times: 0
Loss after 671847480 batches: 0.2244
trigger times: 1
Loss after 671978580 batches: 0.1137
trigger times: 2
Loss after 672109680 batches: 0.0820
trigger times: 0
Loss after 672240780 batches: 0.0649
trigger times: 1
Loss after 672371880 batches: 0.0561
trigger times: 2
Loss after 672502980 batches: 0.0486
trigger times: 0
Loss after 672634080 batches: 0.0454
trigger times: 0
Loss after 672765180 batches: 0.0410
trigger times: 1
Loss after 672896280 batches: 0.0387
trigger times: 2
Loss after 673027380 batches: 0.0373
trigger times: 3
Loss after 673158480 batches: 0.0347
trigger times: 4
Loss after 673289580 batches: 0.0335
trigger times: 5
Loss after 673420680 batches: 0.0321
trigger times: 6
Loss after 673551780 batches: 0.0305
trigger times: 7
Loss after 673682880 batches: 0.0297
trigger times: 8
Loss after 673813980 batches: 0.0291
trigger times: 0
Loss after 673945080 batches: 0.0279
trigger times: 1
Loss after 674076180 batches: 0.0274
trigger times: 0
Loss after 674207280 batches: 0.0266
trigger times: 1
Loss after 674338380 batches: 0.0253
trigger times: 2
Loss after 674469480 batches: 0.0253
trigger times: 3
Loss after 674600580 batches: 0.0246
trigger times: 4
Loss after 674731680 batches: 0.0241
trigger times: 5
Loss after 674862780 batches: 0.0233
trigger times: 6
Loss after 674993880 batches: 0.0237
trigger times: 0
Loss after 675124980 batches: 0.0229
trigger times: 1
Loss after 675256080 batches: 0.0224
trigger times: 2
Loss after 675387180 batches: 0.0218
trigger times: 0
Loss after 675518280 batches: 0.0215
trigger times: 1
Loss after 675649380 batches: 0.0213
trigger times: 2
Loss after 675780480 batches: 0.0215
trigger times: 3
Loss after 675911580 batches: 0.0204
trigger times: 4
Loss after 676042680 batches: 0.0202
trigger times: 5
Loss after 676173780 batches: 0.0201
trigger times: 6
Loss after 676304880 batches: 0.0201
trigger times: 7
Loss after 676435980 batches: 0.0195
trigger times: 8
Loss after 676567080 batches: 0.0193
trigger times: 9
Loss after 676698180 batches: 0.0190
trigger times: 0
Loss after 676829280 batches: 0.0187
trigger times: 0
Loss after 676960380 batches: 0.0189
trigger times: 0
Loss after 677091480 batches: 0.0185
trigger times: 1
Loss after 677222580 batches: 0.0185
trigger times: 2
Loss after 677353680 batches: 0.0183
trigger times: 3
Loss after 677484780 batches: 0.0177
trigger times: 4
Loss after 677615880 batches: 0.0177
trigger times: 5
Loss after 677746980 batches: 0.0177
trigger times: 0
Loss after 677878080 batches: 0.0172
trigger times: 1
Loss after 678009180 batches: 0.0176
trigger times: 2
Loss after 678140280 batches: 0.0172
trigger times: 3
Loss after 678271380 batches: 0.0167
trigger times: 0
Loss after 678402480 batches: 0.0169
trigger times: 1
Loss after 678533580 batches: 0.0169
trigger times: 2
Loss after 678664680 batches: 0.0168
trigger times: 3
Loss after 678795780 batches: 0.0166
trigger times: 4
Loss after 678926880 batches: 0.0160
trigger times: 5
Loss after 679057980 batches: 0.0160
trigger times: 6
Loss after 679189080 batches: 0.0158
trigger times: 7
Loss after 679320180 batches: 0.0157
trigger times: 8
Loss after 679451280 batches: 0.0156
trigger times: 9
Loss after 679582380 batches: 0.0153
trigger times: 10
Loss after 679713480 batches: 0.0158
trigger times: 0
Loss after 679844580 batches: 0.0156
trigger times: 1
Loss after 679975680 batches: 0.0151
trigger times: 2
Loss after 680106780 batches: 0.0151
trigger times: 3
Loss after 680237880 batches: 0.0150
trigger times: 4
Loss after 680368980 batches: 0.0151
trigger times: 5
Loss after 680500080 batches: 0.0146
trigger times: 6
Loss after 680631180 batches: 0.0145
trigger times: 7
Loss after 680762280 batches: 0.0145
trigger times: 8
Loss after 680893380 batches: 0.0146
trigger times: 9
Loss after 681024480 batches: 0.0142
trigger times: 10
Loss after 681155580 batches: 0.0140
trigger times: 11
Loss after 681286680 batches: 0.0142
trigger times: 12
Loss after 681417780 batches: 0.0139
trigger times: 13
Loss after 681548880 batches: 0.0141
trigger times: 14
Loss after 681679980 batches: 0.0139
trigger times: 15
Loss after 681811080 batches: 0.0136
trigger times: 16
Loss after 681942180 batches: 0.0136
trigger times: 17
Loss after 682073280 batches: 0.0134
trigger times: 18
Loss after 682204380 batches: 0.0132
trigger times: 19
Loss after 682335480 batches: 0.0132
trigger times: 20
Early stopping!
Start to test process.
Loss after 682466580 batches: 0.0132
Time to train on one home:  612.9462027549744
trigger times: 0
Loss after 682597680 batches: 0.4916
trigger times: 0
Loss after 682728780 batches: 0.2620
trigger times: 1
Loss after 682859880 batches: 0.1825
trigger times: 0
Loss after 682990980 batches: 0.1468
trigger times: 0
Loss after 683122080 batches: 0.1288
trigger times: 0
Loss after 683253180 batches: 0.1139
trigger times: 1
Loss after 683384280 batches: 0.1060
trigger times: 2
Loss after 683515380 batches: 0.1000
trigger times: 3
Loss after 683646480 batches: 0.0950
trigger times: 4
Loss after 683777580 batches: 0.0897
trigger times: 0
Loss after 683908680 batches: 0.0857
trigger times: 1
Loss after 684039780 batches: 0.0829
trigger times: 2
Loss after 684170880 batches: 0.0808
trigger times: 0
Loss after 684301980 batches: 0.0782
trigger times: 0
Loss after 684433080 batches: 0.0760
trigger times: 1
Loss after 684564180 batches: 0.0748
trigger times: 2
Loss after 684695280 batches: 0.0720
trigger times: 0
Loss after 684826380 batches: 0.0700
trigger times: 1
Loss after 684957480 batches: 0.0693
trigger times: 0
Loss after 685088580 batches: 0.0678
trigger times: 0
Loss after 685219680 batches: 0.0681
trigger times: 1
Loss after 685350780 batches: 0.0663
trigger times: 2
Loss after 685481880 batches: 0.0654
trigger times: 3
Loss after 685612980 batches: 0.0648
trigger times: 0
Loss after 685744080 batches: 0.0636
trigger times: 1
Loss after 685875180 batches: 0.0631
trigger times: 2
Loss after 686006280 batches: 0.0611
trigger times: 3
Loss after 686137380 batches: 0.0603
trigger times: 4
Loss after 686268480 batches: 0.0599
trigger times: 0
Loss after 686399580 batches: 0.0594
trigger times: 0
Loss after 686530680 batches: 0.0595
trigger times: 0
Loss after 686661780 batches: 0.0586
trigger times: 1
Loss after 686792880 batches: 0.0582
trigger times: 2
Loss after 686923980 batches: 0.0571
trigger times: 0
Loss after 687055080 batches: 0.0566
trigger times: 1
Loss after 687186180 batches: 0.0561
trigger times: 2
Loss after 687317280 batches: 0.0563
trigger times: 3
Loss after 687448380 batches: 0.0551
trigger times: 4
Loss after 687579480 batches: 0.0556
trigger times: 5
Loss after 687710580 batches: 0.0548
trigger times: 6
Loss after 687841680 batches: 0.0539
trigger times: 7
Loss after 687972780 batches: 0.0530
trigger times: 0
Loss after 688103880 batches: 0.0524
trigger times: 1
Loss after 688234980 batches: 0.0530
trigger times: 0
Loss after 688366080 batches: 0.0525
trigger times: 1
Loss after 688497180 batches: 0.0522
trigger times: 2
Loss after 688628280 batches: 0.0514
trigger times: 3
Loss after 688759380 batches: 0.0524
trigger times: 4
Loss after 688890480 batches: 0.0514
trigger times: 0
Loss after 689021580 batches: 0.0510
trigger times: 1
Loss after 689152680 batches: 0.0510
trigger times: 2
Loss after 689283780 batches: 0.0498
trigger times: 3
Loss after 689414880 batches: 0.0500
trigger times: 4
Loss after 689545980 batches: 0.0495
trigger times: 0
Loss after 689677080 batches: 0.0497
trigger times: 1
Loss after 689808180 batches: 0.0489
trigger times: 2
Loss after 689939280 batches: 0.0488
trigger times: 3
Loss after 690070380 batches: 0.0487
trigger times: 4
Loss after 690201480 batches: 0.0487
trigger times: 5
Loss after 690332580 batches: 0.0482
trigger times: 6
Loss after 690463680 batches: 0.0475
trigger times: 0
Loss after 690594780 batches: 0.0481
trigger times: 1
Loss after 690725880 batches: 0.0475
trigger times: 2
Loss after 690856980 batches: 0.0474
trigger times: 3
Loss after 690988080 batches: 0.0471
trigger times: 4
Loss after 691119180 batches: 0.0466
trigger times: 5
Loss after 691250280 batches: 0.0474
trigger times: 6
Loss after 691381380 batches: 0.0466
trigger times: 7
Loss after 691512480 batches: 0.0468
trigger times: 8
Loss after 691643580 batches: 0.0460
trigger times: 9
Loss after 691774680 batches: 0.0458
trigger times: 10
Loss after 691905780 batches: 0.0461
trigger times: 0
Loss after 692036880 batches: 0.0454
trigger times: 0
Loss after 692167980 batches: 0.0451
trigger times: 1
Loss after 692299080 batches: 0.0447
trigger times: 2
Loss after 692430180 batches: 0.0454
trigger times: 3
Loss after 692561280 batches: 0.0451
trigger times: 4
Loss after 692692380 batches: 0.0451
trigger times: 5
Loss after 692823480 batches: 0.0447
trigger times: 6
Loss after 692954580 batches: 0.0446
trigger times: 7
Loss after 693085680 batches: 0.0444
trigger times: 8
Loss after 693216780 batches: 0.0440
trigger times: 0
Loss after 693347880 batches: 0.0439
trigger times: 1
Loss after 693478980 batches: 0.0440
trigger times: 0
Loss after 693610080 batches: 0.0438
trigger times: 0
Loss after 693741180 batches: 0.0441
trigger times: 1
Loss after 693872280 batches: 0.0436
trigger times: 2
Loss after 694003380 batches: 0.0442
trigger times: 0
Loss after 694134480 batches: 0.0431
trigger times: 1
Loss after 694265580 batches: 0.0426
trigger times: 2
Loss after 694396680 batches: 0.0432
trigger times: 3
Loss after 694527780 batches: 0.0427
trigger times: 4
Loss after 694658880 batches: 0.0431
trigger times: 5
Loss after 694789980 batches: 0.0429
trigger times: 0
Loss after 694921080 batches: 0.0424
trigger times: 1
Loss after 695052180 batches: 0.0422
trigger times: 2
Loss after 695183280 batches: 0.0425
trigger times: 3
Loss after 695314380 batches: 0.0417
trigger times: 4
Loss after 695445480 batches: 0.0421
trigger times: 5
Loss after 695576580 batches: 0.0414
trigger times: 6
Loss after 695707680 batches: 0.0416
trigger times: 7
Loss after 695838780 batches: 0.0415
trigger times: 8
Loss after 695969880 batches: 0.0418
trigger times: 9
Loss after 696100980 batches: 0.0414
trigger times: 0
Loss after 696232080 batches: 0.0414
trigger times: 1
Loss after 696363180 batches: 0.0415
trigger times: 2
Loss after 696494280 batches: 0.0413
trigger times: 3
Loss after 696625380 batches: 0.0411
trigger times: 4
Loss after 696756480 batches: 0.0412
trigger times: 5
Loss after 696887580 batches: 0.0412
trigger times: 6
Loss after 697018680 batches: 0.0408
trigger times: 7
Loss after 697149780 batches: 0.0406
trigger times: 8
Loss after 697280880 batches: 0.0408
trigger times: 0
Loss after 697411980 batches: 0.0403
trigger times: 1
Loss after 697543080 batches: 0.0402
trigger times: 0
Loss after 697674180 batches: 0.0408
trigger times: 1
Loss after 697805280 batches: 0.0402
trigger times: 2
Loss after 697936380 batches: 0.0402
trigger times: 3
Loss after 698067480 batches: 0.0399
trigger times: 4
Loss after 698198580 batches: 0.0404
trigger times: 5
Loss after 698329680 batches: 0.0397
trigger times: 6
Loss after 698460780 batches: 0.0398
trigger times: 0
Loss after 698591880 batches: 0.0395
trigger times: 1
Loss after 698722980 batches: 0.0398
trigger times: 2
Loss after 698854080 batches: 0.0393
trigger times: 3
Loss after 698985180 batches: 0.0394
trigger times: 4
Loss after 699116280 batches: 0.0398
trigger times: 5
Loss after 699247380 batches: 0.0396
trigger times: 6
Loss after 699378480 batches: 0.0393
trigger times: 7
Loss after 699509580 batches: 0.0390
trigger times: 8
Loss after 699640680 batches: 0.0388
trigger times: 9
Loss after 699771780 batches: 0.0389
trigger times: 0
Loss after 699902880 batches: 0.0388
trigger times: 1
Loss after 700033980 batches: 0.0386
trigger times: 2
Loss after 700165080 batches: 0.0385
trigger times: 3
Loss after 700296180 batches: 0.0388
trigger times: 4
Loss after 700427280 batches: 0.0384
trigger times: 5
Loss after 700558380 batches: 0.0385
trigger times: 6
Loss after 700689480 batches: 0.0381
trigger times: 0
Loss after 700820580 batches: 0.0383
trigger times: 1
Loss after 700951680 batches: 0.0381
trigger times: 2
Loss after 701082780 batches: 0.0384
trigger times: 3
Loss after 701213880 batches: 0.0385
trigger times: 4
Loss after 701344980 batches: 0.0379
trigger times: 5
Loss after 701476080 batches: 0.0388
trigger times: 6
Loss after 701607180 batches: 0.0375
trigger times: 7
Loss after 701738280 batches: 0.0379
trigger times: 8
Loss after 701869380 batches: 0.0377
trigger times: 9
Loss after 702000480 batches: 0.0377
trigger times: 0
Loss after 702131580 batches: 0.0375
trigger times: 1
Loss after 702262680 batches: 0.0379
trigger times: 2
Loss after 702393780 batches: 0.0375
trigger times: 3
Loss after 702524880 batches: 0.0373
trigger times: 4
Loss after 702655980 batches: 0.0373
trigger times: 5
Loss after 702787080 batches: 0.0369
trigger times: 6
Loss after 702918180 batches: 0.0371
trigger times: 7
Loss after 703049280 batches: 0.0369
trigger times: 8
Loss after 703180380 batches: 0.0372
trigger times: 9
Loss after 703311480 batches: 0.0372
trigger times: 10
Loss after 703442580 batches: 0.0372
trigger times: 11
Loss after 703573680 batches: 0.0372
trigger times: 12
Loss after 703704780 batches: 0.0370
trigger times: 13
Loss after 703835880 batches: 0.0369
trigger times: 14
Loss after 703966980 batches: 0.0369
trigger times: 15
Loss after 704098080 batches: 0.0366
trigger times: 16
Loss after 704229180 batches: 0.0369
trigger times: 17
Loss after 704360280 batches: 0.0368
trigger times: 18
Loss after 704491380 batches: 0.0370
trigger times: 19
Loss after 704622480 batches: 0.0371
trigger times: 0
Loss after 704753580 batches: 0.0372
trigger times: 1
Loss after 704884680 batches: 0.0366
trigger times: 2
Loss after 705015780 batches: 0.0373
trigger times: 3
Loss after 705146880 batches: 0.0360
trigger times: 4
Loss after 705277980 batches: 0.0361
trigger times: 5
Loss after 705409080 batches: 0.0365
trigger times: 6
Loss after 705540180 batches: 0.0363
trigger times: 7
Loss after 705671280 batches: 0.0362
trigger times: 8
Loss after 705802380 batches: 0.0363
trigger times: 9
Loss after 705933480 batches: 0.0360
trigger times: 10
Loss after 706064580 batches: 0.0360
trigger times: 11
Loss after 706195680 batches: 0.0405
trigger times: 12
Loss after 706326780 batches: 0.0390
trigger times: 13
Loss after 706457880 batches: 0.0369
trigger times: 14
Loss after 706588980 batches: 0.0374
trigger times: 15
Loss after 706720080 batches: 0.0368
trigger times: 16
Loss after 706851180 batches: 0.0362
trigger times: 17
Loss after 706982280 batches: 0.0366
trigger times: 18
Loss after 707113380 batches: 0.0361
trigger times: 19
Loss after 707244480 batches: 0.0358
trigger times: 20
Early stopping!
Start to test process.
Loss after 707375580 batches: 0.0360
Time to train on one home:  1392.1466009616852
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143]]
Round_6_results:  [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5607 < 5608; dropping {'Training_Loss': 0.459123955596061, 'Validation_Loss': 0.8001798921161227, 'Training_R2': 0.5357219340864968, 'Validation_R2': 0.254602328475821, 'Training_F1': 0.733202169039677, 'Validation_F1': 0.4911094522704214, 'Training_NEP': 0.5343152191486061, 'Validation_NEP': 1.1708346526734033, 'Training_NDE': 0.31273318760286917, 'Validation_NDE': 0.5935954126273425, 'Training_MAE': 35.06808687284443, 'Validation_MAE': 32.10914367172453, 'Training_MSE': 4127.0522, 'Validation_MSE': 2192.1316}.
trigger times: 0
Loss after 707478180 batches: 0.4591
trigger times: 0
Loss after 707580780 batches: 0.2215
trigger times: 1
Loss after 707683380 batches: 0.1456
trigger times: 2
Loss after 707785980 batches: 0.1135
trigger times: 3
Loss after 707888580 batches: 0.1016
trigger times: 4
Loss after 707991180 batches: 0.0937
trigger times: 5
Loss after 708093780 batches: 0.0818
trigger times: 6
Loss after 708196380 batches: 0.0719
trigger times: 7
Loss after 708298980 batches: 0.0673
trigger times: 8
Loss after 708401580 batches: 0.0641
trigger times: 9
Loss after 708504180 batches: 0.0625
trigger times: 0
Loss after 708606780 batches: 0.0618
trigger times: 1
Loss after 708709380 batches: 0.0546
trigger times: 2
Loss after 708811980 batches: 0.0549
trigger times: 3
Loss after 708914580 batches: 0.0584
trigger times: 4
Loss after 709017180 batches: 0.0574
trigger times: 5
Loss after 709119780 batches: 0.0491
trigger times: 6
Loss after 709222380 batches: 0.0483
trigger times: 7
Loss after 709324980 batches: 0.0485
trigger times: 8
Loss after 709427580 batches: 0.0461
trigger times: 9
Loss after 709530180 batches: 0.0437
trigger times: 10
Loss after 709632780 batches: 0.0421
trigger times: 11
Loss after 709735380 batches: 0.0432
trigger times: 12
Loss after 709837980 batches: 0.0411
trigger times: 13
Loss after 709940580 batches: 0.0417
trigger times: 0
Loss after 710043180 batches: 0.0399
trigger times: 0
Loss after 710145780 batches: 0.0398
trigger times: 1
Loss after 710248380 batches: 0.0404
trigger times: 2
Loss after 710350980 batches: 0.0392
trigger times: 3
Loss after 710453580 batches: 0.0385
trigger times: 0
Loss after 710556180 batches: 0.0376
trigger times: 1
Loss after 710658780 batches: 0.0387
trigger times: 2
Loss after 710761380 batches: 0.0379
trigger times: 3
Loss after 710863980 batches: 0.0395
trigger times: 4
Loss after 710966580 batches: 0.0356
trigger times: 5
Loss after 711069180 batches: 0.0363
trigger times: 6
Loss after 711171780 batches: 0.0353
trigger times: 7
Loss after 711274380 batches: 0.0352
trigger times: 8
Loss after 711376980 batches: 0.0344
trigger times: 0
Loss after 711479580 batches: 0.0340
trigger times: 1
Loss after 711582180 batches: 0.0331
trigger times: 2
Loss after 711684780 batches: 0.0332
trigger times: 0
Loss after 711787380 batches: 0.0333
trigger times: 0
Loss after 711889980 batches: 0.0355
trigger times: 1
Loss after 711992580 batches: 0.0331
trigger times: 2
Loss after 712095180 batches: 0.0334
trigger times: 3
Loss after 712197780 batches: 0.0318
trigger times: 4
Loss after 712300380 batches: 0.0330
trigger times: 5
Loss after 712402980 batches: 0.0305
trigger times: 6
Loss after 712505580 batches: 0.0302
trigger times: 7
Loss after 712608180 batches: 0.0322
trigger times: 0
Loss after 712710780 batches: 0.0317
trigger times: 1
Loss after 712813380 batches: 0.0307
trigger times: 2
Loss after 712915980 batches: 0.0304
trigger times: 3
Loss after 713018580 batches: 0.0300
trigger times: 4
Loss after 713121180 batches: 0.0299
trigger times: 5
Loss after 713223780 batches: 0.0298
trigger times: 0
Loss after 713326380 batches: 0.0326
trigger times: 1
Loss after 713428980 batches: 0.0300
trigger times: 2
Loss after 713531580 batches: 0.0302
trigger times: 3
Loss after 713634180 batches: 0.0298
trigger times: 0
Loss after 713736780 batches: 0.0317
trigger times: 1
Loss after 713839380 batches: 0.0304
trigger times: 2
Loss after 713941980 batches: 0.0283
trigger times: 3
Loss after 714044580 batches: 0.0281
trigger times: 4
Loss after 714147180 batches: 0.0278
trigger times: 5
Loss after 714249780 batches: 0.0270
trigger times: 0
Loss after 714352380 batches: 0.0276
trigger times: 1
Loss after 714454980 batches: 0.0277
trigger times: 2
Loss after 714557580 batches: 0.0281
trigger times: 3
Loss after 714660180 batches: 0.0299
trigger times: 4
Loss after 714762780 batches: 0.0288
trigger times: 5
Loss after 714865380 batches: 0.0275
trigger times: 6
Loss after 714967980 batches: 0.0272
trigger times: 7
Loss after 715070580 batches: 0.0292
trigger times: 8
Loss after 715173180 batches: 0.0293
trigger times: 9
Loss after 715275780 batches: 0.0279
trigger times: 10
Loss after 715378380 batches: 0.0262
trigger times: 11
Loss after 715480980 batches: 0.0257
trigger times: 12
Loss after 715583580 batches: 0.0274
trigger times: 13
Loss after 715686180 batches: 0.0261
trigger times: 14
Loss after 715788780 batches: 0.0253
trigger times: 15
Loss after 715891380 batches: 0.0248
trigger times: 16
Loss after 715993980 batches: 0.0248
trigger times: 17
Loss after 716096580 batches: 0.0262
trigger times: 18
Loss after 716199180 batches: 0.0247
trigger times: 19
Loss after 716301780 batches: 0.0251
trigger times: 20
Early stopping!
Start to test process.
Loss after 716404380 batches: 0.0250
Time to train on one home:  533.6933987140656
trigger times: 0
Loss after 716535480 batches: 0.3247
trigger times: 1
Loss after 716666580 batches: 0.1652
trigger times: 2
Loss after 716797680 batches: 0.1093
trigger times: 3
Loss after 716928780 batches: 0.0894
trigger times: 4
Loss after 717059880 batches: 0.0763
trigger times: 5
Loss after 717190980 batches: 0.0686
trigger times: 6
Loss after 717322080 batches: 0.0627
trigger times: 7
Loss after 717453180 batches: 0.0572
trigger times: 8
Loss after 717584280 batches: 0.0545
trigger times: 9
Loss after 717715380 batches: 0.0515
trigger times: 10
Loss after 717846480 batches: 0.0497
trigger times: 11
Loss after 717977580 batches: 0.0475
trigger times: 12
Loss after 718108680 batches: 0.0459
trigger times: 13
Loss after 718239780 batches: 0.0447
trigger times: 14
Loss after 718370880 batches: 0.0429
trigger times: 15
Loss after 718501980 batches: 0.0418
trigger times: 16
Loss after 718633080 batches: 0.0407
trigger times: 17
Loss after 718764180 batches: 0.0399
trigger times: 18
Loss after 718895280 batches: 0.0389
trigger times: 19
Loss after 719026380 batches: 0.0382
trigger times: 20
Early stopping!
Start to test process.
Loss after 719157480 batches: 0.0372
Time to train on one home:  164.48675155639648
trigger times: 0
Loss after 719288580 batches: 0.5928
trigger times: 1
Loss after 719419680 batches: 0.3030
trigger times: 2
Loss after 719550780 batches: 0.1851
trigger times: 3
Loss after 719681880 batches: 0.1418
trigger times: 4
Loss after 719812980 batches: 0.1199
trigger times: 5
Loss after 719944080 batches: 0.1054
trigger times: 6
Loss after 720075180 batches: 0.0973
trigger times: 7
Loss after 720206280 batches: 0.0887
trigger times: 8
Loss after 720337380 batches: 0.0852
trigger times: 9
Loss after 720468480 batches: 0.0800
trigger times: 10
Loss after 720599580 batches: 0.0763
trigger times: 11
Loss after 720730680 batches: 0.0722
trigger times: 12
Loss after 720861780 batches: 0.0707
trigger times: 13
Loss after 720992880 batches: 0.0674
trigger times: 14
Loss after 721123980 batches: 0.0652
trigger times: 15
Loss after 721255080 batches: 0.0636
trigger times: 16
Loss after 721386180 batches: 0.0631
trigger times: 17
Loss after 721517280 batches: 0.0619
trigger times: 18
Loss after 721648380 batches: 0.0590
trigger times: 19
Loss after 721779480 batches: 0.0581
trigger times: 20
Early stopping!
Start to test process.
Loss after 721910580 batches: 0.0565
Time to train on one home:  165.5282256603241
trigger times: 0
Loss after 722039220 batches: 0.2461
trigger times: 1
Loss after 722167860 batches: 0.1091
trigger times: 2
Loss after 722296500 batches: 0.0778
trigger times: 3
Loss after 722425140 batches: 0.0645
trigger times: 4
Loss after 722553780 batches: 0.0557
trigger times: 5
Loss after 722682420 batches: 0.0498
trigger times: 0
Loss after 722811060 batches: 0.0468
trigger times: 0
Loss after 722939700 batches: 0.0427
trigger times: 1
Loss after 723068340 batches: 0.0407
trigger times: 2
Loss after 723196980 batches: 0.0387
trigger times: 3
Loss after 723325620 batches: 0.0369
trigger times: 4
Loss after 723454260 batches: 0.0349
trigger times: 0
Loss after 723582900 batches: 0.0346
trigger times: 1
Loss after 723711540 batches: 0.0327
trigger times: 2
Loss after 723840180 batches: 0.0317
trigger times: 3
Loss after 723968820 batches: 0.0312
trigger times: 4
Loss after 724097460 batches: 0.0301
trigger times: 5
Loss after 724226100 batches: 0.0286
trigger times: 6
Loss after 724354740 batches: 0.0281
trigger times: 7
Loss after 724483380 batches: 0.0281
trigger times: 0
Loss after 724612020 batches: 0.0274
trigger times: 1
Loss after 724740660 batches: 0.0274
trigger times: 2
Loss after 724869300 batches: 0.0265
trigger times: 3
Loss after 724997940 batches: 0.0261
trigger times: 4
Loss after 725126580 batches: 0.0254
trigger times: 5
Loss after 725255220 batches: 0.0253
trigger times: 6
Loss after 725383860 batches: 0.0249
trigger times: 7
Loss after 725512500 batches: 0.0239
trigger times: 8
Loss after 725641140 batches: 0.0243
trigger times: 9
Loss after 725769780 batches: 0.0238
trigger times: 0
Loss after 725898420 batches: 0.0233
trigger times: 1
Loss after 726027060 batches: 0.0233
trigger times: 2
Loss after 726155700 batches: 0.0226
trigger times: 3
Loss after 726284340 batches: 0.0225
trigger times: 4
Loss after 726412980 batches: 0.0221
trigger times: 5
Loss after 726541620 batches: 0.0220
trigger times: 6
Loss after 726670260 batches: 0.0222
trigger times: 7
Loss after 726798900 batches: 0.0220
trigger times: 8
Loss after 726927540 batches: 0.0215
trigger times: 9
Loss after 727056180 batches: 0.0212
trigger times: 10
Loss after 727184820 batches: 0.0207
trigger times: 11
Loss after 727313460 batches: 0.0209
trigger times: 12
Loss after 727442100 batches: 0.0204
trigger times: 0
Loss after 727570740 batches: 0.0204
trigger times: 0
Loss after 727699380 batches: 0.0202
trigger times: 0
Loss after 727828020 batches: 0.0198
trigger times: 0
Loss after 727956660 batches: 0.0197
trigger times: 1
Loss after 728085300 batches: 0.0196
trigger times: 2
Loss after 728213940 batches: 0.0197
trigger times: 3
Loss after 728342580 batches: 0.0195
trigger times: 4
Loss after 728471220 batches: 0.0193
trigger times: 5
Loss after 728599860 batches: 0.0188
trigger times: 6
Loss after 728728500 batches: 0.0186
trigger times: 7
Loss after 728857140 batches: 0.0188
trigger times: 0
Loss after 728985780 batches: 0.0190
trigger times: 1
Loss after 729114420 batches: 0.0185
trigger times: 2
Loss after 729243060 batches: 0.0186
trigger times: 3
Loss after 729371700 batches: 0.0184
trigger times: 4
Loss after 729500340 batches: 0.0185
trigger times: 5
Loss after 729628980 batches: 0.0179
trigger times: 6
Loss after 729757620 batches: 0.0181
trigger times: 7
Loss after 729886260 batches: 0.0174
trigger times: 8
Loss after 730014900 batches: 0.0176
trigger times: 9
Loss after 730143540 batches: 0.0172
trigger times: 0
Loss after 730272180 batches: 0.0176
trigger times: 1
Loss after 730400820 batches: 0.0178
trigger times: 2
Loss after 730529460 batches: 0.0172
trigger times: 3
Loss after 730658100 batches: 0.0170
trigger times: 4
Loss after 730786740 batches: 0.0175
trigger times: 5
Loss after 730915380 batches: 0.0169
trigger times: 6
Loss after 731044020 batches: 0.0168
trigger times: 7
Loss after 731172660 batches: 0.0168
trigger times: 8
Loss after 731301300 batches: 0.0166
trigger times: 9
Loss after 731429940 batches: 0.0169
trigger times: 10
Loss after 731558580 batches: 0.0170
trigger times: 11
Loss after 731687220 batches: 0.0163
trigger times: 12
Loss after 731815860 batches: 0.0162
trigger times: 13
Loss after 731944500 batches: 0.0164
trigger times: 14
Loss after 732073140 batches: 0.0160
trigger times: 15
Loss after 732201780 batches: 0.0166
trigger times: 16
Loss after 732330420 batches: 0.0160
trigger times: 17
Loss after 732459060 batches: 0.0157
trigger times: 18
Loss after 732587700 batches: 0.0155
trigger times: 19
Loss after 732716340 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 732844980 batches: 0.0156
Time to train on one home:  621.0414304733276
trigger times: 0
Loss after 732976080 batches: 0.6740
trigger times: 1
Loss after 733107180 batches: 0.3989
trigger times: 2
Loss after 733238280 batches: 0.2445
trigger times: 3
Loss after 733369380 batches: 0.1688
trigger times: 4
Loss after 733500480 batches: 0.1355
trigger times: 5
Loss after 733631580 batches: 0.1165
trigger times: 6
Loss after 733762680 batches: 0.1034
trigger times: 7
Loss after 733893780 batches: 0.0942
trigger times: 8
Loss after 734024880 batches: 0.0897
trigger times: 9
Loss after 734155980 batches: 0.0837
trigger times: 10
Loss after 734287080 batches: 0.0794
trigger times: 11
Loss after 734418180 batches: 0.0769
trigger times: 12
Loss after 734549280 batches: 0.0733
trigger times: 13
Loss after 734680380 batches: 0.0711
trigger times: 14
Loss after 734811480 batches: 0.0694
trigger times: 15
Loss after 734942580 batches: 0.0659
trigger times: 16
Loss after 735073680 batches: 0.0656
trigger times: 17
Loss after 735204780 batches: 0.0638
trigger times: 18
Loss after 735335880 batches: 0.0620
trigger times: 19
Loss after 735466980 batches: 0.0613
trigger times: 20
Early stopping!
Start to test process.
Loss after 735598080 batches: 0.0603
Time to train on one home:  165.77033519744873
trigger times: 0
Loss after 735729180 batches: 0.6703
trigger times: 1
Loss after 735860280 batches: 0.4261
trigger times: 2
Loss after 735991380 batches: 0.2896
trigger times: 3
Loss after 736122480 batches: 0.2305
trigger times: 4
Loss after 736253580 batches: 0.1836
trigger times: 5
Loss after 736384680 batches: 0.1520
trigger times: 6
Loss after 736515780 batches: 0.1330
trigger times: 7
Loss after 736646880 batches: 0.1229
trigger times: 8
Loss after 736777980 batches: 0.1127
trigger times: 9
Loss after 736909080 batches: 0.1045
trigger times: 10
Loss after 737040180 batches: 0.0978
trigger times: 11
Loss after 737171280 batches: 0.0962
trigger times: 12
Loss after 737302380 batches: 0.0916
trigger times: 13
Loss after 737433480 batches: 0.0848
trigger times: 14
Loss after 737564580 batches: 0.0850
trigger times: 15
Loss after 737695680 batches: 0.0763
trigger times: 16
Loss after 737826780 batches: 0.0744
trigger times: 17
Loss after 737957880 batches: 0.0748
trigger times: 18
Loss after 738088980 batches: 0.0749
trigger times: 19
Loss after 738220080 batches: 0.0716
trigger times: 20
Early stopping!
Start to test process.
Loss after 738351180 batches: 0.0709
Time to train on one home:  165.7061984539032
trigger times: 0
Loss after 738482280 batches: 0.1827
trigger times: 1
Loss after 738613380 batches: 0.0655
trigger times: 2
Loss after 738744480 batches: 0.0430
trigger times: 3
Loss after 738875580 batches: 0.0354
trigger times: 4
Loss after 739006680 batches: 0.0314
trigger times: 5
Loss after 739137780 batches: 0.0289
trigger times: 6
Loss after 739268880 batches: 0.0268
trigger times: 7
Loss after 739399980 batches: 0.0250
trigger times: 8
Loss after 739531080 batches: 0.0242
trigger times: 9
Loss after 739662180 batches: 0.0226
trigger times: 10
Loss after 739793280 batches: 0.0227
trigger times: 11
Loss after 739924380 batches: 0.0217
trigger times: 12
Loss after 740055480 batches: 0.0207
trigger times: 13
Loss after 740186580 batches: 0.0203
trigger times: 14
Loss after 740317680 batches: 0.0197
trigger times: 15
Loss after 740448780 batches: 0.0197
trigger times: 16
Loss after 740579880 batches: 0.0193
trigger times: 17
Loss after 740710980 batches: 0.0191
trigger times: 18
Loss after 740842080 batches: 0.0186
trigger times: 19
Loss after 740973180 batches: 0.0183
trigger times: 20
Early stopping!
Start to test process.
Loss after 741104280 batches: 0.0180
Time to train on one home:  165.2420072555542
trigger times: 0
Loss after 741235380 batches: 0.2614
trigger times: 0
Loss after 741366480 batches: 0.1160
trigger times: 0
Loss after 741497580 batches: 0.0780
trigger times: 1
Loss after 741628680 batches: 0.0608
trigger times: 2
Loss after 741759780 batches: 0.0517
trigger times: 3
Loss after 741890880 batches: 0.0448
trigger times: 4
Loss after 742021980 batches: 0.0407
trigger times: 5
Loss after 742153080 batches: 0.0371
trigger times: 6
Loss after 742284180 batches: 0.0351
trigger times: 7
Loss after 742415280 batches: 0.0332
trigger times: 8
Loss after 742546380 batches: 0.0310
trigger times: 9
Loss after 742677480 batches: 0.0298
trigger times: 10
Loss after 742808580 batches: 0.0285
trigger times: 11
Loss after 742939680 batches: 0.0277
trigger times: 12
Loss after 743070780 batches: 0.0261
trigger times: 13
Loss after 743201880 batches: 0.0256
trigger times: 14
Loss after 743332980 batches: 0.0246
trigger times: 15
Loss after 743464080 batches: 0.0242
trigger times: 16
Loss after 743595180 batches: 0.0235
trigger times: 17
Loss after 743726280 batches: 0.0231
trigger times: 18
Loss after 743857380 batches: 0.0227
trigger times: 19
Loss after 743988480 batches: 0.0222
trigger times: 20
Early stopping!
Start to test process.
Loss after 744119580 batches: 0.0217
Time to train on one home:  180.25124788284302
trigger times: 0
Loss after 744198180 batches: 0.5579
trigger times: 1
Loss after 744276780 batches: 0.2808
trigger times: 2
Loss after 744355380 batches: 0.1597
trigger times: 3
Loss after 744433980 batches: 0.1159
trigger times: 4
Loss after 744512580 batches: 0.0929
trigger times: 5
Loss after 744591180 batches: 0.0814
trigger times: 6
Loss after 744669780 batches: 0.0737
trigger times: 7
Loss after 744748380 batches: 0.0672
trigger times: 8
Loss after 744826980 batches: 0.0630
trigger times: 9
Loss after 744905580 batches: 0.0582
trigger times: 10
Loss after 744984180 batches: 0.0579
trigger times: 11
Loss after 745062780 batches: 0.0549
trigger times: 12
Loss after 745141380 batches: 0.0512
trigger times: 13
Loss after 745219980 batches: 0.0516
trigger times: 14
Loss after 745298580 batches: 0.0474
trigger times: 15
Loss after 745377180 batches: 0.0486
trigger times: 16
Loss after 745455780 batches: 0.0459
trigger times: 17
Loss after 745534380 batches: 0.0448
trigger times: 18
Loss after 745612980 batches: 0.0439
trigger times: 19
Loss after 745691580 batches: 0.0431
trigger times: 20
Early stopping!
Start to test process.
Loss after 745770180 batches: 0.0423
Time to train on one home:  110.93476605415344
trigger times: 0
Loss after 745901280 batches: 0.2125
trigger times: 1
Loss after 746032380 batches: 0.0767
trigger times: 2
Loss after 746163480 batches: 0.0508
trigger times: 3
Loss after 746294580 batches: 0.0417
trigger times: 4
Loss after 746425680 batches: 0.0366
trigger times: 5
Loss after 746556780 batches: 0.0334
trigger times: 0
Loss after 746687880 batches: 0.0312
trigger times: 1
Loss after 746818980 batches: 0.0303
trigger times: 2
Loss after 746950080 batches: 0.0280
trigger times: 3
Loss after 747081180 batches: 0.0271
trigger times: 0
Loss after 747212280 batches: 0.0262
trigger times: 1
Loss after 747343380 batches: 0.0245
trigger times: 0
Loss after 747474480 batches: 0.0245
trigger times: 0
Loss after 747605580 batches: 0.0242
trigger times: 1
Loss after 747736680 batches: 0.0235
trigger times: 0
Loss after 747867780 batches: 0.0226
trigger times: 0
Loss after 747998880 batches: 0.0223
trigger times: 0
Loss after 748129980 batches: 0.0220
trigger times: 1
Loss after 748261080 batches: 0.0214
trigger times: 2
Loss after 748392180 batches: 0.0211
trigger times: 3
Loss after 748523280 batches: 0.0207
trigger times: 0
Loss after 748654380 batches: 0.0205
trigger times: 1
Loss after 748785480 batches: 0.0200
trigger times: 2
Loss after 748916580 batches: 0.0196
trigger times: 3
Loss after 749047680 batches: 0.0199
trigger times: 4
Loss after 749178780 batches: 0.0191
trigger times: 5
Loss after 749309880 batches: 0.0191
trigger times: 6
Loss after 749440980 batches: 0.0187
trigger times: 0
Loss after 749572080 batches: 0.0189
trigger times: 0
Loss after 749703180 batches: 0.0185
trigger times: 1
Loss after 749834280 batches: 0.0185
trigger times: 2
Loss after 749965380 batches: 0.0182
trigger times: 3
Loss after 750096480 batches: 0.0177
trigger times: 4
Loss after 750227580 batches: 0.0177
trigger times: 5
Loss after 750358680 batches: 0.0175
trigger times: 6
Loss after 750489780 batches: 0.0174
trigger times: 7
Loss after 750620880 batches: 0.0170
trigger times: 0
Loss after 750751980 batches: 0.0169
trigger times: 1
Loss after 750883080 batches: 0.0167
trigger times: 2
Loss after 751014180 batches: 0.0169
trigger times: 3
Loss after 751145280 batches: 0.0167
trigger times: 4
Loss after 751276380 batches: 0.0162
trigger times: 5
Loss after 751407480 batches: 0.0161
trigger times: 0
Loss after 751538580 batches: 0.0160
trigger times: 1
Loss after 751669680 batches: 0.0159
trigger times: 0
Loss after 751800780 batches: 0.0156
trigger times: 1
Loss after 751931880 batches: 0.0158
trigger times: 2
Loss after 752062980 batches: 0.0156
trigger times: 3
Loss after 752194080 batches: 0.0156
trigger times: 4
Loss after 752325180 batches: 0.0154
trigger times: 5
Loss after 752456280 batches: 0.0152
trigger times: 6
Loss after 752587380 batches: 0.0149
trigger times: 7
Loss after 752718480 batches: 0.0148
trigger times: 8
Loss after 752849580 batches: 0.0150
trigger times: 9
Loss after 752980680 batches: 0.0146
trigger times: 10
Loss after 753111780 batches: 0.0148
trigger times: 11
Loss after 753242880 batches: 0.0147
trigger times: 12
Loss after 753373980 batches: 0.0148
trigger times: 13
Loss after 753505080 batches: 0.0146
trigger times: 14
Loss after 753636180 batches: 0.0144
trigger times: 15
Loss after 753767280 batches: 0.0143
trigger times: 16
Loss after 753898380 batches: 0.0140
trigger times: 17
Loss after 754029480 batches: 0.0139
trigger times: 18
Loss after 754160580 batches: 0.0138
trigger times: 19
Loss after 754291680 batches: 0.0140
trigger times: 20
Early stopping!
Start to test process.
Loss after 754422780 batches: 0.0139
Time to train on one home:  493.2427840232849
trigger times: 0
Loss after 754553880 batches: 0.2970
trigger times: 1
Loss after 754684980 batches: 0.1462
trigger times: 2
Loss after 754816080 batches: 0.1046
trigger times: 3
Loss after 754947180 batches: 0.0858
trigger times: 4
Loss after 755078280 batches: 0.0752
trigger times: 5
Loss after 755209380 batches: 0.0677
trigger times: 6
Loss after 755340480 batches: 0.0635
trigger times: 7
Loss after 755471580 batches: 0.0588
trigger times: 8
Loss after 755602680 batches: 0.0539
trigger times: 9
Loss after 755733780 batches: 0.0501
trigger times: 10
Loss after 755864880 batches: 0.0487
trigger times: 11
Loss after 755995980 batches: 0.0462
trigger times: 12
Loss after 756127080 batches: 0.0440
trigger times: 13
Loss after 756258180 batches: 0.0424
trigger times: 14
Loss after 756389280 batches: 0.0408
trigger times: 15
Loss after 756520380 batches: 0.0397
trigger times: 16
Loss after 756651480 batches: 0.0394
trigger times: 17
Loss after 756782580 batches: 0.0377
trigger times: 18
Loss after 756913680 batches: 0.0369
trigger times: 19
Loss after 757044780 batches: 0.0357
trigger times: 20
Early stopping!
Start to test process.
Loss after 757175880 batches: 0.0347
Time to train on one home:  164.19253706932068
trigger times: 0
Loss after 757306980 batches: 0.4082
trigger times: 0
Loss after 757438080 batches: 0.1698
trigger times: 1
Loss after 757569180 batches: 0.1145
trigger times: 0
Loss after 757700280 batches: 0.0932
trigger times: 1
Loss after 757831380 batches: 0.0773
trigger times: 0
Loss after 757962480 batches: 0.0696
trigger times: 1
Loss after 758093580 batches: 0.0637
trigger times: 2
Loss after 758224680 batches: 0.0591
trigger times: 3
Loss after 758355780 batches: 0.0553
trigger times: 4
Loss after 758486880 batches: 0.0538
trigger times: 5
Loss after 758617980 batches: 0.0501
trigger times: 6
Loss after 758749080 batches: 0.0470
trigger times: 0
Loss after 758880180 batches: 0.0461
trigger times: 1
Loss after 759011280 batches: 0.0456
trigger times: 2
Loss after 759142380 batches: 0.0445
trigger times: 3
Loss after 759273480 batches: 0.0428
trigger times: 4
Loss after 759404580 batches: 0.0411
trigger times: 5
Loss after 759535680 batches: 0.0404
trigger times: 0
Loss after 759666780 batches: 0.0391
trigger times: 1
Loss after 759797880 batches: 0.0385
trigger times: 0
Loss after 759928980 batches: 0.0386
trigger times: 1
Loss after 760060080 batches: 0.0374
trigger times: 2
Loss after 760191180 batches: 0.0374
trigger times: 3
Loss after 760322280 batches: 0.0362
trigger times: 4
Loss after 760453380 batches: 0.0353
trigger times: 5
Loss after 760584480 batches: 0.0354
trigger times: 6
Loss after 760715580 batches: 0.0336
trigger times: 7
Loss after 760846680 batches: 0.0347
trigger times: 8
Loss after 760977780 batches: 0.0329
trigger times: 9
Loss after 761108880 batches: 0.0329
trigger times: 10
Loss after 761239980 batches: 0.0327
trigger times: 11
Loss after 761371080 batches: 0.0325
trigger times: 0
Loss after 761502180 batches: 0.0322
trigger times: 1
Loss after 761633280 batches: 0.0311
trigger times: 2
Loss after 761764380 batches: 0.0307
trigger times: 3
Loss after 761895480 batches: 0.0301
trigger times: 4
Loss after 762026580 batches: 0.0308
trigger times: 5
Loss after 762157680 batches: 0.0304
trigger times: 6
Loss after 762288780 batches: 0.0300
trigger times: 0
Loss after 762419880 batches: 0.0292
trigger times: 1
Loss after 762550980 batches: 0.0306
trigger times: 2
Loss after 762682080 batches: 0.0288
trigger times: 0
Loss after 762813180 batches: 0.0293
trigger times: 1
Loss after 762944280 batches: 0.0293
trigger times: 2
Loss after 763075380 batches: 0.0287
trigger times: 3
Loss after 763206480 batches: 0.0284
trigger times: 4
Loss after 763337580 batches: 0.0277
trigger times: 5
Loss after 763468680 batches: 0.0279
trigger times: 6
Loss after 763599780 batches: 0.0273
trigger times: 7
Loss after 763730880 batches: 0.0271
trigger times: 0
Loss after 763861980 batches: 0.0273
trigger times: 1
Loss after 763993080 batches: 0.0271
trigger times: 2
Loss after 764124180 batches: 0.0272
trigger times: 3
Loss after 764255280 batches: 0.0264
trigger times: 4
Loss after 764386380 batches: 0.0263
trigger times: 5
Loss after 764517480 batches: 0.0263
trigger times: 0
Loss after 764648580 batches: 0.0268
trigger times: 1
Loss after 764779680 batches: 0.0262
trigger times: 0
Loss after 764910780 batches: 0.0262
trigger times: 1
Loss after 765041880 batches: 0.0252
trigger times: 2
Loss after 765172980 batches: 0.0249
trigger times: 3
Loss after 765304080 batches: 0.0261
trigger times: 4
Loss after 765435180 batches: 0.0253
trigger times: 5
Loss after 765566280 batches: 0.0253
trigger times: 6
Loss after 765697380 batches: 0.0255
trigger times: 7
Loss after 765828480 batches: 0.0252
trigger times: 8
Loss after 765959580 batches: 0.0250
trigger times: 9
Loss after 766090680 batches: 0.0245
trigger times: 10
Loss after 766221780 batches: 0.0240
trigger times: 0
Loss after 766352880 batches: 0.0251
trigger times: 1
Loss after 766483980 batches: 0.0244
trigger times: 2
Loss after 766615080 batches: 0.0235
trigger times: 0
Loss after 766746180 batches: 0.0236
trigger times: 1
Loss after 766877280 batches: 0.0236
trigger times: 2
Loss after 767008380 batches: 0.0233
trigger times: 3
Loss after 767139480 batches: 0.0244
trigger times: 4
Loss after 767270580 batches: 0.0234
trigger times: 5
Loss after 767401680 batches: 0.0240
trigger times: 6
Loss after 767532780 batches: 0.0227
trigger times: 7
Loss after 767663880 batches: 0.0230
trigger times: 8
Loss after 767794980 batches: 0.0235
trigger times: 9
Loss after 767926080 batches: 0.0230
trigger times: 10
Loss after 768057180 batches: 0.0226
trigger times: 11
Loss after 768188280 batches: 0.0236
trigger times: 12
Loss after 768319380 batches: 0.0223
trigger times: 13
Loss after 768450480 batches: 0.0225
trigger times: 14
Loss after 768581580 batches: 0.0224
trigger times: 15
Loss after 768712680 batches: 0.0223
trigger times: 16
Loss after 768843780 batches: 0.0222
trigger times: 17
Loss after 768974880 batches: 0.0221
trigger times: 18
Loss after 769105980 batches: 0.0215
trigger times: 19
Loss after 769237080 batches: 0.0218
trigger times: 20
Early stopping!
Start to test process.
Loss after 769368180 batches: 0.0224
Time to train on one home:  687.5420815944672
trigger times: 0
Loss after 769499280 batches: 0.5007
trigger times: 1
Loss after 769630380 batches: 0.2602
trigger times: 2
Loss after 769761480 batches: 0.1723
trigger times: 3
Loss after 769892580 batches: 0.1321
trigger times: 4
Loss after 770023680 batches: 0.1120
trigger times: 5
Loss after 770154780 batches: 0.1003
trigger times: 6
Loss after 770285880 batches: 0.0901
trigger times: 7
Loss after 770416980 batches: 0.0842
trigger times: 8
Loss after 770548080 batches: 0.0803
trigger times: 9
Loss after 770679180 batches: 0.0756
trigger times: 10
Loss after 770810280 batches: 0.0722
trigger times: 11
Loss after 770941380 batches: 0.0696
trigger times: 12
Loss after 771072480 batches: 0.0669
trigger times: 13
Loss after 771203580 batches: 0.0643
trigger times: 14
Loss after 771334680 batches: 0.0635
trigger times: 15
Loss after 771465780 batches: 0.0615
trigger times: 16
Loss after 771596880 batches: 0.0604
trigger times: 17
Loss after 771727980 batches: 0.0577
trigger times: 18
Loss after 771859080 batches: 0.0569
trigger times: 19
Loss after 771990180 batches: 0.0562
trigger times: 20
Early stopping!
Start to test process.
Loss after 772121280 batches: 0.0557
Time to train on one home:  165.09447026252747
trigger times: 0
Loss after 772252380 batches: 0.3676
trigger times: 1
Loss after 772383480 batches: 0.1432
trigger times: 2
Loss after 772514580 batches: 0.0994
trigger times: 3
Loss after 772645680 batches: 0.0790
trigger times: 0
Loss after 772776780 batches: 0.0682
trigger times: 1
Loss after 772907880 batches: 0.0606
trigger times: 0
Loss after 773038980 batches: 0.0564
trigger times: 1
Loss after 773170080 batches: 0.0538
trigger times: 2
Loss after 773301180 batches: 0.0493
trigger times: 0
Loss after 773432280 batches: 0.0479
trigger times: 1
Loss after 773563380 batches: 0.0446
trigger times: 2
Loss after 773694480 batches: 0.0432
trigger times: 0
Loss after 773825580 batches: 0.0420
trigger times: 1
Loss after 773956680 batches: 0.0409
trigger times: 2
Loss after 774087780 batches: 0.0396
trigger times: 3
Loss after 774218880 batches: 0.0390
trigger times: 4
Loss after 774349980 batches: 0.0385
trigger times: 0
Loss after 774481080 batches: 0.0371
trigger times: 1
Loss after 774612180 batches: 0.0364
trigger times: 2
Loss after 774743280 batches: 0.0350
trigger times: 0
Loss after 774874380 batches: 0.0348
trigger times: 1
Loss after 775005480 batches: 0.0345
trigger times: 0
Loss after 775136580 batches: 0.0339
trigger times: 1
Loss after 775267680 batches: 0.0332
trigger times: 2
Loss after 775398780 batches: 0.0335
trigger times: 3
Loss after 775529880 batches: 0.0325
trigger times: 4
Loss after 775660980 batches: 0.0322
trigger times: 5
Loss after 775792080 batches: 0.0324
trigger times: 0
Loss after 775923180 batches: 0.0304
trigger times: 1
Loss after 776054280 batches: 0.0309
trigger times: 2
Loss after 776185380 batches: 0.0311
trigger times: 3
Loss after 776316480 batches: 0.0310
trigger times: 4
Loss after 776447580 batches: 0.0300
trigger times: 5
Loss after 776578680 batches: 0.0299
trigger times: 6
Loss after 776709780 batches: 0.0290
trigger times: 7
Loss after 776840880 batches: 0.0290
trigger times: 8
Loss after 776971980 batches: 0.0281
trigger times: 9
Loss after 777103080 batches: 0.0288
trigger times: 10
Loss after 777234180 batches: 0.0276
trigger times: 11
Loss after 777365280 batches: 0.0272
trigger times: 12
Loss after 777496380 batches: 0.0275
trigger times: 13
Loss after 777627480 batches: 0.0276
trigger times: 14
Loss after 777758580 batches: 0.0275
trigger times: 15
Loss after 777889680 batches: 0.0275
trigger times: 16
Loss after 778020780 batches: 0.0268
trigger times: 17
Loss after 778151880 batches: 0.0268
trigger times: 18
Loss after 778282980 batches: 0.0274
trigger times: 19
Loss after 778414080 batches: 0.0257
trigger times: 20
Early stopping!
Start to test process.
Loss after 778545180 batches: 0.0257
Time to train on one home:  368.37016773223877
trigger times: 0
Loss after 778676280 batches: 0.5305
trigger times: 0
Loss after 778807380 batches: 0.3126
trigger times: 0
Loss after 778938480 batches: 0.2102
trigger times: 0
Loss after 779069580 batches: 0.1569
trigger times: 0
Loss after 779200680 batches: 0.1283
trigger times: 1
Loss after 779331780 batches: 0.1124
trigger times: 2
Loss after 779462880 batches: 0.1001
trigger times: 3
Loss after 779593980 batches: 0.0918
trigger times: 4
Loss after 779725080 batches: 0.0863
trigger times: 5
Loss after 779856180 batches: 0.0796
trigger times: 6
Loss after 779987280 batches: 0.0759
trigger times: 7
Loss after 780118380 batches: 0.0725
trigger times: 8
Loss after 780249480 batches: 0.0691
trigger times: 9
Loss after 780380580 batches: 0.0665
trigger times: 10
Loss after 780511680 batches: 0.0645
trigger times: 11
Loss after 780642780 batches: 0.0631
trigger times: 12
Loss after 780773880 batches: 0.0610
trigger times: 13
Loss after 780904980 batches: 0.0588
trigger times: 14
Loss after 781036080 batches: 0.0584
trigger times: 15
Loss after 781167180 batches: 0.0562
trigger times: 16
Loss after 781298280 batches: 0.0560
trigger times: 17
Loss after 781429380 batches: 0.0544
trigger times: 18
Loss after 781560480 batches: 0.0536
trigger times: 19
Loss after 781691580 batches: 0.0520
trigger times: 20
Early stopping!
Start to test process.
Loss after 781822680 batches: 0.0515
Time to train on one home:  193.38523530960083
trigger times: 0
Loss after 781953780 batches: 0.7676
trigger times: 1
Loss after 782084880 batches: 0.4569
trigger times: 2
Loss after 782215980 batches: 0.2548
trigger times: 3
Loss after 782347080 batches: 0.1861
trigger times: 4
Loss after 782478180 batches: 0.1486
trigger times: 5
Loss after 782609280 batches: 0.1336
trigger times: 6
Loss after 782740380 batches: 0.1204
trigger times: 7
Loss after 782871480 batches: 0.1099
trigger times: 8
Loss after 783002580 batches: 0.1039
trigger times: 9
Loss after 783133680 batches: 0.0995
trigger times: 10
Loss after 783264780 batches: 0.0942
trigger times: 11
Loss after 783395880 batches: 0.0909
trigger times: 12
Loss after 783526980 batches: 0.0862
trigger times: 13
Loss after 783658080 batches: 0.0846
trigger times: 14
Loss after 783789180 batches: 0.0815
trigger times: 15
Loss after 783920280 batches: 0.0782
trigger times: 16
Loss after 784051380 batches: 0.0768
trigger times: 17
Loss after 784182480 batches: 0.0758
trigger times: 18
Loss after 784313580 batches: 0.0728
trigger times: 19
Loss after 784444680 batches: 0.0724
trigger times: 20
Early stopping!
Start to test process.
Loss after 784575780 batches: 0.0700
Time to train on one home:  164.48784685134888
trigger times: 0
Loss after 784669740 batches: 0.6557
trigger times: 1
Loss after 784763700 batches: 0.3608
trigger times: 2
Loss after 784857660 batches: 0.2189
trigger times: 3
Loss after 784951620 batches: 0.1593
trigger times: 4
Loss after 785045580 batches: 0.1323
trigger times: 5
Loss after 785139540 batches: 0.1143
trigger times: 6
Loss after 785233500 batches: 0.1017
trigger times: 7
Loss after 785327460 batches: 0.0942
trigger times: 8
Loss after 785421420 batches: 0.0909
trigger times: 9
Loss after 785515380 batches: 0.0843
trigger times: 10
Loss after 785609340 batches: 0.0781
trigger times: 11
Loss after 785703300 batches: 0.0764
trigger times: 12
Loss after 785797260 batches: 0.0735
trigger times: 13
Loss after 785891220 batches: 0.0706
trigger times: 14
Loss after 785985180 batches: 0.0684
trigger times: 15
Loss after 786079140 batches: 0.0670
trigger times: 16
Loss after 786173100 batches: 0.0638
trigger times: 17
Loss after 786267060 batches: 0.0636
trigger times: 18
Loss after 786361020 batches: 0.0614
trigger times: 19
Loss after 786454980 batches: 0.0609
trigger times: 20
Early stopping!
Start to test process.
Loss after 786548940 batches: 0.0603
Time to train on one home:  126.37512397766113
trigger times: 0
Loss after 786680040 batches: 0.0790
trigger times: 0
Loss after 786811140 batches: 0.0210
trigger times: 1
Loss after 786942240 batches: 0.0147
trigger times: 0
Loss after 787073340 batches: 0.0126
trigger times: 0
Loss after 787204440 batches: 0.0109
trigger times: 0
Loss after 787335540 batches: 0.0099
trigger times: 1
Loss after 787466640 batches: 0.0089
trigger times: 0
Loss after 787597740 batches: 0.0084
trigger times: 1
Loss after 787728840 batches: 0.0080
trigger times: 2
Loss after 787859940 batches: 0.0076
trigger times: 3
Loss after 787991040 batches: 0.0070
trigger times: 4
Loss after 788122140 batches: 0.0067
trigger times: 5
Loss after 788253240 batches: 0.0066
trigger times: 6
Loss after 788384340 batches: 0.0064
trigger times: 7
Loss after 788515440 batches: 0.0059
trigger times: 0
Loss after 788646540 batches: 0.0058
trigger times: 0
Loss after 788777640 batches: 0.0056
trigger times: 1
Loss after 788908740 batches: 0.0055
trigger times: 2
Loss after 789039840 batches: 0.0053
trigger times: 3
Loss after 789170940 batches: 0.0052
trigger times: 4
Loss after 789302040 batches: 0.0050
trigger times: 5
Loss after 789433140 batches: 0.0050
trigger times: 6
Loss after 789564240 batches: 0.0049
trigger times: 7
Loss after 789695340 batches: 0.0048
trigger times: 8
Loss after 789826440 batches: 0.0046
trigger times: 9
Loss after 789957540 batches: 0.0045
trigger times: 10
Loss after 790088640 batches: 0.0045
trigger times: 11
Loss after 790219740 batches: 0.0044
trigger times: 12
Loss after 790350840 batches: 0.0043
trigger times: 13
Loss after 790481940 batches: 0.0045
trigger times: 0
Loss after 790613040 batches: 0.0043
trigger times: 1
Loss after 790744140 batches: 0.0042
trigger times: 0
Loss after 790875240 batches: 0.0042
trigger times: 1
Loss after 791006340 batches: 0.0040
trigger times: 2
Loss after 791137440 batches: 0.0040
trigger times: 3
Loss after 791268540 batches: 0.0040
trigger times: 4
Loss after 791399640 batches: 0.0039
trigger times: 5
Loss after 791530740 batches: 0.0039
trigger times: 6
Loss after 791661840 batches: 0.0037
trigger times: 7
Loss after 791792940 batches: 0.0037
trigger times: 8
Loss after 791924040 batches: 0.0037
trigger times: 9
Loss after 792055140 batches: 0.0036
trigger times: 10
Loss after 792186240 batches: 0.0036
trigger times: 11
Loss after 792317340 batches: 0.0037
trigger times: 12
Loss after 792448440 batches: 0.0036
trigger times: 13
Loss after 792579540 batches: 0.0036
trigger times: 0
Loss after 792710640 batches: 0.0035
trigger times: 0
Loss after 792841740 batches: 0.0034
trigger times: 1
Loss after 792972840 batches: 0.0035
trigger times: 2
Loss after 793103940 batches: 0.0033
trigger times: 3
Loss after 793235040 batches: 0.0034
trigger times: 4
Loss after 793366140 batches: 0.0033
trigger times: 5
Loss after 793497240 batches: 0.0034
trigger times: 6
Loss after 793628340 batches: 0.0033
trigger times: 7
Loss after 793759440 batches: 0.0034
trigger times: 8
Loss after 793890540 batches: 0.0032
trigger times: 9
Loss after 794021640 batches: 0.0032
trigger times: 10
Loss after 794152740 batches: 0.0031
trigger times: 11
Loss after 794283840 batches: 0.0031
trigger times: 12
Loss after 794414940 batches: 0.0030
trigger times: 13
Loss after 794546040 batches: 0.0031
trigger times: 14
Loss after 794677140 batches: 0.0030
trigger times: 15
Loss after 794808240 batches: 0.0030
trigger times: 16
Loss after 794939340 batches: 0.0030
trigger times: 17
Loss after 795070440 batches: 0.0030
trigger times: 18
Loss after 795201540 batches: 0.0030
trigger times: 19
Loss after 795332640 batches: 0.0029
trigger times: 20
Early stopping!
Start to test process.
Loss after 795463740 batches: 0.0029
Time to train on one home:  506.2906768321991
trigger times: 0
Loss after 795594840 batches: 0.2113
trigger times: 1
Loss after 795725940 batches: 0.0987
trigger times: 2
Loss after 795857040 batches: 0.0687
trigger times: 3
Loss after 795988140 batches: 0.0552
trigger times: 4
Loss after 796119240 batches: 0.0484
trigger times: 5
Loss after 796250340 batches: 0.0433
trigger times: 6
Loss after 796381440 batches: 0.0396
trigger times: 7
Loss after 796512540 batches: 0.0370
trigger times: 8
Loss after 796643640 batches: 0.0353
trigger times: 9
Loss after 796774740 batches: 0.0329
trigger times: 10
Loss after 796905840 batches: 0.0316
trigger times: 0
Loss after 797036940 batches: 0.0296
trigger times: 0
Loss after 797168040 batches: 0.0288
trigger times: 1
Loss after 797299140 batches: 0.0281
trigger times: 2
Loss after 797430240 batches: 0.0274
trigger times: 3
Loss after 797561340 batches: 0.0262
trigger times: 4
Loss after 797692440 batches: 0.0255
trigger times: 5
Loss after 797823540 batches: 0.0248
trigger times: 0
Loss after 797954640 batches: 0.0243
trigger times: 0
Loss after 798085740 batches: 0.0238
trigger times: 1
Loss after 798216840 batches: 0.0233
trigger times: 2
Loss after 798347940 batches: 0.0226
trigger times: 3
Loss after 798479040 batches: 0.0226
trigger times: 0
Loss after 798610140 batches: 0.0215
trigger times: 1
Loss after 798741240 batches: 0.0219
trigger times: 0
Loss after 798872340 batches: 0.0213
trigger times: 1
Loss after 799003440 batches: 0.0210
trigger times: 2
Loss after 799134540 batches: 0.0203
trigger times: 0
Loss after 799265640 batches: 0.0204
trigger times: 1
Loss after 799396740 batches: 0.0198
trigger times: 2
Loss after 799527840 batches: 0.0198
trigger times: 0
Loss after 799658940 batches: 0.0195
trigger times: 1
Loss after 799790040 batches: 0.0190
trigger times: 2
Loss after 799921140 batches: 0.0190
trigger times: 3
Loss after 800052240 batches: 0.0189
trigger times: 0
Loss after 800183340 batches: 0.0180
trigger times: 0
Loss after 800314440 batches: 0.0183
trigger times: 1
Loss after 800445540 batches: 0.0179
trigger times: 2
Loss after 800576640 batches: 0.0178
trigger times: 3
Loss after 800707740 batches: 0.0173
trigger times: 4
Loss after 800838840 batches: 0.0174
trigger times: 5
Loss after 800969940 batches: 0.0168
trigger times: 6
Loss after 801101040 batches: 0.0171
trigger times: 7
Loss after 801232140 batches: 0.0169
trigger times: 8
Loss after 801363240 batches: 0.0165
trigger times: 9
Loss after 801494340 batches: 0.0163
trigger times: 10
Loss after 801625440 batches: 0.0165
trigger times: 0
Loss after 801756540 batches: 0.0160
trigger times: 1
Loss after 801887640 batches: 0.0158
trigger times: 2
Loss after 802018740 batches: 0.0159
trigger times: 3
Loss after 802149840 batches: 0.0157
trigger times: 4
Loss after 802280940 batches: 0.0155
trigger times: 5
Loss after 802412040 batches: 0.0154
trigger times: 6
Loss after 802543140 batches: 0.0152
trigger times: 0
Loss after 802674240 batches: 0.0152
trigger times: 1
Loss after 802805340 batches: 0.0150
trigger times: 2
Loss after 802936440 batches: 0.0147
trigger times: 3
Loss after 803067540 batches: 0.0147
trigger times: 4
Loss after 803198640 batches: 0.0146
trigger times: 5
Loss after 803329740 batches: 0.0143
trigger times: 6
Loss after 803460840 batches: 0.0145
trigger times: 7
Loss after 803591940 batches: 0.0142
trigger times: 8
Loss after 803723040 batches: 0.0143
trigger times: 9
Loss after 803854140 batches: 0.0144
trigger times: 0
Loss after 803985240 batches: 0.0142
trigger times: 1
Loss after 804116340 batches: 0.0141
trigger times: 0
Loss after 804247440 batches: 0.0140
trigger times: 1
Loss after 804378540 batches: 0.0139
trigger times: 2
Loss after 804509640 batches: 0.0138
trigger times: 3
Loss after 804640740 batches: 0.0138
trigger times: 4
Loss after 804771840 batches: 0.0132
trigger times: 5
Loss after 804902940 batches: 0.0133
trigger times: 6
Loss after 805034040 batches: 0.0129
trigger times: 7
Loss after 805165140 batches: 0.0135
trigger times: 8
Loss after 805296240 batches: 0.0132
trigger times: 9
Loss after 805427340 batches: 0.0130
trigger times: 10
Loss after 805558440 batches: 0.0129
trigger times: 11
Loss after 805689540 batches: 0.0127
trigger times: 12
Loss after 805820640 batches: 0.0127
trigger times: 13
Loss after 805951740 batches: 0.0127
trigger times: 14
Loss after 806082840 batches: 0.0128
trigger times: 15
Loss after 806213940 batches: 0.0126
trigger times: 16
Loss after 806345040 batches: 0.0125
trigger times: 17
Loss after 806476140 batches: 0.0125
trigger times: 18
Loss after 806607240 batches: 0.0123
trigger times: 19
Loss after 806738340 batches: 0.0124
trigger times: 20
Early stopping!
Start to test process.
Loss after 806869440 batches: 0.0123
Time to train on one home:  642.7825863361359
trigger times: 0
Loss after 807000540 batches: 0.4527
trigger times: 1
Loss after 807131640 batches: 0.2177
trigger times: 0
Loss after 807262740 batches: 0.1529
trigger times: 1
Loss after 807393840 batches: 0.1263
trigger times: 2
Loss after 807524940 batches: 0.1107
trigger times: 3
Loss after 807656040 batches: 0.1014
trigger times: 0
Loss after 807787140 batches: 0.0941
trigger times: 0
Loss after 807918240 batches: 0.0896
trigger times: 1
Loss after 808049340 batches: 0.0859
trigger times: 2
Loss after 808180440 batches: 0.0815
trigger times: 0
Loss after 808311540 batches: 0.0789
trigger times: 0
Loss after 808442640 batches: 0.0764
trigger times: 0
Loss after 808573740 batches: 0.0737
trigger times: 0
Loss after 808704840 batches: 0.0723
trigger times: 1
Loss after 808835940 batches: 0.0711
trigger times: 2
Loss after 808967040 batches: 0.0696
trigger times: 0
Loss after 809098140 batches: 0.0677
trigger times: 0
Loss after 809229240 batches: 0.0674
trigger times: 1
Loss after 809360340 batches: 0.0663
trigger times: 0
Loss after 809491440 batches: 0.0647
trigger times: 1
Loss after 809622540 batches: 0.0628
trigger times: 2
Loss after 809753640 batches: 0.0622
trigger times: 3
Loss after 809884740 batches: 0.0614
trigger times: 0
Loss after 810015840 batches: 0.0603
trigger times: 1
Loss after 810146940 batches: 0.0609
trigger times: 2
Loss after 810278040 batches: 0.0593
trigger times: 0
Loss after 810409140 batches: 0.0591
trigger times: 1
Loss after 810540240 batches: 0.0581
trigger times: 0
Loss after 810671340 batches: 0.0568
trigger times: 0
Loss after 810802440 batches: 0.0576
trigger times: 0
Loss after 810933540 batches: 0.0567
trigger times: 1
Loss after 811064640 batches: 0.0566
trigger times: 2
Loss after 811195740 batches: 0.0556
trigger times: 0
Loss after 811326840 batches: 0.0547
trigger times: 1
Loss after 811457940 batches: 0.0556
trigger times: 2
Loss after 811589040 batches: 0.0544
trigger times: 0
Loss after 811720140 batches: 0.0534
trigger times: 1
Loss after 811851240 batches: 0.0543
trigger times: 0
Loss after 811982340 batches: 0.0531
trigger times: 0
Loss after 812113440 batches: 0.0525
trigger times: 0
Loss after 812244540 batches: 0.0525
trigger times: 1
Loss after 812375640 batches: 0.0529
trigger times: 2
Loss after 812506740 batches: 0.0519
trigger times: 0
Loss after 812637840 batches: 0.0514
trigger times: 1
Loss after 812768940 batches: 0.0511
trigger times: 2
Loss after 812900040 batches: 0.0512
trigger times: 0
Loss after 813031140 batches: 0.0505
trigger times: 1
Loss after 813162240 batches: 0.0501
trigger times: 2
Loss after 813293340 batches: 0.0501
trigger times: 3
Loss after 813424440 batches: 0.0498
trigger times: 4
Loss after 813555540 batches: 0.0489
trigger times: 0
Loss after 813686640 batches: 0.0490
trigger times: 1
Loss after 813817740 batches: 0.0492
trigger times: 2
Loss after 813948840 batches: 0.0490
trigger times: 3
Loss after 814079940 batches: 0.0493
trigger times: 4
Loss after 814211040 batches: 0.0476
trigger times: 5
Loss after 814342140 batches: 0.0482
trigger times: 6
Loss after 814473240 batches: 0.0481
trigger times: 7
Loss after 814604340 batches: 0.0473
trigger times: 8
Loss after 814735440 batches: 0.0476
trigger times: 9
Loss after 814866540 batches: 0.0467
trigger times: 10
Loss after 814997640 batches: 0.0470
trigger times: 11
Loss after 815128740 batches: 0.0477
trigger times: 12
Loss after 815259840 batches: 0.0462
trigger times: 13
Loss after 815390940 batches: 0.0464
trigger times: 0
Loss after 815522040 batches: 0.0467
trigger times: 1
Loss after 815653140 batches: 0.0464
trigger times: 2
Loss after 815784240 batches: 0.0460
trigger times: 3
Loss after 815915340 batches: 0.0455
trigger times: 4
Loss after 816046440 batches: 0.0454
trigger times: 5
Loss after 816177540 batches: 0.0449
trigger times: 6
Loss after 816308640 batches: 0.0456
trigger times: 0
Loss after 816439740 batches: 0.0452
trigger times: 1
Loss after 816570840 batches: 0.0447
trigger times: 2
Loss after 816701940 batches: 0.0442
trigger times: 3
Loss after 816833040 batches: 0.0445
trigger times: 4
Loss after 816964140 batches: 0.0445
trigger times: 5
Loss after 817095240 batches: 0.0437
trigger times: 6
Loss after 817226340 batches: 0.0436
trigger times: 7
Loss after 817357440 batches: 0.0437
trigger times: 8
Loss after 817488540 batches: 0.0437
trigger times: 9
Loss after 817619640 batches: 0.0438
trigger times: 10
Loss after 817750740 batches: 0.0436
trigger times: 11
Loss after 817881840 batches: 0.0435
trigger times: 12
Loss after 818012940 batches: 0.0430
trigger times: 13
Loss after 818144040 batches: 0.0432
trigger times: 0
Loss after 818275140 batches: 0.0429
trigger times: 1
Loss after 818406240 batches: 0.0428
trigger times: 2
Loss after 818537340 batches: 0.0425
trigger times: 3
Loss after 818668440 batches: 0.0430
trigger times: 4
Loss after 818799540 batches: 0.0424
trigger times: 5
Loss after 818930640 batches: 0.0421
trigger times: 6
Loss after 819061740 batches: 0.0425
trigger times: 7
Loss after 819192840 batches: 0.0422
trigger times: 8
Loss after 819323940 batches: 0.0416
trigger times: 0
Loss after 819455040 batches: 0.0422
trigger times: 1
Loss after 819586140 batches: 0.0423
trigger times: 2
Loss after 819717240 batches: 0.0420
trigger times: 3
Loss after 819848340 batches: 0.0417
trigger times: 4
Loss after 819979440 batches: 0.0417
trigger times: 5
Loss after 820110540 batches: 0.0413
trigger times: 6
Loss after 820241640 batches: 0.0417
trigger times: 7
Loss after 820372740 batches: 0.0422
trigger times: 8
Loss after 820503840 batches: 0.0413
trigger times: 9
Loss after 820634940 batches: 0.0412
trigger times: 0
Loss after 820766040 batches: 0.0413
trigger times: 1
Loss after 820897140 batches: 0.0415
trigger times: 2
Loss after 821028240 batches: 0.0408
trigger times: 3
Loss after 821159340 batches: 0.0408
trigger times: 4
Loss after 821290440 batches: 0.0406
trigger times: 5
Loss after 821421540 batches: 0.0408
trigger times: 6
Loss after 821552640 batches: 0.0404
trigger times: 7
Loss after 821683740 batches: 0.0402
trigger times: 8
Loss after 821814840 batches: 0.0409
trigger times: 9
Loss after 821945940 batches: 0.0402
trigger times: 10
Loss after 822077040 batches: 0.0406
trigger times: 0
Loss after 822208140 batches: 0.0397
trigger times: 1
Loss after 822339240 batches: 0.0398
trigger times: 2
Loss after 822470340 batches: 0.0403
trigger times: 0
Loss after 822601440 batches: 0.0400
trigger times: 1
Loss after 822732540 batches: 0.0395
trigger times: 2
Loss after 822863640 batches: 0.0395
trigger times: 3
Loss after 822994740 batches: 0.0393
trigger times: 4
Loss after 823125840 batches: 0.0392
trigger times: 5
Loss after 823256940 batches: 0.0393
trigger times: 6
Loss after 823388040 batches: 0.0397
trigger times: 7
Loss after 823519140 batches: 0.0390
trigger times: 8
Loss after 823650240 batches: 0.0390
trigger times: 9
Loss after 823781340 batches: 0.0388
trigger times: 10
Loss after 823912440 batches: 0.0388
trigger times: 11
Loss after 824043540 batches: 0.0393
trigger times: 12
Loss after 824174640 batches: 0.0383
trigger times: 13
Loss after 824305740 batches: 0.0386
trigger times: 14
Loss after 824436840 batches: 0.0383
trigger times: 15
Loss after 824567940 batches: 0.0386
trigger times: 16
Loss after 824699040 batches: 0.0380
trigger times: 17
Loss after 824830140 batches: 0.0386
trigger times: 18
Loss after 824961240 batches: 0.0379
trigger times: 0
Loss after 825092340 batches: 0.0384
trigger times: 1
Loss after 825223440 batches: 0.0384
trigger times: 2
Loss after 825354540 batches: 0.0387
trigger times: 3
Loss after 825485640 batches: 0.0382
trigger times: 4
Loss after 825616740 batches: 0.0383
trigger times: 5
Loss after 825747840 batches: 0.0381
trigger times: 6
Loss after 825878940 batches: 0.0379
trigger times: 7
Loss after 826010040 batches: 0.0377
trigger times: 8
Loss after 826141140 batches: 0.0377
trigger times: 9
Loss after 826272240 batches: 0.0377
trigger times: 10
Loss after 826403340 batches: 0.0379
trigger times: 11
Loss after 826534440 batches: 0.0375
trigger times: 12
Loss after 826665540 batches: 0.0376
trigger times: 13
Loss after 826796640 batches: 0.0372
trigger times: 14
Loss after 826927740 batches: 0.0374
trigger times: 15
Loss after 827058840 batches: 0.0380
trigger times: 16
Loss after 827189940 batches: 0.0371
trigger times: 17
Loss after 827321040 batches: 0.0372
trigger times: 18
Loss after 827452140 batches: 0.0372
trigger times: 19
Loss after 827583240 batches: 0.0374
trigger times: 20
Early stopping!
Start to test process.
Loss after 827714340 batches: 0.0376
Time to train on one home:  1164.7312443256378
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217]]
Round_7_results:  [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217]
trigger times: 0
Loss after 827816940 batches: 0.4311
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 6560 < 6561; dropping {'Training_Loss': 0.43111489500318256, 'Validation_Loss': 0.7583757440249125, 'Training_R2': 0.5610186110675679, 'Validation_R2': 0.2935873697425432, 'Training_F1': 0.7416986588824567, 'Validation_F1': 0.392725055454098, 'Training_NEP': 0.51713075535554, 'Validation_NEP': 1.1220994058665363, 'Training_NDE': 0.2956935921343978, 'Validation_NDE': 0.5625497808242608, 'Training_MAE': 33.940239026551126, 'Validation_MAE': 30.77262101412674, 'Training_MSE': 3902.1853, 'Validation_MSE': 2077.4807}.
trigger times: 1
Loss after 827919540 batches: 0.2009
trigger times: 2
Loss after 828022140 batches: 0.1410
trigger times: 3
Loss after 828124740 batches: 0.1038
trigger times: 4
Loss after 828227340 batches: 0.0896
trigger times: 5
Loss after 828329940 batches: 0.0810
trigger times: 6
Loss after 828432540 batches: 0.0774
trigger times: 7
Loss after 828535140 batches: 0.0666
trigger times: 8
Loss after 828637740 batches: 0.0627
trigger times: 9
Loss after 828740340 batches: 0.0605
trigger times: 10
Loss after 828842940 batches: 0.0561
trigger times: 11
Loss after 828945540 batches: 0.0542
trigger times: 12
Loss after 829048140 batches: 0.0497
trigger times: 13
Loss after 829150740 batches: 0.0491
trigger times: 14
Loss after 829253340 batches: 0.0486
trigger times: 15
Loss after 829355940 batches: 0.0464
trigger times: 16
Loss after 829458540 batches: 0.0453
trigger times: 17
Loss after 829561140 batches: 0.0450
trigger times: 18
Loss after 829663740 batches: 0.0447
trigger times: 19
Loss after 829766340 batches: 0.0439
trigger times: 20
Early stopping!
Start to test process.
Loss after 829868940 batches: 0.0475
Time to train on one home:  136.07660722732544
trigger times: 0
Loss after 830000040 batches: 0.3448
trigger times: 0
Loss after 830131140 batches: 0.2035
trigger times: 0
Loss after 830262240 batches: 0.1478
trigger times: 1
Loss after 830393340 batches: 0.1177
trigger times: 2
Loss after 830524440 batches: 0.1008
trigger times: 3
Loss after 830655540 batches: 0.0882
trigger times: 4
Loss after 830786640 batches: 0.0787
trigger times: 5
Loss after 830917740 batches: 0.0724
trigger times: 6
Loss after 831048840 batches: 0.0676
trigger times: 7
Loss after 831179940 batches: 0.0629
trigger times: 8
Loss after 831311040 batches: 0.0604
trigger times: 9
Loss after 831442140 batches: 0.0575
trigger times: 10
Loss after 831573240 batches: 0.0546
trigger times: 11
Loss after 831704340 batches: 0.0527
trigger times: 12
Loss after 831835440 batches: 0.0512
trigger times: 13
Loss after 831966540 batches: 0.0493
trigger times: 14
Loss after 832097640 batches: 0.0478
trigger times: 15
Loss after 832228740 batches: 0.0469
trigger times: 16
Loss after 832359840 batches: 0.0457
trigger times: 17
Loss after 832490940 batches: 0.0445
trigger times: 18
Loss after 832622040 batches: 0.0439
trigger times: 19
Loss after 832753140 batches: 0.0428
trigger times: 20
Early stopping!
Start to test process.
Loss after 832884240 batches: 0.0411
Time to train on one home:  179.3377661705017
trigger times: 0
Loss after 833015340 batches: 0.5609
trigger times: 1
Loss after 833146440 batches: 0.2697
trigger times: 2
Loss after 833277540 batches: 0.1709
trigger times: 3
Loss after 833408640 batches: 0.1326
trigger times: 4
Loss after 833539740 batches: 0.1129
trigger times: 5
Loss after 833670840 batches: 0.1001
trigger times: 6
Loss after 833801940 batches: 0.0936
trigger times: 7
Loss after 833933040 batches: 0.0863
trigger times: 8
Loss after 834064140 batches: 0.0811
trigger times: 9
Loss after 834195240 batches: 0.0771
trigger times: 10
Loss after 834326340 batches: 0.0746
trigger times: 11
Loss after 834457440 batches: 0.0712
trigger times: 12
Loss after 834588540 batches: 0.0698
trigger times: 13
Loss after 834719640 batches: 0.0664
trigger times: 14
Loss after 834850740 batches: 0.0638
trigger times: 15
Loss after 834981840 batches: 0.0631
trigger times: 16
Loss after 835112940 batches: 0.0619
trigger times: 17
Loss after 835244040 batches: 0.0600
trigger times: 18
Loss after 835375140 batches: 0.0591
trigger times: 19
Loss after 835506240 batches: 0.0575
trigger times: 20
Early stopping!
Start to test process.
Loss after 835637340 batches: 0.0566
Time to train on one home:  165.25714707374573
trigger times: 0
Loss after 835765980 batches: 0.2168
trigger times: 0
Loss after 835894620 batches: 0.0883
trigger times: 0
Loss after 836023260 batches: 0.0622
trigger times: 0
Loss after 836151900 batches: 0.0525
trigger times: 1
Loss after 836280540 batches: 0.0465
trigger times: 2
Loss after 836409180 batches: 0.0427
trigger times: 3
Loss after 836537820 batches: 0.0392
trigger times: 4
Loss after 836666460 batches: 0.0369
trigger times: 5
Loss after 836795100 batches: 0.0358
trigger times: 6
Loss after 836923740 batches: 0.0341
trigger times: 7
Loss after 837052380 batches: 0.0336
trigger times: 8
Loss after 837181020 batches: 0.0316
trigger times: 9
Loss after 837309660 batches: 0.0305
trigger times: 10
Loss after 837438300 batches: 0.0291
trigger times: 11
Loss after 837566940 batches: 0.0283
trigger times: 12
Loss after 837695580 batches: 0.0274
trigger times: 13
Loss after 837824220 batches: 0.0278
trigger times: 14
Loss after 837952860 batches: 0.0274
trigger times: 15
Loss after 838081500 batches: 0.0262
trigger times: 16
Loss after 838210140 batches: 0.0254
trigger times: 17
Loss after 838338780 batches: 0.0252
trigger times: 18
Loss after 838467420 batches: 0.0249
trigger times: 0
Loss after 838596060 batches: 0.0244
trigger times: 1
Loss after 838724700 batches: 0.0242
trigger times: 2
Loss after 838853340 batches: 0.0234
trigger times: 3
Loss after 838981980 batches: 0.0230
trigger times: 4
Loss after 839110620 batches: 0.0235
trigger times: 5
Loss after 839239260 batches: 0.0231
trigger times: 0
Loss after 839367900 batches: 0.0226
trigger times: 1
Loss after 839496540 batches: 0.0225
trigger times: 2
Loss after 839625180 batches: 0.0218
trigger times: 3
Loss after 839753820 batches: 0.0216
trigger times: 4
Loss after 839882460 batches: 0.0218
trigger times: 5
Loss after 840011100 batches: 0.0214
trigger times: 6
Loss after 840139740 batches: 0.0212
trigger times: 7
Loss after 840268380 batches: 0.0207
trigger times: 0
Loss after 840397020 batches: 0.0209
trigger times: 0
Loss after 840525660 batches: 0.0203
trigger times: 0
Loss after 840654300 batches: 0.0202
trigger times: 1
Loss after 840782940 batches: 0.0202
trigger times: 2
Loss after 840911580 batches: 0.0199
trigger times: 3
Loss after 841040220 batches: 0.0199
trigger times: 4
Loss after 841168860 batches: 0.0201
trigger times: 5
Loss after 841297500 batches: 0.0198
trigger times: 6
Loss after 841426140 batches: 0.0194
trigger times: 7
Loss after 841554780 batches: 0.0193
trigger times: 8
Loss after 841683420 batches: 0.0188
trigger times: 9
Loss after 841812060 batches: 0.0192
trigger times: 10
Loss after 841940700 batches: 0.0187
trigger times: 11
Loss after 842069340 batches: 0.0186
trigger times: 12
Loss after 842197980 batches: 0.0181
trigger times: 13
Loss after 842326620 batches: 0.0186
trigger times: 14
Loss after 842455260 batches: 0.0184
trigger times: 15
Loss after 842583900 batches: 0.0180
trigger times: 16
Loss after 842712540 batches: 0.0179
trigger times: 17
Loss after 842841180 batches: 0.0176
trigger times: 0
Loss after 842969820 batches: 0.0174
trigger times: 1
Loss after 843098460 batches: 0.0174
trigger times: 2
Loss after 843227100 batches: 0.0175
trigger times: 3
Loss after 843355740 batches: 0.0173
trigger times: 0
Loss after 843484380 batches: 0.0177
trigger times: 1
Loss after 843613020 batches: 0.0176
trigger times: 2
Loss after 843741660 batches: 0.0173
trigger times: 3
Loss after 843870300 batches: 0.0169
trigger times: 4
Loss after 843998940 batches: 0.0171
trigger times: 5
Loss after 844127580 batches: 0.0167
trigger times: 6
Loss after 844256220 batches: 0.0168
trigger times: 0
Loss after 844384860 batches: 0.0168
trigger times: 1
Loss after 844513500 batches: 0.0164
trigger times: 2
Loss after 844642140 batches: 0.0165
trigger times: 3
Loss after 844770780 batches: 0.0163
trigger times: 4
Loss after 844899420 batches: 0.0169
trigger times: 5
Loss after 845028060 batches: 0.0162
trigger times: 0
Loss after 845156700 batches: 0.0160
trigger times: 1
Loss after 845285340 batches: 0.0158
trigger times: 0
Loss after 845413980 batches: 0.0162
trigger times: 0
Loss after 845542620 batches: 0.0154
trigger times: 1
Loss after 845671260 batches: 0.0159
trigger times: 2
Loss after 845799900 batches: 0.0156
trigger times: 3
Loss after 845928540 batches: 0.0157
trigger times: 4
Loss after 846057180 batches: 0.0154
trigger times: 5
Loss after 846185820 batches: 0.0158
trigger times: 6
Loss after 846314460 batches: 0.0154
trigger times: 7
Loss after 846443100 batches: 0.0151
trigger times: 8
Loss after 846571740 batches: 0.0159
trigger times: 9
Loss after 846700380 batches: 0.0154
trigger times: 10
Loss after 846829020 batches: 0.0157
trigger times: 0
Loss after 846957660 batches: 0.0152
trigger times: 1
Loss after 847086300 batches: 0.0151
trigger times: 2
Loss after 847214940 batches: 0.0150
trigger times: 3
Loss after 847343580 batches: 0.0151
trigger times: 4
Loss after 847472220 batches: 0.0153
trigger times: 5
Loss after 847600860 batches: 0.0153
trigger times: 6
Loss after 847729500 batches: 0.0151
trigger times: 7
Loss after 847858140 batches: 0.0147
trigger times: 8
Loss after 847986780 batches: 0.0151
trigger times: 9
Loss after 848115420 batches: 0.0148
trigger times: 10
Loss after 848244060 batches: 0.0144
trigger times: 11
Loss after 848372700 batches: 0.0145
trigger times: 12
Loss after 848501340 batches: 0.0145
trigger times: 13
Loss after 848629980 batches: 0.0143
trigger times: 14
Loss after 848758620 batches: 0.0149
trigger times: 15
Loss after 848887260 batches: 0.0147
trigger times: 16
Loss after 849015900 batches: 0.0146
trigger times: 17
Loss after 849144540 batches: 0.0145
trigger times: 18
Loss after 849273180 batches: 0.0144
trigger times: 19
Loss after 849401820 batches: 0.0148
trigger times: 20
Early stopping!
Start to test process.
Loss after 849530460 batches: 0.0142
Time to train on one home:  784.790233373642
trigger times: 0
Loss after 849661560 batches: 0.6298
trigger times: 1
Loss after 849792660 batches: 0.3071
trigger times: 2
Loss after 849923760 batches: 0.1802
trigger times: 3
Loss after 850054860 batches: 0.1362
trigger times: 4
Loss after 850185960 batches: 0.1133
trigger times: 5
Loss after 850317060 batches: 0.1002
trigger times: 6
Loss after 850448160 batches: 0.0921
trigger times: 7
Loss after 850579260 batches: 0.0849
trigger times: 8
Loss after 850710360 batches: 0.0798
trigger times: 9
Loss after 850841460 batches: 0.0762
trigger times: 10
Loss after 850972560 batches: 0.0728
trigger times: 11
Loss after 851103660 batches: 0.0700
trigger times: 12
Loss after 851234760 batches: 0.0671
trigger times: 13
Loss after 851365860 batches: 0.0656
trigger times: 14
Loss after 851496960 batches: 0.0635
trigger times: 15
Loss after 851628060 batches: 0.0616
trigger times: 16
Loss after 851759160 batches: 0.0611
trigger times: 17
Loss after 851890260 batches: 0.0596
trigger times: 18
Loss after 852021360 batches: 0.0580
trigger times: 19
Loss after 852152460 batches: 0.0566
trigger times: 20
Early stopping!
Start to test process.
Loss after 852283560 batches: 0.0556
Time to train on one home:  164.17191195487976
trigger times: 0
Loss after 852414660 batches: 0.6368
trigger times: 1
Loss after 852545760 batches: 0.3682
trigger times: 2
Loss after 852676860 batches: 0.2383
trigger times: 3
Loss after 852807960 batches: 0.1797
trigger times: 4
Loss after 852939060 batches: 0.1387
trigger times: 5
Loss after 853070160 batches: 0.1267
trigger times: 6
Loss after 853201260 batches: 0.1110
trigger times: 7
Loss after 853332360 batches: 0.1030
trigger times: 8
Loss after 853463460 batches: 0.0956
trigger times: 9
Loss after 853594560 batches: 0.0905
trigger times: 10
Loss after 853725660 batches: 0.0822
trigger times: 11
Loss after 853856760 batches: 0.0800
trigger times: 12
Loss after 853987860 batches: 0.0756
trigger times: 13
Loss after 854118960 batches: 0.0756
trigger times: 14
Loss after 854250060 batches: 0.0736
trigger times: 15
Loss after 854381160 batches: 0.0692
trigger times: 16
Loss after 854512260 batches: 0.0672
trigger times: 17
Loss after 854643360 batches: 0.0662
trigger times: 18
Loss after 854774460 batches: 0.0646
trigger times: 19
Loss after 854905560 batches: 0.0646
trigger times: 20
Early stopping!
Start to test process.
Loss after 855036660 batches: 0.0606
Time to train on one home:  164.72925472259521
trigger times: 0
Loss after 855167760 batches: 0.1610
trigger times: 0
Loss after 855298860 batches: 0.0534
trigger times: 1
Loss after 855429960 batches: 0.0371
trigger times: 2
Loss after 855561060 batches: 0.0310
trigger times: 3
Loss after 855692160 batches: 0.0280
trigger times: 4
Loss after 855823260 batches: 0.0260
trigger times: 5
Loss after 855954360 batches: 0.0242
trigger times: 6
Loss after 856085460 batches: 0.0227
trigger times: 7
Loss after 856216560 batches: 0.0219
trigger times: 8
Loss after 856347660 batches: 0.0211
trigger times: 9
Loss after 856478760 batches: 0.0208
trigger times: 10
Loss after 856609860 batches: 0.0197
trigger times: 11
Loss after 856740960 batches: 0.0192
trigger times: 0
Loss after 856872060 batches: 0.0190
trigger times: 0
Loss after 857003160 batches: 0.0184
trigger times: 0
Loss after 857134260 batches: 0.0178
trigger times: 0
Loss after 857265360 batches: 0.0178
trigger times: 1
Loss after 857396460 batches: 0.0175
trigger times: 2
Loss after 857527560 batches: 0.0173
trigger times: 3
Loss after 857658660 batches: 0.0167
trigger times: 4
Loss after 857789760 batches: 0.0166
trigger times: 0
Loss after 857920860 batches: 0.0164
trigger times: 1
Loss after 858051960 batches: 0.0162
trigger times: 2
Loss after 858183060 batches: 0.0159
trigger times: 3
Loss after 858314160 batches: 0.0157
trigger times: 4
Loss after 858445260 batches: 0.0157
trigger times: 5
Loss after 858576360 batches: 0.0154
trigger times: 6
Loss after 858707460 batches: 0.0152
trigger times: 7
Loss after 858838560 batches: 0.0148
trigger times: 8
Loss after 858969660 batches: 0.0146
trigger times: 0
Loss after 859100760 batches: 0.0148
trigger times: 1
Loss after 859231860 batches: 0.0145
trigger times: 2
Loss after 859362960 batches: 0.0146
trigger times: 3
Loss after 859494060 batches: 0.0146
trigger times: 4
Loss after 859625160 batches: 0.0146
trigger times: 5
Loss after 859756260 batches: 0.0143
trigger times: 6
Loss after 859887360 batches: 0.0138
trigger times: 7
Loss after 860018460 batches: 0.0141
trigger times: 8
Loss after 860149560 batches: 0.0139
trigger times: 9
Loss after 860280660 batches: 0.0138
trigger times: 10
Loss after 860411760 batches: 0.0137
trigger times: 11
Loss after 860542860 batches: 0.0133
trigger times: 12
Loss after 860673960 batches: 0.0131
trigger times: 13
Loss after 860805060 batches: 0.0131
trigger times: 14
Loss after 860936160 batches: 0.0129
trigger times: 15
Loss after 861067260 batches: 0.0128
trigger times: 16
Loss after 861198360 batches: 0.0125
trigger times: 17
Loss after 861329460 batches: 0.0128
trigger times: 18
Loss after 861460560 batches: 0.0125
trigger times: 19
Loss after 861591660 batches: 0.0125
trigger times: 20
Early stopping!
Start to test process.
Loss after 861722760 batches: 0.0127
Time to train on one home:  382.5261445045471
trigger times: 0
Loss after 861853860 batches: 0.2134
trigger times: 0
Loss after 861984960 batches: 0.0718
trigger times: 0
Loss after 862116060 batches: 0.0504
trigger times: 1
Loss after 862247160 batches: 0.0415
trigger times: 0
Loss after 862378260 batches: 0.0358
trigger times: 1
Loss after 862509360 batches: 0.0335
trigger times: 2
Loss after 862640460 batches: 0.0309
trigger times: 3
Loss after 862771560 batches: 0.0288
trigger times: 4
Loss after 862902660 batches: 0.0274
trigger times: 0
Loss after 863033760 batches: 0.0262
trigger times: 1
Loss after 863164860 batches: 0.0256
trigger times: 2
Loss after 863295960 batches: 0.0246
trigger times: 3
Loss after 863427060 batches: 0.0236
trigger times: 4
Loss after 863558160 batches: 0.0233
trigger times: 5
Loss after 863689260 batches: 0.0228
trigger times: 6
Loss after 863820360 batches: 0.0219
trigger times: 7
Loss after 863951460 batches: 0.0214
trigger times: 8
Loss after 864082560 batches: 0.0208
trigger times: 9
Loss after 864213660 batches: 0.0208
trigger times: 10
Loss after 864344760 batches: 0.0204
trigger times: 11
Loss after 864475860 batches: 0.0199
trigger times: 12
Loss after 864606960 batches: 0.0196
trigger times: 13
Loss after 864738060 batches: 0.0192
trigger times: 14
Loss after 864869160 batches: 0.0188
trigger times: 15
Loss after 865000260 batches: 0.0187
trigger times: 16
Loss after 865131360 batches: 0.0186
trigger times: 17
Loss after 865262460 batches: 0.0181
trigger times: 18
Loss after 865393560 batches: 0.0177
trigger times: 19
Loss after 865524660 batches: 0.0176
trigger times: 20
Early stopping!
Start to test process.
Loss after 865655760 batches: 0.0176
Time to train on one home:  230.86213374137878
trigger times: 0
Loss after 865734360 batches: 0.5239
trigger times: 1
Loss after 865812960 batches: 0.2196
trigger times: 2
Loss after 865891560 batches: 0.1270
trigger times: 3
Loss after 865970160 batches: 0.0913
trigger times: 4
Loss after 866048760 batches: 0.0758
trigger times: 5
Loss after 866127360 batches: 0.0673
trigger times: 6
Loss after 866205960 batches: 0.0617
trigger times: 7
Loss after 866284560 batches: 0.0574
trigger times: 8
Loss after 866363160 batches: 0.0524
trigger times: 9
Loss after 866441760 batches: 0.0491
trigger times: 10
Loss after 866520360 batches: 0.0478
trigger times: 11
Loss after 866598960 batches: 0.0468
trigger times: 12
Loss after 866677560 batches: 0.0440
trigger times: 13
Loss after 866756160 batches: 0.0441
trigger times: 14
Loss after 866834760 batches: 0.0413
trigger times: 15
Loss after 866913360 batches: 0.0402
trigger times: 16
Loss after 866991960 batches: 0.0400
trigger times: 17
Loss after 867070560 batches: 0.0392
trigger times: 18
Loss after 867149160 batches: 0.0388
trigger times: 19
Loss after 867227760 batches: 0.0373
trigger times: 20
Early stopping!
Start to test process.
Loss after 867306360 batches: 0.0366
Time to train on one home:  110.49814963340759
trigger times: 0
Loss after 867437460 batches: 0.1810
trigger times: 1
Loss after 867568560 batches: 0.0630
trigger times: 0
Loss after 867699660 batches: 0.0428
trigger times: 1
Loss after 867830760 batches: 0.0365
trigger times: 2
Loss after 867961860 batches: 0.0325
trigger times: 3
Loss after 868092960 batches: 0.0298
trigger times: 0
Loss after 868224060 batches: 0.0279
trigger times: 1
Loss after 868355160 batches: 0.0261
trigger times: 0
Loss after 868486260 batches: 0.0254
trigger times: 1
Loss after 868617360 batches: 0.0241
trigger times: 0
Loss after 868748460 batches: 0.0234
trigger times: 1
Loss after 868879560 batches: 0.0226
trigger times: 2
Loss after 869010660 batches: 0.0220
trigger times: 0
Loss after 869141760 batches: 0.0211
trigger times: 1
Loss after 869272860 batches: 0.0206
trigger times: 2
Loss after 869403960 batches: 0.0203
trigger times: 3
Loss after 869535060 batches: 0.0200
trigger times: 0
Loss after 869666160 batches: 0.0197
trigger times: 1
Loss after 869797260 batches: 0.0194
trigger times: 0
Loss after 869928360 batches: 0.0188
trigger times: 1
Loss after 870059460 batches: 0.0187
trigger times: 2
Loss after 870190560 batches: 0.0178
trigger times: 3
Loss after 870321660 batches: 0.0177
trigger times: 4
Loss after 870452760 batches: 0.0179
trigger times: 0
Loss after 870583860 batches: 0.0176
trigger times: 1
Loss after 870714960 batches: 0.0174
trigger times: 2
Loss after 870846060 batches: 0.0171
trigger times: 3
Loss after 870977160 batches: 0.0166
trigger times: 4
Loss after 871108260 batches: 0.0165
trigger times: 5
Loss after 871239360 batches: 0.0167
trigger times: 6
Loss after 871370460 batches: 0.0164
trigger times: 7
Loss after 871501560 batches: 0.0160
trigger times: 8
Loss after 871632660 batches: 0.0157
trigger times: 9
Loss after 871763760 batches: 0.0157
trigger times: 10
Loss after 871894860 batches: 0.0156
trigger times: 11
Loss after 872025960 batches: 0.0153
trigger times: 12
Loss after 872157060 batches: 0.0156
trigger times: 13
Loss after 872288160 batches: 0.0150
trigger times: 0
Loss after 872419260 batches: 0.0154
trigger times: 0
Loss after 872550360 batches: 0.0151
trigger times: 1
Loss after 872681460 batches: 0.0148
trigger times: 2
Loss after 872812560 batches: 0.0144
trigger times: 3
Loss after 872943660 batches: 0.0143
trigger times: 0
Loss after 873074760 batches: 0.0144
trigger times: 1
Loss after 873205860 batches: 0.0146
trigger times: 2
Loss after 873336960 batches: 0.0142
trigger times: 3
Loss after 873468060 batches: 0.0139
trigger times: 4
Loss after 873599160 batches: 0.0140
trigger times: 5
Loss after 873730260 batches: 0.0139
trigger times: 6
Loss after 873861360 batches: 0.0138
trigger times: 7
Loss after 873992460 batches: 0.0136
trigger times: 8
Loss after 874123560 batches: 0.0135
trigger times: 9
Loss after 874254660 batches: 0.0135
trigger times: 10
Loss after 874385760 batches: 0.0132
trigger times: 11
Loss after 874516860 batches: 0.0135
trigger times: 12
Loss after 874647960 batches: 0.0131
trigger times: 13
Loss after 874779060 batches: 0.0131
trigger times: 0
Loss after 874910160 batches: 0.0131
trigger times: 1
Loss after 875041260 batches: 0.0130
trigger times: 2
Loss after 875172360 batches: 0.0130
trigger times: 3
Loss after 875303460 batches: 0.0127
trigger times: 4
Loss after 875434560 batches: 0.0125
trigger times: 5
Loss after 875565660 batches: 0.0125
trigger times: 6
Loss after 875696760 batches: 0.0125
trigger times: 7
Loss after 875827860 batches: 0.0124
trigger times: 8
Loss after 875958960 batches: 0.0123
trigger times: 9
Loss after 876090060 batches: 0.0124
trigger times: 10
Loss after 876221160 batches: 0.0124
trigger times: 11
Loss after 876352260 batches: 0.0123
trigger times: 12
Loss after 876483360 batches: 0.0123
trigger times: 13
Loss after 876614460 batches: 0.0120
trigger times: 14
Loss after 876745560 batches: 0.0118
trigger times: 15
Loss after 876876660 batches: 0.0120
trigger times: 16
Loss after 877007760 batches: 0.0119
trigger times: 17
Loss after 877138860 batches: 0.0117
trigger times: 18
Loss after 877269960 batches: 0.0117
trigger times: 19
Loss after 877401060 batches: 0.0116
trigger times: 20
Early stopping!
Start to test process.
Loss after 877532160 batches: 0.0117
Time to train on one home:  578.6776602268219
trigger times: 0
Loss after 877663260 batches: 0.2304
trigger times: 1
Loss after 877794360 batches: 0.0776
trigger times: 2
Loss after 877925460 batches: 0.0542
trigger times: 3
Loss after 878056560 batches: 0.0453
trigger times: 4
Loss after 878187660 batches: 0.0408
trigger times: 5
Loss after 878318760 batches: 0.0373
trigger times: 6
Loss after 878449860 batches: 0.0355
trigger times: 7
Loss after 878580960 batches: 0.0338
trigger times: 8
Loss after 878712060 batches: 0.0319
trigger times: 9
Loss after 878843160 batches: 0.0305
trigger times: 10
Loss after 878974260 batches: 0.0298
trigger times: 11
Loss after 879105360 batches: 0.0286
trigger times: 12
Loss after 879236460 batches: 0.0283
trigger times: 13
Loss after 879367560 batches: 0.0281
trigger times: 14
Loss after 879498660 batches: 0.0271
trigger times: 15
Loss after 879629760 batches: 0.0262
trigger times: 16
Loss after 879760860 batches: 0.0258
trigger times: 17
Loss after 879891960 batches: 0.0254
trigger times: 18
Loss after 880023060 batches: 0.0252
trigger times: 19
Loss after 880154160 batches: 0.0248
trigger times: 20
Early stopping!
Start to test process.
Loss after 880285260 batches: 0.0242
Time to train on one home:  164.66124272346497
trigger times: 0
Loss after 880416360 batches: 0.3749
trigger times: 1
Loss after 880547460 batches: 0.1390
trigger times: 0
Loss after 880678560 batches: 0.0954
trigger times: 0
Loss after 880809660 batches: 0.0782
trigger times: 1
Loss after 880940760 batches: 0.0652
trigger times: 2
Loss after 881071860 batches: 0.0599
trigger times: 3
Loss after 881202960 batches: 0.0553
trigger times: 0
Loss after 881334060 batches: 0.0515
trigger times: 1
Loss after 881465160 batches: 0.0494
trigger times: 2
Loss after 881596260 batches: 0.0480
trigger times: 0
Loss after 881727360 batches: 0.0458
trigger times: 1
Loss after 881858460 batches: 0.0438
trigger times: 2
Loss after 881989560 batches: 0.0422
trigger times: 3
Loss after 882120660 batches: 0.0404
trigger times: 4
Loss after 882251760 batches: 0.0394
trigger times: 5
Loss after 882382860 batches: 0.0387
trigger times: 6
Loss after 882513960 batches: 0.0396
trigger times: 0
Loss after 882645060 batches: 0.0376
trigger times: 1
Loss after 882776160 batches: 0.0374
trigger times: 2
Loss after 882907260 batches: 0.0368
trigger times: 3
Loss after 883038360 batches: 0.0362
trigger times: 4
Loss after 883169460 batches: 0.0342
trigger times: 5
Loss after 883300560 batches: 0.0346
trigger times: 0
Loss after 883431660 batches: 0.0343
trigger times: 1
Loss after 883562760 batches: 0.0331
trigger times: 2
Loss after 883693860 batches: 0.0331
trigger times: 3
Loss after 883824960 batches: 0.0328
trigger times: 4
Loss after 883956060 batches: 0.0321
trigger times: 5
Loss after 884087160 batches: 0.0314
trigger times: 0
Loss after 884218260 batches: 0.0307
trigger times: 1
Loss after 884349360 batches: 0.0313
trigger times: 2
Loss after 884480460 batches: 0.0307
trigger times: 3
Loss after 884611560 batches: 0.0304
trigger times: 4
Loss after 884742660 batches: 0.0296
trigger times: 0
Loss after 884873760 batches: 0.0295
trigger times: 1
Loss after 885004860 batches: 0.0294
trigger times: 2
Loss after 885135960 batches: 0.0298
trigger times: 0
Loss after 885267060 batches: 0.0290
trigger times: 0
Loss after 885398160 batches: 0.0296
trigger times: 0
Loss after 885529260 batches: 0.0283
trigger times: 1
Loss after 885660360 batches: 0.0284
trigger times: 2
Loss after 885791460 batches: 0.0288
trigger times: 3
Loss after 885922560 batches: 0.0275
trigger times: 4
Loss after 886053660 batches: 0.0282
trigger times: 0
Loss after 886184760 batches: 0.0277
trigger times: 1
Loss after 886315860 batches: 0.0276
trigger times: 2
Loss after 886446960 batches: 0.0272
trigger times: 3
Loss after 886578060 batches: 0.0272
trigger times: 4
Loss after 886709160 batches: 0.0274
trigger times: 5
Loss after 886840260 batches: 0.0259
trigger times: 6
Loss after 886971360 batches: 0.0270
trigger times: 7
Loss after 887102460 batches: 0.0263
trigger times: 8
Loss after 887233560 batches: 0.0262
trigger times: 9
Loss after 887364660 batches: 0.0258
trigger times: 0
Loss after 887495760 batches: 0.0261
trigger times: 1
Loss after 887626860 batches: 0.0256
trigger times: 0
Loss after 887757960 batches: 0.0250
trigger times: 1
Loss after 887889060 batches: 0.0258
trigger times: 2
Loss after 888020160 batches: 0.0257
trigger times: 3
Loss after 888151260 batches: 0.0250
trigger times: 4
Loss after 888282360 batches: 0.0243
trigger times: 5
Loss after 888413460 batches: 0.0244
trigger times: 6
Loss after 888544560 batches: 0.0244
trigger times: 7
Loss after 888675660 batches: 0.0238
trigger times: 0
Loss after 888806760 batches: 0.0243
trigger times: 1
Loss after 888937860 batches: 0.0235
trigger times: 2
Loss after 889068960 batches: 0.0233
trigger times: 3
Loss after 889200060 batches: 0.0234
trigger times: 4
Loss after 889331160 batches: 0.0236
trigger times: 5
Loss after 889462260 batches: 0.0235
trigger times: 6
Loss after 889593360 batches: 0.0230
trigger times: 7
Loss after 889724460 batches: 0.0235
trigger times: 8
Loss after 889855560 batches: 0.0230
trigger times: 9
Loss after 889986660 batches: 0.0235
trigger times: 10
Loss after 890117760 batches: 0.0228
trigger times: 11
Loss after 890248860 batches: 0.0231
trigger times: 12
Loss after 890379960 batches: 0.0233
trigger times: 13
Loss after 890511060 batches: 0.0229
trigger times: 14
Loss after 890642160 batches: 0.0231
trigger times: 15
Loss after 890773260 batches: 0.0230
trigger times: 16
Loss after 890904360 batches: 0.0231
trigger times: 17
Loss after 891035460 batches: 0.0224
trigger times: 18
Loss after 891166560 batches: 0.0223
trigger times: 19
Loss after 891297660 batches: 0.0228
trigger times: 20
Early stopping!
Start to test process.
Loss after 891428760 batches: 0.0216
Time to train on one home:  627.899331331253
trigger times: 0
Loss after 891559860 batches: 0.4731
trigger times: 1
Loss after 891690960 batches: 0.2280
trigger times: 2
Loss after 891822060 batches: 0.1504
trigger times: 3
Loss after 891953160 batches: 0.1196
trigger times: 4
Loss after 892084260 batches: 0.1026
trigger times: 5
Loss after 892215360 batches: 0.0919
trigger times: 6
Loss after 892346460 batches: 0.0853
trigger times: 7
Loss after 892477560 batches: 0.0791
trigger times: 8
Loss after 892608660 batches: 0.0750
trigger times: 9
Loss after 892739760 batches: 0.0709
trigger times: 10
Loss after 892870860 batches: 0.0679
trigger times: 11
Loss after 893001960 batches: 0.0653
trigger times: 12
Loss after 893133060 batches: 0.0632
trigger times: 13
Loss after 893264160 batches: 0.0623
trigger times: 14
Loss after 893395260 batches: 0.0605
trigger times: 15
Loss after 893526360 batches: 0.0590
trigger times: 16
Loss after 893657460 batches: 0.0575
trigger times: 17
Loss after 893788560 batches: 0.0565
trigger times: 18
Loss after 893919660 batches: 0.0546
trigger times: 19
Loss after 894050760 batches: 0.0532
trigger times: 20
Early stopping!
Start to test process.
Loss after 894181860 batches: 0.0533
Time to train on one home:  163.84159445762634
trigger times: 0
Loss after 894312960 batches: 0.3364
trigger times: 0
Loss after 894444060 batches: 0.1212
trigger times: 1
Loss after 894575160 batches: 0.0839
trigger times: 0
Loss after 894706260 batches: 0.0678
trigger times: 1
Loss after 894837360 batches: 0.0604
trigger times: 2
Loss after 894968460 batches: 0.0549
trigger times: 3
Loss after 895099560 batches: 0.0495
trigger times: 4
Loss after 895230660 batches: 0.0475
trigger times: 5
Loss after 895361760 batches: 0.0445
trigger times: 0
Loss after 895492860 batches: 0.0430
trigger times: 0
Loss after 895623960 batches: 0.0412
trigger times: 0
Loss after 895755060 batches: 0.0399
trigger times: 1
Loss after 895886160 batches: 0.0384
trigger times: 2
Loss after 896017260 batches: 0.0381
trigger times: 3
Loss after 896148360 batches: 0.0367
trigger times: 0
Loss after 896279460 batches: 0.0365
trigger times: 0
Loss after 896410560 batches: 0.0347
trigger times: 0
Loss after 896541660 batches: 0.0334
trigger times: 1
Loss after 896672760 batches: 0.0338
trigger times: 2
Loss after 896803860 batches: 0.0327
trigger times: 3
Loss after 896934960 batches: 0.0314
trigger times: 4
Loss after 897066060 batches: 0.0316
trigger times: 5
Loss after 897197160 batches: 0.0320
trigger times: 0
Loss after 897328260 batches: 0.0308
trigger times: 1
Loss after 897459360 batches: 0.0305
trigger times: 2
Loss after 897590460 batches: 0.0314
trigger times: 3
Loss after 897721560 batches: 0.0300
trigger times: 4
Loss after 897852660 batches: 0.0300
trigger times: 5
Loss after 897983760 batches: 0.0289
trigger times: 6
Loss after 898114860 batches: 0.0286
trigger times: 7
Loss after 898245960 batches: 0.0279
trigger times: 8
Loss after 898377060 batches: 0.0294
trigger times: 9
Loss after 898508160 batches: 0.0288
trigger times: 10
Loss after 898639260 batches: 0.0281
trigger times: 11
Loss after 898770360 batches: 0.0282
trigger times: 12
Loss after 898901460 batches: 0.0269
trigger times: 13
Loss after 899032560 batches: 0.0277
trigger times: 14
Loss after 899163660 batches: 0.0263
trigger times: 15
Loss after 899294760 batches: 0.0266
trigger times: 16
Loss after 899425860 batches: 0.0260
trigger times: 0
Loss after 899556960 batches: 0.0258
trigger times: 0
Loss after 899688060 batches: 0.0258
trigger times: 0
Loss after 899819160 batches: 0.0254
trigger times: 1
Loss after 899950260 batches: 0.0256
trigger times: 2
Loss after 900081360 batches: 0.0252
trigger times: 3
Loss after 900212460 batches: 0.0256
trigger times: 4
Loss after 900343560 batches: 0.0250
trigger times: 5
Loss after 900474660 batches: 0.0242
trigger times: 6
Loss after 900605760 batches: 0.0247
trigger times: 7
Loss after 900736860 batches: 0.0249
trigger times: 8
Loss after 900867960 batches: 0.0240
trigger times: 9
Loss after 900999060 batches: 0.0241
trigger times: 10
Loss after 901130160 batches: 0.0243
trigger times: 11
Loss after 901261260 batches: 0.0242
trigger times: 12
Loss after 901392360 batches: 0.0237
trigger times: 13
Loss after 901523460 batches: 0.0236
trigger times: 14
Loss after 901654560 batches: 0.0238
trigger times: 15
Loss after 901785660 batches: 0.0231
trigger times: 16
Loss after 901916760 batches: 0.0235
trigger times: 17
Loss after 902047860 batches: 0.0230
trigger times: 18
Loss after 902178960 batches: 0.0222
trigger times: 19
Loss after 902310060 batches: 0.0224
trigger times: 20
Early stopping!
Start to test process.
Loss after 902441160 batches: 0.0221
Time to train on one home:  469.8268895149231
trigger times: 0
Loss after 902572260 batches: 0.5143
trigger times: 0
Loss after 902703360 batches: 0.2793
trigger times: 1
Loss after 902834460 batches: 0.1778
trigger times: 2
Loss after 902965560 batches: 0.1324
trigger times: 3
Loss after 903096660 batches: 0.1093
trigger times: 4
Loss after 903227760 batches: 0.0963
trigger times: 5
Loss after 903358860 batches: 0.0881
trigger times: 6
Loss after 903489960 batches: 0.0810
trigger times: 7
Loss after 903621060 batches: 0.0757
trigger times: 8
Loss after 903752160 batches: 0.0709
trigger times: 9
Loss after 903883260 batches: 0.0673
trigger times: 10
Loss after 904014360 batches: 0.0650
trigger times: 11
Loss after 904145460 batches: 0.0623
trigger times: 12
Loss after 904276560 batches: 0.0600
trigger times: 13
Loss after 904407660 batches: 0.0580
trigger times: 14
Loss after 904538760 batches: 0.0560
trigger times: 15
Loss after 904669860 batches: 0.0543
trigger times: 16
Loss after 904800960 batches: 0.0530
trigger times: 17
Loss after 904932060 batches: 0.0521
trigger times: 18
Loss after 905063160 batches: 0.0506
trigger times: 19
Loss after 905194260 batches: 0.0494
trigger times: 20
Early stopping!
Start to test process.
Loss after 905325360 batches: 0.0486
Time to train on one home:  172.9361560344696
trigger times: 0
Loss after 905456460 batches: 0.7331
trigger times: 1
Loss after 905587560 batches: 0.3778
trigger times: 2
Loss after 905718660 batches: 0.2124
trigger times: 3
Loss after 905849760 batches: 0.1591
trigger times: 4
Loss after 905980860 batches: 0.1320
trigger times: 5
Loss after 906111960 batches: 0.1177
trigger times: 6
Loss after 906243060 batches: 0.1068
trigger times: 7
Loss after 906374160 batches: 0.1004
trigger times: 8
Loss after 906505260 batches: 0.0960
trigger times: 9
Loss after 906636360 batches: 0.0906
trigger times: 10
Loss after 906767460 batches: 0.0859
trigger times: 11
Loss after 906898560 batches: 0.0823
trigger times: 12
Loss after 907029660 batches: 0.0797
trigger times: 13
Loss after 907160760 batches: 0.0773
trigger times: 14
Loss after 907291860 batches: 0.0743
trigger times: 15
Loss after 907422960 batches: 0.0722
trigger times: 16
Loss after 907554060 batches: 0.0704
trigger times: 17
Loss after 907685160 batches: 0.0694
trigger times: 18
Loss after 907816260 batches: 0.0679
trigger times: 19
Loss after 907947360 batches: 0.0658
trigger times: 20
Early stopping!
Start to test process.
Loss after 908078460 batches: 0.0648
Time to train on one home:  164.73560285568237
trigger times: 0
Loss after 908172420 batches: 0.6565
trigger times: 1
Loss after 908266380 batches: 0.3505
trigger times: 2
Loss after 908360340 batches: 0.2015
trigger times: 3
Loss after 908454300 batches: 0.1434
trigger times: 4
Loss after 908548260 batches: 0.1200
trigger times: 5
Loss after 908642220 batches: 0.1044
trigger times: 6
Loss after 908736180 batches: 0.0918
trigger times: 7
Loss after 908830140 batches: 0.0851
trigger times: 8
Loss after 908924100 batches: 0.0788
trigger times: 9
Loss after 909018060 batches: 0.0752
trigger times: 10
Loss after 909112020 batches: 0.0720
trigger times: 11
Loss after 909205980 batches: 0.0695
trigger times: 12
Loss after 909299940 batches: 0.0663
trigger times: 13
Loss after 909393900 batches: 0.0649
trigger times: 14
Loss after 909487860 batches: 0.0622
trigger times: 15
Loss after 909581820 batches: 0.0601
trigger times: 16
Loss after 909675780 batches: 0.0590
trigger times: 17
Loss after 909769740 batches: 0.0574
trigger times: 18
Loss after 909863700 batches: 0.0562
trigger times: 19
Loss after 909957660 batches: 0.0551
trigger times: 20
Early stopping!
Start to test process.
Loss after 910051620 batches: 0.0545
Time to train on one home:  125.5634195804596
trigger times: 0
Loss after 910182720 batches: 0.0755
trigger times: 0
Loss after 910313820 batches: 0.0195
trigger times: 0
Loss after 910444920 batches: 0.0140
trigger times: 1
Loss after 910576020 batches: 0.0121
trigger times: 0
Loss after 910707120 batches: 0.0102
trigger times: 0
Loss after 910838220 batches: 0.0093
trigger times: 0
Loss after 910969320 batches: 0.0083
trigger times: 1
Loss after 911100420 batches: 0.0077
trigger times: 2
Loss after 911231520 batches: 0.0074
trigger times: 3
Loss after 911362620 batches: 0.0069
trigger times: 4
Loss after 911493720 batches: 0.0064
trigger times: 0
Loss after 911624820 batches: 0.0063
trigger times: 1
Loss after 911755920 batches: 0.0060
trigger times: 2
Loss after 911887020 batches: 0.0057
trigger times: 0
Loss after 912018120 batches: 0.0055
trigger times: 1
Loss after 912149220 batches: 0.0053
trigger times: 0
Loss after 912280320 batches: 0.0051
trigger times: 1
Loss after 912411420 batches: 0.0051
trigger times: 0
Loss after 912542520 batches: 0.0050
trigger times: 1
Loss after 912673620 batches: 0.0048
trigger times: 2
Loss after 912804720 batches: 0.0048
trigger times: 3
Loss after 912935820 batches: 0.0047
trigger times: 4
Loss after 913066920 batches: 0.0047
trigger times: 5
Loss after 913198020 batches: 0.0044
trigger times: 6
Loss after 913329120 batches: 0.0044
trigger times: 7
Loss after 913460220 batches: 0.0044
trigger times: 8
Loss after 913591320 batches: 0.0043
trigger times: 9
Loss after 913722420 batches: 0.0042
trigger times: 0
Loss after 913853520 batches: 0.0041
trigger times: 1
Loss after 913984620 batches: 0.0040
trigger times: 2
Loss after 914115720 batches: 0.0040
trigger times: 3
Loss after 914246820 batches: 0.0040
trigger times: 4
Loss after 914377920 batches: 0.0038
trigger times: 5
Loss after 914509020 batches: 0.0038
trigger times: 6
Loss after 914640120 batches: 0.0037
trigger times: 7
Loss after 914771220 batches: 0.0036
trigger times: 8
Loss after 914902320 batches: 0.0037
trigger times: 9
Loss after 915033420 batches: 0.0036
trigger times: 10
Loss after 915164520 batches: 0.0036
trigger times: 0
Loss after 915295620 batches: 0.0035
trigger times: 1
Loss after 915426720 batches: 0.0035
trigger times: 2
Loss after 915557820 batches: 0.0035
trigger times: 3
Loss after 915688920 batches: 0.0034
trigger times: 4
Loss after 915820020 batches: 0.0033
trigger times: 0
Loss after 915951120 batches: 0.0034
trigger times: 1
Loss after 916082220 batches: 0.0033
trigger times: 2
Loss after 916213320 batches: 0.0032
trigger times: 0
Loss after 916344420 batches: 0.0033
trigger times: 1
Loss after 916475520 batches: 0.0031
trigger times: 0
Loss after 916606620 batches: 0.0031
trigger times: 1
Loss after 916737720 batches: 0.0032
trigger times: 2
Loss after 916868820 batches: 0.0031
trigger times: 3
Loss after 916999920 batches: 0.0031
trigger times: 4
Loss after 917131020 batches: 0.0030
trigger times: 5
Loss after 917262120 batches: 0.0031
trigger times: 6
Loss after 917393220 batches: 0.0029
trigger times: 7
Loss after 917524320 batches: 0.0030
trigger times: 8
Loss after 917655420 batches: 0.0031
trigger times: 9
Loss after 917786520 batches: 0.0030
trigger times: 10
Loss after 917917620 batches: 0.0032
trigger times: 11
Loss after 918048720 batches: 0.0031
trigger times: 12
Loss after 918179820 batches: 0.0030
trigger times: 13
Loss after 918310920 batches: 0.0029
trigger times: 14
Loss after 918442020 batches: 0.0030
trigger times: 15
Loss after 918573120 batches: 0.0028
trigger times: 16
Loss after 918704220 batches: 0.0029
trigger times: 17
Loss after 918835320 batches: 0.0027
trigger times: 18
Loss after 918966420 batches: 0.0027
trigger times: 19
Loss after 919097520 batches: 0.0027
trigger times: 20
Early stopping!
Start to test process.
Loss after 919228620 batches: 0.0026
Time to train on one home:  517.9643566608429
trigger times: 0
Loss after 919359720 batches: 0.2025
trigger times: 0
Loss after 919490820 batches: 0.0896
trigger times: 1
Loss after 919621920 batches: 0.0611
trigger times: 2
Loss after 919753020 batches: 0.0486
trigger times: 3
Loss after 919884120 batches: 0.0429
trigger times: 4
Loss after 920015220 batches: 0.0386
trigger times: 0
Loss after 920146320 batches: 0.0349
trigger times: 1
Loss after 920277420 batches: 0.0322
trigger times: 2
Loss after 920408520 batches: 0.0307
trigger times: 3
Loss after 920539620 batches: 0.0301
trigger times: 0
Loss after 920670720 batches: 0.0284
trigger times: 1
Loss after 920801820 batches: 0.0272
trigger times: 2
Loss after 920932920 batches: 0.0265
trigger times: 0
Loss after 921064020 batches: 0.0252
trigger times: 1
Loss after 921195120 batches: 0.0244
trigger times: 2
Loss after 921326220 batches: 0.0245
trigger times: 3
Loss after 921457320 batches: 0.0232
trigger times: 4
Loss after 921588420 batches: 0.0231
trigger times: 5
Loss after 921719520 batches: 0.0222
trigger times: 6
Loss after 921850620 batches: 0.0224
trigger times: 7
Loss after 921981720 batches: 0.0215
trigger times: 8
Loss after 922112820 batches: 0.0213
trigger times: 9
Loss after 922243920 batches: 0.0207
trigger times: 10
Loss after 922375020 batches: 0.0207
trigger times: 0
Loss after 922506120 batches: 0.0207
trigger times: 1
Loss after 922637220 batches: 0.0196
trigger times: 2
Loss after 922768320 batches: 0.0194
trigger times: 3
Loss after 922899420 batches: 0.0191
trigger times: 0
Loss after 923030520 batches: 0.0190
trigger times: 1
Loss after 923161620 batches: 0.0185
trigger times: 0
Loss after 923292720 batches: 0.0182
trigger times: 0
Loss after 923423820 batches: 0.0182
trigger times: 1
Loss after 923554920 batches: 0.0178
trigger times: 2
Loss after 923686020 batches: 0.0180
trigger times: 0
Loss after 923817120 batches: 0.0174
trigger times: 1
Loss after 923948220 batches: 0.0171
trigger times: 2
Loss after 924079320 batches: 0.0168
trigger times: 3
Loss after 924210420 batches: 0.0170
trigger times: 4
Loss after 924341520 batches: 0.0165
trigger times: 5
Loss after 924472620 batches: 0.0165
trigger times: 6
Loss after 924603720 batches: 0.0159
trigger times: 7
Loss after 924734820 batches: 0.0159
trigger times: 8
Loss after 924865920 batches: 0.0156
trigger times: 9
Loss after 924997020 batches: 0.0157
trigger times: 10
Loss after 925128120 batches: 0.0155
trigger times: 0
Loss after 925259220 batches: 0.0153
trigger times: 0
Loss after 925390320 batches: 0.0151
trigger times: 1
Loss after 925521420 batches: 0.0151
trigger times: 0
Loss after 925652520 batches: 0.0150
trigger times: 0
Loss after 925783620 batches: 0.0148
trigger times: 1
Loss after 925914720 batches: 0.0149
trigger times: 2
Loss after 926045820 batches: 0.0146
trigger times: 3
Loss after 926176920 batches: 0.0143
trigger times: 4
Loss after 926308020 batches: 0.0146
trigger times: 5
Loss after 926439120 batches: 0.0145
trigger times: 0
Loss after 926570220 batches: 0.0142
trigger times: 0
Loss after 926701320 batches: 0.0140
trigger times: 1
Loss after 926832420 batches: 0.0138
trigger times: 2
Loss after 926963520 batches: 0.0139
trigger times: 3
Loss after 927094620 batches: 0.0137
trigger times: 4
Loss after 927225720 batches: 0.0136
trigger times: 5
Loss after 927356820 batches: 0.0136
trigger times: 6
Loss after 927487920 batches: 0.0135
trigger times: 7
Loss after 927619020 batches: 0.0133
trigger times: 8
Loss after 927750120 batches: 0.0132
trigger times: 9
Loss after 927881220 batches: 0.0135
trigger times: 10
Loss after 928012320 batches: 0.0132
trigger times: 11
Loss after 928143420 batches: 0.0129
trigger times: 12
Loss after 928274520 batches: 0.0130
trigger times: 13
Loss after 928405620 batches: 0.0130
trigger times: 14
Loss after 928536720 batches: 0.0129
trigger times: 15
Loss after 928667820 batches: 0.0127
trigger times: 16
Loss after 928798920 batches: 0.0128
trigger times: 17
Loss after 928930020 batches: 0.0126
trigger times: 18
Loss after 929061120 batches: 0.0126
trigger times: 19
Loss after 929192220 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 929323320 batches: 0.0127
Time to train on one home:  570.7634077072144
trigger times: 0
Loss after 929454420 batches: 0.4123
trigger times: 0
Loss after 929585520 batches: 0.1819
trigger times: 0
Loss after 929716620 batches: 0.1320
trigger times: 1
Loss after 929847720 batches: 0.1097
trigger times: 2
Loss after 929978820 batches: 0.0983
trigger times: 3
Loss after 930109920 batches: 0.0907
trigger times: 0
Loss after 930241020 batches: 0.0862
trigger times: 0
Loss after 930372120 batches: 0.0810
trigger times: 0
Loss after 930503220 batches: 0.0780
trigger times: 0
Loss after 930634320 batches: 0.0766
trigger times: 1
Loss after 930765420 batches: 0.0741
trigger times: 0
Loss after 930896520 batches: 0.0713
trigger times: 0
Loss after 931027620 batches: 0.0691
trigger times: 1
Loss after 931158720 batches: 0.0677
trigger times: 0
Loss after 931289820 batches: 0.0660
trigger times: 1
Loss after 931420920 batches: 0.0660
trigger times: 2
Loss after 931552020 batches: 0.0639
trigger times: 3
Loss after 931683120 batches: 0.0624
trigger times: 4
Loss after 931814220 batches: 0.0610
trigger times: 0
Loss after 931945320 batches: 0.0601
trigger times: 0
Loss after 932076420 batches: 0.0594
trigger times: 1
Loss after 932207520 batches: 0.0594
trigger times: 2
Loss after 932338620 batches: 0.0587
trigger times: 0
Loss after 932469720 batches: 0.0582
trigger times: 1
Loss after 932600820 batches: 0.0573
trigger times: 0
Loss after 932731920 batches: 0.0567
trigger times: 0
Loss after 932863020 batches: 0.0566
trigger times: 1
Loss after 932994120 batches: 0.0560
trigger times: 2
Loss after 933125220 batches: 0.0554
trigger times: 0
Loss after 933256320 batches: 0.0547
trigger times: 1
Loss after 933387420 batches: 0.0552
trigger times: 0
Loss after 933518520 batches: 0.0529
trigger times: 0
Loss after 933649620 batches: 0.0535
trigger times: 1
Loss after 933780720 batches: 0.0527
trigger times: 2
Loss after 933911820 batches: 0.0522
trigger times: 3
Loss after 934042920 batches: 0.0525
trigger times: 4
Loss after 934174020 batches: 0.0520
trigger times: 5
Loss after 934305120 batches: 0.0513
trigger times: 6
Loss after 934436220 batches: 0.0521
trigger times: 7
Loss after 934567320 batches: 0.0513
trigger times: 8
Loss after 934698420 batches: 0.0504
trigger times: 0
Loss after 934829520 batches: 0.0504
trigger times: 1
Loss after 934960620 batches: 0.0498
trigger times: 0
Loss after 935091720 batches: 0.0493
trigger times: 0
Loss after 935222820 batches: 0.0503
trigger times: 1
Loss after 935353920 batches: 0.0498
trigger times: 2
Loss after 935485020 batches: 0.0493
trigger times: 3
Loss after 935616120 batches: 0.0485
trigger times: 4
Loss after 935747220 batches: 0.0486
trigger times: 5
Loss after 935878320 batches: 0.0483
trigger times: 6
Loss after 936009420 batches: 0.0484
trigger times: 7
Loss after 936140520 batches: 0.0476
trigger times: 8
Loss after 936271620 batches: 0.0474
trigger times: 9
Loss after 936402720 batches: 0.0474
trigger times: 10
Loss after 936533820 batches: 0.0475
trigger times: 11
Loss after 936664920 batches: 0.0472
trigger times: 12
Loss after 936796020 batches: 0.0465
trigger times: 13
Loss after 936927120 batches: 0.0475
trigger times: 14
Loss after 937058220 batches: 0.0477
trigger times: 15
Loss after 937189320 batches: 0.0470
trigger times: 16
Loss after 937320420 batches: 0.0461
trigger times: 0
Loss after 937451520 batches: 0.0454
trigger times: 1
Loss after 937582620 batches: 0.0461
trigger times: 2
Loss after 937713720 batches: 0.0456
trigger times: 3
Loss after 937844820 batches: 0.0457
trigger times: 4
Loss after 937975920 batches: 0.0455
trigger times: 5
Loss after 938107020 batches: 0.0450
trigger times: 6
Loss after 938238120 batches: 0.0450
trigger times: 7
Loss after 938369220 batches: 0.0450
trigger times: 0
Loss after 938500320 batches: 0.0459
trigger times: 1
Loss after 938631420 batches: 0.0442
trigger times: 2
Loss after 938762520 batches: 0.0443
trigger times: 0
Loss after 938893620 batches: 0.0443
trigger times: 1
Loss after 939024720 batches: 0.0438
trigger times: 2
Loss after 939155820 batches: 0.0438
trigger times: 3
Loss after 939286920 batches: 0.0438
trigger times: 4
Loss after 939418020 batches: 0.0437
trigger times: 5
Loss after 939549120 batches: 0.0431
trigger times: 6
Loss after 939680220 batches: 0.0438
trigger times: 7
Loss after 939811320 batches: 0.0432
trigger times: 8
Loss after 939942420 batches: 0.0432
trigger times: 9
Loss after 940073520 batches: 0.0427
trigger times: 10
Loss after 940204620 batches: 0.0432
trigger times: 11
Loss after 940335720 batches: 0.0435
trigger times: 12
Loss after 940466820 batches: 0.0423
trigger times: 13
Loss after 940597920 batches: 0.0425
trigger times: 14
Loss after 940729020 batches: 0.0425
trigger times: 15
Loss after 940860120 batches: 0.0433
trigger times: 16
Loss after 940991220 batches: 0.0427
trigger times: 17
Loss after 941122320 batches: 0.0421
trigger times: 18
Loss after 941253420 batches: 0.0418
trigger times: 19
Loss after 941384520 batches: 0.0414
trigger times: 20
Early stopping!
Start to test process.
Loss after 941515620 batches: 0.0418
Time to train on one home:  686.7620983123779
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642, 0.03501720033327798]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217], [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944]]
Round_8_results:  [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 7449 < 7450; dropping {'Training_Loss': 0.4631409432206835, 'Validation_Loss': 0.7720789511998495, 'Training_R2': 0.5305618474801117, 'Validation_R2': 0.2803997489570954, 'Training_F1': 0.7335123339979159, 'Validation_F1': 0.3941422034216734, 'Training_NEP': 0.534998589932675, 'Validation_NEP': 1.083416074061475, 'Training_NDE': 0.31620897173138846, 'Validation_NDE': 0.5730517068441046, 'Training_MAE': 35.11293774956151, 'Validation_MAE': 29.711763568719228, 'Training_MSE': 4172.9204, 'Validation_MSE': 2116.2644}.
trigger times: 0
Loss after 941618220 batches: 0.4631
trigger times: 1
Loss after 941720820 batches: 0.2106
trigger times: 2
Loss after 941823420 batches: 0.1309
trigger times: 3
Loss after 941926020 batches: 0.1053
trigger times: 4
Loss after 942028620 batches: 0.0933
trigger times: 5
Loss after 942131220 batches: 0.0824
trigger times: 6
Loss after 942233820 batches: 0.0747
trigger times: 7
Loss after 942336420 batches: 0.0650
trigger times: 8
Loss after 942439020 batches: 0.0597
trigger times: 9
Loss after 942541620 batches: 0.0587
trigger times: 10
Loss after 942644220 batches: 0.0562
trigger times: 11
Loss after 942746820 batches: 0.0570
trigger times: 12
Loss after 942849420 batches: 0.0580
trigger times: 13
Loss after 942952020 batches: 0.0572
trigger times: 14
Loss after 943054620 batches: 0.0528
trigger times: 15
Loss after 943157220 batches: 0.0506
trigger times: 16
Loss after 943259820 batches: 0.0479
trigger times: 17
Loss after 943362420 batches: 0.0453
trigger times: 18
Loss after 943465020 batches: 0.0437
trigger times: 19
Loss after 943567620 batches: 0.0424
trigger times: 20
Early stopping!
Start to test process.
Loss after 943670220 batches: 0.0445
Time to train on one home:  136.07770228385925
trigger times: 0
Loss after 943801320 batches: 0.2891
trigger times: 0
Loss after 943932420 batches: 0.1237
trigger times: 0
Loss after 944063520 batches: 0.0840
trigger times: 1
Loss after 944194620 batches: 0.0684
trigger times: 2
Loss after 944325720 batches: 0.0610
trigger times: 3
Loss after 944456820 batches: 0.0550
trigger times: 4
Loss after 944587920 batches: 0.0512
trigger times: 5
Loss after 944719020 batches: 0.0481
trigger times: 6
Loss after 944850120 batches: 0.0459
trigger times: 7
Loss after 944981220 batches: 0.0443
trigger times: 8
Loss after 945112320 batches: 0.0425
trigger times: 9
Loss after 945243420 batches: 0.0408
trigger times: 10
Loss after 945374520 batches: 0.0395
trigger times: 11
Loss after 945505620 batches: 0.0381
trigger times: 12
Loss after 945636720 batches: 0.0376
trigger times: 13
Loss after 945767820 batches: 0.0364
trigger times: 14
Loss after 945898920 batches: 0.0361
trigger times: 15
Loss after 946030020 batches: 0.0347
trigger times: 16
Loss after 946161120 batches: 0.0342
trigger times: 17
Loss after 946292220 batches: 0.0338
trigger times: 18
Loss after 946423320 batches: 0.0331
trigger times: 19
Loss after 946554420 batches: 0.0323
trigger times: 20
Early stopping!
Start to test process.
Loss after 946685520 batches: 0.0324
Time to train on one home:  178.36790919303894
trigger times: 0
Loss after 946816620 batches: 0.5437
trigger times: 1
Loss after 946947720 batches: 0.2465
trigger times: 0
Loss after 947078820 batches: 0.1518
trigger times: 1
Loss after 947209920 batches: 0.1202
trigger times: 2
Loss after 947341020 batches: 0.1034
trigger times: 3
Loss after 947472120 batches: 0.0933
trigger times: 4
Loss after 947603220 batches: 0.0855
trigger times: 5
Loss after 947734320 batches: 0.0793
trigger times: 6
Loss after 947865420 batches: 0.0752
trigger times: 7
Loss after 947996520 batches: 0.0705
trigger times: 8
Loss after 948127620 batches: 0.0683
trigger times: 9
Loss after 948258720 batches: 0.0670
trigger times: 10
Loss after 948389820 batches: 0.0633
trigger times: 11
Loss after 948520920 batches: 0.0620
trigger times: 12
Loss after 948652020 batches: 0.0594
trigger times: 13
Loss after 948783120 batches: 0.0584
trigger times: 14
Loss after 948914220 batches: 0.0567
trigger times: 15
Loss after 949045320 batches: 0.0564
trigger times: 16
Loss after 949176420 batches: 0.0548
trigger times: 17
Loss after 949307520 batches: 0.0537
trigger times: 18
Loss after 949438620 batches: 0.0535
trigger times: 19
Loss after 949569720 batches: 0.0522
trigger times: 20
Early stopping!
Start to test process.
Loss after 949700820 batches: 0.0517
Time to train on one home:  178.78248524665833
trigger times: 0
Loss after 949829460 batches: 0.2129
trigger times: 0
Loss after 949958100 batches: 0.0842
trigger times: 1
Loss after 950086740 batches: 0.0600
trigger times: 2
Loss after 950215380 batches: 0.0494
trigger times: 3
Loss after 950344020 batches: 0.0442
trigger times: 4
Loss after 950472660 batches: 0.0400
trigger times: 5
Loss after 950601300 batches: 0.0371
trigger times: 6
Loss after 950729940 batches: 0.0350
trigger times: 7
Loss after 950858580 batches: 0.0330
trigger times: 8
Loss after 950987220 batches: 0.0327
trigger times: 9
Loss after 951115860 batches: 0.0309
trigger times: 10
Loss after 951244500 batches: 0.0292
trigger times: 11
Loss after 951373140 batches: 0.0288
trigger times: 12
Loss after 951501780 batches: 0.0281
trigger times: 13
Loss after 951630420 batches: 0.0277
trigger times: 14
Loss after 951759060 batches: 0.0266
trigger times: 15
Loss after 951887700 batches: 0.0263
trigger times: 16
Loss after 952016340 batches: 0.0260
trigger times: 17
Loss after 952144980 batches: 0.0253
trigger times: 18
Loss after 952273620 batches: 0.0247
trigger times: 19
Loss after 952402260 batches: 0.0240
trigger times: 20
Early stopping!
Start to test process.
Loss after 952530900 batches: 0.0245
Time to train on one home:  169.48598408699036
trigger times: 0
Loss after 952662000 batches: 0.6069
trigger times: 1
Loss after 952793100 batches: 0.2637
trigger times: 2
Loss after 952924200 batches: 0.1527
trigger times: 3
Loss after 953055300 batches: 0.1178
trigger times: 4
Loss after 953186400 batches: 0.1009
trigger times: 5
Loss after 953317500 batches: 0.0901
trigger times: 6
Loss after 953448600 batches: 0.0828
trigger times: 7
Loss after 953579700 batches: 0.0778
trigger times: 8
Loss after 953710800 batches: 0.0733
trigger times: 9
Loss after 953841900 batches: 0.0699
trigger times: 10
Loss after 953973000 batches: 0.0674
trigger times: 11
Loss after 954104100 batches: 0.0646
trigger times: 12
Loss after 954235200 batches: 0.0629
trigger times: 13
Loss after 954366300 batches: 0.0613
trigger times: 14
Loss after 954497400 batches: 0.0601
trigger times: 15
Loss after 954628500 batches: 0.0579
trigger times: 16
Loss after 954759600 batches: 0.0566
trigger times: 17
Loss after 954890700 batches: 0.0556
trigger times: 18
Loss after 955021800 batches: 0.0546
trigger times: 19
Loss after 955152900 batches: 0.0526
trigger times: 20
Early stopping!
Start to test process.
Loss after 955284000 batches: 0.0528
Time to train on one home:  165.20579051971436
trigger times: 0
Loss after 955415100 batches: 0.6154
trigger times: 1
Loss after 955546200 batches: 0.3335
trigger times: 2
Loss after 955677300 batches: 0.2240
trigger times: 3
Loss after 955808400 batches: 0.1608
trigger times: 4
Loss after 955939500 batches: 0.1374
trigger times: 5
Loss after 956070600 batches: 0.1135
trigger times: 6
Loss after 956201700 batches: 0.1026
trigger times: 7
Loss after 956332800 batches: 0.0938
trigger times: 8
Loss after 956463900 batches: 0.0884
trigger times: 9
Loss after 956595000 batches: 0.0833
trigger times: 10
Loss after 956726100 batches: 0.0811
trigger times: 11
Loss after 956857200 batches: 0.0760
trigger times: 12
Loss after 956988300 batches: 0.0719
trigger times: 13
Loss after 957119400 batches: 0.0708
trigger times: 14
Loss after 957250500 batches: 0.0678
trigger times: 15
Loss after 957381600 batches: 0.0659
trigger times: 16
Loss after 957512700 batches: 0.0665
trigger times: 17
Loss after 957643800 batches: 0.0632
trigger times: 18
Loss after 957774900 batches: 0.0627
trigger times: 19
Loss after 957906000 batches: 0.0629
trigger times: 20
Early stopping!
Start to test process.
Loss after 958037100 batches: 0.0582
Time to train on one home:  164.74203157424927
trigger times: 0
Loss after 958168200 batches: 0.1752
trigger times: 1
Loss after 958299300 batches: 0.0626
trigger times: 2
Loss after 958430400 batches: 0.0414
trigger times: 3
Loss after 958561500 batches: 0.0342
trigger times: 4
Loss after 958692600 batches: 0.0302
trigger times: 5
Loss after 958823700 batches: 0.0274
trigger times: 6
Loss after 958954800 batches: 0.0257
trigger times: 7
Loss after 959085900 batches: 0.0241
trigger times: 8
Loss after 959217000 batches: 0.0230
trigger times: 9
Loss after 959348100 batches: 0.0220
trigger times: 10
Loss after 959479200 batches: 0.0215
trigger times: 11
Loss after 959610300 batches: 0.0207
trigger times: 12
Loss after 959741400 batches: 0.0202
trigger times: 13
Loss after 959872500 batches: 0.0194
trigger times: 14
Loss after 960003600 batches: 0.0188
trigger times: 15
Loss after 960134700 batches: 0.0190
trigger times: 16
Loss after 960265800 batches: 0.0186
trigger times: 17
Loss after 960396900 batches: 0.0178
trigger times: 18
Loss after 960528000 batches: 0.0176
trigger times: 19
Loss after 960659100 batches: 0.0174
trigger times: 20
Early stopping!
Start to test process.
Loss after 960790200 batches: 0.0172
Time to train on one home:  164.62774419784546
trigger times: 0
Loss after 960921300 batches: 0.2022
trigger times: 0
Loss after 961052400 batches: 0.0649
trigger times: 0
Loss after 961183500 batches: 0.0458
trigger times: 0
Loss after 961314600 batches: 0.0374
trigger times: 1
Loss after 961445700 batches: 0.0334
trigger times: 2
Loss after 961576800 batches: 0.0306
trigger times: 3
Loss after 961707900 batches: 0.0283
trigger times: 4
Loss after 961839000 batches: 0.0268
trigger times: 5
Loss after 961970100 batches: 0.0257
trigger times: 6
Loss after 962101200 batches: 0.0246
trigger times: 7
Loss after 962232300 batches: 0.0241
trigger times: 8
Loss after 962363400 batches: 0.0225
trigger times: 9
Loss after 962494500 batches: 0.0226
trigger times: 10
Loss after 962625600 batches: 0.0224
trigger times: 11
Loss after 962756700 batches: 0.0217
trigger times: 12
Loss after 962887800 batches: 0.0206
trigger times: 13
Loss after 963018900 batches: 0.0201
trigger times: 14
Loss after 963150000 batches: 0.0199
trigger times: 15
Loss after 963281100 batches: 0.0197
trigger times: 16
Loss after 963412200 batches: 0.0193
trigger times: 17
Loss after 963543300 batches: 0.0191
trigger times: 18
Loss after 963674400 batches: 0.0187
trigger times: 19
Loss after 963805500 batches: 0.0181
trigger times: 20
Early stopping!
Start to test process.
Loss after 963936600 batches: 0.0181
Time to train on one home:  187.18897199630737
trigger times: 0
Loss after 964015200 batches: 0.4787
trigger times: 1
Loss after 964093800 batches: 0.1759
trigger times: 2
Loss after 964172400 batches: 0.1047
trigger times: 3
Loss after 964251000 batches: 0.0794
trigger times: 4
Loss after 964329600 batches: 0.0671
trigger times: 5
Loss after 964408200 batches: 0.0598
trigger times: 6
Loss after 964486800 batches: 0.0543
trigger times: 7
Loss after 964565400 batches: 0.0507
trigger times: 8
Loss after 964644000 batches: 0.0481
trigger times: 9
Loss after 964722600 batches: 0.0457
trigger times: 10
Loss after 964801200 batches: 0.0447
trigger times: 11
Loss after 964879800 batches: 0.0431
trigger times: 12
Loss after 964958400 batches: 0.0409
trigger times: 13
Loss after 965037000 batches: 0.0400
trigger times: 14
Loss after 965115600 batches: 0.0387
trigger times: 15
Loss after 965194200 batches: 0.0372
trigger times: 16
Loss after 965272800 batches: 0.0367
trigger times: 17
Loss after 965351400 batches: 0.0355
trigger times: 18
Loss after 965430000 batches: 0.0349
trigger times: 19
Loss after 965508600 batches: 0.0351
trigger times: 20
Early stopping!
Start to test process.
Loss after 965587200 batches: 0.0350
Time to train on one home:  110.75477957725525
trigger times: 0
Loss after 965718300 batches: 0.1565
trigger times: 0
Loss after 965849400 batches: 0.0536
trigger times: 1
Loss after 965980500 batches: 0.0387
trigger times: 0
Loss after 966111600 batches: 0.0338
trigger times: 0
Loss after 966242700 batches: 0.0298
trigger times: 1
Loss after 966373800 batches: 0.0273
trigger times: 2
Loss after 966504900 batches: 0.0259
trigger times: 0
Loss after 966636000 batches: 0.0247
trigger times: 0
Loss after 966767100 batches: 0.0233
trigger times: 1
Loss after 966898200 batches: 0.0224
trigger times: 2
Loss after 967029300 batches: 0.0221
trigger times: 0
Loss after 967160400 batches: 0.0211
trigger times: 0
Loss after 967291500 batches: 0.0205
trigger times: 0
Loss after 967422600 batches: 0.0199
trigger times: 1
Loss after 967553700 batches: 0.0195
trigger times: 2
Loss after 967684800 batches: 0.0194
trigger times: 3
Loss after 967815900 batches: 0.0187
trigger times: 4
Loss after 967947000 batches: 0.0182
trigger times: 5
Loss after 968078100 batches: 0.0180
trigger times: 6
Loss after 968209200 batches: 0.0177
trigger times: 7
Loss after 968340300 batches: 0.0174
trigger times: 8
Loss after 968471400 batches: 0.0172
trigger times: 9
Loss after 968602500 batches: 0.0172
trigger times: 10
Loss after 968733600 batches: 0.0168
trigger times: 11
Loss after 968864700 batches: 0.0167
trigger times: 12
Loss after 968995800 batches: 0.0164
trigger times: 13
Loss after 969126900 batches: 0.0159
trigger times: 0
Loss after 969258000 batches: 0.0159
trigger times: 1
Loss after 969389100 batches: 0.0156
trigger times: 2
Loss after 969520200 batches: 0.0150
trigger times: 3
Loss after 969651300 batches: 0.0152
trigger times: 4
Loss after 969782400 batches: 0.0148
trigger times: 5
Loss after 969913500 batches: 0.0153
trigger times: 6
Loss after 970044600 batches: 0.0149
trigger times: 7
Loss after 970175700 batches: 0.0148
trigger times: 8
Loss after 970306800 batches: 0.0146
trigger times: 9
Loss after 970437900 batches: 0.0146
trigger times: 10
Loss after 970569000 batches: 0.0143
trigger times: 11
Loss after 970700100 batches: 0.0142
trigger times: 12
Loss after 970831200 batches: 0.0144
trigger times: 13
Loss after 970962300 batches: 0.0142
trigger times: 14
Loss after 971093400 batches: 0.0138
trigger times: 0
Loss after 971224500 batches: 0.0138
trigger times: 1
Loss after 971355600 batches: 0.0139
trigger times: 2
Loss after 971486700 batches: 0.0136
trigger times: 3
Loss after 971617800 batches: 0.0137
trigger times: 4
Loss after 971748900 batches: 0.0136
trigger times: 5
Loss after 971880000 batches: 0.0132
trigger times: 6
Loss after 972011100 batches: 0.0133
trigger times: 7
Loss after 972142200 batches: 0.0132
trigger times: 8
Loss after 972273300 batches: 0.0130
trigger times: 9
Loss after 972404400 batches: 0.0128
trigger times: 10
Loss after 972535500 batches: 0.0129
trigger times: 11
Loss after 972666600 batches: 0.0127
trigger times: 12
Loss after 972797700 batches: 0.0126
trigger times: 13
Loss after 972928800 batches: 0.0125
trigger times: 14
Loss after 973059900 batches: 0.0126
trigger times: 0
Loss after 973191000 batches: 0.0126
trigger times: 1
Loss after 973322100 batches: 0.0124
trigger times: 2
Loss after 973453200 batches: 0.0125
trigger times: 0
Loss after 973584300 batches: 0.0122
trigger times: 1
Loss after 973715400 batches: 0.0122
trigger times: 0
Loss after 973846500 batches: 0.0122
trigger times: 1
Loss after 973977600 batches: 0.0122
trigger times: 2
Loss after 974108700 batches: 0.0121
trigger times: 3
Loss after 974239800 batches: 0.0119
trigger times: 4
Loss after 974370900 batches: 0.0118
trigger times: 5
Loss after 974502000 batches: 0.0117
trigger times: 6
Loss after 974633100 batches: 0.0118
trigger times: 7
Loss after 974764200 batches: 0.0118
trigger times: 8
Loss after 974895300 batches: 0.0117
trigger times: 9
Loss after 975026400 batches: 0.0114
trigger times: 10
Loss after 975157500 batches: 0.0115
trigger times: 11
Loss after 975288600 batches: 0.0113
trigger times: 12
Loss after 975419700 batches: 0.0115
trigger times: 13
Loss after 975550800 batches: 0.0111
trigger times: 14
Loss after 975681900 batches: 0.0111
trigger times: 15
Loss after 975813000 batches: 0.0112
trigger times: 16
Loss after 975944100 batches: 0.0112
trigger times: 17
Loss after 976075200 batches: 0.0112
trigger times: 18
Loss after 976206300 batches: 0.0110
trigger times: 19
Loss after 976337400 batches: 0.0110
trigger times: 20
Early stopping!
Start to test process.
Loss after 976468500 batches: 0.0110
Time to train on one home:  613.4915719032288
trigger times: 0
Loss after 976599600 batches: 0.2251
trigger times: 0
Loss after 976730700 batches: 0.0766
trigger times: 0
Loss after 976861800 batches: 0.0537
trigger times: 1
Loss after 976992900 batches: 0.0447
trigger times: 2
Loss after 977124000 batches: 0.0410
trigger times: 3
Loss after 977255100 batches: 0.0377
trigger times: 4
Loss after 977386200 batches: 0.0356
trigger times: 5
Loss after 977517300 batches: 0.0335
trigger times: 6
Loss after 977648400 batches: 0.0328
trigger times: 7
Loss after 977779500 batches: 0.0311
trigger times: 8
Loss after 977910600 batches: 0.0299
trigger times: 9
Loss after 978041700 batches: 0.0294
trigger times: 10
Loss after 978172800 batches: 0.0282
trigger times: 11
Loss after 978303900 batches: 0.0273
trigger times: 12
Loss after 978435000 batches: 0.0269
trigger times: 13
Loss after 978566100 batches: 0.0264
trigger times: 14
Loss after 978697200 batches: 0.0258
trigger times: 15
Loss after 978828300 batches: 0.0251
trigger times: 16
Loss after 978959400 batches: 0.0251
trigger times: 17
Loss after 979090500 batches: 0.0247
trigger times: 18
Loss after 979221600 batches: 0.0243
trigger times: 19
Loss after 979352700 batches: 0.0237
trigger times: 20
Early stopping!
Start to test process.
Loss after 979483800 batches: 0.0234
Time to train on one home:  178.88850378990173
trigger times: 0
Loss after 979614900 batches: 0.3844
trigger times: 1
Loss after 979746000 batches: 0.1419
trigger times: 0
Loss after 979877100 batches: 0.0971
trigger times: 0
Loss after 980008200 batches: 0.0792
trigger times: 1
Loss after 980139300 batches: 0.0660
trigger times: 0
Loss after 980270400 batches: 0.0623
trigger times: 0
Loss after 980401500 batches: 0.0568
trigger times: 1
Loss after 980532600 batches: 0.0533
trigger times: 0
Loss after 980663700 batches: 0.0513
trigger times: 0
Loss after 980794800 batches: 0.0486
trigger times: 0
Loss after 980925900 batches: 0.0473
trigger times: 1
Loss after 981057000 batches: 0.0443
trigger times: 2
Loss after 981188100 batches: 0.0424
trigger times: 3
Loss after 981319200 batches: 0.0429
trigger times: 0
Loss after 981450300 batches: 0.0407
trigger times: 1
Loss after 981581400 batches: 0.0402
trigger times: 0
Loss after 981712500 batches: 0.0385
trigger times: 1
Loss after 981843600 batches: 0.0377
trigger times: 2
Loss after 981974700 batches: 0.0384
trigger times: 3
Loss after 982105800 batches: 0.0380
trigger times: 0
Loss after 982236900 batches: 0.0358
trigger times: 1
Loss after 982368000 batches: 0.0358
trigger times: 2
Loss after 982499100 batches: 0.0344
trigger times: 3
Loss after 982630200 batches: 0.0339
trigger times: 4
Loss after 982761300 batches: 0.0339
trigger times: 5
Loss after 982892400 batches: 0.0341
trigger times: 0
Loss after 983023500 batches: 0.0325
trigger times: 1
Loss after 983154600 batches: 0.0321
trigger times: 2
Loss after 983285700 batches: 0.0321
trigger times: 3
Loss after 983416800 batches: 0.0330
trigger times: 4
Loss after 983547900 batches: 0.0313
trigger times: 5
Loss after 983679000 batches: 0.0309
trigger times: 6
Loss after 983810100 batches: 0.0313
trigger times: 0
Loss after 983941200 batches: 0.0307
trigger times: 1
Loss after 984072300 batches: 0.0302
trigger times: 2
Loss after 984203400 batches: 0.0292
trigger times: 3
Loss after 984334500 batches: 0.0299
trigger times: 4
Loss after 984465600 batches: 0.0296
trigger times: 0
Loss after 984596700 batches: 0.0288
trigger times: 1
Loss after 984727800 batches: 0.0286
trigger times: 2
Loss after 984858900 batches: 0.0284
trigger times: 3
Loss after 984990000 batches: 0.0287
trigger times: 0
Loss after 985121100 batches: 0.0282
trigger times: 1
Loss after 985252200 batches: 0.0285
trigger times: 0
Loss after 985383300 batches: 0.0277
trigger times: 0
Loss after 985514400 batches: 0.0279
trigger times: 1
Loss after 985645500 batches: 0.0278
trigger times: 2
Loss after 985776600 batches: 0.0266
trigger times: 3
Loss after 985907700 batches: 0.0264
trigger times: 4
Loss after 986038800 batches: 0.0266
trigger times: 5
Loss after 986169900 batches: 0.0262
trigger times: 6
Loss after 986301000 batches: 0.0255
trigger times: 7
Loss after 986432100 batches: 0.0269
trigger times: 8
Loss after 986563200 batches: 0.0265
trigger times: 9
Loss after 986694300 batches: 0.0260
trigger times: 10
Loss after 986825400 batches: 0.0254
trigger times: 0
Loss after 986956500 batches: 0.0251
trigger times: 0
Loss after 987087600 batches: 0.0251
trigger times: 1
Loss after 987218700 batches: 0.0245
trigger times: 2
Loss after 987349800 batches: 0.0247
trigger times: 0
Loss after 987480900 batches: 0.0247
trigger times: 1
Loss after 987612000 batches: 0.0246
trigger times: 2
Loss after 987743100 batches: 0.0248
trigger times: 3
Loss after 987874200 batches: 0.0252
trigger times: 0
Loss after 988005300 batches: 0.0249
trigger times: 1
Loss after 988136400 batches: 0.0241
trigger times: 0
Loss after 988267500 batches: 0.0240
trigger times: 1
Loss after 988398600 batches: 0.0234
trigger times: 2
Loss after 988529700 batches: 0.0238
trigger times: 3
Loss after 988660800 batches: 0.0231
trigger times: 4
Loss after 988791900 batches: 0.0229
trigger times: 5
Loss after 988923000 batches: 0.0234
trigger times: 6
Loss after 989054100 batches: 0.0231
trigger times: 7
Loss after 989185200 batches: 0.0235
trigger times: 8
Loss after 989316300 batches: 0.0233
trigger times: 9
Loss after 989447400 batches: 0.0233
trigger times: 10
Loss after 989578500 batches: 0.0231
trigger times: 11
Loss after 989709600 batches: 0.0232
trigger times: 12
Loss after 989840700 batches: 0.0225
trigger times: 13
Loss after 989971800 batches: 0.0221
trigger times: 14
Loss after 990102900 batches: 0.0226
trigger times: 15
Loss after 990234000 batches: 0.0219
trigger times: 16
Loss after 990365100 batches: 0.0220
trigger times: 0
Loss after 990496200 batches: 0.0223
trigger times: 1
Loss after 990627300 batches: 0.0225
trigger times: 0
Loss after 990758400 batches: 0.0223
trigger times: 1
Loss after 990889500 batches: 0.0221
trigger times: 2
Loss after 991020600 batches: 0.0220
trigger times: 3
Loss after 991151700 batches: 0.0219
trigger times: 4
Loss after 991282800 batches: 0.0222
trigger times: 5
Loss after 991413900 batches: 0.0223
trigger times: 6
Loss after 991545000 batches: 0.0220
trigger times: 7
Loss after 991676100 batches: 0.0218
trigger times: 8
Loss after 991807200 batches: 0.0207
trigger times: 9
Loss after 991938300 batches: 0.0219
trigger times: 10
Loss after 992069400 batches: 0.0214
trigger times: 11
Loss after 992200500 batches: 0.0216
trigger times: 12
Loss after 992331600 batches: 0.0217
trigger times: 13
Loss after 992462700 batches: 0.0211
trigger times: 14
Loss after 992593800 batches: 0.0210
trigger times: 15
Loss after 992724900 batches: 0.0209
trigger times: 16
Loss after 992856000 batches: 0.0215
trigger times: 17
Loss after 992987100 batches: 0.0208
trigger times: 18
Loss after 993118200 batches: 0.0207
trigger times: 19
Loss after 993249300 batches: 0.0211
trigger times: 20
Early stopping!
Start to test process.
Loss after 993380400 batches: 0.0205
Time to train on one home:  782.4114005565643
trigger times: 0
Loss after 993511500 batches: 0.4377
trigger times: 1
Loss after 993642600 batches: 0.1996
trigger times: 2
Loss after 993773700 batches: 0.1323
trigger times: 3
Loss after 993904800 batches: 0.1065
trigger times: 4
Loss after 994035900 batches: 0.0932
trigger times: 5
Loss after 994167000 batches: 0.0833
trigger times: 6
Loss after 994298100 batches: 0.0775
trigger times: 7
Loss after 994429200 batches: 0.0722
trigger times: 8
Loss after 994560300 batches: 0.0691
trigger times: 9
Loss after 994691400 batches: 0.0655
trigger times: 10
Loss after 994822500 batches: 0.0635
trigger times: 11
Loss after 994953600 batches: 0.0607
trigger times: 12
Loss after 995084700 batches: 0.0594
trigger times: 13
Loss after 995215800 batches: 0.0582
trigger times: 14
Loss after 995346900 batches: 0.0565
trigger times: 15
Loss after 995478000 batches: 0.0549
trigger times: 16
Loss after 995609100 batches: 0.0527
trigger times: 17
Loss after 995740200 batches: 0.0524
trigger times: 18
Loss after 995871300 batches: 0.0517
trigger times: 19
Loss after 996002400 batches: 0.0505
trigger times: 20
Early stopping!
Start to test process.
Loss after 996133500 batches: 0.0497
Time to train on one home:  165.36210680007935
trigger times: 0
Loss after 996264600 batches: 0.3172
trigger times: 0
Loss after 996395700 batches: 0.1159
trigger times: 0
Loss after 996526800 batches: 0.0811
trigger times: 0
Loss after 996657900 batches: 0.0661
trigger times: 1
Loss after 996789000 batches: 0.0586
trigger times: 2
Loss after 996920100 batches: 0.0531
trigger times: 3
Loss after 997051200 batches: 0.0500
trigger times: 0
Loss after 997182300 batches: 0.0472
trigger times: 0
Loss after 997313400 batches: 0.0433
trigger times: 0
Loss after 997444500 batches: 0.0415
trigger times: 1
Loss after 997575600 batches: 0.0408
trigger times: 0
Loss after 997706700 batches: 0.0400
trigger times: 1
Loss after 997837800 batches: 0.0388
trigger times: 2
Loss after 997968900 batches: 0.0372
trigger times: 3
Loss after 998100000 batches: 0.0356
trigger times: 4
Loss after 998231100 batches: 0.0357
trigger times: 5
Loss after 998362200 batches: 0.0352
trigger times: 6
Loss after 998493300 batches: 0.0342
trigger times: 7
Loss after 998624400 batches: 0.0331
trigger times: 8
Loss after 998755500 batches: 0.0319
trigger times: 9
Loss after 998886600 batches: 0.0328
trigger times: 10
Loss after 999017700 batches: 0.0315
trigger times: 11
Loss after 999148800 batches: 0.0315
trigger times: 12
Loss after 999279900 batches: 0.0308
trigger times: 13
Loss after 999411000 batches: 0.0301
trigger times: 0
Loss after 999542100 batches: 0.0296
trigger times: 1
Loss after 999673200 batches: 0.0305
trigger times: 2
Loss after 999804300 batches: 0.0294
trigger times: 3
Loss after 999935400 batches: 0.0292
trigger times: 4
Loss after 1000066500 batches: 0.0288
trigger times: 5
Loss after 1000197600 batches: 0.0290
trigger times: 6
Loss after 1000328700 batches: 0.0280
trigger times: 7
Loss after 1000459800 batches: 0.0277
trigger times: 8
Loss after 1000590900 batches: 0.0273
trigger times: 9
Loss after 1000722000 batches: 0.0273
trigger times: 10
Loss after 1000853100 batches: 0.0269
trigger times: 11
Loss after 1000984200 batches: 0.0267
trigger times: 12
Loss after 1001115300 batches: 0.0264
trigger times: 13
Loss after 1001246400 batches: 0.0258
trigger times: 14
Loss after 1001377500 batches: 0.0257
trigger times: 15
Loss after 1001508600 batches: 0.0258
trigger times: 16
Loss after 1001639700 batches: 0.0252
trigger times: 17
Loss after 1001770800 batches: 0.0253
trigger times: 18
Loss after 1001901900 batches: 0.0264
trigger times: 19
Loss after 1002033000 batches: 0.0253
trigger times: 20
Early stopping!
Start to test process.
Loss after 1002164100 batches: 0.0250
Time to train on one home:  346.00938606262207
trigger times: 0
Loss after 1002295200 batches: 0.5185
trigger times: 0
Loss after 1002426300 batches: 0.2712
trigger times: 1
Loss after 1002557400 batches: 0.1686
trigger times: 0
Loss after 1002688500 batches: 0.1225
trigger times: 1
Loss after 1002819600 batches: 0.1004
trigger times: 2
Loss after 1002950700 batches: 0.0888
trigger times: 3
Loss after 1003081800 batches: 0.0810
trigger times: 4
Loss after 1003212900 batches: 0.0740
trigger times: 5
Loss after 1003344000 batches: 0.0689
trigger times: 6
Loss after 1003475100 batches: 0.0646
trigger times: 7
Loss after 1003606200 batches: 0.0616
trigger times: 8
Loss after 1003737300 batches: 0.0584
trigger times: 9
Loss after 1003868400 batches: 0.0557
trigger times: 10
Loss after 1003999500 batches: 0.0539
trigger times: 11
Loss after 1004130600 batches: 0.0527
trigger times: 12
Loss after 1004261700 batches: 0.0516
trigger times: 13
Loss after 1004392800 batches: 0.0503
trigger times: 14
Loss after 1004523900 batches: 0.0490
trigger times: 15
Loss after 1004655000 batches: 0.0476
trigger times: 16
Loss after 1004786100 batches: 0.0467
trigger times: 17
Loss after 1004917200 batches: 0.0460
trigger times: 18
Loss after 1005048300 batches: 0.0449
trigger times: 19
Loss after 1005179400 batches: 0.0439
trigger times: 20
Early stopping!
Start to test process.
Loss after 1005310500 batches: 0.0432
Time to train on one home:  186.77949404716492
trigger times: 0
Loss after 1005441600 batches: 0.7076
trigger times: 1
Loss after 1005572700 batches: 0.3308
trigger times: 2
Loss after 1005703800 batches: 0.1859
trigger times: 3
Loss after 1005834900 batches: 0.1447
trigger times: 4
Loss after 1005966000 batches: 0.1222
trigger times: 5
Loss after 1006097100 batches: 0.1095
trigger times: 6
Loss after 1006228200 batches: 0.1004
trigger times: 7
Loss after 1006359300 batches: 0.0920
trigger times: 8
Loss after 1006490400 batches: 0.0880
trigger times: 9
Loss after 1006621500 batches: 0.0830
trigger times: 10
Loss after 1006752600 batches: 0.0797
trigger times: 11
Loss after 1006883700 batches: 0.0766
trigger times: 12
Loss after 1007014800 batches: 0.0754
trigger times: 13
Loss after 1007145900 batches: 0.0726
trigger times: 14
Loss after 1007277000 batches: 0.0689
trigger times: 15
Loss after 1007408100 batches: 0.0681
trigger times: 16
Loss after 1007539200 batches: 0.0665
trigger times: 17
Loss after 1007670300 batches: 0.0644
trigger times: 18
Loss after 1007801400 batches: 0.0646
trigger times: 19
Loss after 1007932500 batches: 0.0624
trigger times: 20
Early stopping!
Start to test process.
Loss after 1008063600 batches: 0.0608
Time to train on one home:  164.92355275154114
trigger times: 0
Loss after 1008157560 batches: 0.6337
trigger times: 1
Loss after 1008251520 batches: 0.3124
trigger times: 2
Loss after 1008345480 batches: 0.1774
trigger times: 3
Loss after 1008439440 batches: 0.1293
trigger times: 4
Loss after 1008533400 batches: 0.1072
trigger times: 5
Loss after 1008627360 batches: 0.0963
trigger times: 6
Loss after 1008721320 batches: 0.0882
trigger times: 7
Loss after 1008815280 batches: 0.0806
trigger times: 8
Loss after 1008909240 batches: 0.0757
trigger times: 9
Loss after 1009003200 batches: 0.0730
trigger times: 10
Loss after 1009097160 batches: 0.0685
trigger times: 11
Loss after 1009191120 batches: 0.0666
trigger times: 12
Loss after 1009285080 batches: 0.0636
trigger times: 13
Loss after 1009379040 batches: 0.0632
trigger times: 14
Loss after 1009473000 batches: 0.0607
trigger times: 15
Loss after 1009566960 batches: 0.0587
trigger times: 16
Loss after 1009660920 batches: 0.0571
trigger times: 17
Loss after 1009754880 batches: 0.0562
trigger times: 0
Loss after 1009848840 batches: 0.0547
trigger times: 0
Loss after 1009942800 batches: 0.0531
trigger times: 1
Loss after 1010036760 batches: 0.0522
trigger times: 2
Loss after 1010130720 batches: 0.0505
trigger times: 3
Loss after 1010224680 batches: 0.0504
trigger times: 4
Loss after 1010318640 batches: 0.0502
trigger times: 0
Loss after 1010412600 batches: 0.0487
trigger times: 1
Loss after 1010506560 batches: 0.0483
trigger times: 2
Loss after 1010600520 batches: 0.0475
trigger times: 3
Loss after 1010694480 batches: 0.0467
trigger times: 4
Loss after 1010788440 batches: 0.0459
trigger times: 5
Loss after 1010882400 batches: 0.0456
trigger times: 6
Loss after 1010976360 batches: 0.0451
trigger times: 7
Loss after 1011070320 batches: 0.0451
trigger times: 8
Loss after 1011164280 batches: 0.0447
trigger times: 0
Loss after 1011258240 batches: 0.0441
trigger times: 1
Loss after 1011352200 batches: 0.0433
trigger times: 2
Loss after 1011446160 batches: 0.0427
trigger times: 3
Loss after 1011540120 batches: 0.0419
trigger times: 0
Loss after 1011634080 batches: 0.0419
trigger times: 1
Loss after 1011728040 batches: 0.0425
trigger times: 0
Loss after 1011822000 batches: 0.0413
trigger times: 1
Loss after 1011915960 batches: 0.0404
trigger times: 2
Loss after 1012009920 batches: 0.0400
trigger times: 3
Loss after 1012103880 batches: 0.0408
trigger times: 4
Loss after 1012197840 batches: 0.0399
trigger times: 5
Loss after 1012291800 batches: 0.0402
trigger times: 6
Loss after 1012385760 batches: 0.0399
trigger times: 0
Loss after 1012479720 batches: 0.0389
trigger times: 1
Loss after 1012573680 batches: 0.0387
trigger times: 2
Loss after 1012667640 batches: 0.0388
trigger times: 3
Loss after 1012761600 batches: 0.0383
trigger times: 4
Loss after 1012855560 batches: 0.0383
trigger times: 5
Loss after 1012949520 batches: 0.0369
trigger times: 6
Loss after 1013043480 batches: 0.0371
trigger times: 7
Loss after 1013137440 batches: 0.0372
trigger times: 8
Loss after 1013231400 batches: 0.0363
trigger times: 9
Loss after 1013325360 batches: 0.0369
trigger times: 10
Loss after 1013419320 batches: 0.0363
trigger times: 11
Loss after 1013513280 batches: 0.0360
trigger times: 12
Loss after 1013607240 batches: 0.0355
trigger times: 13
Loss after 1013701200 batches: 0.0353
trigger times: 14
Loss after 1013795160 batches: 0.0352
trigger times: 0
Loss after 1013889120 batches: 0.0355
trigger times: 1
Loss after 1013983080 batches: 0.0349
trigger times: 2
Loss after 1014077040 batches: 0.0352
trigger times: 3
Loss after 1014171000 batches: 0.0344
trigger times: 4
Loss after 1014264960 batches: 0.0341
trigger times: 5
Loss after 1014358920 batches: 0.0349
trigger times: 6
Loss after 1014452880 batches: 0.0337
trigger times: 7
Loss after 1014546840 batches: 0.0339
trigger times: 8
Loss after 1014640800 batches: 0.0335
trigger times: 9
Loss after 1014734760 batches: 0.0328
trigger times: 10
Loss after 1014828720 batches: 0.0325
trigger times: 11
Loss after 1014922680 batches: 0.0326
trigger times: 12
Loss after 1015016640 batches: 0.0329
trigger times: 13
Loss after 1015110600 batches: 0.0324
trigger times: 14
Loss after 1015204560 batches: 0.0324
trigger times: 15
Loss after 1015298520 batches: 0.0321
trigger times: 16
Loss after 1015392480 batches: 0.0325
trigger times: 17
Loss after 1015486440 batches: 0.0324
trigger times: 18
Loss after 1015580400 batches: 0.0327
trigger times: 19
Loss after 1015674360 batches: 0.0319
trigger times: 20
Early stopping!
Start to test process.
Loss after 1015768320 batches: 0.0315
Time to train on one home:  456.94593691825867
trigger times: 0
Loss after 1015899420 batches: 0.0667
trigger times: 0
Loss after 1016030520 batches: 0.0160
trigger times: 1
Loss after 1016161620 batches: 0.0116
trigger times: 0
Loss after 1016292720 batches: 0.0098
trigger times: 0
Loss after 1016423820 batches: 0.0088
trigger times: 0
Loss after 1016554920 batches: 0.0078
trigger times: 0
Loss after 1016686020 batches: 0.0073
trigger times: 1
Loss after 1016817120 batches: 0.0066
trigger times: 0
Loss after 1016948220 batches: 0.0065
trigger times: 1
Loss after 1017079320 batches: 0.0059
trigger times: 2
Loss after 1017210420 batches: 0.0057
trigger times: 3
Loss after 1017341520 batches: 0.0054
trigger times: 4
Loss after 1017472620 batches: 0.0053
trigger times: 0
Loss after 1017603720 batches: 0.0053
trigger times: 1
Loss after 1017734820 batches: 0.0051
trigger times: 0
Loss after 1017865920 batches: 0.0049
trigger times: 1
Loss after 1017997020 batches: 0.0049
trigger times: 2
Loss after 1018128120 batches: 0.0047
trigger times: 0
Loss after 1018259220 batches: 0.0045
trigger times: 0
Loss after 1018390320 batches: 0.0044
trigger times: 1
Loss after 1018521420 batches: 0.0042
trigger times: 2
Loss after 1018652520 batches: 0.0042
trigger times: 3
Loss after 1018783620 batches: 0.0041
trigger times: 4
Loss after 1018914720 batches: 0.0041
trigger times: 5
Loss after 1019045820 batches: 0.0041
trigger times: 6
Loss after 1019176920 batches: 0.0040
trigger times: 7
Loss after 1019308020 batches: 0.0039
trigger times: 0
Loss after 1019439120 batches: 0.0039
trigger times: 1
Loss after 1019570220 batches: 0.0038
trigger times: 2
Loss after 1019701320 batches: 0.0038
trigger times: 3
Loss after 1019832420 batches: 0.0036
trigger times: 4
Loss after 1019963520 batches: 0.0037
trigger times: 5
Loss after 1020094620 batches: 0.0035
trigger times: 6
Loss after 1020225720 batches: 0.0036
trigger times: 7
Loss after 1020356820 batches: 0.0034
trigger times: 8
Loss after 1020487920 batches: 0.0035
trigger times: 9
Loss after 1020619020 batches: 0.0034
trigger times: 10
Loss after 1020750120 batches: 0.0034
trigger times: 11
Loss after 1020881220 batches: 0.0033
trigger times: 12
Loss after 1021012320 batches: 0.0034
trigger times: 13
Loss after 1021143420 batches: 0.0033
trigger times: 14
Loss after 1021274520 batches: 0.0033
trigger times: 15
Loss after 1021405620 batches: 0.0032
trigger times: 16
Loss after 1021536720 batches: 0.0031
trigger times: 17
Loss after 1021667820 batches: 0.0031
trigger times: 18
Loss after 1021798920 batches: 0.0031
trigger times: 19
Loss after 1021930020 batches: 0.0031
trigger times: 20
Early stopping!
Start to test process.
Loss after 1022061120 batches: 0.0032
Time to train on one home:  360.1382405757904
trigger times: 0
Loss after 1022192220 batches: 0.1842
trigger times: 1
Loss after 1022323320 batches: 0.0742
trigger times: 2
Loss after 1022454420 batches: 0.0517
trigger times: 3
Loss after 1022585520 batches: 0.0431
trigger times: 4
Loss after 1022716620 batches: 0.0379
trigger times: 5
Loss after 1022847720 batches: 0.0345
trigger times: 6
Loss after 1022978820 batches: 0.0328
trigger times: 7
Loss after 1023109920 batches: 0.0306
trigger times: 0
Loss after 1023241020 batches: 0.0286
trigger times: 1
Loss after 1023372120 batches: 0.0274
trigger times: 2
Loss after 1023503220 batches: 0.0261
trigger times: 3
Loss after 1023634320 batches: 0.0252
trigger times: 4
Loss after 1023765420 batches: 0.0250
trigger times: 5
Loss after 1023896520 batches: 0.0238
trigger times: 6
Loss after 1024027620 batches: 0.0230
trigger times: 7
Loss after 1024158720 batches: 0.0226
trigger times: 8
Loss after 1024289820 batches: 0.0219
trigger times: 9
Loss after 1024420920 batches: 0.0210
trigger times: 0
Loss after 1024552020 batches: 0.0210
trigger times: 0
Loss after 1024683120 batches: 0.0201
trigger times: 1
Loss after 1024814220 batches: 0.0203
trigger times: 2
Loss after 1024945320 batches: 0.0195
trigger times: 0
Loss after 1025076420 batches: 0.0191
trigger times: 0
Loss after 1025207520 batches: 0.0189
trigger times: 1
Loss after 1025338620 batches: 0.0183
trigger times: 0
Loss after 1025469720 batches: 0.0181
trigger times: 0
Loss after 1025600820 batches: 0.0179
trigger times: 1
Loss after 1025731920 batches: 0.0179
trigger times: 0
Loss after 1025863020 batches: 0.0176
trigger times: 1
Loss after 1025994120 batches: 0.0172
trigger times: 2
Loss after 1026125220 batches: 0.0169
trigger times: 3
Loss after 1026256320 batches: 0.0165
trigger times: 4
Loss after 1026387420 batches: 0.0163
trigger times: 0
Loss after 1026518520 batches: 0.0162
trigger times: 1
Loss after 1026649620 batches: 0.0162
trigger times: 0
Loss after 1026780720 batches: 0.0159
trigger times: 1
Loss after 1026911820 batches: 0.0158
trigger times: 2
Loss after 1027042920 batches: 0.0157
trigger times: 0
Loss after 1027174020 batches: 0.0155
trigger times: 1
Loss after 1027305120 batches: 0.0153
trigger times: 2
Loss after 1027436220 batches: 0.0153
trigger times: 0
Loss after 1027567320 batches: 0.0154
trigger times: 1
Loss after 1027698420 batches: 0.0149
trigger times: 2
Loss after 1027829520 batches: 0.0150
trigger times: 3
Loss after 1027960620 batches: 0.0145
trigger times: 0
Loss after 1028091720 batches: 0.0145
trigger times: 0
Loss after 1028222820 batches: 0.0143
trigger times: 1
Loss after 1028353920 batches: 0.0141
trigger times: 2
Loss after 1028485020 batches: 0.0141
trigger times: 3
Loss after 1028616120 batches: 0.0141
trigger times: 4
Loss after 1028747220 batches: 0.0140
trigger times: 5
Loss after 1028878320 batches: 0.0138
trigger times: 6
Loss after 1029009420 batches: 0.0139
trigger times: 7
Loss after 1029140520 batches: 0.0136
trigger times: 8
Loss after 1029271620 batches: 0.0135
trigger times: 0
Loss after 1029402720 batches: 0.0133
trigger times: 0
Loss after 1029533820 batches: 0.0134
trigger times: 1
Loss after 1029664920 batches: 0.0134
trigger times: 2
Loss after 1029796020 batches: 0.0130
trigger times: 3
Loss after 1029927120 batches: 0.0132
trigger times: 4
Loss after 1030058220 batches: 0.0129
trigger times: 5
Loss after 1030189320 batches: 0.0128
trigger times: 6
Loss after 1030320420 batches: 0.0125
trigger times: 7
Loss after 1030451520 batches: 0.0128
trigger times: 8
Loss after 1030582620 batches: 0.0127
trigger times: 9
Loss after 1030713720 batches: 0.0131
trigger times: 10
Loss after 1030844820 batches: 0.0127
trigger times: 0
Loss after 1030975920 batches: 0.0125
trigger times: 1
Loss after 1031107020 batches: 0.0126
trigger times: 2
Loss after 1031238120 batches: 0.0121
trigger times: 3
Loss after 1031369220 batches: 0.0120
trigger times: 4
Loss after 1031500320 batches: 0.0122
trigger times: 5
Loss after 1031631420 batches: 0.0121
trigger times: 0
Loss after 1031762520 batches: 0.0120
trigger times: 1
Loss after 1031893620 batches: 0.0118
trigger times: 2
Loss after 1032024720 batches: 0.0117
trigger times: 3
Loss after 1032155820 batches: 0.0119
trigger times: 4
Loss after 1032286920 batches: 0.0118
trigger times: 5
Loss after 1032418020 batches: 0.0115
trigger times: 0
Loss after 1032549120 batches: 0.0115
trigger times: 1
Loss after 1032680220 batches: 0.0115
trigger times: 0
Loss after 1032811320 batches: 0.0115
trigger times: 1
Loss after 1032942420 batches: 0.0114
trigger times: 0
Loss after 1033073520 batches: 0.0123
trigger times: 1
Loss after 1033204620 batches: 0.0118
trigger times: 2
Loss after 1033335720 batches: 0.0113
trigger times: 3
Loss after 1033466820 batches: 0.0114
trigger times: 4
Loss after 1033597920 batches: 0.0111
trigger times: 5
Loss after 1033729020 batches: 0.0110
trigger times: 6
Loss after 1033860120 batches: 0.0110
trigger times: 7
Loss after 1033991220 batches: 0.0110
trigger times: 8
Loss after 1034122320 batches: 0.0110
trigger times: 9
Loss after 1034253420 batches: 0.0110
trigger times: 10
Loss after 1034384520 batches: 0.0108
trigger times: 11
Loss after 1034515620 batches: 0.0107
trigger times: 12
Loss after 1034646720 batches: 0.0109
trigger times: 13
Loss after 1034777820 batches: 0.0110
trigger times: 14
Loss after 1034908920 batches: 0.0106
trigger times: 15
Loss after 1035040020 batches: 0.0107
trigger times: 16
Loss after 1035171120 batches: 0.0107
trigger times: 17
Loss after 1035302220 batches: 0.0106
trigger times: 18
Loss after 1035433320 batches: 0.0106
trigger times: 0
Loss after 1035564420 batches: 0.0104
trigger times: 1
Loss after 1035695520 batches: 0.0106
trigger times: 2
Loss after 1035826620 batches: 0.0104
trigger times: 3
Loss after 1035957720 batches: 0.0105
trigger times: 4
Loss after 1036088820 batches: 0.0104
trigger times: 5
Loss after 1036219920 batches: 0.0102
trigger times: 6
Loss after 1036351020 batches: 0.0102
trigger times: 7
Loss after 1036482120 batches: 0.0101
trigger times: 8
Loss after 1036613220 batches: 0.0100
trigger times: 9
Loss after 1036744320 batches: 0.0101
trigger times: 10
Loss after 1036875420 batches: 0.0102
trigger times: 11
Loss after 1037006520 batches: 0.0104
trigger times: 12
Loss after 1037137620 batches: 0.0103
trigger times: 13
Loss after 1037268720 batches: 0.0098
trigger times: 14
Loss after 1037399820 batches: 0.0100
trigger times: 15
Loss after 1037530920 batches: 0.0098
trigger times: 16
Loss after 1037662020 batches: 0.0099
trigger times: 0
Loss after 1037793120 batches: 0.0098
trigger times: 1
Loss after 1037924220 batches: 0.0098
trigger times: 2
Loss after 1038055320 batches: 0.0098
trigger times: 3
Loss after 1038186420 batches: 0.0097
trigger times: 4
Loss after 1038317520 batches: 0.0096
trigger times: 5
Loss after 1038448620 batches: 0.0096
trigger times: 6
Loss after 1038579720 batches: 0.0096
trigger times: 7
Loss after 1038710820 batches: 0.0096
trigger times: 8
Loss after 1038841920 batches: 0.0096
trigger times: 9
Loss after 1038973020 batches: 0.0092
trigger times: 10
Loss after 1039104120 batches: 0.0094
trigger times: 11
Loss after 1039235220 batches: 0.0094
trigger times: 12
Loss after 1039366320 batches: 0.0094
trigger times: 13
Loss after 1039497420 batches: 0.0095
trigger times: 14
Loss after 1039628520 batches: 0.0093
trigger times: 15
Loss after 1039759620 batches: 0.0093
trigger times: 16
Loss after 1039890720 batches: 0.0094
trigger times: 17
Loss after 1040021820 batches: 0.0091
trigger times: 18
Loss after 1040152920 batches: 0.0092
trigger times: 19
Loss after 1040284020 batches: 0.0092
trigger times: 20
Early stopping!
Start to test process.
Loss after 1040415120 batches: 0.0090
Time to train on one home:  1029.2618775367737
trigger times: 0
Loss after 1040546220 batches: 0.4877
trigger times: 1
Loss after 1040677320 batches: 0.2416
trigger times: 2
Loss after 1040808420 batches: 0.1712
trigger times: 3
Loss after 1040939520 batches: 0.1402
trigger times: 4
Loss after 1041070620 batches: 0.1228
trigger times: 5
Loss after 1041201720 batches: 0.1107
trigger times: 6
Loss after 1041332820 batches: 0.1043
trigger times: 0
Loss after 1041463920 batches: 0.0985
trigger times: 1
Loss after 1041595020 batches: 0.0933
trigger times: 2
Loss after 1041726120 batches: 0.0903
trigger times: 3
Loss after 1041857220 batches: 0.0867
trigger times: 4
Loss after 1041988320 batches: 0.0833
trigger times: 5
Loss after 1042119420 batches: 0.0802
trigger times: 6
Loss after 1042250520 batches: 0.0781
trigger times: 7
Loss after 1042381620 batches: 0.0776
trigger times: 8
Loss after 1042512720 batches: 0.0743
trigger times: 9
Loss after 1042643820 batches: 0.0741
trigger times: 0
Loss after 1042774920 batches: 0.0717
trigger times: 1
Loss after 1042906020 batches: 0.0710
trigger times: 2
Loss after 1043037120 batches: 0.0701
trigger times: 3
Loss after 1043168220 batches: 0.0685
trigger times: 0
Loss after 1043299320 batches: 0.0671
trigger times: 0
Loss after 1043430420 batches: 0.0667
trigger times: 0
Loss after 1043561520 batches: 0.0647
trigger times: 1
Loss after 1043692620 batches: 0.0648
trigger times: 2
Loss after 1043823720 batches: 0.0633
trigger times: 0
Loss after 1043954820 batches: 0.0629
trigger times: 1
Loss after 1044085920 batches: 0.0620
trigger times: 0
Loss after 1044217020 batches: 0.0618
trigger times: 1
Loss after 1044348120 batches: 0.0612
trigger times: 0
Loss after 1044479220 batches: 0.0599
trigger times: 1
Loss after 1044610320 batches: 0.0598
trigger times: 0
Loss after 1044741420 batches: 0.0591
trigger times: 1
Loss after 1044872520 batches: 0.0580
trigger times: 0
Loss after 1045003620 batches: 0.0584
trigger times: 0
Loss after 1045134720 batches: 0.0586
trigger times: 1
Loss after 1045265820 batches: 0.0579
trigger times: 2
Loss after 1045396920 batches: 0.0574
trigger times: 3
Loss after 1045528020 batches: 0.0560
trigger times: 4
Loss after 1045659120 batches: 0.0557
trigger times: 5
Loss after 1045790220 batches: 0.0547
trigger times: 0
Loss after 1045921320 batches: 0.0548
trigger times: 1
Loss after 1046052420 batches: 0.0542
trigger times: 2
Loss after 1046183520 batches: 0.0544
trigger times: 3
Loss after 1046314620 batches: 0.0540
trigger times: 4
Loss after 1046445720 batches: 0.0537
trigger times: 5
Loss after 1046576820 batches: 0.0528
trigger times: 6
Loss after 1046707920 batches: 0.0531
trigger times: 7
Loss after 1046839020 batches: 0.0532
trigger times: 8
Loss after 1046970120 batches: 0.0524
trigger times: 9
Loss after 1047101220 batches: 0.0513
trigger times: 10
Loss after 1047232320 batches: 0.0525
trigger times: 11
Loss after 1047363420 batches: 0.0514
trigger times: 12
Loss after 1047494520 batches: 0.0512
trigger times: 13
Loss after 1047625620 batches: 0.0514
trigger times: 14
Loss after 1047756720 batches: 0.0511
trigger times: 0
Loss after 1047887820 batches: 0.0505
trigger times: 1
Loss after 1048018920 batches: 0.0502
trigger times: 2
Loss after 1048150020 batches: 0.0500
trigger times: 3
Loss after 1048281120 batches: 0.0502
trigger times: 4
Loss after 1048412220 batches: 0.0492
trigger times: 5
Loss after 1048543320 batches: 0.0494
trigger times: 6
Loss after 1048674420 batches: 0.0493
trigger times: 7
Loss after 1048805520 batches: 0.0492
trigger times: 8
Loss after 1048936620 batches: 0.0487
trigger times: 9
Loss after 1049067720 batches: 0.0486
trigger times: 0
Loss after 1049198820 batches: 0.0480
trigger times: 1
Loss after 1049329920 batches: 0.0481
trigger times: 2
Loss after 1049461020 batches: 0.0480
trigger times: 3
Loss after 1049592120 batches: 0.0480
trigger times: 4
Loss after 1049723220 batches: 0.0478
trigger times: 0
Loss after 1049854320 batches: 0.0473
trigger times: 1
Loss after 1049985420 batches: 0.0475
trigger times: 0
Loss after 1050116520 batches: 0.0468
trigger times: 1
Loss after 1050247620 batches: 0.0471
trigger times: 2
Loss after 1050378720 batches: 0.0464
trigger times: 0
Loss after 1050509820 batches: 0.0465
trigger times: 1
Loss after 1050640920 batches: 0.0467
trigger times: 2
Loss after 1050772020 batches: 0.0465
trigger times: 3
Loss after 1050903120 batches: 0.0459
trigger times: 4
Loss after 1051034220 batches: 0.0461
trigger times: 5
Loss after 1051165320 batches: 0.0460
trigger times: 6
Loss after 1051296420 batches: 0.0457
trigger times: 7
Loss after 1051427520 batches: 0.0457
trigger times: 8
Loss after 1051558620 batches: 0.0450
trigger times: 9
Loss after 1051689720 batches: 0.0454
trigger times: 10
Loss after 1051820820 batches: 0.0456
trigger times: 0
Loss after 1051951920 batches: 0.0447
trigger times: 1
Loss after 1052083020 batches: 0.0443
trigger times: 0
Loss after 1052214120 batches: 0.0449
trigger times: 1
Loss after 1052345220 batches: 0.0446
trigger times: 2
Loss after 1052476320 batches: 0.0445
trigger times: 3
Loss after 1052607420 batches: 0.0439
trigger times: 4
Loss after 1052738520 batches: 0.0444
trigger times: 5
Loss after 1052869620 batches: 0.0445
trigger times: 6
Loss after 1053000720 batches: 0.0437
trigger times: 7
Loss after 1053131820 batches: 0.0438
trigger times: 8
Loss after 1053262920 batches: 0.0438
trigger times: 9
Loss after 1053394020 batches: 0.0436
trigger times: 10
Loss after 1053525120 batches: 0.0431
trigger times: 11
Loss after 1053656220 batches: 0.0430
trigger times: 12
Loss after 1053787320 batches: 0.0435
trigger times: 0
Loss after 1053918420 batches: 0.0434
trigger times: 0
Loss after 1054049520 batches: 0.0429
trigger times: 1
Loss after 1054180620 batches: 0.0432
trigger times: 2
Loss after 1054311720 batches: 0.0437
trigger times: 3
Loss after 1054442820 batches: 0.0428
trigger times: 4
Loss after 1054573920 batches: 0.0426
trigger times: 5
Loss after 1054705020 batches: 0.0425
trigger times: 6
Loss after 1054836120 batches: 0.0428
trigger times: 7
Loss after 1054967220 batches: 0.0430
trigger times: 8
Loss after 1055098320 batches: 0.0424
trigger times: 9
Loss after 1055229420 batches: 0.0425
trigger times: 10
Loss after 1055360520 batches: 0.0423
trigger times: 11
Loss after 1055491620 batches: 0.0423
trigger times: 12
Loss after 1055622720 batches: 0.0421
trigger times: 13
Loss after 1055753820 batches: 0.0422
trigger times: 14
Loss after 1055884920 batches: 0.0419
trigger times: 15
Loss after 1056016020 batches: 0.0416
trigger times: 16
Loss after 1056147120 batches: 0.0417
trigger times: 17
Loss after 1056278220 batches: 0.0415
trigger times: 18
Loss after 1056409320 batches: 0.0417
trigger times: 19
Loss after 1056540420 batches: 0.0413
trigger times: 20
Early stopping!
Start to test process.
Loss after 1056671520 batches: 0.0409
Time to train on one home:  912.6273131370544
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642, 0.03501720033327798, 0.03263172240869951]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217], [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944], [0.6491197148958842, 0.29803996281550993, 0.44160060986867417, 1.203530351478871, 0.5750418631238572, 28.433835927602637, 1775.19]]
Round_9_results:  [0.6491197148958842, 0.29803996281550993, 0.44160060986867417, 1.203530351478871, 0.5750418631238572, 28.433835927602637, 1775.19]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 8364 < 8365; dropping {'Training_Loss': 0.43965139027152744, 'Validation_Loss': 0.7759535643789504, 'Training_R2': 0.5544173553571724, 'Validation_R2': 0.27693186259458524, 'Training_F1': 0.7456194541210793, 'Validation_F1': 0.4330893908765108, 'Training_NEP': 0.5101799500617189, 'Validation_NEP': 1.1149888503229968, 'Training_NDE': 0.30014013374827275, 'Validation_NDE': 0.5758133487366657, 'Training_MAE': 33.4840449389705, 'Validation_MAE': 30.577620081235004, 'Training_MSE': 3960.865, 'Validation_MSE': 2126.4626}.
trigger times: 0
Loss after 1056774120 batches: 0.4397
trigger times: 1
Loss after 1056876720 batches: 0.1864
trigger times: 2
Loss after 1056979320 batches: 0.1170
trigger times: 3
Loss after 1057081920 batches: 0.0927
trigger times: 4
Loss after 1057184520 batches: 0.0808
trigger times: 5
Loss after 1057287120 batches: 0.0788
trigger times: 6
Loss after 1057389720 batches: 0.0639
trigger times: 7
Loss after 1057492320 batches: 0.0585
trigger times: 8
Loss after 1057594920 batches: 0.0541
trigger times: 9
Loss after 1057697520 batches: 0.0547
trigger times: 10
Loss after 1057800120 batches: 0.0503
trigger times: 11
Loss after 1057902720 batches: 0.0482
trigger times: 12
Loss after 1058005320 batches: 0.0458
trigger times: 13
Loss after 1058107920 batches: 0.0445
trigger times: 14
Loss after 1058210520 batches: 0.0432
trigger times: 15
Loss after 1058313120 batches: 0.0408
trigger times: 16
Loss after 1058415720 batches: 0.0400
trigger times: 17
Loss after 1058518320 batches: 0.0415
trigger times: 18
Loss after 1058620920 batches: 0.0385
trigger times: 19
Loss after 1058723520 batches: 0.0384
trigger times: 20
Early stopping!
Start to test process.
Loss after 1058826120 batches: 0.0385
Time to train on one home:  135.75822234153748
trigger times: 0
Loss after 1058957220 batches: 0.2641
trigger times: 1
Loss after 1059088320 batches: 0.1099
trigger times: 2
Loss after 1059219420 batches: 0.0771
trigger times: 3
Loss after 1059350520 batches: 0.0644
trigger times: 4
Loss after 1059481620 batches: 0.0568
trigger times: 5
Loss after 1059612720 batches: 0.0509
trigger times: 6
Loss after 1059743820 batches: 0.0481
trigger times: 7
Loss after 1059874920 batches: 0.0450
trigger times: 8
Loss after 1060006020 batches: 0.0428
trigger times: 9
Loss after 1060137120 batches: 0.0414
trigger times: 10
Loss after 1060268220 batches: 0.0395
trigger times: 11
Loss after 1060399320 batches: 0.0382
trigger times: 12
Loss after 1060530420 batches: 0.0369
trigger times: 13
Loss after 1060661520 batches: 0.0363
trigger times: 14
Loss after 1060792620 batches: 0.0351
trigger times: 15
Loss after 1060923720 batches: 0.0346
trigger times: 16
Loss after 1061054820 batches: 0.0335
trigger times: 17
Loss after 1061185920 batches: 0.0333
trigger times: 18
Loss after 1061317020 batches: 0.0323
trigger times: 19
Loss after 1061448120 batches: 0.0323
trigger times: 20
Early stopping!
Start to test process.
Loss after 1061579220 batches: 0.0313
Time to train on one home:  164.83000373840332
trigger times: 0
Loss after 1061710320 batches: 0.5231
trigger times: 1
Loss after 1061841420 batches: 0.2224
trigger times: 2
Loss after 1061972520 batches: 0.1413
trigger times: 3
Loss after 1062103620 batches: 0.1138
trigger times: 4
Loss after 1062234720 batches: 0.0984
trigger times: 5
Loss after 1062365820 batches: 0.0876
trigger times: 6
Loss after 1062496920 batches: 0.0816
trigger times: 7
Loss after 1062628020 batches: 0.0764
trigger times: 8
Loss after 1062759120 batches: 0.0723
trigger times: 9
Loss after 1062890220 batches: 0.0699
trigger times: 10
Loss after 1063021320 batches: 0.0669
trigger times: 11
Loss after 1063152420 batches: 0.0645
trigger times: 12
Loss after 1063283520 batches: 0.0616
trigger times: 13
Loss after 1063414620 batches: 0.0614
trigger times: 14
Loss after 1063545720 batches: 0.0589
trigger times: 15
Loss after 1063676820 batches: 0.0567
trigger times: 16
Loss after 1063807920 batches: 0.0562
trigger times: 17
Loss after 1063939020 batches: 0.0549
trigger times: 18
Loss after 1064070120 batches: 0.0541
trigger times: 19
Loss after 1064201220 batches: 0.0535
trigger times: 20
Early stopping!
Start to test process.
Loss after 1064332320 batches: 0.0520
Time to train on one home:  164.06390833854675
trigger times: 0
Loss after 1064460960 batches: 0.2046
trigger times: 0
Loss after 1064589600 batches: 0.0732
trigger times: 0
Loss after 1064718240 batches: 0.0516
trigger times: 0
Loss after 1064846880 batches: 0.0445
trigger times: 1
Loss after 1064975520 batches: 0.0392
trigger times: 2
Loss after 1065104160 batches: 0.0369
trigger times: 3
Loss after 1065232800 batches: 0.0341
trigger times: 4
Loss after 1065361440 batches: 0.0327
trigger times: 0
Loss after 1065490080 batches: 0.0313
trigger times: 0
Loss after 1065618720 batches: 0.0305
trigger times: 1
Loss after 1065747360 batches: 0.0292
trigger times: 0
Loss after 1065876000 batches: 0.0290
trigger times: 1
Loss after 1066004640 batches: 0.0273
trigger times: 2
Loss after 1066133280 batches: 0.0270
trigger times: 3
Loss after 1066261920 batches: 0.0264
trigger times: 4
Loss after 1066390560 batches: 0.0254
trigger times: 5
Loss after 1066519200 batches: 0.0252
trigger times: 6
Loss after 1066647840 batches: 0.0247
trigger times: 7
Loss after 1066776480 batches: 0.0244
trigger times: 8
Loss after 1066905120 batches: 0.0237
trigger times: 9
Loss after 1067033760 batches: 0.0232
trigger times: 10
Loss after 1067162400 batches: 0.0229
trigger times: 11
Loss after 1067291040 batches: 0.0231
trigger times: 0
Loss after 1067419680 batches: 0.0223
trigger times: 1
Loss after 1067548320 batches: 0.0221
trigger times: 0
Loss after 1067676960 batches: 0.0219
trigger times: 0
Loss after 1067805600 batches: 0.0213
trigger times: 0
Loss after 1067934240 batches: 0.0217
trigger times: 1
Loss after 1068062880 batches: 0.0211
trigger times: 2
Loss after 1068191520 batches: 0.0211
trigger times: 3
Loss after 1068320160 batches: 0.0206
trigger times: 0
Loss after 1068448800 batches: 0.0205
trigger times: 1
Loss after 1068577440 batches: 0.0203
trigger times: 2
Loss after 1068706080 batches: 0.0199
trigger times: 0
Loss after 1068834720 batches: 0.0198
trigger times: 0
Loss after 1068963360 batches: 0.0196
trigger times: 1
Loss after 1069092000 batches: 0.0191
trigger times: 2
Loss after 1069220640 batches: 0.0196
trigger times: 0
Loss after 1069349280 batches: 0.0194
trigger times: 0
Loss after 1069477920 batches: 0.0189
trigger times: 1
Loss after 1069606560 batches: 0.0187
trigger times: 2
Loss after 1069735200 batches: 0.0188
trigger times: 3
Loss after 1069863840 batches: 0.0190
trigger times: 4
Loss after 1069992480 batches: 0.0188
trigger times: 5
Loss after 1070121120 batches: 0.0188
trigger times: 6
Loss after 1070249760 batches: 0.0187
trigger times: 7
Loss after 1070378400 batches: 0.0183
trigger times: 0
Loss after 1070507040 batches: 0.0178
trigger times: 1
Loss after 1070635680 batches: 0.0180
trigger times: 2
Loss after 1070764320 batches: 0.0179
trigger times: 3
Loss after 1070892960 batches: 0.0177
trigger times: 4
Loss after 1071021600 batches: 0.0178
trigger times: 5
Loss after 1071150240 batches: 0.0176
trigger times: 0
Loss after 1071278880 batches: 0.0176
trigger times: 1
Loss after 1071407520 batches: 0.0179
trigger times: 2
Loss after 1071536160 batches: 0.0169
trigger times: 3
Loss after 1071664800 batches: 0.0170
trigger times: 4
Loss after 1071793440 batches: 0.0169
trigger times: 5
Loss after 1071922080 batches: 0.0171
trigger times: 6
Loss after 1072050720 batches: 0.0170
trigger times: 7
Loss after 1072179360 batches: 0.0168
trigger times: 8
Loss after 1072308000 batches: 0.0168
trigger times: 9
Loss after 1072436640 batches: 0.0164
trigger times: 10
Loss after 1072565280 batches: 0.0168
trigger times: 11
Loss after 1072693920 batches: 0.0160
trigger times: 12
Loss after 1072822560 batches: 0.0167
trigger times: 13
Loss after 1072951200 batches: 0.0163
trigger times: 14
Loss after 1073079840 batches: 0.0162
trigger times: 15
Loss after 1073208480 batches: 0.0161
trigger times: 16
Loss after 1073337120 batches: 0.0162
trigger times: 17
Loss after 1073465760 batches: 0.0160
trigger times: 0
Loss after 1073594400 batches: 0.0160
trigger times: 0
Loss after 1073723040 batches: 0.0158
trigger times: 1
Loss after 1073851680 batches: 0.0156
trigger times: 2
Loss after 1073980320 batches: 0.0157
trigger times: 3
Loss after 1074108960 batches: 0.0159
trigger times: 4
Loss after 1074237600 batches: 0.0153
trigger times: 0
Loss after 1074366240 batches: 0.0159
trigger times: 1
Loss after 1074494880 batches: 0.0158
trigger times: 2
Loss after 1074623520 batches: 0.0156
trigger times: 3
Loss after 1074752160 batches: 0.0155
trigger times: 0
Loss after 1074880800 batches: 0.0156
trigger times: 1
Loss after 1075009440 batches: 0.0156
trigger times: 2
Loss after 1075138080 batches: 0.0155
trigger times: 3
Loss after 1075266720 batches: 0.0152
trigger times: 4
Loss after 1075395360 batches: 0.0153
trigger times: 5
Loss after 1075524000 batches: 0.0150
trigger times: 6
Loss after 1075652640 batches: 0.0151
trigger times: 7
Loss after 1075781280 batches: 0.0150
trigger times: 8
Loss after 1075909920 batches: 0.0150
trigger times: 9
Loss after 1076038560 batches: 0.0146
trigger times: 10
Loss after 1076167200 batches: 0.0148
trigger times: 11
Loss after 1076295840 batches: 0.0148
trigger times: 12
Loss after 1076424480 batches: 0.0150
trigger times: 13
Loss after 1076553120 batches: 0.0147
trigger times: 14
Loss after 1076681760 batches: 0.0149
trigger times: 0
Loss after 1076810400 batches: 0.0148
trigger times: 1
Loss after 1076939040 batches: 0.0144
trigger times: 2
Loss after 1077067680 batches: 0.0141
trigger times: 3
Loss after 1077196320 batches: 0.0147
trigger times: 4
Loss after 1077324960 batches: 0.0144
trigger times: 0
Loss after 1077453600 batches: 0.0148
trigger times: 1
Loss after 1077582240 batches: 0.0143
trigger times: 2
Loss after 1077710880 batches: 0.0140
trigger times: 3
Loss after 1077839520 batches: 0.0142
trigger times: 4
Loss after 1077968160 batches: 0.0141
trigger times: 5
Loss after 1078096800 batches: 0.0141
trigger times: 6
Loss after 1078225440 batches: 0.0140
trigger times: 7
Loss after 1078354080 batches: 0.0141
trigger times: 8
Loss after 1078482720 batches: 0.0139
trigger times: 9
Loss after 1078611360 batches: 0.0138
trigger times: 0
Loss after 1078740000 batches: 0.0140
trigger times: 0
Loss after 1078868640 batches: 0.0138
trigger times: 1
Loss after 1078997280 batches: 0.0141
trigger times: 2
Loss after 1079125920 batches: 0.0141
trigger times: 3
Loss after 1079254560 batches: 0.0136
trigger times: 4
Loss after 1079383200 batches: 0.0136
trigger times: 5
Loss after 1079511840 batches: 0.0138
trigger times: 6
Loss after 1079640480 batches: 0.0138
trigger times: 7
Loss after 1079769120 batches: 0.0135
trigger times: 8
Loss after 1079897760 batches: 0.0136
trigger times: 9
Loss after 1080026400 batches: 0.0134
trigger times: 10
Loss after 1080155040 batches: 0.0138
trigger times: 11
Loss after 1080283680 batches: 0.0136
trigger times: 12
Loss after 1080412320 batches: 0.0133
trigger times: 0
Loss after 1080540960 batches: 0.0133
trigger times: 1
Loss after 1080669600 batches: 0.0132
trigger times: 2
Loss after 1080798240 batches: 0.0131
trigger times: 3
Loss after 1080926880 batches: 0.0131
trigger times: 4
Loss after 1081055520 batches: 0.0131
trigger times: 5
Loss after 1081184160 batches: 0.0128
trigger times: 6
Loss after 1081312800 batches: 0.0128
trigger times: 7
Loss after 1081441440 batches: 0.0128
trigger times: 0
Loss after 1081570080 batches: 0.0132
trigger times: 1
Loss after 1081698720 batches: 0.0131
trigger times: 2
Loss after 1081827360 batches: 0.0130
trigger times: 3
Loss after 1081956000 batches: 0.0128
trigger times: 4
Loss after 1082084640 batches: 0.0128
trigger times: 5
Loss after 1082213280 batches: 0.0127
trigger times: 6
Loss after 1082341920 batches: 0.0128
trigger times: 7
Loss after 1082470560 batches: 0.0130
trigger times: 8
Loss after 1082599200 batches: 0.0126
trigger times: 9
Loss after 1082727840 batches: 0.0125
trigger times: 10
Loss after 1082856480 batches: 0.0126
trigger times: 11
Loss after 1082985120 batches: 0.0124
trigger times: 12
Loss after 1083113760 batches: 0.0129
trigger times: 13
Loss after 1083242400 batches: 0.0123
trigger times: 14
Loss after 1083371040 batches: 0.0119
trigger times: 15
Loss after 1083499680 batches: 0.0122
trigger times: 16
Loss after 1083628320 batches: 0.0119
trigger times: 17
Loss after 1083756960 batches: 0.0125
trigger times: 18
Loss after 1083885600 batches: 0.0124
trigger times: 19
Loss after 1084014240 batches: 0.0123
trigger times: 20
Early stopping!
Start to test process.
Loss after 1084142880 batches: 0.0122
Time to train on one home:  1115.9857380390167
trigger times: 0
Loss after 1084273980 batches: 0.5773
trigger times: 1
Loss after 1084405080 batches: 0.2325
trigger times: 2
Loss after 1084536180 batches: 0.1384
trigger times: 3
Loss after 1084667280 batches: 0.1077
trigger times: 4
Loss after 1084798380 batches: 0.0945
trigger times: 5
Loss after 1084929480 batches: 0.0858
trigger times: 6
Loss after 1085060580 batches: 0.0805
trigger times: 7
Loss after 1085191680 batches: 0.0752
trigger times: 8
Loss after 1085322780 batches: 0.0711
trigger times: 9
Loss after 1085453880 batches: 0.0669
trigger times: 10
Loss after 1085584980 batches: 0.0657
trigger times: 11
Loss after 1085716080 batches: 0.0624
trigger times: 12
Loss after 1085847180 batches: 0.0611
trigger times: 13
Loss after 1085978280 batches: 0.0595
trigger times: 14
Loss after 1086109380 batches: 0.0577
trigger times: 15
Loss after 1086240480 batches: 0.0564
trigger times: 16
Loss after 1086371580 batches: 0.0553
trigger times: 17
Loss after 1086502680 batches: 0.0541
trigger times: 18
Loss after 1086633780 batches: 0.0527
trigger times: 19
Loss after 1086764880 batches: 0.0520
trigger times: 20
Early stopping!
Start to test process.
Loss after 1086895980 batches: 0.0521
Time to train on one home:  164.2284300327301
trigger times: 0
Loss after 1087027080 batches: 0.5973
trigger times: 1
Loss after 1087158180 batches: 0.3213
trigger times: 2
Loss after 1087289280 batches: 0.2074
trigger times: 3
Loss after 1087420380 batches: 0.1483
trigger times: 4
Loss after 1087551480 batches: 0.1273
trigger times: 5
Loss after 1087682580 batches: 0.1076
trigger times: 6
Loss after 1087813680 batches: 0.0982
trigger times: 7
Loss after 1087944780 batches: 0.0901
trigger times: 8
Loss after 1088075880 batches: 0.0851
trigger times: 9
Loss after 1088206980 batches: 0.0816
trigger times: 10
Loss after 1088338080 batches: 0.0747
trigger times: 11
Loss after 1088469180 batches: 0.0719
trigger times: 12
Loss after 1088600280 batches: 0.0702
trigger times: 13
Loss after 1088731380 batches: 0.0695
trigger times: 14
Loss after 1088862480 batches: 0.0662
trigger times: 15
Loss after 1088993580 batches: 0.0636
trigger times: 16
Loss after 1089124680 batches: 0.0590
trigger times: 17
Loss after 1089255780 batches: 0.0599
trigger times: 18
Loss after 1089386880 batches: 0.0583
trigger times: 19
Loss after 1089517980 batches: 0.0577
trigger times: 20
Early stopping!
Start to test process.
Loss after 1089649080 batches: 0.0571
Time to train on one home:  163.6129310131073
trigger times: 0
Loss after 1089780180 batches: 0.1337
trigger times: 1
Loss after 1089911280 batches: 0.0426
trigger times: 2
Loss after 1090042380 batches: 0.0303
trigger times: 3
Loss after 1090173480 batches: 0.0262
trigger times: 4
Loss after 1090304580 batches: 0.0238
trigger times: 5
Loss after 1090435680 batches: 0.0217
trigger times: 6
Loss after 1090566780 batches: 0.0207
trigger times: 7
Loss after 1090697880 batches: 0.0198
trigger times: 8
Loss after 1090828980 batches: 0.0189
trigger times: 9
Loss after 1090960080 batches: 0.0180
trigger times: 10
Loss after 1091091180 batches: 0.0178
trigger times: 0
Loss after 1091222280 batches: 0.0171
trigger times: 1
Loss after 1091353380 batches: 0.0166
trigger times: 2
Loss after 1091484480 batches: 0.0164
trigger times: 3
Loss after 1091615580 batches: 0.0157
trigger times: 4
Loss after 1091746680 batches: 0.0156
trigger times: 5
Loss after 1091877780 batches: 0.0154
trigger times: 6
Loss after 1092008880 batches: 0.0151
trigger times: 7
Loss after 1092139980 batches: 0.0153
trigger times: 8
Loss after 1092271080 batches: 0.0148
trigger times: 9
Loss after 1092402180 batches: 0.0147
trigger times: 10
Loss after 1092533280 batches: 0.0146
trigger times: 11
Loss after 1092664380 batches: 0.0145
trigger times: 12
Loss after 1092795480 batches: 0.0140
trigger times: 13
Loss after 1092926580 batches: 0.0139
trigger times: 14
Loss after 1093057680 batches: 0.0137
trigger times: 15
Loss after 1093188780 batches: 0.0135
trigger times: 0
Loss after 1093319880 batches: 0.0135
trigger times: 0
Loss after 1093450980 batches: 0.0133
trigger times: 1
Loss after 1093582080 batches: 0.0132
trigger times: 2
Loss after 1093713180 batches: 0.0131
trigger times: 3
Loss after 1093844280 batches: 0.0131
trigger times: 4
Loss after 1093975380 batches: 0.0127
trigger times: 5
Loss after 1094106480 batches: 0.0130
trigger times: 6
Loss after 1094237580 batches: 0.0126
trigger times: 7
Loss after 1094368680 batches: 0.0127
trigger times: 8
Loss after 1094499780 batches: 0.0126
trigger times: 9
Loss after 1094630880 batches: 0.0124
trigger times: 10
Loss after 1094761980 batches: 0.0125
trigger times: 11
Loss after 1094893080 batches: 0.0120
trigger times: 0
Loss after 1095024180 batches: 0.0120
trigger times: 1
Loss after 1095155280 batches: 0.0124
trigger times: 2
Loss after 1095286380 batches: 0.0118
trigger times: 3
Loss after 1095417480 batches: 0.0120
trigger times: 4
Loss after 1095548580 batches: 0.0116
trigger times: 5
Loss after 1095679680 batches: 0.0117
trigger times: 6
Loss after 1095810780 batches: 0.0118
trigger times: 7
Loss after 1095941880 batches: 0.0117
trigger times: 8
Loss after 1096072980 batches: 0.0116
trigger times: 9
Loss after 1096204080 batches: 0.0116
trigger times: 10
Loss after 1096335180 batches: 0.0113
trigger times: 11
Loss after 1096466280 batches: 0.0109
trigger times: 12
Loss after 1096597380 batches: 0.0114
trigger times: 13
Loss after 1096728480 batches: 0.0112
trigger times: 14
Loss after 1096859580 batches: 0.0108
trigger times: 15
Loss after 1096990680 batches: 0.0106
trigger times: 16
Loss after 1097121780 batches: 0.0111
trigger times: 17
Loss after 1097252880 batches: 0.0108
trigger times: 0
Loss after 1097383980 batches: 0.0106
trigger times: 1
Loss after 1097515080 batches: 0.0106
trigger times: 2
Loss after 1097646180 batches: 0.0108
trigger times: 3
Loss after 1097777280 batches: 0.0107
trigger times: 4
Loss after 1097908380 batches: 0.0106
trigger times: 0
Loss after 1098039480 batches: 0.0107
trigger times: 1
Loss after 1098170580 batches: 0.0105
trigger times: 2
Loss after 1098301680 batches: 0.0106
trigger times: 3
Loss after 1098432780 batches: 0.0103
trigger times: 0
Loss after 1098563880 batches: 0.0104
trigger times: 1
Loss after 1098694980 batches: 0.0102
trigger times: 2
Loss after 1098826080 batches: 0.0103
trigger times: 3
Loss after 1098957180 batches: 0.0106
trigger times: 0
Loss after 1099088280 batches: 0.0100
trigger times: 1
Loss after 1099219380 batches: 0.0102
trigger times: 0
Loss after 1099350480 batches: 0.0100
trigger times: 1
Loss after 1099481580 batches: 0.0099
trigger times: 2
Loss after 1099612680 batches: 0.0098
trigger times: 3
Loss after 1099743780 batches: 0.0099
trigger times: 4
Loss after 1099874880 batches: 0.0100
trigger times: 5
Loss after 1100005980 batches: 0.0099
trigger times: 6
Loss after 1100137080 batches: 0.0101
trigger times: 7
Loss after 1100268180 batches: 0.0101
trigger times: 8
Loss after 1100399280 batches: 0.0100
trigger times: 9
Loss after 1100530380 batches: 0.0097
trigger times: 10
Loss after 1100661480 batches: 0.0095
trigger times: 11
Loss after 1100792580 batches: 0.0096
trigger times: 12
Loss after 1100923680 batches: 0.0096
trigger times: 13
Loss after 1101054780 batches: 0.0095
trigger times: 14
Loss after 1101185880 batches: 0.0096
trigger times: 15
Loss after 1101316980 batches: 0.0095
trigger times: 16
Loss after 1101448080 batches: 0.0093
trigger times: 17
Loss after 1101579180 batches: 0.0095
trigger times: 18
Loss after 1101710280 batches: 0.0095
trigger times: 19
Loss after 1101841380 batches: 0.0095
trigger times: 20
Early stopping!
Start to test process.
Loss after 1101972480 batches: 0.0093
Time to train on one home:  692.2410566806793
trigger times: 0
Loss after 1102103580 batches: 0.1961
trigger times: 0
Loss after 1102234680 batches: 0.0592
trigger times: 1
Loss after 1102365780 batches: 0.0412
trigger times: 2
Loss after 1102496880 batches: 0.0349
trigger times: 3
Loss after 1102627980 batches: 0.0318
trigger times: 0
Loss after 1102759080 batches: 0.0292
trigger times: 1
Loss after 1102890180 batches: 0.0266
trigger times: 0
Loss after 1103021280 batches: 0.0256
trigger times: 1
Loss after 1103152380 batches: 0.0242
trigger times: 2
Loss after 1103283480 batches: 0.0235
trigger times: 3
Loss after 1103414580 batches: 0.0223
trigger times: 4
Loss after 1103545680 batches: 0.0218
trigger times: 5
Loss after 1103676780 batches: 0.0213
trigger times: 6
Loss after 1103807880 batches: 0.0207
trigger times: 7
Loss after 1103938980 batches: 0.0206
trigger times: 8
Loss after 1104070080 batches: 0.0195
trigger times: 9
Loss after 1104201180 batches: 0.0193
trigger times: 10
Loss after 1104332280 batches: 0.0188
trigger times: 11
Loss after 1104463380 batches: 0.0186
trigger times: 12
Loss after 1104594480 batches: 0.0183
trigger times: 13
Loss after 1104725580 batches: 0.0181
trigger times: 14
Loss after 1104856680 batches: 0.0177
trigger times: 15
Loss after 1104987780 batches: 0.0182
trigger times: 16
Loss after 1105118880 batches: 0.0175
trigger times: 17
Loss after 1105249980 batches: 0.0170
trigger times: 18
Loss after 1105381080 batches: 0.0167
trigger times: 19
Loss after 1105512180 batches: 0.0162
trigger times: 20
Early stopping!
Start to test process.
Loss after 1105643280 batches: 0.0163
Time to train on one home:  215.25143218040466
trigger times: 0
Loss after 1105721880 batches: 0.5306
trigger times: 1
Loss after 1105800480 batches: 0.2258
trigger times: 2
Loss after 1105879080 batches: 0.1233
trigger times: 3
Loss after 1105957680 batches: 0.0909
trigger times: 4
Loss after 1106036280 batches: 0.0747
trigger times: 5
Loss after 1106114880 batches: 0.0654
trigger times: 6
Loss after 1106193480 batches: 0.0602
trigger times: 0
Loss after 1106272080 batches: 0.0567
trigger times: 1
Loss after 1106350680 batches: 0.0535
trigger times: 2
Loss after 1106429280 batches: 0.0501
trigger times: 0
Loss after 1106507880 batches: 0.0473
trigger times: 1
Loss after 1106586480 batches: 0.0454
trigger times: 2
Loss after 1106665080 batches: 0.0446
trigger times: 3
Loss after 1106743680 batches: 0.0439
trigger times: 4
Loss after 1106822280 batches: 0.0413
trigger times: 5
Loss after 1106900880 batches: 0.0400
trigger times: 6
Loss after 1106979480 batches: 0.0394
trigger times: 7
Loss after 1107058080 batches: 0.0394
trigger times: 8
Loss after 1107136680 batches: 0.0374
trigger times: 9
Loss after 1107215280 batches: 0.0369
trigger times: 10
Loss after 1107293880 batches: 0.0366
trigger times: 11
Loss after 1107372480 batches: 0.0359
trigger times: 12
Loss after 1107451080 batches: 0.0354
trigger times: 13
Loss after 1107529680 batches: 0.0349
trigger times: 14
Loss after 1107608280 batches: 0.0342
trigger times: 15
Loss after 1107686880 batches: 0.0334
trigger times: 16
Loss after 1107765480 batches: 0.0340
trigger times: 17
Loss after 1107844080 batches: 0.0327
trigger times: 18
Loss after 1107922680 batches: 0.0323
trigger times: 19
Loss after 1108001280 batches: 0.0316
trigger times: 20
Early stopping!
Start to test process.
Loss after 1108079880 batches: 0.0322
Time to train on one home:  157.82085394859314
trigger times: 0
Loss after 1108210980 batches: 0.1485
trigger times: 0
Loss after 1108342080 batches: 0.0498
trigger times: 1
Loss after 1108473180 batches: 0.0366
trigger times: 0
Loss after 1108604280 batches: 0.0302
trigger times: 0
Loss after 1108735380 batches: 0.0277
trigger times: 1
Loss after 1108866480 batches: 0.0254
trigger times: 0
Loss after 1108997580 batches: 0.0240
trigger times: 0
Loss after 1109128680 batches: 0.0231
trigger times: 0
Loss after 1109259780 batches: 0.0220
trigger times: 1
Loss after 1109390880 batches: 0.0213
trigger times: 2
Loss after 1109521980 batches: 0.0207
trigger times: 3
Loss after 1109653080 batches: 0.0204
trigger times: 4
Loss after 1109784180 batches: 0.0192
trigger times: 5
Loss after 1109915280 batches: 0.0196
trigger times: 6
Loss after 1110046380 batches: 0.0185
trigger times: 7
Loss after 1110177480 batches: 0.0179
trigger times: 0
Loss after 1110308580 batches: 0.0177
trigger times: 1
Loss after 1110439680 batches: 0.0174
trigger times: 0
Loss after 1110570780 batches: 0.0172
trigger times: 1
Loss after 1110701880 batches: 0.0168
trigger times: 2
Loss after 1110832980 batches: 0.0168
trigger times: 3
Loss after 1110964080 batches: 0.0165
trigger times: 4
Loss after 1111095180 batches: 0.0159
trigger times: 5
Loss after 1111226280 batches: 0.0158
trigger times: 6
Loss after 1111357380 batches: 0.0159
trigger times: 0
Loss after 1111488480 batches: 0.0156
trigger times: 1
Loss after 1111619580 batches: 0.0156
trigger times: 2
Loss after 1111750680 batches: 0.0155
trigger times: 3
Loss after 1111881780 batches: 0.0152
trigger times: 4
Loss after 1112012880 batches: 0.0151
trigger times: 0
Loss after 1112143980 batches: 0.0150
trigger times: 1
Loss after 1112275080 batches: 0.0147
trigger times: 2
Loss after 1112406180 batches: 0.0146
trigger times: 3
Loss after 1112537280 batches: 0.0142
trigger times: 4
Loss after 1112668380 batches: 0.0142
trigger times: 5
Loss after 1112799480 batches: 0.0142
trigger times: 6
Loss after 1112930580 batches: 0.0141
trigger times: 7
Loss after 1113061680 batches: 0.0138
trigger times: 8
Loss after 1113192780 batches: 0.0138
trigger times: 9
Loss after 1113323880 batches: 0.0133
trigger times: 10
Loss after 1113454980 batches: 0.0138
trigger times: 11
Loss after 1113586080 batches: 0.0135
trigger times: 12
Loss after 1113717180 batches: 0.0134
trigger times: 13
Loss after 1113848280 batches: 0.0132
trigger times: 14
Loss after 1113979380 batches: 0.0128
trigger times: 0
Loss after 1114110480 batches: 0.0128
trigger times: 1
Loss after 1114241580 batches: 0.0131
trigger times: 0
Loss after 1114372680 batches: 0.0128
trigger times: 1
Loss after 1114503780 batches: 0.0126
trigger times: 2
Loss after 1114634880 batches: 0.0123
trigger times: 3
Loss after 1114765980 batches: 0.0125
trigger times: 4
Loss after 1114897080 batches: 0.0124
trigger times: 5
Loss after 1115028180 batches: 0.0123
trigger times: 6
Loss after 1115159280 batches: 0.0122
trigger times: 7
Loss after 1115290380 batches: 0.0121
trigger times: 8
Loss after 1115421480 batches: 0.0121
trigger times: 9
Loss after 1115552580 batches: 0.0122
trigger times: 10
Loss after 1115683680 batches: 0.0121
trigger times: 11
Loss after 1115814780 batches: 0.0119
trigger times: 12
Loss after 1115945880 batches: 0.0122
trigger times: 13
Loss after 1116076980 batches: 0.0117
trigger times: 14
Loss after 1116208080 batches: 0.0115
trigger times: 15
Loss after 1116339180 batches: 0.0117
trigger times: 16
Loss after 1116470280 batches: 0.0114
trigger times: 17
Loss after 1116601380 batches: 0.0115
trigger times: 18
Loss after 1116732480 batches: 0.0116
trigger times: 19
Loss after 1116863580 batches: 0.0114
trigger times: 20
Early stopping!
Start to test process.
Loss after 1116994680 batches: 0.0115
Time to train on one home:  505.91432309150696
trigger times: 0
Loss after 1117125780 batches: 0.2021
trigger times: 0
Loss after 1117256880 batches: 0.0644
trigger times: 0
Loss after 1117387980 batches: 0.0480
trigger times: 0
Loss after 1117519080 batches: 0.0406
trigger times: 1
Loss after 1117650180 batches: 0.0369
trigger times: 2
Loss after 1117781280 batches: 0.0339
trigger times: 3
Loss after 1117912380 batches: 0.0319
trigger times: 4
Loss after 1118043480 batches: 0.0303
trigger times: 5
Loss after 1118174580 batches: 0.0287
trigger times: 6
Loss after 1118305680 batches: 0.0282
trigger times: 7
Loss after 1118436780 batches: 0.0274
trigger times: 8
Loss after 1118567880 batches: 0.0258
trigger times: 0
Loss after 1118698980 batches: 0.0257
trigger times: 1
Loss after 1118830080 batches: 0.0253
trigger times: 2
Loss after 1118961180 batches: 0.0244
trigger times: 3
Loss after 1119092280 batches: 0.0244
trigger times: 4
Loss after 1119223380 batches: 0.0238
trigger times: 5
Loss after 1119354480 batches: 0.0231
trigger times: 6
Loss after 1119485580 batches: 0.0228
trigger times: 7
Loss after 1119616680 batches: 0.0222
trigger times: 8
Loss after 1119747780 batches: 0.0221
trigger times: 9
Loss after 1119878880 batches: 0.0220
trigger times: 10
Loss after 1120009980 batches: 0.0219
trigger times: 11
Loss after 1120141080 batches: 0.0211
trigger times: 12
Loss after 1120272180 batches: 0.0210
trigger times: 13
Loss after 1120403280 batches: 0.0205
trigger times: 14
Loss after 1120534380 batches: 0.0204
trigger times: 15
Loss after 1120665480 batches: 0.0201
trigger times: 16
Loss after 1120796580 batches: 0.0201
trigger times: 17
Loss after 1120927680 batches: 0.0201
trigger times: 18
Loss after 1121058780 batches: 0.0198
trigger times: 19
Loss after 1121189880 batches: 0.0197
trigger times: 20
Early stopping!
Start to test process.
Loss after 1121320980 batches: 0.0192
Time to train on one home:  252.02638292312622
trigger times: 0
Loss after 1121452080 batches: 0.3208
trigger times: 0
Loss after 1121583180 batches: 0.1112
trigger times: 0
Loss after 1121714280 batches: 0.0758
trigger times: 1
Loss after 1121845380 batches: 0.0619
trigger times: 2
Loss after 1121976480 batches: 0.0561
trigger times: 3
Loss after 1122107580 batches: 0.0506
trigger times: 0
Loss after 1122238680 batches: 0.0482
trigger times: 1
Loss after 1122369780 batches: 0.0445
trigger times: 0
Loss after 1122500880 batches: 0.0425
trigger times: 1
Loss after 1122631980 batches: 0.0420
trigger times: 0
Loss after 1122763080 batches: 0.0405
trigger times: 1
Loss after 1122894180 batches: 0.0399
trigger times: 2
Loss after 1123025280 batches: 0.0394
trigger times: 3
Loss after 1123156380 batches: 0.0362
trigger times: 0
Loss after 1123287480 batches: 0.0355
trigger times: 1
Loss after 1123418580 batches: 0.0340
trigger times: 2
Loss after 1123549680 batches: 0.0336
trigger times: 3
Loss after 1123680780 batches: 0.0341
trigger times: 4
Loss after 1123811880 batches: 0.0330
trigger times: 5
Loss after 1123942980 batches: 0.0325
trigger times: 6
Loss after 1124074080 batches: 0.0318
trigger times: 7
Loss after 1124205180 batches: 0.0315
trigger times: 8
Loss after 1124336280 batches: 0.0313
trigger times: 9
Loss after 1124467380 batches: 0.0300
trigger times: 10
Loss after 1124598480 batches: 0.0309
trigger times: 11
Loss after 1124729580 batches: 0.0305
trigger times: 12
Loss after 1124860680 batches: 0.0298
trigger times: 13
Loss after 1124991780 batches: 0.0296
trigger times: 14
Loss after 1125122880 batches: 0.0284
trigger times: 0
Loss after 1125253980 batches: 0.0289
trigger times: 0
Loss after 1125385080 batches: 0.0281
trigger times: 1
Loss after 1125516180 batches: 0.0281
trigger times: 0
Loss after 1125647280 batches: 0.0282
trigger times: 1
Loss after 1125778380 batches: 0.0282
trigger times: 2
Loss after 1125909480 batches: 0.0275
trigger times: 3
Loss after 1126040580 batches: 0.0270
trigger times: 4
Loss after 1126171680 batches: 0.0270
trigger times: 5
Loss after 1126302780 batches: 0.0279
trigger times: 6
Loss after 1126433880 batches: 0.0269
trigger times: 7
Loss after 1126564980 batches: 0.0269
trigger times: 8
Loss after 1126696080 batches: 0.0267
trigger times: 9
Loss after 1126827180 batches: 0.0268
trigger times: 10
Loss after 1126958280 batches: 0.0270
trigger times: 11
Loss after 1127089380 batches: 0.0263
trigger times: 12
Loss after 1127220480 batches: 0.0263
trigger times: 13
Loss after 1127351580 batches: 0.0258
trigger times: 14
Loss after 1127482680 batches: 0.0245
trigger times: 15
Loss after 1127613780 batches: 0.0246
trigger times: 16
Loss after 1127744880 batches: 0.0250
trigger times: 17
Loss after 1127875980 batches: 0.0245
trigger times: 18
Loss after 1128007080 batches: 0.0241
trigger times: 0
Loss after 1128138180 batches: 0.0245
trigger times: 1
Loss after 1128269280 batches: 0.0236
trigger times: 2
Loss after 1128400380 batches: 0.0241
trigger times: 3
Loss after 1128531480 batches: 0.0237
trigger times: 4
Loss after 1128662580 batches: 0.0237
trigger times: 5
Loss after 1128793680 batches: 0.0237
trigger times: 6
Loss after 1128924780 batches: 0.0235
trigger times: 7
Loss after 1129055880 batches: 0.0231
trigger times: 8
Loss after 1129186980 batches: 0.0234
trigger times: 9
Loss after 1129318080 batches: 0.0230
trigger times: 10
Loss after 1129449180 batches: 0.0228
trigger times: 11
Loss after 1129580280 batches: 0.0225
trigger times: 12
Loss after 1129711380 batches: 0.0227
trigger times: 13
Loss after 1129842480 batches: 0.0226
trigger times: 14
Loss after 1129973580 batches: 0.0221
trigger times: 15
Loss after 1130104680 batches: 0.0226
trigger times: 16
Loss after 1130235780 batches: 0.0220
trigger times: 17
Loss after 1130366880 batches: 0.0219
trigger times: 18
Loss after 1130497980 batches: 0.0224
trigger times: 19
Loss after 1130629080 batches: 0.0225
trigger times: 20
Early stopping!
Start to test process.
Loss after 1130760180 batches: 0.0219
Time to train on one home:  534.5830311775208
trigger times: 0
Loss after 1130891280 batches: 0.4197
trigger times: 1
Loss after 1131022380 batches: 0.1799
trigger times: 2
Loss after 1131153480 batches: 0.1193
trigger times: 3
Loss after 1131284580 batches: 0.0962
trigger times: 4
Loss after 1131415680 batches: 0.0855
trigger times: 5
Loss after 1131546780 batches: 0.0779
trigger times: 6
Loss after 1131677880 batches: 0.0711
trigger times: 7
Loss after 1131808980 batches: 0.0672
trigger times: 8
Loss after 1131940080 batches: 0.0652
trigger times: 9
Loss after 1132071180 batches: 0.0628
trigger times: 10
Loss after 1132202280 batches: 0.0601
trigger times: 11
Loss after 1132333380 batches: 0.0569
trigger times: 12
Loss after 1132464480 batches: 0.0566
trigger times: 13
Loss after 1132595580 batches: 0.0548
trigger times: 14
Loss after 1132726680 batches: 0.0527
trigger times: 15
Loss after 1132857780 batches: 0.0525
trigger times: 16
Loss after 1132988880 batches: 0.0507
trigger times: 17
Loss after 1133119980 batches: 0.0508
trigger times: 18
Loss after 1133251080 batches: 0.0492
trigger times: 19
Loss after 1133382180 batches: 0.0489
trigger times: 20
Early stopping!
Start to test process.
Loss after 1133513280 batches: 0.0477
Time to train on one home:  164.36306023597717
trigger times: 0
Loss after 1133644380 batches: 0.2838
trigger times: 1
Loss after 1133775480 batches: 0.0951
trigger times: 2
Loss after 1133906580 batches: 0.0690
trigger times: 3
Loss after 1134037680 batches: 0.0568
trigger times: 4
Loss after 1134168780 batches: 0.0495
trigger times: 5
Loss after 1134299880 batches: 0.0465
trigger times: 6
Loss after 1134430980 batches: 0.0437
trigger times: 7
Loss after 1134562080 batches: 0.0410
trigger times: 8
Loss after 1134693180 batches: 0.0390
trigger times: 9
Loss after 1134824280 batches: 0.0374
trigger times: 10
Loss after 1134955380 batches: 0.0356
trigger times: 11
Loss after 1135086480 batches: 0.0355
trigger times: 12
Loss after 1135217580 batches: 0.0353
trigger times: 13
Loss after 1135348680 batches: 0.0337
trigger times: 14
Loss after 1135479780 batches: 0.0327
trigger times: 15
Loss after 1135610880 batches: 0.0317
trigger times: 16
Loss after 1135741980 batches: 0.0310
trigger times: 17
Loss after 1135873080 batches: 0.0306
trigger times: 18
Loss after 1136004180 batches: 0.0308
trigger times: 19
Loss after 1136135280 batches: 0.0296
trigger times: 20
Early stopping!
Start to test process.
Loss after 1136266380 batches: 0.0295
Time to train on one home:  164.76838564872742
trigger times: 0
Loss after 1136397480 batches: 0.4720
trigger times: 0
Loss after 1136528580 batches: 0.1980
trigger times: 1
Loss after 1136659680 batches: 0.1209
trigger times: 2
Loss after 1136790780 batches: 0.0943
trigger times: 3
Loss after 1136921880 batches: 0.0806
trigger times: 4
Loss after 1137052980 batches: 0.0720
trigger times: 5
Loss after 1137184080 batches: 0.0654
trigger times: 6
Loss after 1137315180 batches: 0.0615
trigger times: 7
Loss after 1137446280 batches: 0.0581
trigger times: 8
Loss after 1137577380 batches: 0.0556
trigger times: 9
Loss after 1137708480 batches: 0.0531
trigger times: 10
Loss after 1137839580 batches: 0.0516
trigger times: 11
Loss after 1137970680 batches: 0.0499
trigger times: 12
Loss after 1138101780 batches: 0.0483
trigger times: 13
Loss after 1138232880 batches: 0.0466
trigger times: 14
Loss after 1138363980 batches: 0.0459
trigger times: 15
Loss after 1138495080 batches: 0.0445
trigger times: 16
Loss after 1138626180 batches: 0.0439
trigger times: 17
Loss after 1138757280 batches: 0.0429
trigger times: 18
Loss after 1138888380 batches: 0.0422
trigger times: 19
Loss after 1139019480 batches: 0.0415
trigger times: 20
Early stopping!
Start to test process.
Loss after 1139150580 batches: 0.0408
Time to train on one home:  171.74811911582947
trigger times: 0
Loss after 1139281680 batches: 0.6701
trigger times: 1
Loss after 1139412780 batches: 0.2738
trigger times: 2
Loss after 1139543880 batches: 0.1651
trigger times: 0
Loss after 1139674980 batches: 0.1300
trigger times: 1
Loss after 1139806080 batches: 0.1139
trigger times: 2
Loss after 1139937180 batches: 0.1028
trigger times: 0
Loss after 1140068280 batches: 0.0944
trigger times: 1
Loss after 1140199380 batches: 0.0878
trigger times: 2
Loss after 1140330480 batches: 0.0822
trigger times: 3
Loss after 1140461580 batches: 0.0788
trigger times: 4
Loss after 1140592680 batches: 0.0758
trigger times: 0
Loss after 1140723780 batches: 0.0739
trigger times: 1
Loss after 1140854880 batches: 0.0706
trigger times: 2
Loss after 1140985980 batches: 0.0680
trigger times: 3
Loss after 1141117080 batches: 0.0663
trigger times: 4
Loss after 1141248180 batches: 0.0655
trigger times: 5
Loss after 1141379280 batches: 0.0640
trigger times: 0
Loss after 1141510380 batches: 0.0622
trigger times: 1
Loss after 1141641480 batches: 0.0609
trigger times: 2
Loss after 1141772580 batches: 0.0608
trigger times: 3
Loss after 1141903680 batches: 0.0585
trigger times: 4
Loss after 1142034780 batches: 0.0579
trigger times: 5
Loss after 1142165880 batches: 0.0563
trigger times: 6
Loss after 1142296980 batches: 0.0558
trigger times: 7
Loss after 1142428080 batches: 0.0560
trigger times: 8
Loss after 1142559180 batches: 0.0534
trigger times: 9
Loss after 1142690280 batches: 0.0536
trigger times: 10
Loss after 1142821380 batches: 0.0521
trigger times: 11
Loss after 1142952480 batches: 0.0510
trigger times: 12
Loss after 1143083580 batches: 0.0511
trigger times: 13
Loss after 1143214680 batches: 0.0499
trigger times: 0
Loss after 1143345780 batches: 0.0491
trigger times: 1
Loss after 1143476880 batches: 0.0493
trigger times: 2
Loss after 1143607980 batches: 0.0494
trigger times: 3
Loss after 1143739080 batches: 0.0488
trigger times: 4
Loss after 1143870180 batches: 0.0483
trigger times: 5
Loss after 1144001280 batches: 0.0472
trigger times: 0
Loss after 1144132380 batches: 0.0469
trigger times: 1
Loss after 1144263480 batches: 0.0460
trigger times: 0
Loss after 1144394580 batches: 0.0469
trigger times: 1
Loss after 1144525680 batches: 0.0453
trigger times: 2
Loss after 1144656780 batches: 0.0457
trigger times: 3
Loss after 1144787880 batches: 0.0450
trigger times: 4
Loss after 1144918980 batches: 0.0446
trigger times: 5
Loss after 1145050080 batches: 0.0450
trigger times: 0
Loss after 1145181180 batches: 0.0442
trigger times: 1
Loss after 1145312280 batches: 0.0433
trigger times: 0
Loss after 1145443380 batches: 0.0426
trigger times: 1
Loss after 1145574480 batches: 0.0438
trigger times: 2
Loss after 1145705580 batches: 0.0424
trigger times: 3
Loss after 1145836680 batches: 0.0423
trigger times: 4
Loss after 1145967780 batches: 0.0418
trigger times: 5
Loss after 1146098880 batches: 0.0416
trigger times: 6
Loss after 1146229980 batches: 0.0410
trigger times: 0
Loss after 1146361080 batches: 0.0419
trigger times: 1
Loss after 1146492180 batches: 0.0413
trigger times: 0
Loss after 1146623280 batches: 0.0402
trigger times: 1
Loss after 1146754380 batches: 0.0402
trigger times: 2
Loss after 1146885480 batches: 0.0396
trigger times: 3
Loss after 1147016580 batches: 0.0395
trigger times: 4
Loss after 1147147680 batches: 0.0399
trigger times: 5
Loss after 1147278780 batches: 0.0388
trigger times: 6
Loss after 1147409880 batches: 0.0383
trigger times: 0
Loss after 1147540980 batches: 0.0392
trigger times: 1
Loss after 1147672080 batches: 0.0386
trigger times: 2
Loss after 1147803180 batches: 0.0383
trigger times: 3
Loss after 1147934280 batches: 0.0376
trigger times: 4
Loss after 1148065380 batches: 0.0381
trigger times: 5
Loss after 1148196480 batches: 0.0374
trigger times: 6
Loss after 1148327580 batches: 0.0377
trigger times: 7
Loss after 1148458680 batches: 0.0368
trigger times: 8
Loss after 1148589780 batches: 0.0370
trigger times: 9
Loss after 1148720880 batches: 0.0366
trigger times: 10
Loss after 1148851980 batches: 0.0374
trigger times: 11
Loss after 1148983080 batches: 0.0363
trigger times: 12
Loss after 1149114180 batches: 0.0360
trigger times: 13
Loss after 1149245280 batches: 0.0354
trigger times: 14
Loss after 1149376380 batches: 0.0355
trigger times: 15
Loss after 1149507480 batches: 0.0356
trigger times: 16
Loss after 1149638580 batches: 0.0353
trigger times: 17
Loss after 1149769680 batches: 0.0348
trigger times: 18
Loss after 1149900780 batches: 0.0351
trigger times: 19
Loss after 1150031880 batches: 0.0355
trigger times: 20
Early stopping!
Start to test process.
Loss after 1150162980 batches: 0.0346
Time to train on one home:  620.0163021087646
trigger times: 0
Loss after 1150256940 batches: 0.5710
trigger times: 0
Loss after 1150350900 batches: 0.2367
trigger times: 0
Loss after 1150444860 batches: 0.1453
trigger times: 0
Loss after 1150538820 batches: 0.1104
trigger times: 0
Loss after 1150632780 batches: 0.0948
trigger times: 0
Loss after 1150726740 batches: 0.0856
trigger times: 1
Loss after 1150820700 batches: 0.0775
trigger times: 2
Loss after 1150914660 batches: 0.0721
trigger times: 3
Loss after 1151008620 batches: 0.0679
trigger times: 4
Loss after 1151102580 batches: 0.0666
trigger times: 5
Loss after 1151196540 batches: 0.0634
trigger times: 6
Loss after 1151290500 batches: 0.0603
trigger times: 0
Loss after 1151384460 batches: 0.0587
trigger times: 1
Loss after 1151478420 batches: 0.0562
trigger times: 2
Loss after 1151572380 batches: 0.0553
trigger times: 3
Loss after 1151666340 batches: 0.0534
trigger times: 0
Loss after 1151760300 batches: 0.0526
trigger times: 1
Loss after 1151854260 batches: 0.0516
trigger times: 2
Loss after 1151948220 batches: 0.0505
trigger times: 3
Loss after 1152042180 batches: 0.0494
trigger times: 4
Loss after 1152136140 batches: 0.0492
trigger times: 5
Loss after 1152230100 batches: 0.0474
trigger times: 6
Loss after 1152324060 batches: 0.0472
trigger times: 7
Loss after 1152418020 batches: 0.0466
trigger times: 8
Loss after 1152511980 batches: 0.0451
trigger times: 9
Loss after 1152605940 batches: 0.0457
trigger times: 10
Loss after 1152699900 batches: 0.0440
trigger times: 11
Loss after 1152793860 batches: 0.0428
trigger times: 12
Loss after 1152887820 batches: 0.0430
trigger times: 13
Loss after 1152981780 batches: 0.0426
trigger times: 14
Loss after 1153075740 batches: 0.0423
trigger times: 15
Loss after 1153169700 batches: 0.0420
trigger times: 16
Loss after 1153263660 batches: 0.0418
trigger times: 17
Loss after 1153357620 batches: 0.0413
trigger times: 18
Loss after 1153451580 batches: 0.0405
trigger times: 19
Loss after 1153545540 batches: 0.0402
trigger times: 20
Early stopping!
Start to test process.
Loss after 1153639500 batches: 0.0399
Time to train on one home:  213.7853648662567
trigger times: 0
Loss after 1153770600 batches: 0.0687
trigger times: 1
Loss after 1153901700 batches: 0.0164
trigger times: 2
Loss after 1154032800 batches: 0.0119
trigger times: 3
Loss after 1154163900 batches: 0.0095
trigger times: 4
Loss after 1154295000 batches: 0.0083
trigger times: 5
Loss after 1154426100 batches: 0.0079
trigger times: 6
Loss after 1154557200 batches: 0.0071
trigger times: 7
Loss after 1154688300 batches: 0.0069
trigger times: 8
Loss after 1154819400 batches: 0.0065
trigger times: 9
Loss after 1154950500 batches: 0.0059
trigger times: 10
Loss after 1155081600 batches: 0.0058
trigger times: 11
Loss after 1155212700 batches: 0.0057
trigger times: 12
Loss after 1155343800 batches: 0.0054
trigger times: 13
Loss after 1155474900 batches: 0.0052
trigger times: 14
Loss after 1155606000 batches: 0.0051
trigger times: 15
Loss after 1155737100 batches: 0.0051
trigger times: 16
Loss after 1155868200 batches: 0.0048
trigger times: 17
Loss after 1155999300 batches: 0.0046
trigger times: 18
Loss after 1156130400 batches: 0.0045
trigger times: 19
Loss after 1156261500 batches: 0.0044
trigger times: 20
Early stopping!
Start to test process.
Loss after 1156392600 batches: 0.0043
Time to train on one home:  165.38235235214233
trigger times: 0
Loss after 1156523700 batches: 0.1593
trigger times: 0
Loss after 1156654800 batches: 0.0562
trigger times: 0
Loss after 1156785900 batches: 0.0406
trigger times: 0
Loss after 1156917000 batches: 0.0338
trigger times: 0
Loss after 1157048100 batches: 0.0306
trigger times: 0
Loss after 1157179200 batches: 0.0276
trigger times: 1
Loss after 1157310300 batches: 0.0255
trigger times: 2
Loss after 1157441400 batches: 0.0246
trigger times: 3
Loss after 1157572500 batches: 0.0230
trigger times: 4
Loss after 1157703600 batches: 0.0219
trigger times: 0
Loss after 1157834700 batches: 0.0210
trigger times: 1
Loss after 1157965800 batches: 0.0207
trigger times: 2
Loss after 1158096900 batches: 0.0201
trigger times: 0
Loss after 1158228000 batches: 0.0195
trigger times: 1
Loss after 1158359100 batches: 0.0190
trigger times: 2
Loss after 1158490200 batches: 0.0187
trigger times: 3
Loss after 1158621300 batches: 0.0182
trigger times: 4
Loss after 1158752400 batches: 0.0177
trigger times: 0
Loss after 1158883500 batches: 0.0178
trigger times: 1
Loss after 1159014600 batches: 0.0173
trigger times: 2
Loss after 1159145700 batches: 0.0167
trigger times: 3
Loss after 1159276800 batches: 0.0165
trigger times: 4
Loss after 1159407900 batches: 0.0165
trigger times: 5
Loss after 1159539000 batches: 0.0162
trigger times: 6
Loss after 1159670100 batches: 0.0158
trigger times: 7
Loss after 1159801200 batches: 0.0155
trigger times: 8
Loss after 1159932300 batches: 0.0158
trigger times: 9
Loss after 1160063400 batches: 0.0155
trigger times: 10
Loss after 1160194500 batches: 0.0153
trigger times: 0
Loss after 1160325600 batches: 0.0152
trigger times: 1
Loss after 1160456700 batches: 0.0150
trigger times: 2
Loss after 1160587800 batches: 0.0148
trigger times: 3
Loss after 1160718900 batches: 0.0148
trigger times: 4
Loss after 1160850000 batches: 0.0146
trigger times: 5
Loss after 1160981100 batches: 0.0145
trigger times: 6
Loss after 1161112200 batches: 0.0142
trigger times: 7
Loss after 1161243300 batches: 0.0140
trigger times: 8
Loss after 1161374400 batches: 0.0142
trigger times: 9
Loss after 1161505500 batches: 0.0138
trigger times: 10
Loss after 1161636600 batches: 0.0139
trigger times: 11
Loss after 1161767700 batches: 0.0134
trigger times: 12
Loss after 1161898800 batches: 0.0134
trigger times: 13
Loss after 1162029900 batches: 0.0133
trigger times: 14
Loss after 1162161000 batches: 0.0135
trigger times: 15
Loss after 1162292100 batches: 0.0129
trigger times: 16
Loss after 1162423200 batches: 0.0130
trigger times: 17
Loss after 1162554300 batches: 0.0129
trigger times: 18
Loss after 1162685400 batches: 0.0129
trigger times: 19
Loss after 1162816500 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 1162947600 batches: 0.0126
Time to train on one home:  376.52871346473694
trigger times: 0
Loss after 1163078700 batches: 0.3688
trigger times: 0
Loss after 1163209800 batches: 0.1428
trigger times: 0
Loss after 1163340900 batches: 0.1056
trigger times: 0
Loss after 1163472000 batches: 0.0909
trigger times: 1
Loss after 1163603100 batches: 0.0826
trigger times: 0
Loss after 1163734200 batches: 0.0771
trigger times: 1
Loss after 1163865300 batches: 0.0737
trigger times: 2
Loss after 1163996400 batches: 0.0712
trigger times: 3
Loss after 1164127500 batches: 0.0679
trigger times: 0
Loss after 1164258600 batches: 0.0655
trigger times: 1
Loss after 1164389700 batches: 0.0641
trigger times: 2
Loss after 1164520800 batches: 0.0635
trigger times: 0
Loss after 1164651900 batches: 0.0616
trigger times: 1
Loss after 1164783000 batches: 0.0601
trigger times: 0
Loss after 1164914100 batches: 0.0597
trigger times: 1
Loss after 1165045200 batches: 0.0583
trigger times: 2
Loss after 1165176300 batches: 0.0580
trigger times: 3
Loss after 1165307400 batches: 0.0567
trigger times: 4
Loss after 1165438500 batches: 0.0559
trigger times: 5
Loss after 1165569600 batches: 0.0553
trigger times: 6
Loss after 1165700700 batches: 0.0546
trigger times: 7
Loss after 1165831800 batches: 0.0537
trigger times: 8
Loss after 1165962900 batches: 0.0536
trigger times: 9
Loss after 1166094000 batches: 0.0534
trigger times: 10
Loss after 1166225100 batches: 0.0531
trigger times: 11
Loss after 1166356200 batches: 0.0517
trigger times: 0
Loss after 1166487300 batches: 0.0522
trigger times: 1
Loss after 1166618400 batches: 0.0512
trigger times: 2
Loss after 1166749500 batches: 0.0509
trigger times: 3
Loss after 1166880600 batches: 0.0510
trigger times: 4
Loss after 1167011700 batches: 0.0506
trigger times: 5
Loss after 1167142800 batches: 0.0503
trigger times: 0
Loss after 1167273900 batches: 0.0499
trigger times: 0
Loss after 1167405000 batches: 0.0497
trigger times: 0
Loss after 1167536100 batches: 0.0490
trigger times: 1
Loss after 1167667200 batches: 0.0489
trigger times: 2
Loss after 1167798300 batches: 0.0484
trigger times: 3
Loss after 1167929400 batches: 0.0476
trigger times: 4
Loss after 1168060500 batches: 0.0481
trigger times: 5
Loss after 1168191600 batches: 0.0475
trigger times: 6
Loss after 1168322700 batches: 0.0481
trigger times: 7
Loss after 1168453800 batches: 0.0472
trigger times: 8
Loss after 1168584900 batches: 0.0472
trigger times: 9
Loss after 1168716000 batches: 0.0468
trigger times: 10
Loss after 1168847100 batches: 0.0464
trigger times: 11
Loss after 1168978200 batches: 0.0460
trigger times: 12
Loss after 1169109300 batches: 0.0459
trigger times: 0
Loss after 1169240400 batches: 0.0463
trigger times: 0
Loss after 1169371500 batches: 0.0460
trigger times: 1
Loss after 1169502600 batches: 0.0460
trigger times: 2
Loss after 1169633700 batches: 0.0453
trigger times: 3
Loss after 1169764800 batches: 0.0457
trigger times: 4
Loss after 1169895900 batches: 0.0448
trigger times: 5
Loss after 1170027000 batches: 0.0448
trigger times: 6
Loss after 1170158100 batches: 0.0445
trigger times: 7
Loss after 1170289200 batches: 0.0448
trigger times: 8
Loss after 1170420300 batches: 0.0448
trigger times: 9
Loss after 1170551400 batches: 0.0447
trigger times: 10
Loss after 1170682500 batches: 0.0441
trigger times: 11
Loss after 1170813600 batches: 0.0444
trigger times: 12
Loss after 1170944700 batches: 0.0439
trigger times: 0
Loss after 1171075800 batches: 0.0439
trigger times: 1
Loss after 1171206900 batches: 0.0438
trigger times: 2
Loss after 1171338000 batches: 0.0441
trigger times: 3
Loss after 1171469100 batches: 0.0432
trigger times: 4
Loss after 1171600200 batches: 0.0431
trigger times: 5
Loss after 1171731300 batches: 0.0433
trigger times: 6
Loss after 1171862400 batches: 0.0431
trigger times: 7
Loss after 1171993500 batches: 0.0434
trigger times: 8
Loss after 1172124600 batches: 0.0428
trigger times: 9
Loss after 1172255700 batches: 0.0432
trigger times: 10
Loss after 1172386800 batches: 0.0425
trigger times: 11
Loss after 1172517900 batches: 0.0424
trigger times: 12
Loss after 1172649000 batches: 0.0417
trigger times: 13
Loss after 1172780100 batches: 0.0422
trigger times: 14
Loss after 1172911200 batches: 0.0422
trigger times: 15
Loss after 1173042300 batches: 0.0417
trigger times: 0
Loss after 1173173400 batches: 0.0419
trigger times: 1
Loss after 1173304500 batches: 0.0425
trigger times: 0
Loss after 1173435600 batches: 0.0418
trigger times: 1
Loss after 1173566700 batches: 0.0426
trigger times: 2
Loss after 1173697800 batches: 0.0418
trigger times: 3
Loss after 1173828900 batches: 0.0413
trigger times: 4
Loss after 1173960000 batches: 0.0416
trigger times: 5
Loss after 1174091100 batches: 0.0414
trigger times: 6
Loss after 1174222200 batches: 0.0407
trigger times: 7
Loss after 1174353300 batches: 0.0411
trigger times: 8
Loss after 1174484400 batches: 0.0407
trigger times: 9
Loss after 1174615500 batches: 0.0411
trigger times: 10
Loss after 1174746600 batches: 0.0404
trigger times: 11
Loss after 1174877700 batches: 0.0406
trigger times: 12
Loss after 1175008800 batches: 0.0408
trigger times: 0
Loss after 1175139900 batches: 0.0407
trigger times: 1
Loss after 1175271000 batches: 0.0404
trigger times: 2
Loss after 1175402100 batches: 0.0404
trigger times: 3
Loss after 1175533200 batches: 0.0402
trigger times: 4
Loss after 1175664300 batches: 0.0400
trigger times: 5
Loss after 1175795400 batches: 0.0400
trigger times: 6
Loss after 1175926500 batches: 0.0400
trigger times: 7
Loss after 1176057600 batches: 0.0402
trigger times: 8
Loss after 1176188700 batches: 0.0400
trigger times: 9
Loss after 1176319800 batches: 0.0394
trigger times: 10
Loss after 1176450900 batches: 0.0397
trigger times: 11
Loss after 1176582000 batches: 0.0399
trigger times: 12
Loss after 1176713100 batches: 0.0397
trigger times: 13
Loss after 1176844200 batches: 0.0395
trigger times: 14
Loss after 1176975300 batches: 0.0392
trigger times: 15
Loss after 1177106400 batches: 0.0400
trigger times: 16
Loss after 1177237500 batches: 0.0396
trigger times: 17
Loss after 1177368600 batches: 0.0398
trigger times: 18
Loss after 1177499700 batches: 0.0395
trigger times: 19
Loss after 1177630800 batches: 0.0395
trigger times: 20
Early stopping!
Start to test process.
Loss after 1177761900 batches: 0.0392
Time to train on one home:  832.0321855545044
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642, 0.03501720033327798, 0.03263172240869951, 0.030110256078310443]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217], [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944], [0.6491197148958842, 0.29803996281550993, 0.44160060986867417, 1.203530351478871, 0.5750418631238572, 28.433835927602637, 1775.19], [0.6497863001293607, 0.29730357979276123, 0.44701117944787117, 1.2068712720184533, 0.5756451041104418, 28.512766372815683, 1777.0521]]
Round_10_results:  [0.6497863001293607, 0.29730357979276123, 0.44701117944787117, 1.2068712720184533, 0.5756451041104418, 28.512766372815683, 1777.0521]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 9318 < 9319; dropping {'Training_Loss': 0.4433403728263719, 'Validation_Loss': 1.9142353137334187, 'Training_R2': 0.5513778133605645, 'Validation_R2': -0.7884125938927489, 'Training_F1': 0.7465750017903504, 'Validation_F1': 0.4674383514110856, 'Training_NEP': 0.5076441288405766, 'Validation_NEP': 1.3593995288805907, 'Training_NDE': 0.30218753966132567, 'Validation_NDE': 1.4241975151987931, 'Training_MAE': 33.31761434577362, 'Validation_MAE': 37.28037488507541, 'Training_MSE': 3987.8835, 'Validation_MSE': 5259.5225}.
trigger times: 0
Loss after 1177864500 batches: 0.4433
trigger times: 0
Loss after 1177967100 batches: 0.2043
trigger times: 1
Loss after 1178069700 batches: 0.1238
trigger times: 0
Loss after 1178172300 batches: 0.0965
trigger times: 1
Loss after 1178274900 batches: 0.0768
trigger times: 2
Loss after 1178377500 batches: 0.0700
trigger times: 0
Loss after 1178480100 batches: 0.0636
trigger times: 1
Loss after 1178582700 batches: 0.0580
trigger times: 2
Loss after 1178685300 batches: 0.0535
trigger times: 0
Loss after 1178787900 batches: 0.0518
trigger times: 1
Loss after 1178890500 batches: 0.0524
trigger times: 2
Loss after 1178993100 batches: 0.0483
trigger times: 3
Loss after 1179095700 batches: 0.0440
trigger times: 4
Loss after 1179198300 batches: 0.0438
trigger times: 5
Loss after 1179300900 batches: 0.0419
trigger times: 6
Loss after 1179403500 batches: 0.0413
trigger times: 0
Loss after 1179506100 batches: 0.0410
trigger times: 0
Loss after 1179608700 batches: 0.0382
trigger times: 1
Loss after 1179711300 batches: 0.0396
trigger times: 2
Loss after 1179813900 batches: 0.0374
trigger times: 3
Loss after 1179916500 batches: 0.0365
trigger times: 4
Loss after 1180019100 batches: 0.0365
trigger times: 5
Loss after 1180121700 batches: 0.0364
trigger times: 6
Loss after 1180224300 batches: 0.0355
trigger times: 7
Loss after 1180326900 batches: 0.0359
trigger times: 8
Loss after 1180429500 batches: 0.0352
trigger times: 9
Loss after 1180532100 batches: 0.0341
trigger times: 10
Loss after 1180634700 batches: 0.0350
trigger times: 0
Loss after 1180737300 batches: 0.0340
trigger times: 1
Loss after 1180839900 batches: 0.0333
trigger times: 2
Loss after 1180942500 batches: 0.0323
trigger times: 0
Loss after 1181045100 batches: 0.0332
trigger times: 1
Loss after 1181147700 batches: 0.0327
trigger times: 2
Loss after 1181250300 batches: 0.0328
trigger times: 0
Loss after 1181352900 batches: 0.0326
trigger times: 1
Loss after 1181455500 batches: 0.0324
trigger times: 2
Loss after 1181558100 batches: 0.0369
trigger times: 3
Loss after 1181660700 batches: 0.0339
trigger times: 0
Loss after 1181763300 batches: 0.0295
trigger times: 1
Loss after 1181865900 batches: 0.0292
trigger times: 2
Loss after 1181968500 batches: 0.0291
trigger times: 3
Loss after 1182071100 batches: 0.0324
trigger times: 4
Loss after 1182173700 batches: 0.0299
trigger times: 5
Loss after 1182276300 batches: 0.0292
trigger times: 6
Loss after 1182378900 batches: 0.0289
trigger times: 7
Loss after 1182481500 batches: 0.0295
trigger times: 8
Loss after 1182584100 batches: 0.0291
trigger times: 9
Loss after 1182686700 batches: 0.0288
trigger times: 10
Loss after 1182789300 batches: 0.0276
trigger times: 11
Loss after 1182891900 batches: 0.0264
trigger times: 12
Loss after 1182994500 batches: 0.0280
trigger times: 13
Loss after 1183097100 batches: 0.0264
trigger times: 0
Loss after 1183199700 batches: 0.0265
trigger times: 1
Loss after 1183302300 batches: 0.0264
trigger times: 0
Loss after 1183404900 batches: 0.0264
trigger times: 1
Loss after 1183507500 batches: 0.0263
trigger times: 2
Loss after 1183610100 batches: 0.0265
trigger times: 3
Loss after 1183712700 batches: 0.0272
trigger times: 4
Loss after 1183815300 batches: 0.0277
trigger times: 5
Loss after 1183917900 batches: 0.0258
trigger times: 6
Loss after 1184020500 batches: 0.0267
trigger times: 7
Loss after 1184123100 batches: 0.0259
trigger times: 8
Loss after 1184225700 batches: 0.0251
trigger times: 9
Loss after 1184328300 batches: 0.0246
trigger times: 0
Loss after 1184430900 batches: 0.0249
trigger times: 1
Loss after 1184533500 batches: 0.0247
trigger times: 0
Loss after 1184636100 batches: 0.0252
trigger times: 1
Loss after 1184738700 batches: 0.0246
trigger times: 2
Loss after 1184841300 batches: 0.0242
trigger times: 0
Loss after 1184943900 batches: 0.0248
trigger times: 0
Loss after 1185046500 batches: 0.0252
trigger times: 0
Loss after 1185149100 batches: 0.0243
trigger times: 1
Loss after 1185251700 batches: 0.0258
trigger times: 2
Loss after 1185354300 batches: 0.0242
trigger times: 0
Loss after 1185456900 batches: 0.0245
trigger times: 1
Loss after 1185559500 batches: 0.0297
trigger times: 2
Loss after 1185662100 batches: 0.0288
trigger times: 3
Loss after 1185764700 batches: 0.0245
trigger times: 4
Loss after 1185867300 batches: 0.0234
trigger times: 5
Loss after 1185969900 batches: 0.0250
trigger times: 0
Loss after 1186072500 batches: 0.0234
trigger times: 1
Loss after 1186175100 batches: 0.0234
trigger times: 2
Loss after 1186277700 batches: 0.0243
trigger times: 3
Loss after 1186380300 batches: 0.0261
trigger times: 4
Loss after 1186482900 batches: 0.0268
trigger times: 5
Loss after 1186585500 batches: 0.0245
trigger times: 6
Loss after 1186688100 batches: 0.0235
trigger times: 7
Loss after 1186790700 batches: 0.0278
trigger times: 8
Loss after 1186893300 batches: 0.0256
trigger times: 9
Loss after 1186995900 batches: 0.0231
trigger times: 10
Loss after 1187098500 batches: 0.0230
trigger times: 11
Loss after 1187201100 batches: 0.0228
trigger times: 12
Loss after 1187303700 batches: 0.0237
trigger times: 13
Loss after 1187406300 batches: 0.0226
trigger times: 14
Loss after 1187508900 batches: 0.0226
trigger times: 15
Loss after 1187611500 batches: 0.0233
trigger times: 16
Loss after 1187714100 batches: 0.0226
trigger times: 17
Loss after 1187816700 batches: 0.0225
trigger times: 18
Loss after 1187919300 batches: 0.0221
trigger times: 19
Loss after 1188021900 batches: 0.0224
trigger times: 20
Early stopping!
Start to test process.
Loss after 1188124500 batches: 0.0247
Time to train on one home:  609.4128804206848
trigger times: 0
Loss after 1188255600 batches: 0.2613
trigger times: 1
Loss after 1188386700 batches: 0.1119
trigger times: 2
Loss after 1188517800 batches: 0.0779
trigger times: 3
Loss after 1188648900 batches: 0.0646
trigger times: 4
Loss after 1188780000 batches: 0.0571
trigger times: 5
Loss after 1188911100 batches: 0.0516
trigger times: 6
Loss after 1189042200 batches: 0.0478
trigger times: 7
Loss after 1189173300 batches: 0.0453
trigger times: 8
Loss after 1189304400 batches: 0.0424
trigger times: 9
Loss after 1189435500 batches: 0.0407
trigger times: 10
Loss after 1189566600 batches: 0.0386
trigger times: 11
Loss after 1189697700 batches: 0.0373
trigger times: 12
Loss after 1189828800 batches: 0.0366
trigger times: 13
Loss after 1189959900 batches: 0.0355
trigger times: 14
Loss after 1190091000 batches: 0.0351
trigger times: 15
Loss after 1190222100 batches: 0.0340
trigger times: 16
Loss after 1190353200 batches: 0.0334
trigger times: 17
Loss after 1190484300 batches: 0.0323
trigger times: 18
Loss after 1190615400 batches: 0.0318
trigger times: 19
Loss after 1190746500 batches: 0.0314
trigger times: 20
Early stopping!
Start to test process.
Loss after 1190877600 batches: 0.0313
Time to train on one home:  164.59732127189636
trigger times: 0
Loss after 1191008700 batches: 0.4954
trigger times: 1
Loss after 1191139800 batches: 0.1958
trigger times: 2
Loss after 1191270900 batches: 0.1256
trigger times: 3
Loss after 1191402000 batches: 0.1016
trigger times: 4
Loss after 1191533100 batches: 0.0895
trigger times: 5
Loss after 1191664200 batches: 0.0810
trigger times: 6
Loss after 1191795300 batches: 0.0741
trigger times: 7
Loss after 1191926400 batches: 0.0699
trigger times: 8
Loss after 1192057500 batches: 0.0661
trigger times: 9
Loss after 1192188600 batches: 0.0635
trigger times: 10
Loss after 1192319700 batches: 0.0613
trigger times: 11
Loss after 1192450800 batches: 0.0595
trigger times: 12
Loss after 1192581900 batches: 0.0565
trigger times: 13
Loss after 1192713000 batches: 0.0554
trigger times: 0
Loss after 1192844100 batches: 0.0541
trigger times: 0
Loss after 1192975200 batches: 0.0522
trigger times: 1
Loss after 1193106300 batches: 0.0512
trigger times: 2
Loss after 1193237400 batches: 0.0505
trigger times: 3
Loss after 1193368500 batches: 0.0496
trigger times: 4
Loss after 1193499600 batches: 0.0483
trigger times: 5
Loss after 1193630700 batches: 0.0479
trigger times: 0
Loss after 1193761800 batches: 0.0471
trigger times: 1
Loss after 1193892900 batches: 0.0464
trigger times: 2
Loss after 1194024000 batches: 0.0461
trigger times: 3
Loss after 1194155100 batches: 0.0450
trigger times: 4
Loss after 1194286200 batches: 0.0448
trigger times: 5
Loss after 1194417300 batches: 0.0439
trigger times: 6
Loss after 1194548400 batches: 0.0432
trigger times: 7
Loss after 1194679500 batches: 0.0432
trigger times: 0
Loss after 1194810600 batches: 0.0430
trigger times: 0
Loss after 1194941700 batches: 0.0414
trigger times: 1
Loss after 1195072800 batches: 0.0412
trigger times: 2
Loss after 1195203900 batches: 0.0408
trigger times: 0
Loss after 1195335000 batches: 0.0408
trigger times: 1
Loss after 1195466100 batches: 0.0405
trigger times: 2
Loss after 1195597200 batches: 0.0401
trigger times: 3
Loss after 1195728300 batches: 0.0398
trigger times: 4
Loss after 1195859400 batches: 0.0388
trigger times: 5
Loss after 1195990500 batches: 0.0386
trigger times: 6
Loss after 1196121600 batches: 0.0388
trigger times: 7
Loss after 1196252700 batches: 0.0381
trigger times: 8
Loss after 1196383800 batches: 0.0381
trigger times: 9
Loss after 1196514900 batches: 0.0374
trigger times: 10
Loss after 1196646000 batches: 0.0375
trigger times: 11
Loss after 1196777100 batches: 0.0375
trigger times: 12
Loss after 1196908200 batches: 0.0364
trigger times: 13
Loss after 1197039300 batches: 0.0367
trigger times: 14
Loss after 1197170400 batches: 0.0363
trigger times: 15
Loss after 1197301500 batches: 0.0364
trigger times: 16
Loss after 1197432600 batches: 0.0364
trigger times: 17
Loss after 1197563700 batches: 0.0362
trigger times: 18
Loss after 1197694800 batches: 0.0351
trigger times: 19
Loss after 1197825900 batches: 0.0349
trigger times: 20
Early stopping!
Start to test process.
Loss after 1197957000 batches: 0.0347
Time to train on one home:  404.7827055454254
trigger times: 0
Loss after 1198085640 batches: 0.1857
trigger times: 0
Loss after 1198214280 batches: 0.0688
trigger times: 0
Loss after 1198342920 batches: 0.0479
trigger times: 1
Loss after 1198471560 batches: 0.0402
trigger times: 2
Loss after 1198600200 batches: 0.0360
trigger times: 3
Loss after 1198728840 batches: 0.0336
trigger times: 4
Loss after 1198857480 batches: 0.0316
trigger times: 5
Loss after 1198986120 batches: 0.0299
trigger times: 6
Loss after 1199114760 batches: 0.0293
trigger times: 7
Loss after 1199243400 batches: 0.0273
trigger times: 8
Loss after 1199372040 batches: 0.0267
trigger times: 9
Loss after 1199500680 batches: 0.0264
trigger times: 10
Loss after 1199629320 batches: 0.0252
trigger times: 0
Loss after 1199757960 batches: 0.0242
trigger times: 1
Loss after 1199886600 batches: 0.0239
trigger times: 0
Loss after 1200015240 batches: 0.0234
trigger times: 1
Loss after 1200143880 batches: 0.0234
trigger times: 2
Loss after 1200272520 batches: 0.0227
trigger times: 3
Loss after 1200401160 batches: 0.0218
trigger times: 4
Loss after 1200529800 batches: 0.0221
trigger times: 0
Loss after 1200658440 batches: 0.0215
trigger times: 0
Loss after 1200787080 batches: 0.0213
trigger times: 1
Loss after 1200915720 batches: 0.0209
trigger times: 2
Loss after 1201044360 batches: 0.0209
trigger times: 3
Loss after 1201173000 batches: 0.0209
trigger times: 4
Loss after 1201301640 batches: 0.0200
trigger times: 5
Loss after 1201430280 batches: 0.0201
trigger times: 0
Loss after 1201558920 batches: 0.0199
trigger times: 0
Loss after 1201687560 batches: 0.0199
trigger times: 1
Loss after 1201816200 batches: 0.0193
trigger times: 0
Loss after 1201944840 batches: 0.0192
trigger times: 0
Loss after 1202073480 batches: 0.0193
trigger times: 0
Loss after 1202202120 batches: 0.0191
trigger times: 1
Loss after 1202330760 batches: 0.0191
trigger times: 2
Loss after 1202459400 batches: 0.0186
trigger times: 3
Loss after 1202588040 batches: 0.0185
trigger times: 4
Loss after 1202716680 batches: 0.0182
trigger times: 5
Loss after 1202845320 batches: 0.0181
trigger times: 6
Loss after 1202973960 batches: 0.0180
trigger times: 7
Loss after 1203102600 batches: 0.0180
trigger times: 8
Loss after 1203231240 batches: 0.0180
trigger times: 0
Loss after 1203359880 batches: 0.0172
trigger times: 1
Loss after 1203488520 batches: 0.0175
trigger times: 0
Loss after 1203617160 batches: 0.0175
trigger times: 1
Loss after 1203745800 batches: 0.0173
trigger times: 2
Loss after 1203874440 batches: 0.0172
trigger times: 3
Loss after 1204003080 batches: 0.0168
trigger times: 0
Loss after 1204131720 batches: 0.0172
trigger times: 0
Loss after 1204260360 batches: 0.0168
trigger times: 0
Loss after 1204389000 batches: 0.0167
trigger times: 1
Loss after 1204517640 batches: 0.0166
trigger times: 2
Loss after 1204646280 batches: 0.0169
trigger times: 3
Loss after 1204774920 batches: 0.0165
trigger times: 0
Loss after 1204903560 batches: 0.0166
trigger times: 0
Loss after 1205032200 batches: 0.0163
trigger times: 1
Loss after 1205160840 batches: 0.0162
trigger times: 2
Loss after 1205289480 batches: 0.0159
trigger times: 0
Loss after 1205418120 batches: 0.0156
trigger times: 0
Loss after 1205546760 batches: 0.0160
trigger times: 1
Loss after 1205675400 batches: 0.0159
trigger times: 2
Loss after 1205804040 batches: 0.0156
trigger times: 3
Loss after 1205932680 batches: 0.0154
trigger times: 4
Loss after 1206061320 batches: 0.0153
trigger times: 5
Loss after 1206189960 batches: 0.0152
trigger times: 6
Loss after 1206318600 batches: 0.0156
trigger times: 7
Loss after 1206447240 batches: 0.0153
trigger times: 0
Loss after 1206575880 batches: 0.0154
trigger times: 1
Loss after 1206704520 batches: 0.0153
trigger times: 2
Loss after 1206833160 batches: 0.0149
trigger times: 3
Loss after 1206961800 batches: 0.0149
trigger times: 4
Loss after 1207090440 batches: 0.0146
trigger times: 5
Loss after 1207219080 batches: 0.0147
trigger times: 6
Loss after 1207347720 batches: 0.0145
trigger times: 7
Loss after 1207476360 batches: 0.0147
trigger times: 8
Loss after 1207605000 batches: 0.0144
trigger times: 9
Loss after 1207733640 batches: 0.0143
trigger times: 10
Loss after 1207862280 batches: 0.0142
trigger times: 0
Loss after 1207990920 batches: 0.0142
trigger times: 1
Loss after 1208119560 batches: 0.0143
trigger times: 2
Loss after 1208248200 batches: 0.0143
trigger times: 3
Loss after 1208376840 batches: 0.0139
trigger times: 4
Loss after 1208505480 batches: 0.0138
trigger times: 0
Loss after 1208634120 batches: 0.0140
trigger times: 1
Loss after 1208762760 batches: 0.0141
trigger times: 2
Loss after 1208891400 batches: 0.0139
trigger times: 3
Loss after 1209020040 batches: 0.0135
trigger times: 4
Loss after 1209148680 batches: 0.0145
trigger times: 5
Loss after 1209277320 batches: 0.0142
trigger times: 6
Loss after 1209405960 batches: 0.0138
trigger times: 7
Loss after 1209534600 batches: 0.0137
trigger times: 0
Loss after 1209663240 batches: 0.0136
trigger times: 1
Loss after 1209791880 batches: 0.0136
trigger times: 2
Loss after 1209920520 batches: 0.0135
trigger times: 0
Loss after 1210049160 batches: 0.0135
trigger times: 1
Loss after 1210177800 batches: 0.0135
trigger times: 2
Loss after 1210306440 batches: 0.0133
trigger times: 3
Loss after 1210435080 batches: 0.0133
trigger times: 0
Loss after 1210563720 batches: 0.0132
trigger times: 1
Loss after 1210692360 batches: 0.0135
trigger times: 2
Loss after 1210821000 batches: 0.0132
trigger times: 3
Loss after 1210949640 batches: 0.0132
trigger times: 4
Loss after 1211078280 batches: 0.0135
trigger times: 5
Loss after 1211206920 batches: 0.0130
trigger times: 6
Loss after 1211335560 batches: 0.0131
trigger times: 7
Loss after 1211464200 batches: 0.0133
trigger times: 8
Loss after 1211592840 batches: 0.0132
trigger times: 9
Loss after 1211721480 batches: 0.0132
trigger times: 10
Loss after 1211850120 batches: 0.0128
trigger times: 11
Loss after 1211978760 batches: 0.0126
trigger times: 12
Loss after 1212107400 batches: 0.0130
trigger times: 13
Loss after 1212236040 batches: 0.0125
trigger times: 0
Loss after 1212364680 batches: 0.0128
trigger times: 1
Loss after 1212493320 batches: 0.0127
trigger times: 2
Loss after 1212621960 batches: 0.0125
trigger times: 3
Loss after 1212750600 batches: 0.0128
trigger times: 4
Loss after 1212879240 batches: 0.0127
trigger times: 5
Loss after 1213007880 batches: 0.0125
trigger times: 6
Loss after 1213136520 batches: 0.0125
trigger times: 7
Loss after 1213265160 batches: 0.0126
trigger times: 8
Loss after 1213393800 batches: 0.0124
trigger times: 9
Loss after 1213522440 batches: 0.0123
trigger times: 10
Loss after 1213651080 batches: 0.0121
trigger times: 11
Loss after 1213779720 batches: 0.0122
trigger times: 12
Loss after 1213908360 batches: 0.0122
trigger times: 13
Loss after 1214037000 batches: 0.0124
trigger times: 14
Loss after 1214165640 batches: 0.0121
trigger times: 15
Loss after 1214294280 batches: 0.0122
trigger times: 0
Loss after 1214422920 batches: 0.0123
trigger times: 1
Loss after 1214551560 batches: 0.0122
trigger times: 0
Loss after 1214680200 batches: 0.0120
trigger times: 1
Loss after 1214808840 batches: 0.0119
trigger times: 0
Loss after 1214937480 batches: 0.0122
trigger times: 1
Loss after 1215066120 batches: 0.0119
trigger times: 2
Loss after 1215194760 batches: 0.0119
trigger times: 3
Loss after 1215323400 batches: 0.0116
trigger times: 4
Loss after 1215452040 batches: 0.0116
trigger times: 5
Loss after 1215580680 batches: 0.0121
trigger times: 6
Loss after 1215709320 batches: 0.0116
trigger times: 7
Loss after 1215837960 batches: 0.0115
trigger times: 8
Loss after 1215966600 batches: 0.0116
trigger times: 9
Loss after 1216095240 batches: 0.0115
trigger times: 10
Loss after 1216223880 batches: 0.0114
trigger times: 11
Loss after 1216352520 batches: 0.0114
trigger times: 12
Loss after 1216481160 batches: 0.0118
trigger times: 13
Loss after 1216609800 batches: 0.0116
trigger times: 14
Loss after 1216738440 batches: 0.0113
trigger times: 15
Loss after 1216867080 batches: 0.0114
trigger times: 16
Loss after 1216995720 batches: 0.0113
trigger times: 17
Loss after 1217124360 batches: 0.0114
trigger times: 18
Loss after 1217253000 batches: 0.0116
trigger times: 19
Loss after 1217381640 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 1217510280 batches: 0.0110
Time to train on one home:  1098.3308410644531
trigger times: 0
Loss after 1217641380 batches: 0.5458
trigger times: 1
Loss after 1217772480 batches: 0.2052
trigger times: 2
Loss after 1217903580 batches: 0.1253
trigger times: 3
Loss after 1218034680 batches: 0.1003
trigger times: 4
Loss after 1218165780 batches: 0.0865
trigger times: 5
Loss after 1218296880 batches: 0.0788
trigger times: 6
Loss after 1218427980 batches: 0.0733
trigger times: 7
Loss after 1218559080 batches: 0.0687
trigger times: 8
Loss after 1218690180 batches: 0.0663
trigger times: 9
Loss after 1218821280 batches: 0.0637
trigger times: 10
Loss after 1218952380 batches: 0.0603
trigger times: 11
Loss after 1219083480 batches: 0.0583
trigger times: 12
Loss after 1219214580 batches: 0.0570
trigger times: 13
Loss after 1219345680 batches: 0.0545
trigger times: 14
Loss after 1219476780 batches: 0.0539
trigger times: 15
Loss after 1219607880 batches: 0.0531
trigger times: 16
Loss after 1219738980 batches: 0.0512
trigger times: 17
Loss after 1219870080 batches: 0.0500
trigger times: 18
Loss after 1220001180 batches: 0.0492
trigger times: 19
Loss after 1220132280 batches: 0.0486
trigger times: 20
Early stopping!
Start to test process.
Loss after 1220263380 batches: 0.0473
Time to train on one home:  163.97270035743713
trigger times: 0
Loss after 1220394480 batches: 0.5824
trigger times: 1
Loss after 1220525580 batches: 0.2930
trigger times: 2
Loss after 1220656680 batches: 0.1798
trigger times: 3
Loss after 1220787780 batches: 0.1367
trigger times: 4
Loss after 1220918880 batches: 0.1125
trigger times: 5
Loss after 1221049980 batches: 0.0949
trigger times: 6
Loss after 1221181080 batches: 0.0873
trigger times: 7
Loss after 1221312180 batches: 0.0840
trigger times: 0
Loss after 1221443280 batches: 0.0757
trigger times: 0
Loss after 1221574380 batches: 0.0718
trigger times: 1
Loss after 1221705480 batches: 0.0664
trigger times: 2
Loss after 1221836580 batches: 0.0635
trigger times: 3
Loss after 1221967680 batches: 0.0628
trigger times: 4
Loss after 1222098780 batches: 0.0608
trigger times: 5
Loss after 1222229880 batches: 0.0589
trigger times: 6
Loss after 1222360980 batches: 0.0575
trigger times: 7
Loss after 1222492080 batches: 0.0562
trigger times: 0
Loss after 1222623180 batches: 0.0550
trigger times: 1
Loss after 1222754280 batches: 0.0537
trigger times: 2
Loss after 1222885380 batches: 0.0525
trigger times: 3
Loss after 1223016480 batches: 0.0534
trigger times: 0
Loss after 1223147580 batches: 0.0524
trigger times: 1
Loss after 1223278680 batches: 0.0502
trigger times: 2
Loss after 1223409780 batches: 0.0504
trigger times: 3
Loss after 1223540880 batches: 0.0485
trigger times: 4
Loss after 1223671980 batches: 0.0480
trigger times: 0
Loss after 1223803080 batches: 0.0475
trigger times: 1
Loss after 1223934180 batches: 0.0465
trigger times: 2
Loss after 1224065280 batches: 0.0478
trigger times: 3
Loss after 1224196380 batches: 0.0459
trigger times: 4
Loss after 1224327480 batches: 0.0464
trigger times: 5
Loss after 1224458580 batches: 0.0456
trigger times: 6
Loss after 1224589680 batches: 0.0447
trigger times: 0
Loss after 1224720780 batches: 0.0450
trigger times: 1
Loss after 1224851880 batches: 0.0430
trigger times: 0
Loss after 1224982980 batches: 0.0423
trigger times: 1
Loss after 1225114080 batches: 0.0434
trigger times: 0
Loss after 1225245180 batches: 0.0450
trigger times: 1
Loss after 1225376280 batches: 0.0427
trigger times: 2
Loss after 1225507380 batches: 0.0420
trigger times: 3
Loss after 1225638480 batches: 0.0429
trigger times: 4
Loss after 1225769580 batches: 0.0420
trigger times: 0
Loss after 1225900680 batches: 0.0398
trigger times: 1
Loss after 1226031780 batches: 0.0415
trigger times: 0
Loss after 1226162880 batches: 0.0421
trigger times: 1
Loss after 1226293980 batches: 0.0401
trigger times: 2
Loss after 1226425080 batches: 0.0386
trigger times: 3
Loss after 1226556180 batches: 0.0409
trigger times: 0
Loss after 1226687280 batches: 0.0402
trigger times: 1
Loss after 1226818380 batches: 0.0390
trigger times: 0
Loss after 1226949480 batches: 0.0389
trigger times: 1
Loss after 1227080580 batches: 0.0409
trigger times: 2
Loss after 1227211680 batches: 0.0415
trigger times: 3
Loss after 1227342780 batches: 0.0389
trigger times: 4
Loss after 1227473880 batches: 0.0389
trigger times: 0
Loss after 1227604980 batches: 0.0383
trigger times: 1
Loss after 1227736080 batches: 0.0389
trigger times: 0
Loss after 1227867180 batches: 0.0375
trigger times: 1
Loss after 1227998280 batches: 0.0383
trigger times: 2
Loss after 1228129380 batches: 0.0361
trigger times: 3
Loss after 1228260480 batches: 0.0366
trigger times: 4
Loss after 1228391580 batches: 0.0352
trigger times: 0
Loss after 1228522680 batches: 0.0347
trigger times: 1
Loss after 1228653780 batches: 0.0361
trigger times: 2
Loss after 1228784880 batches: 0.0354
trigger times: 3
Loss after 1228915980 batches: 0.0359
trigger times: 4
Loss after 1229047080 batches: 0.0362
trigger times: 5
Loss after 1229178180 batches: 0.0342
trigger times: 0
Loss after 1229309280 batches: 0.0359
trigger times: 1
Loss after 1229440380 batches: 0.0348
trigger times: 2
Loss after 1229571480 batches: 0.0332
trigger times: 3
Loss after 1229702580 batches: 0.0332
trigger times: 0
Loss after 1229833680 batches: 0.0343
trigger times: 1
Loss after 1229964780 batches: 0.0327
trigger times: 2
Loss after 1230095880 batches: 0.0341
trigger times: 3
Loss after 1230226980 batches: 0.0340
trigger times: 4
Loss after 1230358080 batches: 0.0326
trigger times: 5
Loss after 1230489180 batches: 0.0317
trigger times: 6
Loss after 1230620280 batches: 0.0337
trigger times: 0
Loss after 1230751380 batches: 0.0335
trigger times: 1
Loss after 1230882480 batches: 0.0345
trigger times: 2
Loss after 1231013580 batches: 0.0321
trigger times: 0
Loss after 1231144680 batches: 0.0326
trigger times: 1
Loss after 1231275780 batches: 0.0328
trigger times: 0
Loss after 1231406880 batches: 0.0324
trigger times: 1
Loss after 1231537980 batches: 0.0318
trigger times: 2
Loss after 1231669080 batches: 0.0330
trigger times: 3
Loss after 1231800180 batches: 0.0326
trigger times: 4
Loss after 1231931280 batches: 0.0318
trigger times: 5
Loss after 1232062380 batches: 0.0318
trigger times: 6
Loss after 1232193480 batches: 0.0318
trigger times: 7
Loss after 1232324580 batches: 0.0316
trigger times: 8
Loss after 1232455680 batches: 0.0315
trigger times: 9
Loss after 1232586780 batches: 0.0315
trigger times: 0
Loss after 1232717880 batches: 0.0310
trigger times: 1
Loss after 1232848980 batches: 0.0305
trigger times: 0
Loss after 1232980080 batches: 0.0314
trigger times: 1
Loss after 1233111180 batches: 0.0297
trigger times: 0
Loss after 1233242280 batches: 0.0313
trigger times: 1
Loss after 1233373380 batches: 0.0313
trigger times: 0
Loss after 1233504480 batches: 0.0313
trigger times: 1
Loss after 1233635580 batches: 0.0335
trigger times: 2
Loss after 1233766680 batches: 0.0316
trigger times: 3
Loss after 1233897780 batches: 0.0297
trigger times: 4
Loss after 1234028880 batches: 0.0308
trigger times: 5
Loss after 1234159980 batches: 0.0298
trigger times: 6
Loss after 1234291080 batches: 0.0298
trigger times: 7
Loss after 1234422180 batches: 0.0286
trigger times: 8
Loss after 1234553280 batches: 0.0306
trigger times: 9
Loss after 1234684380 batches: 0.0308
trigger times: 0
Loss after 1234815480 batches: 0.0303
trigger times: 1
Loss after 1234946580 batches: 0.0303
trigger times: 2
Loss after 1235077680 batches: 0.0288
trigger times: 0
Loss after 1235208780 batches: 0.0296
trigger times: 1
Loss after 1235339880 batches: 0.0299
trigger times: 0
Loss after 1235470980 batches: 0.0305
trigger times: 1
Loss after 1235602080 batches: 0.0307
trigger times: 2
Loss after 1235733180 batches: 0.0311
trigger times: 3
Loss after 1235864280 batches: 0.0311
trigger times: 4
Loss after 1235995380 batches: 0.0297
trigger times: 5
Loss after 1236126480 batches: 0.0287
trigger times: 6
Loss after 1236257580 batches: 0.0279
trigger times: 7
Loss after 1236388680 batches: 0.0295
trigger times: 8
Loss after 1236519780 batches: 0.0273
trigger times: 9
Loss after 1236650880 batches: 0.0270
trigger times: 10
Loss after 1236781980 batches: 0.0283
trigger times: 0
Loss after 1236913080 batches: 0.0279
trigger times: 1
Loss after 1237044180 batches: 0.0272
trigger times: 2
Loss after 1237175280 batches: 0.0297
trigger times: 3
Loss after 1237306380 batches: 0.0281
trigger times: 4
Loss after 1237437480 batches: 0.0286
trigger times: 5
Loss after 1237568580 batches: 0.0271
trigger times: 6
Loss after 1237699680 batches: 0.0270
trigger times: 7
Loss after 1237830780 batches: 0.0260
trigger times: 8
Loss after 1237961880 batches: 0.0267
trigger times: 9
Loss after 1238092980 batches: 0.0270
trigger times: 10
Loss after 1238224080 batches: 0.0259
trigger times: 0
Loss after 1238355180 batches: 0.0264
trigger times: 1
Loss after 1238486280 batches: 0.0267
trigger times: 2
Loss after 1238617380 batches: 0.0264
trigger times: 3
Loss after 1238748480 batches: 0.0258
trigger times: 4
Loss after 1238879580 batches: 0.0262
trigger times: 5
Loss after 1239010680 batches: 0.0255
trigger times: 6
Loss after 1239141780 batches: 0.0273
trigger times: 7
Loss after 1239272880 batches: 0.0276
trigger times: 8
Loss after 1239403980 batches: 0.0271
trigger times: 9
Loss after 1239535080 batches: 0.0257
trigger times: 10
Loss after 1239666180 batches: 0.0263
trigger times: 11
Loss after 1239797280 batches: 0.0262
trigger times: 12
Loss after 1239928380 batches: 0.0270
trigger times: 13
Loss after 1240059480 batches: 0.0261
trigger times: 14
Loss after 1240190580 batches: 0.0264
trigger times: 15
Loss after 1240321680 batches: 0.0253
trigger times: 16
Loss after 1240452780 batches: 0.0249
trigger times: 17
Loss after 1240583880 batches: 0.0253
trigger times: 18
Loss after 1240714980 batches: 0.0251
trigger times: 19
Loss after 1240846080 batches: 0.0251
trigger times: 20
Early stopping!
Start to test process.
Loss after 1240977180 batches: 0.0248
Time to train on one home:  1158.7943737506866
trigger times: 0
Loss after 1241108280 batches: 0.1527
trigger times: 1
Loss after 1241239380 batches: 0.0506
trigger times: 2
Loss after 1241370480 batches: 0.0351
trigger times: 3
Loss after 1241501580 batches: 0.0291
trigger times: 4
Loss after 1241632680 batches: 0.0260
trigger times: 5
Loss after 1241763780 batches: 0.0242
trigger times: 6
Loss after 1241894880 batches: 0.0225
trigger times: 7
Loss after 1242025980 batches: 0.0217
trigger times: 8
Loss after 1242157080 batches: 0.0204
trigger times: 9
Loss after 1242288180 batches: 0.0195
trigger times: 10
Loss after 1242419280 batches: 0.0191
trigger times: 11
Loss after 1242550380 batches: 0.0185
trigger times: 12
Loss after 1242681480 batches: 0.0179
trigger times: 13
Loss after 1242812580 batches: 0.0175
trigger times: 14
Loss after 1242943680 batches: 0.0170
trigger times: 15
Loss after 1243074780 batches: 0.0166
trigger times: 16
Loss after 1243205880 batches: 0.0164
trigger times: 17
Loss after 1243336980 batches: 0.0159
trigger times: 18
Loss after 1243468080 batches: 0.0157
trigger times: 19
Loss after 1243599180 batches: 0.0155
trigger times: 20
Early stopping!
Start to test process.
Loss after 1243730280 batches: 0.0156
Time to train on one home:  164.70699381828308
trigger times: 0
Loss after 1243861380 batches: 0.2146
trigger times: 0
Loss after 1243992480 batches: 0.0702
trigger times: 0
Loss after 1244123580 batches: 0.0459
trigger times: 0
Loss after 1244254680 batches: 0.0380
trigger times: 1
Loss after 1244385780 batches: 0.0335
trigger times: 2
Loss after 1244516880 batches: 0.0307
trigger times: 3
Loss after 1244647980 batches: 0.0287
trigger times: 4
Loss after 1244779080 batches: 0.0273
trigger times: 5
Loss after 1244910180 batches: 0.0263
trigger times: 6
Loss after 1245041280 batches: 0.0251
trigger times: 7
Loss after 1245172380 batches: 0.0242
trigger times: 8
Loss after 1245303480 batches: 0.0235
trigger times: 9
Loss after 1245434580 batches: 0.0225
trigger times: 10
Loss after 1245565680 batches: 0.0221
trigger times: 11
Loss after 1245696780 batches: 0.0215
trigger times: 12
Loss after 1245827880 batches: 0.0211
trigger times: 13
Loss after 1245958980 batches: 0.0203
trigger times: 14
Loss after 1246090080 batches: 0.0201
trigger times: 15
Loss after 1246221180 batches: 0.0197
trigger times: 16
Loss after 1246352280 batches: 0.0191
trigger times: 17
Loss after 1246483380 batches: 0.0188
trigger times: 18
Loss after 1246614480 batches: 0.0182
trigger times: 19
Loss after 1246745580 batches: 0.0182
trigger times: 20
Early stopping!
Start to test process.
Loss after 1246876680 batches: 0.0182
Time to train on one home:  187.17348790168762
trigger times: 0
Loss after 1246955280 batches: 0.4622
trigger times: 1
Loss after 1247033880 batches: 0.1529
trigger times: 2
Loss after 1247112480 batches: 0.0867
trigger times: 3
Loss after 1247191080 batches: 0.0674
trigger times: 4
Loss after 1247269680 batches: 0.0583
trigger times: 5
Loss after 1247348280 batches: 0.0525
trigger times: 6
Loss after 1247426880 batches: 0.0491
trigger times: 7
Loss after 1247505480 batches: 0.0453
trigger times: 8
Loss after 1247584080 batches: 0.0439
trigger times: 9
Loss after 1247662680 batches: 0.0419
trigger times: 10
Loss after 1247741280 batches: 0.0401
trigger times: 11
Loss after 1247819880 batches: 0.0384
trigger times: 12
Loss after 1247898480 batches: 0.0383
trigger times: 13
Loss after 1247977080 batches: 0.0363
trigger times: 14
Loss after 1248055680 batches: 0.0356
trigger times: 15
Loss after 1248134280 batches: 0.0343
trigger times: 16
Loss after 1248212880 batches: 0.0342
trigger times: 17
Loss after 1248291480 batches: 0.0331
trigger times: 18
Loss after 1248370080 batches: 0.0336
trigger times: 19
Loss after 1248448680 batches: 0.0319
trigger times: 0
Loss after 1248527280 batches: 0.0316
trigger times: 1
Loss after 1248605880 batches: 0.0312
trigger times: 0
Loss after 1248684480 batches: 0.0306
trigger times: 1
Loss after 1248763080 batches: 0.0296
trigger times: 2
Loss after 1248841680 batches: 0.0293
trigger times: 0
Loss after 1248920280 batches: 0.0293
trigger times: 1
Loss after 1248998880 batches: 0.0286
trigger times: 2
Loss after 1249077480 batches: 0.0290
trigger times: 0
Loss after 1249156080 batches: 0.0279
trigger times: 1
Loss after 1249234680 batches: 0.0284
trigger times: 2
Loss after 1249313280 batches: 0.0272
trigger times: 0
Loss after 1249391880 batches: 0.0275
trigger times: 0
Loss after 1249470480 batches: 0.0272
trigger times: 0
Loss after 1249549080 batches: 0.0264
trigger times: 1
Loss after 1249627680 batches: 0.0272
trigger times: 2
Loss after 1249706280 batches: 0.0272
trigger times: 3
Loss after 1249784880 batches: 0.0268
trigger times: 0
Loss after 1249863480 batches: 0.0255
trigger times: 1
Loss after 1249942080 batches: 0.0262
trigger times: 2
Loss after 1250020680 batches: 0.0249
trigger times: 3
Loss after 1250099280 batches: 0.0251
trigger times: 4
Loss after 1250177880 batches: 0.0260
trigger times: 5
Loss after 1250256480 batches: 0.0258
trigger times: 6
Loss after 1250335080 batches: 0.0252
trigger times: 7
Loss after 1250413680 batches: 0.0246
trigger times: 8
Loss after 1250492280 batches: 0.0241
trigger times: 9
Loss after 1250570880 batches: 0.0246
trigger times: 10
Loss after 1250649480 batches: 0.0243
trigger times: 11
Loss after 1250728080 batches: 0.0245
trigger times: 0
Loss after 1250806680 batches: 0.0244
trigger times: 1
Loss after 1250885280 batches: 0.0241
trigger times: 2
Loss after 1250963880 batches: 0.0242
trigger times: 3
Loss after 1251042480 batches: 0.0234
trigger times: 4
Loss after 1251121080 batches: 0.0239
trigger times: 5
Loss after 1251199680 batches: 0.0240
trigger times: 6
Loss after 1251278280 batches: 0.0225
trigger times: 7
Loss after 1251356880 batches: 0.0235
trigger times: 8
Loss after 1251435480 batches: 0.0228
trigger times: 0
Loss after 1251514080 batches: 0.0228
trigger times: 1
Loss after 1251592680 batches: 0.0233
trigger times: 2
Loss after 1251671280 batches: 0.0226
trigger times: 3
Loss after 1251749880 batches: 0.0226
trigger times: 4
Loss after 1251828480 batches: 0.0223
trigger times: 5
Loss after 1251907080 batches: 0.0224
trigger times: 6
Loss after 1251985680 batches: 0.0224
trigger times: 7
Loss after 1252064280 batches: 0.0217
trigger times: 0
Loss after 1252142880 batches: 0.0217
trigger times: 1
Loss after 1252221480 batches: 0.0219
trigger times: 2
Loss after 1252300080 batches: 0.0216
trigger times: 3
Loss after 1252378680 batches: 0.0211
trigger times: 4
Loss after 1252457280 batches: 0.0213
trigger times: 5
Loss after 1252535880 batches: 0.0217
trigger times: 6
Loss after 1252614480 batches: 0.0218
trigger times: 7
Loss after 1252693080 batches: 0.0211
trigger times: 8
Loss after 1252771680 batches: 0.0211
trigger times: 9
Loss after 1252850280 batches: 0.0213
trigger times: 0
Loss after 1252928880 batches: 0.0211
trigger times: 0
Loss after 1253007480 batches: 0.0203
trigger times: 1
Loss after 1253086080 batches: 0.0206
trigger times: 2
Loss after 1253164680 batches: 0.0210
trigger times: 3
Loss after 1253243280 batches: 0.0201
trigger times: 4
Loss after 1253321880 batches: 0.0204
trigger times: 5
Loss after 1253400480 batches: 0.0208
trigger times: 6
Loss after 1253479080 batches: 0.0208
trigger times: 7
Loss after 1253557680 batches: 0.0204
trigger times: 8
Loss after 1253636280 batches: 0.0198
trigger times: 0
Loss after 1253714880 batches: 0.0203
trigger times: 1
Loss after 1253793480 batches: 0.0201
trigger times: 2
Loss after 1253872080 batches: 0.0203
trigger times: 3
Loss after 1253950680 batches: 0.0202
trigger times: 4
Loss after 1254029280 batches: 0.0204
trigger times: 5
Loss after 1254107880 batches: 0.0197
trigger times: 6
Loss after 1254186480 batches: 0.0197
trigger times: 7
Loss after 1254265080 batches: 0.0198
trigger times: 8
Loss after 1254343680 batches: 0.0198
trigger times: 9
Loss after 1254422280 batches: 0.0191
trigger times: 10
Loss after 1254500880 batches: 0.0189
trigger times: 11
Loss after 1254579480 batches: 0.0194
trigger times: 12
Loss after 1254658080 batches: 0.0195
trigger times: 13
Loss after 1254736680 batches: 0.0194
trigger times: 14
Loss after 1254815280 batches: 0.0190
trigger times: 15
Loss after 1254893880 batches: 0.0191
trigger times: 16
Loss after 1254972480 batches: 0.0194
trigger times: 17
Loss after 1255051080 batches: 0.0191
trigger times: 18
Loss after 1255129680 batches: 0.0190
trigger times: 19
Loss after 1255208280 batches: 0.0190
trigger times: 20
Early stopping!
Start to test process.
Loss after 1255286880 batches: 0.0189
Time to train on one home:  513.4997971057892
trigger times: 0
Loss after 1255417980 batches: 0.1673
trigger times: 0
Loss after 1255549080 batches: 0.0597
trigger times: 0
Loss after 1255680180 batches: 0.0423
trigger times: 0
Loss after 1255811280 batches: 0.0363
trigger times: 1
Loss after 1255942380 batches: 0.0326
trigger times: 2
Loss after 1256073480 batches: 0.0293
trigger times: 3
Loss after 1256204580 batches: 0.0278
trigger times: 0
Loss after 1256335680 batches: 0.0263
trigger times: 1
Loss after 1256466780 batches: 0.0248
trigger times: 2
Loss after 1256597880 batches: 0.0241
trigger times: 3
Loss after 1256728980 batches: 0.0232
trigger times: 4
Loss after 1256860080 batches: 0.0225
trigger times: 5
Loss after 1256991180 batches: 0.0217
trigger times: 6
Loss after 1257122280 batches: 0.0209
trigger times: 7
Loss after 1257253380 batches: 0.0203
trigger times: 8
Loss after 1257384480 batches: 0.0200
trigger times: 9
Loss after 1257515580 batches: 0.0197
trigger times: 10
Loss after 1257646680 batches: 0.0192
trigger times: 11
Loss after 1257777780 batches: 0.0186
trigger times: 12
Loss after 1257908880 batches: 0.0186
trigger times: 13
Loss after 1258039980 batches: 0.0183
trigger times: 14
Loss after 1258171080 batches: 0.0179
trigger times: 15
Loss after 1258302180 batches: 0.0176
trigger times: 16
Loss after 1258433280 batches: 0.0173
trigger times: 17
Loss after 1258564380 batches: 0.0168
trigger times: 18
Loss after 1258695480 batches: 0.0168
trigger times: 19
Loss after 1258826580 batches: 0.0170
trigger times: 20
Early stopping!
Start to test process.
Loss after 1258957680 batches: 0.0166
Time to train on one home:  214.83669066429138
trigger times: 0
Loss after 1259088780 batches: 0.1864
trigger times: 1
Loss after 1259219880 batches: 0.0600
trigger times: 2
Loss after 1259350980 batches: 0.0442
trigger times: 3
Loss after 1259482080 batches: 0.0375
trigger times: 4
Loss after 1259613180 batches: 0.0340
trigger times: 5
Loss after 1259744280 batches: 0.0311
trigger times: 6
Loss after 1259875380 batches: 0.0295
trigger times: 0
Loss after 1260006480 batches: 0.0282
trigger times: 1
Loss after 1260137580 batches: 0.0271
trigger times: 2
Loss after 1260268680 batches: 0.0259
trigger times: 3
Loss after 1260399780 batches: 0.0256
trigger times: 4
Loss after 1260530880 batches: 0.0248
trigger times: 5
Loss after 1260661980 batches: 0.0246
trigger times: 6
Loss after 1260793080 batches: 0.0235
trigger times: 0
Loss after 1260924180 batches: 0.0227
trigger times: 1
Loss after 1261055280 batches: 0.0227
trigger times: 2
Loss after 1261186380 batches: 0.0221
trigger times: 3
Loss after 1261317480 batches: 0.0216
trigger times: 4
Loss after 1261448580 batches: 0.0220
trigger times: 0
Loss after 1261579680 batches: 0.0210
trigger times: 0
Loss after 1261710780 batches: 0.0207
trigger times: 1
Loss after 1261841880 batches: 0.0205
trigger times: 0
Loss after 1261972980 batches: 0.0199
trigger times: 1
Loss after 1262104080 batches: 0.0196
trigger times: 2
Loss after 1262235180 batches: 0.0196
trigger times: 3
Loss after 1262366280 batches: 0.0194
trigger times: 4
Loss after 1262497380 batches: 0.0190
trigger times: 5
Loss after 1262628480 batches: 0.0191
trigger times: 6
Loss after 1262759580 batches: 0.0186
trigger times: 7
Loss after 1262890680 batches: 0.0184
trigger times: 0
Loss after 1263021780 batches: 0.0185
trigger times: 0
Loss after 1263152880 batches: 0.0184
trigger times: 1
Loss after 1263283980 batches: 0.0181
trigger times: 2
Loss after 1263415080 batches: 0.0179
trigger times: 3
Loss after 1263546180 batches: 0.0178
trigger times: 4
Loss after 1263677280 batches: 0.0177
trigger times: 5
Loss after 1263808380 batches: 0.0173
trigger times: 6
Loss after 1263939480 batches: 0.0173
trigger times: 7
Loss after 1264070580 batches: 0.0172
trigger times: 8
Loss after 1264201680 batches: 0.0172
trigger times: 9
Loss after 1264332780 batches: 0.0168
trigger times: 10
Loss after 1264463880 batches: 0.0169
trigger times: 0
Loss after 1264594980 batches: 0.0170
trigger times: 0
Loss after 1264726080 batches: 0.0172
trigger times: 0
Loss after 1264857180 batches: 0.0167
trigger times: 1
Loss after 1264988280 batches: 0.0166
trigger times: 0
Loss after 1265119380 batches: 0.0164
trigger times: 1
Loss after 1265250480 batches: 0.0165
trigger times: 2
Loss after 1265381580 batches: 0.0161
trigger times: 3
Loss after 1265512680 batches: 0.0163
trigger times: 4
Loss after 1265643780 batches: 0.0159
trigger times: 5
Loss after 1265774880 batches: 0.0157
trigger times: 6
Loss after 1265905980 batches: 0.0159
trigger times: 7
Loss after 1266037080 batches: 0.0155
trigger times: 8
Loss after 1266168180 batches: 0.0157
trigger times: 9
Loss after 1266299280 batches: 0.0153
trigger times: 10
Loss after 1266430380 batches: 0.0153
trigger times: 11
Loss after 1266561480 batches: 0.0152
trigger times: 12
Loss after 1266692580 batches: 0.0151
trigger times: 13
Loss after 1266823680 batches: 0.0148
trigger times: 14
Loss after 1266954780 batches: 0.0150
trigger times: 15
Loss after 1267085880 batches: 0.0150
trigger times: 16
Loss after 1267216980 batches: 0.0150
trigger times: 17
Loss after 1267348080 batches: 0.0152
trigger times: 18
Loss after 1267479180 batches: 0.0149
trigger times: 0
Loss after 1267610280 batches: 0.0147
trigger times: 1
Loss after 1267741380 batches: 0.0150
trigger times: 2
Loss after 1267872480 batches: 0.0148
trigger times: 3
Loss after 1268003580 batches: 0.0146
trigger times: 4
Loss after 1268134680 batches: 0.0145
trigger times: 5
Loss after 1268265780 batches: 0.0142
trigger times: 6
Loss after 1268396880 batches: 0.0140
trigger times: 7
Loss after 1268527980 batches: 0.0140
trigger times: 8
Loss after 1268659080 batches: 0.0142
trigger times: 9
Loss after 1268790180 batches: 0.0140
trigger times: 10
Loss after 1268921280 batches: 0.0141
trigger times: 11
Loss after 1269052380 batches: 0.0140
trigger times: 12
Loss after 1269183480 batches: 0.0141
trigger times: 13
Loss after 1269314580 batches: 0.0140
trigger times: 14
Loss after 1269445680 batches: 0.0140
trigger times: 15
Loss after 1269576780 batches: 0.0140
trigger times: 16
Loss after 1269707880 batches: 0.0140
trigger times: 17
Loss after 1269838980 batches: 0.0138
trigger times: 18
Loss after 1269970080 batches: 0.0139
trigger times: 19
Loss after 1270101180 batches: 0.0136
trigger times: 20
Early stopping!
Start to test process.
Loss after 1270232280 batches: 0.0136
Time to train on one home:  636.7772080898285
trigger times: 0
Loss after 1270363380 batches: 0.2975
trigger times: 0
Loss after 1270494480 batches: 0.0960
trigger times: 0
Loss after 1270625580 batches: 0.0678
trigger times: 0
Loss after 1270756680 batches: 0.0564
trigger times: 0
Loss after 1270887780 batches: 0.0516
trigger times: 1
Loss after 1271018880 batches: 0.0468
trigger times: 0
Loss after 1271149980 batches: 0.0435
trigger times: 0
Loss after 1271281080 batches: 0.0410
trigger times: 0
Loss after 1271412180 batches: 0.0392
trigger times: 1
Loss after 1271543280 batches: 0.0377
trigger times: 0
Loss after 1271674380 batches: 0.0360
trigger times: 1
Loss after 1271805480 batches: 0.0353
trigger times: 0
Loss after 1271936580 batches: 0.0339
trigger times: 1
Loss after 1272067680 batches: 0.0331
trigger times: 2
Loss after 1272198780 batches: 0.0330
trigger times: 3
Loss after 1272329880 batches: 0.0317
trigger times: 0
Loss after 1272460980 batches: 0.0312
trigger times: 1
Loss after 1272592080 batches: 0.0313
trigger times: 2
Loss after 1272723180 batches: 0.0301
trigger times: 3
Loss after 1272854280 batches: 0.0298
trigger times: 4
Loss after 1272985380 batches: 0.0292
trigger times: 5
Loss after 1273116480 batches: 0.0286
trigger times: 0
Loss after 1273247580 batches: 0.0283
trigger times: 0
Loss after 1273378680 batches: 0.0280
trigger times: 0
Loss after 1273509780 batches: 0.0279
trigger times: 1
Loss after 1273640880 batches: 0.0274
trigger times: 2
Loss after 1273771980 batches: 0.0266
trigger times: 3
Loss after 1273903080 batches: 0.0283
trigger times: 4
Loss after 1274034180 batches: 0.0270
trigger times: 5
Loss after 1274165280 batches: 0.0266
trigger times: 6
Loss after 1274296380 batches: 0.0265
trigger times: 7
Loss after 1274427480 batches: 0.0257
trigger times: 8
Loss after 1274558580 batches: 0.0263
trigger times: 9
Loss after 1274689680 batches: 0.0262
trigger times: 10
Loss after 1274820780 batches: 0.0254
trigger times: 11
Loss after 1274951880 batches: 0.0259
trigger times: 12
Loss after 1275082980 batches: 0.0253
trigger times: 13
Loss after 1275214080 batches: 0.0253
trigger times: 14
Loss after 1275345180 batches: 0.0254
trigger times: 0
Loss after 1275476280 batches: 0.0247
trigger times: 1
Loss after 1275607380 batches: 0.0239
trigger times: 0
Loss after 1275738480 batches: 0.0241
trigger times: 1
Loss after 1275869580 batches: 0.0236
trigger times: 2
Loss after 1276000680 batches: 0.0248
trigger times: 3
Loss after 1276131780 batches: 0.0242
trigger times: 4
Loss after 1276262880 batches: 0.0233
trigger times: 5
Loss after 1276393980 batches: 0.0228
trigger times: 6
Loss after 1276525080 batches: 0.0231
trigger times: 7
Loss after 1276656180 batches: 0.0225
trigger times: 0
Loss after 1276787280 batches: 0.0225
trigger times: 1
Loss after 1276918380 batches: 0.0221
trigger times: 2
Loss after 1277049480 batches: 0.0231
trigger times: 3
Loss after 1277180580 batches: 0.0224
trigger times: 4
Loss after 1277311680 batches: 0.0226
trigger times: 5
Loss after 1277442780 batches: 0.0220
trigger times: 6
Loss after 1277573880 batches: 0.0230
trigger times: 7
Loss after 1277704980 batches: 0.0219
trigger times: 8
Loss after 1277836080 batches: 0.0222
trigger times: 9
Loss after 1277967180 batches: 0.0215
trigger times: 10
Loss after 1278098280 batches: 0.0225
trigger times: 11
Loss after 1278229380 batches: 0.0221
trigger times: 12
Loss after 1278360480 batches: 0.0218
trigger times: 13
Loss after 1278491580 batches: 0.0212
trigger times: 14
Loss after 1278622680 batches: 0.0211
trigger times: 15
Loss after 1278753780 batches: 0.0213
trigger times: 16
Loss after 1278884880 batches: 0.0210
trigger times: 17
Loss after 1279015980 batches: 0.0224
trigger times: 18
Loss after 1279147080 batches: 0.0221
trigger times: 19
Loss after 1279278180 batches: 0.0210
trigger times: 20
Early stopping!
Start to test process.
Loss after 1279409280 batches: 0.0208
Time to train on one home:  523.860946893692
trigger times: 0
Loss after 1279540380 batches: 0.4000
trigger times: 1
Loss after 1279671480 batches: 0.1648
trigger times: 2
Loss after 1279802580 batches: 0.1092
trigger times: 3
Loss after 1279933680 batches: 0.0905
trigger times: 4
Loss after 1280064780 batches: 0.0796
trigger times: 5
Loss after 1280195880 batches: 0.0724
trigger times: 6
Loss after 1280326980 batches: 0.0678
trigger times: 7
Loss after 1280458080 batches: 0.0639
trigger times: 8
Loss after 1280589180 batches: 0.0609
trigger times: 9
Loss after 1280720280 batches: 0.0586
trigger times: 10
Loss after 1280851380 batches: 0.0562
trigger times: 11
Loss after 1280982480 batches: 0.0555
trigger times: 12
Loss after 1281113580 batches: 0.0536
trigger times: 13
Loss after 1281244680 batches: 0.0522
trigger times: 14
Loss after 1281375780 batches: 0.0503
trigger times: 15
Loss after 1281506880 batches: 0.0485
trigger times: 16
Loss after 1281637980 batches: 0.0485
trigger times: 17
Loss after 1281769080 batches: 0.0476
trigger times: 18
Loss after 1281900180 batches: 0.0466
trigger times: 19
Loss after 1282031280 batches: 0.0463
trigger times: 20
Early stopping!
Start to test process.
Loss after 1282162380 batches: 0.0453
Time to train on one home:  166.3539423942566
trigger times: 0
Loss after 1282293480 batches: 0.3203
trigger times: 0
Loss after 1282424580 batches: 0.1045
trigger times: 1
Loss after 1282555680 batches: 0.0734
trigger times: 0
Loss after 1282686780 batches: 0.0605
trigger times: 0
Loss after 1282817880 batches: 0.0535
trigger times: 1
Loss after 1282948980 batches: 0.0499
trigger times: 2
Loss after 1283080080 batches: 0.0467
trigger times: 3
Loss after 1283211180 batches: 0.0434
trigger times: 4
Loss after 1283342280 batches: 0.0418
trigger times: 0
Loss after 1283473380 batches: 0.0401
trigger times: 1
Loss after 1283604480 batches: 0.0386
trigger times: 2
Loss after 1283735580 batches: 0.0370
trigger times: 3
Loss after 1283866680 batches: 0.0363
trigger times: 0
Loss after 1283997780 batches: 0.0362
trigger times: 0
Loss after 1284128880 batches: 0.0351
trigger times: 1
Loss after 1284259980 batches: 0.0338
trigger times: 2
Loss after 1284391080 batches: 0.0337
trigger times: 3
Loss after 1284522180 batches: 0.0322
trigger times: 4
Loss after 1284653280 batches: 0.0323
trigger times: 5
Loss after 1284784380 batches: 0.0319
trigger times: 6
Loss after 1284915480 batches: 0.0308
trigger times: 7
Loss after 1285046580 batches: 0.0312
trigger times: 8
Loss after 1285177680 batches: 0.0307
trigger times: 9
Loss after 1285308780 batches: 0.0297
trigger times: 10
Loss after 1285439880 batches: 0.0294
trigger times: 11
Loss after 1285570980 batches: 0.0296
trigger times: 12
Loss after 1285702080 batches: 0.0287
trigger times: 0
Loss after 1285833180 batches: 0.0282
trigger times: 1
Loss after 1285964280 batches: 0.0280
trigger times: 2
Loss after 1286095380 batches: 0.0273
trigger times: 3
Loss after 1286226480 batches: 0.0275
trigger times: 4
Loss after 1286357580 batches: 0.0268
trigger times: 5
Loss after 1286488680 batches: 0.0279
trigger times: 6
Loss after 1286619780 batches: 0.0272
trigger times: 7
Loss after 1286750880 batches: 0.0268
trigger times: 8
Loss after 1286881980 batches: 0.0266
trigger times: 9
Loss after 1287013080 batches: 0.0259
trigger times: 10
Loss after 1287144180 batches: 0.0259
trigger times: 11
Loss after 1287275280 batches: 0.0262
trigger times: 12
Loss after 1287406380 batches: 0.0258
trigger times: 13
Loss after 1287537480 batches: 0.0253
trigger times: 14
Loss after 1287668580 batches: 0.0251
trigger times: 15
Loss after 1287799680 batches: 0.0252
trigger times: 16
Loss after 1287930780 batches: 0.0253
trigger times: 17
Loss after 1288061880 batches: 0.0253
trigger times: 18
Loss after 1288192980 batches: 0.0251
trigger times: 19
Loss after 1288324080 batches: 0.0253
trigger times: 20
Early stopping!
Start to test process.
Loss after 1288455180 batches: 0.0245
Time to train on one home:  359.28255820274353
trigger times: 0
Loss after 1288586280 batches: 0.4652
trigger times: 0
Loss after 1288717380 batches: 0.1849
trigger times: 1
Loss after 1288848480 batches: 0.1121
trigger times: 2
Loss after 1288979580 batches: 0.0874
trigger times: 3
Loss after 1289110680 batches: 0.0752
trigger times: 4
Loss after 1289241780 batches: 0.0675
trigger times: 5
Loss after 1289372880 batches: 0.0623
trigger times: 6
Loss after 1289503980 batches: 0.0589
trigger times: 7
Loss after 1289635080 batches: 0.0554
trigger times: 8
Loss after 1289766180 batches: 0.0524
trigger times: 9
Loss after 1289897280 batches: 0.0508
trigger times: 10
Loss after 1290028380 batches: 0.0491
trigger times: 11
Loss after 1290159480 batches: 0.0473
trigger times: 12
Loss after 1290290580 batches: 0.0455
trigger times: 13
Loss after 1290421680 batches: 0.0445
trigger times: 14
Loss after 1290552780 batches: 0.0440
trigger times: 15
Loss after 1290683880 batches: 0.0424
trigger times: 16
Loss after 1290814980 batches: 0.0418
trigger times: 17
Loss after 1290946080 batches: 0.0410
trigger times: 18
Loss after 1291077180 batches: 0.0399
trigger times: 19
Loss after 1291208280 batches: 0.0393
trigger times: 20
Early stopping!
Start to test process.
Loss after 1291339380 batches: 0.0390
Time to train on one home:  171.63528037071228
trigger times: 0
Loss after 1291470480 batches: 0.5707
trigger times: 0
Loss after 1291601580 batches: 0.2098
trigger times: 0
Loss after 1291732680 batches: 0.1402
trigger times: 1
Loss after 1291863780 batches: 0.1136
trigger times: 2
Loss after 1291994880 batches: 0.1002
trigger times: 3
Loss after 1292125980 batches: 0.0912
trigger times: 4
Loss after 1292257080 batches: 0.0866
trigger times: 5
Loss after 1292388180 batches: 0.0790
trigger times: 0
Loss after 1292519280 batches: 0.0758
trigger times: 1
Loss after 1292650380 batches: 0.0726
trigger times: 0
Loss after 1292781480 batches: 0.0696
trigger times: 1
Loss after 1292912580 batches: 0.0676
trigger times: 2
Loss after 1293043680 batches: 0.0659
trigger times: 3
Loss after 1293174780 batches: 0.0635
trigger times: 4
Loss after 1293305880 batches: 0.0602
trigger times: 5
Loss after 1293436980 batches: 0.0613
trigger times: 6
Loss after 1293568080 batches: 0.0582
trigger times: 7
Loss after 1293699180 batches: 0.0575
trigger times: 8
Loss after 1293830280 batches: 0.0569
trigger times: 9
Loss after 1293961380 batches: 0.0557
trigger times: 0
Loss after 1294092480 batches: 0.0553
trigger times: 1
Loss after 1294223580 batches: 0.0535
trigger times: 2
Loss after 1294354680 batches: 0.0527
trigger times: 0
Loss after 1294485780 batches: 0.0516
trigger times: 1
Loss after 1294616880 batches: 0.0513
trigger times: 2
Loss after 1294747980 batches: 0.0504
trigger times: 0
Loss after 1294879080 batches: 0.0499
trigger times: 1
Loss after 1295010180 batches: 0.0489
trigger times: 2
Loss after 1295141280 batches: 0.0481
trigger times: 3
Loss after 1295272380 batches: 0.0483
trigger times: 4
Loss after 1295403480 batches: 0.0482
trigger times: 5
Loss after 1295534580 batches: 0.0466
trigger times: 6
Loss after 1295665680 batches: 0.0463
trigger times: 7
Loss after 1295796780 batches: 0.0456
trigger times: 0
Loss after 1295927880 batches: 0.0451
trigger times: 1
Loss after 1296058980 batches: 0.0444
trigger times: 2
Loss after 1296190080 batches: 0.0441
trigger times: 3
Loss after 1296321180 batches: 0.0437
trigger times: 0
Loss after 1296452280 batches: 0.0434
trigger times: 0
Loss after 1296583380 batches: 0.0436
trigger times: 1
Loss after 1296714480 batches: 0.0422
trigger times: 2
Loss after 1296845580 batches: 0.0429
trigger times: 3
Loss after 1296976680 batches: 0.0416
trigger times: 4
Loss after 1297107780 batches: 0.0409
trigger times: 0
Loss after 1297238880 batches: 0.0417
trigger times: 1
Loss after 1297369980 batches: 0.0413
trigger times: 0
Loss after 1297501080 batches: 0.0405
trigger times: 0
Loss after 1297632180 batches: 0.0399
trigger times: 1
Loss after 1297763280 batches: 0.0398
trigger times: 2
Loss after 1297894380 batches: 0.0397
trigger times: 3
Loss after 1298025480 batches: 0.0389
trigger times: 4
Loss after 1298156580 batches: 0.0395
trigger times: 5
Loss after 1298287680 batches: 0.0386
trigger times: 6
Loss after 1298418780 batches: 0.0384
trigger times: 7
Loss after 1298549880 batches: 0.0376
trigger times: 8
Loss after 1298680980 batches: 0.0379
trigger times: 9
Loss after 1298812080 batches: 0.0376
trigger times: 10
Loss after 1298943180 batches: 0.0371
trigger times: 11
Loss after 1299074280 batches: 0.0371
trigger times: 12
Loss after 1299205380 batches: 0.0371
trigger times: 13
Loss after 1299336480 batches: 0.0369
trigger times: 14
Loss after 1299467580 batches: 0.0363
trigger times: 15
Loss after 1299598680 batches: 0.0361
trigger times: 0
Loss after 1299729780 batches: 0.0364
trigger times: 1
Loss after 1299860880 batches: 0.0357
trigger times: 0
Loss after 1299991980 batches: 0.0357
trigger times: 1
Loss after 1300123080 batches: 0.0360
trigger times: 2
Loss after 1300254180 batches: 0.0353
trigger times: 3
Loss after 1300385280 batches: 0.0348
trigger times: 4
Loss after 1300516380 batches: 0.0350
trigger times: 5
Loss after 1300647480 batches: 0.0352
trigger times: 6
Loss after 1300778580 batches: 0.0347
trigger times: 7
Loss after 1300909680 batches: 0.0345
trigger times: 8
Loss after 1301040780 batches: 0.0338
trigger times: 9
Loss after 1301171880 batches: 0.0339
trigger times: 10
Loss after 1301302980 batches: 0.0330
trigger times: 11
Loss after 1301434080 batches: 0.0335
trigger times: 12
Loss after 1301565180 batches: 0.0335
trigger times: 13
Loss after 1301696280 batches: 0.0328
trigger times: 14
Loss after 1301827380 batches: 0.0332
trigger times: 15
Loss after 1301958480 batches: 0.0330
trigger times: 16
Loss after 1302089580 batches: 0.0331
trigger times: 17
Loss after 1302220680 batches: 0.0328
trigger times: 18
Loss after 1302351780 batches: 0.0324
trigger times: 0
Loss after 1302482880 batches: 0.0337
trigger times: 1
Loss after 1302613980 batches: 0.0327
trigger times: 2
Loss after 1302745080 batches: 0.0318
trigger times: 3
Loss after 1302876180 batches: 0.0322
trigger times: 4
Loss after 1303007280 batches: 0.0324
trigger times: 5
Loss after 1303138380 batches: 0.0318
trigger times: 6
Loss after 1303269480 batches: 0.0313
trigger times: 7
Loss after 1303400580 batches: 0.0313
trigger times: 8
Loss after 1303531680 batches: 0.0317
trigger times: 9
Loss after 1303662780 batches: 0.0310
trigger times: 10
Loss after 1303793880 batches: 0.0306
trigger times: 11
Loss after 1303924980 batches: 0.0313
trigger times: 12
Loss after 1304056080 batches: 0.0310
trigger times: 13
Loss after 1304187180 batches: 0.0305
trigger times: 14
Loss after 1304318280 batches: 0.0307
trigger times: 15
Loss after 1304449380 batches: 0.0311
trigger times: 16
Loss after 1304580480 batches: 0.0308
trigger times: 0
Loss after 1304711580 batches: 0.0303
trigger times: 1
Loss after 1304842680 batches: 0.0299
trigger times: 2
Loss after 1304973780 batches: 0.0301
trigger times: 3
Loss after 1305104880 batches: 0.0305
trigger times: 4
Loss after 1305235980 batches: 0.0300
trigger times: 5
Loss after 1305367080 batches: 0.0295
trigger times: 6
Loss after 1305498180 batches: 0.0301
trigger times: 7
Loss after 1305629280 batches: 0.0298
trigger times: 8
Loss after 1305760380 batches: 0.0293
trigger times: 9
Loss after 1305891480 batches: 0.0291
trigger times: 10
Loss after 1306022580 batches: 0.0292
trigger times: 11
Loss after 1306153680 batches: 0.0291
trigger times: 12
Loss after 1306284780 batches: 0.0292
trigger times: 13
Loss after 1306415880 batches: 0.0288
trigger times: 14
Loss after 1306546980 batches: 0.0289
trigger times: 15
Loss after 1306678080 batches: 0.0292
trigger times: 16
Loss after 1306809180 batches: 0.0289
trigger times: 17
Loss after 1306940280 batches: 0.0280
trigger times: 18
Loss after 1307071380 batches: 0.0280
trigger times: 19
Loss after 1307202480 batches: 0.0279
trigger times: 20
Early stopping!
Start to test process.
Loss after 1307333580 batches: 0.0286
Time to train on one home:  897.5118317604065
trigger times: 0
Loss after 1307427540 batches: 0.6053
trigger times: 0
Loss after 1307521500 batches: 0.2972
trigger times: 0
Loss after 1307615460 batches: 0.1663
trigger times: 0
Loss after 1307709420 batches: 0.1247
trigger times: 0
Loss after 1307803380 batches: 0.1036
trigger times: 1
Loss after 1307897340 batches: 0.0913
trigger times: 0
Loss after 1307991300 batches: 0.0819
trigger times: 1
Loss after 1308085260 batches: 0.0771
trigger times: 2
Loss after 1308179220 batches: 0.0716
trigger times: 3
Loss after 1308273180 batches: 0.0676
trigger times: 0
Loss after 1308367140 batches: 0.0661
trigger times: 1
Loss after 1308461100 batches: 0.0631
trigger times: 2
Loss after 1308555060 batches: 0.0610
trigger times: 3
Loss after 1308649020 batches: 0.0594
trigger times: 0
Loss after 1308742980 batches: 0.0571
trigger times: 0
Loss after 1308836940 batches: 0.0559
trigger times: 1
Loss after 1308930900 batches: 0.0559
trigger times: 0
Loss after 1309024860 batches: 0.0537
trigger times: 1
Loss after 1309118820 batches: 0.0531
trigger times: 2
Loss after 1309212780 batches: 0.0517
trigger times: 3
Loss after 1309306740 batches: 0.0500
trigger times: 0
Loss after 1309400700 batches: 0.0506
trigger times: 1
Loss after 1309494660 batches: 0.0485
trigger times: 2
Loss after 1309588620 batches: 0.0483
trigger times: 3
Loss after 1309682580 batches: 0.0474
trigger times: 4
Loss after 1309776540 batches: 0.0460
trigger times: 5
Loss after 1309870500 batches: 0.0453
trigger times: 6
Loss after 1309964460 batches: 0.0456
trigger times: 7
Loss after 1310058420 batches: 0.0441
trigger times: 8
Loss after 1310152380 batches: 0.0442
trigger times: 9
Loss after 1310246340 batches: 0.0442
trigger times: 10
Loss after 1310340300 batches: 0.0430
trigger times: 11
Loss after 1310434260 batches: 0.0429
trigger times: 12
Loss after 1310528220 batches: 0.0430
trigger times: 0
Loss after 1310622180 batches: 0.0420
trigger times: 1
Loss after 1310716140 batches: 0.0412
trigger times: 2
Loss after 1310810100 batches: 0.0410
trigger times: 3
Loss after 1310904060 batches: 0.0403
trigger times: 4
Loss after 1310998020 batches: 0.0399
trigger times: 0
Loss after 1311091980 batches: 0.0391
trigger times: 1
Loss after 1311185940 batches: 0.0394
trigger times: 0
Loss after 1311279900 batches: 0.0386
trigger times: 1
Loss after 1311373860 batches: 0.0387
trigger times: 2
Loss after 1311467820 batches: 0.0381
trigger times: 3
Loss after 1311561780 batches: 0.0384
trigger times: 4
Loss after 1311655740 batches: 0.0374
trigger times: 5
Loss after 1311749700 batches: 0.0370
trigger times: 6
Loss after 1311843660 batches: 0.0369
trigger times: 7
Loss after 1311937620 batches: 0.0367
trigger times: 8
Loss after 1312031580 batches: 0.0369
trigger times: 9
Loss after 1312125540 batches: 0.0361
trigger times: 10
Loss after 1312219500 batches: 0.0363
trigger times: 11
Loss after 1312313460 batches: 0.0364
trigger times: 0
Loss after 1312407420 batches: 0.0360
trigger times: 1
Loss after 1312501380 batches: 0.0353
trigger times: 2
Loss after 1312595340 batches: 0.0349
trigger times: 3
Loss after 1312689300 batches: 0.0348
trigger times: 4
Loss after 1312783260 batches: 0.0346
trigger times: 5
Loss after 1312877220 batches: 0.0343
trigger times: 6
Loss after 1312971180 batches: 0.0337
trigger times: 7
Loss after 1313065140 batches: 0.0339
trigger times: 8
Loss after 1313159100 batches: 0.0337
trigger times: 0
Loss after 1313253060 batches: 0.0344
trigger times: 0
Loss after 1313347020 batches: 0.0337
trigger times: 1
Loss after 1313440980 batches: 0.0338
trigger times: 0
Loss after 1313534940 batches: 0.0337
trigger times: 1
Loss after 1313628900 batches: 0.0325
trigger times: 0
Loss after 1313722860 batches: 0.0326
trigger times: 1
Loss after 1313816820 batches: 0.0323
trigger times: 2
Loss after 1313910780 batches: 0.0322
trigger times: 3
Loss after 1314004740 batches: 0.0315
trigger times: 4
Loss after 1314098700 batches: 0.0330
trigger times: 5
Loss after 1314192660 batches: 0.0316
trigger times: 6
Loss after 1314286620 batches: 0.0316
trigger times: 7
Loss after 1314380580 batches: 0.0313
trigger times: 8
Loss after 1314474540 batches: 0.0312
trigger times: 9
Loss after 1314568500 batches: 0.0311
trigger times: 10
Loss after 1314662460 batches: 0.0311
trigger times: 11
Loss after 1314756420 batches: 0.0306
trigger times: 0
Loss after 1314850380 batches: 0.0309
trigger times: 1
Loss after 1314944340 batches: 0.0303
trigger times: 2
Loss after 1315038300 batches: 0.0304
trigger times: 3
Loss after 1315132260 batches: 0.0298
trigger times: 4
Loss after 1315226220 batches: 0.0298
trigger times: 5
Loss after 1315320180 batches: 0.0297
trigger times: 6
Loss after 1315414140 batches: 0.0293
trigger times: 7
Loss after 1315508100 batches: 0.0296
trigger times: 8
Loss after 1315602060 batches: 0.0298
trigger times: 9
Loss after 1315696020 batches: 0.0299
trigger times: 10
Loss after 1315789980 batches: 0.0289
trigger times: 11
Loss after 1315883940 batches: 0.0288
trigger times: 12
Loss after 1315977900 batches: 0.0292
trigger times: 13
Loss after 1316071860 batches: 0.0287
trigger times: 14
Loss after 1316165820 batches: 0.0286
trigger times: 15
Loss after 1316259780 batches: 0.0287
trigger times: 16
Loss after 1316353740 batches: 0.0286
trigger times: 17
Loss after 1316447700 batches: 0.0283
trigger times: 18
Loss after 1316541660 batches: 0.0282
trigger times: 19
Loss after 1316635620 batches: 0.0281
trigger times: 20
Early stopping!
Start to test process.
Loss after 1316729580 batches: 0.0282
Time to train on one home:  556.2193975448608
trigger times: 0
Loss after 1316860680 batches: 0.0740
trigger times: 1
Loss after 1316991780 batches: 0.0164
trigger times: 2
Loss after 1317122880 batches: 0.0122
trigger times: 3
Loss after 1317253980 batches: 0.0098
trigger times: 4
Loss after 1317385080 batches: 0.0087
trigger times: 5
Loss after 1317516180 batches: 0.0077
trigger times: 6
Loss after 1317647280 batches: 0.0071
trigger times: 7
Loss after 1317778380 batches: 0.0067
trigger times: 8
Loss after 1317909480 batches: 0.0063
trigger times: 9
Loss after 1318040580 batches: 0.0060
trigger times: 10
Loss after 1318171680 batches: 0.0059
trigger times: 11
Loss after 1318302780 batches: 0.0056
trigger times: 12
Loss after 1318433880 batches: 0.0053
trigger times: 13
Loss after 1318564980 batches: 0.0052
trigger times: 0
Loss after 1318696080 batches: 0.0051
trigger times: 1
Loss after 1318827180 batches: 0.0049
trigger times: 2
Loss after 1318958280 batches: 0.0047
trigger times: 3
Loss after 1319089380 batches: 0.0047
trigger times: 0
Loss after 1319220480 batches: 0.0045
trigger times: 1
Loss after 1319351580 batches: 0.0044
trigger times: 2
Loss after 1319482680 batches: 0.0043
trigger times: 3
Loss after 1319613780 batches: 0.0042
trigger times: 4
Loss after 1319744880 batches: 0.0041
trigger times: 5
Loss after 1319875980 batches: 0.0041
trigger times: 6
Loss after 1320007080 batches: 0.0040
trigger times: 7
Loss after 1320138180 batches: 0.0039
trigger times: 8
Loss after 1320269280 batches: 0.0038
trigger times: 9
Loss after 1320400380 batches: 0.0038
trigger times: 10
Loss after 1320531480 batches: 0.0038
trigger times: 11
Loss after 1320662580 batches: 0.0038
trigger times: 12
Loss after 1320793680 batches: 0.0035
trigger times: 13
Loss after 1320924780 batches: 0.0035
trigger times: 14
Loss after 1321055880 batches: 0.0037
trigger times: 15
Loss after 1321186980 batches: 0.0035
trigger times: 16
Loss after 1321318080 batches: 0.0035
trigger times: 17
Loss after 1321449180 batches: 0.0034
trigger times: 18
Loss after 1321580280 batches: 0.0034
trigger times: 19
Loss after 1321711380 batches: 0.0032
trigger times: 20
Early stopping!
Start to test process.
Loss after 1321842480 batches: 0.0032
Time to train on one home:  294.6138286590576
trigger times: 0
Loss after 1321973580 batches: 0.2552
trigger times: 0
Loss after 1322104680 batches: 0.1079
trigger times: 0
Loss after 1322235780 batches: 0.0749
trigger times: 1
Loss after 1322366880 batches: 0.0620
trigger times: 2
Loss after 1322497980 batches: 0.0543
trigger times: 3
Loss after 1322629080 batches: 0.0485
trigger times: 4
Loss after 1322760180 batches: 0.0445
trigger times: 5
Loss after 1322891280 batches: 0.0417
trigger times: 6
Loss after 1323022380 batches: 0.0396
trigger times: 7
Loss after 1323153480 batches: 0.0368
trigger times: 8
Loss after 1323284580 batches: 0.0350
trigger times: 9
Loss after 1323415680 batches: 0.0334
trigger times: 10
Loss after 1323546780 batches: 0.0321
trigger times: 11
Loss after 1323677880 batches: 0.0306
trigger times: 12
Loss after 1323808980 batches: 0.0295
trigger times: 13
Loss after 1323940080 batches: 0.0286
trigger times: 14
Loss after 1324071180 batches: 0.0279
trigger times: 15
Loss after 1324202280 batches: 0.0272
trigger times: 16
Loss after 1324333380 batches: 0.0261
trigger times: 17
Loss after 1324464480 batches: 0.0254
trigger times: 18
Loss after 1324595580 batches: 0.0250
trigger times: 19
Loss after 1324726680 batches: 0.0241
trigger times: 20
Early stopping!
Start to test process.
Loss after 1324857780 batches: 0.0233
Time to train on one home:  179.36772775650024
trigger times: 0
Loss after 1324988880 batches: 0.3287
trigger times: 0
Loss after 1325119980 batches: 0.1263
trigger times: 1
Loss after 1325251080 batches: 0.0962
trigger times: 0
Loss after 1325382180 batches: 0.0838
trigger times: 0
Loss after 1325513280 batches: 0.0771
trigger times: 1
Loss after 1325644380 batches: 0.0721
trigger times: 2
Loss after 1325775480 batches: 0.0693
trigger times: 3
Loss after 1325906580 batches: 0.0664
trigger times: 4
Loss after 1326037680 batches: 0.0631
trigger times: 0
Loss after 1326168780 batches: 0.0621
trigger times: 1
Loss after 1326299880 batches: 0.0606
trigger times: 2
Loss after 1326430980 batches: 0.0593
trigger times: 3
Loss after 1326562080 batches: 0.0587
trigger times: 4
Loss after 1326693180 batches: 0.0568
trigger times: 5
Loss after 1326824280 batches: 0.0564
trigger times: 0
Loss after 1326955380 batches: 0.0562
trigger times: 1
Loss after 1327086480 batches: 0.0553
trigger times: 2
Loss after 1327217580 batches: 0.0539
trigger times: 0
Loss after 1327348680 batches: 0.0533
trigger times: 1
Loss after 1327479780 batches: 0.0523
trigger times: 2
Loss after 1327610880 batches: 0.0520
trigger times: 3
Loss after 1327741980 batches: 0.0512
trigger times: 4
Loss after 1327873080 batches: 0.0512
trigger times: 5
Loss after 1328004180 batches: 0.0508
trigger times: 6
Loss after 1328135280 batches: 0.0498
trigger times: 0
Loss after 1328266380 batches: 0.0502
trigger times: 1
Loss after 1328397480 batches: 0.0498
trigger times: 2
Loss after 1328528580 batches: 0.0495
trigger times: 3
Loss after 1328659680 batches: 0.0490
trigger times: 4
Loss after 1328790780 batches: 0.0491
trigger times: 0
Loss after 1328921880 batches: 0.0484
trigger times: 1
Loss after 1329052980 batches: 0.0483
trigger times: 2
Loss after 1329184080 batches: 0.0476
trigger times: 3
Loss after 1329315180 batches: 0.0473
trigger times: 4
Loss after 1329446280 batches: 0.0466
trigger times: 5
Loss after 1329577380 batches: 0.0468
trigger times: 0
Loss after 1329708480 batches: 0.0471
trigger times: 1
Loss after 1329839580 batches: 0.0465
trigger times: 2
Loss after 1329970680 batches: 0.0463
trigger times: 3
Loss after 1330101780 batches: 0.0461
trigger times: 4
Loss after 1330232880 batches: 0.0461
trigger times: 0
Loss after 1330363980 batches: 0.0453
trigger times: 1
Loss after 1330495080 batches: 0.0453
trigger times: 2
Loss after 1330626180 batches: 0.0455
trigger times: 0
Loss after 1330757280 batches: 0.0451
trigger times: 1
Loss after 1330888380 batches: 0.0447
trigger times: 2
Loss after 1331019480 batches: 0.0446
trigger times: 0
Loss after 1331150580 batches: 0.0447
trigger times: 1
Loss after 1331281680 batches: 0.0448
trigger times: 2
Loss after 1331412780 batches: 0.0441
trigger times: 3
Loss after 1331543880 batches: 0.0445
trigger times: 4
Loss after 1331674980 batches: 0.0438
trigger times: 5
Loss after 1331806080 batches: 0.0430
trigger times: 6
Loss after 1331937180 batches: 0.0435
trigger times: 0
Loss after 1332068280 batches: 0.0429
trigger times: 1
Loss after 1332199380 batches: 0.0431
trigger times: 2
Loss after 1332330480 batches: 0.0426
trigger times: 3
Loss after 1332461580 batches: 0.0435
trigger times: 0
Loss after 1332592680 batches: 0.0430
trigger times: 0
Loss after 1332723780 batches: 0.0435
trigger times: 1
Loss after 1332854880 batches: 0.0433
trigger times: 2
Loss after 1332985980 batches: 0.0427
trigger times: 3
Loss after 1333117080 batches: 0.0425
trigger times: 4
Loss after 1333248180 batches: 0.0428
trigger times: 5
Loss after 1333379280 batches: 0.0424
trigger times: 0
Loss after 1333510380 batches: 0.0420
trigger times: 0
Loss after 1333641480 batches: 0.0419
trigger times: 1
Loss after 1333772580 batches: 0.0419
trigger times: 2
Loss after 1333903680 batches: 0.0416
trigger times: 3
Loss after 1334034780 batches: 0.0419
trigger times: 4
Loss after 1334165880 batches: 0.0413
trigger times: 5
Loss after 1334296980 batches: 0.0418
trigger times: 6
Loss after 1334428080 batches: 0.0408
trigger times: 7
Loss after 1334559180 batches: 0.0409
trigger times: 0
Loss after 1334690280 batches: 0.0409
trigger times: 1
Loss after 1334821380 batches: 0.0413
trigger times: 2
Loss after 1334952480 batches: 0.0407
trigger times: 3
Loss after 1335083580 batches: 0.0406
trigger times: 4
Loss after 1335214680 batches: 0.0403
trigger times: 5
Loss after 1335345780 batches: 0.0406
trigger times: 6
Loss after 1335476880 batches: 0.0405
trigger times: 7
Loss after 1335607980 batches: 0.0404
trigger times: 8
Loss after 1335739080 batches: 0.0407
trigger times: 9
Loss after 1335870180 batches: 0.0400
trigger times: 10
Loss after 1336001280 batches: 0.0400
trigger times: 11
Loss after 1336132380 batches: 0.0398
trigger times: 12
Loss after 1336263480 batches: 0.0398
trigger times: 13
Loss after 1336394580 batches: 0.0397
trigger times: 14
Loss after 1336525680 batches: 0.0401
trigger times: 15
Loss after 1336656780 batches: 0.0397
trigger times: 16
Loss after 1336787880 batches: 0.0407
trigger times: 17
Loss after 1336918980 batches: 0.0397
trigger times: 18
Loss after 1337050080 batches: 0.0399
trigger times: 19
Loss after 1337181180 batches: 0.0395
trigger times: 20
Early stopping!
Start to test process.
Loss after 1337312280 batches: 0.0392
Time to train on one home:  702.5793399810791
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642, 0.03501720033327798, 0.03263172240869951, 0.030110256078310443, 0.02544188748067248]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217], [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944], [0.6491197148958842, 0.29803996281550993, 0.44160060986867417, 1.203530351478871, 0.5750418631238572, 28.433835927602637, 1775.19], [0.6497863001293607, 0.29730357979276123, 0.44701117944787117, 1.2068712720184533, 0.5756451041104418, 28.512766372815683, 1777.0521], [0.6468338668346405, 0.30051325220168756, 0.4517286880074036, 1.1964129191375579, 0.5730157578453048, 28.26568403748201, 1768.9353]]
Round_11_results:  [0.6468338668346405, 0.30051325220168756, 0.4517286880074036, 1.1964129191375579, 0.5730157578453048, 28.26568403748201, 1768.9353]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 10631 < 10632; dropping {'Training_Loss': 0.4205686985736802, 'Validation_Loss': 0.8506003552012973, 'Training_R2': 0.5728919441352415, 'Validation_R2': 0.20737294459962363, 'Training_F1': 0.7501264198595031, 'Validation_F1': 0.31653418542822015, 'Training_NEP': 0.5002727171961607, 'Validation_NEP': 1.2186869986974296, 'Training_NDE': 0.2876958305119137, 'Validation_NDE': 0.6312064042914302, 'Training_MAE': 32.83381509271514, 'Validation_MAE': 33.42145352692587, 'Training_MSE': 3796.6409, 'Validation_MSE': 2331.028}.
trigger times: 0
Loss after 1337414880 batches: 0.4206
trigger times: 0
Loss after 1337517480 batches: 0.1730
trigger times: 1
Loss after 1337620080 batches: 0.1118
trigger times: 2
Loss after 1337722680 batches: 0.0928
trigger times: 0
Loss after 1337825280 batches: 0.0743
trigger times: 1
Loss after 1337927880 batches: 0.0710
trigger times: 2
Loss after 1338030480 batches: 0.0608
trigger times: 3
Loss after 1338133080 batches: 0.0571
trigger times: 4
Loss after 1338235680 batches: 0.0567
trigger times: 5
Loss after 1338338280 batches: 0.0533
trigger times: 6
Loss after 1338440880 batches: 0.0483
trigger times: 7
Loss after 1338543480 batches: 0.0443
trigger times: 8
Loss after 1338646080 batches: 0.0429
trigger times: 9
Loss after 1338748680 batches: 0.0417
trigger times: 10
Loss after 1338851280 batches: 0.0401
trigger times: 11
Loss after 1338953880 batches: 0.0386
trigger times: 12
Loss after 1339056480 batches: 0.0384
trigger times: 13
Loss after 1339159080 batches: 0.0403
trigger times: 14
Loss after 1339261680 batches: 0.0394
trigger times: 15
Loss after 1339364280 batches: 0.0380
trigger times: 16
Loss after 1339466880 batches: 0.0367
trigger times: 17
Loss after 1339569480 batches: 0.0358
trigger times: 18
Loss after 1339672080 batches: 0.0348
trigger times: 19
Loss after 1339774680 batches: 0.0338
trigger times: 20
Early stopping!
Start to test process.
Loss after 1339877280 batches: 0.0336
Time to train on one home:  160.58162331581116
trigger times: 0
Loss after 1340008380 batches: 0.2660
trigger times: 1
Loss after 1340139480 batches: 0.1031
trigger times: 2
Loss after 1340270580 batches: 0.0697
trigger times: 3
Loss after 1340401680 batches: 0.0563
trigger times: 4
Loss after 1340532780 batches: 0.0499
trigger times: 5
Loss after 1340663880 batches: 0.0459
trigger times: 6
Loss after 1340794980 batches: 0.0420
trigger times: 7
Loss after 1340926080 batches: 0.0407
trigger times: 8
Loss after 1341057180 batches: 0.0382
trigger times: 9
Loss after 1341188280 batches: 0.0364
trigger times: 10
Loss after 1341319380 batches: 0.0352
trigger times: 11
Loss after 1341450480 batches: 0.0341
trigger times: 12
Loss after 1341581580 batches: 0.0333
trigger times: 13
Loss after 1341712680 batches: 0.0324
trigger times: 14
Loss after 1341843780 batches: 0.0314
trigger times: 15
Loss after 1341974880 batches: 0.0306
trigger times: 16
Loss after 1342105980 batches: 0.0297
trigger times: 17
Loss after 1342237080 batches: 0.0293
trigger times: 18
Loss after 1342368180 batches: 0.0289
trigger times: 19
Loss after 1342499280 batches: 0.0285
trigger times: 20
Early stopping!
Start to test process.
Loss after 1342630380 batches: 0.0282
Time to train on one home:  165.48790740966797
trigger times: 0
Loss after 1342761480 batches: 0.4645
trigger times: 1
Loss after 1342892580 batches: 0.1657
trigger times: 2
Loss after 1343023680 batches: 0.1102
trigger times: 3
Loss after 1343154780 batches: 0.0901
trigger times: 4
Loss after 1343285880 batches: 0.0802
trigger times: 5
Loss after 1343416980 batches: 0.0725
trigger times: 6
Loss after 1343548080 batches: 0.0691
trigger times: 7
Loss after 1343679180 batches: 0.0643
trigger times: 8
Loss after 1343810280 batches: 0.0618
trigger times: 9
Loss after 1343941380 batches: 0.0583
trigger times: 10
Loss after 1344072480 batches: 0.0570
trigger times: 11
Loss after 1344203580 batches: 0.0545
trigger times: 12
Loss after 1344334680 batches: 0.0530
trigger times: 13
Loss after 1344465780 batches: 0.0516
trigger times: 14
Loss after 1344596880 batches: 0.0504
trigger times: 15
Loss after 1344727980 batches: 0.0497
trigger times: 16
Loss after 1344859080 batches: 0.0482
trigger times: 17
Loss after 1344990180 batches: 0.0469
trigger times: 18
Loss after 1345121280 batches: 0.0469
trigger times: 19
Loss after 1345252380 batches: 0.0458
trigger times: 20
Early stopping!
Start to test process.
Loss after 1345383480 batches: 0.0450
Time to train on one home:  164.5843825340271
trigger times: 0
Loss after 1345512120 batches: 0.1852
trigger times: 0
Loss after 1345640760 batches: 0.0643
trigger times: 1
Loss after 1345769400 batches: 0.0456
trigger times: 2
Loss after 1345898040 batches: 0.0383
trigger times: 3
Loss after 1346026680 batches: 0.0337
trigger times: 4
Loss after 1346155320 batches: 0.0317
trigger times: 5
Loss after 1346283960 batches: 0.0297
trigger times: 6
Loss after 1346412600 batches: 0.0279
trigger times: 7
Loss after 1346541240 batches: 0.0266
trigger times: 8
Loss after 1346669880 batches: 0.0254
trigger times: 9
Loss after 1346798520 batches: 0.0253
trigger times: 10
Loss after 1346927160 batches: 0.0243
trigger times: 11
Loss after 1347055800 batches: 0.0235
trigger times: 12
Loss after 1347184440 batches: 0.0235
trigger times: 13
Loss after 1347313080 batches: 0.0227
trigger times: 14
Loss after 1347441720 batches: 0.0221
trigger times: 15
Loss after 1347570360 batches: 0.0217
trigger times: 16
Loss after 1347699000 batches: 0.0211
trigger times: 17
Loss after 1347827640 batches: 0.0207
trigger times: 18
Loss after 1347956280 batches: 0.0203
trigger times: 19
Loss after 1348084920 batches: 0.0205
trigger times: 20
Early stopping!
Start to test process.
Loss after 1348213560 batches: 0.0207
Time to train on one home:  168.975412607193
trigger times: 0
Loss after 1348344660 batches: 0.5816
trigger times: 1
Loss after 1348475760 batches: 0.2148
trigger times: 2
Loss after 1348606860 batches: 0.1271
trigger times: 3
Loss after 1348737960 batches: 0.0993
trigger times: 4
Loss after 1348869060 batches: 0.0850
trigger times: 5
Loss after 1349000160 batches: 0.0777
trigger times: 6
Loss after 1349131260 batches: 0.0725
trigger times: 7
Loss after 1349262360 batches: 0.0688
trigger times: 8
Loss after 1349393460 batches: 0.0642
trigger times: 9
Loss after 1349524560 batches: 0.0611
trigger times: 10
Loss after 1349655660 batches: 0.0592
trigger times: 11
Loss after 1349786760 batches: 0.0574
trigger times: 12
Loss after 1349917860 batches: 0.0560
trigger times: 13
Loss after 1350048960 batches: 0.0539
trigger times: 14
Loss after 1350180060 batches: 0.0528
trigger times: 15
Loss after 1350311160 batches: 0.0511
trigger times: 16
Loss after 1350442260 batches: 0.0506
trigger times: 17
Loss after 1350573360 batches: 0.0490
trigger times: 18
Loss after 1350704460 batches: 0.0486
trigger times: 19
Loss after 1350835560 batches: 0.0478
trigger times: 20
Early stopping!
Start to test process.
Loss after 1350966660 batches: 0.0462
Time to train on one home:  164.60322451591492
trigger times: 0
Loss after 1351097760 batches: 0.5226
trigger times: 0
Loss after 1351228860 batches: 0.2491
trigger times: 1
Loss after 1351359960 batches: 0.1531
trigger times: 2
Loss after 1351491060 batches: 0.1120
trigger times: 0
Loss after 1351622160 batches: 0.0924
trigger times: 0
Loss after 1351753260 batches: 0.0790
trigger times: 1
Loss after 1351884360 batches: 0.0770
trigger times: 0
Loss after 1352015460 batches: 0.0721
trigger times: 0
Loss after 1352146560 batches: 0.0639
trigger times: 1
Loss after 1352277660 batches: 0.0594
trigger times: 0
Loss after 1352408760 batches: 0.0568
trigger times: 0
Loss after 1352539860 batches: 0.0564
trigger times: 0
Loss after 1352670960 batches: 0.0546
trigger times: 0
Loss after 1352802060 batches: 0.0539
trigger times: 1
Loss after 1352933160 batches: 0.0512
trigger times: 0
Loss after 1353064260 batches: 0.0528
trigger times: 1
Loss after 1353195360 batches: 0.0515
trigger times: 2
Loss after 1353326460 batches: 0.0481
trigger times: 3
Loss after 1353457560 batches: 0.0463
trigger times: 4
Loss after 1353588660 batches: 0.0455
trigger times: 5
Loss after 1353719760 batches: 0.0473
trigger times: 6
Loss after 1353850860 batches: 0.0461
trigger times: 7
Loss after 1353981960 batches: 0.0451
trigger times: 8
Loss after 1354113060 batches: 0.0429
trigger times: 0
Loss after 1354244160 batches: 0.0417
trigger times: 1
Loss after 1354375260 batches: 0.0421
trigger times: 2
Loss after 1354506360 batches: 0.0421
trigger times: 0
Loss after 1354637460 batches: 0.0406
trigger times: 1
Loss after 1354768560 batches: 0.0404
trigger times: 2
Loss after 1354899660 batches: 0.0406
trigger times: 3
Loss after 1355030760 batches: 0.0397
trigger times: 4
Loss after 1355161860 batches: 0.0391
trigger times: 5
Loss after 1355292960 batches: 0.0398
trigger times: 0
Loss after 1355424060 batches: 0.0398
trigger times: 0
Loss after 1355555160 batches: 0.0404
trigger times: 1
Loss after 1355686260 batches: 0.0384
trigger times: 2
Loss after 1355817360 batches: 0.0375
trigger times: 3
Loss after 1355948460 batches: 0.0366
trigger times: 4
Loss after 1356079560 batches: 0.0374
trigger times: 5
Loss after 1356210660 batches: 0.0358
trigger times: 6
Loss after 1356341760 batches: 0.0361
trigger times: 7
Loss after 1356472860 batches: 0.0354
trigger times: 0
Loss after 1356603960 batches: 0.0355
trigger times: 0
Loss after 1356735060 batches: 0.0342
trigger times: 0
Loss after 1356866160 batches: 0.0354
trigger times: 0
Loss after 1356997260 batches: 0.0359
trigger times: 1
Loss after 1357128360 batches: 0.0362
trigger times: 0
Loss after 1357259460 batches: 0.0363
trigger times: 1
Loss after 1357390560 batches: 0.0350
trigger times: 2
Loss after 1357521660 batches: 0.0338
trigger times: 3
Loss after 1357652760 batches: 0.0333
trigger times: 4
Loss after 1357783860 batches: 0.0317
trigger times: 5
Loss after 1357914960 batches: 0.0329
trigger times: 6
Loss after 1358046060 batches: 0.0345
trigger times: 7
Loss after 1358177160 batches: 0.0325
trigger times: 0
Loss after 1358308260 batches: 0.0329
trigger times: 1
Loss after 1358439360 batches: 0.0317
trigger times: 0
Loss after 1358570460 batches: 0.0315
trigger times: 1
Loss after 1358701560 batches: 0.0343
trigger times: 2
Loss after 1358832660 batches: 0.0317
trigger times: 3
Loss after 1358963760 batches: 0.0322
trigger times: 4
Loss after 1359094860 batches: 0.0312
trigger times: 5
Loss after 1359225960 batches: 0.0305
trigger times: 6
Loss after 1359357060 batches: 0.0316
trigger times: 7
Loss after 1359488160 batches: 0.0300
trigger times: 8
Loss after 1359619260 batches: 0.0308
trigger times: 9
Loss after 1359750360 batches: 0.0309
trigger times: 10
Loss after 1359881460 batches: 0.0323
trigger times: 11
Loss after 1360012560 batches: 0.0307
trigger times: 12
Loss after 1360143660 batches: 0.0313
trigger times: 0
Loss after 1360274760 batches: 0.0304
trigger times: 0
Loss after 1360405860 batches: 0.0305
trigger times: 1
Loss after 1360536960 batches: 0.0295
trigger times: 2
Loss after 1360668060 batches: 0.0286
trigger times: 0
Loss after 1360799160 batches: 0.0302
trigger times: 1
Loss after 1360930260 batches: 0.0292
trigger times: 2
Loss after 1361061360 batches: 0.0304
trigger times: 0
Loss after 1361192460 batches: 0.0293
trigger times: 0
Loss after 1361323560 batches: 0.0284
trigger times: 1
Loss after 1361454660 batches: 0.0279
trigger times: 2
Loss after 1361585760 batches: 0.0290
trigger times: 3
Loss after 1361716860 batches: 0.0286
trigger times: 4
Loss after 1361847960 batches: 0.0288
trigger times: 5
Loss after 1361979060 batches: 0.0283
trigger times: 6
Loss after 1362110160 batches: 0.0290
trigger times: 7
Loss after 1362241260 batches: 0.0300
trigger times: 8
Loss after 1362372360 batches: 0.0285
trigger times: 9
Loss after 1362503460 batches: 0.0278
trigger times: 10
Loss after 1362634560 batches: 0.0285
trigger times: 11
Loss after 1362765660 batches: 0.0281
trigger times: 0
Loss after 1362896760 batches: 0.0278
trigger times: 1
Loss after 1363027860 batches: 0.0287
trigger times: 2
Loss after 1363158960 batches: 0.0275
trigger times: 3
Loss after 1363290060 batches: 0.0273
trigger times: 4
Loss after 1363421160 batches: 0.0278
trigger times: 5
Loss after 1363552260 batches: 0.0281
trigger times: 6
Loss after 1363683360 batches: 0.0282
trigger times: 7
Loss after 1363814460 batches: 0.0276
trigger times: 8
Loss after 1363945560 batches: 0.0275
trigger times: 9
Loss after 1364076660 batches: 0.0278
trigger times: 10
Loss after 1364207760 batches: 0.0277
trigger times: 11
Loss after 1364338860 batches: 0.0273
trigger times: 12
Loss after 1364469960 batches: 0.0273
trigger times: 13
Loss after 1364601060 batches: 0.0274
trigger times: 14
Loss after 1364732160 batches: 0.0266
trigger times: 0
Loss after 1364863260 batches: 0.0258
trigger times: 1
Loss after 1364994360 batches: 0.0257
trigger times: 2
Loss after 1365125460 batches: 0.0253
trigger times: 3
Loss after 1365256560 batches: 0.0259
trigger times: 4
Loss after 1365387660 batches: 0.0257
trigger times: 5
Loss after 1365518760 batches: 0.0261
trigger times: 0
Loss after 1365649860 batches: 0.0263
trigger times: 1
Loss after 1365780960 batches: 0.0269
trigger times: 2
Loss after 1365912060 batches: 0.0261
trigger times: 3
Loss after 1366043160 batches: 0.0267
trigger times: 4
Loss after 1366174260 batches: 0.0256
trigger times: 5
Loss after 1366305360 batches: 0.0253
trigger times: 6
Loss after 1366436460 batches: 0.0264
trigger times: 7
Loss after 1366567560 batches: 0.0253
trigger times: 8
Loss after 1366698660 batches: 0.0251
trigger times: 9
Loss after 1366829760 batches: 0.0262
trigger times: 10
Loss after 1366960860 batches: 0.0250
trigger times: 11
Loss after 1367091960 batches: 0.0253
trigger times: 12
Loss after 1367223060 batches: 0.0244
trigger times: 13
Loss after 1367354160 batches: 0.0248
trigger times: 14
Loss after 1367485260 batches: 0.0253
trigger times: 15
Loss after 1367616360 batches: 0.0251
trigger times: 16
Loss after 1367747460 batches: 0.0246
trigger times: 17
Loss after 1367878560 batches: 0.0233
trigger times: 18
Loss after 1368009660 batches: 0.0245
trigger times: 19
Loss after 1368140760 batches: 0.0247
trigger times: 20
Early stopping!
Start to test process.
Loss after 1368271860 batches: 0.0247
Time to train on one home:  971.0125620365143
trigger times: 0
Loss after 1368402960 batches: 0.1295
trigger times: 0
Loss after 1368534060 batches: 0.0383
trigger times: 1
Loss after 1368665160 batches: 0.0268
trigger times: 2
Loss after 1368796260 batches: 0.0238
trigger times: 3
Loss after 1368927360 batches: 0.0216
trigger times: 4
Loss after 1369058460 batches: 0.0202
trigger times: 5
Loss after 1369189560 batches: 0.0189
trigger times: 6
Loss after 1369320660 batches: 0.0177
trigger times: 7
Loss after 1369451760 batches: 0.0171
trigger times: 8
Loss after 1369582860 batches: 0.0166
trigger times: 9
Loss after 1369713960 batches: 0.0158
trigger times: 10
Loss after 1369845060 batches: 0.0156
trigger times: 11
Loss after 1369976160 batches: 0.0153
trigger times: 12
Loss after 1370107260 batches: 0.0152
trigger times: 13
Loss after 1370238360 batches: 0.0147
trigger times: 14
Loss after 1370369460 batches: 0.0146
trigger times: 15
Loss after 1370500560 batches: 0.0138
trigger times: 16
Loss after 1370631660 batches: 0.0139
trigger times: 17
Loss after 1370762760 batches: 0.0137
trigger times: 18
Loss after 1370893860 batches: 0.0137
trigger times: 19
Loss after 1371024960 batches: 0.0135
trigger times: 20
Early stopping!
Start to test process.
Loss after 1371156060 batches: 0.0130
Time to train on one home:  171.81935715675354
trigger times: 0
Loss after 1371287160 batches: 0.2157
trigger times: 0
Loss after 1371418260 batches: 0.0580
trigger times: 0
Loss after 1371549360 batches: 0.0390
trigger times: 1
Loss after 1371680460 batches: 0.0321
trigger times: 2
Loss after 1371811560 batches: 0.0284
trigger times: 3
Loss after 1371942660 batches: 0.0263
trigger times: 4
Loss after 1372073760 batches: 0.0248
trigger times: 5
Loss after 1372204860 batches: 0.0234
trigger times: 6
Loss after 1372335960 batches: 0.0223
trigger times: 7
Loss after 1372467060 batches: 0.0219
trigger times: 8
Loss after 1372598160 batches: 0.0207
trigger times: 9
Loss after 1372729260 batches: 0.0203
trigger times: 10
Loss after 1372860360 batches: 0.0196
trigger times: 11
Loss after 1372991460 batches: 0.0191
trigger times: 12
Loss after 1373122560 batches: 0.0187
trigger times: 13
Loss after 1373253660 batches: 0.0185
trigger times: 14
Loss after 1373384760 batches: 0.0178
trigger times: 15
Loss after 1373515860 batches: 0.0174
trigger times: 16
Loss after 1373646960 batches: 0.0176
trigger times: 17
Loss after 1373778060 batches: 0.0167
trigger times: 18
Loss after 1373909160 batches: 0.0166
trigger times: 19
Loss after 1374040260 batches: 0.0166
trigger times: 20
Early stopping!
Start to test process.
Loss after 1374171360 batches: 0.0158
Time to train on one home:  178.7024290561676
trigger times: 0
Loss after 1374249960 batches: 0.4057
trigger times: 0
Loss after 1374328560 batches: 0.1237
trigger times: 0
Loss after 1374407160 batches: 0.0755
trigger times: 1
Loss after 1374485760 batches: 0.0596
trigger times: 0
Loss after 1374564360 batches: 0.0519
trigger times: 1
Loss after 1374642960 batches: 0.0465
trigger times: 2
Loss after 1374721560 batches: 0.0437
trigger times: 3
Loss after 1374800160 batches: 0.0412
trigger times: 4
Loss after 1374878760 batches: 0.0401
trigger times: 5
Loss after 1374957360 batches: 0.0379
trigger times: 0
Loss after 1375035960 batches: 0.0355
trigger times: 0
Loss after 1375114560 batches: 0.0345
trigger times: 1
Loss after 1375193160 batches: 0.0334
trigger times: 0
Loss after 1375271760 batches: 0.0333
trigger times: 0
Loss after 1375350360 batches: 0.0325
trigger times: 0
Loss after 1375428960 batches: 0.0311
trigger times: 1
Loss after 1375507560 batches: 0.0304
trigger times: 0
Loss after 1375586160 batches: 0.0299
trigger times: 1
Loss after 1375664760 batches: 0.0297
trigger times: 2
Loss after 1375743360 batches: 0.0284
trigger times: 0
Loss after 1375821960 batches: 0.0289
trigger times: 1
Loss after 1375900560 batches: 0.0280
trigger times: 2
Loss after 1375979160 batches: 0.0275
trigger times: 3
Loss after 1376057760 batches: 0.0275
trigger times: 4
Loss after 1376136360 batches: 0.0272
trigger times: 5
Loss after 1376214960 batches: 0.0270
trigger times: 6
Loss after 1376293560 batches: 0.0265
trigger times: 7
Loss after 1376372160 batches: 0.0260
trigger times: 8
Loss after 1376450760 batches: 0.0258
trigger times: 9
Loss after 1376529360 batches: 0.0259
trigger times: 10
Loss after 1376607960 batches: 0.0252
trigger times: 11
Loss after 1376686560 batches: 0.0254
trigger times: 12
Loss after 1376765160 batches: 0.0254
trigger times: 13
Loss after 1376843760 batches: 0.0248
trigger times: 14
Loss after 1376922360 batches: 0.0240
trigger times: 15
Loss after 1377000960 batches: 0.0235
trigger times: 16
Loss after 1377079560 batches: 0.0238
trigger times: 17
Loss after 1377158160 batches: 0.0240
trigger times: 18
Loss after 1377236760 batches: 0.0230
trigger times: 19
Loss after 1377315360 batches: 0.0232
trigger times: 20
Early stopping!
Start to test process.
Loss after 1377393960 batches: 0.0228
Time to train on one home:  206.20761036872864
trigger times: 0
Loss after 1377525060 batches: 0.1529
trigger times: 0
Loss after 1377656160 batches: 0.0444
trigger times: 1
Loss after 1377787260 batches: 0.0324
trigger times: 0
Loss after 1377918360 batches: 0.0285
trigger times: 1
Loss after 1378049460 batches: 0.0254
trigger times: 0
Loss after 1378180560 batches: 0.0234
trigger times: 1
Loss after 1378311660 batches: 0.0222
trigger times: 2
Loss after 1378442760 batches: 0.0211
trigger times: 3
Loss after 1378573860 batches: 0.0205
trigger times: 4
Loss after 1378704960 batches: 0.0197
trigger times: 0
Loss after 1378836060 batches: 0.0192
trigger times: 1
Loss after 1378967160 batches: 0.0186
trigger times: 2
Loss after 1379098260 batches: 0.0183
trigger times: 3
Loss after 1379229360 batches: 0.0174
trigger times: 4
Loss after 1379360460 batches: 0.0173
trigger times: 5
Loss after 1379491560 batches: 0.0169
trigger times: 6
Loss after 1379622660 batches: 0.0165
trigger times: 7
Loss after 1379753760 batches: 0.0159
trigger times: 8
Loss after 1379884860 batches: 0.0158
trigger times: 9
Loss after 1380015960 batches: 0.0158
trigger times: 10
Loss after 1380147060 batches: 0.0158
trigger times: 11
Loss after 1380278160 batches: 0.0155
trigger times: 12
Loss after 1380409260 batches: 0.0152
trigger times: 13
Loss after 1380540360 batches: 0.0149
trigger times: 14
Loss after 1380671460 batches: 0.0147
trigger times: 0
Loss after 1380802560 batches: 0.0148
trigger times: 1
Loss after 1380933660 batches: 0.0144
trigger times: 2
Loss after 1381064760 batches: 0.0142
trigger times: 3
Loss after 1381195860 batches: 0.0139
trigger times: 0
Loss after 1381326960 batches: 0.0140
trigger times: 1
Loss after 1381458060 batches: 0.0138
trigger times: 2
Loss after 1381589160 batches: 0.0138
trigger times: 0
Loss after 1381720260 batches: 0.0137
trigger times: 1
Loss after 1381851360 batches: 0.0134
trigger times: 2
Loss after 1381982460 batches: 0.0136
trigger times: 3
Loss after 1382113560 batches: 0.0132
trigger times: 4
Loss after 1382244660 batches: 0.0134
trigger times: 5
Loss after 1382375760 batches: 0.0129
trigger times: 6
Loss after 1382506860 batches: 0.0131
trigger times: 7
Loss after 1382637960 batches: 0.0131
trigger times: 8
Loss after 1382769060 batches: 0.0126
trigger times: 0
Loss after 1382900160 batches: 0.0126
trigger times: 1
Loss after 1383031260 batches: 0.0124
trigger times: 0
Loss after 1383162360 batches: 0.0125
trigger times: 1
Loss after 1383293460 batches: 0.0124
trigger times: 2
Loss after 1383424560 batches: 0.0124
trigger times: 3
Loss after 1383555660 batches: 0.0121
trigger times: 4
Loss after 1383686760 batches: 0.0121
trigger times: 5
Loss after 1383817860 batches: 0.0124
trigger times: 6
Loss after 1383948960 batches: 0.0122
trigger times: 7
Loss after 1384080060 batches: 0.0119
trigger times: 0
Loss after 1384211160 batches: 0.0120
trigger times: 1
Loss after 1384342260 batches: 0.0114
trigger times: 2
Loss after 1384473360 batches: 0.0116
trigger times: 3
Loss after 1384604460 batches: 0.0117
trigger times: 0
Loss after 1384735560 batches: 0.0118
trigger times: 1
Loss after 1384866660 batches: 0.0117
trigger times: 2
Loss after 1384997760 batches: 0.0115
trigger times: 3
Loss after 1385128860 batches: 0.0115
trigger times: 4
Loss after 1385259960 batches: 0.0115
trigger times: 5
Loss after 1385391060 batches: 0.0112
trigger times: 6
Loss after 1385522160 batches: 0.0111
trigger times: 7
Loss after 1385653260 batches: 0.0111
trigger times: 8
Loss after 1385784360 batches: 0.0110
trigger times: 9
Loss after 1385915460 batches: 0.0108
trigger times: 10
Loss after 1386046560 batches: 0.0109
trigger times: 11
Loss after 1386177660 batches: 0.0111
trigger times: 12
Loss after 1386308760 batches: 0.0109
trigger times: 13
Loss after 1386439860 batches: 0.0104
trigger times: 14
Loss after 1386570960 batches: 0.0108
trigger times: 15
Loss after 1386702060 batches: 0.0107
trigger times: 16
Loss after 1386833160 batches: 0.0107
trigger times: 17
Loss after 1386964260 batches: 0.0107
trigger times: 18
Loss after 1387095360 batches: 0.0106
trigger times: 19
Loss after 1387226460 batches: 0.0104
trigger times: 20
Early stopping!
Start to test process.
Loss after 1387357560 batches: 0.0105
Time to train on one home:  563.3213880062103
trigger times: 0
Loss after 1387488660 batches: 0.2280
trigger times: 0
Loss after 1387619760 batches: 0.0791
trigger times: 1
Loss after 1387750860 batches: 0.0549
trigger times: 2
Loss after 1387881960 batches: 0.0458
trigger times: 3
Loss after 1388013060 batches: 0.0407
trigger times: 4
Loss after 1388144160 batches: 0.0372
trigger times: 5
Loss after 1388275260 batches: 0.0348
trigger times: 6
Loss after 1388406360 batches: 0.0331
trigger times: 7
Loss after 1388537460 batches: 0.0318
trigger times: 8
Loss after 1388668560 batches: 0.0304
trigger times: 9
Loss after 1388799660 batches: 0.0289
trigger times: 10
Loss after 1388930760 batches: 0.0284
trigger times: 11
Loss after 1389061860 batches: 0.0274
trigger times: 12
Loss after 1389192960 batches: 0.0264
trigger times: 13
Loss after 1389324060 batches: 0.0256
trigger times: 14
Loss after 1389455160 batches: 0.0253
trigger times: 0
Loss after 1389586260 batches: 0.0242
trigger times: 1
Loss after 1389717360 batches: 0.0244
trigger times: 2
Loss after 1389848460 batches: 0.0238
trigger times: 3
Loss after 1389979560 batches: 0.0235
trigger times: 0
Loss after 1390110660 batches: 0.0231
trigger times: 0
Loss after 1390241760 batches: 0.0226
trigger times: 1
Loss after 1390372860 batches: 0.0221
trigger times: 2
Loss after 1390503960 batches: 0.0216
trigger times: 3
Loss after 1390635060 batches: 0.0213
trigger times: 0
Loss after 1390766160 batches: 0.0209
trigger times: 1
Loss after 1390897260 batches: 0.0208
trigger times: 2
Loss after 1391028360 batches: 0.0207
trigger times: 3
Loss after 1391159460 batches: 0.0203
trigger times: 4
Loss after 1391290560 batches: 0.0202
trigger times: 5
Loss after 1391421660 batches: 0.0200
trigger times: 6
Loss after 1391552760 batches: 0.0198
trigger times: 7
Loss after 1391683860 batches: 0.0194
trigger times: 8
Loss after 1391814960 batches: 0.0196
trigger times: 9
Loss after 1391946060 batches: 0.0190
trigger times: 10
Loss after 1392077160 batches: 0.0191
trigger times: 11
Loss after 1392208260 batches: 0.0186
trigger times: 12
Loss after 1392339360 batches: 0.0188
trigger times: 0
Loss after 1392470460 batches: 0.0185
trigger times: 1
Loss after 1392601560 batches: 0.0185
trigger times: 2
Loss after 1392732660 batches: 0.0183
trigger times: 0
Loss after 1392863760 batches: 0.0179
trigger times: 1
Loss after 1392994860 batches: 0.0179
trigger times: 2
Loss after 1393125960 batches: 0.0177
trigger times: 3
Loss after 1393257060 batches: 0.0180
trigger times: 4
Loss after 1393388160 batches: 0.0175
trigger times: 0
Loss after 1393519260 batches: 0.0175
trigger times: 1
Loss after 1393650360 batches: 0.0171
trigger times: 2
Loss after 1393781460 batches: 0.0171
trigger times: 3
Loss after 1393912560 batches: 0.0172
trigger times: 4
Loss after 1394043660 batches: 0.0167
trigger times: 5
Loss after 1394174760 batches: 0.0165
trigger times: 6
Loss after 1394305860 batches: 0.0166
trigger times: 7
Loss after 1394436960 batches: 0.0162
trigger times: 8
Loss after 1394568060 batches: 0.0165
trigger times: 0
Loss after 1394699160 batches: 0.0163
trigger times: 1
Loss after 1394830260 batches: 0.0162
trigger times: 2
Loss after 1394961360 batches: 0.0162
trigger times: 0
Loss after 1395092460 batches: 0.0160
trigger times: 1
Loss after 1395223560 batches: 0.0160
trigger times: 2
Loss after 1395354660 batches: 0.0159
trigger times: 3
Loss after 1395485760 batches: 0.0157
trigger times: 4
Loss after 1395616860 batches: 0.0155
trigger times: 5
Loss after 1395747960 batches: 0.0153
trigger times: 6
Loss after 1395879060 batches: 0.0153
trigger times: 7
Loss after 1396010160 batches: 0.0154
trigger times: 0
Loss after 1396141260 batches: 0.0153
trigger times: 1
Loss after 1396272360 batches: 0.0152
trigger times: 2
Loss after 1396403460 batches: 0.0151
trigger times: 3
Loss after 1396534560 batches: 0.0153
trigger times: 0
Loss after 1396665660 batches: 0.0153
trigger times: 1
Loss after 1396796760 batches: 0.0150
trigger times: 2
Loss after 1396927860 batches: 0.0147
trigger times: 3
Loss after 1397058960 batches: 0.0146
trigger times: 4
Loss after 1397190060 batches: 0.0146
trigger times: 5
Loss after 1397321160 batches: 0.0143
trigger times: 6
Loss after 1397452260 batches: 0.0146
trigger times: 7
Loss after 1397583360 batches: 0.0146
trigger times: 8
Loss after 1397714460 batches: 0.0145
trigger times: 9
Loss after 1397845560 batches: 0.0142
trigger times: 10
Loss after 1397976660 batches: 0.0143
trigger times: 11
Loss after 1398107760 batches: 0.0142
trigger times: 12
Loss after 1398238860 batches: 0.0142
trigger times: 13
Loss after 1398369960 batches: 0.0142
trigger times: 14
Loss after 1398501060 batches: 0.0139
trigger times: 15
Loss after 1398632160 batches: 0.0141
trigger times: 16
Loss after 1398763260 batches: 0.0139
trigger times: 17
Loss after 1398894360 batches: 0.0138
trigger times: 18
Loss after 1399025460 batches: 0.0138
trigger times: 19
Loss after 1399156560 batches: 0.0138
trigger times: 20
Early stopping!
Start to test process.
Loss after 1399287660 batches: 0.0138
Time to train on one home:  670.8104102611542
trigger times: 0
Loss after 1399418760 batches: 0.2938
trigger times: 0
Loss after 1399549860 batches: 0.0890
trigger times: 0
Loss after 1399680960 batches: 0.0618
trigger times: 1
Loss after 1399812060 batches: 0.0535
trigger times: 2
Loss after 1399943160 batches: 0.0475
trigger times: 0
Loss after 1400074260 batches: 0.0433
trigger times: 1
Loss after 1400205360 batches: 0.0402
trigger times: 2
Loss after 1400336460 batches: 0.0387
trigger times: 3
Loss after 1400467560 batches: 0.0367
trigger times: 4
Loss after 1400598660 batches: 0.0355
trigger times: 0
Loss after 1400729760 batches: 0.0346
trigger times: 1
Loss after 1400860860 batches: 0.0339
trigger times: 0
Loss after 1400991960 batches: 0.0324
trigger times: 1
Loss after 1401123060 batches: 0.0321
trigger times: 0
Loss after 1401254160 batches: 0.0309
trigger times: 1
Loss after 1401385260 batches: 0.0304
trigger times: 2
Loss after 1401516360 batches: 0.0302
trigger times: 3
Loss after 1401647460 batches: 0.0291
trigger times: 4
Loss after 1401778560 batches: 0.0284
trigger times: 5
Loss after 1401909660 batches: 0.0283
trigger times: 6
Loss after 1402040760 batches: 0.0288
trigger times: 7
Loss after 1402171860 batches: 0.0285
trigger times: 8
Loss after 1402302960 batches: 0.0276
trigger times: 9
Loss after 1402434060 batches: 0.0268
trigger times: 10
Loss after 1402565160 batches: 0.0267
trigger times: 11
Loss after 1402696260 batches: 0.0264
trigger times: 12
Loss after 1402827360 batches: 0.0261
trigger times: 13
Loss after 1402958460 batches: 0.0274
trigger times: 14
Loss after 1403089560 batches: 0.0254
trigger times: 15
Loss after 1403220660 batches: 0.0249
trigger times: 16
Loss after 1403351760 batches: 0.0255
trigger times: 17
Loss after 1403482860 batches: 0.0247
trigger times: 18
Loss after 1403613960 batches: 0.0241
trigger times: 19
Loss after 1403745060 batches: 0.0244
trigger times: 20
Early stopping!
Start to test process.
Loss after 1403876160 batches: 0.0247
Time to train on one home:  265.07694244384766
trigger times: 0
Loss after 1404007260 batches: 0.4106
trigger times: 1
Loss after 1404138360 batches: 0.1634
trigger times: 2
Loss after 1404269460 batches: 0.1075
trigger times: 3
Loss after 1404400560 batches: 0.0863
trigger times: 4
Loss after 1404531660 batches: 0.0770
trigger times: 5
Loss after 1404662760 batches: 0.0693
trigger times: 6
Loss after 1404793860 batches: 0.0649
trigger times: 7
Loss after 1404924960 batches: 0.0614
trigger times: 8
Loss after 1405056060 batches: 0.0590
trigger times: 9
Loss after 1405187160 batches: 0.0572
trigger times: 10
Loss after 1405318260 batches: 0.0539
trigger times: 11
Loss after 1405449360 batches: 0.0520
trigger times: 12
Loss after 1405580460 batches: 0.0514
trigger times: 13
Loss after 1405711560 batches: 0.0495
trigger times: 14
Loss after 1405842660 batches: 0.0484
trigger times: 15
Loss after 1405973760 batches: 0.0478
trigger times: 16
Loss after 1406104860 batches: 0.0468
trigger times: 17
Loss after 1406235960 batches: 0.0464
trigger times: 18
Loss after 1406367060 batches: 0.0456
trigger times: 19
Loss after 1406498160 batches: 0.0441
trigger times: 20
Early stopping!
Start to test process.
Loss after 1406629260 batches: 0.0433
Time to train on one home:  164.10407710075378
trigger times: 0
Loss after 1406760360 batches: 0.2823
trigger times: 0
Loss after 1406891460 batches: 0.0906
trigger times: 1
Loss after 1407022560 batches: 0.0625
trigger times: 2
Loss after 1407153660 batches: 0.0517
trigger times: 3
Loss after 1407284760 batches: 0.0469
trigger times: 4
Loss after 1407415860 batches: 0.0433
trigger times: 5
Loss after 1407546960 batches: 0.0412
trigger times: 6
Loss after 1407678060 batches: 0.0380
trigger times: 7
Loss after 1407809160 batches: 0.0364
trigger times: 8
Loss after 1407940260 batches: 0.0364
trigger times: 9
Loss after 1408071360 batches: 0.0346
trigger times: 10
Loss after 1408202460 batches: 0.0333
trigger times: 11
Loss after 1408333560 batches: 0.0325
trigger times: 12
Loss after 1408464660 batches: 0.0318
trigger times: 13
Loss after 1408595760 batches: 0.0309
trigger times: 14
Loss after 1408726860 batches: 0.0309
trigger times: 15
Loss after 1408857960 batches: 0.0301
trigger times: 16
Loss after 1408989060 batches: 0.0295
trigger times: 17
Loss after 1409120160 batches: 0.0287
trigger times: 18
Loss after 1409251260 batches: 0.0288
trigger times: 19
Loss after 1409382360 batches: 0.0290
trigger times: 20
Early stopping!
Start to test process.
Loss after 1409513460 batches: 0.0277
Time to train on one home:  172.09958124160767
trigger times: 0
Loss after 1409644560 batches: 0.4794
trigger times: 0
Loss after 1409775660 batches: 0.1863
trigger times: 1
Loss after 1409906760 batches: 0.1108
trigger times: 2
Loss after 1410037860 batches: 0.0850
trigger times: 3
Loss after 1410168960 batches: 0.0727
trigger times: 4
Loss after 1410300060 batches: 0.0642
trigger times: 5
Loss after 1410431160 batches: 0.0591
trigger times: 6
Loss after 1410562260 batches: 0.0560
trigger times: 7
Loss after 1410693360 batches: 0.0537
trigger times: 8
Loss after 1410824460 batches: 0.0504
trigger times: 9
Loss after 1410955560 batches: 0.0482
trigger times: 10
Loss after 1411086660 batches: 0.0468
trigger times: 11
Loss after 1411217760 batches: 0.0451
trigger times: 12
Loss after 1411348860 batches: 0.0439
trigger times: 13
Loss after 1411479960 batches: 0.0428
trigger times: 14
Loss after 1411611060 batches: 0.0412
trigger times: 15
Loss after 1411742160 batches: 0.0405
trigger times: 16
Loss after 1411873260 batches: 0.0396
trigger times: 17
Loss after 1412004360 batches: 0.0389
trigger times: 18
Loss after 1412135460 batches: 0.0384
trigger times: 19
Loss after 1412266560 batches: 0.0377
trigger times: 20
Early stopping!
Start to test process.
Loss after 1412397660 batches: 0.0369
Time to train on one home:  171.6281816959381
trigger times: 0
Loss after 1412528760 batches: 0.5566
trigger times: 0
Loss after 1412659860 batches: 0.1938
trigger times: 1
Loss after 1412790960 batches: 0.1279
trigger times: 0
Loss after 1412922060 batches: 0.1066
trigger times: 1
Loss after 1413053160 batches: 0.0944
trigger times: 0
Loss after 1413184260 batches: 0.0859
trigger times: 0
Loss after 1413315360 batches: 0.0780
trigger times: 0
Loss after 1413446460 batches: 0.0740
trigger times: 1
Loss after 1413577560 batches: 0.0718
trigger times: 2
Loss after 1413708660 batches: 0.0672
trigger times: 0
Loss after 1413839760 batches: 0.0650
trigger times: 1
Loss after 1413970860 batches: 0.0626
trigger times: 2
Loss after 1414101960 batches: 0.0616
trigger times: 3
Loss after 1414233060 batches: 0.0592
trigger times: 4
Loss after 1414364160 batches: 0.0579
trigger times: 0
Loss after 1414495260 batches: 0.0565
trigger times: 1
Loss after 1414626360 batches: 0.0558
trigger times: 2
Loss after 1414757460 batches: 0.0541
trigger times: 3
Loss after 1414888560 batches: 0.0530
trigger times: 4
Loss after 1415019660 batches: 0.0517
trigger times: 5
Loss after 1415150760 batches: 0.0519
trigger times: 6
Loss after 1415281860 batches: 0.0505
trigger times: 7
Loss after 1415412960 batches: 0.0498
trigger times: 8
Loss after 1415544060 batches: 0.0486
trigger times: 9
Loss after 1415675160 batches: 0.0476
trigger times: 10
Loss after 1415806260 batches: 0.0466
trigger times: 11
Loss after 1415937360 batches: 0.0469
trigger times: 12
Loss after 1416068460 batches: 0.0461
trigger times: 13
Loss after 1416199560 batches: 0.0454
trigger times: 14
Loss after 1416330660 batches: 0.0445
trigger times: 15
Loss after 1416461760 batches: 0.0447
trigger times: 16
Loss after 1416592860 batches: 0.0441
trigger times: 17
Loss after 1416723960 batches: 0.0439
trigger times: 0
Loss after 1416855060 batches: 0.0434
trigger times: 1
Loss after 1416986160 batches: 0.0428
trigger times: 2
Loss after 1417117260 batches: 0.0419
trigger times: 3
Loss after 1417248360 batches: 0.0418
trigger times: 4
Loss after 1417379460 batches: 0.0417
trigger times: 5
Loss after 1417510560 batches: 0.0418
trigger times: 6
Loss after 1417641660 batches: 0.0407
trigger times: 0
Loss after 1417772760 batches: 0.0407
trigger times: 1
Loss after 1417903860 batches: 0.0405
trigger times: 2
Loss after 1418034960 batches: 0.0398
trigger times: 3
Loss after 1418166060 batches: 0.0398
trigger times: 4
Loss after 1418297160 batches: 0.0395
trigger times: 5
Loss after 1418428260 batches: 0.0388
trigger times: 6
Loss after 1418559360 batches: 0.0380
trigger times: 7
Loss after 1418690460 batches: 0.0378
trigger times: 0
Loss after 1418821560 batches: 0.0378
trigger times: 1
Loss after 1418952660 batches: 0.0381
trigger times: 2
Loss after 1419083760 batches: 0.0373
trigger times: 3
Loss after 1419214860 batches: 0.0370
trigger times: 4
Loss after 1419345960 batches: 0.0370
trigger times: 5
Loss after 1419477060 batches: 0.0375
trigger times: 6
Loss after 1419608160 batches: 0.0366
trigger times: 7
Loss after 1419739260 batches: 0.0358
trigger times: 8
Loss after 1419870360 batches: 0.0361
trigger times: 0
Loss after 1420001460 batches: 0.0358
trigger times: 1
Loss after 1420132560 batches: 0.0351
trigger times: 2
Loss after 1420263660 batches: 0.0356
trigger times: 3
Loss after 1420394760 batches: 0.0350
trigger times: 4
Loss after 1420525860 batches: 0.0346
trigger times: 5
Loss after 1420656960 batches: 0.0346
trigger times: 6
Loss after 1420788060 batches: 0.0348
trigger times: 7
Loss after 1420919160 batches: 0.0344
trigger times: 0
Loss after 1421050260 batches: 0.0336
trigger times: 1
Loss after 1421181360 batches: 0.0338
trigger times: 0
Loss after 1421312460 batches: 0.0332
trigger times: 1
Loss after 1421443560 batches: 0.0336
trigger times: 2
Loss after 1421574660 batches: 0.0335
trigger times: 3
Loss after 1421705760 batches: 0.0336
trigger times: 4
Loss after 1421836860 batches: 0.0331
trigger times: 5
Loss after 1421967960 batches: 0.0325
trigger times: 6
Loss after 1422099060 batches: 0.0333
trigger times: 7
Loss after 1422230160 batches: 0.0326
trigger times: 8
Loss after 1422361260 batches: 0.0323
trigger times: 9
Loss after 1422492360 batches: 0.0323
trigger times: 10
Loss after 1422623460 batches: 0.0329
trigger times: 11
Loss after 1422754560 batches: 0.0324
trigger times: 12
Loss after 1422885660 batches: 0.0316
trigger times: 13
Loss after 1423016760 batches: 0.0314
trigger times: 14
Loss after 1423147860 batches: 0.0315
trigger times: 15
Loss after 1423278960 batches: 0.0313
trigger times: 16
Loss after 1423410060 batches: 0.0313
trigger times: 17
Loss after 1423541160 batches: 0.0317
trigger times: 0
Loss after 1423672260 batches: 0.0317
trigger times: 1
Loss after 1423803360 batches: 0.0316
trigger times: 2
Loss after 1423934460 batches: 0.0309
trigger times: 3
Loss after 1424065560 batches: 0.0310
trigger times: 4
Loss after 1424196660 batches: 0.0307
trigger times: 5
Loss after 1424327760 batches: 0.0310
trigger times: 6
Loss after 1424458860 batches: 0.0304
trigger times: 0
Loss after 1424589960 batches: 0.0304
trigger times: 1
Loss after 1424721060 batches: 0.0305
trigger times: 2
Loss after 1424852160 batches: 0.0302
trigger times: 3
Loss after 1424983260 batches: 0.0305
trigger times: 4
Loss after 1425114360 batches: 0.0298
trigger times: 5
Loss after 1425245460 batches: 0.0302
trigger times: 6
Loss after 1425376560 batches: 0.0304
trigger times: 7
Loss after 1425507660 batches: 0.0300
trigger times: 8
Loss after 1425638760 batches: 0.0302
trigger times: 9
Loss after 1425769860 batches: 0.0300
trigger times: 10
Loss after 1425900960 batches: 0.0299
trigger times: 11
Loss after 1426032060 batches: 0.0289
trigger times: 12
Loss after 1426163160 batches: 0.0289
trigger times: 13
Loss after 1426294260 batches: 0.0295
trigger times: 14
Loss after 1426425360 batches: 0.0292
trigger times: 15
Loss after 1426556460 batches: 0.0294
trigger times: 16
Loss after 1426687560 batches: 0.0289
trigger times: 17
Loss after 1426818660 batches: 0.0283
trigger times: 18
Loss after 1426949760 batches: 0.0285
trigger times: 19
Loss after 1427080860 batches: 0.0286
trigger times: 20
Early stopping!
Start to test process.
Loss after 1427211960 batches: 0.0290
Time to train on one home:  834.5484991073608
trigger times: 0
Loss after 1427305920 batches: 0.5372
trigger times: 0
Loss after 1427399880 batches: 0.1962
trigger times: 1
Loss after 1427493840 batches: 0.1158
trigger times: 0
Loss after 1427587800 batches: 0.0902
trigger times: 0
Loss after 1427681760 batches: 0.0792
trigger times: 1
Loss after 1427775720 batches: 0.0707
trigger times: 0
Loss after 1427869680 batches: 0.0643
trigger times: 1
Loss after 1427963640 batches: 0.0601
trigger times: 2
Loss after 1428057600 batches: 0.0575
trigger times: 3
Loss after 1428151560 batches: 0.0559
trigger times: 4
Loss after 1428245520 batches: 0.0527
trigger times: 0
Loss after 1428339480 batches: 0.0509
trigger times: 1
Loss after 1428433440 batches: 0.0491
trigger times: 2
Loss after 1428527400 batches: 0.0477
trigger times: 3
Loss after 1428621360 batches: 0.0470
trigger times: 4
Loss after 1428715320 batches: 0.0465
trigger times: 5
Loss after 1428809280 batches: 0.0453
trigger times: 6
Loss after 1428903240 batches: 0.0434
trigger times: 7
Loss after 1428997200 batches: 0.0430
trigger times: 8
Loss after 1429091160 batches: 0.0422
trigger times: 9
Loss after 1429185120 batches: 0.0409
trigger times: 10
Loss after 1429279080 batches: 0.0400
trigger times: 11
Loss after 1429373040 batches: 0.0398
trigger times: 12
Loss after 1429467000 batches: 0.0395
trigger times: 13
Loss after 1429560960 batches: 0.0394
trigger times: 14
Loss after 1429654920 batches: 0.0387
trigger times: 15
Loss after 1429748880 batches: 0.0378
trigger times: 16
Loss after 1429842840 batches: 0.0374
trigger times: 0
Loss after 1429936800 batches: 0.0383
trigger times: 1
Loss after 1430030760 batches: 0.0365
trigger times: 2
Loss after 1430124720 batches: 0.0363
trigger times: 3
Loss after 1430218680 batches: 0.0363
trigger times: 4
Loss after 1430312640 batches: 0.0359
trigger times: 5
Loss after 1430406600 batches: 0.0348
trigger times: 6
Loss after 1430500560 batches: 0.0353
trigger times: 7
Loss after 1430594520 batches: 0.0344
trigger times: 8
Loss after 1430688480 batches: 0.0341
trigger times: 9
Loss after 1430782440 batches: 0.0335
trigger times: 10
Loss after 1430876400 batches: 0.0336
trigger times: 11
Loss after 1430970360 batches: 0.0338
trigger times: 0
Loss after 1431064320 batches: 0.0329
trigger times: 1
Loss after 1431158280 batches: 0.0332
trigger times: 2
Loss after 1431252240 batches: 0.0329
trigger times: 3
Loss after 1431346200 batches: 0.0323
trigger times: 4
Loss after 1431440160 batches: 0.0320
trigger times: 0
Loss after 1431534120 batches: 0.0325
trigger times: 1
Loss after 1431628080 batches: 0.0318
trigger times: 2
Loss after 1431722040 batches: 0.0317
trigger times: 3
Loss after 1431816000 batches: 0.0315
trigger times: 4
Loss after 1431909960 batches: 0.0312
trigger times: 5
Loss after 1432003920 batches: 0.0320
trigger times: 6
Loss after 1432097880 batches: 0.0309
trigger times: 7
Loss after 1432191840 batches: 0.0304
trigger times: 8
Loss after 1432285800 batches: 0.0306
trigger times: 9
Loss after 1432379760 batches: 0.0304
trigger times: 10
Loss after 1432473720 batches: 0.0306
trigger times: 11
Loss after 1432567680 batches: 0.0306
trigger times: 12
Loss after 1432661640 batches: 0.0298
trigger times: 13
Loss after 1432755600 batches: 0.0295
trigger times: 14
Loss after 1432849560 batches: 0.0293
trigger times: 15
Loss after 1432943520 batches: 0.0289
trigger times: 16
Loss after 1433037480 batches: 0.0292
trigger times: 17
Loss after 1433131440 batches: 0.0288
trigger times: 18
Loss after 1433225400 batches: 0.0293
trigger times: 19
Loss after 1433319360 batches: 0.0284
trigger times: 20
Early stopping!
Start to test process.
Loss after 1433413320 batches: 0.0287
Time to train on one home:  369.18015718460083
trigger times: 0
Loss after 1433544420 batches: 0.0671
trigger times: 1
Loss after 1433675520 batches: 0.0137
trigger times: 2
Loss after 1433806620 batches: 0.0108
trigger times: 0
Loss after 1433937720 batches: 0.0089
trigger times: 1
Loss after 1434068820 batches: 0.0080
trigger times: 2
Loss after 1434199920 batches: 0.0072
trigger times: 3
Loss after 1434331020 batches: 0.0068
trigger times: 0
Loss after 1434462120 batches: 0.0062
trigger times: 1
Loss after 1434593220 batches: 0.0058
trigger times: 2
Loss after 1434724320 batches: 0.0055
trigger times: 3
Loss after 1434855420 batches: 0.0053
trigger times: 0
Loss after 1434986520 batches: 0.0050
trigger times: 1
Loss after 1435117620 batches: 0.0049
trigger times: 0
Loss after 1435248720 batches: 0.0047
trigger times: 1
Loss after 1435379820 batches: 0.0048
trigger times: 2
Loss after 1435510920 batches: 0.0046
trigger times: 3
Loss after 1435642020 batches: 0.0044
trigger times: 4
Loss after 1435773120 batches: 0.0044
trigger times: 5
Loss after 1435904220 batches: 0.0042
trigger times: 6
Loss after 1436035320 batches: 0.0041
trigger times: 7
Loss after 1436166420 batches: 0.0039
trigger times: 8
Loss after 1436297520 batches: 0.0038
trigger times: 9
Loss after 1436428620 batches: 0.0038
trigger times: 10
Loss after 1436559720 batches: 0.0038
trigger times: 11
Loss after 1436690820 batches: 0.0038
trigger times: 12
Loss after 1436821920 batches: 0.0037
trigger times: 13
Loss after 1436953020 batches: 0.0036
trigger times: 14
Loss after 1437084120 batches: 0.0036
trigger times: 15
Loss after 1437215220 batches: 0.0035
trigger times: 16
Loss after 1437346320 batches: 0.0034
trigger times: 17
Loss after 1437477420 batches: 0.0034
trigger times: 18
Loss after 1437608520 batches: 0.0034
trigger times: 19
Loss after 1437739620 batches: 0.0033
trigger times: 20
Early stopping!
Start to test process.
Loss after 1437870720 batches: 0.0034
Time to train on one home:  259.7255582809448
trigger times: 0
Loss after 1438001820 batches: 0.1722
trigger times: 1
Loss after 1438132920 batches: 0.0511
trigger times: 0
Loss after 1438264020 batches: 0.0355
trigger times: 1
Loss after 1438395120 batches: 0.0300
trigger times: 0
Loss after 1438526220 batches: 0.0266
trigger times: 0
Loss after 1438657320 batches: 0.0244
trigger times: 1
Loss after 1438788420 batches: 0.0233
trigger times: 2
Loss after 1438919520 batches: 0.0220
trigger times: 3
Loss after 1439050620 batches: 0.0213
trigger times: 4
Loss after 1439181720 batches: 0.0203
trigger times: 0
Loss after 1439312820 batches: 0.0197
trigger times: 1
Loss after 1439443920 batches: 0.0191
trigger times: 2
Loss after 1439575020 batches: 0.0185
trigger times: 3
Loss after 1439706120 batches: 0.0178
trigger times: 4
Loss after 1439837220 batches: 0.0176
trigger times: 5
Loss after 1439968320 batches: 0.0172
trigger times: 6
Loss after 1440099420 batches: 0.0168
trigger times: 7
Loss after 1440230520 batches: 0.0167
trigger times: 0
Loss after 1440361620 batches: 0.0166
trigger times: 1
Loss after 1440492720 batches: 0.0162
trigger times: 2
Loss after 1440623820 batches: 0.0160
trigger times: 3
Loss after 1440754920 batches: 0.0157
trigger times: 4
Loss after 1440886020 batches: 0.0158
trigger times: 0
Loss after 1441017120 batches: 0.0153
trigger times: 1
Loss after 1441148220 batches: 0.0153
trigger times: 2
Loss after 1441279320 batches: 0.0152
trigger times: 3
Loss after 1441410420 batches: 0.0146
trigger times: 0
Loss after 1441541520 batches: 0.0150
trigger times: 1
Loss after 1441672620 batches: 0.0145
trigger times: 2
Loss after 1441803720 batches: 0.0143
trigger times: 3
Loss after 1441934820 batches: 0.0142
trigger times: 4
Loss after 1442065920 batches: 0.0142
trigger times: 5
Loss after 1442197020 batches: 0.0142
trigger times: 6
Loss after 1442328120 batches: 0.0138
trigger times: 7
Loss after 1442459220 batches: 0.0137
trigger times: 8
Loss after 1442590320 batches: 0.0135
trigger times: 9
Loss after 1442721420 batches: 0.0135
trigger times: 10
Loss after 1442852520 batches: 0.0136
trigger times: 11
Loss after 1442983620 batches: 0.0135
trigger times: 12
Loss after 1443114720 batches: 0.0133
trigger times: 13
Loss after 1443245820 batches: 0.0133
trigger times: 14
Loss after 1443376920 batches: 0.0132
trigger times: 15
Loss after 1443508020 batches: 0.0131
trigger times: 16
Loss after 1443639120 batches: 0.0129
trigger times: 17
Loss after 1443770220 batches: 0.0129
trigger times: 18
Loss after 1443901320 batches: 0.0127
trigger times: 19
Loss after 1444032420 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 1444163520 batches: 0.0126
Time to train on one home:  358.94050312042236
trigger times: 0
Loss after 1444294620 batches: 0.3452
trigger times: 0
Loss after 1444425720 batches: 0.1313
trigger times: 0
Loss after 1444556820 batches: 0.0966
trigger times: 0
Loss after 1444687920 batches: 0.0837
trigger times: 0
Loss after 1444819020 batches: 0.0768
trigger times: 0
Loss after 1444950120 batches: 0.0719
trigger times: 1
Loss after 1445081220 batches: 0.0683
trigger times: 0
Loss after 1445212320 batches: 0.0657
trigger times: 1
Loss after 1445343420 batches: 0.0629
trigger times: 2
Loss after 1445474520 batches: 0.0616
trigger times: 3
Loss after 1445605620 batches: 0.0599
trigger times: 4
Loss after 1445736720 batches: 0.0590
trigger times: 5
Loss after 1445867820 batches: 0.0574
trigger times: 6
Loss after 1445998920 batches: 0.0562
trigger times: 0
Loss after 1446130020 batches: 0.0550
trigger times: 1
Loss after 1446261120 batches: 0.0551
trigger times: 2
Loss after 1446392220 batches: 0.0540
trigger times: 0
Loss after 1446523320 batches: 0.0537
trigger times: 1
Loss after 1446654420 batches: 0.0528
trigger times: 0
Loss after 1446785520 batches: 0.0521
trigger times: 1
Loss after 1446916620 batches: 0.0518
trigger times: 2
Loss after 1447047720 batches: 0.0512
trigger times: 0
Loss after 1447178820 batches: 0.0506
trigger times: 1
Loss after 1447309920 batches: 0.0502
trigger times: 2
Loss after 1447441020 batches: 0.0498
trigger times: 3
Loss after 1447572120 batches: 0.0495
trigger times: 4
Loss after 1447703220 batches: 0.0489
trigger times: 0
Loss after 1447834320 batches: 0.0488
trigger times: 1
Loss after 1447965420 batches: 0.0485
trigger times: 2
Loss after 1448096520 batches: 0.0482
trigger times: 0
Loss after 1448227620 batches: 0.0479
trigger times: 1
Loss after 1448358720 batches: 0.0472
trigger times: 2
Loss after 1448489820 batches: 0.0467
trigger times: 3
Loss after 1448620920 batches: 0.0473
trigger times: 0
Loss after 1448752020 batches: 0.0468
trigger times: 1
Loss after 1448883120 batches: 0.0462
trigger times: 2
Loss after 1449014220 batches: 0.0461
trigger times: 3
Loss after 1449145320 batches: 0.0465
trigger times: 4
Loss after 1449276420 batches: 0.0460
trigger times: 5
Loss after 1449407520 batches: 0.0458
trigger times: 6
Loss after 1449538620 batches: 0.0460
trigger times: 7
Loss after 1449669720 batches: 0.0462
trigger times: 8
Loss after 1449800820 batches: 0.0452
trigger times: 9
Loss after 1449931920 batches: 0.0445
trigger times: 10
Loss after 1450063020 batches: 0.0449
trigger times: 11
Loss after 1450194120 batches: 0.0447
trigger times: 12
Loss after 1450325220 batches: 0.0444
trigger times: 13
Loss after 1450456320 batches: 0.0445
trigger times: 0
Loss after 1450587420 batches: 0.0442
trigger times: 1
Loss after 1450718520 batches: 0.0437
trigger times: 2
Loss after 1450849620 batches: 0.0441
trigger times: 3
Loss after 1450980720 batches: 0.0435
trigger times: 4
Loss after 1451111820 batches: 0.0433
trigger times: 5
Loss after 1451242920 batches: 0.0435
trigger times: 6
Loss after 1451374020 batches: 0.0433
trigger times: 7
Loss after 1451505120 batches: 0.0432
trigger times: 8
Loss after 1451636220 batches: 0.0432
trigger times: 9
Loss after 1451767320 batches: 0.0428
trigger times: 10
Loss after 1451898420 batches: 0.0428
trigger times: 11
Loss after 1452029520 batches: 0.0425
trigger times: 12
Loss after 1452160620 batches: 0.0421
trigger times: 13
Loss after 1452291720 batches: 0.0420
trigger times: 14
Loss after 1452422820 batches: 0.0424
trigger times: 15
Loss after 1452553920 batches: 0.0423
trigger times: 16
Loss after 1452685020 batches: 0.0419
trigger times: 17
Loss after 1452816120 batches: 0.0422
trigger times: 18
Loss after 1452947220 batches: 0.0417
trigger times: 19
Loss after 1453078320 batches: 0.0413
trigger times: 20
Early stopping!
Start to test process.
Loss after 1453209420 batches: 0.0416
Time to train on one home:  512.0813779830933
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642, 0.03501720033327798, 0.03263172240869951, 0.030110256078310443, 0.02544188748067248, 0.02612378438006769]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217], [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944], [0.6491197148958842, 0.29803996281550993, 0.44160060986867417, 1.203530351478871, 0.5750418631238572, 28.433835927602637, 1775.19], [0.6497863001293607, 0.29730357979276123, 0.44701117944787117, 1.2068712720184533, 0.5756451041104418, 28.512766372815683, 1777.0521], [0.6468338668346405, 0.30051325220168756, 0.4517286880074036, 1.1964129191375579, 0.5730157578453048, 28.26568403748201, 1768.9353], [0.6481483081976572, 0.29911867824708416, 0.44591549869242414, 1.1883481958771474, 0.5741581852808264, 28.07515205986611, 1772.462]]
Round_12_results:  [0.6481483081976572, 0.29911867824708416, 0.44591549869242414, 1.1883481958771474, 0.5741581852808264, 28.07515205986611, 1772.462]
trigger times: 0
Loss after 1453312020 batches: 0.3953
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 11556 < 11557; dropping {'Training_Loss': 0.39532797783613205, 'Validation_Loss': 0.747794164551629, 'Training_R2': 0.5988344828659353, 'Validation_R2': 0.3030783945315172, 'Training_F1': 0.7587403558885591, 'Validation_F1': 0.43905275809866734, 'Training_NEP': 0.483929289926317, 'Validation_NEP': 1.0873776838204232, 'Training_NDE': 0.2702211888533701, 'Validation_NDE': 0.5549916289932423, 'Training_MAE': 31.76116601449472, 'Validation_MAE': 29.820407344021678, 'Training_MSE': 3566.034, 'Validation_MSE': 2049.5688}.
trigger times: 1
Loss after 1453414620 batches: 0.1645
trigger times: 2
Loss after 1453517220 batches: 0.1051
trigger times: 3
Loss after 1453619820 batches: 0.0804
trigger times: 4
Loss after 1453722420 batches: 0.0662
trigger times: 5
Loss after 1453825020 batches: 0.0595
trigger times: 6
Loss after 1453927620 batches: 0.0532
trigger times: 7
Loss after 1454030220 batches: 0.0479
trigger times: 8
Loss after 1454132820 batches: 0.0438
trigger times: 9
Loss after 1454235420 batches: 0.0433
trigger times: 10
Loss after 1454338020 batches: 0.0437
trigger times: 11
Loss after 1454440620 batches: 0.0462
trigger times: 12
Loss after 1454543220 batches: 0.0388
trigger times: 13
Loss after 1454645820 batches: 0.0387
trigger times: 14
Loss after 1454748420 batches: 0.0377
trigger times: 15
Loss after 1454851020 batches: 0.0367
trigger times: 16
Loss after 1454953620 batches: 0.0368
trigger times: 17
Loss after 1455056220 batches: 0.0363
trigger times: 18
Loss after 1455158820 batches: 0.0333
trigger times: 19
Loss after 1455261420 batches: 0.0323
trigger times: 20
Early stopping!
Start to test process.
Loss after 1455364020 batches: 0.0319
Time to train on one home:  136.33102750778198
trigger times: 0
Loss after 1455495120 batches: 0.2543
trigger times: 1
Loss after 1455626220 batches: 0.0997
trigger times: 0
Loss after 1455757320 batches: 0.0678
trigger times: 1
Loss after 1455888420 batches: 0.0554
trigger times: 2
Loss after 1456019520 batches: 0.0492
trigger times: 3
Loss after 1456150620 batches: 0.0454
trigger times: 4
Loss after 1456281720 batches: 0.0425
trigger times: 5
Loss after 1456412820 batches: 0.0399
trigger times: 6
Loss after 1456543920 batches: 0.0380
trigger times: 7
Loss after 1456675020 batches: 0.0365
trigger times: 8
Loss after 1456806120 batches: 0.0349
trigger times: 9
Loss after 1456937220 batches: 0.0340
trigger times: 10
Loss after 1457068320 batches: 0.0329
trigger times: 11
Loss after 1457199420 batches: 0.0316
trigger times: 12
Loss after 1457330520 batches: 0.0311
trigger times: 13
Loss after 1457461620 batches: 0.0308
trigger times: 14
Loss after 1457592720 batches: 0.0297
trigger times: 15
Loss after 1457723820 batches: 0.0299
trigger times: 16
Loss after 1457854920 batches: 0.0290
trigger times: 17
Loss after 1457986020 batches: 0.0285
trigger times: 18
Loss after 1458117120 batches: 0.0279
trigger times: 19
Loss after 1458248220 batches: 0.0276
trigger times: 20
Early stopping!
Start to test process.
Loss after 1458379320 batches: 0.0269
Time to train on one home:  178.73604536056519
trigger times: 0
Loss after 1458510420 batches: 0.4512
trigger times: 0
Loss after 1458641520 batches: 0.1598
trigger times: 1
Loss after 1458772620 batches: 0.1065
trigger times: 0
Loss after 1458903720 batches: 0.0887
trigger times: 1
Loss after 1459034820 batches: 0.0776
trigger times: 2
Loss after 1459165920 batches: 0.0707
trigger times: 3
Loss after 1459297020 batches: 0.0662
trigger times: 4
Loss after 1459428120 batches: 0.0629
trigger times: 5
Loss after 1459559220 batches: 0.0595
trigger times: 6
Loss after 1459690320 batches: 0.0575
trigger times: 7
Loss after 1459821420 batches: 0.0545
trigger times: 8
Loss after 1459952520 batches: 0.0533
trigger times: 9
Loss after 1460083620 batches: 0.0519
trigger times: 10
Loss after 1460214720 batches: 0.0502
trigger times: 11
Loss after 1460345820 batches: 0.0486
trigger times: 12
Loss after 1460476920 batches: 0.0477
trigger times: 13
Loss after 1460608020 batches: 0.0472
trigger times: 14
Loss after 1460739120 batches: 0.0454
trigger times: 15
Loss after 1460870220 batches: 0.0451
trigger times: 16
Loss after 1461001320 batches: 0.0442
trigger times: 17
Loss after 1461132420 batches: 0.0437
trigger times: 18
Loss after 1461263520 batches: 0.0425
trigger times: 19
Loss after 1461394620 batches: 0.0419
trigger times: 20
Early stopping!
Start to test process.
Loss after 1461525720 batches: 0.0421
Time to train on one home:  185.60020804405212
trigger times: 0
Loss after 1461654360 batches: 0.1632
trigger times: 0
Loss after 1461783000 batches: 0.0515
trigger times: 0
Loss after 1461911640 batches: 0.0377
trigger times: 1
Loss after 1462040280 batches: 0.0331
trigger times: 2
Loss after 1462168920 batches: 0.0297
trigger times: 0
Loss after 1462297560 batches: 0.0279
trigger times: 1
Loss after 1462426200 batches: 0.0265
trigger times: 2
Loss after 1462554840 batches: 0.0250
trigger times: 3
Loss after 1462683480 batches: 0.0236
trigger times: 4
Loss after 1462812120 batches: 0.0231
trigger times: 5
Loss after 1462940760 batches: 0.0225
trigger times: 0
Loss after 1463069400 batches: 0.0220
trigger times: 1
Loss after 1463198040 batches: 0.0217
trigger times: 2
Loss after 1463326680 batches: 0.0208
trigger times: 3
Loss after 1463455320 batches: 0.0209
trigger times: 4
Loss after 1463583960 batches: 0.0204
trigger times: 5
Loss after 1463712600 batches: 0.0197
trigger times: 0
Loss after 1463841240 batches: 0.0198
trigger times: 1
Loss after 1463969880 batches: 0.0191
trigger times: 2
Loss after 1464098520 batches: 0.0188
trigger times: 3
Loss after 1464227160 batches: 0.0189
trigger times: 4
Loss after 1464355800 batches: 0.0188
trigger times: 5
Loss after 1464484440 batches: 0.0184
trigger times: 0
Loss after 1464613080 batches: 0.0182
trigger times: 0
Loss after 1464741720 batches: 0.0175
trigger times: 1
Loss after 1464870360 batches: 0.0176
trigger times: 2
Loss after 1464999000 batches: 0.0181
trigger times: 3
Loss after 1465127640 batches: 0.0179
trigger times: 4
Loss after 1465256280 batches: 0.0174
trigger times: 5
Loss after 1465384920 batches: 0.0170
trigger times: 6
Loss after 1465513560 batches: 0.0171
trigger times: 7
Loss after 1465642200 batches: 0.0170
trigger times: 8
Loss after 1465770840 batches: 0.0170
trigger times: 9
Loss after 1465899480 batches: 0.0166
trigger times: 10
Loss after 1466028120 batches: 0.0166
trigger times: 11
Loss after 1466156760 batches: 0.0161
trigger times: 12
Loss after 1466285400 batches: 0.0162
trigger times: 13
Loss after 1466414040 batches: 0.0162
trigger times: 14
Loss after 1466542680 batches: 0.0164
trigger times: 15
Loss after 1466671320 batches: 0.0157
trigger times: 16
Loss after 1466799960 batches: 0.0160
trigger times: 17
Loss after 1466928600 batches: 0.0156
trigger times: 18
Loss after 1467057240 batches: 0.0159
trigger times: 19
Loss after 1467185880 batches: 0.0155
trigger times: 20
Early stopping!
Start to test process.
Loss after 1467314520 batches: 0.0156
Time to train on one home:  333.2373158931732
trigger times: 0
Loss after 1467445620 batches: 0.5217
trigger times: 1
Loss after 1467576720 batches: 0.1824
trigger times: 2
Loss after 1467707820 batches: 0.1139
trigger times: 0
Loss after 1467838920 batches: 0.0909
trigger times: 1
Loss after 1467970020 batches: 0.0798
trigger times: 2
Loss after 1468101120 batches: 0.0733
trigger times: 3
Loss after 1468232220 batches: 0.0686
trigger times: 4
Loss after 1468363320 batches: 0.0642
trigger times: 5
Loss after 1468494420 batches: 0.0616
trigger times: 6
Loss after 1468625520 batches: 0.0584
trigger times: 7
Loss after 1468756620 batches: 0.0559
trigger times: 8
Loss after 1468887720 batches: 0.0541
trigger times: 9
Loss after 1469018820 batches: 0.0527
trigger times: 10
Loss after 1469149920 batches: 0.0513
trigger times: 11
Loss after 1469281020 batches: 0.0508
trigger times: 12
Loss after 1469412120 batches: 0.0492
trigger times: 13
Loss after 1469543220 batches: 0.0476
trigger times: 14
Loss after 1469674320 batches: 0.0471
trigger times: 15
Loss after 1469805420 batches: 0.0466
trigger times: 16
Loss after 1469936520 batches: 0.0455
trigger times: 17
Loss after 1470067620 batches: 0.0447
trigger times: 18
Loss after 1470198720 batches: 0.0444
trigger times: 19
Loss after 1470329820 batches: 0.0435
trigger times: 20
Early stopping!
Start to test process.
Loss after 1470460920 batches: 0.0423
Time to train on one home:  187.0056450366974
trigger times: 0
Loss after 1470592020 batches: 0.5049
trigger times: 1
Loss after 1470723120 batches: 0.2088
trigger times: 2
Loss after 1470854220 batches: 0.1274
trigger times: 3
Loss after 1470985320 batches: 0.0978
trigger times: 4
Loss after 1471116420 batches: 0.0809
trigger times: 5
Loss after 1471247520 batches: 0.0742
trigger times: 6
Loss after 1471378620 batches: 0.0655
trigger times: 7
Loss after 1471509720 batches: 0.0607
trigger times: 8
Loss after 1471640820 batches: 0.0586
trigger times: 9
Loss after 1471771920 batches: 0.0568
trigger times: 10
Loss after 1471903020 batches: 0.0552
trigger times: 11
Loss after 1472034120 batches: 0.0519
trigger times: 0
Loss after 1472165220 batches: 0.0542
trigger times: 1
Loss after 1472296320 batches: 0.0506
trigger times: 2
Loss after 1472427420 batches: 0.0489
trigger times: 0
Loss after 1472558520 batches: 0.0473
trigger times: 1
Loss after 1472689620 batches: 0.0488
trigger times: 2
Loss after 1472820720 batches: 0.0468
trigger times: 0
Loss after 1472951820 batches: 0.0454
trigger times: 0
Loss after 1473082920 batches: 0.0429
trigger times: 1
Loss after 1473214020 batches: 0.0437
trigger times: 0
Loss after 1473345120 batches: 0.0422
trigger times: 1
Loss after 1473476220 batches: 0.0431
trigger times: 0
Loss after 1473607320 batches: 0.0422
trigger times: 1
Loss after 1473738420 batches: 0.0406
trigger times: 0
Loss after 1473869520 batches: 0.0397
trigger times: 1
Loss after 1474000620 batches: 0.0397
trigger times: 0
Loss after 1474131720 batches: 0.0393
trigger times: 1
Loss after 1474262820 batches: 0.0380
trigger times: 0
Loss after 1474393920 batches: 0.0374
trigger times: 1
Loss after 1474525020 batches: 0.0374
trigger times: 2
Loss after 1474656120 batches: 0.0369
trigger times: 3
Loss after 1474787220 batches: 0.0369
trigger times: 4
Loss after 1474918320 batches: 0.0381
trigger times: 5
Loss after 1475049420 batches: 0.0366
trigger times: 6
Loss after 1475180520 batches: 0.0363
trigger times: 7
Loss after 1475311620 batches: 0.0354
trigger times: 8
Loss after 1475442720 batches: 0.0344
trigger times: 0
Loss after 1475573820 batches: 0.0355
trigger times: 1
Loss after 1475704920 batches: 0.0358
trigger times: 2
Loss after 1475836020 batches: 0.0346
trigger times: 3
Loss after 1475967120 batches: 0.0350
trigger times: 4
Loss after 1476098220 batches: 0.0351
trigger times: 0
Loss after 1476229320 batches: 0.0342
trigger times: 1
Loss after 1476360420 batches: 0.0341
trigger times: 2
Loss after 1476491520 batches: 0.0330
trigger times: 0
Loss after 1476622620 batches: 0.0322
trigger times: 1
Loss after 1476753720 batches: 0.0322
trigger times: 2
Loss after 1476884820 batches: 0.0327
trigger times: 3
Loss after 1477015920 batches: 0.0325
trigger times: 4
Loss after 1477147020 batches: 0.0326
trigger times: 5
Loss after 1477278120 batches: 0.0327
trigger times: 6
Loss after 1477409220 batches: 0.0314
trigger times: 0
Loss after 1477540320 batches: 0.0304
trigger times: 1
Loss after 1477671420 batches: 0.0308
trigger times: 0
Loss after 1477802520 batches: 0.0321
trigger times: 1
Loss after 1477933620 batches: 0.0306
trigger times: 0
Loss after 1478064720 batches: 0.0316
trigger times: 1
Loss after 1478195820 batches: 0.0315
trigger times: 2
Loss after 1478326920 batches: 0.0304
trigger times: 0
Loss after 1478458020 batches: 0.0298
trigger times: 1
Loss after 1478589120 batches: 0.0303
trigger times: 2
Loss after 1478720220 batches: 0.0290
trigger times: 3
Loss after 1478851320 batches: 0.0289
trigger times: 4
Loss after 1478982420 batches: 0.0304
trigger times: 5
Loss after 1479113520 batches: 0.0306
trigger times: 0
Loss after 1479244620 batches: 0.0292
trigger times: 0
Loss after 1479375720 batches: 0.0295
trigger times: 0
Loss after 1479506820 batches: 0.0284
trigger times: 1
Loss after 1479637920 batches: 0.0281
trigger times: 0
Loss after 1479769020 batches: 0.0290
trigger times: 1
Loss after 1479900120 batches: 0.0280
trigger times: 2
Loss after 1480031220 batches: 0.0268
trigger times: 3
Loss after 1480162320 batches: 0.0275
trigger times: 4
Loss after 1480293420 batches: 0.0294
trigger times: 5
Loss after 1480424520 batches: 0.0296
trigger times: 6
Loss after 1480555620 batches: 0.0289
trigger times: 7
Loss after 1480686720 batches: 0.0279
trigger times: 8
Loss after 1480817820 batches: 0.0285
trigger times: 9
Loss after 1480948920 batches: 0.0281
trigger times: 10
Loss after 1481080020 batches: 0.0272
trigger times: 0
Loss after 1481211120 batches: 0.0278
trigger times: 1
Loss after 1481342220 batches: 0.0285
trigger times: 2
Loss after 1481473320 batches: 0.0272
trigger times: 3
Loss after 1481604420 batches: 0.0268
trigger times: 4
Loss after 1481735520 batches: 0.0268
trigger times: 5
Loss after 1481866620 batches: 0.0268
trigger times: 0
Loss after 1481997720 batches: 0.0267
trigger times: 1
Loss after 1482128820 batches: 0.0284
trigger times: 0
Loss after 1482259920 batches: 0.0285
trigger times: 1
Loss after 1482391020 batches: 0.0277
trigger times: 2
Loss after 1482522120 batches: 0.0261
trigger times: 3
Loss after 1482653220 batches: 0.0269
trigger times: 4
Loss after 1482784320 batches: 0.0260
trigger times: 5
Loss after 1482915420 batches: 0.0264
trigger times: 6
Loss after 1483046520 batches: 0.0267
trigger times: 7
Loss after 1483177620 batches: 0.0260
trigger times: 8
Loss after 1483308720 batches: 0.0264
trigger times: 9
Loss after 1483439820 batches: 0.0264
trigger times: 10
Loss after 1483570920 batches: 0.0270
trigger times: 11
Loss after 1483702020 batches: 0.0250
trigger times: 12
Loss after 1483833120 batches: 0.0252
trigger times: 13
Loss after 1483964220 batches: 0.0251
trigger times: 14
Loss after 1484095320 batches: 0.0254
trigger times: 0
Loss after 1484226420 batches: 0.0265
trigger times: 0
Loss after 1484357520 batches: 0.0265
trigger times: 1
Loss after 1484488620 batches: 0.0255
trigger times: 0
Loss after 1484619720 batches: 0.0262
trigger times: 1
Loss after 1484750820 batches: 0.0255
trigger times: 2
Loss after 1484881920 batches: 0.0260
trigger times: 3
Loss after 1485013020 batches: 0.0268
trigger times: 4
Loss after 1485144120 batches: 0.0242
trigger times: 5
Loss after 1485275220 batches: 0.0247
trigger times: 6
Loss after 1485406320 batches: 0.0238
trigger times: 7
Loss after 1485537420 batches: 0.0252
trigger times: 8
Loss after 1485668520 batches: 0.0250
trigger times: 0
Loss after 1485799620 batches: 0.0258
trigger times: 1
Loss after 1485930720 batches: 0.0240
trigger times: 2
Loss after 1486061820 batches: 0.0251
trigger times: 3
Loss after 1486192920 batches: 0.0246
trigger times: 4
Loss after 1486324020 batches: 0.0248
trigger times: 5
Loss after 1486455120 batches: 0.0255
trigger times: 6
Loss after 1486586220 batches: 0.0238
trigger times: 7
Loss after 1486717320 batches: 0.0257
trigger times: 8
Loss after 1486848420 batches: 0.0261
trigger times: 9
Loss after 1486979520 batches: 0.0241
trigger times: 10
Loss after 1487110620 batches: 0.0236
trigger times: 11
Loss after 1487241720 batches: 0.0234
trigger times: 12
Loss after 1487372820 batches: 0.0245
trigger times: 13
Loss after 1487503920 batches: 0.0242
trigger times: 14
Loss after 1487635020 batches: 0.0235
trigger times: 15
Loss after 1487766120 batches: 0.0242
trigger times: 16
Loss after 1487897220 batches: 0.0242
trigger times: 17
Loss after 1488028320 batches: 0.0241
trigger times: 18
Loss after 1488159420 batches: 0.0238
trigger times: 19
Loss after 1488290520 batches: 0.0235
trigger times: 20
Early stopping!
Start to test process.
Loss after 1488421620 batches: 0.0236
Time to train on one home:  1004.4640736579895
trigger times: 0
Loss after 1488552720 batches: 0.1153
trigger times: 1
Loss after 1488683820 batches: 0.0358
trigger times: 0
Loss after 1488814920 batches: 0.0273
trigger times: 1
Loss after 1488946020 batches: 0.0229
trigger times: 2
Loss after 1489077120 batches: 0.0205
trigger times: 3
Loss after 1489208220 batches: 0.0194
trigger times: 4
Loss after 1489339320 batches: 0.0182
trigger times: 5
Loss after 1489470420 batches: 0.0173
trigger times: 6
Loss after 1489601520 batches: 0.0167
trigger times: 7
Loss after 1489732620 batches: 0.0164
trigger times: 8
Loss after 1489863720 batches: 0.0159
trigger times: 9
Loss after 1489994820 batches: 0.0151
trigger times: 10
Loss after 1490125920 batches: 0.0147
trigger times: 11
Loss after 1490257020 batches: 0.0147
trigger times: 12
Loss after 1490388120 batches: 0.0143
trigger times: 13
Loss after 1490519220 batches: 0.0143
trigger times: 14
Loss after 1490650320 batches: 0.0137
trigger times: 15
Loss after 1490781420 batches: 0.0133
trigger times: 16
Loss after 1490912520 batches: 0.0134
trigger times: 17
Loss after 1491043620 batches: 0.0131
trigger times: 18
Loss after 1491174720 batches: 0.0126
trigger times: 19
Loss after 1491305820 batches: 0.0128
trigger times: 20
Early stopping!
Start to test process.
Loss after 1491436920 batches: 0.0127
Time to train on one home:  179.4090268611908
trigger times: 0
Loss after 1491568020 batches: 0.1936
trigger times: 0
Loss after 1491699120 batches: 0.0517
trigger times: 0
Loss after 1491830220 batches: 0.0356
trigger times: 0
Loss after 1491961320 batches: 0.0306
trigger times: 1
Loss after 1492092420 batches: 0.0272
trigger times: 0
Loss after 1492223520 batches: 0.0250
trigger times: 1
Loss after 1492354620 batches: 0.0242
trigger times: 2
Loss after 1492485720 batches: 0.0224
trigger times: 3
Loss after 1492616820 batches: 0.0217
trigger times: 4
Loss after 1492747920 batches: 0.0207
trigger times: 5
Loss after 1492879020 batches: 0.0200
trigger times: 6
Loss after 1493010120 batches: 0.0198
trigger times: 7
Loss after 1493141220 batches: 0.0189
trigger times: 8
Loss after 1493272320 batches: 0.0184
trigger times: 9
Loss after 1493403420 batches: 0.0181
trigger times: 10
Loss after 1493534520 batches: 0.0174
trigger times: 11
Loss after 1493665620 batches: 0.0170
trigger times: 12
Loss after 1493796720 batches: 0.0169
trigger times: 13
Loss after 1493927820 batches: 0.0172
trigger times: 14
Loss after 1494058920 batches: 0.0165
trigger times: 15
Loss after 1494190020 batches: 0.0160
trigger times: 16
Loss after 1494321120 batches: 0.0162
trigger times: 17
Loss after 1494452220 batches: 0.0158
trigger times: 18
Loss after 1494583320 batches: 0.0153
trigger times: 19
Loss after 1494714420 batches: 0.0152
trigger times: 20
Early stopping!
Start to test process.
Loss after 1494845520 batches: 0.0152
Time to train on one home:  200.83280754089355
trigger times: 0
Loss after 1494924120 batches: 0.3899
trigger times: 0
Loss after 1495002720 batches: 0.1204
trigger times: 1
Loss after 1495081320 batches: 0.0720
trigger times: 2
Loss after 1495159920 batches: 0.0580
trigger times: 0
Loss after 1495238520 batches: 0.0507
trigger times: 1
Loss after 1495317120 batches: 0.0475
trigger times: 2
Loss after 1495395720 batches: 0.0419
trigger times: 3
Loss after 1495474320 batches: 0.0407
trigger times: 4
Loss after 1495552920 batches: 0.0373
trigger times: 0
Loss after 1495631520 batches: 0.0361
trigger times: 1
Loss after 1495710120 batches: 0.0351
trigger times: 2
Loss after 1495788720 batches: 0.0342
trigger times: 0
Loss after 1495867320 batches: 0.0320
trigger times: 1
Loss after 1495945920 batches: 0.0314
trigger times: 2
Loss after 1496024520 batches: 0.0312
trigger times: 3
Loss after 1496103120 batches: 0.0305
trigger times: 4
Loss after 1496181720 batches: 0.0300
trigger times: 0
Loss after 1496260320 batches: 0.0291
trigger times: 1
Loss after 1496338920 batches: 0.0280
trigger times: 2
Loss after 1496417520 batches: 0.0276
trigger times: 3
Loss after 1496496120 batches: 0.0274
trigger times: 4
Loss after 1496574720 batches: 0.0272
trigger times: 5
Loss after 1496653320 batches: 0.0275
trigger times: 6
Loss after 1496731920 batches: 0.0270
trigger times: 7
Loss after 1496810520 batches: 0.0258
trigger times: 8
Loss after 1496889120 batches: 0.0260
trigger times: 9
Loss after 1496967720 batches: 0.0249
trigger times: 10
Loss after 1497046320 batches: 0.0256
trigger times: 11
Loss after 1497124920 batches: 0.0251
trigger times: 12
Loss after 1497203520 batches: 0.0247
trigger times: 13
Loss after 1497282120 batches: 0.0241
trigger times: 14
Loss after 1497360720 batches: 0.0235
trigger times: 0
Loss after 1497439320 batches: 0.0242
trigger times: 1
Loss after 1497517920 batches: 0.0244
trigger times: 2
Loss after 1497596520 batches: 0.0239
trigger times: 3
Loss after 1497675120 batches: 0.0235
trigger times: 4
Loss after 1497753720 batches: 0.0238
trigger times: 5
Loss after 1497832320 batches: 0.0227
trigger times: 6
Loss after 1497910920 batches: 0.0228
trigger times: 7
Loss after 1497989520 batches: 0.0225
trigger times: 8
Loss after 1498068120 batches: 0.0226
trigger times: 0
Loss after 1498146720 batches: 0.0223
trigger times: 1
Loss after 1498225320 batches: 0.0226
trigger times: 2
Loss after 1498303920 batches: 0.0216
trigger times: 3
Loss after 1498382520 batches: 0.0225
trigger times: 4
Loss after 1498461120 batches: 0.0220
trigger times: 5
Loss after 1498539720 batches: 0.0224
trigger times: 6
Loss after 1498618320 batches: 0.0214
trigger times: 7
Loss after 1498696920 batches: 0.0212
trigger times: 8
Loss after 1498775520 batches: 0.0215
trigger times: 9
Loss after 1498854120 batches: 0.0205
trigger times: 10
Loss after 1498932720 batches: 0.0209
trigger times: 0
Loss after 1499011320 batches: 0.0220
trigger times: 1
Loss after 1499089920 batches: 0.0219
trigger times: 2
Loss after 1499168520 batches: 0.0206
trigger times: 3
Loss after 1499247120 batches: 0.0204
trigger times: 4
Loss after 1499325720 batches: 0.0202
trigger times: 5
Loss after 1499404320 batches: 0.0211
trigger times: 6
Loss after 1499482920 batches: 0.0205
trigger times: 7
Loss after 1499561520 batches: 0.0201
trigger times: 8
Loss after 1499640120 batches: 0.0205
trigger times: 9
Loss after 1499718720 batches: 0.0199
trigger times: 10
Loss after 1499797320 batches: 0.0203
trigger times: 11
Loss after 1499875920 batches: 0.0197
trigger times: 12
Loss after 1499954520 batches: 0.0196
trigger times: 13
Loss after 1500033120 batches: 0.0206
trigger times: 14
Loss after 1500111720 batches: 0.0189
trigger times: 15
Loss after 1500190320 batches: 0.0195
trigger times: 16
Loss after 1500268920 batches: 0.0192
trigger times: 17
Loss after 1500347520 batches: 0.0193
trigger times: 18
Loss after 1500426120 batches: 0.0186
trigger times: 19
Loss after 1500504720 batches: 0.0196
trigger times: 20
Early stopping!
Start to test process.
Loss after 1500583320 batches: 0.0190
Time to train on one home:  356.2297420501709
trigger times: 0
Loss after 1500714420 batches: 0.1286
trigger times: 0
Loss after 1500845520 batches: 0.0408
trigger times: 0
Loss after 1500976620 batches: 0.0300
trigger times: 1
Loss after 1501107720 batches: 0.0261
trigger times: 0
Loss after 1501238820 batches: 0.0233
trigger times: 0
Loss after 1501369920 batches: 0.0221
trigger times: 1
Loss after 1501501020 batches: 0.0206
trigger times: 2
Loss after 1501632120 batches: 0.0200
trigger times: 3
Loss after 1501763220 batches: 0.0190
trigger times: 4
Loss after 1501894320 batches: 0.0187
trigger times: 5
Loss after 1502025420 batches: 0.0179
trigger times: 6
Loss after 1502156520 batches: 0.0173
trigger times: 7
Loss after 1502287620 batches: 0.0167
trigger times: 8
Loss after 1502418720 batches: 0.0164
trigger times: 9
Loss after 1502549820 batches: 0.0161
trigger times: 10
Loss after 1502680920 batches: 0.0160
trigger times: 0
Loss after 1502812020 batches: 0.0155
trigger times: 1
Loss after 1502943120 batches: 0.0153
trigger times: 2
Loss after 1503074220 batches: 0.0151
trigger times: 3
Loss after 1503205320 batches: 0.0149
trigger times: 4
Loss after 1503336420 batches: 0.0146
trigger times: 5
Loss after 1503467520 batches: 0.0144
trigger times: 6
Loss after 1503598620 batches: 0.0142
trigger times: 0
Loss after 1503729720 batches: 0.0142
trigger times: 1
Loss after 1503860820 batches: 0.0138
trigger times: 2
Loss after 1503991920 batches: 0.0138
trigger times: 3
Loss after 1504123020 batches: 0.0134
trigger times: 4
Loss after 1504254120 batches: 0.0134
trigger times: 5
Loss after 1504385220 batches: 0.0135
trigger times: 6
Loss after 1504516320 batches: 0.0133
trigger times: 7
Loss after 1504647420 batches: 0.0134
trigger times: 0
Loss after 1504778520 batches: 0.0130
trigger times: 0
Loss after 1504909620 batches: 0.0128
trigger times: 1
Loss after 1505040720 batches: 0.0126
trigger times: 0
Loss after 1505171820 batches: 0.0126
trigger times: 1
Loss after 1505302920 batches: 0.0123
trigger times: 2
Loss after 1505434020 batches: 0.0123
trigger times: 3
Loss after 1505565120 batches: 0.0122
trigger times: 4
Loss after 1505696220 batches: 0.0122
trigger times: 0
Loss after 1505827320 batches: 0.0120
trigger times: 1
Loss after 1505958420 batches: 0.0122
trigger times: 2
Loss after 1506089520 batches: 0.0121
trigger times: 3
Loss after 1506220620 batches: 0.0117
trigger times: 4
Loss after 1506351720 batches: 0.0119
trigger times: 5
Loss after 1506482820 batches: 0.0115
trigger times: 6
Loss after 1506613920 batches: 0.0116
trigger times: 7
Loss after 1506745020 batches: 0.0113
trigger times: 8
Loss after 1506876120 batches: 0.0113
trigger times: 9
Loss after 1507007220 batches: 0.0114
trigger times: 10
Loss after 1507138320 batches: 0.0113
trigger times: 11
Loss after 1507269420 batches: 0.0112
trigger times: 12
Loss after 1507400520 batches: 0.0111
trigger times: 13
Loss after 1507531620 batches: 0.0109
trigger times: 14
Loss after 1507662720 batches: 0.0109
trigger times: 15
Loss after 1507793820 batches: 0.0109
trigger times: 0
Loss after 1507924920 batches: 0.0108
trigger times: 1
Loss after 1508056020 batches: 0.0109
trigger times: 2
Loss after 1508187120 batches: 0.0109
trigger times: 3
Loss after 1508318220 batches: 0.0107
trigger times: 4
Loss after 1508449320 batches: 0.0105
trigger times: 5
Loss after 1508580420 batches: 0.0104
trigger times: 6
Loss after 1508711520 batches: 0.0103
trigger times: 7
Loss after 1508842620 batches: 0.0104
trigger times: 0
Loss after 1508973720 batches: 0.0105
trigger times: 1
Loss after 1509104820 batches: 0.0103
trigger times: 2
Loss after 1509235920 batches: 0.0105
trigger times: 3
Loss after 1509367020 batches: 0.0104
trigger times: 0
Loss after 1509498120 batches: 0.0101
trigger times: 1
Loss after 1509629220 batches: 0.0103
trigger times: 2
Loss after 1509760320 batches: 0.0100
trigger times: 3
Loss after 1509891420 batches: 0.0102
trigger times: 4
Loss after 1510022520 batches: 0.0102
trigger times: 5
Loss after 1510153620 batches: 0.0100
trigger times: 6
Loss after 1510284720 batches: 0.0098
trigger times: 7
Loss after 1510415820 batches: 0.0099
trigger times: 8
Loss after 1510546920 batches: 0.0098
trigger times: 9
Loss after 1510678020 batches: 0.0099
trigger times: 10
Loss after 1510809120 batches: 0.0098
trigger times: 0
Loss after 1510940220 batches: 0.0098
trigger times: 0
Loss after 1511071320 batches: 0.0095
trigger times: 1
Loss after 1511202420 batches: 0.0098
trigger times: 2
Loss after 1511333520 batches: 0.0096
trigger times: 3
Loss after 1511464620 batches: 0.0096
trigger times: 4
Loss after 1511595720 batches: 0.0095
trigger times: 5
Loss after 1511726820 batches: 0.0095
trigger times: 6
Loss after 1511857920 batches: 0.0095
trigger times: 7
Loss after 1511989020 batches: 0.0093
trigger times: 8
Loss after 1512120120 batches: 0.0094
trigger times: 9
Loss after 1512251220 batches: 0.0093
trigger times: 10
Loss after 1512382320 batches: 0.0095
trigger times: 11
Loss after 1512513420 batches: 0.0094
trigger times: 12
Loss after 1512644520 batches: 0.0092
trigger times: 13
Loss after 1512775620 batches: 0.0091
trigger times: 14
Loss after 1512906720 batches: 0.0092
trigger times: 15
Loss after 1513037820 batches: 0.0092
trigger times: 0
Loss after 1513168920 batches: 0.0092
trigger times: 1
Loss after 1513300020 batches: 0.0092
trigger times: 2
Loss after 1513431120 batches: 0.0092
trigger times: 3
Loss after 1513562220 batches: 0.0091
trigger times: 4
Loss after 1513693320 batches: 0.0089
trigger times: 5
Loss after 1513824420 batches: 0.0088
trigger times: 6
Loss after 1513955520 batches: 0.0089
trigger times: 7
Loss after 1514086620 batches: 0.0091
trigger times: 8
Loss after 1514217720 batches: 0.0089
trigger times: 9
Loss after 1514348820 batches: 0.0088
trigger times: 10
Loss after 1514479920 batches: 0.0090
trigger times: 11
Loss after 1514611020 batches: 0.0089
trigger times: 12
Loss after 1514742120 batches: 0.0088
trigger times: 13
Loss after 1514873220 batches: 0.0088
trigger times: 14
Loss after 1515004320 batches: 0.0088
trigger times: 15
Loss after 1515135420 batches: 0.0087
trigger times: 0
Loss after 1515266520 batches: 0.0085
trigger times: 1
Loss after 1515397620 batches: 0.0088
trigger times: 2
Loss after 1515528720 batches: 0.0086
trigger times: 3
Loss after 1515659820 batches: 0.0088
trigger times: 4
Loss after 1515790920 batches: 0.0086
trigger times: 5
Loss after 1515922020 batches: 0.0085
trigger times: 6
Loss after 1516053120 batches: 0.0086
trigger times: 7
Loss after 1516184220 batches: 0.0086
trigger times: 8
Loss after 1516315320 batches: 0.0084
trigger times: 9
Loss after 1516446420 batches: 0.0085
trigger times: 0
Loss after 1516577520 batches: 0.0086
trigger times: 1
Loss after 1516708620 batches: 0.0085
trigger times: 2
Loss after 1516839720 batches: 0.0084
trigger times: 3
Loss after 1516970820 batches: 0.0084
trigger times: 4
Loss after 1517101920 batches: 0.0083
trigger times: 5
Loss after 1517233020 batches: 0.0082
trigger times: 6
Loss after 1517364120 batches: 0.0082
trigger times: 7
Loss after 1517495220 batches: 0.0081
trigger times: 8
Loss after 1517626320 batches: 0.0081
trigger times: 9
Loss after 1517757420 batches: 0.0081
trigger times: 10
Loss after 1517888520 batches: 0.0081
trigger times: 11
Loss after 1518019620 batches: 0.0082
trigger times: 12
Loss after 1518150720 batches: 0.0082
trigger times: 13
Loss after 1518281820 batches: 0.0081
trigger times: 14
Loss after 1518412920 batches: 0.0081
trigger times: 15
Loss after 1518544020 batches: 0.0081
trigger times: 16
Loss after 1518675120 batches: 0.0080
trigger times: 17
Loss after 1518806220 batches: 0.0081
trigger times: 18
Loss after 1518937320 batches: 0.0079
trigger times: 19
Loss after 1519068420 batches: 0.0080
trigger times: 20
Early stopping!
Start to test process.
Loss after 1519199520 batches: 0.0078
Time to train on one home:  1045.1407780647278
trigger times: 0
Loss after 1519330620 batches: 0.1767
trigger times: 0
Loss after 1519461720 batches: 0.0555
trigger times: 1
Loss after 1519592820 batches: 0.0407
trigger times: 2
Loss after 1519723920 batches: 0.0353
trigger times: 3
Loss after 1519855020 batches: 0.0316
trigger times: 4
Loss after 1519986120 batches: 0.0293
trigger times: 5
Loss after 1520117220 batches: 0.0275
trigger times: 0
Loss after 1520248320 batches: 0.0265
trigger times: 1
Loss after 1520379420 batches: 0.0257
trigger times: 2
Loss after 1520510520 batches: 0.0246
trigger times: 3
Loss after 1520641620 batches: 0.0238
trigger times: 4
Loss after 1520772720 batches: 0.0233
trigger times: 5
Loss after 1520903820 batches: 0.0223
trigger times: 6
Loss after 1521034920 batches: 0.0215
trigger times: 7
Loss after 1521166020 batches: 0.0213
trigger times: 8
Loss after 1521297120 batches: 0.0206
trigger times: 9
Loss after 1521428220 batches: 0.0200
trigger times: 10
Loss after 1521559320 batches: 0.0204
trigger times: 11
Loss after 1521690420 batches: 0.0198
trigger times: 12
Loss after 1521821520 batches: 0.0193
trigger times: 13
Loss after 1521952620 batches: 0.0194
trigger times: 14
Loss after 1522083720 batches: 0.0189
trigger times: 15
Loss after 1522214820 batches: 0.0185
trigger times: 16
Loss after 1522345920 batches: 0.0183
trigger times: 17
Loss after 1522477020 batches: 0.0181
trigger times: 18
Loss after 1522608120 batches: 0.0181
trigger times: 19
Loss after 1522739220 batches: 0.0179
trigger times: 20
Early stopping!
Start to test process.
Loss after 1522870320 batches: 0.0174
Time to train on one home:  216.41313409805298
trigger times: 0
Loss after 1523001420 batches: 0.2883
trigger times: 0
Loss after 1523132520 batches: 0.0898
trigger times: 0
Loss after 1523263620 batches: 0.0612
trigger times: 0
Loss after 1523394720 batches: 0.0512
trigger times: 1
Loss after 1523525820 batches: 0.0462
trigger times: 2
Loss after 1523656920 batches: 0.0425
trigger times: 3
Loss after 1523788020 batches: 0.0407
trigger times: 0
Loss after 1523919120 batches: 0.0392
trigger times: 1
Loss after 1524050220 batches: 0.0371
trigger times: 2
Loss after 1524181320 batches: 0.0358
trigger times: 0
Loss after 1524312420 batches: 0.0351
trigger times: 1
Loss after 1524443520 batches: 0.0334
trigger times: 2
Loss after 1524574620 batches: 0.0329
trigger times: 0
Loss after 1524705720 batches: 0.0308
trigger times: 1
Loss after 1524836820 batches: 0.0299
trigger times: 2
Loss after 1524967920 batches: 0.0297
trigger times: 0
Loss after 1525099020 batches: 0.0284
trigger times: 1
Loss after 1525230120 batches: 0.0284
trigger times: 0
Loss after 1525361220 batches: 0.0292
trigger times: 0
Loss after 1525492320 batches: 0.0288
trigger times: 1
Loss after 1525623420 batches: 0.0273
trigger times: 2
Loss after 1525754520 batches: 0.0271
trigger times: 3
Loss after 1525885620 batches: 0.0265
trigger times: 4
Loss after 1526016720 batches: 0.0266
trigger times: 5
Loss after 1526147820 batches: 0.0264
trigger times: 0
Loss after 1526278920 batches: 0.0266
trigger times: 1
Loss after 1526410020 batches: 0.0254
trigger times: 2
Loss after 1526541120 batches: 0.0252
trigger times: 3
Loss after 1526672220 batches: 0.0251
trigger times: 4
Loss after 1526803320 batches: 0.0255
trigger times: 5
Loss after 1526934420 batches: 0.0245
trigger times: 6
Loss after 1527065520 batches: 0.0251
trigger times: 7
Loss after 1527196620 batches: 0.0251
trigger times: 0
Loss after 1527327720 batches: 0.0246
trigger times: 0
Loss after 1527458820 batches: 0.0248
trigger times: 1
Loss after 1527589920 batches: 0.0233
trigger times: 2
Loss after 1527721020 batches: 0.0234
trigger times: 3
Loss after 1527852120 batches: 0.0229
trigger times: 4
Loss after 1527983220 batches: 0.0241
trigger times: 5
Loss after 1528114320 batches: 0.0232
trigger times: 6
Loss after 1528245420 batches: 0.0230
trigger times: 7
Loss after 1528376520 batches: 0.0225
trigger times: 0
Loss after 1528507620 batches: 0.0222
trigger times: 0
Loss after 1528638720 batches: 0.0225
trigger times: 1
Loss after 1528769820 batches: 0.0227
trigger times: 2
Loss after 1528900920 batches: 0.0222
trigger times: 3
Loss after 1529032020 batches: 0.0229
trigger times: 4
Loss after 1529163120 batches: 0.0218
trigger times: 5
Loss after 1529294220 batches: 0.0223
trigger times: 6
Loss after 1529425320 batches: 0.0214
trigger times: 7
Loss after 1529556420 batches: 0.0212
trigger times: 8
Loss after 1529687520 batches: 0.0216
trigger times: 9
Loss after 1529818620 batches: 0.0212
trigger times: 10
Loss after 1529949720 batches: 0.0208
trigger times: 0
Loss after 1530080820 batches: 0.0213
trigger times: 0
Loss after 1530211920 batches: 0.0213
trigger times: 1
Loss after 1530343020 batches: 0.0208
trigger times: 2
Loss after 1530474120 batches: 0.0207
trigger times: 3
Loss after 1530605220 batches: 0.0208
trigger times: 4
Loss after 1530736320 batches: 0.0205
trigger times: 5
Loss after 1530867420 batches: 0.0208
trigger times: 6
Loss after 1530998520 batches: 0.0201
trigger times: 7
Loss after 1531129620 batches: 0.0203
trigger times: 0
Loss after 1531260720 batches: 0.0200
trigger times: 1
Loss after 1531391820 batches: 0.0198
trigger times: 2
Loss after 1531522920 batches: 0.0195
trigger times: 0
Loss after 1531654020 batches: 0.0193
trigger times: 1
Loss after 1531785120 batches: 0.0196
trigger times: 2
Loss after 1531916220 batches: 0.0197
trigger times: 3
Loss after 1532047320 batches: 0.0194
trigger times: 4
Loss after 1532178420 batches: 0.0194
trigger times: 5
Loss after 1532309520 batches: 0.0190
trigger times: 6
Loss after 1532440620 batches: 0.0189
trigger times: 7
Loss after 1532571720 batches: 0.0190
trigger times: 0
Loss after 1532702820 batches: 0.0199
trigger times: 0
Loss after 1532833920 batches: 0.0199
trigger times: 1
Loss after 1532965020 batches: 0.0191
trigger times: 2
Loss after 1533096120 batches: 0.0187
trigger times: 3
Loss after 1533227220 batches: 0.0190
trigger times: 4
Loss after 1533358320 batches: 0.0193
trigger times: 5
Loss after 1533489420 batches: 0.0184
trigger times: 6
Loss after 1533620520 batches: 0.0186
trigger times: 7
Loss after 1533751620 batches: 0.0187
trigger times: 8
Loss after 1533882720 batches: 0.0188
trigger times: 9
Loss after 1534013820 batches: 0.0186
trigger times: 10
Loss after 1534144920 batches: 0.0191
trigger times: 11
Loss after 1534276020 batches: 0.0188
trigger times: 12
Loss after 1534407120 batches: 0.0192
trigger times: 13
Loss after 1534538220 batches: 0.0185
trigger times: 14
Loss after 1534669320 batches: 0.0186
trigger times: 15
Loss after 1534800420 batches: 0.0184
trigger times: 16
Loss after 1534931520 batches: 0.0175
trigger times: 17
Loss after 1535062620 batches: 0.0183
trigger times: 18
Loss after 1535193720 batches: 0.0183
trigger times: 19
Loss after 1535324820 batches: 0.0185
trigger times: 20
Early stopping!
Start to test process.
Loss after 1535455920 batches: 0.0180
Time to train on one home:  710.8722138404846
trigger times: 0
Loss after 1535587020 batches: 0.3840
trigger times: 1
Loss after 1535718120 batches: 0.1483
trigger times: 2
Loss after 1535849220 batches: 0.1001
trigger times: 3
Loss after 1535980320 batches: 0.0831
trigger times: 4
Loss after 1536111420 batches: 0.0729
trigger times: 5
Loss after 1536242520 batches: 0.0675
trigger times: 6
Loss after 1536373620 batches: 0.0628
trigger times: 7
Loss after 1536504720 batches: 0.0588
trigger times: 8
Loss after 1536635820 batches: 0.0570
trigger times: 9
Loss after 1536766920 batches: 0.0542
trigger times: 10
Loss after 1536898020 batches: 0.0527
trigger times: 11
Loss after 1537029120 batches: 0.0508
trigger times: 12
Loss after 1537160220 batches: 0.0496
trigger times: 13
Loss after 1537291320 batches: 0.0488
trigger times: 14
Loss after 1537422420 batches: 0.0484
trigger times: 15
Loss after 1537553520 batches: 0.0464
trigger times: 16
Loss after 1537684620 batches: 0.0447
trigger times: 17
Loss after 1537815720 batches: 0.0438
trigger times: 18
Loss after 1537946820 batches: 0.0442
trigger times: 19
Loss after 1538077920 batches: 0.0428
trigger times: 20
Early stopping!
Start to test process.
Loss after 1538209020 batches: 0.0423
Time to train on one home:  164.3159008026123
trigger times: 0
Loss after 1538340120 batches: 0.2706
trigger times: 0
Loss after 1538471220 batches: 0.0841
trigger times: 1
Loss after 1538602320 batches: 0.0601
trigger times: 2
Loss after 1538733420 batches: 0.0505
trigger times: 0
Loss after 1538864520 batches: 0.0463
trigger times: 1
Loss after 1538995620 batches: 0.0423
trigger times: 2
Loss after 1539126720 batches: 0.0390
trigger times: 3
Loss after 1539257820 batches: 0.0380
trigger times: 0
Loss after 1539388920 batches: 0.0363
trigger times: 1
Loss after 1539520020 batches: 0.0346
trigger times: 2
Loss after 1539651120 batches: 0.0333
trigger times: 3
Loss after 1539782220 batches: 0.0323
trigger times: 4
Loss after 1539913320 batches: 0.0319
trigger times: 5
Loss after 1540044420 batches: 0.0309
trigger times: 6
Loss after 1540175520 batches: 0.0308
trigger times: 7
Loss after 1540306620 batches: 0.0307
trigger times: 8
Loss after 1540437720 batches: 0.0296
trigger times: 9
Loss after 1540568820 batches: 0.0289
trigger times: 10
Loss after 1540699920 batches: 0.0276
trigger times: 11
Loss after 1540831020 batches: 0.0282
trigger times: 12
Loss after 1540962120 batches: 0.0271
trigger times: 13
Loss after 1541093220 batches: 0.0268
trigger times: 0
Loss after 1541224320 batches: 0.0261
trigger times: 1
Loss after 1541355420 batches: 0.0265
trigger times: 2
Loss after 1541486520 batches: 0.0266
trigger times: 3
Loss after 1541617620 batches: 0.0264
trigger times: 4
Loss after 1541748720 batches: 0.0260
trigger times: 5
Loss after 1541879820 batches: 0.0252
trigger times: 6
Loss after 1542010920 batches: 0.0248
trigger times: 7
Loss after 1542142020 batches: 0.0248
trigger times: 8
Loss after 1542273120 batches: 0.0246
trigger times: 9
Loss after 1542404220 batches: 0.0252
trigger times: 10
Loss after 1542535320 batches: 0.0235
trigger times: 11
Loss after 1542666420 batches: 0.0237
trigger times: 12
Loss after 1542797520 batches: 0.0241
trigger times: 13
Loss after 1542928620 batches: 0.0241
trigger times: 14
Loss after 1543059720 batches: 0.0239
trigger times: 15
Loss after 1543190820 batches: 0.0240
trigger times: 16
Loss after 1543321920 batches: 0.0233
trigger times: 17
Loss after 1543453020 batches: 0.0226
trigger times: 18
Loss after 1543584120 batches: 0.0227
trigger times: 19
Loss after 1543715220 batches: 0.0232
trigger times: 20
Early stopping!
Start to test process.
Loss after 1543846320 batches: 0.0221
Time to train on one home:  324.6953866481781
trigger times: 0
Loss after 1543977420 batches: 0.4396
trigger times: 0
Loss after 1544108520 batches: 0.1529
trigger times: 1
Loss after 1544239620 batches: 0.0929
trigger times: 2
Loss after 1544370720 batches: 0.0752
trigger times: 3
Loss after 1544501820 batches: 0.0646
trigger times: 4
Loss after 1544632920 batches: 0.0581
trigger times: 5
Loss after 1544764020 batches: 0.0547
trigger times: 6
Loss after 1544895120 batches: 0.0511
trigger times: 7
Loss after 1545026220 batches: 0.0487
trigger times: 8
Loss after 1545157320 batches: 0.0464
trigger times: 9
Loss after 1545288420 batches: 0.0446
trigger times: 10
Loss after 1545419520 batches: 0.0426
trigger times: 11
Loss after 1545550620 batches: 0.0418
trigger times: 12
Loss after 1545681720 batches: 0.0407
trigger times: 13
Loss after 1545812820 batches: 0.0391
trigger times: 14
Loss after 1545943920 batches: 0.0384
trigger times: 15
Loss after 1546075020 batches: 0.0373
trigger times: 16
Loss after 1546206120 batches: 0.0370
trigger times: 17
Loss after 1546337220 batches: 0.0359
trigger times: 18
Loss after 1546468320 batches: 0.0354
trigger times: 19
Loss after 1546599420 batches: 0.0347
trigger times: 20
Early stopping!
Start to test process.
Loss after 1546730520 batches: 0.0340
Time to train on one home:  170.56252670288086
trigger times: 0
Loss after 1546861620 batches: 0.4714
trigger times: 0
Loss after 1546992720 batches: 0.1536
trigger times: 0
Loss after 1547123820 batches: 0.1089
trigger times: 0
Loss after 1547254920 batches: 0.0895
trigger times: 1
Loss after 1547386020 batches: 0.0801
trigger times: 0
Loss after 1547517120 batches: 0.0743
trigger times: 1
Loss after 1547648220 batches: 0.0699
trigger times: 2
Loss after 1547779320 batches: 0.0657
trigger times: 3
Loss after 1547910420 batches: 0.0629
trigger times: 4
Loss after 1548041520 batches: 0.0602
trigger times: 5
Loss after 1548172620 batches: 0.0584
trigger times: 0
Loss after 1548303720 batches: 0.0567
trigger times: 0
Loss after 1548434820 batches: 0.0553
trigger times: 1
Loss after 1548565920 batches: 0.0533
trigger times: 2
Loss after 1548697020 batches: 0.0516
trigger times: 3
Loss after 1548828120 batches: 0.0505
trigger times: 0
Loss after 1548959220 batches: 0.0503
trigger times: 1
Loss after 1549090320 batches: 0.0491
trigger times: 2
Loss after 1549221420 batches: 0.0471
trigger times: 3
Loss after 1549352520 batches: 0.0471
trigger times: 4
Loss after 1549483620 batches: 0.0462
trigger times: 5
Loss after 1549614720 batches: 0.0461
trigger times: 6
Loss after 1549745820 batches: 0.0450
trigger times: 7
Loss after 1549876920 batches: 0.0449
trigger times: 8
Loss after 1550008020 batches: 0.0447
trigger times: 9
Loss after 1550139120 batches: 0.0439
trigger times: 10
Loss after 1550270220 batches: 0.0424
trigger times: 11
Loss after 1550401320 batches: 0.0424
trigger times: 0
Loss after 1550532420 batches: 0.0421
trigger times: 0
Loss after 1550663520 batches: 0.0416
trigger times: 1
Loss after 1550794620 batches: 0.0413
trigger times: 2
Loss after 1550925720 batches: 0.0407
trigger times: 0
Loss after 1551056820 batches: 0.0402
trigger times: 1
Loss after 1551187920 batches: 0.0398
trigger times: 2
Loss after 1551319020 batches: 0.0390
trigger times: 0
Loss after 1551450120 batches: 0.0393
trigger times: 1
Loss after 1551581220 batches: 0.0389
trigger times: 2
Loss after 1551712320 batches: 0.0391
trigger times: 0
Loss after 1551843420 batches: 0.0380
trigger times: 1
Loss after 1551974520 batches: 0.0382
trigger times: 2
Loss after 1552105620 batches: 0.0374
trigger times: 0
Loss after 1552236720 batches: 0.0370
trigger times: 1
Loss after 1552367820 batches: 0.0377
trigger times: 2
Loss after 1552498920 batches: 0.0373
trigger times: 3
Loss after 1552630020 batches: 0.0365
trigger times: 4
Loss after 1552761120 batches: 0.0361
trigger times: 5
Loss after 1552892220 batches: 0.0361
trigger times: 0
Loss after 1553023320 batches: 0.0360
trigger times: 0
Loss after 1553154420 batches: 0.0356
trigger times: 1
Loss after 1553285520 batches: 0.0354
trigger times: 2
Loss after 1553416620 batches: 0.0354
trigger times: 3
Loss after 1553547720 batches: 0.0354
trigger times: 4
Loss after 1553678820 batches: 0.0348
trigger times: 5
Loss after 1553809920 batches: 0.0346
trigger times: 6
Loss after 1553941020 batches: 0.0344
trigger times: 0
Loss after 1554072120 batches: 0.0339
trigger times: 0
Loss after 1554203220 batches: 0.0334
trigger times: 1
Loss after 1554334320 batches: 0.0341
trigger times: 2
Loss after 1554465420 batches: 0.0335
trigger times: 3
Loss after 1554596520 batches: 0.0336
trigger times: 4
Loss after 1554727620 batches: 0.0332
trigger times: 5
Loss after 1554858720 batches: 0.0333
trigger times: 6
Loss after 1554989820 batches: 0.0331
trigger times: 0
Loss after 1555120920 batches: 0.0333
trigger times: 1
Loss after 1555252020 batches: 0.0331
trigger times: 2
Loss after 1555383120 batches: 0.0330
trigger times: 3
Loss after 1555514220 batches: 0.0324
trigger times: 4
Loss after 1555645320 batches: 0.0319
trigger times: 5
Loss after 1555776420 batches: 0.0316
trigger times: 6
Loss after 1555907520 batches: 0.0320
trigger times: 7
Loss after 1556038620 batches: 0.0319
trigger times: 0
Loss after 1556169720 batches: 0.0313
trigger times: 1
Loss after 1556300820 batches: 0.0315
trigger times: 2
Loss after 1556431920 batches: 0.0312
trigger times: 3
Loss after 1556563020 batches: 0.0310
trigger times: 4
Loss after 1556694120 batches: 0.0310
trigger times: 0
Loss after 1556825220 batches: 0.0309
trigger times: 0
Loss after 1556956320 batches: 0.0313
trigger times: 1
Loss after 1557087420 batches: 0.0304
trigger times: 2
Loss after 1557218520 batches: 0.0306
trigger times: 3
Loss after 1557349620 batches: 0.0305
trigger times: 4
Loss after 1557480720 batches: 0.0306
trigger times: 0
Loss after 1557611820 batches: 0.0298
trigger times: 1
Loss after 1557742920 batches: 0.0302
trigger times: 2
Loss after 1557874020 batches: 0.0297
trigger times: 3
Loss after 1558005120 batches: 0.0297
trigger times: 4
Loss after 1558136220 batches: 0.0300
trigger times: 5
Loss after 1558267320 batches: 0.0299
trigger times: 6
Loss after 1558398420 batches: 0.0300
trigger times: 0
Loss after 1558529520 batches: 0.0294
trigger times: 1
Loss after 1558660620 batches: 0.0294
trigger times: 2
Loss after 1558791720 batches: 0.0295
trigger times: 0
Loss after 1558922820 batches: 0.0285
trigger times: 1
Loss after 1559053920 batches: 0.0307
trigger times: 2
Loss after 1559185020 batches: 0.0287
trigger times: 3
Loss after 1559316120 batches: 0.0292
trigger times: 4
Loss after 1559447220 batches: 0.0286
trigger times: 5
Loss after 1559578320 batches: 0.0292
trigger times: 6
Loss after 1559709420 batches: 0.0287
trigger times: 7
Loss after 1559840520 batches: 0.0284
trigger times: 8
Loss after 1559971620 batches: 0.0283
trigger times: 9
Loss after 1560102720 batches: 0.0280
trigger times: 10
Loss after 1560233820 batches: 0.0282
trigger times: 11
Loss after 1560364920 batches: 0.0288
trigger times: 12
Loss after 1560496020 batches: 0.0281
trigger times: 13
Loss after 1560627120 batches: 0.0282
trigger times: 14
Loss after 1560758220 batches: 0.0277
trigger times: 15
Loss after 1560889320 batches: 0.0275
trigger times: 16
Loss after 1561020420 batches: 0.0276
trigger times: 17
Loss after 1561151520 batches: 0.0273
trigger times: 18
Loss after 1561282620 batches: 0.0274
trigger times: 19
Loss after 1561413720 batches: 0.0273
trigger times: 20
Early stopping!
Start to test process.
Loss after 1561544820 batches: 0.0272
Time to train on one home:  833.7208154201508
trigger times: 0
Loss after 1561638780 batches: 0.4926
trigger times: 0
Loss after 1561732740 batches: 0.1708
trigger times: 0
Loss after 1561826700 batches: 0.1059
trigger times: 0
Loss after 1561920660 batches: 0.0848
trigger times: 0
Loss after 1562014620 batches: 0.0744
trigger times: 0
Loss after 1562108580 batches: 0.0673
trigger times: 1
Loss after 1562202540 batches: 0.0617
trigger times: 2
Loss after 1562296500 batches: 0.0579
trigger times: 0
Loss after 1562390460 batches: 0.0558
trigger times: 1
Loss after 1562484420 batches: 0.0530
trigger times: 2
Loss after 1562578380 batches: 0.0511
trigger times: 0
Loss after 1562672340 batches: 0.0499
trigger times: 1
Loss after 1562766300 batches: 0.0475
trigger times: 2
Loss after 1562860260 batches: 0.0458
trigger times: 3
Loss after 1562954220 batches: 0.0455
trigger times: 4
Loss after 1563048180 batches: 0.0443
trigger times: 0
Loss after 1563142140 batches: 0.0439
trigger times: 1
Loss after 1563236100 batches: 0.0419
trigger times: 2
Loss after 1563330060 batches: 0.0424
trigger times: 0
Loss after 1563424020 batches: 0.0407
trigger times: 1
Loss after 1563517980 batches: 0.0402
trigger times: 0
Loss after 1563611940 batches: 0.0392
trigger times: 1
Loss after 1563705900 batches: 0.0397
trigger times: 2
Loss after 1563799860 batches: 0.0397
trigger times: 0
Loss after 1563893820 batches: 0.0382
trigger times: 1
Loss after 1563987780 batches: 0.0366
trigger times: 2
Loss after 1564081740 batches: 0.0370
trigger times: 3
Loss after 1564175700 batches: 0.0364
trigger times: 0
Loss after 1564269660 batches: 0.0365
trigger times: 1
Loss after 1564363620 batches: 0.0362
trigger times: 0
Loss after 1564457580 batches: 0.0356
trigger times: 1
Loss after 1564551540 batches: 0.0349
trigger times: 2
Loss after 1564645500 batches: 0.0344
trigger times: 3
Loss after 1564739460 batches: 0.0336
trigger times: 0
Loss after 1564833420 batches: 0.0341
trigger times: 1
Loss after 1564927380 batches: 0.0343
trigger times: 2
Loss after 1565021340 batches: 0.0332
trigger times: 3
Loss after 1565115300 batches: 0.0330
trigger times: 4
Loss after 1565209260 batches: 0.0330
trigger times: 0
Loss after 1565303220 batches: 0.0327
trigger times: 1
Loss after 1565397180 batches: 0.0321
trigger times: 2
Loss after 1565491140 batches: 0.0325
trigger times: 3
Loss after 1565585100 batches: 0.0318
trigger times: 4
Loss after 1565679060 batches: 0.0316
trigger times: 5
Loss after 1565773020 batches: 0.0309
trigger times: 6
Loss after 1565866980 batches: 0.0310
trigger times: 7
Loss after 1565960940 batches: 0.0318
trigger times: 8
Loss after 1566054900 batches: 0.0311
trigger times: 9
Loss after 1566148860 batches: 0.0309
trigger times: 10
Loss after 1566242820 batches: 0.0304
trigger times: 11
Loss after 1566336780 batches: 0.0299
trigger times: 12
Loss after 1566430740 batches: 0.0304
trigger times: 13
Loss after 1566524700 batches: 0.0297
trigger times: 14
Loss after 1566618660 batches: 0.0297
trigger times: 0
Loss after 1566712620 batches: 0.0296
trigger times: 1
Loss after 1566806580 batches: 0.0297
trigger times: 2
Loss after 1566900540 batches: 0.0292
trigger times: 3
Loss after 1566994500 batches: 0.0291
trigger times: 4
Loss after 1567088460 batches: 0.0285
trigger times: 5
Loss after 1567182420 batches: 0.0292
trigger times: 6
Loss after 1567276380 batches: 0.0288
trigger times: 7
Loss after 1567370340 batches: 0.0285
trigger times: 8
Loss after 1567464300 batches: 0.0287
trigger times: 9
Loss after 1567558260 batches: 0.0284
trigger times: 10
Loss after 1567652220 batches: 0.0282
trigger times: 11
Loss after 1567746180 batches: 0.0280
trigger times: 12
Loss after 1567840140 batches: 0.0282
trigger times: 13
Loss after 1567934100 batches: 0.0282
trigger times: 14
Loss after 1568028060 batches: 0.0274
trigger times: 15
Loss after 1568122020 batches: 0.0276
trigger times: 16
Loss after 1568215980 batches: 0.0275
trigger times: 17
Loss after 1568309940 batches: 0.0267
trigger times: 18
Loss after 1568403900 batches: 0.0268
trigger times: 19
Loss after 1568497860 batches: 0.0273
trigger times: 20
Early stopping!
Start to test process.
Loss after 1568591820 batches: 0.0268
Time to train on one home:  419.16746044158936
trigger times: 0
Loss after 1568722920 batches: 0.0665
trigger times: 1
Loss after 1568854020 batches: 0.0142
trigger times: 2
Loss after 1568985120 batches: 0.0103
trigger times: 3
Loss after 1569116220 batches: 0.0085
trigger times: 4
Loss after 1569247320 batches: 0.0075
trigger times: 5
Loss after 1569378420 batches: 0.0068
trigger times: 6
Loss after 1569509520 batches: 0.0064
trigger times: 7
Loss after 1569640620 batches: 0.0060
trigger times: 8
Loss after 1569771720 batches: 0.0057
trigger times: 9
Loss after 1569902820 batches: 0.0052
trigger times: 10
Loss after 1570033920 batches: 0.0051
trigger times: 11
Loss after 1570165020 batches: 0.0049
trigger times: 12
Loss after 1570296120 batches: 0.0049
trigger times: 13
Loss after 1570427220 batches: 0.0047
trigger times: 14
Loss after 1570558320 batches: 0.0046
trigger times: 15
Loss after 1570689420 batches: 0.0044
trigger times: 16
Loss after 1570820520 batches: 0.0042
trigger times: 17
Loss after 1570951620 batches: 0.0041
trigger times: 18
Loss after 1571082720 batches: 0.0040
trigger times: 19
Loss after 1571213820 batches: 0.0039
trigger times: 20
Early stopping!
Start to test process.
Loss after 1571344920 batches: 0.0038
Time to train on one home:  163.83452248573303
trigger times: 0
Loss after 1571476020 batches: 0.1527
trigger times: 1
Loss after 1571607120 batches: 0.0471
trigger times: 2
Loss after 1571738220 batches: 0.0330
trigger times: 3
Loss after 1571869320 batches: 0.0281
trigger times: 4
Loss after 1572000420 batches: 0.0250
trigger times: 0
Loss after 1572131520 batches: 0.0232
trigger times: 1
Loss after 1572262620 batches: 0.0219
trigger times: 2
Loss after 1572393720 batches: 0.0205
trigger times: 3
Loss after 1572524820 batches: 0.0198
trigger times: 4
Loss after 1572655920 batches: 0.0190
trigger times: 5
Loss after 1572787020 batches: 0.0185
trigger times: 6
Loss after 1572918120 batches: 0.0178
trigger times: 7
Loss after 1573049220 batches: 0.0175
trigger times: 8
Loss after 1573180320 batches: 0.0169
trigger times: 9
Loss after 1573311420 batches: 0.0166
trigger times: 10
Loss after 1573442520 batches: 0.0164
trigger times: 11
Loss after 1573573620 batches: 0.0162
trigger times: 12
Loss after 1573704720 batches: 0.0159
trigger times: 13
Loss after 1573835820 batches: 0.0153
trigger times: 14
Loss after 1573966920 batches: 0.0152
trigger times: 15
Loss after 1574098020 batches: 0.0150
trigger times: 16
Loss after 1574229120 batches: 0.0148
trigger times: 17
Loss after 1574360220 batches: 0.0144
trigger times: 18
Loss after 1574491320 batches: 0.0140
trigger times: 19
Loss after 1574622420 batches: 0.0141
trigger times: 20
Early stopping!
Start to test process.
Loss after 1574753520 batches: 0.0140
Time to train on one home:  200.93903517723083
trigger times: 0
Loss after 1574884620 batches: 0.3083
trigger times: 0
Loss after 1575015720 batches: 0.1122
trigger times: 0
Loss after 1575146820 batches: 0.0871
trigger times: 0
Loss after 1575277920 batches: 0.0765
trigger times: 0
Loss after 1575409020 batches: 0.0711
trigger times: 1
Loss after 1575540120 batches: 0.0661
trigger times: 0
Loss after 1575671220 batches: 0.0635
trigger times: 1
Loss after 1575802320 batches: 0.0617
trigger times: 2
Loss after 1575933420 batches: 0.0599
trigger times: 3
Loss after 1576064520 batches: 0.0578
trigger times: 0
Loss after 1576195620 batches: 0.0570
trigger times: 1
Loss after 1576326720 batches: 0.0558
trigger times: 2
Loss after 1576457820 batches: 0.0549
trigger times: 3
Loss after 1576588920 batches: 0.0539
trigger times: 4
Loss after 1576720020 batches: 0.0531
trigger times: 5
Loss after 1576851120 batches: 0.0525
trigger times: 6
Loss after 1576982220 batches: 0.0517
trigger times: 0
Loss after 1577113320 batches: 0.0508
trigger times: 0
Loss after 1577244420 batches: 0.0505
trigger times: 1
Loss after 1577375520 batches: 0.0498
trigger times: 0
Loss after 1577506620 batches: 0.0497
trigger times: 1
Loss after 1577637720 batches: 0.0493
trigger times: 2
Loss after 1577768820 batches: 0.0484
trigger times: 3
Loss after 1577899920 batches: 0.0481
trigger times: 4
Loss after 1578031020 batches: 0.0479
trigger times: 5
Loss after 1578162120 batches: 0.0474
trigger times: 6
Loss after 1578293220 batches: 0.0468
trigger times: 0
Loss after 1578424320 batches: 0.0470
trigger times: 1
Loss after 1578555420 batches: 0.0469
trigger times: 2
Loss after 1578686520 batches: 0.0459
trigger times: 3
Loss after 1578817620 batches: 0.0456
trigger times: 4
Loss after 1578948720 batches: 0.0456
trigger times: 5
Loss after 1579079820 batches: 0.0454
trigger times: 6
Loss after 1579210920 batches: 0.0461
trigger times: 7
Loss after 1579342020 batches: 0.0448
trigger times: 8
Loss after 1579473120 batches: 0.0451
trigger times: 9
Loss after 1579604220 batches: 0.0446
trigger times: 10
Loss after 1579735320 batches: 0.0444
trigger times: 11
Loss after 1579866420 batches: 0.0440
trigger times: 12
Loss after 1579997520 batches: 0.0443
trigger times: 13
Loss after 1580128620 batches: 0.0440
trigger times: 14
Loss after 1580259720 batches: 0.0437
trigger times: 15
Loss after 1580390820 batches: 0.0439
trigger times: 16
Loss after 1580521920 batches: 0.0432
trigger times: 17
Loss after 1580653020 batches: 0.0431
trigger times: 18
Loss after 1580784120 batches: 0.0431
trigger times: 19
Loss after 1580915220 batches: 0.0431
trigger times: 20
Early stopping!
Start to test process.
Loss after 1581046320 batches: 0.0428
Time to train on one home:  360.11707854270935
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642, 0.03501720033327798, 0.03263172240869951, 0.030110256078310443, 0.02544188748067248, 0.02612378438006769, 0.024274968770150986]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217], [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944], [0.6491197148958842, 0.29803996281550993, 0.44160060986867417, 1.203530351478871, 0.5750418631238572, 28.433835927602637, 1775.19], [0.6497863001293607, 0.29730357979276123, 0.44701117944787117, 1.2068712720184533, 0.5756451041104418, 28.512766372815683, 1777.0521], [0.6468338668346405, 0.30051325220168756, 0.4517286880074036, 1.1964129191375579, 0.5730157578453048, 28.26568403748201, 1768.9353], [0.6481483081976572, 0.29911867824708416, 0.44591549869242414, 1.1883481958771474, 0.5741581852808264, 28.07515205986611, 1772.462], [0.6422967116038004, 0.30545179637907305, 0.45038470512861073, 1.1723096038740957, 0.568970129184914, 27.696234575181023, 1756.4463]]
Round_13_results:  [0.6422967116038004, 0.30545179637907305, 0.45038470512861073, 1.1723096038740957, 0.568970129184914, 27.696234575181023, 1756.4463]
trigger times: 0
Loss after 1581148920 batches: 0.4041
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 12587 < 12588; dropping {'Training_Loss': 0.40406023817402975, 'Validation_Loss': 0.9686378969086541, 'Training_R2': 0.5915695552719237, 'Validation_R2': 0.09629105991262954, 'Training_F1': 0.7562124473583197, 'Validation_F1': 0.43782036542946884, 'Training_NEP': 0.4885526918646878, 'Validation_NEP': 1.1316565098517923, 'Training_NDE': 0.2751147733902756, 'Validation_NDE': 0.7196661616735144, 'Training_MAE': 32.064608355293494, 'Validation_MAE': 31.03471645539807, 'Training_MSE': 3630.6123, 'Validation_MSE': 2657.7073}.
trigger times: 1
Loss after 1581251520 batches: 0.1620
trigger times: 2
Loss after 1581354120 batches: 0.1013
trigger times: 3
Loss after 1581456720 batches: 0.0764
trigger times: 4
Loss after 1581559320 batches: 0.0646
trigger times: 5
Loss after 1581661920 batches: 0.0579
trigger times: 6
Loss after 1581764520 batches: 0.0516
trigger times: 7
Loss after 1581867120 batches: 0.0490
trigger times: 8
Loss after 1581969720 batches: 0.0472
trigger times: 9
Loss after 1582072320 batches: 0.0438
trigger times: 10
Loss after 1582174920 batches: 0.0440
trigger times: 11
Loss after 1582277520 batches: 0.0416
trigger times: 12
Loss after 1582380120 batches: 0.0418
trigger times: 13
Loss after 1582482720 batches: 0.0362
trigger times: 14
Loss after 1582585320 batches: 0.0376
trigger times: 15
Loss after 1582687920 batches: 0.0351
trigger times: 16
Loss after 1582790520 batches: 0.0346
trigger times: 17
Loss after 1582893120 batches: 0.0338
trigger times: 18
Loss after 1582995720 batches: 0.0329
trigger times: 19
Loss after 1583098320 batches: 0.0323
trigger times: 20
Early stopping!
Start to test process.
Loss after 1583200920 batches: 0.0341
Time to train on one home:  136.87972354888916
trigger times: 0
Loss after 1583332020 batches: 0.2299
trigger times: 1
Loss after 1583463120 batches: 0.0860
trigger times: 2
Loss after 1583594220 batches: 0.0602
trigger times: 3
Loss after 1583725320 batches: 0.0504
trigger times: 4
Loss after 1583856420 batches: 0.0454
trigger times: 5
Loss after 1583987520 batches: 0.0415
trigger times: 6
Loss after 1584118620 batches: 0.0385
trigger times: 7
Loss after 1584249720 batches: 0.0362
trigger times: 8
Loss after 1584380820 batches: 0.0351
trigger times: 9
Loss after 1584511920 batches: 0.0332
trigger times: 10
Loss after 1584643020 batches: 0.0326
trigger times: 11
Loss after 1584774120 batches: 0.0315
trigger times: 12
Loss after 1584905220 batches: 0.0306
trigger times: 13
Loss after 1585036320 batches: 0.0298
trigger times: 14
Loss after 1585167420 batches: 0.0291
trigger times: 15
Loss after 1585298520 batches: 0.0287
trigger times: 16
Loss after 1585429620 batches: 0.0281
trigger times: 17
Loss after 1585560720 batches: 0.0275
trigger times: 18
Loss after 1585691820 batches: 0.0275
trigger times: 19
Loss after 1585822920 batches: 0.0269
trigger times: 20
Early stopping!
Start to test process.
Loss after 1585954020 batches: 0.0262
Time to train on one home:  164.84048771858215
trigger times: 0
Loss after 1586085120 batches: 0.4433
trigger times: 1
Loss after 1586216220 batches: 0.1563
trigger times: 2
Loss after 1586347320 batches: 0.1042
trigger times: 3
Loss after 1586478420 batches: 0.0873
trigger times: 4
Loss after 1586609520 batches: 0.0770
trigger times: 5
Loss after 1586740620 batches: 0.0706
trigger times: 6
Loss after 1586871720 batches: 0.0656
trigger times: 7
Loss after 1587002820 batches: 0.0621
trigger times: 8
Loss after 1587133920 batches: 0.0582
trigger times: 9
Loss after 1587265020 batches: 0.0562
trigger times: 10
Loss after 1587396120 batches: 0.0536
trigger times: 11
Loss after 1587527220 batches: 0.0523
trigger times: 12
Loss after 1587658320 batches: 0.0507
trigger times: 13
Loss after 1587789420 batches: 0.0493
trigger times: 14
Loss after 1587920520 batches: 0.0475
trigger times: 15
Loss after 1588051620 batches: 0.0465
trigger times: 16
Loss after 1588182720 batches: 0.0466
trigger times: 17
Loss after 1588313820 batches: 0.0448
trigger times: 18
Loss after 1588444920 batches: 0.0438
trigger times: 19
Loss after 1588576020 batches: 0.0429
trigger times: 20
Early stopping!
Start to test process.
Loss after 1588707120 batches: 0.0428
Time to train on one home:  164.66381311416626
trigger times: 0
Loss after 1588835760 batches: 0.1609
trigger times: 0
Loss after 1588964400 batches: 0.0506
trigger times: 1
Loss after 1589093040 batches: 0.0371
trigger times: 2
Loss after 1589221680 batches: 0.0320
trigger times: 3
Loss after 1589350320 batches: 0.0296
trigger times: 0
Loss after 1589478960 batches: 0.0274
trigger times: 0
Loss after 1589607600 batches: 0.0261
trigger times: 1
Loss after 1589736240 batches: 0.0247
trigger times: 2
Loss after 1589864880 batches: 0.0234
trigger times: 0
Loss after 1589993520 batches: 0.0231
trigger times: 1
Loss after 1590122160 batches: 0.0221
trigger times: 2
Loss after 1590250800 batches: 0.0218
trigger times: 3
Loss after 1590379440 batches: 0.0210
trigger times: 0
Loss after 1590508080 batches: 0.0208
trigger times: 1
Loss after 1590636720 batches: 0.0197
trigger times: 2
Loss after 1590765360 batches: 0.0201
trigger times: 3
Loss after 1590894000 batches: 0.0191
trigger times: 4
Loss after 1591022640 batches: 0.0194
trigger times: 5
Loss after 1591151280 batches: 0.0186
trigger times: 6
Loss after 1591279920 batches: 0.0186
trigger times: 0
Loss after 1591408560 batches: 0.0185
trigger times: 1
Loss after 1591537200 batches: 0.0181
trigger times: 2
Loss after 1591665840 batches: 0.0179
trigger times: 3
Loss after 1591794480 batches: 0.0174
trigger times: 0
Loss after 1591923120 batches: 0.0175
trigger times: 1
Loss after 1592051760 batches: 0.0171
trigger times: 2
Loss after 1592180400 batches: 0.0172
trigger times: 3
Loss after 1592309040 batches: 0.0170
trigger times: 4
Loss after 1592437680 batches: 0.0167
trigger times: 0
Loss after 1592566320 batches: 0.0167
trigger times: 1
Loss after 1592694960 batches: 0.0165
trigger times: 2
Loss after 1592823600 batches: 0.0163
trigger times: 3
Loss after 1592952240 batches: 0.0164
trigger times: 4
Loss after 1593080880 batches: 0.0163
trigger times: 0
Loss after 1593209520 batches: 0.0163
trigger times: 1
Loss after 1593338160 batches: 0.0162
trigger times: 2
Loss after 1593466800 batches: 0.0160
trigger times: 3
Loss after 1593595440 batches: 0.0160
trigger times: 4
Loss after 1593724080 batches: 0.0156
trigger times: 5
Loss after 1593852720 batches: 0.0157
trigger times: 0
Loss after 1593981360 batches: 0.0154
trigger times: 0
Loss after 1594110000 batches: 0.0154
trigger times: 1
Loss after 1594238640 batches: 0.0154
trigger times: 2
Loss after 1594367280 batches: 0.0151
trigger times: 3
Loss after 1594495920 batches: 0.0150
trigger times: 4
Loss after 1594624560 batches: 0.0149
trigger times: 5
Loss after 1594753200 batches: 0.0146
trigger times: 6
Loss after 1594881840 batches: 0.0148
trigger times: 7
Loss after 1595010480 batches: 0.0148
trigger times: 8
Loss after 1595139120 batches: 0.0145
trigger times: 9
Loss after 1595267760 batches: 0.0143
trigger times: 10
Loss after 1595396400 batches: 0.0147
trigger times: 11
Loss after 1595525040 batches: 0.0143
trigger times: 0
Loss after 1595653680 batches: 0.0150
trigger times: 1
Loss after 1595782320 batches: 0.0143
trigger times: 2
Loss after 1595910960 batches: 0.0145
trigger times: 3
Loss after 1596039600 batches: 0.0141
trigger times: 4
Loss after 1596168240 batches: 0.0140
trigger times: 5
Loss after 1596296880 batches: 0.0141
trigger times: 6
Loss after 1596425520 batches: 0.0140
trigger times: 7
Loss after 1596554160 batches: 0.0140
trigger times: 8
Loss after 1596682800 batches: 0.0140
trigger times: 0
Loss after 1596811440 batches: 0.0137
trigger times: 1
Loss after 1596940080 batches: 0.0136
trigger times: 2
Loss after 1597068720 batches: 0.0135
trigger times: 3
Loss after 1597197360 batches: 0.0134
trigger times: 4
Loss after 1597326000 batches: 0.0131
trigger times: 5
Loss after 1597454640 batches: 0.0137
trigger times: 6
Loss after 1597583280 batches: 0.0133
trigger times: 7
Loss after 1597711920 batches: 0.0136
trigger times: 8
Loss after 1597840560 batches: 0.0134
trigger times: 9
Loss after 1597969200 batches: 0.0133
trigger times: 0
Loss after 1598097840 batches: 0.0132
trigger times: 1
Loss after 1598226480 batches: 0.0135
trigger times: 0
Loss after 1598355120 batches: 0.0131
trigger times: 1
Loss after 1598483760 batches: 0.0129
trigger times: 2
Loss after 1598612400 batches: 0.0128
trigger times: 3
Loss after 1598741040 batches: 0.0127
trigger times: 4
Loss after 1598869680 batches: 0.0129
trigger times: 5
Loss after 1598998320 batches: 0.0126
trigger times: 6
Loss after 1599126960 batches: 0.0128
trigger times: 7
Loss after 1599255600 batches: 0.0130
trigger times: 8
Loss after 1599384240 batches: 0.0129
trigger times: 9
Loss after 1599512880 batches: 0.0124
trigger times: 10
Loss after 1599641520 batches: 0.0123
trigger times: 11
Loss after 1599770160 batches: 0.0125
trigger times: 12
Loss after 1599898800 batches: 0.0126
trigger times: 0
Loss after 1600027440 batches: 0.0124
trigger times: 0
Loss after 1600156080 batches: 0.0125
trigger times: 1
Loss after 1600284720 batches: 0.0126
trigger times: 2
Loss after 1600413360 batches: 0.0128
trigger times: 3
Loss after 1600542000 batches: 0.0123
trigger times: 4
Loss after 1600670640 batches: 0.0126
trigger times: 5
Loss after 1600799280 batches: 0.0124
trigger times: 6
Loss after 1600927920 batches: 0.0125
trigger times: 7
Loss after 1601056560 batches: 0.0124
trigger times: 0
Loss after 1601185200 batches: 0.0122
trigger times: 1
Loss after 1601313840 batches: 0.0121
trigger times: 2
Loss after 1601442480 batches: 0.0120
trigger times: 3
Loss after 1601571120 batches: 0.0124
trigger times: 0
Loss after 1601699760 batches: 0.0122
trigger times: 1
Loss after 1601828400 batches: 0.0124
trigger times: 2
Loss after 1601957040 batches: 0.0118
trigger times: 3
Loss after 1602085680 batches: 0.0120
trigger times: 4
Loss after 1602214320 batches: 0.0119
trigger times: 5
Loss after 1602342960 batches: 0.0118
trigger times: 6
Loss after 1602471600 batches: 0.0117
trigger times: 7
Loss after 1602600240 batches: 0.0121
trigger times: 8
Loss after 1602728880 batches: 0.0115
trigger times: 9
Loss after 1602857520 batches: 0.0115
trigger times: 10
Loss after 1602986160 batches: 0.0116
trigger times: 11
Loss after 1603114800 batches: 0.0117
trigger times: 12
Loss after 1603243440 batches: 0.0115
trigger times: 13
Loss after 1603372080 batches: 0.0116
trigger times: 14
Loss after 1603500720 batches: 0.0119
trigger times: 0
Loss after 1603629360 batches: 0.0120
trigger times: 1
Loss after 1603758000 batches: 0.0115
trigger times: 2
Loss after 1603886640 batches: 0.0113
trigger times: 3
Loss after 1604015280 batches: 0.0114
trigger times: 0
Loss after 1604143920 batches: 0.0112
trigger times: 1
Loss after 1604272560 batches: 0.0114
trigger times: 2
Loss after 1604401200 batches: 0.0114
trigger times: 3
Loss after 1604529840 batches: 0.0115
trigger times: 4
Loss after 1604658480 batches: 0.0114
trigger times: 5
Loss after 1604787120 batches: 0.0111
trigger times: 6
Loss after 1604915760 batches: 0.0111
trigger times: 7
Loss after 1605044400 batches: 0.0112
trigger times: 8
Loss after 1605173040 batches: 0.0111
trigger times: 9
Loss after 1605301680 batches: 0.0111
trigger times: 10
Loss after 1605430320 batches: 0.0112
trigger times: 11
Loss after 1605558960 batches: 0.0109
trigger times: 12
Loss after 1605687600 batches: 0.0111
trigger times: 13
Loss after 1605816240 batches: 0.0111
trigger times: 14
Loss after 1605944880 batches: 0.0111
trigger times: 15
Loss after 1606073520 batches: 0.0111
trigger times: 16
Loss after 1606202160 batches: 0.0109
trigger times: 17
Loss after 1606330800 batches: 0.0109
trigger times: 18
Loss after 1606459440 batches: 0.0109
trigger times: 19
Loss after 1606588080 batches: 0.0109
trigger times: 20
Early stopping!
Start to test process.
Loss after 1606716720 batches: 0.0105
Time to train on one home:  1011.299970626831
trigger times: 0
Loss after 1606847820 batches: 0.5059
trigger times: 1
Loss after 1606978920 batches: 0.1718
trigger times: 2
Loss after 1607110020 batches: 0.1080
trigger times: 3
Loss after 1607241120 batches: 0.0876
trigger times: 4
Loss after 1607372220 batches: 0.0768
trigger times: 5
Loss after 1607503320 batches: 0.0700
trigger times: 6
Loss after 1607634420 batches: 0.0649
trigger times: 7
Loss after 1607765520 batches: 0.0621
trigger times: 8
Loss after 1607896620 batches: 0.0581
trigger times: 9
Loss after 1608027720 batches: 0.0560
trigger times: 10
Loss after 1608158820 batches: 0.0537
trigger times: 11
Loss after 1608289920 batches: 0.0523
trigger times: 12
Loss after 1608421020 batches: 0.0509
trigger times: 13
Loss after 1608552120 batches: 0.0495
trigger times: 14
Loss after 1608683220 batches: 0.0480
trigger times: 15
Loss after 1608814320 batches: 0.0464
trigger times: 16
Loss after 1608945420 batches: 0.0462
trigger times: 17
Loss after 1609076520 batches: 0.0446
trigger times: 18
Loss after 1609207620 batches: 0.0448
trigger times: 19
Loss after 1609338720 batches: 0.0432
trigger times: 20
Early stopping!
Start to test process.
Loss after 1609469820 batches: 0.0428
Time to train on one home:  164.543199300766
trigger times: 0
Loss after 1609600920 batches: 0.4494
trigger times: 0
Loss after 1609732020 batches: 0.1777
trigger times: 0
Loss after 1609863120 batches: 0.1147
trigger times: 0
Loss after 1609994220 batches: 0.0864
trigger times: 0
Loss after 1610125320 batches: 0.0747
trigger times: 1
Loss after 1610256420 batches: 0.0650
trigger times: 2
Loss after 1610387520 batches: 0.0609
trigger times: 3
Loss after 1610518620 batches: 0.0550
trigger times: 0
Loss after 1610649720 batches: 0.0553
trigger times: 1
Loss after 1610780820 batches: 0.0508
trigger times: 2
Loss after 1610911920 batches: 0.0485
trigger times: 0
Loss after 1611043020 batches: 0.0487
trigger times: 1
Loss after 1611174120 batches: 0.0463
trigger times: 2
Loss after 1611305220 batches: 0.0449
trigger times: 3
Loss after 1611436320 batches: 0.0433
trigger times: 0
Loss after 1611567420 batches: 0.0428
trigger times: 1
Loss after 1611698520 batches: 0.0423
trigger times: 2
Loss after 1611829620 batches: 0.0415
trigger times: 0
Loss after 1611960720 batches: 0.0414
trigger times: 1
Loss after 1612091820 batches: 0.0394
trigger times: 2
Loss after 1612222920 batches: 0.0401
trigger times: 3
Loss after 1612354020 batches: 0.0390
trigger times: 4
Loss after 1612485120 batches: 0.0401
trigger times: 5
Loss after 1612616220 batches: 0.0387
trigger times: 0
Loss after 1612747320 batches: 0.0380
trigger times: 0
Loss after 1612878420 batches: 0.0378
trigger times: 0
Loss after 1613009520 batches: 0.0368
trigger times: 1
Loss after 1613140620 batches: 0.0361
trigger times: 0
Loss after 1613271720 batches: 0.0371
trigger times: 1
Loss after 1613402820 batches: 0.0374
trigger times: 2
Loss after 1613533920 batches: 0.0360
trigger times: 0
Loss after 1613665020 batches: 0.0368
trigger times: 0
Loss after 1613796120 batches: 0.0351
trigger times: 1
Loss after 1613927220 batches: 0.0363
trigger times: 2
Loss after 1614058320 batches: 0.0366
trigger times: 3
Loss after 1614189420 batches: 0.0351
trigger times: 4
Loss after 1614320520 batches: 0.0339
trigger times: 5
Loss after 1614451620 batches: 0.0336
trigger times: 6
Loss after 1614582720 batches: 0.0348
trigger times: 7
Loss after 1614713820 batches: 0.0327
trigger times: 0
Loss after 1614844920 batches: 0.0336
trigger times: 1
Loss after 1614976020 batches: 0.0327
trigger times: 2
Loss after 1615107120 batches: 0.0323
trigger times: 3
Loss after 1615238220 batches: 0.0330
trigger times: 4
Loss after 1615369320 batches: 0.0329
trigger times: 5
Loss after 1615500420 batches: 0.0314
trigger times: 6
Loss after 1615631520 batches: 0.0323
trigger times: 0
Loss after 1615762620 batches: 0.0303
trigger times: 1
Loss after 1615893720 batches: 0.0306
trigger times: 2
Loss after 1616024820 batches: 0.0312
trigger times: 3
Loss after 1616155920 batches: 0.0291
trigger times: 4
Loss after 1616287020 batches: 0.0305
trigger times: 0
Loss after 1616418120 batches: 0.0317
trigger times: 1
Loss after 1616549220 batches: 0.0300
trigger times: 2
Loss after 1616680320 batches: 0.0293
trigger times: 3
Loss after 1616811420 batches: 0.0300
trigger times: 4
Loss after 1616942520 batches: 0.0303
trigger times: 5
Loss after 1617073620 batches: 0.0302
trigger times: 6
Loss after 1617204720 batches: 0.0294
trigger times: 0
Loss after 1617335820 batches: 0.0291
trigger times: 1
Loss after 1617466920 batches: 0.0291
trigger times: 2
Loss after 1617598020 batches: 0.0301
trigger times: 3
Loss after 1617729120 batches: 0.0273
trigger times: 4
Loss after 1617860220 batches: 0.0274
trigger times: 5
Loss after 1617991320 batches: 0.0281
trigger times: 6
Loss after 1618122420 batches: 0.0283
trigger times: 7
Loss after 1618253520 batches: 0.0272
trigger times: 8
Loss after 1618384620 batches: 0.0283
trigger times: 9
Loss after 1618515720 batches: 0.0298
trigger times: 10
Loss after 1618646820 batches: 0.0278
trigger times: 0
Loss after 1618777920 batches: 0.0282
trigger times: 1
Loss after 1618909020 batches: 0.0282
trigger times: 2
Loss after 1619040120 batches: 0.0293
trigger times: 3
Loss after 1619171220 batches: 0.0282
trigger times: 4
Loss after 1619302320 batches: 0.0266
trigger times: 5
Loss after 1619433420 batches: 0.0282
trigger times: 0
Loss after 1619564520 batches: 0.0282
trigger times: 1
Loss after 1619695620 batches: 0.0282
trigger times: 2
Loss after 1619826720 batches: 0.0272
trigger times: 3
Loss after 1619957820 batches: 0.0275
trigger times: 4
Loss after 1620088920 batches: 0.0269
trigger times: 5
Loss after 1620220020 batches: 0.0266
trigger times: 6
Loss after 1620351120 batches: 0.0263
trigger times: 7
Loss after 1620482220 batches: 0.0265
trigger times: 8
Loss after 1620613320 batches: 0.0267
trigger times: 0
Loss after 1620744420 batches: 0.0263
trigger times: 1
Loss after 1620875520 batches: 0.0257
trigger times: 2
Loss after 1621006620 batches: 0.0261
trigger times: 3
Loss after 1621137720 batches: 0.0255
trigger times: 4
Loss after 1621268820 batches: 0.0255
trigger times: 5
Loss after 1621399920 batches: 0.0255
trigger times: 6
Loss after 1621531020 batches: 0.0257
trigger times: 7
Loss after 1621662120 batches: 0.0266
trigger times: 8
Loss after 1621793220 batches: 0.0260
trigger times: 9
Loss after 1621924320 batches: 0.0258
trigger times: 10
Loss after 1622055420 batches: 0.0260
trigger times: 0
Loss after 1622186520 batches: 0.0244
trigger times: 1
Loss after 1622317620 batches: 0.0249
trigger times: 2
Loss after 1622448720 batches: 0.0249
trigger times: 0
Loss after 1622579820 batches: 0.0255
trigger times: 1
Loss after 1622710920 batches: 0.0255
trigger times: 2
Loss after 1622842020 batches: 0.0251
trigger times: 3
Loss after 1622973120 batches: 0.0263
trigger times: 4
Loss after 1623104220 batches: 0.0242
trigger times: 5
Loss after 1623235320 batches: 0.0256
trigger times: 6
Loss after 1623366420 batches: 0.0265
trigger times: 7
Loss after 1623497520 batches: 0.0242
trigger times: 8
Loss after 1623628620 batches: 0.0249
trigger times: 9
Loss after 1623759720 batches: 0.0246
trigger times: 10
Loss after 1623890820 batches: 0.0247
trigger times: 11
Loss after 1624021920 batches: 0.0245
trigger times: 12
Loss after 1624153020 batches: 0.0236
trigger times: 13
Loss after 1624284120 batches: 0.0245
trigger times: 14
Loss after 1624415220 batches: 0.0247
trigger times: 15
Loss after 1624546320 batches: 0.0252
trigger times: 16
Loss after 1624677420 batches: 0.0242
trigger times: 17
Loss after 1624808520 batches: 0.0243
trigger times: 18
Loss after 1624939620 batches: 0.0237
trigger times: 0
Loss after 1625070720 batches: 0.0247
trigger times: 1
Loss after 1625201820 batches: 0.0236
trigger times: 2
Loss after 1625332920 batches: 0.0244
trigger times: 3
Loss after 1625464020 batches: 0.0241
trigger times: 4
Loss after 1625595120 batches: 0.0245
trigger times: 5
Loss after 1625726220 batches: 0.0233
trigger times: 0
Loss after 1625857320 batches: 0.0241
trigger times: 1
Loss after 1625988420 batches: 0.0230
trigger times: 2
Loss after 1626119520 batches: 0.0228
trigger times: 3
Loss after 1626250620 batches: 0.0233
trigger times: 4
Loss after 1626381720 batches: 0.0237
trigger times: 5
Loss after 1626512820 batches: 0.0236
trigger times: 6
Loss after 1626643920 batches: 0.0232
trigger times: 7
Loss after 1626775020 batches: 0.0224
trigger times: 8
Loss after 1626906120 batches: 0.0227
trigger times: 9
Loss after 1627037220 batches: 0.0223
trigger times: 0
Loss after 1627168320 batches: 0.0231
trigger times: 1
Loss after 1627299420 batches: 0.0247
trigger times: 2
Loss after 1627430520 batches: 0.0233
trigger times: 3
Loss after 1627561620 batches: 0.0225
trigger times: 4
Loss after 1627692720 batches: 0.0226
trigger times: 5
Loss after 1627823820 batches: 0.0236
trigger times: 6
Loss after 1627954920 batches: 0.0239
trigger times: 7
Loss after 1628086020 batches: 0.0242
trigger times: 8
Loss after 1628217120 batches: 0.0218
trigger times: 9
Loss after 1628348220 batches: 0.0222
trigger times: 10
Loss after 1628479320 batches: 0.0230
trigger times: 11
Loss after 1628610420 batches: 0.0229
trigger times: 12
Loss after 1628741520 batches: 0.0236
trigger times: 13
Loss after 1628872620 batches: 0.0228
trigger times: 14
Loss after 1629003720 batches: 0.0223
trigger times: 15
Loss after 1629134820 batches: 0.0216
trigger times: 16
Loss after 1629265920 batches: 0.0221
trigger times: 17
Loss after 1629397020 batches: 0.0220
trigger times: 18
Loss after 1629528120 batches: 0.0222
trigger times: 19
Loss after 1629659220 batches: 0.0217
trigger times: 20
Early stopping!
Start to test process.
Loss after 1629790320 batches: 0.0211
Time to train on one home:  1135.2831387519836
trigger times: 0
Loss after 1629921420 batches: 0.1125
trigger times: 1
Loss after 1630052520 batches: 0.0339
trigger times: 2
Loss after 1630183620 batches: 0.0252
trigger times: 3
Loss after 1630314720 batches: 0.0216
trigger times: 4
Loss after 1630445820 batches: 0.0197
trigger times: 5
Loss after 1630576920 batches: 0.0185
trigger times: 6
Loss after 1630708020 batches: 0.0174
trigger times: 7
Loss after 1630839120 batches: 0.0168
trigger times: 8
Loss after 1630970220 batches: 0.0162
trigger times: 9
Loss after 1631101320 batches: 0.0154
trigger times: 10
Loss after 1631232420 batches: 0.0148
trigger times: 11
Loss after 1631363520 batches: 0.0145
trigger times: 12
Loss after 1631494620 batches: 0.0141
trigger times: 13
Loss after 1631625720 batches: 0.0138
trigger times: 14
Loss after 1631756820 batches: 0.0137
trigger times: 15
Loss after 1631887920 batches: 0.0133
trigger times: 16
Loss after 1632019020 batches: 0.0134
trigger times: 17
Loss after 1632150120 batches: 0.0131
trigger times: 18
Loss after 1632281220 batches: 0.0127
trigger times: 19
Loss after 1632412320 batches: 0.0125
trigger times: 20
Early stopping!
Start to test process.
Loss after 1632543420 batches: 0.0122
Time to train on one home:  164.0353193283081
trigger times: 0
Loss after 1632674520 batches: 0.2223
trigger times: 0
Loss after 1632805620 batches: 0.0797
trigger times: 0
Loss after 1632936720 batches: 0.0511
trigger times: 0
Loss after 1633067820 batches: 0.0401
trigger times: 0
Loss after 1633198920 batches: 0.0347
trigger times: 1
Loss after 1633330020 batches: 0.0314
trigger times: 0
Loss after 1633461120 batches: 0.0286
trigger times: 0
Loss after 1633592220 batches: 0.0265
trigger times: 1
Loss after 1633723320 batches: 0.0253
trigger times: 2
Loss after 1633854420 batches: 0.0243
trigger times: 3
Loss after 1633985520 batches: 0.0229
trigger times: 4
Loss after 1634116620 batches: 0.0223
trigger times: 5
Loss after 1634247720 batches: 0.0215
trigger times: 6
Loss after 1634378820 batches: 0.0210
trigger times: 7
Loss after 1634509920 batches: 0.0200
trigger times: 8
Loss after 1634641020 batches: 0.0195
trigger times: 9
Loss after 1634772120 batches: 0.0192
trigger times: 10
Loss after 1634903220 batches: 0.0190
trigger times: 11
Loss after 1635034320 batches: 0.0182
trigger times: 12
Loss after 1635165420 batches: 0.0180
trigger times: 13
Loss after 1635296520 batches: 0.0175
trigger times: 14
Loss after 1635427620 batches: 0.0176
trigger times: 15
Loss after 1635558720 batches: 0.0174
trigger times: 16
Loss after 1635689820 batches: 0.0169
trigger times: 17
Loss after 1635820920 batches: 0.0164
trigger times: 18
Loss after 1635952020 batches: 0.0165
trigger times: 19
Loss after 1636083120 batches: 0.0163
trigger times: 20
Early stopping!
Start to test process.
Loss after 1636214220 batches: 0.0159
Time to train on one home:  214.34343099594116
trigger times: 0
Loss after 1636292820 batches: 0.3828
trigger times: 1
Loss after 1636371420 batches: 0.1129
trigger times: 2
Loss after 1636450020 batches: 0.0685
trigger times: 0
Loss after 1636528620 batches: 0.0547
trigger times: 0
Loss after 1636607220 batches: 0.0474
trigger times: 1
Loss after 1636685820 batches: 0.0428
trigger times: 0
Loss after 1636764420 batches: 0.0406
trigger times: 0
Loss after 1636843020 batches: 0.0380
trigger times: 0
Loss after 1636921620 batches: 0.0361
trigger times: 1
Loss after 1637000220 batches: 0.0347
trigger times: 2
Loss after 1637078820 batches: 0.0326
trigger times: 3
Loss after 1637157420 batches: 0.0324
trigger times: 4
Loss after 1637236020 batches: 0.0316
trigger times: 5
Loss after 1637314620 batches: 0.0309
trigger times: 6
Loss after 1637393220 batches: 0.0288
trigger times: 7
Loss after 1637471820 batches: 0.0283
trigger times: 8
Loss after 1637550420 batches: 0.0277
trigger times: 9
Loss after 1637629020 batches: 0.0278
trigger times: 10
Loss after 1637707620 batches: 0.0273
trigger times: 11
Loss after 1637786220 batches: 0.0270
trigger times: 12
Loss after 1637864820 batches: 0.0258
trigger times: 13
Loss after 1637943420 batches: 0.0264
trigger times: 14
Loss after 1638022020 batches: 0.0253
trigger times: 15
Loss after 1638100620 batches: 0.0254
trigger times: 16
Loss after 1638179220 batches: 0.0250
trigger times: 17
Loss after 1638257820 batches: 0.0242
trigger times: 18
Loss after 1638336420 batches: 0.0247
trigger times: 19
Loss after 1638415020 batches: 0.0254
trigger times: 20
Early stopping!
Start to test process.
Loss after 1638493620 batches: 0.0240
Time to train on one home:  148.2962555885315
trigger times: 0
Loss after 1638624720 batches: 0.1237
trigger times: 0
Loss after 1638755820 batches: 0.0406
trigger times: 1
Loss after 1638886920 batches: 0.0286
trigger times: 2
Loss after 1639018020 batches: 0.0249
trigger times: 3
Loss after 1639149120 batches: 0.0224
trigger times: 4
Loss after 1639280220 batches: 0.0207
trigger times: 5
Loss after 1639411320 batches: 0.0195
trigger times: 6
Loss after 1639542420 batches: 0.0187
trigger times: 7
Loss after 1639673520 batches: 0.0180
trigger times: 8
Loss after 1639804620 batches: 0.0176
trigger times: 9
Loss after 1639935720 batches: 0.0172
trigger times: 10
Loss after 1640066820 batches: 0.0164
trigger times: 11
Loss after 1640197920 batches: 0.0161
trigger times: 12
Loss after 1640329020 batches: 0.0159
trigger times: 13
Loss after 1640460120 batches: 0.0155
trigger times: 0
Loss after 1640591220 batches: 0.0149
trigger times: 1
Loss after 1640722320 batches: 0.0150
trigger times: 2
Loss after 1640853420 batches: 0.0146
trigger times: 3
Loss after 1640984520 batches: 0.0144
trigger times: 4
Loss after 1641115620 batches: 0.0139
trigger times: 5
Loss after 1641246720 batches: 0.0136
trigger times: 6
Loss after 1641377820 batches: 0.0133
trigger times: 7
Loss after 1641508920 batches: 0.0138
trigger times: 8
Loss after 1641640020 batches: 0.0137
trigger times: 9
Loss after 1641771120 batches: 0.0129
trigger times: 10
Loss after 1641902220 batches: 0.0131
trigger times: 11
Loss after 1642033320 batches: 0.0128
trigger times: 12
Loss after 1642164420 batches: 0.0128
trigger times: 13
Loss after 1642295520 batches: 0.0125
trigger times: 14
Loss after 1642426620 batches: 0.0125
trigger times: 15
Loss after 1642557720 batches: 0.0124
trigger times: 16
Loss after 1642688820 batches: 0.0124
trigger times: 17
Loss after 1642819920 batches: 0.0123
trigger times: 0
Loss after 1642951020 batches: 0.0119
trigger times: 1
Loss after 1643082120 batches: 0.0119
trigger times: 2
Loss after 1643213220 batches: 0.0118
trigger times: 3
Loss after 1643344320 batches: 0.0119
trigger times: 4
Loss after 1643475420 batches: 0.0118
trigger times: 5
Loss after 1643606520 batches: 0.0116
trigger times: 6
Loss after 1643737620 batches: 0.0114
trigger times: 7
Loss after 1643868720 batches: 0.0116
trigger times: 8
Loss after 1643999820 batches: 0.0116
trigger times: 9
Loss after 1644130920 batches: 0.0111
trigger times: 10
Loss after 1644262020 batches: 0.0110
trigger times: 11
Loss after 1644393120 batches: 0.0110
trigger times: 12
Loss after 1644524220 batches: 0.0108
trigger times: 0
Loss after 1644655320 batches: 0.0108
trigger times: 1
Loss after 1644786420 batches: 0.0108
trigger times: 2
Loss after 1644917520 batches: 0.0108
trigger times: 3
Loss after 1645048620 batches: 0.0110
trigger times: 4
Loss after 1645179720 batches: 0.0109
trigger times: 5
Loss after 1645310820 batches: 0.0104
trigger times: 6
Loss after 1645441920 batches: 0.0104
trigger times: 7
Loss after 1645573020 batches: 0.0106
trigger times: 8
Loss after 1645704120 batches: 0.0104
trigger times: 9
Loss after 1645835220 batches: 0.0102
trigger times: 10
Loss after 1645966320 batches: 0.0102
trigger times: 11
Loss after 1646097420 batches: 0.0100
trigger times: 12
Loss after 1646228520 batches: 0.0100
trigger times: 13
Loss after 1646359620 batches: 0.0101
trigger times: 14
Loss after 1646490720 batches: 0.0100
trigger times: 15
Loss after 1646621820 batches: 0.0099
trigger times: 16
Loss after 1646752920 batches: 0.0101
trigger times: 17
Loss after 1646884020 batches: 0.0100
trigger times: 18
Loss after 1647015120 batches: 0.0097
trigger times: 19
Loss after 1647146220 batches: 0.0100
trigger times: 20
Early stopping!
Start to test process.
Loss after 1647277320 batches: 0.0097
Time to train on one home:  497.7902045249939
trigger times: 0
Loss after 1647408420 batches: 0.1590
trigger times: 0
Loss after 1647539520 batches: 0.0494
trigger times: 0
Loss after 1647670620 batches: 0.0365
trigger times: 0
Loss after 1647801720 batches: 0.0322
trigger times: 0
Loss after 1647932820 batches: 0.0285
trigger times: 0
Loss after 1648063920 batches: 0.0269
trigger times: 1
Loss after 1648195020 batches: 0.0255
trigger times: 2
Loss after 1648326120 batches: 0.0245
trigger times: 3
Loss after 1648457220 batches: 0.0234
trigger times: 4
Loss after 1648588320 batches: 0.0220
trigger times: 5
Loss after 1648719420 batches: 0.0221
trigger times: 6
Loss after 1648850520 batches: 0.0214
trigger times: 7
Loss after 1648981620 batches: 0.0210
trigger times: 8
Loss after 1649112720 batches: 0.0203
trigger times: 9
Loss after 1649243820 batches: 0.0200
trigger times: 10
Loss after 1649374920 batches: 0.0195
trigger times: 11
Loss after 1649506020 batches: 0.0193
trigger times: 12
Loss after 1649637120 batches: 0.0186
trigger times: 13
Loss after 1649768220 batches: 0.0187
trigger times: 14
Loss after 1649899320 batches: 0.0184
trigger times: 15
Loss after 1650030420 batches: 0.0183
trigger times: 16
Loss after 1650161520 batches: 0.0176
trigger times: 17
Loss after 1650292620 batches: 0.0178
trigger times: 18
Loss after 1650423720 batches: 0.0174
trigger times: 19
Loss after 1650554820 batches: 0.0172
trigger times: 20
Early stopping!
Start to test process.
Loss after 1650685920 batches: 0.0173
Time to train on one home:  200.79942202568054
trigger times: 0
Loss after 1650817020 batches: 0.2649
trigger times: 0
Loss after 1650948120 batches: 0.0818
trigger times: 1
Loss after 1651079220 batches: 0.0583
trigger times: 0
Loss after 1651210320 batches: 0.0504
trigger times: 0
Loss after 1651341420 batches: 0.0450
trigger times: 0
Loss after 1651472520 batches: 0.0400
trigger times: 0
Loss after 1651603620 batches: 0.0376
trigger times: 0
Loss after 1651734720 batches: 0.0362
trigger times: 0
Loss after 1651865820 batches: 0.0338
trigger times: 1
Loss after 1651996920 batches: 0.0324
trigger times: 2
Loss after 1652128020 batches: 0.0316
trigger times: 0
Loss after 1652259120 batches: 0.0315
trigger times: 1
Loss after 1652390220 batches: 0.0301
trigger times: 2
Loss after 1652521320 batches: 0.0290
trigger times: 0
Loss after 1652652420 batches: 0.0288
trigger times: 1
Loss after 1652783520 batches: 0.0286
trigger times: 2
Loss after 1652914620 batches: 0.0273
trigger times: 0
Loss after 1653045720 batches: 0.0262
trigger times: 0
Loss after 1653176820 batches: 0.0271
trigger times: 1
Loss after 1653307920 batches: 0.0268
trigger times: 2
Loss after 1653439020 batches: 0.0257
trigger times: 3
Loss after 1653570120 batches: 0.0252
trigger times: 4
Loss after 1653701220 batches: 0.0250
trigger times: 5
Loss after 1653832320 batches: 0.0251
trigger times: 0
Loss after 1653963420 batches: 0.0249
trigger times: 1
Loss after 1654094520 batches: 0.0247
trigger times: 2
Loss after 1654225620 batches: 0.0242
trigger times: 3
Loss after 1654356720 batches: 0.0235
trigger times: 4
Loss after 1654487820 batches: 0.0234
trigger times: 5
Loss after 1654618920 batches: 0.0232
trigger times: 6
Loss after 1654750020 batches: 0.0227
trigger times: 0
Loss after 1654881120 batches: 0.0231
trigger times: 1
Loss after 1655012220 batches: 0.0222
trigger times: 2
Loss after 1655143320 batches: 0.0227
trigger times: 0
Loss after 1655274420 batches: 0.0231
trigger times: 1
Loss after 1655405520 batches: 0.0232
trigger times: 2
Loss after 1655536620 batches: 0.0223
trigger times: 3
Loss after 1655667720 batches: 0.0227
trigger times: 4
Loss after 1655798820 batches: 0.0223
trigger times: 5
Loss after 1655929920 batches: 0.0214
trigger times: 6
Loss after 1656061020 batches: 0.0216
trigger times: 7
Loss after 1656192120 batches: 0.0213
trigger times: 8
Loss after 1656323220 batches: 0.0211
trigger times: 0
Loss after 1656454320 batches: 0.0212
trigger times: 1
Loss after 1656585420 batches: 0.0209
trigger times: 2
Loss after 1656716520 batches: 0.0206
trigger times: 3
Loss after 1656847620 batches: 0.0211
trigger times: 4
Loss after 1656978720 batches: 0.0206
trigger times: 0
Loss after 1657109820 batches: 0.0205
trigger times: 1
Loss after 1657240920 batches: 0.0203
trigger times: 2
Loss after 1657372020 batches: 0.0203
trigger times: 0
Loss after 1657503120 batches: 0.0200
trigger times: 1
Loss after 1657634220 batches: 0.0204
trigger times: 2
Loss after 1657765320 batches: 0.0204
trigger times: 3
Loss after 1657896420 batches: 0.0205
trigger times: 4
Loss after 1658027520 batches: 0.0204
trigger times: 5
Loss after 1658158620 batches: 0.0197
trigger times: 0
Loss after 1658289720 batches: 0.0198
trigger times: 1
Loss after 1658420820 batches: 0.0196
trigger times: 2
Loss after 1658551920 batches: 0.0193
trigger times: 3
Loss after 1658683020 batches: 0.0193
trigger times: 4
Loss after 1658814120 batches: 0.0190
trigger times: 5
Loss after 1658945220 batches: 0.0196
trigger times: 6
Loss after 1659076320 batches: 0.0191
trigger times: 7
Loss after 1659207420 batches: 0.0194
trigger times: 8
Loss after 1659338520 batches: 0.0196
trigger times: 9
Loss after 1659469620 batches: 0.0190
trigger times: 10
Loss after 1659600720 batches: 0.0190
trigger times: 11
Loss after 1659731820 batches: 0.0188
trigger times: 12
Loss after 1659862920 batches: 0.0187
trigger times: 13
Loss after 1659994020 batches: 0.0187
trigger times: 14
Loss after 1660125120 batches: 0.0186
trigger times: 15
Loss after 1660256220 batches: 0.0183
trigger times: 16
Loss after 1660387320 batches: 0.0190
trigger times: 17
Loss after 1660518420 batches: 0.0181
trigger times: 18
Loss after 1660649520 batches: 0.0180
trigger times: 19
Loss after 1660780620 batches: 0.0178
trigger times: 0
Loss after 1660911720 batches: 0.0180
trigger times: 1
Loss after 1661042820 batches: 0.0182
trigger times: 0
Loss after 1661173920 batches: 0.0178
trigger times: 1
Loss after 1661305020 batches: 0.0184
trigger times: 2
Loss after 1661436120 batches: 0.0180
trigger times: 3
Loss after 1661567220 batches: 0.0174
trigger times: 4
Loss after 1661698320 batches: 0.0179
trigger times: 5
Loss after 1661829420 batches: 0.0180
trigger times: 6
Loss after 1661960520 batches: 0.0175
trigger times: 7
Loss after 1662091620 batches: 0.0176
trigger times: 8
Loss after 1662222720 batches: 0.0177
trigger times: 9
Loss after 1662353820 batches: 0.0177
trigger times: 10
Loss after 1662484920 batches: 0.0172
trigger times: 11
Loss after 1662616020 batches: 0.0175
trigger times: 12
Loss after 1662747120 batches: 0.0174
trigger times: 13
Loss after 1662878220 batches: 0.0171
trigger times: 14
Loss after 1663009320 batches: 0.0165
trigger times: 15
Loss after 1663140420 batches: 0.0170
trigger times: 16
Loss after 1663271520 batches: 0.0170
trigger times: 17
Loss after 1663402620 batches: 0.0176
trigger times: 18
Loss after 1663533720 batches: 0.0173
trigger times: 19
Loss after 1663664820 batches: 0.0169
trigger times: 20
Early stopping!
Start to test process.
Loss after 1663795920 batches: 0.0167
Time to train on one home:  739.9839808940887
trigger times: 0
Loss after 1663927020 batches: 0.3679
trigger times: 1
Loss after 1664058120 batches: 0.1372
trigger times: 2
Loss after 1664189220 batches: 0.0930
trigger times: 3
Loss after 1664320320 batches: 0.0780
trigger times: 0
Loss after 1664451420 batches: 0.0682
trigger times: 1
Loss after 1664582520 batches: 0.0634
trigger times: 2
Loss after 1664713620 batches: 0.0588
trigger times: 3
Loss after 1664844720 batches: 0.0566
trigger times: 4
Loss after 1664975820 batches: 0.0535
trigger times: 5
Loss after 1665106920 batches: 0.0519
trigger times: 6
Loss after 1665238020 batches: 0.0496
trigger times: 7
Loss after 1665369120 batches: 0.0488
trigger times: 8
Loss after 1665500220 batches: 0.0474
trigger times: 9
Loss after 1665631320 batches: 0.0461
trigger times: 10
Loss after 1665762420 batches: 0.0442
trigger times: 11
Loss after 1665893520 batches: 0.0437
trigger times: 12
Loss after 1666024620 batches: 0.0428
trigger times: 13
Loss after 1666155720 batches: 0.0423
trigger times: 14
Loss after 1666286820 batches: 0.0417
trigger times: 15
Loss after 1666417920 batches: 0.0411
trigger times: 16
Loss after 1666549020 batches: 0.0403
trigger times: 17
Loss after 1666680120 batches: 0.0396
trigger times: 18
Loss after 1666811220 batches: 0.0390
trigger times: 19
Loss after 1666942320 batches: 0.0385
trigger times: 20
Early stopping!
Start to test process.
Loss after 1667073420 batches: 0.0387
Time to train on one home:  192.74588203430176
trigger times: 0
Loss after 1667204520 batches: 0.2631
trigger times: 0
Loss after 1667335620 batches: 0.0829
trigger times: 1
Loss after 1667466720 batches: 0.0597
trigger times: 2
Loss after 1667597820 batches: 0.0486
trigger times: 3
Loss after 1667728920 batches: 0.0437
trigger times: 4
Loss after 1667860020 batches: 0.0396
trigger times: 5
Loss after 1667991120 batches: 0.0375
trigger times: 6
Loss after 1668122220 batches: 0.0357
trigger times: 7
Loss after 1668253320 batches: 0.0342
trigger times: 8
Loss after 1668384420 batches: 0.0336
trigger times: 9
Loss after 1668515520 batches: 0.0322
trigger times: 10
Loss after 1668646620 batches: 0.0314
trigger times: 11
Loss after 1668777720 batches: 0.0304
trigger times: 12
Loss after 1668908820 batches: 0.0294
trigger times: 13
Loss after 1669039920 batches: 0.0299
trigger times: 14
Loss after 1669171020 batches: 0.0286
trigger times: 15
Loss after 1669302120 batches: 0.0292
trigger times: 16
Loss after 1669433220 batches: 0.0281
trigger times: 17
Loss after 1669564320 batches: 0.0274
trigger times: 18
Loss after 1669695420 batches: 0.0265
trigger times: 19
Loss after 1669826520 batches: 0.0274
trigger times: 20
Early stopping!
Start to test process.
Loss after 1669957620 batches: 0.0262
Time to train on one home:  170.71956968307495
trigger times: 0
Loss after 1670088720 batches: 0.4493
trigger times: 0
Loss after 1670219820 batches: 0.1576
trigger times: 1
Loss after 1670350920 batches: 0.0938
trigger times: 2
Loss after 1670482020 batches: 0.0747
trigger times: 3
Loss after 1670613120 batches: 0.0649
trigger times: 4
Loss after 1670744220 batches: 0.0586
trigger times: 5
Loss after 1670875320 batches: 0.0535
trigger times: 6
Loss after 1671006420 batches: 0.0508
trigger times: 7
Loss after 1671137520 batches: 0.0486
trigger times: 8
Loss after 1671268620 batches: 0.0462
trigger times: 9
Loss after 1671399720 batches: 0.0442
trigger times: 10
Loss after 1671530820 batches: 0.0428
trigger times: 11
Loss after 1671661920 batches: 0.0418
trigger times: 12
Loss after 1671793020 batches: 0.0403
trigger times: 13
Loss after 1671924120 batches: 0.0389
trigger times: 14
Loss after 1672055220 batches: 0.0383
trigger times: 15
Loss after 1672186320 batches: 0.0373
trigger times: 16
Loss after 1672317420 batches: 0.0366
trigger times: 17
Loss after 1672448520 batches: 0.0358
trigger times: 18
Loss after 1672579620 batches: 0.0355
trigger times: 19
Loss after 1672710720 batches: 0.0346
trigger times: 20
Early stopping!
Start to test process.
Loss after 1672841820 batches: 0.0345
Time to train on one home:  172.3894007205963
trigger times: 0
Loss after 1672972920 batches: 0.4449
trigger times: 0
Loss after 1673104020 batches: 0.1435
trigger times: 0
Loss after 1673235120 batches: 0.0989
trigger times: 0
Loss after 1673366220 batches: 0.0848
trigger times: 1
Loss after 1673497320 batches: 0.0748
trigger times: 2
Loss after 1673628420 batches: 0.0693
trigger times: 3
Loss after 1673759520 batches: 0.0647
trigger times: 0
Loss after 1673890620 batches: 0.0627
trigger times: 1
Loss after 1674021720 batches: 0.0593
trigger times: 2
Loss after 1674152820 batches: 0.0568
trigger times: 3
Loss after 1674283920 batches: 0.0552
trigger times: 0
Loss after 1674415020 batches: 0.0537
trigger times: 1
Loss after 1674546120 batches: 0.0517
trigger times: 2
Loss after 1674677220 batches: 0.0506
trigger times: 3
Loss after 1674808320 batches: 0.0492
trigger times: 4
Loss after 1674939420 batches: 0.0485
trigger times: 5
Loss after 1675070520 batches: 0.0476
trigger times: 0
Loss after 1675201620 batches: 0.0467
trigger times: 0
Loss after 1675332720 batches: 0.0453
trigger times: 1
Loss after 1675463820 batches: 0.0451
trigger times: 2
Loss after 1675594920 batches: 0.0433
trigger times: 3
Loss after 1675726020 batches: 0.0442
trigger times: 4
Loss after 1675857120 batches: 0.0433
trigger times: 0
Loss after 1675988220 batches: 0.0430
trigger times: 1
Loss after 1676119320 batches: 0.0421
trigger times: 2
Loss after 1676250420 batches: 0.0414
trigger times: 0
Loss after 1676381520 batches: 0.0410
trigger times: 1
Loss after 1676512620 batches: 0.0403
trigger times: 2
Loss after 1676643720 batches: 0.0400
trigger times: 3
Loss after 1676774820 batches: 0.0398
trigger times: 4
Loss after 1676905920 batches: 0.0393
trigger times: 5
Loss after 1677037020 batches: 0.0392
trigger times: 0
Loss after 1677168120 batches: 0.0382
trigger times: 0
Loss after 1677299220 batches: 0.0381
trigger times: 1
Loss after 1677430320 batches: 0.0380
trigger times: 2
Loss after 1677561420 batches: 0.0374
trigger times: 3
Loss after 1677692520 batches: 0.0372
trigger times: 4
Loss after 1677823620 batches: 0.0370
trigger times: 0
Loss after 1677954720 batches: 0.0368
trigger times: 1
Loss after 1678085820 batches: 0.0368
trigger times: 2
Loss after 1678216920 batches: 0.0365
trigger times: 3
Loss after 1678348020 batches: 0.0359
trigger times: 0
Loss after 1678479120 batches: 0.0357
trigger times: 0
Loss after 1678610220 batches: 0.0356
trigger times: 1
Loss after 1678741320 batches: 0.0349
trigger times: 2
Loss after 1678872420 batches: 0.0349
trigger times: 3
Loss after 1679003520 batches: 0.0347
trigger times: 4
Loss after 1679134620 batches: 0.0347
trigger times: 5
Loss after 1679265720 batches: 0.0342
trigger times: 6
Loss after 1679396820 batches: 0.0336
trigger times: 0
Loss after 1679527920 batches: 0.0335
trigger times: 1
Loss after 1679659020 batches: 0.0343
trigger times: 2
Loss after 1679790120 batches: 0.0334
trigger times: 3
Loss after 1679921220 batches: 0.0329
trigger times: 4
Loss after 1680052320 batches: 0.0332
trigger times: 5
Loss after 1680183420 batches: 0.0332
trigger times: 6
Loss after 1680314520 batches: 0.0321
trigger times: 7
Loss after 1680445620 batches: 0.0330
trigger times: 0
Loss after 1680576720 batches: 0.0325
trigger times: 0
Loss after 1680707820 batches: 0.0323
trigger times: 1
Loss after 1680838920 batches: 0.0318
trigger times: 2
Loss after 1680970020 batches: 0.0322
trigger times: 3
Loss after 1681101120 batches: 0.0323
trigger times: 0
Loss after 1681232220 batches: 0.0315
trigger times: 1
Loss after 1681363320 batches: 0.0311
trigger times: 2
Loss after 1681494420 batches: 0.0309
trigger times: 3
Loss after 1681625520 batches: 0.0310
trigger times: 4
Loss after 1681756620 batches: 0.0313
trigger times: 5
Loss after 1681887720 batches: 0.0312
trigger times: 6
Loss after 1682018820 batches: 0.0310
trigger times: 7
Loss after 1682149920 batches: 0.0305
trigger times: 8
Loss after 1682281020 batches: 0.0307
trigger times: 9
Loss after 1682412120 batches: 0.0297
trigger times: 10
Loss after 1682543220 batches: 0.0300
trigger times: 11
Loss after 1682674320 batches: 0.0301
trigger times: 12
Loss after 1682805420 batches: 0.0301
trigger times: 13
Loss after 1682936520 batches: 0.0296
trigger times: 0
Loss after 1683067620 batches: 0.0293
trigger times: 1
Loss after 1683198720 batches: 0.0297
trigger times: 2
Loss after 1683329820 batches: 0.0295
trigger times: 0
Loss after 1683460920 batches: 0.0299
trigger times: 1
Loss after 1683592020 batches: 0.0294
trigger times: 2
Loss after 1683723120 batches: 0.0291
trigger times: 3
Loss after 1683854220 batches: 0.0298
trigger times: 4
Loss after 1683985320 batches: 0.0288
trigger times: 5
Loss after 1684116420 batches: 0.0290
trigger times: 6
Loss after 1684247520 batches: 0.0290
trigger times: 7
Loss after 1684378620 batches: 0.0285
trigger times: 8
Loss after 1684509720 batches: 0.0289
trigger times: 9
Loss after 1684640820 batches: 0.0281
trigger times: 10
Loss after 1684771920 batches: 0.0280
trigger times: 11
Loss after 1684903020 batches: 0.0287
trigger times: 12
Loss after 1685034120 batches: 0.0278
trigger times: 13
Loss after 1685165220 batches: 0.0280
trigger times: 14
Loss after 1685296320 batches: 0.0282
trigger times: 15
Loss after 1685427420 batches: 0.0278
trigger times: 16
Loss after 1685558520 batches: 0.0276
trigger times: 17
Loss after 1685689620 batches: 0.0285
trigger times: 18
Loss after 1685820720 batches: 0.0277
trigger times: 19
Loss after 1685951820 batches: 0.0278
trigger times: 20
Early stopping!
Start to test process.
Loss after 1686082920 batches: 0.0278
Time to train on one home:  745.0933978557587
trigger times: 0
Loss after 1686176880 batches: 0.4871
trigger times: 0
Loss after 1686270840 batches: 0.1635
trigger times: 0
Loss after 1686364800 batches: 0.1012
trigger times: 0
Loss after 1686458760 batches: 0.0820
trigger times: 1
Loss after 1686552720 batches: 0.0704
trigger times: 0
Loss after 1686646680 batches: 0.0650
trigger times: 0
Loss after 1686740640 batches: 0.0598
trigger times: 0
Loss after 1686834600 batches: 0.0566
trigger times: 1
Loss after 1686928560 batches: 0.0530
trigger times: 0
Loss after 1687022520 batches: 0.0507
trigger times: 1
Loss after 1687116480 batches: 0.0491
trigger times: 2
Loss after 1687210440 batches: 0.0482
trigger times: 3
Loss after 1687304400 batches: 0.0474
trigger times: 0
Loss after 1687398360 batches: 0.0458
trigger times: 1
Loss after 1687492320 batches: 0.0438
trigger times: 2
Loss after 1687586280 batches: 0.0432
trigger times: 3
Loss after 1687680240 batches: 0.0436
trigger times: 0
Loss after 1687774200 batches: 0.0419
trigger times: 1
Loss after 1687868160 batches: 0.0410
trigger times: 2
Loss after 1687962120 batches: 0.0403
trigger times: 3
Loss after 1688056080 batches: 0.0387
trigger times: 4
Loss after 1688150040 batches: 0.0384
trigger times: 5
Loss after 1688244000 batches: 0.0376
trigger times: 6
Loss after 1688337960 batches: 0.0385
trigger times: 7
Loss after 1688431920 batches: 0.0372
trigger times: 8
Loss after 1688525880 batches: 0.0369
trigger times: 9
Loss after 1688619840 batches: 0.0360
trigger times: 0
Loss after 1688713800 batches: 0.0356
trigger times: 1
Loss after 1688807760 batches: 0.0349
trigger times: 2
Loss after 1688901720 batches: 0.0351
trigger times: 3
Loss after 1688995680 batches: 0.0348
trigger times: 4
Loss after 1689089640 batches: 0.0344
trigger times: 5
Loss after 1689183600 batches: 0.0339
trigger times: 6
Loss after 1689277560 batches: 0.0331
trigger times: 7
Loss after 1689371520 batches: 0.0332
trigger times: 8
Loss after 1689465480 batches: 0.0336
trigger times: 9
Loss after 1689559440 batches: 0.0328
trigger times: 10
Loss after 1689653400 batches: 0.0320
trigger times: 11
Loss after 1689747360 batches: 0.0322
trigger times: 12
Loss after 1689841320 batches: 0.0319
trigger times: 13
Loss after 1689935280 batches: 0.0310
trigger times: 14
Loss after 1690029240 batches: 0.0313
trigger times: 15
Loss after 1690123200 batches: 0.0312
trigger times: 16
Loss after 1690217160 batches: 0.0306
trigger times: 17
Loss after 1690311120 batches: 0.0309
trigger times: 0
Loss after 1690405080 batches: 0.0307
trigger times: 1
Loss after 1690499040 batches: 0.0309
trigger times: 2
Loss after 1690593000 batches: 0.0302
trigger times: 3
Loss after 1690686960 batches: 0.0303
trigger times: 4
Loss after 1690780920 batches: 0.0300
trigger times: 5
Loss after 1690874880 batches: 0.0300
trigger times: 0
Loss after 1690968840 batches: 0.0302
trigger times: 1
Loss after 1691062800 batches: 0.0292
trigger times: 2
Loss after 1691156760 batches: 0.0291
trigger times: 3
Loss after 1691250720 batches: 0.0293
trigger times: 4
Loss after 1691344680 batches: 0.0289
trigger times: 5
Loss after 1691438640 batches: 0.0287
trigger times: 6
Loss after 1691532600 batches: 0.0286
trigger times: 7
Loss after 1691626560 batches: 0.0280
trigger times: 8
Loss after 1691720520 batches: 0.0286
trigger times: 9
Loss after 1691814480 batches: 0.0282
trigger times: 10
Loss after 1691908440 batches: 0.0282
trigger times: 11
Loss after 1692002400 batches: 0.0280
trigger times: 12
Loss after 1692096360 batches: 0.0284
trigger times: 13
Loss after 1692190320 batches: 0.0279
trigger times: 14
Loss after 1692284280 batches: 0.0273
trigger times: 15
Loss after 1692378240 batches: 0.0274
trigger times: 16
Loss after 1692472200 batches: 0.0274
trigger times: 17
Loss after 1692566160 batches: 0.0272
trigger times: 18
Loss after 1692660120 batches: 0.0267
trigger times: 19
Loss after 1692754080 batches: 0.0269
trigger times: 20
Early stopping!
Start to test process.
Loss after 1692848040 batches: 0.0268
Time to train on one home:  404.5312786102295
trigger times: 0
Loss after 1692979140 batches: 0.0688
trigger times: 1
Loss after 1693110240 batches: 0.0137
trigger times: 2
Loss after 1693241340 batches: 0.0103
trigger times: 0
Loss after 1693372440 batches: 0.0084
trigger times: 1
Loss after 1693503540 batches: 0.0074
trigger times: 2
Loss after 1693634640 batches: 0.0067
trigger times: 3
Loss after 1693765740 batches: 0.0063
trigger times: 4
Loss after 1693896840 batches: 0.0059
trigger times: 5
Loss after 1694027940 batches: 0.0054
trigger times: 6
Loss after 1694159040 batches: 0.0053
trigger times: 7
Loss after 1694290140 batches: 0.0051
trigger times: 8
Loss after 1694421240 batches: 0.0048
trigger times: 9
Loss after 1694552340 batches: 0.0048
trigger times: 10
Loss after 1694683440 batches: 0.0046
trigger times: 11
Loss after 1694814540 batches: 0.0045
trigger times: 12
Loss after 1694945640 batches: 0.0043
trigger times: 13
Loss after 1695076740 batches: 0.0042
trigger times: 14
Loss after 1695207840 batches: 0.0042
trigger times: 15
Loss after 1695338940 batches: 0.0040
trigger times: 16
Loss after 1695470040 batches: 0.0040
trigger times: 17
Loss after 1695601140 batches: 0.0039
trigger times: 18
Loss after 1695732240 batches: 0.0038
trigger times: 19
Loss after 1695863340 batches: 0.0038
trigger times: 20
Early stopping!
Start to test process.
Loss after 1695994440 batches: 0.0037
Time to train on one home:  186.44169521331787
trigger times: 0
Loss after 1696125540 batches: 0.1447
trigger times: 1
Loss after 1696256640 batches: 0.0434
trigger times: 2
Loss after 1696387740 batches: 0.0314
trigger times: 3
Loss after 1696518840 batches: 0.0263
trigger times: 4
Loss after 1696649940 batches: 0.0240
trigger times: 0
Loss after 1696781040 batches: 0.0221
trigger times: 1
Loss after 1696912140 batches: 0.0209
trigger times: 2
Loss after 1697043240 batches: 0.0194
trigger times: 3
Loss after 1697174340 batches: 0.0189
trigger times: 0
Loss after 1697305440 batches: 0.0177
trigger times: 1
Loss after 1697436540 batches: 0.0176
trigger times: 0
Loss after 1697567640 batches: 0.0168
trigger times: 0
Loss after 1697698740 batches: 0.0162
trigger times: 1
Loss after 1697829840 batches: 0.0161
trigger times: 2
Loss after 1697960940 batches: 0.0160
trigger times: 3
Loss after 1698092040 batches: 0.0155
trigger times: 0
Loss after 1698223140 batches: 0.0154
trigger times: 0
Loss after 1698354240 batches: 0.0151
trigger times: 0
Loss after 1698485340 batches: 0.0148
trigger times: 0
Loss after 1698616440 batches: 0.0145
trigger times: 0
Loss after 1698747540 batches: 0.0143
trigger times: 0
Loss after 1698878640 batches: 0.0139
trigger times: 1
Loss after 1699009740 batches: 0.0140
trigger times: 2
Loss after 1699140840 batches: 0.0138
trigger times: 3
Loss after 1699271940 batches: 0.0136
trigger times: 0
Loss after 1699403040 batches: 0.0133
trigger times: 0
Loss after 1699534140 batches: 0.0134
trigger times: 1
Loss after 1699665240 batches: 0.0129
trigger times: 2
Loss after 1699796340 batches: 0.0127
trigger times: 3
Loss after 1699927440 batches: 0.0129
trigger times: 4
Loss after 1700058540 batches: 0.0128
trigger times: 5
Loss after 1700189640 batches: 0.0126
trigger times: 6
Loss after 1700320740 batches: 0.0124
trigger times: 0
Loss after 1700451840 batches: 0.0124
trigger times: 1
Loss after 1700582940 batches: 0.0122
trigger times: 2
Loss after 1700714040 batches: 0.0120
trigger times: 3
Loss after 1700845140 batches: 0.0123
trigger times: 4
Loss after 1700976240 batches: 0.0118
trigger times: 0
Loss after 1701107340 batches: 0.0116
trigger times: 1
Loss after 1701238440 batches: 0.0117
trigger times: 2
Loss after 1701369540 batches: 0.0116
trigger times: 3
Loss after 1701500640 batches: 0.0115
trigger times: 4
Loss after 1701631740 batches: 0.0113
trigger times: 5
Loss after 1701762840 batches: 0.0113
trigger times: 6
Loss after 1701893940 batches: 0.0113
trigger times: 7
Loss after 1702025040 batches: 0.0112
trigger times: 8
Loss after 1702156140 batches: 0.0112
trigger times: 9
Loss after 1702287240 batches: 0.0112
trigger times: 10
Loss after 1702418340 batches: 0.0111
trigger times: 11
Loss after 1702549440 batches: 0.0110
trigger times: 12
Loss after 1702680540 batches: 0.0108
trigger times: 13
Loss after 1702811640 batches: 0.0107
trigger times: 14
Loss after 1702942740 batches: 0.0109
trigger times: 15
Loss after 1703073840 batches: 0.0105
trigger times: 16
Loss after 1703204940 batches: 0.0105
trigger times: 17
Loss after 1703336040 batches: 0.0104
trigger times: 18
Loss after 1703467140 batches: 0.0103
trigger times: 19
Loss after 1703598240 batches: 0.0104
trigger times: 20
Early stopping!
Start to test process.
Loss after 1703729340 batches: 0.0102
Time to train on one home:  441.4023103713989
trigger times: 0
Loss after 1703860440 batches: 0.3198
trigger times: 0
Loss after 1703991540 batches: 0.1152
trigger times: 0
Loss after 1704122640 batches: 0.0876
trigger times: 1
Loss after 1704253740 batches: 0.0766
trigger times: 0
Loss after 1704384840 batches: 0.0712
trigger times: 0
Loss after 1704515940 batches: 0.0664
trigger times: 0
Loss after 1704647040 batches: 0.0641
trigger times: 0
Loss after 1704778140 batches: 0.0617
trigger times: 1
Loss after 1704909240 batches: 0.0596
trigger times: 2
Loss after 1705040340 batches: 0.0583
trigger times: 3
Loss after 1705171440 batches: 0.0572
trigger times: 0
Loss after 1705302540 batches: 0.0565
trigger times: 0
Loss after 1705433640 batches: 0.0549
trigger times: 1
Loss after 1705564740 batches: 0.0548
trigger times: 2
Loss after 1705695840 batches: 0.0530
trigger times: 3
Loss after 1705826940 batches: 0.0518
trigger times: 4
Loss after 1705958040 batches: 0.0518
trigger times: 5
Loss after 1706089140 batches: 0.0511
trigger times: 6
Loss after 1706220240 batches: 0.0504
trigger times: 7
Loss after 1706351340 batches: 0.0506
trigger times: 8
Loss after 1706482440 batches: 0.0499
trigger times: 9
Loss after 1706613540 batches: 0.0491
trigger times: 10
Loss after 1706744640 batches: 0.0485
trigger times: 0
Loss after 1706875740 batches: 0.0490
trigger times: 1
Loss after 1707006840 batches: 0.0479
trigger times: 2
Loss after 1707137940 batches: 0.0478
trigger times: 3
Loss after 1707269040 batches: 0.0464
trigger times: 4
Loss after 1707400140 batches: 0.0477
trigger times: 5
Loss after 1707531240 batches: 0.0470
trigger times: 6
Loss after 1707662340 batches: 0.0465
trigger times: 7
Loss after 1707793440 batches: 0.0465
trigger times: 8
Loss after 1707924540 batches: 0.0462
trigger times: 9
Loss after 1708055640 batches: 0.0460
trigger times: 0
Loss after 1708186740 batches: 0.0461
trigger times: 1
Loss after 1708317840 batches: 0.0455
trigger times: 2
Loss after 1708448940 batches: 0.0448
trigger times: 3
Loss after 1708580040 batches: 0.0452
trigger times: 4
Loss after 1708711140 batches: 0.0447
trigger times: 5
Loss after 1708842240 batches: 0.0449
trigger times: 6
Loss after 1708973340 batches: 0.0441
trigger times: 7
Loss after 1709104440 batches: 0.0440
trigger times: 8
Loss after 1709235540 batches: 0.0435
trigger times: 9
Loss after 1709366640 batches: 0.0437
trigger times: 0
Loss after 1709497740 batches: 0.0437
trigger times: 1
Loss after 1709628840 batches: 0.0436
trigger times: 2
Loss after 1709759940 batches: 0.0439
trigger times: 3
Loss after 1709891040 batches: 0.0432
trigger times: 4
Loss after 1710022140 batches: 0.0429
trigger times: 5
Loss after 1710153240 batches: 0.0427
trigger times: 6
Loss after 1710284340 batches: 0.0428
trigger times: 7
Loss after 1710415440 batches: 0.0431
trigger times: 8
Loss after 1710546540 batches: 0.0421
trigger times: 9
Loss after 1710677640 batches: 0.0421
trigger times: 10
Loss after 1710808740 batches: 0.0418
trigger times: 11
Loss after 1710939840 batches: 0.0422
trigger times: 12
Loss after 1711070940 batches: 0.0418
trigger times: 13
Loss after 1711202040 batches: 0.0414
trigger times: 0
Loss after 1711333140 batches: 0.0423
trigger times: 1
Loss after 1711464240 batches: 0.0418
trigger times: 2
Loss after 1711595340 batches: 0.0417
trigger times: 3
Loss after 1711726440 batches: 0.0420
trigger times: 4
Loss after 1711857540 batches: 0.0414
trigger times: 5
Loss after 1711988640 batches: 0.0413
trigger times: 6
Loss after 1712119740 batches: 0.0414
trigger times: 7
Loss after 1712250840 batches: 0.0413
trigger times: 8
Loss after 1712381940 batches: 0.0407
trigger times: 9
Loss after 1712513040 batches: 0.0412
trigger times: 10
Loss after 1712644140 batches: 0.0405
trigger times: 11
Loss after 1712775240 batches: 0.0405
trigger times: 12
Loss after 1712906340 batches: 0.0405
trigger times: 13
Loss after 1713037440 batches: 0.0404
trigger times: 14
Loss after 1713168540 batches: 0.0398
trigger times: 15
Loss after 1713299640 batches: 0.0401
trigger times: 16
Loss after 1713430740 batches: 0.0406
trigger times: 17
Loss after 1713561840 batches: 0.0402
trigger times: 18
Loss after 1713692940 batches: 0.0398
trigger times: 19
Loss after 1713824040 batches: 0.0398
trigger times: 20
Early stopping!
Start to test process.
Loss after 1713955140 batches: 0.0399
Time to train on one home:  578.1102027893066
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642, 0.03501720033327798, 0.03263172240869951, 0.030110256078310443, 0.02544188748067248, 0.02612378438006769, 0.024274968770150986, 0.02405710057046961]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217], [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944], [0.6491197148958842, 0.29803996281550993, 0.44160060986867417, 1.203530351478871, 0.5750418631238572, 28.433835927602637, 1775.19], [0.6497863001293607, 0.29730357979276123, 0.44701117944787117, 1.2068712720184533, 0.5756451041104418, 28.512766372815683, 1777.0521], [0.6468338668346405, 0.30051325220168756, 0.4517286880074036, 1.1964129191375579, 0.5730157578453048, 28.26568403748201, 1768.9353], [0.6481483081976572, 0.29911867824708416, 0.44591549869242414, 1.1883481958771474, 0.5741581852808264, 28.07515205986611, 1772.462], [0.6422967116038004, 0.30545179637907305, 0.45038470512861073, 1.1723096038740957, 0.568970129184914, 27.696234575181023, 1756.4463], [0.635419938299391, 0.3128809605404973, 0.4559346003460791, 1.1641353165126298, 0.5628841981140037, 27.503114106407303, 1737.6584]]
Round_14_results:  [0.635419938299391, 0.3128809605404973, 0.4559346003460791, 1.1641353165126298, 0.5628841981140037, 27.503114106407303, 1737.6584]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 13640 < 13641; dropping {'Training_Loss': 0.41140291414090563, 'Validation_Loss': 0.7500001788139343, 'Training_R2': 0.5842567820490093, 'Validation_R2': 0.3009943656178399, 'Training_F1': 0.7554962727573233, 'Validation_F1': 0.4436705412423788, 'Training_NEP': 0.4919906779304189, 'Validation_NEP': 1.081031001634613, 'Training_NDE': 0.2800405862772562, 'Validation_NDE': 0.5566512397623667, 'Training_MAE': 32.2902496803016, 'Validation_MAE': 29.64635498771531, 'Training_MSE': 3695.617, 'Validation_MSE': 2055.6978}.
trigger times: 0
Loss after 1714057740 batches: 0.4114
trigger times: 1
Loss after 1714160340 batches: 0.1789
trigger times: 2
Loss after 1714262940 batches: 0.1039
trigger times: 3
Loss after 1714365540 batches: 0.0865
trigger times: 4
Loss after 1714468140 batches: 0.0639
trigger times: 5
Loss after 1714570740 batches: 0.0584
trigger times: 6
Loss after 1714673340 batches: 0.0571
trigger times: 7
Loss after 1714775940 batches: 0.0527
trigger times: 8
Loss after 1714878540 batches: 0.0477
trigger times: 9
Loss after 1714981140 batches: 0.0458
trigger times: 10
Loss after 1715083740 batches: 0.0408
trigger times: 11
Loss after 1715186340 batches: 0.0422
trigger times: 12
Loss after 1715288940 batches: 0.0389
trigger times: 13
Loss after 1715391540 batches: 0.0373
trigger times: 14
Loss after 1715494140 batches: 0.0356
trigger times: 15
Loss after 1715596740 batches: 0.0341
trigger times: 16
Loss after 1715699340 batches: 0.0340
trigger times: 17
Loss after 1715801940 batches: 0.0338
trigger times: 18
Loss after 1715904540 batches: 0.0343
trigger times: 19
Loss after 1716007140 batches: 0.0357
trigger times: 20
Early stopping!
Start to test process.
Loss after 1716109740 batches: 0.0343
Time to train on one home:  136.6769847869873
trigger times: 0
Loss after 1716240840 batches: 0.2208
trigger times: 0
Loss after 1716371940 batches: 0.0821
trigger times: 1
Loss after 1716503040 batches: 0.0582
trigger times: 0
Loss after 1716634140 batches: 0.0487
trigger times: 1
Loss after 1716765240 batches: 0.0433
trigger times: 2
Loss after 1716896340 batches: 0.0405
trigger times: 3
Loss after 1717027440 batches: 0.0376
trigger times: 4
Loss after 1717158540 batches: 0.0356
trigger times: 5
Loss after 1717289640 batches: 0.0339
trigger times: 6
Loss after 1717420740 batches: 0.0325
trigger times: 7
Loss after 1717551840 batches: 0.0315
trigger times: 8
Loss after 1717682940 batches: 0.0299
trigger times: 9
Loss after 1717814040 batches: 0.0298
trigger times: 10
Loss after 1717945140 batches: 0.0290
trigger times: 11
Loss after 1718076240 batches: 0.0284
trigger times: 12
Loss after 1718207340 batches: 0.0273
trigger times: 13
Loss after 1718338440 batches: 0.0271
trigger times: 14
Loss after 1718469540 batches: 0.0264
trigger times: 15
Loss after 1718600640 batches: 0.0260
trigger times: 16
Loss after 1718731740 batches: 0.0258
trigger times: 17
Loss after 1718862840 batches: 0.0254
trigger times: 18
Loss after 1718993940 batches: 0.0247
trigger times: 19
Loss after 1719125040 batches: 0.0250
trigger times: 20
Early stopping!
Start to test process.
Loss after 1719256140 batches: 0.0241
Time to train on one home:  185.99492835998535
trigger times: 0
Loss after 1719387240 batches: 0.4631
trigger times: 1
Loss after 1719518340 batches: 0.1651
trigger times: 2
Loss after 1719649440 batches: 0.1051
trigger times: 3
Loss after 1719780540 batches: 0.0854
trigger times: 4
Loss after 1719911640 batches: 0.0756
trigger times: 5
Loss after 1720042740 batches: 0.0694
trigger times: 6
Loss after 1720173840 batches: 0.0640
trigger times: 7
Loss after 1720304940 batches: 0.0605
trigger times: 8
Loss after 1720436040 batches: 0.0576
trigger times: 9
Loss after 1720567140 batches: 0.0549
trigger times: 10
Loss after 1720698240 batches: 0.0537
trigger times: 11
Loss after 1720829340 batches: 0.0514
trigger times: 12
Loss after 1720960440 batches: 0.0493
trigger times: 13
Loss after 1721091540 batches: 0.0482
trigger times: 14
Loss after 1721222640 batches: 0.0471
trigger times: 15
Loss after 1721353740 batches: 0.0465
trigger times: 16
Loss after 1721484840 batches: 0.0451
trigger times: 17
Loss after 1721615940 batches: 0.0442
trigger times: 18
Loss after 1721747040 batches: 0.0434
trigger times: 19
Loss after 1721878140 batches: 0.0432
trigger times: 20
Early stopping!
Start to test process.
Loss after 1722009240 batches: 0.0415
Time to train on one home:  165.14299082756042
trigger times: 0
Loss after 1722137880 batches: 0.1783
trigger times: 0
Loss after 1722266520 batches: 0.0584
trigger times: 0
Loss after 1722395160 batches: 0.0396
trigger times: 0
Loss after 1722523800 batches: 0.0333
trigger times: 1
Loss after 1722652440 batches: 0.0305
trigger times: 2
Loss after 1722781080 batches: 0.0281
trigger times: 0
Loss after 1722909720 batches: 0.0266
trigger times: 1
Loss after 1723038360 batches: 0.0250
trigger times: 0
Loss after 1723167000 batches: 0.0239
trigger times: 1
Loss after 1723295640 batches: 0.0235
trigger times: 0
Loss after 1723424280 batches: 0.0222
trigger times: 0
Loss after 1723552920 batches: 0.0215
trigger times: 1
Loss after 1723681560 batches: 0.0211
trigger times: 2
Loss after 1723810200 batches: 0.0208
trigger times: 3
Loss after 1723938840 batches: 0.0201
trigger times: 4
Loss after 1724067480 batches: 0.0196
trigger times: 5
Loss after 1724196120 batches: 0.0194
trigger times: 6
Loss after 1724324760 batches: 0.0190
trigger times: 0
Loss after 1724453400 batches: 0.0189
trigger times: 1
Loss after 1724582040 batches: 0.0185
trigger times: 0
Loss after 1724710680 batches: 0.0183
trigger times: 1
Loss after 1724839320 batches: 0.0183
trigger times: 2
Loss after 1724967960 batches: 0.0176
trigger times: 3
Loss after 1725096600 batches: 0.0181
trigger times: 4
Loss after 1725225240 batches: 0.0171
trigger times: 5
Loss after 1725353880 batches: 0.0171
trigger times: 6
Loss after 1725482520 batches: 0.0169
trigger times: 7
Loss after 1725611160 batches: 0.0169
trigger times: 8
Loss after 1725739800 batches: 0.0165
trigger times: 9
Loss after 1725868440 batches: 0.0164
trigger times: 10
Loss after 1725997080 batches: 0.0163
trigger times: 11
Loss after 1726125720 batches: 0.0160
trigger times: 12
Loss after 1726254360 batches: 0.0158
trigger times: 13
Loss after 1726383000 batches: 0.0158
trigger times: 14
Loss after 1726511640 batches: 0.0156
trigger times: 15
Loss after 1726640280 batches: 0.0155
trigger times: 16
Loss after 1726768920 batches: 0.0159
trigger times: 17
Loss after 1726897560 batches: 0.0154
trigger times: 0
Loss after 1727026200 batches: 0.0152
trigger times: 1
Loss after 1727154840 batches: 0.0152
trigger times: 2
Loss after 1727283480 batches: 0.0151
trigger times: 0
Loss after 1727412120 batches: 0.0150
trigger times: 1
Loss after 1727540760 batches: 0.0149
trigger times: 2
Loss after 1727669400 batches: 0.0145
trigger times: 3
Loss after 1727798040 batches: 0.0149
trigger times: 4
Loss after 1727926680 batches: 0.0148
trigger times: 5
Loss after 1728055320 batches: 0.0143
trigger times: 6
Loss after 1728183960 batches: 0.0144
trigger times: 7
Loss after 1728312600 batches: 0.0142
trigger times: 8
Loss after 1728441240 batches: 0.0141
trigger times: 9
Loss after 1728569880 batches: 0.0139
trigger times: 0
Loss after 1728698520 batches: 0.0139
trigger times: 1
Loss after 1728827160 batches: 0.0141
trigger times: 2
Loss after 1728955800 batches: 0.0138
trigger times: 3
Loss after 1729084440 batches: 0.0141
trigger times: 4
Loss after 1729213080 batches: 0.0138
trigger times: 5
Loss after 1729341720 batches: 0.0138
trigger times: 6
Loss after 1729470360 batches: 0.0136
trigger times: 7
Loss after 1729599000 batches: 0.0138
trigger times: 8
Loss after 1729727640 batches: 0.0135
trigger times: 0
Loss after 1729856280 batches: 0.0137
trigger times: 1
Loss after 1729984920 batches: 0.0130
trigger times: 2
Loss after 1730113560 batches: 0.0135
trigger times: 3
Loss after 1730242200 batches: 0.0134
trigger times: 4
Loss after 1730370840 batches: 0.0135
trigger times: 0
Loss after 1730499480 batches: 0.0132
trigger times: 1
Loss after 1730628120 batches: 0.0131
trigger times: 2
Loss after 1730756760 batches: 0.0131
trigger times: 3
Loss after 1730885400 batches: 0.0129
trigger times: 4
Loss after 1731014040 batches: 0.0127
trigger times: 5
Loss after 1731142680 batches: 0.0126
trigger times: 6
Loss after 1731271320 batches: 0.0131
trigger times: 7
Loss after 1731399960 batches: 0.0129
trigger times: 8
Loss after 1731528600 batches: 0.0125
trigger times: 0
Loss after 1731657240 batches: 0.0127
trigger times: 1
Loss after 1731785880 batches: 0.0127
trigger times: 2
Loss after 1731914520 batches: 0.0125
trigger times: 3
Loss after 1732043160 batches: 0.0123
trigger times: 4
Loss after 1732171800 batches: 0.0123
trigger times: 5
Loss after 1732300440 batches: 0.0124
trigger times: 6
Loss after 1732429080 batches: 0.0122
trigger times: 7
Loss after 1732557720 batches: 0.0121
trigger times: 8
Loss after 1732686360 batches: 0.0124
trigger times: 9
Loss after 1732815000 batches: 0.0124
trigger times: 10
Loss after 1732943640 batches: 0.0121
trigger times: 11
Loss after 1733072280 batches: 0.0123
trigger times: 12
Loss after 1733200920 batches: 0.0123
trigger times: 13
Loss after 1733329560 batches: 0.0122
trigger times: 14
Loss after 1733458200 batches: 0.0122
trigger times: 15
Loss after 1733586840 batches: 0.0119
trigger times: 16
Loss after 1733715480 batches: 0.0122
trigger times: 17
Loss after 1733844120 batches: 0.0120
trigger times: 18
Loss after 1733972760 batches: 0.0117
trigger times: 19
Loss after 1734101400 batches: 0.0117
trigger times: 20
Early stopping!
Start to test process.
Loss after 1734230040 batches: 0.0117
Time to train on one home:  690.3851182460785
trigger times: 0
Loss after 1734361140 batches: 0.4953
trigger times: 0
Loss after 1734492240 batches: 0.1613
trigger times: 1
Loss after 1734623340 batches: 0.1029
trigger times: 2
Loss after 1734754440 batches: 0.0840
trigger times: 0
Loss after 1734885540 batches: 0.0741
trigger times: 0
Loss after 1735016640 batches: 0.0673
trigger times: 1
Loss after 1735147740 batches: 0.0632
trigger times: 2
Loss after 1735278840 batches: 0.0595
trigger times: 3
Loss after 1735409940 batches: 0.0565
trigger times: 4
Loss after 1735541040 batches: 0.0548
trigger times: 0
Loss after 1735672140 batches: 0.0521
trigger times: 0
Loss after 1735803240 batches: 0.0508
trigger times: 0
Loss after 1735934340 batches: 0.0491
trigger times: 1
Loss after 1736065440 batches: 0.0475
trigger times: 2
Loss after 1736196540 batches: 0.0466
trigger times: 3
Loss after 1736327640 batches: 0.0452
trigger times: 4
Loss after 1736458740 batches: 0.0442
trigger times: 5
Loss after 1736589840 batches: 0.0433
trigger times: 6
Loss after 1736720940 batches: 0.0431
trigger times: 7
Loss after 1736852040 batches: 0.0417
trigger times: 8
Loss after 1736983140 batches: 0.0413
trigger times: 9
Loss after 1737114240 batches: 0.0408
trigger times: 10
Loss after 1737245340 batches: 0.0399
trigger times: 11
Loss after 1737376440 batches: 0.0397
trigger times: 12
Loss after 1737507540 batches: 0.0388
trigger times: 13
Loss after 1737638640 batches: 0.0387
trigger times: 14
Loss after 1737769740 batches: 0.0376
trigger times: 15
Loss after 1737900840 batches: 0.0375
trigger times: 16
Loss after 1738031940 batches: 0.0372
trigger times: 17
Loss after 1738163040 batches: 0.0367
trigger times: 18
Loss after 1738294140 batches: 0.0370
trigger times: 19
Loss after 1738425240 batches: 0.0365
trigger times: 20
Early stopping!
Start to test process.
Loss after 1738556340 batches: 0.0355
Time to train on one home:  251.36098337173462
trigger times: 0
Loss after 1738687440 batches: 0.4305
trigger times: 0
Loss after 1738818540 batches: 0.1689
trigger times: 0
Loss after 1738949640 batches: 0.1020
trigger times: 1
Loss after 1739080740 batches: 0.0810
trigger times: 2
Loss after 1739211840 batches: 0.0708
trigger times: 3
Loss after 1739342940 batches: 0.0624
trigger times: 4
Loss after 1739474040 batches: 0.0560
trigger times: 0
Loss after 1739605140 batches: 0.0527
trigger times: 1
Loss after 1739736240 batches: 0.0499
trigger times: 0
Loss after 1739867340 batches: 0.0487
trigger times: 0
Loss after 1739998440 batches: 0.0469
trigger times: 1
Loss after 1740129540 batches: 0.0483
trigger times: 2
Loss after 1740260640 batches: 0.0448
trigger times: 0
Loss after 1740391740 batches: 0.0424
trigger times: 0
Loss after 1740522840 batches: 0.0426
trigger times: 1
Loss after 1740653940 batches: 0.0422
trigger times: 2
Loss after 1740785040 batches: 0.0410
trigger times: 0
Loss after 1740916140 batches: 0.0398
trigger times: 0
Loss after 1741047240 batches: 0.0388
trigger times: 1
Loss after 1741178340 batches: 0.0409
trigger times: 0
Loss after 1741309440 batches: 0.0405
trigger times: 1
Loss after 1741440540 batches: 0.0378
trigger times: 2
Loss after 1741571640 batches: 0.0368
trigger times: 0
Loss after 1741702740 batches: 0.0366
trigger times: 1
Loss after 1741833840 batches: 0.0364
trigger times: 2
Loss after 1741964940 batches: 0.0351
trigger times: 3
Loss after 1742096040 batches: 0.0348
trigger times: 4
Loss after 1742227140 batches: 0.0341
trigger times: 0
Loss after 1742358240 batches: 0.0351
trigger times: 1
Loss after 1742489340 batches: 0.0347
trigger times: 0
Loss after 1742620440 batches: 0.0339
trigger times: 1
Loss after 1742751540 batches: 0.0344
trigger times: 2
Loss after 1742882640 batches: 0.0320
trigger times: 3
Loss after 1743013740 batches: 0.0319
trigger times: 4
Loss after 1743144840 batches: 0.0328
trigger times: 5
Loss after 1743275940 batches: 0.0310
trigger times: 6
Loss after 1743407040 batches: 0.0320
trigger times: 0
Loss after 1743538140 batches: 0.0317
trigger times: 1
Loss after 1743669240 batches: 0.0322
trigger times: 0
Loss after 1743800340 batches: 0.0309
trigger times: 1
Loss after 1743931440 batches: 0.0320
trigger times: 2
Loss after 1744062540 batches: 0.0306
trigger times: 3
Loss after 1744193640 batches: 0.0300
trigger times: 4
Loss after 1744324740 batches: 0.0295
trigger times: 5
Loss after 1744455840 batches: 0.0304
trigger times: 6
Loss after 1744586940 batches: 0.0301
trigger times: 7
Loss after 1744718040 batches: 0.0303
trigger times: 0
Loss after 1744849140 batches: 0.0285
trigger times: 1
Loss after 1744980240 batches: 0.0290
trigger times: 2
Loss after 1745111340 batches: 0.0295
trigger times: 3
Loss after 1745242440 batches: 0.0293
trigger times: 0
Loss after 1745373540 batches: 0.0277
trigger times: 1
Loss after 1745504640 batches: 0.0279
trigger times: 2
Loss after 1745635740 batches: 0.0297
trigger times: 0
Loss after 1745766840 batches: 0.0284
trigger times: 1
Loss after 1745897940 batches: 0.0285
trigger times: 2
Loss after 1746029040 batches: 0.0294
trigger times: 0
Loss after 1746160140 batches: 0.0283
trigger times: 1
Loss after 1746291240 batches: 0.0285
trigger times: 2
Loss after 1746422340 batches: 0.0275
trigger times: 3
Loss after 1746553440 batches: 0.0268
trigger times: 0
Loss after 1746684540 batches: 0.0275
trigger times: 0
Loss after 1746815640 batches: 0.0290
trigger times: 1
Loss after 1746946740 batches: 0.0280
trigger times: 2
Loss after 1747077840 batches: 0.0270
trigger times: 3
Loss after 1747208940 batches: 0.0262
trigger times: 0
Loss after 1747340040 batches: 0.0283
trigger times: 1
Loss after 1747471140 batches: 0.0261
trigger times: 2
Loss after 1747602240 batches: 0.0262
trigger times: 3
Loss after 1747733340 batches: 0.0263
trigger times: 0
Loss after 1747864440 batches: 0.0256
trigger times: 1
Loss after 1747995540 batches: 0.0264
trigger times: 2
Loss after 1748126640 batches: 0.0256
trigger times: 3
Loss after 1748257740 batches: 0.0273
trigger times: 0
Loss after 1748388840 batches: 0.0264
trigger times: 1
Loss after 1748519940 batches: 0.0264
trigger times: 2
Loss after 1748651040 batches: 0.0255
trigger times: 3
Loss after 1748782140 batches: 0.0280
trigger times: 4
Loss after 1748913240 batches: 0.0257
trigger times: 5
Loss after 1749044340 batches: 0.0264
trigger times: 6
Loss after 1749175440 batches: 0.0276
trigger times: 7
Loss after 1749306540 batches: 0.0263
trigger times: 8
Loss after 1749437640 batches: 0.0251
trigger times: 9
Loss after 1749568740 batches: 0.0256
trigger times: 10
Loss after 1749699840 batches: 0.0258
trigger times: 11
Loss after 1749830940 batches: 0.0249
trigger times: 12
Loss after 1749962040 batches: 0.0254
trigger times: 13
Loss after 1750093140 batches: 0.0245
trigger times: 14
Loss after 1750224240 batches: 0.0252
trigger times: 15
Loss after 1750355340 batches: 0.0258
trigger times: 16
Loss after 1750486440 batches: 0.0238
trigger times: 17
Loss after 1750617540 batches: 0.0249
trigger times: 0
Loss after 1750748640 batches: 0.0246
trigger times: 1
Loss after 1750879740 batches: 0.0248
trigger times: 2
Loss after 1751010840 batches: 0.0256
trigger times: 3
Loss after 1751141940 batches: 0.0248
trigger times: 4
Loss after 1751273040 batches: 0.0253
trigger times: 5
Loss after 1751404140 batches: 0.0241
trigger times: 6
Loss after 1751535240 batches: 0.0241
trigger times: 7
Loss after 1751666340 batches: 0.0239
trigger times: 8
Loss after 1751797440 batches: 0.0245
trigger times: 9
Loss after 1751928540 batches: 0.0250
trigger times: 10
Loss after 1752059640 batches: 0.0239
trigger times: 11
Loss after 1752190740 batches: 0.0237
trigger times: 12
Loss after 1752321840 batches: 0.0237
trigger times: 0
Loss after 1752452940 batches: 0.0258
trigger times: 1
Loss after 1752584040 batches: 0.0236
trigger times: 2
Loss after 1752715140 batches: 0.0226
trigger times: 3
Loss after 1752846240 batches: 0.0235
trigger times: 4
Loss after 1752977340 batches: 0.0228
trigger times: 5
Loss after 1753108440 batches: 0.0237
trigger times: 6
Loss after 1753239540 batches: 0.0254
trigger times: 7
Loss after 1753370640 batches: 0.0249
trigger times: 8
Loss after 1753501740 batches: 0.0237
trigger times: 9
Loss after 1753632840 batches: 0.0218
trigger times: 10
Loss after 1753763940 batches: 0.0230
trigger times: 11
Loss after 1753895040 batches: 0.0236
trigger times: 12
Loss after 1754026140 batches: 0.0231
trigger times: 13
Loss after 1754157240 batches: 0.0229
trigger times: 14
Loss after 1754288340 batches: 0.0215
trigger times: 15
Loss after 1754419440 batches: 0.0226
trigger times: 16
Loss after 1754550540 batches: 0.0228
trigger times: 17
Loss after 1754681640 batches: 0.0225
trigger times: 18
Loss after 1754812740 batches: 0.0217
trigger times: 19
Loss after 1754943840 batches: 0.0228
trigger times: 0
Loss after 1755074940 batches: 0.0224
trigger times: 1
Loss after 1755206040 batches: 0.0223
trigger times: 2
Loss after 1755337140 batches: 0.0221
trigger times: 3
Loss after 1755468240 batches: 0.0231
trigger times: 4
Loss after 1755599340 batches: 0.0228
trigger times: 5
Loss after 1755730440 batches: 0.0227
trigger times: 6
Loss after 1755861540 batches: 0.0223
trigger times: 7
Loss after 1755992640 batches: 0.0228
trigger times: 8
Loss after 1756123740 batches: 0.0232
trigger times: 0
Loss after 1756254840 batches: 0.0225
trigger times: 1
Loss after 1756385940 batches: 0.0214
trigger times: 2
Loss after 1756517040 batches: 0.0212
trigger times: 3
Loss after 1756648140 batches: 0.0211
trigger times: 4
Loss after 1756779240 batches: 0.0223
trigger times: 5
Loss after 1756910340 batches: 0.0214
trigger times: 6
Loss after 1757041440 batches: 0.0216
trigger times: 7
Loss after 1757172540 batches: 0.0225
trigger times: 8
Loss after 1757303640 batches: 0.0219
trigger times: 9
Loss after 1757434740 batches: 0.0210
trigger times: 10
Loss after 1757565840 batches: 0.0213
trigger times: 11
Loss after 1757696940 batches: 0.0211
trigger times: 12
Loss after 1757828040 batches: 0.0215
trigger times: 13
Loss after 1757959140 batches: 0.0212
trigger times: 14
Loss after 1758090240 batches: 0.0215
trigger times: 0
Loss after 1758221340 batches: 0.0206
trigger times: 1
Loss after 1758352440 batches: 0.0210
trigger times: 2
Loss after 1758483540 batches: 0.0211
trigger times: 3
Loss after 1758614640 batches: 0.0201
trigger times: 4
Loss after 1758745740 batches: 0.0226
trigger times: 5
Loss after 1758876840 batches: 0.0207
trigger times: 6
Loss after 1759007940 batches: 0.0208
trigger times: 7
Loss after 1759139040 batches: 0.0211
trigger times: 8
Loss after 1759270140 batches: 0.0211
trigger times: 9
Loss after 1759401240 batches: 0.0208
trigger times: 0
Loss after 1759532340 batches: 0.0202
trigger times: 1
Loss after 1759663440 batches: 0.0203
trigger times: 2
Loss after 1759794540 batches: 0.0214
trigger times: 3
Loss after 1759925640 batches: 0.0224
trigger times: 0
Loss after 1760056740 batches: 0.0198
trigger times: 1
Loss after 1760187840 batches: 0.0208
trigger times: 2
Loss after 1760318940 batches: 0.0211
trigger times: 3
Loss after 1760450040 batches: 0.0198
trigger times: 4
Loss after 1760581140 batches: 0.0203
trigger times: 5
Loss after 1760712240 batches: 0.0201
trigger times: 6
Loss after 1760843340 batches: 0.0213
trigger times: 7
Loss after 1760974440 batches: 0.0205
trigger times: 8
Loss after 1761105540 batches: 0.0199
trigger times: 9
Loss after 1761236640 batches: 0.0207
trigger times: 10
Loss after 1761367740 batches: 0.0200
trigger times: 11
Loss after 1761498840 batches: 0.0213
trigger times: 12
Loss after 1761629940 batches: 0.0203
trigger times: 13
Loss after 1761761040 batches: 0.0209
trigger times: 14
Loss after 1761892140 batches: 0.0201
trigger times: 15
Loss after 1762023240 batches: 0.0200
trigger times: 16
Loss after 1762154340 batches: 0.0202
trigger times: 17
Loss after 1762285440 batches: 0.0209
trigger times: 18
Loss after 1762416540 batches: 0.0204
trigger times: 19
Loss after 1762547640 batches: 0.0204
trigger times: 20
Early stopping!
Start to test process.
Loss after 1762678740 batches: 0.0193
Time to train on one home:  1347.5577986240387
trigger times: 0
Loss after 1762809840 batches: 0.1093
trigger times: 0
Loss after 1762940940 batches: 0.0331
trigger times: 1
Loss after 1763072040 batches: 0.0240
trigger times: 2
Loss after 1763203140 batches: 0.0213
trigger times: 3
Loss after 1763334240 batches: 0.0193
trigger times: 4
Loss after 1763465340 batches: 0.0179
trigger times: 5
Loss after 1763596440 batches: 0.0166
trigger times: 6
Loss after 1763727540 batches: 0.0165
trigger times: 7
Loss after 1763858640 batches: 0.0156
trigger times: 8
Loss after 1763989740 batches: 0.0150
trigger times: 9
Loss after 1764120840 batches: 0.0144
trigger times: 10
Loss after 1764251940 batches: 0.0139
trigger times: 11
Loss after 1764383040 batches: 0.0138
trigger times: 12
Loss after 1764514140 batches: 0.0136
trigger times: 13
Loss after 1764645240 batches: 0.0133
trigger times: 14
Loss after 1764776340 batches: 0.0129
trigger times: 15
Loss after 1764907440 batches: 0.0126
trigger times: 16
Loss after 1765038540 batches: 0.0124
trigger times: 17
Loss after 1765169640 batches: 0.0126
trigger times: 18
Loss after 1765300740 batches: 0.0122
trigger times: 19
Loss after 1765431840 batches: 0.0121
trigger times: 20
Early stopping!
Start to test process.
Loss after 1765562940 batches: 0.0120
Time to train on one home:  172.66758155822754
trigger times: 0
Loss after 1765694040 batches: 0.1854
trigger times: 0
Loss after 1765825140 batches: 0.0496
trigger times: 0
Loss after 1765956240 batches: 0.0331
trigger times: 0
Loss after 1766087340 batches: 0.0286
trigger times: 0
Loss after 1766218440 batches: 0.0252
trigger times: 0
Loss after 1766349540 batches: 0.0234
trigger times: 1
Loss after 1766480640 batches: 0.0220
trigger times: 2
Loss after 1766611740 batches: 0.0209
trigger times: 3
Loss after 1766742840 batches: 0.0201
trigger times: 4
Loss after 1766873940 batches: 0.0194
trigger times: 5
Loss after 1767005040 batches: 0.0189
trigger times: 6
Loss after 1767136140 batches: 0.0181
trigger times: 7
Loss after 1767267240 batches: 0.0179
trigger times: 8
Loss after 1767398340 batches: 0.0172
trigger times: 9
Loss after 1767529440 batches: 0.0171
trigger times: 10
Loss after 1767660540 batches: 0.0167
trigger times: 11
Loss after 1767791640 batches: 0.0167
trigger times: 12
Loss after 1767922740 batches: 0.0164
trigger times: 13
Loss after 1768053840 batches: 0.0157
trigger times: 14
Loss after 1768184940 batches: 0.0157
trigger times: 15
Loss after 1768316040 batches: 0.0153
trigger times: 16
Loss after 1768447140 batches: 0.0152
trigger times: 17
Loss after 1768578240 batches: 0.0151
trigger times: 18
Loss after 1768709340 batches: 0.0149
trigger times: 19
Loss after 1768840440 batches: 0.0148
trigger times: 20
Early stopping!
Start to test process.
Loss after 1768971540 batches: 0.0145
Time to train on one home:  201.15356254577637
trigger times: 0
Loss after 1769050140 batches: 0.4748
trigger times: 1
Loss after 1769128740 batches: 0.1771
trigger times: 2
Loss after 1769207340 batches: 0.0950
trigger times: 3
Loss after 1769285940 batches: 0.0689
trigger times: 4
Loss after 1769364540 batches: 0.0581
trigger times: 5
Loss after 1769443140 batches: 0.0519
trigger times: 6
Loss after 1769521740 batches: 0.0473
trigger times: 7
Loss after 1769600340 batches: 0.0450
trigger times: 8
Loss after 1769678940 batches: 0.0410
trigger times: 9
Loss after 1769757540 batches: 0.0386
trigger times: 10
Loss after 1769836140 batches: 0.0379
trigger times: 11
Loss after 1769914740 batches: 0.0357
trigger times: 12
Loss after 1769993340 batches: 0.0345
trigger times: 13
Loss after 1770071940 batches: 0.0333
trigger times: 14
Loss after 1770150540 batches: 0.0327
trigger times: 15
Loss after 1770229140 batches: 0.0321
trigger times: 16
Loss after 1770307740 batches: 0.0312
trigger times: 17
Loss after 1770386340 batches: 0.0309
trigger times: 18
Loss after 1770464940 batches: 0.0313
trigger times: 19
Loss after 1770543540 batches: 0.0297
trigger times: 20
Early stopping!
Start to test process.
Loss after 1770622140 batches: 0.0292
Time to train on one home:  111.11776518821716
trigger times: 0
Loss after 1770753240 batches: 0.1189
trigger times: 0
Loss after 1770884340 batches: 0.0359
trigger times: 0
Loss after 1771015440 batches: 0.0266
trigger times: 0
Loss after 1771146540 batches: 0.0233
trigger times: 1
Loss after 1771277640 batches: 0.0208
trigger times: 2
Loss after 1771408740 batches: 0.0195
trigger times: 3
Loss after 1771539840 batches: 0.0185
trigger times: 4
Loss after 1771670940 batches: 0.0176
trigger times: 5
Loss after 1771802040 batches: 0.0171
trigger times: 6
Loss after 1771933140 batches: 0.0164
trigger times: 7
Loss after 1772064240 batches: 0.0161
trigger times: 8
Loss after 1772195340 batches: 0.0157
trigger times: 9
Loss after 1772326440 batches: 0.0152
trigger times: 10
Loss after 1772457540 batches: 0.0149
trigger times: 11
Loss after 1772588640 batches: 0.0147
trigger times: 12
Loss after 1772719740 batches: 0.0146
trigger times: 13
Loss after 1772850840 batches: 0.0140
trigger times: 14
Loss after 1772981940 batches: 0.0136
trigger times: 15
Loss after 1773113040 batches: 0.0137
trigger times: 16
Loss after 1773244140 batches: 0.0133
trigger times: 17
Loss after 1773375240 batches: 0.0133
trigger times: 18
Loss after 1773506340 batches: 0.0129
trigger times: 19
Loss after 1773637440 batches: 0.0130
trigger times: 20
Early stopping!
Start to test process.
Loss after 1773768540 batches: 0.0130
Time to train on one home:  186.29311966896057
trigger times: 0
Loss after 1773899640 batches: 0.1654
trigger times: 0
Loss after 1774030740 batches: 0.0500
trigger times: 0
Loss after 1774161840 batches: 0.0360
trigger times: 0
Loss after 1774292940 batches: 0.0315
trigger times: 1
Loss after 1774424040 batches: 0.0284
trigger times: 0
Loss after 1774555140 batches: 0.0260
trigger times: 1
Loss after 1774686240 batches: 0.0252
trigger times: 2
Loss after 1774817340 batches: 0.0236
trigger times: 3
Loss after 1774948440 batches: 0.0232
trigger times: 0
Loss after 1775079540 batches: 0.0220
trigger times: 1
Loss after 1775210640 batches: 0.0218
trigger times: 2
Loss after 1775341740 batches: 0.0209
trigger times: 3
Loss after 1775472840 batches: 0.0205
trigger times: 0
Loss after 1775603940 batches: 0.0201
trigger times: 0
Loss after 1775735040 batches: 0.0196
trigger times: 0
Loss after 1775866140 batches: 0.0198
trigger times: 1
Loss after 1775997240 batches: 0.0187
trigger times: 2
Loss after 1776128340 batches: 0.0186
trigger times: 3
Loss after 1776259440 batches: 0.0180
trigger times: 4
Loss after 1776390540 batches: 0.0181
trigger times: 5
Loss after 1776521640 batches: 0.0179
trigger times: 6
Loss after 1776652740 batches: 0.0176
trigger times: 7
Loss after 1776783840 batches: 0.0174
trigger times: 8
Loss after 1776914940 batches: 0.0174
trigger times: 9
Loss after 1777046040 batches: 0.0170
trigger times: 10
Loss after 1777177140 batches: 0.0170
trigger times: 11
Loss after 1777308240 batches: 0.0164
trigger times: 12
Loss after 1777439340 batches: 0.0165
trigger times: 13
Loss after 1777570440 batches: 0.0162
trigger times: 14
Loss after 1777701540 batches: 0.0161
trigger times: 15
Loss after 1777832640 batches: 0.0159
trigger times: 16
Loss after 1777963740 batches: 0.0158
trigger times: 17
Loss after 1778094840 batches: 0.0158
trigger times: 0
Loss after 1778225940 batches: 0.0157
trigger times: 1
Loss after 1778357040 batches: 0.0156
trigger times: 2
Loss after 1778488140 batches: 0.0151
trigger times: 3
Loss after 1778619240 batches: 0.0151
trigger times: 0
Loss after 1778750340 batches: 0.0152
trigger times: 1
Loss after 1778881440 batches: 0.0149
trigger times: 2
Loss after 1779012540 batches: 0.0152
trigger times: 3
Loss after 1779143640 batches: 0.0148
trigger times: 4
Loss after 1779274740 batches: 0.0147
trigger times: 5
Loss after 1779405840 batches: 0.0145
trigger times: 6
Loss after 1779536940 batches: 0.0144
trigger times: 7
Loss after 1779668040 batches: 0.0144
trigger times: 8
Loss after 1779799140 batches: 0.0143
trigger times: 9
Loss after 1779930240 batches: 0.0142
trigger times: 10
Loss after 1780061340 batches: 0.0140
trigger times: 11
Loss after 1780192440 batches: 0.0143
trigger times: 12
Loss after 1780323540 batches: 0.0139
trigger times: 0
Loss after 1780454640 batches: 0.0137
trigger times: 1
Loss after 1780585740 batches: 0.0137
trigger times: 2
Loss after 1780716840 batches: 0.0135
trigger times: 3
Loss after 1780847940 batches: 0.0135
trigger times: 4
Loss after 1780979040 batches: 0.0134
trigger times: 5
Loss after 1781110140 batches: 0.0135
trigger times: 6
Loss after 1781241240 batches: 0.0135
trigger times: 7
Loss after 1781372340 batches: 0.0134
trigger times: 0
Loss after 1781503440 batches: 0.0135
trigger times: 1
Loss after 1781634540 batches: 0.0132
trigger times: 2
Loss after 1781765640 batches: 0.0131
trigger times: 3
Loss after 1781896740 batches: 0.0131
trigger times: 4
Loss after 1782027840 batches: 0.0130
trigger times: 5
Loss after 1782158940 batches: 0.0131
trigger times: 6
Loss after 1782290040 batches: 0.0129
trigger times: 7
Loss after 1782421140 batches: 0.0126
trigger times: 8
Loss after 1782552240 batches: 0.0128
trigger times: 9
Loss after 1782683340 batches: 0.0127
trigger times: 10
Loss after 1782814440 batches: 0.0127
trigger times: 11
Loss after 1782945540 batches: 0.0124
trigger times: 12
Loss after 1783076640 batches: 0.0126
trigger times: 13
Loss after 1783207740 batches: 0.0124
trigger times: 14
Loss after 1783338840 batches: 0.0127
trigger times: 15
Loss after 1783469940 batches: 0.0124
trigger times: 16
Loss after 1783601040 batches: 0.0122
trigger times: 17
Loss after 1783732140 batches: 0.0123
trigger times: 18
Loss after 1783863240 batches: 0.0120
trigger times: 19
Loss after 1783994340 batches: 0.0120
trigger times: 20
Early stopping!
Start to test process.
Loss after 1784125440 batches: 0.0122
Time to train on one home:  585.1838381290436
trigger times: 0
Loss after 1784256540 batches: 0.2597
trigger times: 0
Loss after 1784387640 batches: 0.0811
trigger times: 0
Loss after 1784518740 batches: 0.0534
trigger times: 0
Loss after 1784649840 batches: 0.0447
trigger times: 1
Loss after 1784780940 batches: 0.0409
trigger times: 2
Loss after 1784912040 batches: 0.0370
trigger times: 0
Loss after 1785043140 batches: 0.0356
trigger times: 0
Loss after 1785174240 batches: 0.0329
trigger times: 0
Loss after 1785305340 batches: 0.0326
trigger times: 0
Loss after 1785436440 batches: 0.0311
trigger times: 1
Loss after 1785567540 batches: 0.0303
trigger times: 2
Loss after 1785698640 batches: 0.0287
trigger times: 0
Loss after 1785829740 batches: 0.0292
trigger times: 0
Loss after 1785960840 batches: 0.0285
trigger times: 1
Loss after 1786091940 batches: 0.0273
trigger times: 2
Loss after 1786223040 batches: 0.0268
trigger times: 0
Loss after 1786354140 batches: 0.0262
trigger times: 1
Loss after 1786485240 batches: 0.0249
trigger times: 0
Loss after 1786616340 batches: 0.0248
trigger times: 1
Loss after 1786747440 batches: 0.0244
trigger times: 2
Loss after 1786878540 batches: 0.0249
trigger times: 3
Loss after 1787009640 batches: 0.0240
trigger times: 4
Loss after 1787140740 batches: 0.0235
trigger times: 5
Loss after 1787271840 batches: 0.0241
trigger times: 0
Loss after 1787402940 batches: 0.0234
trigger times: 1
Loss after 1787534040 batches: 0.0236
trigger times: 2
Loss after 1787665140 batches: 0.0231
trigger times: 3
Loss after 1787796240 batches: 0.0238
trigger times: 0
Loss after 1787927340 batches: 0.0227
trigger times: 0
Loss after 1788058440 batches: 0.0219
trigger times: 1
Loss after 1788189540 batches: 0.0220
trigger times: 2
Loss after 1788320640 batches: 0.0219
trigger times: 3
Loss after 1788451740 batches: 0.0222
trigger times: 4
Loss after 1788582840 batches: 0.0215
trigger times: 5
Loss after 1788713940 batches: 0.0212
trigger times: 6
Loss after 1788845040 batches: 0.0214
trigger times: 7
Loss after 1788976140 batches: 0.0219
trigger times: 8
Loss after 1789107240 batches: 0.0215
trigger times: 9
Loss after 1789238340 batches: 0.0209
trigger times: 10
Loss after 1789369440 batches: 0.0209
trigger times: 11
Loss after 1789500540 batches: 0.0211
trigger times: 12
Loss after 1789631640 batches: 0.0206
trigger times: 0
Loss after 1789762740 batches: 0.0205
trigger times: 0
Loss after 1789893840 batches: 0.0204
trigger times: 1
Loss after 1790024940 batches: 0.0208
trigger times: 2
Loss after 1790156040 batches: 0.0200
trigger times: 3
Loss after 1790287140 batches: 0.0199
trigger times: 4
Loss after 1790418240 batches: 0.0193
trigger times: 5
Loss after 1790549340 batches: 0.0199
trigger times: 6
Loss after 1790680440 batches: 0.0190
trigger times: 7
Loss after 1790811540 batches: 0.0195
trigger times: 8
Loss after 1790942640 batches: 0.0198
trigger times: 9
Loss after 1791073740 batches: 0.0193
trigger times: 10
Loss after 1791204840 batches: 0.0190
trigger times: 11
Loss after 1791335940 batches: 0.0188
trigger times: 12
Loss after 1791467040 batches: 0.0191
trigger times: 13
Loss after 1791598140 batches: 0.0189
trigger times: 14
Loss after 1791729240 batches: 0.0194
trigger times: 0
Loss after 1791860340 batches: 0.0190
trigger times: 1
Loss after 1791991440 batches: 0.0189
trigger times: 2
Loss after 1792122540 batches: 0.0188
trigger times: 3
Loss after 1792253640 batches: 0.0185
trigger times: 4
Loss after 1792384740 batches: 0.0184
trigger times: 5
Loss after 1792515840 batches: 0.0184
trigger times: 6
Loss after 1792646940 batches: 0.0184
trigger times: 7
Loss after 1792778040 batches: 0.0179
trigger times: 8
Loss after 1792909140 batches: 0.0178
trigger times: 9
Loss after 1793040240 batches: 0.0183
trigger times: 10
Loss after 1793171340 batches: 0.0176
trigger times: 11
Loss after 1793302440 batches: 0.0178
trigger times: 12
Loss after 1793433540 batches: 0.0177
trigger times: 13
Loss after 1793564640 batches: 0.0179
trigger times: 14
Loss after 1793695740 batches: 0.0179
trigger times: 15
Loss after 1793826840 batches: 0.0178
trigger times: 16
Loss after 1793957940 batches: 0.0174
trigger times: 17
Loss after 1794089040 batches: 0.0171
trigger times: 18
Loss after 1794220140 batches: 0.0174
trigger times: 19
Loss after 1794351240 batches: 0.0178
trigger times: 20
Early stopping!
Start to test process.
Loss after 1794482340 batches: 0.0169
Time to train on one home:  583.841180562973
trigger times: 0
Loss after 1794613440 batches: 0.3646
trigger times: 1
Loss after 1794744540 batches: 0.1301
trigger times: 2
Loss after 1794875640 batches: 0.0890
trigger times: 3
Loss after 1795006740 batches: 0.0747
trigger times: 4
Loss after 1795137840 batches: 0.0665
trigger times: 5
Loss after 1795268940 batches: 0.0616
trigger times: 0
Loss after 1795400040 batches: 0.0572
trigger times: 1
Loss after 1795531140 batches: 0.0547
trigger times: 2
Loss after 1795662240 batches: 0.0529
trigger times: 3
Loss after 1795793340 batches: 0.0492
trigger times: 4
Loss after 1795924440 batches: 0.0482
trigger times: 5
Loss after 1796055540 batches: 0.0461
trigger times: 6
Loss after 1796186640 batches: 0.0449
trigger times: 7
Loss after 1796317740 batches: 0.0440
trigger times: 8
Loss after 1796448840 batches: 0.0436
trigger times: 9
Loss after 1796579940 batches: 0.0421
trigger times: 10
Loss after 1796711040 batches: 0.0413
trigger times: 11
Loss after 1796842140 batches: 0.0407
trigger times: 12
Loss after 1796973240 batches: 0.0404
trigger times: 13
Loss after 1797104340 batches: 0.0397
trigger times: 14
Loss after 1797235440 batches: 0.0389
trigger times: 15
Loss after 1797366540 batches: 0.0382
trigger times: 16
Loss after 1797497640 batches: 0.0379
trigger times: 17
Loss after 1797628740 batches: 0.0372
trigger times: 18
Loss after 1797759840 batches: 0.0363
trigger times: 19
Loss after 1797890940 batches: 0.0365
trigger times: 20
Early stopping!
Start to test process.
Loss after 1798022040 batches: 0.0359
Time to train on one home:  208.11075067520142
trigger times: 0
Loss after 1798153140 batches: 0.2588
trigger times: 0
Loss after 1798284240 batches: 0.0804
trigger times: 0
Loss after 1798415340 batches: 0.0582
trigger times: 1
Loss after 1798546440 batches: 0.0481
trigger times: 2
Loss after 1798677540 batches: 0.0427
trigger times: 3
Loss after 1798808640 batches: 0.0406
trigger times: 0
Loss after 1798939740 batches: 0.0367
trigger times: 1
Loss after 1799070840 batches: 0.0351
trigger times: 2
Loss after 1799201940 batches: 0.0336
trigger times: 3
Loss after 1799333040 batches: 0.0323
trigger times: 0
Loss after 1799464140 batches: 0.0320
trigger times: 1
Loss after 1799595240 batches: 0.0307
trigger times: 2
Loss after 1799726340 batches: 0.0302
trigger times: 3
Loss after 1799857440 batches: 0.0300
trigger times: 4
Loss after 1799988540 batches: 0.0293
trigger times: 5
Loss after 1800119640 batches: 0.0282
trigger times: 6
Loss after 1800250740 batches: 0.0280
trigger times: 7
Loss after 1800381840 batches: 0.0267
trigger times: 0
Loss after 1800512940 batches: 0.0272
trigger times: 1
Loss after 1800644040 batches: 0.0267
trigger times: 2
Loss after 1800775140 batches: 0.0264
trigger times: 3
Loss after 1800906240 batches: 0.0252
trigger times: 4
Loss after 1801037340 batches: 0.0250
trigger times: 5
Loss after 1801168440 batches: 0.0245
trigger times: 6
Loss after 1801299540 batches: 0.0242
trigger times: 7
Loss after 1801430640 batches: 0.0248
trigger times: 8
Loss after 1801561740 batches: 0.0248
trigger times: 9
Loss after 1801692840 batches: 0.0241
trigger times: 10
Loss after 1801823940 batches: 0.0241
trigger times: 11
Loss after 1801955040 batches: 0.0240
trigger times: 12
Loss after 1802086140 batches: 0.0243
trigger times: 13
Loss after 1802217240 batches: 0.0233
trigger times: 14
Loss after 1802348340 batches: 0.0235
trigger times: 15
Loss after 1802479440 batches: 0.0235
trigger times: 16
Loss after 1802610540 batches: 0.0228
trigger times: 17
Loss after 1802741640 batches: 0.0226
trigger times: 18
Loss after 1802872740 batches: 0.0227
trigger times: 19
Loss after 1803003840 batches: 0.0227
trigger times: 20
Early stopping!
Start to test process.
Loss after 1803134940 batches: 0.0223
Time to train on one home:  294.3460042476654
trigger times: 0
Loss after 1803266040 batches: 0.4374
trigger times: 0
Loss after 1803397140 batches: 0.1405
trigger times: 1
Loss after 1803528240 batches: 0.0858
trigger times: 2
Loss after 1803659340 batches: 0.0688
trigger times: 3
Loss after 1803790440 batches: 0.0602
trigger times: 4
Loss after 1803921540 batches: 0.0555
trigger times: 5
Loss after 1804052640 batches: 0.0506
trigger times: 6
Loss after 1804183740 batches: 0.0479
trigger times: 7
Loss after 1804314840 batches: 0.0452
trigger times: 8
Loss after 1804445940 batches: 0.0433
trigger times: 9
Loss after 1804577040 batches: 0.0419
trigger times: 10
Loss after 1804708140 batches: 0.0405
trigger times: 11
Loss after 1804839240 batches: 0.0395
trigger times: 12
Loss after 1804970340 batches: 0.0380
trigger times: 13
Loss after 1805101440 batches: 0.0371
trigger times: 14
Loss after 1805232540 batches: 0.0365
trigger times: 15
Loss after 1805363640 batches: 0.0354
trigger times: 16
Loss after 1805494740 batches: 0.0347
trigger times: 17
Loss after 1805625840 batches: 0.0343
trigger times: 18
Loss after 1805756940 batches: 0.0333
trigger times: 19
Loss after 1805888040 batches: 0.0329
trigger times: 20
Early stopping!
Start to test process.
Loss after 1806019140 batches: 0.0322
Time to train on one home:  171.7800543308258
trigger times: 0
Loss after 1806150240 batches: 0.4416
trigger times: 0
Loss after 1806281340 batches: 0.1383
trigger times: 1
Loss after 1806412440 batches: 0.0971
trigger times: 0
Loss after 1806543540 batches: 0.0814
trigger times: 0
Loss after 1806674640 batches: 0.0731
trigger times: 1
Loss after 1806805740 batches: 0.0676
trigger times: 2
Loss after 1806936840 batches: 0.0633
trigger times: 0
Loss after 1807067940 batches: 0.0596
trigger times: 1
Loss after 1807199040 batches: 0.0566
trigger times: 2
Loss after 1807330140 batches: 0.0545
trigger times: 0
Loss after 1807461240 batches: 0.0530
trigger times: 1
Loss after 1807592340 batches: 0.0510
trigger times: 2
Loss after 1807723440 batches: 0.0492
trigger times: 3
Loss after 1807854540 batches: 0.0483
trigger times: 4
Loss after 1807985640 batches: 0.0475
trigger times: 5
Loss after 1808116740 batches: 0.0470
trigger times: 6
Loss after 1808247840 batches: 0.0465
trigger times: 7
Loss after 1808378940 batches: 0.0449
trigger times: 8
Loss after 1808510040 batches: 0.0439
trigger times: 9
Loss after 1808641140 batches: 0.0437
trigger times: 10
Loss after 1808772240 batches: 0.0423
trigger times: 11
Loss after 1808903340 batches: 0.0425
trigger times: 12
Loss after 1809034440 batches: 0.0414
trigger times: 13
Loss after 1809165540 batches: 0.0408
trigger times: 14
Loss after 1809296640 batches: 0.0402
trigger times: 15
Loss after 1809427740 batches: 0.0401
trigger times: 16
Loss after 1809558840 batches: 0.0391
trigger times: 17
Loss after 1809689940 batches: 0.0393
trigger times: 18
Loss after 1809821040 batches: 0.0386
trigger times: 19
Loss after 1809952140 batches: 0.0389
trigger times: 20
Early stopping!
Start to test process.
Loss after 1810083240 batches: 0.0380
Time to train on one home:  237.95503449440002
trigger times: 0
Loss after 1810177200 batches: 0.4732
trigger times: 0
Loss after 1810271160 batches: 0.1494
trigger times: 1
Loss after 1810365120 batches: 0.0958
trigger times: 0
Loss after 1810459080 batches: 0.0790
trigger times: 0
Loss after 1810553040 batches: 0.0659
trigger times: 1
Loss after 1810647000 batches: 0.0606
trigger times: 0
Loss after 1810740960 batches: 0.0561
trigger times: 1
Loss after 1810834920 batches: 0.0530
trigger times: 2
Loss after 1810928880 batches: 0.0501
trigger times: 0
Loss after 1811022840 batches: 0.0491
trigger times: 1
Loss after 1811116800 batches: 0.0471
trigger times: 2
Loss after 1811210760 batches: 0.0448
trigger times: 3
Loss after 1811304720 batches: 0.0437
trigger times: 4
Loss after 1811398680 batches: 0.0427
trigger times: 0
Loss after 1811492640 batches: 0.0425
trigger times: 1
Loss after 1811586600 batches: 0.0412
trigger times: 2
Loss after 1811680560 batches: 0.0406
trigger times: 0
Loss after 1811774520 batches: 0.0393
trigger times: 1
Loss after 1811868480 batches: 0.0376
trigger times: 0
Loss after 1811962440 batches: 0.0370
trigger times: 1
Loss after 1812056400 batches: 0.0373
trigger times: 2
Loss after 1812150360 batches: 0.0362
trigger times: 3
Loss after 1812244320 batches: 0.0358
trigger times: 4
Loss after 1812338280 batches: 0.0358
trigger times: 5
Loss after 1812432240 batches: 0.0352
trigger times: 6
Loss after 1812526200 batches: 0.0352
trigger times: 7
Loss after 1812620160 batches: 0.0347
trigger times: 8
Loss after 1812714120 batches: 0.0339
trigger times: 9
Loss after 1812808080 batches: 0.0333
trigger times: 10
Loss after 1812902040 batches: 0.0334
trigger times: 11
Loss after 1812996000 batches: 0.0331
trigger times: 0
Loss after 1813089960 batches: 0.0327
trigger times: 1
Loss after 1813183920 batches: 0.0323
trigger times: 0
Loss after 1813277880 batches: 0.0328
trigger times: 1
Loss after 1813371840 batches: 0.0317
trigger times: 0
Loss after 1813465800 batches: 0.0318
trigger times: 1
Loss after 1813559760 batches: 0.0316
trigger times: 2
Loss after 1813653720 batches: 0.0314
trigger times: 3
Loss after 1813747680 batches: 0.0306
trigger times: 4
Loss after 1813841640 batches: 0.0314
trigger times: 5
Loss after 1813935600 batches: 0.0301
trigger times: 6
Loss after 1814029560 batches: 0.0301
trigger times: 7
Loss after 1814123520 batches: 0.0298
trigger times: 8
Loss after 1814217480 batches: 0.0291
trigger times: 0
Loss after 1814311440 batches: 0.0297
trigger times: 0
Loss after 1814405400 batches: 0.0295
trigger times: 1
Loss after 1814499360 batches: 0.0292
trigger times: 2
Loss after 1814593320 batches: 0.0291
trigger times: 3
Loss after 1814687280 batches: 0.0288
trigger times: 4
Loss after 1814781240 batches: 0.0290
trigger times: 5
Loss after 1814875200 batches: 0.0285
trigger times: 0
Loss after 1814969160 batches: 0.0282
trigger times: 1
Loss after 1815063120 batches: 0.0283
trigger times: 2
Loss after 1815157080 batches: 0.0276
trigger times: 3
Loss after 1815251040 batches: 0.0280
trigger times: 4
Loss after 1815345000 batches: 0.0280
trigger times: 5
Loss after 1815438960 batches: 0.0274
trigger times: 6
Loss after 1815532920 batches: 0.0274
trigger times: 7
Loss after 1815626880 batches: 0.0272
trigger times: 8
Loss after 1815720840 batches: 0.0276
trigger times: 9
Loss after 1815814800 batches: 0.0272
trigger times: 10
Loss after 1815908760 batches: 0.0269
trigger times: 11
Loss after 1816002720 batches: 0.0268
trigger times: 12
Loss after 1816096680 batches: 0.0268
trigger times: 0
Loss after 1816190640 batches: 0.0264
trigger times: 1
Loss after 1816284600 batches: 0.0258
trigger times: 2
Loss after 1816378560 batches: 0.0266
trigger times: 3
Loss after 1816472520 batches: 0.0254
trigger times: 4
Loss after 1816566480 batches: 0.0258
trigger times: 5
Loss after 1816660440 batches: 0.0261
trigger times: 6
Loss after 1816754400 batches: 0.0253
trigger times: 7
Loss after 1816848360 batches: 0.0250
trigger times: 8
Loss after 1816942320 batches: 0.0255
trigger times: 9
Loss after 1817036280 batches: 0.0251
trigger times: 10
Loss after 1817130240 batches: 0.0256
trigger times: 11
Loss after 1817224200 batches: 0.0250
trigger times: 12
Loss after 1817318160 batches: 0.0247
trigger times: 13
Loss after 1817412120 batches: 0.0249
trigger times: 0
Loss after 1817506080 batches: 0.0248
trigger times: 1
Loss after 1817600040 batches: 0.0245
trigger times: 2
Loss after 1817694000 batches: 0.0246
trigger times: 3
Loss after 1817787960 batches: 0.0247
trigger times: 4
Loss after 1817881920 batches: 0.0245
trigger times: 5
Loss after 1817975880 batches: 0.0243
trigger times: 6
Loss after 1818069840 batches: 0.0241
trigger times: 0
Loss after 1818163800 batches: 0.0240
trigger times: 1
Loss after 1818257760 batches: 0.0239
trigger times: 2
Loss after 1818351720 batches: 0.0242
trigger times: 3
Loss after 1818445680 batches: 0.0238
trigger times: 4
Loss after 1818539640 batches: 0.0236
trigger times: 5
Loss after 1818633600 batches: 0.0236
trigger times: 6
Loss after 1818727560 batches: 0.0234
trigger times: 7
Loss after 1818821520 batches: 0.0235
trigger times: 8
Loss after 1818915480 batches: 0.0238
trigger times: 9
Loss after 1819009440 batches: 0.0232
trigger times: 10
Loss after 1819103400 batches: 0.0240
trigger times: 11
Loss after 1819197360 batches: 0.0237
trigger times: 12
Loss after 1819291320 batches: 0.0237
trigger times: 13
Loss after 1819385280 batches: 0.0231
trigger times: 14
Loss after 1819479240 batches: 0.0228
trigger times: 15
Loss after 1819573200 batches: 0.0233
trigger times: 16
Loss after 1819667160 batches: 0.0233
trigger times: 17
Loss after 1819761120 batches: 0.0228
trigger times: 18
Loss after 1819855080 batches: 0.0229
trigger times: 19
Loss after 1819949040 batches: 0.0230
trigger times: 20
Early stopping!
Start to test process.
Loss after 1820043000 batches: 0.0227
Time to train on one home:  588.8005819320679
trigger times: 0
Loss after 1820174100 batches: 0.0722
trigger times: 1
Loss after 1820305200 batches: 0.0146
trigger times: 2
Loss after 1820436300 batches: 0.0104
trigger times: 3
Loss after 1820567400 batches: 0.0086
trigger times: 4
Loss after 1820698500 batches: 0.0078
trigger times: 5
Loss after 1820829600 batches: 0.0069
trigger times: 6
Loss after 1820960700 batches: 0.0062
trigger times: 7
Loss after 1821091800 batches: 0.0059
trigger times: 8
Loss after 1821222900 batches: 0.0055
trigger times: 9
Loss after 1821354000 batches: 0.0053
trigger times: 10
Loss after 1821485100 batches: 0.0051
trigger times: 11
Loss after 1821616200 batches: 0.0051
trigger times: 12
Loss after 1821747300 batches: 0.0048
trigger times: 13
Loss after 1821878400 batches: 0.0046
trigger times: 14
Loss after 1822009500 batches: 0.0045
trigger times: 15
Loss after 1822140600 batches: 0.0044
trigger times: 16
Loss after 1822271700 batches: 0.0043
trigger times: 17
Loss after 1822402800 batches: 0.0042
trigger times: 18
Loss after 1822533900 batches: 0.0041
trigger times: 19
Loss after 1822665000 batches: 0.0040
trigger times: 20
Early stopping!
Start to test process.
Loss after 1822796100 batches: 0.0039
Time to train on one home:  165.49157452583313
trigger times: 0
Loss after 1822927200 batches: 0.2205
trigger times: 0
Loss after 1823058300 batches: 0.0800
trigger times: 0
Loss after 1823189400 batches: 0.0556
trigger times: 1
Loss after 1823320500 batches: 0.0449
trigger times: 2
Loss after 1823451600 batches: 0.0381
trigger times: 3
Loss after 1823582700 batches: 0.0344
trigger times: 4
Loss after 1823713800 batches: 0.0317
trigger times: 5
Loss after 1823844900 batches: 0.0293
trigger times: 6
Loss after 1823976000 batches: 0.0278
trigger times: 7
Loss after 1824107100 batches: 0.0264
trigger times: 8
Loss after 1824238200 batches: 0.0258
trigger times: 9
Loss after 1824369300 batches: 0.0245
trigger times: 10
Loss after 1824500400 batches: 0.0237
trigger times: 11
Loss after 1824631500 batches: 0.0226
trigger times: 12
Loss after 1824762600 batches: 0.0222
trigger times: 13
Loss after 1824893700 batches: 0.0218
trigger times: 14
Loss after 1825024800 batches: 0.0205
trigger times: 15
Loss after 1825155900 batches: 0.0199
trigger times: 16
Loss after 1825287000 batches: 0.0195
trigger times: 17
Loss after 1825418100 batches: 0.0193
trigger times: 18
Loss after 1825549200 batches: 0.0189
trigger times: 19
Loss after 1825680300 batches: 0.0186
trigger times: 20
Early stopping!
Start to test process.
Loss after 1825811400 batches: 0.0182
Time to train on one home:  179.59788489341736
trigger times: 0
Loss after 1825942500 batches: 0.3047
trigger times: 1
Loss after 1826073600 batches: 0.1094
trigger times: 0
Loss after 1826204700 batches: 0.0843
trigger times: 0
Loss after 1826335800 batches: 0.0756
trigger times: 1
Loss after 1826466900 batches: 0.0691
trigger times: 2
Loss after 1826598000 batches: 0.0650
trigger times: 3
Loss after 1826729100 batches: 0.0617
trigger times: 4
Loss after 1826860200 batches: 0.0601
trigger times: 0
Loss after 1826991300 batches: 0.0580
trigger times: 1
Loss after 1827122400 batches: 0.0570
trigger times: 0
Loss after 1827253500 batches: 0.0557
trigger times: 1
Loss after 1827384600 batches: 0.0541
trigger times: 2
Loss after 1827515700 batches: 0.0529
trigger times: 3
Loss after 1827646800 batches: 0.0524
trigger times: 4
Loss after 1827777900 batches: 0.0520
trigger times: 5
Loss after 1827909000 batches: 0.0513
trigger times: 6
Loss after 1828040100 batches: 0.0503
trigger times: 7
Loss after 1828171200 batches: 0.0500
trigger times: 8
Loss after 1828302300 batches: 0.0493
trigger times: 9
Loss after 1828433400 batches: 0.0484
trigger times: 10
Loss after 1828564500 batches: 0.0484
trigger times: 11
Loss after 1828695600 batches: 0.0480
trigger times: 12
Loss after 1828826700 batches: 0.0482
trigger times: 13
Loss after 1828957800 batches: 0.0484
trigger times: 0
Loss after 1829088900 batches: 0.0474
trigger times: 1
Loss after 1829220000 batches: 0.0465
trigger times: 2
Loss after 1829351100 batches: 0.0464
trigger times: 3
Loss after 1829482200 batches: 0.0463
trigger times: 4
Loss after 1829613300 batches: 0.0463
trigger times: 5
Loss after 1829744400 batches: 0.0456
trigger times: 6
Loss after 1829875500 batches: 0.0453
trigger times: 7
Loss after 1830006600 batches: 0.0453
trigger times: 8
Loss after 1830137700 batches: 0.0448
trigger times: 9
Loss after 1830268800 batches: 0.0445
trigger times: 0
Loss after 1830399900 batches: 0.0445
trigger times: 1
Loss after 1830531000 batches: 0.0445
trigger times: 2
Loss after 1830662100 batches: 0.0442
trigger times: 3
Loss after 1830793200 batches: 0.0436
trigger times: 4
Loss after 1830924300 batches: 0.0441
trigger times: 5
Loss after 1831055400 batches: 0.0433
trigger times: 6
Loss after 1831186500 batches: 0.0432
trigger times: 7
Loss after 1831317600 batches: 0.0432
trigger times: 8
Loss after 1831448700 batches: 0.0435
trigger times: 9
Loss after 1831579800 batches: 0.0430
trigger times: 10
Loss after 1831710900 batches: 0.0422
trigger times: 11
Loss after 1831842000 batches: 0.0425
trigger times: 12
Loss after 1831973100 batches: 0.0419
trigger times: 0
Loss after 1832104200 batches: 0.0424
trigger times: 1
Loss after 1832235300 batches: 0.0427
trigger times: 2
Loss after 1832366400 batches: 0.0417
trigger times: 3
Loss after 1832497500 batches: 0.0416
trigger times: 4
Loss after 1832628600 batches: 0.0412
trigger times: 5
Loss after 1832759700 batches: 0.0414
trigger times: 6
Loss after 1832890800 batches: 0.0415
trigger times: 7
Loss after 1833021900 batches: 0.0422
trigger times: 8
Loss after 1833153000 batches: 0.0412
trigger times: 9
Loss after 1833284100 batches: 0.0409
trigger times: 10
Loss after 1833415200 batches: 0.0404
trigger times: 11
Loss after 1833546300 batches: 0.0414
trigger times: 12
Loss after 1833677400 batches: 0.0415
trigger times: 13
Loss after 1833808500 batches: 0.0407
trigger times: 14
Loss after 1833939600 batches: 0.0404
trigger times: 15
Loss after 1834070700 batches: 0.0402
trigger times: 16
Loss after 1834201800 batches: 0.0409
trigger times: 17
Loss after 1834332900 batches: 0.0402
trigger times: 18
Loss after 1834464000 batches: 0.0401
trigger times: 19
Loss after 1834595100 batches: 0.0404
trigger times: 20
Early stopping!
Start to test process.
Loss after 1834726200 batches: 0.0401
Time to train on one home:  506.32954120635986
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642, 0.03501720033327798, 0.03263172240869951, 0.030110256078310443, 0.02544188748067248, 0.02612378438006769, 0.024274968770150986, 0.02405710057046961, 0.02388874572137445]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217], [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944], [0.6491197148958842, 0.29803996281550993, 0.44160060986867417, 1.203530351478871, 0.5750418631238572, 28.433835927602637, 1775.19], [0.6497863001293607, 0.29730357979276123, 0.44701117944787117, 1.2068712720184533, 0.5756451041104418, 28.512766372815683, 1777.0521], [0.6468338668346405, 0.30051325220168756, 0.4517286880074036, 1.1964129191375579, 0.5730157578453048, 28.26568403748201, 1768.9353], [0.6481483081976572, 0.29911867824708416, 0.44591549869242414, 1.1883481958771474, 0.5741581852808264, 28.07515205986611, 1772.462], [0.6422967116038004, 0.30545179637907305, 0.45038470512861073, 1.1723096038740957, 0.568970129184914, 27.696234575181023, 1756.4463], [0.635419938299391, 0.3128809605404973, 0.4559346003460791, 1.1641353165126298, 0.5628841981140037, 27.503114106407303, 1737.6584], [0.6225904193189409, 0.32673875385346274, 0.47098132348279487, 1.1438745784657927, 0.551531968836913, 27.024446908120186, 1702.6134]]
Round_15_results:  [0.6225904193189409, 0.32673875385346274, 0.47098132348279487, 1.1438745784657927, 0.551531968836913, 27.024446908120186, 1702.6134]
trigger times: 0
Loss after 1834828800 batches: 0.4036
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 14606 < 14607; dropping {'Training_Loss': 0.40362567844845, 'Validation_Loss': 0.8003573616345724, 'Training_R2': 0.5913524453863235, 'Validation_R2': 0.25409047654325645, 'Training_F1': 0.757316242654086, 'Validation_F1': 0.4106277479800856, 'Training_NEP': 0.4873705943562788, 'Validation_NEP': 1.1360573987835403, 'Training_NDE': 0.27526101649665796, 'Validation_NDE': 0.5940030245246181, 'Training_MAE': 31.987025129827607, 'Validation_MAE': 31.15540708807634, 'Training_MSE': 3632.542, 'Validation_MSE': 2193.6367}.
trigger times: 1
Loss after 1834931400 batches: 0.1778
trigger times: 2
Loss after 1835034000 batches: 0.1068
trigger times: 3
Loss after 1835136600 batches: 0.0873
trigger times: 4
Loss after 1835239200 batches: 0.0698
trigger times: 5
Loss after 1835341800 batches: 0.0589
trigger times: 6
Loss after 1835444400 batches: 0.0610
trigger times: 7
Loss after 1835547000 batches: 0.0501
trigger times: 8
Loss after 1835649600 batches: 0.0448
trigger times: 9
Loss after 1835752200 batches: 0.0421
trigger times: 10
Loss after 1835854800 batches: 0.0438
trigger times: 11
Loss after 1835957400 batches: 0.0386
trigger times: 12
Loss after 1836060000 batches: 0.0386
trigger times: 13
Loss after 1836162600 batches: 0.0371
trigger times: 14
Loss after 1836265200 batches: 0.0344
trigger times: 15
Loss after 1836367800 batches: 0.0332
trigger times: 16
Loss after 1836470400 batches: 0.0320
trigger times: 17
Loss after 1836573000 batches: 0.0325
trigger times: 18
Loss after 1836675600 batches: 0.0326
trigger times: 19
Loss after 1836778200 batches: 0.0328
trigger times: 20
Early stopping!
Start to test process.
Loss after 1836880800 batches: 0.0312
Time to train on one home:  135.99142217636108
trigger times: 0
Loss after 1837011900 batches: 0.2711
trigger times: 1
Loss after 1837143000 batches: 0.1112
trigger times: 2
Loss after 1837274100 batches: 0.0752
trigger times: 3
Loss after 1837405200 batches: 0.0601
trigger times: 4
Loss after 1837536300 batches: 0.0525
trigger times: 5
Loss after 1837667400 batches: 0.0471
trigger times: 6
Loss after 1837798500 batches: 0.0439
trigger times: 7
Loss after 1837929600 batches: 0.0405
trigger times: 8
Loss after 1838060700 batches: 0.0388
trigger times: 9
Loss after 1838191800 batches: 0.0367
trigger times: 10
Loss after 1838322900 batches: 0.0354
trigger times: 11
Loss after 1838454000 batches: 0.0341
trigger times: 12
Loss after 1838585100 batches: 0.0334
trigger times: 13
Loss after 1838716200 batches: 0.0321
trigger times: 14
Loss after 1838847300 batches: 0.0314
trigger times: 15
Loss after 1838978400 batches: 0.0306
trigger times: 16
Loss after 1839109500 batches: 0.0298
trigger times: 17
Loss after 1839240600 batches: 0.0291
trigger times: 18
Loss after 1839371700 batches: 0.0286
trigger times: 19
Loss after 1839502800 batches: 0.0281
trigger times: 20
Early stopping!
Start to test process.
Loss after 1839633900 batches: 0.0277
Time to train on one home:  164.83310961723328
trigger times: 0
Loss after 1839765000 batches: 0.4343
trigger times: 1
Loss after 1839896100 batches: 0.1467
trigger times: 2
Loss after 1840027200 batches: 0.0975
trigger times: 3
Loss after 1840158300 batches: 0.0817
trigger times: 4
Loss after 1840289400 batches: 0.0721
trigger times: 5
Loss after 1840420500 batches: 0.0661
trigger times: 6
Loss after 1840551600 batches: 0.0615
trigger times: 7
Loss after 1840682700 batches: 0.0584
trigger times: 8
Loss after 1840813800 batches: 0.0558
trigger times: 9
Loss after 1840944900 batches: 0.0537
trigger times: 10
Loss after 1841076000 batches: 0.0519
trigger times: 11
Loss after 1841207100 batches: 0.0502
trigger times: 0
Loss after 1841338200 batches: 0.0484
trigger times: 1
Loss after 1841469300 batches: 0.0468
trigger times: 2
Loss after 1841600400 batches: 0.0454
trigger times: 0
Loss after 1841731500 batches: 0.0448
trigger times: 1
Loss after 1841862600 batches: 0.0439
trigger times: 2
Loss after 1841993700 batches: 0.0431
trigger times: 3
Loss after 1842124800 batches: 0.0420
trigger times: 4
Loss after 1842255900 batches: 0.0411
trigger times: 5
Loss after 1842387000 batches: 0.0410
trigger times: 6
Loss after 1842518100 batches: 0.0402
trigger times: 7
Loss after 1842649200 batches: 0.0396
trigger times: 8
Loss after 1842780300 batches: 0.0391
trigger times: 9
Loss after 1842911400 batches: 0.0387
trigger times: 10
Loss after 1843042500 batches: 0.0375
trigger times: 11
Loss after 1843173600 batches: 0.0377
trigger times: 12
Loss after 1843304700 batches: 0.0375
trigger times: 13
Loss after 1843435800 batches: 0.0371
trigger times: 14
Loss after 1843566900 batches: 0.0366
trigger times: 15
Loss after 1843698000 batches: 0.0361
trigger times: 16
Loss after 1843829100 batches: 0.0360
trigger times: 17
Loss after 1843960200 batches: 0.0354
trigger times: 18
Loss after 1844091300 batches: 0.0352
trigger times: 0
Loss after 1844222400 batches: 0.0351
trigger times: 1
Loss after 1844353500 batches: 0.0340
trigger times: 2
Loss after 1844484600 batches: 0.0341
trigger times: 3
Loss after 1844615700 batches: 0.0343
trigger times: 4
Loss after 1844746800 batches: 0.0337
trigger times: 5
Loss after 1844877900 batches: 0.0330
trigger times: 6
Loss after 1845009000 batches: 0.0333
trigger times: 7
Loss after 1845140100 batches: 0.0331
trigger times: 8
Loss after 1845271200 batches: 0.0327
trigger times: 9
Loss after 1845402300 batches: 0.0325
trigger times: 10
Loss after 1845533400 batches: 0.0322
trigger times: 11
Loss after 1845664500 batches: 0.0317
trigger times: 12
Loss after 1845795600 batches: 0.0315
trigger times: 13
Loss after 1845926700 batches: 0.0318
trigger times: 14
Loss after 1846057800 batches: 0.0318
trigger times: 15
Loss after 1846188900 batches: 0.0310
trigger times: 16
Loss after 1846320000 batches: 0.0308
trigger times: 17
Loss after 1846451100 batches: 0.0305
trigger times: 18
Loss after 1846582200 batches: 0.0306
trigger times: 19
Loss after 1846713300 batches: 0.0309
trigger times: 20
Early stopping!
Start to test process.
Loss after 1846844400 batches: 0.0307
Time to train on one home:  411.13730549812317
trigger times: 0
Loss after 1846973040 batches: 0.1430
trigger times: 0
Loss after 1847101680 batches: 0.0453
trigger times: 1
Loss after 1847230320 batches: 0.0330
trigger times: 0
Loss after 1847358960 batches: 0.0288
trigger times: 1
Loss after 1847487600 batches: 0.0263
trigger times: 0
Loss after 1847616240 batches: 0.0244
trigger times: 1
Loss after 1847744880 batches: 0.0231
trigger times: 2
Loss after 1847873520 batches: 0.0218
trigger times: 3
Loss after 1848002160 batches: 0.0214
trigger times: 4
Loss after 1848130800 batches: 0.0205
trigger times: 5
Loss after 1848259440 batches: 0.0199
trigger times: 6
Loss after 1848388080 batches: 0.0192
trigger times: 7
Loss after 1848516720 batches: 0.0188
trigger times: 8
Loss after 1848645360 batches: 0.0182
trigger times: 9
Loss after 1848774000 batches: 0.0179
trigger times: 10
Loss after 1848902640 batches: 0.0181
trigger times: 11
Loss after 1849031280 batches: 0.0174
trigger times: 12
Loss after 1849159920 batches: 0.0173
trigger times: 13
Loss after 1849288560 batches: 0.0170
trigger times: 14
Loss after 1849417200 batches: 0.0167
trigger times: 0
Loss after 1849545840 batches: 0.0165
trigger times: 1
Loss after 1849674480 batches: 0.0163
trigger times: 2
Loss after 1849803120 batches: 0.0157
trigger times: 3
Loss after 1849931760 batches: 0.0160
trigger times: 4
Loss after 1850060400 batches: 0.0156
trigger times: 5
Loss after 1850189040 batches: 0.0158
trigger times: 6
Loss after 1850317680 batches: 0.0155
trigger times: 7
Loss after 1850446320 batches: 0.0152
trigger times: 8
Loss after 1850574960 batches: 0.0152
trigger times: 9
Loss after 1850703600 batches: 0.0152
trigger times: 10
Loss after 1850832240 batches: 0.0151
trigger times: 11
Loss after 1850960880 batches: 0.0146
trigger times: 0
Loss after 1851089520 batches: 0.0148
trigger times: 1
Loss after 1851218160 batches: 0.0145
trigger times: 2
Loss after 1851346800 batches: 0.0143
trigger times: 3
Loss after 1851475440 batches: 0.0147
trigger times: 0
Loss after 1851604080 batches: 0.0146
trigger times: 1
Loss after 1851732720 batches: 0.0140
trigger times: 0
Loss after 1851861360 batches: 0.0142
trigger times: 1
Loss after 1851990000 batches: 0.0140
trigger times: 2
Loss after 1852118640 batches: 0.0138
trigger times: 3
Loss after 1852247280 batches: 0.0139
trigger times: 4
Loss after 1852375920 batches: 0.0137
trigger times: 5
Loss after 1852504560 batches: 0.0135
trigger times: 6
Loss after 1852633200 batches: 0.0133
trigger times: 7
Loss after 1852761840 batches: 0.0133
trigger times: 8
Loss after 1852890480 batches: 0.0138
trigger times: 0
Loss after 1853019120 batches: 0.0135
trigger times: 1
Loss after 1853147760 batches: 0.0132
trigger times: 2
Loss after 1853276400 batches: 0.0132
trigger times: 3
Loss after 1853405040 batches: 0.0131
trigger times: 4
Loss after 1853533680 batches: 0.0130
trigger times: 5
Loss after 1853662320 batches: 0.0132
trigger times: 6
Loss after 1853790960 batches: 0.0128
trigger times: 7
Loss after 1853919600 batches: 0.0131
trigger times: 0
Loss after 1854048240 batches: 0.0130
trigger times: 0
Loss after 1854176880 batches: 0.0128
trigger times: 0
Loss after 1854305520 batches: 0.0128
trigger times: 1
Loss after 1854434160 batches: 0.0127
trigger times: 2
Loss after 1854562800 batches: 0.0125
trigger times: 3
Loss after 1854691440 batches: 0.0127
trigger times: 4
Loss after 1854820080 batches: 0.0125
trigger times: 5
Loss after 1854948720 batches: 0.0129
trigger times: 6
Loss after 1855077360 batches: 0.0122
trigger times: 7
Loss after 1855206000 batches: 0.0121
trigger times: 8
Loss after 1855334640 batches: 0.0120
trigger times: 9
Loss after 1855463280 batches: 0.0119
trigger times: 10
Loss after 1855591920 batches: 0.0119
trigger times: 11
Loss after 1855720560 batches: 0.0119
trigger times: 12
Loss after 1855849200 batches: 0.0120
trigger times: 13
Loss after 1855977840 batches: 0.0123
trigger times: 14
Loss after 1856106480 batches: 0.0120
trigger times: 15
Loss after 1856235120 batches: 0.0119
trigger times: 16
Loss after 1856363760 batches: 0.0118
trigger times: 17
Loss after 1856492400 batches: 0.0118
trigger times: 18
Loss after 1856621040 batches: 0.0119
trigger times: 19
Loss after 1856749680 batches: 0.0116
trigger times: 20
Early stopping!
Start to test process.
Loss after 1856878320 batches: 0.0118
Time to train on one home:  568.0950853824615
trigger times: 0
Loss after 1857009420 batches: 0.4635
trigger times: 0
Loss after 1857140520 batches: 0.1441
trigger times: 0
Loss after 1857271620 batches: 0.0973
trigger times: 1
Loss after 1857402720 batches: 0.0809
trigger times: 2
Loss after 1857533820 batches: 0.0719
trigger times: 3
Loss after 1857664920 batches: 0.0652
trigger times: 4
Loss after 1857796020 batches: 0.0610
trigger times: 0
Loss after 1857927120 batches: 0.0574
trigger times: 1
Loss after 1858058220 batches: 0.0550
trigger times: 2
Loss after 1858189320 batches: 0.0527
trigger times: 3
Loss after 1858320420 batches: 0.0508
trigger times: 4
Loss after 1858451520 batches: 0.0488
trigger times: 5
Loss after 1858582620 batches: 0.0473
trigger times: 6
Loss after 1858713720 batches: 0.0463
trigger times: 7
Loss after 1858844820 batches: 0.0452
trigger times: 8
Loss after 1858975920 batches: 0.0443
trigger times: 9
Loss after 1859107020 batches: 0.0434
trigger times: 10
Loss after 1859238120 batches: 0.0426
trigger times: 11
Loss after 1859369220 batches: 0.0419
trigger times: 12
Loss after 1859500320 batches: 0.0413
trigger times: 13
Loss after 1859631420 batches: 0.0407
trigger times: 14
Loss after 1859762520 batches: 0.0401
trigger times: 15
Loss after 1859893620 batches: 0.0395
trigger times: 16
Loss after 1860024720 batches: 0.0391
trigger times: 17
Loss after 1860155820 batches: 0.0385
trigger times: 18
Loss after 1860286920 batches: 0.0384
trigger times: 19
Loss after 1860418020 batches: 0.0375
trigger times: 20
Early stopping!
Start to test process.
Loss after 1860549120 batches: 0.0374
Time to train on one home:  215.57965779304504
trigger times: 0
Loss after 1860680220 batches: 0.3964
trigger times: 0
Loss after 1860811320 batches: 0.1395
trigger times: 0
Loss after 1860942420 batches: 0.0923
trigger times: 1
Loss after 1861073520 batches: 0.0722
trigger times: 2
Loss after 1861204620 batches: 0.0596
trigger times: 3
Loss after 1861335720 batches: 0.0549
trigger times: 4
Loss after 1861466820 batches: 0.0514
trigger times: 5
Loss after 1861597920 batches: 0.0512
trigger times: 6
Loss after 1861729020 batches: 0.0476
trigger times: 0
Loss after 1861860120 batches: 0.0440
trigger times: 0
Loss after 1861991220 batches: 0.0440
trigger times: 1
Loss after 1862122320 batches: 0.0420
trigger times: 2
Loss after 1862253420 batches: 0.0396
trigger times: 3
Loss after 1862384520 batches: 0.0384
trigger times: 0
Loss after 1862515620 batches: 0.0417
trigger times: 1
Loss after 1862646720 batches: 0.0383
trigger times: 2
Loss after 1862777820 batches: 0.0369
trigger times: 3
Loss after 1862908920 batches: 0.0385
trigger times: 4
Loss after 1863040020 batches: 0.0376
trigger times: 5
Loss after 1863171120 batches: 0.0351
trigger times: 6
Loss after 1863302220 batches: 0.0352
trigger times: 0
Loss after 1863433320 batches: 0.0370
trigger times: 1
Loss after 1863564420 batches: 0.0341
trigger times: 2
Loss after 1863695520 batches: 0.0366
trigger times: 3
Loss after 1863826620 batches: 0.0342
trigger times: 4
Loss after 1863957720 batches: 0.0338
trigger times: 5
Loss after 1864088820 batches: 0.0322
trigger times: 6
Loss after 1864219920 batches: 0.0335
trigger times: 0
Loss after 1864351020 batches: 0.0317
trigger times: 1
Loss after 1864482120 batches: 0.0318
trigger times: 2
Loss after 1864613220 batches: 0.0312
trigger times: 3
Loss after 1864744320 batches: 0.0326
trigger times: 0
Loss after 1864875420 batches: 0.0311
trigger times: 1
Loss after 1865006520 batches: 0.0321
trigger times: 2
Loss after 1865137620 batches: 0.0318
trigger times: 0
Loss after 1865268720 batches: 0.0300
trigger times: 1
Loss after 1865399820 batches: 0.0297
trigger times: 2
Loss after 1865530920 batches: 0.0306
trigger times: 3
Loss after 1865662020 batches: 0.0303
trigger times: 4
Loss after 1865793120 batches: 0.0299
trigger times: 5
Loss after 1865924220 batches: 0.0289
trigger times: 6
Loss after 1866055320 batches: 0.0289
trigger times: 7
Loss after 1866186420 batches: 0.0305
trigger times: 8
Loss after 1866317520 batches: 0.0290
trigger times: 9
Loss after 1866448620 batches: 0.0279
trigger times: 10
Loss after 1866579720 batches: 0.0295
trigger times: 11
Loss after 1866710820 batches: 0.0288
trigger times: 12
Loss after 1866841920 batches: 0.0293
trigger times: 13
Loss after 1866973020 batches: 0.0286
trigger times: 14
Loss after 1867104120 batches: 0.0278
trigger times: 15
Loss after 1867235220 batches: 0.0267
trigger times: 16
Loss after 1867366320 batches: 0.0273
trigger times: 17
Loss after 1867497420 batches: 0.0276
trigger times: 18
Loss after 1867628520 batches: 0.0268
trigger times: 19
Loss after 1867759620 batches: 0.0266
trigger times: 20
Early stopping!
Start to test process.
Loss after 1867890720 batches: 0.0267
Time to train on one home:  418.8522071838379
trigger times: 0
Loss after 1868021820 batches: 0.0990
trigger times: 0
Loss after 1868152920 batches: 0.0313
trigger times: 1
Loss after 1868284020 batches: 0.0230
trigger times: 2
Loss after 1868415120 batches: 0.0201
trigger times: 0
Loss after 1868546220 batches: 0.0183
trigger times: 1
Loss after 1868677320 batches: 0.0170
trigger times: 2
Loss after 1868808420 batches: 0.0165
trigger times: 3
Loss after 1868939520 batches: 0.0154
trigger times: 4
Loss after 1869070620 batches: 0.0149
trigger times: 0
Loss after 1869201720 batches: 0.0146
trigger times: 1
Loss after 1869332820 batches: 0.0140
trigger times: 2
Loss after 1869463920 batches: 0.0135
trigger times: 0
Loss after 1869595020 batches: 0.0133
trigger times: 1
Loss after 1869726120 batches: 0.0128
trigger times: 2
Loss after 1869857220 batches: 0.0125
trigger times: 3
Loss after 1869988320 batches: 0.0127
trigger times: 4
Loss after 1870119420 batches: 0.0124
trigger times: 5
Loss after 1870250520 batches: 0.0121
trigger times: 0
Loss after 1870381620 batches: 0.0117
trigger times: 1
Loss after 1870512720 batches: 0.0118
trigger times: 2
Loss after 1870643820 batches: 0.0114
trigger times: 3
Loss after 1870774920 batches: 0.0113
trigger times: 0
Loss after 1870906020 batches: 0.0116
trigger times: 1
Loss after 1871037120 batches: 0.0113
trigger times: 2
Loss after 1871168220 batches: 0.0112
trigger times: 3
Loss after 1871299320 batches: 0.0111
trigger times: 4
Loss after 1871430420 batches: 0.0107
trigger times: 5
Loss after 1871561520 batches: 0.0109
trigger times: 0
Loss after 1871692620 batches: 0.0105
trigger times: 1
Loss after 1871823720 batches: 0.0106
trigger times: 2
Loss after 1871954820 batches: 0.0105
trigger times: 3
Loss after 1872085920 batches: 0.0103
trigger times: 4
Loss after 1872217020 batches: 0.0105
trigger times: 5
Loss after 1872348120 batches: 0.0103
trigger times: 6
Loss after 1872479220 batches: 0.0101
trigger times: 7
Loss after 1872610320 batches: 0.0101
trigger times: 8
Loss after 1872741420 batches: 0.0103
trigger times: 9
Loss after 1872872520 batches: 0.0098
trigger times: 10
Loss after 1873003620 batches: 0.0101
trigger times: 11
Loss after 1873134720 batches: 0.0097
trigger times: 12
Loss after 1873265820 batches: 0.0099
trigger times: 13
Loss after 1873396920 batches: 0.0098
trigger times: 14
Loss after 1873528020 batches: 0.0095
trigger times: 15
Loss after 1873659120 batches: 0.0096
trigger times: 16
Loss after 1873790220 batches: 0.0094
trigger times: 17
Loss after 1873921320 batches: 0.0093
trigger times: 18
Loss after 1874052420 batches: 0.0092
trigger times: 19
Loss after 1874183520 batches: 0.0096
trigger times: 20
Early stopping!
Start to test process.
Loss after 1874314620 batches: 0.0091
Time to train on one home:  369.78381514549255
trigger times: 0
Loss after 1874445720 batches: 0.1771
trigger times: 0
Loss after 1874576820 batches: 0.0467
trigger times: 0
Loss after 1874707920 batches: 0.0327
trigger times: 0
Loss after 1874839020 batches: 0.0278
trigger times: 1
Loss after 1874970120 batches: 0.0252
trigger times: 2
Loss after 1875101220 batches: 0.0235
trigger times: 0
Loss after 1875232320 batches: 0.0219
trigger times: 1
Loss after 1875363420 batches: 0.0210
trigger times: 2
Loss after 1875494520 batches: 0.0197
trigger times: 3
Loss after 1875625620 batches: 0.0192
trigger times: 4
Loss after 1875756720 batches: 0.0188
trigger times: 5
Loss after 1875887820 batches: 0.0178
trigger times: 6
Loss after 1876018920 batches: 0.0177
trigger times: 7
Loss after 1876150020 batches: 0.0173
trigger times: 8
Loss after 1876281120 batches: 0.0169
trigger times: 9
Loss after 1876412220 batches: 0.0165
trigger times: 10
Loss after 1876543320 batches: 0.0160
trigger times: 11
Loss after 1876674420 batches: 0.0158
trigger times: 12
Loss after 1876805520 batches: 0.0156
trigger times: 13
Loss after 1876936620 batches: 0.0152
trigger times: 14
Loss after 1877067720 batches: 0.0151
trigger times: 15
Loss after 1877198820 batches: 0.0151
trigger times: 16
Loss after 1877329920 batches: 0.0147
trigger times: 17
Loss after 1877461020 batches: 0.0144
trigger times: 18
Loss after 1877592120 batches: 0.0140
trigger times: 19
Loss after 1877723220 batches: 0.0142
trigger times: 20
Early stopping!
Start to test process.
Loss after 1877854320 batches: 0.0139
Time to train on one home:  208.54736733436584
trigger times: 0
Loss after 1877932920 batches: 0.3895
trigger times: 1
Loss after 1878011520 batches: 0.1115
trigger times: 2
Loss after 1878090120 batches: 0.0653
trigger times: 3
Loss after 1878168720 batches: 0.0518
trigger times: 0
Loss after 1878247320 batches: 0.0454
trigger times: 1
Loss after 1878325920 batches: 0.0401
trigger times: 0
Loss after 1878404520 batches: 0.0375
trigger times: 1
Loss after 1878483120 batches: 0.0362
trigger times: 0
Loss after 1878561720 batches: 0.0345
trigger times: 1
Loss after 1878640320 batches: 0.0330
trigger times: 2
Loss after 1878718920 batches: 0.0313
trigger times: 3
Loss after 1878797520 batches: 0.0300
trigger times: 4
Loss after 1878876120 batches: 0.0298
trigger times: 0
Loss after 1878954720 batches: 0.0294
trigger times: 1
Loss after 1879033320 batches: 0.0282
trigger times: 2
Loss after 1879111920 batches: 0.0280
trigger times: 3
Loss after 1879190520 batches: 0.0273
trigger times: 0
Loss after 1879269120 batches: 0.0265
trigger times: 1
Loss after 1879347720 batches: 0.0262
trigger times: 2
Loss after 1879426320 batches: 0.0251
trigger times: 3
Loss after 1879504920 batches: 0.0251
trigger times: 4
Loss after 1879583520 batches: 0.0248
trigger times: 5
Loss after 1879662120 batches: 0.0253
trigger times: 6
Loss after 1879740720 batches: 0.0245
trigger times: 7
Loss after 1879819320 batches: 0.0240
trigger times: 8
Loss after 1879897920 batches: 0.0245
trigger times: 9
Loss after 1879976520 batches: 0.0234
trigger times: 10
Loss after 1880055120 batches: 0.0236
trigger times: 11
Loss after 1880133720 batches: 0.0230
trigger times: 12
Loss after 1880212320 batches: 0.0228
trigger times: 13
Loss after 1880290920 batches: 0.0228
trigger times: 14
Loss after 1880369520 batches: 0.0222
trigger times: 15
Loss after 1880448120 batches: 0.0216
trigger times: 16
Loss after 1880526720 batches: 0.0225
trigger times: 17
Loss after 1880605320 batches: 0.0217
trigger times: 18
Loss after 1880683920 batches: 0.0218
trigger times: 19
Loss after 1880762520 batches: 0.0218
trigger times: 20
Early stopping!
Start to test process.
Loss after 1880841120 batches: 0.0210
Time to train on one home:  190.22125029563904
trigger times: 0
Loss after 1880972220 batches: 0.1195
trigger times: 0
Loss after 1881103320 batches: 0.0345
trigger times: 0
Loss after 1881234420 batches: 0.0267
trigger times: 1
Loss after 1881365520 batches: 0.0229
trigger times: 2
Loss after 1881496620 batches: 0.0207
trigger times: 3
Loss after 1881627720 batches: 0.0197
trigger times: 4
Loss after 1881758820 batches: 0.0183
trigger times: 5
Loss after 1881889920 batches: 0.0176
trigger times: 0
Loss after 1882021020 batches: 0.0169
trigger times: 1
Loss after 1882152120 batches: 0.0164
trigger times: 2
Loss after 1882283220 batches: 0.0162
trigger times: 3
Loss after 1882414320 batches: 0.0153
trigger times: 4
Loss after 1882545420 batches: 0.0150
trigger times: 0
Loss after 1882676520 batches: 0.0147
trigger times: 1
Loss after 1882807620 batches: 0.0146
trigger times: 2
Loss after 1882938720 batches: 0.0144
trigger times: 3
Loss after 1883069820 batches: 0.0140
trigger times: 4
Loss after 1883200920 batches: 0.0137
trigger times: 5
Loss after 1883332020 batches: 0.0135
trigger times: 6
Loss after 1883463120 batches: 0.0135
trigger times: 7
Loss after 1883594220 batches: 0.0131
trigger times: 8
Loss after 1883725320 batches: 0.0127
trigger times: 9
Loss after 1883856420 batches: 0.0127
trigger times: 10
Loss after 1883987520 batches: 0.0127
trigger times: 0
Loss after 1884118620 batches: 0.0126
trigger times: 1
Loss after 1884249720 batches: 0.0125
trigger times: 2
Loss after 1884380820 batches: 0.0122
trigger times: 3
Loss after 1884511920 batches: 0.0124
trigger times: 4
Loss after 1884643020 batches: 0.0119
trigger times: 5
Loss after 1884774120 batches: 0.0119
trigger times: 6
Loss after 1884905220 batches: 0.0118
trigger times: 7
Loss after 1885036320 batches: 0.0117
trigger times: 8
Loss after 1885167420 batches: 0.0117
trigger times: 9
Loss after 1885298520 batches: 0.0114
trigger times: 10
Loss after 1885429620 batches: 0.0113
trigger times: 11
Loss after 1885560720 batches: 0.0113
trigger times: 12
Loss after 1885691820 batches: 0.0113
trigger times: 13
Loss after 1885822920 batches: 0.0112
trigger times: 0
Loss after 1885954020 batches: 0.0111
trigger times: 1
Loss after 1886085120 batches: 0.0109
trigger times: 2
Loss after 1886216220 batches: 0.0111
trigger times: 3
Loss after 1886347320 batches: 0.0107
trigger times: 4
Loss after 1886478420 batches: 0.0104
trigger times: 5
Loss after 1886609520 batches: 0.0107
trigger times: 6
Loss after 1886740620 batches: 0.0107
trigger times: 7
Loss after 1886871720 batches: 0.0104
trigger times: 8
Loss after 1887002820 batches: 0.0103
trigger times: 9
Loss after 1887133920 batches: 0.0103
trigger times: 10
Loss after 1887265020 batches: 0.0102
trigger times: 11
Loss after 1887396120 batches: 0.0102
trigger times: 12
Loss after 1887527220 batches: 0.0100
trigger times: 0
Loss after 1887658320 batches: 0.0100
trigger times: 1
Loss after 1887789420 batches: 0.0101
trigger times: 2
Loss after 1887920520 batches: 0.0102
trigger times: 3
Loss after 1888051620 batches: 0.0099
trigger times: 4
Loss after 1888182720 batches: 0.0098
trigger times: 0
Loss after 1888313820 batches: 0.0098
trigger times: 1
Loss after 1888444920 batches: 0.0098
trigger times: 2
Loss after 1888576020 batches: 0.0097
trigger times: 3
Loss after 1888707120 batches: 0.0097
trigger times: 4
Loss after 1888838220 batches: 0.0097
trigger times: 5
Loss after 1888969320 batches: 0.0097
trigger times: 6
Loss after 1889100420 batches: 0.0097
trigger times: 7
Loss after 1889231520 batches: 0.0094
trigger times: 8
Loss after 1889362620 batches: 0.0095
trigger times: 9
Loss after 1889493720 batches: 0.0093
trigger times: 10
Loss after 1889624820 batches: 0.0094
trigger times: 11
Loss after 1889755920 batches: 0.0093
trigger times: 0
Loss after 1889887020 batches: 0.0094
trigger times: 1
Loss after 1890018120 batches: 0.0092
trigger times: 2
Loss after 1890149220 batches: 0.0092
trigger times: 3
Loss after 1890280320 batches: 0.0091
trigger times: 4
Loss after 1890411420 batches: 0.0090
trigger times: 5
Loss after 1890542520 batches: 0.0091
trigger times: 6
Loss after 1890673620 batches: 0.0090
trigger times: 7
Loss after 1890804720 batches: 0.0088
trigger times: 0
Loss after 1890935820 batches: 0.0089
trigger times: 1
Loss after 1891066920 batches: 0.0087
trigger times: 2
Loss after 1891198020 batches: 0.0089
trigger times: 3
Loss after 1891329120 batches: 0.0088
trigger times: 4
Loss after 1891460220 batches: 0.0087
trigger times: 5
Loss after 1891591320 batches: 0.0087
trigger times: 6
Loss after 1891722420 batches: 0.0088
trigger times: 7
Loss after 1891853520 batches: 0.0087
trigger times: 8
Loss after 1891984620 batches: 0.0085
trigger times: 9
Loss after 1892115720 batches: 0.0086
trigger times: 10
Loss after 1892246820 batches: 0.0087
trigger times: 11
Loss after 1892377920 batches: 0.0085
trigger times: 12
Loss after 1892509020 batches: 0.0086
trigger times: 13
Loss after 1892640120 batches: 0.0084
trigger times: 14
Loss after 1892771220 batches: 0.0085
trigger times: 15
Loss after 1892902320 batches: 0.0085
trigger times: 16
Loss after 1893033420 batches: 0.0086
trigger times: 17
Loss after 1893164520 batches: 0.0085
trigger times: 18
Loss after 1893295620 batches: 0.0083
trigger times: 19
Loss after 1893426720 batches: 0.0085
trigger times: 20
Early stopping!
Start to test process.
Loss after 1893557820 batches: 0.0083
Time to train on one home:  715.3589396476746
trigger times: 0
Loss after 1893688920 batches: 0.1466
trigger times: 1
Loss after 1893820020 batches: 0.0444
trigger times: 0
Loss after 1893951120 batches: 0.0330
trigger times: 1
Loss after 1894082220 batches: 0.0287
trigger times: 0
Loss after 1894213320 batches: 0.0259
trigger times: 1
Loss after 1894344420 batches: 0.0243
trigger times: 2
Loss after 1894475520 batches: 0.0237
trigger times: 3
Loss after 1894606620 batches: 0.0219
trigger times: 4
Loss after 1894737720 batches: 0.0213
trigger times: 0
Loss after 1894868820 batches: 0.0207
trigger times: 0
Loss after 1894999920 batches: 0.0201
trigger times: 1
Loss after 1895131020 batches: 0.0196
trigger times: 0
Loss after 1895262120 batches: 0.0188
trigger times: 0
Loss after 1895393220 batches: 0.0187
trigger times: 1
Loss after 1895524320 batches: 0.0181
trigger times: 2
Loss after 1895655420 batches: 0.0181
trigger times: 3
Loss after 1895786520 batches: 0.0179
trigger times: 4
Loss after 1895917620 batches: 0.0172
trigger times: 5
Loss after 1896048720 batches: 0.0172
trigger times: 6
Loss after 1896179820 batches: 0.0165
trigger times: 7
Loss after 1896310920 batches: 0.0164
trigger times: 8
Loss after 1896442020 batches: 0.0165
trigger times: 9
Loss after 1896573120 batches: 0.0164
trigger times: 10
Loss after 1896704220 batches: 0.0158
trigger times: 11
Loss after 1896835320 batches: 0.0158
trigger times: 12
Loss after 1896966420 batches: 0.0158
trigger times: 13
Loss after 1897097520 batches: 0.0156
trigger times: 14
Loss after 1897228620 batches: 0.0154
trigger times: 15
Loss after 1897359720 batches: 0.0157
trigger times: 16
Loss after 1897490820 batches: 0.0152
trigger times: 17
Loss after 1897621920 batches: 0.0149
trigger times: 18
Loss after 1897753020 batches: 0.0148
trigger times: 19
Loss after 1897884120 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 1898015220 batches: 0.0146
Time to train on one home:  259.41282749176025
trigger times: 0
Loss after 1898146320 batches: 0.2412
trigger times: 0
Loss after 1898277420 batches: 0.0719
trigger times: 0
Loss after 1898408520 batches: 0.0498
trigger times: 0
Loss after 1898539620 batches: 0.0429
trigger times: 0
Loss after 1898670720 batches: 0.0385
trigger times: 1
Loss after 1898801820 batches: 0.0351
trigger times: 2
Loss after 1898932920 batches: 0.0336
trigger times: 0
Loss after 1899064020 batches: 0.0314
trigger times: 0
Loss after 1899195120 batches: 0.0307
trigger times: 1
Loss after 1899326220 batches: 0.0294
trigger times: 2
Loss after 1899457320 batches: 0.0289
trigger times: 3
Loss after 1899588420 batches: 0.0273
trigger times: 4
Loss after 1899719520 batches: 0.0269
trigger times: 0
Loss after 1899850620 batches: 0.0277
trigger times: 0
Loss after 1899981720 batches: 0.0257
trigger times: 1
Loss after 1900112820 batches: 0.0250
trigger times: 0
Loss after 1900243920 batches: 0.0253
trigger times: 1
Loss after 1900375020 batches: 0.0254
trigger times: 2
Loss after 1900506120 batches: 0.0239
trigger times: 3
Loss after 1900637220 batches: 0.0234
trigger times: 4
Loss after 1900768320 batches: 0.0236
trigger times: 5
Loss after 1900899420 batches: 0.0227
trigger times: 6
Loss after 1901030520 batches: 0.0223
trigger times: 7
Loss after 1901161620 batches: 0.0224
trigger times: 8
Loss after 1901292720 batches: 0.0226
trigger times: 9
Loss after 1901423820 batches: 0.0231
trigger times: 10
Loss after 1901554920 batches: 0.0221
trigger times: 11
Loss after 1901686020 batches: 0.0220
trigger times: 0
Loss after 1901817120 batches: 0.0226
trigger times: 1
Loss after 1901948220 batches: 0.0215
trigger times: 2
Loss after 1902079320 batches: 0.0215
trigger times: 3
Loss after 1902210420 batches: 0.0220
trigger times: 4
Loss after 1902341520 batches: 0.0206
trigger times: 0
Loss after 1902472620 batches: 0.0207
trigger times: 1
Loss after 1902603720 batches: 0.0220
trigger times: 2
Loss after 1902734820 batches: 0.0213
trigger times: 3
Loss after 1902865920 batches: 0.0206
trigger times: 4
Loss after 1902997020 batches: 0.0204
trigger times: 5
Loss after 1903128120 batches: 0.0209
trigger times: 6
Loss after 1903259220 batches: 0.0195
trigger times: 0
Loss after 1903390320 batches: 0.0197
trigger times: 1
Loss after 1903521420 batches: 0.0202
trigger times: 2
Loss after 1903652520 batches: 0.0191
trigger times: 3
Loss after 1903783620 batches: 0.0194
trigger times: 4
Loss after 1903914720 batches: 0.0190
trigger times: 5
Loss after 1904045820 batches: 0.0190
trigger times: 6
Loss after 1904176920 batches: 0.0191
trigger times: 7
Loss after 1904308020 batches: 0.0191
trigger times: 8
Loss after 1904439120 batches: 0.0190
trigger times: 9
Loss after 1904570220 batches: 0.0190
trigger times: 10
Loss after 1904701320 batches: 0.0188
trigger times: 11
Loss after 1904832420 batches: 0.0185
trigger times: 12
Loss after 1904963520 batches: 0.0188
trigger times: 13
Loss after 1905094620 batches: 0.0187
trigger times: 14
Loss after 1905225720 batches: 0.0183
trigger times: 15
Loss after 1905356820 batches: 0.0183
trigger times: 16
Loss after 1905487920 batches: 0.0187
trigger times: 17
Loss after 1905619020 batches: 0.0178
trigger times: 18
Loss after 1905750120 batches: 0.0189
trigger times: 19
Loss after 1905881220 batches: 0.0181
trigger times: 20
Early stopping!
Start to test process.
Loss after 1906012320 batches: 0.0192
Time to train on one home:  455.8487284183502
trigger times: 0
Loss after 1906143420 batches: 0.3418
trigger times: 0
Loss after 1906274520 batches: 0.1244
trigger times: 0
Loss after 1906405620 batches: 0.0847
trigger times: 1
Loss after 1906536720 batches: 0.0709
trigger times: 2
Loss after 1906667820 batches: 0.0645
trigger times: 3
Loss after 1906798920 batches: 0.0581
trigger times: 4
Loss after 1906930020 batches: 0.0557
trigger times: 5
Loss after 1907061120 batches: 0.0520
trigger times: 6
Loss after 1907192220 batches: 0.0503
trigger times: 7
Loss after 1907323320 batches: 0.0477
trigger times: 8
Loss after 1907454420 batches: 0.0464
trigger times: 9
Loss after 1907585520 batches: 0.0451
trigger times: 10
Loss after 1907716620 batches: 0.0436
trigger times: 11
Loss after 1907847720 batches: 0.0430
trigger times: 12
Loss after 1907978820 batches: 0.0412
trigger times: 13
Loss after 1908109920 batches: 0.0413
trigger times: 14
Loss after 1908241020 batches: 0.0405
trigger times: 15
Loss after 1908372120 batches: 0.0400
trigger times: 16
Loss after 1908503220 batches: 0.0394
trigger times: 17
Loss after 1908634320 batches: 0.0385
trigger times: 18
Loss after 1908765420 batches: 0.0373
trigger times: 19
Loss after 1908896520 batches: 0.0374
trigger times: 20
Early stopping!
Start to test process.
Loss after 1909027620 batches: 0.0366
Time to train on one home:  180.10988569259644
trigger times: 0
Loss after 1909158720 batches: 0.2971
trigger times: 0
Loss after 1909289820 batches: 0.0865
trigger times: 0
Loss after 1909420920 batches: 0.0588
trigger times: 0
Loss after 1909552020 batches: 0.0503
trigger times: 0
Loss after 1909683120 batches: 0.0452
trigger times: 1
Loss after 1909814220 batches: 0.0410
trigger times: 2
Loss after 1909945320 batches: 0.0380
trigger times: 3
Loss after 1910076420 batches: 0.0370
trigger times: 0
Loss after 1910207520 batches: 0.0346
trigger times: 1
Loss after 1910338620 batches: 0.0337
trigger times: 2
Loss after 1910469720 batches: 0.0323
trigger times: 3
Loss after 1910600820 batches: 0.0315
trigger times: 0
Loss after 1910731920 batches: 0.0307
trigger times: 0
Loss after 1910863020 batches: 0.0304
trigger times: 1
Loss after 1910994120 batches: 0.0305
trigger times: 2
Loss after 1911125220 batches: 0.0292
trigger times: 3
Loss after 1911256320 batches: 0.0291
trigger times: 4
Loss after 1911387420 batches: 0.0278
trigger times: 5
Loss after 1911518520 batches: 0.0273
trigger times: 6
Loss after 1911649620 batches: 0.0268
trigger times: 7
Loss after 1911780720 batches: 0.0264
trigger times: 8
Loss after 1911911820 batches: 0.0264
trigger times: 9
Loss after 1912042920 batches: 0.0264
trigger times: 10
Loss after 1912174020 batches: 0.0254
trigger times: 11
Loss after 1912305120 batches: 0.0252
trigger times: 12
Loss after 1912436220 batches: 0.0251
trigger times: 13
Loss after 1912567320 batches: 0.0242
trigger times: 14
Loss after 1912698420 batches: 0.0244
trigger times: 15
Loss after 1912829520 batches: 0.0241
trigger times: 16
Loss after 1912960620 batches: 0.0241
trigger times: 17
Loss after 1913091720 batches: 0.0244
trigger times: 18
Loss after 1913222820 batches: 0.0236
trigger times: 19
Loss after 1913353920 batches: 0.0234
trigger times: 20
Early stopping!
Start to test process.
Loss after 1913485020 batches: 0.0238
Time to train on one home:  257.78906083106995
trigger times: 0
Loss after 1913616120 batches: 0.4257
trigger times: 0
Loss after 1913747220 batches: 0.1304
trigger times: 0
Loss after 1913878320 batches: 0.0803
trigger times: 1
Loss after 1914009420 batches: 0.0656
trigger times: 2
Loss after 1914140520 batches: 0.0580
trigger times: 3
Loss after 1914271620 batches: 0.0525
trigger times: 4
Loss after 1914402720 batches: 0.0492
trigger times: 5
Loss after 1914533820 batches: 0.0456
trigger times: 6
Loss after 1914664920 batches: 0.0438
trigger times: 7
Loss after 1914796020 batches: 0.0420
trigger times: 8
Loss after 1914927120 batches: 0.0405
trigger times: 9
Loss after 1915058220 batches: 0.0389
trigger times: 10
Loss after 1915189320 batches: 0.0384
trigger times: 11
Loss after 1915320420 batches: 0.0370
trigger times: 12
Loss after 1915451520 batches: 0.0361
trigger times: 13
Loss after 1915582620 batches: 0.0355
trigger times: 14
Loss after 1915713720 batches: 0.0345
trigger times: 15
Loss after 1915844820 batches: 0.0336
trigger times: 16
Loss after 1915975920 batches: 0.0334
trigger times: 17
Loss after 1916107020 batches: 0.0327
trigger times: 18
Loss after 1916238120 batches: 0.0318
trigger times: 19
Loss after 1916369220 batches: 0.0315
trigger times: 20
Early stopping!
Start to test process.
Loss after 1916500320 batches: 0.0311
Time to train on one home:  177.511412858963
trigger times: 0
Loss after 1916631420 batches: 0.4585
trigger times: 0
Loss after 1916762520 batches: 0.1344
trigger times: 0
Loss after 1916893620 batches: 0.0955
trigger times: 1
Loss after 1917024720 batches: 0.0808
trigger times: 0
Loss after 1917155820 batches: 0.0731
trigger times: 1
Loss after 1917286920 batches: 0.0663
trigger times: 0
Loss after 1917418020 batches: 0.0632
trigger times: 0
Loss after 1917549120 batches: 0.0587
trigger times: 0
Loss after 1917680220 batches: 0.0564
trigger times: 1
Loss after 1917811320 batches: 0.0542
trigger times: 2
Loss after 1917942420 batches: 0.0529
trigger times: 0
Loss after 1918073520 batches: 0.0503
trigger times: 1
Loss after 1918204620 batches: 0.0500
trigger times: 0
Loss after 1918335720 batches: 0.0491
trigger times: 0
Loss after 1918466820 batches: 0.0469
trigger times: 1
Loss after 1918597920 batches: 0.0462
trigger times: 2
Loss after 1918729020 batches: 0.0454
trigger times: 0
Loss after 1918860120 batches: 0.0442
trigger times: 0
Loss after 1918991220 batches: 0.0439
trigger times: 1
Loss after 1919122320 batches: 0.0431
trigger times: 2
Loss after 1919253420 batches: 0.0427
trigger times: 3
Loss after 1919384520 batches: 0.0418
trigger times: 0
Loss after 1919515620 batches: 0.0418
trigger times: 1
Loss after 1919646720 batches: 0.0408
trigger times: 2
Loss after 1919777820 batches: 0.0401
trigger times: 3
Loss after 1919908920 batches: 0.0396
trigger times: 0
Loss after 1920040020 batches: 0.0394
trigger times: 1
Loss after 1920171120 batches: 0.0388
trigger times: 2
Loss after 1920302220 batches: 0.0390
trigger times: 3
Loss after 1920433320 batches: 0.0383
trigger times: 0
Loss after 1920564420 batches: 0.0381
trigger times: 1
Loss after 1920695520 batches: 0.0376
trigger times: 2
Loss after 1920826620 batches: 0.0378
trigger times: 3
Loss after 1920957720 batches: 0.0369
trigger times: 4
Loss after 1921088820 batches: 0.0366
trigger times: 5
Loss after 1921219920 batches: 0.0365
trigger times: 6
Loss after 1921351020 batches: 0.0360
trigger times: 7
Loss after 1921482120 batches: 0.0369
trigger times: 8
Loss after 1921613220 batches: 0.0361
trigger times: 9
Loss after 1921744320 batches: 0.0352
trigger times: 10
Loss after 1921875420 batches: 0.0358
trigger times: 11
Loss after 1922006520 batches: 0.0343
trigger times: 12
Loss after 1922137620 batches: 0.0340
trigger times: 13
Loss after 1922268720 batches: 0.0344
trigger times: 0
Loss after 1922399820 batches: 0.0340
trigger times: 0
Loss after 1922530920 batches: 0.0336
trigger times: 0
Loss after 1922662020 batches: 0.0332
trigger times: 1
Loss after 1922793120 batches: 0.0335
trigger times: 2
Loss after 1922924220 batches: 0.0334
trigger times: 3
Loss after 1923055320 batches: 0.0332
trigger times: 4
Loss after 1923186420 batches: 0.0327
trigger times: 5
Loss after 1923317520 batches: 0.0327
trigger times: 0
Loss after 1923448620 batches: 0.0326
trigger times: 1
Loss after 1923579720 batches: 0.0320
trigger times: 2
Loss after 1923710820 batches: 0.0317
trigger times: 3
Loss after 1923841920 batches: 0.0320
trigger times: 0
Loss after 1923973020 batches: 0.0315
trigger times: 1
Loss after 1924104120 batches: 0.0315
trigger times: 2
Loss after 1924235220 batches: 0.0311
trigger times: 3
Loss after 1924366320 batches: 0.0312
trigger times: 4
Loss after 1924497420 batches: 0.0308
trigger times: 5
Loss after 1924628520 batches: 0.0306
trigger times: 6
Loss after 1924759620 batches: 0.0312
trigger times: 0
Loss after 1924890720 batches: 0.0306
trigger times: 1
Loss after 1925021820 batches: 0.0303
trigger times: 0
Loss after 1925152920 batches: 0.0305
trigger times: 1
Loss after 1925284020 batches: 0.0304
trigger times: 2
Loss after 1925415120 batches: 0.0303
trigger times: 3
Loss after 1925546220 batches: 0.0300
trigger times: 4
Loss after 1925677320 batches: 0.0299
trigger times: 5
Loss after 1925808420 batches: 0.0296
trigger times: 6
Loss after 1925939520 batches: 0.0297
trigger times: 0
Loss after 1926070620 batches: 0.0296
trigger times: 0
Loss after 1926201720 batches: 0.0291
trigger times: 1
Loss after 1926332820 batches: 0.0288
trigger times: 2
Loss after 1926463920 batches: 0.0289
trigger times: 3
Loss after 1926595020 batches: 0.0294
trigger times: 4
Loss after 1926726120 batches: 0.0287
trigger times: 5
Loss after 1926857220 batches: 0.0291
trigger times: 0
Loss after 1926988320 batches: 0.0285
trigger times: 1
Loss after 1927119420 batches: 0.0280
trigger times: 2
Loss after 1927250520 batches: 0.0286
trigger times: 0
Loss after 1927381620 batches: 0.0283
trigger times: 1
Loss after 1927512720 batches: 0.0280
trigger times: 0
Loss after 1927643820 batches: 0.0284
trigger times: 1
Loss after 1927774920 batches: 0.0281
trigger times: 2
Loss after 1927906020 batches: 0.0282
trigger times: 3
Loss after 1928037120 batches: 0.0282
trigger times: 4
Loss after 1928168220 batches: 0.0281
trigger times: 0
Loss after 1928299320 batches: 0.0273
trigger times: 0
Loss after 1928430420 batches: 0.0275
trigger times: 0
Loss after 1928561520 batches: 0.0273
trigger times: 1
Loss after 1928692620 batches: 0.0272
trigger times: 2
Loss after 1928823720 batches: 0.0269
trigger times: 0
Loss after 1928954820 batches: 0.0269
trigger times: 1
Loss after 1929085920 batches: 0.0279
trigger times: 2
Loss after 1929217020 batches: 0.0274
trigger times: 3
Loss after 1929348120 batches: 0.0266
trigger times: 4
Loss after 1929479220 batches: 0.0271
trigger times: 5
Loss after 1929610320 batches: 0.0271
trigger times: 6
Loss after 1929741420 batches: 0.0265
trigger times: 7
Loss after 1929872520 batches: 0.0267
trigger times: 8
Loss after 1930003620 batches: 0.0267
trigger times: 9
Loss after 1930134720 batches: 0.0263
trigger times: 10
Loss after 1930265820 batches: 0.0266
trigger times: 11
Loss after 1930396920 batches: 0.0266
trigger times: 12
Loss after 1930528020 batches: 0.0261
trigger times: 13
Loss after 1930659120 batches: 0.0262
trigger times: 14
Loss after 1930790220 batches: 0.0267
trigger times: 15
Loss after 1930921320 batches: 0.0266
trigger times: 16
Loss after 1931052420 batches: 0.0260
trigger times: 17
Loss after 1931183520 batches: 0.0260
trigger times: 18
Loss after 1931314620 batches: 0.0259
trigger times: 19
Loss after 1931445720 batches: 0.0260
trigger times: 20
Early stopping!
Start to test process.
Loss after 1931576820 batches: 0.0259
Time to train on one home:  844.3096749782562
trigger times: 0
Loss after 1931670780 batches: 0.4494
trigger times: 0
Loss after 1931764740 batches: 0.1382
trigger times: 0
Loss after 1931858700 batches: 0.0899
trigger times: 0
Loss after 1931952660 batches: 0.0726
trigger times: 1
Loss after 1932046620 batches: 0.0635
trigger times: 2
Loss after 1932140580 batches: 0.0572
trigger times: 3
Loss after 1932234540 batches: 0.0540
trigger times: 0
Loss after 1932328500 batches: 0.0515
trigger times: 1
Loss after 1932422460 batches: 0.0484
trigger times: 0
Loss after 1932516420 batches: 0.0462
trigger times: 0
Loss after 1932610380 batches: 0.0448
trigger times: 1
Loss after 1932704340 batches: 0.0432
trigger times: 2
Loss after 1932798300 batches: 0.0421
trigger times: 3
Loss after 1932892260 batches: 0.0409
trigger times: 4
Loss after 1932986220 batches: 0.0399
trigger times: 0
Loss after 1933080180 batches: 0.0394
trigger times: 1
Loss after 1933174140 batches: 0.0381
trigger times: 2
Loss after 1933268100 batches: 0.0374
trigger times: 3
Loss after 1933362060 batches: 0.0370
trigger times: 4
Loss after 1933456020 batches: 0.0361
trigger times: 5
Loss after 1933549980 batches: 0.0360
trigger times: 6
Loss after 1933643940 batches: 0.0354
trigger times: 7
Loss after 1933737900 batches: 0.0350
trigger times: 8
Loss after 1933831860 batches: 0.0344
trigger times: 0
Loss after 1933925820 batches: 0.0343
trigger times: 1
Loss after 1934019780 batches: 0.0332
trigger times: 2
Loss after 1934113740 batches: 0.0323
trigger times: 3
Loss after 1934207700 batches: 0.0326
trigger times: 4
Loss after 1934301660 batches: 0.0327
trigger times: 5
Loss after 1934395620 batches: 0.0316
trigger times: 6
Loss after 1934489580 batches: 0.0322
trigger times: 7
Loss after 1934583540 batches: 0.0314
trigger times: 8
Loss after 1934677500 batches: 0.0309
trigger times: 9
Loss after 1934771460 batches: 0.0312
trigger times: 10
Loss after 1934865420 batches: 0.0307
trigger times: 11
Loss after 1934959380 batches: 0.0315
trigger times: 12
Loss after 1935053340 batches: 0.0305
trigger times: 13
Loss after 1935147300 batches: 0.0299
trigger times: 14
Loss after 1935241260 batches: 0.0293
trigger times: 15
Loss after 1935335220 batches: 0.0293
trigger times: 16
Loss after 1935429180 batches: 0.0290
trigger times: 17
Loss after 1935523140 batches: 0.0286
trigger times: 18
Loss after 1935617100 batches: 0.0286
trigger times: 19
Loss after 1935711060 batches: 0.0284
trigger times: 20
Early stopping!
Start to test process.
Loss after 1935805020 batches: 0.0282
Time to train on one home:  255.91713643074036
trigger times: 0
Loss after 1935936120 batches: 0.0709
trigger times: 1
Loss after 1936067220 batches: 0.0135
trigger times: 2
Loss after 1936198320 batches: 0.0097
trigger times: 3
Loss after 1936329420 batches: 0.0084
trigger times: 4
Loss after 1936460520 batches: 0.0073
trigger times: 5
Loss after 1936591620 batches: 0.0066
trigger times: 6
Loss after 1936722720 batches: 0.0062
trigger times: 7
Loss after 1936853820 batches: 0.0059
trigger times: 8
Loss after 1936984920 batches: 0.0056
trigger times: 9
Loss after 1937116020 batches: 0.0053
trigger times: 10
Loss after 1937247120 batches: 0.0050
trigger times: 11
Loss after 1937378220 batches: 0.0048
trigger times: 12
Loss after 1937509320 batches: 0.0047
trigger times: 13
Loss after 1937640420 batches: 0.0045
trigger times: 14
Loss after 1937771520 batches: 0.0045
trigger times: 15
Loss after 1937902620 batches: 0.0043
trigger times: 16
Loss after 1938033720 batches: 0.0041
trigger times: 17
Loss after 1938164820 batches: 0.0040
trigger times: 18
Loss after 1938295920 batches: 0.0040
trigger times: 19
Loss after 1938427020 batches: 0.0039
trigger times: 20
Early stopping!
Start to test process.
Loss after 1938558120 batches: 0.0038
Time to train on one home:  164.50716161727905
trigger times: 0
Loss after 1938689220 batches: 0.1462
trigger times: 0
Loss after 1938820320 batches: 0.0418
trigger times: 0
Loss after 1938951420 batches: 0.0296
trigger times: 1
Loss after 1939082520 batches: 0.0253
trigger times: 0
Loss after 1939213620 batches: 0.0229
trigger times: 1
Loss after 1939344720 batches: 0.0213
trigger times: 2
Loss after 1939475820 batches: 0.0201
trigger times: 0
Loss after 1939606920 batches: 0.0187
trigger times: 0
Loss after 1939738020 batches: 0.0180
trigger times: 0
Loss after 1939869120 batches: 0.0173
trigger times: 1
Loss after 1940000220 batches: 0.0167
trigger times: 2
Loss after 1940131320 batches: 0.0164
trigger times: 0
Loss after 1940262420 batches: 0.0157
trigger times: 1
Loss after 1940393520 batches: 0.0154
trigger times: 2
Loss after 1940524620 batches: 0.0153
trigger times: 3
Loss after 1940655720 batches: 0.0150
trigger times: 4
Loss after 1940786820 batches: 0.0148
trigger times: 5
Loss after 1940917920 batches: 0.0140
trigger times: 6
Loss after 1941049020 batches: 0.0140
trigger times: 7
Loss after 1941180120 batches: 0.0140
trigger times: 8
Loss after 1941311220 batches: 0.0136
trigger times: 9
Loss after 1941442320 batches: 0.0132
trigger times: 0
Loss after 1941573420 batches: 0.0134
trigger times: 1
Loss after 1941704520 batches: 0.0131
trigger times: 2
Loss after 1941835620 batches: 0.0130
trigger times: 0
Loss after 1941966720 batches: 0.0130
trigger times: 1
Loss after 1942097820 batches: 0.0128
trigger times: 2
Loss after 1942228920 batches: 0.0125
trigger times: 0
Loss after 1942360020 batches: 0.0127
trigger times: 1
Loss after 1942491120 batches: 0.0126
trigger times: 2
Loss after 1942622220 batches: 0.0124
trigger times: 3
Loss after 1942753320 batches: 0.0121
trigger times: 4
Loss after 1942884420 batches: 0.0119
trigger times: 5
Loss after 1943015520 batches: 0.0119
trigger times: 0
Loss after 1943146620 batches: 0.0118
trigger times: 1
Loss after 1943277720 batches: 0.0117
trigger times: 2
Loss after 1943408820 batches: 0.0117
trigger times: 3
Loss after 1943539920 batches: 0.0116
trigger times: 0
Loss after 1943671020 batches: 0.0115
trigger times: 1
Loss after 1943802120 batches: 0.0115
trigger times: 2
Loss after 1943933220 batches: 0.0111
trigger times: 3
Loss after 1944064320 batches: 0.0114
trigger times: 4
Loss after 1944195420 batches: 0.0111
trigger times: 5
Loss after 1944326520 batches: 0.0109
trigger times: 6
Loss after 1944457620 batches: 0.0108
trigger times: 7
Loss after 1944588720 batches: 0.0109
trigger times: 8
Loss after 1944719820 batches: 0.0106
trigger times: 9
Loss after 1944850920 batches: 0.0106
trigger times: 10
Loss after 1944982020 batches: 0.0107
trigger times: 11
Loss after 1945113120 batches: 0.0106
trigger times: 12
Loss after 1945244220 batches: 0.0104
trigger times: 13
Loss after 1945375320 batches: 0.0105
trigger times: 14
Loss after 1945506420 batches: 0.0104
trigger times: 15
Loss after 1945637520 batches: 0.0103
trigger times: 16
Loss after 1945768620 batches: 0.0104
trigger times: 17
Loss after 1945899720 batches: 0.0104
trigger times: 18
Loss after 1946030820 batches: 0.0102
trigger times: 19
Loss after 1946161920 batches: 0.0101
trigger times: 20
Early stopping!
Start to test process.
Loss after 1946293020 batches: 0.0099
Time to train on one home:  445.80452704429626
trigger times: 0
Loss after 1946424120 batches: 0.2921
trigger times: 1
Loss after 1946555220 batches: 0.1049
trigger times: 0
Loss after 1946686320 batches: 0.0802
trigger times: 1
Loss after 1946817420 batches: 0.0710
trigger times: 0
Loss after 1946948520 batches: 0.0668
trigger times: 1
Loss after 1947079620 batches: 0.0628
trigger times: 0
Loss after 1947210720 batches: 0.0598
trigger times: 1
Loss after 1947341820 batches: 0.0584
trigger times: 2
Loss after 1947472920 batches: 0.0570
trigger times: 0
Loss after 1947604020 batches: 0.0568
trigger times: 0
Loss after 1947735120 batches: 0.0542
trigger times: 0
Loss after 1947866220 batches: 0.0531
trigger times: 1
Loss after 1947997320 batches: 0.0514
trigger times: 2
Loss after 1948128420 batches: 0.0512
trigger times: 3
Loss after 1948259520 batches: 0.0511
trigger times: 4
Loss after 1948390620 batches: 0.0501
trigger times: 5
Loss after 1948521720 batches: 0.0494
trigger times: 6
Loss after 1948652820 batches: 0.0491
trigger times: 7
Loss after 1948783920 batches: 0.0476
trigger times: 8
Loss after 1948915020 batches: 0.0474
trigger times: 9
Loss after 1949046120 batches: 0.0476
trigger times: 10
Loss after 1949177220 batches: 0.0467
trigger times: 11
Loss after 1949308320 batches: 0.0468
trigger times: 12
Loss after 1949439420 batches: 0.0463
trigger times: 13
Loss after 1949570520 batches: 0.0470
trigger times: 14
Loss after 1949701620 batches: 0.0462
trigger times: 15
Loss after 1949832720 batches: 0.0455
trigger times: 16
Loss after 1949963820 batches: 0.0454
trigger times: 17
Loss after 1950094920 batches: 0.0450
trigger times: 18
Loss after 1950226020 batches: 0.0441
trigger times: 19
Loss after 1950357120 batches: 0.0445
trigger times: 20
Early stopping!
Start to test process.
Loss after 1950488220 batches: 0.0440
Time to train on one home:  245.93045353889465
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642, 0.03501720033327798, 0.03263172240869951, 0.030110256078310443, 0.02544188748067248, 0.02612378438006769, 0.024274968770150986, 0.02405710057046961, 0.02388874572137445, 0.022740620412307226]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217], [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944], [0.6491197148958842, 0.29803996281550993, 0.44160060986867417, 1.203530351478871, 0.5750418631238572, 28.433835927602637, 1775.19], [0.6497863001293607, 0.29730357979276123, 0.44701117944787117, 1.2068712720184533, 0.5756451041104418, 28.512766372815683, 1777.0521], [0.6468338668346405, 0.30051325220168756, 0.4517286880074036, 1.1964129191375579, 0.5730157578453048, 28.26568403748201, 1768.9353], [0.6481483081976572, 0.29911867824708416, 0.44591549869242414, 1.1883481958771474, 0.5741581852808264, 28.07515205986611, 1772.462], [0.6422967116038004, 0.30545179637907305, 0.45038470512861073, 1.1723096038740957, 0.568970129184914, 27.696234575181023, 1756.4463], [0.635419938299391, 0.3128809605404973, 0.4559346003460791, 1.1641353165126298, 0.5628841981140037, 27.503114106407303, 1737.6584], [0.6225904193189409, 0.32673875385346274, 0.47098132348279487, 1.1438745784657927, 0.551531968836913, 27.024446908120186, 1702.6134], [0.6290365358193716, 0.31973560183743677, 0.4703045471241807, 1.1605770615000164, 0.5572689130640909, 27.419049056371055, 1720.3236]]
Round_16_results:  [0.6290365358193716, 0.31973560183743677, 0.4703045471241807, 1.1605770615000164, 0.5572689130640909, 27.419049056371055, 1720.3236]
trigger times: 0
Loss after 1950590820 batches: 0.4082
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 15523 < 15524; dropping {'Training_Loss': 0.4082410296513921, 'Validation_Loss': 0.9160613152715895, 'Training_R2': 0.5872033991478602, 'Validation_R2': 0.14541475871516407, 'Training_F1': 0.7567781858060049, 'Validation_F1': 0.45044553041574686, 'Training_NEP': 0.4883895349065051, 'Validation_NEP': 1.1708928975298583, 'Training_NDE': 0.2780557736711391, 'Validation_NDE': 0.6805466374592153, 'Training_MAE': 32.05390006517109, 'Validation_MAE': 32.11074098733164, 'Training_MSE': 3669.4236, 'Validation_MSE': 2513.24}.
trigger times: 0
Loss after 1950693420 batches: 0.1628
trigger times: 1
Loss after 1950796020 batches: 0.1036
trigger times: 2
Loss after 1950898620 batches: 0.0746
trigger times: 3
Loss after 1951001220 batches: 0.0639
trigger times: 4
Loss after 1951103820 batches: 0.0546
trigger times: 0
Loss after 1951206420 batches: 0.0505
trigger times: 1
Loss after 1951309020 batches: 0.0480
trigger times: 0
Loss after 1951411620 batches: 0.0435
trigger times: 1
Loss after 1951514220 batches: 0.0425
trigger times: 0
Loss after 1951616820 batches: 0.0409
trigger times: 0
Loss after 1951719420 batches: 0.0381
trigger times: 1
Loss after 1951822020 batches: 0.0371
trigger times: 0
Loss after 1951924620 batches: 0.0442
trigger times: 0
Loss after 1952027220 batches: 0.0367
trigger times: 0
Loss after 1952129820 batches: 0.0348
trigger times: 0
Loss after 1952232420 batches: 0.0357
trigger times: 1
Loss after 1952335020 batches: 0.0332
trigger times: 2
Loss after 1952437620 batches: 0.0335
trigger times: 3
Loss after 1952540220 batches: 0.0315
trigger times: 0
Loss after 1952642820 batches: 0.0334
trigger times: 1
Loss after 1952745420 batches: 0.0302
trigger times: 2
Loss after 1952848020 batches: 0.0293
trigger times: 3
Loss after 1952950620 batches: 0.0289
trigger times: 4
Loss after 1953053220 batches: 0.0299
trigger times: 0
Loss after 1953155820 batches: 0.0287
trigger times: 1
Loss after 1953258420 batches: 0.0313
trigger times: 2
Loss after 1953361020 batches: 0.0282
trigger times: 3
Loss after 1953463620 batches: 0.0272
trigger times: 4
Loss after 1953566220 batches: 0.0286
trigger times: 5
Loss after 1953668820 batches: 0.0277
trigger times: 6
Loss after 1953771420 batches: 0.0264
trigger times: 7
Loss after 1953874020 batches: 0.0255
trigger times: 8
Loss after 1953976620 batches: 0.0264
trigger times: 9
Loss after 1954079220 batches: 0.0257
trigger times: 10
Loss after 1954181820 batches: 0.0260
trigger times: 11
Loss after 1954284420 batches: 0.0249
trigger times: 12
Loss after 1954387020 batches: 0.0251
trigger times: 13
Loss after 1954489620 batches: 0.0265
trigger times: 14
Loss after 1954592220 batches: 0.0280
trigger times: 15
Loss after 1954694820 batches: 0.0252
trigger times: 16
Loss after 1954797420 batches: 0.0238
trigger times: 17
Loss after 1954900020 batches: 0.0239
trigger times: 18
Loss after 1955002620 batches: 0.0239
trigger times: 19
Loss after 1955105220 batches: 0.0243
trigger times: 20
Early stopping!
Start to test process.
Loss after 1955207820 batches: 0.0250
Time to train on one home:  280.8335871696472
trigger times: 0
Loss after 1955338920 batches: 0.2387
trigger times: 1
Loss after 1955470020 batches: 0.0861
trigger times: 0
Loss after 1955601120 batches: 0.0580
trigger times: 0
Loss after 1955732220 batches: 0.0484
trigger times: 1
Loss after 1955863320 batches: 0.0425
trigger times: 2
Loss after 1955994420 batches: 0.0395
trigger times: 3
Loss after 1956125520 batches: 0.0370
trigger times: 4
Loss after 1956256620 batches: 0.0349
trigger times: 5
Loss after 1956387720 batches: 0.0327
trigger times: 6
Loss after 1956518820 batches: 0.0316
trigger times: 7
Loss after 1956649920 batches: 0.0310
trigger times: 8
Loss after 1956781020 batches: 0.0300
trigger times: 9
Loss after 1956912120 batches: 0.0288
trigger times: 10
Loss after 1957043220 batches: 0.0281
trigger times: 11
Loss after 1957174320 batches: 0.0276
trigger times: 12
Loss after 1957305420 batches: 0.0271
trigger times: 13
Loss after 1957436520 batches: 0.0264
trigger times: 14
Loss after 1957567620 batches: 0.0258
trigger times: 15
Loss after 1957698720 batches: 0.0257
trigger times: 16
Loss after 1957829820 batches: 0.0250
trigger times: 17
Loss after 1957960920 batches: 0.0248
trigger times: 18
Loss after 1958092020 batches: 0.0248
trigger times: 19
Loss after 1958223120 batches: 0.0243
trigger times: 20
Early stopping!
Start to test process.
Loss after 1958354220 batches: 0.0236
Time to train on one home:  186.83151054382324
trigger times: 0
Loss after 1958485320 batches: 0.4059
trigger times: 0
Loss after 1958616420 batches: 0.1400
trigger times: 1
Loss after 1958747520 batches: 0.0956
trigger times: 2
Loss after 1958878620 batches: 0.0798
trigger times: 3
Loss after 1959009720 batches: 0.0707
trigger times: 0
Loss after 1959140820 batches: 0.0648
trigger times: 1
Loss after 1959271920 batches: 0.0602
trigger times: 0
Loss after 1959403020 batches: 0.0558
trigger times: 1
Loss after 1959534120 batches: 0.0543
trigger times: 2
Loss after 1959665220 batches: 0.0516
trigger times: 3
Loss after 1959796320 batches: 0.0498
trigger times: 4
Loss after 1959927420 batches: 0.0477
trigger times: 5
Loss after 1960058520 batches: 0.0462
trigger times: 6
Loss after 1960189620 batches: 0.0452
trigger times: 0
Loss after 1960320720 batches: 0.0449
trigger times: 1
Loss after 1960451820 batches: 0.0436
trigger times: 2
Loss after 1960582920 batches: 0.0427
trigger times: 3
Loss after 1960714020 batches: 0.0416
trigger times: 4
Loss after 1960845120 batches: 0.0409
trigger times: 5
Loss after 1960976220 batches: 0.0407
trigger times: 0
Loss after 1961107320 batches: 0.0392
trigger times: 1
Loss after 1961238420 batches: 0.0390
trigger times: 0
Loss after 1961369520 batches: 0.0384
trigger times: 1
Loss after 1961500620 batches: 0.0382
trigger times: 2
Loss after 1961631720 batches: 0.0375
trigger times: 3
Loss after 1961762820 batches: 0.0371
trigger times: 0
Loss after 1961893920 batches: 0.0361
trigger times: 1
Loss after 1962025020 batches: 0.0360
trigger times: 2
Loss after 1962156120 batches: 0.0354
trigger times: 3
Loss after 1962287220 batches: 0.0352
trigger times: 0
Loss after 1962418320 batches: 0.0352
trigger times: 1
Loss after 1962549420 batches: 0.0346
trigger times: 2
Loss after 1962680520 batches: 0.0340
trigger times: 3
Loss after 1962811620 batches: 0.0340
trigger times: 4
Loss after 1962942720 batches: 0.0341
trigger times: 5
Loss after 1963073820 batches: 0.0337
trigger times: 6
Loss after 1963204920 batches: 0.0335
trigger times: 7
Loss after 1963336020 batches: 0.0330
trigger times: 8
Loss after 1963467120 batches: 0.0332
trigger times: 9
Loss after 1963598220 batches: 0.0325
trigger times: 10
Loss after 1963729320 batches: 0.0320
trigger times: 11
Loss after 1963860420 batches: 0.0320
trigger times: 12
Loss after 1963991520 batches: 0.0317
trigger times: 13
Loss after 1964122620 batches: 0.0314
trigger times: 14
Loss after 1964253720 batches: 0.0313
trigger times: 0
Loss after 1964384820 batches: 0.0311
trigger times: 1
Loss after 1964515920 batches: 0.0305
trigger times: 2
Loss after 1964647020 batches: 0.0308
trigger times: 3
Loss after 1964778120 batches: 0.0305
trigger times: 4
Loss after 1964909220 batches: 0.0307
trigger times: 5
Loss after 1965040320 batches: 0.0303
trigger times: 0
Loss after 1965171420 batches: 0.0302
trigger times: 1
Loss after 1965302520 batches: 0.0296
trigger times: 2
Loss after 1965433620 batches: 0.0296
trigger times: 3
Loss after 1965564720 batches: 0.0294
trigger times: 4
Loss after 1965695820 batches: 0.0298
trigger times: 5
Loss after 1965826920 batches: 0.0294
trigger times: 6
Loss after 1965958020 batches: 0.0289
trigger times: 7
Loss after 1966089120 batches: 0.0287
trigger times: 0
Loss after 1966220220 batches: 0.0286
trigger times: 1
Loss after 1966351320 batches: 0.0286
trigger times: 2
Loss after 1966482420 batches: 0.0290
trigger times: 0
Loss after 1966613520 batches: 0.0285
trigger times: 0
Loss after 1966744620 batches: 0.0282
trigger times: 1
Loss after 1966875720 batches: 0.0282
trigger times: 2
Loss after 1967006820 batches: 0.0279
trigger times: 3
Loss after 1967137920 batches: 0.0279
trigger times: 4
Loss after 1967269020 batches: 0.0276
trigger times: 5
Loss after 1967400120 batches: 0.0273
trigger times: 6
Loss after 1967531220 batches: 0.0272
trigger times: 7
Loss after 1967662320 batches: 0.0270
trigger times: 8
Loss after 1967793420 batches: 0.0273
trigger times: 9
Loss after 1967924520 batches: 0.0273
trigger times: 0
Loss after 1968055620 batches: 0.0269
trigger times: 1
Loss after 1968186720 batches: 0.0272
trigger times: 2
Loss after 1968317820 batches: 0.0271
trigger times: 3
Loss after 1968448920 batches: 0.0267
trigger times: 4
Loss after 1968580020 batches: 0.0267
trigger times: 5
Loss after 1968711120 batches: 0.0267
trigger times: 6
Loss after 1968842220 batches: 0.0266
trigger times: 7
Loss after 1968973320 batches: 0.0262
trigger times: 8
Loss after 1969104420 batches: 0.0263
trigger times: 9
Loss after 1969235520 batches: 0.0264
trigger times: 10
Loss after 1969366620 batches: 0.0260
trigger times: 11
Loss after 1969497720 batches: 0.0260
trigger times: 12
Loss after 1969628820 batches: 0.0261
trigger times: 13
Loss after 1969759920 batches: 0.0259
trigger times: 14
Loss after 1969891020 batches: 0.0259
trigger times: 15
Loss after 1970022120 batches: 0.0259
trigger times: 16
Loss after 1970153220 batches: 0.0255
trigger times: 17
Loss after 1970284320 batches: 0.0260
trigger times: 18
Loss after 1970415420 batches: 0.0254
trigger times: 19
Loss after 1970546520 batches: 0.0253
trigger times: 20
Early stopping!
Start to test process.
Loss after 1970677620 batches: 0.0259
Time to train on one home:  698.0508189201355
trigger times: 0
Loss after 1970806260 batches: 0.1435
trigger times: 0
Loss after 1970934900 batches: 0.0441
trigger times: 0
Loss after 1971063540 batches: 0.0318
trigger times: 1
Loss after 1971192180 batches: 0.0272
trigger times: 0
Loss after 1971320820 batches: 0.0252
trigger times: 1
Loss after 1971449460 batches: 0.0234
trigger times: 2
Loss after 1971578100 batches: 0.0219
trigger times: 3
Loss after 1971706740 batches: 0.0210
trigger times: 4
Loss after 1971835380 batches: 0.0205
trigger times: 5
Loss after 1971964020 batches: 0.0198
trigger times: 0
Loss after 1972092660 batches: 0.0190
trigger times: 1
Loss after 1972221300 batches: 0.0186
trigger times: 0
Loss after 1972349940 batches: 0.0178
trigger times: 1
Loss after 1972478580 batches: 0.0178
trigger times: 2
Loss after 1972607220 batches: 0.0177
trigger times: 3
Loss after 1972735860 batches: 0.0171
trigger times: 0
Loss after 1972864500 batches: 0.0170
trigger times: 1
Loss after 1972993140 batches: 0.0165
trigger times: 2
Loss after 1973121780 batches: 0.0164
trigger times: 3
Loss after 1973250420 batches: 0.0164
trigger times: 4
Loss after 1973379060 batches: 0.0160
trigger times: 5
Loss after 1973507700 batches: 0.0156
trigger times: 0
Loss after 1973636340 batches: 0.0153
trigger times: 1
Loss after 1973764980 batches: 0.0157
trigger times: 2
Loss after 1973893620 batches: 0.0151
trigger times: 3
Loss after 1974022260 batches: 0.0148
trigger times: 4
Loss after 1974150900 batches: 0.0149
trigger times: 5
Loss after 1974279540 batches: 0.0148
trigger times: 6
Loss after 1974408180 batches: 0.0145
trigger times: 7
Loss after 1974536820 batches: 0.0145
trigger times: 8
Loss after 1974665460 batches: 0.0146
trigger times: 9
Loss after 1974794100 batches: 0.0144
trigger times: 10
Loss after 1974922740 batches: 0.0150
trigger times: 0
Loss after 1975051380 batches: 0.0145
trigger times: 1
Loss after 1975180020 batches: 0.0139
trigger times: 2
Loss after 1975308660 batches: 0.0138
trigger times: 3
Loss after 1975437300 batches: 0.0140
trigger times: 4
Loss after 1975565940 batches: 0.0137
trigger times: 5
Loss after 1975694580 batches: 0.0138
trigger times: 6
Loss after 1975823220 batches: 0.0135
trigger times: 7
Loss after 1975951860 batches: 0.0135
trigger times: 8
Loss after 1976080500 batches: 0.0130
trigger times: 0
Loss after 1976209140 batches: 0.0133
trigger times: 1
Loss after 1976337780 batches: 0.0132
trigger times: 2
Loss after 1976466420 batches: 0.0130
trigger times: 3
Loss after 1976595060 batches: 0.0131
trigger times: 4
Loss after 1976723700 batches: 0.0132
trigger times: 5
Loss after 1976852340 batches: 0.0129
trigger times: 6
Loss after 1976980980 batches: 0.0129
trigger times: 7
Loss after 1977109620 batches: 0.0130
trigger times: 0
Loss after 1977238260 batches: 0.0128
trigger times: 1
Loss after 1977366900 batches: 0.0127
trigger times: 2
Loss after 1977495540 batches: 0.0123
trigger times: 3
Loss after 1977624180 batches: 0.0126
trigger times: 0
Loss after 1977752820 batches: 0.0122
trigger times: 0
Loss after 1977881460 batches: 0.0123
trigger times: 1
Loss after 1978010100 batches: 0.0123
trigger times: 2
Loss after 1978138740 batches: 0.0124
trigger times: 3
Loss after 1978267380 batches: 0.0124
trigger times: 4
Loss after 1978396020 batches: 0.0123
trigger times: 5
Loss after 1978524660 batches: 0.0122
trigger times: 6
Loss after 1978653300 batches: 0.0124
trigger times: 7
Loss after 1978781940 batches: 0.0125
trigger times: 8
Loss after 1978910580 batches: 0.0119
trigger times: 9
Loss after 1979039220 batches: 0.0117
trigger times: 10
Loss after 1979167860 batches: 0.0120
trigger times: 0
Loss after 1979296500 batches: 0.0121
trigger times: 1
Loss after 1979425140 batches: 0.0120
trigger times: 2
Loss after 1979553780 batches: 0.0119
trigger times: 3
Loss after 1979682420 batches: 0.0116
trigger times: 4
Loss after 1979811060 batches: 0.0118
trigger times: 5
Loss after 1979939700 batches: 0.0119
trigger times: 6
Loss after 1980068340 batches: 0.0118
trigger times: 7
Loss after 1980196980 batches: 0.0116
trigger times: 8
Loss after 1980325620 batches: 0.0115
trigger times: 0
Loss after 1980454260 batches: 0.0114
trigger times: 1
Loss after 1980582900 batches: 0.0114
trigger times: 2
Loss after 1980711540 batches: 0.0114
trigger times: 3
Loss after 1980840180 batches: 0.0115
trigger times: 4
Loss after 1980968820 batches: 0.0113
trigger times: 5
Loss after 1981097460 batches: 0.0114
trigger times: 6
Loss after 1981226100 batches: 0.0111
trigger times: 7
Loss after 1981354740 batches: 0.0110
trigger times: 8
Loss after 1981483380 batches: 0.0109
trigger times: 9
Loss after 1981612020 batches: 0.0111
trigger times: 10
Loss after 1981740660 batches: 0.0112
trigger times: 11
Loss after 1981869300 batches: 0.0111
trigger times: 12
Loss after 1981997940 batches: 0.0112
trigger times: 13
Loss after 1982126580 batches: 0.0109
trigger times: 14
Loss after 1982255220 batches: 0.0109
trigger times: 15
Loss after 1982383860 batches: 0.0110
trigger times: 16
Loss after 1982512500 batches: 0.0110
trigger times: 17
Loss after 1982641140 batches: 0.0108
trigger times: 18
Loss after 1982769780 batches: 0.0108
trigger times: 19
Loss after 1982898420 batches: 0.0109
trigger times: 20
Early stopping!
Start to test process.
Loss after 1983027060 batches: 0.0106
Time to train on one home:  696.6742243766785
trigger times: 0
Loss after 1983158160 batches: 0.4454
trigger times: 1
Loss after 1983289260 batches: 0.1384
trigger times: 0
Loss after 1983420360 batches: 0.0919
trigger times: 0
Loss after 1983551460 batches: 0.0777
trigger times: 1
Loss after 1983682560 batches: 0.0691
trigger times: 2
Loss after 1983813660 batches: 0.0636
trigger times: 0
Loss after 1983944760 batches: 0.0587
trigger times: 1
Loss after 1984075860 batches: 0.0553
trigger times: 2
Loss after 1984206960 batches: 0.0530
trigger times: 3
Loss after 1984338060 batches: 0.0504
trigger times: 4
Loss after 1984469160 batches: 0.0494
trigger times: 5
Loss after 1984600260 batches: 0.0470
trigger times: 6
Loss after 1984731360 batches: 0.0460
trigger times: 7
Loss after 1984862460 batches: 0.0450
trigger times: 8
Loss after 1984993560 batches: 0.0440
trigger times: 9
Loss after 1985124660 batches: 0.0428
trigger times: 10
Loss after 1985255760 batches: 0.0420
trigger times: 11
Loss after 1985386860 batches: 0.0415
trigger times: 12
Loss after 1985517960 batches: 0.0405
trigger times: 13
Loss after 1985649060 batches: 0.0399
trigger times: 14
Loss after 1985780160 batches: 0.0396
trigger times: 15
Loss after 1985911260 batches: 0.0386
trigger times: 16
Loss after 1986042360 batches: 0.0383
trigger times: 17
Loss after 1986173460 batches: 0.0378
trigger times: 18
Loss after 1986304560 batches: 0.0373
trigger times: 19
Loss after 1986435660 batches: 0.0368
trigger times: 20
Early stopping!
Start to test process.
Loss after 1986566760 batches: 0.0361
Time to train on one home:  208.97138094902039
trigger times: 0
Loss after 1986697860 batches: 0.4068
trigger times: 0
Loss after 1986828960 batches: 0.1442
trigger times: 0
Loss after 1986960060 batches: 0.0886
trigger times: 0
Loss after 1987091160 batches: 0.0716
trigger times: 1
Loss after 1987222260 batches: 0.0625
trigger times: 2
Loss after 1987353360 batches: 0.0553
trigger times: 0
Loss after 1987484460 batches: 0.0520
trigger times: 0
Loss after 1987615560 batches: 0.0492
trigger times: 0
Loss after 1987746660 batches: 0.0482
trigger times: 1
Loss after 1987877760 batches: 0.0446
trigger times: 2
Loss after 1988008860 batches: 0.0423
trigger times: 3
Loss after 1988139960 batches: 0.0421
trigger times: 4
Loss after 1988271060 batches: 0.0404
trigger times: 5
Loss after 1988402160 batches: 0.0385
trigger times: 6
Loss after 1988533260 batches: 0.0387
trigger times: 0
Loss after 1988664360 batches: 0.0393
trigger times: 1
Loss after 1988795460 batches: 0.0367
trigger times: 0
Loss after 1988926560 batches: 0.0363
trigger times: 1
Loss after 1989057660 batches: 0.0367
trigger times: 2
Loss after 1989188760 batches: 0.0364
trigger times: 0
Loss after 1989319860 batches: 0.0345
trigger times: 1
Loss after 1989450960 batches: 0.0334
trigger times: 2
Loss after 1989582060 batches: 0.0349
trigger times: 3
Loss after 1989713160 batches: 0.0337
trigger times: 4
Loss after 1989844260 batches: 0.0348
trigger times: 5
Loss after 1989975360 batches: 0.0334
trigger times: 6
Loss after 1990106460 batches: 0.0335
trigger times: 7
Loss after 1990237560 batches: 0.0337
trigger times: 8
Loss after 1990368660 batches: 0.0334
trigger times: 9
Loss after 1990499760 batches: 0.0330
trigger times: 10
Loss after 1990630860 batches: 0.0317
trigger times: 11
Loss after 1990761960 batches: 0.0309
trigger times: 12
Loss after 1990893060 batches: 0.0327
trigger times: 13
Loss after 1991024160 batches: 0.0317
trigger times: 14
Loss after 1991155260 batches: 0.0303
trigger times: 15
Loss after 1991286360 batches: 0.0306
trigger times: 0
Loss after 1991417460 batches: 0.0323
trigger times: 0
Loss after 1991548560 batches: 0.0319
trigger times: 1
Loss after 1991679660 batches: 0.0309
trigger times: 2
Loss after 1991810760 batches: 0.0302
trigger times: 3
Loss after 1991941860 batches: 0.0286
trigger times: 4
Loss after 1992072960 batches: 0.0303
trigger times: 5
Loss after 1992204060 batches: 0.0300
trigger times: 6
Loss after 1992335160 batches: 0.0296
trigger times: 7
Loss after 1992466260 batches: 0.0292
trigger times: 8
Loss after 1992597360 batches: 0.0301
trigger times: 9
Loss after 1992728460 batches: 0.0286
trigger times: 0
Loss after 1992859560 batches: 0.0291
trigger times: 1
Loss after 1992990660 batches: 0.0298
trigger times: 2
Loss after 1993121760 batches: 0.0282
trigger times: 3
Loss after 1993252860 batches: 0.0291
trigger times: 4
Loss after 1993383960 batches: 0.0273
trigger times: 5
Loss after 1993515060 batches: 0.0275
trigger times: 6
Loss after 1993646160 batches: 0.0285
trigger times: 7
Loss after 1993777260 batches: 0.0280
trigger times: 8
Loss after 1993908360 batches: 0.0283
trigger times: 9
Loss after 1994039460 batches: 0.0268
trigger times: 10
Loss after 1994170560 batches: 0.0270
trigger times: 11
Loss after 1994301660 batches: 0.0273
trigger times: 0
Loss after 1994432760 batches: 0.0279
trigger times: 1
Loss after 1994563860 batches: 0.0277
trigger times: 2
Loss after 1994694960 batches: 0.0265
trigger times: 3
Loss after 1994826060 batches: 0.0262
trigger times: 4
Loss after 1994957160 batches: 0.0265
trigger times: 5
Loss after 1995088260 batches: 0.0265
trigger times: 6
Loss after 1995219360 batches: 0.0261
trigger times: 7
Loss after 1995350460 batches: 0.0263
trigger times: 8
Loss after 1995481560 batches: 0.0247
trigger times: 9
Loss after 1995612660 batches: 0.0252
trigger times: 10
Loss after 1995743760 batches: 0.0259
trigger times: 11
Loss after 1995874860 batches: 0.0261
trigger times: 12
Loss after 1996005960 batches: 0.0257
trigger times: 13
Loss after 1996137060 batches: 0.0245
trigger times: 14
Loss after 1996268160 batches: 0.0244
trigger times: 15
Loss after 1996399260 batches: 0.0246
trigger times: 16
Loss after 1996530360 batches: 0.0251
trigger times: 17
Loss after 1996661460 batches: 0.0244
trigger times: 0
Loss after 1996792560 batches: 0.0245
trigger times: 0
Loss after 1996923660 batches: 0.0247
trigger times: 1
Loss after 1997054760 batches: 0.0246
trigger times: 2
Loss after 1997185860 batches: 0.0244
trigger times: 3
Loss after 1997316960 batches: 0.0257
trigger times: 4
Loss after 1997448060 batches: 0.0251
trigger times: 5
Loss after 1997579160 batches: 0.0253
trigger times: 6
Loss after 1997710260 batches: 0.0250
trigger times: 7
Loss after 1997841360 batches: 0.0253
trigger times: 8
Loss after 1997972460 batches: 0.0257
trigger times: 9
Loss after 1998103560 batches: 0.0250
trigger times: 10
Loss after 1998234660 batches: 0.0247
trigger times: 11
Loss after 1998365760 batches: 0.0255
trigger times: 12
Loss after 1998496860 batches: 0.0262
trigger times: 13
Loss after 1998627960 batches: 0.0256
trigger times: 14
Loss after 1998759060 batches: 0.0249
trigger times: 15
Loss after 1998890160 batches: 0.0233
trigger times: 16
Loss after 1999021260 batches: 0.0241
trigger times: 17
Loss after 1999152360 batches: 0.0243
trigger times: 18
Loss after 1999283460 batches: 0.0242
trigger times: 19
Loss after 1999414560 batches: 0.0236
trigger times: 20
Early stopping!
Start to test process.
Loss after 1999545660 batches: 0.0238
Time to train on one home:  731.3711166381836
trigger times: 0
Loss after 1999676760 batches: 0.1041
trigger times: 0
Loss after 1999807860 batches: 0.0326
trigger times: 1
Loss after 1999938960 batches: 0.0242
trigger times: 2
Loss after 2000070060 batches: 0.0206
trigger times: 3
Loss after 2000201160 batches: 0.0190
trigger times: 4
Loss after 2000332260 batches: 0.0173
trigger times: 5
Loss after 2000463360 batches: 0.0167
trigger times: 0
Loss after 2000594460 batches: 0.0158
trigger times: 1
Loss after 2000725560 batches: 0.0150
trigger times: 2
Loss after 2000856660 batches: 0.0147
trigger times: 3
Loss after 2000987760 batches: 0.0143
trigger times: 4
Loss after 2001118860 batches: 0.0139
trigger times: 0
Loss after 2001249960 batches: 0.0137
trigger times: 1
Loss after 2001381060 batches: 0.0133
trigger times: 0
Loss after 2001512160 batches: 0.0129
trigger times: 0
Loss after 2001643260 batches: 0.0125
trigger times: 1
Loss after 2001774360 batches: 0.0125
trigger times: 2
Loss after 2001905460 batches: 0.0122
trigger times: 3
Loss after 2002036560 batches: 0.0120
trigger times: 4
Loss after 2002167660 batches: 0.0117
trigger times: 5
Loss after 2002298760 batches: 0.0116
trigger times: 0
Loss after 2002429860 batches: 0.0115
trigger times: 1
Loss after 2002560960 batches: 0.0115
trigger times: 2
Loss after 2002692060 batches: 0.0111
trigger times: 0
Loss after 2002823160 batches: 0.0113
trigger times: 1
Loss after 2002954260 batches: 0.0109
trigger times: 2
Loss after 2003085360 batches: 0.0109
trigger times: 0
Loss after 2003216460 batches: 0.0108
trigger times: 0
Loss after 2003347560 batches: 0.0108
trigger times: 1
Loss after 2003478660 batches: 0.0104
trigger times: 2
Loss after 2003609760 batches: 0.0103
trigger times: 3
Loss after 2003740860 batches: 0.0104
trigger times: 4
Loss after 2003871960 batches: 0.0103
trigger times: 0
Loss after 2004003060 batches: 0.0103
trigger times: 1
Loss after 2004134160 batches: 0.0100
trigger times: 2
Loss after 2004265260 batches: 0.0100
trigger times: 3
Loss after 2004396360 batches: 0.0099
trigger times: 4
Loss after 2004527460 batches: 0.0097
trigger times: 0
Loss after 2004658560 batches: 0.0099
trigger times: 1
Loss after 2004789660 batches: 0.0099
trigger times: 2
Loss after 2004920760 batches: 0.0097
trigger times: 3
Loss after 2005051860 batches: 0.0096
trigger times: 4
Loss after 2005182960 batches: 0.0094
trigger times: 5
Loss after 2005314060 batches: 0.0094
trigger times: 6
Loss after 2005445160 batches: 0.0094
trigger times: 7
Loss after 2005576260 batches: 0.0093
trigger times: 8
Loss after 2005707360 batches: 0.0092
trigger times: 9
Loss after 2005838460 batches: 0.0090
trigger times: 10
Loss after 2005969560 batches: 0.0092
trigger times: 0
Loss after 2006100660 batches: 0.0091
trigger times: 1
Loss after 2006231760 batches: 0.0089
trigger times: 2
Loss after 2006362860 batches: 0.0090
trigger times: 0
Loss after 2006493960 batches: 0.0090
trigger times: 1
Loss after 2006625060 batches: 0.0088
trigger times: 2
Loss after 2006756160 batches: 0.0089
trigger times: 3
Loss after 2006887260 batches: 0.0088
trigger times: 0
Loss after 2007018360 batches: 0.0089
trigger times: 1
Loss after 2007149460 batches: 0.0087
trigger times: 2
Loss after 2007280560 batches: 0.0084
trigger times: 3
Loss after 2007411660 batches: 0.0085
trigger times: 4
Loss after 2007542760 batches: 0.0085
trigger times: 5
Loss after 2007673860 batches: 0.0086
trigger times: 6
Loss after 2007804960 batches: 0.0086
trigger times: 7
Loss after 2007936060 batches: 0.0085
trigger times: 8
Loss after 2008067160 batches: 0.0085
trigger times: 9
Loss after 2008198260 batches: 0.0083
trigger times: 0
Loss after 2008329360 batches: 0.0085
trigger times: 1
Loss after 2008460460 batches: 0.0085
trigger times: 2
Loss after 2008591560 batches: 0.0082
trigger times: 3
Loss after 2008722660 batches: 0.0080
trigger times: 4
Loss after 2008853760 batches: 0.0083
trigger times: 5
Loss after 2008984860 batches: 0.0081
trigger times: 0
Loss after 2009115960 batches: 0.0080
trigger times: 0
Loss after 2009247060 batches: 0.0081
trigger times: 1
Loss after 2009378160 batches: 0.0080
trigger times: 2
Loss after 2009509260 batches: 0.0082
trigger times: 3
Loss after 2009640360 batches: 0.0080
trigger times: 4
Loss after 2009771460 batches: 0.0079
trigger times: 5
Loss after 2009902560 batches: 0.0080
trigger times: 6
Loss after 2010033660 batches: 0.0079
trigger times: 0
Loss after 2010164760 batches: 0.0080
trigger times: 1
Loss after 2010295860 batches: 0.0078
trigger times: 2
Loss after 2010426960 batches: 0.0077
trigger times: 3
Loss after 2010558060 batches: 0.0079
trigger times: 4
Loss after 2010689160 batches: 0.0075
trigger times: 5
Loss after 2010820260 batches: 0.0078
trigger times: 6
Loss after 2010951360 batches: 0.0078
trigger times: 0
Loss after 2011082460 batches: 0.0078
trigger times: 1
Loss after 2011213560 batches: 0.0076
trigger times: 2
Loss after 2011344660 batches: 0.0076
trigger times: 3
Loss after 2011475760 batches: 0.0077
trigger times: 4
Loss after 2011606860 batches: 0.0075
trigger times: 5
Loss after 2011737960 batches: 0.0074
trigger times: 6
Loss after 2011869060 batches: 0.0076
trigger times: 7
Loss after 2012000160 batches: 0.0077
trigger times: 8
Loss after 2012131260 batches: 0.0076
trigger times: 9
Loss after 2012262360 batches: 0.0075
trigger times: 10
Loss after 2012393460 batches: 0.0074
trigger times: 11
Loss after 2012524560 batches: 0.0073
trigger times: 12
Loss after 2012655660 batches: 0.0074
trigger times: 13
Loss after 2012786760 batches: 0.0074
trigger times: 14
Loss after 2012917860 batches: 0.0074
trigger times: 15
Loss after 2013048960 batches: 0.0075
trigger times: 16
Loss after 2013180060 batches: 0.0075
trigger times: 17
Loss after 2013311160 batches: 0.0074
trigger times: 18
Loss after 2013442260 batches: 0.0073
trigger times: 19
Loss after 2013573360 batches: 0.0072
trigger times: 20
Early stopping!
Start to test process.
Loss after 2013704460 batches: 0.0071
Time to train on one home:  797.0063905715942
trigger times: 0
Loss after 2013835560 batches: 0.1860
trigger times: 0
Loss after 2013966660 batches: 0.0468
trigger times: 0
Loss after 2014097760 batches: 0.0319
trigger times: 1
Loss after 2014228860 batches: 0.0268
trigger times: 0
Loss after 2014359960 batches: 0.0245
trigger times: 0
Loss after 2014491060 batches: 0.0219
trigger times: 1
Loss after 2014622160 batches: 0.0211
trigger times: 2
Loss after 2014753260 batches: 0.0198
trigger times: 3
Loss after 2014884360 batches: 0.0186
trigger times: 4
Loss after 2015015460 batches: 0.0185
trigger times: 5
Loss after 2015146560 batches: 0.0180
trigger times: 6
Loss after 2015277660 batches: 0.0176
trigger times: 7
Loss after 2015408760 batches: 0.0173
trigger times: 8
Loss after 2015539860 batches: 0.0164
trigger times: 9
Loss after 2015670960 batches: 0.0160
trigger times: 10
Loss after 2015802060 batches: 0.0160
trigger times: 11
Loss after 2015933160 batches: 0.0156
trigger times: 12
Loss after 2016064260 batches: 0.0153
trigger times: 13
Loss after 2016195360 batches: 0.0150
trigger times: 14
Loss after 2016326460 batches: 0.0149
trigger times: 15
Loss after 2016457560 batches: 0.0148
trigger times: 16
Loss after 2016588660 batches: 0.0143
trigger times: 17
Loss after 2016719760 batches: 0.0144
trigger times: 18
Loss after 2016850860 batches: 0.0142
trigger times: 19
Loss after 2016981960 batches: 0.0137
trigger times: 20
Early stopping!
Start to test process.
Loss after 2017113060 batches: 0.0136
Time to train on one home:  201.04077577590942
trigger times: 0
Loss after 2017191660 batches: 0.3579
trigger times: 0
Loss after 2017270260 batches: 0.1017
trigger times: 1
Loss after 2017348860 batches: 0.0622
trigger times: 0
Loss after 2017427460 batches: 0.0499
trigger times: 1
Loss after 2017506060 batches: 0.0434
trigger times: 2
Loss after 2017584660 batches: 0.0380
trigger times: 3
Loss after 2017663260 batches: 0.0367
trigger times: 4
Loss after 2017741860 batches: 0.0349
trigger times: 5
Loss after 2017820460 batches: 0.0333
trigger times: 0
Loss after 2017899060 batches: 0.0317
trigger times: 1
Loss after 2017977660 batches: 0.0315
trigger times: 0
Loss after 2018056260 batches: 0.0298
trigger times: 1
Loss after 2018134860 batches: 0.0292
trigger times: 2
Loss after 2018213460 batches: 0.0285
trigger times: 0
Loss after 2018292060 batches: 0.0278
trigger times: 1
Loss after 2018370660 batches: 0.0277
trigger times: 2
Loss after 2018449260 batches: 0.0266
trigger times: 3
Loss after 2018527860 batches: 0.0259
trigger times: 4
Loss after 2018606460 batches: 0.0256
trigger times: 5
Loss after 2018685060 batches: 0.0252
trigger times: 6
Loss after 2018763660 batches: 0.0243
trigger times: 0
Loss after 2018842260 batches: 0.0241
trigger times: 0
Loss after 2018920860 batches: 0.0237
trigger times: 1
Loss after 2018999460 batches: 0.0244
trigger times: 2
Loss after 2019078060 batches: 0.0238
trigger times: 3
Loss after 2019156660 batches: 0.0233
trigger times: 4
Loss after 2019235260 batches: 0.0232
trigger times: 5
Loss after 2019313860 batches: 0.0221
trigger times: 6
Loss after 2019392460 batches: 0.0222
trigger times: 7
Loss after 2019471060 batches: 0.0220
trigger times: 8
Loss after 2019549660 batches: 0.0219
trigger times: 9
Loss after 2019628260 batches: 0.0220
trigger times: 10
Loss after 2019706860 batches: 0.0218
trigger times: 11
Loss after 2019785460 batches: 0.0212
trigger times: 12
Loss after 2019864060 batches: 0.0211
trigger times: 13
Loss after 2019942660 batches: 0.0214
trigger times: 14
Loss after 2020021260 batches: 0.0212
trigger times: 15
Loss after 2020099860 batches: 0.0207
trigger times: 16
Loss after 2020178460 batches: 0.0207
trigger times: 17
Loss after 2020257060 batches: 0.0200
trigger times: 18
Loss after 2020335660 batches: 0.0201
trigger times: 19
Loss after 2020414260 batches: 0.0205
trigger times: 20
Early stopping!
Start to test process.
Loss after 2020492860 batches: 0.0202
Time to train on one home:  214.76836037635803
trigger times: 0
Loss after 2020623960 batches: 0.1140
trigger times: 1
Loss after 2020755060 batches: 0.0343
trigger times: 2
Loss after 2020886160 batches: 0.0255
trigger times: 3
Loss after 2021017260 batches: 0.0219
trigger times: 4
Loss after 2021148360 batches: 0.0201
trigger times: 5
Loss after 2021279460 batches: 0.0188
trigger times: 6
Loss after 2021410560 batches: 0.0175
trigger times: 7
Loss after 2021541660 batches: 0.0171
trigger times: 8
Loss after 2021672760 batches: 0.0161
trigger times: 9
Loss after 2021803860 batches: 0.0155
trigger times: 10
Loss after 2021934960 batches: 0.0153
trigger times: 11
Loss after 2022066060 batches: 0.0149
trigger times: 12
Loss after 2022197160 batches: 0.0146
trigger times: 13
Loss after 2022328260 batches: 0.0142
trigger times: 14
Loss after 2022459360 batches: 0.0137
trigger times: 15
Loss after 2022590460 batches: 0.0135
trigger times: 16
Loss after 2022721560 batches: 0.0134
trigger times: 17
Loss after 2022852660 batches: 0.0134
trigger times: 18
Loss after 2022983760 batches: 0.0129
trigger times: 19
Loss after 2023114860 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 2023245960 batches: 0.0124
Time to train on one home:  164.55166697502136
trigger times: 0
Loss after 2023377060 batches: 0.1515
trigger times: 1
Loss after 2023508160 batches: 0.0461
trigger times: 0
Loss after 2023639260 batches: 0.0332
trigger times: 0
Loss after 2023770360 batches: 0.0293
trigger times: 1
Loss after 2023901460 batches: 0.0265
trigger times: 2
Loss after 2024032560 batches: 0.0248
trigger times: 3
Loss after 2024163660 batches: 0.0234
trigger times: 4
Loss after 2024294760 batches: 0.0225
trigger times: 0
Loss after 2024425860 batches: 0.0216
trigger times: 1
Loss after 2024556960 batches: 0.0209
trigger times: 2
Loss after 2024688060 batches: 0.0202
trigger times: 3
Loss after 2024819160 batches: 0.0197
trigger times: 0
Loss after 2024950260 batches: 0.0191
trigger times: 1
Loss after 2025081360 batches: 0.0189
trigger times: 2
Loss after 2025212460 batches: 0.0184
trigger times: 3
Loss after 2025343560 batches: 0.0181
trigger times: 0
Loss after 2025474660 batches: 0.0181
trigger times: 1
Loss after 2025605760 batches: 0.0176
trigger times: 2
Loss after 2025736860 batches: 0.0175
trigger times: 0
Loss after 2025867960 batches: 0.0170
trigger times: 0
Loss after 2025999060 batches: 0.0166
trigger times: 1
Loss after 2026130160 batches: 0.0166
trigger times: 2
Loss after 2026261260 batches: 0.0167
trigger times: 3
Loss after 2026392360 batches: 0.0163
trigger times: 4
Loss after 2026523460 batches: 0.0157
trigger times: 0
Loss after 2026654560 batches: 0.0157
trigger times: 1
Loss after 2026785660 batches: 0.0158
trigger times: 2
Loss after 2026916760 batches: 0.0154
trigger times: 3
Loss after 2027047860 batches: 0.0153
trigger times: 0
Loss after 2027178960 batches: 0.0151
trigger times: 1
Loss after 2027310060 batches: 0.0150
trigger times: 2
Loss after 2027441160 batches: 0.0148
trigger times: 3
Loss after 2027572260 batches: 0.0148
trigger times: 4
Loss after 2027703360 batches: 0.0150
trigger times: 5
Loss after 2027834460 batches: 0.0146
trigger times: 0
Loss after 2027965560 batches: 0.0144
trigger times: 1
Loss after 2028096660 batches: 0.0144
trigger times: 2
Loss after 2028227760 batches: 0.0142
trigger times: 3
Loss after 2028358860 batches: 0.0142
trigger times: 4
Loss after 2028489960 batches: 0.0143
trigger times: 5
Loss after 2028621060 batches: 0.0141
trigger times: 0
Loss after 2028752160 batches: 0.0139
trigger times: 1
Loss after 2028883260 batches: 0.0140
trigger times: 2
Loss after 2029014360 batches: 0.0136
trigger times: 3
Loss after 2029145460 batches: 0.0138
trigger times: 4
Loss after 2029276560 batches: 0.0136
trigger times: 5
Loss after 2029407660 batches: 0.0136
trigger times: 6
Loss after 2029538760 batches: 0.0132
trigger times: 7
Loss after 2029669860 batches: 0.0134
trigger times: 8
Loss after 2029800960 batches: 0.0132
trigger times: 9
Loss after 2029932060 batches: 0.0134
trigger times: 10
Loss after 2030063160 batches: 0.0131
trigger times: 11
Loss after 2030194260 batches: 0.0130
trigger times: 12
Loss after 2030325360 batches: 0.0129
trigger times: 13
Loss after 2030456460 batches: 0.0128
trigger times: 14
Loss after 2030587560 batches: 0.0128
trigger times: 15
Loss after 2030718660 batches: 0.0128
trigger times: 16
Loss after 2030849760 batches: 0.0125
trigger times: 17
Loss after 2030980860 batches: 0.0126
trigger times: 18
Loss after 2031111960 batches: 0.0126
trigger times: 19
Loss after 2031243060 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 2031374160 batches: 0.0124
Time to train on one home:  463.5827479362488
trigger times: 0
Loss after 2031505260 batches: 0.2329
trigger times: 0
Loss after 2031636360 batches: 0.0685
trigger times: 0
Loss after 2031767460 batches: 0.0488
trigger times: 0
Loss after 2031898560 batches: 0.0408
trigger times: 0
Loss after 2032029660 batches: 0.0383
trigger times: 1
Loss after 2032160760 batches: 0.0339
trigger times: 2
Loss after 2032291860 batches: 0.0321
trigger times: 3
Loss after 2032422960 batches: 0.0318
trigger times: 4
Loss after 2032554060 batches: 0.0303
trigger times: 0
Loss after 2032685160 batches: 0.0298
trigger times: 0
Loss after 2032816260 batches: 0.0279
trigger times: 0
Loss after 2032947360 batches: 0.0278
trigger times: 1
Loss after 2033078460 batches: 0.0264
trigger times: 0
Loss after 2033209560 batches: 0.0260
trigger times: 1
Loss after 2033340660 batches: 0.0254
trigger times: 2
Loss after 2033471760 batches: 0.0245
trigger times: 3
Loss after 2033602860 batches: 0.0242
trigger times: 0
Loss after 2033733960 batches: 0.0240
trigger times: 1
Loss after 2033865060 batches: 0.0236
trigger times: 2
Loss after 2033996160 batches: 0.0229
trigger times: 0
Loss after 2034127260 batches: 0.0227
trigger times: 1
Loss after 2034258360 batches: 0.0231
trigger times: 0
Loss after 2034389460 batches: 0.0227
trigger times: 1
Loss after 2034520560 batches: 0.0221
trigger times: 0
Loss after 2034651660 batches: 0.0221
trigger times: 1
Loss after 2034782760 batches: 0.0222
trigger times: 2
Loss after 2034913860 batches: 0.0220
trigger times: 0
Loss after 2035044960 batches: 0.0213
trigger times: 1
Loss after 2035176060 batches: 0.0210
trigger times: 2
Loss after 2035307160 batches: 0.0211
trigger times: 3
Loss after 2035438260 batches: 0.0214
trigger times: 0
Loss after 2035569360 batches: 0.0210
trigger times: 1
Loss after 2035700460 batches: 0.0209
trigger times: 2
Loss after 2035831560 batches: 0.0202
trigger times: 3
Loss after 2035962660 batches: 0.0203
trigger times: 4
Loss after 2036093760 batches: 0.0205
trigger times: 5
Loss after 2036224860 batches: 0.0201
trigger times: 6
Loss after 2036355960 batches: 0.0199
trigger times: 7
Loss after 2036487060 batches: 0.0193
trigger times: 8
Loss after 2036618160 batches: 0.0201
trigger times: 9
Loss after 2036749260 batches: 0.0192
trigger times: 10
Loss after 2036880360 batches: 0.0188
trigger times: 11
Loss after 2037011460 batches: 0.0189
trigger times: 12
Loss after 2037142560 batches: 0.0196
trigger times: 13
Loss after 2037273660 batches: 0.0190
trigger times: 14
Loss after 2037404760 batches: 0.0191
trigger times: 15
Loss after 2037535860 batches: 0.0194
trigger times: 16
Loss after 2037666960 batches: 0.0190
trigger times: 17
Loss after 2037798060 batches: 0.0199
trigger times: 18
Loss after 2037929160 batches: 0.0191
trigger times: 19
Loss after 2038060260 batches: 0.0185
trigger times: 20
Early stopping!
Start to test process.
Loss after 2038191360 batches: 0.0187
Time to train on one home:  389.3130090236664
trigger times: 0
Loss after 2038322460 batches: 0.3503
trigger times: 1
Loss after 2038453560 batches: 0.1201
trigger times: 2
Loss after 2038584660 batches: 0.0842
trigger times: 3
Loss after 2038715760 batches: 0.0694
trigger times: 4
Loss after 2038846860 batches: 0.0622
trigger times: 5
Loss after 2038977960 batches: 0.0578
trigger times: 6
Loss after 2039109060 batches: 0.0547
trigger times: 7
Loss after 2039240160 batches: 0.0512
trigger times: 8
Loss after 2039371260 batches: 0.0489
trigger times: 9
Loss after 2039502360 batches: 0.0478
trigger times: 10
Loss after 2039633460 batches: 0.0459
trigger times: 11
Loss after 2039764560 batches: 0.0444
trigger times: 12
Loss after 2039895660 batches: 0.0432
trigger times: 13
Loss after 2040026760 batches: 0.0419
trigger times: 14
Loss after 2040157860 batches: 0.0410
trigger times: 15
Loss after 2040288960 batches: 0.0407
trigger times: 16
Loss after 2040420060 batches: 0.0400
trigger times: 17
Loss after 2040551160 batches: 0.0395
trigger times: 18
Loss after 2040682260 batches: 0.0381
trigger times: 19
Loss after 2040813360 batches: 0.0375
trigger times: 20
Early stopping!
Start to test process.
Loss after 2040944460 batches: 0.0367
Time to train on one home:  163.90847182273865
trigger times: 0
Loss after 2041075560 batches: 0.2513
trigger times: 1
Loss after 2041206660 batches: 0.0748
trigger times: 2
Loss after 2041337760 batches: 0.0526
trigger times: 3
Loss after 2041468860 batches: 0.0439
trigger times: 4
Loss after 2041599960 batches: 0.0388
trigger times: 5
Loss after 2041731060 batches: 0.0368
trigger times: 6
Loss after 2041862160 batches: 0.0347
trigger times: 0
Loss after 2041993260 batches: 0.0334
trigger times: 1
Loss after 2042124360 batches: 0.0321
trigger times: 2
Loss after 2042255460 batches: 0.0306
trigger times: 3
Loss after 2042386560 batches: 0.0306
trigger times: 4
Loss after 2042517660 batches: 0.0295
trigger times: 5
Loss after 2042648760 batches: 0.0284
trigger times: 6
Loss after 2042779860 batches: 0.0282
trigger times: 7
Loss after 2042910960 batches: 0.0269
trigger times: 8
Loss after 2043042060 batches: 0.0267
trigger times: 9
Loss after 2043173160 batches: 0.0264
trigger times: 10
Loss after 2043304260 batches: 0.0258
trigger times: 11
Loss after 2043435360 batches: 0.0258
trigger times: 12
Loss after 2043566460 batches: 0.0252
trigger times: 13
Loss after 2043697560 batches: 0.0251
trigger times: 14
Loss after 2043828660 batches: 0.0241
trigger times: 15
Loss after 2043959760 batches: 0.0238
trigger times: 16
Loss after 2044090860 batches: 0.0230
trigger times: 17
Loss after 2044221960 batches: 0.0236
trigger times: 18
Loss after 2044353060 batches: 0.0227
trigger times: 19
Loss after 2044484160 batches: 0.0236
trigger times: 20
Early stopping!
Start to test process.
Loss after 2044615260 batches: 0.0222
Time to train on one home:  214.82437944412231
trigger times: 0
Loss after 2044746360 batches: 0.4151
trigger times: 0
Loss after 2044877460 batches: 0.1200
trigger times: 0
Loss after 2045008560 batches: 0.0764
trigger times: 1
Loss after 2045139660 batches: 0.0623
trigger times: 2
Loss after 2045270760 batches: 0.0555
trigger times: 3
Loss after 2045401860 batches: 0.0506
trigger times: 4
Loss after 2045532960 batches: 0.0473
trigger times: 5
Loss after 2045664060 batches: 0.0445
trigger times: 6
Loss after 2045795160 batches: 0.0422
trigger times: 7
Loss after 2045926260 batches: 0.0405
trigger times: 8
Loss after 2046057360 batches: 0.0391
trigger times: 9
Loss after 2046188460 batches: 0.0373
trigger times: 10
Loss after 2046319560 batches: 0.0367
trigger times: 11
Loss after 2046450660 batches: 0.0356
trigger times: 12
Loss after 2046581760 batches: 0.0344
trigger times: 13
Loss after 2046712860 batches: 0.0340
trigger times: 14
Loss after 2046843960 batches: 0.0333
trigger times: 15
Loss after 2046975060 batches: 0.0327
trigger times: 16
Loss after 2047106160 batches: 0.0319
trigger times: 17
Loss after 2047237260 batches: 0.0315
trigger times: 18
Loss after 2047368360 batches: 0.0308
trigger times: 19
Loss after 2047499460 batches: 0.0304
trigger times: 20
Early stopping!
Start to test process.
Loss after 2047630560 batches: 0.0298
Time to train on one home:  178.26659440994263
trigger times: 0
Loss after 2047761660 batches: 0.4126
trigger times: 1
Loss after 2047892760 batches: 0.1226
trigger times: 0
Loss after 2048023860 batches: 0.0884
trigger times: 1
Loss after 2048154960 batches: 0.0741
trigger times: 2
Loss after 2048286060 batches: 0.0667
trigger times: 3
Loss after 2048417160 batches: 0.0631
trigger times: 4
Loss after 2048548260 batches: 0.0586
trigger times: 0
Loss after 2048679360 batches: 0.0554
trigger times: 1
Loss after 2048810460 batches: 0.0532
trigger times: 0
Loss after 2048941560 batches: 0.0521
trigger times: 0
Loss after 2049072660 batches: 0.0494
trigger times: 0
Loss after 2049203760 batches: 0.0482
trigger times: 1
Loss after 2049334860 batches: 0.0470
trigger times: 0
Loss after 2049465960 batches: 0.0460
trigger times: 0
Loss after 2049597060 batches: 0.0450
trigger times: 1
Loss after 2049728160 batches: 0.0441
trigger times: 2
Loss after 2049859260 batches: 0.0433
trigger times: 0
Loss after 2049990360 batches: 0.0423
trigger times: 1
Loss after 2050121460 batches: 0.0413
trigger times: 0
Loss after 2050252560 batches: 0.0408
trigger times: 1
Loss after 2050383660 batches: 0.0416
trigger times: 2
Loss after 2050514760 batches: 0.0398
trigger times: 0
Loss after 2050645860 batches: 0.0393
trigger times: 1
Loss after 2050776960 batches: 0.0391
trigger times: 2
Loss after 2050908060 batches: 0.0388
trigger times: 3
Loss after 2051039160 batches: 0.0379
trigger times: 4
Loss after 2051170260 batches: 0.0381
trigger times: 5
Loss after 2051301360 batches: 0.0373
trigger times: 6
Loss after 2051432460 batches: 0.0364
trigger times: 7
Loss after 2051563560 batches: 0.0365
trigger times: 8
Loss after 2051694660 batches: 0.0360
trigger times: 9
Loss after 2051825760 batches: 0.0359
trigger times: 0
Loss after 2051956860 batches: 0.0355
trigger times: 1
Loss after 2052087960 batches: 0.0355
trigger times: 2
Loss after 2052219060 batches: 0.0349
trigger times: 3
Loss after 2052350160 batches: 0.0343
trigger times: 0
Loss after 2052481260 batches: 0.0347
trigger times: 1
Loss after 2052612360 batches: 0.0338
trigger times: 0
Loss after 2052743460 batches: 0.0343
trigger times: 1
Loss after 2052874560 batches: 0.0341
trigger times: 2
Loss after 2053005660 batches: 0.0343
trigger times: 3
Loss after 2053136760 batches: 0.0334
trigger times: 4
Loss after 2053267860 batches: 0.0333
trigger times: 5
Loss after 2053398960 batches: 0.0326
trigger times: 6
Loss after 2053530060 batches: 0.0331
trigger times: 7
Loss after 2053661160 batches: 0.0324
trigger times: 8
Loss after 2053792260 batches: 0.0325
trigger times: 9
Loss after 2053923360 batches: 0.0320
trigger times: 10
Loss after 2054054460 batches: 0.0321
trigger times: 11
Loss after 2054185560 batches: 0.0316
trigger times: 12
Loss after 2054316660 batches: 0.0315
trigger times: 13
Loss after 2054447760 batches: 0.0312
trigger times: 14
Loss after 2054578860 batches: 0.0316
trigger times: 15
Loss after 2054709960 batches: 0.0307
trigger times: 16
Loss after 2054841060 batches: 0.0310
trigger times: 17
Loss after 2054972160 batches: 0.0304
trigger times: 18
Loss after 2055103260 batches: 0.0308
trigger times: 19
Loss after 2055234360 batches: 0.0300
trigger times: 20
Early stopping!
Start to test process.
Loss after 2055365460 batches: 0.0302
Time to train on one home:  439.6104874610901
trigger times: 0
Loss after 2055459420 batches: 0.4622
trigger times: 0
Loss after 2055553380 batches: 0.1400
trigger times: 1
Loss after 2055647340 batches: 0.0882
trigger times: 2
Loss after 2055741300 batches: 0.0710
trigger times: 3
Loss after 2055835260 batches: 0.0635
trigger times: 4
Loss after 2055929220 batches: 0.0562
trigger times: 5
Loss after 2056023180 batches: 0.0528
trigger times: 0
Loss after 2056117140 batches: 0.0503
trigger times: 1
Loss after 2056211100 batches: 0.0483
trigger times: 2
Loss after 2056305060 batches: 0.0457
trigger times: 3
Loss after 2056399020 batches: 0.0441
trigger times: 0
Loss after 2056492980 batches: 0.0426
trigger times: 1
Loss after 2056586940 batches: 0.0412
trigger times: 2
Loss after 2056680900 batches: 0.0405
trigger times: 3
Loss after 2056774860 batches: 0.0392
trigger times: 0
Loss after 2056868820 batches: 0.0384
trigger times: 1
Loss after 2056962780 batches: 0.0378
trigger times: 2
Loss after 2057056740 batches: 0.0368
trigger times: 3
Loss after 2057150700 batches: 0.0371
trigger times: 4
Loss after 2057244660 batches: 0.0367
trigger times: 5
Loss after 2057338620 batches: 0.0354
trigger times: 6
Loss after 2057432580 batches: 0.0349
trigger times: 7
Loss after 2057526540 batches: 0.0343
trigger times: 0
Loss after 2057620500 batches: 0.0345
trigger times: 0
Loss after 2057714460 batches: 0.0329
trigger times: 1
Loss after 2057808420 batches: 0.0322
trigger times: 2
Loss after 2057902380 batches: 0.0322
trigger times: 3
Loss after 2057996340 batches: 0.0320
trigger times: 4
Loss after 2058090300 batches: 0.0320
trigger times: 5
Loss after 2058184260 batches: 0.0316
trigger times: 0
Loss after 2058278220 batches: 0.0318
trigger times: 1
Loss after 2058372180 batches: 0.0307
trigger times: 2
Loss after 2058466140 batches: 0.0302
trigger times: 3
Loss after 2058560100 batches: 0.0297
trigger times: 4
Loss after 2058654060 batches: 0.0298
trigger times: 5
Loss after 2058748020 batches: 0.0301
trigger times: 6
Loss after 2058841980 batches: 0.0299
trigger times: 7
Loss after 2058935940 batches: 0.0300
trigger times: 8
Loss after 2059029900 batches: 0.0290
trigger times: 9
Loss after 2059123860 batches: 0.0293
trigger times: 10
Loss after 2059217820 batches: 0.0285
trigger times: 11
Loss after 2059311780 batches: 0.0291
trigger times: 12
Loss after 2059405740 batches: 0.0282
trigger times: 0
Loss after 2059499700 batches: 0.0280
trigger times: 1
Loss after 2059593660 batches: 0.0278
trigger times: 2
Loss after 2059687620 batches: 0.0279
trigger times: 3
Loss after 2059781580 batches: 0.0278
trigger times: 4
Loss after 2059875540 batches: 0.0276
trigger times: 5
Loss after 2059969500 batches: 0.0273
trigger times: 6
Loss after 2060063460 batches: 0.0274
trigger times: 7
Loss after 2060157420 batches: 0.0270
trigger times: 8
Loss after 2060251380 batches: 0.0268
trigger times: 9
Loss after 2060345340 batches: 0.0273
trigger times: 0
Loss after 2060439300 batches: 0.0269
trigger times: 1
Loss after 2060533260 batches: 0.0266
trigger times: 2
Loss after 2060627220 batches: 0.0271
trigger times: 0
Loss after 2060721180 batches: 0.0260
trigger times: 1
Loss after 2060815140 batches: 0.0261
trigger times: 2
Loss after 2060909100 batches: 0.0257
trigger times: 0
Loss after 2061003060 batches: 0.0259
trigger times: 1
Loss after 2061097020 batches: 0.0257
trigger times: 2
Loss after 2061190980 batches: 0.0256
trigger times: 3
Loss after 2061284940 batches: 0.0254
trigger times: 4
Loss after 2061378900 batches: 0.0260
trigger times: 5
Loss after 2061472860 batches: 0.0255
trigger times: 6
Loss after 2061566820 batches: 0.0255
trigger times: 7
Loss after 2061660780 batches: 0.0245
trigger times: 8
Loss after 2061754740 batches: 0.0249
trigger times: 9
Loss after 2061848700 batches: 0.0250
trigger times: 10
Loss after 2061942660 batches: 0.0244
trigger times: 11
Loss after 2062036620 batches: 0.0246
trigger times: 12
Loss after 2062130580 batches: 0.0247
trigger times: 13
Loss after 2062224540 batches: 0.0245
trigger times: 14
Loss after 2062318500 batches: 0.0243
trigger times: 15
Loss after 2062412460 batches: 0.0243
trigger times: 16
Loss after 2062506420 batches: 0.0243
trigger times: 17
Loss after 2062600380 batches: 0.0237
trigger times: 18
Loss after 2062694340 batches: 0.0240
trigger times: 19
Loss after 2062788300 batches: 0.0239
trigger times: 20
Early stopping!
Start to test process.
Loss after 2062882260 batches: 0.0234
Time to train on one home:  445.4847733974457
trigger times: 0
Loss after 2063013360 batches: 0.0705
trigger times: 1
Loss after 2063144460 batches: 0.0138
trigger times: 2
Loss after 2063275560 batches: 0.0100
trigger times: 3
Loss after 2063406660 batches: 0.0082
trigger times: 4
Loss after 2063537760 batches: 0.0074
trigger times: 5
Loss after 2063668860 batches: 0.0067
trigger times: 6
Loss after 2063799960 batches: 0.0062
trigger times: 7
Loss after 2063931060 batches: 0.0059
trigger times: 0
Loss after 2064062160 batches: 0.0055
trigger times: 1
Loss after 2064193260 batches: 0.0054
trigger times: 2
Loss after 2064324360 batches: 0.0051
trigger times: 3
Loss after 2064455460 batches: 0.0049
trigger times: 4
Loss after 2064586560 batches: 0.0047
trigger times: 0
Loss after 2064717660 batches: 0.0045
trigger times: 1
Loss after 2064848760 batches: 0.0045
trigger times: 2
Loss after 2064979860 batches: 0.0044
trigger times: 3
Loss after 2065110960 batches: 0.0042
trigger times: 4
Loss after 2065242060 batches: 0.0042
trigger times: 5
Loss after 2065373160 batches: 0.0040
trigger times: 6
Loss after 2065504260 batches: 0.0039
trigger times: 7
Loss after 2065635360 batches: 0.0038
trigger times: 8
Loss after 2065766460 batches: 0.0038
trigger times: 0
Loss after 2065897560 batches: 0.0037
trigger times: 1
Loss after 2066028660 batches: 0.0037
trigger times: 2
Loss after 2066159760 batches: 0.0036
trigger times: 0
Loss after 2066290860 batches: 0.0035
trigger times: 1
Loss after 2066421960 batches: 0.0035
trigger times: 2
Loss after 2066553060 batches: 0.0035
trigger times: 3
Loss after 2066684160 batches: 0.0034
trigger times: 4
Loss after 2066815260 batches: 0.0033
trigger times: 5
Loss after 2066946360 batches: 0.0033
trigger times: 6
Loss after 2067077460 batches: 0.0033
trigger times: 7
Loss after 2067208560 batches: 0.0033
trigger times: 8
Loss after 2067339660 batches: 0.0031
trigger times: 9
Loss after 2067470760 batches: 0.0032
trigger times: 10
Loss after 2067601860 batches: 0.0031
trigger times: 11
Loss after 2067732960 batches: 0.0030
trigger times: 12
Loss after 2067864060 batches: 0.0030
trigger times: 13
Loss after 2067995160 batches: 0.0030
trigger times: 14
Loss after 2068126260 batches: 0.0029
trigger times: 15
Loss after 2068257360 batches: 0.0029
trigger times: 16
Loss after 2068388460 batches: 0.0029
trigger times: 17
Loss after 2068519560 batches: 0.0028
trigger times: 18
Loss after 2068650660 batches: 0.0028
trigger times: 19
Loss after 2068781760 batches: 0.0027
trigger times: 20
Early stopping!
Start to test process.
Loss after 2068912860 batches: 0.0028
Time to train on one home:  346.57208824157715
trigger times: 0
Loss after 2069043960 batches: 0.1253
trigger times: 0
Loss after 2069175060 batches: 0.0381
trigger times: 0
Loss after 2069306160 batches: 0.0275
trigger times: 1
Loss after 2069437260 batches: 0.0234
trigger times: 0
Loss after 2069568360 batches: 0.0208
trigger times: 1
Loss after 2069699460 batches: 0.0194
trigger times: 2
Loss after 2069830560 batches: 0.0184
trigger times: 3
Loss after 2069961660 batches: 0.0173
trigger times: 0
Loss after 2070092760 batches: 0.0167
trigger times: 0
Loss after 2070223860 batches: 0.0160
trigger times: 1
Loss after 2070354960 batches: 0.0157
trigger times: 0
Loss after 2070486060 batches: 0.0152
trigger times: 1
Loss after 2070617160 batches: 0.0148
trigger times: 2
Loss after 2070748260 batches: 0.0145
trigger times: 3
Loss after 2070879360 batches: 0.0142
trigger times: 4
Loss after 2071010460 batches: 0.0136
trigger times: 5
Loss after 2071141560 batches: 0.0134
trigger times: 6
Loss after 2071272660 batches: 0.0134
trigger times: 7
Loss after 2071403760 batches: 0.0136
trigger times: 8
Loss after 2071534860 batches: 0.0129
trigger times: 9
Loss after 2071665960 batches: 0.0127
trigger times: 10
Loss after 2071797060 batches: 0.0126
trigger times: 11
Loss after 2071928160 batches: 0.0125
trigger times: 12
Loss after 2072059260 batches: 0.0123
trigger times: 13
Loss after 2072190360 batches: 0.0121
trigger times: 14
Loss after 2072321460 batches: 0.0120
trigger times: 15
Loss after 2072452560 batches: 0.0118
trigger times: 16
Loss after 2072583660 batches: 0.0115
trigger times: 17
Loss after 2072714760 batches: 0.0116
trigger times: 18
Loss after 2072845860 batches: 0.0115
trigger times: 19
Loss after 2072976960 batches: 0.0114
trigger times: 20
Early stopping!
Start to test process.
Loss after 2073108060 batches: 0.0112
Time to train on one home:  244.16446232795715
trigger times: 0
Loss after 2073239160 batches: 0.3064
trigger times: 0
Loss after 2073370260 batches: 0.1065
trigger times: 0
Loss after 2073501360 batches: 0.0821
trigger times: 0
Loss after 2073632460 batches: 0.0721
trigger times: 1
Loss after 2073763560 batches: 0.0669
trigger times: 2
Loss after 2073894660 batches: 0.0637
trigger times: 3
Loss after 2074025760 batches: 0.0607
trigger times: 4
Loss after 2074156860 batches: 0.0586
trigger times: 5
Loss after 2074287960 batches: 0.0566
trigger times: 6
Loss after 2074419060 batches: 0.0546
trigger times: 7
Loss after 2074550160 batches: 0.0543
trigger times: 8
Loss after 2074681260 batches: 0.0533
trigger times: 9
Loss after 2074812360 batches: 0.0518
trigger times: 0
Loss after 2074943460 batches: 0.0513
trigger times: 1
Loss after 2075074560 batches: 0.0507
trigger times: 2
Loss after 2075205660 batches: 0.0507
trigger times: 3
Loss after 2075336760 batches: 0.0499
trigger times: 4
Loss after 2075467860 batches: 0.0494
trigger times: 0
Loss after 2075598960 batches: 0.0479
trigger times: 1
Loss after 2075730060 batches: 0.0482
trigger times: 2
Loss after 2075861160 batches: 0.0479
trigger times: 3
Loss after 2075992260 batches: 0.0473
trigger times: 4
Loss after 2076123360 batches: 0.0468
trigger times: 5
Loss after 2076254460 batches: 0.0470
trigger times: 6
Loss after 2076385560 batches: 0.0460
trigger times: 7
Loss after 2076516660 batches: 0.0455
trigger times: 8
Loss after 2076647760 batches: 0.0454
trigger times: 9
Loss after 2076778860 batches: 0.0458
trigger times: 10
Loss after 2076909960 batches: 0.0451
trigger times: 11
Loss after 2077041060 batches: 0.0444
trigger times: 12
Loss after 2077172160 batches: 0.0447
trigger times: 13
Loss after 2077303260 batches: 0.0443
trigger times: 14
Loss after 2077434360 batches: 0.0442
trigger times: 15
Loss after 2077565460 batches: 0.0440
trigger times: 16
Loss after 2077696560 batches: 0.0434
trigger times: 17
Loss after 2077827660 batches: 0.0431
trigger times: 18
Loss after 2077958760 batches: 0.0435
trigger times: 19
Loss after 2078089860 batches: 0.0436
trigger times: 20
Early stopping!
Start to test process.
Loss after 2078220960 batches: 0.0427
Time to train on one home:  295.6491208076477
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642, 0.03501720033327798, 0.03263172240869951, 0.030110256078310443, 0.02544188748067248, 0.02612378438006769, 0.024274968770150986, 0.02405710057046961, 0.02388874572137445, 0.022740620412307226, 0.02141669577274335]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217], [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944], [0.6491197148958842, 0.29803996281550993, 0.44160060986867417, 1.203530351478871, 0.5750418631238572, 28.433835927602637, 1775.19], [0.6497863001293607, 0.29730357979276123, 0.44701117944787117, 1.2068712720184533, 0.5756451041104418, 28.512766372815683, 1777.0521], [0.6468338668346405, 0.30051325220168756, 0.4517286880074036, 1.1964129191375579, 0.5730157578453048, 28.26568403748201, 1768.9353], [0.6481483081976572, 0.29911867824708416, 0.44591549869242414, 1.1883481958771474, 0.5741581852808264, 28.07515205986611, 1772.462], [0.6422967116038004, 0.30545179637907305, 0.45038470512861073, 1.1723096038740957, 0.568970129184914, 27.696234575181023, 1756.4463], [0.635419938299391, 0.3128809605404973, 0.4559346003460791, 1.1641353165126298, 0.5628841981140037, 27.503114106407303, 1737.6584], [0.6225904193189409, 0.32673875385346274, 0.47098132348279487, 1.1438745784657927, 0.551531968836913, 27.024446908120186, 1702.6134], [0.6290365358193716, 0.31973560183743677, 0.4703045471241807, 1.1605770615000164, 0.5572689130640909, 27.419049056371055, 1720.3236], [0.6229561699761285, 0.3263064240524094, 0.4742132552861605, 1.1461675708751116, 0.5518861310699636, 27.078619675653382, 1703.7068]]
Round_17_results:  [0.6229561699761285, 0.3263064240524094, 0.4742132552861605, 1.1461675708751116, 0.5518861310699636, 27.078619675653382, 1703.7068]
trigger times: 0
Loss after 2078323560 batches: 0.3867
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 16549 < 16550; dropping {'Training_Loss': 0.38670946870531353, 'Validation_Loss': 0.8204721477296617, 'Training_R2': 0.6088915884628764, 'Validation_R2': 0.2354229757715438, 'Training_F1': 0.7618987196007663, 'Validation_F1': 0.33931317273193234, 'Training_NEP': 0.4786971263464011, 'Validation_NEP': 1.1557851771743843, 'Training_NDE': 0.2634468203826086, 'Validation_NDE': 0.6088688380985289, 'Training_MAE': 31.417769531711034, 'Validation_MAE': 31.69642461709224, 'Training_MSE': 3476.6338, 'Validation_MSE': 2248.5356}.
trigger times: 1
Loss after 2078426160 batches: 0.1580
trigger times: 2
Loss after 2078528760 batches: 0.0942
trigger times: 3
Loss after 2078631360 batches: 0.0719
trigger times: 4
Loss after 2078733960 batches: 0.0629
trigger times: 5
Loss after 2078836560 batches: 0.0565
trigger times: 6
Loss after 2078939160 batches: 0.0521
trigger times: 0
Loss after 2079041760 batches: 0.0496
trigger times: 1
Loss after 2079144360 batches: 0.0433
trigger times: 2
Loss after 2079246960 batches: 0.0428
trigger times: 3
Loss after 2079349560 batches: 0.0413
trigger times: 4
Loss after 2079452160 batches: 0.0377
trigger times: 5
Loss after 2079554760 batches: 0.0352
trigger times: 6
Loss after 2079657360 batches: 0.0351
trigger times: 7
Loss after 2079759960 batches: 0.0337
trigger times: 0
Loss after 2079862560 batches: 0.0356
trigger times: 1
Loss after 2079965160 batches: 0.0345
trigger times: 2
Loss after 2080067760 batches: 0.0312
trigger times: 3
Loss after 2080170360 batches: 0.0307
trigger times: 4
Loss after 2080272960 batches: 0.0300
trigger times: 5
Loss after 2080375560 batches: 0.0283
trigger times: 6
Loss after 2080478160 batches: 0.0287
trigger times: 7
Loss after 2080580760 batches: 0.0280
trigger times: 8
Loss after 2080683360 batches: 0.0288
trigger times: 9
Loss after 2080785960 batches: 0.0276
trigger times: 10
Loss after 2080888560 batches: 0.0280
trigger times: 11
Loss after 2080991160 batches: 0.0295
trigger times: 12
Loss after 2081093760 batches: 0.0297
trigger times: 13
Loss after 2081196360 batches: 0.0274
trigger times: 14
Loss after 2081298960 batches: 0.0265
trigger times: 15
Loss after 2081401560 batches: 0.0276
trigger times: 0
Loss after 2081504160 batches: 0.0252
trigger times: 1
Loss after 2081606760 batches: 0.0261
trigger times: 2
Loss after 2081709360 batches: 0.0259
trigger times: 3
Loss after 2081811960 batches: 0.0253
trigger times: 4
Loss after 2081914560 batches: 0.0248
trigger times: 5
Loss after 2082017160 batches: 0.0250
trigger times: 6
Loss after 2082119760 batches: 0.0250
trigger times: 7
Loss after 2082222360 batches: 0.0246
trigger times: 8
Loss after 2082324960 batches: 0.0257
trigger times: 9
Loss after 2082427560 batches: 0.0258
trigger times: 10
Loss after 2082530160 batches: 0.0242
trigger times: 11
Loss after 2082632760 batches: 0.0234
trigger times: 12
Loss after 2082735360 batches: 0.0243
trigger times: 13
Loss after 2082837960 batches: 0.0236
trigger times: 14
Loss after 2082940560 batches: 0.0242
trigger times: 15
Loss after 2083043160 batches: 0.0233
trigger times: 0
Loss after 2083145760 batches: 0.0234
trigger times: 1
Loss after 2083248360 batches: 0.0248
trigger times: 2
Loss after 2083350960 batches: 0.0237
trigger times: 0
Loss after 2083453560 batches: 0.0244
trigger times: 1
Loss after 2083556160 batches: 0.0229
trigger times: 2
Loss after 2083658760 batches: 0.0229
trigger times: 3
Loss after 2083761360 batches: 0.0223
trigger times: 4
Loss after 2083863960 batches: 0.0222
trigger times: 5
Loss after 2083966560 batches: 0.0218
trigger times: 0
Loss after 2084069160 batches: 0.0240
trigger times: 1
Loss after 2084171760 batches: 0.0225
trigger times: 2
Loss after 2084274360 batches: 0.0233
trigger times: 3
Loss after 2084376960 batches: 0.0228
trigger times: 4
Loss after 2084479560 batches: 0.0227
trigger times: 5
Loss after 2084582160 batches: 0.0219
trigger times: 6
Loss after 2084684760 batches: 0.0211
trigger times: 7
Loss after 2084787360 batches: 0.0209
trigger times: 8
Loss after 2084889960 batches: 0.0218
trigger times: 9
Loss after 2084992560 batches: 0.0216
trigger times: 10
Loss after 2085095160 batches: 0.0222
trigger times: 11
Loss after 2085197760 batches: 0.0206
trigger times: 12
Loss after 2085300360 batches: 0.0215
trigger times: 13
Loss after 2085402960 batches: 0.0220
trigger times: 14
Loss after 2085505560 batches: 0.0217
trigger times: 15
Loss after 2085608160 batches: 0.0207
trigger times: 16
Loss after 2085710760 batches: 0.0197
trigger times: 17
Loss after 2085813360 batches: 0.0205
trigger times: 18
Loss after 2085915960 batches: 0.0214
trigger times: 19
Loss after 2086018560 batches: 0.0222
trigger times: 20
Early stopping!
Start to test process.
Loss after 2086121160 batches: 0.0211
Time to train on one home:  463.7449095249176
trigger times: 0
Loss after 2086252260 batches: 0.2352
trigger times: 1
Loss after 2086383360 batches: 0.0858
trigger times: 2
Loss after 2086514460 batches: 0.0592
trigger times: 3
Loss after 2086645560 batches: 0.0490
trigger times: 4
Loss after 2086776660 batches: 0.0437
trigger times: 5
Loss after 2086907760 batches: 0.0394
trigger times: 6
Loss after 2087038860 batches: 0.0375
trigger times: 7
Loss after 2087169960 batches: 0.0346
trigger times: 8
Loss after 2087301060 batches: 0.0335
trigger times: 9
Loss after 2087432160 batches: 0.0317
trigger times: 10
Loss after 2087563260 batches: 0.0307
trigger times: 11
Loss after 2087694360 batches: 0.0295
trigger times: 12
Loss after 2087825460 batches: 0.0293
trigger times: 13
Loss after 2087956560 batches: 0.0279
trigger times: 14
Loss after 2088087660 batches: 0.0273
trigger times: 15
Loss after 2088218760 batches: 0.0269
trigger times: 16
Loss after 2088349860 batches: 0.0258
trigger times: 17
Loss after 2088480960 batches: 0.0260
trigger times: 18
Loss after 2088612060 batches: 0.0252
trigger times: 19
Loss after 2088743160 batches: 0.0250
trigger times: 20
Early stopping!
Start to test process.
Loss after 2088874260 batches: 0.0246
Time to train on one home:  164.668879032135
trigger times: 0
Loss after 2089005360 batches: 0.3592
trigger times: 1
Loss after 2089136460 batches: 0.1177
trigger times: 0
Loss after 2089267560 batches: 0.0828
trigger times: 0
Loss after 2089398660 batches: 0.0690
trigger times: 1
Loss after 2089529760 batches: 0.0626
trigger times: 2
Loss after 2089660860 batches: 0.0577
trigger times: 3
Loss after 2089791960 batches: 0.0534
trigger times: 4
Loss after 2089923060 batches: 0.0514
trigger times: 5
Loss after 2090054160 batches: 0.0486
trigger times: 6
Loss after 2090185260 batches: 0.0463
trigger times: 7
Loss after 2090316360 batches: 0.0450
trigger times: 8
Loss after 2090447460 batches: 0.0437
trigger times: 9
Loss after 2090578560 batches: 0.0429
trigger times: 0
Loss after 2090709660 batches: 0.0416
trigger times: 1
Loss after 2090840760 batches: 0.0407
trigger times: 2
Loss after 2090971860 batches: 0.0395
trigger times: 3
Loss after 2091102960 batches: 0.0387
trigger times: 4
Loss after 2091234060 batches: 0.0379
trigger times: 5
Loss after 2091365160 batches: 0.0375
trigger times: 6
Loss after 2091496260 batches: 0.0372
trigger times: 7
Loss after 2091627360 batches: 0.0362
trigger times: 8
Loss after 2091758460 batches: 0.0357
trigger times: 9
Loss after 2091889560 batches: 0.0354
trigger times: 10
Loss after 2092020660 batches: 0.0345
trigger times: 11
Loss after 2092151760 batches: 0.0345
trigger times: 12
Loss after 2092282860 batches: 0.0344
trigger times: 13
Loss after 2092413960 batches: 0.0337
trigger times: 14
Loss after 2092545060 batches: 0.0332
trigger times: 15
Loss after 2092676160 batches: 0.0331
trigger times: 0
Loss after 2092807260 batches: 0.0330
trigger times: 0
Loss after 2092938360 batches: 0.0328
trigger times: 1
Loss after 2093069460 batches: 0.0322
trigger times: 2
Loss after 2093200560 batches: 0.0321
trigger times: 3
Loss after 2093331660 batches: 0.0313
trigger times: 4
Loss after 2093462760 batches: 0.0310
trigger times: 5
Loss after 2093593860 batches: 0.0317
trigger times: 6
Loss after 2093724960 batches: 0.0311
trigger times: 7
Loss after 2093856060 batches: 0.0307
trigger times: 8
Loss after 2093987160 batches: 0.0304
trigger times: 9
Loss after 2094118260 batches: 0.0304
trigger times: 10
Loss after 2094249360 batches: 0.0300
trigger times: 11
Loss after 2094380460 batches: 0.0298
trigger times: 0
Loss after 2094511560 batches: 0.0295
trigger times: 1
Loss after 2094642660 batches: 0.0293
trigger times: 2
Loss after 2094773760 batches: 0.0295
trigger times: 3
Loss after 2094904860 batches: 0.0291
trigger times: 4
Loss after 2095035960 batches: 0.0289
trigger times: 5
Loss after 2095167060 batches: 0.0293
trigger times: 6
Loss after 2095298160 batches: 0.0286
trigger times: 7
Loss after 2095429260 batches: 0.0286
trigger times: 8
Loss after 2095560360 batches: 0.0282
trigger times: 9
Loss after 2095691460 batches: 0.0284
trigger times: 10
Loss after 2095822560 batches: 0.0281
trigger times: 11
Loss after 2095953660 batches: 0.0277
trigger times: 12
Loss after 2096084760 batches: 0.0279
trigger times: 13
Loss after 2096215860 batches: 0.0279
trigger times: 14
Loss after 2096346960 batches: 0.0274
trigger times: 15
Loss after 2096478060 batches: 0.0278
trigger times: 16
Loss after 2096609160 batches: 0.0270
trigger times: 17
Loss after 2096740260 batches: 0.0275
trigger times: 18
Loss after 2096871360 batches: 0.0269
trigger times: 19
Loss after 2097002460 batches: 0.0275
trigger times: 20
Early stopping!
Start to test process.
Loss after 2097133560 batches: 0.0268
Time to train on one home:  467.2472622394562
trigger times: 0
Loss after 2097262200 batches: 0.1268
trigger times: 0
Loss after 2097390840 batches: 0.0410
trigger times: 0
Loss after 2097519480 batches: 0.0296
trigger times: 1
Loss after 2097648120 batches: 0.0260
trigger times: 2
Loss after 2097776760 batches: 0.0237
trigger times: 3
Loss after 2097905400 batches: 0.0221
trigger times: 4
Loss after 2098034040 batches: 0.0205
trigger times: 5
Loss after 2098162680 batches: 0.0202
trigger times: 0
Loss after 2098291320 batches: 0.0192
trigger times: 1
Loss after 2098419960 batches: 0.0185
trigger times: 0
Loss after 2098548600 batches: 0.0183
trigger times: 1
Loss after 2098677240 batches: 0.0182
trigger times: 2
Loss after 2098805880 batches: 0.0175
trigger times: 3
Loss after 2098934520 batches: 0.0168
trigger times: 4
Loss after 2099063160 batches: 0.0167
trigger times: 5
Loss after 2099191800 batches: 0.0167
trigger times: 6
Loss after 2099320440 batches: 0.0164
trigger times: 7
Loss after 2099449080 batches: 0.0156
trigger times: 8
Loss after 2099577720 batches: 0.0158
trigger times: 0
Loss after 2099706360 batches: 0.0153
trigger times: 1
Loss after 2099835000 batches: 0.0151
trigger times: 2
Loss after 2099963640 batches: 0.0153
trigger times: 3
Loss after 2100092280 batches: 0.0148
trigger times: 4
Loss after 2100220920 batches: 0.0145
trigger times: 5
Loss after 2100349560 batches: 0.0148
trigger times: 6
Loss after 2100478200 batches: 0.0145
trigger times: 7
Loss after 2100606840 batches: 0.0143
trigger times: 8
Loss after 2100735480 batches: 0.0142
trigger times: 0
Loss after 2100864120 batches: 0.0141
trigger times: 1
Loss after 2100992760 batches: 0.0142
trigger times: 2
Loss after 2101121400 batches: 0.0136
trigger times: 0
Loss after 2101250040 batches: 0.0135
trigger times: 1
Loss after 2101378680 batches: 0.0134
trigger times: 2
Loss after 2101507320 batches: 0.0134
trigger times: 3
Loss after 2101635960 batches: 0.0138
trigger times: 4
Loss after 2101764600 batches: 0.0136
trigger times: 5
Loss after 2101893240 batches: 0.0133
trigger times: 6
Loss after 2102021880 batches: 0.0133
trigger times: 7
Loss after 2102150520 batches: 0.0130
trigger times: 8
Loss after 2102279160 batches: 0.0132
trigger times: 9
Loss after 2102407800 batches: 0.0126
trigger times: 10
Loss after 2102536440 batches: 0.0129
trigger times: 11
Loss after 2102665080 batches: 0.0129
trigger times: 12
Loss after 2102793720 batches: 0.0127
trigger times: 13
Loss after 2102922360 batches: 0.0129
trigger times: 14
Loss after 2103051000 batches: 0.0125
trigger times: 15
Loss after 2103179640 batches: 0.0124
trigger times: 16
Loss after 2103308280 batches: 0.0122
trigger times: 17
Loss after 2103436920 batches: 0.0125
trigger times: 18
Loss after 2103565560 batches: 0.0121
trigger times: 19
Loss after 2103694200 batches: 0.0120
trigger times: 20
Early stopping!
Start to test process.
Loss after 2103822840 batches: 0.0120
Time to train on one home:  382.8540937900543
trigger times: 0
Loss after 2103953940 batches: 0.4482
trigger times: 0
Loss after 2104085040 batches: 0.1320
trigger times: 0
Loss after 2104216140 batches: 0.0882
trigger times: 0
Loss after 2104347240 batches: 0.0733
trigger times: 0
Loss after 2104478340 batches: 0.0661
trigger times: 0
Loss after 2104609440 batches: 0.0605
trigger times: 0
Loss after 2104740540 batches: 0.0556
trigger times: 0
Loss after 2104871640 batches: 0.0526
trigger times: 0
Loss after 2105002740 batches: 0.0504
trigger times: 1
Loss after 2105133840 batches: 0.0485
trigger times: 2
Loss after 2105264940 batches: 0.0472
trigger times: 3
Loss after 2105396040 batches: 0.0461
trigger times: 0
Loss after 2105527140 batches: 0.0442
trigger times: 1
Loss after 2105658240 batches: 0.0432
trigger times: 2
Loss after 2105789340 batches: 0.0418
trigger times: 3
Loss after 2105920440 batches: 0.0408
trigger times: 4
Loss after 2106051540 batches: 0.0408
trigger times: 5
Loss after 2106182640 batches: 0.0397
trigger times: 6
Loss after 2106313740 batches: 0.0389
trigger times: 7
Loss after 2106444840 batches: 0.0382
trigger times: 8
Loss after 2106575940 batches: 0.0378
trigger times: 9
Loss after 2106707040 batches: 0.0371
trigger times: 10
Loss after 2106838140 batches: 0.0367
trigger times: 11
Loss after 2106969240 batches: 0.0364
trigger times: 12
Loss after 2107100340 batches: 0.0358
trigger times: 13
Loss after 2107231440 batches: 0.0352
trigger times: 14
Loss after 2107362540 batches: 0.0348
trigger times: 15
Loss after 2107493640 batches: 0.0344
trigger times: 16
Loss after 2107624740 batches: 0.0341
trigger times: 17
Loss after 2107755840 batches: 0.0339
trigger times: 18
Loss after 2107886940 batches: 0.0340
trigger times: 19
Loss after 2108018040 batches: 0.0337
trigger times: 20
Early stopping!
Start to test process.
Loss after 2108149140 batches: 0.0329
Time to train on one home:  251.56013226509094
trigger times: 0
Loss after 2108280240 batches: 0.3998
trigger times: 1
Loss after 2108411340 batches: 0.1379
trigger times: 0
Loss after 2108542440 batches: 0.0865
trigger times: 0
Loss after 2108673540 batches: 0.0670
trigger times: 1
Loss after 2108804640 batches: 0.0574
trigger times: 0
Loss after 2108935740 batches: 0.0513
trigger times: 1
Loss after 2109066840 batches: 0.0486
trigger times: 2
Loss after 2109197940 batches: 0.0463
trigger times: 3
Loss after 2109329040 batches: 0.0466
trigger times: 0
Loss after 2109460140 batches: 0.0431
trigger times: 1
Loss after 2109591240 batches: 0.0436
trigger times: 2
Loss after 2109722340 batches: 0.0410
trigger times: 3
Loss after 2109853440 batches: 0.0376
trigger times: 4
Loss after 2109984540 batches: 0.0370
trigger times: 5
Loss after 2110115640 batches: 0.0368
trigger times: 0
Loss after 2110246740 batches: 0.0365
trigger times: 0
Loss after 2110377840 batches: 0.0363
trigger times: 1
Loss after 2110508940 batches: 0.0352
trigger times: 0
Loss after 2110640040 batches: 0.0340
trigger times: 1
Loss after 2110771140 batches: 0.0341
trigger times: 0
Loss after 2110902240 batches: 0.0344
trigger times: 1
Loss after 2111033340 batches: 0.0346
trigger times: 2
Loss after 2111164440 batches: 0.0339
trigger times: 3
Loss after 2111295540 batches: 0.0324
trigger times: 4
Loss after 2111426640 batches: 0.0321
trigger times: 5
Loss after 2111557740 batches: 0.0315
trigger times: 0
Loss after 2111688840 batches: 0.0326
trigger times: 1
Loss after 2111819940 batches: 0.0314
trigger times: 2
Loss after 2111951040 batches: 0.0312
trigger times: 3
Loss after 2112082140 batches: 0.0300
trigger times: 0
Loss after 2112213240 batches: 0.0308
trigger times: 1
Loss after 2112344340 batches: 0.0307
trigger times: 0
Loss after 2112475440 batches: 0.0297
trigger times: 0
Loss after 2112606540 batches: 0.0303
trigger times: 0
Loss after 2112737640 batches: 0.0278
trigger times: 1
Loss after 2112868740 batches: 0.0305
trigger times: 0
Loss after 2112999840 batches: 0.0302
trigger times: 1
Loss after 2113130940 batches: 0.0292
trigger times: 2
Loss after 2113262040 batches: 0.0303
trigger times: 3
Loss after 2113393140 batches: 0.0292
trigger times: 0
Loss after 2113524240 batches: 0.0298
trigger times: 1
Loss after 2113655340 batches: 0.0291
trigger times: 2
Loss after 2113786440 batches: 0.0287
trigger times: 3
Loss after 2113917540 batches: 0.0276
trigger times: 0
Loss after 2114048640 batches: 0.0270
trigger times: 1
Loss after 2114179740 batches: 0.0275
trigger times: 2
Loss after 2114310840 batches: 0.0267
trigger times: 3
Loss after 2114441940 batches: 0.0273
trigger times: 4
Loss after 2114573040 batches: 0.0268
trigger times: 5
Loss after 2114704140 batches: 0.0285
trigger times: 6
Loss after 2114835240 batches: 0.0269
trigger times: 7
Loss after 2114966340 batches: 0.0265
trigger times: 8
Loss after 2115097440 batches: 0.0262
trigger times: 9
Loss after 2115228540 batches: 0.0270
trigger times: 10
Loss after 2115359640 batches: 0.0270
trigger times: 11
Loss after 2115490740 batches: 0.0277
trigger times: 12
Loss after 2115621840 batches: 0.0271
trigger times: 13
Loss after 2115752940 batches: 0.0258
trigger times: 14
Loss after 2115884040 batches: 0.0254
trigger times: 15
Loss after 2116015140 batches: 0.0256
trigger times: 0
Loss after 2116146240 batches: 0.0253
trigger times: 1
Loss after 2116277340 batches: 0.0258
trigger times: 0
Loss after 2116408440 batches: 0.0273
trigger times: 1
Loss after 2116539540 batches: 0.0253
trigger times: 2
Loss after 2116670640 batches: 0.0271
trigger times: 3
Loss after 2116801740 batches: 0.0247
trigger times: 4
Loss after 2116932840 batches: 0.0253
trigger times: 5
Loss after 2117063940 batches: 0.0245
trigger times: 6
Loss after 2117195040 batches: 0.0238
trigger times: 7
Loss after 2117326140 batches: 0.0248
trigger times: 8
Loss after 2117457240 batches: 0.0241
trigger times: 9
Loss after 2117588340 batches: 0.0237
trigger times: 10
Loss after 2117719440 batches: 0.0241
trigger times: 11
Loss after 2117850540 batches: 0.0254
trigger times: 12
Loss after 2117981640 batches: 0.0250
trigger times: 13
Loss after 2118112740 batches: 0.0253
trigger times: 14
Loss after 2118243840 batches: 0.0249
trigger times: 15
Loss after 2118374940 batches: 0.0231
trigger times: 16
Loss after 2118506040 batches: 0.0227
trigger times: 17
Loss after 2118637140 batches: 0.0238
trigger times: 18
Loss after 2118768240 batches: 0.0243
trigger times: 19
Loss after 2118899340 batches: 0.0230
trigger times: 20
Early stopping!
Start to test process.
Loss after 2119030440 batches: 0.0239
Time to train on one home:  614.3586628437042
trigger times: 0
Loss after 2119161540 batches: 0.0884
trigger times: 1
Loss after 2119292640 batches: 0.0284
trigger times: 2
Loss after 2119423740 batches: 0.0216
trigger times: 3
Loss after 2119554840 batches: 0.0181
trigger times: 4
Loss after 2119685940 batches: 0.0165
trigger times: 5
Loss after 2119817040 batches: 0.0154
trigger times: 6
Loss after 2119948140 batches: 0.0146
trigger times: 7
Loss after 2120079240 batches: 0.0140
trigger times: 8
Loss after 2120210340 batches: 0.0134
trigger times: 9
Loss after 2120341440 batches: 0.0133
trigger times: 10
Loss after 2120472540 batches: 0.0128
trigger times: 11
Loss after 2120603640 batches: 0.0125
trigger times: 12
Loss after 2120734740 batches: 0.0122
trigger times: 13
Loss after 2120865840 batches: 0.0120
trigger times: 14
Loss after 2120996940 batches: 0.0117
trigger times: 15
Loss after 2121128040 batches: 0.0113
trigger times: 16
Loss after 2121259140 batches: 0.0111
trigger times: 17
Loss after 2121390240 batches: 0.0111
trigger times: 18
Loss after 2121521340 batches: 0.0107
trigger times: 19
Loss after 2121652440 batches: 0.0107
trigger times: 20
Early stopping!
Start to test process.
Loss after 2121783540 batches: 0.0105
Time to train on one home:  163.86043238639832
trigger times: 0
Loss after 2121914640 batches: 0.1820
trigger times: 0
Loss after 2122045740 batches: 0.0470
trigger times: 0
Loss after 2122176840 batches: 0.0316
trigger times: 0
Loss after 2122307940 batches: 0.0273
trigger times: 0
Loss after 2122439040 batches: 0.0241
trigger times: 1
Loss after 2122570140 batches: 0.0224
trigger times: 0
Loss after 2122701240 batches: 0.0214
trigger times: 1
Loss after 2122832340 batches: 0.0201
trigger times: 2
Loss after 2122963440 batches: 0.0191
trigger times: 3
Loss after 2123094540 batches: 0.0184
trigger times: 4
Loss after 2123225640 batches: 0.0182
trigger times: 0
Loss after 2123356740 batches: 0.0173
trigger times: 1
Loss after 2123487840 batches: 0.0169
trigger times: 2
Loss after 2123618940 batches: 0.0164
trigger times: 3
Loss after 2123750040 batches: 0.0161
trigger times: 4
Loss after 2123881140 batches: 0.0159
trigger times: 5
Loss after 2124012240 batches: 0.0154
trigger times: 6
Loss after 2124143340 batches: 0.0151
trigger times: 7
Loss after 2124274440 batches: 0.0150
trigger times: 8
Loss after 2124405540 batches: 0.0150
trigger times: 9
Loss after 2124536640 batches: 0.0143
trigger times: 10
Loss after 2124667740 batches: 0.0142
trigger times: 11
Loss after 2124798840 batches: 0.0141
trigger times: 12
Loss after 2124929940 batches: 0.0139
trigger times: 13
Loss after 2125061040 batches: 0.0136
trigger times: 14
Loss after 2125192140 batches: 0.0134
trigger times: 15
Loss after 2125323240 batches: 0.0132
trigger times: 16
Loss after 2125454340 batches: 0.0129
trigger times: 17
Loss after 2125585440 batches: 0.0130
trigger times: 18
Loss after 2125716540 batches: 0.0130
trigger times: 19
Loss after 2125847640 batches: 0.0129
trigger times: 20
Early stopping!
Start to test process.
Loss after 2125978740 batches: 0.0131
Time to train on one home:  244.25726985931396
trigger times: 0
Loss after 2126057340 batches: 0.3493
trigger times: 0
Loss after 2126135940 batches: 0.0966
trigger times: 0
Loss after 2126214540 batches: 0.0594
trigger times: 0
Loss after 2126293140 batches: 0.0465
trigger times: 1
Loss after 2126371740 batches: 0.0410
trigger times: 2
Loss after 2126450340 batches: 0.0382
trigger times: 3
Loss after 2126528940 batches: 0.0358
trigger times: 4
Loss after 2126607540 batches: 0.0339
trigger times: 5
Loss after 2126686140 batches: 0.0321
trigger times: 0
Loss after 2126764740 batches: 0.0305
trigger times: 0
Loss after 2126843340 batches: 0.0289
trigger times: 1
Loss after 2126921940 batches: 0.0290
trigger times: 2
Loss after 2127000540 batches: 0.0274
trigger times: 3
Loss after 2127079140 batches: 0.0273
trigger times: 4
Loss after 2127157740 batches: 0.0269
trigger times: 5
Loss after 2127236340 batches: 0.0265
trigger times: 6
Loss after 2127314940 batches: 0.0252
trigger times: 7
Loss after 2127393540 batches: 0.0245
trigger times: 0
Loss after 2127472140 batches: 0.0247
trigger times: 1
Loss after 2127550740 batches: 0.0237
trigger times: 2
Loss after 2127629340 batches: 0.0232
trigger times: 3
Loss after 2127707940 batches: 0.0233
trigger times: 4
Loss after 2127786540 batches: 0.0225
trigger times: 5
Loss after 2127865140 batches: 0.0232
trigger times: 6
Loss after 2127943740 batches: 0.0221
trigger times: 7
Loss after 2128022340 batches: 0.0226
trigger times: 8
Loss after 2128100940 batches: 0.0221
trigger times: 9
Loss after 2128179540 batches: 0.0219
trigger times: 10
Loss after 2128258140 batches: 0.0218
trigger times: 11
Loss after 2128336740 batches: 0.0214
trigger times: 12
Loss after 2128415340 batches: 0.0212
trigger times: 13
Loss after 2128493940 batches: 0.0215
trigger times: 14
Loss after 2128572540 batches: 0.0211
trigger times: 0
Loss after 2128651140 batches: 0.0204
trigger times: 1
Loss after 2128729740 batches: 0.0201
trigger times: 2
Loss after 2128808340 batches: 0.0205
trigger times: 3
Loss after 2128886940 batches: 0.0199
trigger times: 4
Loss after 2128965540 batches: 0.0197
trigger times: 5
Loss after 2129044140 batches: 0.0199
trigger times: 6
Loss after 2129122740 batches: 0.0194
trigger times: 7
Loss after 2129201340 batches: 0.0197
trigger times: 8
Loss after 2129279940 batches: 0.0197
trigger times: 9
Loss after 2129358540 batches: 0.0191
trigger times: 10
Loss after 2129437140 batches: 0.0194
trigger times: 11
Loss after 2129515740 batches: 0.0188
trigger times: 12
Loss after 2129594340 batches: 0.0187
trigger times: 13
Loss after 2129672940 batches: 0.0191
trigger times: 14
Loss after 2129751540 batches: 0.0188
trigger times: 15
Loss after 2129830140 batches: 0.0184
trigger times: 16
Loss after 2129908740 batches: 0.0188
trigger times: 17
Loss after 2129987340 batches: 0.0185
trigger times: 18
Loss after 2130065940 batches: 0.0187
trigger times: 19
Loss after 2130144540 batches: 0.0188
trigger times: 20
Early stopping!
Start to test process.
Loss after 2130223140 batches: 0.0183
Time to train on one home:  267.3251554965973
trigger times: 0
Loss after 2130354240 batches: 0.1242
trigger times: 1
Loss after 2130485340 batches: 0.0342
trigger times: 2
Loss after 2130616440 batches: 0.0252
trigger times: 3
Loss after 2130747540 batches: 0.0220
trigger times: 0
Loss after 2130878640 batches: 0.0193
trigger times: 1
Loss after 2131009740 batches: 0.0184
trigger times: 2
Loss after 2131140840 batches: 0.0173
trigger times: 3
Loss after 2131271940 batches: 0.0166
trigger times: 4
Loss after 2131403040 batches: 0.0156
trigger times: 5
Loss after 2131534140 batches: 0.0154
trigger times: 6
Loss after 2131665240 batches: 0.0149
trigger times: 0
Loss after 2131796340 batches: 0.0148
trigger times: 1
Loss after 2131927440 batches: 0.0142
trigger times: 2
Loss after 2132058540 batches: 0.0140
trigger times: 0
Loss after 2132189640 batches: 0.0137
trigger times: 1
Loss after 2132320740 batches: 0.0136
trigger times: 2
Loss after 2132451840 batches: 0.0135
trigger times: 0
Loss after 2132582940 batches: 0.0131
trigger times: 1
Loss after 2132714040 batches: 0.0130
trigger times: 2
Loss after 2132845140 batches: 0.0124
trigger times: 3
Loss after 2132976240 batches: 0.0123
trigger times: 4
Loss after 2133107340 batches: 0.0122
trigger times: 5
Loss after 2133238440 batches: 0.0123
trigger times: 0
Loss after 2133369540 batches: 0.0118
trigger times: 0
Loss after 2133500640 batches: 0.0119
trigger times: 0
Loss after 2133631740 batches: 0.0116
trigger times: 0
Loss after 2133762840 batches: 0.0116
trigger times: 0
Loss after 2133893940 batches: 0.0117
trigger times: 1
Loss after 2134025040 batches: 0.0114
trigger times: 2
Loss after 2134156140 batches: 0.0116
trigger times: 0
Loss after 2134287240 batches: 0.0113
trigger times: 1
Loss after 2134418340 batches: 0.0111
trigger times: 2
Loss after 2134549440 batches: 0.0111
trigger times: 3
Loss after 2134680540 batches: 0.0112
trigger times: 4
Loss after 2134811640 batches: 0.0108
trigger times: 5
Loss after 2134942740 batches: 0.0105
trigger times: 0
Loss after 2135073840 batches: 0.0105
trigger times: 1
Loss after 2135204940 batches: 0.0105
trigger times: 2
Loss after 2135336040 batches: 0.0106
trigger times: 3
Loss after 2135467140 batches: 0.0104
trigger times: 4
Loss after 2135598240 batches: 0.0102
trigger times: 5
Loss after 2135729340 batches: 0.0103
trigger times: 6
Loss after 2135860440 batches: 0.0101
trigger times: 7
Loss after 2135991540 batches: 0.0101
trigger times: 8
Loss after 2136122640 batches: 0.0101
trigger times: 9
Loss after 2136253740 batches: 0.0099
trigger times: 10
Loss after 2136384840 batches: 0.0100
trigger times: 0
Loss after 2136515940 batches: 0.0100
trigger times: 1
Loss after 2136647040 batches: 0.0099
trigger times: 2
Loss after 2136778140 batches: 0.0098
trigger times: 3
Loss after 2136909240 batches: 0.0095
trigger times: 4
Loss after 2137040340 batches: 0.0096
trigger times: 5
Loss after 2137171440 batches: 0.0095
trigger times: 6
Loss after 2137302540 batches: 0.0096
trigger times: 7
Loss after 2137433640 batches: 0.0094
trigger times: 8
Loss after 2137564740 batches: 0.0094
trigger times: 9
Loss after 2137695840 batches: 0.0094
trigger times: 10
Loss after 2137826940 batches: 0.0093
trigger times: 11
Loss after 2137958040 batches: 0.0093
trigger times: 12
Loss after 2138089140 batches: 0.0094
trigger times: 13
Loss after 2138220240 batches: 0.0094
trigger times: 14
Loss after 2138351340 batches: 0.0093
trigger times: 15
Loss after 2138482440 batches: 0.0090
trigger times: 16
Loss after 2138613540 batches: 0.0090
trigger times: 17
Loss after 2138744640 batches: 0.0089
trigger times: 18
Loss after 2138875740 batches: 0.0089
trigger times: 19
Loss after 2139006840 batches: 0.0089
trigger times: 20
Early stopping!
Start to test process.
Loss after 2139137940 batches: 0.0089
Time to train on one home:  505.6534492969513
trigger times: 0
Loss after 2139269040 batches: 0.1436
trigger times: 0
Loss after 2139400140 batches: 0.0433
trigger times: 1
Loss after 2139531240 batches: 0.0319
trigger times: 2
Loss after 2139662340 batches: 0.0273
trigger times: 3
Loss after 2139793440 batches: 0.0250
trigger times: 4
Loss after 2139924540 batches: 0.0237
trigger times: 0
Loss after 2140055640 batches: 0.0224
trigger times: 1
Loss after 2140186740 batches: 0.0212
trigger times: 2
Loss after 2140317840 batches: 0.0203
trigger times: 3
Loss after 2140448940 batches: 0.0197
trigger times: 4
Loss after 2140580040 batches: 0.0190
trigger times: 5
Loss after 2140711140 batches: 0.0187
trigger times: 0
Loss after 2140842240 batches: 0.0184
trigger times: 1
Loss after 2140973340 batches: 0.0179
trigger times: 0
Loss after 2141104440 batches: 0.0172
trigger times: 1
Loss after 2141235540 batches: 0.0175
trigger times: 2
Loss after 2141366640 batches: 0.0167
trigger times: 3
Loss after 2141497740 batches: 0.0163
trigger times: 0
Loss after 2141628840 batches: 0.0165
trigger times: 1
Loss after 2141759940 batches: 0.0161
trigger times: 2
Loss after 2141891040 batches: 0.0160
trigger times: 3
Loss after 2142022140 batches: 0.0159
trigger times: 4
Loss after 2142153240 batches: 0.0154
trigger times: 5
Loss after 2142284340 batches: 0.0155
trigger times: 6
Loss after 2142415440 batches: 0.0153
trigger times: 7
Loss after 2142546540 batches: 0.0149
trigger times: 0
Loss after 2142677640 batches: 0.0150
trigger times: 1
Loss after 2142808740 batches: 0.0147
trigger times: 2
Loss after 2142939840 batches: 0.0146
trigger times: 3
Loss after 2143070940 batches: 0.0143
trigger times: 4
Loss after 2143202040 batches: 0.0142
trigger times: 5
Loss after 2143333140 batches: 0.0143
trigger times: 6
Loss after 2143464240 batches: 0.0144
trigger times: 7
Loss after 2143595340 batches: 0.0141
trigger times: 8
Loss after 2143726440 batches: 0.0137
trigger times: 9
Loss after 2143857540 batches: 0.0137
trigger times: 10
Loss after 2143988640 batches: 0.0138
trigger times: 11
Loss after 2144119740 batches: 0.0134
trigger times: 12
Loss after 2144250840 batches: 0.0133
trigger times: 13
Loss after 2144381940 batches: 0.0134
trigger times: 14
Loss after 2144513040 batches: 0.0132
trigger times: 15
Loss after 2144644140 batches: 0.0132
trigger times: 0
Loss after 2144775240 batches: 0.0132
trigger times: 1
Loss after 2144906340 batches: 0.0132
trigger times: 2
Loss after 2145037440 batches: 0.0128
trigger times: 3
Loss after 2145168540 batches: 0.0127
trigger times: 4
Loss after 2145299640 batches: 0.0129
trigger times: 5
Loss after 2145430740 batches: 0.0129
trigger times: 6
Loss after 2145561840 batches: 0.0126
trigger times: 7
Loss after 2145692940 batches: 0.0126
trigger times: 8
Loss after 2145824040 batches: 0.0124
trigger times: 9
Loss after 2145955140 batches: 0.0123
trigger times: 10
Loss after 2146086240 batches: 0.0123
trigger times: 11
Loss after 2146217340 batches: 0.0124
trigger times: 12
Loss after 2146348440 batches: 0.0122
trigger times: 13
Loss after 2146479540 batches: 0.0121
trigger times: 14
Loss after 2146610640 batches: 0.0123
trigger times: 15
Loss after 2146741740 batches: 0.0123
trigger times: 16
Loss after 2146872840 batches: 0.0121
trigger times: 17
Loss after 2147003940 batches: 0.0119
trigger times: 18
Loss after 2147135040 batches: 0.0117
trigger times: 19
Loss after 2147266140 batches: 0.0121
trigger times: 20
Early stopping!
Start to test process.
Loss after 2147397240 batches: 0.0119
Time to train on one home:  469.3932509422302
trigger times: 0
Loss after 2147528340 batches: 0.2442
trigger times: 0
Loss after 2147659440 batches: 0.0672
trigger times: 0
Loss after 2147790540 batches: 0.0466
trigger times: 0
Loss after 2147921640 batches: 0.0405
trigger times: 0
Loss after 2148052740 batches: 0.0360
trigger times: 0
Loss after 2148183840 batches: 0.0347
trigger times: 0
Loss after 2148314940 batches: 0.0312
trigger times: 0
Loss after 2148446040 batches: 0.0301
trigger times: 1
Loss after 2148577140 batches: 0.0282
trigger times: 2
Loss after 2148708240 batches: 0.0277
trigger times: 3
Loss after 2148839340 batches: 0.0270
trigger times: 0
Loss after 2148970440 batches: 0.0268
trigger times: 0
Loss after 2149101540 batches: 0.0258
trigger times: 1
Loss after 2149232640 batches: 0.0251
trigger times: 2
Loss after 2149363740 batches: 0.0245
trigger times: 3
Loss after 2149494840 batches: 0.0246
trigger times: 4
Loss after 2149625940 batches: 0.0244
trigger times: 5
Loss after 2149757040 batches: 0.0232
trigger times: 6
Loss after 2149888140 batches: 0.0232
trigger times: 7
Loss after 2150019240 batches: 0.0230
trigger times: 8
Loss after 2150150340 batches: 0.0226
trigger times: 9
Loss after 2150281440 batches: 0.0219
trigger times: 10
Loss after 2150412540 batches: 0.0221
trigger times: 0
Loss after 2150543640 batches: 0.0221
trigger times: 1
Loss after 2150674740 batches: 0.0214
trigger times: 2
Loss after 2150805840 batches: 0.0217
trigger times: 3
Loss after 2150936940 batches: 0.0213
trigger times: 4
Loss after 2151068040 batches: 0.0207
trigger times: 5
Loss after 2151199140 batches: 0.0214
trigger times: 6
Loss after 2151330240 batches: 0.0210
trigger times: 7
Loss after 2151461340 batches: 0.0204
trigger times: 8
Loss after 2151592440 batches: 0.0202
trigger times: 9
Loss after 2151723540 batches: 0.0204
trigger times: 0
Loss after 2151854640 batches: 0.0198
trigger times: 1
Loss after 2151985740 batches: 0.0192
trigger times: 2
Loss after 2152116840 batches: 0.0193
trigger times: 3
Loss after 2152247940 batches: 0.0201
trigger times: 4
Loss after 2152379040 batches: 0.0193
trigger times: 5
Loss after 2152510140 batches: 0.0195
trigger times: 6
Loss after 2152641240 batches: 0.0194
trigger times: 7
Loss after 2152772340 batches: 0.0194
trigger times: 8
Loss after 2152903440 batches: 0.0196
trigger times: 9
Loss after 2153034540 batches: 0.0190
trigger times: 0
Loss after 2153165640 batches: 0.0192
trigger times: 1
Loss after 2153296740 batches: 0.0192
trigger times: 2
Loss after 2153427840 batches: 0.0188
trigger times: 3
Loss after 2153558940 batches: 0.0182
trigger times: 4
Loss after 2153690040 batches: 0.0189
trigger times: 5
Loss after 2153821140 batches: 0.0182
trigger times: 6
Loss after 2153952240 batches: 0.0183
trigger times: 7
Loss after 2154083340 batches: 0.0185
trigger times: 8
Loss after 2154214440 batches: 0.0180
trigger times: 9
Loss after 2154345540 batches: 0.0181
trigger times: 10
Loss after 2154476640 batches: 0.0176
trigger times: 11
Loss after 2154607740 batches: 0.0179
trigger times: 12
Loss after 2154738840 batches: 0.0180
trigger times: 0
Loss after 2154869940 batches: 0.0175
trigger times: 1
Loss after 2155001040 batches: 0.0174
trigger times: 2
Loss after 2155132140 batches: 0.0175
trigger times: 3
Loss after 2155263240 batches: 0.0175
trigger times: 4
Loss after 2155394340 batches: 0.0174
trigger times: 5
Loss after 2155525440 batches: 0.0172
trigger times: 6
Loss after 2155656540 batches: 0.0178
trigger times: 7
Loss after 2155787640 batches: 0.0169
trigger times: 8
Loss after 2155918740 batches: 0.0170
trigger times: 9
Loss after 2156049840 batches: 0.0171
trigger times: 10
Loss after 2156180940 batches: 0.0175
trigger times: 11
Loss after 2156312040 batches: 0.0164
trigger times: 12
Loss after 2156443140 batches: 0.0170
trigger times: 13
Loss after 2156574240 batches: 0.0170
trigger times: 14
Loss after 2156705340 batches: 0.0168
trigger times: 15
Loss after 2156836440 batches: 0.0172
trigger times: 16
Loss after 2156967540 batches: 0.0162
trigger times: 17
Loss after 2157098640 batches: 0.0165
trigger times: 18
Loss after 2157229740 batches: 0.0162
trigger times: 19
Loss after 2157360840 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 2157491940 batches: 0.0158
Time to train on one home:  569.8325607776642
trigger times: 0
Loss after 2157623040 batches: 0.3311
trigger times: 1
Loss after 2157754140 batches: 0.1128
trigger times: 2
Loss after 2157885240 batches: 0.0783
trigger times: 3
Loss after 2158016340 batches: 0.0660
trigger times: 4
Loss after 2158147440 batches: 0.0595
trigger times: 5
Loss after 2158278540 batches: 0.0547
trigger times: 6
Loss after 2158409640 batches: 0.0519
trigger times: 7
Loss after 2158540740 batches: 0.0502
trigger times: 8
Loss after 2158671840 batches: 0.0476
trigger times: 9
Loss after 2158802940 batches: 0.0455
trigger times: 10
Loss after 2158934040 batches: 0.0445
trigger times: 11
Loss after 2159065140 batches: 0.0428
trigger times: 12
Loss after 2159196240 batches: 0.0417
trigger times: 13
Loss after 2159327340 batches: 0.0408
trigger times: 14
Loss after 2159458440 batches: 0.0398
trigger times: 15
Loss after 2159589540 batches: 0.0389
trigger times: 16
Loss after 2159720640 batches: 0.0382
trigger times: 17
Loss after 2159851740 batches: 0.0373
trigger times: 18
Loss after 2159982840 batches: 0.0372
trigger times: 19
Loss after 2160113940 batches: 0.0362
trigger times: 20
Early stopping!
Start to test process.
Loss after 2160245040 batches: 0.0362
Time to train on one home:  164.91290068626404
trigger times: 0
Loss after 2160376140 batches: 0.2444
trigger times: 0
Loss after 2160507240 batches: 0.0733
trigger times: 1
Loss after 2160638340 batches: 0.0521
trigger times: 0
Loss after 2160769440 batches: 0.0434
trigger times: 1
Loss after 2160900540 batches: 0.0396
trigger times: 2
Loss after 2161031640 batches: 0.0365
trigger times: 3
Loss after 2161162740 batches: 0.0349
trigger times: 4
Loss after 2161293840 batches: 0.0324
trigger times: 5
Loss after 2161424940 batches: 0.0319
trigger times: 6
Loss after 2161556040 batches: 0.0308
trigger times: 7
Loss after 2161687140 batches: 0.0301
trigger times: 0
Loss after 2161818240 batches: 0.0285
trigger times: 1
Loss after 2161949340 batches: 0.0288
trigger times: 2
Loss after 2162080440 batches: 0.0271
trigger times: 3
Loss after 2162211540 batches: 0.0263
trigger times: 4
Loss after 2162342640 batches: 0.0271
trigger times: 5
Loss after 2162473740 batches: 0.0260
trigger times: 6
Loss after 2162604840 batches: 0.0253
trigger times: 7
Loss after 2162735940 batches: 0.0250
trigger times: 8
Loss after 2162867040 batches: 0.0247
trigger times: 9
Loss after 2162998140 batches: 0.0246
trigger times: 10
Loss after 2163129240 batches: 0.0246
trigger times: 11
Loss after 2163260340 batches: 0.0234
trigger times: 12
Loss after 2163391440 batches: 0.0233
trigger times: 13
Loss after 2163522540 batches: 0.0236
trigger times: 14
Loss after 2163653640 batches: 0.0231
trigger times: 15
Loss after 2163784740 batches: 0.0230
trigger times: 16
Loss after 2163915840 batches: 0.0224
trigger times: 17
Loss after 2164046940 batches: 0.0224
trigger times: 18
Loss after 2164178040 batches: 0.0216
trigger times: 19
Loss after 2164309140 batches: 0.0218
trigger times: 20
Early stopping!
Start to test process.
Loss after 2164440240 batches: 0.0221
Time to train on one home:  243.47618222236633
trigger times: 0
Loss after 2164571340 batches: 0.4089
trigger times: 0
Loss after 2164702440 batches: 0.1157
trigger times: 1
Loss after 2164833540 batches: 0.0734
trigger times: 2
Loss after 2164964640 batches: 0.0606
trigger times: 3
Loss after 2165095740 batches: 0.0534
trigger times: 4
Loss after 2165226840 batches: 0.0491
trigger times: 5
Loss after 2165357940 batches: 0.0461
trigger times: 6
Loss after 2165489040 batches: 0.0431
trigger times: 7
Loss after 2165620140 batches: 0.0413
trigger times: 8
Loss after 2165751240 batches: 0.0413
trigger times: 9
Loss after 2165882340 batches: 0.0386
trigger times: 10
Loss after 2166013440 batches: 0.0372
trigger times: 11
Loss after 2166144540 batches: 0.0357
trigger times: 12
Loss after 2166275640 batches: 0.0347
trigger times: 13
Loss after 2166406740 batches: 0.0342
trigger times: 14
Loss after 2166537840 batches: 0.0328
trigger times: 15
Loss after 2166668940 batches: 0.0326
trigger times: 16
Loss after 2166800040 batches: 0.0317
trigger times: 17
Loss after 2166931140 batches: 0.0311
trigger times: 18
Loss after 2167062240 batches: 0.0306
trigger times: 19
Loss after 2167193340 batches: 0.0302
trigger times: 20
Early stopping!
Start to test process.
Loss after 2167324440 batches: 0.0297
Time to train on one home:  173.4693205356598
trigger times: 0
Loss after 2167455540 batches: 0.4217
trigger times: 0
Loss after 2167586640 batches: 0.1225
trigger times: 0
Loss after 2167717740 batches: 0.0874
trigger times: 1
Loss after 2167848840 batches: 0.0734
trigger times: 0
Loss after 2167979940 batches: 0.0664
trigger times: 0
Loss after 2168111040 batches: 0.0613
trigger times: 1
Loss after 2168242140 batches: 0.0566
trigger times: 2
Loss after 2168373240 batches: 0.0546
trigger times: 0
Loss after 2168504340 batches: 0.0524
trigger times: 0
Loss after 2168635440 batches: 0.0509
trigger times: 1
Loss after 2168766540 batches: 0.0483
trigger times: 2
Loss after 2168897640 batches: 0.0478
trigger times: 3
Loss after 2169028740 batches: 0.0458
trigger times: 4
Loss after 2169159840 batches: 0.0448
trigger times: 5
Loss after 2169290940 batches: 0.0437
trigger times: 6
Loss after 2169422040 batches: 0.0430
trigger times: 7
Loss after 2169553140 batches: 0.0422
trigger times: 0
Loss after 2169684240 batches: 0.0416
trigger times: 0
Loss after 2169815340 batches: 0.0411
trigger times: 1
Loss after 2169946440 batches: 0.0405
trigger times: 2
Loss after 2170077540 batches: 0.0395
trigger times: 3
Loss after 2170208640 batches: 0.0390
trigger times: 4
Loss after 2170339740 batches: 0.0388
trigger times: 5
Loss after 2170470840 batches: 0.0381
trigger times: 6
Loss after 2170601940 batches: 0.0377
trigger times: 7
Loss after 2170733040 batches: 0.0371
trigger times: 8
Loss after 2170864140 batches: 0.0374
trigger times: 9
Loss after 2170995240 batches: 0.0372
trigger times: 10
Loss after 2171126340 batches: 0.0365
trigger times: 0
Loss after 2171257440 batches: 0.0363
trigger times: 1
Loss after 2171388540 batches: 0.0354
trigger times: 0
Loss after 2171519640 batches: 0.0352
trigger times: 1
Loss after 2171650740 batches: 0.0354
trigger times: 2
Loss after 2171781840 batches: 0.0347
trigger times: 0
Loss after 2171912940 batches: 0.0339
trigger times: 1
Loss after 2172044040 batches: 0.0342
trigger times: 2
Loss after 2172175140 batches: 0.0341
trigger times: 3
Loss after 2172306240 batches: 0.0338
trigger times: 4
Loss after 2172437340 batches: 0.0333
trigger times: 0
Loss after 2172568440 batches: 0.0330
trigger times: 0
Loss after 2172699540 batches: 0.0328
trigger times: 1
Loss after 2172830640 batches: 0.0334
trigger times: 2
Loss after 2172961740 batches: 0.0328
trigger times: 3
Loss after 2173092840 batches: 0.0327
trigger times: 0
Loss after 2173223940 batches: 0.0324
trigger times: 1
Loss after 2173355040 batches: 0.0320
trigger times: 2
Loss after 2173486140 batches: 0.0319
trigger times: 3
Loss after 2173617240 batches: 0.0317
trigger times: 4
Loss after 2173748340 batches: 0.0318
trigger times: 5
Loss after 2173879440 batches: 0.0312
trigger times: 6
Loss after 2174010540 batches: 0.0311
trigger times: 7
Loss after 2174141640 batches: 0.0312
trigger times: 8
Loss after 2174272740 batches: 0.0310
trigger times: 9
Loss after 2174403840 batches: 0.0302
trigger times: 10
Loss after 2174534940 batches: 0.0297
trigger times: 11
Loss after 2174666040 batches: 0.0305
trigger times: 12
Loss after 2174797140 batches: 0.0299
trigger times: 13
Loss after 2174928240 batches: 0.0303
trigger times: 0
Loss after 2175059340 batches: 0.0295
trigger times: 1
Loss after 2175190440 batches: 0.0292
trigger times: 2
Loss after 2175321540 batches: 0.0296
trigger times: 0
Loss after 2175452640 batches: 0.0292
trigger times: 1
Loss after 2175583740 batches: 0.0294
trigger times: 2
Loss after 2175714840 batches: 0.0294
trigger times: 3
Loss after 2175845940 batches: 0.0287
trigger times: 4
Loss after 2175977040 batches: 0.0292
trigger times: 5
Loss after 2176108140 batches: 0.0285
trigger times: 6
Loss after 2176239240 batches: 0.0286
trigger times: 0
Loss after 2176370340 batches: 0.0281
trigger times: 1
Loss after 2176501440 batches: 0.0285
trigger times: 0
Loss after 2176632540 batches: 0.0283
trigger times: 1
Loss after 2176763640 batches: 0.0279
trigger times: 2
Loss after 2176894740 batches: 0.0277
trigger times: 0
Loss after 2177025840 batches: 0.0280
trigger times: 1
Loss after 2177156940 batches: 0.0277
trigger times: 2
Loss after 2177288040 batches: 0.0280
trigger times: 3
Loss after 2177419140 batches: 0.0274
trigger times: 4
Loss after 2177550240 batches: 0.0276
trigger times: 5
Loss after 2177681340 batches: 0.0277
trigger times: 6
Loss after 2177812440 batches: 0.0276
trigger times: 7
Loss after 2177943540 batches: 0.0271
trigger times: 8
Loss after 2178074640 batches: 0.0273
trigger times: 9
Loss after 2178205740 batches: 0.0273
trigger times: 10
Loss after 2178336840 batches: 0.0266
trigger times: 11
Loss after 2178467940 batches: 0.0271
trigger times: 0
Loss after 2178599040 batches: 0.0264
trigger times: 1
Loss after 2178730140 batches: 0.0266
trigger times: 2
Loss after 2178861240 batches: 0.0266
trigger times: 0
Loss after 2178992340 batches: 0.0264
trigger times: 1
Loss after 2179123440 batches: 0.0265
trigger times: 2
Loss after 2179254540 batches: 0.0261
trigger times: 3
Loss after 2179385640 batches: 0.0260
trigger times: 4
Loss after 2179516740 batches: 0.0260
trigger times: 5
Loss after 2179647840 batches: 0.0265
trigger times: 6
Loss after 2179778940 batches: 0.0263
trigger times: 7
Loss after 2179910040 batches: 0.0261
trigger times: 8
Loss after 2180041140 batches: 0.0258
trigger times: 9
Loss after 2180172240 batches: 0.0259
trigger times: 10
Loss after 2180303340 batches: 0.0261
trigger times: 11
Loss after 2180434440 batches: 0.0264
trigger times: 12
Loss after 2180565540 batches: 0.0257
trigger times: 0
Loss after 2180696640 batches: 0.0255
trigger times: 1
Loss after 2180827740 batches: 0.0258
trigger times: 2
Loss after 2180958840 batches: 0.0257
trigger times: 3
Loss after 2181089940 batches: 0.0257
trigger times: 0
Loss after 2181221040 batches: 0.0257
trigger times: 0
Loss after 2181352140 batches: 0.0257
trigger times: 1
Loss after 2181483240 batches: 0.0253
trigger times: 2
Loss after 2181614340 batches: 0.0251
trigger times: 3
Loss after 2181745440 batches: 0.0253
trigger times: 4
Loss after 2181876540 batches: 0.0252
trigger times: 5
Loss after 2182007640 batches: 0.0253
trigger times: 6
Loss after 2182138740 batches: 0.0250
trigger times: 7
Loss after 2182269840 batches: 0.0250
trigger times: 8
Loss after 2182400940 batches: 0.0250
trigger times: 9
Loss after 2182532040 batches: 0.0246
trigger times: 10
Loss after 2182663140 batches: 0.0246
trigger times: 11
Loss after 2182794240 batches: 0.0249
trigger times: 12
Loss after 2182925340 batches: 0.0248
trigger times: 13
Loss after 2183056440 batches: 0.0243
trigger times: 14
Loss after 2183187540 batches: 0.0243
trigger times: 15
Loss after 2183318640 batches: 0.0246
trigger times: 16
Loss after 2183449740 batches: 0.0242
trigger times: 17
Loss after 2183580840 batches: 0.0242
trigger times: 18
Loss after 2183711940 batches: 0.0240
trigger times: 19
Loss after 2183843040 batches: 0.0239
trigger times: 20
Early stopping!
Start to test process.
Loss after 2183974140 batches: 0.0240
Time to train on one home:  934.9761004447937
trigger times: 0
Loss after 2184068100 batches: 0.4567
trigger times: 0
Loss after 2184162060 batches: 0.1339
trigger times: 1
Loss after 2184256020 batches: 0.0836
trigger times: 0
Loss after 2184349980 batches: 0.0673
trigger times: 1
Loss after 2184443940 batches: 0.0594
trigger times: 0
Loss after 2184537900 batches: 0.0532
trigger times: 1
Loss after 2184631860 batches: 0.0501
trigger times: 2
Loss after 2184725820 batches: 0.0475
trigger times: 0
Loss after 2184819780 batches: 0.0454
trigger times: 0
Loss after 2184913740 batches: 0.0433
trigger times: 1
Loss after 2185007700 batches: 0.0418
trigger times: 2
Loss after 2185101660 batches: 0.0416
trigger times: 3
Loss after 2185195620 batches: 0.0398
trigger times: 4
Loss after 2185289580 batches: 0.0385
trigger times: 5
Loss after 2185383540 batches: 0.0374
trigger times: 0
Loss after 2185477500 batches: 0.0367
trigger times: 0
Loss after 2185571460 batches: 0.0359
trigger times: 1
Loss after 2185665420 batches: 0.0350
trigger times: 2
Loss after 2185759380 batches: 0.0349
trigger times: 3
Loss after 2185853340 batches: 0.0336
trigger times: 0
Loss after 2185947300 batches: 0.0331
trigger times: 1
Loss after 2186041260 batches: 0.0330
trigger times: 2
Loss after 2186135220 batches: 0.0325
trigger times: 3
Loss after 2186229180 batches: 0.0326
trigger times: 4
Loss after 2186323140 batches: 0.0324
trigger times: 5
Loss after 2186417100 batches: 0.0311
trigger times: 6
Loss after 2186511060 batches: 0.0310
trigger times: 0
Loss after 2186605020 batches: 0.0307
trigger times: 1
Loss after 2186698980 batches: 0.0302
trigger times: 2
Loss after 2186792940 batches: 0.0306
trigger times: 3
Loss after 2186886900 batches: 0.0299
trigger times: 4
Loss after 2186980860 batches: 0.0304
trigger times: 5
Loss after 2187074820 batches: 0.0293
trigger times: 0
Loss after 2187168780 batches: 0.0290
trigger times: 0
Loss after 2187262740 batches: 0.0293
trigger times: 1
Loss after 2187356700 batches: 0.0288
trigger times: 2
Loss after 2187450660 batches: 0.0283
trigger times: 3
Loss after 2187544620 batches: 0.0286
trigger times: 4
Loss after 2187638580 batches: 0.0286
trigger times: 0
Loss after 2187732540 batches: 0.0277
trigger times: 1
Loss after 2187826500 batches: 0.0279
trigger times: 2
Loss after 2187920460 batches: 0.0274
trigger times: 3
Loss after 2188014420 batches: 0.0272
trigger times: 4
Loss after 2188108380 batches: 0.0270
trigger times: 5
Loss after 2188202340 batches: 0.0270
trigger times: 6
Loss after 2188296300 batches: 0.0269
trigger times: 7
Loss after 2188390260 batches: 0.0269
trigger times: 0
Loss after 2188484220 batches: 0.0261
trigger times: 1
Loss after 2188578180 batches: 0.0266
trigger times: 2
Loss after 2188672140 batches: 0.0265
trigger times: 3
Loss after 2188766100 batches: 0.0265
trigger times: 4
Loss after 2188860060 batches: 0.0261
trigger times: 5
Loss after 2188954020 batches: 0.0256
trigger times: 6
Loss after 2189047980 batches: 0.0255
trigger times: 7
Loss after 2189141940 batches: 0.0253
trigger times: 8
Loss after 2189235900 batches: 0.0258
trigger times: 9
Loss after 2189329860 batches: 0.0253
trigger times: 10
Loss after 2189423820 batches: 0.0253
trigger times: 0
Loss after 2189517780 batches: 0.0251
trigger times: 1
Loss after 2189611740 batches: 0.0250
trigger times: 2
Loss after 2189705700 batches: 0.0249
trigger times: 3
Loss after 2189799660 batches: 0.0249
trigger times: 4
Loss after 2189893620 batches: 0.0248
trigger times: 5
Loss after 2189987580 batches: 0.0246
trigger times: 6
Loss after 2190081540 batches: 0.0246
trigger times: 7
Loss after 2190175500 batches: 0.0244
trigger times: 8
Loss after 2190269460 batches: 0.0240
trigger times: 9
Loss after 2190363420 batches: 0.0243
trigger times: 10
Loss after 2190457380 batches: 0.0240
trigger times: 11
Loss after 2190551340 batches: 0.0240
trigger times: 12
Loss after 2190645300 batches: 0.0248
trigger times: 13
Loss after 2190739260 batches: 0.0236
trigger times: 14
Loss after 2190833220 batches: 0.0234
trigger times: 0
Loss after 2190927180 batches: 0.0236
trigger times: 1
Loss after 2191021140 batches: 0.0233
trigger times: 2
Loss after 2191115100 batches: 0.0240
trigger times: 3
Loss after 2191209060 batches: 0.0231
trigger times: 4
Loss after 2191303020 batches: 0.0232
trigger times: 0
Loss after 2191396980 batches: 0.0229
trigger times: 1
Loss after 2191490940 batches: 0.0230
trigger times: 0
Loss after 2191584900 batches: 0.0228
trigger times: 1
Loss after 2191678860 batches: 0.0230
trigger times: 2
Loss after 2191772820 batches: 0.0232
trigger times: 3
Loss after 2191866780 batches: 0.0226
trigger times: 4
Loss after 2191960740 batches: 0.0227
trigger times: 5
Loss after 2192054700 batches: 0.0227
trigger times: 6
Loss after 2192148660 batches: 0.0224
trigger times: 7
Loss after 2192242620 batches: 0.0221
trigger times: 8
Loss after 2192336580 batches: 0.0223
trigger times: 9
Loss after 2192430540 batches: 0.0225
trigger times: 10
Loss after 2192524500 batches: 0.0221
trigger times: 11
Loss after 2192618460 batches: 0.0221
trigger times: 12
Loss after 2192712420 batches: 0.0220
trigger times: 13
Loss after 2192806380 batches: 0.0220
trigger times: 14
Loss after 2192900340 batches: 0.0221
trigger times: 15
Loss after 2192994300 batches: 0.0222
trigger times: 16
Loss after 2193088260 batches: 0.0221
trigger times: 17
Loss after 2193182220 batches: 0.0217
trigger times: 18
Loss after 2193276180 batches: 0.0217
trigger times: 19
Loss after 2193370140 batches: 0.0214
trigger times: 20
Early stopping!
Start to test process.
Loss after 2193464100 batches: 0.0218
Time to train on one home:  560.9407305717468
trigger times: 0
Loss after 2193595200 batches: 0.0623
trigger times: 1
Loss after 2193726300 batches: 0.0121
trigger times: 2
Loss after 2193857400 batches: 0.0088
trigger times: 3
Loss after 2193988500 batches: 0.0075
trigger times: 4
Loss after 2194119600 batches: 0.0066
trigger times: 5
Loss after 2194250700 batches: 0.0062
trigger times: 6
Loss after 2194381800 batches: 0.0055
trigger times: 7
Loss after 2194512900 batches: 0.0052
trigger times: 8
Loss after 2194644000 batches: 0.0050
trigger times: 9
Loss after 2194775100 batches: 0.0048
trigger times: 10
Loss after 2194906200 batches: 0.0046
trigger times: 11
Loss after 2195037300 batches: 0.0044
trigger times: 12
Loss after 2195168400 batches: 0.0042
trigger times: 13
Loss after 2195299500 batches: 0.0042
trigger times: 14
Loss after 2195430600 batches: 0.0040
trigger times: 15
Loss after 2195561700 batches: 0.0041
trigger times: 16
Loss after 2195692800 batches: 0.0038
trigger times: 17
Loss after 2195823900 batches: 0.0039
trigger times: 18
Loss after 2195955000 batches: 0.0037
trigger times: 19
Loss after 2196086100 batches: 0.0036
trigger times: 20
Early stopping!
Start to test process.
Loss after 2196217200 batches: 0.0035
Time to train on one home:  164.17837762832642
trigger times: 0
Loss after 2196348300 batches: 0.1535
trigger times: 0
Loss after 2196479400 batches: 0.0464
trigger times: 0
Loss after 2196610500 batches: 0.0323
trigger times: 1
Loss after 2196741600 batches: 0.0271
trigger times: 0
Loss after 2196872700 batches: 0.0241
trigger times: 0
Loss after 2197003800 batches: 0.0222
trigger times: 0
Loss after 2197134900 batches: 0.0204
trigger times: 1
Loss after 2197266000 batches: 0.0198
trigger times: 2
Loss after 2197397100 batches: 0.0183
trigger times: 3
Loss after 2197528200 batches: 0.0177
trigger times: 4
Loss after 2197659300 batches: 0.0171
trigger times: 5
Loss after 2197790400 batches: 0.0166
trigger times: 6
Loss after 2197921500 batches: 0.0161
trigger times: 7
Loss after 2198052600 batches: 0.0157
trigger times: 8
Loss after 2198183700 batches: 0.0154
trigger times: 9
Loss after 2198314800 batches: 0.0150
trigger times: 10
Loss after 2198445900 batches: 0.0146
trigger times: 11
Loss after 2198577000 batches: 0.0145
trigger times: 12
Loss after 2198708100 batches: 0.0141
trigger times: 13
Loss after 2198839200 batches: 0.0139
trigger times: 14
Loss after 2198970300 batches: 0.0137
trigger times: 15
Loss after 2199101400 batches: 0.0135
trigger times: 16
Loss after 2199232500 batches: 0.0135
trigger times: 17
Loss after 2199363600 batches: 0.0133
trigger times: 18
Loss after 2199494700 batches: 0.0131
trigger times: 19
Loss after 2199625800 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 2199756900 batches: 0.0126
Time to train on one home:  208.38651394844055
trigger times: 0
Loss after 2199888000 batches: 0.2964
trigger times: 0
Loss after 2200019100 batches: 0.1019
trigger times: 0
Loss after 2200150200 batches: 0.0790
trigger times: 0
Loss after 2200281300 batches: 0.0713
trigger times: 0
Loss after 2200412400 batches: 0.0660
trigger times: 1
Loss after 2200543500 batches: 0.0617
trigger times: 2
Loss after 2200674600 batches: 0.0594
trigger times: 3
Loss after 2200805700 batches: 0.0574
trigger times: 4
Loss after 2200936800 batches: 0.0561
trigger times: 5
Loss after 2201067900 batches: 0.0545
trigger times: 6
Loss after 2201199000 batches: 0.0539
trigger times: 7
Loss after 2201330100 batches: 0.0520
trigger times: 8
Loss after 2201461200 batches: 0.0517
trigger times: 9
Loss after 2201592300 batches: 0.0507
trigger times: 10
Loss after 2201723400 batches: 0.0503
trigger times: 11
Loss after 2201854500 batches: 0.0489
trigger times: 0
Loss after 2201985600 batches: 0.0485
trigger times: 1
Loss after 2202116700 batches: 0.0483
trigger times: 2
Loss after 2202247800 batches: 0.0477
trigger times: 0
Loss after 2202378900 batches: 0.0471
trigger times: 1
Loss after 2202510000 batches: 0.0469
trigger times: 2
Loss after 2202641100 batches: 0.0464
trigger times: 3
Loss after 2202772200 batches: 0.0459
trigger times: 0
Loss after 2202903300 batches: 0.0456
trigger times: 1
Loss after 2203034400 batches: 0.0452
trigger times: 2
Loss after 2203165500 batches: 0.0459
trigger times: 3
Loss after 2203296600 batches: 0.0449
trigger times: 4
Loss after 2203427700 batches: 0.0446
trigger times: 5
Loss after 2203558800 batches: 0.0446
trigger times: 6
Loss after 2203689900 batches: 0.0442
trigger times: 7
Loss after 2203821000 batches: 0.0439
trigger times: 8
Loss after 2203952100 batches: 0.0453
trigger times: 9
Loss after 2204083200 batches: 0.0439
trigger times: 10
Loss after 2204214300 batches: 0.0434
trigger times: 11
Loss after 2204345400 batches: 0.0432
trigger times: 12
Loss after 2204476500 batches: 0.0428
trigger times: 13
Loss after 2204607600 batches: 0.0424
trigger times: 14
Loss after 2204738700 batches: 0.0423
trigger times: 15
Loss after 2204869800 batches: 0.0420
trigger times: 16
Loss after 2205000900 batches: 0.0428
trigger times: 17
Loss after 2205132000 batches: 0.0419
trigger times: 18
Loss after 2205263100 batches: 0.0420
trigger times: 0
Loss after 2205394200 batches: 0.0417
trigger times: 1
Loss after 2205525300 batches: 0.0415
trigger times: 2
Loss after 2205656400 batches: 0.0414
trigger times: 0
Loss after 2205787500 batches: 0.0413
trigger times: 1
Loss after 2205918600 batches: 0.0414
trigger times: 2
Loss after 2206049700 batches: 0.0408
trigger times: 3
Loss after 2206180800 batches: 0.0408
trigger times: 4
Loss after 2206311900 batches: 0.0412
trigger times: 5
Loss after 2206443000 batches: 0.0410
trigger times: 6
Loss after 2206574100 batches: 0.0404
trigger times: 7
Loss after 2206705200 batches: 0.0405
trigger times: 0
Loss after 2206836300 batches: 0.0407
trigger times: 1
Loss after 2206967400 batches: 0.0406
trigger times: 2
Loss after 2207098500 batches: 0.0402
trigger times: 3
Loss after 2207229600 batches: 0.0409
trigger times: 4
Loss after 2207360700 batches: 0.0398
trigger times: 5
Loss after 2207491800 batches: 0.0403
trigger times: 0
Loss after 2207622900 batches: 0.0404
trigger times: 0
Loss after 2207754000 batches: 0.0396
trigger times: 1
Loss after 2207885100 batches: 0.0393
trigger times: 2
Loss after 2208016200 batches: 0.0398
trigger times: 0
Loss after 2208147300 batches: 0.0393
trigger times: 1
Loss after 2208278400 batches: 0.0394
trigger times: 2
Loss after 2208409500 batches: 0.0392
trigger times: 3
Loss after 2208540600 batches: 0.0390
trigger times: 4
Loss after 2208671700 batches: 0.0389
trigger times: 5
Loss after 2208802800 batches: 0.0390
trigger times: 6
Loss after 2208933900 batches: 0.0390
trigger times: 7
Loss after 2209065000 batches: 0.0389
trigger times: 8
Loss after 2209196100 batches: 0.0391
trigger times: 9
Loss after 2209327200 batches: 0.0385
trigger times: 10
Loss after 2209458300 batches: 0.0383
trigger times: 11
Loss after 2209589400 batches: 0.0385
trigger times: 12
Loss after 2209720500 batches: 0.0386
trigger times: 13
Loss after 2209851600 batches: 0.0383
trigger times: 14
Loss after 2209982700 batches: 0.0384
trigger times: 15
Loss after 2210113800 batches: 0.0382
trigger times: 16
Loss after 2210244900 batches: 0.0380
trigger times: 17
Loss after 2210376000 batches: 0.0381
trigger times: 18
Loss after 2210507100 batches: 0.0377
trigger times: 19
Loss after 2210638200 batches: 0.0380
trigger times: 20
Early stopping!
Start to test process.
Loss after 2210769300 batches: 0.0380
Time to train on one home:  621.5632758140564
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642, 0.03501720033327798, 0.03263172240869951, 0.030110256078310443, 0.02544188748067248, 0.02612378438006769, 0.024274968770150986, 0.02405710057046961, 0.02388874572137445, 0.022740620412307226, 0.02141669577274335, 0.020377826915994508]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217], [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944], [0.6491197148958842, 0.29803996281550993, 0.44160060986867417, 1.203530351478871, 0.5750418631238572, 28.433835927602637, 1775.19], [0.6497863001293607, 0.29730357979276123, 0.44701117944787117, 1.2068712720184533, 0.5756451041104418, 28.512766372815683, 1777.0521], [0.6468338668346405, 0.30051325220168756, 0.4517286880074036, 1.1964129191375579, 0.5730157578453048, 28.26568403748201, 1768.9353], [0.6481483081976572, 0.29911867824708416, 0.44591549869242414, 1.1883481958771474, 0.5741581852808264, 28.07515205986611, 1772.462], [0.6422967116038004, 0.30545179637907305, 0.45038470512861073, 1.1723096038740957, 0.568970129184914, 27.696234575181023, 1756.4463], [0.635419938299391, 0.3128809605404973, 0.4559346003460791, 1.1641353165126298, 0.5628841981140037, 27.503114106407303, 1737.6584], [0.6225904193189409, 0.32673875385346274, 0.47098132348279487, 1.1438745784657927, 0.551531968836913, 27.024446908120186, 1702.6134], [0.6290365358193716, 0.31973560183743677, 0.4703045471241807, 1.1605770615000164, 0.5572689130640909, 27.419049056371055, 1720.3236], [0.6229561699761285, 0.3263064240524094, 0.4742132552861605, 1.1461675708751116, 0.5518861310699636, 27.078619675653382, 1703.7068], [0.6310632460647159, 0.31756252541073104, 0.469797775342069, 1.1473480016614066, 0.5590490855111376, 27.106507776074086, 1725.8193]]
Round_18_results:  [0.6310632460647159, 0.31756252541073104, 0.469797775342069, 1.1473480016614066, 0.5590490855111376, 27.106507776074086, 1725.8193]
trigger times: 0
Loss after 2210871900 batches: 0.3762
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 17628 < 17629; dropping {'Training_Loss': 0.37616624044520514, 'Validation_Loss': 0.7568029099040561, 'Training_R2': 0.6221778602264636, 'Validation_R2': 0.2943981646828602, 'Training_F1': 0.7654628888021281, 'Validation_F1': 0.4052353503812536, 'Training_NEP': 0.4705326042870963, 'Validation_NEP': 1.1339657758546162, 'Training_NDE': 0.25449731700297135, 'Validation_NDE': 0.5619041064741253, 'Training_MAE': 30.88191698888589, 'Validation_MAE': 31.09804610975327, 'Training_MSE': 3358.5295, 'Validation_MSE': 2075.0964}.
trigger times: 1
Loss after 2210974500 batches: 0.1350
trigger times: 2
Loss after 2211077100 batches: 0.0808
trigger times: 3
Loss after 2211179700 batches: 0.0665
trigger times: 4
Loss after 2211282300 batches: 0.0560
trigger times: 5
Loss after 2211384900 batches: 0.0484
trigger times: 6
Loss after 2211487500 batches: 0.0471
trigger times: 7
Loss after 2211590100 batches: 0.0438
trigger times: 0
Loss after 2211692700 batches: 0.0407
trigger times: 1
Loss after 2211795300 batches: 0.0377
trigger times: 2
Loss after 2211897900 batches: 0.0362
trigger times: 3
Loss after 2212000500 batches: 0.0339
trigger times: 4
Loss after 2212103100 batches: 0.0328
trigger times: 5
Loss after 2212205700 batches: 0.0313
trigger times: 6
Loss after 2212308300 batches: 0.0327
trigger times: 7
Loss after 2212410900 batches: 0.0321
trigger times: 8
Loss after 2212513500 batches: 0.0335
trigger times: 9
Loss after 2212616100 batches: 0.0311
trigger times: 10
Loss after 2212718700 batches: 0.0297
trigger times: 0
Loss after 2212821300 batches: 0.0322
trigger times: 0
Loss after 2212923900 batches: 0.0291
trigger times: 1
Loss after 2213026500 batches: 0.0279
trigger times: 2
Loss after 2213129100 batches: 0.0266
trigger times: 3
Loss after 2213231700 batches: 0.0262
trigger times: 4
Loss after 2213334300 batches: 0.0267
trigger times: 5
Loss after 2213436900 batches: 0.0255
trigger times: 6
Loss after 2213539500 batches: 0.0248
trigger times: 7
Loss after 2213642100 batches: 0.0247
trigger times: 8
Loss after 2213744700 batches: 0.0242
trigger times: 9
Loss after 2213847300 batches: 0.0252
trigger times: 10
Loss after 2213949900 batches: 0.0244
trigger times: 11
Loss after 2214052500 batches: 0.0266
trigger times: 12
Loss after 2214155100 batches: 0.0239
trigger times: 13
Loss after 2214257700 batches: 0.0238
trigger times: 0
Loss after 2214360300 batches: 0.0240
trigger times: 1
Loss after 2214462900 batches: 0.0259
trigger times: 2
Loss after 2214565500 batches: 0.0245
trigger times: 3
Loss after 2214668100 batches: 0.0229
trigger times: 0
Loss after 2214770700 batches: 0.0250
trigger times: 1
Loss after 2214873300 batches: 0.0290
trigger times: 2
Loss after 2214975900 batches: 0.0233
trigger times: 3
Loss after 2215078500 batches: 0.0224
trigger times: 4
Loss after 2215181100 batches: 0.0221
trigger times: 5
Loss after 2215283700 batches: 0.0223
trigger times: 6
Loss after 2215386300 batches: 0.0226
trigger times: 7
Loss after 2215488900 batches: 0.0230
trigger times: 8
Loss after 2215591500 batches: 0.0234
trigger times: 9
Loss after 2215694100 batches: 0.0234
trigger times: 10
Loss after 2215796700 batches: 0.0232
trigger times: 11
Loss after 2215899300 batches: 0.0231
trigger times: 12
Loss after 2216001900 batches: 0.0226
trigger times: 13
Loss after 2216104500 batches: 0.0220
trigger times: 14
Loss after 2216207100 batches: 0.0220
trigger times: 15
Loss after 2216309700 batches: 0.0228
trigger times: 16
Loss after 2216412300 batches: 0.0214
trigger times: 17
Loss after 2216514900 batches: 0.0230
trigger times: 18
Loss after 2216617500 batches: 0.0219
trigger times: 19
Loss after 2216720100 batches: 0.0222
trigger times: 20
Early stopping!
Start to test process.
Loss after 2216822700 batches: 0.0203
Time to train on one home:  356.66942977905273
trigger times: 0
Loss after 2216953800 batches: 0.2091
trigger times: 0
Loss after 2217084900 batches: 0.0711
trigger times: 1
Loss after 2217216000 batches: 0.0502
trigger times: 2
Loss after 2217347100 batches: 0.0428
trigger times: 3
Loss after 2217478200 batches: 0.0380
trigger times: 4
Loss after 2217609300 batches: 0.0353
trigger times: 5
Loss after 2217740400 batches: 0.0332
trigger times: 6
Loss after 2217871500 batches: 0.0314
trigger times: 7
Loss after 2218002600 batches: 0.0301
trigger times: 8
Loss after 2218133700 batches: 0.0288
trigger times: 9
Loss after 2218264800 batches: 0.0279
trigger times: 10
Loss after 2218395900 batches: 0.0267
trigger times: 11
Loss after 2218527000 batches: 0.0259
trigger times: 12
Loss after 2218658100 batches: 0.0259
trigger times: 13
Loss after 2218789200 batches: 0.0250
trigger times: 14
Loss after 2218920300 batches: 0.0244
trigger times: 15
Loss after 2219051400 batches: 0.0241
trigger times: 16
Loss after 2219182500 batches: 0.0238
trigger times: 17
Loss after 2219313600 batches: 0.0232
trigger times: 18
Loss after 2219444700 batches: 0.0234
trigger times: 19
Loss after 2219575800 batches: 0.0227
trigger times: 20
Early stopping!
Start to test process.
Loss after 2219706900 batches: 0.0224
Time to train on one home:  171.4850034713745
trigger times: 0
Loss after 2219838000 batches: 0.3688
trigger times: 0
Loss after 2219969100 batches: 0.1171
trigger times: 1
Loss after 2220100200 batches: 0.0801
trigger times: 2
Loss after 2220231300 batches: 0.0687
trigger times: 3
Loss after 2220362400 batches: 0.0618
trigger times: 4
Loss after 2220493500 batches: 0.0554
trigger times: 5
Loss after 2220624600 batches: 0.0527
trigger times: 0
Loss after 2220755700 batches: 0.0495
trigger times: 1
Loss after 2220886800 batches: 0.0473
trigger times: 2
Loss after 2221017900 batches: 0.0454
trigger times: 0
Loss after 2221149000 batches: 0.0441
trigger times: 1
Loss after 2221280100 batches: 0.0427
trigger times: 2
Loss after 2221411200 batches: 0.0417
trigger times: 3
Loss after 2221542300 batches: 0.0405
trigger times: 4
Loss after 2221673400 batches: 0.0391
trigger times: 5
Loss after 2221804500 batches: 0.0389
trigger times: 6
Loss after 2221935600 batches: 0.0377
trigger times: 7
Loss after 2222066700 batches: 0.0380
trigger times: 8
Loss after 2222197800 batches: 0.0367
trigger times: 9
Loss after 2222328900 batches: 0.0360
trigger times: 0
Loss after 2222460000 batches: 0.0356
trigger times: 1
Loss after 2222591100 batches: 0.0355
trigger times: 2
Loss after 2222722200 batches: 0.0346
trigger times: 3
Loss after 2222853300 batches: 0.0343
trigger times: 4
Loss after 2222984400 batches: 0.0340
trigger times: 5
Loss after 2223115500 batches: 0.0337
trigger times: 6
Loss after 2223246600 batches: 0.0334
trigger times: 7
Loss after 2223377700 batches: 0.0328
trigger times: 8
Loss after 2223508800 batches: 0.0321
trigger times: 9
Loss after 2223639900 batches: 0.0319
trigger times: 10
Loss after 2223771000 batches: 0.0319
trigger times: 11
Loss after 2223902100 batches: 0.0318
trigger times: 12
Loss after 2224033200 batches: 0.0315
trigger times: 13
Loss after 2224164300 batches: 0.0313
trigger times: 14
Loss after 2224295400 batches: 0.0305
trigger times: 15
Loss after 2224426500 batches: 0.0308
trigger times: 16
Loss after 2224557600 batches: 0.0302
trigger times: 17
Loss after 2224688700 batches: 0.0303
trigger times: 18
Loss after 2224819800 batches: 0.0299
trigger times: 19
Loss after 2224950900 batches: 0.0300
trigger times: 20
Early stopping!
Start to test process.
Loss after 2225082000 batches: 0.0293
Time to train on one home:  308.0131697654724
trigger times: 0
Loss after 2225210640 batches: 0.1399
trigger times: 0
Loss after 2225339280 batches: 0.0394
trigger times: 0
Loss after 2225467920 batches: 0.0290
trigger times: 0
Loss after 2225596560 batches: 0.0251
trigger times: 0
Loss after 2225725200 batches: 0.0229
trigger times: 1
Loss after 2225853840 batches: 0.0212
trigger times: 0
Loss after 2225982480 batches: 0.0204
trigger times: 1
Loss after 2226111120 batches: 0.0196
trigger times: 2
Loss after 2226239760 batches: 0.0190
trigger times: 3
Loss after 2226368400 batches: 0.0180
trigger times: 4
Loss after 2226497040 batches: 0.0178
trigger times: 5
Loss after 2226625680 batches: 0.0176
trigger times: 6
Loss after 2226754320 batches: 0.0173
trigger times: 0
Loss after 2226882960 batches: 0.0167
trigger times: 1
Loss after 2227011600 batches: 0.0163
trigger times: 2
Loss after 2227140240 batches: 0.0158
trigger times: 3
Loss after 2227268880 batches: 0.0158
trigger times: 4
Loss after 2227397520 batches: 0.0152
trigger times: 5
Loss after 2227526160 batches: 0.0147
trigger times: 6
Loss after 2227654800 batches: 0.0150
trigger times: 7
Loss after 2227783440 batches: 0.0152
trigger times: 8
Loss after 2227912080 batches: 0.0149
trigger times: 9
Loss after 2228040720 batches: 0.0150
trigger times: 10
Loss after 2228169360 batches: 0.0146
trigger times: 11
Loss after 2228298000 batches: 0.0142
trigger times: 12
Loss after 2228426640 batches: 0.0141
trigger times: 13
Loss after 2228555280 batches: 0.0143
trigger times: 14
Loss after 2228683920 batches: 0.0140
trigger times: 15
Loss after 2228812560 batches: 0.0138
trigger times: 16
Loss after 2228941200 batches: 0.0136
trigger times: 17
Loss after 2229069840 batches: 0.0137
trigger times: 18
Loss after 2229198480 batches: 0.0136
trigger times: 19
Loss after 2229327120 batches: 0.0135
trigger times: 20
Early stopping!
Start to test process.
Loss after 2229455760 batches: 0.0132
Time to train on one home:  255.4847822189331
trigger times: 0
Loss after 2229586860 batches: 0.4380
trigger times: 0
Loss after 2229717960 batches: 0.1297
trigger times: 0
Loss after 2229849060 batches: 0.0859
trigger times: 1
Loss after 2229980160 batches: 0.0726
trigger times: 2
Loss after 2230111260 batches: 0.0641
trigger times: 0
Loss after 2230242360 batches: 0.0585
trigger times: 0
Loss after 2230373460 batches: 0.0554
trigger times: 1
Loss after 2230504560 batches: 0.0517
trigger times: 0
Loss after 2230635660 batches: 0.0493
trigger times: 1
Loss after 2230766760 batches: 0.0482
trigger times: 2
Loss after 2230897860 batches: 0.0465
trigger times: 3
Loss after 2231028960 batches: 0.0445
trigger times: 4
Loss after 2231160060 batches: 0.0434
trigger times: 5
Loss after 2231291160 batches: 0.0428
trigger times: 6
Loss after 2231422260 batches: 0.0414
trigger times: 0
Loss after 2231553360 batches: 0.0410
trigger times: 1
Loss after 2231684460 batches: 0.0398
trigger times: 2
Loss after 2231815560 batches: 0.0393
trigger times: 3
Loss after 2231946660 batches: 0.0384
trigger times: 4
Loss after 2232077760 batches: 0.0379
trigger times: 5
Loss after 2232208860 batches: 0.0368
trigger times: 6
Loss after 2232339960 batches: 0.0366
trigger times: 7
Loss after 2232471060 batches: 0.0357
trigger times: 8
Loss after 2232602160 batches: 0.0354
trigger times: 9
Loss after 2232733260 batches: 0.0349
trigger times: 10
Loss after 2232864360 batches: 0.0349
trigger times: 11
Loss after 2232995460 batches: 0.0345
trigger times: 12
Loss after 2233126560 batches: 0.0337
trigger times: 13
Loss after 2233257660 batches: 0.0337
trigger times: 14
Loss after 2233388760 batches: 0.0332
trigger times: 15
Loss after 2233519860 batches: 0.0330
trigger times: 16
Loss after 2233650960 batches: 0.0329
trigger times: 17
Loss after 2233782060 batches: 0.0328
trigger times: 18
Loss after 2233913160 batches: 0.0320
trigger times: 19
Loss after 2234044260 batches: 0.0316
trigger times: 20
Early stopping!
Start to test process.
Loss after 2234175360 batches: 0.0317
Time to train on one home:  273.4065501689911
trigger times: 0
Loss after 2234306460 batches: 0.3952
trigger times: 0
Loss after 2234437560 batches: 0.1315
trigger times: 0
Loss after 2234568660 batches: 0.0820
trigger times: 0
Loss after 2234699760 batches: 0.0653
trigger times: 0
Loss after 2234830860 batches: 0.0575
trigger times: 0
Loss after 2234961960 batches: 0.0519
trigger times: 0
Loss after 2235093060 batches: 0.0489
trigger times: 0
Loss after 2235224160 batches: 0.0454
trigger times: 1
Loss after 2235355260 batches: 0.0438
trigger times: 0
Loss after 2235486360 batches: 0.0432
trigger times: 1
Loss after 2235617460 batches: 0.0417
trigger times: 2
Loss after 2235748560 batches: 0.0397
trigger times: 0
Loss after 2235879660 batches: 0.0383
trigger times: 0
Loss after 2236010760 batches: 0.0382
trigger times: 1
Loss after 2236141860 batches: 0.0376
trigger times: 0
Loss after 2236272960 batches: 0.0350
trigger times: 1
Loss after 2236404060 batches: 0.0357
trigger times: 2
Loss after 2236535160 batches: 0.0346
trigger times: 3
Loss after 2236666260 batches: 0.0328
trigger times: 4
Loss after 2236797360 batches: 0.0344
trigger times: 5
Loss after 2236928460 batches: 0.0343
trigger times: 0
Loss after 2237059560 batches: 0.0337
trigger times: 1
Loss after 2237190660 batches: 0.0326
trigger times: 2
Loss after 2237321760 batches: 0.0321
trigger times: 0
Loss after 2237452860 batches: 0.0312
trigger times: 1
Loss after 2237583960 batches: 0.0313
trigger times: 2
Loss after 2237715060 batches: 0.0320
trigger times: 0
Loss after 2237846160 batches: 0.0301
trigger times: 1
Loss after 2237977260 batches: 0.0306
trigger times: 2
Loss after 2238108360 batches: 0.0315
trigger times: 3
Loss after 2238239460 batches: 0.0308
trigger times: 4
Loss after 2238370560 batches: 0.0302
trigger times: 5
Loss after 2238501660 batches: 0.0299
trigger times: 6
Loss after 2238632760 batches: 0.0297
trigger times: 7
Loss after 2238763860 batches: 0.0292
trigger times: 8
Loss after 2238894960 batches: 0.0284
trigger times: 9
Loss after 2239026060 batches: 0.0293
trigger times: 10
Loss after 2239157160 batches: 0.0285
trigger times: 11
Loss after 2239288260 batches: 0.0280
trigger times: 12
Loss after 2239419360 batches: 0.0297
trigger times: 13
Loss after 2239550460 batches: 0.0283
trigger times: 14
Loss after 2239681560 batches: 0.0279
trigger times: 15
Loss after 2239812660 batches: 0.0290
trigger times: 16
Loss after 2239943760 batches: 0.0286
trigger times: 17
Loss after 2240074860 batches: 0.0273
trigger times: 18
Loss after 2240205960 batches: 0.0288
trigger times: 0
Loss after 2240337060 batches: 0.0279
trigger times: 1
Loss after 2240468160 batches: 0.0270
trigger times: 2
Loss after 2240599260 batches: 0.0271
trigger times: 3
Loss after 2240730360 batches: 0.0267
trigger times: 4
Loss after 2240861460 batches: 0.0273
trigger times: 5
Loss after 2240992560 batches: 0.0281
trigger times: 6
Loss after 2241123660 batches: 0.0256
trigger times: 7
Loss after 2241254760 batches: 0.0266
trigger times: 0
Loss after 2241385860 batches: 0.0252
trigger times: 1
Loss after 2241516960 batches: 0.0251
trigger times: 2
Loss after 2241648060 batches: 0.0260
trigger times: 0
Loss after 2241779160 batches: 0.0279
trigger times: 1
Loss after 2241910260 batches: 0.0269
trigger times: 2
Loss after 2242041360 batches: 0.0241
trigger times: 0
Loss after 2242172460 batches: 0.0245
trigger times: 1
Loss after 2242303560 batches: 0.0239
trigger times: 2
Loss after 2242434660 batches: 0.0243
trigger times: 3
Loss after 2242565760 batches: 0.0251
trigger times: 4
Loss after 2242696860 batches: 0.0259
trigger times: 5
Loss after 2242827960 batches: 0.0256
trigger times: 6
Loss after 2242959060 batches: 0.0255
trigger times: 7
Loss after 2243090160 batches: 0.0258
trigger times: 8
Loss after 2243221260 batches: 0.0243
trigger times: 9
Loss after 2243352360 batches: 0.0245
trigger times: 10
Loss after 2243483460 batches: 0.0254
trigger times: 11
Loss after 2243614560 batches: 0.0247
trigger times: 12
Loss after 2243745660 batches: 0.0237
trigger times: 13
Loss after 2243876760 batches: 0.0249
trigger times: 14
Loss after 2244007860 batches: 0.0245
trigger times: 15
Loss after 2244138960 batches: 0.0243
trigger times: 16
Loss after 2244270060 batches: 0.0237
trigger times: 17
Loss after 2244401160 batches: 0.0229
trigger times: 18
Loss after 2244532260 batches: 0.0234
trigger times: 19
Loss after 2244663360 batches: 0.0237
trigger times: 20
Early stopping!
Start to test process.
Loss after 2244794460 batches: 0.0232
Time to train on one home:  601.0118944644928
trigger times: 0
Loss after 2244925560 batches: 0.0971
trigger times: 1
Loss after 2245056660 batches: 0.0290
trigger times: 0
Loss after 2245187760 batches: 0.0209
trigger times: 1
Loss after 2245318860 batches: 0.0178
trigger times: 2
Loss after 2245449960 batches: 0.0165
trigger times: 3
Loss after 2245581060 batches: 0.0154
trigger times: 4
Loss after 2245712160 batches: 0.0150
trigger times: 5
Loss after 2245843260 batches: 0.0142
trigger times: 6
Loss after 2245974360 batches: 0.0135
trigger times: 7
Loss after 2246105460 batches: 0.0130
trigger times: 8
Loss after 2246236560 batches: 0.0127
trigger times: 9
Loss after 2246367660 batches: 0.0126
trigger times: 10
Loss after 2246498760 batches: 0.0120
trigger times: 11
Loss after 2246629860 batches: 0.0118
trigger times: 12
Loss after 2246760960 batches: 0.0118
trigger times: 13
Loss after 2246892060 batches: 0.0114
trigger times: 14
Loss after 2247023160 batches: 0.0112
trigger times: 15
Loss after 2247154260 batches: 0.0109
trigger times: 16
Loss after 2247285360 batches: 0.0108
trigger times: 17
Loss after 2247416460 batches: 0.0109
trigger times: 18
Loss after 2247547560 batches: 0.0108
trigger times: 19
Loss after 2247678660 batches: 0.0106
trigger times: 20
Early stopping!
Start to test process.
Loss after 2247809760 batches: 0.0102
Time to train on one home:  179.41841793060303
trigger times: 0
Loss after 2247940860 batches: 0.1822
trigger times: 0
Loss after 2248071960 batches: 0.0471
trigger times: 0
Loss after 2248203060 batches: 0.0313
trigger times: 0
Loss after 2248334160 batches: 0.0258
trigger times: 0
Loss after 2248465260 batches: 0.0228
trigger times: 1
Loss after 2248596360 batches: 0.0214
trigger times: 2
Loss after 2248727460 batches: 0.0199
trigger times: 3
Loss after 2248858560 batches: 0.0198
trigger times: 0
Loss after 2248989660 batches: 0.0184
trigger times: 1
Loss after 2249120760 batches: 0.0178
trigger times: 2
Loss after 2249251860 batches: 0.0170
trigger times: 3
Loss after 2249382960 batches: 0.0166
trigger times: 4
Loss after 2249514060 batches: 0.0163
trigger times: 5
Loss after 2249645160 batches: 0.0158
trigger times: 6
Loss after 2249776260 batches: 0.0155
trigger times: 7
Loss after 2249907360 batches: 0.0155
trigger times: 8
Loss after 2250038460 batches: 0.0149
trigger times: 9
Loss after 2250169560 batches: 0.0147
trigger times: 10
Loss after 2250300660 batches: 0.0146
trigger times: 11
Loss after 2250431760 batches: 0.0145
trigger times: 12
Loss after 2250562860 batches: 0.0143
trigger times: 13
Loss after 2250693960 batches: 0.0138
trigger times: 14
Loss after 2250825060 batches: 0.0137
trigger times: 15
Loss after 2250956160 batches: 0.0136
trigger times: 16
Loss after 2251087260 batches: 0.0133
trigger times: 17
Loss after 2251218360 batches: 0.0131
trigger times: 18
Loss after 2251349460 batches: 0.0130
trigger times: 19
Loss after 2251480560 batches: 0.0130
trigger times: 20
Early stopping!
Start to test process.
Loss after 2251611660 batches: 0.0127
Time to train on one home:  222.0255446434021
trigger times: 0
Loss after 2251690260 batches: 0.3585
trigger times: 0
Loss after 2251768860 batches: 0.0995
trigger times: 0
Loss after 2251847460 batches: 0.0598
trigger times: 1
Loss after 2251926060 batches: 0.0470
trigger times: 2
Loss after 2252004660 batches: 0.0393
trigger times: 3
Loss after 2252083260 batches: 0.0359
trigger times: 4
Loss after 2252161860 batches: 0.0347
trigger times: 5
Loss after 2252240460 batches: 0.0326
trigger times: 6
Loss after 2252319060 batches: 0.0302
trigger times: 7
Loss after 2252397660 batches: 0.0292
trigger times: 8
Loss after 2252476260 batches: 0.0286
trigger times: 9
Loss after 2252554860 batches: 0.0282
trigger times: 10
Loss after 2252633460 batches: 0.0268
trigger times: 0
Loss after 2252712060 batches: 0.0264
trigger times: 1
Loss after 2252790660 batches: 0.0258
trigger times: 0
Loss after 2252869260 batches: 0.0251
trigger times: 1
Loss after 2252947860 batches: 0.0257
trigger times: 2
Loss after 2253026460 batches: 0.0251
trigger times: 3
Loss after 2253105060 batches: 0.0242
trigger times: 4
Loss after 2253183660 batches: 0.0237
trigger times: 5
Loss after 2253262260 batches: 0.0233
trigger times: 6
Loss after 2253340860 batches: 0.0222
trigger times: 7
Loss after 2253419460 batches: 0.0225
trigger times: 8
Loss after 2253498060 batches: 0.0220
trigger times: 9
Loss after 2253576660 batches: 0.0221
trigger times: 10
Loss after 2253655260 batches: 0.0215
trigger times: 11
Loss after 2253733860 batches: 0.0216
trigger times: 12
Loss after 2253812460 batches: 0.0204
trigger times: 13
Loss after 2253891060 batches: 0.0207
trigger times: 14
Loss after 2253969660 batches: 0.0206
trigger times: 15
Loss after 2254048260 batches: 0.0209
trigger times: 0
Loss after 2254126860 batches: 0.0205
trigger times: 1
Loss after 2254205460 batches: 0.0205
trigger times: 2
Loss after 2254284060 batches: 0.0204
trigger times: 3
Loss after 2254362660 batches: 0.0200
trigger times: 0
Loss after 2254441260 batches: 0.0195
trigger times: 1
Loss after 2254519860 batches: 0.0198
trigger times: 2
Loss after 2254598460 batches: 0.0191
trigger times: 3
Loss after 2254677060 batches: 0.0196
trigger times: 4
Loss after 2254755660 batches: 0.0187
trigger times: 5
Loss after 2254834260 batches: 0.0193
trigger times: 6
Loss after 2254912860 batches: 0.0188
trigger times: 7
Loss after 2254991460 batches: 0.0188
trigger times: 8
Loss after 2255070060 batches: 0.0190
trigger times: 9
Loss after 2255148660 batches: 0.0180
trigger times: 10
Loss after 2255227260 batches: 0.0186
trigger times: 11
Loss after 2255305860 batches: 0.0189
trigger times: 12
Loss after 2255384460 batches: 0.0185
trigger times: 13
Loss after 2255463060 batches: 0.0188
trigger times: 14
Loss after 2255541660 batches: 0.0179
trigger times: 15
Loss after 2255620260 batches: 0.0181
trigger times: 16
Loss after 2255698860 batches: 0.0179
trigger times: 17
Loss after 2255777460 batches: 0.0178
trigger times: 18
Loss after 2255856060 batches: 0.0177
trigger times: 19
Loss after 2255934660 batches: 0.0174
trigger times: 20
Early stopping!
Start to test process.
Loss after 2256013260 batches: 0.0174
Time to train on one home:  274.4709885120392
trigger times: 0
Loss after 2256144360 batches: 0.1166
trigger times: 0
Loss after 2256275460 batches: 0.0319
trigger times: 1
Loss after 2256406560 batches: 0.0240
trigger times: 2
Loss after 2256537660 batches: 0.0206
trigger times: 3
Loss after 2256668760 batches: 0.0189
trigger times: 4
Loss after 2256799860 batches: 0.0176
trigger times: 0
Loss after 2256930960 batches: 0.0171
trigger times: 0
Loss after 2257062060 batches: 0.0161
trigger times: 0
Loss after 2257193160 batches: 0.0155
trigger times: 1
Loss after 2257324260 batches: 0.0149
trigger times: 2
Loss after 2257455360 batches: 0.0146
trigger times: 3
Loss after 2257586460 batches: 0.0139
trigger times: 4
Loss after 2257717560 batches: 0.0139
trigger times: 5
Loss after 2257848660 batches: 0.0136
trigger times: 6
Loss after 2257979760 batches: 0.0133
trigger times: 7
Loss after 2258110860 batches: 0.0129
trigger times: 8
Loss after 2258241960 batches: 0.0130
trigger times: 9
Loss after 2258373060 batches: 0.0126
trigger times: 10
Loss after 2258504160 batches: 0.0123
trigger times: 11
Loss after 2258635260 batches: 0.0124
trigger times: 12
Loss after 2258766360 batches: 0.0121
trigger times: 13
Loss after 2258897460 batches: 0.0117
trigger times: 14
Loss after 2259028560 batches: 0.0118
trigger times: 15
Loss after 2259159660 batches: 0.0114
trigger times: 16
Loss after 2259290760 batches: 0.0115
trigger times: 17
Loss after 2259421860 batches: 0.0112
trigger times: 18
Loss after 2259552960 batches: 0.0112
trigger times: 19
Loss after 2259684060 batches: 0.0110
trigger times: 20
Early stopping!
Start to test process.
Loss after 2259815160 batches: 0.0109
Time to train on one home:  221.9355022907257
trigger times: 0
Loss after 2259946260 batches: 0.1412
trigger times: 0
Loss after 2260077360 batches: 0.0411
trigger times: 0
Loss after 2260208460 batches: 0.0305
trigger times: 1
Loss after 2260339560 batches: 0.0268
trigger times: 2
Loss after 2260470660 batches: 0.0243
trigger times: 0
Loss after 2260601760 batches: 0.0227
trigger times: 1
Loss after 2260732860 batches: 0.0216
trigger times: 2
Loss after 2260863960 batches: 0.0205
trigger times: 0
Loss after 2260995060 batches: 0.0198
trigger times: 0
Loss after 2261126160 batches: 0.0191
trigger times: 1
Loss after 2261257260 batches: 0.0183
trigger times: 2
Loss after 2261388360 batches: 0.0180
trigger times: 0
Loss after 2261519460 batches: 0.0178
trigger times: 1
Loss after 2261650560 batches: 0.0172
trigger times: 0
Loss after 2261781660 batches: 0.0166
trigger times: 0
Loss after 2261912760 batches: 0.0166
trigger times: 1
Loss after 2262043860 batches: 0.0163
trigger times: 0
Loss after 2262174960 batches: 0.0163
trigger times: 0
Loss after 2262306060 batches: 0.0158
trigger times: 0
Loss after 2262437160 batches: 0.0160
trigger times: 1
Loss after 2262568260 batches: 0.0154
trigger times: 2
Loss after 2262699360 batches: 0.0150
trigger times: 3
Loss after 2262830460 batches: 0.0149
trigger times: 4
Loss after 2262961560 batches: 0.0146
trigger times: 5
Loss after 2263092660 batches: 0.0148
trigger times: 6
Loss after 2263223760 batches: 0.0146
trigger times: 7
Loss after 2263354860 batches: 0.0143
trigger times: 8
Loss after 2263485960 batches: 0.0142
trigger times: 9
Loss after 2263617060 batches: 0.0141
trigger times: 0
Loss after 2263748160 batches: 0.0140
trigger times: 0
Loss after 2263879260 batches: 0.0139
trigger times: 1
Loss after 2264010360 batches: 0.0138
trigger times: 0
Loss after 2264141460 batches: 0.0136
trigger times: 1
Loss after 2264272560 batches: 0.0133
trigger times: 0
Loss after 2264403660 batches: 0.0136
trigger times: 1
Loss after 2264534760 batches: 0.0133
trigger times: 2
Loss after 2264665860 batches: 0.0133
trigger times: 3
Loss after 2264796960 batches: 0.0131
trigger times: 4
Loss after 2264928060 batches: 0.0131
trigger times: 5
Loss after 2265059160 batches: 0.0129
trigger times: 6
Loss after 2265190260 batches: 0.0130
trigger times: 7
Loss after 2265321360 batches: 0.0127
trigger times: 8
Loss after 2265452460 batches: 0.0126
trigger times: 9
Loss after 2265583560 batches: 0.0127
trigger times: 10
Loss after 2265714660 batches: 0.0127
trigger times: 11
Loss after 2265845760 batches: 0.0124
trigger times: 12
Loss after 2265976860 batches: 0.0125
trigger times: 13
Loss after 2266107960 batches: 0.0124
trigger times: 14
Loss after 2266239060 batches: 0.0121
trigger times: 15
Loss after 2266370160 batches: 0.0122
trigger times: 16
Loss after 2266501260 batches: 0.0122
trigger times: 17
Loss after 2266632360 batches: 0.0121
trigger times: 18
Loss after 2266763460 batches: 0.0120
trigger times: 19
Loss after 2266894560 batches: 0.0120
trigger times: 20
Early stopping!
Start to test process.
Loss after 2267025660 batches: 0.0120
Time to train on one home:  412.54800486564636
trigger times: 0
Loss after 2267156760 batches: 0.2326
trigger times: 0
Loss after 2267287860 batches: 0.0644
trigger times: 0
Loss after 2267418960 batches: 0.0457
trigger times: 1
Loss after 2267550060 batches: 0.0397
trigger times: 0
Loss after 2267681160 batches: 0.0355
trigger times: 0
Loss after 2267812260 batches: 0.0326
trigger times: 1
Loss after 2267943360 batches: 0.0309
trigger times: 2
Loss after 2268074460 batches: 0.0298
trigger times: 0
Loss after 2268205560 batches: 0.0293
trigger times: 1
Loss after 2268336660 batches: 0.0272
trigger times: 2
Loss after 2268467760 batches: 0.0270
trigger times: 0
Loss after 2268598860 batches: 0.0257
trigger times: 1
Loss after 2268729960 batches: 0.0255
trigger times: 2
Loss after 2268861060 batches: 0.0246
trigger times: 0
Loss after 2268992160 batches: 0.0243
trigger times: 1
Loss after 2269123260 batches: 0.0236
trigger times: 2
Loss after 2269254360 batches: 0.0232
trigger times: 0
Loss after 2269385460 batches: 0.0227
trigger times: 1
Loss after 2269516560 batches: 0.0223
trigger times: 2
Loss after 2269647660 batches: 0.0218
trigger times: 3
Loss after 2269778760 batches: 0.0215
trigger times: 4
Loss after 2269909860 batches: 0.0215
trigger times: 5
Loss after 2270040960 batches: 0.0216
trigger times: 6
Loss after 2270172060 batches: 0.0215
trigger times: 7
Loss after 2270303160 batches: 0.0212
trigger times: 8
Loss after 2270434260 batches: 0.0215
trigger times: 0
Loss after 2270565360 batches: 0.0207
trigger times: 0
Loss after 2270696460 batches: 0.0198
trigger times: 1
Loss after 2270827560 batches: 0.0206
trigger times: 2
Loss after 2270958660 batches: 0.0199
trigger times: 3
Loss after 2271089760 batches: 0.0193
trigger times: 0
Loss after 2271220860 batches: 0.0197
trigger times: 1
Loss after 2271351960 batches: 0.0196
trigger times: 2
Loss after 2271483060 batches: 0.0198
trigger times: 3
Loss after 2271614160 batches: 0.0189
trigger times: 4
Loss after 2271745260 batches: 0.0194
trigger times: 0
Loss after 2271876360 batches: 0.0192
trigger times: 1
Loss after 2272007460 batches: 0.0196
trigger times: 2
Loss after 2272138560 batches: 0.0183
trigger times: 3
Loss after 2272269660 batches: 0.0193
trigger times: 4
Loss after 2272400760 batches: 0.0186
trigger times: 5
Loss after 2272531860 batches: 0.0186
trigger times: 0
Loss after 2272662960 batches: 0.0184
trigger times: 1
Loss after 2272794060 batches: 0.0177
trigger times: 2
Loss after 2272925160 batches: 0.0181
trigger times: 3
Loss after 2273056260 batches: 0.0181
trigger times: 4
Loss after 2273187360 batches: 0.0173
trigger times: 5
Loss after 2273318460 batches: 0.0179
trigger times: 6
Loss after 2273449560 batches: 0.0178
trigger times: 7
Loss after 2273580660 batches: 0.0172
trigger times: 8
Loss after 2273711760 batches: 0.0170
trigger times: 9
Loss after 2273842860 batches: 0.0174
trigger times: 10
Loss after 2273973960 batches: 0.0181
trigger times: 11
Loss after 2274105060 batches: 0.0183
trigger times: 12
Loss after 2274236160 batches: 0.0171
trigger times: 13
Loss after 2274367260 batches: 0.0170
trigger times: 14
Loss after 2274498360 batches: 0.0173
trigger times: 15
Loss after 2274629460 batches: 0.0170
trigger times: 16
Loss after 2274760560 batches: 0.0169
trigger times: 17
Loss after 2274891660 batches: 0.0167
trigger times: 18
Loss after 2275022760 batches: 0.0166
trigger times: 19
Loss after 2275153860 batches: 0.0173
trigger times: 20
Early stopping!
Start to test process.
Loss after 2275284960 batches: 0.0165
Time to train on one home:  469.68869280815125
trigger times: 0
Loss after 2275416060 batches: 0.3366
trigger times: 1
Loss after 2275547160 batches: 0.1122
trigger times: 2
Loss after 2275678260 batches: 0.0781
trigger times: 3
Loss after 2275809360 batches: 0.0658
trigger times: 0
Loss after 2275940460 batches: 0.0584
trigger times: 1
Loss after 2276071560 batches: 0.0539
trigger times: 2
Loss after 2276202660 batches: 0.0508
trigger times: 3
Loss after 2276333760 batches: 0.0482
trigger times: 4
Loss after 2276464860 batches: 0.0465
trigger times: 5
Loss after 2276595960 batches: 0.0442
trigger times: 6
Loss after 2276727060 batches: 0.0439
trigger times: 7
Loss after 2276858160 batches: 0.0418
trigger times: 8
Loss after 2276989260 batches: 0.0407
trigger times: 9
Loss after 2277120360 batches: 0.0397
trigger times: 10
Loss after 2277251460 batches: 0.0389
trigger times: 11
Loss after 2277382560 batches: 0.0383
trigger times: 12
Loss after 2277513660 batches: 0.0374
trigger times: 13
Loss after 2277644760 batches: 0.0368
trigger times: 14
Loss after 2277775860 batches: 0.0358
trigger times: 15
Loss after 2277906960 batches: 0.0356
trigger times: 16
Loss after 2278038060 batches: 0.0353
trigger times: 17
Loss after 2278169160 batches: 0.0349
trigger times: 18
Loss after 2278300260 batches: 0.0341
trigger times: 19
Loss after 2278431360 batches: 0.0340
trigger times: 20
Early stopping!
Start to test process.
Loss after 2278562460 batches: 0.0335
Time to train on one home:  193.11424827575684
trigger times: 0
Loss after 2278693560 batches: 0.2434
trigger times: 1
Loss after 2278824660 batches: 0.0713
trigger times: 0
Loss after 2278955760 batches: 0.0501
trigger times: 0
Loss after 2279086860 batches: 0.0430
trigger times: 0
Loss after 2279217960 batches: 0.0380
trigger times: 0
Loss after 2279349060 batches: 0.0356
trigger times: 1
Loss after 2279480160 batches: 0.0336
trigger times: 0
Loss after 2279611260 batches: 0.0318
trigger times: 1
Loss after 2279742360 batches: 0.0310
trigger times: 2
Loss after 2279873460 batches: 0.0296
trigger times: 3
Loss after 2280004560 batches: 0.0287
trigger times: 4
Loss after 2280135660 batches: 0.0279
trigger times: 5
Loss after 2280266760 batches: 0.0283
trigger times: 6
Loss after 2280397860 batches: 0.0265
trigger times: 7
Loss after 2280528960 batches: 0.0262
trigger times: 0
Loss after 2280660060 batches: 0.0254
trigger times: 1
Loss after 2280791160 batches: 0.0249
trigger times: 2
Loss after 2280922260 batches: 0.0251
trigger times: 3
Loss after 2281053360 batches: 0.0247
trigger times: 4
Loss after 2281184460 batches: 0.0236
trigger times: 5
Loss after 2281315560 batches: 0.0238
trigger times: 6
Loss after 2281446660 batches: 0.0235
trigger times: 7
Loss after 2281577760 batches: 0.0236
trigger times: 8
Loss after 2281708860 batches: 0.0229
trigger times: 9
Loss after 2281839960 batches: 0.0230
trigger times: 10
Loss after 2281971060 batches: 0.0224
trigger times: 11
Loss after 2282102160 batches: 0.0217
trigger times: 12
Loss after 2282233260 batches: 0.0221
trigger times: 13
Loss after 2282364360 batches: 0.0217
trigger times: 14
Loss after 2282495460 batches: 0.0222
trigger times: 15
Loss after 2282626560 batches: 0.0215
trigger times: 16
Loss after 2282757660 batches: 0.0211
trigger times: 17
Loss after 2282888760 batches: 0.0209
trigger times: 18
Loss after 2283019860 batches: 0.0211
trigger times: 19
Loss after 2283150960 batches: 0.0204
trigger times: 20
Early stopping!
Start to test process.
Loss after 2283282060 batches: 0.0209
Time to train on one home:  273.7331132888794
trigger times: 0
Loss after 2283413160 batches: 0.4054
trigger times: 1
Loss after 2283544260 batches: 0.1134
trigger times: 2
Loss after 2283675360 batches: 0.0717
trigger times: 3
Loss after 2283806460 batches: 0.0591
trigger times: 4
Loss after 2283937560 batches: 0.0521
trigger times: 5
Loss after 2284068660 batches: 0.0477
trigger times: 6
Loss after 2284199760 batches: 0.0447
trigger times: 7
Loss after 2284330860 batches: 0.0425
trigger times: 8
Loss after 2284461960 batches: 0.0405
trigger times: 9
Loss after 2284593060 batches: 0.0382
trigger times: 10
Loss after 2284724160 batches: 0.0368
trigger times: 11
Loss after 2284855260 batches: 0.0360
trigger times: 12
Loss after 2284986360 batches: 0.0343
trigger times: 13
Loss after 2285117460 batches: 0.0335
trigger times: 14
Loss after 2285248560 batches: 0.0326
trigger times: 15
Loss after 2285379660 batches: 0.0323
trigger times: 16
Loss after 2285510760 batches: 0.0314
trigger times: 17
Loss after 2285641860 batches: 0.0306
trigger times: 18
Loss after 2285772960 batches: 0.0302
trigger times: 19
Loss after 2285904060 batches: 0.0298
trigger times: 20
Early stopping!
Start to test process.
Loss after 2286035160 batches: 0.0290
Time to train on one home:  163.51564598083496
trigger times: 0
Loss after 2286166260 batches: 0.3893
trigger times: 0
Loss after 2286297360 batches: 0.1145
trigger times: 0
Loss after 2286428460 batches: 0.0827
trigger times: 1
Loss after 2286559560 batches: 0.0704
trigger times: 2
Loss after 2286690660 batches: 0.0635
trigger times: 0
Loss after 2286821760 batches: 0.0585
trigger times: 0
Loss after 2286952860 batches: 0.0550
trigger times: 1
Loss after 2287083960 batches: 0.0523
trigger times: 2
Loss after 2287215060 batches: 0.0501
trigger times: 0
Loss after 2287346160 batches: 0.0484
trigger times: 1
Loss after 2287477260 batches: 0.0469
trigger times: 2
Loss after 2287608360 batches: 0.0454
trigger times: 0
Loss after 2287739460 batches: 0.0444
trigger times: 1
Loss after 2287870560 batches: 0.0428
trigger times: 0
Loss after 2288001660 batches: 0.0421
trigger times: 1
Loss after 2288132760 batches: 0.0408
trigger times: 0
Loss after 2288263860 batches: 0.0401
trigger times: 1
Loss after 2288394960 batches: 0.0395
trigger times: 2
Loss after 2288526060 batches: 0.0391
trigger times: 3
Loss after 2288657160 batches: 0.0392
trigger times: 4
Loss after 2288788260 batches: 0.0387
trigger times: 5
Loss after 2288919360 batches: 0.0382
trigger times: 6
Loss after 2289050460 batches: 0.0367
trigger times: 0
Loss after 2289181560 batches: 0.0363
trigger times: 0
Loss after 2289312660 batches: 0.0362
trigger times: 1
Loss after 2289443760 batches: 0.0352
trigger times: 2
Loss after 2289574860 batches: 0.0350
trigger times: 0
Loss after 2289705960 batches: 0.0353
trigger times: 1
Loss after 2289837060 batches: 0.0346
trigger times: 0
Loss after 2289968160 batches: 0.0347
trigger times: 1
Loss after 2290099260 batches: 0.0341
trigger times: 2
Loss after 2290230360 batches: 0.0334
trigger times: 3
Loss after 2290361460 batches: 0.0332
trigger times: 4
Loss after 2290492560 batches: 0.0328
trigger times: 5
Loss after 2290623660 batches: 0.0328
trigger times: 6
Loss after 2290754760 batches: 0.0329
trigger times: 7
Loss after 2290885860 batches: 0.0331
trigger times: 8
Loss after 2291016960 batches: 0.0324
trigger times: 9
Loss after 2291148060 batches: 0.0320
trigger times: 10
Loss after 2291279160 batches: 0.0319
trigger times: 11
Loss after 2291410260 batches: 0.0314
trigger times: 12
Loss after 2291541360 batches: 0.0315
trigger times: 13
Loss after 2291672460 batches: 0.0311
trigger times: 14
Loss after 2291803560 batches: 0.0308
trigger times: 15
Loss after 2291934660 batches: 0.0309
trigger times: 16
Loss after 2292065760 batches: 0.0307
trigger times: 17
Loss after 2292196860 batches: 0.0302
trigger times: 0
Loss after 2292327960 batches: 0.0301
trigger times: 1
Loss after 2292459060 batches: 0.0297
trigger times: 0
Loss after 2292590160 batches: 0.0306
trigger times: 1
Loss after 2292721260 batches: 0.0300
trigger times: 2
Loss after 2292852360 batches: 0.0293
trigger times: 3
Loss after 2292983460 batches: 0.0294
trigger times: 4
Loss after 2293114560 batches: 0.0287
trigger times: 5
Loss after 2293245660 batches: 0.0292
trigger times: 6
Loss after 2293376760 batches: 0.0292
trigger times: 7
Loss after 2293507860 batches: 0.0291
trigger times: 8
Loss after 2293638960 batches: 0.0287
trigger times: 9
Loss after 2293770060 batches: 0.0287
trigger times: 10
Loss after 2293901160 batches: 0.0286
trigger times: 11
Loss after 2294032260 batches: 0.0284
trigger times: 12
Loss after 2294163360 batches: 0.0285
trigger times: 13
Loss after 2294294460 batches: 0.0282
trigger times: 14
Loss after 2294425560 batches: 0.0276
trigger times: 15
Loss after 2294556660 batches: 0.0281
trigger times: 16
Loss after 2294687760 batches: 0.0276
trigger times: 17
Loss after 2294818860 batches: 0.0277
trigger times: 18
Loss after 2294949960 batches: 0.0278
trigger times: 19
Loss after 2295081060 batches: 0.0276
trigger times: 20
Early stopping!
Start to test process.
Loss after 2295212160 batches: 0.0273
Time to train on one home:  520.3146677017212
trigger times: 0
Loss after 2295306120 batches: 0.4412
trigger times: 0
Loss after 2295400080 batches: 0.1294
trigger times: 0
Loss after 2295494040 batches: 0.0823
trigger times: 0
Loss after 2295588000 batches: 0.0656
trigger times: 1
Loss after 2295681960 batches: 0.0584
trigger times: 2
Loss after 2295775920 batches: 0.0534
trigger times: 0
Loss after 2295869880 batches: 0.0485
trigger times: 0
Loss after 2295963840 batches: 0.0467
trigger times: 1
Loss after 2296057800 batches: 0.0441
trigger times: 2
Loss after 2296151760 batches: 0.0432
trigger times: 3
Loss after 2296245720 batches: 0.0409
trigger times: 0
Loss after 2296339680 batches: 0.0398
trigger times: 1
Loss after 2296433640 batches: 0.0391
trigger times: 2
Loss after 2296527600 batches: 0.0380
trigger times: 0
Loss after 2296621560 batches: 0.0368
trigger times: 1
Loss after 2296715520 batches: 0.0354
trigger times: 2
Loss after 2296809480 batches: 0.0351
trigger times: 3
Loss after 2296903440 batches: 0.0348
trigger times: 4
Loss after 2296997400 batches: 0.0339
trigger times: 5
Loss after 2297091360 batches: 0.0338
trigger times: 6
Loss after 2297185320 batches: 0.0334
trigger times: 7
Loss after 2297279280 batches: 0.0325
trigger times: 8
Loss after 2297373240 batches: 0.0329
trigger times: 9
Loss after 2297467200 batches: 0.0316
trigger times: 10
Loss after 2297561160 batches: 0.0316
trigger times: 0
Loss after 2297655120 batches: 0.0309
trigger times: 1
Loss after 2297749080 batches: 0.0308
trigger times: 0
Loss after 2297843040 batches: 0.0306
trigger times: 1
Loss after 2297937000 batches: 0.0299
trigger times: 2
Loss after 2298030960 batches: 0.0300
trigger times: 0
Loss after 2298124920 batches: 0.0295
trigger times: 0
Loss after 2298218880 batches: 0.0292
trigger times: 0
Loss after 2298312840 batches: 0.0287
trigger times: 1
Loss after 2298406800 batches: 0.0285
trigger times: 2
Loss after 2298500760 batches: 0.0287
trigger times: 3
Loss after 2298594720 batches: 0.0283
trigger times: 4
Loss after 2298688680 batches: 0.0281
trigger times: 5
Loss after 2298782640 batches: 0.0277
trigger times: 6
Loss after 2298876600 batches: 0.0275
trigger times: 7
Loss after 2298970560 batches: 0.0273
trigger times: 8
Loss after 2299064520 batches: 0.0275
trigger times: 9
Loss after 2299158480 batches: 0.0274
trigger times: 0
Loss after 2299252440 batches: 0.0269
trigger times: 1
Loss after 2299346400 batches: 0.0270
trigger times: 0
Loss after 2299440360 batches: 0.0261
trigger times: 1
Loss after 2299534320 batches: 0.0262
trigger times: 2
Loss after 2299628280 batches: 0.0259
trigger times: 3
Loss after 2299722240 batches: 0.0262
trigger times: 0
Loss after 2299816200 batches: 0.0262
trigger times: 1
Loss after 2299910160 batches: 0.0264
trigger times: 2
Loss after 2300004120 batches: 0.0258
trigger times: 3
Loss after 2300098080 batches: 0.0254
trigger times: 4
Loss after 2300192040 batches: 0.0254
trigger times: 5
Loss after 2300286000 batches: 0.0251
trigger times: 6
Loss after 2300379960 batches: 0.0248
trigger times: 7
Loss after 2300473920 batches: 0.0253
trigger times: 8
Loss after 2300567880 batches: 0.0251
trigger times: 9
Loss after 2300661840 batches: 0.0247
trigger times: 10
Loss after 2300755800 batches: 0.0248
trigger times: 11
Loss after 2300849760 batches: 0.0247
trigger times: 12
Loss after 2300943720 batches: 0.0241
trigger times: 13
Loss after 2301037680 batches: 0.0242
trigger times: 14
Loss after 2301131640 batches: 0.0241
trigger times: 15
Loss after 2301225600 batches: 0.0240
trigger times: 16
Loss after 2301319560 batches: 0.0242
trigger times: 17
Loss after 2301413520 batches: 0.0238
trigger times: 18
Loss after 2301507480 batches: 0.0238
trigger times: 19
Loss after 2301601440 batches: 0.0237
trigger times: 20
Early stopping!
Start to test process.
Loss after 2301695400 batches: 0.0238
Time to train on one home:  387.98328399658203
trigger times: 0
Loss after 2301826500 batches: 0.0629
trigger times: 0
Loss after 2301957600 batches: 0.0119
trigger times: 0
Loss after 2302088700 batches: 0.0086
trigger times: 0
Loss after 2302219800 batches: 0.0074
trigger times: 1
Loss after 2302350900 batches: 0.0066
trigger times: 2
Loss after 2302482000 batches: 0.0059
trigger times: 3
Loss after 2302613100 batches: 0.0055
trigger times: 4
Loss after 2302744200 batches: 0.0051
trigger times: 5
Loss after 2302875300 batches: 0.0050
trigger times: 6
Loss after 2303006400 batches: 0.0048
trigger times: 7
Loss after 2303137500 batches: 0.0045
trigger times: 8
Loss after 2303268600 batches: 0.0043
trigger times: 9
Loss after 2303399700 batches: 0.0042
trigger times: 10
Loss after 2303530800 batches: 0.0041
trigger times: 11
Loss after 2303661900 batches: 0.0040
trigger times: 12
Loss after 2303793000 batches: 0.0040
trigger times: 13
Loss after 2303924100 batches: 0.0037
trigger times: 14
Loss after 2304055200 batches: 0.0039
trigger times: 15
Loss after 2304186300 batches: 0.0036
trigger times: 16
Loss after 2304317400 batches: 0.0035
trigger times: 17
Loss after 2304448500 batches: 0.0036
trigger times: 18
Loss after 2304579600 batches: 0.0036
trigger times: 19
Loss after 2304710700 batches: 0.0034
trigger times: 20
Early stopping!
Start to test process.
Loss after 2304841800 batches: 0.0033
Time to train on one home:  186.72602152824402
trigger times: 0
Loss after 2304972900 batches: 0.1312
trigger times: 0
Loss after 2305104000 batches: 0.0367
trigger times: 0
Loss after 2305235100 batches: 0.0265
trigger times: 1
Loss after 2305366200 batches: 0.0226
trigger times: 0
Loss after 2305497300 batches: 0.0202
trigger times: 0
Loss after 2305628400 batches: 0.0186
trigger times: 1
Loss after 2305759500 batches: 0.0191
trigger times: 0
Loss after 2305890600 batches: 0.0171
trigger times: 1
Loss after 2306021700 batches: 0.0164
trigger times: 0
Loss after 2306152800 batches: 0.0154
trigger times: 1
Loss after 2306283900 batches: 0.0153
trigger times: 2
Loss after 2306415000 batches: 0.0146
trigger times: 3
Loss after 2306546100 batches: 0.0141
trigger times: 0
Loss after 2306677200 batches: 0.0141
trigger times: 1
Loss after 2306808300 batches: 0.0136
trigger times: 2
Loss after 2306939400 batches: 0.0131
trigger times: 3
Loss after 2307070500 batches: 0.0131
trigger times: 4
Loss after 2307201600 batches: 0.0127
trigger times: 5
Loss after 2307332700 batches: 0.0127
trigger times: 6
Loss after 2307463800 batches: 0.0124
trigger times: 7
Loss after 2307594900 batches: 0.0123
trigger times: 8
Loss after 2307726000 batches: 0.0120
trigger times: 9
Loss after 2307857100 batches: 0.0120
trigger times: 10
Loss after 2307988200 batches: 0.0119
trigger times: 11
Loss after 2308119300 batches: 0.0117
trigger times: 12
Loss after 2308250400 batches: 0.0115
trigger times: 13
Loss after 2308381500 batches: 0.0114
trigger times: 14
Loss after 2308512600 batches: 0.0114
trigger times: 15
Loss after 2308643700 batches: 0.0114
trigger times: 16
Loss after 2308774800 batches: 0.0111
trigger times: 17
Loss after 2308905900 batches: 0.0112
trigger times: 18
Loss after 2309037000 batches: 0.0109
trigger times: 19
Loss after 2309168100 batches: 0.0108
trigger times: 20
Early stopping!
Start to test process.
Loss after 2309299200 batches: 0.0107
Time to train on one home:  259.6426091194153
trigger times: 0
Loss after 2309430300 batches: 0.2775
trigger times: 1
Loss after 2309561400 batches: 0.0995
trigger times: 2
Loss after 2309692500 batches: 0.0771
trigger times: 3
Loss after 2309823600 batches: 0.0679
trigger times: 4
Loss after 2309954700 batches: 0.0641
trigger times: 5
Loss after 2310085800 batches: 0.0610
trigger times: 6
Loss after 2310216900 batches: 0.0577
trigger times: 0
Loss after 2310348000 batches: 0.0564
trigger times: 1
Loss after 2310479100 batches: 0.0537
trigger times: 2
Loss after 2310610200 batches: 0.0536
trigger times: 0
Loss after 2310741300 batches: 0.0521
trigger times: 1
Loss after 2310872400 batches: 0.0509
trigger times: 2
Loss after 2311003500 batches: 0.0504
trigger times: 3
Loss after 2311134600 batches: 0.0490
trigger times: 4
Loss after 2311265700 batches: 0.0485
trigger times: 5
Loss after 2311396800 batches: 0.0480
trigger times: 6
Loss after 2311527900 batches: 0.0477
trigger times: 7
Loss after 2311659000 batches: 0.0468
trigger times: 8
Loss after 2311790100 batches: 0.0464
trigger times: 0
Loss after 2311921200 batches: 0.0459
trigger times: 1
Loss after 2312052300 batches: 0.0458
trigger times: 2
Loss after 2312183400 batches: 0.0455
trigger times: 0
Loss after 2312314500 batches: 0.0454
trigger times: 0
Loss after 2312445600 batches: 0.0449
trigger times: 0
Loss after 2312576700 batches: 0.0446
trigger times: 1
Loss after 2312707800 batches: 0.0445
trigger times: 2
Loss after 2312838900 batches: 0.0437
trigger times: 3
Loss after 2312970000 batches: 0.0437
trigger times: 4
Loss after 2313101100 batches: 0.0432
trigger times: 5
Loss after 2313232200 batches: 0.0431
trigger times: 6
Loss after 2313363300 batches: 0.0426
trigger times: 0
Loss after 2313494400 batches: 0.0430
trigger times: 1
Loss after 2313625500 batches: 0.0428
trigger times: 2
Loss after 2313756600 batches: 0.0424
trigger times: 3
Loss after 2313887700 batches: 0.0422
trigger times: 4
Loss after 2314018800 batches: 0.0419
trigger times: 5
Loss after 2314149900 batches: 0.0417
trigger times: 6
Loss after 2314281000 batches: 0.0422
trigger times: 7
Loss after 2314412100 batches: 0.0418
trigger times: 8
Loss after 2314543200 batches: 0.0416
trigger times: 9
Loss after 2314674300 batches: 0.0415
trigger times: 10
Loss after 2314805400 batches: 0.0413
trigger times: 11
Loss after 2314936500 batches: 0.0411
trigger times: 12
Loss after 2315067600 batches: 0.0408
trigger times: 13
Loss after 2315198700 batches: 0.0404
trigger times: 0
Loss after 2315329800 batches: 0.0407
trigger times: 1
Loss after 2315460900 batches: 0.0407
trigger times: 2
Loss after 2315592000 batches: 0.0403
trigger times: 3
Loss after 2315723100 batches: 0.0401
trigger times: 4
Loss after 2315854200 batches: 0.0399
trigger times: 5
Loss after 2315985300 batches: 0.0399
trigger times: 6
Loss after 2316116400 batches: 0.0398
trigger times: 0
Loss after 2316247500 batches: 0.0396
trigger times: 1
Loss after 2316378600 batches: 0.0399
trigger times: 2
Loss after 2316509700 batches: 0.0397
trigger times: 3
Loss after 2316640800 batches: 0.0397
trigger times: 4
Loss after 2316771900 batches: 0.0398
trigger times: 5
Loss after 2316903000 batches: 0.0393
trigger times: 6
Loss after 2317034100 batches: 0.0390
trigger times: 7
Loss after 2317165200 batches: 0.0393
trigger times: 8
Loss after 2317296300 batches: 0.0393
trigger times: 9
Loss after 2317427400 batches: 0.0397
trigger times: 10
Loss after 2317558500 batches: 0.0389
trigger times: 11
Loss after 2317689600 batches: 0.0388
trigger times: 12
Loss after 2317820700 batches: 0.0386
trigger times: 13
Loss after 2317951800 batches: 0.0384
trigger times: 14
Loss after 2318082900 batches: 0.0383
trigger times: 15
Loss after 2318214000 batches: 0.0383
trigger times: 16
Loss after 2318345100 batches: 0.0383
trigger times: 17
Loss after 2318476200 batches: 0.0384
trigger times: 18
Loss after 2318607300 batches: 0.0381
trigger times: 19
Loss after 2318738400 batches: 0.0387
trigger times: 20
Early stopping!
Start to test process.
Loss after 2318869500 batches: 0.0379
Time to train on one home:  543.8789796829224
train_results:  [0.06280048316701156, 0.09085162036455097, 0.08497237821057535, 0.06214484349104733, 0.05578825832376929, 0.046453690863000816, 0.03614220310739596, 0.03671998517496642, 0.03501720033327798, 0.03263172240869951, 0.030110256078310443, 0.02544188748067248, 0.02612378438006769, 0.024274968770150986, 0.02405710057046961, 0.02388874572137445, 0.022740620412307226, 0.02141669577274335, 0.020377826915994508, 0.020317244662795355]
test_results:  [[0.8884346849388547, 0.03893765553149742, 0.22512619282418295, 1.5066871879078116, 0.7872970708674424, 35.59602484685973, 2430.435], [0.7459925909837087, 0.19315568282328555, 0.2987215518677111, 1.15110555815026, 0.660962497610463, 27.195281394920897, 2040.4323], [0.7218866977426741, 0.21922666564410576, 0.22170666654679347, 1.2071368399990507, 0.6396052895920611, 28.519040511543178, 1974.5012], [0.6822267969449362, 0.2620889898324764, 0.33804464760248504, 1.1306225496290971, 0.6044927056079937, 26.7113629770108, 1866.1067], [0.6867310702800751, 0.25717460938422276, 0.38017511496864487, 1.1898779737602583, 0.6085185394722656, 28.111293610663395, 1878.5345], [0.6736688845687442, 0.271296385681043, 0.40135605153338866, 1.2051764474278137, 0.5969500567636024, 28.47272553439636, 1842.8219], [0.6499174767070346, 0.2971208778557737, 0.4251960220434712, 1.1993132522961052, 0.5757947725483533, 28.334205447901105, 1777.5143], [0.6565217508210076, 0.29000023371061867, 0.43049355501974257, 1.2176572600274134, 0.5816279656917915, 28.7675892054821, 1795.5217], [0.646977467669381, 0.30037119565779424, 0.44372103516112954, 1.2056323946933416, 0.5731321297971859, 28.48349745196681, 1769.2944], [0.6491197148958842, 0.29803996281550993, 0.44160060986867417, 1.203530351478871, 0.5750418631238572, 28.433835927602637, 1775.19], [0.6497863001293607, 0.29730357979276123, 0.44701117944787117, 1.2068712720184533, 0.5756451041104418, 28.512766372815683, 1777.0521], [0.6468338668346405, 0.30051325220168756, 0.4517286880074036, 1.1964129191375579, 0.5730157578453048, 28.26568403748201, 1768.9353], [0.6481483081976572, 0.29911867824708416, 0.44591549869242414, 1.1883481958771474, 0.5741581852808264, 28.07515205986611, 1772.462], [0.6422967116038004, 0.30545179637907305, 0.45038470512861073, 1.1723096038740957, 0.568970129184914, 27.696234575181023, 1756.4463], [0.635419938299391, 0.3128809605404973, 0.4559346003460791, 1.1641353165126298, 0.5628841981140037, 27.503114106407303, 1737.6584], [0.6225904193189409, 0.32673875385346274, 0.47098132348279487, 1.1438745784657927, 0.551531968836913, 27.024446908120186, 1702.6134], [0.6290365358193716, 0.31973560183743677, 0.4703045471241807, 1.1605770615000164, 0.5572689130640909, 27.419049056371055, 1720.3236], [0.6229561699761285, 0.3263064240524094, 0.4742132552861605, 1.1461675708751116, 0.5518861310699636, 27.078619675653382, 1703.7068], [0.6310632460647159, 0.31756252541073104, 0.469797775342069, 1.1473480016614066, 0.5590490855111376, 27.106507776074086, 1725.8193], [0.6234406861994002, 0.32582702098136185, 0.47446754798736823, 1.1366931249952432, 0.5522788554710115, 26.85478249587466, 1704.9192]]
Round_19_results:  [0.6234406861994002, 0.32582702098136185, 0.47446754798736823, 1.1366931249952432, 0.5522788554710115, 26.85478249587466, 1704.9192]