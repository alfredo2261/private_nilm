LSTM(
  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))
  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))
  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))
  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))
  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))
  (linear1): Linear(in_features=23500, out_features=1024, bias=True)
  (linear2): Linear(in_features=1024, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (dropout): Dropout(p=0.2, inplace=False)
)
Window Length:  499
trigger times: 0
Loss after 131100 batches: 0.7760
trigger times: 0
Loss after 262200 batches: 0.3826
trigger times: 0
Loss after 393300 batches: 0.2821
trigger times: 0
Loss after 524400 batches: 0.2228
trigger times: 0
Loss after 655500 batches: 0.1845
trigger times: 1
Loss after 786600 batches: 0.1601
trigger times: 2
Loss after 917700 batches: 0.1347
trigger times: 3
Loss after 1048800 batches: 0.1174
trigger times: 0
Loss after 1179900 batches: 0.1053
trigger times: 0
Loss after 1311000 batches: 0.0920
trigger times: 1
Loss after 1442100 batches: 0.0843
trigger times: 2
Loss after 1573200 batches: 0.0744
trigger times: 0
Loss after 1704300 batches: 0.0677
trigger times: 0
Loss after 1835400 batches: 0.0622
trigger times: 1
Loss after 1966500 batches: 0.0573
trigger times: 2
Loss after 2097600 batches: 0.0537
trigger times: 3
Loss after 2228700 batches: 0.0493
trigger times: 4
Loss after 2359800 batches: 0.0466
trigger times: 0
Loss after 2490900 batches: 0.0453
trigger times: 0
Loss after 2622000 batches: 0.0413
trigger times: 1
Loss after 2753100 batches: 0.0398
trigger times: 2
Loss after 2884200 batches: 0.0377
trigger times: 3
Loss after 3015300 batches: 0.0356
trigger times: 4
Loss after 3146400 batches: 0.0347
trigger times: 5
Loss after 3277500 batches: 0.0338
trigger times: 6
Loss after 3408600 batches: 0.0325
trigger times: 0
Loss after 3539700 batches: 0.0320
trigger times: 1
Loss after 3670800 batches: 0.0304
trigger times: 2
Loss after 3801900 batches: 0.0288
trigger times: 3
Loss after 3933000 batches: 0.0282
trigger times: 4
Loss after 4064100 batches: 0.0271
trigger times: 5
Loss after 4195200 batches: 0.0254
trigger times: 6
Loss after 4326300 batches: 0.0253
trigger times: 0
Loss after 4457400 batches: 0.0245
trigger times: 1
Loss after 4588500 batches: 0.0250
trigger times: 2
Loss after 4719600 batches: 0.0237
trigger times: 3
Loss after 4850700 batches: 0.0233
trigger times: 4
Loss after 4981800 batches: 0.0218
trigger times: 5
Loss after 5112900 batches: 0.0223
trigger times: 6
Loss after 5244000 batches: 0.0217
trigger times: 7
Loss after 5375100 batches: 0.0210
trigger times: 8
Loss after 5506200 batches: 0.0204
trigger times: 0
Loss after 5637300 batches: 0.0202
trigger times: 0
Loss after 5768400 batches: 0.0197
trigger times: 1
Loss after 5899500 batches: 0.0194
trigger times: 2
Loss after 6030600 batches: 0.0186
trigger times: 0
Loss after 6161700 batches: 0.0186
trigger times: 1
Loss after 6292800 batches: 0.0184
trigger times: 2
Loss after 6423900 batches: 0.0186
trigger times: 3
Loss after 6555000 batches: 0.0178
trigger times: 4
Loss after 6686100 batches: 0.0177
trigger times: 5
Loss after 6817200 batches: 0.0176
trigger times: 6
Loss after 6948300 batches: 0.0165
trigger times: 7
Loss after 7079400 batches: 0.0170
trigger times: 8
Loss after 7210500 batches: 0.0170
trigger times: 9
Loss after 7341600 batches: 0.0160
trigger times: 10
Loss after 7472700 batches: 0.0154
trigger times: 11
Loss after 7603800 batches: 0.0157
trigger times: 12
Loss after 7734900 batches: 0.0156
trigger times: 13
Loss after 7866000 batches: 0.0154
trigger times: 14
Loss after 7997100 batches: 0.0149
trigger times: 15
Loss after 8128200 batches: 0.0149
trigger times: 16
Loss after 8259300 batches: 0.0148
trigger times: 17
Loss after 8390400 batches: 0.0146
trigger times: 18
Loss after 8521500 batches: 0.0147
trigger times: 19
Loss after 8652600 batches: 0.0141
trigger times: 20
Early stopping!
Start to test process.
Loss after 8783700 batches: 0.0146
Time to train on one home:  485.31966495513916
trigger times: 0
Loss after 8886300 batches: 0.9644
trigger times: 0
Loss after 8988900 batches: 0.7556
trigger times: 0
Loss after 9091500 batches: 0.6551
trigger times: 0
Loss after 9194100 batches: 0.5818
trigger times: 1
Loss after 9296700 batches: 0.5020
trigger times: 2
Loss after 9399300 batches: 0.4463
trigger times: 3
Loss after 9501900 batches: 0.4078
trigger times: 4
Loss after 9604500 batches: 0.3852
trigger times: 5
Loss after 9707100 batches: 0.3381
trigger times: 6
Loss after 9809700 batches: 0.2894
trigger times: 7
Loss after 9912300 batches: 0.2639
trigger times: 8
Loss after 10014900 batches: 0.2495
trigger times: 9
Loss after 10117500 batches: 0.2575
trigger times: 10
Loss after 10220100 batches: 0.2108
trigger times: 11
Loss after 10322700 batches: 0.1971
trigger times: 12
Loss after 10425300 batches: 0.1883
trigger times: 13
Loss after 10527900 batches: 0.1796
trigger times: 14
Loss after 10630500 batches: 0.1591
trigger times: 15
Loss after 10733100 batches: 0.1720
trigger times: 16
Loss after 10835700 batches: 0.1456
trigger times: 17
Loss after 10938300 batches: 0.1368
trigger times: 18
Loss after 11040900 batches: 0.1338
trigger times: 19
Loss after 11143500 batches: 0.1188
trigger times: 20
Early stopping!
Start to test process.
Loss after 11246100 batches: 0.1132
Time to train on one home:  148.7353801727295
trigger times: 0
Loss after 11377200 batches: 0.8837
trigger times: 0
Loss after 11508300 batches: 0.5505
trigger times: 1
Loss after 11639400 batches: 0.4521
trigger times: 2
Loss after 11770500 batches: 0.3725
trigger times: 3
Loss after 11901600 batches: 0.3141
trigger times: 4
Loss after 12032700 batches: 0.2718
trigger times: 5
Loss after 12163800 batches: 0.2353
trigger times: 6
Loss after 12294900 batches: 0.2123
trigger times: 7
Loss after 12426000 batches: 0.1874
trigger times: 8
Loss after 12557100 batches: 0.1722
trigger times: 9
Loss after 12688200 batches: 0.1555
trigger times: 10
Loss after 12819300 batches: 0.1393
trigger times: 11
Loss after 12950400 batches: 0.1285
trigger times: 12
Loss after 13081500 batches: 0.1170
trigger times: 13
Loss after 13212600 batches: 0.1062
trigger times: 14
Loss after 13343700 batches: 0.0968
trigger times: 15
Loss after 13474800 batches: 0.0880
trigger times: 16
Loss after 13605900 batches: 0.0821
trigger times: 17
Loss after 13737000 batches: 0.0746
trigger times: 18
Loss after 13868100 batches: 0.0685
trigger times: 19
Loss after 13999200 batches: 0.0640
trigger times: 20
Early stopping!
Start to test process.
Loss after 14130300 batches: 0.0603
Time to train on one home:  166.16529774665833
trigger times: 0
Loss after 14261400 batches: 0.9945
trigger times: 0
Loss after 14392500 batches: 0.8581
trigger times: 0
Loss after 14523600 batches: 0.7747
trigger times: 0
Loss after 14654700 batches: 0.7017
trigger times: 1
Loss after 14785800 batches: 0.6459
trigger times: 2
Loss after 14916900 batches: 0.5946
trigger times: 3
Loss after 15048000 batches: 0.5410
trigger times: 4
Loss after 15179100 batches: 0.4737
trigger times: 5
Loss after 15310200 batches: 0.4028
trigger times: 6
Loss after 15441300 batches: 0.3344
trigger times: 7
Loss after 15572400 batches: 0.2759
trigger times: 8
Loss after 15703500 batches: 0.2328
trigger times: 9
Loss after 15834600 batches: 0.2034
trigger times: 10
Loss after 15965700 batches: 0.1768
trigger times: 11
Loss after 16096800 batches: 0.1589
trigger times: 12
Loss after 16227900 batches: 0.1469
trigger times: 13
Loss after 16359000 batches: 0.1346
trigger times: 14
Loss after 16490100 batches: 0.1276
trigger times: 15
Loss after 16621200 batches: 0.1203
trigger times: 16
Loss after 16752300 batches: 0.1122
trigger times: 17
Loss after 16883400 batches: 0.1076
trigger times: 18
Loss after 17014500 batches: 0.1031
trigger times: 19
Loss after 17145600 batches: 0.0989
trigger times: 20
Early stopping!
Start to test process.
Loss after 17276700 batches: 0.0952
Time to train on one home:  180.40202474594116
trigger times: 0
Loss after 17405340 batches: 0.7706
trigger times: 0
Loss after 17533980 batches: 0.4634
trigger times: 0
Loss after 17662620 batches: 0.4053
trigger times: 0
Loss after 17791260 batches: 0.3521
trigger times: 0
Loss after 17919900 batches: 0.2949
trigger times: 1
Loss after 18048540 batches: 0.2394
trigger times: 2
Loss after 18177180 batches: 0.1983
trigger times: 3
Loss after 18305820 batches: 0.1655
trigger times: 4
Loss after 18434460 batches: 0.1493
trigger times: 5
Loss after 18563100 batches: 0.1256
trigger times: 6
Loss after 18691740 batches: 0.1122
trigger times: 7
Loss after 18820380 batches: 0.1013
trigger times: 8
Loss after 18949020 batches: 0.0921
trigger times: 9
Loss after 19077660 batches: 0.0862
trigger times: 10
Loss after 19206300 batches: 0.0786
trigger times: 11
Loss after 19334940 batches: 0.0733
trigger times: 12
Loss after 19463580 batches: 0.0694
trigger times: 13
Loss after 19592220 batches: 0.0667
trigger times: 14
Loss after 19720860 batches: 0.0614
trigger times: 15
Loss after 19849500 batches: 0.0591
trigger times: 16
Loss after 19978140 batches: 0.0563
trigger times: 17
Loss after 20106780 batches: 0.0531
trigger times: 18
Loss after 20235420 batches: 0.0503
trigger times: 19
Loss after 20364060 batches: 0.0484
trigger times: 20
Early stopping!
Start to test process.
Loss after 20492700 batches: 0.0467
Time to train on one home:  184.57797646522522
train_results:  [0.0659879873575018]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285]]
Round_0_results:  [0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 163 < 164; dropping {'Training_Loss': 0.5031390162009113, 'Validation_Loss': 0.2939141458935208, 'Training_R2': 0.4940720130534356, 'Validation_R2': 0.7263782004350081, 'Training_F1': 0.586548783642341, 'Validation_F1': 0.6827105798521621, 'Training_NEP': 0.8386375940842514, 'Validation_NEP': 0.6261541379673712, 'Training_NDE': 0.37980912003647244, 'Validation_NDE': 0.2178979774440424, 'Training_MAE': 27.77401391837544, 'Validation_MAE': 17.17174421745389, 'Training_MSE': 1671.1006, 'Validation_MSE': 804.6912}.
trigger times: 0
Loss after 20623800 batches: 0.5031
trigger times: 0
Loss after 20754900 batches: 0.1995
trigger times: 1
Loss after 20886000 batches: 0.1510
trigger times: 0
Loss after 21017100 batches: 0.1253
trigger times: 1
Loss after 21148200 batches: 0.0970
trigger times: 0
Loss after 21279300 batches: 0.0803
trigger times: 0
Loss after 21410400 batches: 0.0666
trigger times: 0
Loss after 21541500 batches: 0.0560
trigger times: 0
Loss after 21672600 batches: 0.0495
trigger times: 0
Loss after 21803700 batches: 0.0444
trigger times: 1
Loss after 21934800 batches: 0.0402
trigger times: 0
Loss after 22065900 batches: 0.0365
trigger times: 0
Loss after 22197000 batches: 0.0337
trigger times: 0
Loss after 22328100 batches: 0.0322
trigger times: 1
Loss after 22459200 batches: 0.0297
trigger times: 2
Loss after 22590300 batches: 0.0280
trigger times: 3
Loss after 22721400 batches: 0.0264
trigger times: 4
Loss after 22852500 batches: 0.0261
trigger times: 5
Loss after 22983600 batches: 0.0248
trigger times: 6
Loss after 23114700 batches: 0.0238
trigger times: 7
Loss after 23245800 batches: 0.0230
trigger times: 8
Loss after 23376900 batches: 0.0219
trigger times: 0
Loss after 23508000 batches: 0.0209
trigger times: 1
Loss after 23639100 batches: 0.0205
trigger times: 2
Loss after 23770200 batches: 0.0198
trigger times: 3
Loss after 23901300 batches: 0.0197
trigger times: 4
Loss after 24032400 batches: 0.0190
trigger times: 5
Loss after 24163500 batches: 0.0189
trigger times: 0
Loss after 24294600 batches: 0.0177
trigger times: 1
Loss after 24425700 batches: 0.0177
trigger times: 2
Loss after 24556800 batches: 0.0173
trigger times: 3
Loss after 24687900 batches: 0.0171
trigger times: 4
Loss after 24819000 batches: 0.0160
trigger times: 5
Loss after 24950100 batches: 0.0165
trigger times: 6
Loss after 25081200 batches: 0.0161
trigger times: 7
Loss after 25212300 batches: 0.0158
trigger times: 8
Loss after 25343400 batches: 0.0155
trigger times: 9
Loss after 25474500 batches: 0.0152
trigger times: 10
Loss after 25605600 batches: 0.0148
trigger times: 11
Loss after 25736700 batches: 0.0148
trigger times: 12
Loss after 25867800 batches: 0.0144
trigger times: 13
Loss after 25998900 batches: 0.0141
trigger times: 14
Loss after 26130000 batches: 0.0142
trigger times: 15
Loss after 26261100 batches: 0.0142
trigger times: 16
Loss after 26392200 batches: 0.0140
trigger times: 17
Loss after 26523300 batches: 0.0138
trigger times: 18
Loss after 26654400 batches: 0.0135
trigger times: 19
Loss after 26785500 batches: 0.0133
trigger times: 0
Loss after 26916600 batches: 0.0133
trigger times: 1
Loss after 27047700 batches: 0.0129
trigger times: 2
Loss after 27178800 batches: 0.0127
trigger times: 3
Loss after 27309900 batches: 0.0124
trigger times: 4
Loss after 27441000 batches: 0.0125
trigger times: 5
Loss after 27572100 batches: 0.0122
trigger times: 6
Loss after 27703200 batches: 0.0123
trigger times: 7
Loss after 27834300 batches: 0.0123
trigger times: 8
Loss after 27965400 batches: 0.0120
trigger times: 9
Loss after 28096500 batches: 0.0120
trigger times: 10
Loss after 28227600 batches: 0.0119
trigger times: 11
Loss after 28358700 batches: 0.0116
trigger times: 12
Loss after 28489800 batches: 0.0117
trigger times: 13
Loss after 28620900 batches: 0.0114
trigger times: 14
Loss after 28752000 batches: 0.0114
trigger times: 15
Loss after 28883100 batches: 0.0112
trigger times: 16
Loss after 29014200 batches: 0.0112
trigger times: 17
Loss after 29145300 batches: 0.0110
trigger times: 18
Loss after 29276400 batches: 0.0114
trigger times: 19
Loss after 29407500 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 29538600 batches: 0.0110
Time to train on one home:  498.6378107070923
trigger times: 0
Loss after 29641200 batches: 0.7972
trigger times: 0
Loss after 29743800 batches: 0.5298
trigger times: 1
Loss after 29846400 batches: 0.4335
trigger times: 2
Loss after 29949000 batches: 0.3653
trigger times: 3
Loss after 30051600 batches: 0.2957
trigger times: 4
Loss after 30154200 batches: 0.2611
trigger times: 5
Loss after 30256800 batches: 0.2362
trigger times: 6
Loss after 30359400 batches: 0.2061
trigger times: 7
Loss after 30462000 batches: 0.1756
trigger times: 8
Loss after 30564600 batches: 0.1660
trigger times: 9
Loss after 30667200 batches: 0.1568
trigger times: 10
Loss after 30769800 batches: 0.1552
trigger times: 11
Loss after 30872400 batches: 0.1338
trigger times: 12
Loss after 30975000 batches: 0.1250
trigger times: 13
Loss after 31077600 batches: 0.1295
trigger times: 14
Loss after 31180200 batches: 0.1096
trigger times: 15
Loss after 31282800 batches: 0.1030
trigger times: 16
Loss after 31385400 batches: 0.1076
trigger times: 17
Loss after 31488000 batches: 0.0947
trigger times: 18
Loss after 31590600 batches: 0.0960
trigger times: 19
Loss after 31693200 batches: 0.0944
trigger times: 20
Early stopping!
Start to test process.
Loss after 31795800 batches: 0.0878
Time to train on one home:  137.60408735275269
trigger times: 0
Loss after 31926900 batches: 0.6138
trigger times: 0
Loss after 32058000 batches: 0.3554
trigger times: 1
Loss after 32189100 batches: 0.2803
trigger times: 0
Loss after 32320200 batches: 0.2271
trigger times: 1
Loss after 32451300 batches: 0.1833
trigger times: 2
Loss after 32582400 batches: 0.1516
trigger times: 3
Loss after 32713500 batches: 0.1274
trigger times: 4
Loss after 32844600 batches: 0.1112
trigger times: 5
Loss after 32975700 batches: 0.0990
trigger times: 6
Loss after 33106800 batches: 0.0900
trigger times: 7
Loss after 33237900 batches: 0.0822
trigger times: 8
Loss after 33369000 batches: 0.0760
trigger times: 9
Loss after 33500100 batches: 0.0703
trigger times: 10
Loss after 33631200 batches: 0.0675
trigger times: 11
Loss after 33762300 batches: 0.0632
trigger times: 12
Loss after 33893400 batches: 0.0603
trigger times: 13
Loss after 34024500 batches: 0.0581
trigger times: 14
Loss after 34155600 batches: 0.0559
trigger times: 15
Loss after 34286700 batches: 0.0536
trigger times: 16
Loss after 34417800 batches: 0.0517
trigger times: 17
Loss after 34548900 batches: 0.0495
trigger times: 18
Loss after 34680000 batches: 0.0480
trigger times: 19
Loss after 34811100 batches: 0.0466
trigger times: 20
Early stopping!
Start to test process.
Loss after 34942200 batches: 0.0463
Time to train on one home:  181.38920331001282
trigger times: 0
Loss after 35073300 batches: 0.8051
trigger times: 1
Loss after 35204400 batches: 0.6128
trigger times: 2
Loss after 35335500 batches: 0.4772
trigger times: 3
Loss after 35466600 batches: 0.3581
trigger times: 4
Loss after 35597700 batches: 0.2701
trigger times: 5
Loss after 35728800 batches: 0.2151
trigger times: 6
Loss after 35859900 batches: 0.1808
trigger times: 7
Loss after 35991000 batches: 0.1591
trigger times: 8
Loss after 36122100 batches: 0.1406
trigger times: 9
Loss after 36253200 batches: 0.1275
trigger times: 10
Loss after 36384300 batches: 0.1170
trigger times: 11
Loss after 36515400 batches: 0.1096
trigger times: 12
Loss after 36646500 batches: 0.1045
trigger times: 13
Loss after 36777600 batches: 0.0975
trigger times: 14
Loss after 36908700 batches: 0.0932
trigger times: 15
Loss after 37039800 batches: 0.0896
trigger times: 16
Loss after 37170900 batches: 0.0849
trigger times: 17
Loss after 37302000 batches: 0.0829
trigger times: 18
Loss after 37433100 batches: 0.0802
trigger times: 19
Loss after 37564200 batches: 0.0761
trigger times: 20
Early stopping!
Start to test process.
Loss after 37695300 batches: 0.0740
Time to train on one home:  159.84588599205017
trigger times: 0
Loss after 37823940 batches: 0.5274
trigger times: 0
Loss after 37952580 batches: 0.3159
trigger times: 1
Loss after 38081220 batches: 0.2161
trigger times: 2
Loss after 38209860 batches: 0.1553
trigger times: 3
Loss after 38338500 batches: 0.1216
trigger times: 4
Loss after 38467140 batches: 0.1024
trigger times: 5
Loss after 38595780 batches: 0.0878
trigger times: 6
Loss after 38724420 batches: 0.0789
trigger times: 7
Loss after 38853060 batches: 0.0721
trigger times: 8
Loss after 38981700 batches: 0.0648
trigger times: 9
Loss after 39110340 batches: 0.0602
trigger times: 10
Loss after 39238980 batches: 0.0567
trigger times: 11
Loss after 39367620 batches: 0.0536
trigger times: 12
Loss after 39496260 batches: 0.0503
trigger times: 13
Loss after 39624900 batches: 0.0473
trigger times: 14
Loss after 39753540 batches: 0.0459
trigger times: 15
Loss after 39882180 batches: 0.0434
trigger times: 16
Loss after 40010820 batches: 0.0424
trigger times: 17
Loss after 40139460 batches: 0.0412
trigger times: 18
Loss after 40268100 batches: 0.0395
trigger times: 19
Loss after 40396740 batches: 0.0377
trigger times: 20
Early stopping!
Start to test process.
Loss after 40525380 batches: 0.0372
Time to train on one home:  164.38844084739685
train_results:  [0.0659879873575018, 0.05127666929189745]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341]]
Round_1_results:  [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341]
trigger times: 0
Loss after 40656480 batches: 0.2631
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 321 < 322; dropping {'Training_Loss': 0.26311454289364367, 'Validation_Loss': 0.2807894895474116, 'Training_R2': 0.7354380732507086, 'Validation_R2': 0.7388427034365754, 'Training_F1': 0.7090673164886616, 'Validation_F1': 0.7128524661969133, 'Training_NEP': 0.5878593138653757, 'Validation_NEP': 0.6049905136709022, 'Training_NDE': 0.19861133439217094, 'Validation_NDE': 0.2079719043087709, 'Training_MAE': 19.468734624485858, 'Validation_MAE': 16.591349836745998, 'Training_MSE': 873.85876, 'Validation_MSE': 768.0345}.
trigger times: 0
Loss after 40787580 batches: 0.0838
trigger times: 1
Loss after 40918680 batches: 0.0507
trigger times: 0
Loss after 41049780 batches: 0.0388
trigger times: 1
Loss after 41180880 batches: 0.0330
trigger times: 2
Loss after 41311980 batches: 0.0294
trigger times: 3
Loss after 41443080 batches: 0.0270
trigger times: 4
Loss after 41574180 batches: 0.0250
trigger times: 5
Loss after 41705280 batches: 0.0233
trigger times: 0
Loss after 41836380 batches: 0.0221
trigger times: 1
Loss after 41967480 batches: 0.0212
trigger times: 0
Loss after 42098580 batches: 0.0202
trigger times: 0
Loss after 42229680 batches: 0.0194
trigger times: 1
Loss after 42360780 batches: 0.0185
trigger times: 2
Loss after 42491880 batches: 0.0178
trigger times: 3
Loss after 42622980 batches: 0.0170
trigger times: 4
Loss after 42754080 batches: 0.0169
trigger times: 5
Loss after 42885180 batches: 0.0164
trigger times: 6
Loss after 43016280 batches: 0.0164
trigger times: 7
Loss after 43147380 batches: 0.0157
trigger times: 8
Loss after 43278480 batches: 0.0153
trigger times: 9
Loss after 43409580 batches: 0.0151
trigger times: 10
Loss after 43540680 batches: 0.0147
trigger times: 11
Loss after 43671780 batches: 0.0146
trigger times: 12
Loss after 43802880 batches: 0.0146
trigger times: 13
Loss after 43933980 batches: 0.0139
trigger times: 14
Loss after 44065080 batches: 0.0139
trigger times: 15
Loss after 44196180 batches: 0.0135
trigger times: 16
Loss after 44327280 batches: 0.0137
trigger times: 17
Loss after 44458380 batches: 0.0132
trigger times: 18
Loss after 44589480 batches: 0.0134
trigger times: 19
Loss after 44720580 batches: 0.0131
trigger times: 20
Early stopping!
Start to test process.
Loss after 44851680 batches: 0.0128
Time to train on one home:  245.67178750038147
trigger times: 0
Loss after 44954280 batches: 0.5042
trigger times: 1
Loss after 45056880 batches: 0.2935
trigger times: 2
Loss after 45159480 batches: 0.2087
trigger times: 3
Loss after 45262080 batches: 0.1562
trigger times: 4
Loss after 45364680 batches: 0.1280
trigger times: 5
Loss after 45467280 batches: 0.1124
trigger times: 6
Loss after 45569880 batches: 0.1052
trigger times: 7
Loss after 45672480 batches: 0.0999
trigger times: 8
Loss after 45775080 batches: 0.1019
trigger times: 9
Loss after 45877680 batches: 0.0839
trigger times: 10
Loss after 45980280 batches: 0.0777
trigger times: 11
Loss after 46082880 batches: 0.0703
trigger times: 12
Loss after 46185480 batches: 0.0716
trigger times: 13
Loss after 46288080 batches: 0.0684
trigger times: 14
Loss after 46390680 batches: 0.0690
trigger times: 15
Loss after 46493280 batches: 0.0580
trigger times: 16
Loss after 46595880 batches: 0.0579
trigger times: 17
Loss after 46698480 batches: 0.0523
trigger times: 18
Loss after 46801080 batches: 0.0565
trigger times: 19
Loss after 46903680 batches: 0.0504
trigger times: 20
Early stopping!
Start to test process.
Loss after 47006280 batches: 0.0478
Time to train on one home:  135.78426933288574
trigger times: 0
Loss after 47137380 batches: 0.3277
trigger times: 1
Loss after 47268480 batches: 0.1516
trigger times: 0
Loss after 47399580 batches: 0.0963
trigger times: 1
Loss after 47530680 batches: 0.0764
trigger times: 2
Loss after 47661780 batches: 0.0645
trigger times: 3
Loss after 47792880 batches: 0.0571
trigger times: 4
Loss after 47923980 batches: 0.0520
trigger times: 5
Loss after 48055080 batches: 0.0475
trigger times: 6
Loss after 48186180 batches: 0.0454
trigger times: 7
Loss after 48317280 batches: 0.0426
trigger times: 0
Loss after 48448380 batches: 0.0411
trigger times: 1
Loss after 48579480 batches: 0.0385
trigger times: 2
Loss after 48710580 batches: 0.0372
trigger times: 3
Loss after 48841680 batches: 0.0354
trigger times: 4
Loss after 48972780 batches: 0.0339
trigger times: 5
Loss after 49103880 batches: 0.0330
trigger times: 6
Loss after 49234980 batches: 0.0323
trigger times: 0
Loss after 49366080 batches: 0.0313
trigger times: 0
Loss after 49497180 batches: 0.0304
trigger times: 0
Loss after 49628280 batches: 0.0299
trigger times: 1
Loss after 49759380 batches: 0.0292
trigger times: 2
Loss after 49890480 batches: 0.0283
trigger times: 3
Loss after 50021580 batches: 0.0279
trigger times: 4
Loss after 50152680 batches: 0.0279
trigger times: 5
Loss after 50283780 batches: 0.0266
trigger times: 6
Loss after 50414880 batches: 0.0263
trigger times: 7
Loss after 50545980 batches: 0.0261
trigger times: 8
Loss after 50677080 batches: 0.0254
trigger times: 9
Loss after 50808180 batches: 0.0251
trigger times: 10
Loss after 50939280 batches: 0.0247
trigger times: 11
Loss after 51070380 batches: 0.0239
trigger times: 12
Loss after 51201480 batches: 0.0235
trigger times: 0
Loss after 51332580 batches: 0.0234
trigger times: 1
Loss after 51463680 batches: 0.0233
trigger times: 0
Loss after 51594780 batches: 0.0228
trigger times: 1
Loss after 51725880 batches: 0.0227
trigger times: 2
Loss after 51856980 batches: 0.0224
trigger times: 3
Loss after 51988080 batches: 0.0222
trigger times: 4
Loss after 52119180 batches: 0.0220
trigger times: 0
Loss after 52250280 batches: 0.0218
trigger times: 1
Loss after 52381380 batches: 0.0216
trigger times: 2
Loss after 52512480 batches: 0.0216
trigger times: 0
Loss after 52643580 batches: 0.0211
trigger times: 1
Loss after 52774680 batches: 0.0210
trigger times: 2
Loss after 52905780 batches: 0.0206
trigger times: 0
Loss after 53036880 batches: 0.0205
trigger times: 1
Loss after 53167980 batches: 0.0202
trigger times: 2
Loss after 53299080 batches: 0.0204
trigger times: 3
Loss after 53430180 batches: 0.0200
trigger times: 4
Loss after 53561280 batches: 0.0198
trigger times: 5
Loss after 53692380 batches: 0.0197
trigger times: 0
Loss after 53823480 batches: 0.0196
trigger times: 0
Loss after 53954580 batches: 0.0189
trigger times: 1
Loss after 54085680 batches: 0.0188
trigger times: 2
Loss after 54216780 batches: 0.0190
trigger times: 0
Loss after 54347880 batches: 0.0186
trigger times: 0
Loss after 54478980 batches: 0.0186
trigger times: 1
Loss after 54610080 batches: 0.0186
trigger times: 2
Loss after 54741180 batches: 0.0185
trigger times: 3
Loss after 54872280 batches: 0.0181
trigger times: 4
Loss after 55003380 batches: 0.0178
trigger times: 5
Loss after 55134480 batches: 0.0178
trigger times: 6
Loss after 55265580 batches: 0.0179
trigger times: 7
Loss after 55396680 batches: 0.0176
trigger times: 8
Loss after 55527780 batches: 0.0177
trigger times: 9
Loss after 55658880 batches: 0.0177
trigger times: 10
Loss after 55789980 batches: 0.0177
trigger times: 11
Loss after 55921080 batches: 0.0172
trigger times: 12
Loss after 56052180 batches: 0.0173
trigger times: 13
Loss after 56183280 batches: 0.0169
trigger times: 14
Loss after 56314380 batches: 0.0169
trigger times: 15
Loss after 56445480 batches: 0.0167
trigger times: 16
Loss after 56576580 batches: 0.0166
trigger times: 17
Loss after 56707680 batches: 0.0167
trigger times: 18
Loss after 56838780 batches: 0.0164
trigger times: 0
Loss after 56969880 batches: 0.0166
trigger times: 1
Loss after 57100980 batches: 0.0162
trigger times: 2
Loss after 57232080 batches: 0.0165
trigger times: 3
Loss after 57363180 batches: 0.0161
trigger times: 4
Loss after 57494280 batches: 0.0162
trigger times: 5
Loss after 57625380 batches: 0.0161
trigger times: 0
Loss after 57756480 batches: 0.0161
trigger times: 1
Loss after 57887580 batches: 0.0159
trigger times: 2
Loss after 58018680 batches: 0.0156
trigger times: 3
Loss after 58149780 batches: 0.0159
trigger times: 4
Loss after 58280880 batches: 0.0160
trigger times: 5
Loss after 58411980 batches: 0.0158
trigger times: 6
Loss after 58543080 batches: 0.0154
trigger times: 0
Loss after 58674180 batches: 0.0156
trigger times: 1
Loss after 58805280 batches: 0.0154
trigger times: 2
Loss after 58936380 batches: 0.0152
trigger times: 3
Loss after 59067480 batches: 0.0151
trigger times: 4
Loss after 59198580 batches: 0.0154
trigger times: 5
Loss after 59329680 batches: 0.0151
trigger times: 6
Loss after 59460780 batches: 0.0151
trigger times: 7
Loss after 59591880 batches: 0.0152
trigger times: 8
Loss after 59722980 batches: 0.0150
trigger times: 9
Loss after 59854080 batches: 0.0149
trigger times: 10
Loss after 59985180 batches: 0.0149
trigger times: 11
Loss after 60116280 batches: 0.0147
trigger times: 12
Loss after 60247380 batches: 0.0148
trigger times: 13
Loss after 60378480 batches: 0.0147
trigger times: 14
Loss after 60509580 batches: 0.0148
trigger times: 15
Loss after 60640680 batches: 0.0146
trigger times: 16
Loss after 60771780 batches: 0.0146
trigger times: 17
Loss after 60902880 batches: 0.0145
trigger times: 18
Loss after 61033980 batches: 0.0143
trigger times: 19
Loss after 61165080 batches: 0.0143
trigger times: 20
Early stopping!
Start to test process.
Loss after 61296180 batches: 0.0143
Time to train on one home:  807.72212433815
trigger times: 0
Loss after 61427280 batches: 0.6348
trigger times: 1
Loss after 61558380 batches: 0.3335
trigger times: 2
Loss after 61689480 batches: 0.1884
trigger times: 3
Loss after 61820580 batches: 0.1411
trigger times: 4
Loss after 61951680 batches: 0.1166
trigger times: 5
Loss after 62082780 batches: 0.1012
trigger times: 6
Loss after 62213880 batches: 0.0912
trigger times: 7
Loss after 62344980 batches: 0.0860
trigger times: 8
Loss after 62476080 batches: 0.0797
trigger times: 9
Loss after 62607180 batches: 0.0755
trigger times: 10
Loss after 62738280 batches: 0.0714
trigger times: 11
Loss after 62869380 batches: 0.0685
trigger times: 12
Loss after 63000480 batches: 0.0652
trigger times: 13
Loss after 63131580 batches: 0.0632
trigger times: 14
Loss after 63262680 batches: 0.0608
trigger times: 15
Loss after 63393780 batches: 0.0585
trigger times: 16
Loss after 63524880 batches: 0.0574
trigger times: 17
Loss after 63655980 batches: 0.0552
trigger times: 18
Loss after 63787080 batches: 0.0534
trigger times: 19
Loss after 63918180 batches: 0.0537
trigger times: 20
Early stopping!
Start to test process.
Loss after 64049280 batches: 0.0520
Time to train on one home:  205.50591588020325
trigger times: 0
Loss after 64177920 batches: 0.2851
trigger times: 0
Loss after 64306560 batches: 0.1087
trigger times: 1
Loss after 64435200 batches: 0.0729
trigger times: 2
Loss after 64563840 batches: 0.0564
trigger times: 3
Loss after 64692480 batches: 0.0498
trigger times: 4
Loss after 64821120 batches: 0.0438
trigger times: 5
Loss after 64949760 batches: 0.0399
trigger times: 6
Loss after 65078400 batches: 0.0376
trigger times: 7
Loss after 65207040 batches: 0.0343
trigger times: 8
Loss after 65335680 batches: 0.0334
trigger times: 9
Loss after 65464320 batches: 0.0307
trigger times: 10
Loss after 65592960 batches: 0.0294
trigger times: 11
Loss after 65721600 batches: 0.0283
trigger times: 12
Loss after 65850240 batches: 0.0270
trigger times: 13
Loss after 65978880 batches: 0.0259
trigger times: 0
Loss after 66107520 batches: 0.0257
trigger times: 1
Loss after 66236160 batches: 0.0251
trigger times: 2
Loss after 66364800 batches: 0.0248
trigger times: 3
Loss after 66493440 batches: 0.0235
trigger times: 4
Loss after 66622080 batches: 0.0226
trigger times: 5
Loss after 66750720 batches: 0.0226
trigger times: 6
Loss after 66879360 batches: 0.0216
trigger times: 7
Loss after 67008000 batches: 0.0214
trigger times: 8
Loss after 67136640 batches: 0.0212
trigger times: 9
Loss after 67265280 batches: 0.0208
trigger times: 10
Loss after 67393920 batches: 0.0207
trigger times: 11
Loss after 67522560 batches: 0.0195
trigger times: 12
Loss after 67651200 batches: 0.0195
trigger times: 13
Loss after 67779840 batches: 0.0189
trigger times: 14
Loss after 67908480 batches: 0.0186
trigger times: 15
Loss after 68037120 batches: 0.0187
trigger times: 16
Loss after 68165760 batches: 0.0183
trigger times: 17
Loss after 68294400 batches: 0.0183
trigger times: 18
Loss after 68423040 batches: 0.0178
trigger times: 19
Loss after 68551680 batches: 0.0179
trigger times: 20
Early stopping!
Start to test process.
Loss after 68680320 batches: 0.0177
Time to train on one home:  268.06844544410706
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798]]
Round_2_results:  [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798]
trigger times: 0
Loss after 68811420 batches: 0.1620
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 541 < 542; dropping {'Training_Loss': 0.16197583086366923, 'Validation_Loss': 0.2663210862212711, 'Training_R2': 0.8369711141599907, 'Validation_R2': 0.7521661020475436, 'Training_F1': 0.7817031977466782, 'Validation_F1': 0.6976721168979343, 'Training_NEP': 0.43749876065722143, 'Validation_NEP': 0.5804573363474633, 'Training_NDE': 0.12238867836730331, 'Validation_NDE': 0.19736185198608966, 'Training_MAE': 14.489091299363986, 'Validation_MAE': 15.91854833261941, 'Training_MSE': 538.4911, 'Validation_MSE': 728.85187}.
trigger times: 0
Loss after 68942520 batches: 0.0449
trigger times: 0
Loss after 69073620 batches: 0.0321
trigger times: 0
Loss after 69204720 batches: 0.0272
trigger times: 1
Loss after 69335820 batches: 0.0241
trigger times: 0
Loss after 69466920 batches: 0.0230
trigger times: 0
Loss after 69598020 batches: 0.0212
trigger times: 1
Loss after 69729120 batches: 0.0196
trigger times: 0
Loss after 69860220 batches: 0.0186
trigger times: 1
Loss after 69991320 batches: 0.0181
trigger times: 2
Loss after 70122420 batches: 0.0175
trigger times: 3
Loss after 70253520 batches: 0.0170
trigger times: 4
Loss after 70384620 batches: 0.0163
trigger times: 5
Loss after 70515720 batches: 0.0160
trigger times: 6
Loss after 70646820 batches: 0.0157
trigger times: 0
Loss after 70777920 batches: 0.0156
trigger times: 1
Loss after 70909020 batches: 0.0146
trigger times: 2
Loss after 71040120 batches: 0.0148
trigger times: 3
Loss after 71171220 batches: 0.0142
trigger times: 4
Loss after 71302320 batches: 0.0141
trigger times: 5
Loss after 71433420 batches: 0.0139
trigger times: 6
Loss after 71564520 batches: 0.0138
trigger times: 7
Loss after 71695620 batches: 0.0133
trigger times: 8
Loss after 71826720 batches: 0.0128
trigger times: 9
Loss after 71957820 batches: 0.0128
trigger times: 10
Loss after 72088920 batches: 0.0127
trigger times: 0
Loss after 72220020 batches: 0.0127
trigger times: 1
Loss after 72351120 batches: 0.0126
trigger times: 0
Loss after 72482220 batches: 0.0126
trigger times: 0
Loss after 72613320 batches: 0.0123
trigger times: 0
Loss after 72744420 batches: 0.0122
trigger times: 1
Loss after 72875520 batches: 0.0117
trigger times: 0
Loss after 73006620 batches: 0.0116
trigger times: 1
Loss after 73137720 batches: 0.0117
trigger times: 2
Loss after 73268820 batches: 0.0114
trigger times: 3
Loss after 73399920 batches: 0.0112
trigger times: 4
Loss after 73531020 batches: 0.0114
trigger times: 5
Loss after 73662120 batches: 0.0114
trigger times: 6
Loss after 73793220 batches: 0.0111
trigger times: 7
Loss after 73924320 batches: 0.0108
trigger times: 8
Loss after 74055420 batches: 0.0108
trigger times: 9
Loss after 74186520 batches: 0.0108
trigger times: 10
Loss after 74317620 batches: 0.0108
trigger times: 11
Loss after 74448720 batches: 0.0107
trigger times: 12
Loss after 74579820 batches: 0.0106
trigger times: 13
Loss after 74710920 batches: 0.0106
trigger times: 14
Loss after 74842020 batches: 0.0103
trigger times: 15
Loss after 74973120 batches: 0.0103
trigger times: 16
Loss after 75104220 batches: 0.0101
trigger times: 17
Loss after 75235320 batches: 0.0101
trigger times: 18
Loss after 75366420 batches: 0.0103
trigger times: 19
Loss after 75497520 batches: 0.0101
trigger times: 20
Early stopping!
Start to test process.
Loss after 75628620 batches: 0.0105
Time to train on one home:  391.47810077667236
trigger times: 0
Loss after 75731220 batches: 0.4289
trigger times: 1
Loss after 75833820 batches: 0.1965
trigger times: 2
Loss after 75936420 batches: 0.1426
trigger times: 3
Loss after 76039020 batches: 0.1040
trigger times: 4
Loss after 76141620 batches: 0.0843
trigger times: 5
Loss after 76244220 batches: 0.0733
trigger times: 6
Loss after 76346820 batches: 0.0662
trigger times: 7
Loss after 76449420 batches: 0.0594
trigger times: 8
Loss after 76552020 batches: 0.0561
trigger times: 9
Loss after 76654620 batches: 0.0543
trigger times: 10
Loss after 76757220 batches: 0.0529
trigger times: 11
Loss after 76859820 batches: 0.0529
trigger times: 12
Loss after 76962420 batches: 0.0459
trigger times: 13
Loss after 77065020 batches: 0.0441
trigger times: 14
Loss after 77167620 batches: 0.0437
trigger times: 15
Loss after 77270220 batches: 0.0572
trigger times: 16
Loss after 77372820 batches: 0.0431
trigger times: 17
Loss after 77475420 batches: 0.0399
trigger times: 18
Loss after 77578020 batches: 0.0390
trigger times: 19
Loss after 77680620 batches: 0.0375
trigger times: 20
Early stopping!
Start to test process.
Loss after 77783220 batches: 0.0382
Time to train on one home:  133.78152585029602
trigger times: 0
Loss after 77914320 batches: 0.1967
trigger times: 0
Loss after 78045420 batches: 0.0655
trigger times: 1
Loss after 78176520 batches: 0.0450
trigger times: 2
Loss after 78307620 batches: 0.0374
trigger times: 0
Loss after 78438720 batches: 0.0327
trigger times: 0
Loss after 78569820 batches: 0.0298
trigger times: 0
Loss after 78700920 batches: 0.0278
trigger times: 1
Loss after 78832020 batches: 0.0264
trigger times: 2
Loss after 78963120 batches: 0.0256
trigger times: 0
Loss after 79094220 batches: 0.0242
trigger times: 1
Loss after 79225320 batches: 0.0230
trigger times: 2
Loss after 79356420 batches: 0.0225
trigger times: 3
Loss after 79487520 batches: 0.0216
trigger times: 4
Loss after 79618620 batches: 0.0211
trigger times: 5
Loss after 79749720 batches: 0.0205
trigger times: 6
Loss after 79880820 batches: 0.0203
trigger times: 7
Loss after 80011920 batches: 0.0199
trigger times: 0
Loss after 80143020 batches: 0.0195
trigger times: 1
Loss after 80274120 batches: 0.0189
trigger times: 2
Loss after 80405220 batches: 0.0186
trigger times: 3
Loss after 80536320 batches: 0.0183
trigger times: 4
Loss after 80667420 batches: 0.0183
trigger times: 5
Loss after 80798520 batches: 0.0177
trigger times: 6
Loss after 80929620 batches: 0.0176
trigger times: 7
Loss after 81060720 batches: 0.0174
trigger times: 8
Loss after 81191820 batches: 0.0172
trigger times: 0
Loss after 81322920 batches: 0.0175
trigger times: 1
Loss after 81454020 batches: 0.0168
trigger times: 2
Loss after 81585120 batches: 0.0168
trigger times: 3
Loss after 81716220 batches: 0.0164
trigger times: 4
Loss after 81847320 batches: 0.0163
trigger times: 5
Loss after 81978420 batches: 0.0162
trigger times: 6
Loss after 82109520 batches: 0.0161
trigger times: 7
Loss after 82240620 batches: 0.0159
trigger times: 8
Loss after 82371720 batches: 0.0156
trigger times: 9
Loss after 82502820 batches: 0.0159
trigger times: 10
Loss after 82633920 batches: 0.0158
trigger times: 11
Loss after 82765020 batches: 0.0154
trigger times: 12
Loss after 82896120 batches: 0.0152
trigger times: 13
Loss after 83027220 batches: 0.0153
trigger times: 14
Loss after 83158320 batches: 0.0152
trigger times: 0
Loss after 83289420 batches: 0.0149
trigger times: 1
Loss after 83420520 batches: 0.0150
trigger times: 2
Loss after 83551620 batches: 0.0147
trigger times: 0
Loss after 83682720 batches: 0.0148
trigger times: 1
Loss after 83813820 batches: 0.0144
trigger times: 2
Loss after 83944920 batches: 0.0146
trigger times: 3
Loss after 84076020 batches: 0.0143
trigger times: 4
Loss after 84207120 batches: 0.0143
trigger times: 5
Loss after 84338220 batches: 0.0143
trigger times: 6
Loss after 84469320 batches: 0.0140
trigger times: 7
Loss after 84600420 batches: 0.0141
trigger times: 8
Loss after 84731520 batches: 0.0143
trigger times: 9
Loss after 84862620 batches: 0.0139
trigger times: 10
Loss after 84993720 batches: 0.0139
trigger times: 11
Loss after 85124820 batches: 0.0137
trigger times: 12
Loss after 85255920 batches: 0.0138
trigger times: 13
Loss after 85387020 batches: 0.0136
trigger times: 14
Loss after 85518120 batches: 0.0137
trigger times: 15
Loss after 85649220 batches: 0.0135
trigger times: 16
Loss after 85780320 batches: 0.0134
trigger times: 17
Loss after 85911420 batches: 0.0134
trigger times: 18
Loss after 86042520 batches: 0.0135
trigger times: 19
Loss after 86173620 batches: 0.0131
trigger times: 20
Early stopping!
Start to test process.
Loss after 86304720 batches: 0.0133
Time to train on one home:  479.44121956825256
trigger times: 0
Loss after 86435820 batches: 0.4881
trigger times: 1
Loss after 86566920 batches: 0.1671
trigger times: 2
Loss after 86698020 batches: 0.1078
trigger times: 3
Loss after 86829120 batches: 0.0882
trigger times: 4
Loss after 86960220 batches: 0.0765
trigger times: 5
Loss after 87091320 batches: 0.0701
trigger times: 6
Loss after 87222420 batches: 0.0647
trigger times: 7
Loss after 87353520 batches: 0.0597
trigger times: 8
Loss after 87484620 batches: 0.0567
trigger times: 9
Loss after 87615720 batches: 0.0539
trigger times: 10
Loss after 87746820 batches: 0.0508
trigger times: 11
Loss after 87877920 batches: 0.0497
trigger times: 12
Loss after 88009020 batches: 0.0479
trigger times: 13
Loss after 88140120 batches: 0.0464
trigger times: 14
Loss after 88271220 batches: 0.0448
trigger times: 15
Loss after 88402320 batches: 0.0433
trigger times: 16
Loss after 88533420 batches: 0.0422
trigger times: 17
Loss after 88664520 batches: 0.0407
trigger times: 18
Loss after 88795620 batches: 0.0400
trigger times: 19
Loss after 88926720 batches: 0.0393
trigger times: 20
Early stopping!
Start to test process.
Loss after 89057820 batches: 0.0381
Time to train on one home:  161.53324961662292
trigger times: 0
Loss after 89186460 batches: 0.1920
trigger times: 0
Loss after 89315100 batches: 0.0606
trigger times: 0
Loss after 89443740 batches: 0.0415
trigger times: 1
Loss after 89572380 batches: 0.0357
trigger times: 2
Loss after 89701020 batches: 0.0316
trigger times: 0
Loss after 89829660 batches: 0.0282
trigger times: 1
Loss after 89958300 batches: 0.0264
trigger times: 0
Loss after 90086940 batches: 0.0249
trigger times: 1
Loss after 90215580 batches: 0.0236
trigger times: 2
Loss after 90344220 batches: 0.0224
trigger times: 3
Loss after 90472860 batches: 0.0217
trigger times: 4
Loss after 90601500 batches: 0.0212
trigger times: 5
Loss after 90730140 batches: 0.0204
trigger times: 6
Loss after 90858780 batches: 0.0198
trigger times: 7
Loss after 90987420 batches: 0.0193
trigger times: 8
Loss after 91116060 batches: 0.0191
trigger times: 9
Loss after 91244700 batches: 0.0185
trigger times: 10
Loss after 91373340 batches: 0.0179
trigger times: 11
Loss after 91501980 batches: 0.0178
trigger times: 12
Loss after 91630620 batches: 0.0176
trigger times: 13
Loss after 91759260 batches: 0.0169
trigger times: 14
Loss after 91887900 batches: 0.0168
trigger times: 15
Loss after 92016540 batches: 0.0164
trigger times: 16
Loss after 92145180 batches: 0.0164
trigger times: 17
Loss after 92273820 batches: 0.0163
trigger times: 18
Loss after 92402460 batches: 0.0156
trigger times: 19
Loss after 92531100 batches: 0.0160
trigger times: 20
Early stopping!
Start to test process.
Loss after 92659740 batches: 0.0157
Time to train on one home:  210.70766162872314
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896]]
Round_3_results:  [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 729 < 730; dropping {'Training_Loss': 0.11364064260192637, 'Validation_Loss': 0.2906811618142658, 'Training_R2': 0.8857211130459629, 'Validation_R2': 0.7294337177508727, 'Training_F1': 0.8181985269745544, 'Validation_F1': 0.7204731724710183, 'Training_NEP': 0.3641397169629816, 'Validation_NEP': 0.5565878530256699, 'Training_NDE': 0.08579118888978288, 'Validation_NDE': 0.21546472452256246, 'Training_MAE': 12.059585258882525, 'Validation_MAE': 15.263948071515689, 'Training_MSE': 377.4678, 'Validation_MSE': 795.70526}.
trigger times: 0
Loss after 92790840 batches: 0.1136
trigger times: 0
Loss after 92921940 batches: 0.0367
trigger times: 1
Loss after 93053040 batches: 0.0253
trigger times: 0
Loss after 93184140 batches: 0.0220
trigger times: 0
Loss after 93315240 batches: 0.0196
trigger times: 0
Loss after 93446340 batches: 0.0178
trigger times: 0
Loss after 93577440 batches: 0.0176
trigger times: 0
Loss after 93708540 batches: 0.0164
trigger times: 1
Loss after 93839640 batches: 0.0159
trigger times: 2
Loss after 93970740 batches: 0.0154
trigger times: 3
Loss after 94101840 batches: 0.0144
trigger times: 4
Loss after 94232940 batches: 0.0141
trigger times: 5
Loss after 94364040 batches: 0.0137
trigger times: 6
Loss after 94495140 batches: 0.0133
trigger times: 7
Loss after 94626240 batches: 0.0131
trigger times: 8
Loss after 94757340 batches: 0.0127
trigger times: 9
Loss after 94888440 batches: 0.0129
trigger times: 10
Loss after 95019540 batches: 0.0126
trigger times: 11
Loss after 95150640 batches: 0.0123
trigger times: 12
Loss after 95281740 batches: 0.0120
trigger times: 13
Loss after 95412840 batches: 0.0119
trigger times: 14
Loss after 95543940 batches: 0.0119
trigger times: 15
Loss after 95675040 batches: 0.0114
trigger times: 16
Loss after 95806140 batches: 0.0116
trigger times: 17
Loss after 95937240 batches: 0.0112
trigger times: 18
Loss after 96068340 batches: 0.0111
trigger times: 19
Loss after 96199440 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 96330540 batches: 0.0112
Time to train on one home:  211.27889943122864
trigger times: 0
Loss after 96433140 batches: 0.3595
trigger times: 1
Loss after 96535740 batches: 0.1329
trigger times: 2
Loss after 96638340 batches: 0.0888
trigger times: 3
Loss after 96740940 batches: 0.0714
trigger times: 4
Loss after 96843540 batches: 0.0627
trigger times: 5
Loss after 96946140 batches: 0.0535
trigger times: 6
Loss after 97048740 batches: 0.0498
trigger times: 7
Loss after 97151340 batches: 0.0460
trigger times: 8
Loss after 97253940 batches: 0.0460
trigger times: 9
Loss after 97356540 batches: 0.0415
trigger times: 10
Loss after 97459140 batches: 0.0377
trigger times: 11
Loss after 97561740 batches: 0.0382
trigger times: 12
Loss after 97664340 batches: 0.0366
trigger times: 13
Loss after 97766940 batches: 0.0366
trigger times: 14
Loss after 97869540 batches: 0.0349
trigger times: 15
Loss after 97972140 batches: 0.0370
trigger times: 16
Loss after 98074740 batches: 0.0321
trigger times: 17
Loss after 98177340 batches: 0.0321
trigger times: 18
Loss after 98279940 batches: 0.0326
trigger times: 19
Loss after 98382540 batches: 0.0314
trigger times: 20
Early stopping!
Start to test process.
Loss after 98485140 batches: 0.0293
Time to train on one home:  134.05495953559875
trigger times: 0
Loss after 98616240 batches: 0.1238
trigger times: 0
Loss after 98747340 batches: 0.0449
trigger times: 0
Loss after 98878440 batches: 0.0315
trigger times: 1
Loss after 99009540 batches: 0.0275
trigger times: 2
Loss after 99140640 batches: 0.0251
trigger times: 0
Loss after 99271740 batches: 0.0233
trigger times: 0
Loss after 99402840 batches: 0.0222
trigger times: 0
Loss after 99533940 batches: 0.0211
trigger times: 1
Loss after 99665040 batches: 0.0204
trigger times: 2
Loss after 99796140 batches: 0.0198
trigger times: 3
Loss after 99927240 batches: 0.0191
trigger times: 4
Loss after 100058340 batches: 0.0186
trigger times: 5
Loss after 100189440 batches: 0.0182
trigger times: 6
Loss after 100320540 batches: 0.0178
trigger times: 7
Loss after 100451640 batches: 0.0174
trigger times: 0
Loss after 100582740 batches: 0.0172
trigger times: 1
Loss after 100713840 batches: 0.0169
trigger times: 2
Loss after 100844940 batches: 0.0166
trigger times: 3
Loss after 100976040 batches: 0.0163
trigger times: 4
Loss after 101107140 batches: 0.0162
trigger times: 5
Loss after 101238240 batches: 0.0161
trigger times: 6
Loss after 101369340 batches: 0.0158
trigger times: 7
Loss after 101500440 batches: 0.0157
trigger times: 8
Loss after 101631540 batches: 0.0154
trigger times: 9
Loss after 101762640 batches: 0.0152
trigger times: 10
Loss after 101893740 batches: 0.0151
trigger times: 11
Loss after 102024840 batches: 0.0148
trigger times: 12
Loss after 102155940 batches: 0.0150
trigger times: 13
Loss after 102287040 batches: 0.0149
trigger times: 14
Loss after 102418140 batches: 0.0147
trigger times: 15
Loss after 102549240 batches: 0.0145
trigger times: 16
Loss after 102680340 batches: 0.0146
trigger times: 17
Loss after 102811440 batches: 0.0143
trigger times: 18
Loss after 102942540 batches: 0.0142
trigger times: 19
Loss after 103073640 batches: 0.0141
trigger times: 20
Early stopping!
Start to test process.
Loss after 103204740 batches: 0.0137
Time to train on one home:  267.9897663593292
trigger times: 0
Loss after 103335840 batches: 0.3809
trigger times: 0
Loss after 103466940 batches: 0.1198
trigger times: 1
Loss after 103598040 batches: 0.0822
trigger times: 2
Loss after 103729140 batches: 0.0691
trigger times: 3
Loss after 103860240 batches: 0.0624
trigger times: 4
Loss after 103991340 batches: 0.0571
trigger times: 5
Loss after 104122440 batches: 0.0525
trigger times: 0
Loss after 104253540 batches: 0.0492
trigger times: 1
Loss after 104384640 batches: 0.0465
trigger times: 2
Loss after 104515740 batches: 0.0446
trigger times: 3
Loss after 104646840 batches: 0.0434
trigger times: 0
Loss after 104777940 batches: 0.0421
trigger times: 1
Loss after 104909040 batches: 0.0403
trigger times: 2
Loss after 105040140 batches: 0.0394
trigger times: 3
Loss after 105171240 batches: 0.0380
trigger times: 4
Loss after 105302340 batches: 0.0370
trigger times: 0
Loss after 105433440 batches: 0.0365
trigger times: 1
Loss after 105564540 batches: 0.0353
trigger times: 2
Loss after 105695640 batches: 0.0350
trigger times: 0
Loss after 105826740 batches: 0.0339
trigger times: 1
Loss after 105957840 batches: 0.0335
trigger times: 2
Loss after 106088940 batches: 0.0330
trigger times: 3
Loss after 106220040 batches: 0.0320
trigger times: 4
Loss after 106351140 batches: 0.0318
trigger times: 0
Loss after 106482240 batches: 0.0311
trigger times: 0
Loss after 106613340 batches: 0.0306
trigger times: 1
Loss after 106744440 batches: 0.0302
trigger times: 2
Loss after 106875540 batches: 0.0299
trigger times: 3
Loss after 107006640 batches: 0.0298
trigger times: 4
Loss after 107137740 batches: 0.0295
trigger times: 5
Loss after 107268840 batches: 0.0291
trigger times: 6
Loss after 107399940 batches: 0.0284
trigger times: 7
Loss after 107531040 batches: 0.0282
trigger times: 0
Loss after 107662140 batches: 0.0285
trigger times: 0
Loss after 107793240 batches: 0.0280
trigger times: 1
Loss after 107924340 batches: 0.0270
trigger times: 2
Loss after 108055440 batches: 0.0269
trigger times: 3
Loss after 108186540 batches: 0.0267
trigger times: 4
Loss after 108317640 batches: 0.0263
trigger times: 5
Loss after 108448740 batches: 0.0266
trigger times: 6
Loss after 108579840 batches: 0.0262
trigger times: 7
Loss after 108710940 batches: 0.0259
trigger times: 0
Loss after 108842040 batches: 0.0263
trigger times: 0
Loss after 108973140 batches: 0.0260
trigger times: 1
Loss after 109104240 batches: 0.0256
trigger times: 2
Loss after 109235340 batches: 0.0251
trigger times: 3
Loss after 109366440 batches: 0.0250
trigger times: 4
Loss after 109497540 batches: 0.0246
trigger times: 5
Loss after 109628640 batches: 0.0249
trigger times: 0
Loss after 109759740 batches: 0.0246
trigger times: 1
Loss after 109890840 batches: 0.0244
trigger times: 0
Loss after 110021940 batches: 0.0241
trigger times: 1
Loss after 110153040 batches: 0.0240
trigger times: 2
Loss after 110284140 batches: 0.0240
trigger times: 3
Loss after 110415240 batches: 0.0234
trigger times: 0
Loss after 110546340 batches: 0.0232
trigger times: 1
Loss after 110677440 batches: 0.0233
trigger times: 2
Loss after 110808540 batches: 0.0234
trigger times: 0
Loss after 110939640 batches: 0.0234
trigger times: 1
Loss after 111070740 batches: 0.0232
trigger times: 0
Loss after 111201840 batches: 0.0231
trigger times: 0
Loss after 111332940 batches: 0.0227
trigger times: 1
Loss after 111464040 batches: 0.0225
trigger times: 2
Loss after 111595140 batches: 0.0225
trigger times: 3
Loss after 111726240 batches: 0.0224
trigger times: 4
Loss after 111857340 batches: 0.0220
trigger times: 5
Loss after 111988440 batches: 0.0220
trigger times: 6
Loss after 112119540 batches: 0.0216
trigger times: 7
Loss after 112250640 batches: 0.0218
trigger times: 8
Loss after 112381740 batches: 0.0221
trigger times: 9
Loss after 112512840 batches: 0.0216
trigger times: 10
Loss after 112643940 batches: 0.0217
trigger times: 11
Loss after 112775040 batches: 0.0213
trigger times: 12
Loss after 112906140 batches: 0.0214
trigger times: 13
Loss after 113037240 batches: 0.0209
trigger times: 14
Loss after 113168340 batches: 0.0209
trigger times: 15
Loss after 113299440 batches: 0.0210
trigger times: 0
Loss after 113430540 batches: 0.0210
trigger times: 0
Loss after 113561640 batches: 0.0207
trigger times: 1
Loss after 113692740 batches: 0.0208
trigger times: 2
Loss after 113823840 batches: 0.0208
trigger times: 3
Loss after 113954940 batches: 0.0208
trigger times: 4
Loss after 114086040 batches: 0.0204
trigger times: 0
Loss after 114217140 batches: 0.0206
trigger times: 1
Loss after 114348240 batches: 0.0206
trigger times: 2
Loss after 114479340 batches: 0.0204
trigger times: 0
Loss after 114610440 batches: 0.0199
trigger times: 1
Loss after 114741540 batches: 0.0199
trigger times: 2
Loss after 114872640 batches: 0.0202
trigger times: 3
Loss after 115003740 batches: 0.0206
trigger times: 0
Loss after 115134840 batches: 0.0202
trigger times: 1
Loss after 115265940 batches: 0.0199
trigger times: 2
Loss after 115397040 batches: 0.0200
trigger times: 3
Loss after 115528140 batches: 0.0199
trigger times: 0
Loss after 115659240 batches: 0.0198
trigger times: 1
Loss after 115790340 batches: 0.0200
trigger times: 2
Loss after 115921440 batches: 0.0196
trigger times: 3
Loss after 116052540 batches: 0.0196
trigger times: 4
Loss after 116183640 batches: 0.0193
trigger times: 5
Loss after 116314740 batches: 0.0196
trigger times: 6
Loss after 116445840 batches: 0.0198
trigger times: 7
Loss after 116576940 batches: 0.0194
trigger times: 8
Loss after 116708040 batches: 0.0192
trigger times: 9
Loss after 116839140 batches: 0.0189
trigger times: 10
Loss after 116970240 batches: 0.0187
trigger times: 11
Loss after 117101340 batches: 0.0186
trigger times: 12
Loss after 117232440 batches: 0.0189
trigger times: 13
Loss after 117363540 batches: 0.0190
trigger times: 14
Loss after 117494640 batches: 0.0189
trigger times: 15
Loss after 117625740 batches: 0.0190
trigger times: 16
Loss after 117756840 batches: 0.0190
trigger times: 17
Loss after 117887940 batches: 0.0187
trigger times: 18
Loss after 118019040 batches: 0.0188
trigger times: 19
Loss after 118150140 batches: 0.0188
trigger times: 20
Early stopping!
Start to test process.
Loss after 118281240 batches: 0.0190
Time to train on one home:  836.3399982452393
trigger times: 0
Loss after 118409880 batches: 0.1413
trigger times: 1
Loss after 118538520 batches: 0.0445
trigger times: 0
Loss after 118667160 batches: 0.0327
trigger times: 0
Loss after 118795800 batches: 0.0280
trigger times: 1
Loss after 118924440 batches: 0.0252
trigger times: 2
Loss after 119053080 batches: 0.0232
trigger times: 0
Loss after 119181720 batches: 0.0222
trigger times: 1
Loss after 119310360 batches: 0.0211
trigger times: 2
Loss after 119439000 batches: 0.0201
trigger times: 3
Loss after 119567640 batches: 0.0194
trigger times: 4
Loss after 119696280 batches: 0.0181
trigger times: 5
Loss after 119824920 batches: 0.0182
trigger times: 0
Loss after 119953560 batches: 0.0170
trigger times: 1
Loss after 120082200 batches: 0.0168
trigger times: 2
Loss after 120210840 batches: 0.0169
trigger times: 3
Loss after 120339480 batches: 0.0163
trigger times: 4
Loss after 120468120 batches: 0.0158
trigger times: 5
Loss after 120596760 batches: 0.0159
trigger times: 6
Loss after 120725400 batches: 0.0152
trigger times: 7
Loss after 120854040 batches: 0.0153
trigger times: 8
Loss after 120982680 batches: 0.0152
trigger times: 9
Loss after 121111320 batches: 0.0145
trigger times: 10
Loss after 121239960 batches: 0.0146
trigger times: 11
Loss after 121368600 batches: 0.0145
trigger times: 12
Loss after 121497240 batches: 0.0143
trigger times: 13
Loss after 121625880 batches: 0.0139
trigger times: 14
Loss after 121754520 batches: 0.0145
trigger times: 15
Loss after 121883160 batches: 0.0138
trigger times: 16
Loss after 122011800 batches: 0.0137
trigger times: 17
Loss after 122140440 batches: 0.0135
trigger times: 18
Loss after 122269080 batches: 0.0138
trigger times: 19
Loss after 122397720 batches: 0.0135
trigger times: 20
Early stopping!
Start to test process.
Loss after 122526360 batches: 0.0130
Time to train on one home:  242.23693919181824
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264]]
Round_4_results:  [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264]
trigger times: 0
Loss after 122657460 batches: 0.1093
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 962 < 963; dropping {'Training_Loss': 0.10930705604688176, 'Validation_Loss': 0.2391816129287084, 'Training_R2': 0.8899186057514615, 'Validation_R2': 0.7771499303670228, 'Training_F1': 0.8210436159280127, 'Validation_F1': 0.6978677980974582, 'Training_NEP': 0.35860208443426206, 'Validation_NEP': 0.5621526622818533, 'Training_NDE': 0.08264005661015383, 'Validation_NDE': 0.17746604811271952, 'Training_MAE': 11.876189851840882, 'Validation_MAE': 15.416558228299609, 'Training_MSE': 363.6033, 'Validation_MSE': 655.37726}.
trigger times: 0
Loss after 122788560 batches: 0.0302
trigger times: 0
Loss after 122919660 batches: 0.0222
trigger times: 1
Loss after 123050760 batches: 0.0195
trigger times: 0
Loss after 123181860 batches: 0.0175
trigger times: 1
Loss after 123312960 batches: 0.0164
trigger times: 2
Loss after 123444060 batches: 0.0155
trigger times: 0
Loss after 123575160 batches: 0.0151
trigger times: 0
Loss after 123706260 batches: 0.0142
trigger times: 1
Loss after 123837360 batches: 0.0142
trigger times: 2
Loss after 123968460 batches: 0.0138
trigger times: 3
Loss after 124099560 batches: 0.0132
trigger times: 4
Loss after 124230660 batches: 0.0130
trigger times: 0
Loss after 124361760 batches: 0.0126
trigger times: 1
Loss after 124492860 batches: 0.0124
trigger times: 0
Loss after 124623960 batches: 0.0124
trigger times: 1
Loss after 124755060 batches: 0.0123
trigger times: 2
Loss after 124886160 batches: 0.0119
trigger times: 3
Loss after 125017260 batches: 0.0115
trigger times: 0
Loss after 125148360 batches: 0.0117
trigger times: 1
Loss after 125279460 batches: 0.0115
trigger times: 2
Loss after 125410560 batches: 0.0115
trigger times: 3
Loss after 125541660 batches: 0.0112
trigger times: 4
Loss after 125672760 batches: 0.0111
trigger times: 5
Loss after 125803860 batches: 0.0108
trigger times: 6
Loss after 125934960 batches: 0.0109
trigger times: 7
Loss after 126066060 batches: 0.0108
trigger times: 8
Loss after 126197160 batches: 0.0105
trigger times: 9
Loss after 126328260 batches: 0.0105
trigger times: 10
Loss after 126459360 batches: 0.0103
trigger times: 11
Loss after 126590460 batches: 0.0103
trigger times: 12
Loss after 126721560 batches: 0.0101
trigger times: 13
Loss after 126852660 batches: 0.0100
trigger times: 14
Loss after 126983760 batches: 0.0099
trigger times: 15
Loss after 127114860 batches: 0.0097
trigger times: 16
Loss after 127245960 batches: 0.0095
trigger times: 17
Loss after 127377060 batches: 0.0096
trigger times: 18
Loss after 127508160 batches: 0.0096
trigger times: 19
Loss after 127639260 batches: 0.0096
trigger times: 20
Early stopping!
Start to test process.
Loss after 127770360 batches: 0.0098
Time to train on one home:  296.2987856864929
trigger times: 0
Loss after 127872960 batches: 0.3340
trigger times: 1
Loss after 127975560 batches: 0.1139
trigger times: 2
Loss after 128078160 batches: 0.0739
trigger times: 3
Loss after 128180760 batches: 0.0614
trigger times: 4
Loss after 128283360 batches: 0.0525
trigger times: 5
Loss after 128385960 batches: 0.0457
trigger times: 6
Loss after 128488560 batches: 0.0422
trigger times: 7
Loss after 128591160 batches: 0.0388
trigger times: 8
Loss after 128693760 batches: 0.0368
trigger times: 9
Loss after 128796360 batches: 0.0376
trigger times: 10
Loss after 128898960 batches: 0.0375
trigger times: 11
Loss after 129001560 batches: 0.0498
trigger times: 12
Loss after 129104160 batches: 0.0379
trigger times: 13
Loss after 129206760 batches: 0.0307
trigger times: 14
Loss after 129309360 batches: 0.0287
trigger times: 15
Loss after 129411960 batches: 0.0281
trigger times: 16
Loss after 129514560 batches: 0.0276
trigger times: 17
Loss after 129617160 batches: 0.0291
trigger times: 18
Loss after 129719760 batches: 0.0270
trigger times: 19
Loss after 129822360 batches: 0.0271
trigger times: 20
Early stopping!
Start to test process.
Loss after 129924960 batches: 0.0259
Time to train on one home:  135.364319562912
trigger times: 0
Loss after 130056060 batches: 0.1151
trigger times: 0
Loss after 130187160 batches: 0.0386
trigger times: 1
Loss after 130318260 batches: 0.0286
trigger times: 0
Loss after 130449360 batches: 0.0245
trigger times: 1
Loss after 130580460 batches: 0.0228
trigger times: 0
Loss after 130711560 batches: 0.0214
trigger times: 1
Loss after 130842660 batches: 0.0203
trigger times: 2
Loss after 130973760 batches: 0.0195
trigger times: 3
Loss after 131104860 batches: 0.0188
trigger times: 0
Loss after 131235960 batches: 0.0180
trigger times: 1
Loss after 131367060 batches: 0.0177
trigger times: 2
Loss after 131498160 batches: 0.0173
trigger times: 3
Loss after 131629260 batches: 0.0170
trigger times: 4
Loss after 131760360 batches: 0.0165
trigger times: 5
Loss after 131891460 batches: 0.0161
trigger times: 6
Loss after 132022560 batches: 0.0162
trigger times: 7
Loss after 132153660 batches: 0.0157
trigger times: 8
Loss after 132284760 batches: 0.0155
trigger times: 9
Loss after 132415860 batches: 0.0154
trigger times: 10
Loss after 132546960 batches: 0.0153
trigger times: 11
Loss after 132678060 batches: 0.0151
trigger times: 12
Loss after 132809160 batches: 0.0148
trigger times: 13
Loss after 132940260 batches: 0.0148
trigger times: 14
Loss after 133071360 batches: 0.0148
trigger times: 15
Loss after 133202460 batches: 0.0145
trigger times: 16
Loss after 133333560 batches: 0.0146
trigger times: 17
Loss after 133464660 batches: 0.0142
trigger times: 18
Loss after 133595760 batches: 0.0142
trigger times: 19
Loss after 133726860 batches: 0.0141
trigger times: 20
Early stopping!
Start to test process.
Loss after 133857960 batches: 0.0142
Time to train on one home:  233.1346037387848
trigger times: 0
Loss after 133989060 batches: 0.2655
trigger times: 1
Loss after 134120160 batches: 0.0816
trigger times: 0
Loss after 134251260 batches: 0.0568
trigger times: 0
Loss after 134382360 batches: 0.0466
trigger times: 1
Loss after 134513460 batches: 0.0425
trigger times: 0
Loss after 134644560 batches: 0.0393
trigger times: 1
Loss after 134775660 batches: 0.0361
trigger times: 2
Loss after 134906760 batches: 0.0345
trigger times: 0
Loss after 135037860 batches: 0.0333
trigger times: 1
Loss after 135168960 batches: 0.0318
trigger times: 2
Loss after 135300060 batches: 0.0302
trigger times: 3
Loss after 135431160 batches: 0.0299
trigger times: 4
Loss after 135562260 batches: 0.0291
trigger times: 5
Loss after 135693360 batches: 0.0284
trigger times: 6
Loss after 135824460 batches: 0.0275
trigger times: 7
Loss after 135955560 batches: 0.0272
trigger times: 0
Loss after 136086660 batches: 0.0268
trigger times: 1
Loss after 136217760 batches: 0.0262
trigger times: 2
Loss after 136348860 batches: 0.0254
trigger times: 3
Loss after 136479960 batches: 0.0254
trigger times: 4
Loss after 136611060 batches: 0.0251
trigger times: 5
Loss after 136742160 batches: 0.0249
trigger times: 0
Loss after 136873260 batches: 0.0246
trigger times: 1
Loss after 137004360 batches: 0.0238
trigger times: 2
Loss after 137135460 batches: 0.0238
trigger times: 3
Loss after 137266560 batches: 0.0235
trigger times: 4
Loss after 137397660 batches: 0.0233
trigger times: 5
Loss after 137528760 batches: 0.0231
trigger times: 6
Loss after 137659860 batches: 0.0231
trigger times: 7
Loss after 137790960 batches: 0.0227
trigger times: 8
Loss after 137922060 batches: 0.0226
trigger times: 9
Loss after 138053160 batches: 0.0224
trigger times: 0
Loss after 138184260 batches: 0.0222
trigger times: 0
Loss after 138315360 batches: 0.0221
trigger times: 1
Loss after 138446460 batches: 0.0220
trigger times: 2
Loss after 138577560 batches: 0.0218
trigger times: 3
Loss after 138708660 batches: 0.0215
trigger times: 4
Loss after 138839760 batches: 0.0215
trigger times: 5
Loss after 138970860 batches: 0.0213
trigger times: 6
Loss after 139101960 batches: 0.0212
trigger times: 7
Loss after 139233060 batches: 0.0210
trigger times: 0
Loss after 139364160 batches: 0.0211
trigger times: 1
Loss after 139495260 batches: 0.0209
trigger times: 2
Loss after 139626360 batches: 0.0208
trigger times: 3
Loss after 139757460 batches: 0.0203
trigger times: 0
Loss after 139888560 batches: 0.0203
trigger times: 1
Loss after 140019660 batches: 0.0201
trigger times: 2
Loss after 140150760 batches: 0.0202
trigger times: 3
Loss after 140281860 batches: 0.0198
trigger times: 4
Loss after 140412960 batches: 0.0200
trigger times: 5
Loss after 140544060 batches: 0.0196
trigger times: 0
Loss after 140675160 batches: 0.0198
trigger times: 1
Loss after 140806260 batches: 0.0196
trigger times: 2
Loss after 140937360 batches: 0.0193
trigger times: 3
Loss after 141068460 batches: 0.0193
trigger times: 4
Loss after 141199560 batches: 0.0192
trigger times: 5
Loss after 141330660 batches: 0.0191
trigger times: 6
Loss after 141461760 batches: 0.0189
trigger times: 0
Loss after 141592860 batches: 0.0189
trigger times: 0
Loss after 141723960 batches: 0.0187
trigger times: 1
Loss after 141855060 batches: 0.0189
trigger times: 2
Loss after 141986160 batches: 0.0189
trigger times: 3
Loss after 142117260 batches: 0.0188
trigger times: 0
Loss after 142248360 batches: 0.0188
trigger times: 1
Loss after 142379460 batches: 0.0185
trigger times: 2
Loss after 142510560 batches: 0.0184
trigger times: 3
Loss after 142641660 batches: 0.0181
trigger times: 4
Loss after 142772760 batches: 0.0186
trigger times: 5
Loss after 142903860 batches: 0.0184
trigger times: 6
Loss after 143034960 batches: 0.0181
trigger times: 0
Loss after 143166060 batches: 0.0181
trigger times: 1
Loss after 143297160 batches: 0.0178
trigger times: 2
Loss after 143428260 batches: 0.0178
trigger times: 3
Loss after 143559360 batches: 0.0177
trigger times: 4
Loss after 143690460 batches: 0.0178
trigger times: 5
Loss after 143821560 batches: 0.0177
trigger times: 6
Loss after 143952660 batches: 0.0177
trigger times: 7
Loss after 144083760 batches: 0.0176
trigger times: 0
Loss after 144214860 batches: 0.0175
trigger times: 1
Loss after 144345960 batches: 0.0175
trigger times: 2
Loss after 144477060 batches: 0.0176
trigger times: 3
Loss after 144608160 batches: 0.0177
trigger times: 0
Loss after 144739260 batches: 0.0174
trigger times: 1
Loss after 144870360 batches: 0.0176
trigger times: 2
Loss after 145001460 batches: 0.0170
trigger times: 3
Loss after 145132560 batches: 0.0173
trigger times: 4
Loss after 145263660 batches: 0.0174
trigger times: 5
Loss after 145394760 batches: 0.0171
trigger times: 6
Loss after 145525860 batches: 0.0174
trigger times: 7
Loss after 145656960 batches: 0.0172
trigger times: 8
Loss after 145788060 batches: 0.0172
trigger times: 9
Loss after 145919160 batches: 0.0173
trigger times: 10
Loss after 146050260 batches: 0.0168
trigger times: 11
Loss after 146181360 batches: 0.0169
trigger times: 12
Loss after 146312460 batches: 0.0172
trigger times: 13
Loss after 146443560 batches: 0.0170
trigger times: 14
Loss after 146574660 batches: 0.0168
trigger times: 15
Loss after 146705760 batches: 0.0169
trigger times: 16
Loss after 146836860 batches: 0.0168
trigger times: 17
Loss after 146967960 batches: 0.0166
trigger times: 18
Loss after 147099060 batches: 0.0165
trigger times: 19
Loss after 147230160 batches: 0.0167
trigger times: 20
Early stopping!
Start to test process.
Loss after 147361260 batches: 0.0163
Time to train on one home:  767.3367028236389
trigger times: 0
Loss after 147489900 batches: 0.1431
trigger times: 0
Loss after 147618540 batches: 0.0391
trigger times: 1
Loss after 147747180 batches: 0.0282
trigger times: 2
Loss after 147875820 batches: 0.0238
trigger times: 0
Loss after 148004460 batches: 0.0214
trigger times: 0
Loss after 148133100 batches: 0.0202
trigger times: 0
Loss after 148261740 batches: 0.0200
trigger times: 1
Loss after 148390380 batches: 0.0187
trigger times: 2
Loss after 148519020 batches: 0.0176
trigger times: 3
Loss after 148647660 batches: 0.0170
trigger times: 4
Loss after 148776300 batches: 0.0166
trigger times: 5
Loss after 148904940 batches: 0.0162
trigger times: 6
Loss after 149033580 batches: 0.0159
trigger times: 7
Loss after 149162220 batches: 0.0156
trigger times: 8
Loss after 149290860 batches: 0.0150
trigger times: 9
Loss after 149419500 batches: 0.0148
trigger times: 10
Loss after 149548140 batches: 0.0145
trigger times: 11
Loss after 149676780 batches: 0.0143
trigger times: 12
Loss after 149805420 batches: 0.0143
trigger times: 13
Loss after 149934060 batches: 0.0138
trigger times: 14
Loss after 150062700 batches: 0.0138
trigger times: 15
Loss after 150191340 batches: 0.0138
trigger times: 16
Loss after 150319980 batches: 0.0136
trigger times: 17
Loss after 150448620 batches: 0.0135
trigger times: 18
Loss after 150577260 batches: 0.0130
trigger times: 19
Loss after 150705900 batches: 0.0131
trigger times: 20
Early stopping!
Start to test process.
Loss after 150834540 batches: 0.0127
Time to train on one home:  201.90033221244812
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141]]
Round_5_results:  [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1183 < 1184; dropping {'Training_Loss': 0.09307873579409887, 'Validation_Loss': 0.23825218776861826, 'Training_R2': 0.9063153755614672, 'Validation_R2': 0.7781497982167744, 'Training_F1': 0.8319470677357109, 'Validation_F1': 0.7169473234153073, 'Training_NEP': 0.33635621072691424, 'Validation_NEP': 0.5486493556033767, 'Training_NDE': 0.07033071047066769, 'Validation_NDE': 0.17666980606432064, 'Training_MAE': 11.13945063297845, 'Validation_MAE': 15.046241537388081, 'Training_MSE': 309.4441, 'Validation_MSE': 652.43677}.
trigger times: 0
Loss after 150965640 batches: 0.0931
trigger times: 0
Loss after 151096740 batches: 0.0281
trigger times: 1
Loss after 151227840 batches: 0.0206
trigger times: 2
Loss after 151358940 batches: 0.0179
trigger times: 3
Loss after 151490040 batches: 0.0162
trigger times: 4
Loss after 151621140 batches: 0.0153
trigger times: 5
Loss after 151752240 batches: 0.0145
trigger times: 0
Loss after 151883340 batches: 0.0141
trigger times: 1
Loss after 152014440 batches: 0.0134
trigger times: 2
Loss after 152145540 batches: 0.0129
trigger times: 3
Loss after 152276640 batches: 0.0125
trigger times: 4
Loss after 152407740 batches: 0.0123
trigger times: 5
Loss after 152538840 batches: 0.0128
trigger times: 6
Loss after 152669940 batches: 0.0121
trigger times: 7
Loss after 152801040 batches: 0.0119
trigger times: 8
Loss after 152932140 batches: 0.0117
trigger times: 0
Loss after 153063240 batches: 0.0113
trigger times: 1
Loss after 153194340 batches: 0.0114
trigger times: 2
Loss after 153325440 batches: 0.0111
trigger times: 3
Loss after 153456540 batches: 0.0109
trigger times: 4
Loss after 153587640 batches: 0.0107
trigger times: 0
Loss after 153718740 batches: 0.0106
trigger times: 1
Loss after 153849840 batches: 0.0107
trigger times: 2
Loss after 153980940 batches: 0.0104
trigger times: 3
Loss after 154112040 batches: 0.0104
trigger times: 4
Loss after 154243140 batches: 0.0103
trigger times: 5
Loss after 154374240 batches: 0.0101
trigger times: 6
Loss after 154505340 batches: 0.0100
trigger times: 7
Loss after 154636440 batches: 0.0097
trigger times: 8
Loss after 154767540 batches: 0.0098
trigger times: 9
Loss after 154898640 batches: 0.0096
trigger times: 10
Loss after 155029740 batches: 0.0099
trigger times: 11
Loss after 155160840 batches: 0.0096
trigger times: 12
Loss after 155291940 batches: 0.0094
trigger times: 13
Loss after 155423040 batches: 0.0095
trigger times: 14
Loss after 155554140 batches: 0.0094
trigger times: 15
Loss after 155685240 batches: 0.0092
trigger times: 16
Loss after 155816340 batches: 0.0091
trigger times: 17
Loss after 155947440 batches: 0.0092
trigger times: 18
Loss after 156078540 batches: 0.0090
trigger times: 19
Loss after 156209640 batches: 0.0092
trigger times: 20
Early stopping!
Start to test process.
Loss after 156340740 batches: 0.0092
Time to train on one home:  311.41043043136597
trigger times: 0
Loss after 156443340 batches: 0.2999
trigger times: 1
Loss after 156545940 batches: 0.0964
trigger times: 0
Loss after 156648540 batches: 0.0702
trigger times: 1
Loss after 156751140 batches: 0.0534
trigger times: 2
Loss after 156853740 batches: 0.0458
trigger times: 3
Loss after 156956340 batches: 0.0405
trigger times: 4
Loss after 157058940 batches: 0.0365
trigger times: 5
Loss after 157161540 batches: 0.0375
trigger times: 6
Loss after 157264140 batches: 0.0378
trigger times: 7
Loss after 157366740 batches: 0.0340
trigger times: 8
Loss after 157469340 batches: 0.0309
trigger times: 9
Loss after 157571940 batches: 0.0292
trigger times: 10
Loss after 157674540 batches: 0.0304
trigger times: 11
Loss after 157777140 batches: 0.0289
trigger times: 12
Loss after 157879740 batches: 0.0305
trigger times: 13
Loss after 157982340 batches: 0.0281
trigger times: 14
Loss after 158084940 batches: 0.0265
trigger times: 15
Loss after 158187540 batches: 0.0256
trigger times: 16
Loss after 158290140 batches: 0.0278
trigger times: 17
Loss after 158392740 batches: 0.0256
trigger times: 18
Loss after 158495340 batches: 0.0246
trigger times: 19
Loss after 158597940 batches: 0.0243
trigger times: 20
Early stopping!
Start to test process.
Loss after 158700540 batches: 0.0253
Time to train on one home:  144.617906332016
trigger times: 0
Loss after 158831640 batches: 0.1065
trigger times: 0
Loss after 158962740 batches: 0.0357
trigger times: 0
Loss after 159093840 batches: 0.0267
trigger times: 1
Loss after 159224940 batches: 0.0236
trigger times: 2
Loss after 159356040 batches: 0.0216
trigger times: 3
Loss after 159487140 batches: 0.0203
trigger times: 4
Loss after 159618240 batches: 0.0192
trigger times: 5
Loss after 159749340 batches: 0.0187
trigger times: 6
Loss after 159880440 batches: 0.0184
trigger times: 0
Loss after 160011540 batches: 0.0174
trigger times: 1
Loss after 160142640 batches: 0.0172
trigger times: 2
Loss after 160273740 batches: 0.0168
trigger times: 3
Loss after 160404840 batches: 0.0162
trigger times: 4
Loss after 160535940 batches: 0.0160
trigger times: 5
Loss after 160667040 batches: 0.0160
trigger times: 6
Loss after 160798140 batches: 0.0156
trigger times: 7
Loss after 160929240 batches: 0.0152
trigger times: 8
Loss after 161060340 batches: 0.0152
trigger times: 9
Loss after 161191440 batches: 0.0151
trigger times: 10
Loss after 161322540 batches: 0.0146
trigger times: 11
Loss after 161453640 batches: 0.0145
trigger times: 12
Loss after 161584740 batches: 0.0146
trigger times: 13
Loss after 161715840 batches: 0.0147
trigger times: 14
Loss after 161846940 batches: 0.0144
trigger times: 15
Loss after 161978040 batches: 0.0142
trigger times: 16
Loss after 162109140 batches: 0.0140
trigger times: 17
Loss after 162240240 batches: 0.0139
trigger times: 18
Loss after 162371340 batches: 0.0138
trigger times: 0
Loss after 162502440 batches: 0.0135
trigger times: 1
Loss after 162633540 batches: 0.0135
trigger times: 2
Loss after 162764640 batches: 0.0134
trigger times: 3
Loss after 162895740 batches: 0.0133
trigger times: 4
Loss after 163026840 batches: 0.0133
trigger times: 5
Loss after 163157940 batches: 0.0132
trigger times: 6
Loss after 163289040 batches: 0.0131
trigger times: 7
Loss after 163420140 batches: 0.0132
trigger times: 8
Loss after 163551240 batches: 0.0132
trigger times: 9
Loss after 163682340 batches: 0.0126
trigger times: 10
Loss after 163813440 batches: 0.0128
trigger times: 11
Loss after 163944540 batches: 0.0128
trigger times: 12
Loss after 164075640 batches: 0.0130
trigger times: 13
Loss after 164206740 batches: 0.0128
trigger times: 14
Loss after 164337840 batches: 0.0127
trigger times: 15
Loss after 164468940 batches: 0.0124
trigger times: 16
Loss after 164600040 batches: 0.0124
trigger times: 17
Loss after 164731140 batches: 0.0125
trigger times: 0
Loss after 164862240 batches: 0.0125
trigger times: 1
Loss after 164993340 batches: 0.0123
trigger times: 2
Loss after 165124440 batches: 0.0122
trigger times: 3
Loss after 165255540 batches: 0.0121
trigger times: 4
Loss after 165386640 batches: 0.0121
trigger times: 5
Loss after 165517740 batches: 0.0120
trigger times: 6
Loss after 165648840 batches: 0.0121
trigger times: 7
Loss after 165779940 batches: 0.0120
trigger times: 8
Loss after 165911040 batches: 0.0119
trigger times: 9
Loss after 166042140 batches: 0.0120
trigger times: 10
Loss after 166173240 batches: 0.0119
trigger times: 11
Loss after 166304340 batches: 0.0116
trigger times: 12
Loss after 166435440 batches: 0.0117
trigger times: 13
Loss after 166566540 batches: 0.0117
trigger times: 14
Loss after 166697640 batches: 0.0117
trigger times: 15
Loss after 166828740 batches: 0.0116
trigger times: 16
Loss after 166959840 batches: 0.0115
trigger times: 17
Loss after 167090940 batches: 0.0115
trigger times: 18
Loss after 167222040 batches: 0.0115
trigger times: 19
Loss after 167353140 batches: 0.0114
trigger times: 20
Early stopping!
Start to test process.
Loss after 167484240 batches: 0.0113
Time to train on one home:  500.4019105434418
trigger times: 0
Loss after 167615340 batches: 0.2221
trigger times: 0
Loss after 167746440 batches: 0.0633
trigger times: 0
Loss after 167877540 batches: 0.0444
trigger times: 1
Loss after 168008640 batches: 0.0377
trigger times: 2
Loss after 168139740 batches: 0.0340
trigger times: 3
Loss after 168270840 batches: 0.0314
trigger times: 4
Loss after 168401940 batches: 0.0297
trigger times: 5
Loss after 168533040 batches: 0.0288
trigger times: 0
Loss after 168664140 batches: 0.0274
trigger times: 1
Loss after 168795240 batches: 0.0268
trigger times: 0
Loss after 168926340 batches: 0.0256
trigger times: 1
Loss after 169057440 batches: 0.0255
trigger times: 0
Loss after 169188540 batches: 0.0246
trigger times: 1
Loss after 169319640 batches: 0.0245
trigger times: 2
Loss after 169450740 batches: 0.0240
trigger times: 0
Loss after 169581840 batches: 0.0235
trigger times: 1
Loss after 169712940 batches: 0.0230
trigger times: 2
Loss after 169844040 batches: 0.0223
trigger times: 3
Loss after 169975140 batches: 0.0227
trigger times: 4
Loss after 170106240 batches: 0.0223
trigger times: 0
Loss after 170237340 batches: 0.0222
trigger times: 1
Loss after 170368440 batches: 0.0217
trigger times: 2
Loss after 170499540 batches: 0.0214
trigger times: 3
Loss after 170630640 batches: 0.0213
trigger times: 4
Loss after 170761740 batches: 0.0208
trigger times: 5
Loss after 170892840 batches: 0.0207
trigger times: 6
Loss after 171023940 batches: 0.0204
trigger times: 7
Loss after 171155040 batches: 0.0205
trigger times: 0
Loss after 171286140 batches: 0.0204
trigger times: 1
Loss after 171417240 batches: 0.0199
trigger times: 2
Loss after 171548340 batches: 0.0201
trigger times: 0
Loss after 171679440 batches: 0.0201
trigger times: 1
Loss after 171810540 batches: 0.0195
trigger times: 2
Loss after 171941640 batches: 0.0195
trigger times: 3
Loss after 172072740 batches: 0.0196
trigger times: 4
Loss after 172203840 batches: 0.0189
trigger times: 5
Loss after 172334940 batches: 0.0192
trigger times: 6
Loss after 172466040 batches: 0.0193
trigger times: 0
Loss after 172597140 batches: 0.0190
trigger times: 1
Loss after 172728240 batches: 0.0189
trigger times: 2
Loss after 172859340 batches: 0.0191
trigger times: 0
Loss after 172990440 batches: 0.0187
trigger times: 0
Loss after 173121540 batches: 0.0186
trigger times: 1
Loss after 173252640 batches: 0.0183
trigger times: 2
Loss after 173383740 batches: 0.0187
trigger times: 3
Loss after 173514840 batches: 0.0181
trigger times: 4
Loss after 173645940 batches: 0.0183
trigger times: 0
Loss after 173777040 batches: 0.0182
trigger times: 0
Loss after 173908140 batches: 0.0181
trigger times: 1
Loss after 174039240 batches: 0.0181
trigger times: 2
Loss after 174170340 batches: 0.0181
trigger times: 3
Loss after 174301440 batches: 0.0177
trigger times: 4
Loss after 174432540 batches: 0.0178
trigger times: 5
Loss after 174563640 batches: 0.0176
trigger times: 6
Loss after 174694740 batches: 0.0176
trigger times: 0
Loss after 174825840 batches: 0.0175
trigger times: 1
Loss after 174956940 batches: 0.0173
trigger times: 2
Loss after 175088040 batches: 0.0174
trigger times: 3
Loss after 175219140 batches: 0.0171
trigger times: 4
Loss after 175350240 batches: 0.0171
trigger times: 5
Loss after 175481340 batches: 0.0171
trigger times: 0
Loss after 175612440 batches: 0.0172
trigger times: 1
Loss after 175743540 batches: 0.0170
trigger times: 2
Loss after 175874640 batches: 0.0170
trigger times: 3
Loss after 176005740 batches: 0.0169
trigger times: 4
Loss after 176136840 batches: 0.0170
trigger times: 5
Loss after 176267940 batches: 0.0171
trigger times: 6
Loss after 176399040 batches: 0.0168
trigger times: 7
Loss after 176530140 batches: 0.0166
trigger times: 8
Loss after 176661240 batches: 0.0163
trigger times: 9
Loss after 176792340 batches: 0.0166
trigger times: 0
Loss after 176923440 batches: 0.0163
trigger times: 1
Loss after 177054540 batches: 0.0165
trigger times: 2
Loss after 177185640 batches: 0.0166
trigger times: 3
Loss after 177316740 batches: 0.0164
trigger times: 4
Loss after 177447840 batches: 0.0160
trigger times: 5
Loss after 177578940 batches: 0.0163
trigger times: 6
Loss after 177710040 batches: 0.0163
trigger times: 7
Loss after 177841140 batches: 0.0159
trigger times: 8
Loss after 177972240 batches: 0.0162
trigger times: 9
Loss after 178103340 batches: 0.0161
trigger times: 10
Loss after 178234440 batches: 0.0159
trigger times: 11
Loss after 178365540 batches: 0.0161
trigger times: 12
Loss after 178496640 batches: 0.0162
trigger times: 13
Loss after 178627740 batches: 0.0159
trigger times: 14
Loss after 178758840 batches: 0.0155
trigger times: 15
Loss after 178889940 batches: 0.0159
trigger times: 16
Loss after 179021040 batches: 0.0158
trigger times: 17
Loss after 179152140 batches: 0.0159
trigger times: 0
Loss after 179283240 batches: 0.0159
trigger times: 1
Loss after 179414340 batches: 0.0158
trigger times: 2
Loss after 179545440 batches: 0.0156
trigger times: 3
Loss after 179676540 batches: 0.0155
trigger times: 4
Loss after 179807640 batches: 0.0157
trigger times: 5
Loss after 179938740 batches: 0.0155
trigger times: 6
Loss after 180069840 batches: 0.0153
trigger times: 7
Loss after 180200940 batches: 0.0156
trigger times: 8
Loss after 180332040 batches: 0.0158
trigger times: 9
Loss after 180463140 batches: 0.0155
trigger times: 10
Loss after 180594240 batches: 0.0156
trigger times: 11
Loss after 180725340 batches: 0.0153
trigger times: 12
Loss after 180856440 batches: 0.0156
trigger times: 0
Loss after 180987540 batches: 0.0153
trigger times: 1
Loss after 181118640 batches: 0.0154
trigger times: 2
Loss after 181249740 batches: 0.0154
trigger times: 3
Loss after 181380840 batches: 0.0158
trigger times: 4
Loss after 181511940 batches: 0.0153
trigger times: 0
Loss after 181643040 batches: 0.0152
trigger times: 1
Loss after 181774140 batches: 0.0151
trigger times: 2
Loss after 181905240 batches: 0.0151
trigger times: 3
Loss after 182036340 batches: 0.0154
trigger times: 4
Loss after 182167440 batches: 0.0152
trigger times: 5
Loss after 182298540 batches: 0.0152
trigger times: 6
Loss after 182429640 batches: 0.0149
trigger times: 7
Loss after 182560740 batches: 0.0151
trigger times: 8
Loss after 182691840 batches: 0.0151
trigger times: 9
Loss after 182822940 batches: 0.0152
trigger times: 10
Loss after 182954040 batches: 0.0149
trigger times: 11
Loss after 183085140 batches: 0.0148
trigger times: 0
Loss after 183216240 batches: 0.0148
trigger times: 0
Loss after 183347340 batches: 0.0149
trigger times: 1
Loss after 183478440 batches: 0.0146
trigger times: 2
Loss after 183609540 batches: 0.0146
trigger times: 3
Loss after 183740640 batches: 0.0148
trigger times: 4
Loss after 183871740 batches: 0.0145
trigger times: 5
Loss after 184002840 batches: 0.0145
trigger times: 6
Loss after 184133940 batches: 0.0145
trigger times: 7
Loss after 184265040 batches: 0.0145
trigger times: 0
Loss after 184396140 batches: 0.0144
trigger times: 1
Loss after 184527240 batches: 0.0142
trigger times: 2
Loss after 184658340 batches: 0.0144
trigger times: 3
Loss after 184789440 batches: 0.0143
trigger times: 4
Loss after 184920540 batches: 0.0143
trigger times: 0
Loss after 185051640 batches: 0.0146
trigger times: 1
Loss after 185182740 batches: 0.0141
trigger times: 2
Loss after 185313840 batches: 0.0145
trigger times: 3
Loss after 185444940 batches: 0.0143
trigger times: 4
Loss after 185576040 batches: 0.0142
trigger times: 5
Loss after 185707140 batches: 0.0144
trigger times: 6
Loss after 185838240 batches: 0.0141
trigger times: 7
Loss after 185969340 batches: 0.0141
trigger times: 8
Loss after 186100440 batches: 0.0144
trigger times: 9
Loss after 186231540 batches: 0.0141
trigger times: 10
Loss after 186362640 batches: 0.0142
trigger times: 11
Loss after 186493740 batches: 0.0140
trigger times: 12
Loss after 186624840 batches: 0.0140
trigger times: 13
Loss after 186755940 batches: 0.0138
trigger times: 14
Loss after 186887040 batches: 0.0141
trigger times: 15
Loss after 187018140 batches: 0.0139
trigger times: 16
Loss after 187149240 batches: 0.0138
trigger times: 0
Loss after 187280340 batches: 0.0138
trigger times: 1
Loss after 187411440 batches: 0.0138
trigger times: 0
Loss after 187542540 batches: 0.0139
trigger times: 1
Loss after 187673640 batches: 0.0136
trigger times: 2
Loss after 187804740 batches: 0.0138
trigger times: 3
Loss after 187935840 batches: 0.0137
trigger times: 4
Loss after 188066940 batches: 0.0136
trigger times: 5
Loss after 188198040 batches: 0.0135
trigger times: 6
Loss after 188329140 batches: 0.0136
trigger times: 7
Loss after 188460240 batches: 0.0136
trigger times: 8
Loss after 188591340 batches: 0.0135
trigger times: 9
Loss after 188722440 batches: 0.0136
trigger times: 10
Loss after 188853540 batches: 0.0135
trigger times: 11
Loss after 188984640 batches: 0.0135
trigger times: 12
Loss after 189115740 batches: 0.0135
trigger times: 13
Loss after 189246840 batches: 0.0135
trigger times: 14
Loss after 189377940 batches: 0.0135
trigger times: 15
Loss after 189509040 batches: 0.0137
trigger times: 16
Loss after 189640140 batches: 0.0139
trigger times: 17
Loss after 189771240 batches: 0.0136
trigger times: 18
Loss after 189902340 batches: 0.0136
trigger times: 19
Loss after 190033440 batches: 0.0134
trigger times: 20
Early stopping!
Start to test process.
Loss after 190164540 batches: 0.0134
Time to train on one home:  1252.438571214676
trigger times: 0
Loss after 190293180 batches: 0.1133
trigger times: 0
Loss after 190421820 batches: 0.0334
trigger times: 1
Loss after 190550460 batches: 0.0249
trigger times: 2
Loss after 190679100 batches: 0.0221
trigger times: 3
Loss after 190807740 batches: 0.0203
trigger times: 4
Loss after 190936380 batches: 0.0190
trigger times: 5
Loss after 191065020 batches: 0.0178
trigger times: 6
Loss after 191193660 batches: 0.0171
trigger times: 7
Loss after 191322300 batches: 0.0165
trigger times: 8
Loss after 191450940 batches: 0.0162
trigger times: 9
Loss after 191579580 batches: 0.0153
trigger times: 10
Loss after 191708220 batches: 0.0153
trigger times: 0
Loss after 191836860 batches: 0.0150
trigger times: 1
Loss after 191965500 batches: 0.0146
trigger times: 2
Loss after 192094140 batches: 0.0145
trigger times: 3
Loss after 192222780 batches: 0.0139
trigger times: 4
Loss after 192351420 batches: 0.0139
trigger times: 5
Loss after 192480060 batches: 0.0136
trigger times: 6
Loss after 192608700 batches: 0.0135
trigger times: 7
Loss after 192737340 batches: 0.0132
trigger times: 8
Loss after 192865980 batches: 0.0132
trigger times: 9
Loss after 192994620 batches: 0.0128
trigger times: 10
Loss after 193123260 batches: 0.0129
trigger times: 11
Loss after 193251900 batches: 0.0130
trigger times: 12
Loss after 193380540 batches: 0.0127
trigger times: 13
Loss after 193509180 batches: 0.0125
trigger times: 14
Loss after 193637820 batches: 0.0122
trigger times: 15
Loss after 193766460 batches: 0.0120
trigger times: 16
Loss after 193895100 batches: 0.0122
trigger times: 17
Loss after 194023740 batches: 0.0120
trigger times: 18
Loss after 194152380 batches: 0.0119
trigger times: 19
Loss after 194281020 batches: 0.0120
trigger times: 20
Early stopping!
Start to test process.
Loss after 194409660 batches: 0.0118
Time to train on one home:  245.07377409934998
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039]]
Round_6_results:  [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039]
trigger times: 0
Loss after 194540760 batches: 0.0914
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1521 < 1522; dropping {'Training_Loss': 0.0913801568226432, 'Validation_Loss': 0.22664464347892338, 'Training_R2': 0.9079611828399525, 'Validation_R2': 0.7891176014914343, 'Training_F1': 0.8371460306158776, 'Validation_F1': 0.712324969661408, 'Training_NEP': 0.3260935835545068, 'Validation_NEP': 0.5414300357746314, 'Training_NDE': 0.0690951737335844, 'Validation_NDE': 0.1679356256943649, 'Training_MAE': 10.799572774012702, 'Validation_MAE': 14.848257836560622, 'Training_MSE': 304.0079, 'Validation_MSE': 620.18164}.
trigger times: 0
Loss after 194671860 batches: 0.0261
trigger times: 1
Loss after 194802960 batches: 0.0197
trigger times: 0
Loss after 194934060 batches: 0.0170
trigger times: 1
Loss after 195065160 batches: 0.0159
trigger times: 2
Loss after 195196260 batches: 0.0147
trigger times: 3
Loss after 195327360 batches: 0.0143
trigger times: 0
Loss after 195458460 batches: 0.0134
trigger times: 1
Loss after 195589560 batches: 0.0127
trigger times: 0
Loss after 195720660 batches: 0.0122
trigger times: 1
Loss after 195851760 batches: 0.0121
trigger times: 0
Loss after 195982860 batches: 0.0119
trigger times: 1
Loss after 196113960 batches: 0.0118
trigger times: 2
Loss after 196245060 batches: 0.0114
trigger times: 3
Loss after 196376160 batches: 0.0113
trigger times: 4
Loss after 196507260 batches: 0.0110
trigger times: 5
Loss after 196638360 batches: 0.0108
trigger times: 6
Loss after 196769460 batches: 0.0107
trigger times: 7
Loss after 196900560 batches: 0.0105
trigger times: 8
Loss after 197031660 batches: 0.0105
trigger times: 9
Loss after 197162760 batches: 0.0103
trigger times: 10
Loss after 197293860 batches: 0.0101
trigger times: 11
Loss after 197424960 batches: 0.0100
trigger times: 12
Loss after 197556060 batches: 0.0102
trigger times: 13
Loss after 197687160 batches: 0.0099
trigger times: 14
Loss after 197818260 batches: 0.0097
trigger times: 0
Loss after 197949360 batches: 0.0098
trigger times: 1
Loss after 198080460 batches: 0.0097
trigger times: 2
Loss after 198211560 batches: 0.0094
trigger times: 3
Loss after 198342660 batches: 0.0095
trigger times: 4
Loss after 198473760 batches: 0.0091
trigger times: 5
Loss after 198604860 batches: 0.0091
trigger times: 6
Loss after 198735960 batches: 0.0093
trigger times: 7
Loss after 198867060 batches: 0.0090
trigger times: 8
Loss after 198998160 batches: 0.0090
trigger times: 9
Loss after 199129260 batches: 0.0090
trigger times: 10
Loss after 199260360 batches: 0.0088
trigger times: 11
Loss after 199391460 batches: 0.0087
trigger times: 12
Loss after 199522560 batches: 0.0088
trigger times: 13
Loss after 199653660 batches: 0.0087
trigger times: 14
Loss after 199784760 batches: 0.0087
trigger times: 15
Loss after 199915860 batches: 0.0086
trigger times: 0
Loss after 200046960 batches: 0.0085
trigger times: 1
Loss after 200178060 batches: 0.0084
trigger times: 0
Loss after 200309160 batches: 0.0084
trigger times: 1
Loss after 200440260 batches: 0.0086
trigger times: 2
Loss after 200571360 batches: 0.0083
trigger times: 3
Loss after 200702460 batches: 0.0083
trigger times: 4
Loss after 200833560 batches: 0.0083
trigger times: 5
Loss after 200964660 batches: 0.0081
trigger times: 6
Loss after 201095760 batches: 0.0081
trigger times: 7
Loss after 201226860 batches: 0.0081
trigger times: 8
Loss after 201357960 batches: 0.0078
trigger times: 9
Loss after 201489060 batches: 0.0079
trigger times: 10
Loss after 201620160 batches: 0.0079
trigger times: 11
Loss after 201751260 batches: 0.0080
trigger times: 12
Loss after 201882360 batches: 0.0078
trigger times: 13
Loss after 202013460 batches: 0.0078
trigger times: 14
Loss after 202144560 batches: 0.0077
trigger times: 15
Loss after 202275660 batches: 0.0077
trigger times: 16
Loss after 202406760 batches: 0.0081
trigger times: 17
Loss after 202537860 batches: 0.0077
trigger times: 18
Loss after 202668960 batches: 0.0077
trigger times: 19
Loss after 202800060 batches: 0.0077
trigger times: 20
Early stopping!
Start to test process.
Loss after 202931160 batches: 0.0079
Time to train on one home:  480.17949056625366
trigger times: 0
Loss after 203033760 batches: 0.3062
trigger times: 0
Loss after 203136360 batches: 0.0943
trigger times: 0
Loss after 203238960 batches: 0.0611
trigger times: 1
Loss after 203341560 batches: 0.0503
trigger times: 2
Loss after 203444160 batches: 0.0429
trigger times: 3
Loss after 203546760 batches: 0.0395
trigger times: 4
Loss after 203649360 batches: 0.0373
trigger times: 5
Loss after 203751960 batches: 0.0340
trigger times: 6
Loss after 203854560 batches: 0.0318
trigger times: 7
Loss after 203957160 batches: 0.0315
trigger times: 8
Loss after 204059760 batches: 0.0327
trigger times: 9
Loss after 204162360 batches: 0.0291
trigger times: 10
Loss after 204264960 batches: 0.0288
trigger times: 11
Loss after 204367560 batches: 0.0275
trigger times: 12
Loss after 204470160 batches: 0.0259
trigger times: 13
Loss after 204572760 batches: 0.0265
trigger times: 14
Loss after 204675360 batches: 0.0254
trigger times: 15
Loss after 204777960 batches: 0.0243
trigger times: 16
Loss after 204880560 batches: 0.0251
trigger times: 17
Loss after 204983160 batches: 0.0241
trigger times: 18
Loss after 205085760 batches: 0.0238
trigger times: 19
Loss after 205188360 batches: 0.0231
trigger times: 20
Early stopping!
Start to test process.
Loss after 205290960 batches: 0.0254
Time to train on one home:  145.58510303497314
trigger times: 0
Loss after 205422060 batches: 0.1443
trigger times: 1
Loss after 205553160 batches: 0.0431
trigger times: 2
Loss after 205684260 batches: 0.0301
trigger times: 0
Loss after 205815360 batches: 0.0256
trigger times: 1
Loss after 205946460 batches: 0.0228
trigger times: 2
Loss after 206077560 batches: 0.0211
trigger times: 3
Loss after 206208660 batches: 0.0199
trigger times: 4
Loss after 206339760 batches: 0.0189
trigger times: 5
Loss after 206470860 batches: 0.0181
trigger times: 6
Loss after 206601960 batches: 0.0177
trigger times: 7
Loss after 206733060 batches: 0.0172
trigger times: 8
Loss after 206864160 batches: 0.0169
trigger times: 9
Loss after 206995260 batches: 0.0163
trigger times: 10
Loss after 207126360 batches: 0.0161
trigger times: 11
Loss after 207257460 batches: 0.0156
trigger times: 0
Loss after 207388560 batches: 0.0156
trigger times: 0
Loss after 207519660 batches: 0.0153
trigger times: 1
Loss after 207650760 batches: 0.0151
trigger times: 2
Loss after 207781860 batches: 0.0147
trigger times: 3
Loss after 207912960 batches: 0.0146
trigger times: 4
Loss after 208044060 batches: 0.0143
trigger times: 5
Loss after 208175160 batches: 0.0144
trigger times: 6
Loss after 208306260 batches: 0.0139
trigger times: 7
Loss after 208437360 batches: 0.0138
trigger times: 8
Loss after 208568460 batches: 0.0137
trigger times: 9
Loss after 208699560 batches: 0.0137
trigger times: 10
Loss after 208830660 batches: 0.0137
trigger times: 11
Loss after 208961760 batches: 0.0134
trigger times: 12
Loss after 209092860 batches: 0.0133
trigger times: 13
Loss after 209223960 batches: 0.0132
trigger times: 0
Loss after 209355060 batches: 0.0131
trigger times: 0
Loss after 209486160 batches: 0.0129
trigger times: 1
Loss after 209617260 batches: 0.0129
trigger times: 2
Loss after 209748360 batches: 0.0129
trigger times: 3
Loss after 209879460 batches: 0.0128
trigger times: 4
Loss after 210010560 batches: 0.0126
trigger times: 5
Loss after 210141660 batches: 0.0126
trigger times: 6
Loss after 210272760 batches: 0.0125
trigger times: 7
Loss after 210403860 batches: 0.0123
trigger times: 8
Loss after 210534960 batches: 0.0124
trigger times: 9
Loss after 210666060 batches: 0.0123
trigger times: 10
Loss after 210797160 batches: 0.0122
trigger times: 0
Loss after 210928260 batches: 0.0121
trigger times: 1
Loss after 211059360 batches: 0.0122
trigger times: 2
Loss after 211190460 batches: 0.0122
trigger times: 3
Loss after 211321560 batches: 0.0120
trigger times: 4
Loss after 211452660 batches: 0.0118
trigger times: 5
Loss after 211583760 batches: 0.0119
trigger times: 6
Loss after 211714860 batches: 0.0118
trigger times: 7
Loss after 211845960 batches: 0.0117
trigger times: 8
Loss after 211977060 batches: 0.0118
trigger times: 9
Loss after 212108160 batches: 0.0117
trigger times: 10
Loss after 212239260 batches: 0.0117
trigger times: 11
Loss after 212370360 batches: 0.0116
trigger times: 12
Loss after 212501460 batches: 0.0114
trigger times: 13
Loss after 212632560 batches: 0.0115
trigger times: 14
Loss after 212763660 batches: 0.0113
trigger times: 15
Loss after 212894760 batches: 0.0114
trigger times: 16
Loss after 213025860 batches: 0.0113
trigger times: 17
Loss after 213156960 batches: 0.0112
trigger times: 18
Loss after 213288060 batches: 0.0113
trigger times: 19
Loss after 213419160 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 213550260 batches: 0.0113
Time to train on one home:  461.2253029346466
trigger times: 0
Loss after 213681360 batches: 0.1519
trigger times: 0
Loss after 213812460 batches: 0.0473
trigger times: 0
Loss after 213943560 batches: 0.0341
trigger times: 0
Loss after 214074660 batches: 0.0294
trigger times: 0
Loss after 214205760 batches: 0.0266
trigger times: 1
Loss after 214336860 batches: 0.0252
trigger times: 2
Loss after 214467960 batches: 0.0243
trigger times: 3
Loss after 214599060 batches: 0.0233
trigger times: 0
Loss after 214730160 batches: 0.0229
trigger times: 1
Loss after 214861260 batches: 0.0219
trigger times: 0
Loss after 214992360 batches: 0.0217
trigger times: 0
Loss after 215123460 batches: 0.0209
trigger times: 1
Loss after 215254560 batches: 0.0204
trigger times: 0
Loss after 215385660 batches: 0.0202
trigger times: 1
Loss after 215516760 batches: 0.0198
trigger times: 2
Loss after 215647860 batches: 0.0196
trigger times: 3
Loss after 215778960 batches: 0.0193
trigger times: 4
Loss after 215910060 batches: 0.0190
trigger times: 5
Loss after 216041160 batches: 0.0186
trigger times: 6
Loss after 216172260 batches: 0.0186
trigger times: 7
Loss after 216303360 batches: 0.0186
trigger times: 8
Loss after 216434460 batches: 0.0183
trigger times: 9
Loss after 216565560 batches: 0.0183
trigger times: 10
Loss after 216696660 batches: 0.0182
trigger times: 11
Loss after 216827760 batches: 0.0181
trigger times: 12
Loss after 216958860 batches: 0.0177
trigger times: 0
Loss after 217089960 batches: 0.0178
trigger times: 1
Loss after 217221060 batches: 0.0177
trigger times: 2
Loss after 217352160 batches: 0.0175
trigger times: 3
Loss after 217483260 batches: 0.0173
trigger times: 4
Loss after 217614360 batches: 0.0174
trigger times: 5
Loss after 217745460 batches: 0.0171
trigger times: 6
Loss after 217876560 batches: 0.0169
trigger times: 7
Loss after 218007660 batches: 0.0170
trigger times: 0
Loss after 218138760 batches: 0.0169
trigger times: 1
Loss after 218269860 batches: 0.0169
trigger times: 2
Loss after 218400960 batches: 0.0166
trigger times: 3
Loss after 218532060 batches: 0.0171
trigger times: 4
Loss after 218663160 batches: 0.0168
trigger times: 5
Loss after 218794260 batches: 0.0166
trigger times: 6
Loss after 218925360 batches: 0.0164
trigger times: 7
Loss after 219056460 batches: 0.0164
trigger times: 0
Loss after 219187560 batches: 0.0163
trigger times: 1
Loss after 219318660 batches: 0.0162
trigger times: 2
Loss after 219449760 batches: 0.0162
trigger times: 3
Loss after 219580860 batches: 0.0162
trigger times: 4
Loss after 219711960 batches: 0.0159
trigger times: 5
Loss after 219843060 batches: 0.0159
trigger times: 6
Loss after 219974160 batches: 0.0158
trigger times: 7
Loss after 220105260 batches: 0.0161
trigger times: 8
Loss after 220236360 batches: 0.0159
trigger times: 9
Loss after 220367460 batches: 0.0158
trigger times: 10
Loss after 220498560 batches: 0.0155
trigger times: 11
Loss after 220629660 batches: 0.0156
trigger times: 12
Loss after 220760760 batches: 0.0156
trigger times: 0
Loss after 220891860 batches: 0.0154
trigger times: 1
Loss after 221022960 batches: 0.0155
trigger times: 2
Loss after 221154060 batches: 0.0151
trigger times: 3
Loss after 221285160 batches: 0.0154
trigger times: 4
Loss after 221416260 batches: 0.0153
trigger times: 5
Loss after 221547360 batches: 0.0153
trigger times: 6
Loss after 221678460 batches: 0.0151
trigger times: 7
Loss after 221809560 batches: 0.0153
trigger times: 8
Loss after 221940660 batches: 0.0151
trigger times: 9
Loss after 222071760 batches: 0.0152
trigger times: 10
Loss after 222202860 batches: 0.0150
trigger times: 11
Loss after 222333960 batches: 0.0147
trigger times: 12
Loss after 222465060 batches: 0.0149
trigger times: 13
Loss after 222596160 batches: 0.0149
trigger times: 14
Loss after 222727260 batches: 0.0149
trigger times: 15
Loss after 222858360 batches: 0.0148
trigger times: 16
Loss after 222989460 batches: 0.0148
trigger times: 17
Loss after 223120560 batches: 0.0145
trigger times: 18
Loss after 223251660 batches: 0.0147
trigger times: 19
Loss after 223382760 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 223513860 batches: 0.0145
Time to train on one home:  556.9215750694275
trigger times: 0
Loss after 223642500 batches: 0.1170
trigger times: 1
Loss after 223771140 batches: 0.0324
trigger times: 2
Loss after 223899780 batches: 0.0240
trigger times: 0
Loss after 224028420 batches: 0.0212
trigger times: 1
Loss after 224157060 batches: 0.0191
trigger times: 2
Loss after 224285700 batches: 0.0179
trigger times: 3
Loss after 224414340 batches: 0.0172
trigger times: 4
Loss after 224542980 batches: 0.0167
trigger times: 5
Loss after 224671620 batches: 0.0157
trigger times: 6
Loss after 224800260 batches: 0.0155
trigger times: 7
Loss after 224928900 batches: 0.0151
trigger times: 8
Loss after 225057540 batches: 0.0145
trigger times: 9
Loss after 225186180 batches: 0.0144
trigger times: 10
Loss after 225314820 batches: 0.0142
trigger times: 11
Loss after 225443460 batches: 0.0140
trigger times: 12
Loss after 225572100 batches: 0.0136
trigger times: 13
Loss after 225700740 batches: 0.0132
trigger times: 14
Loss after 225829380 batches: 0.0130
trigger times: 15
Loss after 225958020 batches: 0.0128
trigger times: 16
Loss after 226086660 batches: 0.0128
trigger times: 17
Loss after 226215300 batches: 0.0125
trigger times: 18
Loss after 226343940 batches: 0.0126
trigger times: 19
Loss after 226472580 batches: 0.0123
trigger times: 20
Early stopping!
Start to test process.
Loss after 226601220 batches: 0.0121
Time to train on one home:  182.97259163856506
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806]]
Round_7_results:  [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1772 < 1773; dropping {'Training_Loss': 0.09472902857188908, 'Validation_Loss': 0.23412686420811546, 'Training_R2': 0.9046979635728722, 'Validation_R2': 0.7822895293471409, 'Training_F1': 0.8317592557823827, 'Validation_F1': 0.7490986882821281, 'Training_NEP': 0.33691181110100404, 'Validation_NEP': 0.5021228357792453, 'Training_NDE': 0.07154493035961307, 'Validation_NDE': 0.17337314241433724, 'Training_MAE': 11.157851015493945, 'Validation_MAE': 13.770291337103826, 'Training_MSE': 314.7865, 'Validation_MSE': 640.26227}.
trigger times: 0
Loss after 226732320 batches: 0.0947
trigger times: 1
Loss after 226863420 batches: 0.0258
trigger times: 0
Loss after 226994520 batches: 0.0188
trigger times: 1
Loss after 227125620 batches: 0.0162
trigger times: 2
Loss after 227256720 batches: 0.0148
trigger times: 3
Loss after 227387820 batches: 0.0141
trigger times: 4
Loss after 227518920 batches: 0.0133
trigger times: 5
Loss after 227650020 batches: 0.0127
trigger times: 0
Loss after 227781120 batches: 0.0122
trigger times: 0
Loss after 227912220 batches: 0.0120
trigger times: 1
Loss after 228043320 batches: 0.0115
trigger times: 0
Loss after 228174420 batches: 0.0116
trigger times: 1
Loss after 228305520 batches: 0.0112
trigger times: 2
Loss after 228436620 batches: 0.0109
trigger times: 3
Loss after 228567720 batches: 0.0107
trigger times: 4
Loss after 228698820 batches: 0.0105
trigger times: 5
Loss after 228829920 batches: 0.0104
trigger times: 0
Loss after 228961020 batches: 0.0104
trigger times: 1
Loss after 229092120 batches: 0.0098
trigger times: 2
Loss after 229223220 batches: 0.0097
trigger times: 3
Loss after 229354320 batches: 0.0100
trigger times: 4
Loss after 229485420 batches: 0.0097
trigger times: 5
Loss after 229616520 batches: 0.0097
trigger times: 6
Loss after 229747620 batches: 0.0097
trigger times: 7
Loss after 229878720 batches: 0.0094
trigger times: 8
Loss after 230009820 batches: 0.0093
trigger times: 9
Loss after 230140920 batches: 0.0091
trigger times: 10
Loss after 230272020 batches: 0.0089
trigger times: 11
Loss after 230403120 batches: 0.0091
trigger times: 12
Loss after 230534220 batches: 0.0092
trigger times: 13
Loss after 230665320 batches: 0.0090
trigger times: 14
Loss after 230796420 batches: 0.0088
trigger times: 0
Loss after 230927520 batches: 0.0088
trigger times: 1
Loss after 231058620 batches: 0.0089
trigger times: 2
Loss after 231189720 batches: 0.0089
trigger times: 0
Loss after 231320820 batches: 0.0086
trigger times: 1
Loss after 231451920 batches: 0.0086
trigger times: 2
Loss after 231583020 batches: 0.0086
trigger times: 3
Loss after 231714120 batches: 0.0083
trigger times: 4
Loss after 231845220 batches: 0.0086
trigger times: 5
Loss after 231976320 batches: 0.0083
trigger times: 6
Loss after 232107420 batches: 0.0083
trigger times: 7
Loss after 232238520 batches: 0.0081
trigger times: 8
Loss after 232369620 batches: 0.0081
trigger times: 9
Loss after 232500720 batches: 0.0082
trigger times: 10
Loss after 232631820 batches: 0.0084
trigger times: 11
Loss after 232762920 batches: 0.0081
trigger times: 12
Loss after 232894020 batches: 0.0080
trigger times: 13
Loss after 233025120 batches: 0.0078
trigger times: 14
Loss after 233156220 batches: 0.0078
trigger times: 15
Loss after 233287320 batches: 0.0078
trigger times: 16
Loss after 233418420 batches: 0.0079
trigger times: 17
Loss after 233549520 batches: 0.0076
trigger times: 18
Loss after 233680620 batches: 0.0078
trigger times: 19
Loss after 233811720 batches: 0.0077
trigger times: 20
Early stopping!
Start to test process.
Loss after 233942820 batches: 0.0076
Time to train on one home:  410.9720034599304
trigger times: 0
Loss after 234045420 batches: 0.2813
trigger times: 1
Loss after 234148020 batches: 0.0842
trigger times: 2
Loss after 234250620 batches: 0.0593
trigger times: 0
Loss after 234353220 batches: 0.0535
trigger times: 1
Loss after 234455820 batches: 0.0438
trigger times: 2
Loss after 234558420 batches: 0.0363
trigger times: 3
Loss after 234661020 batches: 0.0331
trigger times: 4
Loss after 234763620 batches: 0.0320
trigger times: 5
Loss after 234866220 batches: 0.0313
trigger times: 6
Loss after 234968820 batches: 0.0303
trigger times: 7
Loss after 235071420 batches: 0.0284
trigger times: 8
Loss after 235174020 batches: 0.0269
trigger times: 9
Loss after 235276620 batches: 0.0285
trigger times: 10
Loss after 235379220 batches: 0.0266
trigger times: 11
Loss after 235481820 batches: 0.0259
trigger times: 12
Loss after 235584420 batches: 0.0246
trigger times: 13
Loss after 235687020 batches: 0.0258
trigger times: 14
Loss after 235789620 batches: 0.0248
trigger times: 15
Loss after 235892220 batches: 0.0236
trigger times: 16
Loss after 235994820 batches: 0.0238
trigger times: 17
Loss after 236097420 batches: 0.0228
trigger times: 18
Loss after 236200020 batches: 0.0256
trigger times: 19
Loss after 236302620 batches: 0.0230
trigger times: 20
Early stopping!
Start to test process.
Loss after 236405220 batches: 0.0224
Time to train on one home:  150.7385139465332
trigger times: 0
Loss after 236536320 batches: 0.0811
trigger times: 1
Loss after 236667420 batches: 0.0300
trigger times: 0
Loss after 236798520 batches: 0.0226
trigger times: 1
Loss after 236929620 batches: 0.0200
trigger times: 2
Loss after 237060720 batches: 0.0186
trigger times: 0
Loss after 237191820 batches: 0.0175
trigger times: 0
Loss after 237322920 batches: 0.0168
trigger times: 0
Loss after 237454020 batches: 0.0163
trigger times: 0
Loss after 237585120 batches: 0.0157
trigger times: 1
Loss after 237716220 batches: 0.0154
trigger times: 2
Loss after 237847320 batches: 0.0151
trigger times: 3
Loss after 237978420 batches: 0.0147
trigger times: 4
Loss after 238109520 batches: 0.0145
trigger times: 5
Loss after 238240620 batches: 0.0142
trigger times: 6
Loss after 238371720 batches: 0.0141
trigger times: 7
Loss after 238502820 batches: 0.0138
trigger times: 8
Loss after 238633920 batches: 0.0135
trigger times: 9
Loss after 238765020 batches: 0.0132
trigger times: 10
Loss after 238896120 batches: 0.0133
trigger times: 11
Loss after 239027220 batches: 0.0133
trigger times: 12
Loss after 239158320 batches: 0.0130
trigger times: 13
Loss after 239289420 batches: 0.0131
trigger times: 14
Loss after 239420520 batches: 0.0129
trigger times: 15
Loss after 239551620 batches: 0.0127
trigger times: 16
Loss after 239682720 batches: 0.0126
trigger times: 17
Loss after 239813820 batches: 0.0124
trigger times: 18
Loss after 239944920 batches: 0.0125
trigger times: 19
Loss after 240076020 batches: 0.0124
trigger times: 20
Early stopping!
Start to test process.
Loss after 240207120 batches: 0.0123
Time to train on one home:  218.48815155029297
trigger times: 0
Loss after 240338220 batches: 0.1410
trigger times: 0
Loss after 240469320 batches: 0.0400
trigger times: 0
Loss after 240600420 batches: 0.0299
trigger times: 1
Loss after 240731520 batches: 0.0267
trigger times: 2
Loss after 240862620 batches: 0.0248
trigger times: 3
Loss after 240993720 batches: 0.0236
trigger times: 4
Loss after 241124820 batches: 0.0225
trigger times: 5
Loss after 241255920 batches: 0.0215
trigger times: 0
Loss after 241387020 batches: 0.0209
trigger times: 1
Loss after 241518120 batches: 0.0203
trigger times: 2
Loss after 241649220 batches: 0.0200
trigger times: 3
Loss after 241780320 batches: 0.0201
trigger times: 0
Loss after 241911420 batches: 0.0197
trigger times: 1
Loss after 242042520 batches: 0.0191
trigger times: 2
Loss after 242173620 batches: 0.0189
trigger times: 0
Loss after 242304720 batches: 0.0188
trigger times: 1
Loss after 242435820 batches: 0.0185
trigger times: 0
Loss after 242566920 batches: 0.0182
trigger times: 1
Loss after 242698020 batches: 0.0181
trigger times: 2
Loss after 242829120 batches: 0.0178
trigger times: 3
Loss after 242960220 batches: 0.0176
trigger times: 4
Loss after 243091320 batches: 0.0175
trigger times: 5
Loss after 243222420 batches: 0.0174
trigger times: 6
Loss after 243353520 batches: 0.0173
trigger times: 0
Loss after 243484620 batches: 0.0171
trigger times: 1
Loss after 243615720 batches: 0.0167
trigger times: 2
Loss after 243746820 batches: 0.0169
trigger times: 3
Loss after 243877920 batches: 0.0171
trigger times: 4
Loss after 244009020 batches: 0.0167
trigger times: 5
Loss after 244140120 batches: 0.0167
trigger times: 6
Loss after 244271220 batches: 0.0165
trigger times: 7
Loss after 244402320 batches: 0.0162
trigger times: 8
Loss after 244533420 batches: 0.0164
trigger times: 9
Loss after 244664520 batches: 0.0163
trigger times: 10
Loss after 244795620 batches: 0.0162
trigger times: 11
Loss after 244926720 batches: 0.0160
trigger times: 12
Loss after 245057820 batches: 0.0161
trigger times: 13
Loss after 245188920 batches: 0.0162
trigger times: 14
Loss after 245320020 batches: 0.0160
trigger times: 15
Loss after 245451120 batches: 0.0159
trigger times: 16
Loss after 245582220 batches: 0.0161
trigger times: 17
Loss after 245713320 batches: 0.0157
trigger times: 18
Loss after 245844420 batches: 0.0158
trigger times: 19
Loss after 245975520 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 246106620 batches: 0.0155
Time to train on one home:  332.23774576187134
trigger times: 0
Loss after 246235260 batches: 0.1017
trigger times: 0
Loss after 246363900 batches: 0.0296
trigger times: 1
Loss after 246492540 batches: 0.0225
trigger times: 2
Loss after 246621180 batches: 0.0197
trigger times: 3
Loss after 246749820 batches: 0.0185
trigger times: 0
Loss after 246878460 batches: 0.0172
trigger times: 1
Loss after 247007100 batches: 0.0160
trigger times: 2
Loss after 247135740 batches: 0.0158
trigger times: 3
Loss after 247264380 batches: 0.0153
trigger times: 4
Loss after 247393020 batches: 0.0147
trigger times: 5
Loss after 247521660 batches: 0.0142
trigger times: 6
Loss after 247650300 batches: 0.0140
trigger times: 7
Loss after 247778940 batches: 0.0137
trigger times: 8
Loss after 247907580 batches: 0.0136
trigger times: 9
Loss after 248036220 batches: 0.0134
trigger times: 10
Loss after 248164860 batches: 0.0130
trigger times: 11
Loss after 248293500 batches: 0.0130
trigger times: 12
Loss after 248422140 batches: 0.0125
trigger times: 13
Loss after 248550780 batches: 0.0125
trigger times: 14
Loss after 248679420 batches: 0.0124
trigger times: 15
Loss after 248808060 batches: 0.0122
trigger times: 16
Loss after 248936700 batches: 0.0120
trigger times: 17
Loss after 249065340 batches: 0.0120
trigger times: 18
Loss after 249193980 batches: 0.0118
trigger times: 19
Loss after 249322620 batches: 0.0116
trigger times: 20
Early stopping!
Start to test process.
Loss after 249451260 batches: 0.0116
Time to train on one home:  193.86439752578735
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765, 0.013892934501727369]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806], [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324]]
Round_8_results:  [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1952 < 1953; dropping {'Training_Loss': 0.07016874874397269, 'Validation_Loss': 0.2332336422469881, 'Training_R2': 0.9293272068141295, 'Validation_R2': 0.7829764867511856, 'Training_F1': 0.8544494232238843, 'Validation_F1': 0.7261042615740368, 'Training_NEP': 0.2912102590855945, 'Validation_NEP': 0.5341518008857598, 'Training_NDE': 0.05305532028865626, 'Validation_NDE': 0.1728260857501038, 'Training_MAE': 9.644306248694681, 'Validation_MAE': 14.648658440360887, 'Training_MSE': 233.4351, 'Validation_MSE': 638.242}.
trigger times: 0
Loss after 249582360 batches: 0.0702
trigger times: 0
Loss after 249713460 batches: 0.0214
trigger times: 1
Loss after 249844560 batches: 0.0161
trigger times: 0
Loss after 249975660 batches: 0.0141
trigger times: 0
Loss after 250106760 batches: 0.0132
trigger times: 0
Loss after 250237860 batches: 0.0124
trigger times: 1
Loss after 250368960 batches: 0.0120
trigger times: 0
Loss after 250500060 batches: 0.0115
trigger times: 0
Loss after 250631160 batches: 0.0110
trigger times: 1
Loss after 250762260 batches: 0.0109
trigger times: 2
Loss after 250893360 batches: 0.0104
trigger times: 3
Loss after 251024460 batches: 0.0101
trigger times: 4
Loss after 251155560 batches: 0.0101
trigger times: 5
Loss after 251286660 batches: 0.0098
trigger times: 6
Loss after 251417760 batches: 0.0097
trigger times: 7
Loss after 251548860 batches: 0.0094
trigger times: 8
Loss after 251679960 batches: 0.0093
trigger times: 9
Loss after 251811060 batches: 0.0092
trigger times: 10
Loss after 251942160 batches: 0.0093
trigger times: 11
Loss after 252073260 batches: 0.0092
trigger times: 12
Loss after 252204360 batches: 0.0090
trigger times: 13
Loss after 252335460 batches: 0.0091
trigger times: 14
Loss after 252466560 batches: 0.0086
trigger times: 15
Loss after 252597660 batches: 0.0087
trigger times: 16
Loss after 252728760 batches: 0.0090
trigger times: 17
Loss after 252859860 batches: 0.0086
trigger times: 18
Loss after 252990960 batches: 0.0086
trigger times: 19
Loss after 253122060 batches: 0.0084
trigger times: 20
Early stopping!
Start to test process.
Loss after 253253160 batches: 0.0085
Time to train on one home:  218.86183714866638
trigger times: 0
Loss after 253355760 batches: 0.2210
trigger times: 0
Loss after 253458360 batches: 0.0637
trigger times: 1
Loss after 253560960 batches: 0.0465
trigger times: 2
Loss after 253663560 batches: 0.0390
trigger times: 3
Loss after 253766160 batches: 0.0374
trigger times: 4
Loss after 253868760 batches: 0.0358
trigger times: 5
Loss after 253971360 batches: 0.0342
trigger times: 6
Loss after 254073960 batches: 0.0298
trigger times: 7
Loss after 254176560 batches: 0.0284
trigger times: 8
Loss after 254279160 batches: 0.0274
trigger times: 9
Loss after 254381760 batches: 0.0259
trigger times: 10
Loss after 254484360 batches: 0.0249
trigger times: 11
Loss after 254586960 batches: 0.0244
trigger times: 12
Loss after 254689560 batches: 0.0248
trigger times: 13
Loss after 254792160 batches: 0.0278
trigger times: 14
Loss after 254894760 batches: 0.0271
trigger times: 15
Loss after 254997360 batches: 0.0256
trigger times: 16
Loss after 255099960 batches: 0.0235
trigger times: 17
Loss after 255202560 batches: 0.0226
trigger times: 18
Loss after 255305160 batches: 0.0235
trigger times: 19
Loss after 255407760 batches: 0.0233
trigger times: 20
Early stopping!
Start to test process.
Loss after 255510360 batches: 0.0237
Time to train on one home:  139.13049364089966
trigger times: 0
Loss after 255641460 batches: 0.0873
trigger times: 1
Loss after 255772560 batches: 0.0291
trigger times: 0
Loss after 255903660 batches: 0.0222
trigger times: 1
Loss after 256034760 batches: 0.0196
trigger times: 2
Loss after 256165860 batches: 0.0184
trigger times: 3
Loss after 256296960 batches: 0.0172
trigger times: 4
Loss after 256428060 batches: 0.0165
trigger times: 5
Loss after 256559160 batches: 0.0161
trigger times: 6
Loss after 256690260 batches: 0.0156
trigger times: 7
Loss after 256821360 batches: 0.0150
trigger times: 8
Loss after 256952460 batches: 0.0150
trigger times: 9
Loss after 257083560 batches: 0.0143
trigger times: 10
Loss after 257214660 batches: 0.0142
trigger times: 11
Loss after 257345760 batches: 0.0142
trigger times: 12
Loss after 257476860 batches: 0.0138
trigger times: 13
Loss after 257607960 batches: 0.0136
trigger times: 14
Loss after 257739060 batches: 0.0134
trigger times: 15
Loss after 257870160 batches: 0.0133
trigger times: 16
Loss after 258001260 batches: 0.0131
trigger times: 17
Loss after 258132360 batches: 0.0131
trigger times: 0
Loss after 258263460 batches: 0.0129
trigger times: 1
Loss after 258394560 batches: 0.0128
trigger times: 2
Loss after 258525660 batches: 0.0126
trigger times: 3
Loss after 258656760 batches: 0.0126
trigger times: 4
Loss after 258787860 batches: 0.0124
trigger times: 5
Loss after 258918960 batches: 0.0125
trigger times: 6
Loss after 259050060 batches: 0.0124
trigger times: 7
Loss after 259181160 batches: 0.0120
trigger times: 8
Loss after 259312260 batches: 0.0121
trigger times: 0
Loss after 259443360 batches: 0.0119
trigger times: 0
Loss after 259574460 batches: 0.0119
trigger times: 1
Loss after 259705560 batches: 0.0119
trigger times: 2
Loss after 259836660 batches: 0.0118
trigger times: 3
Loss after 259967760 batches: 0.0117
trigger times: 4
Loss after 260098860 batches: 0.0117
trigger times: 5
Loss after 260229960 batches: 0.0117
trigger times: 6
Loss after 260361060 batches: 0.0116
trigger times: 7
Loss after 260492160 batches: 0.0116
trigger times: 8
Loss after 260623260 batches: 0.0117
trigger times: 9
Loss after 260754360 batches: 0.0116
trigger times: 10
Loss after 260885460 batches: 0.0114
trigger times: 11
Loss after 261016560 batches: 0.0113
trigger times: 12
Loss after 261147660 batches: 0.0114
trigger times: 13
Loss after 261278760 batches: 0.0114
trigger times: 0
Loss after 261409860 batches: 0.0112
trigger times: 1
Loss after 261540960 batches: 0.0111
trigger times: 2
Loss after 261672060 batches: 0.0110
trigger times: 3
Loss after 261803160 batches: 0.0110
trigger times: 4
Loss after 261934260 batches: 0.0112
trigger times: 5
Loss after 262065360 batches: 0.0111
trigger times: 6
Loss after 262196460 batches: 0.0109
trigger times: 7
Loss after 262327560 batches: 0.0109
trigger times: 8
Loss after 262458660 batches: 0.0107
trigger times: 9
Loss after 262589760 batches: 0.0108
trigger times: 10
Loss after 262720860 batches: 0.0109
trigger times: 11
Loss after 262851960 batches: 0.0108
trigger times: 12
Loss after 262983060 batches: 0.0106
trigger times: 13
Loss after 263114160 batches: 0.0106
trigger times: 14
Loss after 263245260 batches: 0.0105
trigger times: 15
Loss after 263376360 batches: 0.0106
trigger times: 16
Loss after 263507460 batches: 0.0105
trigger times: 17
Loss after 263638560 batches: 0.0104
trigger times: 18
Loss after 263769660 batches: 0.0105
trigger times: 19
Loss after 263900760 batches: 0.0105
trigger times: 20
Early stopping!
Start to test process.
Loss after 264031860 batches: 0.0106
Time to train on one home:  474.68110632896423
trigger times: 0
Loss after 264162960 batches: 0.1327
trigger times: 0
Loss after 264294060 batches: 0.0370
trigger times: 1
Loss after 264425160 batches: 0.0286
trigger times: 2
Loss after 264556260 batches: 0.0253
trigger times: 3
Loss after 264687360 batches: 0.0236
trigger times: 0
Loss after 264818460 batches: 0.0223
trigger times: 0
Loss after 264949560 batches: 0.0217
trigger times: 0
Loss after 265080660 batches: 0.0212
trigger times: 1
Loss after 265211760 batches: 0.0207
trigger times: 2
Loss after 265342860 batches: 0.0198
trigger times: 0
Loss after 265473960 batches: 0.0194
trigger times: 1
Loss after 265605060 batches: 0.0191
trigger times: 2
Loss after 265736160 batches: 0.0190
trigger times: 3
Loss after 265867260 batches: 0.0186
trigger times: 4
Loss after 265998360 batches: 0.0184
trigger times: 5
Loss after 266129460 batches: 0.0185
trigger times: 0
Loss after 266260560 batches: 0.0183
trigger times: 1
Loss after 266391660 batches: 0.0179
trigger times: 0
Loss after 266522760 batches: 0.0178
trigger times: 1
Loss after 266653860 batches: 0.0177
trigger times: 0
Loss after 266784960 batches: 0.0174
trigger times: 1
Loss after 266916060 batches: 0.0175
trigger times: 2
Loss after 267047160 batches: 0.0171
trigger times: 3
Loss after 267178260 batches: 0.0171
trigger times: 4
Loss after 267309360 batches: 0.0171
trigger times: 5
Loss after 267440460 batches: 0.0168
trigger times: 6
Loss after 267571560 batches: 0.0164
trigger times: 7
Loss after 267702660 batches: 0.0165
trigger times: 8
Loss after 267833760 batches: 0.0164
trigger times: 0
Loss after 267964860 batches: 0.0164
trigger times: 1
Loss after 268095960 batches: 0.0161
trigger times: 2
Loss after 268227060 batches: 0.0163
trigger times: 3
Loss after 268358160 batches: 0.0160
trigger times: 4
Loss after 268489260 batches: 0.0162
trigger times: 5
Loss after 268620360 batches: 0.0159
trigger times: 6
Loss after 268751460 batches: 0.0161
trigger times: 7
Loss after 268882560 batches: 0.0163
trigger times: 8
Loss after 269013660 batches: 0.0159
trigger times: 9
Loss after 269144760 batches: 0.0159
trigger times: 10
Loss after 269275860 batches: 0.0157
trigger times: 11
Loss after 269406960 batches: 0.0157
trigger times: 12
Loss after 269538060 batches: 0.0156
trigger times: 0
Loss after 269669160 batches: 0.0157
trigger times: 1
Loss after 269800260 batches: 0.0154
trigger times: 2
Loss after 269931360 batches: 0.0153
trigger times: 3
Loss after 270062460 batches: 0.0153
trigger times: 4
Loss after 270193560 batches: 0.0156
trigger times: 5
Loss after 270324660 batches: 0.0154
trigger times: 6
Loss after 270455760 batches: 0.0152
trigger times: 7
Loss after 270586860 batches: 0.0152
trigger times: 0
Loss after 270717960 batches: 0.0149
trigger times: 1
Loss after 270849060 batches: 0.0154
trigger times: 2
Loss after 270980160 batches: 0.0149
trigger times: 3
Loss after 271111260 batches: 0.0149
trigger times: 4
Loss after 271242360 batches: 0.0150
trigger times: 5
Loss after 271373460 batches: 0.0147
trigger times: 6
Loss after 271504560 batches: 0.0149
trigger times: 7
Loss after 271635660 batches: 0.0149
trigger times: 8
Loss after 271766760 batches: 0.0148
trigger times: 9
Loss after 271897860 batches: 0.0149
trigger times: 10
Loss after 272028960 batches: 0.0145
trigger times: 11
Loss after 272160060 batches: 0.0145
trigger times: 12
Loss after 272291160 batches: 0.0143
trigger times: 13
Loss after 272422260 batches: 0.0144
trigger times: 14
Loss after 272553360 batches: 0.0145
trigger times: 15
Loss after 272684460 batches: 0.0144
trigger times: 16
Loss after 272815560 batches: 0.0144
trigger times: 17
Loss after 272946660 batches: 0.0142
trigger times: 18
Loss after 273077760 batches: 0.0142
trigger times: 19
Loss after 273208860 batches: 0.0143
trigger times: 20
Early stopping!
Start to test process.
Loss after 273339960 batches: 0.0143
Time to train on one home:  517.5106472969055
trigger times: 0
Loss after 273468600 batches: 0.0897
trigger times: 0
Loss after 273597240 batches: 0.0275
trigger times: 1
Loss after 273725880 batches: 0.0208
trigger times: 0
Loss after 273854520 batches: 0.0188
trigger times: 0
Loss after 273983160 batches: 0.0173
trigger times: 1
Loss after 274111800 batches: 0.0161
trigger times: 2
Loss after 274240440 batches: 0.0156
trigger times: 3
Loss after 274369080 batches: 0.0149
trigger times: 4
Loss after 274497720 batches: 0.0145
trigger times: 5
Loss after 274626360 batches: 0.0139
trigger times: 6
Loss after 274755000 batches: 0.0139
trigger times: 7
Loss after 274883640 batches: 0.0139
trigger times: 8
Loss after 275012280 batches: 0.0137
trigger times: 9
Loss after 275140920 batches: 0.0130
trigger times: 10
Loss after 275269560 batches: 0.0127
trigger times: 11
Loss after 275398200 batches: 0.0124
trigger times: 12
Loss after 275526840 batches: 0.0124
trigger times: 13
Loss after 275655480 batches: 0.0126
trigger times: 14
Loss after 275784120 batches: 0.0120
trigger times: 15
Loss after 275912760 batches: 0.0121
trigger times: 16
Loss after 276041400 batches: 0.0118
trigger times: 17
Loss after 276170040 batches: 0.0119
trigger times: 18
Loss after 276298680 batches: 0.0115
trigger times: 19
Loss after 276427320 batches: 0.0114
trigger times: 20
Early stopping!
Start to test process.
Loss after 276555960 batches: 0.0112
Time to train on one home:  187.06429076194763
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765, 0.013892934501727369, 0.013666319778952196]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806], [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324], [0.4718517627980974, 0.4902251689616832, 0.5549523261866275, 0.907874701331254, 0.4176047824455843, 21.448865222845452, 1289.172]]
Round_9_results:  [0.4718517627980974, 0.4902251689616832, 0.5549523261866275, 0.907874701331254, 0.4176047824455843, 21.448865222845452, 1289.172]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2164 < 2165; dropping {'Training_Loss': 0.0771620104194812, 'Validation_Loss': 0.2553883012798097, 'Training_R2': 0.9223218562862117, 'Validation_R2': 0.7624733704701302, 'Training_F1': 0.8477659951138833, 'Validation_F1': 0.7306708723126404, 'Training_NEP': 0.304364572945913, 'Validation_NEP': 0.5437768112679541, 'Training_NDE': 0.05831436127512889, 'Validation_NDE': 0.18915368675281852, 'Training_MAE': 10.079951035930945, 'Validation_MAE': 14.912616156762645, 'Training_MSE': 256.5741, 'Validation_MSE': 698.53937}.
trigger times: 0
Loss after 276687060 batches: 0.0772
trigger times: 0
Loss after 276818160 batches: 0.0208
trigger times: 0
Loss after 276949260 batches: 0.0164
trigger times: 0
Loss after 277080360 batches: 0.0141
trigger times: 0
Loss after 277211460 batches: 0.0132
trigger times: 1
Loss after 277342560 batches: 0.0124
trigger times: 2
Loss after 277473660 batches: 0.0118
trigger times: 0
Loss after 277604760 batches: 0.0114
trigger times: 1
Loss after 277735860 batches: 0.0111
trigger times: 2
Loss after 277866960 batches: 0.0109
trigger times: 3
Loss after 277998060 batches: 0.0104
trigger times: 4
Loss after 278129160 batches: 0.0103
trigger times: 5
Loss after 278260260 batches: 0.0103
trigger times: 6
Loss after 278391360 batches: 0.0099
trigger times: 0
Loss after 278522460 batches: 0.0101
trigger times: 1
Loss after 278653560 batches: 0.0100
trigger times: 2
Loss after 278784660 batches: 0.0095
trigger times: 3
Loss after 278915760 batches: 0.0093
trigger times: 4
Loss after 279046860 batches: 0.0093
trigger times: 5
Loss after 279177960 batches: 0.0091
trigger times: 6
Loss after 279309060 batches: 0.0092
trigger times: 7
Loss after 279440160 batches: 0.0089
trigger times: 8
Loss after 279571260 batches: 0.0091
trigger times: 0
Loss after 279702360 batches: 0.0089
trigger times: 1
Loss after 279833460 batches: 0.0086
trigger times: 2
Loss after 279964560 batches: 0.0087
trigger times: 3
Loss after 280095660 batches: 0.0085
trigger times: 4
Loss after 280226760 batches: 0.0087
trigger times: 5
Loss after 280357860 batches: 0.0083
trigger times: 6
Loss after 280488960 batches: 0.0084
trigger times: 7
Loss after 280620060 batches: 0.0086
trigger times: 8
Loss after 280751160 batches: 0.0084
trigger times: 9
Loss after 280882260 batches: 0.0082
trigger times: 10
Loss after 281013360 batches: 0.0081
trigger times: 11
Loss after 281144460 batches: 0.0083
trigger times: 12
Loss after 281275560 batches: 0.0083
trigger times: 13
Loss after 281406660 batches: 0.0081
trigger times: 14
Loss after 281537760 batches: 0.0084
trigger times: 15
Loss after 281668860 batches: 0.0082
trigger times: 16
Loss after 281799960 batches: 0.0081
trigger times: 17
Loss after 281931060 batches: 0.0082
trigger times: 18
Loss after 282062160 batches: 0.0077
trigger times: 19
Loss after 282193260 batches: 0.0078
trigger times: 20
Early stopping!
Start to test process.
Loss after 282324360 batches: 0.0078
Time to train on one home:  325.93443417549133
trigger times: 0
Loss after 282426960 batches: 0.2140
trigger times: 0
Loss after 282529560 batches: 0.0696
trigger times: 1
Loss after 282632160 batches: 0.0484
trigger times: 2
Loss after 282734760 batches: 0.0384
trigger times: 3
Loss after 282837360 batches: 0.0349
trigger times: 4
Loss after 282939960 batches: 0.0329
trigger times: 5
Loss after 283042560 batches: 0.0331
trigger times: 6
Loss after 283145160 batches: 0.0291
trigger times: 7
Loss after 283247760 batches: 0.0279
trigger times: 8
Loss after 283350360 batches: 0.0269
trigger times: 9
Loss after 283452960 batches: 0.0256
trigger times: 10
Loss after 283555560 batches: 0.0253
trigger times: 11
Loss after 283658160 batches: 0.0246
trigger times: 12
Loss after 283760760 batches: 0.0257
trigger times: 13
Loss after 283863360 batches: 0.0279
trigger times: 14
Loss after 283965960 batches: 0.0244
trigger times: 15
Loss after 284068560 batches: 0.0228
trigger times: 16
Loss after 284171160 batches: 0.0223
trigger times: 17
Loss after 284273760 batches: 0.0216
trigger times: 18
Loss after 284376360 batches: 0.0217
trigger times: 19
Loss after 284478960 batches: 0.0224
trigger times: 20
Early stopping!
Start to test process.
Loss after 284581560 batches: 0.0217
Time to train on one home:  139.42564916610718
trigger times: 0
Loss after 284712660 batches: 0.0692
trigger times: 0
Loss after 284843760 batches: 0.0261
trigger times: 1
Loss after 284974860 batches: 0.0203
trigger times: 2
Loss after 285105960 batches: 0.0183
trigger times: 3
Loss after 285237060 batches: 0.0171
trigger times: 4
Loss after 285368160 batches: 0.0163
trigger times: 5
Loss after 285499260 batches: 0.0155
trigger times: 6
Loss after 285630360 batches: 0.0148
trigger times: 7
Loss after 285761460 batches: 0.0147
trigger times: 8
Loss after 285892560 batches: 0.0141
trigger times: 9
Loss after 286023660 batches: 0.0139
trigger times: 10
Loss after 286154760 batches: 0.0136
trigger times: 0
Loss after 286285860 batches: 0.0135
trigger times: 1
Loss after 286416960 batches: 0.0132
trigger times: 2
Loss after 286548060 batches: 0.0131
trigger times: 3
Loss after 286679160 batches: 0.0129
trigger times: 4
Loss after 286810260 batches: 0.0128
trigger times: 5
Loss after 286941360 batches: 0.0125
trigger times: 6
Loss after 287072460 batches: 0.0127
trigger times: 7
Loss after 287203560 batches: 0.0125
trigger times: 8
Loss after 287334660 batches: 0.0122
trigger times: 0
Loss after 287465760 batches: 0.0122
trigger times: 1
Loss after 287596860 batches: 0.0120
trigger times: 2
Loss after 287727960 batches: 0.0118
trigger times: 3
Loss after 287859060 batches: 0.0120
trigger times: 4
Loss after 287990160 batches: 0.0118
trigger times: 5
Loss after 288121260 batches: 0.0115
trigger times: 6
Loss after 288252360 batches: 0.0115
trigger times: 7
Loss after 288383460 batches: 0.0116
trigger times: 8
Loss after 288514560 batches: 0.0116
trigger times: 9
Loss after 288645660 batches: 0.0117
trigger times: 10
Loss after 288776760 batches: 0.0113
trigger times: 11
Loss after 288907860 batches: 0.0115
trigger times: 12
Loss after 289038960 batches: 0.0113
trigger times: 13
Loss after 289170060 batches: 0.0113
trigger times: 14
Loss after 289301160 batches: 0.0111
trigger times: 0
Loss after 289432260 batches: 0.0111
trigger times: 1
Loss after 289563360 batches: 0.0110
trigger times: 2
Loss after 289694460 batches: 0.0109
trigger times: 3
Loss after 289825560 batches: 0.0110
trigger times: 4
Loss after 289956660 batches: 0.0110
trigger times: 5
Loss after 290087760 batches: 0.0109
trigger times: 6
Loss after 290218860 batches: 0.0107
trigger times: 7
Loss after 290349960 batches: 0.0107
trigger times: 0
Loss after 290481060 batches: 0.0107
trigger times: 1
Loss after 290612160 batches: 0.0105
trigger times: 2
Loss after 290743260 batches: 0.0105
trigger times: 3
Loss after 290874360 batches: 0.0106
trigger times: 4
Loss after 291005460 batches: 0.0106
trigger times: 5
Loss after 291136560 batches: 0.0104
trigger times: 6
Loss after 291267660 batches: 0.0105
trigger times: 7
Loss after 291398760 batches: 0.0103
trigger times: 0
Loss after 291529860 batches: 0.0103
trigger times: 1
Loss after 291660960 batches: 0.0102
trigger times: 2
Loss after 291792060 batches: 0.0103
trigger times: 3
Loss after 291923160 batches: 0.0102
trigger times: 4
Loss after 292054260 batches: 0.0101
trigger times: 5
Loss after 292185360 batches: 0.0101
trigger times: 6
Loss after 292316460 batches: 0.0102
trigger times: 7
Loss after 292447560 batches: 0.0101
trigger times: 8
Loss after 292578660 batches: 0.0100
trigger times: 9
Loss after 292709760 batches: 0.0099
trigger times: 10
Loss after 292840860 batches: 0.0099
trigger times: 11
Loss after 292971960 batches: 0.0098
trigger times: 12
Loss after 293103060 batches: 0.0098
trigger times: 13
Loss after 293234160 batches: 0.0097
trigger times: 14
Loss after 293365260 batches: 0.0099
trigger times: 15
Loss after 293496360 batches: 0.0099
trigger times: 16
Loss after 293627460 batches: 0.0098
trigger times: 17
Loss after 293758560 batches: 0.0097
trigger times: 18
Loss after 293889660 batches: 0.0097
trigger times: 19
Loss after 294020760 batches: 0.0097
trigger times: 20
Early stopping!
Start to test process.
Loss after 294151860 batches: 0.0097
Time to train on one home:  537.438111782074
trigger times: 0
Loss after 294282960 batches: 0.1276
trigger times: 0
Loss after 294414060 batches: 0.0350
trigger times: 1
Loss after 294545160 batches: 0.0273
trigger times: 0
Loss after 294676260 batches: 0.0244
trigger times: 1
Loss after 294807360 batches: 0.0226
trigger times: 2
Loss after 294938460 batches: 0.0217
trigger times: 0
Loss after 295069560 batches: 0.0211
trigger times: 0
Loss after 295200660 batches: 0.0204
trigger times: 1
Loss after 295331760 batches: 0.0196
trigger times: 2
Loss after 295462860 batches: 0.0192
trigger times: 3
Loss after 295593960 batches: 0.0190
trigger times: 4
Loss after 295725060 batches: 0.0184
trigger times: 5
Loss after 295856160 batches: 0.0181
trigger times: 0
Loss after 295987260 batches: 0.0182
trigger times: 1
Loss after 296118360 batches: 0.0176
trigger times: 2
Loss after 296249460 batches: 0.0176
trigger times: 0
Loss after 296380560 batches: 0.0173
trigger times: 1
Loss after 296511660 batches: 0.0172
trigger times: 0
Loss after 296642760 batches: 0.0171
trigger times: 1
Loss after 296773860 batches: 0.0169
trigger times: 2
Loss after 296904960 batches: 0.0167
trigger times: 3
Loss after 297036060 batches: 0.0164
trigger times: 4
Loss after 297167160 batches: 0.0165
trigger times: 5
Loss after 297298260 batches: 0.0163
trigger times: 6
Loss after 297429360 batches: 0.0163
trigger times: 7
Loss after 297560460 batches: 0.0164
trigger times: 8
Loss after 297691560 batches: 0.0163
trigger times: 9
Loss after 297822660 batches: 0.0161
trigger times: 0
Loss after 297953760 batches: 0.0161
trigger times: 1
Loss after 298084860 batches: 0.0159
trigger times: 2
Loss after 298215960 batches: 0.0157
trigger times: 3
Loss after 298347060 batches: 0.0158
trigger times: 4
Loss after 298478160 batches: 0.0157
trigger times: 5
Loss after 298609260 batches: 0.0156
trigger times: 6
Loss after 298740360 batches: 0.0153
trigger times: 7
Loss after 298871460 batches: 0.0155
trigger times: 8
Loss after 299002560 batches: 0.0155
trigger times: 9
Loss after 299133660 batches: 0.0152
trigger times: 10
Loss after 299264760 batches: 0.0153
trigger times: 11
Loss after 299395860 batches: 0.0152
trigger times: 12
Loss after 299526960 batches: 0.0154
trigger times: 13
Loss after 299658060 batches: 0.0153
trigger times: 14
Loss after 299789160 batches: 0.0150
trigger times: 15
Loss after 299920260 batches: 0.0151
trigger times: 16
Loss after 300051360 batches: 0.0150
trigger times: 17
Loss after 300182460 batches: 0.0150
trigger times: 18
Loss after 300313560 batches: 0.0151
trigger times: 0
Loss after 300444660 batches: 0.0145
trigger times: 1
Loss after 300575760 batches: 0.0145
trigger times: 2
Loss after 300706860 batches: 0.0149
trigger times: 3
Loss after 300837960 batches: 0.0147
trigger times: 4
Loss after 300969060 batches: 0.0144
trigger times: 5
Loss after 301100160 batches: 0.0146
trigger times: 6
Loss after 301231260 batches: 0.0145
trigger times: 7
Loss after 301362360 batches: 0.0143
trigger times: 8
Loss after 301493460 batches: 0.0147
trigger times: 9
Loss after 301624560 batches: 0.0145
trigger times: 10
Loss after 301755660 batches: 0.0142
trigger times: 11
Loss after 301886760 batches: 0.0140
trigger times: 12
Loss after 302017860 batches: 0.0143
trigger times: 13
Loss after 302148960 batches: 0.0141
trigger times: 14
Loss after 302280060 batches: 0.0141
trigger times: 0
Loss after 302411160 batches: 0.0140
trigger times: 1
Loss after 302542260 batches: 0.0139
trigger times: 2
Loss after 302673360 batches: 0.0141
trigger times: 3
Loss after 302804460 batches: 0.0140
trigger times: 0
Loss after 302935560 batches: 0.0139
trigger times: 1
Loss after 303066660 batches: 0.0141
trigger times: 2
Loss after 303197760 batches: 0.0141
trigger times: 3
Loss after 303328860 batches: 0.0138
trigger times: 4
Loss after 303459960 batches: 0.0138
trigger times: 5
Loss after 303591060 batches: 0.0138
trigger times: 6
Loss after 303722160 batches: 0.0137
trigger times: 7
Loss after 303853260 batches: 0.0137
trigger times: 8
Loss after 303984360 batches: 0.0138
trigger times: 9
Loss after 304115460 batches: 0.0135
trigger times: 10
Loss after 304246560 batches: 0.0139
trigger times: 11
Loss after 304377660 batches: 0.0139
trigger times: 12
Loss after 304508760 batches: 0.0135
trigger times: 13
Loss after 304639860 batches: 0.0136
trigger times: 14
Loss after 304770960 batches: 0.0135
trigger times: 0
Loss after 304902060 batches: 0.0134
trigger times: 1
Loss after 305033160 batches: 0.0134
trigger times: 2
Loss after 305164260 batches: 0.0135
trigger times: 3
Loss after 305295360 batches: 0.0135
trigger times: 4
Loss after 305426460 batches: 0.0134
trigger times: 5
Loss after 305557560 batches: 0.0133
trigger times: 6
Loss after 305688660 batches: 0.0136
trigger times: 7
Loss after 305819760 batches: 0.0131
trigger times: 8
Loss after 305950860 batches: 0.0133
trigger times: 9
Loss after 306081960 batches: 0.0132
trigger times: 10
Loss after 306213060 batches: 0.0133
trigger times: 11
Loss after 306344160 batches: 0.0133
trigger times: 12
Loss after 306475260 batches: 0.0131
trigger times: 13
Loss after 306606360 batches: 0.0133
trigger times: 14
Loss after 306737460 batches: 0.0131
trigger times: 15
Loss after 306868560 batches: 0.0133
trigger times: 16
Loss after 306999660 batches: 0.0133
trigger times: 17
Loss after 307130760 batches: 0.0130
trigger times: 18
Loss after 307261860 batches: 0.0130
trigger times: 19
Loss after 307392960 batches: 0.0130
trigger times: 20
Early stopping!
Start to test process.
Loss after 307524060 batches: 0.0130
Time to train on one home:  761.8622560501099
trigger times: 0
Loss after 307652700 batches: 0.0870
trigger times: 1
Loss after 307781340 batches: 0.0259
trigger times: 2
Loss after 307909980 batches: 0.0205
trigger times: 3
Loss after 308038620 batches: 0.0182
trigger times: 4
Loss after 308167260 batches: 0.0166
trigger times: 5
Loss after 308295900 batches: 0.0160
trigger times: 6
Loss after 308424540 batches: 0.0155
trigger times: 7
Loss after 308553180 batches: 0.0147
trigger times: 8
Loss after 308681820 batches: 0.0145
trigger times: 0
Loss after 308810460 batches: 0.0139
trigger times: 1
Loss after 308939100 batches: 0.0137
trigger times: 2
Loss after 309067740 batches: 0.0133
trigger times: 3
Loss after 309196380 batches: 0.0128
trigger times: 4
Loss after 309325020 batches: 0.0128
trigger times: 5
Loss after 309453660 batches: 0.0125
trigger times: 6
Loss after 309582300 batches: 0.0124
trigger times: 7
Loss after 309710940 batches: 0.0120
trigger times: 8
Loss after 309839580 batches: 0.0120
trigger times: 9
Loss after 309968220 batches: 0.0118
trigger times: 10
Loss after 310096860 batches: 0.0116
trigger times: 11
Loss after 310225500 batches: 0.0116
trigger times: 12
Loss after 310354140 batches: 0.0114
trigger times: 13
Loss after 310482780 batches: 0.0116
trigger times: 14
Loss after 310611420 batches: 0.0113
trigger times: 15
Loss after 310740060 batches: 0.0112
trigger times: 16
Loss after 310868700 batches: 0.0112
trigger times: 17
Loss after 310997340 batches: 0.0111
trigger times: 18
Loss after 311125980 batches: 0.0109
trigger times: 19
Loss after 311254620 batches: 0.0108
trigger times: 20
Early stopping!
Start to test process.
Loss after 311383260 batches: 0.0107
Time to train on one home:  237.21109700202942
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765, 0.013892934501727369, 0.013666319778952196, 0.012580597168674625]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806], [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324], [0.4718517627980974, 0.4902251689616832, 0.5549523261866275, 0.907874701331254, 0.4176047824455843, 21.448865222845452, 1289.172], [0.4761729836463928, 0.48556449560991066, 0.5560194037302575, 0.9166221286580782, 0.42142278082960144, 21.655526328727763, 1300.9584]]
Round_10_results:  [0.4761729836463928, 0.48556449560991066, 0.5560194037302575, 0.9166221286580782, 0.42142278082960144, 21.655526328727763, 1300.9584]
trigger times: 0
Loss after 311514360 batches: 0.0720
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2435 < 2436; dropping {'Training_Loss': 0.07195380762360006, 'Validation_Loss': 0.25291605293750763, 'Training_R2': 0.9274984367122004, 'Validation_R2': 0.7647649763608134, 'Training_F1': 0.8538768636290722, 'Validation_F1': 0.7277980422246711, 'Training_NEP': 0.29227042324603003, 'Validation_NEP': 0.5333187762801689, 'Training_NDE': 0.05442821046489396, 'Validation_NDE': 0.18732877262144249, 'Training_MAE': 9.679416783156057, 'Validation_MAE': 14.6258134496682, 'Training_MSE': 239.47559, 'Validation_MSE': 691.8}.
trigger times: 0
Loss after 311645460 batches: 0.0197
trigger times: 0
Loss after 311776560 batches: 0.0155
trigger times: 0
Loss after 311907660 batches: 0.0138
trigger times: 1
Loss after 312038760 batches: 0.0127
trigger times: 0
Loss after 312169860 batches: 0.0119
trigger times: 0
Loss after 312300960 batches: 0.0115
trigger times: 1
Loss after 312432060 batches: 0.0111
trigger times: 2
Loss after 312563160 batches: 0.0108
trigger times: 0
Loss after 312694260 batches: 0.0102
trigger times: 1
Loss after 312825360 batches: 0.0101
trigger times: 2
Loss after 312956460 batches: 0.0099
trigger times: 3
Loss after 313087560 batches: 0.0099
trigger times: 0
Loss after 313218660 batches: 0.0097
trigger times: 1
Loss after 313349760 batches: 0.0095
trigger times: 2
Loss after 313480860 batches: 0.0093
trigger times: 3
Loss after 313611960 batches: 0.0092
trigger times: 4
Loss after 313743060 batches: 0.0089
trigger times: 5
Loss after 313874160 batches: 0.0091
trigger times: 6
Loss after 314005260 batches: 0.0089
trigger times: 7
Loss after 314136360 batches: 0.0088
trigger times: 0
Loss after 314267460 batches: 0.0088
trigger times: 1
Loss after 314398560 batches: 0.0088
trigger times: 2
Loss after 314529660 batches: 0.0085
trigger times: 3
Loss after 314660760 batches: 0.0084
trigger times: 4
Loss after 314791860 batches: 0.0084
trigger times: 5
Loss after 314922960 batches: 0.0085
trigger times: 0
Loss after 315054060 batches: 0.0084
trigger times: 1
Loss after 315185160 batches: 0.0082
trigger times: 2
Loss after 315316260 batches: 0.0081
trigger times: 3
Loss after 315447360 batches: 0.0080
trigger times: 0
Loss after 315578460 batches: 0.0080
trigger times: 1
Loss after 315709560 batches: 0.0079
trigger times: 2
Loss after 315840660 batches: 0.0079
trigger times: 3
Loss after 315971760 batches: 0.0080
trigger times: 4
Loss after 316102860 batches: 0.0078
trigger times: 5
Loss after 316233960 batches: 0.0080
trigger times: 6
Loss after 316365060 batches: 0.0078
trigger times: 7
Loss after 316496160 batches: 0.0078
trigger times: 8
Loss after 316627260 batches: 0.0078
trigger times: 9
Loss after 316758360 batches: 0.0077
trigger times: 10
Loss after 316889460 batches: 0.0078
trigger times: 11
Loss after 317020560 batches: 0.0076
trigger times: 12
Loss after 317151660 batches: 0.0074
trigger times: 13
Loss after 317282760 batches: 0.0075
trigger times: 14
Loss after 317413860 batches: 0.0073
trigger times: 15
Loss after 317544960 batches: 0.0072
trigger times: 16
Loss after 317676060 batches: 0.0074
trigger times: 0
Loss after 317807160 batches: 0.0074
trigger times: 0
Loss after 317938260 batches: 0.0073
trigger times: 1
Loss after 318069360 batches: 0.0073
trigger times: 2
Loss after 318200460 batches: 0.0073
trigger times: 3
Loss after 318331560 batches: 0.0072
trigger times: 4
Loss after 318462660 batches: 0.0072
trigger times: 5
Loss after 318593760 batches: 0.0072
trigger times: 6
Loss after 318724860 batches: 0.0070
trigger times: 7
Loss after 318855960 batches: 0.0070
trigger times: 8
Loss after 318987060 batches: 0.0070
trigger times: 9
Loss after 319118160 batches: 0.0070
trigger times: 10
Loss after 319249260 batches: 0.0068
trigger times: 11
Loss after 319380360 batches: 0.0069
trigger times: 12
Loss after 319511460 batches: 0.0069
trigger times: 13
Loss after 319642560 batches: 0.0070
trigger times: 14
Loss after 319773660 batches: 0.0069
trigger times: 15
Loss after 319904760 batches: 0.0068
trigger times: 16
Loss after 320035860 batches: 0.0066
trigger times: 17
Loss after 320166960 batches: 0.0067
trigger times: 18
Loss after 320298060 batches: 0.0066
trigger times: 19
Loss after 320429160 batches: 0.0068
trigger times: 20
Early stopping!
Start to test process.
Loss after 320560260 batches: 0.0066
Time to train on one home:  526.2600786685944
trigger times: 0
Loss after 320662860 batches: 0.2273
trigger times: 0
Loss after 320765460 batches: 0.0651
trigger times: 1
Loss after 320868060 batches: 0.0443
trigger times: 2
Loss after 320970660 batches: 0.0385
trigger times: 0
Loss after 321073260 batches: 0.0345
trigger times: 1
Loss after 321175860 batches: 0.0316
trigger times: 2
Loss after 321278460 batches: 0.0295
trigger times: 3
Loss after 321381060 batches: 0.0292
trigger times: 4
Loss after 321483660 batches: 0.0321
trigger times: 5
Loss after 321586260 batches: 0.0293
trigger times: 6
Loss after 321688860 batches: 0.0274
trigger times: 7
Loss after 321791460 batches: 0.0253
trigger times: 8
Loss after 321894060 batches: 0.0256
trigger times: 9
Loss after 321996660 batches: 0.0241
trigger times: 10
Loss after 322099260 batches: 0.0233
trigger times: 11
Loss after 322201860 batches: 0.0223
trigger times: 12
Loss after 322304460 batches: 0.0220
trigger times: 13
Loss after 322407060 batches: 0.0212
trigger times: 14
Loss after 322509660 batches: 0.0232
trigger times: 15
Loss after 322612260 batches: 0.0233
trigger times: 16
Loss after 322714860 batches: 0.0230
trigger times: 17
Loss after 322817460 batches: 0.0224
trigger times: 18
Loss after 322920060 batches: 0.0231
trigger times: 19
Loss after 323022660 batches: 0.0205
trigger times: 20
Early stopping!
Start to test process.
Loss after 323125260 batches: 0.0200
Time to train on one home:  163.30034756660461
trigger times: 0
Loss after 323256360 batches: 0.0597
trigger times: 0
Loss after 323387460 batches: 0.0241
trigger times: 0
Loss after 323518560 batches: 0.0191
trigger times: 1
Loss after 323649660 batches: 0.0175
trigger times: 2
Loss after 323780760 batches: 0.0162
trigger times: 3
Loss after 323911860 batches: 0.0157
trigger times: 4
Loss after 324042960 batches: 0.0149
trigger times: 0
Loss after 324174060 batches: 0.0142
trigger times: 1
Loss after 324305160 batches: 0.0140
trigger times: 2
Loss after 324436260 batches: 0.0136
trigger times: 0
Loss after 324567360 batches: 0.0133
trigger times: 1
Loss after 324698460 batches: 0.0129
trigger times: 2
Loss after 324829560 batches: 0.0129
trigger times: 0
Loss after 324960660 batches: 0.0127
trigger times: 1
Loss after 325091760 batches: 0.0125
trigger times: 2
Loss after 325222860 batches: 0.0123
trigger times: 3
Loss after 325353960 batches: 0.0121
trigger times: 4
Loss after 325485060 batches: 0.0119
trigger times: 5
Loss after 325616160 batches: 0.0119
trigger times: 6
Loss after 325747260 batches: 0.0118
trigger times: 7
Loss after 325878360 batches: 0.0117
trigger times: 8
Loss after 326009460 batches: 0.0117
trigger times: 9
Loss after 326140560 batches: 0.0114
trigger times: 0
Loss after 326271660 batches: 0.0114
trigger times: 1
Loss after 326402760 batches: 0.0113
trigger times: 2
Loss after 326533860 batches: 0.0112
trigger times: 3
Loss after 326664960 batches: 0.0112
trigger times: 4
Loss after 326796060 batches: 0.0111
trigger times: 5
Loss after 326927160 batches: 0.0109
trigger times: 6
Loss after 327058260 batches: 0.0110
trigger times: 7
Loss after 327189360 batches: 0.0109
trigger times: 8
Loss after 327320460 batches: 0.0109
trigger times: 9
Loss after 327451560 batches: 0.0107
trigger times: 10
Loss after 327582660 batches: 0.0106
trigger times: 11
Loss after 327713760 batches: 0.0105
trigger times: 12
Loss after 327844860 batches: 0.0106
trigger times: 13
Loss after 327975960 batches: 0.0104
trigger times: 14
Loss after 328107060 batches: 0.0105
trigger times: 15
Loss after 328238160 batches: 0.0105
trigger times: 16
Loss after 328369260 batches: 0.0105
trigger times: 17
Loss after 328500360 batches: 0.0105
trigger times: 18
Loss after 328631460 batches: 0.0102
trigger times: 19
Loss after 328762560 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 328893660 batches: 0.0103
Time to train on one home:  325.4842174053192
trigger times: 0
Loss after 329024760 batches: 0.1185
trigger times: 0
Loss after 329155860 batches: 0.0332
trigger times: 1
Loss after 329286960 batches: 0.0253
trigger times: 2
Loss after 329418060 batches: 0.0228
trigger times: 0
Loss after 329549160 batches: 0.0208
trigger times: 0
Loss after 329680260 batches: 0.0202
trigger times: 1
Loss after 329811360 batches: 0.0199
trigger times: 2
Loss after 329942460 batches: 0.0193
trigger times: 3
Loss after 330073560 batches: 0.0187
trigger times: 4
Loss after 330204660 batches: 0.0183
trigger times: 5
Loss after 330335760 batches: 0.0178
trigger times: 6
Loss after 330466860 batches: 0.0174
trigger times: 7
Loss after 330597960 batches: 0.0172
trigger times: 8
Loss after 330729060 batches: 0.0172
trigger times: 9
Loss after 330860160 batches: 0.0170
trigger times: 10
Loss after 330991260 batches: 0.0166
trigger times: 11
Loss after 331122360 batches: 0.0167
trigger times: 12
Loss after 331253460 batches: 0.0168
trigger times: 13
Loss after 331384560 batches: 0.0166
trigger times: 14
Loss after 331515660 batches: 0.0161
trigger times: 15
Loss after 331646760 batches: 0.0160
trigger times: 16
Loss after 331777860 batches: 0.0160
trigger times: 17
Loss after 331908960 batches: 0.0158
trigger times: 18
Loss after 332040060 batches: 0.0158
trigger times: 19
Loss after 332171160 batches: 0.0157
trigger times: 20
Early stopping!
Start to test process.
Loss after 332302260 batches: 0.0155
Time to train on one home:  199.1852729320526
trigger times: 0
Loss after 332430900 batches: 0.0815
trigger times: 0
Loss after 332559540 batches: 0.0248
trigger times: 1
Loss after 332688180 batches: 0.0192
trigger times: 2
Loss after 332816820 batches: 0.0173
trigger times: 3
Loss after 332945460 batches: 0.0161
trigger times: 4
Loss after 333074100 batches: 0.0154
trigger times: 5
Loss after 333202740 batches: 0.0149
trigger times: 6
Loss after 333331380 batches: 0.0141
trigger times: 7
Loss after 333460020 batches: 0.0137
trigger times: 8
Loss after 333588660 batches: 0.0133
trigger times: 9
Loss after 333717300 batches: 0.0131
trigger times: 10
Loss after 333845940 batches: 0.0129
trigger times: 11
Loss after 333974580 batches: 0.0125
trigger times: 12
Loss after 334103220 batches: 0.0122
trigger times: 13
Loss after 334231860 batches: 0.0122
trigger times: 14
Loss after 334360500 batches: 0.0120
trigger times: 15
Loss after 334489140 batches: 0.0118
trigger times: 16
Loss after 334617780 batches: 0.0117
trigger times: 17
Loss after 334746420 batches: 0.0114
trigger times: 18
Loss after 334875060 batches: 0.0115
trigger times: 19
Loss after 335003700 batches: 0.0114
trigger times: 20
Early stopping!
Start to test process.
Loss after 335132340 batches: 0.0111
Time to train on one home:  168.31943655014038
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765, 0.013892934501727369, 0.013666319778952196, 0.012580597168674625, 0.012713179911946834]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806], [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324], [0.4718517627980974, 0.4902251689616832, 0.5549523261866275, 0.907874701331254, 0.4176047824455843, 21.448865222845452, 1289.172], [0.4761729836463928, 0.48556449560991066, 0.5560194037302575, 0.9166221286580782, 0.42142278082960144, 21.655526328727763, 1300.9584], [0.46398472951518166, 0.49880328154927556, 0.5698825241915405, 0.8946974979237889, 0.41057763904260586, 21.137549069320855, 1267.4788]]
Round_11_results:  [0.46398472951518166, 0.49880328154927556, 0.5698825241915405, 0.8946974979237889, 0.41057763904260586, 21.137549069320855, 1267.4788]
trigger times: 0
Loss after 335263440 batches: 0.0875
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2622 < 2623; dropping {'Training_Loss': 0.08749303119784256, 'Validation_Loss': 0.2488766825861401, 'Training_R2': 0.9118977153002366, 'Validation_R2': 0.768444481472959, 'Training_F1': 0.8368989521577533, 'Validation_F1': 0.729305167803226, 'Training_NEP': 0.3265880637199962, 'Validation_NEP': 0.5424996096185607, 'Training_NDE': 0.0661399489420894, 'Validation_NDE': 0.18439860871196534, 'Training_MAE': 10.815948976433795, 'Validation_MAE': 14.877589988751224, 'Training_MSE': 291.00546, 'Validation_MSE': 680.979}.
trigger times: 0
Loss after 335394540 batches: 0.0228
trigger times: 1
Loss after 335525640 batches: 0.0171
trigger times: 2
Loss after 335656740 batches: 0.0145
trigger times: 3
Loss after 335787840 batches: 0.0132
trigger times: 0
Loss after 335918940 batches: 0.0126
trigger times: 1
Loss after 336050040 batches: 0.0117
trigger times: 2
Loss after 336181140 batches: 0.0115
trigger times: 3
Loss after 336312240 batches: 0.0108
trigger times: 4
Loss after 336443340 batches: 0.0107
trigger times: 5
Loss after 336574440 batches: 0.0106
trigger times: 6
Loss after 336705540 batches: 0.0102
trigger times: 7
Loss after 336836640 batches: 0.0098
trigger times: 8
Loss after 336967740 batches: 0.0096
trigger times: 9
Loss after 337098840 batches: 0.0095
trigger times: 10
Loss after 337229940 batches: 0.0095
trigger times: 0
Loss after 337361040 batches: 0.0093
trigger times: 0
Loss after 337492140 batches: 0.0090
trigger times: 1
Loss after 337623240 batches: 0.0089
trigger times: 2
Loss after 337754340 batches: 0.0091
trigger times: 0
Loss after 337885440 batches: 0.0089
trigger times: 1
Loss after 338016540 batches: 0.0087
trigger times: 2
Loss after 338147640 batches: 0.0085
trigger times: 3
Loss after 338278740 batches: 0.0085
trigger times: 0
Loss after 338409840 batches: 0.0084
trigger times: 1
Loss after 338540940 batches: 0.0084
trigger times: 2
Loss after 338672040 batches: 0.0082
trigger times: 3
Loss after 338803140 batches: 0.0081
trigger times: 4
Loss after 338934240 batches: 0.0080
trigger times: 5
Loss after 339065340 batches: 0.0082
trigger times: 6
Loss after 339196440 batches: 0.0080
trigger times: 7
Loss after 339327540 batches: 0.0080
trigger times: 8
Loss after 339458640 batches: 0.0079
trigger times: 9
Loss after 339589740 batches: 0.0079
trigger times: 10
Loss after 339720840 batches: 0.0078
trigger times: 11
Loss after 339851940 batches: 0.0078
trigger times: 12
Loss after 339983040 batches: 0.0078
trigger times: 13
Loss after 340114140 batches: 0.0078
trigger times: 14
Loss after 340245240 batches: 0.0078
trigger times: 15
Loss after 340376340 batches: 0.0075
trigger times: 16
Loss after 340507440 batches: 0.0076
trigger times: 17
Loss after 340638540 batches: 0.0077
trigger times: 0
Loss after 340769640 batches: 0.0077
trigger times: 1
Loss after 340900740 batches: 0.0075
trigger times: 2
Loss after 341031840 batches: 0.0074
trigger times: 3
Loss after 341162940 batches: 0.0073
trigger times: 4
Loss after 341294040 batches: 0.0072
trigger times: 5
Loss after 341425140 batches: 0.0075
trigger times: 6
Loss after 341556240 batches: 0.0073
trigger times: 7
Loss after 341687340 batches: 0.0073
trigger times: 8
Loss after 341818440 batches: 0.0072
trigger times: 9
Loss after 341949540 batches: 0.0071
trigger times: 10
Loss after 342080640 batches: 0.0070
trigger times: 11
Loss after 342211740 batches: 0.0071
trigger times: 12
Loss after 342342840 batches: 0.0073
trigger times: 13
Loss after 342473940 batches: 0.0070
trigger times: 14
Loss after 342605040 batches: 0.0069
trigger times: 15
Loss after 342736140 batches: 0.0070
trigger times: 16
Loss after 342867240 batches: 0.0070
trigger times: 17
Loss after 342998340 batches: 0.0068
trigger times: 18
Loss after 343129440 batches: 0.0070
trigger times: 19
Loss after 343260540 batches: 0.0069
trigger times: 20
Early stopping!
Start to test process.
Loss after 343391640 batches: 0.0067
Time to train on one home:  462.1398763656616
trigger times: 0
Loss after 343494240 batches: 0.1969
trigger times: 1
Loss after 343596840 batches: 0.0583
trigger times: 2
Loss after 343699440 batches: 0.0426
trigger times: 3
Loss after 343802040 batches: 0.0395
trigger times: 4
Loss after 343904640 batches: 0.0325
trigger times: 5
Loss after 344007240 batches: 0.0297
trigger times: 6
Loss after 344109840 batches: 0.0287
trigger times: 7
Loss after 344212440 batches: 0.0277
trigger times: 8
Loss after 344315040 batches: 0.0272
trigger times: 9
Loss after 344417640 batches: 0.0265
trigger times: 10
Loss after 344520240 batches: 0.0262
trigger times: 11
Loss after 344622840 batches: 0.0244
trigger times: 12
Loss after 344725440 batches: 0.0241
trigger times: 13
Loss after 344828040 batches: 0.0231
trigger times: 14
Loss after 344930640 batches: 0.0214
trigger times: 15
Loss after 345033240 batches: 0.0214
trigger times: 16
Loss after 345135840 batches: 0.0214
trigger times: 17
Loss after 345238440 batches: 0.0204
trigger times: 18
Loss after 345341040 batches: 0.0207
trigger times: 19
Loss after 345443640 batches: 0.0242
trigger times: 20
Early stopping!
Start to test process.
Loss after 345546240 batches: 0.0210
Time to train on one home:  133.46633124351501
trigger times: 0
Loss after 345677340 batches: 0.0600
trigger times: 1
Loss after 345808440 batches: 0.0225
trigger times: 2
Loss after 345939540 batches: 0.0182
trigger times: 3
Loss after 346070640 batches: 0.0167
trigger times: 4
Loss after 346201740 batches: 0.0153
trigger times: 5
Loss after 346332840 batches: 0.0146
trigger times: 0
Loss after 346463940 batches: 0.0139
trigger times: 1
Loss after 346595040 batches: 0.0138
trigger times: 2
Loss after 346726140 batches: 0.0136
trigger times: 3
Loss after 346857240 batches: 0.0131
trigger times: 4
Loss after 346988340 batches: 0.0129
trigger times: 5
Loss after 347119440 batches: 0.0128
trigger times: 6
Loss after 347250540 batches: 0.0125
trigger times: 7
Loss after 347381640 batches: 0.0122
trigger times: 8
Loss after 347512740 batches: 0.0121
trigger times: 9
Loss after 347643840 batches: 0.0119
trigger times: 10
Loss after 347774940 batches: 0.0118
trigger times: 11
Loss after 347906040 batches: 0.0117
trigger times: 12
Loss after 348037140 batches: 0.0117
trigger times: 13
Loss after 348168240 batches: 0.0115
trigger times: 14
Loss after 348299340 batches: 0.0114
trigger times: 15
Loss after 348430440 batches: 0.0113
trigger times: 16
Loss after 348561540 batches: 0.0113
trigger times: 17
Loss after 348692640 batches: 0.0112
trigger times: 18
Loss after 348823740 batches: 0.0110
trigger times: 19
Loss after 348954840 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 349085940 batches: 0.0109
Time to train on one home:  204.64034461975098
trigger times: 0
Loss after 349217040 batches: 0.1224
trigger times: 1
Loss after 349348140 batches: 0.0311
trigger times: 0
Loss after 349479240 batches: 0.0254
trigger times: 1
Loss after 349610340 batches: 0.0230
trigger times: 0
Loss after 349741440 batches: 0.0217
trigger times: 1
Loss after 349872540 batches: 0.0205
trigger times: 2
Loss after 350003640 batches: 0.0199
trigger times: 3
Loss after 350134740 batches: 0.0191
trigger times: 4
Loss after 350265840 batches: 0.0187
trigger times: 5
Loss after 350396940 batches: 0.0185
trigger times: 6
Loss after 350528040 batches: 0.0180
trigger times: 7
Loss after 350659140 batches: 0.0177
trigger times: 8
Loss after 350790240 batches: 0.0176
trigger times: 9
Loss after 350921340 batches: 0.0174
trigger times: 0
Loss after 351052440 batches: 0.0170
trigger times: 0
Loss after 351183540 batches: 0.0168
trigger times: 1
Loss after 351314640 batches: 0.0169
trigger times: 2
Loss after 351445740 batches: 0.0167
trigger times: 3
Loss after 351576840 batches: 0.0166
trigger times: 4
Loss after 351707940 batches: 0.0161
trigger times: 5
Loss after 351839040 batches: 0.0162
trigger times: 6
Loss after 351970140 batches: 0.0159
trigger times: 7
Loss after 352101240 batches: 0.0160
trigger times: 0
Loss after 352232340 batches: 0.0156
trigger times: 1
Loss after 352363440 batches: 0.0155
trigger times: 2
Loss after 352494540 batches: 0.0157
trigger times: 3
Loss after 352625640 batches: 0.0156
trigger times: 4
Loss after 352756740 batches: 0.0152
trigger times: 5
Loss after 352887840 batches: 0.0154
trigger times: 6
Loss after 353018940 batches: 0.0153
trigger times: 7
Loss after 353150040 batches: 0.0153
trigger times: 8
Loss after 353281140 batches: 0.0152
trigger times: 9
Loss after 353412240 batches: 0.0149
trigger times: 10
Loss after 353543340 batches: 0.0154
trigger times: 11
Loss after 353674440 batches: 0.0153
trigger times: 0
Loss after 353805540 batches: 0.0152
trigger times: 1
Loss after 353936640 batches: 0.0149
trigger times: 2
Loss after 354067740 batches: 0.0149
trigger times: 3
Loss after 354198840 batches: 0.0148
trigger times: 4
Loss after 354329940 batches: 0.0147
trigger times: 0
Loss after 354461040 batches: 0.0147
trigger times: 1
Loss after 354592140 batches: 0.0147
trigger times: 2
Loss after 354723240 batches: 0.0143
trigger times: 0
Loss after 354854340 batches: 0.0146
trigger times: 1
Loss after 354985440 batches: 0.0146
trigger times: 2
Loss after 355116540 batches: 0.0144
trigger times: 3
Loss after 355247640 batches: 0.0144
trigger times: 4
Loss after 355378740 batches: 0.0142
trigger times: 5
Loss after 355509840 batches: 0.0143
trigger times: 0
Loss after 355640940 batches: 0.0146
trigger times: 1
Loss after 355772040 batches: 0.0142
trigger times: 2
Loss after 355903140 batches: 0.0140
trigger times: 3
Loss after 356034240 batches: 0.0141
trigger times: 4
Loss after 356165340 batches: 0.0140
trigger times: 5
Loss after 356296440 batches: 0.0139
trigger times: 6
Loss after 356427540 batches: 0.0139
trigger times: 7
Loss after 356558640 batches: 0.0139
trigger times: 8
Loss after 356689740 batches: 0.0140
trigger times: 9
Loss after 356820840 batches: 0.0140
trigger times: 10
Loss after 356951940 batches: 0.0137
trigger times: 11
Loss after 357083040 batches: 0.0138
trigger times: 12
Loss after 357214140 batches: 0.0135
trigger times: 13
Loss after 357345240 batches: 0.0135
trigger times: 14
Loss after 357476340 batches: 0.0140
trigger times: 15
Loss after 357607440 batches: 0.0136
trigger times: 16
Loss after 357738540 batches: 0.0135
trigger times: 17
Loss after 357869640 batches: 0.0137
trigger times: 18
Loss after 358000740 batches: 0.0139
trigger times: 19
Loss after 358131840 batches: 0.0134
trigger times: 20
Early stopping!
Start to test process.
Loss after 358262940 batches: 0.0135
Time to train on one home:  510.9793961048126
trigger times: 0
Loss after 358391580 batches: 0.0750
trigger times: 1
Loss after 358520220 batches: 0.0229
trigger times: 0
Loss after 358648860 batches: 0.0184
trigger times: 1
Loss after 358777500 batches: 0.0167
trigger times: 2
Loss after 358906140 batches: 0.0159
trigger times: 3
Loss after 359034780 batches: 0.0152
trigger times: 4
Loss after 359163420 batches: 0.0146
trigger times: 5
Loss after 359292060 batches: 0.0140
trigger times: 6
Loss after 359420700 batches: 0.0135
trigger times: 7
Loss after 359549340 batches: 0.0131
trigger times: 8
Loss after 359677980 batches: 0.0127
trigger times: 9
Loss after 359806620 batches: 0.0125
trigger times: 10
Loss after 359935260 batches: 0.0123
trigger times: 11
Loss after 360063900 batches: 0.0121
trigger times: 12
Loss after 360192540 batches: 0.0117
trigger times: 13
Loss after 360321180 batches: 0.0117
trigger times: 14
Loss after 360449820 batches: 0.0117
trigger times: 15
Loss after 360578460 batches: 0.0115
trigger times: 16
Loss after 360707100 batches: 0.0114
trigger times: 17
Loss after 360835740 batches: 0.0112
trigger times: 18
Loss after 360964380 batches: 0.0108
trigger times: 19
Loss after 361093020 batches: 0.0109
trigger times: 20
Early stopping!
Start to test process.
Loss after 361221660 batches: 0.0108
Time to train on one home:  173.4330711364746
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765, 0.013892934501727369, 0.013666319778952196, 0.012580597168674625, 0.012713179911946834, 0.012576136202707514]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806], [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324], [0.4718517627980974, 0.4902251689616832, 0.5549523261866275, 0.907874701331254, 0.4176047824455843, 21.448865222845452, 1289.172], [0.4761729836463928, 0.48556449560991066, 0.5560194037302575, 0.9166221286580782, 0.42142278082960144, 21.655526328727763, 1300.9584], [0.46398472951518166, 0.49880328154927556, 0.5698825241915405, 0.8946974979237889, 0.41057763904260586, 21.137549069320855, 1267.4788], [0.469333012898763, 0.49303020279751153, 0.5671409183881702, 0.9028353043263873, 0.41530691390943525, 21.329807662365685, 1282.0784]]
Round_12_results:  [0.469333012898763, 0.49303020279751153, 0.5671409183881702, 0.9028353043263873, 0.41530691390943525, 21.329807662365685, 1282.0784]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2826 < 2827; dropping {'Training_Loss': 0.06253824527111818, 'Validation_Loss': 0.22758097449938455, 'Training_R2': 0.9369976568034563, 'Validation_R2': 0.7882847343655629, 'Training_F1': 0.8664869580805543, 'Validation_F1': 0.7386896239885155, 'Training_NEP': 0.2670613516334372, 'Validation_NEP': 0.5192944434203091, 'Training_NDE': 0.04729697733097024, 'Validation_NDE': 0.1685988771695599, 'Training_MAE': 8.844542326327046, 'Validation_MAE': 14.24120805925793, 'Training_MSE': 208.09927, 'Validation_MSE': 622.631}.
trigger times: 0
Loss after 361352760 batches: 0.0625
trigger times: 0
Loss after 361483860 batches: 0.0174
trigger times: 0
Loss after 361614960 batches: 0.0133
trigger times: 0
Loss after 361746060 batches: 0.0121
trigger times: 0
Loss after 361877160 batches: 0.0111
trigger times: 1
Loss after 362008260 batches: 0.0107
trigger times: 2
Loss after 362139360 batches: 0.0102
trigger times: 3
Loss after 362270460 batches: 0.0099
trigger times: 4
Loss after 362401560 batches: 0.0098
trigger times: 5
Loss after 362532660 batches: 0.0094
trigger times: 0
Loss after 362663760 batches: 0.0093
trigger times: 1
Loss after 362794860 batches: 0.0091
trigger times: 2
Loss after 362925960 batches: 0.0090
trigger times: 3
Loss after 363057060 batches: 0.0089
trigger times: 0
Loss after 363188160 batches: 0.0085
trigger times: 1
Loss after 363319260 batches: 0.0087
trigger times: 2
Loss after 363450360 batches: 0.0087
trigger times: 3
Loss after 363581460 batches: 0.0083
trigger times: 4
Loss after 363712560 batches: 0.0082
trigger times: 5
Loss after 363843660 batches: 0.0081
trigger times: 6
Loss after 363974760 batches: 0.0079
trigger times: 7
Loss after 364105860 batches: 0.0079
trigger times: 8
Loss after 364236960 batches: 0.0079
trigger times: 0
Loss after 364368060 batches: 0.0079
trigger times: 1
Loss after 364499160 batches: 0.0076
trigger times: 2
Loss after 364630260 batches: 0.0078
trigger times: 3
Loss after 364761360 batches: 0.0076
trigger times: 4
Loss after 364892460 batches: 0.0077
trigger times: 5
Loss after 365023560 batches: 0.0074
trigger times: 6
Loss after 365154660 batches: 0.0073
trigger times: 7
Loss after 365285760 batches: 0.0077
trigger times: 8
Loss after 365416860 batches: 0.0075
trigger times: 9
Loss after 365547960 batches: 0.0075
trigger times: 10
Loss after 365679060 batches: 0.0074
trigger times: 11
Loss after 365810160 batches: 0.0074
trigger times: 12
Loss after 365941260 batches: 0.0073
trigger times: 13
Loss after 366072360 batches: 0.0072
trigger times: 14
Loss after 366203460 batches: 0.0073
trigger times: 15
Loss after 366334560 batches: 0.0073
trigger times: 16
Loss after 366465660 batches: 0.0072
trigger times: 17
Loss after 366596760 batches: 0.0070
trigger times: 18
Loss after 366727860 batches: 0.0074
trigger times: 19
Loss after 366858960 batches: 0.0072
trigger times: 20
Early stopping!
Start to test process.
Loss after 366990060 batches: 0.0070
Time to train on one home:  325.6253523826599
trigger times: 0
Loss after 367092660 batches: 0.1896
trigger times: 0
Loss after 367195260 batches: 0.0549
trigger times: 1
Loss after 367297860 batches: 0.0412
trigger times: 0
Loss after 367400460 batches: 0.0335
trigger times: 1
Loss after 367503060 batches: 0.0309
trigger times: 2
Loss after 367605660 batches: 0.0289
trigger times: 3
Loss after 367708260 batches: 0.0280
trigger times: 4
Loss after 367810860 batches: 0.0262
trigger times: 5
Loss after 367913460 batches: 0.0248
trigger times: 6
Loss after 368016060 batches: 0.0266
trigger times: 7
Loss after 368118660 batches: 0.0248
trigger times: 8
Loss after 368221260 batches: 0.0239
trigger times: 9
Loss after 368323860 batches: 0.0233
trigger times: 10
Loss after 368426460 batches: 0.0240
trigger times: 11
Loss after 368529060 batches: 0.0234
trigger times: 12
Loss after 368631660 batches: 0.0250
trigger times: 13
Loss after 368734260 batches: 0.0225
trigger times: 14
Loss after 368836860 batches: 0.0209
trigger times: 15
Loss after 368939460 batches: 0.0213
trigger times: 16
Loss after 369042060 batches: 0.0207
trigger times: 17
Loss after 369144660 batches: 0.0210
trigger times: 18
Loss after 369247260 batches: 0.0244
trigger times: 19
Loss after 369349860 batches: 0.0241
trigger times: 20
Early stopping!
Start to test process.
Loss after 369452460 batches: 0.0203
Time to train on one home:  150.45602297782898
trigger times: 0
Loss after 369583560 batches: 0.0642
trigger times: 0
Loss after 369714660 batches: 0.0223
trigger times: 1
Loss after 369845760 batches: 0.0181
trigger times: 0
Loss after 369976860 batches: 0.0164
trigger times: 1
Loss after 370107960 batches: 0.0156
trigger times: 0
Loss after 370239060 batches: 0.0147
trigger times: 1
Loss after 370370160 batches: 0.0140
trigger times: 2
Loss after 370501260 batches: 0.0137
trigger times: 3
Loss after 370632360 batches: 0.0134
trigger times: 4
Loss after 370763460 batches: 0.0133
trigger times: 5
Loss after 370894560 batches: 0.0131
trigger times: 6
Loss after 371025660 batches: 0.0128
trigger times: 7
Loss after 371156760 batches: 0.0128
trigger times: 0
Loss after 371287860 batches: 0.0127
trigger times: 1
Loss after 371418960 batches: 0.0122
trigger times: 2
Loss after 371550060 batches: 0.0121
trigger times: 3
Loss after 371681160 batches: 0.0120
trigger times: 4
Loss after 371812260 batches: 0.0117
trigger times: 5
Loss after 371943360 batches: 0.0119
trigger times: 6
Loss after 372074460 batches: 0.0115
trigger times: 0
Loss after 372205560 batches: 0.0117
trigger times: 1
Loss after 372336660 batches: 0.0115
trigger times: 2
Loss after 372467760 batches: 0.0112
trigger times: 3
Loss after 372598860 batches: 0.0114
trigger times: 4
Loss after 372729960 batches: 0.0111
trigger times: 5
Loss after 372861060 batches: 0.0110
trigger times: 6
Loss after 372992160 batches: 0.0109
trigger times: 7
Loss after 373123260 batches: 0.0109
trigger times: 8
Loss after 373254360 batches: 0.0108
trigger times: 9
Loss after 373385460 batches: 0.0108
trigger times: 10
Loss after 373516560 batches: 0.0107
trigger times: 11
Loss after 373647660 batches: 0.0105
trigger times: 12
Loss after 373778760 batches: 0.0106
trigger times: 13
Loss after 373909860 batches: 0.0107
trigger times: 14
Loss after 374040960 batches: 0.0106
trigger times: 15
Loss after 374172060 batches: 0.0106
trigger times: 16
Loss after 374303160 batches: 0.0105
trigger times: 17
Loss after 374434260 batches: 0.0104
trigger times: 18
Loss after 374565360 batches: 0.0104
trigger times: 19
Loss after 374696460 batches: 0.0103
trigger times: 20
Early stopping!
Start to test process.
Loss after 374827560 batches: 0.0103
Time to train on one home:  303.1696968078613
trigger times: 0
Loss after 374958660 batches: 0.1173
trigger times: 0
Loss after 375089760 batches: 0.0303
trigger times: 0
Loss after 375220860 batches: 0.0237
trigger times: 1
Loss after 375351960 batches: 0.0217
trigger times: 2
Loss after 375483060 batches: 0.0208
trigger times: 3
Loss after 375614160 batches: 0.0199
trigger times: 4
Loss after 375745260 batches: 0.0190
trigger times: 5
Loss after 375876360 batches: 0.0187
trigger times: 6
Loss after 376007460 batches: 0.0181
trigger times: 0
Loss after 376138560 batches: 0.0179
trigger times: 1
Loss after 376269660 batches: 0.0173
trigger times: 2
Loss after 376400760 batches: 0.0174
trigger times: 3
Loss after 376531860 batches: 0.0169
trigger times: 4
Loss after 376662960 batches: 0.0165
trigger times: 5
Loss after 376794060 batches: 0.0167
trigger times: 6
Loss after 376925160 batches: 0.0162
trigger times: 7
Loss after 377056260 batches: 0.0163
trigger times: 8
Loss after 377187360 batches: 0.0163
trigger times: 9
Loss after 377318460 batches: 0.0160
trigger times: 10
Loss after 377449560 batches: 0.0159
trigger times: 11
Loss after 377580660 batches: 0.0157
trigger times: 12
Loss after 377711760 batches: 0.0157
trigger times: 13
Loss after 377842860 batches: 0.0155
trigger times: 14
Loss after 377973960 batches: 0.0154
trigger times: 15
Loss after 378105060 batches: 0.0155
trigger times: 16
Loss after 378236160 batches: 0.0153
trigger times: 17
Loss after 378367260 batches: 0.0153
trigger times: 18
Loss after 378498360 batches: 0.0151
trigger times: 19
Loss after 378629460 batches: 0.0150
trigger times: 20
Early stopping!
Start to test process.
Loss after 378760560 batches: 0.0149
Time to train on one home:  224.84878063201904
trigger times: 0
Loss after 378889200 batches: 0.0717
trigger times: 1
Loss after 379017840 batches: 0.0221
trigger times: 2
Loss after 379146480 batches: 0.0183
trigger times: 0
Loss after 379275120 batches: 0.0167
trigger times: 1
Loss after 379403760 batches: 0.0155
trigger times: 2
Loss after 379532400 batches: 0.0147
trigger times: 3
Loss after 379661040 batches: 0.0141
trigger times: 4
Loss after 379789680 batches: 0.0136
trigger times: 5
Loss after 379918320 batches: 0.0131
trigger times: 6
Loss after 380046960 batches: 0.0129
trigger times: 7
Loss after 380175600 batches: 0.0126
trigger times: 8
Loss after 380304240 batches: 0.0123
trigger times: 9
Loss after 380432880 batches: 0.0122
trigger times: 10
Loss after 380561520 batches: 0.0120
trigger times: 11
Loss after 380690160 batches: 0.0117
trigger times: 12
Loss after 380818800 batches: 0.0116
trigger times: 13
Loss after 380947440 batches: 0.0114
trigger times: 14
Loss after 381076080 batches: 0.0114
trigger times: 15
Loss after 381204720 batches: 0.0111
trigger times: 16
Loss after 381333360 batches: 0.0109
trigger times: 17
Loss after 381462000 batches: 0.0110
trigger times: 18
Loss after 381590640 batches: 0.0111
trigger times: 19
Loss after 381719280 batches: 0.0107
trigger times: 20
Early stopping!
Start to test process.
Loss after 381847920 batches: 0.0105
Time to train on one home:  179.20240211486816
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765, 0.013892934501727369, 0.013666319778952196, 0.012580597168674625, 0.012713179911946834, 0.012576136202707514, 0.012611944796672613]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806], [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324], [0.4718517627980974, 0.4902251689616832, 0.5549523261866275, 0.907874701331254, 0.4176047824455843, 21.448865222845452, 1289.172], [0.4761729836463928, 0.48556449560991066, 0.5560194037302575, 0.9166221286580782, 0.42142278082960144, 21.655526328727763, 1300.9584], [0.46398472951518166, 0.49880328154927556, 0.5698825241915405, 0.8946974979237889, 0.41057763904260586, 21.137549069320855, 1267.4788], [0.469333012898763, 0.49303020279751153, 0.5671409183881702, 0.9028353043263873, 0.41530691390943525, 21.329807662365685, 1282.0784], [0.4544395903746287, 0.5091638361980007, 0.5806109676618026, 0.8752694784842265, 0.40209032875056233, 20.678555146596764, 1241.278]]
Round_13_results:  [0.4544395903746287, 0.5091638361980007, 0.5806109676618026, 0.8752694784842265, 0.40209032875056233, 20.678555146596764, 1241.278]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2989 < 2990; dropping {'Training_Loss': 0.058225058300315206, 'Validation_Loss': 0.22851851085821787, 'Training_R2': 0.9413543782416968, 'Validation_R2': 0.7874069278979937, 'Training_F1': 0.870962501110364, 'Validation_F1': 0.7543178592988012, 'Training_NEP': 0.258097289516433, 'Validation_NEP': 0.4950125291713025, 'Training_NDE': 0.04402630921535779, 'Validation_NDE': 0.16929791596754568, 'Training_MAE': 8.547670366664045, 'Validation_MAE': 13.57529723105892, 'Training_MSE': 193.70885, 'Validation_MSE': 625.2125}.
trigger times: 0
Loss after 381979020 batches: 0.0582
trigger times: 0
Loss after 382110120 batches: 0.0158
trigger times: 0
Loss after 382241220 batches: 0.0127
trigger times: 0
Loss after 382372320 batches: 0.0115
trigger times: 1
Loss after 382503420 batches: 0.0108
trigger times: 0
Loss after 382634520 batches: 0.0102
trigger times: 1
Loss after 382765620 batches: 0.0099
trigger times: 0
Loss after 382896720 batches: 0.0096
trigger times: 1
Loss after 383027820 batches: 0.0095
trigger times: 2
Loss after 383158920 batches: 0.0090
trigger times: 3
Loss after 383290020 batches: 0.0088
trigger times: 4
Loss after 383421120 batches: 0.0089
trigger times: 5
Loss after 383552220 batches: 0.0087
trigger times: 6
Loss after 383683320 batches: 0.0085
trigger times: 0
Loss after 383814420 batches: 0.0083
trigger times: 1
Loss after 383945520 batches: 0.0082
trigger times: 2
Loss after 384076620 batches: 0.0082
trigger times: 3
Loss after 384207720 batches: 0.0081
trigger times: 4
Loss after 384338820 batches: 0.0081
trigger times: 5
Loss after 384469920 batches: 0.0078
trigger times: 6
Loss after 384601020 batches: 0.0079
trigger times: 7
Loss after 384732120 batches: 0.0077
trigger times: 8
Loss after 384863220 batches: 0.0077
trigger times: 9
Loss after 384994320 batches: 0.0077
trigger times: 10
Loss after 385125420 batches: 0.0080
trigger times: 11
Loss after 385256520 batches: 0.0076
trigger times: 12
Loss after 385387620 batches: 0.0076
trigger times: 13
Loss after 385518720 batches: 0.0074
trigger times: 14
Loss after 385649820 batches: 0.0075
trigger times: 15
Loss after 385780920 batches: 0.0073
trigger times: 16
Loss after 385912020 batches: 0.0073
trigger times: 17
Loss after 386043120 batches: 0.0073
trigger times: 18
Loss after 386174220 batches: 0.0073
trigger times: 19
Loss after 386305320 batches: 0.0072
trigger times: 20
Early stopping!
Start to test process.
Loss after 386436420 batches: 0.0072
Time to train on one home:  260.88815999031067
trigger times: 0
Loss after 386539020 batches: 0.1720
trigger times: 0
Loss after 386641620 batches: 0.0507
trigger times: 1
Loss after 386744220 batches: 0.0377
trigger times: 2
Loss after 386846820 batches: 0.0339
trigger times: 3
Loss after 386949420 batches: 0.0301
trigger times: 4
Loss after 387052020 batches: 0.0288
trigger times: 5
Loss after 387154620 batches: 0.0278
trigger times: 6
Loss after 387257220 batches: 0.0258
trigger times: 7
Loss after 387359820 batches: 0.0248
trigger times: 8
Loss after 387462420 batches: 0.0240
trigger times: 9
Loss after 387565020 batches: 0.0242
trigger times: 10
Loss after 387667620 batches: 0.0308
trigger times: 11
Loss after 387770220 batches: 0.0245
trigger times: 12
Loss after 387872820 batches: 0.0230
trigger times: 13
Loss after 387975420 batches: 0.0220
trigger times: 14
Loss after 388078020 batches: 0.0211
trigger times: 15
Loss after 388180620 batches: 0.0216
trigger times: 16
Loss after 388283220 batches: 0.0235
trigger times: 17
Loss after 388385820 batches: 0.0223
trigger times: 18
Loss after 388488420 batches: 0.0213
trigger times: 19
Loss after 388591020 batches: 0.0224
trigger times: 20
Early stopping!
Start to test process.
Loss after 388693620 batches: 0.0208
Time to train on one home:  138.02124214172363
trigger times: 0
Loss after 388824720 batches: 0.0598
trigger times: 0
Loss after 388955820 batches: 0.0214
trigger times: 1
Loss after 389086920 batches: 0.0174
trigger times: 2
Loss after 389218020 batches: 0.0159
trigger times: 0
Loss after 389349120 batches: 0.0150
trigger times: 1
Loss after 389480220 batches: 0.0143
trigger times: 2
Loss after 389611320 batches: 0.0138
trigger times: 0
Loss after 389742420 batches: 0.0135
trigger times: 1
Loss after 389873520 batches: 0.0132
trigger times: 0
Loss after 390004620 batches: 0.0129
trigger times: 1
Loss after 390135720 batches: 0.0127
trigger times: 2
Loss after 390266820 batches: 0.0124
trigger times: 3
Loss after 390397920 batches: 0.0123
trigger times: 4
Loss after 390529020 batches: 0.0120
trigger times: 5
Loss after 390660120 batches: 0.0121
trigger times: 6
Loss after 390791220 batches: 0.0118
trigger times: 7
Loss after 390922320 batches: 0.0117
trigger times: 0
Loss after 391053420 batches: 0.0116
trigger times: 1
Loss after 391184520 batches: 0.0115
trigger times: 2
Loss after 391315620 batches: 0.0114
trigger times: 3
Loss after 391446720 batches: 0.0113
trigger times: 4
Loss after 391577820 batches: 0.0111
trigger times: 5
Loss after 391708920 batches: 0.0111
trigger times: 6
Loss after 391840020 batches: 0.0110
trigger times: 7
Loss after 391971120 batches: 0.0109
trigger times: 8
Loss after 392102220 batches: 0.0108
trigger times: 9
Loss after 392233320 batches: 0.0108
trigger times: 10
Loss after 392364420 batches: 0.0107
trigger times: 11
Loss after 392495520 batches: 0.0106
trigger times: 12
Loss after 392626620 batches: 0.0107
trigger times: 13
Loss after 392757720 batches: 0.0107
trigger times: 14
Loss after 392888820 batches: 0.0105
trigger times: 15
Loss after 393019920 batches: 0.0106
trigger times: 16
Loss after 393151020 batches: 0.0103
trigger times: 17
Loss after 393282120 batches: 0.0106
trigger times: 18
Loss after 393413220 batches: 0.0105
trigger times: 19
Loss after 393544320 batches: 0.0103
trigger times: 20
Early stopping!
Start to test process.
Loss after 393675420 batches: 0.0104
Time to train on one home:  279.5867192745209
trigger times: 0
Loss after 393806520 batches: 0.1136
trigger times: 0
Loss after 393937620 batches: 0.0292
trigger times: 1
Loss after 394068720 batches: 0.0238
trigger times: 0
Loss after 394199820 batches: 0.0217
trigger times: 0
Loss after 394330920 batches: 0.0205
trigger times: 1
Loss after 394462020 batches: 0.0197
trigger times: 0
Loss after 394593120 batches: 0.0191
trigger times: 1
Loss after 394724220 batches: 0.0187
trigger times: 0
Loss after 394855320 batches: 0.0179
trigger times: 1
Loss after 394986420 batches: 0.0178
trigger times: 2
Loss after 395117520 batches: 0.0178
trigger times: 3
Loss after 395248620 batches: 0.0175
trigger times: 4
Loss after 395379720 batches: 0.0170
trigger times: 5
Loss after 395510820 batches: 0.0167
trigger times: 6
Loss after 395641920 batches: 0.0166
trigger times: 7
Loss after 395773020 batches: 0.0166
trigger times: 8
Loss after 395904120 batches: 0.0160
trigger times: 9
Loss after 396035220 batches: 0.0162
trigger times: 10
Loss after 396166320 batches: 0.0160
trigger times: 11
Loss after 396297420 batches: 0.0160
trigger times: 12
Loss after 396428520 batches: 0.0159
trigger times: 13
Loss after 396559620 batches: 0.0158
trigger times: 14
Loss after 396690720 batches: 0.0154
trigger times: 15
Loss after 396821820 batches: 0.0152
trigger times: 16
Loss after 396952920 batches: 0.0153
trigger times: 17
Loss after 397084020 batches: 0.0151
trigger times: 18
Loss after 397215120 batches: 0.0151
trigger times: 19
Loss after 397346220 batches: 0.0152
trigger times: 20
Early stopping!
Start to test process.
Loss after 397477320 batches: 0.0151
Time to train on one home:  216.0765814781189
trigger times: 0
Loss after 397605960 batches: 0.0682
trigger times: 0
Loss after 397734600 batches: 0.0217
trigger times: 1
Loss after 397863240 batches: 0.0173
trigger times: 2
Loss after 397991880 batches: 0.0159
trigger times: 3
Loss after 398120520 batches: 0.0149
trigger times: 4
Loss after 398249160 batches: 0.0143
trigger times: 5
Loss after 398377800 batches: 0.0137
trigger times: 6
Loss after 398506440 batches: 0.0132
trigger times: 7
Loss after 398635080 batches: 0.0129
trigger times: 8
Loss after 398763720 batches: 0.0125
trigger times: 9
Loss after 398892360 batches: 0.0122
trigger times: 10
Loss after 399021000 batches: 0.0121
trigger times: 11
Loss after 399149640 batches: 0.0118
trigger times: 12
Loss after 399278280 batches: 0.0119
trigger times: 13
Loss after 399406920 batches: 0.0115
trigger times: 14
Loss after 399535560 batches: 0.0117
trigger times: 15
Loss after 399664200 batches: 0.0113
trigger times: 16
Loss after 399792840 batches: 0.0110
trigger times: 17
Loss after 399921480 batches: 0.0109
trigger times: 18
Loss after 400050120 batches: 0.0107
trigger times: 19
Loss after 400178760 batches: 0.0108
trigger times: 20
Early stopping!
Start to test process.
Loss after 400307400 batches: 0.0107
Time to train on one home:  164.31403160095215
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765, 0.013892934501727369, 0.013666319778952196, 0.012580597168674625, 0.012713179911946834, 0.012576136202707514, 0.012611944796672613, 0.012838088819402186]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806], [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324], [0.4718517627980974, 0.4902251689616832, 0.5549523261866275, 0.907874701331254, 0.4176047824455843, 21.448865222845452, 1289.172], [0.4761729836463928, 0.48556449560991066, 0.5560194037302575, 0.9166221286580782, 0.42142278082960144, 21.655526328727763, 1300.9584], [0.46398472951518166, 0.49880328154927556, 0.5698825241915405, 0.8946974979237889, 0.41057763904260586, 21.137549069320855, 1267.4788], [0.469333012898763, 0.49303020279751153, 0.5671409183881702, 0.9028353043263873, 0.41530691390943525, 21.329807662365685, 1282.0784], [0.4544395903746287, 0.5091638361980007, 0.5806109676618026, 0.8752694784842265, 0.40209032875056233, 20.678555146596764, 1241.278], [0.4510269910097122, 0.5128308573472844, 0.5837508100122663, 0.8712460572392167, 0.39908632487271106, 20.583500377593502, 1232.0043]]
Round_14_results:  [0.4510269910097122, 0.5128308573472844, 0.5837508100122663, 0.8712460572392167, 0.39908632487271106, 20.583500377593502, 1232.0043]
trigger times: 0
Loss after 400438500 batches: 0.0584
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3135 < 3136; dropping {'Training_Loss': 0.058418324744364, 'Validation_Loss': 0.24472332994143167, 'Training_R2': 0.9411438611944479, 'Validation_R2': 0.7724813214565388, 'Training_F1': 0.8684983114386784, 'Validation_F1': 0.7466736946233098, 'Training_NEP': 0.26305997818276894, 'Validation_NEP': 0.5070356633058268, 'Training_NDE': 0.044184348099410874, 'Validation_NDE': 0.18118388214746747, 'Training_MAE': 8.712024773220175, 'Validation_MAE': 13.90502144995556, 'Training_MSE': 194.40419, 'Validation_MSE': 669.1071}.
trigger times: 0
Loss after 400569600 batches: 0.0156
trigger times: 0
Loss after 400700700 batches: 0.0126
trigger times: 0
Loss after 400831800 batches: 0.0116
trigger times: 0
Loss after 400962900 batches: 0.0105
trigger times: 1
Loss after 401094000 batches: 0.0100
trigger times: 2
Loss after 401225100 batches: 0.0100
trigger times: 3
Loss after 401356200 batches: 0.0095
trigger times: 0
Loss after 401487300 batches: 0.0094
trigger times: 1
Loss after 401618400 batches: 0.0091
trigger times: 2
Loss after 401749500 batches: 0.0089
trigger times: 3
Loss after 401880600 batches: 0.0088
trigger times: 0
Loss after 402011700 batches: 0.0087
trigger times: 0
Loss after 402142800 batches: 0.0085
trigger times: 1
Loss after 402273900 batches: 0.0085
trigger times: 2
Loss after 402405000 batches: 0.0082
trigger times: 3
Loss after 402536100 batches: 0.0083
trigger times: 4
Loss after 402667200 batches: 0.0080
trigger times: 5
Loss after 402798300 batches: 0.0080
trigger times: 6
Loss after 402929400 batches: 0.0078
trigger times: 7
Loss after 403060500 batches: 0.0076
trigger times: 8
Loss after 403191600 batches: 0.0079
trigger times: 9
Loss after 403322700 batches: 0.0077
trigger times: 10
Loss after 403453800 batches: 0.0075
trigger times: 11
Loss after 403584900 batches: 0.0075
trigger times: 12
Loss after 403716000 batches: 0.0075
trigger times: 13
Loss after 403847100 batches: 0.0075
trigger times: 14
Loss after 403978200 batches: 0.0074
trigger times: 15
Loss after 404109300 batches: 0.0074
trigger times: 16
Loss after 404240400 batches: 0.0072
trigger times: 17
Loss after 404371500 batches: 0.0072
trigger times: 18
Loss after 404502600 batches: 0.0074
trigger times: 19
Loss after 404633700 batches: 0.0073
trigger times: 20
Early stopping!
Start to test process.
Loss after 404764800 batches: 0.0072
Time to train on one home:  252.4130198955536
trigger times: 0
Loss after 404867400 batches: 0.1543
trigger times: 1
Loss after 404970000 batches: 0.0444
trigger times: 2
Loss after 405072600 batches: 0.0357
trigger times: 3
Loss after 405175200 batches: 0.0313
trigger times: 4
Loss after 405277800 batches: 0.0291
trigger times: 5
Loss after 405380400 batches: 0.0279
trigger times: 6
Loss after 405483000 batches: 0.0263
trigger times: 7
Loss after 405585600 batches: 0.0263
trigger times: 8
Loss after 405688200 batches: 0.0261
trigger times: 9
Loss after 405790800 batches: 0.0280
trigger times: 10
Loss after 405893400 batches: 0.0245
trigger times: 11
Loss after 405996000 batches: 0.0254
trigger times: 12
Loss after 406098600 batches: 0.0228
trigger times: 13
Loss after 406201200 batches: 0.0232
trigger times: 14
Loss after 406303800 batches: 0.0221
trigger times: 15
Loss after 406406400 batches: 0.0275
trigger times: 16
Loss after 406509000 batches: 0.0219
trigger times: 17
Loss after 406611600 batches: 0.0207
trigger times: 18
Loss after 406714200 batches: 0.0198
trigger times: 19
Loss after 406816800 batches: 0.0195
trigger times: 20
Early stopping!
Start to test process.
Loss after 406919400 batches: 0.0199
Time to train on one home:  131.6609959602356
trigger times: 0
Loss after 407050500 batches: 0.0540
trigger times: 0
Loss after 407181600 batches: 0.0206
trigger times: 1
Loss after 407312700 batches: 0.0168
trigger times: 2
Loss after 407443800 batches: 0.0154
trigger times: 3
Loss after 407574900 batches: 0.0147
trigger times: 4
Loss after 407706000 batches: 0.0142
trigger times: 5
Loss after 407837100 batches: 0.0135
trigger times: 6
Loss after 407968200 batches: 0.0131
trigger times: 7
Loss after 408099300 batches: 0.0129
trigger times: 8
Loss after 408230400 batches: 0.0126
trigger times: 9
Loss after 408361500 batches: 0.0123
trigger times: 0
Loss after 408492600 batches: 0.0121
trigger times: 1
Loss after 408623700 batches: 0.0122
trigger times: 2
Loss after 408754800 batches: 0.0118
trigger times: 3
Loss after 408885900 batches: 0.0118
trigger times: 4
Loss after 409017000 batches: 0.0117
trigger times: 5
Loss after 409148100 batches: 0.0115
trigger times: 6
Loss after 409279200 batches: 0.0114
trigger times: 7
Loss after 409410300 batches: 0.0112
trigger times: 8
Loss after 409541400 batches: 0.0111
trigger times: 9
Loss after 409672500 batches: 0.0113
trigger times: 10
Loss after 409803600 batches: 0.0110
trigger times: 11
Loss after 409934700 batches: 0.0110
trigger times: 12
Loss after 410065800 batches: 0.0109
trigger times: 13
Loss after 410196900 batches: 0.0108
trigger times: 14
Loss after 410328000 batches: 0.0109
trigger times: 15
Loss after 410459100 batches: 0.0105
trigger times: 16
Loss after 410590200 batches: 0.0106
trigger times: 17
Loss after 410721300 batches: 0.0108
trigger times: 18
Loss after 410852400 batches: 0.0105
trigger times: 19
Loss after 410983500 batches: 0.0104
trigger times: 20
Early stopping!
Start to test process.
Loss after 411114600 batches: 0.0105
Time to train on one home:  236.40274477005005
trigger times: 0
Loss after 411245700 batches: 0.1158
trigger times: 1
Loss after 411376800 batches: 0.0289
trigger times: 0
Loss after 411507900 batches: 0.0236
trigger times: 0
Loss after 411639000 batches: 0.0216
trigger times: 0
Loss after 411770100 batches: 0.0204
trigger times: 1
Loss after 411901200 batches: 0.0197
trigger times: 2
Loss after 412032300 batches: 0.0192
trigger times: 3
Loss after 412163400 batches: 0.0187
trigger times: 0
Loss after 412294500 batches: 0.0183
trigger times: 1
Loss after 412425600 batches: 0.0176
trigger times: 2
Loss after 412556700 batches: 0.0174
trigger times: 3
Loss after 412687800 batches: 0.0172
trigger times: 0
Loss after 412818900 batches: 0.0170
trigger times: 1
Loss after 412950000 batches: 0.0167
trigger times: 2
Loss after 413081100 batches: 0.0167
trigger times: 0
Loss after 413212200 batches: 0.0164
trigger times: 1
Loss after 413343300 batches: 0.0162
trigger times: 2
Loss after 413474400 batches: 0.0160
trigger times: 3
Loss after 413605500 batches: 0.0160
trigger times: 4
Loss after 413736600 batches: 0.0159
trigger times: 5
Loss after 413867700 batches: 0.0159
trigger times: 6
Loss after 413998800 batches: 0.0158
trigger times: 7
Loss after 414129900 batches: 0.0157
trigger times: 8
Loss after 414261000 batches: 0.0154
trigger times: 9
Loss after 414392100 batches: 0.0155
trigger times: 10
Loss after 414523200 batches: 0.0151
trigger times: 11
Loss after 414654300 batches: 0.0152
trigger times: 12
Loss after 414785400 batches: 0.0152
trigger times: 13
Loss after 414916500 batches: 0.0151
trigger times: 14
Loss after 415047600 batches: 0.0150
trigger times: 15
Loss after 415178700 batches: 0.0150
trigger times: 16
Loss after 415309800 batches: 0.0150
trigger times: 17
Loss after 415440900 batches: 0.0150
trigger times: 18
Loss after 415572000 batches: 0.0146
trigger times: 19
Loss after 415703100 batches: 0.0148
trigger times: 20
Early stopping!
Start to test process.
Loss after 415834200 batches: 0.0146
Time to train on one home:  265.3703372478485
trigger times: 0
Loss after 415962840 batches: 0.0653
trigger times: 1
Loss after 416091480 batches: 0.0209
trigger times: 2
Loss after 416220120 batches: 0.0171
trigger times: 3
Loss after 416348760 batches: 0.0155
trigger times: 4
Loss after 416477400 batches: 0.0147
trigger times: 5
Loss after 416606040 batches: 0.0142
trigger times: 6
Loss after 416734680 batches: 0.0134
trigger times: 7
Loss after 416863320 batches: 0.0128
trigger times: 8
Loss after 416991960 batches: 0.0127
trigger times: 9
Loss after 417120600 batches: 0.0121
trigger times: 10
Loss after 417249240 batches: 0.0121
trigger times: 11
Loss after 417377880 batches: 0.0118
trigger times: 12
Loss after 417506520 batches: 0.0117
trigger times: 13
Loss after 417635160 batches: 0.0114
trigger times: 14
Loss after 417763800 batches: 0.0113
trigger times: 15
Loss after 417892440 batches: 0.0114
trigger times: 16
Loss after 418021080 batches: 0.0110
trigger times: 17
Loss after 418149720 batches: 0.0109
trigger times: 18
Loss after 418278360 batches: 0.0108
trigger times: 19
Loss after 418407000 batches: 0.0110
trigger times: 20
Early stopping!
Start to test process.
Loss after 418535640 batches: 0.0105
Time to train on one home:  157.40564918518066
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765, 0.013892934501727369, 0.013666319778952196, 0.012580597168674625, 0.012713179911946834, 0.012576136202707514, 0.012611944796672613, 0.012838088819402186, 0.012534839612687296]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806], [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324], [0.4718517627980974, 0.4902251689616832, 0.5549523261866275, 0.907874701331254, 0.4176047824455843, 21.448865222845452, 1289.172], [0.4761729836463928, 0.48556449560991066, 0.5560194037302575, 0.9166221286580782, 0.42142278082960144, 21.655526328727763, 1300.9584], [0.46398472951518166, 0.49880328154927556, 0.5698825241915405, 0.8946974979237889, 0.41057763904260586, 21.137549069320855, 1267.4788], [0.469333012898763, 0.49303020279751153, 0.5671409183881702, 0.9028353043263873, 0.41530691390943525, 21.329807662365685, 1282.0784], [0.4544395903746287, 0.5091638361980007, 0.5806109676618026, 0.8752694784842265, 0.40209032875056233, 20.678555146596764, 1241.278], [0.4510269910097122, 0.5128308573472844, 0.5837508100122663, 0.8712460572392167, 0.39908632487271106, 20.583500377593502, 1232.0043], [0.4509678996271557, 0.5128822638311595, 0.5867076027273368, 0.8703151627515375, 0.39904421295935666, 20.56150766166735, 1231.8744]]
Round_15_results:  [0.4509678996271557, 0.5128822638311595, 0.5867076027273368, 0.8703151627515375, 0.39904421295935666, 20.56150766166735, 1231.8744]
trigger times: 0
Loss after 418666740 batches: 0.0519
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3279 < 3280; dropping {'Training_Loss': 0.051880928178159695, 'Validation_Loss': 0.22828941957818139, 'Training_R2': 0.9477347542919927, 'Validation_R2': 0.787766828524746, 'Training_F1': 0.8774564107078762, 'Validation_F1': 0.7529752137162138, 'Training_NEP': 0.2450051524968965, 'Validation_NEP': 0.4949820549824481, 'Training_NDE': 0.03923644766255005, 'Validation_NDE': 0.16901131008024126, 'Training_MAE': 8.11408475308451, 'Validation_MAE': 13.574461502370868, 'Training_MSE': 172.63422, 'Validation_MSE': 624.15405}.
trigger times: 0
Loss after 418797840 batches: 0.0149
trigger times: 0
Loss after 418928940 batches: 0.0123
trigger times: 0
Loss after 419060040 batches: 0.0112
trigger times: 1
Loss after 419191140 batches: 0.0103
trigger times: 2
Loss after 419322240 batches: 0.0099
trigger times: 3
Loss after 419453340 batches: 0.0098
trigger times: 0
Loss after 419584440 batches: 0.0094
trigger times: 0
Loss after 419715540 batches: 0.0090
trigger times: 1
Loss after 419846640 batches: 0.0087
trigger times: 2
Loss after 419977740 batches: 0.0088
trigger times: 3
Loss after 420108840 batches: 0.0086
trigger times: 0
Loss after 420239940 batches: 0.0086
trigger times: 1
Loss after 420371040 batches: 0.0083
trigger times: 2
Loss after 420502140 batches: 0.0083
trigger times: 3
Loss after 420633240 batches: 0.0081
trigger times: 4
Loss after 420764340 batches: 0.0080
trigger times: 5
Loss after 420895440 batches: 0.0079
trigger times: 6
Loss after 421026540 batches: 0.0078
trigger times: 7
Loss after 421157640 batches: 0.0078
trigger times: 8
Loss after 421288740 batches: 0.0076
trigger times: 9
Loss after 421419840 batches: 0.0075
trigger times: 10
Loss after 421550940 batches: 0.0076
trigger times: 11
Loss after 421682040 batches: 0.0077
trigger times: 12
Loss after 421813140 batches: 0.0076
trigger times: 0
Loss after 421944240 batches: 0.0073
trigger times: 1
Loss after 422075340 batches: 0.0072
trigger times: 2
Loss after 422206440 batches: 0.0072
trigger times: 3
Loss after 422337540 batches: 0.0071
trigger times: 0
Loss after 422468640 batches: 0.0071
trigger times: 1
Loss after 422599740 batches: 0.0072
trigger times: 2
Loss after 422730840 batches: 0.0071
trigger times: 3
Loss after 422861940 batches: 0.0071
trigger times: 4
Loss after 422993040 batches: 0.0071
trigger times: 5
Loss after 423124140 batches: 0.0072
trigger times: 6
Loss after 423255240 batches: 0.0072
trigger times: 7
Loss after 423386340 batches: 0.0070
trigger times: 8
Loss after 423517440 batches: 0.0070
trigger times: 9
Loss after 423648540 batches: 0.0069
trigger times: 10
Loss after 423779640 batches: 0.0068
trigger times: 11
Loss after 423910740 batches: 0.0069
trigger times: 12
Loss after 424041840 batches: 0.0067
trigger times: 13
Loss after 424172940 batches: 0.0068
trigger times: 14
Loss after 424304040 batches: 0.0066
trigger times: 0
Loss after 424435140 batches: 0.0066
trigger times: 1
Loss after 424566240 batches: 0.0066
trigger times: 2
Loss after 424697340 batches: 0.0067
trigger times: 3
Loss after 424828440 batches: 0.0067
trigger times: 4
Loss after 424959540 batches: 0.0066
trigger times: 5
Loss after 425090640 batches: 0.0065
trigger times: 6
Loss after 425221740 batches: 0.0065
trigger times: 7
Loss after 425352840 batches: 0.0065
trigger times: 8
Loss after 425483940 batches: 0.0064
trigger times: 9
Loss after 425615040 batches: 0.0068
trigger times: 10
Loss after 425746140 batches: 0.0063
trigger times: 0
Loss after 425877240 batches: 0.0065
trigger times: 1
Loss after 426008340 batches: 0.0064
trigger times: 2
Loss after 426139440 batches: 0.0063
trigger times: 3
Loss after 426270540 batches: 0.0064
trigger times: 4
Loss after 426401640 batches: 0.0062
trigger times: 5
Loss after 426532740 batches: 0.0065
trigger times: 6
Loss after 426663840 batches: 0.0064
trigger times: 7
Loss after 426794940 batches: 0.0061
trigger times: 8
Loss after 426926040 batches: 0.0061
trigger times: 9
Loss after 427057140 batches: 0.0063
trigger times: 10
Loss after 427188240 batches: 0.0062
trigger times: 11
Loss after 427319340 batches: 0.0060
trigger times: 12
Loss after 427450440 batches: 0.0062
trigger times: 13
Loss after 427581540 batches: 0.0059
trigger times: 14
Loss after 427712640 batches: 0.0062
trigger times: 15
Loss after 427843740 batches: 0.0061
trigger times: 16
Loss after 427974840 batches: 0.0059
trigger times: 17
Loss after 428105940 batches: 0.0060
trigger times: 18
Loss after 428237040 batches: 0.0060
trigger times: 19
Loss after 428368140 batches: 0.0059
trigger times: 20
Early stopping!
Start to test process.
Loss after 428499240 batches: 0.0060
Time to train on one home:  547.9946615695953
trigger times: 0
Loss after 428601840 batches: 0.1400
trigger times: 1
Loss after 428704440 batches: 0.0463
trigger times: 2
Loss after 428807040 batches: 0.0359
trigger times: 3
Loss after 428909640 batches: 0.0313
trigger times: 4
Loss after 429012240 batches: 0.0288
trigger times: 5
Loss after 429114840 batches: 0.0265
trigger times: 6
Loss after 429217440 batches: 0.0260
trigger times: 7
Loss after 429320040 batches: 0.0261
trigger times: 8
Loss after 429422640 batches: 0.0239
trigger times: 9
Loss after 429525240 batches: 0.0240
trigger times: 10
Loss after 429627840 batches: 0.0224
trigger times: 11
Loss after 429730440 batches: 0.0222
trigger times: 12
Loss after 429833040 batches: 0.0214
trigger times: 13
Loss after 429935640 batches: 0.0209
trigger times: 14
Loss after 430038240 batches: 0.0206
trigger times: 15
Loss after 430140840 batches: 0.0210
trigger times: 16
Loss after 430243440 batches: 0.0247
trigger times: 17
Loss after 430346040 batches: 0.0217
trigger times: 18
Loss after 430448640 batches: 0.0195
trigger times: 19
Loss after 430551240 batches: 0.0196
trigger times: 20
Early stopping!
Start to test process.
Loss after 430653840 batches: 0.0183
Time to train on one home:  131.3361575603485
trigger times: 0
Loss after 430784940 batches: 0.0512
trigger times: 0
Loss after 430916040 batches: 0.0196
trigger times: 0
Loss after 431047140 batches: 0.0161
trigger times: 1
Loss after 431178240 batches: 0.0152
trigger times: 0
Loss after 431309340 batches: 0.0143
trigger times: 1
Loss after 431440440 batches: 0.0137
trigger times: 2
Loss after 431571540 batches: 0.0136
trigger times: 3
Loss after 431702640 batches: 0.0131
trigger times: 4
Loss after 431833740 batches: 0.0128
trigger times: 5
Loss after 431964840 batches: 0.0124
trigger times: 0
Loss after 432095940 batches: 0.0123
trigger times: 1
Loss after 432227040 batches: 0.0121
trigger times: 2
Loss after 432358140 batches: 0.0121
trigger times: 3
Loss after 432489240 batches: 0.0118
trigger times: 4
Loss after 432620340 batches: 0.0117
trigger times: 0
Loss after 432751440 batches: 0.0114
trigger times: 1
Loss after 432882540 batches: 0.0114
trigger times: 0
Loss after 433013640 batches: 0.0114
trigger times: 1
Loss after 433144740 batches: 0.0111
trigger times: 2
Loss after 433275840 batches: 0.0112
trigger times: 3
Loss after 433406940 batches: 0.0109
trigger times: 4
Loss after 433538040 batches: 0.0109
trigger times: 5
Loss after 433669140 batches: 0.0108
trigger times: 6
Loss after 433800240 batches: 0.0111
trigger times: 7
Loss after 433931340 batches: 0.0107
trigger times: 8
Loss after 434062440 batches: 0.0105
trigger times: 9
Loss after 434193540 batches: 0.0106
trigger times: 10
Loss after 434324640 batches: 0.0105
trigger times: 11
Loss after 434455740 batches: 0.0105
trigger times: 12
Loss after 434586840 batches: 0.0106
trigger times: 13
Loss after 434717940 batches: 0.0105
trigger times: 0
Loss after 434849040 batches: 0.0105
trigger times: 1
Loss after 434980140 batches: 0.0104
trigger times: 2
Loss after 435111240 batches: 0.0103
trigger times: 3
Loss after 435242340 batches: 0.0102
trigger times: 4
Loss after 435373440 batches: 0.0101
trigger times: 5
Loss after 435504540 batches: 0.0101
trigger times: 6
Loss after 435635640 batches: 0.0100
trigger times: 7
Loss after 435766740 batches: 0.0100
trigger times: 8
Loss after 435897840 batches: 0.0099
trigger times: 9
Loss after 436028940 batches: 0.0098
trigger times: 10
Loss after 436160040 batches: 0.0099
trigger times: 11
Loss after 436291140 batches: 0.0096
trigger times: 12
Loss after 436422240 batches: 0.0097
trigger times: 13
Loss after 436553340 batches: 0.0097
trigger times: 14
Loss after 436684440 batches: 0.0099
trigger times: 15
Loss after 436815540 batches: 0.0096
trigger times: 16
Loss after 436946640 batches: 0.0095
trigger times: 17
Loss after 437077740 batches: 0.0096
trigger times: 18
Loss after 437208840 batches: 0.0096
trigger times: 19
Loss after 437339940 batches: 0.0096
trigger times: 20
Early stopping!
Start to test process.
Loss after 437471040 batches: 0.0096
Time to train on one home:  378.71382117271423
trigger times: 0
Loss after 437602140 batches: 0.1112
trigger times: 0
Loss after 437733240 batches: 0.0280
trigger times: 1
Loss after 437864340 batches: 0.0232
trigger times: 2
Loss after 437995440 batches: 0.0212
trigger times: 3
Loss after 438126540 batches: 0.0200
trigger times: 4
Loss after 438257640 batches: 0.0195
trigger times: 5
Loss after 438388740 batches: 0.0189
trigger times: 6
Loss after 438519840 batches: 0.0185
trigger times: 7
Loss after 438650940 batches: 0.0181
trigger times: 8
Loss after 438782040 batches: 0.0177
trigger times: 9
Loss after 438913140 batches: 0.0174
trigger times: 10
Loss after 439044240 batches: 0.0173
trigger times: 11
Loss after 439175340 batches: 0.0170
trigger times: 12
Loss after 439306440 batches: 0.0168
trigger times: 13
Loss after 439437540 batches: 0.0165
trigger times: 14
Loss after 439568640 batches: 0.0164
trigger times: 15
Loss after 439699740 batches: 0.0163
trigger times: 16
Loss after 439830840 batches: 0.0162
trigger times: 0
Loss after 439961940 batches: 0.0157
trigger times: 0
Loss after 440093040 batches: 0.0157
trigger times: 0
Loss after 440224140 batches: 0.0158
trigger times: 1
Loss after 440355240 batches: 0.0157
trigger times: 2
Loss after 440486340 batches: 0.0154
trigger times: 3
Loss after 440617440 batches: 0.0156
trigger times: 4
Loss after 440748540 batches: 0.0153
trigger times: 5
Loss after 440879640 batches: 0.0153
trigger times: 0
Loss after 441010740 batches: 0.0152
trigger times: 1
Loss after 441141840 batches: 0.0152
trigger times: 2
Loss after 441272940 batches: 0.0147
trigger times: 3
Loss after 441404040 batches: 0.0149
trigger times: 4
Loss after 441535140 batches: 0.0150
trigger times: 5
Loss after 441666240 batches: 0.0148
trigger times: 0
Loss after 441797340 batches: 0.0146
trigger times: 1
Loss after 441928440 batches: 0.0148
trigger times: 0
Loss after 442059540 batches: 0.0146
trigger times: 1
Loss after 442190640 batches: 0.0144
trigger times: 2
Loss after 442321740 batches: 0.0148
trigger times: 3
Loss after 442452840 batches: 0.0146
trigger times: 4
Loss after 442583940 batches: 0.0145
trigger times: 5
Loss after 442715040 batches: 0.0144
trigger times: 6
Loss after 442846140 batches: 0.0141
trigger times: 7
Loss after 442977240 batches: 0.0140
trigger times: 8
Loss after 443108340 batches: 0.0141
trigger times: 9
Loss after 443239440 batches: 0.0141
trigger times: 10
Loss after 443370540 batches: 0.0141
trigger times: 11
Loss after 443501640 batches: 0.0141
trigger times: 12
Loss after 443632740 batches: 0.0138
trigger times: 13
Loss after 443763840 batches: 0.0141
trigger times: 14
Loss after 443894940 batches: 0.0138
trigger times: 15
Loss after 444026040 batches: 0.0140
trigger times: 16
Loss after 444157140 batches: 0.0137
trigger times: 0
Loss after 444288240 batches: 0.0139
trigger times: 0
Loss after 444419340 batches: 0.0138
trigger times: 1
Loss after 444550440 batches: 0.0136
trigger times: 2
Loss after 444681540 batches: 0.0135
trigger times: 3
Loss after 444812640 batches: 0.0135
trigger times: 4
Loss after 444943740 batches: 0.0136
trigger times: 5
Loss after 445074840 batches: 0.0136
trigger times: 6
Loss after 445205940 batches: 0.0135
trigger times: 7
Loss after 445337040 batches: 0.0136
trigger times: 8
Loss after 445468140 batches: 0.0133
trigger times: 9
Loss after 445599240 batches: 0.0133
trigger times: 10
Loss after 445730340 batches: 0.0135
trigger times: 11
Loss after 445861440 batches: 0.0133
trigger times: 12
Loss after 445992540 batches: 0.0135
trigger times: 13
Loss after 446123640 batches: 0.0132
trigger times: 14
Loss after 446254740 batches: 0.0132
trigger times: 15
Loss after 446385840 batches: 0.0131
trigger times: 16
Loss after 446516940 batches: 0.0134
trigger times: 17
Loss after 446648040 batches: 0.0131
trigger times: 18
Loss after 446779140 batches: 0.0131
trigger times: 19
Loss after 446910240 batches: 0.0129
trigger times: 20
Early stopping!
Start to test process.
Loss after 447041340 batches: 0.0130
Time to train on one home:  526.063470363617
trigger times: 0
Loss after 447169980 batches: 0.0609
trigger times: 0
Loss after 447298620 batches: 0.0200
trigger times: 1
Loss after 447427260 batches: 0.0164
trigger times: 2
Loss after 447555900 batches: 0.0152
trigger times: 3
Loss after 447684540 batches: 0.0144
trigger times: 4
Loss after 447813180 batches: 0.0136
trigger times: 5
Loss after 447941820 batches: 0.0132
trigger times: 6
Loss after 448070460 batches: 0.0130
trigger times: 7
Loss after 448199100 batches: 0.0124
trigger times: 8
Loss after 448327740 batches: 0.0121
trigger times: 9
Loss after 448456380 batches: 0.0121
trigger times: 10
Loss after 448585020 batches: 0.0115
trigger times: 11
Loss after 448713660 batches: 0.0114
trigger times: 12
Loss after 448842300 batches: 0.0113
trigger times: 13
Loss after 448970940 batches: 0.0110
trigger times: 14
Loss after 449099580 batches: 0.0109
trigger times: 15
Loss after 449228220 batches: 0.0110
trigger times: 16
Loss after 449356860 batches: 0.0108
trigger times: 17
Loss after 449485500 batches: 0.0105
trigger times: 18
Loss after 449614140 batches: 0.0106
trigger times: 19
Loss after 449742780 batches: 0.0105
trigger times: 20
Early stopping!
Start to test process.
Loss after 449871420 batches: 0.0103
Time to train on one home:  164.41494846343994
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765, 0.013892934501727369, 0.013666319778952196, 0.012580597168674625, 0.012713179911946834, 0.012576136202707514, 0.012611944796672613, 0.012838088819402186, 0.012534839612687296, 0.011462489906889636]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806], [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324], [0.4718517627980974, 0.4902251689616832, 0.5549523261866275, 0.907874701331254, 0.4176047824455843, 21.448865222845452, 1289.172], [0.4761729836463928, 0.48556449560991066, 0.5560194037302575, 0.9166221286580782, 0.42142278082960144, 21.655526328727763, 1300.9584], [0.46398472951518166, 0.49880328154927556, 0.5698825241915405, 0.8946974979237889, 0.41057763904260586, 21.137549069320855, 1267.4788], [0.469333012898763, 0.49303020279751153, 0.5671409183881702, 0.9028353043263873, 0.41530691390943525, 21.329807662365685, 1282.0784], [0.4544395903746287, 0.5091638361980007, 0.5806109676618026, 0.8752694784842265, 0.40209032875056233, 20.678555146596764, 1241.278], [0.4510269910097122, 0.5128308573472844, 0.5837508100122663, 0.8712460572392167, 0.39908632487271106, 20.583500377593502, 1232.0043], [0.4509678996271557, 0.5128822638311595, 0.5867076027273368, 0.8703151627515375, 0.39904421295935666, 20.56150766166735, 1231.8744], [0.45386744373374516, 0.5097697277751618, 0.5898363987389426, 0.8749398916603841, 0.40159398564992166, 20.67076853975173, 1239.7456]]
Round_16_results:  [0.45386744373374516, 0.5097697277751618, 0.5898363987389426, 0.8749398916603841, 0.40159398564992166, 20.67076853975173, 1239.7456]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3523 < 3524; dropping {'Training_Loss': 0.0702819111844841, 'Validation_Loss': 0.2620777388413747, 'Training_R2': 0.9291935566915683, 'Validation_R2': 0.756274666745179, 'Training_F1': 0.8558722794369588, 'Validation_F1': 0.7328870606004233, 'Training_NEP': 0.2884308575914261, 'Validation_NEP': 0.5256795004094468, 'Training_NDE': 0.05315565380795619, 'Validation_NDE': 0.19409000764022252, 'Training_MAE': 9.552257983355373, 'Validation_MAE': 14.416312811878846, 'Training_MSE': 233.87656, 'Validation_MSE': 716.7691}.
trigger times: 0
Loss after 450002520 batches: 0.0703
trigger times: 0
Loss after 450133620 batches: 0.0172
trigger times: 1
Loss after 450264720 batches: 0.0130
trigger times: 2
Loss after 450395820 batches: 0.0114
trigger times: 0
Loss after 450526920 batches: 0.0108
trigger times: 0
Loss after 450658020 batches: 0.0101
trigger times: 1
Loss after 450789120 batches: 0.0099
trigger times: 2
Loss after 450920220 batches: 0.0093
trigger times: 3
Loss after 451051320 batches: 0.0092
trigger times: 0
Loss after 451182420 batches: 0.0089
trigger times: 1
Loss after 451313520 batches: 0.0088
trigger times: 0
Loss after 451444620 batches: 0.0085
trigger times: 1
Loss after 451575720 batches: 0.0086
trigger times: 0
Loss after 451706820 batches: 0.0083
trigger times: 1
Loss after 451837920 batches: 0.0080
trigger times: 2
Loss after 451969020 batches: 0.0080
trigger times: 3
Loss after 452100120 batches: 0.0080
trigger times: 4
Loss after 452231220 batches: 0.0078
trigger times: 5
Loss after 452362320 batches: 0.0078
trigger times: 6
Loss after 452493420 batches: 0.0077
trigger times: 7
Loss after 452624520 batches: 0.0075
trigger times: 8
Loss after 452755620 batches: 0.0075
trigger times: 9
Loss after 452886720 batches: 0.0075
trigger times: 10
Loss after 453017820 batches: 0.0074
trigger times: 0
Loss after 453148920 batches: 0.0074
trigger times: 0
Loss after 453280020 batches: 0.0074
trigger times: 1
Loss after 453411120 batches: 0.0072
trigger times: 2
Loss after 453542220 batches: 0.0072
trigger times: 3
Loss after 453673320 batches: 0.0072
trigger times: 4
Loss after 453804420 batches: 0.0070
trigger times: 5
Loss after 453935520 batches: 0.0072
trigger times: 6
Loss after 454066620 batches: 0.0071
trigger times: 7
Loss after 454197720 batches: 0.0070
trigger times: 8
Loss after 454328820 batches: 0.0069
trigger times: 9
Loss after 454459920 batches: 0.0070
trigger times: 10
Loss after 454591020 batches: 0.0068
trigger times: 0
Loss after 454722120 batches: 0.0068
trigger times: 1
Loss after 454853220 batches: 0.0069
trigger times: 2
Loss after 454984320 batches: 0.0068
trigger times: 0
Loss after 455115420 batches: 0.0067
trigger times: 0
Loss after 455246520 batches: 0.0067
trigger times: 1
Loss after 455377620 batches: 0.0068
trigger times: 2
Loss after 455508720 batches: 0.0068
trigger times: 3
Loss after 455639820 batches: 0.0066
trigger times: 4
Loss after 455770920 batches: 0.0066
trigger times: 5
Loss after 455902020 batches: 0.0064
trigger times: 6
Loss after 456033120 batches: 0.0065
trigger times: 7
Loss after 456164220 batches: 0.0065
trigger times: 8
Loss after 456295320 batches: 0.0064
trigger times: 9
Loss after 456426420 batches: 0.0064
trigger times: 10
Loss after 456557520 batches: 0.0066
trigger times: 11
Loss after 456688620 batches: 0.0065
trigger times: 12
Loss after 456819720 batches: 0.0064
trigger times: 13
Loss after 456950820 batches: 0.0064
trigger times: 14
Loss after 457081920 batches: 0.0064
trigger times: 15
Loss after 457213020 batches: 0.0065
trigger times: 16
Loss after 457344120 batches: 0.0063
trigger times: 17
Loss after 457475220 batches: 0.0063
trigger times: 18
Loss after 457606320 batches: 0.0063
trigger times: 19
Loss after 457737420 batches: 0.0063
trigger times: 20
Early stopping!
Start to test process.
Loss after 457868520 batches: 0.0060
Time to train on one home:  442.2918629646301
trigger times: 0
Loss after 457971120 batches: 0.1558
trigger times: 0
Loss after 458073720 batches: 0.0478
trigger times: 1
Loss after 458176320 batches: 0.0372
trigger times: 2
Loss after 458278920 batches: 0.0339
trigger times: 3
Loss after 458381520 batches: 0.0297
trigger times: 4
Loss after 458484120 batches: 0.0275
trigger times: 5
Loss after 458586720 batches: 0.0255
trigger times: 6
Loss after 458689320 batches: 0.0250
trigger times: 7
Loss after 458791920 batches: 0.0237
trigger times: 8
Loss after 458894520 batches: 0.0246
trigger times: 9
Loss after 458997120 batches: 0.0229
trigger times: 10
Loss after 459099720 batches: 0.0236
trigger times: 11
Loss after 459202320 batches: 0.0221
trigger times: 12
Loss after 459304920 batches: 0.0217
trigger times: 13
Loss after 459407520 batches: 0.0219
trigger times: 14
Loss after 459510120 batches: 0.0220
trigger times: 15
Loss after 459612720 batches: 0.0207
trigger times: 16
Loss after 459715320 batches: 0.0198
trigger times: 17
Loss after 459817920 batches: 0.0198
trigger times: 18
Loss after 459920520 batches: 0.0204
trigger times: 19
Loss after 460023120 batches: 0.0198
trigger times: 20
Early stopping!
Start to test process.
Loss after 460125720 batches: 0.0191
Time to train on one home:  137.74776220321655
trigger times: 0
Loss after 460256820 batches: 0.0585
trigger times: 1
Loss after 460387920 batches: 0.0202
trigger times: 2
Loss after 460519020 batches: 0.0164
trigger times: 3
Loss after 460650120 batches: 0.0150
trigger times: 4
Loss after 460781220 batches: 0.0143
trigger times: 5
Loss after 460912320 batches: 0.0136
trigger times: 6
Loss after 461043420 batches: 0.0129
trigger times: 0
Loss after 461174520 batches: 0.0127
trigger times: 0
Loss after 461305620 batches: 0.0123
trigger times: 1
Loss after 461436720 batches: 0.0124
trigger times: 2
Loss after 461567820 batches: 0.0121
trigger times: 3
Loss after 461698920 batches: 0.0118
trigger times: 4
Loss after 461830020 batches: 0.0116
trigger times: 5
Loss after 461961120 batches: 0.0115
trigger times: 6
Loss after 462092220 batches: 0.0115
trigger times: 0
Loss after 462223320 batches: 0.0113
trigger times: 1
Loss after 462354420 batches: 0.0111
trigger times: 2
Loss after 462485520 batches: 0.0111
trigger times: 0
Loss after 462616620 batches: 0.0109
trigger times: 1
Loss after 462747720 batches: 0.0108
trigger times: 2
Loss after 462878820 batches: 0.0109
trigger times: 3
Loss after 463009920 batches: 0.0108
trigger times: 4
Loss after 463141020 batches: 0.0107
trigger times: 5
Loss after 463272120 batches: 0.0107
trigger times: 6
Loss after 463403220 batches: 0.0104
trigger times: 7
Loss after 463534320 batches: 0.0104
trigger times: 8
Loss after 463665420 batches: 0.0103
trigger times: 9
Loss after 463796520 batches: 0.0102
trigger times: 10
Loss after 463927620 batches: 0.0103
trigger times: 11
Loss after 464058720 batches: 0.0102
trigger times: 12
Loss after 464189820 batches: 0.0103
trigger times: 13
Loss after 464320920 batches: 0.0101
trigger times: 14
Loss after 464452020 batches: 0.0100
trigger times: 15
Loss after 464583120 batches: 0.0099
trigger times: 16
Loss after 464714220 batches: 0.0100
trigger times: 17
Loss after 464845320 batches: 0.0101
trigger times: 18
Loss after 464976420 batches: 0.0099
trigger times: 19
Loss after 465107520 batches: 0.0098
trigger times: 20
Early stopping!
Start to test process.
Loss after 465238620 batches: 0.0098
Time to train on one home:  287.6992347240448
trigger times: 0
Loss after 465369720 batches: 0.1069
trigger times: 1
Loss after 465500820 batches: 0.0279
trigger times: 0
Loss after 465631920 batches: 0.0227
trigger times: 1
Loss after 465763020 batches: 0.0212
trigger times: 2
Loss after 465894120 batches: 0.0198
trigger times: 3
Loss after 466025220 batches: 0.0192
trigger times: 0
Loss after 466156320 batches: 0.0184
trigger times: 1
Loss after 466287420 batches: 0.0179
trigger times: 2
Loss after 466418520 batches: 0.0175
trigger times: 3
Loss after 466549620 batches: 0.0172
trigger times: 4
Loss after 466680720 batches: 0.0170
trigger times: 5
Loss after 466811820 batches: 0.0167
trigger times: 0
Loss after 466942920 batches: 0.0166
trigger times: 1
Loss after 467074020 batches: 0.0161
trigger times: 2
Loss after 467205120 batches: 0.0162
trigger times: 3
Loss after 467336220 batches: 0.0159
trigger times: 4
Loss after 467467320 batches: 0.0159
trigger times: 5
Loss after 467598420 batches: 0.0158
trigger times: 6
Loss after 467729520 batches: 0.0156
trigger times: 7
Loss after 467860620 batches: 0.0154
trigger times: 8
Loss after 467991720 batches: 0.0154
trigger times: 9
Loss after 468122820 batches: 0.0152
trigger times: 10
Loss after 468253920 batches: 0.0152
trigger times: 11
Loss after 468385020 batches: 0.0148
trigger times: 0
Loss after 468516120 batches: 0.0148
trigger times: 1
Loss after 468647220 batches: 0.0149
trigger times: 2
Loss after 468778320 batches: 0.0149
trigger times: 3
Loss after 468909420 batches: 0.0146
trigger times: 4
Loss after 469040520 batches: 0.0147
trigger times: 5
Loss after 469171620 batches: 0.0146
trigger times: 6
Loss after 469302720 batches: 0.0144
trigger times: 7
Loss after 469433820 batches: 0.0144
trigger times: 8
Loss after 469564920 batches: 0.0144
trigger times: 9
Loss after 469696020 batches: 0.0144
trigger times: 10
Loss after 469827120 batches: 0.0141
trigger times: 11
Loss after 469958220 batches: 0.0143
trigger times: 12
Loss after 470089320 batches: 0.0140
trigger times: 13
Loss after 470220420 batches: 0.0141
trigger times: 14
Loss after 470351520 batches: 0.0141
trigger times: 15
Loss after 470482620 batches: 0.0138
trigger times: 16
Loss after 470613720 batches: 0.0139
trigger times: 17
Loss after 470744820 batches: 0.0138
trigger times: 18
Loss after 470875920 batches: 0.0139
trigger times: 19
Loss after 471007020 batches: 0.0139
trigger times: 20
Early stopping!
Start to test process.
Loss after 471138120 batches: 0.0138
Time to train on one home:  329.8993194103241
trigger times: 0
Loss after 471266760 batches: 0.0653
trigger times: 1
Loss after 471395400 batches: 0.0198
trigger times: 2
Loss after 471524040 batches: 0.0161
trigger times: 3
Loss after 471652680 batches: 0.0149
trigger times: 4
Loss after 471781320 batches: 0.0141
trigger times: 5
Loss after 471909960 batches: 0.0135
trigger times: 6
Loss after 472038600 batches: 0.0132
trigger times: 7
Loss after 472167240 batches: 0.0128
trigger times: 8
Loss after 472295880 batches: 0.0125
trigger times: 9
Loss after 472424520 batches: 0.0123
trigger times: 10
Loss after 472553160 batches: 0.0121
trigger times: 11
Loss after 472681800 batches: 0.0116
trigger times: 12
Loss after 472810440 batches: 0.0114
trigger times: 13
Loss after 472939080 batches: 0.0113
trigger times: 14
Loss after 473067720 batches: 0.0111
trigger times: 15
Loss after 473196360 batches: 0.0112
trigger times: 16
Loss after 473325000 batches: 0.0108
trigger times: 17
Loss after 473453640 batches: 0.0108
trigger times: 18
Loss after 473582280 batches: 0.0107
trigger times: 19
Loss after 473710920 batches: 0.0105
trigger times: 20
Early stopping!
Start to test process.
Loss after 473839560 batches: 0.0103
Time to train on one home:  157.92410802841187
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765, 0.013892934501727369, 0.013666319778952196, 0.012580597168674625, 0.012713179911946834, 0.012576136202707514, 0.012611944796672613, 0.012838088819402186, 0.012534839612687296, 0.011462489906889636, 0.011815904885568062]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806], [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324], [0.4718517627980974, 0.4902251689616832, 0.5549523261866275, 0.907874701331254, 0.4176047824455843, 21.448865222845452, 1289.172], [0.4761729836463928, 0.48556449560991066, 0.5560194037302575, 0.9166221286580782, 0.42142278082960144, 21.655526328727763, 1300.9584], [0.46398472951518166, 0.49880328154927556, 0.5698825241915405, 0.8946974979237889, 0.41057763904260586, 21.137549069320855, 1267.4788], [0.469333012898763, 0.49303020279751153, 0.5671409183881702, 0.9028353043263873, 0.41530691390943525, 21.329807662365685, 1282.0784], [0.4544395903746287, 0.5091638361980007, 0.5806109676618026, 0.8752694784842265, 0.40209032875056233, 20.678555146596764, 1241.278], [0.4510269910097122, 0.5128308573472844, 0.5837508100122663, 0.8712460572392167, 0.39908632487271106, 20.583500377593502, 1232.0043], [0.4509678996271557, 0.5128822638311595, 0.5867076027273368, 0.8703151627515375, 0.39904421295935666, 20.56150766166735, 1231.8744], [0.45386744373374516, 0.5097697277751618, 0.5898363987389426, 0.8749398916603841, 0.40159398564992166, 20.67076853975173, 1239.7456], [0.45670779380533433, 0.5067050230151762, 0.5878313777496765, 0.8791882433222485, 0.40410457520167925, 20.771137370473596, 1247.4961]]
Round_17_results:  [0.45670779380533433, 0.5067050230151762, 0.5878313777496765, 0.8791882433222485, 0.40410457520167925, 20.771137370473596, 1247.4961]
trigger times: 0
Loss after 473970660 batches: 0.0544
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3711 < 3712; dropping {'Training_Loss': 0.05442917241521601, 'Validation_Loss': 0.23631875879234737, 'Training_R2': 0.9451942344099503, 'Validation_R2': 0.7801966506813043, 'Training_F1': 0.8749170395091546, 'Validation_F1': 0.7355091178943418, 'Training_NEP': 0.25028102183401096, 'Validation_NEP': 0.514552105176125, 'Training_NDE': 0.04114366103229724, 'Validation_NDE': 0.17503980065957425, 'Training_MAE': 8.28881108235258, 'Validation_MAE': 14.11115346984624, 'Training_MSE': 181.02567, 'Validation_MSE': 646.4172}.
trigger times: 0
Loss after 474101760 batches: 0.0146
trigger times: 0
Loss after 474232860 batches: 0.0114
trigger times: 0
Loss after 474363960 batches: 0.0101
trigger times: 0
Loss after 474495060 batches: 0.0097
trigger times: 1
Loss after 474626160 batches: 0.0092
trigger times: 2
Loss after 474757260 batches: 0.0090
trigger times: 0
Loss after 474888360 batches: 0.0086
trigger times: 1
Loss after 475019460 batches: 0.0083
trigger times: 2
Loss after 475150560 batches: 0.0083
trigger times: 3
Loss after 475281660 batches: 0.0081
trigger times: 4
Loss after 475412760 batches: 0.0078
trigger times: 5
Loss after 475543860 batches: 0.0077
trigger times: 0
Loss after 475674960 batches: 0.0079
trigger times: 1
Loss after 475806060 batches: 0.0078
trigger times: 2
Loss after 475937160 batches: 0.0075
trigger times: 3
Loss after 476068260 batches: 0.0075
trigger times: 4
Loss after 476199360 batches: 0.0075
trigger times: 5
Loss after 476330460 batches: 0.0074
trigger times: 6
Loss after 476461560 batches: 0.0073
trigger times: 7
Loss after 476592660 batches: 0.0073
trigger times: 8
Loss after 476723760 batches: 0.0072
trigger times: 9
Loss after 476854860 batches: 0.0071
trigger times: 10
Loss after 476985960 batches: 0.0073
trigger times: 11
Loss after 477117060 batches: 0.0070
trigger times: 0
Loss after 477248160 batches: 0.0070
trigger times: 1
Loss after 477379260 batches: 0.0070
trigger times: 0
Loss after 477510360 batches: 0.0069
trigger times: 1
Loss after 477641460 batches: 0.0069
trigger times: 2
Loss after 477772560 batches: 0.0067
trigger times: 3
Loss after 477903660 batches: 0.0066
trigger times: 4
Loss after 478034760 batches: 0.0067
trigger times: 5
Loss after 478165860 batches: 0.0066
trigger times: 6
Loss after 478296960 batches: 0.0065
trigger times: 7
Loss after 478428060 batches: 0.0067
trigger times: 8
Loss after 478559160 batches: 0.0065
trigger times: 9
Loss after 478690260 batches: 0.0066
trigger times: 10
Loss after 478821360 batches: 0.0065
trigger times: 11
Loss after 478952460 batches: 0.0065
trigger times: 12
Loss after 479083560 batches: 0.0064
trigger times: 13
Loss after 479214660 batches: 0.0065
trigger times: 14
Loss after 479345760 batches: 0.0063
trigger times: 15
Loss after 479476860 batches: 0.0063
trigger times: 16
Loss after 479607960 batches: 0.0064
trigger times: 17
Loss after 479739060 batches: 0.0063
trigger times: 18
Loss after 479870160 batches: 0.0063
trigger times: 19
Loss after 480001260 batches: 0.0063
trigger times: 20
Early stopping!
Start to test process.
Loss after 480132360 batches: 0.0063
Time to train on one home:  350.98242259025574
trigger times: 0
Loss after 480234960 batches: 0.1546
trigger times: 1
Loss after 480337560 batches: 0.0461
trigger times: 2
Loss after 480440160 batches: 0.0431
trigger times: 3
Loss after 480542760 batches: 0.0330
trigger times: 4
Loss after 480645360 batches: 0.0292
trigger times: 5
Loss after 480747960 batches: 0.0260
trigger times: 6
Loss after 480850560 batches: 0.0245
trigger times: 7
Loss after 480953160 batches: 0.0279
trigger times: 8
Loss after 481055760 batches: 0.0266
trigger times: 9
Loss after 481158360 batches: 0.0233
trigger times: 10
Loss after 481260960 batches: 0.0227
trigger times: 11
Loss after 481363560 batches: 0.0218
trigger times: 12
Loss after 481466160 batches: 0.0221
trigger times: 13
Loss after 481568760 batches: 0.0209
trigger times: 14
Loss after 481671360 batches: 0.0203
trigger times: 15
Loss after 481773960 batches: 0.0206
trigger times: 16
Loss after 481876560 batches: 0.0195
trigger times: 17
Loss after 481979160 batches: 0.0204
trigger times: 18
Loss after 482081760 batches: 0.0228
trigger times: 19
Loss after 482184360 batches: 0.0215
trigger times: 20
Early stopping!
Start to test process.
Loss after 482286960 batches: 0.0193
Time to train on one home:  131.88701605796814
trigger times: 0
Loss after 482418060 batches: 0.0520
trigger times: 1
Loss after 482549160 batches: 0.0186
trigger times: 2
Loss after 482680260 batches: 0.0156
trigger times: 0
Loss after 482811360 batches: 0.0145
trigger times: 1
Loss after 482942460 batches: 0.0138
trigger times: 0
Loss after 483073560 batches: 0.0132
trigger times: 1
Loss after 483204660 batches: 0.0129
trigger times: 2
Loss after 483335760 batches: 0.0124
trigger times: 3
Loss after 483466860 batches: 0.0123
trigger times: 4
Loss after 483597960 batches: 0.0120
trigger times: 5
Loss after 483729060 batches: 0.0119
trigger times: 6
Loss after 483860160 batches: 0.0116
trigger times: 7
Loss after 483991260 batches: 0.0115
trigger times: 8
Loss after 484122360 batches: 0.0114
trigger times: 9
Loss after 484253460 batches: 0.0112
trigger times: 10
Loss after 484384560 batches: 0.0111
trigger times: 11
Loss after 484515660 batches: 0.0109
trigger times: 12
Loss after 484646760 batches: 0.0110
trigger times: 13
Loss after 484777860 batches: 0.0109
trigger times: 0
Loss after 484908960 batches: 0.0108
trigger times: 1
Loss after 485040060 batches: 0.0107
trigger times: 2
Loss after 485171160 batches: 0.0107
trigger times: 3
Loss after 485302260 batches: 0.0104
trigger times: 4
Loss after 485433360 batches: 0.0104
trigger times: 5
Loss after 485564460 batches: 0.0104
trigger times: 6
Loss after 485695560 batches: 0.0103
trigger times: 7
Loss after 485826660 batches: 0.0104
trigger times: 8
Loss after 485957760 batches: 0.0102
trigger times: 9
Loss after 486088860 batches: 0.0103
trigger times: 10
Loss after 486219960 batches: 0.0101
trigger times: 11
Loss after 486351060 batches: 0.0102
trigger times: 12
Loss after 486482160 batches: 0.0102
trigger times: 13
Loss after 486613260 batches: 0.0100
trigger times: 14
Loss after 486744360 batches: 0.0100
trigger times: 15
Loss after 486875460 batches: 0.0099
trigger times: 16
Loss after 487006560 batches: 0.0099
trigger times: 17
Loss after 487137660 batches: 0.0098
trigger times: 18
Loss after 487268760 batches: 0.0098
trigger times: 19
Loss after 487399860 batches: 0.0099
trigger times: 20
Early stopping!
Start to test process.
Loss after 487530960 batches: 0.0098
Time to train on one home:  293.97812056541443
trigger times: 0
Loss after 487662060 batches: 0.1036
trigger times: 1
Loss after 487793160 batches: 0.0270
trigger times: 2
Loss after 487924260 batches: 0.0221
trigger times: 3
Loss after 488055360 batches: 0.0206
trigger times: 0
Loss after 488186460 batches: 0.0195
trigger times: 1
Loss after 488317560 batches: 0.0186
trigger times: 2
Loss after 488448660 batches: 0.0180
trigger times: 0
Loss after 488579760 batches: 0.0176
trigger times: 1
Loss after 488710860 batches: 0.0172
trigger times: 2
Loss after 488841960 batches: 0.0171
trigger times: 3
Loss after 488973060 batches: 0.0168
trigger times: 4
Loss after 489104160 batches: 0.0164
trigger times: 5
Loss after 489235260 batches: 0.0164
trigger times: 6
Loss after 489366360 batches: 0.0163
trigger times: 7
Loss after 489497460 batches: 0.0160
trigger times: 8
Loss after 489628560 batches: 0.0158
trigger times: 9
Loss after 489759660 batches: 0.0158
trigger times: 10
Loss after 489890760 batches: 0.0155
trigger times: 11
Loss after 490021860 batches: 0.0156
trigger times: 12
Loss after 490152960 batches: 0.0154
trigger times: 13
Loss after 490284060 batches: 0.0151
trigger times: 14
Loss after 490415160 batches: 0.0151
trigger times: 15
Loss after 490546260 batches: 0.0150
trigger times: 16
Loss after 490677360 batches: 0.0149
trigger times: 17
Loss after 490808460 batches: 0.0147
trigger times: 18
Loss after 490939560 batches: 0.0146
trigger times: 19
Loss after 491070660 batches: 0.0144
trigger times: 20
Early stopping!
Start to test process.
Loss after 491201760 batches: 0.0146
Time to train on one home:  209.09268140792847
trigger times: 0
Loss after 491330400 batches: 0.0636
trigger times: 1
Loss after 491459040 batches: 0.0198
trigger times: 0
Loss after 491587680 batches: 0.0162
trigger times: 1
Loss after 491716320 batches: 0.0148
trigger times: 2
Loss after 491844960 batches: 0.0139
trigger times: 3
Loss after 491973600 batches: 0.0136
trigger times: 4
Loss after 492102240 batches: 0.0131
trigger times: 5
Loss after 492230880 batches: 0.0125
trigger times: 6
Loss after 492359520 batches: 0.0127
trigger times: 7
Loss after 492488160 batches: 0.0120
trigger times: 8
Loss after 492616800 batches: 0.0118
trigger times: 9
Loss after 492745440 batches: 0.0114
trigger times: 10
Loss after 492874080 batches: 0.0113
trigger times: 11
Loss after 493002720 batches: 0.0112
trigger times: 12
Loss after 493131360 batches: 0.0110
trigger times: 13
Loss after 493260000 batches: 0.0109
trigger times: 14
Loss after 493388640 batches: 0.0107
trigger times: 15
Loss after 493517280 batches: 0.0106
trigger times: 16
Loss after 493645920 batches: 0.0104
trigger times: 17
Loss after 493774560 batches: 0.0105
trigger times: 18
Loss after 493903200 batches: 0.0103
trigger times: 19
Loss after 494031840 batches: 0.0103
trigger times: 20
Early stopping!
Start to test process.
Loss after 494160480 batches: 0.0103
Time to train on one home:  171.1441843509674
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765, 0.013892934501727369, 0.013666319778952196, 0.012580597168674625, 0.012713179911946834, 0.012576136202707514, 0.012611944796672613, 0.012838088819402186, 0.012534839612687296, 0.011462489906889636, 0.011815904885568062, 0.012044141928347562]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806], [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324], [0.4718517627980974, 0.4902251689616832, 0.5549523261866275, 0.907874701331254, 0.4176047824455843, 21.448865222845452, 1289.172], [0.4761729836463928, 0.48556449560991066, 0.5560194037302575, 0.9166221286580782, 0.42142278082960144, 21.655526328727763, 1300.9584], [0.46398472951518166, 0.49880328154927556, 0.5698825241915405, 0.8946974979237889, 0.41057763904260586, 21.137549069320855, 1267.4788], [0.469333012898763, 0.49303020279751153, 0.5671409183881702, 0.9028353043263873, 0.41530691390943525, 21.329807662365685, 1282.0784], [0.4544395903746287, 0.5091638361980007, 0.5806109676618026, 0.8752694784842265, 0.40209032875056233, 20.678555146596764, 1241.278], [0.4510269910097122, 0.5128308573472844, 0.5837508100122663, 0.8712460572392167, 0.39908632487271106, 20.583500377593502, 1232.0043], [0.4509678996271557, 0.5128822638311595, 0.5867076027273368, 0.8703151627515375, 0.39904421295935666, 20.56150766166735, 1231.8744], [0.45386744373374516, 0.5097697277751618, 0.5898363987389426, 0.8749398916603841, 0.40159398564992166, 20.67076853975173, 1239.7456], [0.45670779380533433, 0.5067050230151762, 0.5878313777496765, 0.8791882433222485, 0.40410457520167925, 20.771137370473596, 1247.4961], [0.44424038959874046, 0.5202382198144997, 0.5975807384653797, 0.8552961328632953, 0.39301825363169574, 20.206677697379956, 1213.2719]]
Round_18_results:  [0.44424038959874046, 0.5202382198144997, 0.5975807384653797, 0.8552961328632953, 0.39301825363169574, 20.206677697379956, 1213.2719]
trigger times: 0
Loss after 494291580 batches: 0.0570
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3871 < 3872; dropping {'Training_Loss': 0.057007961765915716, 'Validation_Loss': 0.23347615036699507, 'Training_R2': 0.9425797097638899, 'Validation_R2': 0.782856492893039, 'Training_F1': 0.8716908545400315, 'Validation_F1': 0.7585333168325584, 'Training_NEP': 0.2565893540479892, 'Validation_NEP': 0.4796481893656657, 'Training_NDE': 0.04310643109197913, 'Validation_NDE': 0.1729216425333623, 'Training_MAE': 8.4977305345077, 'Validation_MAE': 13.153943290847177, 'Training_MSE': 189.66154, 'Validation_MSE': 638.5949}.
trigger times: 0
Loss after 494422680 batches: 0.0142
trigger times: 1
Loss after 494553780 batches: 0.0111
trigger times: 0
Loss after 494684880 batches: 0.0102
trigger times: 1
Loss after 494815980 batches: 0.0096
trigger times: 2
Loss after 494947080 batches: 0.0092
trigger times: 3
Loss after 495078180 batches: 0.0088
trigger times: 4
Loss after 495209280 batches: 0.0086
trigger times: 5
Loss after 495340380 batches: 0.0084
trigger times: 6
Loss after 495471480 batches: 0.0082
trigger times: 0
Loss after 495602580 batches: 0.0080
trigger times: 1
Loss after 495733680 batches: 0.0078
trigger times: 2
Loss after 495864780 batches: 0.0077
trigger times: 3
Loss after 495995880 batches: 0.0076
trigger times: 4
Loss after 496126980 batches: 0.0076
trigger times: 5
Loss after 496258080 batches: 0.0074
trigger times: 6
Loss after 496389180 batches: 0.0075
trigger times: 7
Loss after 496520280 batches: 0.0073
trigger times: 8
Loss after 496651380 batches: 0.0072
trigger times: 9
Loss after 496782480 batches: 0.0073
trigger times: 10
Loss after 496913580 batches: 0.0074
trigger times: 11
Loss after 497044680 batches: 0.0071
trigger times: 12
Loss after 497175780 batches: 0.0070
trigger times: 13
Loss after 497306880 batches: 0.0070
trigger times: 14
Loss after 497437980 batches: 0.0071
trigger times: 15
Loss after 497569080 batches: 0.0069
trigger times: 0
Loss after 497700180 batches: 0.0070
trigger times: 1
Loss after 497831280 batches: 0.0069
trigger times: 2
Loss after 497962380 batches: 0.0068
trigger times: 3
Loss after 498093480 batches: 0.0067
trigger times: 4
Loss after 498224580 batches: 0.0068
trigger times: 5
Loss after 498355680 batches: 0.0068
trigger times: 6
Loss after 498486780 batches: 0.0068
trigger times: 7
Loss after 498617880 batches: 0.0066
trigger times: 8
Loss after 498748980 batches: 0.0068
trigger times: 9
Loss after 498880080 batches: 0.0066
trigger times: 10
Loss after 499011180 batches: 0.0068
trigger times: 11
Loss after 499142280 batches: 0.0065
trigger times: 12
Loss after 499273380 batches: 0.0065
trigger times: 13
Loss after 499404480 batches: 0.0064
trigger times: 14
Loss after 499535580 batches: 0.0063
trigger times: 15
Loss after 499666680 batches: 0.0064
trigger times: 16
Loss after 499797780 batches: 0.0065
trigger times: 17
Loss after 499928880 batches: 0.0062
trigger times: 18
Loss after 500059980 batches: 0.0064
trigger times: 19
Loss after 500191080 batches: 0.0063
trigger times: 20
Early stopping!
Start to test process.
Loss after 500322180 batches: 0.0062
Time to train on one home:  343.1210765838623
trigger times: 0
Loss after 500424780 batches: 0.1420
trigger times: 0
Loss after 500527380 batches: 0.0426
trigger times: 1
Loss after 500629980 batches: 0.0334
trigger times: 2
Loss after 500732580 batches: 0.0310
trigger times: 3
Loss after 500835180 batches: 0.0283
trigger times: 0
Loss after 500937780 batches: 0.0255
trigger times: 1
Loss after 501040380 batches: 0.0251
trigger times: 2
Loss after 501142980 batches: 0.0237
trigger times: 3
Loss after 501245580 batches: 0.0242
trigger times: 4
Loss after 501348180 batches: 0.0239
trigger times: 5
Loss after 501450780 batches: 0.0216
trigger times: 6
Loss after 501553380 batches: 0.0222
trigger times: 7
Loss after 501655980 batches: 0.0245
trigger times: 8
Loss after 501758580 batches: 0.0216
trigger times: 9
Loss after 501861180 batches: 0.0219
trigger times: 10
Loss after 501963780 batches: 0.0214
trigger times: 11
Loss after 502066380 batches: 0.0210
trigger times: 12
Loss after 502168980 batches: 0.0192
trigger times: 13
Loss after 502271580 batches: 0.0189
trigger times: 14
Loss after 502374180 batches: 0.0186
trigger times: 15
Loss after 502476780 batches: 0.0192
trigger times: 16
Loss after 502579380 batches: 0.0182
trigger times: 17
Loss after 502681980 batches: 0.0181
trigger times: 18
Loss after 502784580 batches: 0.0177
trigger times: 19
Loss after 502887180 batches: 0.0174
trigger times: 20
Early stopping!
Start to test process.
Loss after 502989780 batches: 0.0174
Time to train on one home:  160.3533799648285
trigger times: 0
Loss after 503120880 batches: 0.0509
trigger times: 1
Loss after 503251980 batches: 0.0189
trigger times: 0
Loss after 503383080 batches: 0.0157
trigger times: 1
Loss after 503514180 batches: 0.0143
trigger times: 2
Loss after 503645280 batches: 0.0137
trigger times: 0
Loss after 503776380 batches: 0.0131
trigger times: 1
Loss after 503907480 batches: 0.0126
trigger times: 2
Loss after 504038580 batches: 0.0125
trigger times: 3
Loss after 504169680 batches: 0.0121
trigger times: 4
Loss after 504300780 batches: 0.0120
trigger times: 5
Loss after 504431880 batches: 0.0118
trigger times: 6
Loss after 504562980 batches: 0.0116
trigger times: 7
Loss after 504694080 batches: 0.0114
trigger times: 8
Loss after 504825180 batches: 0.0112
trigger times: 9
Loss after 504956280 batches: 0.0111
trigger times: 10
Loss after 505087380 batches: 0.0110
trigger times: 11
Loss after 505218480 batches: 0.0108
trigger times: 12
Loss after 505349580 batches: 0.0107
trigger times: 13
Loss after 505480680 batches: 0.0107
trigger times: 14
Loss after 505611780 batches: 0.0107
trigger times: 0
Loss after 505742880 batches: 0.0105
trigger times: 1
Loss after 505873980 batches: 0.0107
trigger times: 2
Loss after 506005080 batches: 0.0105
trigger times: 3
Loss after 506136180 batches: 0.0103
trigger times: 4
Loss after 506267280 batches: 0.0102
trigger times: 5
Loss after 506398380 batches: 0.0103
trigger times: 6
Loss after 506529480 batches: 0.0101
trigger times: 7
Loss after 506660580 batches: 0.0101
trigger times: 8
Loss after 506791680 batches: 0.0100
trigger times: 9
Loss after 506922780 batches: 0.0099
trigger times: 10
Loss after 507053880 batches: 0.0101
trigger times: 11
Loss after 507184980 batches: 0.0099
trigger times: 12
Loss after 507316080 batches: 0.0098
trigger times: 13
Loss after 507447180 batches: 0.0099
trigger times: 14
Loss after 507578280 batches: 0.0097
trigger times: 15
Loss after 507709380 batches: 0.0097
trigger times: 16
Loss after 507840480 batches: 0.0097
trigger times: 17
Loss after 507971580 batches: 0.0097
trigger times: 18
Loss after 508102680 batches: 0.0100
trigger times: 19
Loss after 508233780 batches: 0.0097
trigger times: 20
Early stopping!
Start to test process.
Loss after 508364880 batches: 0.0095
Time to train on one home:  301.2025558948517
trigger times: 0
Loss after 508495980 batches: 0.1054
trigger times: 1
Loss after 508627080 batches: 0.0263
trigger times: 2
Loss after 508758180 batches: 0.0225
trigger times: 3
Loss after 508889280 batches: 0.0206
trigger times: 4
Loss after 509020380 batches: 0.0195
trigger times: 5
Loss after 509151480 batches: 0.0186
trigger times: 6
Loss after 509282580 batches: 0.0182
trigger times: 7
Loss after 509413680 batches: 0.0177
trigger times: 8
Loss after 509544780 batches: 0.0175
trigger times: 9
Loss after 509675880 batches: 0.0172
trigger times: 0
Loss after 509806980 batches: 0.0168
trigger times: 0
Loss after 509938080 batches: 0.0168
trigger times: 1
Loss after 510069180 batches: 0.0165
trigger times: 2
Loss after 510200280 batches: 0.0161
trigger times: 3
Loss after 510331380 batches: 0.0162
trigger times: 4
Loss after 510462480 batches: 0.0159
trigger times: 5
Loss after 510593580 batches: 0.0159
trigger times: 6
Loss after 510724680 batches: 0.0157
trigger times: 7
Loss after 510855780 batches: 0.0155
trigger times: 8
Loss after 510986880 batches: 0.0153
trigger times: 9
Loss after 511117980 batches: 0.0152
trigger times: 10
Loss after 511249080 batches: 0.0154
trigger times: 11
Loss after 511380180 batches: 0.0152
trigger times: 12
Loss after 511511280 batches: 0.0151
trigger times: 13
Loss after 511642380 batches: 0.0150
trigger times: 14
Loss after 511773480 batches: 0.0150
trigger times: 15
Loss after 511904580 batches: 0.0148
trigger times: 16
Loss after 512035680 batches: 0.0148
trigger times: 17
Loss after 512166780 batches: 0.0145
trigger times: 18
Loss after 512297880 batches: 0.0146
trigger times: 19
Loss after 512428980 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 512560080 batches: 0.0144
Time to train on one home:  236.86685252189636
trigger times: 0
Loss after 512688720 batches: 0.0579
trigger times: 0
Loss after 512817360 batches: 0.0192
trigger times: 1
Loss after 512946000 batches: 0.0160
trigger times: 2
Loss after 513074640 batches: 0.0145
trigger times: 3
Loss after 513203280 batches: 0.0138
trigger times: 4
Loss after 513331920 batches: 0.0132
trigger times: 5
Loss after 513460560 batches: 0.0129
trigger times: 6
Loss after 513589200 batches: 0.0125
trigger times: 7
Loss after 513717840 batches: 0.0122
trigger times: 8
Loss after 513846480 batches: 0.0120
trigger times: 9
Loss after 513975120 batches: 0.0117
trigger times: 10
Loss after 514103760 batches: 0.0115
trigger times: 11
Loss after 514232400 batches: 0.0114
trigger times: 12
Loss after 514361040 batches: 0.0108
trigger times: 13
Loss after 514489680 batches: 0.0109
trigger times: 14
Loss after 514618320 batches: 0.0110
trigger times: 15
Loss after 514746960 batches: 0.0108
trigger times: 16
Loss after 514875600 batches: 0.0105
trigger times: 17
Loss after 515004240 batches: 0.0105
trigger times: 18
Loss after 515132880 batches: 0.0105
trigger times: 19
Loss after 515261520 batches: 0.0101
trigger times: 20
Early stopping!
Start to test process.
Loss after 515390160 batches: 0.0102
Time to train on one home:  164.5284342765808
train_results:  [0.0659879873575018, 0.05127666929189745, 0.028927590247304534, 0.023148100453605484, 0.01726877913671345, 0.015775044789698444, 0.014172783408445813, 0.014250902559234765, 0.013892934501727369, 0.013666319778952196, 0.012580597168674625, 0.012713179911946834, 0.012576136202707514, 0.012611944796672613, 0.012838088819402186, 0.012534839612687296, 0.011462489906889636, 0.011815904885568062, 0.012044141928347562, 0.011551833402559062]
test_results:  [[0.8984029127491845, 0.028145052108920265, 0.2210337818102859, 1.5293241675172422, 0.7961383131765748, 36.13083160376463, 2457.7285], [0.6066901932160059, 0.3442085542570783, 0.42491372655237375, 0.9992038636857513, 0.5372208029010428, 23.60654941801529, 1658.4341], [0.5738437854581409, 0.3798185280976911, 0.454875491463851, 1.0475337235715672, 0.5080493050687297, 24.748359680389417, 1568.3798], [0.5220911188258065, 0.43588634510203095, 0.49102284176159583, 0.9762139757469454, 0.4621188528441548, 23.063405075338153, 1426.5896], [0.5160997642411126, 0.44231729953045495, 0.5108403821910773, 0.9907924599716846, 0.45685064978373824, 23.407826990423235, 1410.3264], [0.5023446612887912, 0.4571902217324104, 0.5258314502657142, 0.9690608470897513, 0.4446668324151422, 22.89440984696043, 1372.7141], [0.5039418389399847, 0.45545441758901795, 0.5260863750295321, 0.9834027532422971, 0.44608879377442096, 23.233242520294738, 1377.1039], [0.49032413131660885, 0.4702131076681245, 0.5366453806152074, 0.944253839560551, 0.43399855474258514, 22.308335402661328, 1339.7806], [0.48050736056433785, 0.48082964340175616, 0.54796437325753, 0.9290026944536683, 0.42530154613127574, 21.948021633137937, 1312.9324], [0.4718517627980974, 0.4902251689616832, 0.5549523261866275, 0.907874701331254, 0.4176047824455843, 21.448865222845452, 1289.172], [0.4761729836463928, 0.48556449560991066, 0.5560194037302575, 0.9166221286580782, 0.42142278082960144, 21.655526328727763, 1300.9584], [0.46398472951518166, 0.49880328154927556, 0.5698825241915405, 0.8946974979237889, 0.41057763904260586, 21.137549069320855, 1267.4788], [0.469333012898763, 0.49303020279751153, 0.5671409183881702, 0.9028353043263873, 0.41530691390943525, 21.329807662365685, 1282.0784], [0.4544395903746287, 0.5091638361980007, 0.5806109676618026, 0.8752694784842265, 0.40209032875056233, 20.678555146596764, 1241.278], [0.4510269910097122, 0.5128308573472844, 0.5837508100122663, 0.8712460572392167, 0.39908632487271106, 20.583500377593502, 1232.0043], [0.4509678996271557, 0.5128822638311595, 0.5867076027273368, 0.8703151627515375, 0.39904421295935666, 20.56150766166735, 1231.8744], [0.45386744373374516, 0.5097697277751618, 0.5898363987389426, 0.8749398916603841, 0.40159398564992166, 20.67076853975173, 1239.7456], [0.45670779380533433, 0.5067050230151762, 0.5878313777496765, 0.8791882433222485, 0.40410457520167925, 20.771137370473596, 1247.4961], [0.44424038959874046, 0.5202382198144997, 0.5975807384653797, 0.8552961328632953, 0.39301825363169574, 20.206677697379956, 1213.2719], [0.45246780084239113, 0.5113156109383815, 0.5933235222903817, 0.87436049071403, 0.4003276065296585, 20.657079984723026, 1235.8362]]
Round_19_results:  [0.45246780084239113, 0.5113156109383815, 0.5933235222903817, 0.87436049071403, 0.4003276065296585, 20.657079984723026, 1235.8362]