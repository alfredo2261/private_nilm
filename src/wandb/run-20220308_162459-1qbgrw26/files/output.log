LSTM(
  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))
  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))
  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))
  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))
  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))
  (linear1): Linear(in_features=23500, out_features=1024, bias=True)
  (linear2): Linear(in_features=1024, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (dropout): Dropout(p=0.2, inplace=False)
)
Window Length:  499
trigger times: 0
Loss after 131100 batches: 0.8065
trigger times: 0
Loss after 262200 batches: 0.3936
trigger times: 1
Loss after 393300 batches: 0.2832
trigger times: 0
Loss after 524400 batches: 0.2197
trigger times: 1
Loss after 655500 batches: 0.1943
trigger times: 2
Loss after 786600 batches: 0.1623
trigger times: 3
Loss after 917700 batches: 0.1439
trigger times: 4
Loss after 1048800 batches: 0.1287
trigger times: 5
Loss after 1179900 batches: 0.1140
trigger times: 6
Loss after 1311000 batches: 0.1033
trigger times: 7
Loss after 1442100 batches: 0.0933
trigger times: 8
Loss after 1573200 batches: 0.0830
trigger times: 0
Loss after 1704300 batches: 0.0746
trigger times: 1
Loss after 1835400 batches: 0.0685
trigger times: 2
Loss after 1966500 batches: 0.0628
trigger times: 0
Loss after 2097600 batches: 0.0590
trigger times: 0
Loss after 2228700 batches: 0.0538
trigger times: 1
Loss after 2359800 batches: 0.0501
trigger times: 2
Loss after 2490900 batches: 0.0479
trigger times: 0
Loss after 2622000 batches: 0.0443
trigger times: 1
Loss after 2753100 batches: 0.0411
trigger times: 0
Loss after 2884200 batches: 0.0392
trigger times: 0
Loss after 3015300 batches: 0.0395
trigger times: 1
Loss after 3146400 batches: 0.0356
trigger times: 2
Loss after 3277500 batches: 0.0347
trigger times: 0
Loss after 3408600 batches: 0.0334
trigger times: 1
Loss after 3539700 batches: 0.0312
trigger times: 0
Loss after 3670800 batches: 0.0312
trigger times: 0
Loss after 3801900 batches: 0.0301
trigger times: 1
Loss after 3933000 batches: 0.0295
trigger times: 0
Loss after 4064100 batches: 0.0282
trigger times: 1
Loss after 4195200 batches: 0.0278
trigger times: 0
Loss after 4326300 batches: 0.0271
trigger times: 1
Loss after 4457400 batches: 0.0271
trigger times: 2
Loss after 4588500 batches: 0.0262
trigger times: 3
Loss after 4719600 batches: 0.0257
trigger times: 4
Loss after 4850700 batches: 0.0244
trigger times: 5
Loss after 4981800 batches: 0.0238
trigger times: 0
Loss after 5112900 batches: 0.0229
trigger times: 1
Loss after 5244000 batches: 0.0229
trigger times: 2
Loss after 5375100 batches: 0.0218
trigger times: 0
Loss after 5506200 batches: 0.0221
trigger times: 1
Loss after 5637300 batches: 0.0213
trigger times: 2
Loss after 5768400 batches: 0.0211
trigger times: 0
Loss after 5899500 batches: 0.0206
trigger times: 1
Loss after 6030600 batches: 0.0206
trigger times: 2
Loss after 6161700 batches: 0.0198
trigger times: 3
Loss after 6292800 batches: 0.0192
trigger times: 4
Loss after 6423900 batches: 0.0191
trigger times: 5
Loss after 6555000 batches: 0.0190
trigger times: 6
Loss after 6686100 batches: 0.0185
trigger times: 0
Loss after 6817200 batches: 0.0182
trigger times: 1
Loss after 6948300 batches: 0.0183
trigger times: 2
Loss after 7079400 batches: 0.0183
trigger times: 3
Loss after 7210500 batches: 0.0175
trigger times: 4
Loss after 7341600 batches: 0.0178
trigger times: 5
Loss after 7472700 batches: 0.0175
trigger times: 6
Loss after 7603800 batches: 0.0169
trigger times: 7
Loss after 7734900 batches: 0.0162
trigger times: 8
Loss after 7866000 batches: 0.0161
trigger times: 9
Loss after 7997100 batches: 0.0163
trigger times: 10
Loss after 8128200 batches: 0.0161
trigger times: 11
Loss after 8259300 batches: 0.0155
trigger times: 12
Loss after 8390400 batches: 0.0156
trigger times: 13
Loss after 8521500 batches: 0.0157
trigger times: 14
Loss after 8652600 batches: 0.0154
trigger times: 15
Loss after 8783700 batches: 0.0154
trigger times: 16
Loss after 8914800 batches: 0.0154
trigger times: 17
Loss after 9045900 batches: 0.0153
trigger times: 18
Loss after 9177000 batches: 0.0146
trigger times: 19
Loss after 9308100 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 9439200 batches: 0.0144
Time to train on one home:  519.9957544803619
trigger times: 0
Loss after 9541800 batches: 0.9693
trigger times: 1
Loss after 9644400 batches: 0.7736
trigger times: 0
Loss after 9747000 batches: 0.6619
trigger times: 0
Loss after 9849600 batches: 0.5738
trigger times: 0
Loss after 9952200 batches: 0.5227
trigger times: 0
Loss after 10054800 batches: 0.4633
trigger times: 0
Loss after 10157400 batches: 0.4026
trigger times: 1
Loss after 10260000 batches: 0.3669
trigger times: 2
Loss after 10362600 batches: 0.3184
trigger times: 3
Loss after 10465200 batches: 0.2858
trigger times: 4
Loss after 10567800 batches: 0.2619
trigger times: 5
Loss after 10670400 batches: 0.2832
trigger times: 6
Loss after 10773000 batches: 0.2348
trigger times: 7
Loss after 10875600 batches: 0.2033
trigger times: 8
Loss after 10978200 batches: 0.1938
trigger times: 9
Loss after 11080800 batches: 0.1758
trigger times: 10
Loss after 11183400 batches: 0.1639
trigger times: 11
Loss after 11286000 batches: 0.1574
trigger times: 12
Loss after 11388600 batches: 0.1471
trigger times: 13
Loss after 11491200 batches: 0.1389
trigger times: 14
Loss after 11593800 batches: 0.1358
trigger times: 15
Loss after 11696400 batches: 0.1242
trigger times: 16
Loss after 11799000 batches: 0.1145
trigger times: 17
Loss after 11901600 batches: 0.1164
trigger times: 18
Loss after 12004200 batches: 0.1119
trigger times: 19
Loss after 12106800 batches: 0.1086
trigger times: 20
Early stopping!
Start to test process.
Loss after 12209400 batches: 0.0996
Time to train on one home:  166.38900709152222
trigger times: 0
Loss after 12340500 batches: 0.8118
trigger times: 0
Loss after 12471600 batches: 0.5116
trigger times: 1
Loss after 12602700 batches: 0.4242
trigger times: 2
Loss after 12733800 batches: 0.3586
trigger times: 3
Loss after 12864900 batches: 0.3129
trigger times: 4
Loss after 12996000 batches: 0.2809
trigger times: 5
Loss after 13127100 batches: 0.2489
trigger times: 6
Loss after 13258200 batches: 0.2200
trigger times: 7
Loss after 13389300 batches: 0.1962
trigger times: 8
Loss after 13520400 batches: 0.1723
trigger times: 9
Loss after 13651500 batches: 0.1523
trigger times: 10
Loss after 13782600 batches: 0.1340
trigger times: 11
Loss after 13913700 batches: 0.1203
trigger times: 12
Loss after 14044800 batches: 0.1085
trigger times: 13
Loss after 14175900 batches: 0.0992
trigger times: 14
Loss after 14307000 batches: 0.0892
trigger times: 15
Loss after 14438100 batches: 0.0820
trigger times: 16
Loss after 14569200 batches: 0.0757
trigger times: 17
Loss after 14700300 batches: 0.0718
trigger times: 18
Loss after 14831400 batches: 0.0665
trigger times: 19
Loss after 14962500 batches: 0.0635
trigger times: 20
Early stopping!
Start to test process.
Loss after 15093600 batches: 0.0586
Time to train on one home:  166.946852684021
trigger times: 0
Loss after 15224700 batches: 0.9733
trigger times: 0
Loss after 15355800 batches: 0.7915
trigger times: 1
Loss after 15486900 batches: 0.6867
trigger times: 2
Loss after 15618000 batches: 0.6207
trigger times: 3
Loss after 15749100 batches: 0.5567
trigger times: 4
Loss after 15880200 batches: 0.4755
trigger times: 5
Loss after 16011300 batches: 0.3858
trigger times: 6
Loss after 16142400 batches: 0.3044
trigger times: 7
Loss after 16273500 batches: 0.2471
trigger times: 8
Loss after 16404600 batches: 0.2107
trigger times: 9
Loss after 16535700 batches: 0.1792
trigger times: 10
Loss after 16666800 batches: 0.1592
trigger times: 11
Loss after 16797900 batches: 0.1477
trigger times: 12
Loss after 16929000 batches: 0.1357
trigger times: 13
Loss after 17060100 batches: 0.1245
trigger times: 14
Loss after 17191200 batches: 0.1186
trigger times: 15
Loss after 17322300 batches: 0.1109
trigger times: 16
Loss after 17453400 batches: 0.1079
trigger times: 17
Loss after 17584500 batches: 0.1021
trigger times: 18
Loss after 17715600 batches: 0.0977
trigger times: 19
Loss after 17846700 batches: 0.0941
trigger times: 20
Early stopping!
Start to test process.
Loss after 17977800 batches: 0.0906
Time to train on one home:  166.75115394592285
trigger times: 0
Loss after 18106440 batches: 0.8117
trigger times: 0
Loss after 18235080 batches: 0.4763
trigger times: 0
Loss after 18363720 batches: 0.4052
trigger times: 0
Loss after 18492360 batches: 0.3513
trigger times: 1
Loss after 18621000 batches: 0.2912
trigger times: 2
Loss after 18749640 batches: 0.2297
trigger times: 3
Loss after 18878280 batches: 0.1853
trigger times: 4
Loss after 19006920 batches: 0.1553
trigger times: 5
Loss after 19135560 batches: 0.1285
trigger times: 6
Loss after 19264200 batches: 0.1148
trigger times: 7
Loss after 19392840 batches: 0.1023
trigger times: 8
Loss after 19521480 batches: 0.0930
trigger times: 9
Loss after 19650120 batches: 0.0857
trigger times: 10
Loss after 19778760 batches: 0.0790
trigger times: 11
Loss after 19907400 batches: 0.0751
trigger times: 12
Loss after 20036040 batches: 0.0720
trigger times: 13
Loss after 20164680 batches: 0.0680
trigger times: 14
Loss after 20293320 batches: 0.0634
trigger times: 15
Loss after 20421960 batches: 0.0597
trigger times: 16
Loss after 20550600 batches: 0.0573
trigger times: 17
Loss after 20679240 batches: 0.0555
trigger times: 18
Loss after 20807880 batches: 0.0528
trigger times: 19
Loss after 20936520 batches: 0.0506
trigger times: 20
Early stopping!
Start to test process.
Loss after 21065160 batches: 0.0485
Time to train on one home:  178.14934396743774
trigger times: 0
Loss after 21196260 batches: 0.9281
trigger times: 1
Loss after 21327360 batches: 0.7701
trigger times: 2
Loss after 21458460 batches: 0.6890
trigger times: 3
Loss after 21589560 batches: 0.6019
trigger times: 4
Loss after 21720660 batches: 0.5166
trigger times: 5
Loss after 21851760 batches: 0.4274
trigger times: 6
Loss after 21982860 batches: 0.3328
trigger times: 7
Loss after 22113960 batches: 0.2599
trigger times: 8
Loss after 22245060 batches: 0.2061
trigger times: 9
Loss after 22376160 batches: 0.1740
trigger times: 10
Loss after 22507260 batches: 0.1511
trigger times: 11
Loss after 22638360 batches: 0.1365
trigger times: 12
Loss after 22769460 batches: 0.1240
trigger times: 13
Loss after 22900560 batches: 0.1158
trigger times: 14
Loss after 23031660 batches: 0.1073
trigger times: 15
Loss after 23162760 batches: 0.1001
trigger times: 16
Loss after 23293860 batches: 0.0962
trigger times: 17
Loss after 23424960 batches: 0.0911
trigger times: 18
Loss after 23556060 batches: 0.0867
trigger times: 19
Loss after 23687160 batches: 0.0840
trigger times: 20
Early stopping!
Start to test process.
Loss after 23818260 batches: 0.0816
Time to train on one home:  160.1234860420227
train_results:  [0.06554386674915233]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608]]
Round_0_results:  [0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 189 < 190; dropping {'Training_Loss': 0.6989746138734637, 'Validation_Loss': 0.43889309962590534, 'Training_R2': 0.2979813041949384, 'Validation_R2': 0.5912626714407831, 'Training_F1': 0.4943065436251116, 'Validation_F1': 0.6621594321487089, 'Training_NEP': 1.030897362574096, 'Validation_NEP': 0.7432643025784001, 'Training_NDE': 0.5270178957920246, 'Validation_NDE': 0.3254968622402462, 'Training_MAE': 34.141276158522686, 'Validation_MAE': 20.383390791399076, 'Training_MSE': 2318.796, 'Validation_MSE': 1202.051}.
trigger times: 0
Loss after 23949360 batches: 0.6990
trigger times: 0
Loss after 24080460 batches: 0.2952
trigger times: 0
Loss after 24211560 batches: 0.2229
trigger times: 1
Loss after 24342660 batches: 0.1888
trigger times: 0
Loss after 24473760 batches: 0.1657
trigger times: 1
Loss after 24604860 batches: 0.1457
trigger times: 0
Loss after 24735960 batches: 0.1277
trigger times: 0
Loss after 24867060 batches: 0.1153
trigger times: 0
Loss after 24998160 batches: 0.1073
trigger times: 0
Loss after 25129260 batches: 0.0950
trigger times: 0
Loss after 25260360 batches: 0.0874
trigger times: 1
Loss after 25391460 batches: 0.0797
trigger times: 0
Loss after 25522560 batches: 0.0769
trigger times: 1
Loss after 25653660 batches: 0.0705
trigger times: 2
Loss after 25784760 batches: 0.0691
trigger times: 3
Loss after 25915860 batches: 0.0637
trigger times: 4
Loss after 26046960 batches: 0.0606
trigger times: 5
Loss after 26178060 batches: 0.0585
trigger times: 6
Loss after 26309160 batches: 0.0554
trigger times: 0
Loss after 26440260 batches: 0.0531
trigger times: 1
Loss after 26571360 batches: 0.0529
trigger times: 2
Loss after 26702460 batches: 0.0493
trigger times: 3
Loss after 26833560 batches: 0.0480
trigger times: 4
Loss after 26964660 batches: 0.0461
trigger times: 0
Loss after 27095760 batches: 0.0454
trigger times: 1
Loss after 27226860 batches: 0.0441
trigger times: 2
Loss after 27357960 batches: 0.0427
trigger times: 3
Loss after 27489060 batches: 0.0418
trigger times: 4
Loss after 27620160 batches: 0.0400
trigger times: 5
Loss after 27751260 batches: 0.0391
trigger times: 6
Loss after 27882360 batches: 0.0383
trigger times: 7
Loss after 28013460 batches: 0.0374
trigger times: 8
Loss after 28144560 batches: 0.0370
trigger times: 9
Loss after 28275660 batches: 0.0357
trigger times: 10
Loss after 28406760 batches: 0.0354
trigger times: 11
Loss after 28537860 batches: 0.0344
trigger times: 0
Loss after 28668960 batches: 0.0341
trigger times: 1
Loss after 28800060 batches: 0.0336
trigger times: 2
Loss after 28931160 batches: 0.0332
trigger times: 3
Loss after 29062260 batches: 0.0318
trigger times: 0
Loss after 29193360 batches: 0.0318
trigger times: 1
Loss after 29324460 batches: 0.0305
trigger times: 2
Loss after 29455560 batches: 0.0303
trigger times: 3
Loss after 29586660 batches: 0.0297
trigger times: 4
Loss after 29717760 batches: 0.0296
trigger times: 5
Loss after 29848860 batches: 0.0291
trigger times: 6
Loss after 29979960 batches: 0.0288
trigger times: 7
Loss after 30111060 batches: 0.0281
trigger times: 8
Loss after 30242160 batches: 0.0268
trigger times: 9
Loss after 30373260 batches: 0.0266
trigger times: 10
Loss after 30504360 batches: 0.0270
trigger times: 11
Loss after 30635460 batches: 0.0268
trigger times: 0
Loss after 30766560 batches: 0.0260
trigger times: 1
Loss after 30897660 batches: 0.0256
trigger times: 2
Loss after 31028760 batches: 0.0253
trigger times: 3
Loss after 31159860 batches: 0.0250
trigger times: 4
Loss after 31290960 batches: 0.0248
trigger times: 5
Loss after 31422060 batches: 0.0241
trigger times: 6
Loss after 31553160 batches: 0.0236
trigger times: 7
Loss after 31684260 batches: 0.0233
trigger times: 0
Loss after 31815360 batches: 0.0233
trigger times: 1
Loss after 31946460 batches: 0.0230
trigger times: 2
Loss after 32077560 batches: 0.0229
trigger times: 3
Loss after 32208660 batches: 0.0224
trigger times: 4
Loss after 32339760 batches: 0.0221
trigger times: 5
Loss after 32470860 batches: 0.0220
trigger times: 0
Loss after 32601960 batches: 0.0215
trigger times: 1
Loss after 32733060 batches: 0.0219
trigger times: 2
Loss after 32864160 batches: 0.0212
trigger times: 3
Loss after 32995260 batches: 0.0210
trigger times: 0
Loss after 33126360 batches: 0.0206
trigger times: 1
Loss after 33257460 batches: 0.0204
trigger times: 2
Loss after 33388560 batches: 0.0206
trigger times: 0
Loss after 33519660 batches: 0.0201
trigger times: 1
Loss after 33650760 batches: 0.0198
trigger times: 2
Loss after 33781860 batches: 0.0197
trigger times: 0
Loss after 33912960 batches: 0.0193
trigger times: 1
Loss after 34044060 batches: 0.0191
trigger times: 2
Loss after 34175160 batches: 0.0188
trigger times: 3
Loss after 34306260 batches: 0.0190
trigger times: 4
Loss after 34437360 batches: 0.0186
trigger times: 5
Loss after 34568460 batches: 0.0184
trigger times: 6
Loss after 34699560 batches: 0.0184
trigger times: 7
Loss after 34830660 batches: 0.0177
trigger times: 8
Loss after 34961760 batches: 0.0180
trigger times: 9
Loss after 35092860 batches: 0.0176
trigger times: 10
Loss after 35223960 batches: 0.0177
trigger times: 11
Loss after 35355060 batches: 0.0172
trigger times: 12
Loss after 35486160 batches: 0.0174
trigger times: 0
Loss after 35617260 batches: 0.0177
trigger times: 1
Loss after 35748360 batches: 0.0177
trigger times: 2
Loss after 35879460 batches: 0.0169
trigger times: 3
Loss after 36010560 batches: 0.0163
trigger times: 0
Loss after 36141660 batches: 0.0169
trigger times: 1
Loss after 36272760 batches: 0.0165
trigger times: 2
Loss after 36403860 batches: 0.0164
trigger times: 3
Loss after 36534960 batches: 0.0159
trigger times: 4
Loss after 36666060 batches: 0.0158
trigger times: 5
Loss after 36797160 batches: 0.0160
trigger times: 6
Loss after 36928260 batches: 0.0159
trigger times: 7
Loss after 37059360 batches: 0.0154
trigger times: 8
Loss after 37190460 batches: 0.0154
trigger times: 9
Loss after 37321560 batches: 0.0156
trigger times: 10
Loss after 37452660 batches: 0.0152
trigger times: 11
Loss after 37583760 batches: 0.0154
trigger times: 12
Loss after 37714860 batches: 0.0151
trigger times: 13
Loss after 37845960 batches: 0.0148
trigger times: 14
Loss after 37977060 batches: 0.0145
trigger times: 15
Loss after 38108160 batches: 0.0148
trigger times: 16
Loss after 38239260 batches: 0.0146
trigger times: 17
Loss after 38370360 batches: 0.0144
trigger times: 18
Loss after 38501460 batches: 0.0144
trigger times: 19
Loss after 38632560 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 38763660 batches: 0.0145
Time to train on one home:  817.0364668369293
trigger times: 0
Loss after 38866260 batches: 0.7590
trigger times: 0
Loss after 38968860 batches: 0.5388
trigger times: 1
Loss after 39071460 batches: 0.4512
trigger times: 2
Loss after 39174060 batches: 0.3785
trigger times: 3
Loss after 39276660 batches: 0.3456
trigger times: 4
Loss after 39379260 batches: 0.2891
trigger times: 5
Loss after 39481860 batches: 0.2588
trigger times: 6
Loss after 39584460 batches: 0.2333
trigger times: 7
Loss after 39687060 batches: 0.2104
trigger times: 8
Loss after 39789660 batches: 0.2061
trigger times: 9
Loss after 39892260 batches: 0.1776
trigger times: 10
Loss after 39994860 batches: 0.1677
trigger times: 11
Loss after 40097460 batches: 0.1591
trigger times: 12
Loss after 40200060 batches: 0.1460
trigger times: 13
Loss after 40302660 batches: 0.1416
trigger times: 14
Loss after 40405260 batches: 0.1361
trigger times: 15
Loss after 40507860 batches: 0.1430
trigger times: 16
Loss after 40610460 batches: 0.1501
trigger times: 17
Loss after 40713060 batches: 0.1246
trigger times: 18
Loss after 40815660 batches: 0.1161
trigger times: 19
Loss after 40918260 batches: 0.1099
trigger times: 20
Early stopping!
Start to test process.
Loss after 41020860 batches: 0.1050
Time to train on one home:  138.17867994308472
trigger times: 0
Loss after 41151960 batches: 0.5144
trigger times: 1
Loss after 41283060 batches: 0.3178
trigger times: 2
Loss after 41414160 batches: 0.2466
trigger times: 3
Loss after 41545260 batches: 0.1953
trigger times: 4
Loss after 41676360 batches: 0.1628
trigger times: 5
Loss after 41807460 batches: 0.1404
trigger times: 6
Loss after 41938560 batches: 0.1244
trigger times: 7
Loss after 42069660 batches: 0.1135
trigger times: 8
Loss after 42200760 batches: 0.1041
trigger times: 9
Loss after 42331860 batches: 0.0977
trigger times: 10
Loss after 42462960 batches: 0.0907
trigger times: 11
Loss after 42594060 batches: 0.0855
trigger times: 12
Loss after 42725160 batches: 0.0814
trigger times: 13
Loss after 42856260 batches: 0.0772
trigger times: 14
Loss after 42987360 batches: 0.0742
trigger times: 15
Loss after 43118460 batches: 0.0698
trigger times: 16
Loss after 43249560 batches: 0.0686
trigger times: 17
Loss after 43380660 batches: 0.0653
trigger times: 18
Loss after 43511760 batches: 0.0625
trigger times: 19
Loss after 43642860 batches: 0.0603
trigger times: 20
Early stopping!
Start to test process.
Loss after 43773960 batches: 0.0585
Time to train on one home:  159.72324442863464
trigger times: 0
Loss after 43905060 batches: 0.7749
trigger times: 1
Loss after 44036160 batches: 0.5916
trigger times: 2
Loss after 44167260 batches: 0.4470
trigger times: 3
Loss after 44298360 batches: 0.3357
trigger times: 4
Loss after 44429460 batches: 0.2628
trigger times: 5
Loss after 44560560 batches: 0.2201
trigger times: 6
Loss after 44691660 batches: 0.1877
trigger times: 7
Loss after 44822760 batches: 0.1666
trigger times: 8
Loss after 44953860 batches: 0.1520
trigger times: 9
Loss after 45084960 batches: 0.1408
trigger times: 10
Loss after 45216060 batches: 0.1303
trigger times: 11
Loss after 45347160 batches: 0.1220
trigger times: 12
Loss after 45478260 batches: 0.1159
trigger times: 13
Loss after 45609360 batches: 0.1098
trigger times: 14
Loss after 45740460 batches: 0.1056
trigger times: 15
Loss after 45871560 batches: 0.1009
trigger times: 16
Loss after 46002660 batches: 0.0984
trigger times: 17
Loss after 46133760 batches: 0.0939
trigger times: 18
Loss after 46264860 batches: 0.0925
trigger times: 19
Loss after 46395960 batches: 0.0891
trigger times: 20
Early stopping!
Start to test process.
Loss after 46527060 batches: 0.0863
Time to train on one home:  159.5164589881897
trigger times: 0
Loss after 46655700 batches: 0.5093
trigger times: 0
Loss after 46784340 batches: 0.3054
trigger times: 1
Loss after 46912980 batches: 0.2138
trigger times: 2
Loss after 47041620 batches: 0.1600
trigger times: 3
Loss after 47170260 batches: 0.1319
trigger times: 4
Loss after 47298900 batches: 0.1105
trigger times: 5
Loss after 47427540 batches: 0.0982
trigger times: 6
Loss after 47556180 batches: 0.0882
trigger times: 7
Loss after 47684820 batches: 0.0811
trigger times: 8
Loss after 47813460 batches: 0.0771
trigger times: 9
Loss after 47942100 batches: 0.0703
trigger times: 10
Loss after 48070740 batches: 0.0665
trigger times: 11
Loss after 48199380 batches: 0.0622
trigger times: 12
Loss after 48328020 batches: 0.0610
trigger times: 13
Loss after 48456660 batches: 0.0571
trigger times: 14
Loss after 48585300 batches: 0.0554
trigger times: 15
Loss after 48713940 batches: 0.0532
trigger times: 16
Loss after 48842580 batches: 0.0506
trigger times: 17
Loss after 48971220 batches: 0.0494
trigger times: 18
Loss after 49099860 batches: 0.0483
trigger times: 19
Loss after 49228500 batches: 0.0469
trigger times: 20
Early stopping!
Start to test process.
Loss after 49357140 batches: 0.0447
Time to train on one home:  164.69661259651184
trigger times: 0
Loss after 49488240 batches: 0.7753
trigger times: 1
Loss after 49619340 batches: 0.5367
trigger times: 2
Loss after 49750440 batches: 0.3842
trigger times: 3
Loss after 49881540 batches: 0.2575
trigger times: 4
Loss after 50012640 batches: 0.1923
trigger times: 5
Loss after 50143740 batches: 0.1571
trigger times: 6
Loss after 50274840 batches: 0.1349
trigger times: 7
Loss after 50405940 batches: 0.1194
trigger times: 8
Loss after 50537040 batches: 0.1080
trigger times: 9
Loss after 50668140 batches: 0.1014
trigger times: 10
Loss after 50799240 batches: 0.0945
trigger times: 11
Loss after 50930340 batches: 0.0892
trigger times: 12
Loss after 51061440 batches: 0.0846
trigger times: 13
Loss after 51192540 batches: 0.0813
trigger times: 14
Loss after 51323640 batches: 0.0775
trigger times: 15
Loss after 51454740 batches: 0.0747
trigger times: 16
Loss after 51585840 batches: 0.0712
trigger times: 17
Loss after 51716940 batches: 0.0693
trigger times: 18
Loss after 51848040 batches: 0.0680
trigger times: 19
Loss after 51979140 batches: 0.0665
trigger times: 20
Early stopping!
Start to test process.
Loss after 52110240 batches: 0.0644
Time to train on one home:  160.10008192062378
train_results:  [0.06554386674915233, 0.06223066226547774]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958]]
Round_1_results:  [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958]
trigger times: 0
Loss after 52241340 batches: 0.3007
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 410 < 411; dropping {'Training_Loss': 0.3006580857173452, 'Validation_Loss': 0.21606172124544779, 'Training_R2': 0.6977534526167264, 'Validation_R2': 0.7990687048369962, 'Training_F1': 0.7009960804346705, 'Validation_F1': 0.7196422227600516, 'Training_NEP': 0.600695417692326, 'Validation_NEP': 0.5467420652280333, 'Training_NDE': 0.2269018480051546, 'Validation_NDE': 0.1600110915535113, 'Training_MAE': 19.893840926495514, 'Validation_MAE': 14.993935722433111, 'Training_MSE': 998.3326, 'Validation_MSE': 590.91656}.
trigger times: 0
Loss after 52372440 batches: 0.1110
trigger times: 1
Loss after 52503540 batches: 0.0734
trigger times: 0
Loss after 52634640 batches: 0.0566
trigger times: 0
Loss after 52765740 batches: 0.0469
trigger times: 1
Loss after 52896840 batches: 0.0423
trigger times: 2
Loss after 53027940 batches: 0.0378
trigger times: 3
Loss after 53159040 batches: 0.0350
trigger times: 0
Loss after 53290140 batches: 0.0323
trigger times: 1
Loss after 53421240 batches: 0.0305
trigger times: 2
Loss after 53552340 batches: 0.0289
trigger times: 0
Loss after 53683440 batches: 0.0275
trigger times: 1
Loss after 53814540 batches: 0.0262
trigger times: 2
Loss after 53945640 batches: 0.0259
trigger times: 0
Loss after 54076740 batches: 0.0249
trigger times: 1
Loss after 54207840 batches: 0.0239
trigger times: 0
Loss after 54338940 batches: 0.0233
trigger times: 1
Loss after 54470040 batches: 0.0225
trigger times: 2
Loss after 54601140 batches: 0.0219
trigger times: 3
Loss after 54732240 batches: 0.0216
trigger times: 4
Loss after 54863340 batches: 0.0209
trigger times: 5
Loss after 54994440 batches: 0.0206
trigger times: 6
Loss after 55125540 batches: 0.0199
trigger times: 7
Loss after 55256640 batches: 0.0195
trigger times: 8
Loss after 55387740 batches: 0.0194
trigger times: 9
Loss after 55518840 batches: 0.0190
trigger times: 0
Loss after 55649940 batches: 0.0188
trigger times: 1
Loss after 55781040 batches: 0.0180
trigger times: 2
Loss after 55912140 batches: 0.0183
trigger times: 3
Loss after 56043240 batches: 0.0181
trigger times: 4
Loss after 56174340 batches: 0.0176
trigger times: 5
Loss after 56305440 batches: 0.0173
trigger times: 6
Loss after 56436540 batches: 0.0174
trigger times: 7
Loss after 56567640 batches: 0.0170
trigger times: 8
Loss after 56698740 batches: 0.0173
trigger times: 9
Loss after 56829840 batches: 0.0163
trigger times: 10
Loss after 56960940 batches: 0.0163
trigger times: 11
Loss after 57092040 batches: 0.0160
trigger times: 12
Loss after 57223140 batches: 0.0157
trigger times: 13
Loss after 57354240 batches: 0.0158
trigger times: 14
Loss after 57485340 batches: 0.0154
trigger times: 15
Loss after 57616440 batches: 0.0153
trigger times: 16
Loss after 57747540 batches: 0.0150
trigger times: 17
Loss after 57878640 batches: 0.0146
trigger times: 18
Loss after 58009740 batches: 0.0149
trigger times: 19
Loss after 58140840 batches: 0.0148
trigger times: 20
Early stopping!
Start to test process.
Loss after 58271940 batches: 0.0147
Time to train on one home:  343.44076800346375
trigger times: 0
Loss after 58374540 batches: 0.6040
trigger times: 1
Loss after 58477140 batches: 0.3449
trigger times: 2
Loss after 58579740 batches: 0.2511
trigger times: 3
Loss after 58682340 batches: 0.1913
trigger times: 4
Loss after 58784940 batches: 0.1764
trigger times: 5
Loss after 58887540 batches: 0.1418
trigger times: 6
Loss after 58990140 batches: 0.1262
trigger times: 7
Loss after 59092740 batches: 0.1173
trigger times: 8
Loss after 59195340 batches: 0.1103
trigger times: 9
Loss after 59297940 batches: 0.0997
trigger times: 10
Loss after 59400540 batches: 0.0964
trigger times: 11
Loss after 59503140 batches: 0.0897
trigger times: 12
Loss after 59605740 batches: 0.0894
trigger times: 13
Loss after 59708340 batches: 0.0822
trigger times: 14
Loss after 59810940 batches: 0.0764
trigger times: 15
Loss after 59913540 batches: 0.0769
trigger times: 16
Loss after 60016140 batches: 0.0749
trigger times: 17
Loss after 60118740 batches: 0.0741
trigger times: 18
Loss after 60221340 batches: 0.0721
trigger times: 19
Loss after 60323940 batches: 0.0619
trigger times: 20
Early stopping!
Start to test process.
Loss after 60426540 batches: 0.0592
Time to train on one home:  131.5660605430603
trigger times: 0
Loss after 60557640 batches: 0.3919
trigger times: 1
Loss after 60688740 batches: 0.1864
trigger times: 2
Loss after 60819840 batches: 0.1204
trigger times: 3
Loss after 60950940 batches: 0.0931
trigger times: 4
Loss after 61082040 batches: 0.0804
trigger times: 5
Loss after 61213140 batches: 0.0716
trigger times: 6
Loss after 61344240 batches: 0.0639
trigger times: 7
Loss after 61475340 batches: 0.0584
trigger times: 8
Loss after 61606440 batches: 0.0548
trigger times: 9
Loss after 61737540 batches: 0.0515
trigger times: 10
Loss after 61868640 batches: 0.0497
trigger times: 11
Loss after 61999740 batches: 0.0469
trigger times: 12
Loss after 62130840 batches: 0.0447
trigger times: 13
Loss after 62261940 batches: 0.0432
trigger times: 14
Loss after 62393040 batches: 0.0410
trigger times: 15
Loss after 62524140 batches: 0.0398
trigger times: 16
Loss after 62655240 batches: 0.0385
trigger times: 17
Loss after 62786340 batches: 0.0374
trigger times: 18
Loss after 62917440 batches: 0.0367
trigger times: 19
Loss after 63048540 batches: 0.0351
trigger times: 20
Early stopping!
Start to test process.
Loss after 63179640 batches: 0.0347
Time to train on one home:  159.9926896095276
trigger times: 0
Loss after 63310740 batches: 0.6358
trigger times: 1
Loss after 63441840 batches: 0.3322
trigger times: 2
Loss after 63572940 batches: 0.1965
trigger times: 3
Loss after 63704040 batches: 0.1473
trigger times: 4
Loss after 63835140 batches: 0.1230
trigger times: 5
Loss after 63966240 batches: 0.1090
trigger times: 6
Loss after 64097340 batches: 0.0985
trigger times: 7
Loss after 64228440 batches: 0.0914
trigger times: 8
Loss after 64359540 batches: 0.0860
trigger times: 9
Loss after 64490640 batches: 0.0802
trigger times: 10
Loss after 64621740 batches: 0.0775
trigger times: 11
Loss after 64752840 batches: 0.0732
trigger times: 12
Loss after 64883940 batches: 0.0711
trigger times: 13
Loss after 65015040 batches: 0.0688
trigger times: 14
Loss after 65146140 batches: 0.0665
trigger times: 15
Loss after 65277240 batches: 0.0641
trigger times: 16
Loss after 65408340 batches: 0.0615
trigger times: 17
Loss after 65539440 batches: 0.0600
trigger times: 18
Loss after 65670540 batches: 0.0592
trigger times: 19
Loss after 65801640 batches: 0.0574
trigger times: 20
Early stopping!
Start to test process.
Loss after 65932740 batches: 0.0559
Time to train on one home:  160.02877235412598
trigger times: 0
Loss after 66061380 batches: 0.3559
trigger times: 1
Loss after 66190020 batches: 0.1428
trigger times: 2
Loss after 66318660 batches: 0.0934
trigger times: 3
Loss after 66447300 batches: 0.0743
trigger times: 4
Loss after 66575940 batches: 0.0631
trigger times: 5
Loss after 66704580 batches: 0.0563
trigger times: 6
Loss after 66833220 batches: 0.0512
trigger times: 0
Loss after 66961860 batches: 0.0481
trigger times: 0
Loss after 67090500 batches: 0.0448
trigger times: 0
Loss after 67219140 batches: 0.0422
trigger times: 1
Loss after 67347780 batches: 0.0403
trigger times: 0
Loss after 67476420 batches: 0.0377
trigger times: 1
Loss after 67605060 batches: 0.0364
trigger times: 2
Loss after 67733700 batches: 0.0354
trigger times: 3
Loss after 67862340 batches: 0.0341
trigger times: 4
Loss after 67990980 batches: 0.0335
trigger times: 5
Loss after 68119620 batches: 0.0322
trigger times: 0
Loss after 68248260 batches: 0.0318
trigger times: 1
Loss after 68376900 batches: 0.0308
trigger times: 2
Loss after 68505540 batches: 0.0301
trigger times: 3
Loss after 68634180 batches: 0.0297
trigger times: 4
Loss after 68762820 batches: 0.0289
trigger times: 5
Loss after 68891460 batches: 0.0281
trigger times: 0
Loss after 69020100 batches: 0.0273
trigger times: 0
Loss after 69148740 batches: 0.0266
trigger times: 1
Loss after 69277380 batches: 0.0263
trigger times: 2
Loss after 69406020 batches: 0.0258
trigger times: 3
Loss after 69534660 batches: 0.0251
trigger times: 4
Loss after 69663300 batches: 0.0247
trigger times: 5
Loss after 69791940 batches: 0.0249
trigger times: 6
Loss after 69920580 batches: 0.0239
trigger times: 7
Loss after 70049220 batches: 0.0239
trigger times: 8
Loss after 70177860 batches: 0.0237
trigger times: 0
Loss after 70306500 batches: 0.0232
trigger times: 1
Loss after 70435140 batches: 0.0229
trigger times: 2
Loss after 70563780 batches: 0.0223
trigger times: 3
Loss after 70692420 batches: 0.0223
trigger times: 4
Loss after 70821060 batches: 0.0226
trigger times: 5
Loss after 70949700 batches: 0.0221
trigger times: 6
Loss after 71078340 batches: 0.0213
trigger times: 7
Loss after 71206980 batches: 0.0214
trigger times: 8
Loss after 71335620 batches: 0.0209
trigger times: 9
Loss after 71464260 batches: 0.0204
trigger times: 0
Loss after 71592900 batches: 0.0204
trigger times: 1
Loss after 71721540 batches: 0.0202
trigger times: 2
Loss after 71850180 batches: 0.0197
trigger times: 3
Loss after 71978820 batches: 0.0194
trigger times: 4
Loss after 72107460 batches: 0.0197
trigger times: 5
Loss after 72236100 batches: 0.0196
trigger times: 6
Loss after 72364740 batches: 0.0195
trigger times: 7
Loss after 72493380 batches: 0.0189
trigger times: 8
Loss after 72622020 batches: 0.0189
trigger times: 9
Loss after 72750660 batches: 0.0186
trigger times: 10
Loss after 72879300 batches: 0.0186
trigger times: 11
Loss after 73007940 batches: 0.0184
trigger times: 12
Loss after 73136580 batches: 0.0180
trigger times: 13
Loss after 73265220 batches: 0.0179
trigger times: 14
Loss after 73393860 batches: 0.0177
trigger times: 0
Loss after 73522500 batches: 0.0178
trigger times: 1
Loss after 73651140 batches: 0.0178
trigger times: 2
Loss after 73779780 batches: 0.0174
trigger times: 3
Loss after 73908420 batches: 0.0173
trigger times: 4
Loss after 74037060 batches: 0.0175
trigger times: 5
Loss after 74165700 batches: 0.0168
trigger times: 6
Loss after 74294340 batches: 0.0168
trigger times: 7
Loss after 74422980 batches: 0.0167
trigger times: 8
Loss after 74551620 batches: 0.0165
trigger times: 9
Loss after 74680260 batches: 0.0166
trigger times: 10
Loss after 74808900 batches: 0.0163
trigger times: 11
Loss after 74937540 batches: 0.0164
trigger times: 12
Loss after 75066180 batches: 0.0165
trigger times: 13
Loss after 75194820 batches: 0.0159
trigger times: 14
Loss after 75323460 batches: 0.0156
trigger times: 15
Loss after 75452100 batches: 0.0161
trigger times: 16
Loss after 75580740 batches: 0.0158
trigger times: 17
Loss after 75709380 batches: 0.0153
trigger times: 18
Loss after 75838020 batches: 0.0155
trigger times: 19
Loss after 75966660 batches: 0.0155
trigger times: 20
Early stopping!
Start to test process.
Loss after 76095300 batches: 0.0155
Time to train on one home:  560.7736985683441
trigger times: 0
Loss after 76226400 batches: 0.6292
trigger times: 1
Loss after 76357500 batches: 0.2940
trigger times: 2
Loss after 76488600 batches: 0.1628
trigger times: 3
Loss after 76619700 batches: 0.1190
trigger times: 4
Loss after 76750800 batches: 0.1006
trigger times: 5
Loss after 76881900 batches: 0.0884
trigger times: 6
Loss after 77013000 batches: 0.0798
trigger times: 7
Loss after 77144100 batches: 0.0728
trigger times: 8
Loss after 77275200 batches: 0.0687
trigger times: 9
Loss after 77406300 batches: 0.0653
trigger times: 10
Loss after 77537400 batches: 0.0614
trigger times: 11
Loss after 77668500 batches: 0.0586
trigger times: 12
Loss after 77799600 batches: 0.0570
trigger times: 13
Loss after 77930700 batches: 0.0550
trigger times: 14
Loss after 78061800 batches: 0.0530
trigger times: 15
Loss after 78192900 batches: 0.0507
trigger times: 16
Loss after 78324000 batches: 0.0499
trigger times: 17
Loss after 78455100 batches: 0.0488
trigger times: 18
Loss after 78586200 batches: 0.0470
trigger times: 19
Loss after 78717300 batches: 0.0463
trigger times: 20
Early stopping!
Start to test process.
Loss after 78848400 batches: 0.0448
Time to train on one home:  159.8442862033844
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253]]
Round_2_results:  [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253]
trigger times: 0
Loss after 78979500 batches: 0.2629
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 620 < 621; dropping {'Training_Loss': 0.2628820725488213, 'Validation_Loss': 0.264083797732989, 'Training_R2': 0.7354998946337357, 'Validation_R2': 0.7547301967667581, 'Training_F1': 0.7270898887604947, 'Validation_F1': 0.6791900774312343, 'Training_NEP': 0.5457478908739515, 'Validation_NEP': 0.5865798887455561, 'Training_NDE': 0.198564923982601, 'Validation_NDE': 0.19531994211567721, 'Training_MAE': 18.07408781096737, 'Validation_MAE': 16.08645412028904, 'Training_MSE': 873.6546, 'Validation_MSE': 721.3112}.
trigger times: 0
Loss after 79110600 batches: 0.0848
trigger times: 1
Loss after 79241700 batches: 0.0567
trigger times: 0
Loss after 79372800 batches: 0.0450
trigger times: 1
Loss after 79503900 batches: 0.0388
trigger times: 0
Loss after 79635000 batches: 0.0351
trigger times: 0
Loss after 79766100 batches: 0.0323
trigger times: 1
Loss after 79897200 batches: 0.0296
trigger times: 2
Loss after 80028300 batches: 0.0279
trigger times: 3
Loss after 80159400 batches: 0.0267
trigger times: 0
Loss after 80290500 batches: 0.0251
trigger times: 1
Loss after 80421600 batches: 0.0246
trigger times: 2
Loss after 80552700 batches: 0.0237
trigger times: 3
Loss after 80683800 batches: 0.0224
trigger times: 4
Loss after 80814900 batches: 0.0221
trigger times: 5
Loss after 80946000 batches: 0.0211
trigger times: 6
Loss after 81077100 batches: 0.0205
trigger times: 7
Loss after 81208200 batches: 0.0205
trigger times: 8
Loss after 81339300 batches: 0.0199
trigger times: 0
Loss after 81470400 batches: 0.0194
trigger times: 1
Loss after 81601500 batches: 0.0192
trigger times: 2
Loss after 81732600 batches: 0.0186
trigger times: 0
Loss after 81863700 batches: 0.0184
trigger times: 1
Loss after 81994800 batches: 0.0180
trigger times: 2
Loss after 82125900 batches: 0.0176
trigger times: 3
Loss after 82257000 batches: 0.0174
trigger times: 0
Loss after 82388100 batches: 0.0171
trigger times: 1
Loss after 82519200 batches: 0.0166
trigger times: 2
Loss after 82650300 batches: 0.0164
trigger times: 3
Loss after 82781400 batches: 0.0164
trigger times: 0
Loss after 82912500 batches: 0.0159
trigger times: 1
Loss after 83043600 batches: 0.0160
trigger times: 2
Loss after 83174700 batches: 0.0154
trigger times: 3
Loss after 83305800 batches: 0.0154
trigger times: 4
Loss after 83436900 batches: 0.0154
trigger times: 5
Loss after 83568000 batches: 0.0148
trigger times: 6
Loss after 83699100 batches: 0.0147
trigger times: 7
Loss after 83830200 batches: 0.0148
trigger times: 8
Loss after 83961300 batches: 0.0143
trigger times: 9
Loss after 84092400 batches: 0.0147
trigger times: 10
Loss after 84223500 batches: 0.0139
trigger times: 11
Loss after 84354600 batches: 0.0144
trigger times: 12
Loss after 84485700 batches: 0.0137
trigger times: 13
Loss after 84616800 batches: 0.0138
trigger times: 14
Loss after 84747900 batches: 0.0138
trigger times: 0
Loss after 84879000 batches: 0.0140
trigger times: 1
Loss after 85010100 batches: 0.0133
trigger times: 2
Loss after 85141200 batches: 0.0133
trigger times: 3
Loss after 85272300 batches: 0.0132
trigger times: 4
Loss after 85403400 batches: 0.0133
trigger times: 5
Loss after 85534500 batches: 0.0129
trigger times: 6
Loss after 85665600 batches: 0.0128
trigger times: 7
Loss after 85796700 batches: 0.0126
trigger times: 8
Loss after 85927800 batches: 0.0125
trigger times: 9
Loss after 86058900 batches: 0.0124
trigger times: 10
Loss after 86190000 batches: 0.0125
trigger times: 11
Loss after 86321100 batches: 0.0124
trigger times: 12
Loss after 86452200 batches: 0.0123
trigger times: 13
Loss after 86583300 batches: 0.0127
trigger times: 14
Loss after 86714400 batches: 0.0122
trigger times: 15
Loss after 86845500 batches: 0.0123
trigger times: 16
Loss after 86976600 batches: 0.0119
trigger times: 17
Loss after 87107700 batches: 0.0116
trigger times: 18
Loss after 87238800 batches: 0.0116
trigger times: 19
Loss after 87369900 batches: 0.0117
trigger times: 20
Early stopping!
Start to test process.
Loss after 87501000 batches: 0.0116
Time to train on one home:  478.90917921066284
trigger times: 0
Loss after 87603600 batches: 0.4543
trigger times: 1
Loss after 87706200 batches: 0.2108
trigger times: 2
Loss after 87808800 batches: 0.1451
trigger times: 3
Loss after 87911400 batches: 0.1134
trigger times: 4
Loss after 88014000 batches: 0.0976
trigger times: 5
Loss after 88116600 batches: 0.0884
trigger times: 6
Loss after 88219200 batches: 0.0831
trigger times: 7
Loss after 88321800 batches: 0.0762
trigger times: 8
Loss after 88424400 batches: 0.0716
trigger times: 9
Loss after 88527000 batches: 0.0744
trigger times: 10
Loss after 88629600 batches: 0.0657
trigger times: 11
Loss after 88732200 batches: 0.0579
trigger times: 12
Loss after 88834800 batches: 0.0522
trigger times: 13
Loss after 88937400 batches: 0.0513
trigger times: 14
Loss after 89040000 batches: 0.0492
trigger times: 15
Loss after 89142600 batches: 0.0490
trigger times: 16
Loss after 89245200 batches: 0.0456
trigger times: 17
Loss after 89347800 batches: 0.0491
trigger times: 18
Loss after 89450400 batches: 0.0460
trigger times: 19
Loss after 89553000 batches: 0.0508
trigger times: 20
Early stopping!
Start to test process.
Loss after 89655600 batches: 0.0418
Time to train on one home:  132.28033781051636
trigger times: 0
Loss after 89786700 batches: 0.2696
trigger times: 1
Loss after 89917800 batches: 0.1030
trigger times: 2
Loss after 90048900 batches: 0.0723
trigger times: 3
Loss after 90180000 batches: 0.0601
trigger times: 4
Loss after 90311100 batches: 0.0523
trigger times: 5
Loss after 90442200 batches: 0.0475
trigger times: 6
Loss after 90573300 batches: 0.0445
trigger times: 7
Loss after 90704400 batches: 0.0415
trigger times: 8
Loss after 90835500 batches: 0.0395
trigger times: 9
Loss after 90966600 batches: 0.0370
trigger times: 10
Loss after 91097700 batches: 0.0361
trigger times: 11
Loss after 91228800 batches: 0.0346
trigger times: 12
Loss after 91359900 batches: 0.0328
trigger times: 13
Loss after 91491000 batches: 0.0320
trigger times: 14
Loss after 91622100 batches: 0.0308
trigger times: 15
Loss after 91753200 batches: 0.0306
trigger times: 16
Loss after 91884300 batches: 0.0293
trigger times: 17
Loss after 92015400 batches: 0.0289
trigger times: 18
Loss after 92146500 batches: 0.0278
trigger times: 19
Loss after 92277600 batches: 0.0277
trigger times: 20
Early stopping!
Start to test process.
Loss after 92408700 batches: 0.0271
Time to train on one home:  159.5408730506897
trigger times: 0
Loss after 92539800 batches: 0.4979
trigger times: 1
Loss after 92670900 batches: 0.1847
trigger times: 2
Loss after 92802000 batches: 0.1216
trigger times: 3
Loss after 92933100 batches: 0.0986
trigger times: 4
Loss after 93064200 batches: 0.0869
trigger times: 5
Loss after 93195300 batches: 0.0778
trigger times: 6
Loss after 93326400 batches: 0.0710
trigger times: 7
Loss after 93457500 batches: 0.0670
trigger times: 8
Loss after 93588600 batches: 0.0638
trigger times: 9
Loss after 93719700 batches: 0.0597
trigger times: 10
Loss after 93850800 batches: 0.0571
trigger times: 11
Loss after 93981900 batches: 0.0545
trigger times: 12
Loss after 94113000 batches: 0.0525
trigger times: 13
Loss after 94244100 batches: 0.0513
trigger times: 14
Loss after 94375200 batches: 0.0499
trigger times: 15
Loss after 94506300 batches: 0.0479
trigger times: 16
Loss after 94637400 batches: 0.0469
trigger times: 17
Loss after 94768500 batches: 0.0466
trigger times: 18
Loss after 94899600 batches: 0.0444
trigger times: 19
Loss after 95030700 batches: 0.0433
trigger times: 20
Early stopping!
Start to test process.
Loss after 95161800 batches: 0.0428
Time to train on one home:  159.95251846313477
trigger times: 0
Loss after 95290440 batches: 0.2352
trigger times: 0
Loss after 95419080 batches: 0.0732
trigger times: 0
Loss after 95547720 batches: 0.0496
trigger times: 0
Loss after 95676360 batches: 0.0419
trigger times: 0
Loss after 95805000 batches: 0.0354
trigger times: 0
Loss after 95933640 batches: 0.0326
trigger times: 1
Loss after 96062280 batches: 0.0305
trigger times: 2
Loss after 96190920 batches: 0.0281
trigger times: 0
Loss after 96319560 batches: 0.0270
trigger times: 1
Loss after 96448200 batches: 0.0256
trigger times: 0
Loss after 96576840 batches: 0.0247
trigger times: 0
Loss after 96705480 batches: 0.0237
trigger times: 1
Loss after 96834120 batches: 0.0232
trigger times: 2
Loss after 96962760 batches: 0.0224
trigger times: 3
Loss after 97091400 batches: 0.0223
trigger times: 0
Loss after 97220040 batches: 0.0211
trigger times: 1
Loss after 97348680 batches: 0.0206
trigger times: 0
Loss after 97477320 batches: 0.0203
trigger times: 1
Loss after 97605960 batches: 0.0195
trigger times: 2
Loss after 97734600 batches: 0.0192
trigger times: 3
Loss after 97863240 batches: 0.0189
trigger times: 0
Loss after 97991880 batches: 0.0187
trigger times: 1
Loss after 98120520 batches: 0.0185
trigger times: 2
Loss after 98249160 batches: 0.0179
trigger times: 3
Loss after 98377800 batches: 0.0182
trigger times: 0
Loss after 98506440 batches: 0.0181
trigger times: 1
Loss after 98635080 batches: 0.0175
trigger times: 2
Loss after 98763720 batches: 0.0173
trigger times: 3
Loss after 98892360 batches: 0.0169
trigger times: 4
Loss after 99021000 batches: 0.0163
trigger times: 5
Loss after 99149640 batches: 0.0161
trigger times: 6
Loss after 99278280 batches: 0.0166
trigger times: 7
Loss after 99406920 batches: 0.0160
trigger times: 8
Loss after 99535560 batches: 0.0160
trigger times: 9
Loss after 99664200 batches: 0.0160
trigger times: 10
Loss after 99792840 batches: 0.0161
trigger times: 11
Loss after 99921480 batches: 0.0159
trigger times: 12
Loss after 100050120 batches: 0.0152
trigger times: 13
Loss after 100178760 batches: 0.0160
trigger times: 14
Loss after 100307400 batches: 0.0149
trigger times: 15
Loss after 100436040 batches: 0.0151
trigger times: 16
Loss after 100564680 batches: 0.0148
trigger times: 17
Loss after 100693320 batches: 0.0153
trigger times: 18
Loss after 100821960 batches: 0.0149
trigger times: 19
Loss after 100950600 batches: 0.0145
trigger times: 20
Early stopping!
Start to test process.
Loss after 101079240 batches: 0.0143
Time to train on one home:  330.9348180294037
trigger times: 0
Loss after 101210340 batches: 0.4868
trigger times: 1
Loss after 101341440 batches: 0.1637
trigger times: 2
Loss after 101472540 batches: 0.1056
trigger times: 3
Loss after 101603640 batches: 0.0838
trigger times: 4
Loss after 101734740 batches: 0.0748
trigger times: 5
Loss after 101865840 batches: 0.0666
trigger times: 6
Loss after 101996940 batches: 0.0609
trigger times: 7
Loss after 102128040 batches: 0.0565
trigger times: 8
Loss after 102259140 batches: 0.0534
trigger times: 9
Loss after 102390240 batches: 0.0516
trigger times: 10
Loss after 102521340 batches: 0.0486
trigger times: 11
Loss after 102652440 batches: 0.0473
trigger times: 12
Loss after 102783540 batches: 0.0452
trigger times: 13
Loss after 102914640 batches: 0.0438
trigger times: 14
Loss after 103045740 batches: 0.0426
trigger times: 15
Loss after 103176840 batches: 0.0415
trigger times: 16
Loss after 103307940 batches: 0.0409
trigger times: 17
Loss after 103439040 batches: 0.0404
trigger times: 18
Loss after 103570140 batches: 0.0394
trigger times: 19
Loss after 103701240 batches: 0.0382
trigger times: 20
Early stopping!
Start to test process.
Loss after 103832340 batches: 0.0372
Time to train on one home:  159.8928520679474
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427]]
Round_3_results:  [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 816 < 817; dropping {'Training_Loss': 0.16742071228207284, 'Validation_Loss': 0.24535293711556327, 'Training_R2': 0.8314901916094877, 'Validation_R2': 0.7718648061218749, 'Training_F1': 0.7789744239547768, 'Validation_F1': 0.7231725432896905, 'Training_NEP': 0.4422956115103618, 'Validation_NEP': 0.5355077621794709, 'Training_NDE': 0.12650330421249192, 'Validation_NDE': 0.18167484246093674, 'Training_MAE': 14.64795348643893, 'Validation_MAE': 14.685844524573254, 'Training_MSE': 556.59467, 'Validation_MSE': 670.9202}.
trigger times: 0
Loss after 103963440 batches: 0.1674
trigger times: 0
Loss after 104094540 batches: 0.0454
trigger times: 1
Loss after 104225640 batches: 0.0325
trigger times: 0
Loss after 104356740 batches: 0.0275
trigger times: 1
Loss after 104487840 batches: 0.0253
trigger times: 2
Loss after 104618940 batches: 0.0232
trigger times: 0
Loss after 104750040 batches: 0.0215
trigger times: 1
Loss after 104881140 batches: 0.0204
trigger times: 2
Loss after 105012240 batches: 0.0192
trigger times: 3
Loss after 105143340 batches: 0.0188
trigger times: 0
Loss after 105274440 batches: 0.0181
trigger times: 0
Loss after 105405540 batches: 0.0177
trigger times: 1
Loss after 105536640 batches: 0.0171
trigger times: 0
Loss after 105667740 batches: 0.0169
trigger times: 1
Loss after 105798840 batches: 0.0162
trigger times: 2
Loss after 105929940 batches: 0.0156
trigger times: 3
Loss after 106061040 batches: 0.0152
trigger times: 4
Loss after 106192140 batches: 0.0149
trigger times: 5
Loss after 106323240 batches: 0.0146
trigger times: 6
Loss after 106454340 batches: 0.0146
trigger times: 7
Loss after 106585440 batches: 0.0147
trigger times: 8
Loss after 106716540 batches: 0.0143
trigger times: 0
Loss after 106847640 batches: 0.0139
trigger times: 1
Loss after 106978740 batches: 0.0140
trigger times: 2
Loss after 107109840 batches: 0.0136
trigger times: 3
Loss after 107240940 batches: 0.0136
trigger times: 4
Loss after 107372040 batches: 0.0134
trigger times: 5
Loss after 107503140 batches: 0.0134
trigger times: 0
Loss after 107634240 batches: 0.0134
trigger times: 1
Loss after 107765340 batches: 0.0130
trigger times: 2
Loss after 107896440 batches: 0.0132
trigger times: 3
Loss after 108027540 batches: 0.0127
trigger times: 4
Loss after 108158640 batches: 0.0126
trigger times: 5
Loss after 108289740 batches: 0.0125
trigger times: 6
Loss after 108420840 batches: 0.0124
trigger times: 7
Loss after 108551940 batches: 0.0122
trigger times: 8
Loss after 108683040 batches: 0.0121
trigger times: 9
Loss after 108814140 batches: 0.0118
trigger times: 10
Loss after 108945240 batches: 0.0119
trigger times: 11
Loss after 109076340 batches: 0.0121
trigger times: 12
Loss after 109207440 batches: 0.0116
trigger times: 13
Loss after 109338540 batches: 0.0118
trigger times: 14
Loss after 109469640 batches: 0.0116
trigger times: 15
Loss after 109600740 batches: 0.0116
trigger times: 16
Loss after 109731840 batches: 0.0113
trigger times: 17
Loss after 109862940 batches: 0.0114
trigger times: 18
Loss after 109994040 batches: 0.0110
trigger times: 19
Loss after 110125140 batches: 0.0109
trigger times: 20
Early stopping!
Start to test process.
Loss after 110256240 batches: 0.0112
Time to train on one home:  357.6342041492462
trigger times: 0
Loss after 110358840 batches: 0.3882
trigger times: 1
Loss after 110461440 batches: 0.1532
trigger times: 2
Loss after 110564040 batches: 0.1064
trigger times: 3
Loss after 110666640 batches: 0.0889
trigger times: 4
Loss after 110769240 batches: 0.0752
trigger times: 5
Loss after 110871840 batches: 0.0638
trigger times: 6
Loss after 110974440 batches: 0.0574
trigger times: 7
Loss after 111077040 batches: 0.0544
trigger times: 8
Loss after 111179640 batches: 0.0520
trigger times: 9
Loss after 111282240 batches: 0.0499
trigger times: 10
Loss after 111384840 batches: 0.0486
trigger times: 0
Loss after 111487440 batches: 0.0529
trigger times: 1
Loss after 111590040 batches: 0.0413
trigger times: 2
Loss after 111692640 batches: 0.0394
trigger times: 3
Loss after 111795240 batches: 0.0387
trigger times: 4
Loss after 111897840 batches: 0.0363
trigger times: 5
Loss after 112000440 batches: 0.0361
trigger times: 6
Loss after 112103040 batches: 0.0368
trigger times: 7
Loss after 112205640 batches: 0.0344
trigger times: 8
Loss after 112308240 batches: 0.0335
trigger times: 9
Loss after 112410840 batches: 0.0340
trigger times: 10
Loss after 112513440 batches: 0.0319
trigger times: 0
Loss after 112616040 batches: 0.0340
trigger times: 0
Loss after 112718640 batches: 0.0329
trigger times: 1
Loss after 112821240 batches: 0.0327
trigger times: 0
Loss after 112923840 batches: 0.0352
trigger times: 0
Loss after 113026440 batches: 0.0308
trigger times: 1
Loss after 113129040 batches: 0.0294
trigger times: 2
Loss after 113231640 batches: 0.0281
trigger times: 3
Loss after 113334240 batches: 0.0292
trigger times: 4
Loss after 113436840 batches: 0.0291
trigger times: 5
Loss after 113539440 batches: 0.0276
trigger times: 6
Loss after 113642040 batches: 0.0271
trigger times: 7
Loss after 113744640 batches: 0.0281
trigger times: 0
Loss after 113847240 batches: 0.0360
trigger times: 1
Loss after 113949840 batches: 0.0283
trigger times: 2
Loss after 114052440 batches: 0.0272
trigger times: 3
Loss after 114155040 batches: 0.0287
trigger times: 4
Loss after 114257640 batches: 0.0312
trigger times: 5
Loss after 114360240 batches: 0.0262
trigger times: 6
Loss after 114462840 batches: 0.0257
trigger times: 7
Loss after 114565440 batches: 0.0255
trigger times: 8
Loss after 114668040 batches: 0.0259
trigger times: 9
Loss after 114770640 batches: 0.0266
trigger times: 0
Loss after 114873240 batches: 0.0241
trigger times: 1
Loss after 114975840 batches: 0.0245
trigger times: 2
Loss after 115078440 batches: 0.0241
trigger times: 3
Loss after 115181040 batches: 0.0236
trigger times: 4
Loss after 115283640 batches: 0.0231
trigger times: 5
Loss after 115386240 batches: 0.0230
trigger times: 6
Loss after 115488840 batches: 0.0227
trigger times: 0
Loss after 115591440 batches: 0.0238
trigger times: 1
Loss after 115694040 batches: 0.0280
trigger times: 2
Loss after 115796640 batches: 0.0236
trigger times: 3
Loss after 115899240 batches: 0.0230
trigger times: 0
Loss after 116001840 batches: 0.0244
trigger times: 1
Loss after 116104440 batches: 0.0228
trigger times: 2
Loss after 116207040 batches: 0.0227
trigger times: 3
Loss after 116309640 batches: 0.0240
trigger times: 4
Loss after 116412240 batches: 0.0214
trigger times: 5
Loss after 116514840 batches: 0.0214
trigger times: 6
Loss after 116617440 batches: 0.0252
trigger times: 7
Loss after 116720040 batches: 0.0223
trigger times: 8
Loss after 116822640 batches: 0.0221
trigger times: 9
Loss after 116925240 batches: 0.0216
trigger times: 10
Loss after 117027840 batches: 0.0220
trigger times: 11
Loss after 117130440 batches: 0.0243
trigger times: 12
Loss after 117233040 batches: 0.0220
trigger times: 13
Loss after 117335640 batches: 0.0220
trigger times: 14
Loss after 117438240 batches: 0.0221
trigger times: 15
Loss after 117540840 batches: 0.0229
trigger times: 16
Loss after 117643440 batches: 0.0235
trigger times: 17
Loss after 117746040 batches: 0.0243
trigger times: 18
Loss after 117848640 batches: 0.0218
trigger times: 19
Loss after 117951240 batches: 0.0221
trigger times: 20
Early stopping!
Start to test process.
Loss after 118053840 batches: 0.0212
Time to train on one home:  447.029753446579
trigger times: 0
Loss after 118184940 batches: 0.2067
trigger times: 0
Loss after 118316040 batches: 0.0744
trigger times: 1
Loss after 118447140 batches: 0.0539
trigger times: 2
Loss after 118578240 batches: 0.0464
trigger times: 0
Loss after 118709340 batches: 0.0409
trigger times: 1
Loss after 118840440 batches: 0.0380
trigger times: 2
Loss after 118971540 batches: 0.0354
trigger times: 3
Loss after 119102640 batches: 0.0335
trigger times: 4
Loss after 119233740 batches: 0.0324
trigger times: 5
Loss after 119364840 batches: 0.0304
trigger times: 6
Loss after 119495940 batches: 0.0292
trigger times: 7
Loss after 119627040 batches: 0.0284
trigger times: 8
Loss after 119758140 batches: 0.0276
trigger times: 9
Loss after 119889240 batches: 0.0271
trigger times: 10
Loss after 120020340 batches: 0.0265
trigger times: 11
Loss after 120151440 batches: 0.0255
trigger times: 12
Loss after 120282540 batches: 0.0255
trigger times: 13
Loss after 120413640 batches: 0.0249
trigger times: 14
Loss after 120544740 batches: 0.0244
trigger times: 15
Loss after 120675840 batches: 0.0239
trigger times: 16
Loss after 120806940 batches: 0.0231
trigger times: 17
Loss after 120938040 batches: 0.0231
trigger times: 18
Loss after 121069140 batches: 0.0229
trigger times: 19
Loss after 121200240 batches: 0.0224
trigger times: 20
Early stopping!
Start to test process.
Loss after 121331340 batches: 0.0218
Time to train on one home:  187.7018985748291
trigger times: 0
Loss after 121462440 batches: 0.3973
trigger times: 0
Loss after 121593540 batches: 0.1297
trigger times: 1
Loss after 121724640 batches: 0.0922
trigger times: 2
Loss after 121855740 batches: 0.0756
trigger times: 3
Loss after 121986840 batches: 0.0681
trigger times: 4
Loss after 122117940 batches: 0.0616
trigger times: 5
Loss after 122249040 batches: 0.0573
trigger times: 6
Loss after 122380140 batches: 0.0540
trigger times: 0
Loss after 122511240 batches: 0.0519
trigger times: 1
Loss after 122642340 batches: 0.0497
trigger times: 2
Loss after 122773440 batches: 0.0474
trigger times: 3
Loss after 122904540 batches: 0.0454
trigger times: 4
Loss after 123035640 batches: 0.0438
trigger times: 5
Loss after 123166740 batches: 0.0427
trigger times: 6
Loss after 123297840 batches: 0.0421
trigger times: 7
Loss after 123428940 batches: 0.0410
trigger times: 8
Loss after 123560040 batches: 0.0392
trigger times: 9
Loss after 123691140 batches: 0.0388
trigger times: 0
Loss after 123822240 batches: 0.0379
trigger times: 1
Loss after 123953340 batches: 0.0371
trigger times: 0
Loss after 124084440 batches: 0.0365
trigger times: 1
Loss after 124215540 batches: 0.0363
trigger times: 0
Loss after 124346640 batches: 0.0352
trigger times: 1
Loss after 124477740 batches: 0.0348
trigger times: 0
Loss after 124608840 batches: 0.0348
trigger times: 0
Loss after 124739940 batches: 0.0338
trigger times: 1
Loss after 124871040 batches: 0.0332
trigger times: 2
Loss after 125002140 batches: 0.0325
trigger times: 0
Loss after 125133240 batches: 0.0324
trigger times: 1
Loss after 125264340 batches: 0.0317
trigger times: 2
Loss after 125395440 batches: 0.0317
trigger times: 3
Loss after 125526540 batches: 0.0318
trigger times: 4
Loss after 125657640 batches: 0.0315
trigger times: 5
Loss after 125788740 batches: 0.0309
trigger times: 0
Loss after 125919840 batches: 0.0301
trigger times: 0
Loss after 126050940 batches: 0.0302
trigger times: 1
Loss after 126182040 batches: 0.0298
trigger times: 2
Loss after 126313140 batches: 0.0296
trigger times: 3
Loss after 126444240 batches: 0.0294
trigger times: 4
Loss after 126575340 batches: 0.0291
trigger times: 5
Loss after 126706440 batches: 0.0283
trigger times: 0
Loss after 126837540 batches: 0.0285
trigger times: 1
Loss after 126968640 batches: 0.0282
trigger times: 2
Loss after 127099740 batches: 0.0285
trigger times: 3
Loss after 127230840 batches: 0.0275
trigger times: 0
Loss after 127361940 batches: 0.0273
trigger times: 1
Loss after 127493040 batches: 0.0271
trigger times: 2
Loss after 127624140 batches: 0.0273
trigger times: 3
Loss after 127755240 batches: 0.0272
trigger times: 4
Loss after 127886340 batches: 0.0270
trigger times: 5
Loss after 128017440 batches: 0.0269
trigger times: 0
Loss after 128148540 batches: 0.0267
trigger times: 0
Loss after 128279640 batches: 0.0261
trigger times: 1
Loss after 128410740 batches: 0.0258
trigger times: 0
Loss after 128541840 batches: 0.0258
trigger times: 1
Loss after 128672940 batches: 0.0260
trigger times: 0
Loss after 128804040 batches: 0.0255
trigger times: 1
Loss after 128935140 batches: 0.0254
trigger times: 2
Loss after 129066240 batches: 0.0250
trigger times: 3
Loss after 129197340 batches: 0.0249
trigger times: 4
Loss after 129328440 batches: 0.0249
trigger times: 5
Loss after 129459540 batches: 0.0250
trigger times: 0
Loss after 129590640 batches: 0.0250
trigger times: 1
Loss after 129721740 batches: 0.0242
trigger times: 0
Loss after 129852840 batches: 0.0243
trigger times: 1
Loss after 129983940 batches: 0.0239
trigger times: 2
Loss after 130115040 batches: 0.0239
trigger times: 3
Loss after 130246140 batches: 0.0238
trigger times: 4
Loss after 130377240 batches: 0.0239
trigger times: 5
Loss after 130508340 batches: 0.0238
trigger times: 6
Loss after 130639440 batches: 0.0236
trigger times: 7
Loss after 130770540 batches: 0.0234
trigger times: 8
Loss after 130901640 batches: 0.0240
trigger times: 9
Loss after 131032740 batches: 0.0237
trigger times: 10
Loss after 131163840 batches: 0.0230
trigger times: 11
Loss after 131294940 batches: 0.0232
trigger times: 12
Loss after 131426040 batches: 0.0231
trigger times: 0
Loss after 131557140 batches: 0.0230
trigger times: 0
Loss after 131688240 batches: 0.0231
trigger times: 0
Loss after 131819340 batches: 0.0227
trigger times: 0
Loss after 131950440 batches: 0.0226
trigger times: 1
Loss after 132081540 batches: 0.0226
trigger times: 2
Loss after 132212640 batches: 0.0223
trigger times: 3
Loss after 132343740 batches: 0.0224
trigger times: 4
Loss after 132474840 batches: 0.0220
trigger times: 0
Loss after 132605940 batches: 0.0221
trigger times: 1
Loss after 132737040 batches: 0.0221
trigger times: 0
Loss after 132868140 batches: 0.0220
trigger times: 1
Loss after 132999240 batches: 0.0222
trigger times: 2
Loss after 133130340 batches: 0.0221
trigger times: 3
Loss after 133261440 batches: 0.0216
trigger times: 4
Loss after 133392540 batches: 0.0214
trigger times: 5
Loss after 133523640 batches: 0.0214
trigger times: 0
Loss after 133654740 batches: 0.0213
trigger times: 1
Loss after 133785840 batches: 0.0220
trigger times: 2
Loss after 133916940 batches: 0.0213
trigger times: 3
Loss after 134048040 batches: 0.0217
trigger times: 4
Loss after 134179140 batches: 0.0215
trigger times: 5
Loss after 134310240 batches: 0.0212
trigger times: 0
Loss after 134441340 batches: 0.0214
trigger times: 0
Loss after 134572440 batches: 0.0212
trigger times: 0
Loss after 134703540 batches: 0.0209
trigger times: 1
Loss after 134834640 batches: 0.0207
trigger times: 0
Loss after 134965740 batches: 0.0209
trigger times: 1
Loss after 135096840 batches: 0.0205
trigger times: 2
Loss after 135227940 batches: 0.0208
trigger times: 3
Loss after 135359040 batches: 0.0207
trigger times: 4
Loss after 135490140 batches: 0.0207
trigger times: 5
Loss after 135621240 batches: 0.0204
trigger times: 6
Loss after 135752340 batches: 0.0209
trigger times: 7
Loss after 135883440 batches: 0.0206
trigger times: 8
Loss after 136014540 batches: 0.0201
trigger times: 9
Loss after 136145640 batches: 0.0204
trigger times: 10
Loss after 136276740 batches: 0.0202
trigger times: 11
Loss after 136407840 batches: 0.0204
trigger times: 0
Loss after 136538940 batches: 0.0203
trigger times: 1
Loss after 136670040 batches: 0.0201
trigger times: 2
Loss after 136801140 batches: 0.0201
trigger times: 3
Loss after 136932240 batches: 0.0198
trigger times: 0
Loss after 137063340 batches: 0.0195
trigger times: 1
Loss after 137194440 batches: 0.0199
trigger times: 2
Loss after 137325540 batches: 0.0199
trigger times: 3
Loss after 137456640 batches: 0.0200
trigger times: 4
Loss after 137587740 batches: 0.0197
trigger times: 5
Loss after 137718840 batches: 0.0193
trigger times: 6
Loss after 137849940 batches: 0.0192
trigger times: 7
Loss after 137981040 batches: 0.0192
trigger times: 8
Loss after 138112140 batches: 0.0189
trigger times: 9
Loss after 138243240 batches: 0.0189
trigger times: 10
Loss after 138374340 batches: 0.0192
trigger times: 11
Loss after 138505440 batches: 0.0188
trigger times: 12
Loss after 138636540 batches: 0.0188
trigger times: 13
Loss after 138767640 batches: 0.0188
trigger times: 14
Loss after 138898740 batches: 0.0188
trigger times: 15
Loss after 139029840 batches: 0.0188
trigger times: 16
Loss after 139160940 batches: 0.0188
trigger times: 17
Loss after 139292040 batches: 0.0187
trigger times: 18
Loss after 139423140 batches: 0.0185
trigger times: 19
Loss after 139554240 batches: 0.0184
trigger times: 20
Early stopping!
Start to test process.
Loss after 139685340 batches: 0.0181
Time to train on one home:  1000.6136040687561
trigger times: 0
Loss after 139813980 batches: 0.1658
trigger times: 0
Loss after 139942620 batches: 0.0518
trigger times: 0
Loss after 140071260 batches: 0.0366
trigger times: 0
Loss after 140199900 batches: 0.0312
trigger times: 0
Loss after 140328540 batches: 0.0279
trigger times: 1
Loss after 140457180 batches: 0.0259
trigger times: 2
Loss after 140585820 batches: 0.0240
trigger times: 3
Loss after 140714460 batches: 0.0228
trigger times: 0
Loss after 140843100 batches: 0.0216
trigger times: 0
Loss after 140971740 batches: 0.0210
trigger times: 1
Loss after 141100380 batches: 0.0209
trigger times: 2
Loss after 141229020 batches: 0.0200
trigger times: 0
Loss after 141357660 batches: 0.0192
trigger times: 1
Loss after 141486300 batches: 0.0191
trigger times: 2
Loss after 141614940 batches: 0.0182
trigger times: 3
Loss after 141743580 batches: 0.0185
trigger times: 4
Loss after 141872220 batches: 0.0177
trigger times: 5
Loss after 142000860 batches: 0.0171
trigger times: 6
Loss after 142129500 batches: 0.0171
trigger times: 7
Loss after 142258140 batches: 0.0170
trigger times: 8
Loss after 142386780 batches: 0.0164
trigger times: 9
Loss after 142515420 batches: 0.0164
trigger times: 10
Loss after 142644060 batches: 0.0164
trigger times: 11
Loss after 142772700 batches: 0.0160
trigger times: 12
Loss after 142901340 batches: 0.0157
trigger times: 13
Loss after 143029980 batches: 0.0153
trigger times: 14
Loss after 143158620 batches: 0.0153
trigger times: 15
Loss after 143287260 batches: 0.0149
trigger times: 16
Loss after 143415900 batches: 0.0152
trigger times: 17
Loss after 143544540 batches: 0.0149
trigger times: 18
Loss after 143673180 batches: 0.0146
trigger times: 0
Loss after 143801820 batches: 0.0143
trigger times: 1
Loss after 143930460 batches: 0.0144
trigger times: 2
Loss after 144059100 batches: 0.0148
trigger times: 3
Loss after 144187740 batches: 0.0141
trigger times: 4
Loss after 144316380 batches: 0.0140
trigger times: 5
Loss after 144445020 batches: 0.0139
trigger times: 6
Loss after 144573660 batches: 0.0139
trigger times: 7
Loss after 144702300 batches: 0.0139
trigger times: 8
Loss after 144830940 batches: 0.0136
trigger times: 9
Loss after 144959580 batches: 0.0138
trigger times: 10
Loss after 145088220 batches: 0.0134
trigger times: 0
Loss after 145216860 batches: 0.0135
trigger times: 1
Loss after 145345500 batches: 0.0133
trigger times: 2
Loss after 145474140 batches: 0.0130
trigger times: 3
Loss after 145602780 batches: 0.0132
trigger times: 4
Loss after 145731420 batches: 0.0132
trigger times: 5
Loss after 145860060 batches: 0.0131
trigger times: 6
Loss after 145988700 batches: 0.0129
trigger times: 7
Loss after 146117340 batches: 0.0128
trigger times: 0
Loss after 146245980 batches: 0.0129
trigger times: 1
Loss after 146374620 batches: 0.0127
trigger times: 2
Loss after 146503260 batches: 0.0125
trigger times: 3
Loss after 146631900 batches: 0.0126
trigger times: 4
Loss after 146760540 batches: 0.0125
trigger times: 5
Loss after 146889180 batches: 0.0122
trigger times: 6
Loss after 147017820 batches: 0.0123
trigger times: 7
Loss after 147146460 batches: 0.0124
trigger times: 8
Loss after 147275100 batches: 0.0123
trigger times: 9
Loss after 147403740 batches: 0.0121
trigger times: 10
Loss after 147532380 batches: 0.0124
trigger times: 11
Loss after 147661020 batches: 0.0121
trigger times: 12
Loss after 147789660 batches: 0.0119
trigger times: 13
Loss after 147918300 batches: 0.0117
trigger times: 0
Loss after 148046940 batches: 0.0116
trigger times: 1
Loss after 148175580 batches: 0.0115
trigger times: 2
Loss after 148304220 batches: 0.0115
trigger times: 3
Loss after 148432860 batches: 0.0118
trigger times: 0
Loss after 148561500 batches: 0.0115
trigger times: 1
Loss after 148690140 batches: 0.0114
trigger times: 2
Loss after 148818780 batches: 0.0113
trigger times: 3
Loss after 148947420 batches: 0.0114
trigger times: 4
Loss after 149076060 batches: 0.0115
trigger times: 0
Loss after 149204700 batches: 0.0115
trigger times: 1
Loss after 149333340 batches: 0.0114
trigger times: 2
Loss after 149461980 batches: 0.0114
trigger times: 3
Loss after 149590620 batches: 0.0113
trigger times: 4
Loss after 149719260 batches: 0.0112
trigger times: 5
Loss after 149847900 batches: 0.0110
trigger times: 6
Loss after 149976540 batches: 0.0112
trigger times: 7
Loss after 150105180 batches: 0.0112
trigger times: 8
Loss after 150233820 batches: 0.0108
trigger times: 9
Loss after 150362460 batches: 0.0108
trigger times: 10
Loss after 150491100 batches: 0.0110
trigger times: 0
Loss after 150619740 batches: 0.0108
trigger times: 1
Loss after 150748380 batches: 0.0111
trigger times: 2
Loss after 150877020 batches: 0.0108
trigger times: 3
Loss after 151005660 batches: 0.0108
trigger times: 4
Loss after 151134300 batches: 0.0105
trigger times: 5
Loss after 151262940 batches: 0.0108
trigger times: 6
Loss after 151391580 batches: 0.0108
trigger times: 7
Loss after 151520220 batches: 0.0107
trigger times: 8
Loss after 151648860 batches: 0.0107
trigger times: 9
Loss after 151777500 batches: 0.0106
trigger times: 10
Loss after 151906140 batches: 0.0104
trigger times: 11
Loss after 152034780 batches: 0.0105
trigger times: 12
Loss after 152163420 batches: 0.0103
trigger times: 13
Loss after 152292060 batches: 0.0104
trigger times: 14
Loss after 152420700 batches: 0.0104
trigger times: 15
Loss after 152549340 batches: 0.0103
trigger times: 16
Loss after 152677980 batches: 0.0102
trigger times: 17
Loss after 152806620 batches: 0.0102
trigger times: 18
Loss after 152935260 batches: 0.0102
trigger times: 19
Loss after 153063900 batches: 0.0101
trigger times: 20
Early stopping!
Start to test process.
Loss after 153192540 batches: 0.0101
Time to train on one home:  740.9657833576202
trigger times: 0
Loss after 153323640 batches: 0.4011
trigger times: 1
Loss after 153454740 batches: 0.1159
trigger times: 2
Loss after 153585840 batches: 0.0794
trigger times: 3
Loss after 153716940 batches: 0.0669
trigger times: 4
Loss after 153848040 batches: 0.0603
trigger times: 0
Loss after 153979140 batches: 0.0541
trigger times: 1
Loss after 154110240 batches: 0.0504
trigger times: 0
Loss after 154241340 batches: 0.0475
trigger times: 0
Loss after 154372440 batches: 0.0455
trigger times: 1
Loss after 154503540 batches: 0.0435
trigger times: 2
Loss after 154634640 batches: 0.0414
trigger times: 0
Loss after 154765740 batches: 0.0408
trigger times: 1
Loss after 154896840 batches: 0.0395
trigger times: 0
Loss after 155027940 batches: 0.0381
trigger times: 1
Loss after 155159040 batches: 0.0372
trigger times: 2
Loss after 155290140 batches: 0.0363
trigger times: 3
Loss after 155421240 batches: 0.0353
trigger times: 4
Loss after 155552340 batches: 0.0348
trigger times: 0
Loss after 155683440 batches: 0.0339
trigger times: 0
Loss after 155814540 batches: 0.0334
trigger times: 1
Loss after 155945640 batches: 0.0328
trigger times: 0
Loss after 156076740 batches: 0.0323
trigger times: 0
Loss after 156207840 batches: 0.0322
trigger times: 1
Loss after 156338940 batches: 0.0315
trigger times: 0
Loss after 156470040 batches: 0.0307
trigger times: 1
Loss after 156601140 batches: 0.0307
trigger times: 0
Loss after 156732240 batches: 0.0302
trigger times: 0
Loss after 156863340 batches: 0.0297
trigger times: 1
Loss after 156994440 batches: 0.0294
trigger times: 2
Loss after 157125540 batches: 0.0292
trigger times: 0
Loss after 157256640 batches: 0.0293
trigger times: 1
Loss after 157387740 batches: 0.0291
trigger times: 2
Loss after 157518840 batches: 0.0286
trigger times: 3
Loss after 157649940 batches: 0.0280
trigger times: 4
Loss after 157781040 batches: 0.0273
trigger times: 0
Loss after 157912140 batches: 0.0273
trigger times: 1
Loss after 158043240 batches: 0.0270
trigger times: 2
Loss after 158174340 batches: 0.0271
trigger times: 3
Loss after 158305440 batches: 0.0270
trigger times: 4
Loss after 158436540 batches: 0.0266
trigger times: 5
Loss after 158567640 batches: 0.0261
trigger times: 6
Loss after 158698740 batches: 0.0263
trigger times: 7
Loss after 158829840 batches: 0.0257
trigger times: 8
Loss after 158960940 batches: 0.0261
trigger times: 0
Loss after 159092040 batches: 0.0254
trigger times: 1
Loss after 159223140 batches: 0.0255
trigger times: 2
Loss after 159354240 batches: 0.0251
trigger times: 3
Loss after 159485340 batches: 0.0250
trigger times: 4
Loss after 159616440 batches: 0.0248
trigger times: 0
Loss after 159747540 batches: 0.0245
trigger times: 1
Loss after 159878640 batches: 0.0247
trigger times: 2
Loss after 160009740 batches: 0.0244
trigger times: 3
Loss after 160140840 batches: 0.0243
trigger times: 4
Loss after 160271940 batches: 0.0242
trigger times: 5
Loss after 160403040 batches: 0.0243
trigger times: 6
Loss after 160534140 batches: 0.0236
trigger times: 7
Loss after 160665240 batches: 0.0235
trigger times: 8
Loss after 160796340 batches: 0.0235
trigger times: 9
Loss after 160927440 batches: 0.0237
trigger times: 0
Loss after 161058540 batches: 0.0238
trigger times: 1
Loss after 161189640 batches: 0.0230
trigger times: 2
Loss after 161320740 batches: 0.0230
trigger times: 3
Loss after 161451840 batches: 0.0228
trigger times: 4
Loss after 161582940 batches: 0.0226
trigger times: 5
Loss after 161714040 batches: 0.0229
trigger times: 6
Loss after 161845140 batches: 0.0223
trigger times: 7
Loss after 161976240 batches: 0.0223
trigger times: 8
Loss after 162107340 batches: 0.0225
trigger times: 0
Loss after 162238440 batches: 0.0220
trigger times: 1
Loss after 162369540 batches: 0.0220
trigger times: 0
Loss after 162500640 batches: 0.0216
trigger times: 1
Loss after 162631740 batches: 0.0217
trigger times: 2
Loss after 162762840 batches: 0.0216
trigger times: 3
Loss after 162893940 batches: 0.0214
trigger times: 0
Loss after 163025040 batches: 0.0212
trigger times: 1
Loss after 163156140 batches: 0.0215
trigger times: 0
Loss after 163287240 batches: 0.0209
trigger times: 1
Loss after 163418340 batches: 0.0213
trigger times: 2
Loss after 163549440 batches: 0.0211
trigger times: 0
Loss after 163680540 batches: 0.0208
trigger times: 1
Loss after 163811640 batches: 0.0208
trigger times: 2
Loss after 163942740 batches: 0.0207
trigger times: 0
Loss after 164073840 batches: 0.0209
trigger times: 1
Loss after 164204940 batches: 0.0204
trigger times: 2
Loss after 164336040 batches: 0.0207
trigger times: 3
Loss after 164467140 batches: 0.0204
trigger times: 0
Loss after 164598240 batches: 0.0204
trigger times: 1
Loss after 164729340 batches: 0.0207
trigger times: 0
Loss after 164860440 batches: 0.0205
trigger times: 1
Loss after 164991540 batches: 0.0206
trigger times: 0
Loss after 165122640 batches: 0.0204
trigger times: 1
Loss after 165253740 batches: 0.0205
trigger times: 2
Loss after 165384840 batches: 0.0202
trigger times: 3
Loss after 165515940 batches: 0.0200
trigger times: 4
Loss after 165647040 batches: 0.0196
trigger times: 5
Loss after 165778140 batches: 0.0196
trigger times: 0
Loss after 165909240 batches: 0.0196
trigger times: 1
Loss after 166040340 batches: 0.0195
trigger times: 0
Loss after 166171440 batches: 0.0195
trigger times: 1
Loss after 166302540 batches: 0.0196
trigger times: 2
Loss after 166433640 batches: 0.0192
trigger times: 3
Loss after 166564740 batches: 0.0193
trigger times: 4
Loss after 166695840 batches: 0.0192
trigger times: 5
Loss after 166826940 batches: 0.0192
trigger times: 6
Loss after 166958040 batches: 0.0193
trigger times: 7
Loss after 167089140 batches: 0.0194
trigger times: 8
Loss after 167220240 batches: 0.0193
trigger times: 9
Loss after 167351340 batches: 0.0189
trigger times: 10
Loss after 167482440 batches: 0.0188
trigger times: 11
Loss after 167613540 batches: 0.0187
trigger times: 12
Loss after 167744640 batches: 0.0188
trigger times: 13
Loss after 167875740 batches: 0.0190
trigger times: 14
Loss after 168006840 batches: 0.0185
trigger times: 15
Loss after 168137940 batches: 0.0186
trigger times: 16
Loss after 168269040 batches: 0.0185
trigger times: 17
Loss after 168400140 batches: 0.0184
trigger times: 18
Loss after 168531240 batches: 0.0187
trigger times: 19
Loss after 168662340 batches: 0.0183
trigger times: 20
Early stopping!
Start to test process.
Loss after 168793440 batches: 0.0185
Time to train on one home:  850.7961657047272
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147]]
Round_4_results:  [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147]
trigger times: 0
Loss after 168924540 batches: 0.1457
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1330 < 1331; dropping {'Training_Loss': 0.14566809037383996, 'Validation_Loss': 0.2304039498170217, 'Training_R2': 0.8533379963057354, 'Validation_R2': 0.7857816596310421, 'Training_F1': 0.7946275927042734, 'Validation_F1': 0.7292030258667529, 'Training_NEP': 0.4106404227380103, 'Validation_NEP': 0.5081779900809721, 'Training_NDE': 0.11010176942788476, 'Validation_NDE': 0.17059219394077657, 'Training_MAE': 13.599596413307568, 'Validation_MAE': 13.936348789353524, 'Training_MSE': 484.43057, 'Validation_MSE': 629.9923}.
trigger times: 0
Loss after 169055640 batches: 0.0374
trigger times: 1
Loss after 169186740 batches: 0.0267
trigger times: 2
Loss after 169317840 batches: 0.0235
trigger times: 0
Loss after 169448940 batches: 0.0207
trigger times: 0
Loss after 169580040 batches: 0.0193
trigger times: 0
Loss after 169711140 batches: 0.0184
trigger times: 1
Loss after 169842240 batches: 0.0175
trigger times: 2
Loss after 169973340 batches: 0.0164
trigger times: 0
Loss after 170104440 batches: 0.0163
trigger times: 0
Loss after 170235540 batches: 0.0157
trigger times: 1
Loss after 170366640 batches: 0.0155
trigger times: 2
Loss after 170497740 batches: 0.0152
trigger times: 0
Loss after 170628840 batches: 0.0145
trigger times: 1
Loss after 170759940 batches: 0.0144
trigger times: 2
Loss after 170891040 batches: 0.0139
trigger times: 0
Loss after 171022140 batches: 0.0138
trigger times: 1
Loss after 171153240 batches: 0.0137
trigger times: 2
Loss after 171284340 batches: 0.0133
trigger times: 0
Loss after 171415440 batches: 0.0128
trigger times: 1
Loss after 171546540 batches: 0.0131
trigger times: 2
Loss after 171677640 batches: 0.0130
trigger times: 3
Loss after 171808740 batches: 0.0128
trigger times: 4
Loss after 171939840 batches: 0.0124
trigger times: 5
Loss after 172070940 batches: 0.0122
trigger times: 6
Loss after 172202040 batches: 0.0120
trigger times: 7
Loss after 172333140 batches: 0.0118
trigger times: 8
Loss after 172464240 batches: 0.0119
trigger times: 9
Loss after 172595340 batches: 0.0117
trigger times: 10
Loss after 172726440 batches: 0.0116
trigger times: 11
Loss after 172857540 batches: 0.0113
trigger times: 12
Loss after 172988640 batches: 0.0113
trigger times: 13
Loss after 173119740 batches: 0.0113
trigger times: 14
Loss after 173250840 batches: 0.0114
trigger times: 15
Loss after 173381940 batches: 0.0114
trigger times: 0
Loss after 173513040 batches: 0.0111
trigger times: 1
Loss after 173644140 batches: 0.0110
trigger times: 2
Loss after 173775240 batches: 0.0113
trigger times: 3
Loss after 173906340 batches: 0.0109
trigger times: 4
Loss after 174037440 batches: 0.0110
trigger times: 5
Loss after 174168540 batches: 0.0105
trigger times: 6
Loss after 174299640 batches: 0.0106
trigger times: 7
Loss after 174430740 batches: 0.0104
trigger times: 8
Loss after 174561840 batches: 0.0104
trigger times: 9
Loss after 174692940 batches: 0.0104
trigger times: 10
Loss after 174824040 batches: 0.0104
trigger times: 11
Loss after 174955140 batches: 0.0104
trigger times: 12
Loss after 175086240 batches: 0.0102
trigger times: 13
Loss after 175217340 batches: 0.0102
trigger times: 14
Loss after 175348440 batches: 0.0099
trigger times: 15
Loss after 175479540 batches: 0.0101
trigger times: 16
Loss after 175610640 batches: 0.0100
trigger times: 17
Loss after 175741740 batches: 0.0097
trigger times: 18
Loss after 175872840 batches: 0.0102
trigger times: 19
Loss after 176003940 batches: 0.0098
trigger times: 20
Early stopping!
Start to test process.
Loss after 176135040 batches: 0.0097
Time to train on one home:  406.9245443344116
trigger times: 0
Loss after 176237640 batches: 0.4201
trigger times: 1
Loss after 176340240 batches: 0.1394
trigger times: 2
Loss after 176442840 batches: 0.0895
trigger times: 3
Loss after 176545440 batches: 0.0691
trigger times: 4
Loss after 176648040 batches: 0.0568
trigger times: 5
Loss after 176750640 batches: 0.0508
trigger times: 6
Loss after 176853240 batches: 0.0523
trigger times: 7
Loss after 176955840 batches: 0.0480
trigger times: 8
Loss after 177058440 batches: 0.0415
trigger times: 9
Loss after 177161040 batches: 0.0442
trigger times: 10
Loss after 177263640 batches: 0.0378
trigger times: 11
Loss after 177366240 batches: 0.0372
trigger times: 12
Loss after 177468840 batches: 0.0391
trigger times: 13
Loss after 177571440 batches: 0.0349
trigger times: 14
Loss after 177674040 batches: 0.0319
trigger times: 15
Loss after 177776640 batches: 0.0310
trigger times: 16
Loss after 177879240 batches: 0.0317
trigger times: 17
Loss after 177981840 batches: 0.0338
trigger times: 18
Loss after 178084440 batches: 0.0299
trigger times: 19
Loss after 178187040 batches: 0.0288
trigger times: 20
Early stopping!
Start to test process.
Loss after 178289640 batches: 0.0285
Time to train on one home:  132.17515778541565
trigger times: 0
Loss after 178420740 batches: 0.2036
trigger times: 1
Loss after 178551840 batches: 0.0674
trigger times: 2
Loss after 178682940 batches: 0.0472
trigger times: 3
Loss after 178814040 batches: 0.0398
trigger times: 4
Loss after 178945140 batches: 0.0353
trigger times: 5
Loss after 179076240 batches: 0.0324
trigger times: 6
Loss after 179207340 batches: 0.0309
trigger times: 7
Loss after 179338440 batches: 0.0291
trigger times: 8
Loss after 179469540 batches: 0.0275
trigger times: 9
Loss after 179600640 batches: 0.0266
trigger times: 10
Loss after 179731740 batches: 0.0255
trigger times: 11
Loss after 179862840 batches: 0.0245
trigger times: 0
Loss after 179993940 batches: 0.0241
trigger times: 1
Loss after 180125040 batches: 0.0237
trigger times: 2
Loss after 180256140 batches: 0.0232
trigger times: 3
Loss after 180387240 batches: 0.0224
trigger times: 4
Loss after 180518340 batches: 0.0224
trigger times: 5
Loss after 180649440 batches: 0.0216
trigger times: 6
Loss after 180780540 batches: 0.0215
trigger times: 7
Loss after 180911640 batches: 0.0210
trigger times: 0
Loss after 181042740 batches: 0.0208
trigger times: 1
Loss after 181173840 batches: 0.0202
trigger times: 2
Loss after 181304940 batches: 0.0200
trigger times: 3
Loss after 181436040 batches: 0.0199
trigger times: 4
Loss after 181567140 batches: 0.0196
trigger times: 5
Loss after 181698240 batches: 0.0194
trigger times: 6
Loss after 181829340 batches: 0.0194
trigger times: 7
Loss after 181960440 batches: 0.0191
trigger times: 8
Loss after 182091540 batches: 0.0185
trigger times: 9
Loss after 182222640 batches: 0.0185
trigger times: 10
Loss after 182353740 batches: 0.0185
trigger times: 11
Loss after 182484840 batches: 0.0183
trigger times: 12
Loss after 182615940 batches: 0.0179
trigger times: 13
Loss after 182747040 batches: 0.0179
trigger times: 14
Loss after 182878140 batches: 0.0179
trigger times: 15
Loss after 183009240 batches: 0.0178
trigger times: 16
Loss after 183140340 batches: 0.0176
trigger times: 17
Loss after 183271440 batches: 0.0174
trigger times: 18
Loss after 183402540 batches: 0.0173
trigger times: 19
Loss after 183533640 batches: 0.0172
trigger times: 20
Early stopping!
Start to test process.
Loss after 183664740 batches: 0.0171
Time to train on one home:  301.31759762763977
trigger times: 0
Loss after 183795840 batches: 0.3185
trigger times: 0
Loss after 183926940 batches: 0.0932
trigger times: 1
Loss after 184058040 batches: 0.0639
trigger times: 2
Loss after 184189140 batches: 0.0529
trigger times: 3
Loss after 184320240 batches: 0.0468
trigger times: 4
Loss after 184451340 batches: 0.0430
trigger times: 5
Loss after 184582440 batches: 0.0395
trigger times: 0
Loss after 184713540 batches: 0.0373
trigger times: 0
Loss after 184844640 batches: 0.0358
trigger times: 1
Loss after 184975740 batches: 0.0348
trigger times: 2
Loss after 185106840 batches: 0.0333
trigger times: 3
Loss after 185237940 batches: 0.0321
trigger times: 0
Loss after 185369040 batches: 0.0315
trigger times: 1
Loss after 185500140 batches: 0.0305
trigger times: 0
Loss after 185631240 batches: 0.0297
trigger times: 0
Loss after 185762340 batches: 0.0291
trigger times: 1
Loss after 185893440 batches: 0.0286
trigger times: 2
Loss after 186024540 batches: 0.0283
trigger times: 0
Loss after 186155640 batches: 0.0275
trigger times: 1
Loss after 186286740 batches: 0.0270
trigger times: 0
Loss after 186417840 batches: 0.0266
trigger times: 1
Loss after 186548940 batches: 0.0264
trigger times: 2
Loss after 186680040 batches: 0.0258
trigger times: 3
Loss after 186811140 batches: 0.0256
trigger times: 4
Loss after 186942240 batches: 0.0255
trigger times: 5
Loss after 187073340 batches: 0.0253
trigger times: 6
Loss after 187204440 batches: 0.0247
trigger times: 0
Loss after 187335540 batches: 0.0247
trigger times: 0
Loss after 187466640 batches: 0.0242
trigger times: 1
Loss after 187597740 batches: 0.0239
trigger times: 0
Loss after 187728840 batches: 0.0240
trigger times: 1
Loss after 187859940 batches: 0.0238
trigger times: 2
Loss after 187991040 batches: 0.0235
trigger times: 0
Loss after 188122140 batches: 0.0234
trigger times: 1
Loss after 188253240 batches: 0.0230
trigger times: 0
Loss after 188384340 batches: 0.0233
trigger times: 1
Loss after 188515440 batches: 0.0238
trigger times: 2
Loss after 188646540 batches: 0.0230
trigger times: 3
Loss after 188777640 batches: 0.0222
trigger times: 4
Loss after 188908740 batches: 0.0225
trigger times: 5
Loss after 189039840 batches: 0.0224
trigger times: 6
Loss after 189170940 batches: 0.0222
trigger times: 7
Loss after 189302040 batches: 0.0222
trigger times: 8
Loss after 189433140 batches: 0.0221
trigger times: 9
Loss after 189564240 batches: 0.0219
trigger times: 10
Loss after 189695340 batches: 0.0218
trigger times: 11
Loss after 189826440 batches: 0.0216
trigger times: 12
Loss after 189957540 batches: 0.0214
trigger times: 13
Loss after 190088640 batches: 0.0212
trigger times: 14
Loss after 190219740 batches: 0.0210
trigger times: 15
Loss after 190350840 batches: 0.0206
trigger times: 16
Loss after 190481940 batches: 0.0206
trigger times: 17
Loss after 190613040 batches: 0.0208
trigger times: 18
Loss after 190744140 batches: 0.0204
trigger times: 19
Loss after 190875240 batches: 0.0204
trigger times: 20
Early stopping!
Start to test process.
Loss after 191006340 batches: 0.0204
Time to train on one home:  406.7566976547241
trigger times: 0
Loss after 191134980 batches: 0.1435
trigger times: 0
Loss after 191263620 batches: 0.0425
trigger times: 0
Loss after 191392260 batches: 0.0298
trigger times: 0
Loss after 191520900 batches: 0.0247
trigger times: 1
Loss after 191649540 batches: 0.0226
trigger times: 2
Loss after 191778180 batches: 0.0210
trigger times: 3
Loss after 191906820 batches: 0.0194
trigger times: 4
Loss after 192035460 batches: 0.0186
trigger times: 0
Loss after 192164100 batches: 0.0180
trigger times: 1
Loss after 192292740 batches: 0.0179
trigger times: 2
Loss after 192421380 batches: 0.0167
trigger times: 3
Loss after 192550020 batches: 0.0165
trigger times: 4
Loss after 192678660 batches: 0.0162
trigger times: 5
Loss after 192807300 batches: 0.0156
trigger times: 6
Loss after 192935940 batches: 0.0151
trigger times: 7
Loss after 193064580 batches: 0.0149
trigger times: 8
Loss after 193193220 batches: 0.0149
trigger times: 9
Loss after 193321860 batches: 0.0143
trigger times: 10
Loss after 193450500 batches: 0.0145
trigger times: 11
Loss after 193579140 batches: 0.0142
trigger times: 12
Loss after 193707780 batches: 0.0139
trigger times: 13
Loss after 193836420 batches: 0.0136
trigger times: 14
Loss after 193965060 batches: 0.0133
trigger times: 0
Loss after 194093700 batches: 0.0132
trigger times: 1
Loss after 194222340 batches: 0.0135
trigger times: 2
Loss after 194350980 batches: 0.0132
trigger times: 3
Loss after 194479620 batches: 0.0132
trigger times: 4
Loss after 194608260 batches: 0.0131
trigger times: 5
Loss after 194736900 batches: 0.0126
trigger times: 6
Loss after 194865540 batches: 0.0125
trigger times: 7
Loss after 194994180 batches: 0.0127
trigger times: 8
Loss after 195122820 batches: 0.0124
trigger times: 9
Loss after 195251460 batches: 0.0123
trigger times: 10
Loss after 195380100 batches: 0.0123
trigger times: 11
Loss after 195508740 batches: 0.0123
trigger times: 12
Loss after 195637380 batches: 0.0123
trigger times: 13
Loss after 195766020 batches: 0.0121
trigger times: 14
Loss after 195894660 batches: 0.0121
trigger times: 15
Loss after 196023300 batches: 0.0119
trigger times: 16
Loss after 196151940 batches: 0.0121
trigger times: 17
Loss after 196280580 batches: 0.0118
trigger times: 18
Loss after 196409220 batches: 0.0114
trigger times: 19
Loss after 196537860 batches: 0.0114
trigger times: 20
Early stopping!
Start to test process.
Loss after 196666500 batches: 0.0115
Time to train on one home:  317.44207239151
trigger times: 0
Loss after 196797600 batches: 0.5196
trigger times: 1
Loss after 196928700 batches: 0.1621
trigger times: 2
Loss after 197059800 batches: 0.0953
trigger times: 3
Loss after 197190900 batches: 0.0735
trigger times: 4
Loss after 197322000 batches: 0.0630
trigger times: 5
Loss after 197453100 batches: 0.0566
trigger times: 6
Loss after 197584200 batches: 0.0519
trigger times: 7
Loss after 197715300 batches: 0.0481
trigger times: 8
Loss after 197846400 batches: 0.0451
trigger times: 9
Loss after 197977500 batches: 0.0438
trigger times: 10
Loss after 198108600 batches: 0.0415
trigger times: 11
Loss after 198239700 batches: 0.0396
trigger times: 12
Loss after 198370800 batches: 0.0390
trigger times: 13
Loss after 198501900 batches: 0.0380
trigger times: 14
Loss after 198633000 batches: 0.0374
trigger times: 0
Loss after 198764100 batches: 0.0356
trigger times: 1
Loss after 198895200 batches: 0.0347
trigger times: 2
Loss after 199026300 batches: 0.0341
trigger times: 3
Loss after 199157400 batches: 0.0329
trigger times: 4
Loss after 199288500 batches: 0.0327
trigger times: 5
Loss after 199419600 batches: 0.0324
trigger times: 6
Loss after 199550700 batches: 0.0314
trigger times: 0
Loss after 199681800 batches: 0.0314
trigger times: 1
Loss after 199812900 batches: 0.0303
trigger times: 2
Loss after 199944000 batches: 0.0299
trigger times: 3
Loss after 200075100 batches: 0.0294
trigger times: 4
Loss after 200206200 batches: 0.0292
trigger times: 5
Loss after 200337300 batches: 0.0288
trigger times: 6
Loss after 200468400 batches: 0.0284
trigger times: 7
Loss after 200599500 batches: 0.0283
trigger times: 0
Loss after 200730600 batches: 0.0276
trigger times: 1
Loss after 200861700 batches: 0.0274
trigger times: 0
Loss after 200992800 batches: 0.0267
trigger times: 0
Loss after 201123900 batches: 0.0269
trigger times: 1
Loss after 201255000 batches: 0.0265
trigger times: 2
Loss after 201386100 batches: 0.0263
trigger times: 3
Loss after 201517200 batches: 0.0260
trigger times: 4
Loss after 201648300 batches: 0.0259
trigger times: 5
Loss after 201779400 batches: 0.0253
trigger times: 6
Loss after 201910500 batches: 0.0253
trigger times: 7
Loss after 202041600 batches: 0.0253
trigger times: 8
Loss after 202172700 batches: 0.0249
trigger times: 9
Loss after 202303800 batches: 0.0248
trigger times: 10
Loss after 202434900 batches: 0.0248
trigger times: 11
Loss after 202566000 batches: 0.0245
trigger times: 12
Loss after 202697100 batches: 0.0242
trigger times: 13
Loss after 202828200 batches: 0.0243
trigger times: 14
Loss after 202959300 batches: 0.0239
trigger times: 15
Loss after 203090400 batches: 0.0240
trigger times: 16
Loss after 203221500 batches: 0.0237
trigger times: 17
Loss after 203352600 batches: 0.0234
trigger times: 0
Loss after 203483700 batches: 0.0232
trigger times: 1
Loss after 203614800 batches: 0.0229
trigger times: 2
Loss after 203745900 batches: 0.0227
trigger times: 3
Loss after 203877000 batches: 0.0229
trigger times: 4
Loss after 204008100 batches: 0.0225
trigger times: 5
Loss after 204139200 batches: 0.0224
trigger times: 6
Loss after 204270300 batches: 0.0222
trigger times: 7
Loss after 204401400 batches: 0.0222
trigger times: 8
Loss after 204532500 batches: 0.0220
trigger times: 9
Loss after 204663600 batches: 0.0221
trigger times: 0
Loss after 204794700 batches: 0.0216
trigger times: 1
Loss after 204925800 batches: 0.0216
trigger times: 2
Loss after 205056900 batches: 0.0214
trigger times: 3
Loss after 205188000 batches: 0.0212
trigger times: 4
Loss after 205319100 batches: 0.0212
trigger times: 5
Loss after 205450200 batches: 0.0212
trigger times: 6
Loss after 205581300 batches: 0.0210
trigger times: 7
Loss after 205712400 batches: 0.0209
trigger times: 8
Loss after 205843500 batches: 0.0212
trigger times: 9
Loss after 205974600 batches: 0.0209
trigger times: 0
Loss after 206105700 batches: 0.0206
trigger times: 1
Loss after 206236800 batches: 0.0206
trigger times: 2
Loss after 206367900 batches: 0.0205
trigger times: 3
Loss after 206499000 batches: 0.0207
trigger times: 4
Loss after 206630100 batches: 0.0204
trigger times: 5
Loss after 206761200 batches: 0.0202
trigger times: 6
Loss after 206892300 batches: 0.0201
trigger times: 7
Loss after 207023400 batches: 0.0200
trigger times: 8
Loss after 207154500 batches: 0.0202
trigger times: 9
Loss after 207285600 batches: 0.0199
trigger times: 0
Loss after 207416700 batches: 0.0198
trigger times: 0
Loss after 207547800 batches: 0.0198
trigger times: 1
Loss after 207678900 batches: 0.0196
trigger times: 2
Loss after 207810000 batches: 0.0194
trigger times: 3
Loss after 207941100 batches: 0.0195
trigger times: 4
Loss after 208072200 batches: 0.0194
trigger times: 5
Loss after 208203300 batches: 0.0191
trigger times: 6
Loss after 208334400 batches: 0.0193
trigger times: 7
Loss after 208465500 batches: 0.0193
trigger times: 0
Loss after 208596600 batches: 0.0193
trigger times: 1
Loss after 208727700 batches: 0.0192
trigger times: 0
Loss after 208858800 batches: 0.0189
trigger times: 1
Loss after 208989900 batches: 0.0188
trigger times: 2
Loss after 209121000 batches: 0.0189
trigger times: 3
Loss after 209252100 batches: 0.0189
trigger times: 4
Loss after 209383200 batches: 0.0186
trigger times: 5
Loss after 209514300 batches: 0.0187
trigger times: 6
Loss after 209645400 batches: 0.0183
trigger times: 7
Loss after 209776500 batches: 0.0187
trigger times: 8
Loss after 209907600 batches: 0.0185
trigger times: 9
Loss after 210038700 batches: 0.0185
trigger times: 10
Loss after 210169800 batches: 0.0185
trigger times: 11
Loss after 210300900 batches: 0.0184
trigger times: 12
Loss after 210432000 batches: 0.0185
trigger times: 13
Loss after 210563100 batches: 0.0186
trigger times: 14
Loss after 210694200 batches: 0.0182
trigger times: 15
Loss after 210825300 batches: 0.0179
trigger times: 16
Loss after 210956400 batches: 0.0181
trigger times: 17
Loss after 211087500 batches: 0.0183
trigger times: 18
Loss after 211218600 batches: 0.0179
trigger times: 19
Loss after 211349700 batches: 0.0177
trigger times: 20
Early stopping!
Start to test process.
Loss after 211480800 batches: 0.0184
Time to train on one home:  810.4185440540314
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964]]
Round_5_results:  [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964]
trigger times: 0
Loss after 211611900 batches: 0.1459
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1661 < 1662; dropping {'Training_Loss': 0.14590998183725015, 'Validation_Loss': 0.23812955617904663, 'Training_R2': 0.8530396322268887, 'Validation_R2': 0.7786433732650971, 'Training_F1': 0.7967396604522425, 'Validation_F1': 0.733761351967877, 'Training_NEP': 0.4066864143897969, 'Validation_NEP': 0.504744523157012, 'Training_NDE': 0.11032575663784538, 'Validation_NDE': 0.17627674891420567, 'Training_MAE': 13.468647498458873, 'Validation_MAE': 13.842188881716847, 'Training_MSE': 485.41602, 'Validation_MSE': 650.98517}.
trigger times: 0
Loss after 211743000 batches: 0.0348
trigger times: 0
Loss after 211874100 batches: 0.0257
trigger times: 0
Loss after 212005200 batches: 0.0217
trigger times: 1
Loss after 212136300 batches: 0.0190
trigger times: 2
Loss after 212267400 batches: 0.0180
trigger times: 3
Loss after 212398500 batches: 0.0168
trigger times: 4
Loss after 212529600 batches: 0.0163
trigger times: 5
Loss after 212660700 batches: 0.0154
trigger times: 6
Loss after 212791800 batches: 0.0150
trigger times: 7
Loss after 212922900 batches: 0.0148
trigger times: 8
Loss after 213054000 batches: 0.0142
trigger times: 9
Loss after 213185100 batches: 0.0138
trigger times: 0
Loss after 213316200 batches: 0.0135
trigger times: 1
Loss after 213447300 batches: 0.0132
trigger times: 2
Loss after 213578400 batches: 0.0128
trigger times: 3
Loss after 213709500 batches: 0.0125
trigger times: 4
Loss after 213840600 batches: 0.0124
trigger times: 5
Loss after 213971700 batches: 0.0122
trigger times: 6
Loss after 214102800 batches: 0.0122
trigger times: 7
Loss after 214233900 batches: 0.0118
trigger times: 8
Loss after 214365000 batches: 0.0118
trigger times: 9
Loss after 214496100 batches: 0.0114
trigger times: 10
Loss after 214627200 batches: 0.0116
trigger times: 11
Loss after 214758300 batches: 0.0113
trigger times: 12
Loss after 214889400 batches: 0.0112
trigger times: 13
Loss after 215020500 batches: 0.0111
trigger times: 14
Loss after 215151600 batches: 0.0110
trigger times: 15
Loss after 215282700 batches: 0.0109
trigger times: 16
Loss after 215413800 batches: 0.0110
trigger times: 17
Loss after 215544900 batches: 0.0107
trigger times: 18
Loss after 215676000 batches: 0.0108
trigger times: 19
Loss after 215807100 batches: 0.0107
trigger times: 20
Early stopping!
Start to test process.
Loss after 215938200 batches: 0.0109
Time to train on one home:  251.67071843147278
trigger times: 0
Loss after 216040800 batches: 0.3379
trigger times: 1
Loss after 216143400 batches: 0.1059
trigger times: 0
Loss after 216246000 batches: 0.0687
trigger times: 1
Loss after 216348600 batches: 0.0550
trigger times: 2
Loss after 216451200 batches: 0.0488
trigger times: 3
Loss after 216553800 batches: 0.0434
trigger times: 4
Loss after 216656400 batches: 0.0389
trigger times: 5
Loss after 216759000 batches: 0.0386
trigger times: 6
Loss after 216861600 batches: 0.0350
trigger times: 7
Loss after 216964200 batches: 0.0328
trigger times: 8
Loss after 217066800 batches: 0.0332
trigger times: 9
Loss after 217169400 batches: 0.0309
trigger times: 10
Loss after 217272000 batches: 0.0312
trigger times: 11
Loss after 217374600 batches: 0.0322
trigger times: 12
Loss after 217477200 batches: 0.0309
trigger times: 13
Loss after 217579800 batches: 0.0303
trigger times: 14
Loss after 217682400 batches: 0.0321
trigger times: 15
Loss after 217785000 batches: 0.0303
trigger times: 16
Loss after 217887600 batches: 0.0273
trigger times: 17
Loss after 217990200 batches: 0.0263
trigger times: 18
Loss after 218092800 batches: 0.0255
trigger times: 19
Loss after 218195400 batches: 0.0250
trigger times: 20
Early stopping!
Start to test process.
Loss after 218298000 batches: 0.0256
Time to train on one home:  143.41238617897034
trigger times: 0
Loss after 218429100 batches: 0.1741
trigger times: 1
Loss after 218560200 batches: 0.0550
trigger times: 2
Loss after 218691300 batches: 0.0397
trigger times: 3
Loss after 218822400 batches: 0.0343
trigger times: 4
Loss after 218953500 batches: 0.0310
trigger times: 5
Loss after 219084600 batches: 0.0284
trigger times: 6
Loss after 219215700 batches: 0.0266
trigger times: 7
Loss after 219346800 batches: 0.0253
trigger times: 8
Loss after 219477900 batches: 0.0245
trigger times: 9
Loss after 219609000 batches: 0.0235
trigger times: 10
Loss after 219740100 batches: 0.0230
trigger times: 11
Loss after 219871200 batches: 0.0226
trigger times: 12
Loss after 220002300 batches: 0.0219
trigger times: 13
Loss after 220133400 batches: 0.0212
trigger times: 14
Loss after 220264500 batches: 0.0207
trigger times: 15
Loss after 220395600 batches: 0.0202
trigger times: 16
Loss after 220526700 batches: 0.0199
trigger times: 17
Loss after 220657800 batches: 0.0196
trigger times: 18
Loss after 220788900 batches: 0.0192
trigger times: 19
Loss after 220920000 batches: 0.0189
trigger times: 20
Early stopping!
Start to test process.
Loss after 221051100 batches: 0.0188
Time to train on one home:  160.06835293769836
trigger times: 0
Loss after 221182200 batches: 0.2461
trigger times: 0
Loss after 221313300 batches: 0.0745
trigger times: 1
Loss after 221444400 batches: 0.0522
trigger times: 2
Loss after 221575500 batches: 0.0446
trigger times: 3
Loss after 221706600 batches: 0.0402
trigger times: 0
Loss after 221837700 batches: 0.0371
trigger times: 1
Loss after 221968800 batches: 0.0348
trigger times: 2
Loss after 222099900 batches: 0.0331
trigger times: 3
Loss after 222231000 batches: 0.0317
trigger times: 0
Loss after 222362100 batches: 0.0304
trigger times: 1
Loss after 222493200 batches: 0.0295
trigger times: 2
Loss after 222624300 batches: 0.0293
trigger times: 3
Loss after 222755400 batches: 0.0282
trigger times: 0
Loss after 222886500 batches: 0.0278
trigger times: 1
Loss after 223017600 batches: 0.0267
trigger times: 2
Loss after 223148700 batches: 0.0263
trigger times: 0
Loss after 223279800 batches: 0.0260
trigger times: 1
Loss after 223410900 batches: 0.0255
trigger times: 2
Loss after 223542000 batches: 0.0254
trigger times: 3
Loss after 223673100 batches: 0.0248
trigger times: 0
Loss after 223804200 batches: 0.0245
trigger times: 1
Loss after 223935300 batches: 0.0241
trigger times: 2
Loss after 224066400 batches: 0.0239
trigger times: 3
Loss after 224197500 batches: 0.0236
trigger times: 4
Loss after 224328600 batches: 0.0235
trigger times: 0
Loss after 224459700 batches: 0.0231
trigger times: 1
Loss after 224590800 batches: 0.0229
trigger times: 2
Loss after 224721900 batches: 0.0228
trigger times: 3
Loss after 224853000 batches: 0.0225
trigger times: 0
Loss after 224984100 batches: 0.0224
trigger times: 1
Loss after 225115200 batches: 0.0229
trigger times: 2
Loss after 225246300 batches: 0.0220
trigger times: 3
Loss after 225377400 batches: 0.0216
trigger times: 4
Loss after 225508500 batches: 0.0215
trigger times: 5
Loss after 225639600 batches: 0.0215
trigger times: 0
Loss after 225770700 batches: 0.0214
trigger times: 1
Loss after 225901800 batches: 0.0213
trigger times: 2
Loss after 226032900 batches: 0.0213
trigger times: 3
Loss after 226164000 batches: 0.0210
trigger times: 4
Loss after 226295100 batches: 0.0209
trigger times: 5
Loss after 226426200 batches: 0.0206
trigger times: 6
Loss after 226557300 batches: 0.0206
trigger times: 0
Loss after 226688400 batches: 0.0206
trigger times: 0
Loss after 226819500 batches: 0.0202
trigger times: 0
Loss after 226950600 batches: 0.0203
trigger times: 1
Loss after 227081700 batches: 0.0202
trigger times: 2
Loss after 227212800 batches: 0.0200
trigger times: 3
Loss after 227343900 batches: 0.0198
trigger times: 4
Loss after 227475000 batches: 0.0199
trigger times: 5
Loss after 227606100 batches: 0.0200
trigger times: 6
Loss after 227737200 batches: 0.0197
trigger times: 7
Loss after 227868300 batches: 0.0196
trigger times: 8
Loss after 227999400 batches: 0.0195
trigger times: 9
Loss after 228130500 batches: 0.0195
trigger times: 10
Loss after 228261600 batches: 0.0192
trigger times: 11
Loss after 228392700 batches: 0.0193
trigger times: 12
Loss after 228523800 batches: 0.0190
trigger times: 13
Loss after 228654900 batches: 0.0190
trigger times: 14
Loss after 228786000 batches: 0.0188
trigger times: 15
Loss after 228917100 batches: 0.0189
trigger times: 16
Loss after 229048200 batches: 0.0187
trigger times: 17
Loss after 229179300 batches: 0.0188
trigger times: 18
Loss after 229310400 batches: 0.0195
trigger times: 19
Loss after 229441500 batches: 0.0189
trigger times: 20
Early stopping!
Start to test process.
Loss after 229572600 batches: 0.0187
Time to train on one home:  471.4016332626343
trigger times: 0
Loss after 229701240 batches: 0.1268
trigger times: 0
Loss after 229829880 batches: 0.0356
trigger times: 0
Loss after 229958520 batches: 0.0258
trigger times: 0
Loss after 230087160 batches: 0.0219
trigger times: 1
Loss after 230215800 batches: 0.0204
trigger times: 0
Loss after 230344440 batches: 0.0193
trigger times: 1
Loss after 230473080 batches: 0.0178
trigger times: 2
Loss after 230601720 batches: 0.0172
trigger times: 3
Loss after 230730360 batches: 0.0166
trigger times: 4
Loss after 230859000 batches: 0.0160
trigger times: 5
Loss after 230987640 batches: 0.0157
trigger times: 6
Loss after 231116280 batches: 0.0154
trigger times: 7
Loss after 231244920 batches: 0.0149
trigger times: 8
Loss after 231373560 batches: 0.0146
trigger times: 9
Loss after 231502200 batches: 0.0143
trigger times: 10
Loss after 231630840 batches: 0.0140
trigger times: 11
Loss after 231759480 batches: 0.0137
trigger times: 12
Loss after 231888120 batches: 0.0140
trigger times: 13
Loss after 232016760 batches: 0.0136
trigger times: 14
Loss after 232145400 batches: 0.0132
trigger times: 15
Loss after 232274040 batches: 0.0131
trigger times: 16
Loss after 232402680 batches: 0.0131
trigger times: 17
Loss after 232531320 batches: 0.0130
trigger times: 18
Loss after 232659960 batches: 0.0129
trigger times: 19
Loss after 232788600 batches: 0.0126
trigger times: 20
Early stopping!
Start to test process.
Loss after 232917240 batches: 0.0125
Time to train on one home:  192.42998433113098
trigger times: 0
Loss after 233048340 batches: 0.2501
trigger times: 0
Loss after 233179440 batches: 0.0688
trigger times: 0
Loss after 233310540 batches: 0.0481
trigger times: 0
Loss after 233441640 batches: 0.0411
trigger times: 1
Loss after 233572740 batches: 0.0374
trigger times: 2
Loss after 233703840 batches: 0.0354
trigger times: 0
Loss after 233834940 batches: 0.0326
trigger times: 1
Loss after 233966040 batches: 0.0310
trigger times: 2
Loss after 234097140 batches: 0.0298
trigger times: 3
Loss after 234228240 batches: 0.0291
trigger times: 0
Loss after 234359340 batches: 0.0280
trigger times: 0
Loss after 234490440 batches: 0.0269
trigger times: 0
Loss after 234621540 batches: 0.0266
trigger times: 0
Loss after 234752640 batches: 0.0256
trigger times: 0
Loss after 234883740 batches: 0.0256
trigger times: 1
Loss after 235014840 batches: 0.0253
trigger times: 2
Loss after 235145940 batches: 0.0250
trigger times: 0
Loss after 235277040 batches: 0.0243
trigger times: 1
Loss after 235408140 batches: 0.0239
trigger times: 2
Loss after 235539240 batches: 0.0239
trigger times: 3
Loss after 235670340 batches: 0.0232
trigger times: 4
Loss after 235801440 batches: 0.0233
trigger times: 0
Loss after 235932540 batches: 0.0228
trigger times: 1
Loss after 236063640 batches: 0.0228
trigger times: 2
Loss after 236194740 batches: 0.0225
trigger times: 3
Loss after 236325840 batches: 0.0222
trigger times: 4
Loss after 236456940 batches: 0.0221
trigger times: 5
Loss after 236588040 batches: 0.0217
trigger times: 6
Loss after 236719140 batches: 0.0218
trigger times: 7
Loss after 236850240 batches: 0.0212
trigger times: 8
Loss after 236981340 batches: 0.0212
trigger times: 9
Loss after 237112440 batches: 0.0209
trigger times: 10
Loss after 237243540 batches: 0.0206
trigger times: 11
Loss after 237374640 batches: 0.0208
trigger times: 0
Loss after 237505740 batches: 0.0206
trigger times: 1
Loss after 237636840 batches: 0.0203
trigger times: 2
Loss after 237767940 batches: 0.0200
trigger times: 3
Loss after 237899040 batches: 0.0202
trigger times: 4
Loss after 238030140 batches: 0.0198
trigger times: 0
Loss after 238161240 batches: 0.0200
trigger times: 1
Loss after 238292340 batches: 0.0198
trigger times: 2
Loss after 238423440 batches: 0.0197
trigger times: 3
Loss after 238554540 batches: 0.0194
trigger times: 4
Loss after 238685640 batches: 0.0195
trigger times: 5
Loss after 238816740 batches: 0.0195
trigger times: 6
Loss after 238947840 batches: 0.0193
trigger times: 7
Loss after 239078940 batches: 0.0191
trigger times: 8
Loss after 239210040 batches: 0.0195
trigger times: 0
Loss after 239341140 batches: 0.0187
trigger times: 1
Loss after 239472240 batches: 0.0189
trigger times: 2
Loss after 239603340 batches: 0.0185
trigger times: 3
Loss after 239734440 batches: 0.0188
trigger times: 4
Loss after 239865540 batches: 0.0184
trigger times: 5
Loss after 239996640 batches: 0.0185
trigger times: 6
Loss after 240127740 batches: 0.0189
trigger times: 7
Loss after 240258840 batches: 0.0186
trigger times: 8
Loss after 240389940 batches: 0.0182
trigger times: 9
Loss after 240521040 batches: 0.0182
trigger times: 10
Loss after 240652140 batches: 0.0179
trigger times: 11
Loss after 240783240 batches: 0.0179
trigger times: 12
Loss after 240914340 batches: 0.0178
trigger times: 13
Loss after 241045440 batches: 0.0177
trigger times: 14
Loss after 241176540 batches: 0.0177
trigger times: 15
Loss after 241307640 batches: 0.0176
trigger times: 16
Loss after 241438740 batches: 0.0175
trigger times: 17
Loss after 241569840 batches: 0.0176
trigger times: 18
Loss after 241700940 batches: 0.0173
trigger times: 19
Loss after 241832040 batches: 0.0173
trigger times: 20
Early stopping!
Start to test process.
Loss after 241963140 batches: 0.0174
Time to train on one home:  497.8877127170563
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317]]
Round_6_results:  [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317]
trigger times: 0
Loss after 242094240 batches: 0.1137
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1899 < 1900; dropping {'Training_Loss': 0.11371506476458514, 'Validation_Loss': 0.24268428319030338, 'Training_R2': 0.8854578484339536, 'Validation_R2': 0.7743513164902999, 'Training_F1': 0.8204439769707705, 'Validation_F1': 0.7492298768518284, 'Training_NEP': 0.3589280074771243, 'Validation_NEP': 0.4866597461248563, 'Training_NDE': 0.08598882630697215, 'Validation_NDE': 0.17969471667770315, 'Training_MAE': 11.886983776645383, 'Validation_MAE': 13.34622926635127, 'Training_MSE': 378.33737, 'Validation_MSE': 663.6076}.
trigger times: 0
Loss after 242225340 batches: 0.0304
trigger times: 0
Loss after 242356440 batches: 0.0225
trigger times: 1
Loss after 242487540 batches: 0.0195
trigger times: 0
Loss after 242618640 batches: 0.0182
trigger times: 0
Loss after 242749740 batches: 0.0168
trigger times: 1
Loss after 242880840 batches: 0.0159
trigger times: 0
Loss after 243011940 batches: 0.0152
trigger times: 1
Loss after 243143040 batches: 0.0147
trigger times: 2
Loss after 243274140 batches: 0.0141
trigger times: 3
Loss after 243405240 batches: 0.0134
trigger times: 4
Loss after 243536340 batches: 0.0133
trigger times: 5
Loss after 243667440 batches: 0.0130
trigger times: 6
Loss after 243798540 batches: 0.0128
trigger times: 7
Loss after 243929640 batches: 0.0126
trigger times: 8
Loss after 244060740 batches: 0.0126
trigger times: 9
Loss after 244191840 batches: 0.0121
trigger times: 10
Loss after 244322940 batches: 0.0119
trigger times: 11
Loss after 244454040 batches: 0.0118
trigger times: 12
Loss after 244585140 batches: 0.0116
trigger times: 13
Loss after 244716240 batches: 0.0112
trigger times: 14
Loss after 244847340 batches: 0.0114
trigger times: 15
Loss after 244978440 batches: 0.0110
trigger times: 16
Loss after 245109540 batches: 0.0111
trigger times: 17
Loss after 245240640 batches: 0.0110
trigger times: 18
Loss after 245371740 batches: 0.0109
trigger times: 19
Loss after 245502840 batches: 0.0107
trigger times: 20
Early stopping!
Start to test process.
Loss after 245633940 batches: 0.0108
Time to train on one home:  209.8174901008606
trigger times: 0
Loss after 245736540 batches: 0.2826
trigger times: 0
Loss after 245839140 batches: 0.0865
trigger times: 1
Loss after 245941740 batches: 0.0640
trigger times: 0
Loss after 246044340 batches: 0.0503
trigger times: 0
Loss after 246146940 batches: 0.0449
trigger times: 1
Loss after 246249540 batches: 0.0408
trigger times: 0
Loss after 246352140 batches: 0.0366
trigger times: 1
Loss after 246454740 batches: 0.0335
trigger times: 2
Loss after 246557340 batches: 0.0324
trigger times: 3
Loss after 246659940 batches: 0.0324
trigger times: 4
Loss after 246762540 batches: 0.0332
trigger times: 5
Loss after 246865140 batches: 0.0316
trigger times: 6
Loss after 246967740 batches: 0.0338
trigger times: 7
Loss after 247070340 batches: 0.0296
trigger times: 8
Loss after 247172940 batches: 0.0276
trigger times: 9
Loss after 247275540 batches: 0.0258
trigger times: 10
Loss after 247378140 batches: 0.0256
trigger times: 11
Loss after 247480740 batches: 0.0264
trigger times: 12
Loss after 247583340 batches: 0.0250
trigger times: 13
Loss after 247685940 batches: 0.0245
trigger times: 14
Loss after 247788540 batches: 0.0259
trigger times: 15
Loss after 247891140 batches: 0.0243
trigger times: 16
Loss after 247993740 batches: 0.0232
trigger times: 17
Loss after 248096340 batches: 0.0238
trigger times: 18
Loss after 248198940 batches: 0.0250
trigger times: 19
Loss after 248301540 batches: 0.0257
trigger times: 20
Early stopping!
Start to test process.
Loss after 248404140 batches: 0.0237
Time to train on one home:  166.41754484176636
trigger times: 0
Loss after 248535240 batches: 0.1505
trigger times: 0
Loss after 248666340 batches: 0.0494
trigger times: 1
Loss after 248797440 batches: 0.0357
trigger times: 2
Loss after 248928540 batches: 0.0317
trigger times: 0
Loss after 249059640 batches: 0.0288
trigger times: 1
Loss after 249190740 batches: 0.0268
trigger times: 2
Loss after 249321840 batches: 0.0254
trigger times: 3
Loss after 249452940 batches: 0.0243
trigger times: 4
Loss after 249584040 batches: 0.0229
trigger times: 5
Loss after 249715140 batches: 0.0225
trigger times: 6
Loss after 249846240 batches: 0.0218
trigger times: 7
Loss after 249977340 batches: 0.0212
trigger times: 8
Loss after 250108440 batches: 0.0210
trigger times: 9
Loss after 250239540 batches: 0.0206
trigger times: 10
Loss after 250370640 batches: 0.0200
trigger times: 11
Loss after 250501740 batches: 0.0194
trigger times: 0
Loss after 250632840 batches: 0.0193
trigger times: 1
Loss after 250763940 batches: 0.0188
trigger times: 2
Loss after 250895040 batches: 0.0185
trigger times: 3
Loss after 251026140 batches: 0.0183
trigger times: 4
Loss after 251157240 batches: 0.0182
trigger times: 5
Loss after 251288340 batches: 0.0179
trigger times: 6
Loss after 251419440 batches: 0.0179
trigger times: 7
Loss after 251550540 batches: 0.0177
trigger times: 8
Loss after 251681640 batches: 0.0173
trigger times: 9
Loss after 251812740 batches: 0.0174
trigger times: 10
Loss after 251943840 batches: 0.0170
trigger times: 11
Loss after 252074940 batches: 0.0168
trigger times: 12
Loss after 252206040 batches: 0.0166
trigger times: 13
Loss after 252337140 batches: 0.0164
trigger times: 14
Loss after 252468240 batches: 0.0163
trigger times: 15
Loss after 252599340 batches: 0.0163
trigger times: 16
Loss after 252730440 batches: 0.0165
trigger times: 17
Loss after 252861540 batches: 0.0162
trigger times: 18
Loss after 252992640 batches: 0.0159
trigger times: 19
Loss after 253123740 batches: 0.0156
trigger times: 20
Early stopping!
Start to test process.
Loss after 253254840 batches: 0.0156
Time to train on one home:  272.6994218826294
trigger times: 0
Loss after 253385940 batches: 0.2095
trigger times: 0
Loss after 253517040 batches: 0.0638
trigger times: 1
Loss after 253648140 batches: 0.0468
trigger times: 0
Loss after 253779240 batches: 0.0389
trigger times: 1
Loss after 253910340 batches: 0.0358
trigger times: 0
Loss after 254041440 batches: 0.0338
trigger times: 0
Loss after 254172540 batches: 0.0314
trigger times: 1
Loss after 254303640 batches: 0.0297
trigger times: 0
Loss after 254434740 batches: 0.0287
trigger times: 1
Loss after 254565840 batches: 0.0280
trigger times: 0
Loss after 254696940 batches: 0.0271
trigger times: 0
Loss after 254828040 batches: 0.0264
trigger times: 1
Loss after 254959140 batches: 0.0262
trigger times: 0
Loss after 255090240 batches: 0.0252
trigger times: 1
Loss after 255221340 batches: 0.0251
trigger times: 2
Loss after 255352440 batches: 0.0249
trigger times: 3
Loss after 255483540 batches: 0.0244
trigger times: 4
Loss after 255614640 batches: 0.0241
trigger times: 5
Loss after 255745740 batches: 0.0235
trigger times: 0
Loss after 255876840 batches: 0.0235
trigger times: 1
Loss after 256007940 batches: 0.0232
trigger times: 2
Loss after 256139040 batches: 0.0228
trigger times: 3
Loss after 256270140 batches: 0.0226
trigger times: 4
Loss after 256401240 batches: 0.0220
trigger times: 5
Loss after 256532340 batches: 0.0219
trigger times: 0
Loss after 256663440 batches: 0.0218
trigger times: 0
Loss after 256794540 batches: 0.0215
trigger times: 0
Loss after 256925640 batches: 0.0215
trigger times: 0
Loss after 257056740 batches: 0.0216
trigger times: 1
Loss after 257187840 batches: 0.0213
trigger times: 2
Loss after 257318940 batches: 0.0210
trigger times: 3
Loss after 257450040 batches: 0.0208
trigger times: 4
Loss after 257581140 batches: 0.0206
trigger times: 5
Loss after 257712240 batches: 0.0204
trigger times: 6
Loss after 257843340 batches: 0.0207
trigger times: 7
Loss after 257974440 batches: 0.0205
trigger times: 8
Loss after 258105540 batches: 0.0200
trigger times: 9
Loss after 258236640 batches: 0.0199
trigger times: 10
Loss after 258367740 batches: 0.0199
trigger times: 11
Loss after 258498840 batches: 0.0199
trigger times: 12
Loss after 258629940 batches: 0.0196
trigger times: 13
Loss after 258761040 batches: 0.0195
trigger times: 14
Loss after 258892140 batches: 0.0194
trigger times: 15
Loss after 259023240 batches: 0.0196
trigger times: 16
Loss after 259154340 batches: 0.0195
trigger times: 17
Loss after 259285440 batches: 0.0193
trigger times: 18
Loss after 259416540 batches: 0.0191
trigger times: 19
Loss after 259547640 batches: 0.0191
trigger times: 20
Early stopping!
Start to test process.
Loss after 259678740 batches: 0.0191
Time to train on one home:  357.7606942653656
trigger times: 0
Loss after 259807380 batches: 0.1106
trigger times: 0
Loss after 259936020 batches: 0.0324
trigger times: 0
Loss after 260064660 batches: 0.0240
trigger times: 0
Loss after 260193300 batches: 0.0210
trigger times: 1
Loss after 260321940 batches: 0.0198
trigger times: 2
Loss after 260450580 batches: 0.0186
trigger times: 0
Loss after 260579220 batches: 0.0176
trigger times: 1
Loss after 260707860 batches: 0.0165
trigger times: 2
Loss after 260836500 batches: 0.0159
trigger times: 3
Loss after 260965140 batches: 0.0159
trigger times: 4
Loss after 261093780 batches: 0.0151
trigger times: 5
Loss after 261222420 batches: 0.0147
trigger times: 6
Loss after 261351060 batches: 0.0145
trigger times: 7
Loss after 261479700 batches: 0.0138
trigger times: 8
Loss after 261608340 batches: 0.0138
trigger times: 9
Loss after 261736980 batches: 0.0138
trigger times: 10
Loss after 261865620 batches: 0.0137
trigger times: 11
Loss after 261994260 batches: 0.0130
trigger times: 12
Loss after 262122900 batches: 0.0129
trigger times: 13
Loss after 262251540 batches: 0.0127
trigger times: 14
Loss after 262380180 batches: 0.0127
trigger times: 15
Loss after 262508820 batches: 0.0126
trigger times: 16
Loss after 262637460 batches: 0.0124
trigger times: 17
Loss after 262766100 batches: 0.0125
trigger times: 18
Loss after 262894740 batches: 0.0122
trigger times: 19
Loss after 263023380 batches: 0.0120
trigger times: 20
Early stopping!
Start to test process.
Loss after 263152020 batches: 0.0120
Time to train on one home:  198.57454013824463
trigger times: 0
Loss after 263283120 batches: 0.2164
trigger times: 0
Loss after 263414220 batches: 0.0576
trigger times: 0
Loss after 263545320 batches: 0.0422
trigger times: 1
Loss after 263676420 batches: 0.0361
trigger times: 2
Loss after 263807520 batches: 0.0326
trigger times: 3
Loss after 263938620 batches: 0.0305
trigger times: 4
Loss after 264069720 batches: 0.0293
trigger times: 0
Loss after 264200820 batches: 0.0281
trigger times: 0
Loss after 264331920 batches: 0.0267
trigger times: 1
Loss after 264463020 batches: 0.0266
trigger times: 0
Loss after 264594120 batches: 0.0256
trigger times: 1
Loss after 264725220 batches: 0.0253
trigger times: 2
Loss after 264856320 batches: 0.0244
trigger times: 3
Loss after 264987420 batches: 0.0240
trigger times: 4
Loss after 265118520 batches: 0.0236
trigger times: 5
Loss after 265249620 batches: 0.0231
trigger times: 6
Loss after 265380720 batches: 0.0230
trigger times: 7
Loss after 265511820 batches: 0.0229
trigger times: 8
Loss after 265642920 batches: 0.0222
trigger times: 9
Loss after 265774020 batches: 0.0219
trigger times: 10
Loss after 265905120 batches: 0.0216
trigger times: 11
Loss after 266036220 batches: 0.0215
trigger times: 12
Loss after 266167320 batches: 0.0212
trigger times: 13
Loss after 266298420 batches: 0.0212
trigger times: 14
Loss after 266429520 batches: 0.0213
trigger times: 0
Loss after 266560620 batches: 0.0210
trigger times: 1
Loss after 266691720 batches: 0.0203
trigger times: 2
Loss after 266822820 batches: 0.0204
trigger times: 3
Loss after 266953920 batches: 0.0199
trigger times: 4
Loss after 267085020 batches: 0.0201
trigger times: 0
Loss after 267216120 batches: 0.0198
trigger times: 1
Loss after 267347220 batches: 0.0197
trigger times: 2
Loss after 267478320 batches: 0.0196
trigger times: 3
Loss after 267609420 batches: 0.0195
trigger times: 4
Loss after 267740520 batches: 0.0194
trigger times: 5
Loss after 267871620 batches: 0.0190
trigger times: 6
Loss after 268002720 batches: 0.0193
trigger times: 7
Loss after 268133820 batches: 0.0189
trigger times: 8
Loss after 268264920 batches: 0.0188
trigger times: 9
Loss after 268396020 batches: 0.0187
trigger times: 0
Loss after 268527120 batches: 0.0187
trigger times: 1
Loss after 268658220 batches: 0.0184
trigger times: 2
Loss after 268789320 batches: 0.0181
trigger times: 3
Loss after 268920420 batches: 0.0181
trigger times: 4
Loss after 269051520 batches: 0.0181
trigger times: 5
Loss after 269182620 batches: 0.0183
trigger times: 6
Loss after 269313720 batches: 0.0180
trigger times: 7
Loss after 269444820 batches: 0.0178
trigger times: 8
Loss after 269575920 batches: 0.0178
trigger times: 9
Loss after 269707020 batches: 0.0177
trigger times: 10
Loss after 269838120 batches: 0.0175
trigger times: 11
Loss after 269969220 batches: 0.0176
trigger times: 12
Loss after 270100320 batches: 0.0175
trigger times: 13
Loss after 270231420 batches: 0.0179
trigger times: 14
Loss after 270362520 batches: 0.0175
trigger times: 0
Loss after 270493620 batches: 0.0174
trigger times: 1
Loss after 270624720 batches: 0.0172
trigger times: 2
Loss after 270755820 batches: 0.0172
trigger times: 3
Loss after 270886920 batches: 0.0170
trigger times: 4
Loss after 271018020 batches: 0.0171
trigger times: 5
Loss after 271149120 batches: 0.0170
trigger times: 6
Loss after 271280220 batches: 0.0170
trigger times: 7
Loss after 271411320 batches: 0.0168
trigger times: 8
Loss after 271542420 batches: 0.0171
trigger times: 9
Loss after 271673520 batches: 0.0167
trigger times: 10
Loss after 271804620 batches: 0.0166
trigger times: 11
Loss after 271935720 batches: 0.0166
trigger times: 12
Loss after 272066820 batches: 0.0166
trigger times: 13
Loss after 272197920 batches: 0.0164
trigger times: 14
Loss after 272329020 batches: 0.0165
trigger times: 15
Loss after 272460120 batches: 0.0163
trigger times: 16
Loss after 272591220 batches: 0.0162
trigger times: 17
Loss after 272722320 batches: 0.0164
trigger times: 18
Loss after 272853420 batches: 0.0163
trigger times: 19
Loss after 272984520 batches: 0.0162
trigger times: 20
Early stopping!
Start to test process.
Loss after 273115620 batches: 0.0161
Time to train on one home:  548.0776870250702
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638]]
Round_7_results:  [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2143 < 2144; dropping {'Training_Loss': 0.11213578487623413, 'Validation_Loss': 0.2415992236799664, 'Training_R2': 0.8870786120239118, 'Validation_R2': 0.7754755198907182, 'Training_F1': 0.8223775209078472, 'Validation_F1': 0.7259133173742612, 'Training_NEP': 0.3554645384494018, 'Validation_NEP': 0.5020097760965077, 'Training_NDE': 0.08477209031139232, 'Validation_NDE': 0.17879946035099115, 'Training_MAE': 11.772280551246975, 'Validation_MAE': 13.76719077154727, 'Training_MSE': 372.9839, 'Validation_MSE': 660.30145}.
trigger times: 0
Loss after 273246720 batches: 0.1121
trigger times: 0
Loss after 273377820 batches: 0.0297
trigger times: 0
Loss after 273508920 batches: 0.0219
trigger times: 0
Loss after 273640020 batches: 0.0187
trigger times: 0
Loss after 273771120 batches: 0.0171
trigger times: 1
Loss after 273902220 batches: 0.0163
trigger times: 2
Loss after 274033320 batches: 0.0153
trigger times: 0
Loss after 274164420 batches: 0.0148
trigger times: 1
Loss after 274295520 batches: 0.0142
trigger times: 2
Loss after 274426620 batches: 0.0140
trigger times: 3
Loss after 274557720 batches: 0.0134
trigger times: 4
Loss after 274688820 batches: 0.0133
trigger times: 0
Loss after 274819920 batches: 0.0130
trigger times: 1
Loss after 274951020 batches: 0.0125
trigger times: 2
Loss after 275082120 batches: 0.0123
trigger times: 3
Loss after 275213220 batches: 0.0121
trigger times: 4
Loss after 275344320 batches: 0.0119
trigger times: 5
Loss after 275475420 batches: 0.0116
trigger times: 6
Loss after 275606520 batches: 0.0118
trigger times: 7
Loss after 275737620 batches: 0.0115
trigger times: 0
Loss after 275868720 batches: 0.0113
trigger times: 1
Loss after 275999820 batches: 0.0111
trigger times: 2
Loss after 276130920 batches: 0.0110
trigger times: 3
Loss after 276262020 batches: 0.0108
trigger times: 4
Loss after 276393120 batches: 0.0106
trigger times: 5
Loss after 276524220 batches: 0.0108
trigger times: 6
Loss after 276655320 batches: 0.0105
trigger times: 7
Loss after 276786420 batches: 0.0104
trigger times: 8
Loss after 276917520 batches: 0.0106
trigger times: 9
Loss after 277048620 batches: 0.0105
trigger times: 10
Loss after 277179720 batches: 0.0104
trigger times: 11
Loss after 277310820 batches: 0.0101
trigger times: 12
Loss after 277441920 batches: 0.0101
trigger times: 13
Loss after 277573020 batches: 0.0098
trigger times: 14
Loss after 277704120 batches: 0.0099
trigger times: 15
Loss after 277835220 batches: 0.0099
trigger times: 16
Loss after 277966320 batches: 0.0097
trigger times: 17
Loss after 278097420 batches: 0.0100
trigger times: 18
Loss after 278228520 batches: 0.0098
trigger times: 19
Loss after 278359620 batches: 0.0094
trigger times: 20
Early stopping!
Start to test process.
Loss after 278490720 batches: 0.0097
Time to train on one home:  301.2321705818176
trigger times: 0
Loss after 278593320 batches: 0.2613
trigger times: 0
Loss after 278695920 batches: 0.0847
trigger times: 1
Loss after 278798520 batches: 0.0576
trigger times: 2
Loss after 278901120 batches: 0.0452
trigger times: 3
Loss after 279003720 batches: 0.0400
trigger times: 4
Loss after 279106320 batches: 0.0402
trigger times: 5
Loss after 279208920 batches: 0.0346
trigger times: 6
Loss after 279311520 batches: 0.0348
trigger times: 7
Loss after 279414120 batches: 0.0353
trigger times: 8
Loss after 279516720 batches: 0.0308
trigger times: 9
Loss after 279619320 batches: 0.0280
trigger times: 10
Loss after 279721920 batches: 0.0267
trigger times: 11
Loss after 279824520 batches: 0.0269
trigger times: 12
Loss after 279927120 batches: 0.0278
trigger times: 13
Loss after 280029720 batches: 0.0272
trigger times: 14
Loss after 280132320 batches: 0.0265
trigger times: 15
Loss after 280234920 batches: 0.0264
trigger times: 16
Loss after 280337520 batches: 0.0303
trigger times: 17
Loss after 280440120 batches: 0.0349
trigger times: 18
Loss after 280542720 batches: 0.0279
trigger times: 19
Loss after 280645320 batches: 0.0255
trigger times: 20
Early stopping!
Start to test process.
Loss after 280747920 batches: 0.0243
Time to train on one home:  137.349102973938
trigger times: 0
Loss after 280879020 batches: 0.1801
trigger times: 0
Loss after 281010120 batches: 0.0535
trigger times: 0
Loss after 281141220 batches: 0.0375
trigger times: 0
Loss after 281272320 batches: 0.0315
trigger times: 0
Loss after 281403420 batches: 0.0285
trigger times: 1
Loss after 281534520 batches: 0.0266
trigger times: 2
Loss after 281665620 batches: 0.0250
trigger times: 3
Loss after 281796720 batches: 0.0239
trigger times: 4
Loss after 281927820 batches: 0.0231
trigger times: 5
Loss after 282058920 batches: 0.0219
trigger times: 6
Loss after 282190020 batches: 0.0214
trigger times: 7
Loss after 282321120 batches: 0.0213
trigger times: 8
Loss after 282452220 batches: 0.0206
trigger times: 9
Loss after 282583320 batches: 0.0199
trigger times: 10
Loss after 282714420 batches: 0.0194
trigger times: 11
Loss after 282845520 batches: 0.0191
trigger times: 12
Loss after 282976620 batches: 0.0187
trigger times: 13
Loss after 283107720 batches: 0.0187
trigger times: 14
Loss after 283238820 batches: 0.0184
trigger times: 15
Loss after 283369920 batches: 0.0179
trigger times: 16
Loss after 283501020 batches: 0.0177
trigger times: 17
Loss after 283632120 batches: 0.0172
trigger times: 18
Loss after 283763220 batches: 0.0173
trigger times: 19
Loss after 283894320 batches: 0.0171
trigger times: 20
Early stopping!
Start to test process.
Loss after 284025420 batches: 0.0170
Time to train on one home:  187.80611491203308
trigger times: 0
Loss after 284156520 batches: 0.1912
trigger times: 0
Loss after 284287620 batches: 0.0559
trigger times: 0
Loss after 284418720 batches: 0.0419
trigger times: 0
Loss after 284549820 batches: 0.0364
trigger times: 0
Loss after 284680920 batches: 0.0330
trigger times: 0
Loss after 284812020 batches: 0.0309
trigger times: 1
Loss after 284943120 batches: 0.0296
trigger times: 0
Loss after 285074220 batches: 0.0285
trigger times: 1
Loss after 285205320 batches: 0.0273
trigger times: 2
Loss after 285336420 batches: 0.0266
trigger times: 0
Loss after 285467520 batches: 0.0258
trigger times: 0
Loss after 285598620 batches: 0.0254
trigger times: 1
Loss after 285729720 batches: 0.0252
trigger times: 2
Loss after 285860820 batches: 0.0243
trigger times: 3
Loss after 285991920 batches: 0.0239
trigger times: 4
Loss after 286123020 batches: 0.0237
trigger times: 5
Loss after 286254120 batches: 0.0234
trigger times: 6
Loss after 286385220 batches: 0.0227
trigger times: 7
Loss after 286516320 batches: 0.0224
trigger times: 8
Loss after 286647420 batches: 0.0221
trigger times: 9
Loss after 286778520 batches: 0.0219
trigger times: 10
Loss after 286909620 batches: 0.0219
trigger times: 0
Loss after 287040720 batches: 0.0215
trigger times: 1
Loss after 287171820 batches: 0.0212
trigger times: 2
Loss after 287302920 batches: 0.0211
trigger times: 3
Loss after 287434020 batches: 0.0209
trigger times: 4
Loss after 287565120 batches: 0.0209
trigger times: 5
Loss after 287696220 batches: 0.0207
trigger times: 6
Loss after 287827320 batches: 0.0204
trigger times: 7
Loss after 287958420 batches: 0.0199
trigger times: 8
Loss after 288089520 batches: 0.0204
trigger times: 9
Loss after 288220620 batches: 0.0202
trigger times: 10
Loss after 288351720 batches: 0.0200
trigger times: 11
Loss after 288482820 batches: 0.0199
trigger times: 12
Loss after 288613920 batches: 0.0201
trigger times: 13
Loss after 288745020 batches: 0.0196
trigger times: 14
Loss after 288876120 batches: 0.0194
trigger times: 15
Loss after 289007220 batches: 0.0197
trigger times: 16
Loss after 289138320 batches: 0.0195
trigger times: 17
Loss after 289269420 batches: 0.0192
trigger times: 18
Loss after 289400520 batches: 0.0192
trigger times: 19
Loss after 289531620 batches: 0.0189
trigger times: 20
Early stopping!
Start to test process.
Loss after 289662720 batches: 0.0187
Time to train on one home:  314.28057169914246
trigger times: 0
Loss after 289791360 batches: 0.1108
trigger times: 0
Loss after 289920000 batches: 0.0308
trigger times: 0
Loss after 290048640 batches: 0.0224
trigger times: 0
Loss after 290177280 batches: 0.0203
trigger times: 1
Loss after 290305920 batches: 0.0191
trigger times: 2
Loss after 290434560 batches: 0.0174
trigger times: 3
Loss after 290563200 batches: 0.0167
trigger times: 0
Loss after 290691840 batches: 0.0159
trigger times: 1
Loss after 290820480 batches: 0.0156
trigger times: 2
Loss after 290949120 batches: 0.0150
trigger times: 3
Loss after 291077760 batches: 0.0146
trigger times: 4
Loss after 291206400 batches: 0.0143
trigger times: 0
Loss after 291335040 batches: 0.0143
trigger times: 0
Loss after 291463680 batches: 0.0137
trigger times: 1
Loss after 291592320 batches: 0.0139
trigger times: 0
Loss after 291720960 batches: 0.0134
trigger times: 1
Loss after 291849600 batches: 0.0131
trigger times: 2
Loss after 291978240 batches: 0.0132
trigger times: 3
Loss after 292106880 batches: 0.0128
trigger times: 4
Loss after 292235520 batches: 0.0129
trigger times: 5
Loss after 292364160 batches: 0.0127
trigger times: 6
Loss after 292492800 batches: 0.0123
trigger times: 7
Loss after 292621440 batches: 0.0122
trigger times: 8
Loss after 292750080 batches: 0.0121
trigger times: 9
Loss after 292878720 batches: 0.0121
trigger times: 10
Loss after 293007360 batches: 0.0121
trigger times: 11
Loss after 293136000 batches: 0.0119
trigger times: 12
Loss after 293264640 batches: 0.0119
trigger times: 13
Loss after 293393280 batches: 0.0119
trigger times: 14
Loss after 293521920 batches: 0.0118
trigger times: 15
Loss after 293650560 batches: 0.0115
trigger times: 16
Loss after 293779200 batches: 0.0116
trigger times: 17
Loss after 293907840 batches: 0.0113
trigger times: 18
Loss after 294036480 batches: 0.0113
trigger times: 19
Loss after 294165120 batches: 0.0112
trigger times: 20
Early stopping!
Start to test process.
Loss after 294293760 batches: 0.0109
Time to train on one home:  260.88032484054565
trigger times: 0
Loss after 294424860 batches: 0.2112
trigger times: 0
Loss after 294555960 batches: 0.0514
trigger times: 0
Loss after 294687060 batches: 0.0376
trigger times: 0
Loss after 294818160 batches: 0.0331
trigger times: 1
Loss after 294949260 batches: 0.0304
trigger times: 2
Loss after 295080360 batches: 0.0285
trigger times: 3
Loss after 295211460 batches: 0.0270
trigger times: 0
Loss after 295342560 batches: 0.0265
trigger times: 1
Loss after 295473660 batches: 0.0255
trigger times: 2
Loss after 295604760 batches: 0.0244
trigger times: 3
Loss after 295735860 batches: 0.0241
trigger times: 4
Loss after 295866960 batches: 0.0234
trigger times: 5
Loss after 295998060 batches: 0.0231
trigger times: 6
Loss after 296129160 batches: 0.0223
trigger times: 0
Loss after 296260260 batches: 0.0223
trigger times: 1
Loss after 296391360 batches: 0.0216
trigger times: 2
Loss after 296522460 batches: 0.0214
trigger times: 3
Loss after 296653560 batches: 0.0215
trigger times: 4
Loss after 296784660 batches: 0.0210
trigger times: 5
Loss after 296915760 batches: 0.0205
trigger times: 0
Loss after 297046860 batches: 0.0205
trigger times: 1
Loss after 297177960 batches: 0.0203
trigger times: 2
Loss after 297309060 batches: 0.0201
trigger times: 3
Loss after 297440160 batches: 0.0200
trigger times: 4
Loss after 297571260 batches: 0.0197
trigger times: 5
Loss after 297702360 batches: 0.0195
trigger times: 6
Loss after 297833460 batches: 0.0192
trigger times: 0
Loss after 297964560 batches: 0.0192
trigger times: 1
Loss after 298095660 batches: 0.0191
trigger times: 2
Loss after 298226760 batches: 0.0188
trigger times: 3
Loss after 298357860 batches: 0.0187
trigger times: 4
Loss after 298488960 batches: 0.0188
trigger times: 5
Loss after 298620060 batches: 0.0185
trigger times: 6
Loss after 298751160 batches: 0.0185
trigger times: 0
Loss after 298882260 batches: 0.0184
trigger times: 1
Loss after 299013360 batches: 0.0184
trigger times: 2
Loss after 299144460 batches: 0.0181
trigger times: 3
Loss after 299275560 batches: 0.0182
trigger times: 4
Loss after 299406660 batches: 0.0181
trigger times: 5
Loss after 299537760 batches: 0.0179
trigger times: 6
Loss after 299668860 batches: 0.0177
trigger times: 7
Loss after 299799960 batches: 0.0175
trigger times: 8
Loss after 299931060 batches: 0.0176
trigger times: 9
Loss after 300062160 batches: 0.0177
trigger times: 10
Loss after 300193260 batches: 0.0172
trigger times: 11
Loss after 300324360 batches: 0.0173
trigger times: 12
Loss after 300455460 batches: 0.0173
trigger times: 13
Loss after 300586560 batches: 0.0171
trigger times: 14
Loss after 300717660 batches: 0.0170
trigger times: 15
Loss after 300848760 batches: 0.0170
trigger times: 16
Loss after 300979860 batches: 0.0169
trigger times: 17
Loss after 301110960 batches: 0.0169
trigger times: 18
Loss after 301242060 batches: 0.0166
trigger times: 19
Loss after 301373160 batches: 0.0166
trigger times: 20
Early stopping!
Start to test process.
Loss after 301504260 batches: 0.0165
Time to train on one home:  399.51602506637573
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146, 0.01617968764306582]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638], [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354]]
Round_8_results:  [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354]
trigger times: 0
Loss after 301635360 batches: 0.1073
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2365 < 2366; dropping {'Training_Loss': 0.10729362660983824, 'Validation_Loss': 0.21364569581217235, 'Training_R2': 0.8919617837759316, 'Validation_R2': 0.8013758177027277, 'Training_F1': 0.8257853631488712, 'Validation_F1': 0.7602944659530502, 'Training_NEP': 0.34816572191480044, 'Validation_NEP': 0.47147973129771203, 'Training_NDE': 0.08110620660071785, 'Validation_NDE': 0.15817382848464312, 'Training_MAE': 11.530558222735046, 'Validation_MAE': 12.92993028176728, 'Training_MSE': 356.85458, 'Validation_MSE': 584.1316}.
trigger times: 0
Loss after 301766460 batches: 0.0276
trigger times: 0
Loss after 301897560 batches: 0.0202
trigger times: 0
Loss after 302028660 batches: 0.0176
trigger times: 1
Loss after 302159760 batches: 0.0167
trigger times: 0
Loss after 302290860 batches: 0.0155
trigger times: 1
Loss after 302421960 batches: 0.0149
trigger times: 2
Loss after 302553060 batches: 0.0142
trigger times: 3
Loss after 302684160 batches: 0.0135
trigger times: 4
Loss after 302815260 batches: 0.0133
trigger times: 5
Loss after 302946360 batches: 0.0129
trigger times: 6
Loss after 303077460 batches: 0.0125
trigger times: 7
Loss after 303208560 batches: 0.0124
trigger times: 8
Loss after 303339660 batches: 0.0121
trigger times: 9
Loss after 303470760 batches: 0.0117
trigger times: 10
Loss after 303601860 batches: 0.0115
trigger times: 11
Loss after 303732960 batches: 0.0112
trigger times: 12
Loss after 303864060 batches: 0.0114
trigger times: 13
Loss after 303995160 batches: 0.0110
trigger times: 14
Loss after 304126260 batches: 0.0110
trigger times: 15
Loss after 304257360 batches: 0.0110
trigger times: 16
Loss after 304388460 batches: 0.0108
trigger times: 17
Loss after 304519560 batches: 0.0106
trigger times: 18
Loss after 304650660 batches: 0.0104
trigger times: 19
Loss after 304781760 batches: 0.0103
trigger times: 20
Early stopping!
Start to test process.
Loss after 304912860 batches: 0.0102
Time to train on one home:  195.11335229873657
trigger times: 0
Loss after 305015460 batches: 0.2513
trigger times: 0
Loss after 305118060 batches: 0.0744
trigger times: 0
Loss after 305220660 batches: 0.0587
trigger times: 1
Loss after 305323260 batches: 0.0452
trigger times: 2
Loss after 305425860 batches: 0.0401
trigger times: 3
Loss after 305528460 batches: 0.0353
trigger times: 4
Loss after 305631060 batches: 0.0330
trigger times: 5
Loss after 305733660 batches: 0.0321
trigger times: 0
Loss after 305836260 batches: 0.0306
trigger times: 1
Loss after 305938860 batches: 0.0321
trigger times: 2
Loss after 306041460 batches: 0.0292
trigger times: 3
Loss after 306144060 batches: 0.0276
trigger times: 4
Loss after 306246660 batches: 0.0264
trigger times: 5
Loss after 306349260 batches: 0.0281
trigger times: 6
Loss after 306451860 batches: 0.0256
trigger times: 7
Loss after 306554460 batches: 0.0249
trigger times: 8
Loss after 306657060 batches: 0.0243
trigger times: 9
Loss after 306759660 batches: 0.0248
trigger times: 10
Loss after 306862260 batches: 0.0260
trigger times: 11
Loss after 306964860 batches: 0.0240
trigger times: 12
Loss after 307067460 batches: 0.0255
trigger times: 13
Loss after 307170060 batches: 0.0254
trigger times: 14
Loss after 307272660 batches: 0.0228
trigger times: 15
Loss after 307375260 batches: 0.0219
trigger times: 16
Loss after 307477860 batches: 0.0225
trigger times: 17
Loss after 307580460 batches: 0.0215
trigger times: 18
Loss after 307683060 batches: 0.0209
trigger times: 19
Loss after 307785660 batches: 0.0210
trigger times: 20
Early stopping!
Start to test process.
Loss after 307888260 batches: 0.0216
Time to train on one home:  178.20589303970337
trigger times: 0
Loss after 308019360 batches: 0.1159
trigger times: 1
Loss after 308150460 batches: 0.0405
trigger times: 2
Loss after 308281560 batches: 0.0316
trigger times: 3
Loss after 308412660 batches: 0.0276
trigger times: 4
Loss after 308543760 batches: 0.0254
trigger times: 5
Loss after 308674860 batches: 0.0238
trigger times: 6
Loss after 308805960 batches: 0.0224
trigger times: 7
Loss after 308937060 batches: 0.0218
trigger times: 8
Loss after 309068160 batches: 0.0210
trigger times: 9
Loss after 309199260 batches: 0.0205
trigger times: 10
Loss after 309330360 batches: 0.0197
trigger times: 11
Loss after 309461460 batches: 0.0198
trigger times: 12
Loss after 309592560 batches: 0.0191
trigger times: 13
Loss after 309723660 batches: 0.0185
trigger times: 14
Loss after 309854760 batches: 0.0182
trigger times: 15
Loss after 309985860 batches: 0.0181
trigger times: 16
Loss after 310116960 batches: 0.0177
trigger times: 17
Loss after 310248060 batches: 0.0174
trigger times: 18
Loss after 310379160 batches: 0.0173
trigger times: 19
Loss after 310510260 batches: 0.0171
trigger times: 20
Early stopping!
Start to test process.
Loss after 310641360 batches: 0.0166
Time to train on one home:  159.63271069526672
trigger times: 0
Loss after 310772460 batches: 0.1762
trigger times: 0
Loss after 310903560 batches: 0.0518
trigger times: 0
Loss after 311034660 batches: 0.0394
trigger times: 0
Loss after 311165760 batches: 0.0339
trigger times: 1
Loss after 311296860 batches: 0.0318
trigger times: 2
Loss after 311427960 batches: 0.0299
trigger times: 3
Loss after 311559060 batches: 0.0291
trigger times: 0
Loss after 311690160 batches: 0.0277
trigger times: 1
Loss after 311821260 batches: 0.0264
trigger times: 0
Loss after 311952360 batches: 0.0259
trigger times: 1
Loss after 312083460 batches: 0.0248
trigger times: 2
Loss after 312214560 batches: 0.0245
trigger times: 3
Loss after 312345660 batches: 0.0243
trigger times: 4
Loss after 312476760 batches: 0.0237
trigger times: 5
Loss after 312607860 batches: 0.0232
trigger times: 6
Loss after 312738960 batches: 0.0225
trigger times: 7
Loss after 312870060 batches: 0.0226
trigger times: 8
Loss after 313001160 batches: 0.0221
trigger times: 9
Loss after 313132260 batches: 0.0222
trigger times: 10
Loss after 313263360 batches: 0.0220
trigger times: 11
Loss after 313394460 batches: 0.0214
trigger times: 12
Loss after 313525560 batches: 0.0212
trigger times: 13
Loss after 313656660 batches: 0.0209
trigger times: 0
Loss after 313787760 batches: 0.0208
trigger times: 1
Loss after 313918860 batches: 0.0206
trigger times: 2
Loss after 314049960 batches: 0.0206
trigger times: 3
Loss after 314181060 batches: 0.0204
trigger times: 4
Loss after 314312160 batches: 0.0206
trigger times: 5
Loss after 314443260 batches: 0.0200
trigger times: 6
Loss after 314574360 batches: 0.0198
trigger times: 7
Loss after 314705460 batches: 0.0196
trigger times: 8
Loss after 314836560 batches: 0.0197
trigger times: 9
Loss after 314967660 batches: 0.0195
trigger times: 10
Loss after 315098760 batches: 0.0193
trigger times: 11
Loss after 315229860 batches: 0.0195
trigger times: 12
Loss after 315360960 batches: 0.0192
trigger times: 13
Loss after 315492060 batches: 0.0189
trigger times: 14
Loss after 315623160 batches: 0.0191
trigger times: 15
Loss after 315754260 batches: 0.0188
trigger times: 16
Loss after 315885360 batches: 0.0187
trigger times: 0
Loss after 316016460 batches: 0.0189
trigger times: 1
Loss after 316147560 batches: 0.0184
trigger times: 2
Loss after 316278660 batches: 0.0183
trigger times: 3
Loss after 316409760 batches: 0.0184
trigger times: 4
Loss after 316540860 batches: 0.0183
trigger times: 5
Loss after 316671960 batches: 0.0180
trigger times: 0
Loss after 316803060 batches: 0.0184
trigger times: 0
Loss after 316934160 batches: 0.0182
trigger times: 1
Loss after 317065260 batches: 0.0177
trigger times: 2
Loss after 317196360 batches: 0.0179
trigger times: 3
Loss after 317327460 batches: 0.0178
trigger times: 4
Loss after 317458560 batches: 0.0180
trigger times: 5
Loss after 317589660 batches: 0.0179
trigger times: 6
Loss after 317720760 batches: 0.0176
trigger times: 0
Loss after 317851860 batches: 0.0173
trigger times: 1
Loss after 317982960 batches: 0.0174
trigger times: 2
Loss after 318114060 batches: 0.0175
trigger times: 3
Loss after 318245160 batches: 0.0173
trigger times: 4
Loss after 318376260 batches: 0.0169
trigger times: 5
Loss after 318507360 batches: 0.0172
trigger times: 6
Loss after 318638460 batches: 0.0171
trigger times: 7
Loss after 318769560 batches: 0.0172
trigger times: 8
Loss after 318900660 batches: 0.0170
trigger times: 9
Loss after 319031760 batches: 0.0168
trigger times: 10
Loss after 319162860 batches: 0.0168
trigger times: 11
Loss after 319293960 batches: 0.0171
trigger times: 12
Loss after 319425060 batches: 0.0169
trigger times: 13
Loss after 319556160 batches: 0.0167
trigger times: 14
Loss after 319687260 batches: 0.0166
trigger times: 15
Loss after 319818360 batches: 0.0166
trigger times: 16
Loss after 319949460 batches: 0.0164
trigger times: 17
Loss after 320080560 batches: 0.0163
trigger times: 18
Loss after 320211660 batches: 0.0163
trigger times: 0
Loss after 320342760 batches: 0.0163
trigger times: 0
Loss after 320473860 batches: 0.0162
trigger times: 1
Loss after 320604960 batches: 0.0163
trigger times: 2
Loss after 320736060 batches: 0.0160
trigger times: 3
Loss after 320867160 batches: 0.0162
trigger times: 4
Loss after 320998260 batches: 0.0159
trigger times: 5
Loss after 321129360 batches: 0.0162
trigger times: 6
Loss after 321260460 batches: 0.0160
trigger times: 7
Loss after 321391560 batches: 0.0161
trigger times: 8
Loss after 321522660 batches: 0.0159
trigger times: 9
Loss after 321653760 batches: 0.0157
trigger times: 10
Loss after 321784860 batches: 0.0158
trigger times: 11
Loss after 321915960 batches: 0.0156
trigger times: 12
Loss after 322047060 batches: 0.0161
trigger times: 13
Loss after 322178160 batches: 0.0158
trigger times: 14
Loss after 322309260 batches: 0.0158
trigger times: 15
Loss after 322440360 batches: 0.0156
trigger times: 0
Loss after 322571460 batches: 0.0158
trigger times: 1
Loss after 322702560 batches: 0.0157
trigger times: 2
Loss after 322833660 batches: 0.0154
trigger times: 3
Loss after 322964760 batches: 0.0155
trigger times: 4
Loss after 323095860 batches: 0.0154
trigger times: 5
Loss after 323226960 batches: 0.0155
trigger times: 6
Loss after 323358060 batches: 0.0153
trigger times: 7
Loss after 323489160 batches: 0.0155
trigger times: 8
Loss after 323620260 batches: 0.0154
trigger times: 9
Loss after 323751360 batches: 0.0156
trigger times: 10
Loss after 323882460 batches: 0.0155
trigger times: 11
Loss after 324013560 batches: 0.0157
trigger times: 12
Loss after 324144660 batches: 0.0151
trigger times: 13
Loss after 324275760 batches: 0.0152
trigger times: 14
Loss after 324406860 batches: 0.0151
trigger times: 15
Loss after 324537960 batches: 0.0152
trigger times: 16
Loss after 324669060 batches: 0.0155
trigger times: 17
Loss after 324800160 batches: 0.0154
trigger times: 18
Loss after 324931260 batches: 0.0151
trigger times: 19
Loss after 325062360 batches: 0.0149
trigger times: 20
Early stopping!
Start to test process.
Loss after 325193460 batches: 0.0150
Time to train on one home:  793.8168106079102
trigger times: 0
Loss after 325322100 batches: 0.0905
trigger times: 0
Loss after 325450740 batches: 0.0283
trigger times: 0
Loss after 325579380 batches: 0.0214
trigger times: 0
Loss after 325708020 batches: 0.0189
trigger times: 1
Loss after 325836660 batches: 0.0177
trigger times: 2
Loss after 325965300 batches: 0.0167
trigger times: 3
Loss after 326093940 batches: 0.0161
trigger times: 4
Loss after 326222580 batches: 0.0153
trigger times: 5
Loss after 326351220 batches: 0.0147
trigger times: 6
Loss after 326479860 batches: 0.0145
trigger times: 7
Loss after 326608500 batches: 0.0144
trigger times: 8
Loss after 326737140 batches: 0.0138
trigger times: 9
Loss after 326865780 batches: 0.0137
trigger times: 10
Loss after 326994420 batches: 0.0132
trigger times: 11
Loss after 327123060 batches: 0.0132
trigger times: 12
Loss after 327251700 batches: 0.0128
trigger times: 13
Loss after 327380340 batches: 0.0128
trigger times: 14
Loss after 327508980 batches: 0.0124
trigger times: 15
Loss after 327637620 batches: 0.0125
trigger times: 16
Loss after 327766260 batches: 0.0125
trigger times: 17
Loss after 327894900 batches: 0.0123
trigger times: 18
Loss after 328023540 batches: 0.0120
trigger times: 19
Loss after 328152180 batches: 0.0121
trigger times: 20
Early stopping!
Start to test process.
Loss after 328280820 batches: 0.0121
Time to train on one home:  177.70986151695251
trigger times: 0
Loss after 328411920 batches: 0.1803
trigger times: 0
Loss after 328543020 batches: 0.0465
trigger times: 1
Loss after 328674120 batches: 0.0347
trigger times: 2
Loss after 328805220 batches: 0.0308
trigger times: 3
Loss after 328936320 batches: 0.0287
trigger times: 0
Loss after 329067420 batches: 0.0268
trigger times: 0
Loss after 329198520 batches: 0.0257
trigger times: 1
Loss after 329329620 batches: 0.0246
trigger times: 2
Loss after 329460720 batches: 0.0238
trigger times: 3
Loss after 329591820 batches: 0.0230
trigger times: 0
Loss after 329722920 batches: 0.0227
trigger times: 1
Loss after 329854020 batches: 0.0223
trigger times: 2
Loss after 329985120 batches: 0.0218
trigger times: 3
Loss after 330116220 batches: 0.0213
trigger times: 4
Loss after 330247320 batches: 0.0210
trigger times: 5
Loss after 330378420 batches: 0.0209
trigger times: 0
Loss after 330509520 batches: 0.0207
trigger times: 1
Loss after 330640620 batches: 0.0203
trigger times: 2
Loss after 330771720 batches: 0.0200
trigger times: 3
Loss after 330902820 batches: 0.0198
trigger times: 4
Loss after 331033920 batches: 0.0199
trigger times: 5
Loss after 331165020 batches: 0.0196
trigger times: 6
Loss after 331296120 batches: 0.0192
trigger times: 7
Loss after 331427220 batches: 0.0194
trigger times: 8
Loss after 331558320 batches: 0.0191
trigger times: 9
Loss after 331689420 batches: 0.0184
trigger times: 10
Loss after 331820520 batches: 0.0188
trigger times: 11
Loss after 331951620 batches: 0.0189
trigger times: 12
Loss after 332082720 batches: 0.0188
trigger times: 13
Loss after 332213820 batches: 0.0182
trigger times: 14
Loss after 332344920 batches: 0.0182
trigger times: 15
Loss after 332476020 batches: 0.0180
trigger times: 0
Loss after 332607120 batches: 0.0179
trigger times: 1
Loss after 332738220 batches: 0.0179
trigger times: 2
Loss after 332869320 batches: 0.0177
trigger times: 3
Loss after 333000420 batches: 0.0175
trigger times: 4
Loss after 333131520 batches: 0.0178
trigger times: 5
Loss after 333262620 batches: 0.0176
trigger times: 6
Loss after 333393720 batches: 0.0175
trigger times: 7
Loss after 333524820 batches: 0.0171
trigger times: 8
Loss after 333655920 batches: 0.0173
trigger times: 9
Loss after 333787020 batches: 0.0172
trigger times: 10
Loss after 333918120 batches: 0.0169
trigger times: 11
Loss after 334049220 batches: 0.0172
trigger times: 12
Loss after 334180320 batches: 0.0170
trigger times: 13
Loss after 334311420 batches: 0.0169
trigger times: 14
Loss after 334442520 batches: 0.0168
trigger times: 15
Loss after 334573620 batches: 0.0165
trigger times: 16
Loss after 334704720 batches: 0.0167
trigger times: 17
Loss after 334835820 batches: 0.0165
trigger times: 18
Loss after 334966920 batches: 0.0165
trigger times: 19
Loss after 335098020 batches: 0.0165
trigger times: 20
Early stopping!
Start to test process.
Loss after 335229120 batches: 0.0163
Time to train on one home:  385.01373052597046
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146, 0.01617968764306582, 0.015298912286208879]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638], [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354], [0.5489356054200066, 0.40660235248365517, 0.4824471426162714, 1.021275384526291, 0.4861081410983952, 24.12799700883127, 1500.6461]]
Round_9_results:  [0.5489356054200066, 0.40660235248365517, 0.4824471426162714, 1.021275384526291, 0.4861081410983952, 24.12799700883127, 1500.6461]
trigger times: 0
Loss after 335360220 batches: 0.1065
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2629 < 2630; dropping {'Training_Loss': 0.10649743398546048, 'Validation_Loss': 0.22616652482085758, 'Training_R2': 0.8926939387162922, 'Validation_R2': 0.7898404098163039, 'Training_F1': 0.8256962232557495, 'Validation_F1': 0.7441005393399288, 'Training_NEP': 0.3486154385930195, 'Validation_NEP': 0.49731594974403187, 'Training_NDE': 0.08055656489121882, 'Validation_NDE': 0.16736001924663585, 'Training_MAE': 11.545451947233316, 'Validation_MAE': 13.638466579469728, 'Training_MSE': 354.43625, 'Validation_MSE': 618.0559}.
trigger times: 0
Loss after 335491320 batches: 0.0275
trigger times: 0
Loss after 335622420 batches: 0.0203
trigger times: 1
Loss after 335753520 batches: 0.0177
trigger times: 2
Loss after 335884620 batches: 0.0161
trigger times: 0
Loss after 336015720 batches: 0.0155
trigger times: 0
Loss after 336146820 batches: 0.0143
trigger times: 1
Loss after 336277920 batches: 0.0136
trigger times: 2
Loss after 336409020 batches: 0.0134
trigger times: 3
Loss after 336540120 batches: 0.0130
trigger times: 4
Loss after 336671220 batches: 0.0128
trigger times: 5
Loss after 336802320 batches: 0.0125
trigger times: 0
Loss after 336933420 batches: 0.0120
trigger times: 0
Loss after 337064520 batches: 0.0117
trigger times: 1
Loss after 337195620 batches: 0.0116
trigger times: 2
Loss after 337326720 batches: 0.0114
trigger times: 3
Loss after 337457820 batches: 0.0112
trigger times: 4
Loss after 337588920 batches: 0.0111
trigger times: 5
Loss after 337720020 batches: 0.0109
trigger times: 6
Loss after 337851120 batches: 0.0110
trigger times: 7
Loss after 337982220 batches: 0.0107
trigger times: 8
Loss after 338113320 batches: 0.0107
trigger times: 9
Loss after 338244420 batches: 0.0105
trigger times: 10
Loss after 338375520 batches: 0.0103
trigger times: 11
Loss after 338506620 batches: 0.0103
trigger times: 12
Loss after 338637720 batches: 0.0103
trigger times: 13
Loss after 338768820 batches: 0.0102
trigger times: 14
Loss after 338899920 batches: 0.0101
trigger times: 0
Loss after 339031020 batches: 0.0099
trigger times: 1
Loss after 339162120 batches: 0.0099
trigger times: 2
Loss after 339293220 batches: 0.0097
trigger times: 3
Loss after 339424320 batches: 0.0096
trigger times: 4
Loss after 339555420 batches: 0.0095
trigger times: 5
Loss after 339686520 batches: 0.0096
trigger times: 6
Loss after 339817620 batches: 0.0096
trigger times: 7
Loss after 339948720 batches: 0.0094
trigger times: 8
Loss after 340079820 batches: 0.0092
trigger times: 9
Loss after 340210920 batches: 0.0092
trigger times: 10
Loss after 340342020 batches: 0.0091
trigger times: 11
Loss after 340473120 batches: 0.0091
trigger times: 12
Loss after 340604220 batches: 0.0089
trigger times: 13
Loss after 340735320 batches: 0.0090
trigger times: 14
Loss after 340866420 batches: 0.0089
trigger times: 15
Loss after 340997520 batches: 0.0091
trigger times: 16
Loss after 341128620 batches: 0.0089
trigger times: 17
Loss after 341259720 batches: 0.0086
trigger times: 18
Loss after 341390820 batches: 0.0087
trigger times: 19
Loss after 341521920 batches: 0.0088
trigger times: 20
Early stopping!
Start to test process.
Loss after 341653020 batches: 0.0089
Time to train on one home:  357.7510952949524
trigger times: 0
Loss after 341755620 batches: 0.2301
trigger times: 0
Loss after 341858220 batches: 0.0713
trigger times: 0
Loss after 341960820 batches: 0.0499
trigger times: 1
Loss after 342063420 batches: 0.0416
trigger times: 2
Loss after 342166020 batches: 0.0370
trigger times: 3
Loss after 342268620 batches: 0.0334
trigger times: 4
Loss after 342371220 batches: 0.0327
trigger times: 5
Loss after 342473820 batches: 0.0316
trigger times: 6
Loss after 342576420 batches: 0.0296
trigger times: 7
Loss after 342679020 batches: 0.0292
trigger times: 8
Loss after 342781620 batches: 0.0293
trigger times: 9
Loss after 342884220 batches: 0.0270
trigger times: 10
Loss after 342986820 batches: 0.0262
trigger times: 11
Loss after 343089420 batches: 0.0263
trigger times: 12
Loss after 343192020 batches: 0.0276
trigger times: 13
Loss after 343294620 batches: 0.0244
trigger times: 14
Loss after 343397220 batches: 0.0247
trigger times: 15
Loss after 343499820 batches: 0.0241
trigger times: 16
Loss after 343602420 batches: 0.0245
trigger times: 17
Loss after 343705020 batches: 0.0239
trigger times: 18
Loss after 343807620 batches: 0.0242
trigger times: 19
Loss after 343910220 batches: 0.0246
trigger times: 20
Early stopping!
Start to test process.
Loss after 344012820 batches: 0.0228
Time to train on one home:  143.48866510391235
trigger times: 0
Loss after 344143920 batches: 0.1086
trigger times: 0
Loss after 344275020 batches: 0.0390
trigger times: 0
Loss after 344406120 batches: 0.0306
trigger times: 1
Loss after 344537220 batches: 0.0268
trigger times: 2
Loss after 344668320 batches: 0.0250
trigger times: 3
Loss after 344799420 batches: 0.0234
trigger times: 4
Loss after 344930520 batches: 0.0226
trigger times: 5
Loss after 345061620 batches: 0.0215
trigger times: 6
Loss after 345192720 batches: 0.0210
trigger times: 7
Loss after 345323820 batches: 0.0201
trigger times: 8
Loss after 345454920 batches: 0.0199
trigger times: 9
Loss after 345586020 batches: 0.0192
trigger times: 10
Loss after 345717120 batches: 0.0191
trigger times: 11
Loss after 345848220 batches: 0.0186
trigger times: 12
Loss after 345979320 batches: 0.0182
trigger times: 13
Loss after 346110420 batches: 0.0177
trigger times: 14
Loss after 346241520 batches: 0.0176
trigger times: 15
Loss after 346372620 batches: 0.0172
trigger times: 16
Loss after 346503720 batches: 0.0172
trigger times: 17
Loss after 346634820 batches: 0.0166
trigger times: 18
Loss after 346765920 batches: 0.0164
trigger times: 19
Loss after 346897020 batches: 0.0162
trigger times: 20
Early stopping!
Start to test process.
Loss after 347028120 batches: 0.0161
Time to train on one home:  173.77641081809998
trigger times: 0
Loss after 347159220 batches: 0.1419
trigger times: 0
Loss after 347290320 batches: 0.0488
trigger times: 0
Loss after 347421420 batches: 0.0359
trigger times: 0
Loss after 347552520 batches: 0.0311
trigger times: 1
Loss after 347683620 batches: 0.0289
trigger times: 0
Loss after 347814720 batches: 0.0267
trigger times: 1
Loss after 347945820 batches: 0.0255
trigger times: 2
Loss after 348076920 batches: 0.0248
trigger times: 3
Loss after 348208020 batches: 0.0239
trigger times: 4
Loss after 348339120 batches: 0.0231
trigger times: 0
Loss after 348470220 batches: 0.0226
trigger times: 1
Loss after 348601320 batches: 0.0222
trigger times: 0
Loss after 348732420 batches: 0.0219
trigger times: 1
Loss after 348863520 batches: 0.0214
trigger times: 0
Loss after 348994620 batches: 0.0212
trigger times: 0
Loss after 349125720 batches: 0.0208
trigger times: 1
Loss after 349256820 batches: 0.0210
trigger times: 2
Loss after 349387920 batches: 0.0199
trigger times: 3
Loss after 349519020 batches: 0.0201
trigger times: 4
Loss after 349650120 batches: 0.0200
trigger times: 5
Loss after 349781220 batches: 0.0200
trigger times: 6
Loss after 349912320 batches: 0.0196
trigger times: 0
Loss after 350043420 batches: 0.0193
trigger times: 1
Loss after 350174520 batches: 0.0190
trigger times: 2
Loss after 350305620 batches: 0.0187
trigger times: 0
Loss after 350436720 batches: 0.0189
trigger times: 1
Loss after 350567820 batches: 0.0191
trigger times: 0
Loss after 350698920 batches: 0.0190
trigger times: 1
Loss after 350830020 batches: 0.0183
trigger times: 2
Loss after 350961120 batches: 0.0185
trigger times: 3
Loss after 351092220 batches: 0.0181
trigger times: 0
Loss after 351223320 batches: 0.0181
trigger times: 1
Loss after 351354420 batches: 0.0182
trigger times: 0
Loss after 351485520 batches: 0.0183
trigger times: 1
Loss after 351616620 batches: 0.0180
trigger times: 2
Loss after 351747720 batches: 0.0180
trigger times: 3
Loss after 351878820 batches: 0.0177
trigger times: 4
Loss after 352009920 batches: 0.0178
trigger times: 5
Loss after 352141020 batches: 0.0175
trigger times: 6
Loss after 352272120 batches: 0.0173
trigger times: 7
Loss after 352403220 batches: 0.0174
trigger times: 8
Loss after 352534320 batches: 0.0172
trigger times: 9
Loss after 352665420 batches: 0.0172
trigger times: 10
Loss after 352796520 batches: 0.0170
trigger times: 11
Loss after 352927620 batches: 0.0168
trigger times: 12
Loss after 353058720 batches: 0.0169
trigger times: 13
Loss after 353189820 batches: 0.0169
trigger times: 14
Loss after 353320920 batches: 0.0169
trigger times: 0
Loss after 353452020 batches: 0.0169
trigger times: 1
Loss after 353583120 batches: 0.0172
trigger times: 2
Loss after 353714220 batches: 0.0164
trigger times: 3
Loss after 353845320 batches: 0.0163
trigger times: 4
Loss after 353976420 batches: 0.0165
trigger times: 5
Loss after 354107520 batches: 0.0163
trigger times: 6
Loss after 354238620 batches: 0.0161
trigger times: 0
Loss after 354369720 batches: 0.0164
trigger times: 1
Loss after 354500820 batches: 0.0166
trigger times: 2
Loss after 354631920 batches: 0.0163
trigger times: 3
Loss after 354763020 batches: 0.0160
trigger times: 4
Loss after 354894120 batches: 0.0160
trigger times: 5
Loss after 355025220 batches: 0.0163
trigger times: 6
Loss after 355156320 batches: 0.0158
trigger times: 7
Loss after 355287420 batches: 0.0158
trigger times: 8
Loss after 355418520 batches: 0.0161
trigger times: 9
Loss after 355549620 batches: 0.0156
trigger times: 10
Loss after 355680720 batches: 0.0157
trigger times: 11
Loss after 355811820 batches: 0.0158
trigger times: 12
Loss after 355942920 batches: 0.0156
trigger times: 13
Loss after 356074020 batches: 0.0156
trigger times: 14
Loss after 356205120 batches: 0.0155
trigger times: 15
Loss after 356336220 batches: 0.0155
trigger times: 16
Loss after 356467320 batches: 0.0155
trigger times: 17
Loss after 356598420 batches: 0.0153
trigger times: 18
Loss after 356729520 batches: 0.0152
trigger times: 19
Loss after 356860620 batches: 0.0154
trigger times: 20
Early stopping!
Start to test process.
Loss after 356991720 batches: 0.0151
Time to train on one home:  549.2123782634735
trigger times: 0
Loss after 357120360 batches: 0.1120
trigger times: 0
Loss after 357249000 batches: 0.0301
trigger times: 0
Loss after 357377640 batches: 0.0223
trigger times: 1
Loss after 357506280 batches: 0.0198
trigger times: 2
Loss after 357634920 batches: 0.0183
trigger times: 0
Loss after 357763560 batches: 0.0171
trigger times: 0
Loss after 357892200 batches: 0.0164
trigger times: 1
Loss after 358020840 batches: 0.0158
trigger times: 2
Loss after 358149480 batches: 0.0150
trigger times: 3
Loss after 358278120 batches: 0.0150
trigger times: 4
Loss after 358406760 batches: 0.0145
trigger times: 5
Loss after 358535400 batches: 0.0145
trigger times: 6
Loss after 358664040 batches: 0.0134
trigger times: 7
Loss after 358792680 batches: 0.0135
trigger times: 8
Loss after 358921320 batches: 0.0133
trigger times: 9
Loss after 359049960 batches: 0.0129
trigger times: 10
Loss after 359178600 batches: 0.0127
trigger times: 11
Loss after 359307240 batches: 0.0124
trigger times: 12
Loss after 359435880 batches: 0.0126
trigger times: 13
Loss after 359564520 batches: 0.0125
trigger times: 14
Loss after 359693160 batches: 0.0124
trigger times: 15
Loss after 359821800 batches: 0.0121
trigger times: 16
Loss after 359950440 batches: 0.0120
trigger times: 17
Loss after 360079080 batches: 0.0118
trigger times: 18
Loss after 360207720 batches: 0.0119
trigger times: 19
Loss after 360336360 batches: 0.0117
trigger times: 20
Early stopping!
Start to test process.
Loss after 360465000 batches: 0.0120
Time to train on one home:  199.54776859283447
trigger times: 0
Loss after 360596100 batches: 0.1558
trigger times: 0
Loss after 360727200 batches: 0.0421
trigger times: 0
Loss after 360858300 batches: 0.0326
trigger times: 0
Loss after 360989400 batches: 0.0288
trigger times: 1
Loss after 361120500 batches: 0.0267
trigger times: 2
Loss after 361251600 batches: 0.0256
trigger times: 3
Loss after 361382700 batches: 0.0248
trigger times: 4
Loss after 361513800 batches: 0.0238
trigger times: 5
Loss after 361644900 batches: 0.0228
trigger times: 6
Loss after 361776000 batches: 0.0222
trigger times: 7
Loss after 361907100 batches: 0.0221
trigger times: 8
Loss after 362038200 batches: 0.0215
trigger times: 9
Loss after 362169300 batches: 0.0217
trigger times: 10
Loss after 362300400 batches: 0.0208
trigger times: 11
Loss after 362431500 batches: 0.0202
trigger times: 0
Loss after 362562600 batches: 0.0200
trigger times: 1
Loss after 362693700 batches: 0.0196
trigger times: 2
Loss after 362824800 batches: 0.0195
trigger times: 3
Loss after 362955900 batches: 0.0192
trigger times: 4
Loss after 363087000 batches: 0.0192
trigger times: 5
Loss after 363218100 batches: 0.0191
trigger times: 6
Loss after 363349200 batches: 0.0186
trigger times: 7
Loss after 363480300 batches: 0.0187
trigger times: 8
Loss after 363611400 batches: 0.0189
trigger times: 9
Loss after 363742500 batches: 0.0182
trigger times: 10
Loss after 363873600 batches: 0.0181
trigger times: 11
Loss after 364004700 batches: 0.0183
trigger times: 12
Loss after 364135800 batches: 0.0179
trigger times: 13
Loss after 364266900 batches: 0.0180
trigger times: 14
Loss after 364398000 batches: 0.0178
trigger times: 15
Loss after 364529100 batches: 0.0176
trigger times: 16
Loss after 364660200 batches: 0.0177
trigger times: 17
Loss after 364791300 batches: 0.0174
trigger times: 18
Loss after 364922400 batches: 0.0173
trigger times: 19
Loss after 365053500 batches: 0.0173
trigger times: 0
Loss after 365184600 batches: 0.0171
trigger times: 1
Loss after 365315700 batches: 0.0169
trigger times: 2
Loss after 365446800 batches: 0.0169
trigger times: 3
Loss after 365577900 batches: 0.0169
trigger times: 4
Loss after 365709000 batches: 0.0165
trigger times: 5
Loss after 365840100 batches: 0.0167
trigger times: 6
Loss after 365971200 batches: 0.0168
trigger times: 7
Loss after 366102300 batches: 0.0165
trigger times: 8
Loss after 366233400 batches: 0.0165
trigger times: 9
Loss after 366364500 batches: 0.0165
trigger times: 10
Loss after 366495600 batches: 0.0163
trigger times: 11
Loss after 366626700 batches: 0.0162
trigger times: 12
Loss after 366757800 batches: 0.0162
trigger times: 13
Loss after 366888900 batches: 0.0163
trigger times: 0
Loss after 367020000 batches: 0.0163
trigger times: 1
Loss after 367151100 batches: 0.0161
trigger times: 2
Loss after 367282200 batches: 0.0160
trigger times: 3
Loss after 367413300 batches: 0.0158
trigger times: 4
Loss after 367544400 batches: 0.0159
trigger times: 5
Loss after 367675500 batches: 0.0155
trigger times: 6
Loss after 367806600 batches: 0.0156
trigger times: 7
Loss after 367937700 batches: 0.0156
trigger times: 8
Loss after 368068800 batches: 0.0157
trigger times: 0
Loss after 368199900 batches: 0.0156
trigger times: 1
Loss after 368331000 batches: 0.0157
trigger times: 2
Loss after 368462100 batches: 0.0153
trigger times: 3
Loss after 368593200 batches: 0.0156
trigger times: 4
Loss after 368724300 batches: 0.0155
trigger times: 5
Loss after 368855400 batches: 0.0153
trigger times: 6
Loss after 368986500 batches: 0.0154
trigger times: 7
Loss after 369117600 batches: 0.0150
trigger times: 8
Loss after 369248700 batches: 0.0151
trigger times: 9
Loss after 369379800 batches: 0.0153
trigger times: 10
Loss after 369510900 batches: 0.0153
trigger times: 11
Loss after 369642000 batches: 0.0152
trigger times: 12
Loss after 369773100 batches: 0.0149
trigger times: 13
Loss after 369904200 batches: 0.0147
trigger times: 14
Loss after 370035300 batches: 0.0147
trigger times: 15
Loss after 370166400 batches: 0.0145
trigger times: 16
Loss after 370297500 batches: 0.0145
trigger times: 17
Loss after 370428600 batches: 0.0146
trigger times: 18
Loss after 370559700 batches: 0.0148
trigger times: 19
Loss after 370690800 batches: 0.0145
trigger times: 20
Early stopping!
Start to test process.
Loss after 370821900 batches: 0.0147
Time to train on one home:  570.7432196140289
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146, 0.01617968764306582, 0.015298912286208879, 0.01494286251164481]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638], [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354], [0.5489356054200066, 0.40660235248365517, 0.4824471426162714, 1.021275384526291, 0.4861081410983952, 24.12799700883127, 1500.6461], [0.5518252385987176, 0.40347636543781107, 0.4779622827153174, 1.0271923671495102, 0.4886689327670392, 24.267787844092187, 1508.5514]]
Round_10_results:  [0.5518252385987176, 0.40347636543781107, 0.4779622827153174, 1.0271923671495102, 0.4886689327670392, 24.267787844092187, 1508.5514]
trigger times: 0
Loss after 370953000 batches: 0.1105
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2906 < 2907; dropping {'Training_Loss': 0.11054173182204084, 'Validation_Loss': 0.23962426682313284, 'Training_R2': 0.8885934118829169, 'Validation_R2': 0.7773483679868449, 'Training_F1': 0.8226032054451643, 'Validation_F1': 0.7540242538899015, 'Training_NEP': 0.3543284848655827, 'Validation_NEP': 0.4783066308916279, 'Training_NDE': 0.08363490316949795, 'Validation_NDE': 0.1773080228527557, 'Training_MAE': 11.734656709588092, 'Validation_MAE': 13.117152191703893, 'Training_MSE': 367.98044, 'Validation_MSE': 654.79364}.
trigger times: 0
Loss after 371084100 batches: 0.0258
trigger times: 1
Loss after 371215200 batches: 0.0194
trigger times: 2
Loss after 371346300 batches: 0.0173
trigger times: 0
Loss after 371477400 batches: 0.0156
trigger times: 1
Loss after 371608500 batches: 0.0147
trigger times: 2
Loss after 371739600 batches: 0.0137
trigger times: 3
Loss after 371870700 batches: 0.0134
trigger times: 4
Loss after 372001800 batches: 0.0130
trigger times: 5
Loss after 372132900 batches: 0.0125
trigger times: 6
Loss after 372264000 batches: 0.0121
trigger times: 7
Loss after 372395100 batches: 0.0118
trigger times: 0
Loss after 372526200 batches: 0.0118
trigger times: 1
Loss after 372657300 batches: 0.0117
trigger times: 2
Loss after 372788400 batches: 0.0113
trigger times: 3
Loss after 372919500 batches: 0.0113
trigger times: 4
Loss after 373050600 batches: 0.0111
trigger times: 5
Loss after 373181700 batches: 0.0107
trigger times: 6
Loss after 373312800 batches: 0.0106
trigger times: 7
Loss after 373443900 batches: 0.0107
trigger times: 8
Loss after 373575000 batches: 0.0104
trigger times: 9
Loss after 373706100 batches: 0.0102
trigger times: 10
Loss after 373837200 batches: 0.0103
trigger times: 11
Loss after 373968300 batches: 0.0100
trigger times: 12
Loss after 374099400 batches: 0.0101
trigger times: 13
Loss after 374230500 batches: 0.0098
trigger times: 14
Loss after 374361600 batches: 0.0098
trigger times: 0
Loss after 374492700 batches: 0.0096
trigger times: 1
Loss after 374623800 batches: 0.0097
trigger times: 2
Loss after 374754900 batches: 0.0095
trigger times: 3
Loss after 374886000 batches: 0.0094
trigger times: 4
Loss after 375017100 batches: 0.0094
trigger times: 5
Loss after 375148200 batches: 0.0092
trigger times: 6
Loss after 375279300 batches: 0.0093
trigger times: 7
Loss after 375410400 batches: 0.0092
trigger times: 8
Loss after 375541500 batches: 0.0092
trigger times: 9
Loss after 375672600 batches: 0.0090
trigger times: 10
Loss after 375803700 batches: 0.0089
trigger times: 11
Loss after 375934800 batches: 0.0088
trigger times: 12
Loss after 376065900 batches: 0.0090
trigger times: 13
Loss after 376197000 batches: 0.0089
trigger times: 14
Loss after 376328100 batches: 0.0092
trigger times: 15
Loss after 376459200 batches: 0.0088
trigger times: 16
Loss after 376590300 batches: 0.0086
trigger times: 17
Loss after 376721400 batches: 0.0086
trigger times: 18
Loss after 376852500 batches: 0.0086
trigger times: 19
Loss after 376983600 batches: 0.0086
trigger times: 20
Early stopping!
Start to test process.
Loss after 377114700 batches: 0.0086
Time to train on one home:  350.88683795928955
trigger times: 0
Loss after 377217300 batches: 0.2221
trigger times: 0
Loss after 377319900 batches: 0.0644
trigger times: 0
Loss after 377422500 batches: 0.0473
trigger times: 0
Loss after 377525100 batches: 0.0429
trigger times: 1
Loss after 377627700 batches: 0.0370
trigger times: 0
Loss after 377730300 batches: 0.0346
trigger times: 1
Loss after 377832900 batches: 0.0358
trigger times: 2
Loss after 377935500 batches: 0.0361
trigger times: 3
Loss after 378038100 batches: 0.0287
trigger times: 4
Loss after 378140700 batches: 0.0275
trigger times: 5
Loss after 378243300 batches: 0.0286
trigger times: 6
Loss after 378345900 batches: 0.0266
trigger times: 7
Loss after 378448500 batches: 0.0251
trigger times: 0
Loss after 378551100 batches: 0.0277
trigger times: 0
Loss after 378653700 batches: 0.0373
trigger times: 1
Loss after 378756300 batches: 0.0264
trigger times: 2
Loss after 378858900 batches: 0.0239
trigger times: 3
Loss after 378961500 batches: 0.0232
trigger times: 4
Loss after 379064100 batches: 0.0225
trigger times: 5
Loss after 379166700 batches: 0.0218
trigger times: 6
Loss after 379269300 batches: 0.0227
trigger times: 7
Loss after 379371900 batches: 0.0227
trigger times: 8
Loss after 379474500 batches: 0.0216
trigger times: 9
Loss after 379577100 batches: 0.0208
trigger times: 10
Loss after 379679700 batches: 0.0225
trigger times: 11
Loss after 379782300 batches: 0.0206
trigger times: 12
Loss after 379884900 batches: 0.0210
trigger times: 13
Loss after 379987500 batches: 0.0212
trigger times: 14
Loss after 380090100 batches: 0.0222
trigger times: 15
Loss after 380192700 batches: 0.0205
trigger times: 16
Loss after 380295300 batches: 0.0196
trigger times: 17
Loss after 380397900 batches: 0.0193
trigger times: 18
Loss after 380500500 batches: 0.0189
trigger times: 19
Loss after 380603100 batches: 0.0193
trigger times: 20
Early stopping!
Start to test process.
Loss after 380705700 batches: 0.0191
Time to train on one home:  212.04210329055786
trigger times: 0
Loss after 380836800 batches: 0.1148
trigger times: 1
Loss after 380967900 batches: 0.0388
trigger times: 2
Loss after 381099000 batches: 0.0300
trigger times: 3
Loss after 381230100 batches: 0.0267
trigger times: 4
Loss after 381361200 batches: 0.0247
trigger times: 5
Loss after 381492300 batches: 0.0231
trigger times: 6
Loss after 381623400 batches: 0.0221
trigger times: 7
Loss after 381754500 batches: 0.0212
trigger times: 8
Loss after 381885600 batches: 0.0205
trigger times: 9
Loss after 382016700 batches: 0.0203
trigger times: 10
Loss after 382147800 batches: 0.0194
trigger times: 11
Loss after 382278900 batches: 0.0192
trigger times: 12
Loss after 382410000 batches: 0.0186
trigger times: 13
Loss after 382541100 batches: 0.0183
trigger times: 14
Loss after 382672200 batches: 0.0178
trigger times: 15
Loss after 382803300 batches: 0.0176
trigger times: 16
Loss after 382934400 batches: 0.0173
trigger times: 17
Loss after 383065500 batches: 0.0172
trigger times: 18
Loss after 383196600 batches: 0.0168
trigger times: 19
Loss after 383327700 batches: 0.0167
trigger times: 20
Early stopping!
Start to test process.
Loss after 383458800 batches: 0.0164
Time to train on one home:  160.20593547821045
trigger times: 0
Loss after 383589900 batches: 0.1303
trigger times: 0
Loss after 383721000 batches: 0.0435
trigger times: 0
Loss after 383852100 batches: 0.0328
trigger times: 0
Loss after 383983200 batches: 0.0287
trigger times: 1
Loss after 384114300 batches: 0.0264
trigger times: 0
Loss after 384245400 batches: 0.0255
trigger times: 1
Loss after 384376500 batches: 0.0242
trigger times: 2
Loss after 384507600 batches: 0.0233
trigger times: 0
Loss after 384638700 batches: 0.0224
trigger times: 1
Loss after 384769800 batches: 0.0221
trigger times: 2
Loss after 384900900 batches: 0.0218
trigger times: 3
Loss after 385032000 batches: 0.0210
trigger times: 0
Loss after 385163100 batches: 0.0211
trigger times: 1
Loss after 385294200 batches: 0.0204
trigger times: 2
Loss after 385425300 batches: 0.0201
trigger times: 3
Loss after 385556400 batches: 0.0200
trigger times: 4
Loss after 385687500 batches: 0.0196
trigger times: 5
Loss after 385818600 batches: 0.0196
trigger times: 6
Loss after 385949700 batches: 0.0194
trigger times: 0
Loss after 386080800 batches: 0.0191
trigger times: 0
Loss after 386211900 batches: 0.0188
trigger times: 1
Loss after 386343000 batches: 0.0186
trigger times: 2
Loss after 386474100 batches: 0.0186
trigger times: 3
Loss after 386605200 batches: 0.0185
trigger times: 4
Loss after 386736300 batches: 0.0185
trigger times: 5
Loss after 386867400 batches: 0.0180
trigger times: 6
Loss after 386998500 batches: 0.0180
trigger times: 7
Loss after 387129600 batches: 0.0180
trigger times: 8
Loss after 387260700 batches: 0.0178
trigger times: 9
Loss after 387391800 batches: 0.0177
trigger times: 10
Loss after 387522900 batches: 0.0178
trigger times: 11
Loss after 387654000 batches: 0.0177
trigger times: 12
Loss after 387785100 batches: 0.0173
trigger times: 13
Loss after 387916200 batches: 0.0173
trigger times: 14
Loss after 388047300 batches: 0.0171
trigger times: 15
Loss after 388178400 batches: 0.0171
trigger times: 16
Loss after 388309500 batches: 0.0172
trigger times: 17
Loss after 388440600 batches: 0.0171
trigger times: 18
Loss after 388571700 batches: 0.0169
trigger times: 19
Loss after 388702800 batches: 0.0168
trigger times: 20
Early stopping!
Start to test process.
Loss after 388833900 batches: 0.0168
Time to train on one home:  300.8297655582428
trigger times: 0
Loss after 388962540 batches: 0.0903
trigger times: 0
Loss after 389091180 batches: 0.0267
trigger times: 0
Loss after 389219820 batches: 0.0209
trigger times: 1
Loss after 389348460 batches: 0.0183
trigger times: 2
Loss after 389477100 batches: 0.0171
trigger times: 3
Loss after 389605740 batches: 0.0164
trigger times: 4
Loss after 389734380 batches: 0.0160
trigger times: 5
Loss after 389863020 batches: 0.0153
trigger times: 6
Loss after 389991660 batches: 0.0147
trigger times: 7
Loss after 390120300 batches: 0.0141
trigger times: 8
Loss after 390248940 batches: 0.0139
trigger times: 9
Loss after 390377580 batches: 0.0135
trigger times: 10
Loss after 390506220 batches: 0.0134
trigger times: 11
Loss after 390634860 batches: 0.0132
trigger times: 12
Loss after 390763500 batches: 0.0129
trigger times: 13
Loss after 390892140 batches: 0.0129
trigger times: 14
Loss after 391020780 batches: 0.0126
trigger times: 15
Loss after 391149420 batches: 0.0124
trigger times: 16
Loss after 391278060 batches: 0.0124
trigger times: 17
Loss after 391406700 batches: 0.0120
trigger times: 18
Loss after 391535340 batches: 0.0122
trigger times: 19
Loss after 391663980 batches: 0.0119
trigger times: 20
Early stopping!
Start to test process.
Loss after 391792620 batches: 0.0119
Time to train on one home:  170.97161173820496
trigger times: 0
Loss after 391923720 batches: 0.1798
trigger times: 1
Loss after 392054820 batches: 0.0424
trigger times: 0
Loss after 392185920 batches: 0.0324
trigger times: 1
Loss after 392317020 batches: 0.0285
trigger times: 0
Loss after 392448120 batches: 0.0269
trigger times: 1
Loss after 392579220 batches: 0.0252
trigger times: 2
Loss after 392710320 batches: 0.0239
trigger times: 3
Loss after 392841420 batches: 0.0232
trigger times: 4
Loss after 392972520 batches: 0.0225
trigger times: 5
Loss after 393103620 batches: 0.0217
trigger times: 6
Loss after 393234720 batches: 0.0213
trigger times: 7
Loss after 393365820 batches: 0.0210
trigger times: 0
Loss after 393496920 batches: 0.0205
trigger times: 0
Loss after 393628020 batches: 0.0201
trigger times: 1
Loss after 393759120 batches: 0.0196
trigger times: 2
Loss after 393890220 batches: 0.0197
trigger times: 3
Loss after 394021320 batches: 0.0190
trigger times: 4
Loss after 394152420 batches: 0.0191
trigger times: 5
Loss after 394283520 batches: 0.0188
trigger times: 6
Loss after 394414620 batches: 0.0185
trigger times: 7
Loss after 394545720 batches: 0.0186
trigger times: 8
Loss after 394676820 batches: 0.0185
trigger times: 9
Loss after 394807920 batches: 0.0183
trigger times: 10
Loss after 394939020 batches: 0.0179
trigger times: 11
Loss after 395070120 batches: 0.0180
trigger times: 12
Loss after 395201220 batches: 0.0176
trigger times: 13
Loss after 395332320 batches: 0.0175
trigger times: 0
Loss after 395463420 batches: 0.0174
trigger times: 1
Loss after 395594520 batches: 0.0173
trigger times: 2
Loss after 395725620 batches: 0.0173
trigger times: 3
Loss after 395856720 batches: 0.0170
trigger times: 4
Loss after 395987820 batches: 0.0170
trigger times: 5
Loss after 396118920 batches: 0.0169
trigger times: 6
Loss after 396250020 batches: 0.0170
trigger times: 7
Loss after 396381120 batches: 0.0170
trigger times: 8
Loss after 396512220 batches: 0.0164
trigger times: 9
Loss after 396643320 batches: 0.0164
trigger times: 10
Loss after 396774420 batches: 0.0163
trigger times: 11
Loss after 396905520 batches: 0.0165
trigger times: 12
Loss after 397036620 batches: 0.0164
trigger times: 13
Loss after 397167720 batches: 0.0165
trigger times: 0
Loss after 397298820 batches: 0.0163
trigger times: 1
Loss after 397429920 batches: 0.0161
trigger times: 2
Loss after 397561020 batches: 0.0162
trigger times: 3
Loss after 397692120 batches: 0.0158
trigger times: 4
Loss after 397823220 batches: 0.0158
trigger times: 5
Loss after 397954320 batches: 0.0158
trigger times: 6
Loss after 398085420 batches: 0.0157
trigger times: 7
Loss after 398216520 batches: 0.0158
trigger times: 8
Loss after 398347620 batches: 0.0158
trigger times: 0
Loss after 398478720 batches: 0.0152
trigger times: 1
Loss after 398609820 batches: 0.0153
trigger times: 2
Loss after 398740920 batches: 0.0155
trigger times: 3
Loss after 398872020 batches: 0.0156
trigger times: 4
Loss after 399003120 batches: 0.0156
trigger times: 5
Loss after 399134220 batches: 0.0152
trigger times: 6
Loss after 399265320 batches: 0.0153
trigger times: 7
Loss after 399396420 batches: 0.0151
trigger times: 8
Loss after 399527520 batches: 0.0151
trigger times: 9
Loss after 399658620 batches: 0.0153
trigger times: 10
Loss after 399789720 batches: 0.0149
trigger times: 11
Loss after 399920820 batches: 0.0151
trigger times: 12
Loss after 400051920 batches: 0.0148
trigger times: 13
Loss after 400183020 batches: 0.0148
trigger times: 14
Loss after 400314120 batches: 0.0150
trigger times: 15
Loss after 400445220 batches: 0.0148
trigger times: 16
Loss after 400576320 batches: 0.0150
trigger times: 17
Loss after 400707420 batches: 0.0149
trigger times: 18
Loss after 400838520 batches: 0.0147
trigger times: 19
Loss after 400969620 batches: 0.0147
trigger times: 20
Early stopping!
Start to test process.
Loss after 401100720 batches: 0.0145
Time to train on one home:  511.913382768631
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146, 0.01617968764306582, 0.015298912286208879, 0.01494286251164481, 0.014533454579010391]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638], [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354], [0.5489356054200066, 0.40660235248365517, 0.4824471426162714, 1.021275384526291, 0.4861081410983952, 24.12799700883127, 1500.6461], [0.5518252385987176, 0.40347636543781107, 0.4779622827153174, 1.0271923671495102, 0.4886689327670392, 24.267787844092187, 1508.5514], [0.5432945456769731, 0.41270949933158785, 0.48664830013661614, 1.0178919743034478, 0.48110519945531827, 24.048062729621872, 1485.2018]]
Round_11_results:  [0.5432945456769731, 0.41270949933158785, 0.48664830013661614, 1.0178919743034478, 0.48110519945531827, 24.048062729621872, 1485.2018]
trigger times: 0
Loss after 401231820 batches: 0.0891
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3145 < 3146; dropping {'Training_Loss': 0.08912426809657295, 'Validation_Loss': 0.20822505321767595, 'Training_R2': 0.9102119791450676, 'Validation_R2': 0.8063458582850992, 'Training_F1': 0.8398172691783472, 'Validation_F1': 0.7621391456972464, 'Training_NEP': 0.3202622700652038, 'Validation_NEP': 0.46029944020021385, 'Training_NDE': 0.06740546099563795, 'Validation_NDE': 0.15421595015610615, 'Training_MAE': 10.606451235988684, 'Validation_MAE': 12.62332031568746, 'Training_MSE': 296.57346, 'Validation_MSE': 569.51526}.
trigger times: 0
Loss after 401362920 batches: 0.0241
trigger times: 0
Loss after 401494020 batches: 0.0179
trigger times: 1
Loss after 401625120 batches: 0.0156
trigger times: 2
Loss after 401756220 batches: 0.0146
trigger times: 0
Loss after 401887320 batches: 0.0138
trigger times: 1
Loss after 402018420 batches: 0.0134
trigger times: 2
Loss after 402149520 batches: 0.0125
trigger times: 3
Loss after 402280620 batches: 0.0122
trigger times: 4
Loss after 402411720 batches: 0.0118
trigger times: 5
Loss after 402542820 batches: 0.0115
trigger times: 6
Loss after 402673920 batches: 0.0113
trigger times: 0
Loss after 402805020 batches: 0.0112
trigger times: 1
Loss after 402936120 batches: 0.0112
trigger times: 2
Loss after 403067220 batches: 0.0108
trigger times: 3
Loss after 403198320 batches: 0.0108
trigger times: 4
Loss after 403329420 batches: 0.0106
trigger times: 5
Loss after 403460520 batches: 0.0102
trigger times: 6
Loss after 403591620 batches: 0.0102
trigger times: 7
Loss after 403722720 batches: 0.0100
trigger times: 8
Loss after 403853820 batches: 0.0098
trigger times: 9
Loss after 403984920 batches: 0.0099
trigger times: 10
Loss after 404116020 batches: 0.0097
trigger times: 11
Loss after 404247120 batches: 0.0098
trigger times: 12
Loss after 404378220 batches: 0.0092
trigger times: 13
Loss after 404509320 batches: 0.0093
trigger times: 14
Loss after 404640420 batches: 0.0092
trigger times: 15
Loss after 404771520 batches: 0.0095
trigger times: 16
Loss after 404902620 batches: 0.0095
trigger times: 17
Loss after 405033720 batches: 0.0093
trigger times: 18
Loss after 405164820 batches: 0.0090
trigger times: 19
Loss after 405295920 batches: 0.0089
trigger times: 20
Early stopping!
Start to test process.
Loss after 405427020 batches: 0.0087
Time to train on one home:  245.20122647285461
trigger times: 0
Loss after 405529620 batches: 0.2067
trigger times: 0
Loss after 405632220 batches: 0.0700
trigger times: 1
Loss after 405734820 batches: 0.0530
trigger times: 2
Loss after 405837420 batches: 0.0392
trigger times: 3
Loss after 405940020 batches: 0.0339
trigger times: 4
Loss after 406042620 batches: 0.0310
trigger times: 5
Loss after 406145220 batches: 0.0298
trigger times: 6
Loss after 406247820 batches: 0.0295
trigger times: 7
Loss after 406350420 batches: 0.0281
trigger times: 8
Loss after 406453020 batches: 0.0272
trigger times: 9
Loss after 406555620 batches: 0.0316
trigger times: 10
Loss after 406658220 batches: 0.0271
trigger times: 11
Loss after 406760820 batches: 0.0242
trigger times: 12
Loss after 406863420 batches: 0.0268
trigger times: 13
Loss after 406966020 batches: 0.0287
trigger times: 14
Loss after 407068620 batches: 0.0252
trigger times: 15
Loss after 407171220 batches: 0.0247
trigger times: 16
Loss after 407273820 batches: 0.0231
trigger times: 17
Loss after 407376420 batches: 0.0223
trigger times: 18
Loss after 407479020 batches: 0.0214
trigger times: 19
Loss after 407581620 batches: 0.0222
trigger times: 20
Early stopping!
Start to test process.
Loss after 407684220 batches: 0.0220
Time to train on one home:  138.13570642471313
trigger times: 0
Loss after 407815320 batches: 0.1046
trigger times: 1
Loss after 407946420 batches: 0.0368
trigger times: 2
Loss after 408077520 batches: 0.0289
trigger times: 3
Loss after 408208620 batches: 0.0258
trigger times: 4
Loss after 408339720 batches: 0.0241
trigger times: 5
Loss after 408470820 batches: 0.0228
trigger times: 6
Loss after 408601920 batches: 0.0217
trigger times: 7
Loss after 408733020 batches: 0.0208
trigger times: 8
Loss after 408864120 batches: 0.0200
trigger times: 9
Loss after 408995220 batches: 0.0195
trigger times: 10
Loss after 409126320 batches: 0.0190
trigger times: 11
Loss after 409257420 batches: 0.0184
trigger times: 12
Loss after 409388520 batches: 0.0180
trigger times: 13
Loss after 409519620 batches: 0.0180
trigger times: 14
Loss after 409650720 batches: 0.0175
trigger times: 15
Loss after 409781820 batches: 0.0172
trigger times: 16
Loss after 409912920 batches: 0.0168
trigger times: 17
Loss after 410044020 batches: 0.0166
trigger times: 18
Loss after 410175120 batches: 0.0165
trigger times: 19
Loss after 410306220 batches: 0.0162
trigger times: 20
Early stopping!
Start to test process.
Loss after 410437320 batches: 0.0160
Time to train on one home:  160.3515431880951
trigger times: 0
Loss after 410568420 batches: 0.1367
trigger times: 0
Loss after 410699520 batches: 0.0406
trigger times: 0
Loss after 410830620 batches: 0.0315
trigger times: 0
Loss after 410961720 batches: 0.0282
trigger times: 1
Loss after 411092820 batches: 0.0262
trigger times: 0
Loss after 411223920 batches: 0.0246
trigger times: 0
Loss after 411355020 batches: 0.0238
trigger times: 1
Loss after 411486120 batches: 0.0230
trigger times: 2
Loss after 411617220 batches: 0.0224
trigger times: 3
Loss after 411748320 batches: 0.0218
trigger times: 4
Loss after 411879420 batches: 0.0214
trigger times: 0
Loss after 412010520 batches: 0.0211
trigger times: 0
Loss after 412141620 batches: 0.0207
trigger times: 1
Loss after 412272720 batches: 0.0205
trigger times: 2
Loss after 412403820 batches: 0.0201
trigger times: 3
Loss after 412534920 batches: 0.0198
trigger times: 4
Loss after 412666020 batches: 0.0194
trigger times: 5
Loss after 412797120 batches: 0.0193
trigger times: 6
Loss after 412928220 batches: 0.0189
trigger times: 7
Loss after 413059320 batches: 0.0186
trigger times: 8
Loss after 413190420 batches: 0.0189
trigger times: 0
Loss after 413321520 batches: 0.0186
trigger times: 1
Loss after 413452620 batches: 0.0186
trigger times: 2
Loss after 413583720 batches: 0.0187
trigger times: 3
Loss after 413714820 batches: 0.0186
trigger times: 4
Loss after 413845920 batches: 0.0180
trigger times: 5
Loss after 413977020 batches: 0.0180
trigger times: 6
Loss after 414108120 batches: 0.0179
trigger times: 7
Loss after 414239220 batches: 0.0177
trigger times: 8
Loss after 414370320 batches: 0.0176
trigger times: 9
Loss after 414501420 batches: 0.0178
trigger times: 10
Loss after 414632520 batches: 0.0174
trigger times: 11
Loss after 414763620 batches: 0.0176
trigger times: 12
Loss after 414894720 batches: 0.0172
trigger times: 13
Loss after 415025820 batches: 0.0168
trigger times: 14
Loss after 415156920 batches: 0.0170
trigger times: 15
Loss after 415288020 batches: 0.0170
trigger times: 16
Loss after 415419120 batches: 0.0169
trigger times: 17
Loss after 415550220 batches: 0.0167
trigger times: 18
Loss after 415681320 batches: 0.0168
trigger times: 19
Loss after 415812420 batches: 0.0168
trigger times: 20
Early stopping!
Start to test process.
Loss after 415943520 batches: 0.0166
Time to train on one home:  308.826908826828
trigger times: 0
Loss after 416072160 batches: 0.0874
trigger times: 0
Loss after 416200800 batches: 0.0260
trigger times: 0
Loss after 416329440 batches: 0.0205
trigger times: 1
Loss after 416458080 batches: 0.0183
trigger times: 0
Loss after 416586720 batches: 0.0173
trigger times: 1
Loss after 416715360 batches: 0.0162
trigger times: 2
Loss after 416844000 batches: 0.0156
trigger times: 3
Loss after 416972640 batches: 0.0154
trigger times: 4
Loss after 417101280 batches: 0.0148
trigger times: 5
Loss after 417229920 batches: 0.0141
trigger times: 6
Loss after 417358560 batches: 0.0139
trigger times: 7
Loss after 417487200 batches: 0.0134
trigger times: 8
Loss after 417615840 batches: 0.0133
trigger times: 9
Loss after 417744480 batches: 0.0131
trigger times: 10
Loss after 417873120 batches: 0.0130
trigger times: 11
Loss after 418001760 batches: 0.0130
trigger times: 12
Loss after 418130400 batches: 0.0124
trigger times: 13
Loss after 418259040 batches: 0.0124
trigger times: 14
Loss after 418387680 batches: 0.0121
trigger times: 15
Loss after 418516320 batches: 0.0118
trigger times: 16
Loss after 418644960 batches: 0.0118
trigger times: 17
Loss after 418773600 batches: 0.0115
trigger times: 18
Loss after 418902240 batches: 0.0117
trigger times: 19
Loss after 419030880 batches: 0.0116
trigger times: 20
Early stopping!
Start to test process.
Loss after 419159520 batches: 0.0116
Time to train on one home:  185.5515923500061
trigger times: 0
Loss after 419290620 batches: 0.1407
trigger times: 0
Loss after 419421720 batches: 0.0386
trigger times: 0
Loss after 419552820 batches: 0.0297
trigger times: 1
Loss after 419683920 batches: 0.0264
trigger times: 2
Loss after 419815020 batches: 0.0244
trigger times: 3
Loss after 419946120 batches: 0.0232
trigger times: 4
Loss after 420077220 batches: 0.0224
trigger times: 5
Loss after 420208320 batches: 0.0212
trigger times: 6
Loss after 420339420 batches: 0.0209
trigger times: 7
Loss after 420470520 batches: 0.0204
trigger times: 8
Loss after 420601620 batches: 0.0202
trigger times: 9
Loss after 420732720 batches: 0.0198
trigger times: 10
Loss after 420863820 batches: 0.0193
trigger times: 11
Loss after 420994920 batches: 0.0192
trigger times: 12
Loss after 421126020 batches: 0.0190
trigger times: 13
Loss after 421257120 batches: 0.0185
trigger times: 14
Loss after 421388220 batches: 0.0182
trigger times: 15
Loss after 421519320 batches: 0.0181
trigger times: 16
Loss after 421650420 batches: 0.0178
trigger times: 17
Loss after 421781520 batches: 0.0178
trigger times: 18
Loss after 421912620 batches: 0.0175
trigger times: 19
Loss after 422043720 batches: 0.0174
trigger times: 20
Early stopping!
Start to test process.
Loss after 422174820 batches: 0.0172
Time to train on one home:  173.62328910827637
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146, 0.01617968764306582, 0.015298912286208879, 0.01494286251164481, 0.014533454579010391, 0.015354135860208057]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638], [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354], [0.5489356054200066, 0.40660235248365517, 0.4824471426162714, 1.021275384526291, 0.4861081410983952, 24.12799700883127, 1500.6461], [0.5518252385987176, 0.40347636543781107, 0.4779622827153174, 1.0271923671495102, 0.4886689327670392, 24.267787844092187, 1508.5514], [0.5432945456769731, 0.41270949933158785, 0.48664830013661614, 1.0178919743034478, 0.48110519945531827, 24.048062729621872, 1485.2018], [0.528244235449367, 0.4289907862839557, 0.498210151533106, 1.0003101691077434, 0.4677676573059164, 23.63268628013604, 1444.0278]]
Round_12_results:  [0.528244235449367, 0.4289907862839557, 0.498210151533106, 1.0003101691077434, 0.4677676573059164, 23.63268628013604, 1444.0278]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3311 < 3312; dropping {'Training_Loss': 0.08899548322946396, 'Validation_Loss': 0.24068122936619651, 'Training_R2': 0.9103699298608171, 'Validation_R2': 0.7763045636267254, 'Training_F1': 0.8414218517363596, 'Validation_F1': 0.7413384969725416, 'Training_NEP': 0.3170848029180521, 'Validation_NEP': 0.48997214276948864, 'Training_NDE': 0.06728688458969512, 'Validation_NDE': 0.17813925362193753, 'Training_MAE': 10.501219825671885, 'Validation_MAE': 13.437068924638961, 'Training_MSE': 296.05176, 'Validation_MSE': 657.86334}.
trigger times: 0
Loss after 422305920 batches: 0.0890
trigger times: 0
Loss after 422437020 batches: 0.0231
trigger times: 0
Loss after 422568120 batches: 0.0172
trigger times: 1
Loss after 422699220 batches: 0.0153
trigger times: 0
Loss after 422830320 batches: 0.0146
trigger times: 0
Loss after 422961420 batches: 0.0136
trigger times: 1
Loss after 423092520 batches: 0.0129
trigger times: 2
Loss after 423223620 batches: 0.0124
trigger times: 3
Loss after 423354720 batches: 0.0119
trigger times: 4
Loss after 423485820 batches: 0.0116
trigger times: 5
Loss after 423616920 batches: 0.0116
trigger times: 6
Loss after 423748020 batches: 0.0112
trigger times: 7
Loss after 423879120 batches: 0.0110
trigger times: 8
Loss after 424010220 batches: 0.0108
trigger times: 9
Loss after 424141320 batches: 0.0108
trigger times: 10
Loss after 424272420 batches: 0.0107
trigger times: 11
Loss after 424403520 batches: 0.0102
trigger times: 12
Loss after 424534620 batches: 0.0101
trigger times: 13
Loss after 424665720 batches: 0.0099
trigger times: 14
Loss after 424796820 batches: 0.0101
trigger times: 15
Loss after 424927920 batches: 0.0096
trigger times: 16
Loss after 425059020 batches: 0.0097
trigger times: 17
Loss after 425190120 batches: 0.0097
trigger times: 18
Loss after 425321220 batches: 0.0095
trigger times: 19
Loss after 425452320 batches: 0.0092
trigger times: 20
Early stopping!
Start to test process.
Loss after 425583420 batches: 0.0094
Time to train on one home:  194.91955661773682
trigger times: 0
Loss after 425686020 batches: 0.1986
trigger times: 0
Loss after 425788620 batches: 0.0580
trigger times: 1
Loss after 425891220 batches: 0.0490
trigger times: 2
Loss after 425993820 batches: 0.0377
trigger times: 0
Loss after 426096420 batches: 0.0362
trigger times: 1
Loss after 426199020 batches: 0.0335
trigger times: 2
Loss after 426301620 batches: 0.0292
trigger times: 3
Loss after 426404220 batches: 0.0282
trigger times: 4
Loss after 426506820 batches: 0.0272
trigger times: 5
Loss after 426609420 batches: 0.0250
trigger times: 6
Loss after 426712020 batches: 0.0253
trigger times: 7
Loss after 426814620 batches: 0.0239
trigger times: 8
Loss after 426917220 batches: 0.0238
trigger times: 9
Loss after 427019820 batches: 0.0230
trigger times: 10
Loss after 427122420 batches: 0.0233
trigger times: 11
Loss after 427225020 batches: 0.0258
trigger times: 12
Loss after 427327620 batches: 0.0226
trigger times: 13
Loss after 427430220 batches: 0.0221
trigger times: 14
Loss after 427532820 batches: 0.0220
trigger times: 15
Loss after 427635420 batches: 0.0214
trigger times: 16
Loss after 427738020 batches: 0.0212
trigger times: 17
Loss after 427840620 batches: 0.0214
trigger times: 18
Loss after 427943220 batches: 0.0205
trigger times: 19
Loss after 428045820 batches: 0.0195
trigger times: 20
Early stopping!
Start to test process.
Loss after 428148420 batches: 0.0207
Time to train on one home:  154.86123037338257
trigger times: 0
Loss after 428279520 batches: 0.0901
trigger times: 0
Loss after 428410620 batches: 0.0351
trigger times: 1
Loss after 428541720 batches: 0.0277
trigger times: 2
Loss after 428672820 batches: 0.0251
trigger times: 3
Loss after 428803920 batches: 0.0233
trigger times: 4
Loss after 428935020 batches: 0.0220
trigger times: 5
Loss after 429066120 batches: 0.0209
trigger times: 6
Loss after 429197220 batches: 0.0202
trigger times: 7
Loss after 429328320 batches: 0.0197
trigger times: 8
Loss after 429459420 batches: 0.0191
trigger times: 9
Loss after 429590520 batches: 0.0186
trigger times: 10
Loss after 429721620 batches: 0.0182
trigger times: 11
Loss after 429852720 batches: 0.0181
trigger times: 12
Loss after 429983820 batches: 0.0175
trigger times: 13
Loss after 430114920 batches: 0.0175
trigger times: 14
Loss after 430246020 batches: 0.0172
trigger times: 15
Loss after 430377120 batches: 0.0169
trigger times: 16
Loss after 430508220 batches: 0.0165
trigger times: 17
Loss after 430639320 batches: 0.0160
trigger times: 18
Loss after 430770420 batches: 0.0163
trigger times: 19
Loss after 430901520 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 431032620 batches: 0.0158
Time to train on one home:  166.8754072189331
trigger times: 0
Loss after 431163720 batches: 0.1190
trigger times: 0
Loss after 431294820 batches: 0.0381
trigger times: 0
Loss after 431425920 batches: 0.0305
trigger times: 0
Loss after 431557020 batches: 0.0274
trigger times: 1
Loss after 431688120 batches: 0.0256
trigger times: 0
Loss after 431819220 batches: 0.0241
trigger times: 0
Loss after 431950320 batches: 0.0233
trigger times: 1
Loss after 432081420 batches: 0.0230
trigger times: 2
Loss after 432212520 batches: 0.0221
trigger times: 3
Loss after 432343620 batches: 0.0216
trigger times: 0
Loss after 432474720 batches: 0.0211
trigger times: 1
Loss after 432605820 batches: 0.0207
trigger times: 2
Loss after 432736920 batches: 0.0207
trigger times: 3
Loss after 432868020 batches: 0.0199
trigger times: 4
Loss after 432999120 batches: 0.0197
trigger times: 5
Loss after 433130220 batches: 0.0195
trigger times: 6
Loss after 433261320 batches: 0.0192
trigger times: 7
Loss after 433392420 batches: 0.0191
trigger times: 0
Loss after 433523520 batches: 0.0188
trigger times: 1
Loss after 433654620 batches: 0.0185
trigger times: 2
Loss after 433785720 batches: 0.0183
trigger times: 3
Loss after 433916820 batches: 0.0182
trigger times: 4
Loss after 434047920 batches: 0.0181
trigger times: 5
Loss after 434179020 batches: 0.0183
trigger times: 6
Loss after 434310120 batches: 0.0179
trigger times: 7
Loss after 434441220 batches: 0.0178
trigger times: 8
Loss after 434572320 batches: 0.0176
trigger times: 9
Loss after 434703420 batches: 0.0177
trigger times: 10
Loss after 434834520 batches: 0.0175
trigger times: 11
Loss after 434965620 batches: 0.0175
trigger times: 0
Loss after 435096720 batches: 0.0173
trigger times: 1
Loss after 435227820 batches: 0.0173
trigger times: 2
Loss after 435358920 batches: 0.0169
trigger times: 3
Loss after 435490020 batches: 0.0171
trigger times: 4
Loss after 435621120 batches: 0.0169
trigger times: 5
Loss after 435752220 batches: 0.0168
trigger times: 6
Loss after 435883320 batches: 0.0171
trigger times: 7
Loss after 436014420 batches: 0.0168
trigger times: 8
Loss after 436145520 batches: 0.0167
trigger times: 9
Loss after 436276620 batches: 0.0164
trigger times: 10
Loss after 436407720 batches: 0.0165
trigger times: 0
Loss after 436538820 batches: 0.0167
trigger times: 1
Loss after 436669920 batches: 0.0163
trigger times: 2
Loss after 436801020 batches: 0.0164
trigger times: 3
Loss after 436932120 batches: 0.0164
trigger times: 4
Loss after 437063220 batches: 0.0163
trigger times: 5
Loss after 437194320 batches: 0.0161
trigger times: 6
Loss after 437325420 batches: 0.0160
trigger times: 7
Loss after 437456520 batches: 0.0162
trigger times: 8
Loss after 437587620 batches: 0.0161
trigger times: 9
Loss after 437718720 batches: 0.0158
trigger times: 10
Loss after 437849820 batches: 0.0158
trigger times: 11
Loss after 437980920 batches: 0.0158
trigger times: 12
Loss after 438112020 batches: 0.0157
trigger times: 13
Loss after 438243120 batches: 0.0157
trigger times: 14
Loss after 438374220 batches: 0.0155
trigger times: 15
Loss after 438505320 batches: 0.0153
trigger times: 16
Loss after 438636420 batches: 0.0154
trigger times: 17
Loss after 438767520 batches: 0.0158
trigger times: 18
Loss after 438898620 batches: 0.0153
trigger times: 19
Loss after 439029720 batches: 0.0153
trigger times: 20
Early stopping!
Start to test process.
Loss after 439160820 batches: 0.0154
Time to train on one home:  452.2820293903351
trigger times: 0
Loss after 439289460 batches: 0.0800
trigger times: 0
Loss after 439418100 batches: 0.0252
trigger times: 0
Loss after 439546740 batches: 0.0200
trigger times: 1
Loss after 439675380 batches: 0.0177
trigger times: 2
Loss after 439804020 batches: 0.0167
trigger times: 3
Loss after 439932660 batches: 0.0156
trigger times: 4
Loss after 440061300 batches: 0.0151
trigger times: 5
Loss after 440189940 batches: 0.0149
trigger times: 6
Loss after 440318580 batches: 0.0144
trigger times: 7
Loss after 440447220 batches: 0.0141
trigger times: 8
Loss after 440575860 batches: 0.0137
trigger times: 9
Loss after 440704500 batches: 0.0134
trigger times: 10
Loss after 440833140 batches: 0.0129
trigger times: 11
Loss after 440961780 batches: 0.0128
trigger times: 12
Loss after 441090420 batches: 0.0127
trigger times: 13
Loss after 441219060 batches: 0.0122
trigger times: 14
Loss after 441347700 batches: 0.0123
trigger times: 15
Loss after 441476340 batches: 0.0119
trigger times: 16
Loss after 441604980 batches: 0.0118
trigger times: 17
Loss after 441733620 batches: 0.0119
trigger times: 18
Loss after 441862260 batches: 0.0119
trigger times: 19
Loss after 441990900 batches: 0.0116
trigger times: 20
Early stopping!
Start to test process.
Loss after 442119540 batches: 0.0113
Time to train on one home:  171.24963784217834
trigger times: 0
Loss after 442250640 batches: 0.1410
trigger times: 0
Loss after 442381740 batches: 0.0358
trigger times: 1
Loss after 442512840 batches: 0.0285
trigger times: 0
Loss after 442643940 batches: 0.0259
trigger times: 0
Loss after 442775040 batches: 0.0243
trigger times: 0
Loss after 442906140 batches: 0.0229
trigger times: 0
Loss after 443037240 batches: 0.0223
trigger times: 1
Loss after 443168340 batches: 0.0216
trigger times: 2
Loss after 443299440 batches: 0.0209
trigger times: 3
Loss after 443430540 batches: 0.0204
trigger times: 0
Loss after 443561640 batches: 0.0201
trigger times: 0
Loss after 443692740 batches: 0.0197
trigger times: 1
Loss after 443823840 batches: 0.0196
trigger times: 2
Loss after 443954940 batches: 0.0190
trigger times: 0
Loss after 444086040 batches: 0.0189
trigger times: 1
Loss after 444217140 batches: 0.0187
trigger times: 2
Loss after 444348240 batches: 0.0183
trigger times: 3
Loss after 444479340 batches: 0.0181
trigger times: 4
Loss after 444610440 batches: 0.0179
trigger times: 5
Loss after 444741540 batches: 0.0180
trigger times: 0
Loss after 444872640 batches: 0.0177
trigger times: 1
Loss after 445003740 batches: 0.0175
trigger times: 2
Loss after 445134840 batches: 0.0174
trigger times: 0
Loss after 445265940 batches: 0.0172
trigger times: 1
Loss after 445397040 batches: 0.0173
trigger times: 2
Loss after 445528140 batches: 0.0171
trigger times: 3
Loss after 445659240 batches: 0.0170
trigger times: 4
Loss after 445790340 batches: 0.0167
trigger times: 5
Loss after 445921440 batches: 0.0169
trigger times: 6
Loss after 446052540 batches: 0.0167
trigger times: 0
Loss after 446183640 batches: 0.0166
trigger times: 1
Loss after 446314740 batches: 0.0163
trigger times: 2
Loss after 446445840 batches: 0.0163
trigger times: 3
Loss after 446576940 batches: 0.0161
trigger times: 4
Loss after 446708040 batches: 0.0161
trigger times: 5
Loss after 446839140 batches: 0.0162
trigger times: 6
Loss after 446970240 batches: 0.0161
trigger times: 7
Loss after 447101340 batches: 0.0160
trigger times: 8
Loss after 447232440 batches: 0.0157
trigger times: 0
Loss after 447363540 batches: 0.0157
trigger times: 1
Loss after 447494640 batches: 0.0156
trigger times: 2
Loss after 447625740 batches: 0.0156
trigger times: 3
Loss after 447756840 batches: 0.0158
trigger times: 4
Loss after 447887940 batches: 0.0155
trigger times: 5
Loss after 448019040 batches: 0.0157
trigger times: 6
Loss after 448150140 batches: 0.0156
trigger times: 7
Loss after 448281240 batches: 0.0152
trigger times: 8
Loss after 448412340 batches: 0.0155
trigger times: 9
Loss after 448543440 batches: 0.0152
trigger times: 10
Loss after 448674540 batches: 0.0152
trigger times: 11
Loss after 448805640 batches: 0.0152
trigger times: 12
Loss after 448936740 batches: 0.0152
trigger times: 13
Loss after 449067840 batches: 0.0151
trigger times: 14
Loss after 449198940 batches: 0.0149
trigger times: 0
Loss after 449330040 batches: 0.0148
trigger times: 1
Loss after 449461140 batches: 0.0150
trigger times: 2
Loss after 449592240 batches: 0.0148
trigger times: 3
Loss after 449723340 batches: 0.0148
trigger times: 4
Loss after 449854440 batches: 0.0147
trigger times: 5
Loss after 449985540 batches: 0.0146
trigger times: 0
Loss after 450116640 batches: 0.0147
trigger times: 1
Loss after 450247740 batches: 0.0145
trigger times: 2
Loss after 450378840 batches: 0.0145
trigger times: 3
Loss after 450509940 batches: 0.0144
trigger times: 4
Loss after 450641040 batches: 0.0146
trigger times: 5
Loss after 450772140 batches: 0.0145
trigger times: 6
Loss after 450903240 batches: 0.0145
trigger times: 7
Loss after 451034340 batches: 0.0145
trigger times: 8
Loss after 451165440 batches: 0.0142
trigger times: 9
Loss after 451296540 batches: 0.0142
trigger times: 10
Loss after 451427640 batches: 0.0144
trigger times: 11
Loss after 451558740 batches: 0.0142
trigger times: 12
Loss after 451689840 batches: 0.0140
trigger times: 13
Loss after 451820940 batches: 0.0142
trigger times: 14
Loss after 451952040 batches: 0.0141
trigger times: 15
Loss after 452083140 batches: 0.0139
trigger times: 16
Loss after 452214240 batches: 0.0141
trigger times: 17
Loss after 452345340 batches: 0.0138
trigger times: 18
Loss after 452476440 batches: 0.0138
trigger times: 19
Loss after 452607540 batches: 0.0139
trigger times: 20
Early stopping!
Start to test process.
Loss after 452738640 batches: 0.0138
Time to train on one home:  582.8235716819763
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146, 0.01617968764306582, 0.015298912286208879, 0.01494286251164481, 0.014533454579010391, 0.015354135860208057, 0.014391380102152193]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638], [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354], [0.5489356054200066, 0.40660235248365517, 0.4824471426162714, 1.021275384526291, 0.4861081410983952, 24.12799700883127, 1500.6461], [0.5518252385987176, 0.40347636543781107, 0.4779622827153174, 1.0271923671495102, 0.4886689327670392, 24.267787844092187, 1508.5514], [0.5432945456769731, 0.41270949933158785, 0.48664830013661614, 1.0178919743034478, 0.48110519945531827, 24.048062729621872, 1485.2018], [0.528244235449367, 0.4289907862839557, 0.498210151533106, 1.0003101691077434, 0.4677676573059164, 23.63268628013604, 1444.0278], [0.5302241428030862, 0.4268514023330586, 0.4936126497092288, 0.9992097088225125, 0.4695202290591394, 23.60668751146631, 1449.4382]]
Round_13_results:  [0.5302241428030862, 0.4268514023330586, 0.4936126497092288, 0.9992097088225125, 0.4695202290591394, 23.60668751146631, 1449.4382]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3550 < 3551; dropping {'Training_Loss': 0.08413042029682195, 'Validation_Loss': 0.21223664283752441, 'Training_R2': 0.9152552327069018, 'Validation_R2': 0.8027207105675032, 'Training_F1': 0.8451675947158883, 'Validation_F1': 0.7475539081674031, 'Training_NEP': 0.30949735522134125, 'Validation_NEP': 0.4891816238363961, 'Training_NDE': 0.06361940102887963, 'Validation_NDE': 0.15710282670196568, 'Training_MAE': 10.249938605488211, 'Validation_MAE': 13.41538961583141, 'Training_MSE': 279.9154, 'Validation_MSE': 580.17645}.
trigger times: 0
Loss after 452869740 batches: 0.0841
trigger times: 0
Loss after 453000840 batches: 0.0223
trigger times: 0
Loss after 453131940 batches: 0.0178
trigger times: 0
Loss after 453263040 batches: 0.0154
trigger times: 1
Loss after 453394140 batches: 0.0141
trigger times: 2
Loss after 453525240 batches: 0.0136
trigger times: 3
Loss after 453656340 batches: 0.0131
trigger times: 4
Loss after 453787440 batches: 0.0125
trigger times: 0
Loss after 453918540 batches: 0.0120
trigger times: 0
Loss after 454049640 batches: 0.0120
trigger times: 1
Loss after 454180740 batches: 0.0117
trigger times: 2
Loss after 454311840 batches: 0.0112
trigger times: 3
Loss after 454442940 batches: 0.0109
trigger times: 4
Loss after 454574040 batches: 0.0107
trigger times: 5
Loss after 454705140 batches: 0.0106
trigger times: 6
Loss after 454836240 batches: 0.0104
trigger times: 7
Loss after 454967340 batches: 0.0102
trigger times: 8
Loss after 455098440 batches: 0.0101
trigger times: 9
Loss after 455229540 batches: 0.0099
trigger times: 10
Loss after 455360640 batches: 0.0098
trigger times: 11
Loss after 455491740 batches: 0.0099
trigger times: 12
Loss after 455622840 batches: 0.0095
trigger times: 13
Loss after 455753940 batches: 0.0096
trigger times: 14
Loss after 455885040 batches: 0.0094
trigger times: 15
Loss after 456016140 batches: 0.0094
trigger times: 16
Loss after 456147240 batches: 0.0092
trigger times: 17
Loss after 456278340 batches: 0.0092
trigger times: 18
Loss after 456409440 batches: 0.0091
trigger times: 19
Loss after 456540540 batches: 0.0090
trigger times: 20
Early stopping!
Start to test process.
Loss after 456671640 batches: 0.0095
Time to train on one home:  223.41289854049683
trigger times: 0
Loss after 456774240 batches: 0.1843
trigger times: 1
Loss after 456876840 batches: 0.0616
trigger times: 2
Loss after 456979440 batches: 0.0416
trigger times: 3
Loss after 457082040 batches: 0.0364
trigger times: 4
Loss after 457184640 batches: 0.0335
trigger times: 5
Loss after 457287240 batches: 0.0323
trigger times: 6
Loss after 457389840 batches: 0.0288
trigger times: 7
Loss after 457492440 batches: 0.0266
trigger times: 8
Loss after 457595040 batches: 0.0290
trigger times: 9
Loss after 457697640 batches: 0.0319
trigger times: 10
Loss after 457800240 batches: 0.0261
trigger times: 11
Loss after 457902840 batches: 0.0255
trigger times: 12
Loss after 458005440 batches: 0.0239
trigger times: 13
Loss after 458108040 batches: 0.0258
trigger times: 14
Loss after 458210640 batches: 0.0239
trigger times: 15
Loss after 458313240 batches: 0.0222
trigger times: 16
Loss after 458415840 batches: 0.0220
trigger times: 17
Loss after 458518440 batches: 0.0225
trigger times: 18
Loss after 458621040 batches: 0.0212
trigger times: 19
Loss after 458723640 batches: 0.0213
trigger times: 20
Early stopping!
Start to test process.
Loss after 458826240 batches: 0.0203
Time to train on one home:  132.14986562728882
trigger times: 0
Loss after 458957340 batches: 0.0874
trigger times: 1
Loss after 459088440 batches: 0.0341
trigger times: 2
Loss after 459219540 batches: 0.0274
trigger times: 3
Loss after 459350640 batches: 0.0245
trigger times: 4
Loss after 459481740 batches: 0.0230
trigger times: 5
Loss after 459612840 batches: 0.0217
trigger times: 6
Loss after 459743940 batches: 0.0209
trigger times: 7
Loss after 459875040 batches: 0.0200
trigger times: 8
Loss after 460006140 batches: 0.0196
trigger times: 9
Loss after 460137240 batches: 0.0187
trigger times: 10
Loss after 460268340 batches: 0.0184
trigger times: 11
Loss after 460399440 batches: 0.0179
trigger times: 12
Loss after 460530540 batches: 0.0176
trigger times: 13
Loss after 460661640 batches: 0.0175
trigger times: 14
Loss after 460792740 batches: 0.0173
trigger times: 15
Loss after 460923840 batches: 0.0169
trigger times: 16
Loss after 461054940 batches: 0.0167
trigger times: 17
Loss after 461186040 batches: 0.0161
trigger times: 18
Loss after 461317140 batches: 0.0160
trigger times: 19
Loss after 461448240 batches: 0.0160
trigger times: 20
Early stopping!
Start to test process.
Loss after 461579340 batches: 0.0157
Time to train on one home:  160.10145235061646
trigger times: 0
Loss after 461710440 batches: 0.1215
trigger times: 0
Loss after 461841540 batches: 0.0377
trigger times: 1
Loss after 461972640 batches: 0.0296
trigger times: 0
Loss after 462103740 batches: 0.0266
trigger times: 0
Loss after 462234840 batches: 0.0249
trigger times: 0
Loss after 462365940 batches: 0.0235
trigger times: 1
Loss after 462497040 batches: 0.0227
trigger times: 2
Loss after 462628140 batches: 0.0218
trigger times: 3
Loss after 462759240 batches: 0.0212
trigger times: 4
Loss after 462890340 batches: 0.0209
trigger times: 0
Loss after 463021440 batches: 0.0204
trigger times: 0
Loss after 463152540 batches: 0.0201
trigger times: 1
Loss after 463283640 batches: 0.0199
trigger times: 0
Loss after 463414740 batches: 0.0197
trigger times: 1
Loss after 463545840 batches: 0.0191
trigger times: 0
Loss after 463676940 batches: 0.0192
trigger times: 1
Loss after 463808040 batches: 0.0186
trigger times: 2
Loss after 463939140 batches: 0.0186
trigger times: 3
Loss after 464070240 batches: 0.0183
trigger times: 4
Loss after 464201340 batches: 0.0184
trigger times: 5
Loss after 464332440 batches: 0.0180
trigger times: 6
Loss after 464463540 batches: 0.0177
trigger times: 7
Loss after 464594640 batches: 0.0178
trigger times: 8
Loss after 464725740 batches: 0.0177
trigger times: 9
Loss after 464856840 batches: 0.0176
trigger times: 10
Loss after 464987940 batches: 0.0176
trigger times: 11
Loss after 465119040 batches: 0.0173
trigger times: 12
Loss after 465250140 batches: 0.0172
trigger times: 0
Loss after 465381240 batches: 0.0171
trigger times: 1
Loss after 465512340 batches: 0.0167
trigger times: 2
Loss after 465643440 batches: 0.0168
trigger times: 3
Loss after 465774540 batches: 0.0171
trigger times: 4
Loss after 465905640 batches: 0.0165
trigger times: 5
Loss after 466036740 batches: 0.0167
trigger times: 6
Loss after 466167840 batches: 0.0165
trigger times: 7
Loss after 466298940 batches: 0.0164
trigger times: 8
Loss after 466430040 batches: 0.0164
trigger times: 0
Loss after 466561140 batches: 0.0165
trigger times: 1
Loss after 466692240 batches: 0.0165
trigger times: 2
Loss after 466823340 batches: 0.0163
trigger times: 3
Loss after 466954440 batches: 0.0164
trigger times: 4
Loss after 467085540 batches: 0.0160
trigger times: 5
Loss after 467216640 batches: 0.0159
trigger times: 6
Loss after 467347740 batches: 0.0160
trigger times: 7
Loss after 467478840 batches: 0.0158
trigger times: 8
Loss after 467609940 batches: 0.0159
trigger times: 9
Loss after 467741040 batches: 0.0158
trigger times: 10
Loss after 467872140 batches: 0.0160
trigger times: 11
Loss after 468003240 batches: 0.0158
trigger times: 12
Loss after 468134340 batches: 0.0157
trigger times: 13
Loss after 468265440 batches: 0.0156
trigger times: 14
Loss after 468396540 batches: 0.0155
trigger times: 15
Loss after 468527640 batches: 0.0154
trigger times: 16
Loss after 468658740 batches: 0.0154
trigger times: 17
Loss after 468789840 batches: 0.0152
trigger times: 18
Loss after 468920940 batches: 0.0153
trigger times: 19
Loss after 469052040 batches: 0.0151
trigger times: 20
Early stopping!
Start to test process.
Loss after 469183140 batches: 0.0152
Time to train on one home:  421.7649064064026
trigger times: 0
Loss after 469311780 batches: 0.0778
trigger times: 0
Loss after 469440420 batches: 0.0243
trigger times: 1
Loss after 469569060 batches: 0.0198
trigger times: 2
Loss after 469697700 batches: 0.0178
trigger times: 3
Loss after 469826340 batches: 0.0166
trigger times: 4
Loss after 469954980 batches: 0.0157
trigger times: 5
Loss after 470083620 batches: 0.0153
trigger times: 6
Loss after 470212260 batches: 0.0147
trigger times: 7
Loss after 470340900 batches: 0.0144
trigger times: 8
Loss after 470469540 batches: 0.0138
trigger times: 9
Loss after 470598180 batches: 0.0133
trigger times: 10
Loss after 470726820 batches: 0.0131
trigger times: 11
Loss after 470855460 batches: 0.0128
trigger times: 12
Loss after 470984100 batches: 0.0125
trigger times: 13
Loss after 471112740 batches: 0.0124
trigger times: 14
Loss after 471241380 batches: 0.0123
trigger times: 15
Loss after 471370020 batches: 0.0121
trigger times: 16
Loss after 471498660 batches: 0.0123
trigger times: 17
Loss after 471627300 batches: 0.0120
trigger times: 18
Loss after 471755940 batches: 0.0117
trigger times: 19
Loss after 471884580 batches: 0.0114
trigger times: 20
Early stopping!
Start to test process.
Loss after 472013220 batches: 0.0118
Time to train on one home:  164.06792092323303
trigger times: 0
Loss after 472144320 batches: 0.1283
trigger times: 1
Loss after 472275420 batches: 0.0347
trigger times: 0
Loss after 472406520 batches: 0.0277
trigger times: 1
Loss after 472537620 batches: 0.0245
trigger times: 2
Loss after 472668720 batches: 0.0230
trigger times: 3
Loss after 472799820 batches: 0.0221
trigger times: 4
Loss after 472930920 batches: 0.0210
trigger times: 5
Loss after 473062020 batches: 0.0203
trigger times: 6
Loss after 473193120 batches: 0.0199
trigger times: 0
Loss after 473324220 batches: 0.0197
trigger times: 1
Loss after 473455320 batches: 0.0192
trigger times: 2
Loss after 473586420 batches: 0.0190
trigger times: 3
Loss after 473717520 batches: 0.0184
trigger times: 4
Loss after 473848620 batches: 0.0182
trigger times: 0
Loss after 473979720 batches: 0.0182
trigger times: 1
Loss after 474110820 batches: 0.0181
trigger times: 2
Loss after 474241920 batches: 0.0177
trigger times: 3
Loss after 474373020 batches: 0.0174
trigger times: 4
Loss after 474504120 batches: 0.0174
trigger times: 5
Loss after 474635220 batches: 0.0169
trigger times: 6
Loss after 474766320 batches: 0.0169
trigger times: 7
Loss after 474897420 batches: 0.0166
trigger times: 8
Loss after 475028520 batches: 0.0166
trigger times: 9
Loss after 475159620 batches: 0.0167
trigger times: 10
Loss after 475290720 batches: 0.0165
trigger times: 11
Loss after 475421820 batches: 0.0165
trigger times: 12
Loss after 475552920 batches: 0.0162
trigger times: 13
Loss after 475684020 batches: 0.0163
trigger times: 14
Loss after 475815120 batches: 0.0160
trigger times: 15
Loss after 475946220 batches: 0.0159
trigger times: 16
Loss after 476077320 batches: 0.0158
trigger times: 17
Loss after 476208420 batches: 0.0157
trigger times: 18
Loss after 476339520 batches: 0.0160
trigger times: 19
Loss after 476470620 batches: 0.0157
trigger times: 20
Early stopping!
Start to test process.
Loss after 476601720 batches: 0.0156
Time to train on one home:  258.66550493240356
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146, 0.01617968764306582, 0.015298912286208879, 0.01494286251164481, 0.014533454579010391, 0.015354135860208057, 0.014391380102152193, 0.01467373398194618]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638], [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354], [0.5489356054200066, 0.40660235248365517, 0.4824471426162714, 1.021275384526291, 0.4861081410983952, 24.12799700883127, 1500.6461], [0.5518252385987176, 0.40347636543781107, 0.4779622827153174, 1.0271923671495102, 0.4886689327670392, 24.267787844092187, 1508.5514], [0.5432945456769731, 0.41270949933158785, 0.48664830013661614, 1.0178919743034478, 0.48110519945531827, 24.048062729621872, 1485.2018], [0.528244235449367, 0.4289907862839557, 0.498210151533106, 1.0003101691077434, 0.4677676573059164, 23.63268628013604, 1444.0278], [0.5302241428030862, 0.4268514023330586, 0.4936126497092288, 0.9992097088225125, 0.4695202290591394, 23.60668751146631, 1449.4382], [0.5239716635810004, 0.4335993694666116, 0.5025112735686915, 0.997230007025717, 0.46399233090650266, 23.55991634694479, 1432.3732]]
Round_14_results:  [0.5239716635810004, 0.4335993694666116, 0.5025112735686915, 0.997230007025717, 0.46399233090650266, 23.55991634694479, 1432.3732]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3737 < 3738; dropping {'Training_Loss': 0.08244044509417606, 'Validation_Loss': 0.21513832608858743, 'Training_R2': 0.916896896536085, 'Validation_R2': 0.8000187684680726, 'Training_F1': 0.8464247763306609, 'Validation_F1': 0.7500391284498912, 'Training_NEP': 0.3071427667908473, 'Validation_NEP': 0.48274105534188816, 'Training_NDE': 0.06238697485273349, 'Validation_NDE': 0.15925451095948023, 'Training_MAE': 10.171959306322647, 'Validation_MAE': 13.238762507430113, 'Training_MSE': 274.49295, 'Validation_MSE': 588.1225}.
trigger times: 0
Loss after 476732820 batches: 0.0824
trigger times: 0
Loss after 476863920 batches: 0.0216
trigger times: 1
Loss after 476995020 batches: 0.0165
trigger times: 0
Loss after 477126120 batches: 0.0150
trigger times: 1
Loss after 477257220 batches: 0.0141
trigger times: 0
Loss after 477388320 batches: 0.0130
trigger times: 1
Loss after 477519420 batches: 0.0124
trigger times: 2
Loss after 477650520 batches: 0.0123
trigger times: 0
Loss after 477781620 batches: 0.0118
trigger times: 1
Loss after 477912720 batches: 0.0116
trigger times: 2
Loss after 478043820 batches: 0.0114
trigger times: 3
Loss after 478174920 batches: 0.0109
trigger times: 4
Loss after 478306020 batches: 0.0108
trigger times: 5
Loss after 478437120 batches: 0.0105
trigger times: 6
Loss after 478568220 batches: 0.0103
trigger times: 7
Loss after 478699320 batches: 0.0102
trigger times: 8
Loss after 478830420 batches: 0.0100
trigger times: 9
Loss after 478961520 batches: 0.0100
trigger times: 10
Loss after 479092620 batches: 0.0100
trigger times: 11
Loss after 479223720 batches: 0.0099
trigger times: 12
Loss after 479354820 batches: 0.0097
trigger times: 13
Loss after 479485920 batches: 0.0098
trigger times: 14
Loss after 479617020 batches: 0.0094
trigger times: 15
Loss after 479748120 batches: 0.0096
trigger times: 16
Loss after 479879220 batches: 0.0092
trigger times: 17
Loss after 480010320 batches: 0.0091
trigger times: 18
Loss after 480141420 batches: 0.0091
trigger times: 19
Loss after 480272520 batches: 0.0091
trigger times: 20
Early stopping!
Start to test process.
Loss after 480403620 batches: 0.0088
Time to train on one home:  216.45563530921936
trigger times: 0
Loss after 480506220 batches: 0.1855
trigger times: 0
Loss after 480608820 batches: 0.0514
trigger times: 1
Loss after 480711420 batches: 0.0386
trigger times: 2
Loss after 480814020 batches: 0.0340
trigger times: 3
Loss after 480916620 batches: 0.0334
trigger times: 4
Loss after 481019220 batches: 0.0351
trigger times: 5
Loss after 481121820 batches: 0.0296
trigger times: 6
Loss after 481224420 batches: 0.0289
trigger times: 7
Loss after 481327020 batches: 0.0283
trigger times: 8
Loss after 481429620 batches: 0.0266
trigger times: 9
Loss after 481532220 batches: 0.0268
trigger times: 10
Loss after 481634820 batches: 0.0324
trigger times: 11
Loss after 481737420 batches: 0.0266
trigger times: 12
Loss after 481840020 batches: 0.0254
trigger times: 13
Loss after 481942620 batches: 0.0238
trigger times: 14
Loss after 482045220 batches: 0.0229
trigger times: 15
Loss after 482147820 batches: 0.0229
trigger times: 16
Loss after 482250420 batches: 0.0211
trigger times: 17
Loss after 482353020 batches: 0.0202
trigger times: 18
Loss after 482455620 batches: 0.0199
trigger times: 19
Loss after 482558220 batches: 0.0203
trigger times: 20
Early stopping!
Start to test process.
Loss after 482660820 batches: 0.0199
Time to train on one home:  137.82092595100403
trigger times: 0
Loss after 482791920 batches: 0.0921
trigger times: 0
Loss after 482923020 batches: 0.0333
trigger times: 1
Loss after 483054120 batches: 0.0267
trigger times: 2
Loss after 483185220 batches: 0.0242
trigger times: 3
Loss after 483316320 batches: 0.0221
trigger times: 4
Loss after 483447420 batches: 0.0214
trigger times: 5
Loss after 483578520 batches: 0.0201
trigger times: 6
Loss after 483709620 batches: 0.0200
trigger times: 7
Loss after 483840720 batches: 0.0193
trigger times: 8
Loss after 483971820 batches: 0.0190
trigger times: 9
Loss after 484102920 batches: 0.0182
trigger times: 10
Loss after 484234020 batches: 0.0181
trigger times: 11
Loss after 484365120 batches: 0.0174
trigger times: 12
Loss after 484496220 batches: 0.0171
trigger times: 13
Loss after 484627320 batches: 0.0168
trigger times: 14
Loss after 484758420 batches: 0.0165
trigger times: 15
Loss after 484889520 batches: 0.0165
trigger times: 16
Loss after 485020620 batches: 0.0161
trigger times: 17
Loss after 485151720 batches: 0.0160
trigger times: 18
Loss after 485282820 batches: 0.0157
trigger times: 19
Loss after 485413920 batches: 0.0154
trigger times: 20
Early stopping!
Start to test process.
Loss after 485545020 batches: 0.0155
Time to train on one home:  166.53386211395264
trigger times: 0
Loss after 485676120 batches: 0.1091
trigger times: 0
Loss after 485807220 batches: 0.0344
trigger times: 0
Loss after 485938320 batches: 0.0273
trigger times: 1
Loss after 486069420 batches: 0.0251
trigger times: 2
Loss after 486200520 batches: 0.0239
trigger times: 0
Loss after 486331620 batches: 0.0227
trigger times: 1
Loss after 486462720 batches: 0.0219
trigger times: 2
Loss after 486593820 batches: 0.0213
trigger times: 3
Loss after 486724920 batches: 0.0209
trigger times: 4
Loss after 486856020 batches: 0.0206
trigger times: 5
Loss after 486987120 batches: 0.0198
trigger times: 6
Loss after 487118220 batches: 0.0195
trigger times: 7
Loss after 487249320 batches: 0.0195
trigger times: 8
Loss after 487380420 batches: 0.0190
trigger times: 9
Loss after 487511520 batches: 0.0188
trigger times: 10
Loss after 487642620 batches: 0.0182
trigger times: 11
Loss after 487773720 batches: 0.0180
trigger times: 12
Loss after 487904820 batches: 0.0182
trigger times: 13
Loss after 488035920 batches: 0.0176
trigger times: 14
Loss after 488167020 batches: 0.0178
trigger times: 15
Loss after 488298120 batches: 0.0174
trigger times: 16
Loss after 488429220 batches: 0.0174
trigger times: 17
Loss after 488560320 batches: 0.0174
trigger times: 18
Loss after 488691420 batches: 0.0171
trigger times: 19
Loss after 488822520 batches: 0.0172
trigger times: 20
Early stopping!
Start to test process.
Loss after 488953620 batches: 0.0172
Time to train on one home:  195.13041281700134
trigger times: 0
Loss after 489082260 batches: 0.0816
trigger times: 0
Loss after 489210900 batches: 0.0241
trigger times: 0
Loss after 489339540 batches: 0.0191
trigger times: 1
Loss after 489468180 batches: 0.0175
trigger times: 2
Loss after 489596820 batches: 0.0162
trigger times: 3
Loss after 489725460 batches: 0.0155
trigger times: 4
Loss after 489854100 batches: 0.0149
trigger times: 5
Loss after 489982740 batches: 0.0143
trigger times: 6
Loss after 490111380 batches: 0.0137
trigger times: 7
Loss after 490240020 batches: 0.0137
trigger times: 8
Loss after 490368660 batches: 0.0133
trigger times: 9
Loss after 490497300 batches: 0.0131
trigger times: 10
Loss after 490625940 batches: 0.0127
trigger times: 11
Loss after 490754580 batches: 0.0126
trigger times: 12
Loss after 490883220 batches: 0.0125
trigger times: 13
Loss after 491011860 batches: 0.0124
trigger times: 14
Loss after 491140500 batches: 0.0120
trigger times: 15
Loss after 491269140 batches: 0.0120
trigger times: 16
Loss after 491397780 batches: 0.0116
trigger times: 17
Loss after 491526420 batches: 0.0116
trigger times: 18
Loss after 491655060 batches: 0.0114
trigger times: 19
Loss after 491783700 batches: 0.0114
trigger times: 20
Early stopping!
Start to test process.
Loss after 491912340 batches: 0.0116
Time to train on one home:  171.79931235313416
trigger times: 0
Loss after 492043440 batches: 0.1307
trigger times: 0
Loss after 492174540 batches: 0.0342
trigger times: 1
Loss after 492305640 batches: 0.0267
trigger times: 2
Loss after 492436740 batches: 0.0239
trigger times: 0
Loss after 492567840 batches: 0.0227
trigger times: 0
Loss after 492698940 batches: 0.0221
trigger times: 1
Loss after 492830040 batches: 0.0215
trigger times: 2
Loss after 492961140 batches: 0.0205
trigger times: 0
Loss after 493092240 batches: 0.0197
trigger times: 1
Loss after 493223340 batches: 0.0192
trigger times: 2
Loss after 493354440 batches: 0.0190
trigger times: 3
Loss after 493485540 batches: 0.0186
trigger times: 4
Loss after 493616640 batches: 0.0185
trigger times: 0
Loss after 493747740 batches: 0.0182
trigger times: 0
Loss after 493878840 batches: 0.0180
trigger times: 1
Loss after 494009940 batches: 0.0180
trigger times: 0
Loss after 494141040 batches: 0.0176
trigger times: 0
Loss after 494272140 batches: 0.0176
trigger times: 1
Loss after 494403240 batches: 0.0172
trigger times: 2
Loss after 494534340 batches: 0.0170
trigger times: 3
Loss after 494665440 batches: 0.0168
trigger times: 4
Loss after 494796540 batches: 0.0169
trigger times: 5
Loss after 494927640 batches: 0.0169
trigger times: 6
Loss after 495058740 batches: 0.0167
trigger times: 7
Loss after 495189840 batches: 0.0164
trigger times: 8
Loss after 495320940 batches: 0.0162
trigger times: 0
Loss after 495452040 batches: 0.0163
trigger times: 0
Loss after 495583140 batches: 0.0160
trigger times: 1
Loss after 495714240 batches: 0.0161
trigger times: 2
Loss after 495845340 batches: 0.0162
trigger times: 3
Loss after 495976440 batches: 0.0161
trigger times: 4
Loss after 496107540 batches: 0.0159
trigger times: 5
Loss after 496238640 batches: 0.0157
trigger times: 6
Loss after 496369740 batches: 0.0156
trigger times: 7
Loss after 496500840 batches: 0.0154
trigger times: 8
Loss after 496631940 batches: 0.0154
trigger times: 9
Loss after 496763040 batches: 0.0154
trigger times: 10
Loss after 496894140 batches: 0.0155
trigger times: 11
Loss after 497025240 batches: 0.0155
trigger times: 12
Loss after 497156340 batches: 0.0153
trigger times: 13
Loss after 497287440 batches: 0.0154
trigger times: 14
Loss after 497418540 batches: 0.0151
trigger times: 15
Loss after 497549640 batches: 0.0150
trigger times: 16
Loss after 497680740 batches: 0.0151
trigger times: 0
Loss after 497811840 batches: 0.0150
trigger times: 1
Loss after 497942940 batches: 0.0149
trigger times: 2
Loss after 498074040 batches: 0.0146
trigger times: 3
Loss after 498205140 batches: 0.0147
trigger times: 4
Loss after 498336240 batches: 0.0148
trigger times: 5
Loss after 498467340 batches: 0.0148
trigger times: 6
Loss after 498598440 batches: 0.0149
trigger times: 7
Loss after 498729540 batches: 0.0150
trigger times: 8
Loss after 498860640 batches: 0.0146
trigger times: 9
Loss after 498991740 batches: 0.0145
trigger times: 10
Loss after 499122840 batches: 0.0145
trigger times: 11
Loss after 499253940 batches: 0.0144
trigger times: 12
Loss after 499385040 batches: 0.0143
trigger times: 13
Loss after 499516140 batches: 0.0142
trigger times: 14
Loss after 499647240 batches: 0.0142
trigger times: 15
Loss after 499778340 batches: 0.0140
trigger times: 16
Loss after 499909440 batches: 0.0142
trigger times: 17
Loss after 500040540 batches: 0.0140
trigger times: 18
Loss after 500171640 batches: 0.0140
trigger times: 19
Loss after 500302740 batches: 0.0139
trigger times: 20
Early stopping!
Start to test process.
Loss after 500433840 batches: 0.0141
Time to train on one home:  471.0096740722656
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146, 0.01617968764306582, 0.015298912286208879, 0.01494286251164481, 0.014533454579010391, 0.015354135860208057, 0.014391380102152193, 0.01467373398194618, 0.014497973545955688]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638], [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354], [0.5489356054200066, 0.40660235248365517, 0.4824471426162714, 1.021275384526291, 0.4861081410983952, 24.12799700883127, 1500.6461], [0.5518252385987176, 0.40347636543781107, 0.4779622827153174, 1.0271923671495102, 0.4886689327670392, 24.267787844092187, 1508.5514], [0.5432945456769731, 0.41270949933158785, 0.48664830013661614, 1.0178919743034478, 0.48110519945531827, 24.048062729621872, 1485.2018], [0.528244235449367, 0.4289907862839557, 0.498210151533106, 1.0003101691077434, 0.4677676573059164, 23.63268628013604, 1444.0278], [0.5302241428030862, 0.4268514023330586, 0.4936126497092288, 0.9992097088225125, 0.4695202290591394, 23.60668751146631, 1449.4382], [0.5239716635810004, 0.4335993694666116, 0.5025112735686915, 0.997230007025717, 0.46399233090650266, 23.55991634694479, 1432.3732], [0.5264167802201377, 0.4310038387187667, 0.5013357099896009, 1.002685768816802, 0.46611857564690545, 23.688810674734015, 1438.9371]]
Round_15_results:  [0.5264167802201377, 0.4310038387187667, 0.5013357099896009, 1.002685768816802, 0.46611857564690545, 23.688810674734015, 1438.9371]
trigger times: 0
Loss after 500564940 batches: 0.0840
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3924 < 3925; dropping {'Training_Loss': 0.08403151426112877, 'Validation_Loss': 0.20337754570775562, 'Training_R2': 0.9152813028710978, 'Validation_R2': 0.8109603719602504, 'Training_F1': 0.8464043390602036, 'Validation_F1': 0.7647543937137857, 'Training_NEP': 0.3071133637418222, 'Validation_NEP': 0.46440731456125095, 'Training_NDE': 0.06359982969387137, 'Validation_NDE': 0.15054119471519503, 'Training_MAE': 10.170985535651468, 'Validation_MAE': 12.735975273193851, 'Training_MSE': 279.82925, 'Validation_MSE': 555.94446}.
trigger times: 0
Loss after 500696040 batches: 0.0207
trigger times: 0
Loss after 500827140 batches: 0.0166
trigger times: 0
Loss after 500958240 batches: 0.0149
trigger times: 0
Loss after 501089340 batches: 0.0137
trigger times: 1
Loss after 501220440 batches: 0.0131
trigger times: 0
Loss after 501351540 batches: 0.0123
trigger times: 1
Loss after 501482640 batches: 0.0121
trigger times: 2
Loss after 501613740 batches: 0.0118
trigger times: 3
Loss after 501744840 batches: 0.0113
trigger times: 4
Loss after 501875940 batches: 0.0110
trigger times: 5
Loss after 502007040 batches: 0.0107
trigger times: 6
Loss after 502138140 batches: 0.0106
trigger times: 7
Loss after 502269240 batches: 0.0103
trigger times: 8
Loss after 502400340 batches: 0.0102
trigger times: 9
Loss after 502531440 batches: 0.0103
trigger times: 10
Loss after 502662540 batches: 0.0100
trigger times: 11
Loss after 502793640 batches: 0.0100
trigger times: 12
Loss after 502924740 batches: 0.0098
trigger times: 13
Loss after 503055840 batches: 0.0100
trigger times: 14
Loss after 503186940 batches: 0.0096
trigger times: 15
Loss after 503318040 batches: 0.0092
trigger times: 16
Loss after 503449140 batches: 0.0094
trigger times: 17
Loss after 503580240 batches: 0.0091
trigger times: 18
Loss after 503711340 batches: 0.0094
trigger times: 19
Loss after 503842440 batches: 0.0091
trigger times: 20
Early stopping!
Start to test process.
Loss after 503973540 batches: 0.0091
Time to train on one home:  201.64507961273193
trigger times: 0
Loss after 504076140 batches: 0.1757
trigger times: 1
Loss after 504178740 batches: 0.0586
trigger times: 2
Loss after 504281340 batches: 0.0423
trigger times: 3
Loss after 504383940 batches: 0.0362
trigger times: 4
Loss after 504486540 batches: 0.0328
trigger times: 5
Loss after 504589140 batches: 0.0293
trigger times: 6
Loss after 504691740 batches: 0.0289
trigger times: 7
Loss after 504794340 batches: 0.0290
trigger times: 8
Loss after 504896940 batches: 0.0306
trigger times: 9
Loss after 504999540 batches: 0.0260
trigger times: 10
Loss after 505102140 batches: 0.0240
trigger times: 11
Loss after 505204740 batches: 0.0250
trigger times: 12
Loss after 505307340 batches: 0.0254
trigger times: 13
Loss after 505409940 batches: 0.0237
trigger times: 14
Loss after 505512540 batches: 0.0226
trigger times: 15
Loss after 505615140 batches: 0.0222
trigger times: 16
Loss after 505717740 batches: 0.0222
trigger times: 17
Loss after 505820340 batches: 0.0231
trigger times: 18
Loss after 505922940 batches: 0.0228
trigger times: 19
Loss after 506025540 batches: 0.0228
trigger times: 20
Early stopping!
Start to test process.
Loss after 506128140 batches: 0.0217
Time to train on one home:  131.674950838089
trigger times: 0
Loss after 506259240 batches: 0.0801
trigger times: 1
Loss after 506390340 batches: 0.0311
trigger times: 2
Loss after 506521440 batches: 0.0257
trigger times: 0
Loss after 506652540 batches: 0.0233
trigger times: 1
Loss after 506783640 batches: 0.0220
trigger times: 2
Loss after 506914740 batches: 0.0208
trigger times: 3
Loss after 507045840 batches: 0.0199
trigger times: 4
Loss after 507176940 batches: 0.0195
trigger times: 5
Loss after 507308040 batches: 0.0188
trigger times: 6
Loss after 507439140 batches: 0.0183
trigger times: 7
Loss after 507570240 batches: 0.0179
trigger times: 8
Loss after 507701340 batches: 0.0177
trigger times: 9
Loss after 507832440 batches: 0.0170
trigger times: 10
Loss after 507963540 batches: 0.0168
trigger times: 11
Loss after 508094640 batches: 0.0165
trigger times: 12
Loss after 508225740 batches: 0.0162
trigger times: 13
Loss after 508356840 batches: 0.0162
trigger times: 14
Loss after 508487940 batches: 0.0160
trigger times: 15
Loss after 508619040 batches: 0.0156
trigger times: 16
Loss after 508750140 batches: 0.0157
trigger times: 17
Loss after 508881240 batches: 0.0153
trigger times: 18
Loss after 509012340 batches: 0.0153
trigger times: 19
Loss after 509143440 batches: 0.0150
trigger times: 20
Early stopping!
Start to test process.
Loss after 509274540 batches: 0.0148
Time to train on one home:  180.75957202911377
trigger times: 0
Loss after 509405640 batches: 0.1294
trigger times: 0
Loss after 509536740 batches: 0.0346
trigger times: 0
Loss after 509667840 batches: 0.0281
trigger times: 1
Loss after 509798940 batches: 0.0252
trigger times: 0
Loss after 509930040 batches: 0.0238
trigger times: 1
Loss after 510061140 batches: 0.0228
trigger times: 0
Loss after 510192240 batches: 0.0222
trigger times: 1
Loss after 510323340 batches: 0.0214
trigger times: 2
Loss after 510454440 batches: 0.0210
trigger times: 3
Loss after 510585540 batches: 0.0204
trigger times: 4
Loss after 510716640 batches: 0.0199
trigger times: 5
Loss after 510847740 batches: 0.0197
trigger times: 6
Loss after 510978840 batches: 0.0192
trigger times: 7
Loss after 511109940 batches: 0.0192
trigger times: 8
Loss after 511241040 batches: 0.0190
trigger times: 9
Loss after 511372140 batches: 0.0191
trigger times: 10
Loss after 511503240 batches: 0.0186
trigger times: 11
Loss after 511634340 batches: 0.0186
trigger times: 12
Loss after 511765440 batches: 0.0180
trigger times: 13
Loss after 511896540 batches: 0.0181
trigger times: 14
Loss after 512027640 batches: 0.0176
trigger times: 15
Loss after 512158740 batches: 0.0177
trigger times: 16
Loss after 512289840 batches: 0.0174
trigger times: 17
Loss after 512420940 batches: 0.0174
trigger times: 0
Loss after 512552040 batches: 0.0171
trigger times: 1
Loss after 512683140 batches: 0.0173
trigger times: 2
Loss after 512814240 batches: 0.0169
trigger times: 3
Loss after 512945340 batches: 0.0170
trigger times: 4
Loss after 513076440 batches: 0.0167
trigger times: 5
Loss after 513207540 batches: 0.0167
trigger times: 6
Loss after 513338640 batches: 0.0169
trigger times: 0
Loss after 513469740 batches: 0.0167
trigger times: 1
Loss after 513600840 batches: 0.0165
trigger times: 2
Loss after 513731940 batches: 0.0166
trigger times: 3
Loss after 513863040 batches: 0.0162
trigger times: 4
Loss after 513994140 batches: 0.0162
trigger times: 5
Loss after 514125240 batches: 0.0163
trigger times: 6
Loss after 514256340 batches: 0.0162
trigger times: 7
Loss after 514387440 batches: 0.0163
trigger times: 8
Loss after 514518540 batches: 0.0162
trigger times: 9
Loss after 514649640 batches: 0.0159
trigger times: 10
Loss after 514780740 batches: 0.0158
trigger times: 11
Loss after 514911840 batches: 0.0160
trigger times: 12
Loss after 515042940 batches: 0.0160
trigger times: 0
Loss after 515174040 batches: 0.0158
trigger times: 1
Loss after 515305140 batches: 0.0157
trigger times: 2
Loss after 515436240 batches: 0.0156
trigger times: 3
Loss after 515567340 batches: 0.0157
trigger times: 4
Loss after 515698440 batches: 0.0156
trigger times: 5
Loss after 515829540 batches: 0.0156
trigger times: 6
Loss after 515960640 batches: 0.0155
trigger times: 7
Loss after 516091740 batches: 0.0153
trigger times: 8
Loss after 516222840 batches: 0.0156
trigger times: 9
Loss after 516353940 batches: 0.0151
trigger times: 10
Loss after 516485040 batches: 0.0151
trigger times: 11
Loss after 516616140 batches: 0.0151
trigger times: 12
Loss after 516747240 batches: 0.0151
trigger times: 13
Loss after 516878340 batches: 0.0149
trigger times: 0
Loss after 517009440 batches: 0.0148
trigger times: 1
Loss after 517140540 batches: 0.0150
trigger times: 2
Loss after 517271640 batches: 0.0150
trigger times: 3
Loss after 517402740 batches: 0.0149
trigger times: 4
Loss after 517533840 batches: 0.0147
trigger times: 5
Loss after 517664940 batches: 0.0147
trigger times: 6
Loss after 517796040 batches: 0.0147
trigger times: 7
Loss after 517927140 batches: 0.0144
trigger times: 8
Loss after 518058240 batches: 0.0143
trigger times: 9
Loss after 518189340 batches: 0.0146
trigger times: 10
Loss after 518320440 batches: 0.0144
trigger times: 11
Loss after 518451540 batches: 0.0144
trigger times: 12
Loss after 518582640 batches: 0.0146
trigger times: 13
Loss after 518713740 batches: 0.0145
trigger times: 14
Loss after 518844840 batches: 0.0145
trigger times: 15
Loss after 518975940 batches: 0.0142
trigger times: 16
Loss after 519107040 batches: 0.0143
trigger times: 17
Loss after 519238140 batches: 0.0140
trigger times: 18
Loss after 519369240 batches: 0.0141
trigger times: 19
Loss after 519500340 batches: 0.0142
trigger times: 20
Early stopping!
Start to test process.
Loss after 519631440 batches: 0.0142
Time to train on one home:  568.7093040943146
trigger times: 0
Loss after 519760080 batches: 0.0740
trigger times: 1
Loss after 519888720 batches: 0.0236
trigger times: 2
Loss after 520017360 batches: 0.0190
trigger times: 0
Loss after 520146000 batches: 0.0170
trigger times: 1
Loss after 520274640 batches: 0.0161
trigger times: 2
Loss after 520403280 batches: 0.0152
trigger times: 3
Loss after 520531920 batches: 0.0145
trigger times: 4
Loss after 520660560 batches: 0.0142
trigger times: 5
Loss after 520789200 batches: 0.0139
trigger times: 6
Loss after 520917840 batches: 0.0135
trigger times: 7
Loss after 521046480 batches: 0.0132
trigger times: 8
Loss after 521175120 batches: 0.0128
trigger times: 9
Loss after 521303760 batches: 0.0128
trigger times: 10
Loss after 521432400 batches: 0.0125
trigger times: 11
Loss after 521561040 batches: 0.0126
trigger times: 12
Loss after 521689680 batches: 0.0123
trigger times: 13
Loss after 521818320 batches: 0.0121
trigger times: 14
Loss after 521946960 batches: 0.0116
trigger times: 15
Loss after 522075600 batches: 0.0116
trigger times: 16
Loss after 522204240 batches: 0.0114
trigger times: 17
Loss after 522332880 batches: 0.0116
trigger times: 18
Loss after 522461520 batches: 0.0111
trigger times: 19
Loss after 522590160 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 522718800 batches: 0.0112
Time to train on one home:  178.27592206001282
trigger times: 0
Loss after 522849900 batches: 0.1170
trigger times: 1
Loss after 522981000 batches: 0.0325
trigger times: 2
Loss after 523112100 batches: 0.0261
trigger times: 3
Loss after 523243200 batches: 0.0234
trigger times: 4
Loss after 523374300 batches: 0.0218
trigger times: 5
Loss after 523505400 batches: 0.0209
trigger times: 6
Loss after 523636500 batches: 0.0206
trigger times: 7
Loss after 523767600 batches: 0.0200
trigger times: 8
Loss after 523898700 batches: 0.0192
trigger times: 9
Loss after 524029800 batches: 0.0185
trigger times: 0
Loss after 524160900 batches: 0.0185
trigger times: 1
Loss after 524292000 batches: 0.0182
trigger times: 2
Loss after 524423100 batches: 0.0178
trigger times: 3
Loss after 524554200 batches: 0.0176
trigger times: 4
Loss after 524685300 batches: 0.0174
trigger times: 5
Loss after 524816400 batches: 0.0172
trigger times: 6
Loss after 524947500 batches: 0.0173
trigger times: 7
Loss after 525078600 batches: 0.0170
trigger times: 8
Loss after 525209700 batches: 0.0168
trigger times: 9
Loss after 525340800 batches: 0.0165
trigger times: 0
Loss after 525471900 batches: 0.0164
trigger times: 1
Loss after 525603000 batches: 0.0162
trigger times: 2
Loss after 525734100 batches: 0.0162
trigger times: 3
Loss after 525865200 batches: 0.0162
trigger times: 4
Loss after 525996300 batches: 0.0161
trigger times: 5
Loss after 526127400 batches: 0.0157
trigger times: 6
Loss after 526258500 batches: 0.0157
trigger times: 7
Loss after 526389600 batches: 0.0159
trigger times: 8
Loss after 526520700 batches: 0.0156
trigger times: 0
Loss after 526651800 batches: 0.0154
trigger times: 1
Loss after 526782900 batches: 0.0153
trigger times: 0
Loss after 526914000 batches: 0.0155
trigger times: 1
Loss after 527045100 batches: 0.0154
trigger times: 2
Loss after 527176200 batches: 0.0152
trigger times: 3
Loss after 527307300 batches: 0.0152
trigger times: 4
Loss after 527438400 batches: 0.0148
trigger times: 5
Loss after 527569500 batches: 0.0152
trigger times: 6
Loss after 527700600 batches: 0.0150
trigger times: 0
Loss after 527831700 batches: 0.0149
trigger times: 1
Loss after 527962800 batches: 0.0149
trigger times: 2
Loss after 528093900 batches: 0.0150
trigger times: 3
Loss after 528225000 batches: 0.0148
trigger times: 4
Loss after 528356100 batches: 0.0150
trigger times: 5
Loss after 528487200 batches: 0.0147
trigger times: 6
Loss after 528618300 batches: 0.0147
trigger times: 7
Loss after 528749400 batches: 0.0145
trigger times: 8
Loss after 528880500 batches: 0.0145
trigger times: 9
Loss after 529011600 batches: 0.0146
trigger times: 10
Loss after 529142700 batches: 0.0146
trigger times: 11
Loss after 529273800 batches: 0.0142
trigger times: 12
Loss after 529404900 batches: 0.0142
trigger times: 13
Loss after 529536000 batches: 0.0141
trigger times: 14
Loss after 529667100 batches: 0.0142
trigger times: 15
Loss after 529798200 batches: 0.0142
trigger times: 16
Loss after 529929300 batches: 0.0141
trigger times: 17
Loss after 530060400 batches: 0.0141
trigger times: 0
Loss after 530191500 batches: 0.0139
trigger times: 1
Loss after 530322600 batches: 0.0141
trigger times: 2
Loss after 530453700 batches: 0.0139
trigger times: 3
Loss after 530584800 batches: 0.0140
trigger times: 4
Loss after 530715900 batches: 0.0139
trigger times: 5
Loss after 530847000 batches: 0.0137
trigger times: 6
Loss after 530978100 batches: 0.0138
trigger times: 7
Loss after 531109200 batches: 0.0137
trigger times: 8
Loss after 531240300 batches: 0.0135
trigger times: 9
Loss after 531371400 batches: 0.0138
trigger times: 10
Loss after 531502500 batches: 0.0137
trigger times: 11
Loss after 531633600 batches: 0.0136
trigger times: 12
Loss after 531764700 batches: 0.0135
trigger times: 13
Loss after 531895800 batches: 0.0134
trigger times: 14
Loss after 532026900 batches: 0.0136
trigger times: 15
Loss after 532158000 batches: 0.0135
trigger times: 16
Loss after 532289100 batches: 0.0133
trigger times: 17
Loss after 532420200 batches: 0.0134
trigger times: 18
Loss after 532551300 batches: 0.0132
trigger times: 19
Loss after 532682400 batches: 0.0132
trigger times: 20
Early stopping!
Start to test process.
Loss after 532813500 batches: 0.0132
Time to train on one home:  553.5583565235138
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146, 0.01617968764306582, 0.015298912286208879, 0.01494286251164481, 0.014533454579010391, 0.015354135860208057, 0.014391380102152193, 0.01467373398194618, 0.014497973545955688, 0.01401772527095114]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638], [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354], [0.5489356054200066, 0.40660235248365517, 0.4824471426162714, 1.021275384526291, 0.4861081410983952, 24.12799700883127, 1500.6461], [0.5518252385987176, 0.40347636543781107, 0.4779622827153174, 1.0271923671495102, 0.4886689327670392, 24.267787844092187, 1508.5514], [0.5432945456769731, 0.41270949933158785, 0.48664830013661614, 1.0178919743034478, 0.48110519945531827, 24.048062729621872, 1485.2018], [0.528244235449367, 0.4289907862839557, 0.498210151533106, 1.0003101691077434, 0.4677676573059164, 23.63268628013604, 1444.0278], [0.5302241428030862, 0.4268514023330586, 0.4936126497092288, 0.9992097088225125, 0.4695202290591394, 23.60668751146631, 1449.4382], [0.5239716635810004, 0.4335993694666116, 0.5025112735686915, 0.997230007025717, 0.46399233090650266, 23.55991634694479, 1432.3732], [0.5264167802201377, 0.4310038387187667, 0.5013357099896009, 1.002685768816802, 0.46611857564690545, 23.688810674734015, 1438.9371], [0.5271433658070035, 0.4301962949563598, 0.4984083781805379, 1.002372584819366, 0.4667801111262625, 23.68141159054199, 1440.9792]]
Round_16_results:  [0.5271433658070035, 0.4301962949563598, 0.4984083781805379, 1.002372584819366, 0.4667801111262625, 23.68141159054199, 1440.9792]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4176 < 4177; dropping {'Training_Loss': 0.0938764482666299, 'Validation_Loss': 0.2181977786951595, 'Training_R2': 0.905444982153508, 'Validation_R2': 0.7972240451750916, 'Training_F1': 0.8368692173748549, 'Validation_F1': 0.7411184558943892, 'Training_NEP': 0.32621012811610994, 'Validation_NEP': 0.49415011885564886, 'Training_NDE': 0.07098413025152928, 'Validation_NDE': 0.1614800812686606, 'Training_MAE': 10.803432498760209, 'Validation_MAE': 13.551646362282469, 'Training_MSE': 312.31906, 'Validation_MSE': 596.3415}.
trigger times: 0
Loss after 532944600 batches: 0.0939
trigger times: 0
Loss after 533075700 batches: 0.0213
trigger times: 1
Loss after 533206800 batches: 0.0167
trigger times: 0
Loss after 533337900 batches: 0.0151
trigger times: 1
Loss after 533469000 batches: 0.0137
trigger times: 0
Loss after 533600100 batches: 0.0131
trigger times: 1
Loss after 533731200 batches: 0.0126
trigger times: 2
Loss after 533862300 batches: 0.0120
trigger times: 0
Loss after 533993400 batches: 0.0119
trigger times: 1
Loss after 534124500 batches: 0.0115
trigger times: 2
Loss after 534255600 batches: 0.0112
trigger times: 3
Loss after 534386700 batches: 0.0111
trigger times: 4
Loss after 534517800 batches: 0.0105
trigger times: 5
Loss after 534648900 batches: 0.0105
trigger times: 6
Loss after 534780000 batches: 0.0104
trigger times: 7
Loss after 534911100 batches: 0.0103
trigger times: 8
Loss after 535042200 batches: 0.0101
trigger times: 0
Loss after 535173300 batches: 0.0103
trigger times: 1
Loss after 535304400 batches: 0.0097
trigger times: 2
Loss after 535435500 batches: 0.0096
trigger times: 3
Loss after 535566600 batches: 0.0094
trigger times: 4
Loss after 535697700 batches: 0.0095
trigger times: 5
Loss after 535828800 batches: 0.0093
trigger times: 6
Loss after 535959900 batches: 0.0094
trigger times: 7
Loss after 536091000 batches: 0.0095
trigger times: 8
Loss after 536222100 batches: 0.0091
trigger times: 9
Loss after 536353200 batches: 0.0091
trigger times: 10
Loss after 536484300 batches: 0.0091
trigger times: 11
Loss after 536615400 batches: 0.0087
trigger times: 12
Loss after 536746500 batches: 0.0089
trigger times: 13
Loss after 536877600 batches: 0.0088
trigger times: 14
Loss after 537008700 batches: 0.0088
trigger times: 15
Loss after 537139800 batches: 0.0085
trigger times: 16
Loss after 537270900 batches: 0.0088
trigger times: 17
Loss after 537402000 batches: 0.0085
trigger times: 18
Loss after 537533100 batches: 0.0086
trigger times: 19
Loss after 537664200 batches: 0.0086
trigger times: 20
Early stopping!
Start to test process.
Loss after 537795300 batches: 0.0083
Time to train on one home:  279.4014048576355
trigger times: 0
Loss after 537897900 batches: 0.1768
trigger times: 1
Loss after 538000500 batches: 0.0476
trigger times: 2
Loss after 538103100 batches: 0.0383
trigger times: 3
Loss after 538205700 batches: 0.0337
trigger times: 0
Loss after 538308300 batches: 0.0327
trigger times: 1
Loss after 538410900 batches: 0.0316
trigger times: 2
Loss after 538513500 batches: 0.0283
trigger times: 3
Loss after 538616100 batches: 0.0271
trigger times: 4
Loss after 538718700 batches: 0.0265
trigger times: 5
Loss after 538821300 batches: 0.0271
trigger times: 6
Loss after 538923900 batches: 0.0242
trigger times: 7
Loss after 539026500 batches: 0.0231
trigger times: 8
Loss after 539129100 batches: 0.0229
trigger times: 9
Loss after 539231700 batches: 0.0245
trigger times: 10
Loss after 539334300 batches: 0.0229
trigger times: 11
Loss after 539436900 batches: 0.0224
trigger times: 12
Loss after 539539500 batches: 0.0228
trigger times: 13
Loss after 539642100 batches: 0.0217
trigger times: 14
Loss after 539744700 batches: 0.0231
trigger times: 15
Loss after 539847300 batches: 0.0218
trigger times: 16
Loss after 539949900 batches: 0.0216
trigger times: 17
Loss after 540052500 batches: 0.0205
trigger times: 18
Loss after 540155100 batches: 0.0209
trigger times: 19
Loss after 540257700 batches: 0.0212
trigger times: 20
Early stopping!
Start to test process.
Loss after 540360300 batches: 0.0197
Time to train on one home:  154.52463006973267
trigger times: 0
Loss after 540491400 batches: 0.0923
trigger times: 1
Loss after 540622500 batches: 0.0319
trigger times: 2
Loss after 540753600 batches: 0.0260
trigger times: 3
Loss after 540884700 batches: 0.0231
trigger times: 4
Loss after 541015800 batches: 0.0217
trigger times: 5
Loss after 541146900 batches: 0.0208
trigger times: 6
Loss after 541278000 batches: 0.0199
trigger times: 7
Loss after 541409100 batches: 0.0191
trigger times: 8
Loss after 541540200 batches: 0.0186
trigger times: 9
Loss after 541671300 batches: 0.0182
trigger times: 10
Loss after 541802400 batches: 0.0175
trigger times: 11
Loss after 541933500 batches: 0.0172
trigger times: 12
Loss after 542064600 batches: 0.0171
trigger times: 13
Loss after 542195700 batches: 0.0168
trigger times: 14
Loss after 542326800 batches: 0.0166
trigger times: 15
Loss after 542457900 batches: 0.0164
trigger times: 16
Loss after 542589000 batches: 0.0160
trigger times: 17
Loss after 542720100 batches: 0.0158
trigger times: 18
Loss after 542851200 batches: 0.0156
trigger times: 19
Loss after 542982300 batches: 0.0152
trigger times: 20
Early stopping!
Start to test process.
Loss after 543113400 batches: 0.0152
Time to train on one home:  159.67317008972168
trigger times: 0
Loss after 543244500 batches: 0.1080
trigger times: 0
Loss after 543375600 batches: 0.0339
trigger times: 0
Loss after 543506700 batches: 0.0270
trigger times: 1
Loss after 543637800 batches: 0.0243
trigger times: 2
Loss after 543768900 batches: 0.0232
trigger times: 3
Loss after 543900000 batches: 0.0221
trigger times: 4
Loss after 544031100 batches: 0.0213
trigger times: 0
Loss after 544162200 batches: 0.0205
trigger times: 0
Loss after 544293300 batches: 0.0203
trigger times: 1
Loss after 544424400 batches: 0.0197
trigger times: 2
Loss after 544555500 batches: 0.0195
trigger times: 3
Loss after 544686600 batches: 0.0190
trigger times: 4
Loss after 544817700 batches: 0.0191
trigger times: 5
Loss after 544948800 batches: 0.0186
trigger times: 6
Loss after 545079900 batches: 0.0182
trigger times: 7
Loss after 545211000 batches: 0.0180
trigger times: 0
Loss after 545342100 batches: 0.0177
trigger times: 1
Loss after 545473200 batches: 0.0179
trigger times: 2
Loss after 545604300 batches: 0.0175
trigger times: 3
Loss after 545735400 batches: 0.0173
trigger times: 4
Loss after 545866500 batches: 0.0173
trigger times: 5
Loss after 545997600 batches: 0.0171
trigger times: 6
Loss after 546128700 batches: 0.0172
trigger times: 7
Loss after 546259800 batches: 0.0168
trigger times: 8
Loss after 546390900 batches: 0.0168
trigger times: 9
Loss after 546522000 batches: 0.0168
trigger times: 10
Loss after 546653100 batches: 0.0165
trigger times: 11
Loss after 546784200 batches: 0.0164
trigger times: 12
Loss after 546915300 batches: 0.0162
trigger times: 13
Loss after 547046400 batches: 0.0162
trigger times: 14
Loss after 547177500 batches: 0.0162
trigger times: 15
Loss after 547308600 batches: 0.0161
trigger times: 16
Loss after 547439700 batches: 0.0161
trigger times: 17
Loss after 547570800 batches: 0.0159
trigger times: 18
Loss after 547701900 batches: 0.0160
trigger times: 19
Loss after 547833000 batches: 0.0157
trigger times: 20
Early stopping!
Start to test process.
Loss after 547964100 batches: 0.0158
Time to train on one home:  272.35368299484253
trigger times: 0
Loss after 548092740 batches: 0.0772
trigger times: 1
Loss after 548221380 batches: 0.0234
trigger times: 0
Loss after 548350020 batches: 0.0188
trigger times: 1
Loss after 548478660 batches: 0.0168
trigger times: 2
Loss after 548607300 batches: 0.0159
trigger times: 3
Loss after 548735940 batches: 0.0152
trigger times: 4
Loss after 548864580 batches: 0.0146
trigger times: 5
Loss after 548993220 batches: 0.0141
trigger times: 6
Loss after 549121860 batches: 0.0136
trigger times: 7
Loss after 549250500 batches: 0.0135
trigger times: 8
Loss after 549379140 batches: 0.0133
trigger times: 9
Loss after 549507780 batches: 0.0129
trigger times: 10
Loss after 549636420 batches: 0.0127
trigger times: 11
Loss after 549765060 batches: 0.0125
trigger times: 12
Loss after 549893700 batches: 0.0122
trigger times: 13
Loss after 550022340 batches: 0.0120
trigger times: 14
Loss after 550150980 batches: 0.0119
trigger times: 15
Loss after 550279620 batches: 0.0117
trigger times: 16
Loss after 550408260 batches: 0.0116
trigger times: 17
Loss after 550536900 batches: 0.0113
trigger times: 18
Loss after 550665540 batches: 0.0113
trigger times: 19
Loss after 550794180 batches: 0.0113
trigger times: 20
Early stopping!
Start to test process.
Loss after 550922820 batches: 0.0113
Time to train on one home:  170.73368120193481
trigger times: 0
Loss after 551053920 batches: 0.1166
trigger times: 0
Loss after 551185020 batches: 0.0314
trigger times: 0
Loss after 551316120 batches: 0.0249
trigger times: 1
Loss after 551447220 batches: 0.0224
trigger times: 2
Loss after 551578320 batches: 0.0211
trigger times: 3
Loss after 551709420 batches: 0.0200
trigger times: 4
Loss after 551840520 batches: 0.0194
trigger times: 5
Loss after 551971620 batches: 0.0191
trigger times: 6
Loss after 552102720 batches: 0.0188
trigger times: 7
Loss after 552233820 batches: 0.0185
trigger times: 8
Loss after 552364920 batches: 0.0180
trigger times: 9
Loss after 552496020 batches: 0.0178
trigger times: 0
Loss after 552627120 batches: 0.0173
trigger times: 1
Loss after 552758220 batches: 0.0172
trigger times: 0
Loss after 552889320 batches: 0.0169
trigger times: 1
Loss after 553020420 batches: 0.0167
trigger times: 2
Loss after 553151520 batches: 0.0166
trigger times: 3
Loss after 553282620 batches: 0.0161
trigger times: 4
Loss after 553413720 batches: 0.0161
trigger times: 5
Loss after 553544820 batches: 0.0160
trigger times: 0
Loss after 553675920 batches: 0.0161
trigger times: 1
Loss after 553807020 batches: 0.0159
trigger times: 2
Loss after 553938120 batches: 0.0156
trigger times: 3
Loss after 554069220 batches: 0.0157
trigger times: 4
Loss after 554200320 batches: 0.0157
trigger times: 5
Loss after 554331420 batches: 0.0156
trigger times: 6
Loss after 554462520 batches: 0.0156
trigger times: 0
Loss after 554593620 batches: 0.0155
trigger times: 1
Loss after 554724720 batches: 0.0152
trigger times: 2
Loss after 554855820 batches: 0.0151
trigger times: 3
Loss after 554986920 batches: 0.0150
trigger times: 4
Loss after 555118020 batches: 0.0151
trigger times: 0
Loss after 555249120 batches: 0.0149
trigger times: 1
Loss after 555380220 batches: 0.0148
trigger times: 0
Loss after 555511320 batches: 0.0148
trigger times: 1
Loss after 555642420 batches: 0.0146
trigger times: 2
Loss after 555773520 batches: 0.0147
trigger times: 3
Loss after 555904620 batches: 0.0147
trigger times: 4
Loss after 556035720 batches: 0.0146
trigger times: 5
Loss after 556166820 batches: 0.0145
trigger times: 6
Loss after 556297920 batches: 0.0146
trigger times: 7
Loss after 556429020 batches: 0.0146
trigger times: 8
Loss after 556560120 batches: 0.0144
trigger times: 9
Loss after 556691220 batches: 0.0144
trigger times: 10
Loss after 556822320 batches: 0.0145
trigger times: 11
Loss after 556953420 batches: 0.0142
trigger times: 12
Loss after 557084520 batches: 0.0142
trigger times: 13
Loss after 557215620 batches: 0.0142
trigger times: 14
Loss after 557346720 batches: 0.0143
trigger times: 15
Loss after 557477820 batches: 0.0142
trigger times: 16
Loss after 557608920 batches: 0.0139
trigger times: 17
Loss after 557740020 batches: 0.0138
trigger times: 18
Loss after 557871120 batches: 0.0139
trigger times: 19
Loss after 558002220 batches: 0.0139
trigger times: 20
Early stopping!
Start to test process.
Loss after 558133320 batches: 0.0137
Time to train on one home:  398.79683661460876
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146, 0.01617968764306582, 0.015298912286208879, 0.01494286251164481, 0.014533454579010391, 0.015354135860208057, 0.014391380102152193, 0.01467373398194618, 0.014497973545955688, 0.01401772527095114, 0.013981963926499292]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638], [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354], [0.5489356054200066, 0.40660235248365517, 0.4824471426162714, 1.021275384526291, 0.4861081410983952, 24.12799700883127, 1500.6461], [0.5518252385987176, 0.40347636543781107, 0.4779622827153174, 1.0271923671495102, 0.4886689327670392, 24.267787844092187, 1508.5514], [0.5432945456769731, 0.41270949933158785, 0.48664830013661614, 1.0178919743034478, 0.48110519945531827, 24.048062729621872, 1485.2018], [0.528244235449367, 0.4289907862839557, 0.498210151533106, 1.0003101691077434, 0.4677676573059164, 23.63268628013604, 1444.0278], [0.5302241428030862, 0.4268514023330586, 0.4936126497092288, 0.9992097088225125, 0.4695202290591394, 23.60668751146631, 1449.4382], [0.5239716635810004, 0.4335993694666116, 0.5025112735686915, 0.997230007025717, 0.46399233090650266, 23.55991634694479, 1432.3732], [0.5264167802201377, 0.4310038387187667, 0.5013357099896009, 1.002685768816802, 0.46611857564690545, 23.688810674734015, 1438.9371], [0.5271433658070035, 0.4301962949563598, 0.4984083781805379, 1.002372584819366, 0.4667801111262625, 23.68141159054199, 1440.9792], [0.5250462624761794, 0.43247414830277475, 0.5033795941385566, 1.0099164368613416, 0.4649141059936923, 23.85963779892973, 1435.2188]]
Round_17_results:  [0.5250462624761794, 0.43247414830277475, 0.5033795941385566, 1.0099164368613416, 0.4649141059936923, 23.85963779892973, 1435.2188]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4375 < 4376; dropping {'Training_Loss': 0.08287572340582902, 'Validation_Loss': 0.22156150887409845, 'Training_R2': 0.9164361976777451, 'Validation_R2': 0.7941432795835398, 'Training_F1': 0.8480838582427029, 'Validation_F1': 0.7426287991186062, 'Training_NEP': 0.3039749899918381, 'Validation_NEP': 0.48687317094809135, 'Training_NDE': 0.06273282966310645, 'Validation_NDE': 0.16393344058595752, 'Training_MAE': 10.067048821118295, 'Validation_MAE': 13.352082260449727, 'Training_MSE': 276.01468, 'Validation_MSE': 605.4017}.
trigger times: 0
Loss after 558264420 batches: 0.0829
trigger times: 0
Loss after 558395520 batches: 0.0204
trigger times: 0
Loss after 558526620 batches: 0.0163
trigger times: 0
Loss after 558657720 batches: 0.0142
trigger times: 1
Loss after 558788820 batches: 0.0132
trigger times: 2
Loss after 558919920 batches: 0.0127
trigger times: 3
Loss after 559051020 batches: 0.0123
trigger times: 4
Loss after 559182120 batches: 0.0117
trigger times: 0
Loss after 559313220 batches: 0.0115
trigger times: 1
Loss after 559444320 batches: 0.0111
trigger times: 0
Loss after 559575420 batches: 0.0108
trigger times: 1
Loss after 559706520 batches: 0.0105
trigger times: 2
Loss after 559837620 batches: 0.0105
trigger times: 3
Loss after 559968720 batches: 0.0104
trigger times: 4
Loss after 560099820 batches: 0.0101
trigger times: 5
Loss after 560230920 batches: 0.0098
trigger times: 6
Loss after 560362020 batches: 0.0097
trigger times: 7
Loss after 560493120 batches: 0.0097
trigger times: 8
Loss after 560624220 batches: 0.0094
trigger times: 9
Loss after 560755320 batches: 0.0094
trigger times: 10
Loss after 560886420 batches: 0.0093
trigger times: 11
Loss after 561017520 batches: 0.0092
trigger times: 12
Loss after 561148620 batches: 0.0093
trigger times: 13
Loss after 561279720 batches: 0.0090
trigger times: 14
Loss after 561410820 batches: 0.0089
trigger times: 15
Loss after 561541920 batches: 0.0090
trigger times: 16
Loss after 561673020 batches: 0.0087
trigger times: 17
Loss after 561804120 batches: 0.0088
trigger times: 18
Loss after 561935220 batches: 0.0087
trigger times: 19
Loss after 562066320 batches: 0.0086
trigger times: 20
Early stopping!
Start to test process.
Loss after 562197420 batches: 0.0086
Time to train on one home:  230.63837814331055
trigger times: 0
Loss after 562300020 batches: 0.1607
trigger times: 0
Loss after 562402620 batches: 0.0513
trigger times: 1
Loss after 562505220 batches: 0.0393
trigger times: 2
Loss after 562607820 batches: 0.0344
trigger times: 3
Loss after 562710420 batches: 0.0316
trigger times: 4
Loss after 562813020 batches: 0.0286
trigger times: 5
Loss after 562915620 batches: 0.0285
trigger times: 6
Loss after 563018220 batches: 0.0259
trigger times: 7
Loss after 563120820 batches: 0.0248
trigger times: 8
Loss after 563223420 batches: 0.0247
trigger times: 9
Loss after 563326020 batches: 0.0241
trigger times: 10
Loss after 563428620 batches: 0.0235
trigger times: 11
Loss after 563531220 batches: 0.0229
trigger times: 12
Loss after 563633820 batches: 0.0240
trigger times: 13
Loss after 563736420 batches: 0.0228
trigger times: 14
Loss after 563839020 batches: 0.0211
trigger times: 15
Loss after 563941620 batches: 0.0213
trigger times: 16
Loss after 564044220 batches: 0.0205
trigger times: 17
Loss after 564146820 batches: 0.0203
trigger times: 18
Loss after 564249420 batches: 0.0208
trigger times: 19
Loss after 564352020 batches: 0.0207
trigger times: 20
Early stopping!
Start to test process.
Loss after 564454620 batches: 0.0200
Time to train on one home:  137.34878182411194
trigger times: 0
Loss after 564585720 batches: 0.0760
trigger times: 1
Loss after 564716820 batches: 0.0299
trigger times: 2
Loss after 564847920 batches: 0.0249
trigger times: 3
Loss after 564979020 batches: 0.0228
trigger times: 4
Loss after 565110120 batches: 0.0213
trigger times: 5
Loss after 565241220 batches: 0.0203
trigger times: 6
Loss after 565372320 batches: 0.0194
trigger times: 7
Loss after 565503420 batches: 0.0190
trigger times: 8
Loss after 565634520 batches: 0.0185
trigger times: 9
Loss after 565765620 batches: 0.0180
trigger times: 10
Loss after 565896720 batches: 0.0177
trigger times: 11
Loss after 566027820 batches: 0.0169
trigger times: 12
Loss after 566158920 batches: 0.0167
trigger times: 13
Loss after 566290020 batches: 0.0167
trigger times: 14
Loss after 566421120 batches: 0.0161
trigger times: 15
Loss after 566552220 batches: 0.0161
trigger times: 16
Loss after 566683320 batches: 0.0160
trigger times: 17
Loss after 566814420 batches: 0.0157
trigger times: 18
Loss after 566945520 batches: 0.0153
trigger times: 19
Loss after 567076620 batches: 0.0154
trigger times: 20
Early stopping!
Start to test process.
Loss after 567207720 batches: 0.0152
Time to train on one home:  160.02845740318298
trigger times: 0
Loss after 567338820 batches: 0.1110
trigger times: 0
Loss after 567469920 batches: 0.0324
trigger times: 0
Loss after 567601020 batches: 0.0266
trigger times: 0
Loss after 567732120 batches: 0.0239
trigger times: 0
Loss after 567863220 batches: 0.0228
trigger times: 1
Loss after 567994320 batches: 0.0218
trigger times: 2
Loss after 568125420 batches: 0.0214
trigger times: 3
Loss after 568256520 batches: 0.0205
trigger times: 4
Loss after 568387620 batches: 0.0198
trigger times: 0
Loss after 568518720 batches: 0.0195
trigger times: 1
Loss after 568649820 batches: 0.0192
trigger times: 2
Loss after 568780920 batches: 0.0188
trigger times: 3
Loss after 568912020 batches: 0.0185
trigger times: 4
Loss after 569043120 batches: 0.0183
trigger times: 5
Loss after 569174220 batches: 0.0181
trigger times: 6
Loss after 569305320 batches: 0.0179
trigger times: 7
Loss after 569436420 batches: 0.0177
trigger times: 8
Loss after 569567520 batches: 0.0174
trigger times: 9
Loss after 569698620 batches: 0.0176
trigger times: 10
Loss after 569829720 batches: 0.0172
trigger times: 11
Loss after 569960820 batches: 0.0170
trigger times: 12
Loss after 570091920 batches: 0.0170
trigger times: 13
Loss after 570223020 batches: 0.0168
trigger times: 14
Loss after 570354120 batches: 0.0167
trigger times: 15
Loss after 570485220 batches: 0.0167
trigger times: 16
Loss after 570616320 batches: 0.0166
trigger times: 17
Loss after 570747420 batches: 0.0165
trigger times: 18
Loss after 570878520 batches: 0.0163
trigger times: 0
Loss after 571009620 batches: 0.0165
trigger times: 1
Loss after 571140720 batches: 0.0164
trigger times: 2
Loss after 571271820 batches: 0.0163
trigger times: 3
Loss after 571402920 batches: 0.0162
trigger times: 4
Loss after 571534020 batches: 0.0161
trigger times: 5
Loss after 571665120 batches: 0.0162
trigger times: 0
Loss after 571796220 batches: 0.0159
trigger times: 1
Loss after 571927320 batches: 0.0158
trigger times: 2
Loss after 572058420 batches: 0.0157
trigger times: 3
Loss after 572189520 batches: 0.0157
trigger times: 4
Loss after 572320620 batches: 0.0157
trigger times: 0
Loss after 572451720 batches: 0.0157
trigger times: 1
Loss after 572582820 batches: 0.0156
trigger times: 2
Loss after 572713920 batches: 0.0154
trigger times: 3
Loss after 572845020 batches: 0.0155
trigger times: 4
Loss after 572976120 batches: 0.0155
trigger times: 5
Loss after 573107220 batches: 0.0152
trigger times: 6
Loss after 573238320 batches: 0.0149
trigger times: 7
Loss after 573369420 batches: 0.0153
trigger times: 8
Loss after 573500520 batches: 0.0149
trigger times: 9
Loss after 573631620 batches: 0.0150
trigger times: 10
Loss after 573762720 batches: 0.0148
trigger times: 11
Loss after 573893820 batches: 0.0150
trigger times: 12
Loss after 574024920 batches: 0.0151
trigger times: 13
Loss after 574156020 batches: 0.0150
trigger times: 14
Loss after 574287120 batches: 0.0148
trigger times: 15
Loss after 574418220 batches: 0.0147
trigger times: 0
Loss after 574549320 batches: 0.0147
trigger times: 0
Loss after 574680420 batches: 0.0147
trigger times: 1
Loss after 574811520 batches: 0.0146
trigger times: 2
Loss after 574942620 batches: 0.0144
trigger times: 3
Loss after 575073720 batches: 0.0145
trigger times: 4
Loss after 575204820 batches: 0.0145
trigger times: 5
Loss after 575335920 batches: 0.0146
trigger times: 6
Loss after 575467020 batches: 0.0143
trigger times: 7
Loss after 575598120 batches: 0.0142
trigger times: 8
Loss after 575729220 batches: 0.0141
trigger times: 9
Loss after 575860320 batches: 0.0142
trigger times: 10
Loss after 575991420 batches: 0.0142
trigger times: 11
Loss after 576122520 batches: 0.0140
trigger times: 12
Loss after 576253620 batches: 0.0141
trigger times: 13
Loss after 576384720 batches: 0.0144
trigger times: 14
Loss after 576515820 batches: 0.0142
trigger times: 15
Loss after 576646920 batches: 0.0141
trigger times: 16
Loss after 576778020 batches: 0.0137
trigger times: 17
Loss after 576909120 batches: 0.0139
trigger times: 18
Loss after 577040220 batches: 0.0140
trigger times: 19
Loss after 577171320 batches: 0.0140
trigger times: 20
Early stopping!
Start to test process.
Loss after 577302420 batches: 0.0138
Time to train on one home:  554.647344827652
trigger times: 0
Loss after 577431060 batches: 0.0717
trigger times: 0
Loss after 577559700 batches: 0.0226
trigger times: 1
Loss after 577688340 batches: 0.0182
trigger times: 2
Loss after 577816980 batches: 0.0166
trigger times: 3
Loss after 577945620 batches: 0.0156
trigger times: 4
Loss after 578074260 batches: 0.0149
trigger times: 5
Loss after 578202900 batches: 0.0144
trigger times: 6
Loss after 578331540 batches: 0.0140
trigger times: 7
Loss after 578460180 batches: 0.0136
trigger times: 8
Loss after 578588820 batches: 0.0131
trigger times: 9
Loss after 578717460 batches: 0.0130
trigger times: 10
Loss after 578846100 batches: 0.0128
trigger times: 11
Loss after 578974740 batches: 0.0124
trigger times: 12
Loss after 579103380 batches: 0.0126
trigger times: 13
Loss after 579232020 batches: 0.0119
trigger times: 14
Loss after 579360660 batches: 0.0120
trigger times: 15
Loss after 579489300 batches: 0.0119
trigger times: 16
Loss after 579617940 batches: 0.0118
trigger times: 17
Loss after 579746580 batches: 0.0116
trigger times: 18
Loss after 579875220 batches: 0.0116
trigger times: 19
Loss after 580003860 batches: 0.0114
trigger times: 20
Early stopping!
Start to test process.
Loss after 580132500 batches: 0.0111
Time to train on one home:  163.94430327415466
trigger times: 0
Loss after 580263600 batches: 0.1149
trigger times: 0
Loss after 580394700 batches: 0.0303
trigger times: 1
Loss after 580525800 batches: 0.0239
trigger times: 0
Loss after 580656900 batches: 0.0223
trigger times: 1
Loss after 580788000 batches: 0.0207
trigger times: 0
Loss after 580919100 batches: 0.0200
trigger times: 1
Loss after 581050200 batches: 0.0194
trigger times: 2
Loss after 581181300 batches: 0.0187
trigger times: 3
Loss after 581312400 batches: 0.0181
trigger times: 4
Loss after 581443500 batches: 0.0180
trigger times: 5
Loss after 581574600 batches: 0.0176
trigger times: 6
Loss after 581705700 batches: 0.0177
trigger times: 7
Loss after 581836800 batches: 0.0171
trigger times: 8
Loss after 581967900 batches: 0.0168
trigger times: 9
Loss after 582099000 batches: 0.0167
trigger times: 10
Loss after 582230100 batches: 0.0164
trigger times: 0
Loss after 582361200 batches: 0.0162
trigger times: 1
Loss after 582492300 batches: 0.0161
trigger times: 2
Loss after 582623400 batches: 0.0161
trigger times: 3
Loss after 582754500 batches: 0.0158
trigger times: 4
Loss after 582885600 batches: 0.0154
trigger times: 5
Loss after 583016700 batches: 0.0156
trigger times: 6
Loss after 583147800 batches: 0.0158
trigger times: 7
Loss after 583278900 batches: 0.0157
trigger times: 8
Loss after 583410000 batches: 0.0154
trigger times: 9
Loss after 583541100 batches: 0.0152
trigger times: 10
Loss after 583672200 batches: 0.0152
trigger times: 11
Loss after 583803300 batches: 0.0151
trigger times: 12
Loss after 583934400 batches: 0.0150
trigger times: 13
Loss after 584065500 batches: 0.0151
trigger times: 14
Loss after 584196600 batches: 0.0149
trigger times: 15
Loss after 584327700 batches: 0.0149
trigger times: 16
Loss after 584458800 batches: 0.0147
trigger times: 17
Loss after 584589900 batches: 0.0148
trigger times: 18
Loss after 584721000 batches: 0.0147
trigger times: 19
Loss after 584852100 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 584983200 batches: 0.0145
Time to train on one home:  273.00677847862244
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146, 0.01617968764306582, 0.015298912286208879, 0.01494286251164481, 0.014533454579010391, 0.015354135860208057, 0.014391380102152193, 0.01467373398194618, 0.014497973545955688, 0.01401772527095114, 0.013981963926499292, 0.013864090340243098]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638], [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354], [0.5489356054200066, 0.40660235248365517, 0.4824471426162714, 1.021275384526291, 0.4861081410983952, 24.12799700883127, 1500.6461], [0.5518252385987176, 0.40347636543781107, 0.4779622827153174, 1.0271923671495102, 0.4886689327670392, 24.267787844092187, 1508.5514], [0.5432945456769731, 0.41270949933158785, 0.48664830013661614, 1.0178919743034478, 0.48110519945531827, 24.048062729621872, 1485.2018], [0.528244235449367, 0.4289907862839557, 0.498210151533106, 1.0003101691077434, 0.4677676573059164, 23.63268628013604, 1444.0278], [0.5302241428030862, 0.4268514023330586, 0.4936126497092288, 0.9992097088225125, 0.4695202290591394, 23.60668751146631, 1449.4382], [0.5239716635810004, 0.4335993694666116, 0.5025112735686915, 0.997230007025717, 0.46399233090650266, 23.55991634694479, 1432.3732], [0.5264167802201377, 0.4310038387187667, 0.5013357099896009, 1.002685768816802, 0.46611857564690545, 23.688810674734015, 1438.9371], [0.5271433658070035, 0.4301962949563598, 0.4984083781805379, 1.002372584819366, 0.4667801111262625, 23.68141159054199, 1440.9792], [0.5250462624761794, 0.43247414830277475, 0.5033795941385566, 1.0099164368613416, 0.4649141059936923, 23.85963779892973, 1435.2188], [0.5181609375609292, 0.43992770192133335, 0.507634415015371, 0.993479449618137, 0.4588081952115054, 23.471308083901697, 1416.3694]]
Round_18_results:  [0.5181609375609292, 0.43992770192133335, 0.507634415015371, 0.993479449618137, 0.4588081952115054, 23.471308083901697, 1416.3694]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4585 < 4586; dropping {'Training_Loss': 0.09103894799526008, 'Validation_Loss': 0.20904318905538982, 'Training_R2': 0.9082782388699537, 'Validation_R2': 0.8056170886583102, 'Training_F1': 0.8393909324684956, 'Validation_F1': 0.7382208444999224, 'Training_NEP': 0.32123338008753455, 'Validation_NEP': 0.4949030126870031, 'Training_NDE': 0.06885715414410891, 'Validation_NDE': 0.15479630387043888, 'Training_MAE': 10.63861247400943, 'Validation_MAE': 13.57229383470337, 'Training_MSE': 302.96072, 'Validation_MSE': 571.6585}.
trigger times: 0
Loss after 585114300 batches: 0.0910
trigger times: 0
Loss after 585245400 batches: 0.0204
trigger times: 0
Loss after 585376500 batches: 0.0157
trigger times: 0
Loss after 585507600 batches: 0.0141
trigger times: 1
Loss after 585638700 batches: 0.0130
trigger times: 0
Loss after 585769800 batches: 0.0127
trigger times: 1
Loss after 585900900 batches: 0.0119
trigger times: 2
Loss after 586032000 batches: 0.0116
trigger times: 3
Loss after 586163100 batches: 0.0111
trigger times: 4
Loss after 586294200 batches: 0.0108
trigger times: 5
Loss after 586425300 batches: 0.0107
trigger times: 6
Loss after 586556400 batches: 0.0105
trigger times: 7
Loss after 586687500 batches: 0.0105
trigger times: 8
Loss after 586818600 batches: 0.0102
trigger times: 9
Loss after 586949700 batches: 0.0100
trigger times: 10
Loss after 587080800 batches: 0.0098
trigger times: 11
Loss after 587211900 batches: 0.0097
trigger times: 12
Loss after 587343000 batches: 0.0096
trigger times: 13
Loss after 587474100 batches: 0.0095
trigger times: 14
Loss after 587605200 batches: 0.0096
trigger times: 15
Loss after 587736300 batches: 0.0093
trigger times: 16
Loss after 587867400 batches: 0.0092
trigger times: 17
Loss after 587998500 batches: 0.0090
trigger times: 18
Loss after 588129600 batches: 0.0090
trigger times: 19
Loss after 588260700 batches: 0.0090
trigger times: 20
Early stopping!
Start to test process.
Loss after 588391800 batches: 0.0089
Time to train on one home:  195.01930332183838
trigger times: 0
Loss after 588494400 batches: 0.1589
trigger times: 1
Loss after 588597000 batches: 0.0469
trigger times: 2
Loss after 588699600 batches: 0.0370
trigger times: 3
Loss after 588802200 batches: 0.0364
trigger times: 4
Loss after 588904800 batches: 0.0318
trigger times: 5
Loss after 589007400 batches: 0.0307
trigger times: 6
Loss after 589110000 batches: 0.0286
trigger times: 7
Loss after 589212600 batches: 0.0274
trigger times: 8
Loss after 589315200 batches: 0.0263
trigger times: 9
Loss after 589417800 batches: 0.0250
trigger times: 10
Loss after 589520400 batches: 0.0241
trigger times: 11
Loss after 589623000 batches: 0.0234
trigger times: 12
Loss after 589725600 batches: 0.0236
trigger times: 13
Loss after 589828200 batches: 0.0228
trigger times: 14
Loss after 589930800 batches: 0.0222
trigger times: 15
Loss after 590033400 batches: 0.0215
trigger times: 16
Loss after 590136000 batches: 0.0211
trigger times: 17
Loss after 590238600 batches: 0.0206
trigger times: 18
Loss after 590341200 batches: 0.0203
trigger times: 19
Loss after 590443800 batches: 0.0207
trigger times: 20
Early stopping!
Start to test process.
Loss after 590546400 batches: 0.0207
Time to train on one home:  131.78280377388
trigger times: 0
Loss after 590677500 batches: 0.0770
trigger times: 1
Loss after 590808600 batches: 0.0295
trigger times: 2
Loss after 590939700 batches: 0.0244
trigger times: 3
Loss after 591070800 batches: 0.0224
trigger times: 4
Loss after 591201900 batches: 0.0210
trigger times: 5
Loss after 591333000 batches: 0.0199
trigger times: 6
Loss after 591464100 batches: 0.0192
trigger times: 7
Loss after 591595200 batches: 0.0186
trigger times: 8
Loss after 591726300 batches: 0.0184
trigger times: 9
Loss after 591857400 batches: 0.0177
trigger times: 10
Loss after 591988500 batches: 0.0175
trigger times: 11
Loss after 592119600 batches: 0.0170
trigger times: 12
Loss after 592250700 batches: 0.0166
trigger times: 13
Loss after 592381800 batches: 0.0164
trigger times: 14
Loss after 592512900 batches: 0.0166
trigger times: 15
Loss after 592644000 batches: 0.0160
trigger times: 16
Loss after 592775100 batches: 0.0157
trigger times: 17
Loss after 592906200 batches: 0.0154
trigger times: 18
Loss after 593037300 batches: 0.0153
trigger times: 19
Loss after 593168400 batches: 0.0151
trigger times: 20
Early stopping!
Start to test process.
Loss after 593299500 batches: 0.0150
Time to train on one home:  159.73982739448547
trigger times: 0
Loss after 593430600 batches: 0.0979
trigger times: 0
Loss after 593561700 batches: 0.0318
trigger times: 0
Loss after 593692800 batches: 0.0252
trigger times: 1
Loss after 593823900 batches: 0.0231
trigger times: 2
Loss after 593955000 batches: 0.0222
trigger times: 0
Loss after 594086100 batches: 0.0211
trigger times: 1
Loss after 594217200 batches: 0.0202
trigger times: 2
Loss after 594348300 batches: 0.0202
trigger times: 3
Loss after 594479400 batches: 0.0193
trigger times: 4
Loss after 594610500 batches: 0.0186
trigger times: 5
Loss after 594741600 batches: 0.0188
trigger times: 6
Loss after 594872700 batches: 0.0178
trigger times: 0
Loss after 595003800 batches: 0.0178
trigger times: 1
Loss after 595134900 batches: 0.0177
trigger times: 2
Loss after 595266000 batches: 0.0174
trigger times: 3
Loss after 595397100 batches: 0.0175
trigger times: 4
Loss after 595528200 batches: 0.0175
trigger times: 5
Loss after 595659300 batches: 0.0173
trigger times: 6
Loss after 595790400 batches: 0.0169
trigger times: 0
Loss after 595921500 batches: 0.0169
trigger times: 1
Loss after 596052600 batches: 0.0168
trigger times: 2
Loss after 596183700 batches: 0.0163
trigger times: 3
Loss after 596314800 batches: 0.0162
trigger times: 4
Loss after 596445900 batches: 0.0165
trigger times: 0
Loss after 596577000 batches: 0.0161
trigger times: 0
Loss after 596708100 batches: 0.0161
trigger times: 1
Loss after 596839200 batches: 0.0160
trigger times: 2
Loss after 596970300 batches: 0.0159
trigger times: 3
Loss after 597101400 batches: 0.0159
trigger times: 4
Loss after 597232500 batches: 0.0158
trigger times: 5
Loss after 597363600 batches: 0.0157
trigger times: 6
Loss after 597494700 batches: 0.0156
trigger times: 0
Loss after 597625800 batches: 0.0159
trigger times: 1
Loss after 597756900 batches: 0.0156
trigger times: 2
Loss after 597888000 batches: 0.0159
trigger times: 3
Loss after 598019100 batches: 0.0156
trigger times: 4
Loss after 598150200 batches: 0.0151
trigger times: 5
Loss after 598281300 batches: 0.0151
trigger times: 6
Loss after 598412400 batches: 0.0149
trigger times: 7
Loss after 598543500 batches: 0.0150
trigger times: 8
Loss after 598674600 batches: 0.0151
trigger times: 9
Loss after 598805700 batches: 0.0150
trigger times: 10
Loss after 598936800 batches: 0.0150
trigger times: 11
Loss after 599067900 batches: 0.0146
trigger times: 12
Loss after 599199000 batches: 0.0145
trigger times: 13
Loss after 599330100 batches: 0.0147
trigger times: 14
Loss after 599461200 batches: 0.0149
trigger times: 15
Loss after 599592300 batches: 0.0148
trigger times: 16
Loss after 599723400 batches: 0.0146
trigger times: 17
Loss after 599854500 batches: 0.0146
trigger times: 18
Loss after 599985600 batches: 0.0147
trigger times: 19
Loss after 600116700 batches: 0.0144
trigger times: 20
Early stopping!
Start to test process.
Loss after 600247800 batches: 0.0145
Time to train on one home:  385.721435546875
trigger times: 0
Loss after 600376440 batches: 0.0717
trigger times: 0
Loss after 600505080 batches: 0.0224
trigger times: 0
Loss after 600633720 batches: 0.0182
trigger times: 1
Loss after 600762360 batches: 0.0167
trigger times: 2
Loss after 600891000 batches: 0.0157
trigger times: 3
Loss after 601019640 batches: 0.0149
trigger times: 4
Loss after 601148280 batches: 0.0143
trigger times: 5
Loss after 601276920 batches: 0.0140
trigger times: 6
Loss after 601405560 batches: 0.0135
trigger times: 7
Loss after 601534200 batches: 0.0130
trigger times: 8
Loss after 601662840 batches: 0.0132
trigger times: 9
Loss after 601791480 batches: 0.0129
trigger times: 10
Loss after 601920120 batches: 0.0123
trigger times: 11
Loss after 602048760 batches: 0.0124
trigger times: 12
Loss after 602177400 batches: 0.0122
trigger times: 13
Loss after 602306040 batches: 0.0119
trigger times: 14
Loss after 602434680 batches: 0.0118
trigger times: 15
Loss after 602563320 batches: 0.0114
trigger times: 16
Loss after 602691960 batches: 0.0115
trigger times: 17
Loss after 602820600 batches: 0.0113
trigger times: 18
Loss after 602949240 batches: 0.0113
trigger times: 19
Loss after 603077880 batches: 0.0112
trigger times: 20
Early stopping!
Start to test process.
Loss after 603206520 batches: 0.0111
Time to train on one home:  170.95366501808167
trigger times: 0
Loss after 603337620 batches: 0.1174
trigger times: 0
Loss after 603468720 batches: 0.0295
trigger times: 1
Loss after 603599820 batches: 0.0242
trigger times: 2
Loss after 603730920 batches: 0.0219
trigger times: 0
Loss after 603862020 batches: 0.0209
trigger times: 1
Loss after 603993120 batches: 0.0199
trigger times: 2
Loss after 604124220 batches: 0.0194
trigger times: 3
Loss after 604255320 batches: 0.0187
trigger times: 4
Loss after 604386420 batches: 0.0183
trigger times: 5
Loss after 604517520 batches: 0.0181
trigger times: 6
Loss after 604648620 batches: 0.0177
trigger times: 7
Loss after 604779720 batches: 0.0173
trigger times: 8
Loss after 604910820 batches: 0.0172
trigger times: 9
Loss after 605041920 batches: 0.0171
trigger times: 10
Loss after 605173020 batches: 0.0168
trigger times: 11
Loss after 605304120 batches: 0.0167
trigger times: 0
Loss after 605435220 batches: 0.0163
trigger times: 1
Loss after 605566320 batches: 0.0163
trigger times: 2
Loss after 605697420 batches: 0.0161
trigger times: 3
Loss after 605828520 batches: 0.0159
trigger times: 4
Loss after 605959620 batches: 0.0160
trigger times: 5
Loss after 606090720 batches: 0.0158
trigger times: 6
Loss after 606221820 batches: 0.0155
trigger times: 0
Loss after 606352920 batches: 0.0156
trigger times: 1
Loss after 606484020 batches: 0.0155
trigger times: 2
Loss after 606615120 batches: 0.0152
trigger times: 3
Loss after 606746220 batches: 0.0153
trigger times: 4
Loss after 606877320 batches: 0.0152
trigger times: 0
Loss after 607008420 batches: 0.0151
trigger times: 1
Loss after 607139520 batches: 0.0151
trigger times: 2
Loss after 607270620 batches: 0.0151
trigger times: 3
Loss after 607401720 batches: 0.0148
trigger times: 4
Loss after 607532820 batches: 0.0148
trigger times: 5
Loss after 607663920 batches: 0.0146
trigger times: 6
Loss after 607795020 batches: 0.0146
trigger times: 7
Loss after 607926120 batches: 0.0147
trigger times: 8
Loss after 608057220 batches: 0.0147
trigger times: 9
Loss after 608188320 batches: 0.0146
trigger times: 10
Loss after 608319420 batches: 0.0146
trigger times: 11
Loss after 608450520 batches: 0.0147
trigger times: 12
Loss after 608581620 batches: 0.0144
trigger times: 13
Loss after 608712720 batches: 0.0142
trigger times: 14
Loss after 608843820 batches: 0.0142
trigger times: 15
Loss after 608974920 batches: 0.0142
trigger times: 16
Loss after 609106020 batches: 0.0140
trigger times: 17
Loss after 609237120 batches: 0.0142
trigger times: 18
Loss after 609368220 batches: 0.0142
trigger times: 19
Loss after 609499320 batches: 0.0140
trigger times: 20
Early stopping!
Start to test process.
Loss after 609630420 batches: 0.0139
Time to train on one home:  357.36073637008667
train_results:  [0.06554386674915233, 0.06223066226547774, 0.037485537401763726, 0.029132569465974877, 0.01680279941236801, 0.01760238438347469, 0.017312172838501495, 0.016208580929940146, 0.01617968764306582, 0.015298912286208879, 0.01494286251164481, 0.014533454579010391, 0.015354135860208057, 0.014391380102152193, 0.01467373398194618, 0.014497973545955688, 0.01401772527095114, 0.013981963926499292, 0.013864090340243098, 0.014014604699077752]
test_results:  [[0.881203121609158, 0.0467778933013252, 0.2193818152779555, 1.4223397680694405, 0.7808743905214516, 33.60328682105601, 2410.608], [0.7646594842274984, 0.17316140123664958, 0.2858234559915629, 1.1121765740263059, 0.6773417048677852, 26.27557019191831, 2090.9958], [0.6703466541237302, 0.2752493860037184, 0.28828113200132827, 1.0824242768453474, 0.5937117802947626, 25.57266150708694, 1832.8253], [0.6122218370437622, 0.3381946197925635, 0.3799673948607909, 1.057336487687581, 0.5421473854641334, 24.979953496174147, 1673.6427], [0.6246270537376404, 0.3247611201026984, 0.40949277381010846, 1.1049317513120118, 0.553152035701656, 26.10440865857627, 1707.6147], [0.6088337302207947, 0.3418509364688891, 0.42038330853637823, 1.0769407219816431, 0.5391521506325917, 25.44311055799539, 1664.3964], [0.5689044760333167, 0.38505721183519803, 0.4597798526564437, 1.0413076339765248, 0.5037578037051829, 24.60126608212789, 1555.1317], [0.5648374987973107, 0.3894338154776431, 0.46433978462733566, 1.0308796426259135, 0.500172513689528, 24.35490104882948, 1544.0638], [0.5587339268790351, 0.39600906683021553, 0.4688927863204975, 1.027245132103027, 0.49478610336985157, 24.269034435031234, 1527.4354], [0.5489356054200066, 0.40660235248365517, 0.4824471426162714, 1.021275384526291, 0.4861081410983952, 24.12799700883127, 1500.6461], [0.5518252385987176, 0.40347636543781107, 0.4779622827153174, 1.0271923671495102, 0.4886689327670392, 24.267787844092187, 1508.5514], [0.5432945456769731, 0.41270949933158785, 0.48664830013661614, 1.0178919743034478, 0.48110519945531827, 24.048062729621872, 1485.2018], [0.528244235449367, 0.4289907862839557, 0.498210151533106, 1.0003101691077434, 0.4677676573059164, 23.63268628013604, 1444.0278], [0.5302241428030862, 0.4268514023330586, 0.4936126497092288, 0.9992097088225125, 0.4695202290591394, 23.60668751146631, 1449.4382], [0.5239716635810004, 0.4335993694666116, 0.5025112735686915, 0.997230007025717, 0.46399233090650266, 23.55991634694479, 1432.3732], [0.5264167802201377, 0.4310038387187667, 0.5013357099896009, 1.002685768816802, 0.46611857564690545, 23.688810674734015, 1438.9371], [0.5271433658070035, 0.4301962949563598, 0.4984083781805379, 1.002372584819366, 0.4667801111262625, 23.68141159054199, 1440.9792], [0.5250462624761794, 0.43247414830277475, 0.5033795941385566, 1.0099164368613416, 0.4649141059936923, 23.85963779892973, 1435.2188], [0.5181609375609292, 0.43992770192133335, 0.507634415015371, 0.993479449618137, 0.4588081952115054, 23.471308083901697, 1416.3694], [0.5194045801957449, 0.43857140895246893, 0.5063775714362382, 0.9991237907201931, 0.45991926307070335, 23.60465766550392, 1419.7993]]
Round_19_results:  [0.5194045801957449, 0.43857140895246893, 0.5063775714362382, 0.9991237907201931, 0.45991926307070335, 23.60465766550392, 1419.7993]