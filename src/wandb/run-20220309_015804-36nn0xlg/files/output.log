LSTM(
  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))
  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))
  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))
  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))
  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))
  (linear1): Linear(in_features=23500, out_features=1024, bias=True)
  (linear2): Linear(in_features=1024, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (dropout): Dropout(p=0.2, inplace=False)
)
Window Length:  499
trigger times: 0
Loss after 131100 batches: 0.8092
trigger times: 0
Loss after 262200 batches: 0.3931
trigger times: 1
Loss after 393300 batches: 0.2787
trigger times: 0
Loss after 524400 batches: 0.2241
trigger times: 1
Loss after 655500 batches: 0.1904
trigger times: 0
Loss after 786600 batches: 0.1646
trigger times: 0
Loss after 917700 batches: 0.1406
trigger times: 1
Loss after 1048800 batches: 0.1202
trigger times: 0
Loss after 1179900 batches: 0.1051
trigger times: 0
Loss after 1311000 batches: 0.0911
trigger times: 1
Loss after 1442100 batches: 0.0814
trigger times: 2
Loss after 1573200 batches: 0.0754
trigger times: 0
Loss after 1704300 batches: 0.0684
trigger times: 0
Loss after 1835400 batches: 0.0615
trigger times: 1
Loss after 1966500 batches: 0.0565
trigger times: 2
Loss after 2097600 batches: 0.0523
trigger times: 3
Loss after 2228700 batches: 0.0490
trigger times: 4
Loss after 2359800 batches: 0.0467
trigger times: 5
Loss after 2490900 batches: 0.0434
trigger times: 6
Loss after 2622000 batches: 0.0407
trigger times: 7
Loss after 2753100 batches: 0.0385
trigger times: 0
Loss after 2884200 batches: 0.0376
trigger times: 1
Loss after 3015300 batches: 0.0352
trigger times: 0
Loss after 3146400 batches: 0.0340
trigger times: 1
Loss after 3277500 batches: 0.0320
trigger times: 2
Loss after 3408600 batches: 0.0310
trigger times: 0
Loss after 3539700 batches: 0.0301
trigger times: 1
Loss after 3670800 batches: 0.0289
trigger times: 2
Loss after 3801900 batches: 0.0277
trigger times: 3
Loss after 3933000 batches: 0.0278
trigger times: 4
Loss after 4064100 batches: 0.0275
trigger times: 0
Loss after 4195200 batches: 0.0255
trigger times: 1
Loss after 4326300 batches: 0.0254
trigger times: 2
Loss after 4457400 batches: 0.0239
trigger times: 3
Loss after 4588500 batches: 0.0234
trigger times: 4
Loss after 4719600 batches: 0.0229
trigger times: 5
Loss after 4850700 batches: 0.0225
trigger times: 6
Loss after 4981800 batches: 0.0220
trigger times: 7
Loss after 5112900 batches: 0.0216
trigger times: 8
Loss after 5244000 batches: 0.0205
trigger times: 9
Loss after 5375100 batches: 0.0205
trigger times: 10
Loss after 5506200 batches: 0.0206
trigger times: 11
Loss after 5637300 batches: 0.0196
trigger times: 12
Loss after 5768400 batches: 0.0193
trigger times: 13
Loss after 5899500 batches: 0.0192
trigger times: 14
Loss after 6030600 batches: 0.0185
trigger times: 15
Loss after 6161700 batches: 0.0182
trigger times: 0
Loss after 6292800 batches: 0.0175
trigger times: 1
Loss after 6423900 batches: 0.0176
trigger times: 2
Loss after 6555000 batches: 0.0169
trigger times: 3
Loss after 6686100 batches: 0.0170
trigger times: 4
Loss after 6817200 batches: 0.0164
trigger times: 5
Loss after 6948300 batches: 0.0163
trigger times: 6
Loss after 7079400 batches: 0.0158
trigger times: 7
Loss after 7210500 batches: 0.0158
trigger times: 8
Loss after 7341600 batches: 0.0152
trigger times: 9
Loss after 7472700 batches: 0.0152
trigger times: 10
Loss after 7603800 batches: 0.0149
trigger times: 0
Loss after 7734900 batches: 0.0148
trigger times: 1
Loss after 7866000 batches: 0.0145
trigger times: 2
Loss after 7997100 batches: 0.0144
trigger times: 3
Loss after 8128200 batches: 0.0143
trigger times: 4
Loss after 8259300 batches: 0.0141
trigger times: 5
Loss after 8390400 batches: 0.0139
trigger times: 6
Loss after 8521500 batches: 0.0137
trigger times: 7
Loss after 8652600 batches: 0.0137
trigger times: 8
Loss after 8783700 batches: 0.0137
trigger times: 9
Loss after 8914800 batches: 0.0133
trigger times: 10
Loss after 9045900 batches: 0.0130
trigger times: 0
Loss after 9177000 batches: 0.0128
trigger times: 1
Loss after 9308100 batches: 0.0130
trigger times: 0
Loss after 9439200 batches: 0.0127
trigger times: 1
Loss after 9570300 batches: 0.0130
trigger times: 2
Loss after 9701400 batches: 0.0125
trigger times: 3
Loss after 9832500 batches: 0.0122
trigger times: 4
Loss after 9963600 batches: 0.0123
trigger times: 5
Loss after 10094700 batches: 0.0118
trigger times: 6
Loss after 10225800 batches: 0.0126
trigger times: 7
Loss after 10356900 batches: 0.0120
trigger times: 8
Loss after 10488000 batches: 0.0118
trigger times: 9
Loss after 10619100 batches: 0.0115
trigger times: 0
Loss after 10750200 batches: 0.0116
trigger times: 1
Loss after 10881300 batches: 0.0114
trigger times: 2
Loss after 11012400 batches: 0.0117
trigger times: 3
Loss after 11143500 batches: 0.0115
trigger times: 4
Loss after 11274600 batches: 0.0112
trigger times: 5
Loss after 11405700 batches: 0.0110
trigger times: 6
Loss after 11536800 batches: 0.0113
trigger times: 7
Loss after 11667900 batches: 0.0115
trigger times: 8
Loss after 11799000 batches: 0.0118
trigger times: 9
Loss after 11930100 batches: 0.0112
trigger times: 10
Loss after 12061200 batches: 0.0108
trigger times: 11
Loss after 12192300 batches: 0.0109
trigger times: 0
Loss after 12323400 batches: 0.0113
trigger times: 1
Loss after 12454500 batches: 0.0108
trigger times: 2
Loss after 12585600 batches: 0.0109
trigger times: 3
Loss after 12716700 batches: 0.0106
trigger times: 4
Loss after 12847800 batches: 0.0104
trigger times: 5
Loss after 12978900 batches: 0.0104
trigger times: 6
Loss after 13110000 batches: 0.0103
trigger times: 7
Loss after 13241100 batches: 0.0102
trigger times: 8
Loss after 13372200 batches: 0.0101
trigger times: 9
Loss after 13503300 batches: 0.0097
trigger times: 10
Loss after 13634400 batches: 0.0099
trigger times: 11
Loss after 13765500 batches: 0.0096
trigger times: 12
Loss after 13896600 batches: 0.0102
trigger times: 13
Loss after 14027700 batches: 0.0097
trigger times: 14
Loss after 14158800 batches: 0.0096
trigger times: 15
Loss after 14289900 batches: 0.0097
trigger times: 16
Loss after 14421000 batches: 0.0097
trigger times: 17
Loss after 14552100 batches: 0.0097
trigger times: 18
Loss after 14683200 batches: 0.0096
trigger times: 19
Loss after 14814300 batches: 0.0095
trigger times: 20
Early stopping!
Start to test process.
Loss after 14945400 batches: 0.0097
Time to train on one home:  819.8984065055847
trigger times: 0
Loss after 15048000 batches: 0.9166
trigger times: 0
Loss after 15150600 batches: 0.7528
trigger times: 0
Loss after 15253200 batches: 0.6427
trigger times: 1
Loss after 15355800 batches: 0.5516
trigger times: 0
Loss after 15458400 batches: 0.4981
trigger times: 1
Loss after 15561000 batches: 0.4520
trigger times: 2
Loss after 15663600 batches: 0.4087
trigger times: 3
Loss after 15766200 batches: 0.3696
trigger times: 4
Loss after 15868800 batches: 0.3327
trigger times: 5
Loss after 15971400 batches: 0.3014
trigger times: 6
Loss after 16074000 batches: 0.2822
trigger times: 7
Loss after 16176600 batches: 0.2588
trigger times: 8
Loss after 16279200 batches: 0.2544
trigger times: 9
Loss after 16381800 batches: 0.2267
trigger times: 10
Loss after 16484400 batches: 0.2039
trigger times: 11
Loss after 16587000 batches: 0.1875
trigger times: 12
Loss after 16689600 batches: 0.1836
trigger times: 13
Loss after 16792200 batches: 0.1726
trigger times: 14
Loss after 16894800 batches: 0.1557
trigger times: 15
Loss after 16997400 batches: 0.1568
trigger times: 16
Loss after 17100000 batches: 0.1519
trigger times: 17
Loss after 17202600 batches: 0.1430
trigger times: 18
Loss after 17305200 batches: 0.1488
trigger times: 19
Loss after 17407800 batches: 0.1317
trigger times: 20
Early stopping!
Start to test process.
Loss after 17510400 batches: 0.1239
Time to train on one home:  155.02814269065857
trigger times: 0
Loss after 17641500 batches: 0.7936
trigger times: 0
Loss after 17772600 batches: 0.5317
trigger times: 1
Loss after 17903700 batches: 0.4401
trigger times: 2
Loss after 18034800 batches: 0.3653
trigger times: 3
Loss after 18165900 batches: 0.3146
trigger times: 4
Loss after 18297000 batches: 0.2776
trigger times: 5
Loss after 18428100 batches: 0.2418
trigger times: 6
Loss after 18559200 batches: 0.2161
trigger times: 7
Loss after 18690300 batches: 0.1904
trigger times: 8
Loss after 18821400 batches: 0.1704
trigger times: 9
Loss after 18952500 batches: 0.1515
trigger times: 10
Loss after 19083600 batches: 0.1347
trigger times: 11
Loss after 19214700 batches: 0.1207
trigger times: 12
Loss after 19345800 batches: 0.1091
trigger times: 13
Loss after 19476900 batches: 0.1010
trigger times: 14
Loss after 19608000 batches: 0.0915
trigger times: 15
Loss after 19739100 batches: 0.0835
trigger times: 16
Loss after 19870200 batches: 0.0779
trigger times: 17
Loss after 20001300 batches: 0.0715
trigger times: 18
Loss after 20132400 batches: 0.0680
trigger times: 19
Loss after 20263500 batches: 0.0636
trigger times: 20
Early stopping!
Start to test process.
Loss after 20394600 batches: 0.0612
Time to train on one home:  167.18276929855347
trigger times: 0
Loss after 20525700 batches: 0.9628
trigger times: 0
Loss after 20656800 batches: 0.8268
trigger times: 0
Loss after 20787900 batches: 0.7380
trigger times: 1
Loss after 20919000 batches: 0.6580
trigger times: 2
Loss after 21050100 batches: 0.5922
trigger times: 3
Loss after 21181200 batches: 0.5006
trigger times: 4
Loss after 21312300 batches: 0.3841
trigger times: 5
Loss after 21443400 batches: 0.2931
trigger times: 6
Loss after 21574500 batches: 0.2293
trigger times: 7
Loss after 21705600 batches: 0.1921
trigger times: 8
Loss after 21836700 batches: 0.1640
trigger times: 9
Loss after 21967800 batches: 0.1477
trigger times: 10
Loss after 22098900 batches: 0.1319
trigger times: 11
Loss after 22230000 batches: 0.1208
trigger times: 12
Loss after 22361100 batches: 0.1141
trigger times: 13
Loss after 22492200 batches: 0.1071
trigger times: 14
Loss after 22623300 batches: 0.1009
trigger times: 15
Loss after 22754400 batches: 0.0966
trigger times: 16
Loss after 22885500 batches: 0.0920
trigger times: 17
Loss after 23016600 batches: 0.0879
trigger times: 18
Loss after 23147700 batches: 0.0862
trigger times: 19
Loss after 23278800 batches: 0.0824
trigger times: 20
Early stopping!
Start to test process.
Loss after 23409900 batches: 0.0792
Time to train on one home:  175.35740876197815
trigger times: 0
Loss after 23538540 batches: 0.6518
trigger times: 0
Loss after 23667180 batches: 0.4596
trigger times: 0
Loss after 23795820 batches: 0.4148
trigger times: 1
Loss after 23924460 batches: 0.3595
trigger times: 0
Loss after 24053100 batches: 0.2986
trigger times: 1
Loss after 24181740 batches: 0.2369
trigger times: 2
Loss after 24310380 batches: 0.1888
trigger times: 3
Loss after 24439020 batches: 0.1582
trigger times: 4
Loss after 24567660 batches: 0.1325
trigger times: 5
Loss after 24696300 batches: 0.1180
trigger times: 6
Loss after 24824940 batches: 0.1049
trigger times: 7
Loss after 24953580 batches: 0.0951
trigger times: 8
Loss after 25082220 batches: 0.0864
trigger times: 9
Loss after 25210860 batches: 0.0804
trigger times: 10
Loss after 25339500 batches: 0.0731
trigger times: 11
Loss after 25468140 batches: 0.0702
trigger times: 12
Loss after 25596780 batches: 0.0640
trigger times: 13
Loss after 25725420 batches: 0.0618
trigger times: 14
Loss after 25854060 batches: 0.0574
trigger times: 15
Loss after 25982700 batches: 0.0554
trigger times: 16
Loss after 26111340 batches: 0.0538
trigger times: 17
Loss after 26239980 batches: 0.0506
trigger times: 18
Loss after 26368620 batches: 0.0486
trigger times: 19
Loss after 26497260 batches: 0.0455
trigger times: 20
Early stopping!
Start to test process.
Loss after 26625900 batches: 0.0447
Time to train on one home:  185.93884801864624
trigger times: 0
Loss after 26757000 batches: 0.9098
trigger times: 0
Loss after 26888100 batches: 0.7699
trigger times: 1
Loss after 27019200 batches: 0.7016
trigger times: 2
Loss after 27150300 batches: 0.6321
trigger times: 3
Loss after 27281400 batches: 0.5563
trigger times: 4
Loss after 27412500 batches: 0.4569
trigger times: 5
Loss after 27543600 batches: 0.3680
trigger times: 6
Loss after 27674700 batches: 0.2899
trigger times: 7
Loss after 27805800 batches: 0.2324
trigger times: 8
Loss after 27936900 batches: 0.1956
trigger times: 9
Loss after 28068000 batches: 0.1728
trigger times: 10
Loss after 28199100 batches: 0.1532
trigger times: 11
Loss after 28330200 batches: 0.1393
trigger times: 12
Loss after 28461300 batches: 0.1290
trigger times: 13
Loss after 28592400 batches: 0.1192
trigger times: 14
Loss after 28723500 batches: 0.1095
trigger times: 15
Loss after 28854600 batches: 0.1046
trigger times: 16
Loss after 28985700 batches: 0.0983
trigger times: 17
Loss after 29116800 batches: 0.0943
trigger times: 18
Loss after 29247900 batches: 0.0908
trigger times: 19
Loss after 29379000 batches: 0.0871
trigger times: 20
Early stopping!
Start to test process.
Loss after 29510100 batches: 0.0834
Time to train on one home:  167.54596948623657
trigger times: 0
Loss after 29641200 batches: 0.9704
trigger times: 0
Loss after 29772300 batches: 0.8650
trigger times: 0
Loss after 29903400 batches: 0.8024
trigger times: 0
Loss after 30034500 batches: 0.7519
trigger times: 0
Loss after 30165600 batches: 0.6787
trigger times: 1
Loss after 30296700 batches: 0.6005
trigger times: 2
Loss after 30427800 batches: 0.5335
trigger times: 3
Loss after 30558900 batches: 0.4543
trigger times: 4
Loss after 30690000 batches: 0.4056
trigger times: 5
Loss after 30821100 batches: 0.3587
trigger times: 6
Loss after 30952200 batches: 0.3255
trigger times: 7
Loss after 31083300 batches: 0.2988
trigger times: 8
Loss after 31214400 batches: 0.2713
trigger times: 9
Loss after 31345500 batches: 0.2566
trigger times: 10
Loss after 31476600 batches: 0.2349
trigger times: 11
Loss after 31607700 batches: 0.2286
trigger times: 12
Loss after 31738800 batches: 0.2141
trigger times: 13
Loss after 31869900 batches: 0.1957
trigger times: 14
Loss after 32001000 batches: 0.1865
trigger times: 15
Loss after 32132100 batches: 0.1723
trigger times: 16
Loss after 32263200 batches: 0.1669
trigger times: 17
Loss after 32394300 batches: 0.1545
trigger times: 18
Loss after 32525400 batches: 0.1524
trigger times: 19
Loss after 32656500 batches: 0.1459
trigger times: 20
Early stopping!
Start to test process.
Loss after 32787600 batches: 0.1326
Time to train on one home:  187.71579217910767
train_results:  [0.07637831982700095]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306]]
Round_0_results:  [0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306]
trigger times: 0
Loss after 32918700 batches: 0.4932
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 257 < 258; dropping {'Training_Loss': 0.49323612788938126, 'Validation_Loss': 0.35180297162797713, 'Training_R2': 0.5041741547759768, 'Validation_R2': 0.6728288702147989, 'Training_F1': 0.5969350393166162, 'Validation_F1': 0.6930719049528044, 'Training_NEP': 0.8097018302610642, 'Validation_NEP': 0.6978576210446806, 'Training_NDE': 0.37222526293206704, 'Validation_NDE': 0.2605418412261575, 'Training_MAE': 26.81571880635917, 'Validation_MAE': 19.13815120296239, 'Training_MSE': 1637.7328, 'Validation_MSE': 962.1738}.
trigger times: 0
Loss after 33049800 batches: 0.2133
trigger times: 0
Loss after 33180900 batches: 0.1693
trigger times: 1
Loss after 33312000 batches: 0.1402
trigger times: 0
Loss after 33443100 batches: 0.1197
trigger times: 1
Loss after 33574200 batches: 0.1015
trigger times: 0
Loss after 33705300 batches: 0.0880
trigger times: 1
Loss after 33836400 batches: 0.0780
trigger times: 2
Loss after 33967500 batches: 0.0702
trigger times: 0
Loss after 34098600 batches: 0.0657
trigger times: 1
Loss after 34229700 batches: 0.0586
trigger times: 2
Loss after 34360800 batches: 0.0535
trigger times: 0
Loss after 34491900 batches: 0.0508
trigger times: 1
Loss after 34623000 batches: 0.0477
trigger times: 2
Loss after 34754100 batches: 0.0446
trigger times: 0
Loss after 34885200 batches: 0.0429
trigger times: 1
Loss after 35016300 batches: 0.0411
trigger times: 2
Loss after 35147400 batches: 0.0386
trigger times: 3
Loss after 35278500 batches: 0.0372
trigger times: 4
Loss after 35409600 batches: 0.0354
trigger times: 5
Loss after 35540700 batches: 0.0350
trigger times: 0
Loss after 35671800 batches: 0.0339
trigger times: 1
Loss after 35802900 batches: 0.0325
trigger times: 2
Loss after 35934000 batches: 0.0315
trigger times: 3
Loss after 36065100 batches: 0.0302
trigger times: 4
Loss after 36196200 batches: 0.0293
trigger times: 0
Loss after 36327300 batches: 0.0287
trigger times: 1
Loss after 36458400 batches: 0.0285
trigger times: 2
Loss after 36589500 batches: 0.0275
trigger times: 3
Loss after 36720600 batches: 0.0265
trigger times: 4
Loss after 36851700 batches: 0.0259
trigger times: 5
Loss after 36982800 batches: 0.0262
trigger times: 6
Loss after 37113900 batches: 0.0251
trigger times: 7
Loss after 37245000 batches: 0.0248
trigger times: 8
Loss after 37376100 batches: 0.0242
trigger times: 9
Loss after 37507200 batches: 0.0236
trigger times: 10
Loss after 37638300 batches: 0.0234
trigger times: 11
Loss after 37769400 batches: 0.0230
trigger times: 12
Loss after 37900500 batches: 0.0223
trigger times: 13
Loss after 38031600 batches: 0.0216
trigger times: 14
Loss after 38162700 batches: 0.0215
trigger times: 15
Loss after 38293800 batches: 0.0207
trigger times: 16
Loss after 38424900 batches: 0.0210
trigger times: 17
Loss after 38556000 batches: 0.0208
trigger times: 18
Loss after 38687100 batches: 0.0204
trigger times: 19
Loss after 38818200 batches: 0.0203
trigger times: 20
Early stopping!
Start to test process.
Loss after 38949300 batches: 0.0197
Time to train on one home:  344.15293884277344
trigger times: 0
Loss after 39051900 batches: 0.7845
trigger times: 0
Loss after 39154500 batches: 0.5802
trigger times: 0
Loss after 39257100 batches: 0.4840
trigger times: 1
Loss after 39359700 batches: 0.4031
trigger times: 2
Loss after 39462300 batches: 0.3352
trigger times: 3
Loss after 39564900 batches: 0.2964
trigger times: 4
Loss after 39667500 batches: 0.2555
trigger times: 5
Loss after 39770100 batches: 0.2376
trigger times: 6
Loss after 39872700 batches: 0.2104
trigger times: 7
Loss after 39975300 batches: 0.1930
trigger times: 8
Loss after 40077900 batches: 0.1889
trigger times: 9
Loss after 40180500 batches: 0.1652
trigger times: 10
Loss after 40283100 batches: 0.1578
trigger times: 11
Loss after 40385700 batches: 0.1886
trigger times: 12
Loss after 40488300 batches: 0.1626
trigger times: 13
Loss after 40590900 batches: 0.1510
trigger times: 14
Loss after 40693500 batches: 0.1319
trigger times: 15
Loss after 40796100 batches: 0.1189
trigger times: 16
Loss after 40898700 batches: 0.1154
trigger times: 17
Loss after 41001300 batches: 0.1116
trigger times: 18
Loss after 41103900 batches: 0.1082
trigger times: 19
Loss after 41206500 batches: 0.1024
trigger times: 20
Early stopping!
Start to test process.
Loss after 41309100 batches: 0.1001
Time to train on one home:  143.10971784591675
trigger times: 0
Loss after 41440200 batches: 0.5572
trigger times: 1
Loss after 41571300 batches: 0.3526
trigger times: 2
Loss after 41702400 batches: 0.2775
trigger times: 3
Loss after 41833500 batches: 0.2225
trigger times: 4
Loss after 41964600 batches: 0.1800
trigger times: 5
Loss after 42095700 batches: 0.1525
trigger times: 6
Loss after 42226800 batches: 0.1326
trigger times: 7
Loss after 42357900 batches: 0.1179
trigger times: 8
Loss after 42489000 batches: 0.1074
trigger times: 9
Loss after 42620100 batches: 0.0984
trigger times: 10
Loss after 42751200 batches: 0.0902
trigger times: 11
Loss after 42882300 batches: 0.0848
trigger times: 12
Loss after 43013400 batches: 0.0792
trigger times: 13
Loss after 43144500 batches: 0.0751
trigger times: 14
Loss after 43275600 batches: 0.0712
trigger times: 15
Loss after 43406700 batches: 0.0686
trigger times: 16
Loss after 43537800 batches: 0.0649
trigger times: 17
Loss after 43668900 batches: 0.0626
trigger times: 18
Loss after 43800000 batches: 0.0611
trigger times: 19
Loss after 43931100 batches: 0.0585
trigger times: 20
Early stopping!
Start to test process.
Loss after 44062200 batches: 0.0560
Time to train on one home:  160.19452714920044
trigger times: 0
Loss after 44193300 batches: 0.7856
trigger times: 1
Loss after 44324400 batches: 0.6046
trigger times: 2
Loss after 44455500 batches: 0.4632
trigger times: 3
Loss after 44586600 batches: 0.3423
trigger times: 4
Loss after 44717700 batches: 0.2643
trigger times: 5
Loss after 44848800 batches: 0.2178
trigger times: 6
Loss after 44979900 batches: 0.1875
trigger times: 7
Loss after 45111000 batches: 0.1654
trigger times: 8
Loss after 45242100 batches: 0.1495
trigger times: 9
Loss after 45373200 batches: 0.1383
trigger times: 10
Loss after 45504300 batches: 0.1274
trigger times: 11
Loss after 45635400 batches: 0.1182
trigger times: 12
Loss after 45766500 batches: 0.1139
trigger times: 13
Loss after 45897600 batches: 0.1071
trigger times: 14
Loss after 46028700 batches: 0.1025
trigger times: 15
Loss after 46159800 batches: 0.0985
trigger times: 16
Loss after 46290900 batches: 0.0947
trigger times: 17
Loss after 46422000 batches: 0.0914
trigger times: 18
Loss after 46553100 batches: 0.0890
trigger times: 19
Loss after 46684200 batches: 0.0858
trigger times: 20
Early stopping!
Start to test process.
Loss after 46815300 batches: 0.0838
Time to train on one home:  160.31801390647888
trigger times: 0
Loss after 46943940 batches: 0.5230
trigger times: 0
Loss after 47072580 batches: 0.3295
trigger times: 1
Loss after 47201220 batches: 0.2396
trigger times: 2
Loss after 47329860 batches: 0.1817
trigger times: 3
Loss after 47458500 batches: 0.1490
trigger times: 4
Loss after 47587140 batches: 0.1240
trigger times: 5
Loss after 47715780 batches: 0.1062
trigger times: 6
Loss after 47844420 batches: 0.0948
trigger times: 7
Loss after 47973060 batches: 0.0867
trigger times: 8
Loss after 48101700 batches: 0.0787
trigger times: 9
Loss after 48230340 batches: 0.0722
trigger times: 10
Loss after 48358980 batches: 0.0689
trigger times: 11
Loss after 48487620 batches: 0.0643
trigger times: 12
Loss after 48616260 batches: 0.0612
trigger times: 13
Loss after 48744900 batches: 0.0578
trigger times: 14
Loss after 48873540 batches: 0.0559
trigger times: 15
Loss after 49002180 batches: 0.0532
trigger times: 16
Loss after 49130820 batches: 0.0514
trigger times: 17
Loss after 49259460 batches: 0.0496
trigger times: 18
Loss after 49388100 batches: 0.0477
trigger times: 19
Loss after 49516740 batches: 0.0469
trigger times: 20
Early stopping!
Start to test process.
Loss after 49645380 batches: 0.0454
Time to train on one home:  164.0788345336914
trigger times: 0
Loss after 49776480 batches: 0.8025
trigger times: 1
Loss after 49907580 batches: 0.5960
trigger times: 2
Loss after 50038680 batches: 0.4663
trigger times: 3
Loss after 50169780 batches: 0.3582
trigger times: 4
Loss after 50300880 batches: 0.2723
trigger times: 5
Loss after 50431980 batches: 0.2189
trigger times: 6
Loss after 50563080 batches: 0.1884
trigger times: 7
Loss after 50694180 batches: 0.1652
trigger times: 8
Loss after 50825280 batches: 0.1487
trigger times: 9
Loss after 50956380 batches: 0.1382
trigger times: 10
Loss after 51087480 batches: 0.1284
trigger times: 11
Loss after 51218580 batches: 0.1193
trigger times: 12
Loss after 51349680 batches: 0.1126
trigger times: 13
Loss after 51480780 batches: 0.1060
trigger times: 14
Loss after 51611880 batches: 0.1016
trigger times: 15
Loss after 51742980 batches: 0.0980
trigger times: 16
Loss after 51874080 batches: 0.0946
trigger times: 17
Loss after 52005180 batches: 0.0899
trigger times: 18
Loss after 52136280 batches: 0.0870
trigger times: 19
Loss after 52267380 batches: 0.0856
trigger times: 20
Early stopping!
Start to test process.
Loss after 52398480 batches: 0.0819
Time to train on one home:  159.99921989440918
trigger times: 0
Loss after 52529580 batches: 0.8400
trigger times: 1
Loss after 52660680 batches: 0.7092
trigger times: 2
Loss after 52791780 batches: 0.5918
trigger times: 3
Loss after 52922880 batches: 0.5065
trigger times: 4
Loss after 53053980 batches: 0.4415
trigger times: 5
Loss after 53185080 batches: 0.3726
trigger times: 6
Loss after 53316180 batches: 0.3357
trigger times: 7
Loss after 53447280 batches: 0.2993
trigger times: 8
Loss after 53578380 batches: 0.2748
trigger times: 9
Loss after 53709480 batches: 0.2439
trigger times: 10
Loss after 53840580 batches: 0.2318
trigger times: 11
Loss after 53971680 batches: 0.2128
trigger times: 12
Loss after 54102780 batches: 0.1941
trigger times: 13
Loss after 54233880 batches: 0.1811
trigger times: 14
Loss after 54364980 batches: 0.1718
trigger times: 15
Loss after 54496080 batches: 0.1676
trigger times: 16
Loss after 54627180 batches: 0.1579
trigger times: 17
Loss after 54758280 batches: 0.1459
trigger times: 18
Loss after 54889380 batches: 0.1354
trigger times: 19
Loss after 55020480 batches: 0.1257
trigger times: 20
Early stopping!
Start to test process.
Loss after 55151580 batches: 0.1219
Time to train on one home:  159.97834849357605
train_results:  [0.07637831982700095, 0.07267687724368596]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816]]
Round_1_results:  [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816]
trigger times: 0
Loss after 55282680 batches: 0.3053
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 433 < 434; dropping {'Training_Loss': 0.30526174548661933, 'Validation_Loss': 0.3174941721889708, 'Training_R2': 0.6930355476783051, 'Validation_R2': 0.7049099596691564, 'Training_F1': 0.6992690979153761, 'Validation_F1': 0.7089030213359786, 'Training_NEP': 0.6021507258859332, 'Validation_NEP': 0.5898060926149888, 'Training_NDE': 0.23044366298536922, 'Validation_NDE': 0.2349941527107712, 'Training_MAE': 19.942037847680414, 'Validation_MAE': 16.174930015088837, 'Training_MSE': 1013.916, 'Validation_MSE': 867.82697}.
trigger times: 0
Loss after 55413780 batches: 0.1352
trigger times: 0
Loss after 55544880 batches: 0.0913
trigger times: 0
Loss after 55675980 batches: 0.0684
trigger times: 0
Loss after 55807080 batches: 0.0581
trigger times: 1
Loss after 55938180 batches: 0.0514
trigger times: 0
Loss after 56069280 batches: 0.0465
trigger times: 0
Loss after 56200380 batches: 0.0429
trigger times: 1
Loss after 56331480 batches: 0.0396
trigger times: 2
Loss after 56462580 batches: 0.0369
trigger times: 3
Loss after 56593680 batches: 0.0357
trigger times: 4
Loss after 56724780 batches: 0.0340
trigger times: 0
Loss after 56855880 batches: 0.0324
trigger times: 0
Loss after 56986980 batches: 0.0310
trigger times: 1
Loss after 57118080 batches: 0.0297
trigger times: 2
Loss after 57249180 batches: 0.0293
trigger times: 3
Loss after 57380280 batches: 0.0281
trigger times: 0
Loss after 57511380 batches: 0.0272
trigger times: 0
Loss after 57642480 batches: 0.0259
trigger times: 0
Loss after 57773580 batches: 0.0259
trigger times: 1
Loss after 57904680 batches: 0.0252
trigger times: 0
Loss after 58035780 batches: 0.0247
trigger times: 0
Loss after 58166880 batches: 0.0248
trigger times: 1
Loss after 58297980 batches: 0.0235
trigger times: 2
Loss after 58429080 batches: 0.0231
trigger times: 3
Loss after 58560180 batches: 0.0225
trigger times: 4
Loss after 58691280 batches: 0.0224
trigger times: 5
Loss after 58822380 batches: 0.0219
trigger times: 6
Loss after 58953480 batches: 0.0217
trigger times: 7
Loss after 59084580 batches: 0.0209
trigger times: 0
Loss after 59215680 batches: 0.0204
trigger times: 1
Loss after 59346780 batches: 0.0203
trigger times: 0
Loss after 59477880 batches: 0.0198
trigger times: 1
Loss after 59608980 batches: 0.0199
trigger times: 2
Loss after 59740080 batches: 0.0197
trigger times: 3
Loss after 59871180 batches: 0.0190
trigger times: 0
Loss after 60002280 batches: 0.0193
trigger times: 1
Loss after 60133380 batches: 0.0182
trigger times: 0
Loss after 60264480 batches: 0.0185
trigger times: 1
Loss after 60395580 batches: 0.0183
trigger times: 0
Loss after 60526680 batches: 0.0179
trigger times: 1
Loss after 60657780 batches: 0.0175
trigger times: 0
Loss after 60788880 batches: 0.0175
trigger times: 1
Loss after 60919980 batches: 0.0174
trigger times: 2
Loss after 61051080 batches: 0.0172
trigger times: 3
Loss after 61182180 batches: 0.0167
trigger times: 4
Loss after 61313280 batches: 0.0170
trigger times: 5
Loss after 61444380 batches: 0.0163
trigger times: 6
Loss after 61575480 batches: 0.0163
trigger times: 7
Loss after 61706580 batches: 0.0162
trigger times: 8
Loss after 61837680 batches: 0.0159
trigger times: 9
Loss after 61968780 batches: 0.0160
trigger times: 10
Loss after 62099880 batches: 0.0155
trigger times: 11
Loss after 62230980 batches: 0.0154
trigger times: 0
Loss after 62362080 batches: 0.0153
trigger times: 1
Loss after 62493180 batches: 0.0157
trigger times: 0
Loss after 62624280 batches: 0.0156
trigger times: 1
Loss after 62755380 batches: 0.0147
trigger times: 2
Loss after 62886480 batches: 0.0147
trigger times: 3
Loss after 63017580 batches: 0.0148
trigger times: 4
Loss after 63148680 batches: 0.0149
trigger times: 5
Loss after 63279780 batches: 0.0145
trigger times: 6
Loss after 63410880 batches: 0.0142
trigger times: 7
Loss after 63541980 batches: 0.0141
trigger times: 8
Loss after 63673080 batches: 0.0139
trigger times: 9
Loss after 63804180 batches: 0.0142
trigger times: 10
Loss after 63935280 batches: 0.0138
trigger times: 11
Loss after 64066380 batches: 0.0137
trigger times: 12
Loss after 64197480 batches: 0.0134
trigger times: 13
Loss after 64328580 batches: 0.0133
trigger times: 14
Loss after 64459680 batches: 0.0134
trigger times: 15
Loss after 64590780 batches: 0.0132
trigger times: 16
Loss after 64721880 batches: 0.0131
trigger times: 17
Loss after 64852980 batches: 0.0130
trigger times: 18
Loss after 64984080 batches: 0.0130
trigger times: 19
Loss after 65115180 batches: 0.0130
trigger times: 20
Early stopping!
Start to test process.
Loss after 65246280 batches: 0.0128
Time to train on one home:  555.3563852310181
trigger times: 0
Loss after 65348880 batches: 0.5595
trigger times: 1
Loss after 65451480 batches: 0.3287
trigger times: 2
Loss after 65554080 batches: 0.2258
trigger times: 3
Loss after 65656680 batches: 0.1798
trigger times: 4
Loss after 65759280 batches: 0.1535
trigger times: 5
Loss after 65861880 batches: 0.1335
trigger times: 6
Loss after 65964480 batches: 0.1156
trigger times: 7
Loss after 66067080 batches: 0.1041
trigger times: 8
Loss after 66169680 batches: 0.0955
trigger times: 9
Loss after 66272280 batches: 0.0929
trigger times: 10
Loss after 66374880 batches: 0.0880
trigger times: 11
Loss after 66477480 batches: 0.0811
trigger times: 12
Loss after 66580080 batches: 0.0824
trigger times: 13
Loss after 66682680 batches: 0.0778
trigger times: 14
Loss after 66785280 batches: 0.0765
trigger times: 15
Loss after 66887880 batches: 0.0709
trigger times: 16
Loss after 66990480 batches: 0.0674
trigger times: 17
Loss after 67093080 batches: 0.0716
trigger times: 18
Loss after 67195680 batches: 0.0655
trigger times: 19
Loss after 67298280 batches: 0.0670
trigger times: 20
Early stopping!
Start to test process.
Loss after 67400880 batches: 0.0589
Time to train on one home:  132.15937900543213
trigger times: 0
Loss after 67531980 batches: 0.3987
trigger times: 1
Loss after 67663080 batches: 0.1910
trigger times: 2
Loss after 67794180 batches: 0.1212
trigger times: 3
Loss after 67925280 batches: 0.0938
trigger times: 4
Loss after 68056380 batches: 0.0789
trigger times: 0
Loss after 68187480 batches: 0.0701
trigger times: 1
Loss after 68318580 batches: 0.0636
trigger times: 2
Loss after 68449680 batches: 0.0579
trigger times: 3
Loss after 68580780 batches: 0.0549
trigger times: 0
Loss after 68711880 batches: 0.0511
trigger times: 1
Loss after 68842980 batches: 0.0485
trigger times: 0
Loss after 68974080 batches: 0.0458
trigger times: 1
Loss after 69105180 batches: 0.0438
trigger times: 2
Loss after 69236280 batches: 0.0427
trigger times: 3
Loss after 69367380 batches: 0.0409
trigger times: 4
Loss after 69498480 batches: 0.0396
trigger times: 5
Loss after 69629580 batches: 0.0384
trigger times: 6
Loss after 69760680 batches: 0.0378
trigger times: 7
Loss after 69891780 batches: 0.0358
trigger times: 8
Loss after 70022880 batches: 0.0347
trigger times: 9
Loss after 70153980 batches: 0.0342
trigger times: 10
Loss after 70285080 batches: 0.0334
trigger times: 11
Loss after 70416180 batches: 0.0328
trigger times: 12
Loss after 70547280 batches: 0.0322
trigger times: 13
Loss after 70678380 batches: 0.0315
trigger times: 14
Loss after 70809480 batches: 0.0305
trigger times: 15
Loss after 70940580 batches: 0.0301
trigger times: 16
Loss after 71071680 batches: 0.0295
trigger times: 17
Loss after 71202780 batches: 0.0293
trigger times: 18
Loss after 71333880 batches: 0.0288
trigger times: 19
Loss after 71464980 batches: 0.0281
trigger times: 0
Loss after 71596080 batches: 0.0281
trigger times: 1
Loss after 71727180 batches: 0.0280
trigger times: 2
Loss after 71858280 batches: 0.0273
trigger times: 3
Loss after 71989380 batches: 0.0270
trigger times: 4
Loss after 72120480 batches: 0.0264
trigger times: 0
Loss after 72251580 batches: 0.0261
trigger times: 0
Loss after 72382680 batches: 0.0255
trigger times: 1
Loss after 72513780 batches: 0.0259
trigger times: 2
Loss after 72644880 batches: 0.0256
trigger times: 3
Loss after 72775980 batches: 0.0249
trigger times: 4
Loss after 72907080 batches: 0.0248
trigger times: 5
Loss after 73038180 batches: 0.0243
trigger times: 6
Loss after 73169280 batches: 0.0242
trigger times: 7
Loss after 73300380 batches: 0.0239
trigger times: 8
Loss after 73431480 batches: 0.0238
trigger times: 9
Loss after 73562580 batches: 0.0235
trigger times: 10
Loss after 73693680 batches: 0.0233
trigger times: 11
Loss after 73824780 batches: 0.0229
trigger times: 12
Loss after 73955880 batches: 0.0226
trigger times: 13
Loss after 74086980 batches: 0.0222
trigger times: 14
Loss after 74218080 batches: 0.0224
trigger times: 15
Loss after 74349180 batches: 0.0221
trigger times: 16
Loss after 74480280 batches: 0.0219
trigger times: 17
Loss after 74611380 batches: 0.0219
trigger times: 18
Loss after 74742480 batches: 0.0218
trigger times: 19
Loss after 74873580 batches: 0.0216
trigger times: 20
Early stopping!
Start to test process.
Loss after 75004680 batches: 0.0211
Time to train on one home:  422.00011253356934
trigger times: 0
Loss after 75135780 batches: 0.6410
trigger times: 1
Loss after 75266880 batches: 0.3450
trigger times: 2
Loss after 75397980 batches: 0.2084
trigger times: 3
Loss after 75529080 batches: 0.1550
trigger times: 4
Loss after 75660180 batches: 0.1309
trigger times: 5
Loss after 75791280 batches: 0.1151
trigger times: 6
Loss after 75922380 batches: 0.1024
trigger times: 7
Loss after 76053480 batches: 0.0946
trigger times: 8
Loss after 76184580 batches: 0.0890
trigger times: 9
Loss after 76315680 batches: 0.0837
trigger times: 10
Loss after 76446780 batches: 0.0800
trigger times: 11
Loss after 76577880 batches: 0.0770
trigger times: 12
Loss after 76708980 batches: 0.0741
trigger times: 13
Loss after 76840080 batches: 0.0702
trigger times: 14
Loss after 76971180 batches: 0.0686
trigger times: 15
Loss after 77102280 batches: 0.0658
trigger times: 16
Loss after 77233380 batches: 0.0640
trigger times: 17
Loss after 77364480 batches: 0.0620
trigger times: 18
Loss after 77495580 batches: 0.0599
trigger times: 19
Loss after 77626680 batches: 0.0587
trigger times: 20
Early stopping!
Start to test process.
Loss after 77757780 batches: 0.0576
Time to train on one home:  160.58472228050232
trigger times: 0
Loss after 77886420 batches: 0.3641
trigger times: 1
Loss after 78015060 batches: 0.1478
trigger times: 2
Loss after 78143700 batches: 0.0954
trigger times: 3
Loss after 78272340 batches: 0.0751
trigger times: 4
Loss after 78400980 batches: 0.0623
trigger times: 5
Loss after 78529620 batches: 0.0553
trigger times: 6
Loss after 78658260 batches: 0.0505
trigger times: 7
Loss after 78786900 batches: 0.0468
trigger times: 8
Loss after 78915540 batches: 0.0441
trigger times: 9
Loss after 79044180 batches: 0.0411
trigger times: 10
Loss after 79172820 batches: 0.0394
trigger times: 11
Loss after 79301460 batches: 0.0379
trigger times: 12
Loss after 79430100 batches: 0.0365
trigger times: 13
Loss after 79558740 batches: 0.0346
trigger times: 14
Loss after 79687380 batches: 0.0339
trigger times: 15
Loss after 79816020 batches: 0.0331
trigger times: 16
Loss after 79944660 batches: 0.0316
trigger times: 17
Loss after 80073300 batches: 0.0309
trigger times: 18
Loss after 80201940 batches: 0.0306
trigger times: 19
Loss after 80330580 batches: 0.0293
trigger times: 20
Early stopping!
Start to test process.
Loss after 80459220 batches: 0.0292
Time to train on one home:  157.91865038871765
trigger times: 0
Loss after 80590320 batches: 0.6731
trigger times: 1
Loss after 80721420 batches: 0.3638
trigger times: 2
Loss after 80852520 batches: 0.2069
trigger times: 3
Loss after 80983620 batches: 0.1476
trigger times: 4
Loss after 81114720 batches: 0.1189
trigger times: 5
Loss after 81245820 batches: 0.1038
trigger times: 6
Loss after 81376920 batches: 0.0933
trigger times: 7
Loss after 81508020 batches: 0.0856
trigger times: 8
Loss after 81639120 batches: 0.0805
trigger times: 9
Loss after 81770220 batches: 0.0757
trigger times: 10
Loss after 81901320 batches: 0.0712
trigger times: 11
Loss after 82032420 batches: 0.0681
trigger times: 12
Loss after 82163520 batches: 0.0654
trigger times: 13
Loss after 82294620 batches: 0.0634
trigger times: 14
Loss after 82425720 batches: 0.0607
trigger times: 15
Loss after 82556820 batches: 0.0587
trigger times: 16
Loss after 82687920 batches: 0.0577
trigger times: 17
Loss after 82819020 batches: 0.0564
trigger times: 18
Loss after 82950120 batches: 0.0550
trigger times: 19
Loss after 83081220 batches: 0.0532
trigger times: 20
Early stopping!
Start to test process.
Loss after 83212320 batches: 0.0519
Time to train on one home:  159.9938085079193
trigger times: 0
Loss after 83343420 batches: 0.7214
trigger times: 1
Loss after 83474520 batches: 0.4758
trigger times: 2
Loss after 83605620 batches: 0.3366
trigger times: 3
Loss after 83736720 batches: 0.2707
trigger times: 4
Loss after 83867820 batches: 0.2302
trigger times: 5
Loss after 83998920 batches: 0.2024
trigger times: 6
Loss after 84130020 batches: 0.1697
trigger times: 7
Loss after 84261120 batches: 0.1554
trigger times: 8
Loss after 84392220 batches: 0.1439
trigger times: 9
Loss after 84523320 batches: 0.1327
trigger times: 10
Loss after 84654420 batches: 0.1192
trigger times: 11
Loss after 84785520 batches: 0.1093
trigger times: 12
Loss after 84916620 batches: 0.1039
trigger times: 13
Loss after 85047720 batches: 0.1014
trigger times: 14
Loss after 85178820 batches: 0.0937
trigger times: 15
Loss after 85309920 batches: 0.0883
trigger times: 16
Loss after 85441020 batches: 0.0857
trigger times: 17
Loss after 85572120 batches: 0.0829
trigger times: 18
Loss after 85703220 batches: 0.0782
trigger times: 19
Loss after 85834320 batches: 0.0733
trigger times: 20
Early stopping!
Start to test process.
Loss after 85965420 batches: 0.0741
Time to train on one home:  161.37514662742615
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107]]
Round_2_results:  [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107]
trigger times: 0
Loss after 86096520 batches: 0.2128
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 673 < 674; dropping {'Training_Loss': 0.21276606085165492, 'Validation_Loss': 0.2703028834528393, 'Training_R2': 0.7861883553137343, 'Validation_R2': 0.748618423798115, 'Training_F1': 0.7503934214656724, 'Validation_F1': 0.6966525494761152, 'Training_NEP': 0.5001014386904548, 'Validation_NEP': 0.565078243141399, 'Training_NDE': 0.16051219682855428, 'Validation_NDE': 0.20018703593123477, 'Training_MAE': 16.56236784132631, 'Validation_MAE': 15.496789792959836, 'Training_MSE': 706.2284, 'Validation_MSE': 739.2852}.
trigger times: 0
Loss after 86227620 batches: 0.0677
trigger times: 1
Loss after 86358720 batches: 0.0459
trigger times: 0
Loss after 86489820 batches: 0.0378
trigger times: 1
Loss after 86620920 batches: 0.0334
trigger times: 2
Loss after 86752020 batches: 0.0308
trigger times: 3
Loss after 86883120 batches: 0.0284
trigger times: 0
Loss after 87014220 batches: 0.0272
trigger times: 0
Loss after 87145320 batches: 0.0254
trigger times: 1
Loss after 87276420 batches: 0.0247
trigger times: 0
Loss after 87407520 batches: 0.0237
trigger times: 1
Loss after 87538620 batches: 0.0226
trigger times: 2
Loss after 87669720 batches: 0.0217
trigger times: 0
Loss after 87800820 batches: 0.0206
trigger times: 1
Loss after 87931920 batches: 0.0201
trigger times: 2
Loss after 88063020 batches: 0.0197
trigger times: 3
Loss after 88194120 batches: 0.0193
trigger times: 0
Loss after 88325220 batches: 0.0187
trigger times: 0
Loss after 88456320 batches: 0.0186
trigger times: 1
Loss after 88587420 batches: 0.0180
trigger times: 2
Loss after 88718520 batches: 0.0179
trigger times: 3
Loss after 88849620 batches: 0.0178
trigger times: 4
Loss after 88980720 batches: 0.0174
trigger times: 5
Loss after 89111820 batches: 0.0167
trigger times: 6
Loss after 89242920 batches: 0.0164
trigger times: 7
Loss after 89374020 batches: 0.0169
trigger times: 0
Loss after 89505120 batches: 0.0163
trigger times: 1
Loss after 89636220 batches: 0.0161
trigger times: 2
Loss after 89767320 batches: 0.0158
trigger times: 3
Loss after 89898420 batches: 0.0155
trigger times: 4
Loss after 90029520 batches: 0.0151
trigger times: 5
Loss after 90160620 batches: 0.0156
trigger times: 6
Loss after 90291720 batches: 0.0149
trigger times: 7
Loss after 90422820 batches: 0.0148
trigger times: 8
Loss after 90553920 batches: 0.0147
trigger times: 9
Loss after 90685020 batches: 0.0147
trigger times: 10
Loss after 90816120 batches: 0.0147
trigger times: 11
Loss after 90947220 batches: 0.0140
trigger times: 12
Loss after 91078320 batches: 0.0139
trigger times: 13
Loss after 91209420 batches: 0.0136
trigger times: 0
Loss after 91340520 batches: 0.0138
trigger times: 1
Loss after 91471620 batches: 0.0137
trigger times: 2
Loss after 91602720 batches: 0.0133
trigger times: 3
Loss after 91733820 batches: 0.0133
trigger times: 4
Loss after 91864920 batches: 0.0129
trigger times: 5
Loss after 91996020 batches: 0.0129
trigger times: 6
Loss after 92127120 batches: 0.0130
trigger times: 7
Loss after 92258220 batches: 0.0128
trigger times: 8
Loss after 92389320 batches: 0.0129
trigger times: 9
Loss after 92520420 batches: 0.0124
trigger times: 10
Loss after 92651520 batches: 0.0126
trigger times: 11
Loss after 92782620 batches: 0.0122
trigger times: 12
Loss after 92913720 batches: 0.0124
trigger times: 13
Loss after 93044820 batches: 0.0121
trigger times: 14
Loss after 93175920 batches: 0.0119
trigger times: 15
Loss after 93307020 batches: 0.0121
trigger times: 16
Loss after 93438120 batches: 0.0119
trigger times: 17
Loss after 93569220 batches: 0.0120
trigger times: 18
Loss after 93700320 batches: 0.0120
trigger times: 19
Loss after 93831420 batches: 0.0117
trigger times: 20
Early stopping!
Start to test process.
Loss after 93962520 batches: 0.0118
Time to train on one home:  443.268474817276
trigger times: 0
Loss after 94065120 batches: 0.5055
trigger times: 1
Loss after 94167720 batches: 0.2393
trigger times: 2
Loss after 94270320 batches: 0.1512
trigger times: 3
Loss after 94372920 batches: 0.1216
trigger times: 4
Loss after 94475520 batches: 0.1018
trigger times: 5
Loss after 94578120 batches: 0.0888
trigger times: 6
Loss after 94680720 batches: 0.0803
trigger times: 7
Loss after 94783320 batches: 0.0780
trigger times: 8
Loss after 94885920 batches: 0.0683
trigger times: 9
Loss after 94988520 batches: 0.0641
trigger times: 10
Loss after 95091120 batches: 0.0608
trigger times: 11
Loss after 95193720 batches: 0.0599
trigger times: 12
Loss after 95296320 batches: 0.0546
trigger times: 13
Loss after 95398920 batches: 0.0509
trigger times: 14
Loss after 95501520 batches: 0.0476
trigger times: 15
Loss after 95604120 batches: 0.0480
trigger times: 16
Loss after 95706720 batches: 0.0453
trigger times: 17
Loss after 95809320 batches: 0.0444
trigger times: 18
Loss after 95911920 batches: 0.0439
trigger times: 19
Loss after 96014520 batches: 0.0461
trigger times: 20
Early stopping!
Start to test process.
Loss after 96117120 batches: 0.0423
Time to train on one home:  132.89224219322205
trigger times: 0
Loss after 96248220 batches: 0.2621
trigger times: 0
Loss after 96379320 batches: 0.0977
trigger times: 1
Loss after 96510420 batches: 0.0673
trigger times: 2
Loss after 96641520 batches: 0.0548
trigger times: 3
Loss after 96772620 batches: 0.0477
trigger times: 4
Loss after 96903720 batches: 0.0435
trigger times: 5
Loss after 97034820 batches: 0.0406
trigger times: 6
Loss after 97165920 batches: 0.0375
trigger times: 7
Loss after 97297020 batches: 0.0354
trigger times: 8
Loss after 97428120 batches: 0.0339
trigger times: 9
Loss after 97559220 batches: 0.0322
trigger times: 10
Loss after 97690320 batches: 0.0319
trigger times: 11
Loss after 97821420 batches: 0.0307
trigger times: 12
Loss after 97952520 batches: 0.0292
trigger times: 13
Loss after 98083620 batches: 0.0284
trigger times: 14
Loss after 98214720 batches: 0.0276
trigger times: 15
Loss after 98345820 batches: 0.0267
trigger times: 16
Loss after 98476920 batches: 0.0263
trigger times: 0
Loss after 98608020 batches: 0.0258
trigger times: 1
Loss after 98739120 batches: 0.0255
trigger times: 2
Loss after 98870220 batches: 0.0247
trigger times: 3
Loss after 99001320 batches: 0.0242
trigger times: 4
Loss after 99132420 batches: 0.0239
trigger times: 5
Loss after 99263520 batches: 0.0236
trigger times: 6
Loss after 99394620 batches: 0.0231
trigger times: 7
Loss after 99525720 batches: 0.0227
trigger times: 8
Loss after 99656820 batches: 0.0223
trigger times: 9
Loss after 99787920 batches: 0.0222
trigger times: 10
Loss after 99919020 batches: 0.0217
trigger times: 11
Loss after 100050120 batches: 0.0218
trigger times: 12
Loss after 100181220 batches: 0.0218
trigger times: 13
Loss after 100312320 batches: 0.0213
trigger times: 14
Loss after 100443420 batches: 0.0216
trigger times: 15
Loss after 100574520 batches: 0.0209
trigger times: 16
Loss after 100705620 batches: 0.0209
trigger times: 17
Loss after 100836720 batches: 0.0205
trigger times: 18
Loss after 100967820 batches: 0.0204
trigger times: 19
Loss after 101098920 batches: 0.0199
trigger times: 20
Early stopping!
Start to test process.
Loss after 101230020 batches: 0.0201
Time to train on one home:  287.0369596481323
trigger times: 0
Loss after 101361120 batches: 0.5088
trigger times: 1
Loss after 101492220 batches: 0.1954
trigger times: 2
Loss after 101623320 batches: 0.1261
trigger times: 3
Loss after 101754420 batches: 0.1000
trigger times: 4
Loss after 101885520 batches: 0.0871
trigger times: 5
Loss after 102016620 batches: 0.0789
trigger times: 6
Loss after 102147720 batches: 0.0741
trigger times: 7
Loss after 102278820 batches: 0.0685
trigger times: 8
Loss after 102409920 batches: 0.0642
trigger times: 9
Loss after 102541020 batches: 0.0612
trigger times: 10
Loss after 102672120 batches: 0.0586
trigger times: 11
Loss after 102803220 batches: 0.0566
trigger times: 12
Loss after 102934320 batches: 0.0543
trigger times: 13
Loss after 103065420 batches: 0.0523
trigger times: 14
Loss after 103196520 batches: 0.0504
trigger times: 15
Loss after 103327620 batches: 0.0493
trigger times: 16
Loss after 103458720 batches: 0.0486
trigger times: 17
Loss after 103589820 batches: 0.0471
trigger times: 18
Loss after 103720920 batches: 0.0459
trigger times: 19
Loss after 103852020 batches: 0.0446
trigger times: 20
Early stopping!
Start to test process.
Loss after 103983120 batches: 0.0445
Time to train on one home:  159.97775173187256
trigger times: 0
Loss after 104111760 batches: 0.2649
trigger times: 0
Loss after 104240400 batches: 0.0856
trigger times: 0
Loss after 104369040 batches: 0.0601
trigger times: 1
Loss after 104497680 batches: 0.0502
trigger times: 2
Loss after 104626320 batches: 0.0443
trigger times: 3
Loss after 104754960 batches: 0.0399
trigger times: 4
Loss after 104883600 batches: 0.0373
trigger times: 5
Loss after 105012240 batches: 0.0356
trigger times: 6
Loss after 105140880 batches: 0.0328
trigger times: 7
Loss after 105269520 batches: 0.0317
trigger times: 8
Loss after 105398160 batches: 0.0312
trigger times: 9
Loss after 105526800 batches: 0.0295
trigger times: 10
Loss after 105655440 batches: 0.0281
trigger times: 0
Loss after 105784080 batches: 0.0284
trigger times: 1
Loss after 105912720 batches: 0.0267
trigger times: 0
Loss after 106041360 batches: 0.0260
trigger times: 1
Loss after 106170000 batches: 0.0255
trigger times: 2
Loss after 106298640 batches: 0.0251
trigger times: 3
Loss after 106427280 batches: 0.0246
trigger times: 4
Loss after 106555920 batches: 0.0236
trigger times: 5
Loss after 106684560 batches: 0.0225
trigger times: 6
Loss after 106813200 batches: 0.0224
trigger times: 0
Loss after 106941840 batches: 0.0225
trigger times: 1
Loss after 107070480 batches: 0.0217
trigger times: 2
Loss after 107199120 batches: 0.0215
trigger times: 3
Loss after 107327760 batches: 0.0211
trigger times: 4
Loss after 107456400 batches: 0.0213
trigger times: 5
Loss after 107585040 batches: 0.0208
trigger times: 6
Loss after 107713680 batches: 0.0206
trigger times: 7
Loss after 107842320 batches: 0.0197
trigger times: 8
Loss after 107970960 batches: 0.0197
trigger times: 9
Loss after 108099600 batches: 0.0199
trigger times: 10
Loss after 108228240 batches: 0.0199
trigger times: 11
Loss after 108356880 batches: 0.0193
trigger times: 12
Loss after 108485520 batches: 0.0191
trigger times: 13
Loss after 108614160 batches: 0.0185
trigger times: 14
Loss after 108742800 batches: 0.0182
trigger times: 15
Loss after 108871440 batches: 0.0185
trigger times: 16
Loss after 109000080 batches: 0.0185
trigger times: 17
Loss after 109128720 batches: 0.0180
trigger times: 18
Loss after 109257360 batches: 0.0178
trigger times: 0
Loss after 109386000 batches: 0.0178
trigger times: 1
Loss after 109514640 batches: 0.0177
trigger times: 2
Loss after 109643280 batches: 0.0176
trigger times: 3
Loss after 109771920 batches: 0.0173
trigger times: 4
Loss after 109900560 batches: 0.0172
trigger times: 5
Loss after 110029200 batches: 0.0169
trigger times: 6
Loss after 110157840 batches: 0.0170
trigger times: 0
Loss after 110286480 batches: 0.0164
trigger times: 1
Loss after 110415120 batches: 0.0168
trigger times: 2
Loss after 110543760 batches: 0.0167
trigger times: 3
Loss after 110672400 batches: 0.0165
trigger times: 4
Loss after 110801040 batches: 0.0164
trigger times: 5
Loss after 110929680 batches: 0.0161
trigger times: 6
Loss after 111058320 batches: 0.0165
trigger times: 7
Loss after 111186960 batches: 0.0160
trigger times: 8
Loss after 111315600 batches: 0.0158
trigger times: 9
Loss after 111444240 batches: 0.0159
trigger times: 10
Loss after 111572880 batches: 0.0153
trigger times: 11
Loss after 111701520 batches: 0.0155
trigger times: 12
Loss after 111830160 batches: 0.0155
trigger times: 13
Loss after 111958800 batches: 0.0152
trigger times: 14
Loss after 112087440 batches: 0.0153
trigger times: 0
Loss after 112216080 batches: 0.0151
trigger times: 1
Loss after 112344720 batches: 0.0149
trigger times: 2
Loss after 112473360 batches: 0.0148
trigger times: 3
Loss after 112602000 batches: 0.0146
trigger times: 0
Loss after 112730640 batches: 0.0146
trigger times: 1
Loss after 112859280 batches: 0.0148
trigger times: 2
Loss after 112987920 batches: 0.0146
trigger times: 0
Loss after 113116560 batches: 0.0143
trigger times: 1
Loss after 113245200 batches: 0.0146
trigger times: 2
Loss after 113373840 batches: 0.0144
trigger times: 3
Loss after 113502480 batches: 0.0143
trigger times: 0
Loss after 113631120 batches: 0.0142
trigger times: 1
Loss after 113759760 batches: 0.0140
trigger times: 0
Loss after 113888400 batches: 0.0142
trigger times: 1
Loss after 114017040 batches: 0.0139
trigger times: 2
Loss after 114145680 batches: 0.0139
trigger times: 3
Loss after 114274320 batches: 0.0137
trigger times: 4
Loss after 114402960 batches: 0.0137
trigger times: 5
Loss after 114531600 batches: 0.0136
trigger times: 6
Loss after 114660240 batches: 0.0142
trigger times: 7
Loss after 114788880 batches: 0.0137
trigger times: 8
Loss after 114917520 batches: 0.0136
trigger times: 0
Loss after 115046160 batches: 0.0135
trigger times: 1
Loss after 115174800 batches: 0.0133
trigger times: 2
Loss after 115303440 batches: 0.0134
trigger times: 3
Loss after 115432080 batches: 0.0134
trigger times: 4
Loss after 115560720 batches: 0.0134
trigger times: 5
Loss after 115689360 batches: 0.0134
trigger times: 6
Loss after 115818000 batches: 0.0131
trigger times: 7
Loss after 115946640 batches: 0.0130
trigger times: 8
Loss after 116075280 batches: 0.0130
trigger times: 9
Loss after 116203920 batches: 0.0129
trigger times: 10
Loss after 116332560 batches: 0.0130
trigger times: 11
Loss after 116461200 batches: 0.0128
trigger times: 12
Loss after 116589840 batches: 0.0128
trigger times: 13
Loss after 116718480 batches: 0.0127
trigger times: 14
Loss after 116847120 batches: 0.0127
trigger times: 15
Loss after 116975760 batches: 0.0126
trigger times: 16
Loss after 117104400 batches: 0.0127
trigger times: 17
Loss after 117233040 batches: 0.0127
trigger times: 0
Loss after 117361680 batches: 0.0128
trigger times: 1
Loss after 117490320 batches: 0.0126
trigger times: 2
Loss after 117618960 batches: 0.0125
trigger times: 3
Loss after 117747600 batches: 0.0124
trigger times: 4
Loss after 117876240 batches: 0.0126
trigger times: 5
Loss after 118004880 batches: 0.0123
trigger times: 6
Loss after 118133520 batches: 0.0127
trigger times: 7
Loss after 118262160 batches: 0.0122
trigger times: 8
Loss after 118390800 batches: 0.0121
trigger times: 9
Loss after 118519440 batches: 0.0122
trigger times: 10
Loss after 118648080 batches: 0.0121
trigger times: 11
Loss after 118776720 batches: 0.0124
trigger times: 12
Loss after 118905360 batches: 0.0121
trigger times: 13
Loss after 119034000 batches: 0.0123
trigger times: 14
Loss after 119162640 batches: 0.0123
trigger times: 15
Loss after 119291280 batches: 0.0119
trigger times: 16
Loss after 119419920 batches: 0.0117
trigger times: 17
Loss after 119548560 batches: 0.0117
trigger times: 18
Loss after 119677200 batches: 0.0121
trigger times: 19
Loss after 119805840 batches: 0.0120
trigger times: 0
Loss after 119934480 batches: 0.0118
trigger times: 1
Loss after 120063120 batches: 0.0115
trigger times: 2
Loss after 120191760 batches: 0.0118
trigger times: 3
Loss after 120320400 batches: 0.0116
trigger times: 4
Loss after 120449040 batches: 0.0118
trigger times: 5
Loss after 120577680 batches: 0.0114
trigger times: 6
Loss after 120706320 batches: 0.0116
trigger times: 7
Loss after 120834960 batches: 0.0113
trigger times: 8
Loss after 120963600 batches: 0.0115
trigger times: 9
Loss after 121092240 batches: 0.0117
trigger times: 10
Loss after 121220880 batches: 0.0113
trigger times: 11
Loss after 121349520 batches: 0.0113
trigger times: 12
Loss after 121478160 batches: 0.0110
trigger times: 13
Loss after 121606800 batches: 0.0112
trigger times: 14
Loss after 121735440 batches: 0.0110
trigger times: 15
Loss after 121864080 batches: 0.0113
trigger times: 16
Loss after 121992720 batches: 0.0113
trigger times: 17
Loss after 122121360 batches: 0.0110
trigger times: 18
Loss after 122250000 batches: 0.0109
trigger times: 19
Loss after 122378640 batches: 0.0109
trigger times: 20
Early stopping!
Start to test process.
Loss after 122507280 batches: 0.0113
Time to train on one home:  1013.3631749153137
trigger times: 0
Loss after 122638380 batches: 0.5497
trigger times: 1
Loss after 122769480 batches: 0.1995
trigger times: 2
Loss after 122900580 batches: 0.1171
trigger times: 3
Loss after 123031680 batches: 0.0924
trigger times: 4
Loss after 123162780 batches: 0.0792
trigger times: 5
Loss after 123293880 batches: 0.0707
trigger times: 6
Loss after 123424980 batches: 0.0654
trigger times: 7
Loss after 123556080 batches: 0.0601
trigger times: 8
Loss after 123687180 batches: 0.0576
trigger times: 9
Loss after 123818280 batches: 0.0550
trigger times: 10
Loss after 123949380 batches: 0.0526
trigger times: 11
Loss after 124080480 batches: 0.0502
trigger times: 12
Loss after 124211580 batches: 0.0485
trigger times: 13
Loss after 124342680 batches: 0.0470
trigger times: 14
Loss after 124473780 batches: 0.0454
trigger times: 15
Loss after 124604880 batches: 0.0444
trigger times: 16
Loss after 124735980 batches: 0.0429
trigger times: 17
Loss after 124867080 batches: 0.0423
trigger times: 18
Loss after 124998180 batches: 0.0411
trigger times: 19
Loss after 125129280 batches: 0.0401
trigger times: 20
Early stopping!
Start to test process.
Loss after 125260380 batches: 0.0405
Time to train on one home:  160.14277172088623
trigger times: 0
Loss after 125391480 batches: 0.5756
trigger times: 1
Loss after 125522580 batches: 0.3066
trigger times: 2
Loss after 125653680 batches: 0.2076
trigger times: 3
Loss after 125784780 batches: 0.1708
trigger times: 0
Loss after 125915880 batches: 0.1410
trigger times: 0
Loss after 126046980 batches: 0.1208
trigger times: 1
Loss after 126178080 batches: 0.1066
trigger times: 0
Loss after 126309180 batches: 0.0946
trigger times: 1
Loss after 126440280 batches: 0.0836
trigger times: 2
Loss after 126571380 batches: 0.0805
trigger times: 3
Loss after 126702480 batches: 0.0740
trigger times: 4
Loss after 126833580 batches: 0.0679
trigger times: 0
Loss after 126964680 batches: 0.0667
trigger times: 0
Loss after 127095780 batches: 0.0636
trigger times: 1
Loss after 127226880 batches: 0.0643
trigger times: 2
Loss after 127357980 batches: 0.0598
trigger times: 0
Loss after 127489080 batches: 0.0579
trigger times: 1
Loss after 127620180 batches: 0.0536
trigger times: 0
Loss after 127751280 batches: 0.0518
trigger times: 0
Loss after 127882380 batches: 0.0508
trigger times: 1
Loss after 128013480 batches: 0.0524
trigger times: 0
Loss after 128144580 batches: 0.0475
trigger times: 1
Loss after 128275680 batches: 0.0472
trigger times: 0
Loss after 128406780 batches: 0.0480
trigger times: 1
Loss after 128537880 batches: 0.0451
trigger times: 2
Loss after 128668980 batches: 0.0443
trigger times: 3
Loss after 128800080 batches: 0.0466
trigger times: 4
Loss after 128931180 batches: 0.0440
trigger times: 5
Loss after 129062280 batches: 0.0441
trigger times: 6
Loss after 129193380 batches: 0.0420
trigger times: 7
Loss after 129324480 batches: 0.0416
trigger times: 0
Loss after 129455580 batches: 0.0408
trigger times: 1
Loss after 129586680 batches: 0.0414
trigger times: 2
Loss after 129717780 batches: 0.0411
trigger times: 3
Loss after 129848880 batches: 0.0415
trigger times: 4
Loss after 129979980 batches: 0.0388
trigger times: 5
Loss after 130111080 batches: 0.0399
trigger times: 6
Loss after 130242180 batches: 0.0385
trigger times: 7
Loss after 130373280 batches: 0.0391
trigger times: 8
Loss after 130504380 batches: 0.0371
trigger times: 9
Loss after 130635480 batches: 0.0373
trigger times: 10
Loss after 130766580 batches: 0.0378
trigger times: 11
Loss after 130897680 batches: 0.0385
trigger times: 12
Loss after 131028780 batches: 0.0371
trigger times: 13
Loss after 131159880 batches: 0.0372
trigger times: 14
Loss after 131290980 batches: 0.0356
trigger times: 15
Loss after 131422080 batches: 0.0354
trigger times: 16
Loss after 131553180 batches: 0.0377
trigger times: 17
Loss after 131684280 batches: 0.0354
trigger times: 18
Loss after 131815380 batches: 0.0338
trigger times: 19
Loss after 131946480 batches: 0.0349
trigger times: 20
Early stopping!
Start to test process.
Loss after 132077580 batches: 0.0345
Time to train on one home:  379.113974571228
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424]]
Round_3_results:  [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1032 < 1033; dropping {'Training_Loss': 0.16101766194937364, 'Validation_Loss': 0.23741337988111708, 'Training_R2': 0.8380801871755665, 'Validation_R2': 0.7793338102873362, 'Training_F1': 0.779640369908208, 'Validation_F1': 0.7205226907471162, 'Training_NEP': 0.4413222506720968, 'Validation_NEP': 0.5366993339659177, 'Training_NDE': 0.12155607756843384, 'Validation_NDE': 0.1757269212654672, 'Training_MAE': 14.615717705858291, 'Validation_MAE': 14.718522366486143, 'Training_MSE': 534.82776, 'Validation_MSE': 648.95465}.
trigger times: 0
Loss after 132208680 batches: 0.1610
trigger times: 1
Loss after 132339780 batches: 0.0490
trigger times: 0
Loss after 132470880 batches: 0.0337
trigger times: 0
Loss after 132601980 batches: 0.0298
trigger times: 0
Loss after 132733080 batches: 0.0259
trigger times: 0
Loss after 132864180 batches: 0.0241
trigger times: 1
Loss after 132995280 batches: 0.0228
trigger times: 2
Loss after 133126380 batches: 0.0214
trigger times: 3
Loss after 133257480 batches: 0.0209
trigger times: 0
Loss after 133388580 batches: 0.0196
trigger times: 1
Loss after 133519680 batches: 0.0185
trigger times: 2
Loss after 133650780 batches: 0.0183
trigger times: 0
Loss after 133781880 batches: 0.0175
trigger times: 0
Loss after 133912980 batches: 0.0172
trigger times: 0
Loss after 134044080 batches: 0.0165
trigger times: 1
Loss after 134175180 batches: 0.0165
trigger times: 2
Loss after 134306280 batches: 0.0161
trigger times: 3
Loss after 134437380 batches: 0.0159
trigger times: 0
Loss after 134568480 batches: 0.0155
trigger times: 1
Loss after 134699580 batches: 0.0154
trigger times: 2
Loss after 134830680 batches: 0.0151
trigger times: 3
Loss after 134961780 batches: 0.0146
trigger times: 0
Loss after 135092880 batches: 0.0143
trigger times: 1
Loss after 135223980 batches: 0.0144
trigger times: 2
Loss after 135355080 batches: 0.0140
trigger times: 0
Loss after 135486180 batches: 0.0137
trigger times: 1
Loss after 135617280 batches: 0.0137
trigger times: 0
Loss after 135748380 batches: 0.0134
trigger times: 1
Loss after 135879480 batches: 0.0134
trigger times: 2
Loss after 136010580 batches: 0.0131
trigger times: 3
Loss after 136141680 batches: 0.0133
trigger times: 4
Loss after 136272780 batches: 0.0130
trigger times: 5
Loss after 136403880 batches: 0.0130
trigger times: 6
Loss after 136534980 batches: 0.0126
trigger times: 7
Loss after 136666080 batches: 0.0127
trigger times: 0
Loss after 136797180 batches: 0.0120
trigger times: 1
Loss after 136928280 batches: 0.0124
trigger times: 2
Loss after 137059380 batches: 0.0122
trigger times: 3
Loss after 137190480 batches: 0.0123
trigger times: 4
Loss after 137321580 batches: 0.0124
trigger times: 5
Loss after 137452680 batches: 0.0129
trigger times: 6
Loss after 137583780 batches: 0.0119
trigger times: 7
Loss after 137714880 batches: 0.0120
trigger times: 8
Loss after 137845980 batches: 0.0119
trigger times: 9
Loss after 137977080 batches: 0.0118
trigger times: 10
Loss after 138108180 batches: 0.0113
trigger times: 11
Loss after 138239280 batches: 0.0114
trigger times: 12
Loss after 138370380 batches: 0.0112
trigger times: 13
Loss after 138501480 batches: 0.0112
trigger times: 14
Loss after 138632580 batches: 0.0110
trigger times: 15
Loss after 138763680 batches: 0.0111
trigger times: 16
Loss after 138894780 batches: 0.0113
trigger times: 17
Loss after 139025880 batches: 0.0109
trigger times: 18
Loss after 139156980 batches: 0.0108
trigger times: 19
Loss after 139288080 batches: 0.0108
trigger times: 20
Early stopping!
Start to test process.
Loss after 139419180 batches: 0.0106
Time to train on one home:  408.0679495334625
trigger times: 0
Loss after 139521780 batches: 0.4248
trigger times: 1
Loss after 139624380 batches: 0.1729
trigger times: 2
Loss after 139726980 batches: 0.1212
trigger times: 3
Loss after 139829580 batches: 0.0869
trigger times: 4
Loss after 139932180 batches: 0.0734
trigger times: 5
Loss after 140034780 batches: 0.0666
trigger times: 6
Loss after 140137380 batches: 0.0564
trigger times: 7
Loss after 140239980 batches: 0.0543
trigger times: 8
Loss after 140342580 batches: 0.0499
trigger times: 9
Loss after 140445180 batches: 0.0489
trigger times: 10
Loss after 140547780 batches: 0.0501
trigger times: 11
Loss after 140650380 batches: 0.0450
trigger times: 12
Loss after 140752980 batches: 0.0412
trigger times: 13
Loss after 140855580 batches: 0.0397
trigger times: 14
Loss after 140958180 batches: 0.0387
trigger times: 15
Loss after 141060780 batches: 0.0398
trigger times: 16
Loss after 141163380 batches: 0.0369
trigger times: 17
Loss after 141265980 batches: 0.0378
trigger times: 18
Loss after 141368580 batches: 0.0346
trigger times: 19
Loss after 141471180 batches: 0.0345
trigger times: 20
Early stopping!
Start to test process.
Loss after 141573780 batches: 0.0364
Time to train on one home:  133.08829355239868
trigger times: 0
Loss after 141704880 batches: 0.2245
trigger times: 0
Loss after 141835980 batches: 0.0745
trigger times: 0
Loss after 141967080 batches: 0.0513
trigger times: 1
Loss after 142098180 batches: 0.0428
trigger times: 2
Loss after 142229280 batches: 0.0384
trigger times: 3
Loss after 142360380 batches: 0.0346
trigger times: 4
Loss after 142491480 batches: 0.0324
trigger times: 5
Loss after 142622580 batches: 0.0312
trigger times: 6
Loss after 142753680 batches: 0.0291
trigger times: 7
Loss after 142884780 batches: 0.0282
trigger times: 8
Loss after 143015880 batches: 0.0278
trigger times: 9
Loss after 143146980 batches: 0.0264
trigger times: 10
Loss after 143278080 batches: 0.0252
trigger times: 11
Loss after 143409180 batches: 0.0250
trigger times: 12
Loss after 143540280 batches: 0.0240
trigger times: 13
Loss after 143671380 batches: 0.0235
trigger times: 14
Loss after 143802480 batches: 0.0231
trigger times: 15
Loss after 143933580 batches: 0.0229
trigger times: 16
Loss after 144064680 batches: 0.0224
trigger times: 17
Loss after 144195780 batches: 0.0219
trigger times: 18
Loss after 144326880 batches: 0.0216
trigger times: 19
Loss after 144457980 batches: 0.0215
trigger times: 20
Early stopping!
Start to test process.
Loss after 144589080 batches: 0.0211
Time to train on one home:  174.371675491333
trigger times: 0
Loss after 144720180 batches: 0.4518
trigger times: 1
Loss after 144851280 batches: 0.1488
trigger times: 2
Loss after 144982380 batches: 0.1004
trigger times: 3
Loss after 145113480 batches: 0.0820
trigger times: 4
Loss after 145244580 batches: 0.0717
trigger times: 5
Loss after 145375680 batches: 0.0656
trigger times: 6
Loss after 145506780 batches: 0.0608
trigger times: 7
Loss after 145637880 batches: 0.0562
trigger times: 8
Loss after 145768980 batches: 0.0538
trigger times: 9
Loss after 145900080 batches: 0.0515
trigger times: 10
Loss after 146031180 batches: 0.0497
trigger times: 11
Loss after 146162280 batches: 0.0474
trigger times: 12
Loss after 146293380 batches: 0.0456
trigger times: 13
Loss after 146424480 batches: 0.0447
trigger times: 14
Loss after 146555580 batches: 0.0433
trigger times: 15
Loss after 146686680 batches: 0.0420
trigger times: 16
Loss after 146817780 batches: 0.0407
trigger times: 17
Loss after 146948880 batches: 0.0402
trigger times: 18
Loss after 147079980 batches: 0.0387
trigger times: 19
Loss after 147211080 batches: 0.0380
trigger times: 20
Early stopping!
Start to test process.
Loss after 147342180 batches: 0.0375
Time to train on one home:  160.08368253707886
trigger times: 0
Loss after 147470820 batches: 0.1637
trigger times: 0
Loss after 147599460 batches: 0.0541
trigger times: 0
Loss after 147728100 batches: 0.0385
trigger times: 1
Loss after 147856740 batches: 0.0324
trigger times: 0
Loss after 147985380 batches: 0.0291
trigger times: 1
Loss after 148114020 batches: 0.0270
trigger times: 2
Loss after 148242660 batches: 0.0244
trigger times: 0
Loss after 148371300 batches: 0.0230
trigger times: 0
Loss after 148499940 batches: 0.0222
trigger times: 0
Loss after 148628580 batches: 0.0212
trigger times: 1
Loss after 148757220 batches: 0.0208
trigger times: 2
Loss after 148885860 batches: 0.0203
trigger times: 0
Loss after 149014500 batches: 0.0192
trigger times: 1
Loss after 149143140 batches: 0.0187
trigger times: 2
Loss after 149271780 batches: 0.0185
trigger times: 0
Loss after 149400420 batches: 0.0184
trigger times: 1
Loss after 149529060 batches: 0.0173
trigger times: 0
Loss after 149657700 batches: 0.0173
trigger times: 1
Loss after 149786340 batches: 0.0170
trigger times: 2
Loss after 149914980 batches: 0.0168
trigger times: 3
Loss after 150043620 batches: 0.0168
trigger times: 4
Loss after 150172260 batches: 0.0159
trigger times: 5
Loss after 150300900 batches: 0.0160
trigger times: 6
Loss after 150429540 batches: 0.0155
trigger times: 7
Loss after 150558180 batches: 0.0153
trigger times: 8
Loss after 150686820 batches: 0.0153
trigger times: 9
Loss after 150815460 batches: 0.0155
trigger times: 10
Loss after 150944100 batches: 0.0154
trigger times: 11
Loss after 151072740 batches: 0.0154
trigger times: 0
Loss after 151201380 batches: 0.0148
trigger times: 1
Loss after 151330020 batches: 0.0146
trigger times: 2
Loss after 151458660 batches: 0.0144
trigger times: 3
Loss after 151587300 batches: 0.0142
trigger times: 4
Loss after 151715940 batches: 0.0142
trigger times: 5
Loss after 151844580 batches: 0.0142
trigger times: 6
Loss after 151973220 batches: 0.0142
trigger times: 7
Loss after 152101860 batches: 0.0138
trigger times: 8
Loss after 152230500 batches: 0.0138
trigger times: 9
Loss after 152359140 batches: 0.0136
trigger times: 10
Loss after 152487780 batches: 0.0136
trigger times: 11
Loss after 152616420 batches: 0.0135
trigger times: 0
Loss after 152745060 batches: 0.0134
trigger times: 1
Loss after 152873700 batches: 0.0133
trigger times: 2
Loss after 153002340 batches: 0.0134
trigger times: 0
Loss after 153130980 batches: 0.0130
trigger times: 1
Loss after 153259620 batches: 0.0130
trigger times: 2
Loss after 153388260 batches: 0.0130
trigger times: 3
Loss after 153516900 batches: 0.0127
trigger times: 4
Loss after 153645540 batches: 0.0128
trigger times: 0
Loss after 153774180 batches: 0.0125
trigger times: 1
Loss after 153902820 batches: 0.0130
trigger times: 2
Loss after 154031460 batches: 0.0126
trigger times: 3
Loss after 154160100 batches: 0.0124
trigger times: 4
Loss after 154288740 batches: 0.0124
trigger times: 5
Loss after 154417380 batches: 0.0122
trigger times: 6
Loss after 154546020 batches: 0.0120
trigger times: 7
Loss after 154674660 batches: 0.0121
trigger times: 8
Loss after 154803300 batches: 0.0122
trigger times: 9
Loss after 154931940 batches: 0.0120
trigger times: 10
Loss after 155060580 batches: 0.0122
trigger times: 11
Loss after 155189220 batches: 0.0119
trigger times: 12
Loss after 155317860 batches: 0.0117
trigger times: 13
Loss after 155446500 batches: 0.0118
trigger times: 14
Loss after 155575140 batches: 0.0116
trigger times: 15
Loss after 155703780 batches: 0.0117
trigger times: 16
Loss after 155832420 batches: 0.0118
trigger times: 17
Loss after 155961060 batches: 0.0119
trigger times: 0
Loss after 156089700 batches: 0.0122
trigger times: 0
Loss after 156218340 batches: 0.0116
trigger times: 1
Loss after 156346980 batches: 0.0117
trigger times: 2
Loss after 156475620 batches: 0.0116
trigger times: 3
Loss after 156604260 batches: 0.0113
trigger times: 4
Loss after 156732900 batches: 0.0113
trigger times: 5
Loss after 156861540 batches: 0.0113
trigger times: 6
Loss after 156990180 batches: 0.0111
trigger times: 7
Loss after 157118820 batches: 0.0112
trigger times: 8
Loss after 157247460 batches: 0.0112
trigger times: 9
Loss after 157376100 batches: 0.0111
trigger times: 10
Loss after 157504740 batches: 0.0112
trigger times: 11
Loss after 157633380 batches: 0.0112
trigger times: 12
Loss after 157762020 batches: 0.0111
trigger times: 13
Loss after 157890660 batches: 0.0113
trigger times: 14
Loss after 158019300 batches: 0.0110
trigger times: 15
Loss after 158147940 batches: 0.0108
trigger times: 16
Loss after 158276580 batches: 0.0110
trigger times: 17
Loss after 158405220 batches: 0.0109
trigger times: 18
Loss after 158533860 batches: 0.0108
trigger times: 19
Loss after 158662500 batches: 0.0109
trigger times: 20
Early stopping!
Start to test process.
Loss after 158791140 batches: 0.0109
Time to train on one home:  632.2922353744507
trigger times: 0
Loss after 158922240 batches: 0.4800
trigger times: 1
Loss after 159053340 batches: 0.1422
trigger times: 2
Loss after 159184440 batches: 0.0903
trigger times: 3
Loss after 159315540 batches: 0.0733
trigger times: 4
Loss after 159446640 batches: 0.0638
trigger times: 5
Loss after 159577740 batches: 0.0583
trigger times: 6
Loss after 159708840 batches: 0.0543
trigger times: 7
Loss after 159839940 batches: 0.0517
trigger times: 8
Loss after 159971040 batches: 0.0478
trigger times: 9
Loss after 160102140 batches: 0.0463
trigger times: 10
Loss after 160233240 batches: 0.0444
trigger times: 11
Loss after 160364340 batches: 0.0425
trigger times: 12
Loss after 160495440 batches: 0.0414
trigger times: 13
Loss after 160626540 batches: 0.0404
trigger times: 14
Loss after 160757640 batches: 0.0388
trigger times: 15
Loss after 160888740 batches: 0.0385
trigger times: 16
Loss after 161019840 batches: 0.0378
trigger times: 17
Loss after 161150940 batches: 0.0364
trigger times: 18
Loss after 161282040 batches: 0.0362
trigger times: 19
Loss after 161413140 batches: 0.0353
trigger times: 20
Early stopping!
Start to test process.
Loss after 161544240 batches: 0.0349
Time to train on one home:  161.15816688537598
trigger times: 0
Loss after 161675340 batches: 0.5051
trigger times: 0
Loss after 161806440 batches: 0.2243
trigger times: 0
Loss after 161937540 batches: 0.1512
trigger times: 1
Loss after 162068640 batches: 0.1105
trigger times: 0
Loss after 162199740 batches: 0.0870
trigger times: 1
Loss after 162330840 batches: 0.0745
trigger times: 0
Loss after 162461940 batches: 0.0676
trigger times: 1
Loss after 162593040 batches: 0.0612
trigger times: 2
Loss after 162724140 batches: 0.0571
trigger times: 3
Loss after 162855240 batches: 0.0543
trigger times: 4
Loss after 162986340 batches: 0.0516
trigger times: 0
Loss after 163117440 batches: 0.0491
trigger times: 1
Loss after 163248540 batches: 0.0462
trigger times: 2
Loss after 163379640 batches: 0.0461
trigger times: 3
Loss after 163510740 batches: 0.0471
trigger times: 4
Loss after 163641840 batches: 0.0435
trigger times: 0
Loss after 163772940 batches: 0.0416
trigger times: 1
Loss after 163904040 batches: 0.0395
trigger times: 2
Loss after 164035140 batches: 0.0395
trigger times: 3
Loss after 164166240 batches: 0.0394
trigger times: 0
Loss after 164297340 batches: 0.0398
trigger times: 1
Loss after 164428440 batches: 0.0399
trigger times: 2
Loss after 164559540 batches: 0.0395
trigger times: 3
Loss after 164690640 batches: 0.0394
trigger times: 0
Loss after 164821740 batches: 0.0374
trigger times: 0
Loss after 164952840 batches: 0.0373
trigger times: 1
Loss after 165083940 batches: 0.0357
trigger times: 0
Loss after 165215040 batches: 0.0354
trigger times: 1
Loss after 165346140 batches: 0.0359
trigger times: 2
Loss after 165477240 batches: 0.0350
trigger times: 3
Loss after 165608340 batches: 0.0353
trigger times: 4
Loss after 165739440 batches: 0.0332
trigger times: 5
Loss after 165870540 batches: 0.0338
trigger times: 6
Loss after 166001640 batches: 0.0329
trigger times: 7
Loss after 166132740 batches: 0.0325
trigger times: 8
Loss after 166263840 batches: 0.0319
trigger times: 9
Loss after 166394940 batches: 0.0321
trigger times: 10
Loss after 166526040 batches: 0.0319
trigger times: 11
Loss after 166657140 batches: 0.0329
trigger times: 12
Loss after 166788240 batches: 0.0330
trigger times: 13
Loss after 166919340 batches: 0.0337
trigger times: 14
Loss after 167050440 batches: 0.0324
trigger times: 15
Loss after 167181540 batches: 0.0316
trigger times: 16
Loss after 167312640 batches: 0.0310
trigger times: 0
Loss after 167443740 batches: 0.0305
trigger times: 1
Loss after 167574840 batches: 0.0307
trigger times: 0
Loss after 167705940 batches: 0.0300
trigger times: 1
Loss after 167837040 batches: 0.0300
trigger times: 2
Loss after 167968140 batches: 0.0291
trigger times: 3
Loss after 168099240 batches: 0.0311
trigger times: 4
Loss after 168230340 batches: 0.0299
trigger times: 5
Loss after 168361440 batches: 0.0288
trigger times: 6
Loss after 168492540 batches: 0.0280
trigger times: 0
Loss after 168623640 batches: 0.0277
trigger times: 1
Loss after 168754740 batches: 0.0277
trigger times: 2
Loss after 168885840 batches: 0.0276
trigger times: 0
Loss after 169016940 batches: 0.0283
trigger times: 1
Loss after 169148040 batches: 0.0282
trigger times: 2
Loss after 169279140 batches: 0.0277
trigger times: 3
Loss after 169410240 batches: 0.0281
trigger times: 4
Loss after 169541340 batches: 0.0277
trigger times: 5
Loss after 169672440 batches: 0.0275
trigger times: 6
Loss after 169803540 batches: 0.0269
trigger times: 7
Loss after 169934640 batches: 0.0274
trigger times: 8
Loss after 170065740 batches: 0.0269
trigger times: 9
Loss after 170196840 batches: 0.0268
trigger times: 10
Loss after 170327940 batches: 0.0274
trigger times: 11
Loss after 170459040 batches: 0.0255
trigger times: 0
Loss after 170590140 batches: 0.0259
trigger times: 1
Loss after 170721240 batches: 0.0260
trigger times: 2
Loss after 170852340 batches: 0.0273
trigger times: 3
Loss after 170983440 batches: 0.0258
trigger times: 4
Loss after 171114540 batches: 0.0267
trigger times: 5
Loss after 171245640 batches: 0.0267
trigger times: 6
Loss after 171376740 batches: 0.0285
trigger times: 7
Loss after 171507840 batches: 0.0274
trigger times: 8
Loss after 171638940 batches: 0.0257
trigger times: 9
Loss after 171770040 batches: 0.0256
trigger times: 10
Loss after 171901140 batches: 0.0263
trigger times: 11
Loss after 172032240 batches: 0.0263
trigger times: 12
Loss after 172163340 batches: 0.0255
trigger times: 13
Loss after 172294440 batches: 0.0254
trigger times: 14
Loss after 172425540 batches: 0.0247
trigger times: 15
Loss after 172556640 batches: 0.0244
trigger times: 16
Loss after 172687740 batches: 0.0245
trigger times: 17
Loss after 172818840 batches: 0.0244
trigger times: 18
Loss after 172949940 batches: 0.0243
trigger times: 19
Loss after 173081040 batches: 0.0252
trigger times: 20
Early stopping!
Start to test process.
Loss after 173212140 batches: 0.0246
Time to train on one home:  641.8178000450134
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658]]
Round_4_results:  [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658]
trigger times: 0
Loss after 173343240 batches: 0.1315
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1352 < 1353; dropping {'Training_Loss': 0.131458090793974, 'Validation_Loss': 0.23010776937007904, 'Training_R2': 0.8677872746685384, 'Validation_R2': 0.7863534224375515, 'Training_F1': 0.8006456363969608, 'Validation_F1': 0.7068037812812213, 'Training_NEP': 0.3993323821653175, 'Validation_NEP': 0.5403447939871625, 'Training_NDE': 0.09925443968584008, 'Validation_NDE': 0.1701368721816397, 'Training_MAE': 13.225096535802706, 'Validation_MAE': 14.81849600435585, 'Training_MSE': 436.7039, 'Validation_MSE': 628.3107}.
trigger times: 0
Loss after 173474340 batches: 0.0401
trigger times: 1
Loss after 173605440 batches: 0.0292
trigger times: 2
Loss after 173736540 batches: 0.0256
trigger times: 0
Loss after 173867640 batches: 0.0232
trigger times: 0
Loss after 173998740 batches: 0.0204
trigger times: 1
Loss after 174129840 batches: 0.0197
trigger times: 0
Loss after 174260940 batches: 0.0186
trigger times: 0
Loss after 174392040 batches: 0.0183
trigger times: 1
Loss after 174523140 batches: 0.0174
trigger times: 2
Loss after 174654240 batches: 0.0167
trigger times: 3
Loss after 174785340 batches: 0.0165
trigger times: 0
Loss after 174916440 batches: 0.0158
trigger times: 0
Loss after 175047540 batches: 0.0154
trigger times: 1
Loss after 175178640 batches: 0.0151
trigger times: 2
Loss after 175309740 batches: 0.0147
trigger times: 3
Loss after 175440840 batches: 0.0146
trigger times: 4
Loss after 175571940 batches: 0.0145
trigger times: 5
Loss after 175703040 batches: 0.0139
trigger times: 0
Loss after 175834140 batches: 0.0135
trigger times: 1
Loss after 175965240 batches: 0.0136
trigger times: 2
Loss after 176096340 batches: 0.0137
trigger times: 3
Loss after 176227440 batches: 0.0133
trigger times: 4
Loss after 176358540 batches: 0.0132
trigger times: 5
Loss after 176489640 batches: 0.0128
trigger times: 6
Loss after 176620740 batches: 0.0127
trigger times: 0
Loss after 176751840 batches: 0.0128
trigger times: 1
Loss after 176882940 batches: 0.0125
trigger times: 2
Loss after 177014040 batches: 0.0124
trigger times: 3
Loss after 177145140 batches: 0.0122
trigger times: 4
Loss after 177276240 batches: 0.0122
trigger times: 5
Loss after 177407340 batches: 0.0121
trigger times: 6
Loss after 177538440 batches: 0.0121
trigger times: 7
Loss after 177669540 batches: 0.0119
trigger times: 8
Loss after 177800640 batches: 0.0114
trigger times: 9
Loss after 177931740 batches: 0.0118
trigger times: 10
Loss after 178062840 batches: 0.0120
trigger times: 11
Loss after 178193940 batches: 0.0114
trigger times: 12
Loss after 178325040 batches: 0.0114
trigger times: 13
Loss after 178456140 batches: 0.0111
trigger times: 14
Loss after 178587240 batches: 0.0112
trigger times: 15
Loss after 178718340 batches: 0.0114
trigger times: 16
Loss after 178849440 batches: 0.0113
trigger times: 17
Loss after 178980540 batches: 0.0109
trigger times: 18
Loss after 179111640 batches: 0.0109
trigger times: 19
Loss after 179242740 batches: 0.0106
trigger times: 20
Early stopping!
Start to test process.
Loss after 179373840 batches: 0.0107
Time to train on one home:  345.25984287261963
trigger times: 0
Loss after 179476440 batches: 0.3786
trigger times: 1
Loss after 179579040 batches: 0.1514
trigger times: 0
Loss after 179681640 batches: 0.0918
trigger times: 1
Loss after 179784240 batches: 0.0701
trigger times: 2
Loss after 179886840 batches: 0.0584
trigger times: 3
Loss after 179989440 batches: 0.0519
trigger times: 4
Loss after 180092040 batches: 0.0491
trigger times: 5
Loss after 180194640 batches: 0.0447
trigger times: 6
Loss after 180297240 batches: 0.0426
trigger times: 7
Loss after 180399840 batches: 0.0408
trigger times: 8
Loss after 180502440 batches: 0.0407
trigger times: 9
Loss after 180605040 batches: 0.0366
trigger times: 10
Loss after 180707640 batches: 0.0369
trigger times: 11
Loss after 180810240 batches: 0.0350
trigger times: 12
Loss after 180912840 batches: 0.0350
trigger times: 13
Loss after 181015440 batches: 0.0328
trigger times: 14
Loss after 181118040 batches: 0.0332
trigger times: 0
Loss after 181220640 batches: 0.0325
trigger times: 0
Loss after 181323240 batches: 0.0315
trigger times: 1
Loss after 181425840 batches: 0.0314
trigger times: 2
Loss after 181528440 batches: 0.0321
trigger times: 0
Loss after 181631040 batches: 0.0304
trigger times: 1
Loss after 181733640 batches: 0.0279
trigger times: 2
Loss after 181836240 batches: 0.0264
trigger times: 3
Loss after 181938840 batches: 0.0272
trigger times: 0
Loss after 182041440 batches: 0.0299
trigger times: 1
Loss after 182144040 batches: 0.0282
trigger times: 0
Loss after 182246640 batches: 0.0276
trigger times: 1
Loss after 182349240 batches: 0.0257
trigger times: 2
Loss after 182451840 batches: 0.0255
trigger times: 3
Loss after 182554440 batches: 0.0272
trigger times: 4
Loss after 182657040 batches: 0.0269
trigger times: 5
Loss after 182759640 batches: 0.0297
trigger times: 6
Loss after 182862240 batches: 0.0259
trigger times: 0
Loss after 182964840 batches: 0.0282
trigger times: 1
Loss after 183067440 batches: 0.0249
trigger times: 2
Loss after 183170040 batches: 0.0250
trigger times: 0
Loss after 183272640 batches: 0.0233
trigger times: 0
Loss after 183375240 batches: 0.0246
trigger times: 0
Loss after 183477840 batches: 0.0238
trigger times: 1
Loss after 183580440 batches: 0.0241
trigger times: 2
Loss after 183683040 batches: 0.0233
trigger times: 3
Loss after 183785640 batches: 0.0246
trigger times: 4
Loss after 183888240 batches: 0.0273
trigger times: 5
Loss after 183990840 batches: 0.0238
trigger times: 6
Loss after 184093440 batches: 0.0237
trigger times: 0
Loss after 184196040 batches: 0.0236
trigger times: 0
Loss after 184298640 batches: 0.0235
trigger times: 1
Loss after 184401240 batches: 0.0242
trigger times: 2
Loss after 184503840 batches: 0.0215
trigger times: 3
Loss after 184606440 batches: 0.0224
trigger times: 4
Loss after 184709040 batches: 0.0294
trigger times: 5
Loss after 184811640 batches: 0.0239
trigger times: 6
Loss after 184914240 batches: 0.0225
trigger times: 7
Loss after 185016840 batches: 0.0212
trigger times: 8
Loss after 185119440 batches: 0.0217
trigger times: 9
Loss after 185222040 batches: 0.0282
trigger times: 0
Loss after 185324640 batches: 0.0254
trigger times: 1
Loss after 185427240 batches: 0.0204
trigger times: 2
Loss after 185529840 batches: 0.0204
trigger times: 0
Loss after 185632440 batches: 0.0204
trigger times: 1
Loss after 185735040 batches: 0.0202
trigger times: 2
Loss after 185837640 batches: 0.0229
trigger times: 3
Loss after 185940240 batches: 0.0202
trigger times: 4
Loss after 186042840 batches: 0.0202
trigger times: 0
Loss after 186145440 batches: 0.0202
trigger times: 1
Loss after 186248040 batches: 0.0217
trigger times: 2
Loss after 186350640 batches: 0.0217
trigger times: 3
Loss after 186453240 batches: 0.0207
trigger times: 4
Loss after 186555840 batches: 0.0200
trigger times: 5
Loss after 186658440 batches: 0.0204
trigger times: 6
Loss after 186761040 batches: 0.0209
trigger times: 7
Loss after 186863640 batches: 0.0206
trigger times: 8
Loss after 186966240 batches: 0.0199
trigger times: 9
Loss after 187068840 batches: 0.0188
trigger times: 10
Loss after 187171440 batches: 0.0187
trigger times: 11
Loss after 187274040 batches: 0.0194
trigger times: 12
Loss after 187376640 batches: 0.0194
trigger times: 13
Loss after 187479240 batches: 0.0196
trigger times: 14
Loss after 187581840 batches: 0.0207
trigger times: 15
Loss after 187684440 batches: 0.0212
trigger times: 16
Loss after 187787040 batches: 0.0196
trigger times: 17
Loss after 187889640 batches: 0.0200
trigger times: 18
Loss after 187992240 batches: 0.0209
trigger times: 19
Loss after 188094840 batches: 0.0188
trigger times: 20
Early stopping!
Start to test process.
Loss after 188197440 batches: 0.0184
Time to train on one home:  506.33291459083557
trigger times: 0
Loss after 188328540 batches: 0.2305
trigger times: 1
Loss after 188459640 batches: 0.0719
trigger times: 2
Loss after 188590740 batches: 0.0489
trigger times: 3
Loss after 188721840 batches: 0.0405
trigger times: 4
Loss after 188852940 batches: 0.0363
trigger times: 5
Loss after 188984040 batches: 0.0330
trigger times: 6
Loss after 189115140 batches: 0.0309
trigger times: 7
Loss after 189246240 batches: 0.0293
trigger times: 8
Loss after 189377340 batches: 0.0282
trigger times: 9
Loss after 189508440 batches: 0.0268
trigger times: 10
Loss after 189639540 batches: 0.0260
trigger times: 11
Loss after 189770640 batches: 0.0251
trigger times: 12
Loss after 189901740 batches: 0.0241
trigger times: 13
Loss after 190032840 batches: 0.0236
trigger times: 14
Loss after 190163940 batches: 0.0232
trigger times: 15
Loss after 190295040 batches: 0.0226
trigger times: 16
Loss after 190426140 batches: 0.0221
trigger times: 17
Loss after 190557240 batches: 0.0218
trigger times: 18
Loss after 190688340 batches: 0.0215
trigger times: 19
Loss after 190819440 batches: 0.0213
trigger times: 20
Early stopping!
Start to test process.
Loss after 190950540 batches: 0.0210
Time to train on one home:  160.35089349746704
trigger times: 0
Loss after 191081640 batches: 0.3830
trigger times: 1
Loss after 191212740 batches: 0.1207
trigger times: 2
Loss after 191343840 batches: 0.0844
trigger times: 3
Loss after 191474940 batches: 0.0709
trigger times: 4
Loss after 191606040 batches: 0.0622
trigger times: 5
Loss after 191737140 batches: 0.0574
trigger times: 6
Loss after 191868240 batches: 0.0531
trigger times: 7
Loss after 191999340 batches: 0.0507
trigger times: 8
Loss after 192130440 batches: 0.0482
trigger times: 9
Loss after 192261540 batches: 0.0458
trigger times: 10
Loss after 192392640 batches: 0.0436
trigger times: 11
Loss after 192523740 batches: 0.0435
trigger times: 12
Loss after 192654840 batches: 0.0408
trigger times: 13
Loss after 192785940 batches: 0.0400
trigger times: 14
Loss after 192917040 batches: 0.0386
trigger times: 15
Loss after 193048140 batches: 0.0376
trigger times: 16
Loss after 193179240 batches: 0.0367
trigger times: 17
Loss after 193310340 batches: 0.0362
trigger times: 18
Loss after 193441440 batches: 0.0353
trigger times: 19
Loss after 193572540 batches: 0.0348
trigger times: 20
Early stopping!
Start to test process.
Loss after 193703640 batches: 0.0343
Time to train on one home:  160.59544777870178
trigger times: 0
Loss after 193832280 batches: 0.1432
trigger times: 0
Loss after 193960920 batches: 0.0430
trigger times: 0
Loss after 194089560 batches: 0.0309
trigger times: 0
Loss after 194218200 batches: 0.0266
trigger times: 1
Loss after 194346840 batches: 0.0239
trigger times: 2
Loss after 194475480 batches: 0.0221
trigger times: 3
Loss after 194604120 batches: 0.0208
trigger times: 4
Loss after 194732760 batches: 0.0197
trigger times: 5
Loss after 194861400 batches: 0.0190
trigger times: 6
Loss after 194990040 batches: 0.0182
trigger times: 7
Loss after 195118680 batches: 0.0178
trigger times: 0
Loss after 195247320 batches: 0.0175
trigger times: 1
Loss after 195375960 batches: 0.0165
trigger times: 2
Loss after 195504600 batches: 0.0163
trigger times: 3
Loss after 195633240 batches: 0.0161
trigger times: 4
Loss after 195761880 batches: 0.0155
trigger times: 5
Loss after 195890520 batches: 0.0156
trigger times: 6
Loss after 196019160 batches: 0.0154
trigger times: 7
Loss after 196147800 batches: 0.0151
trigger times: 8
Loss after 196276440 batches: 0.0149
trigger times: 9
Loss after 196405080 batches: 0.0146
trigger times: 10
Loss after 196533720 batches: 0.0145
trigger times: 11
Loss after 196662360 batches: 0.0144
trigger times: 12
Loss after 196791000 batches: 0.0139
trigger times: 13
Loss after 196919640 batches: 0.0139
trigger times: 14
Loss after 197048280 batches: 0.0141
trigger times: 15
Loss after 197176920 batches: 0.0140
trigger times: 16
Loss after 197305560 batches: 0.0134
trigger times: 17
Loss after 197434200 batches: 0.0133
trigger times: 18
Loss after 197562840 batches: 0.0134
trigger times: 19
Loss after 197691480 batches: 0.0134
trigger times: 20
Early stopping!
Start to test process.
Loss after 197820120 batches: 0.0132
Time to train on one home:  234.65875029563904
trigger times: 0
Loss after 197951220 batches: 0.3930
trigger times: 0
Loss after 198082320 batches: 0.1105
trigger times: 1
Loss after 198213420 batches: 0.0758
trigger times: 0
Loss after 198344520 batches: 0.0628
trigger times: 1
Loss after 198475620 batches: 0.0559
trigger times: 2
Loss after 198606720 batches: 0.0516
trigger times: 3
Loss after 198737820 batches: 0.0486
trigger times: 4
Loss after 198868920 batches: 0.0460
trigger times: 5
Loss after 199000020 batches: 0.0434
trigger times: 6
Loss after 199131120 batches: 0.0423
trigger times: 7
Loss after 199262220 batches: 0.0400
trigger times: 8
Loss after 199393320 batches: 0.0388
trigger times: 9
Loss after 199524420 batches: 0.0374
trigger times: 0
Loss after 199655520 batches: 0.0367
trigger times: 1
Loss after 199786620 batches: 0.0360
trigger times: 2
Loss after 199917720 batches: 0.0347
trigger times: 3
Loss after 200048820 batches: 0.0345
trigger times: 4
Loss after 200179920 batches: 0.0337
trigger times: 0
Loss after 200311020 batches: 0.0329
trigger times: 1
Loss after 200442120 batches: 0.0327
trigger times: 2
Loss after 200573220 batches: 0.0323
trigger times: 3
Loss after 200704320 batches: 0.0317
trigger times: 0
Loss after 200835420 batches: 0.0311
trigger times: 1
Loss after 200966520 batches: 0.0307
trigger times: 0
Loss after 201097620 batches: 0.0300
trigger times: 1
Loss after 201228720 batches: 0.0302
trigger times: 0
Loss after 201359820 batches: 0.0297
trigger times: 1
Loss after 201490920 batches: 0.0292
trigger times: 0
Loss after 201622020 batches: 0.0292
trigger times: 0
Loss after 201753120 batches: 0.0284
trigger times: 1
Loss after 201884220 batches: 0.0284
trigger times: 2
Loss after 202015320 batches: 0.0282
trigger times: 3
Loss after 202146420 batches: 0.0277
trigger times: 0
Loss after 202277520 batches: 0.0277
trigger times: 1
Loss after 202408620 batches: 0.0277
trigger times: 2
Loss after 202539720 batches: 0.0277
trigger times: 3
Loss after 202670820 batches: 0.0266
trigger times: 4
Loss after 202801920 batches: 0.0263
trigger times: 5
Loss after 202933020 batches: 0.0264
trigger times: 6
Loss after 203064120 batches: 0.0266
trigger times: 7
Loss after 203195220 batches: 0.0260
trigger times: 8
Loss after 203326320 batches: 0.0257
trigger times: 9
Loss after 203457420 batches: 0.0253
trigger times: 10
Loss after 203588520 batches: 0.0253
trigger times: 11
Loss after 203719620 batches: 0.0249
trigger times: 12
Loss after 203850720 batches: 0.0253
trigger times: 13
Loss after 203981820 batches: 0.0247
trigger times: 14
Loss after 204112920 batches: 0.0244
trigger times: 15
Loss after 204244020 batches: 0.0247
trigger times: 16
Loss after 204375120 batches: 0.0240
trigger times: 17
Loss after 204506220 batches: 0.0238
trigger times: 18
Loss after 204637320 batches: 0.0241
trigger times: 19
Loss after 204768420 batches: 0.0234
trigger times: 20
Early stopping!
Start to test process.
Loss after 204899520 batches: 0.0239
Time to train on one home:  394.4545772075653
trigger times: 0
Loss after 205030620 batches: 0.5665
trigger times: 0
Loss after 205161720 batches: 0.2433
trigger times: 0
Loss after 205292820 batches: 0.1438
trigger times: 0
Loss after 205423920 batches: 0.1053
trigger times: 0
Loss after 205555020 batches: 0.0859
trigger times: 0
Loss after 205686120 batches: 0.0719
trigger times: 1
Loss after 205817220 batches: 0.0633
trigger times: 0
Loss after 205948320 batches: 0.0567
trigger times: 1
Loss after 206079420 batches: 0.0520
trigger times: 0
Loss after 206210520 batches: 0.0501
trigger times: 0
Loss after 206341620 batches: 0.0481
trigger times: 1
Loss after 206472720 batches: 0.0455
trigger times: 2
Loss after 206603820 batches: 0.0451
trigger times: 0
Loss after 206734920 batches: 0.0416
trigger times: 1
Loss after 206866020 batches: 0.0420
trigger times: 0
Loss after 206997120 batches: 0.0406
trigger times: 1
Loss after 207128220 batches: 0.0395
trigger times: 0
Loss after 207259320 batches: 0.0397
trigger times: 1
Loss after 207390420 batches: 0.0388
trigger times: 0
Loss after 207521520 batches: 0.0389
trigger times: 1
Loss after 207652620 batches: 0.0387
trigger times: 2
Loss after 207783720 batches: 0.0375
trigger times: 3
Loss after 207914820 batches: 0.0358
trigger times: 4
Loss after 208045920 batches: 0.0344
trigger times: 5
Loss after 208177020 batches: 0.0355
trigger times: 6
Loss after 208308120 batches: 0.0354
trigger times: 7
Loss after 208439220 batches: 0.0338
trigger times: 8
Loss after 208570320 batches: 0.0329
trigger times: 9
Loss after 208701420 batches: 0.0324
trigger times: 10
Loss after 208832520 batches: 0.0325
trigger times: 0
Loss after 208963620 batches: 0.0328
trigger times: 0
Loss after 209094720 batches: 0.0324
trigger times: 1
Loss after 209225820 batches: 0.0312
trigger times: 0
Loss after 209356920 batches: 0.0317
trigger times: 1
Loss after 209488020 batches: 0.0308
trigger times: 0
Loss after 209619120 batches: 0.0308
trigger times: 1
Loss after 209750220 batches: 0.0311
trigger times: 2
Loss after 209881320 batches: 0.0302
trigger times: 3
Loss after 210012420 batches: 0.0305
trigger times: 0
Loss after 210143520 batches: 0.0297
trigger times: 1
Loss after 210274620 batches: 0.0292
trigger times: 2
Loss after 210405720 batches: 0.0287
trigger times: 3
Loss after 210536820 batches: 0.0291
trigger times: 0
Loss after 210667920 batches: 0.0284
trigger times: 1
Loss after 210799020 batches: 0.0286
trigger times: 0
Loss after 210930120 batches: 0.0290
trigger times: 1
Loss after 211061220 batches: 0.0276
trigger times: 2
Loss after 211192320 batches: 0.0281
trigger times: 3
Loss after 211323420 batches: 0.0283
trigger times: 4
Loss after 211454520 batches: 0.0280
trigger times: 5
Loss after 211585620 batches: 0.0278
trigger times: 6
Loss after 211716720 batches: 0.0274
trigger times: 7
Loss after 211847820 batches: 0.0268
trigger times: 8
Loss after 211978920 batches: 0.0284
trigger times: 9
Loss after 212110020 batches: 0.0284
trigger times: 10
Loss after 212241120 batches: 0.0262
trigger times: 0
Loss after 212372220 batches: 0.0264
trigger times: 1
Loss after 212503320 batches: 0.0268
trigger times: 2
Loss after 212634420 batches: 0.0265
trigger times: 3
Loss after 212765520 batches: 0.0265
trigger times: 4
Loss after 212896620 batches: 0.0262
trigger times: 0
Loss after 213027720 batches: 0.0262
trigger times: 1
Loss after 213158820 batches: 0.0265
trigger times: 2
Loss after 213289920 batches: 0.0256
trigger times: 3
Loss after 213421020 batches: 0.0261
trigger times: 4
Loss after 213552120 batches: 0.0253
trigger times: 5
Loss after 213683220 batches: 0.0250
trigger times: 6
Loss after 213814320 batches: 0.0247
trigger times: 7
Loss after 213945420 batches: 0.0253
trigger times: 8
Loss after 214076520 batches: 0.0251
trigger times: 9
Loss after 214207620 batches: 0.0237
trigger times: 10
Loss after 214338720 batches: 0.0240
trigger times: 11
Loss after 214469820 batches: 0.0254
trigger times: 12
Loss after 214600920 batches: 0.0239
trigger times: 13
Loss after 214732020 batches: 0.0239
trigger times: 14
Loss after 214863120 batches: 0.0231
trigger times: 15
Loss after 214994220 batches: 0.0234
trigger times: 16
Loss after 215125320 batches: 0.0242
trigger times: 17
Loss after 215256420 batches: 0.0235
trigger times: 0
Loss after 215387520 batches: 0.0242
trigger times: 1
Loss after 215518620 batches: 0.0233
trigger times: 2
Loss after 215649720 batches: 0.0232
trigger times: 3
Loss after 215780820 batches: 0.0242
trigger times: 4
Loss after 215911920 batches: 0.0234
trigger times: 5
Loss after 216043020 batches: 0.0256
trigger times: 6
Loss after 216174120 batches: 0.0247
trigger times: 7
Loss after 216305220 batches: 0.0231
trigger times: 8
Loss after 216436320 batches: 0.0229
trigger times: 9
Loss after 216567420 batches: 0.0228
trigger times: 10
Loss after 216698520 batches: 0.0230
trigger times: 11
Loss after 216829620 batches: 0.0228
trigger times: 12
Loss after 216960720 batches: 0.0223
trigger times: 0
Loss after 217091820 batches: 0.0232
trigger times: 1
Loss after 217222920 batches: 0.0239
trigger times: 0
Loss after 217354020 batches: 0.0234
trigger times: 1
Loss after 217485120 batches: 0.0222
trigger times: 2
Loss after 217616220 batches: 0.0233
trigger times: 3
Loss after 217747320 batches: 0.0228
trigger times: 4
Loss after 217878420 batches: 0.0225
trigger times: 5
Loss after 218009520 batches: 0.0221
trigger times: 6
Loss after 218140620 batches: 0.0221
trigger times: 7
Loss after 218271720 batches: 0.0215
trigger times: 8
Loss after 218402820 batches: 0.0220
trigger times: 9
Loss after 218533920 batches: 0.0218
trigger times: 10
Loss after 218665020 batches: 0.0224
trigger times: 11
Loss after 218796120 batches: 0.0214
trigger times: 12
Loss after 218927220 batches: 0.0216
trigger times: 13
Loss after 219058320 batches: 0.0218
trigger times: 14
Loss after 219189420 batches: 0.0226
trigger times: 15
Loss after 219320520 batches: 0.0212
trigger times: 16
Loss after 219451620 batches: 0.0217
trigger times: 17
Loss after 219582720 batches: 0.0222
trigger times: 18
Loss after 219713820 batches: 0.0227
trigger times: 19
Loss after 219844920 batches: 0.0207
trigger times: 20
Early stopping!
Start to test process.
Loss after 219976020 batches: 0.0212
Time to train on one home:  824.0086350440979
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738]]
Round_5_results:  [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1728 < 1729; dropping {'Training_Loss': 0.15300700658897184, 'Validation_Loss': 0.2640290922588772, 'Training_R2': 0.8460870070748522, 'Validation_R2': 0.7547118105661119, 'Training_F1': 0.7831389765379034, 'Validation_F1': 0.6998846834236093, 'Training_NEP': 0.4338967184904695, 'Validation_NEP': 0.5684720935770029, 'Training_NDE': 0.1155452157487668, 'Validation_NDE': 0.19533458391665973, 'Training_MAE': 14.36979880642109, 'Validation_MAE': 15.589863252127042, 'Training_MSE': 508.3808, 'Validation_MSE': 721.36523}.
trigger times: 0
Loss after 220107120 batches: 0.1530
trigger times: 0
Loss after 220238220 batches: 0.0412
trigger times: 1
Loss after 220369320 batches: 0.0286
trigger times: 0
Loss after 220500420 batches: 0.0245
trigger times: 0
Loss after 220631520 batches: 0.0220
trigger times: 0
Loss after 220762620 batches: 0.0203
trigger times: 0
Loss after 220893720 batches: 0.0190
trigger times: 1
Loss after 221024820 batches: 0.0184
trigger times: 2
Loss after 221155920 batches: 0.0174
trigger times: 0
Loss after 221287020 batches: 0.0169
trigger times: 0
Loss after 221418120 batches: 0.0170
trigger times: 1
Loss after 221549220 batches: 0.0159
trigger times: 2
Loss after 221680320 batches: 0.0154
trigger times: 0
Loss after 221811420 batches: 0.0152
trigger times: 1
Loss after 221942520 batches: 0.0148
trigger times: 2
Loss after 222073620 batches: 0.0146
trigger times: 3
Loss after 222204720 batches: 0.0142
trigger times: 4
Loss after 222335820 batches: 0.0140
trigger times: 5
Loss after 222466920 batches: 0.0138
trigger times: 6
Loss after 222598020 batches: 0.0135
trigger times: 7
Loss after 222729120 batches: 0.0134
trigger times: 8
Loss after 222860220 batches: 0.0131
trigger times: 9
Loss after 222991320 batches: 0.0130
trigger times: 0
Loss after 223122420 batches: 0.0129
trigger times: 1
Loss after 223253520 batches: 0.0128
trigger times: 2
Loss after 223384620 batches: 0.0127
trigger times: 3
Loss after 223515720 batches: 0.0121
trigger times: 4
Loss after 223646820 batches: 0.0122
trigger times: 5
Loss after 223777920 batches: 0.0123
trigger times: 6
Loss after 223909020 batches: 0.0121
trigger times: 7
Loss after 224040120 batches: 0.0120
trigger times: 8
Loss after 224171220 batches: 0.0118
trigger times: 9
Loss after 224302320 batches: 0.0116
trigger times: 10
Loss after 224433420 batches: 0.0116
trigger times: 11
Loss after 224564520 batches: 0.0119
trigger times: 12
Loss after 224695620 batches: 0.0115
trigger times: 13
Loss after 224826720 batches: 0.0113
trigger times: 14
Loss after 224957820 batches: 0.0113
trigger times: 15
Loss after 225088920 batches: 0.0110
trigger times: 16
Loss after 225220020 batches: 0.0110
trigger times: 17
Loss after 225351120 batches: 0.0111
trigger times: 18
Loss after 225482220 batches: 0.0109
trigger times: 19
Loss after 225613320 batches: 0.0107
trigger times: 20
Early stopping!
Start to test process.
Loss after 225744420 batches: 0.0105
Time to train on one home:  322.2985243797302
trigger times: 0
Loss after 225847020 batches: 0.3376
trigger times: 0
Loss after 225949620 batches: 0.1171
trigger times: 1
Loss after 226052220 batches: 0.0732
trigger times: 2
Loss after 226154820 batches: 0.0568
trigger times: 3
Loss after 226257420 batches: 0.0483
trigger times: 4
Loss after 226360020 batches: 0.0429
trigger times: 5
Loss after 226462620 batches: 0.0424
trigger times: 6
Loss after 226565220 batches: 0.0405
trigger times: 7
Loss after 226667820 batches: 0.0355
trigger times: 8
Loss after 226770420 batches: 0.0347
trigger times: 9
Loss after 226873020 batches: 0.0330
trigger times: 10
Loss after 226975620 batches: 0.0308
trigger times: 0
Loss after 227078220 batches: 0.0306
trigger times: 1
Loss after 227180820 batches: 0.0316
trigger times: 2
Loss after 227283420 batches: 0.0289
trigger times: 3
Loss after 227386020 batches: 0.0272
trigger times: 0
Loss after 227488620 batches: 0.0261
trigger times: 1
Loss after 227591220 batches: 0.0253
trigger times: 2
Loss after 227693820 batches: 0.0246
trigger times: 3
Loss after 227796420 batches: 0.0263
trigger times: 4
Loss after 227899020 batches: 0.0256
trigger times: 5
Loss after 228001620 batches: 0.0242
trigger times: 6
Loss after 228104220 batches: 0.0285
trigger times: 0
Loss after 228206820 batches: 0.0254
trigger times: 1
Loss after 228309420 batches: 0.0247
trigger times: 2
Loss after 228412020 batches: 0.0249
trigger times: 3
Loss after 228514620 batches: 0.0247
trigger times: 0
Loss after 228617220 batches: 0.0248
trigger times: 1
Loss after 228719820 batches: 0.0283
trigger times: 2
Loss after 228822420 batches: 0.0238
trigger times: 3
Loss after 228925020 batches: 0.0234
trigger times: 4
Loss after 229027620 batches: 0.0219
trigger times: 5
Loss after 229130220 batches: 0.0223
trigger times: 0
Loss after 229232820 batches: 0.0226
trigger times: 1
Loss after 229335420 batches: 0.0214
trigger times: 0
Loss after 229438020 batches: 0.0208
trigger times: 1
Loss after 229540620 batches: 0.0208
trigger times: 0
Loss after 229643220 batches: 0.0212
trigger times: 0
Loss after 229745820 batches: 0.0205
trigger times: 1
Loss after 229848420 batches: 0.0210
trigger times: 0
Loss after 229951020 batches: 0.0204
trigger times: 1
Loss after 230053620 batches: 0.0209
trigger times: 2
Loss after 230156220 batches: 0.0205
trigger times: 3
Loss after 230258820 batches: 0.0220
trigger times: 4
Loss after 230361420 batches: 0.0205
trigger times: 0
Loss after 230464020 batches: 0.0199
trigger times: 1
Loss after 230566620 batches: 0.0194
trigger times: 2
Loss after 230669220 batches: 0.0191
trigger times: 3
Loss after 230771820 batches: 0.0193
trigger times: 4
Loss after 230874420 batches: 0.0205
trigger times: 5
Loss after 230977020 batches: 0.0215
trigger times: 0
Loss after 231079620 batches: 0.0222
trigger times: 1
Loss after 231182220 batches: 0.0196
trigger times: 0
Loss after 231284820 batches: 0.0215
trigger times: 0
Loss after 231387420 batches: 0.0198
trigger times: 1
Loss after 231490020 batches: 0.0196
trigger times: 2
Loss after 231592620 batches: 0.0193
trigger times: 3
Loss after 231695220 batches: 0.0181
trigger times: 4
Loss after 231797820 batches: 0.0185
trigger times: 0
Loss after 231900420 batches: 0.0210
trigger times: 1
Loss after 232003020 batches: 0.0189
trigger times: 0
Loss after 232105620 batches: 0.0191
trigger times: 1
Loss after 232208220 batches: 0.0189
trigger times: 2
Loss after 232310820 batches: 0.0184
trigger times: 3
Loss after 232413420 batches: 0.0186
trigger times: 4
Loss after 232516020 batches: 0.0204
trigger times: 5
Loss after 232618620 batches: 0.0183
trigger times: 0
Loss after 232721220 batches: 0.0192
trigger times: 1
Loss after 232823820 batches: 0.0191
trigger times: 2
Loss after 232926420 batches: 0.0185
trigger times: 3
Loss after 233029020 batches: 0.0174
trigger times: 4
Loss after 233131620 batches: 0.0181
trigger times: 5
Loss after 233234220 batches: 0.0176
trigger times: 6
Loss after 233336820 batches: 0.0181
trigger times: 7
Loss after 233439420 batches: 0.0171
trigger times: 8
Loss after 233542020 batches: 0.0167
trigger times: 9
Loss after 233644620 batches: 0.0183
trigger times: 0
Loss after 233747220 batches: 0.0192
trigger times: 1
Loss after 233849820 batches: 0.0206
trigger times: 0
Loss after 233952420 batches: 0.0176
trigger times: 0
Loss after 234055020 batches: 0.0176
trigger times: 1
Loss after 234157620 batches: 0.0178
trigger times: 2
Loss after 234260220 batches: 0.0180
trigger times: 3
Loss after 234362820 batches: 0.0193
trigger times: 4
Loss after 234465420 batches: 0.0180
trigger times: 5
Loss after 234568020 batches: 0.0173
trigger times: 6
Loss after 234670620 batches: 0.0165
trigger times: 7
Loss after 234773220 batches: 0.0179
trigger times: 8
Loss after 234875820 batches: 0.0185
trigger times: 9
Loss after 234978420 batches: 0.0191
trigger times: 10
Loss after 235081020 batches: 0.0170
trigger times: 0
Loss after 235183620 batches: 0.0191
trigger times: 1
Loss after 235286220 batches: 0.0173
trigger times: 2
Loss after 235388820 batches: 0.0180
trigger times: 3
Loss after 235491420 batches: 0.0187
trigger times: 4
Loss after 235594020 batches: 0.0182
trigger times: 5
Loss after 235696620 batches: 0.0163
trigger times: 6
Loss after 235799220 batches: 0.0170
trigger times: 7
Loss after 235901820 batches: 0.0161
trigger times: 8
Loss after 236004420 batches: 0.0159
trigger times: 9
Loss after 236107020 batches: 0.0154
trigger times: 10
Loss after 236209620 batches: 0.0153
trigger times: 11
Loss after 236312220 batches: 0.0155
trigger times: 12
Loss after 236414820 batches: 0.0168
trigger times: 13
Loss after 236517420 batches: 0.0208
trigger times: 14
Loss after 236620020 batches: 0.0169
trigger times: 15
Loss after 236722620 batches: 0.0160
trigger times: 16
Loss after 236825220 batches: 0.0166
trigger times: 17
Loss after 236927820 batches: 0.0157
trigger times: 18
Loss after 237030420 batches: 0.0157
trigger times: 19
Loss after 237133020 batches: 0.0166
trigger times: 20
Early stopping!
Start to test process.
Loss after 237235620 batches: 0.0174
Time to train on one home:  654.6679918766022
trigger times: 0
Loss after 237366720 batches: 0.1933
trigger times: 1
Loss after 237497820 batches: 0.0603
trigger times: 2
Loss after 237628920 batches: 0.0436
trigger times: 0
Loss after 237760020 batches: 0.0374
trigger times: 1
Loss after 237891120 batches: 0.0337
trigger times: 2
Loss after 238022220 batches: 0.0306
trigger times: 3
Loss after 238153320 batches: 0.0289
trigger times: 4
Loss after 238284420 batches: 0.0275
trigger times: 5
Loss after 238415520 batches: 0.0259
trigger times: 6
Loss after 238546620 batches: 0.0256
trigger times: 7
Loss after 238677720 batches: 0.0247
trigger times: 8
Loss after 238808820 batches: 0.0239
trigger times: 9
Loss after 238939920 batches: 0.0233
trigger times: 10
Loss after 239071020 batches: 0.0226
trigger times: 11
Loss after 239202120 batches: 0.0222
trigger times: 12
Loss after 239333220 batches: 0.0219
trigger times: 13
Loss after 239464320 batches: 0.0213
trigger times: 14
Loss after 239595420 batches: 0.0212
trigger times: 15
Loss after 239726520 batches: 0.0206
trigger times: 16
Loss after 239857620 batches: 0.0206
trigger times: 17
Loss after 239988720 batches: 0.0201
trigger times: 18
Loss after 240119820 batches: 0.0197
trigger times: 19
Loss after 240250920 batches: 0.0193
trigger times: 20
Early stopping!
Start to test process.
Loss after 240382020 batches: 0.0191
Time to train on one home:  181.30247330665588
trigger times: 0
Loss after 240513120 batches: 0.3444
trigger times: 1
Loss after 240644220 batches: 0.1052
trigger times: 2
Loss after 240775320 batches: 0.0750
trigger times: 3
Loss after 240906420 batches: 0.0637
trigger times: 4
Loss after 241037520 batches: 0.0570
trigger times: 5
Loss after 241168620 batches: 0.0528
trigger times: 6
Loss after 241299720 batches: 0.0489
trigger times: 7
Loss after 241430820 batches: 0.0462
trigger times: 8
Loss after 241561920 batches: 0.0436
trigger times: 9
Loss after 241693020 batches: 0.0416
trigger times: 10
Loss after 241824120 batches: 0.0406
trigger times: 11
Loss after 241955220 batches: 0.0392
trigger times: 12
Loss after 242086320 batches: 0.0381
trigger times: 13
Loss after 242217420 batches: 0.0369
trigger times: 14
Loss after 242348520 batches: 0.0359
trigger times: 15
Loss after 242479620 batches: 0.0352
trigger times: 16
Loss after 242610720 batches: 0.0343
trigger times: 17
Loss after 242741820 batches: 0.0339
trigger times: 18
Loss after 242872920 batches: 0.0335
trigger times: 19
Loss after 243004020 batches: 0.0325
trigger times: 20
Early stopping!
Start to test process.
Loss after 243135120 batches: 0.0317
Time to train on one home:  160.07684230804443
trigger times: 0
Loss after 243263760 batches: 0.1255
trigger times: 0
Loss after 243392400 batches: 0.0391
trigger times: 0
Loss after 243521040 batches: 0.0282
trigger times: 1
Loss after 243649680 batches: 0.0239
trigger times: 2
Loss after 243778320 batches: 0.0222
trigger times: 3
Loss after 243906960 batches: 0.0213
trigger times: 4
Loss after 244035600 batches: 0.0195
trigger times: 5
Loss after 244164240 batches: 0.0188
trigger times: 6
Loss after 244292880 batches: 0.0183
trigger times: 0
Loss after 244421520 batches: 0.0174
trigger times: 1
Loss after 244550160 batches: 0.0168
trigger times: 2
Loss after 244678800 batches: 0.0169
trigger times: 0
Loss after 244807440 batches: 0.0159
trigger times: 1
Loss after 244936080 batches: 0.0157
trigger times: 2
Loss after 245064720 batches: 0.0155
trigger times: 3
Loss after 245193360 batches: 0.0150
trigger times: 4
Loss after 245322000 batches: 0.0150
trigger times: 5
Loss after 245450640 batches: 0.0148
trigger times: 6
Loss after 245579280 batches: 0.0142
trigger times: 7
Loss after 245707920 batches: 0.0143
trigger times: 8
Loss after 245836560 batches: 0.0140
trigger times: 9
Loss after 245965200 batches: 0.0139
trigger times: 10
Loss after 246093840 batches: 0.0138
trigger times: 11
Loss after 246222480 batches: 0.0134
trigger times: 12
Loss after 246351120 batches: 0.0134
trigger times: 13
Loss after 246479760 batches: 0.0132
trigger times: 14
Loss after 246608400 batches: 0.0134
trigger times: 15
Loss after 246737040 batches: 0.0131
trigger times: 16
Loss after 246865680 batches: 0.0129
trigger times: 17
Loss after 246994320 batches: 0.0129
trigger times: 18
Loss after 247122960 batches: 0.0129
trigger times: 19
Loss after 247251600 batches: 0.0128
trigger times: 20
Early stopping!
Start to test process.
Loss after 247380240 batches: 0.0128
Time to train on one home:  241.48001623153687
trigger times: 0
Loss after 247511340 batches: 0.3077
trigger times: 0
Loss after 247642440 batches: 0.0886
trigger times: 0
Loss after 247773540 batches: 0.0619
trigger times: 0
Loss after 247904640 batches: 0.0525
trigger times: 0
Loss after 248035740 batches: 0.0474
trigger times: 1
Loss after 248166840 batches: 0.0430
trigger times: 2
Loss after 248297940 batches: 0.0410
trigger times: 3
Loss after 248429040 batches: 0.0392
trigger times: 0
Loss after 248560140 batches: 0.0369
trigger times: 1
Loss after 248691240 batches: 0.0355
trigger times: 2
Loss after 248822340 batches: 0.0343
trigger times: 3
Loss after 248953440 batches: 0.0334
trigger times: 4
Loss after 249084540 batches: 0.0323
trigger times: 5
Loss after 249215640 batches: 0.0323
trigger times: 6
Loss after 249346740 batches: 0.0309
trigger times: 7
Loss after 249477840 batches: 0.0310
trigger times: 0
Loss after 249608940 batches: 0.0303
trigger times: 1
Loss after 249740040 batches: 0.0294
trigger times: 2
Loss after 249871140 batches: 0.0289
trigger times: 3
Loss after 250002240 batches: 0.0282
trigger times: 4
Loss after 250133340 batches: 0.0282
trigger times: 5
Loss after 250264440 batches: 0.0281
trigger times: 6
Loss after 250395540 batches: 0.0275
trigger times: 7
Loss after 250526640 batches: 0.0268
trigger times: 8
Loss after 250657740 batches: 0.0263
trigger times: 9
Loss after 250788840 batches: 0.0262
trigger times: 10
Loss after 250919940 batches: 0.0258
trigger times: 11
Loss after 251051040 batches: 0.0257
trigger times: 12
Loss after 251182140 batches: 0.0260
trigger times: 13
Loss after 251313240 batches: 0.0255
trigger times: 14
Loss after 251444340 batches: 0.0248
trigger times: 15
Loss after 251575440 batches: 0.0248
trigger times: 0
Loss after 251706540 batches: 0.0247
trigger times: 1
Loss after 251837640 batches: 0.0244
trigger times: 2
Loss after 251968740 batches: 0.0242
trigger times: 3
Loss after 252099840 batches: 0.0241
trigger times: 4
Loss after 252230940 batches: 0.0237
trigger times: 5
Loss after 252362040 batches: 0.0241
trigger times: 6
Loss after 252493140 batches: 0.0239
trigger times: 0
Loss after 252624240 batches: 0.0236
trigger times: 1
Loss after 252755340 batches: 0.0234
trigger times: 2
Loss after 252886440 batches: 0.0232
trigger times: 0
Loss after 253017540 batches: 0.0231
trigger times: 1
Loss after 253148640 batches: 0.0231
trigger times: 2
Loss after 253279740 batches: 0.0227
trigger times: 3
Loss after 253410840 batches: 0.0225
trigger times: 4
Loss after 253541940 batches: 0.0222
trigger times: 5
Loss after 253673040 batches: 0.0224
trigger times: 6
Loss after 253804140 batches: 0.0224
trigger times: 7
Loss after 253935240 batches: 0.0221
trigger times: 8
Loss after 254066340 batches: 0.0219
trigger times: 9
Loss after 254197440 batches: 0.0213
trigger times: 10
Loss after 254328540 batches: 0.0216
trigger times: 0
Loss after 254459640 batches: 0.0213
trigger times: 1
Loss after 254590740 batches: 0.0213
trigger times: 2
Loss after 254721840 batches: 0.0211
trigger times: 0
Loss after 254852940 batches: 0.0211
trigger times: 0
Loss after 254984040 batches: 0.0211
trigger times: 1
Loss after 255115140 batches: 0.0209
trigger times: 2
Loss after 255246240 batches: 0.0208
trigger times: 3
Loss after 255377340 batches: 0.0210
trigger times: 4
Loss after 255508440 batches: 0.0206
trigger times: 5
Loss after 255639540 batches: 0.0207
trigger times: 6
Loss after 255770640 batches: 0.0205
trigger times: 7
Loss after 255901740 batches: 0.0202
trigger times: 8
Loss after 256032840 batches: 0.0201
trigger times: 9
Loss after 256163940 batches: 0.0200
trigger times: 10
Loss after 256295040 batches: 0.0200
trigger times: 11
Loss after 256426140 batches: 0.0201
trigger times: 12
Loss after 256557240 batches: 0.0198
trigger times: 13
Loss after 256688340 batches: 0.0196
trigger times: 14
Loss after 256819440 batches: 0.0202
trigger times: 15
Loss after 256950540 batches: 0.0197
trigger times: 16
Loss after 257081640 batches: 0.0197
trigger times: 17
Loss after 257212740 batches: 0.0195
trigger times: 18
Loss after 257343840 batches: 0.0194
trigger times: 19
Loss after 257474940 batches: 0.0194
trigger times: 20
Early stopping!
Start to test process.
Loss after 257606040 batches: 0.0192
Time to train on one home:  563.266371011734
trigger times: 0
Loss after 257737140 batches: 0.3511
trigger times: 0
Loss after 257868240 batches: 0.1212
trigger times: 1
Loss after 257999340 batches: 0.0736
trigger times: 0
Loss after 258130440 batches: 0.0586
trigger times: 0
Loss after 258261540 batches: 0.0500
trigger times: 0
Loss after 258392640 batches: 0.0446
trigger times: 1
Loss after 258523740 batches: 0.0429
trigger times: 0
Loss after 258654840 batches: 0.0394
trigger times: 1
Loss after 258785940 batches: 0.0365
trigger times: 0
Loss after 258917040 batches: 0.0365
trigger times: 1
Loss after 259048140 batches: 0.0353
trigger times: 2
Loss after 259179240 batches: 0.0341
trigger times: 3
Loss after 259310340 batches: 0.0336
trigger times: 4
Loss after 259441440 batches: 0.0330
trigger times: 5
Loss after 259572540 batches: 0.0319
trigger times: 6
Loss after 259703640 batches: 0.0311
trigger times: 0
Loss after 259834740 batches: 0.0305
trigger times: 0
Loss after 259965840 batches: 0.0298
trigger times: 0
Loss after 260096940 batches: 0.0296
trigger times: 1
Loss after 260228040 batches: 0.0293
trigger times: 2
Loss after 260359140 batches: 0.0302
trigger times: 0
Loss after 260490240 batches: 0.0286
trigger times: 0
Loss after 260621340 batches: 0.0280
trigger times: 1
Loss after 260752440 batches: 0.0296
trigger times: 2
Loss after 260883540 batches: 0.0290
trigger times: 3
Loss after 261014640 batches: 0.0279
trigger times: 4
Loss after 261145740 batches: 0.0281
trigger times: 5
Loss after 261276840 batches: 0.0285
trigger times: 6
Loss after 261407940 batches: 0.0279
trigger times: 7
Loss after 261539040 batches: 0.0265
trigger times: 8
Loss after 261670140 batches: 0.0265
trigger times: 9
Loss after 261801240 batches: 0.0268
trigger times: 10
Loss after 261932340 batches: 0.0261
trigger times: 0
Loss after 262063440 batches: 0.0266
trigger times: 0
Loss after 262194540 batches: 0.0268
trigger times: 1
Loss after 262325640 batches: 0.0259
trigger times: 2
Loss after 262456740 batches: 0.0251
trigger times: 0
Loss after 262587840 batches: 0.0251
trigger times: 1
Loss after 262718940 batches: 0.0245
trigger times: 2
Loss after 262850040 batches: 0.0253
trigger times: 3
Loss after 262981140 batches: 0.0249
trigger times: 4
Loss after 263112240 batches: 0.0232
trigger times: 5
Loss after 263243340 batches: 0.0250
trigger times: 0
Loss after 263374440 batches: 0.0234
trigger times: 0
Loss after 263505540 batches: 0.0240
trigger times: 0
Loss after 263636640 batches: 0.0238
trigger times: 1
Loss after 263767740 batches: 0.0234
trigger times: 2
Loss after 263898840 batches: 0.0250
trigger times: 3
Loss after 264029940 batches: 0.0238
trigger times: 4
Loss after 264161040 batches: 0.0234
trigger times: 5
Loss after 264292140 batches: 0.0228
trigger times: 6
Loss after 264423240 batches: 0.0228
trigger times: 7
Loss after 264554340 batches: 0.0226
trigger times: 8
Loss after 264685440 batches: 0.0226
trigger times: 9
Loss after 264816540 batches: 0.0222
trigger times: 10
Loss after 264947640 batches: 0.0223
trigger times: 11
Loss after 265078740 batches: 0.0226
trigger times: 12
Loss after 265209840 batches: 0.0219
trigger times: 13
Loss after 265340940 batches: 0.0223
trigger times: 14
Loss after 265472040 batches: 0.0231
trigger times: 15
Loss after 265603140 batches: 0.0222
trigger times: 16
Loss after 265734240 batches: 0.0226
trigger times: 0
Loss after 265865340 batches: 0.0218
trigger times: 1
Loss after 265996440 batches: 0.0223
trigger times: 2
Loss after 266127540 batches: 0.0214
trigger times: 3
Loss after 266258640 batches: 0.0219
trigger times: 4
Loss after 266389740 batches: 0.0205
trigger times: 5
Loss after 266520840 batches: 0.0215
trigger times: 6
Loss after 266651940 batches: 0.0221
trigger times: 7
Loss after 266783040 batches: 0.0224
trigger times: 8
Loss after 266914140 batches: 0.0222
trigger times: 9
Loss after 267045240 batches: 0.0221
trigger times: 10
Loss after 267176340 batches: 0.0215
trigger times: 11
Loss after 267307440 batches: 0.0200
trigger times: 12
Loss after 267438540 batches: 0.0205
trigger times: 13
Loss after 267569640 batches: 0.0212
trigger times: 14
Loss after 267700740 batches: 0.0206
trigger times: 15
Loss after 267831840 batches: 0.0210
trigger times: 16
Loss after 267962940 batches: 0.0213
trigger times: 17
Loss after 268094040 batches: 0.0210
trigger times: 18
Loss after 268225140 batches: 0.0214
trigger times: 19
Loss after 268356240 batches: 0.0207
trigger times: 20
Early stopping!
Start to test process.
Loss after 268487340 batches: 0.0210
Time to train on one home:  600.4398403167725
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821]]
Round_6_results:  [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2123 < 2124; dropping {'Training_Loss': 0.12257056634099979, 'Validation_Loss': 0.2584490130345027, 'Training_R2': 0.876647014268439, 'Validation_R2': 0.7599441395914004, 'Training_F1': 0.8099223578738147, 'Validation_F1': 0.7148396068796232, 'Training_NEP': 0.3801377191963807, 'Validation_NEP': 0.5457937395938718, 'Training_NDE': 0.09260327590757284, 'Validation_NDE': 0.19116783289848538, 'Training_MAE': 12.589407365393038, 'Validation_MAE': 14.967928699182353, 'Training_MSE': 407.43988, 'Validation_MSE': 705.97754}.
trigger times: 0
Loss after 268618440 batches: 0.1226
trigger times: 0
Loss after 268749540 batches: 0.0337
trigger times: 0
Loss after 268880640 batches: 0.0247
trigger times: 1
Loss after 269011740 batches: 0.0221
trigger times: 0
Loss after 269142840 batches: 0.0194
trigger times: 1
Loss after 269273940 batches: 0.0180
trigger times: 0
Loss after 269405040 batches: 0.0171
trigger times: 1
Loss after 269536140 batches: 0.0164
trigger times: 0
Loss after 269667240 batches: 0.0159
trigger times: 1
Loss after 269798340 batches: 0.0153
trigger times: 2
Loss after 269929440 batches: 0.0148
trigger times: 3
Loss after 270060540 batches: 0.0143
trigger times: 4
Loss after 270191640 batches: 0.0141
trigger times: 5
Loss after 270322740 batches: 0.0137
trigger times: 0
Loss after 270453840 batches: 0.0136
trigger times: 1
Loss after 270584940 batches: 0.0129
trigger times: 2
Loss after 270716040 batches: 0.0127
trigger times: 3
Loss after 270847140 batches: 0.0127
trigger times: 0
Loss after 270978240 batches: 0.0125
trigger times: 1
Loss after 271109340 batches: 0.0125
trigger times: 2
Loss after 271240440 batches: 0.0122
trigger times: 0
Loss after 271371540 batches: 0.0120
trigger times: 1
Loss after 271502640 batches: 0.0118
trigger times: 2
Loss after 271633740 batches: 0.0117
trigger times: 3
Loss after 271764840 batches: 0.0116
trigger times: 4
Loss after 271895940 batches: 0.0114
trigger times: 5
Loss after 272027040 batches: 0.0116
trigger times: 6
Loss after 272158140 batches: 0.0113
trigger times: 7
Loss after 272289240 batches: 0.0115
trigger times: 8
Loss after 272420340 batches: 0.0111
trigger times: 9
Loss after 272551440 batches: 0.0110
trigger times: 10
Loss after 272682540 batches: 0.0107
trigger times: 11
Loss after 272813640 batches: 0.0108
trigger times: 12
Loss after 272944740 batches: 0.0109
trigger times: 13
Loss after 273075840 batches: 0.0105
trigger times: 14
Loss after 273206940 batches: 0.0106
trigger times: 15
Loss after 273338040 batches: 0.0107
trigger times: 16
Loss after 273469140 batches: 0.0106
trigger times: 17
Loss after 273600240 batches: 0.0105
trigger times: 18
Loss after 273731340 batches: 0.0102
trigger times: 19
Loss after 273862440 batches: 0.0103
trigger times: 20
Early stopping!
Start to test process.
Loss after 273993540 batches: 0.0104
Time to train on one home:  309.2441554069519
trigger times: 0
Loss after 274096140 batches: 0.3112
trigger times: 1
Loss after 274198740 batches: 0.1021
trigger times: 0
Loss after 274301340 batches: 0.0719
trigger times: 1
Loss after 274403940 batches: 0.0501
trigger times: 2
Loss after 274506540 batches: 0.0398
trigger times: 3
Loss after 274609140 batches: 0.0360
trigger times: 4
Loss after 274711740 batches: 0.0357
trigger times: 5
Loss after 274814340 batches: 0.0323
trigger times: 6
Loss after 274916940 batches: 0.0315
trigger times: 7
Loss after 275019540 batches: 0.0294
trigger times: 8
Loss after 275122140 batches: 0.0286
trigger times: 9
Loss after 275224740 batches: 0.0281
trigger times: 0
Loss after 275327340 batches: 0.0259
trigger times: 1
Loss after 275429940 batches: 0.0256
trigger times: 2
Loss after 275532540 batches: 0.0243
trigger times: 3
Loss after 275635140 batches: 0.0240
trigger times: 4
Loss after 275737740 batches: 0.0235
trigger times: 0
Loss after 275840340 batches: 0.0237
trigger times: 1
Loss after 275942940 batches: 0.0218
trigger times: 2
Loss after 276045540 batches: 0.0224
trigger times: 3
Loss after 276148140 batches: 0.0225
trigger times: 4
Loss after 276250740 batches: 0.0214
trigger times: 5
Loss after 276353340 batches: 0.0217
trigger times: 6
Loss after 276455940 batches: 0.0214
trigger times: 7
Loss after 276558540 batches: 0.0211
trigger times: 8
Loss after 276661140 batches: 0.0204
trigger times: 0
Loss after 276763740 batches: 0.0203
trigger times: 1
Loss after 276866340 batches: 0.0207
trigger times: 2
Loss after 276968940 batches: 0.0205
trigger times: 3
Loss after 277071540 batches: 0.0200
trigger times: 4
Loss after 277174140 batches: 0.0204
trigger times: 5
Loss after 277276740 batches: 0.0201
trigger times: 0
Loss after 277379340 batches: 0.0215
trigger times: 1
Loss after 277481940 batches: 0.0216
trigger times: 2
Loss after 277584540 batches: 0.0220
trigger times: 3
Loss after 277687140 batches: 0.0202
trigger times: 4
Loss after 277789740 batches: 0.0206
trigger times: 5
Loss after 277892340 batches: 0.0207
trigger times: 6
Loss after 277994940 batches: 0.0221
trigger times: 7
Loss after 278097540 batches: 0.0193
trigger times: 8
Loss after 278200140 batches: 0.0185
trigger times: 9
Loss after 278302740 batches: 0.0185
trigger times: 10
Loss after 278405340 batches: 0.0183
trigger times: 11
Loss after 278507940 batches: 0.0186
trigger times: 12
Loss after 278610540 batches: 0.0189
trigger times: 13
Loss after 278713140 batches: 0.0185
trigger times: 14
Loss after 278815740 batches: 0.0183
trigger times: 15
Loss after 278918340 batches: 0.0182
trigger times: 16
Loss after 279020940 batches: 0.0178
trigger times: 17
Loss after 279123540 batches: 0.0178
trigger times: 18
Loss after 279226140 batches: 0.0182
trigger times: 19
Loss after 279328740 batches: 0.0175
trigger times: 20
Early stopping!
Start to test process.
Loss after 279431340 batches: 0.0179
Time to train on one home:  316.8517060279846
trigger times: 0
Loss after 279562440 batches: 0.1835
trigger times: 1
Loss after 279693540 batches: 0.0565
trigger times: 2
Loss after 279824640 batches: 0.0404
trigger times: 0
Loss after 279955740 batches: 0.0349
trigger times: 1
Loss after 280086840 batches: 0.0313
trigger times: 2
Loss after 280217940 batches: 0.0289
trigger times: 3
Loss after 280349040 batches: 0.0273
trigger times: 4
Loss after 280480140 batches: 0.0264
trigger times: 0
Loss after 280611240 batches: 0.0255
trigger times: 1
Loss after 280742340 batches: 0.0241
trigger times: 2
Loss after 280873440 batches: 0.0234
trigger times: 3
Loss after 281004540 batches: 0.0228
trigger times: 4
Loss after 281135640 batches: 0.0224
trigger times: 5
Loss after 281266740 batches: 0.0219
trigger times: 6
Loss after 281397840 batches: 0.0215
trigger times: 7
Loss after 281528940 batches: 0.0210
trigger times: 8
Loss after 281660040 batches: 0.0206
trigger times: 9
Loss after 281791140 batches: 0.0203
trigger times: 10
Loss after 281922240 batches: 0.0197
trigger times: 11
Loss after 282053340 batches: 0.0191
trigger times: 12
Loss after 282184440 batches: 0.0197
trigger times: 13
Loss after 282315540 batches: 0.0189
trigger times: 14
Loss after 282446640 batches: 0.0188
trigger times: 15
Loss after 282577740 batches: 0.0187
trigger times: 16
Loss after 282708840 batches: 0.0182
trigger times: 17
Loss after 282839940 batches: 0.0182
trigger times: 18
Loss after 282971040 batches: 0.0181
trigger times: 19
Loss after 283102140 batches: 0.0177
trigger times: 20
Early stopping!
Start to test process.
Loss after 283233240 batches: 0.0177
Time to train on one home:  217.22500681877136
trigger times: 0
Loss after 283364340 batches: 0.4071
trigger times: 1
Loss after 283495440 batches: 0.1126
trigger times: 2
Loss after 283626540 batches: 0.0762
trigger times: 3
Loss after 283757640 batches: 0.0627
trigger times: 4
Loss after 283888740 batches: 0.0555
trigger times: 5
Loss after 284019840 batches: 0.0513
trigger times: 6
Loss after 284150940 batches: 0.0475
trigger times: 7
Loss after 284282040 batches: 0.0447
trigger times: 8
Loss after 284413140 batches: 0.0429
trigger times: 9
Loss after 284544240 batches: 0.0416
trigger times: 10
Loss after 284675340 batches: 0.0401
trigger times: 11
Loss after 284806440 batches: 0.0379
trigger times: 12
Loss after 284937540 batches: 0.0374
trigger times: 13
Loss after 285068640 batches: 0.0365
trigger times: 14
Loss after 285199740 batches: 0.0349
trigger times: 15
Loss after 285330840 batches: 0.0345
trigger times: 16
Loss after 285461940 batches: 0.0336
trigger times: 17
Loss after 285593040 batches: 0.0330
trigger times: 18
Loss after 285724140 batches: 0.0324
trigger times: 19
Loss after 285855240 batches: 0.0317
trigger times: 20
Early stopping!
Start to test process.
Loss after 285986340 batches: 0.0314
Time to train on one home:  160.0037055015564
trigger times: 0
Loss after 286114980 batches: 0.1222
trigger times: 0
Loss after 286243620 batches: 0.0352
trigger times: 1
Loss after 286372260 batches: 0.0269
trigger times: 0
Loss after 286500900 batches: 0.0233
trigger times: 1
Loss after 286629540 batches: 0.0210
trigger times: 2
Loss after 286758180 batches: 0.0195
trigger times: 3
Loss after 286886820 batches: 0.0186
trigger times: 4
Loss after 287015460 batches: 0.0181
trigger times: 0
Loss after 287144100 batches: 0.0174
trigger times: 1
Loss after 287272740 batches: 0.0167
trigger times: 2
Loss after 287401380 batches: 0.0165
trigger times: 3
Loss after 287530020 batches: 0.0158
trigger times: 4
Loss after 287658660 batches: 0.0154
trigger times: 5
Loss after 287787300 batches: 0.0152
trigger times: 6
Loss after 287915940 batches: 0.0149
trigger times: 7
Loss after 288044580 batches: 0.0148
trigger times: 8
Loss after 288173220 batches: 0.0144
trigger times: 9
Loss after 288301860 batches: 0.0143
trigger times: 10
Loss after 288430500 batches: 0.0138
trigger times: 11
Loss after 288559140 batches: 0.0143
trigger times: 12
Loss after 288687780 batches: 0.0140
trigger times: 13
Loss after 288816420 batches: 0.0137
trigger times: 14
Loss after 288945060 batches: 0.0136
trigger times: 15
Loss after 289073700 batches: 0.0135
trigger times: 16
Loss after 289202340 batches: 0.0132
trigger times: 17
Loss after 289330980 batches: 0.0129
trigger times: 18
Loss after 289459620 batches: 0.0129
trigger times: 19
Loss after 289588260 batches: 0.0130
trigger times: 20
Early stopping!
Start to test process.
Loss after 289716900 batches: 0.0125
Time to train on one home:  213.8788878917694
trigger times: 0
Loss after 289848000 batches: 0.2488
trigger times: 0
Loss after 289979100 batches: 0.0722
trigger times: 0
Loss after 290110200 batches: 0.0509
trigger times: 0
Loss after 290241300 batches: 0.0443
trigger times: 1
Loss after 290372400 batches: 0.0402
trigger times: 0
Loss after 290503500 batches: 0.0373
trigger times: 1
Loss after 290634600 batches: 0.0354
trigger times: 2
Loss after 290765700 batches: 0.0333
trigger times: 3
Loss after 290896800 batches: 0.0318
trigger times: 4
Loss after 291027900 batches: 0.0312
trigger times: 5
Loss after 291159000 batches: 0.0305
trigger times: 6
Loss after 291290100 batches: 0.0294
trigger times: 7
Loss after 291421200 batches: 0.0290
trigger times: 8
Loss after 291552300 batches: 0.0280
trigger times: 9
Loss after 291683400 batches: 0.0273
trigger times: 10
Loss after 291814500 batches: 0.0270
trigger times: 11
Loss after 291945600 batches: 0.0268
trigger times: 12
Loss after 292076700 batches: 0.0262
trigger times: 13
Loss after 292207800 batches: 0.0259
trigger times: 14
Loss after 292338900 batches: 0.0255
trigger times: 15
Loss after 292470000 batches: 0.0252
trigger times: 16
Loss after 292601100 batches: 0.0245
trigger times: 17
Loss after 292732200 batches: 0.0245
trigger times: 18
Loss after 292863300 batches: 0.0241
trigger times: 19
Loss after 292994400 batches: 0.0240
trigger times: 20
Early stopping!
Start to test process.
Loss after 293125500 batches: 0.0240
Time to train on one home:  195.7484827041626
trigger times: 0
Loss after 293256600 batches: 0.2422
trigger times: 0
Loss after 293387700 batches: 0.0990
trigger times: 0
Loss after 293518800 batches: 0.0631
trigger times: 0
Loss after 293649900 batches: 0.0494
trigger times: 0
Loss after 293781000 batches: 0.0423
trigger times: 0
Loss after 293912100 batches: 0.0398
trigger times: 1
Loss after 294043200 batches: 0.0366
trigger times: 0
Loss after 294174300 batches: 0.0349
trigger times: 1
Loss after 294305400 batches: 0.0345
trigger times: 2
Loss after 294436500 batches: 0.0322
trigger times: 0
Loss after 294567600 batches: 0.0311
trigger times: 0
Loss after 294698700 batches: 0.0299
trigger times: 0
Loss after 294829800 batches: 0.0301
trigger times: 1
Loss after 294960900 batches: 0.0296
trigger times: 0
Loss after 295092000 batches: 0.0284
trigger times: 1
Loss after 295223100 batches: 0.0284
trigger times: 0
Loss after 295354200 batches: 0.0277
trigger times: 1
Loss after 295485300 batches: 0.0267
trigger times: 2
Loss after 295616400 batches: 0.0274
trigger times: 0
Loss after 295747500 batches: 0.0259
trigger times: 1
Loss after 295878600 batches: 0.0264
trigger times: 2
Loss after 296009700 batches: 0.0264
trigger times: 3
Loss after 296140800 batches: 0.0262
trigger times: 4
Loss after 296271900 batches: 0.0252
trigger times: 5
Loss after 296403000 batches: 0.0254
trigger times: 6
Loss after 296534100 batches: 0.0250
trigger times: 7
Loss after 296665200 batches: 0.0244
trigger times: 8
Loss after 296796300 batches: 0.0247
trigger times: 9
Loss after 296927400 batches: 0.0248
trigger times: 10
Loss after 297058500 batches: 0.0248
trigger times: 11
Loss after 297189600 batches: 0.0243
trigger times: 12
Loss after 297320700 batches: 0.0241
trigger times: 0
Loss after 297451800 batches: 0.0252
trigger times: 1
Loss after 297582900 batches: 0.0237
trigger times: 0
Loss after 297714000 batches: 0.0244
trigger times: 1
Loss after 297845100 batches: 0.0234
trigger times: 0
Loss after 297976200 batches: 0.0234
trigger times: 1
Loss after 298107300 batches: 0.0247
trigger times: 2
Loss after 298238400 batches: 0.0238
trigger times: 0
Loss after 298369500 batches: 0.0233
trigger times: 1
Loss after 298500600 batches: 0.0231
trigger times: 0
Loss after 298631700 batches: 0.0216
trigger times: 1
Loss after 298762800 batches: 0.0219
trigger times: 0
Loss after 298893900 batches: 0.0235
trigger times: 1
Loss after 299025000 batches: 0.0234
trigger times: 2
Loss after 299156100 batches: 0.0226
trigger times: 3
Loss after 299287200 batches: 0.0237
trigger times: 4
Loss after 299418300 batches: 0.0221
trigger times: 5
Loss after 299549400 batches: 0.0218
trigger times: 6
Loss after 299680500 batches: 0.0218
trigger times: 7
Loss after 299811600 batches: 0.0215
trigger times: 8
Loss after 299942700 batches: 0.0214
trigger times: 0
Loss after 300073800 batches: 0.0233
trigger times: 1
Loss after 300204900 batches: 0.0219
trigger times: 2
Loss after 300336000 batches: 0.0203
trigger times: 3
Loss after 300467100 batches: 0.0201
trigger times: 4
Loss after 300598200 batches: 0.0215
trigger times: 5
Loss after 300729300 batches: 0.0206
trigger times: 6
Loss after 300860400 batches: 0.0214
trigger times: 7
Loss after 300991500 batches: 0.0205
trigger times: 8
Loss after 301122600 batches: 0.0209
trigger times: 9
Loss after 301253700 batches: 0.0212
trigger times: 10
Loss after 301384800 batches: 0.0206
trigger times: 11
Loss after 301515900 batches: 0.0201
trigger times: 12
Loss after 301647000 batches: 0.0203
trigger times: 13
Loss after 301778100 batches: 0.0199
trigger times: 14
Loss after 301909200 batches: 0.0200
trigger times: 15
Loss after 302040300 batches: 0.0196
trigger times: 16
Loss after 302171400 batches: 0.0206
trigger times: 17
Loss after 302302500 batches: 0.0198
trigger times: 18
Loss after 302433600 batches: 0.0205
trigger times: 0
Loss after 302564700 batches: 0.0197
trigger times: 1
Loss after 302695800 batches: 0.0200
trigger times: 2
Loss after 302826900 batches: 0.0198
trigger times: 3
Loss after 302958000 batches: 0.0196
trigger times: 4
Loss after 303089100 batches: 0.0200
trigger times: 5
Loss after 303220200 batches: 0.0203
trigger times: 6
Loss after 303351300 batches: 0.0210
trigger times: 7
Loss after 303482400 batches: 0.0198
trigger times: 8
Loss after 303613500 batches: 0.0192
trigger times: 9
Loss after 303744600 batches: 0.0195
trigger times: 10
Loss after 303875700 batches: 0.0196
trigger times: 11
Loss after 304006800 batches: 0.0191
trigger times: 12
Loss after 304137900 batches: 0.0189
trigger times: 13
Loss after 304269000 batches: 0.0196
trigger times: 14
Loss after 304400100 batches: 0.0204
trigger times: 0
Loss after 304531200 batches: 0.0197
trigger times: 1
Loss after 304662300 batches: 0.0197
trigger times: 2
Loss after 304793400 batches: 0.0183
trigger times: 3
Loss after 304924500 batches: 0.0183
trigger times: 4
Loss after 305055600 batches: 0.0184
trigger times: 5
Loss after 305186700 batches: 0.0189
trigger times: 6
Loss after 305317800 batches: 0.0199
trigger times: 7
Loss after 305448900 batches: 0.0193
trigger times: 8
Loss after 305580000 batches: 0.0197
trigger times: 9
Loss after 305711100 batches: 0.0191
trigger times: 10
Loss after 305842200 batches: 0.0190
trigger times: 11
Loss after 305973300 batches: 0.0189
trigger times: 12
Loss after 306104400 batches: 0.0186
trigger times: 13
Loss after 306235500 batches: 0.0186
trigger times: 14
Loss after 306366600 batches: 0.0194
trigger times: 0
Loss after 306497700 batches: 0.0191
trigger times: 1
Loss after 306628800 batches: 0.0184
trigger times: 2
Loss after 306759900 batches: 0.0191
trigger times: 3
Loss after 306891000 batches: 0.0193
trigger times: 4
Loss after 307022100 batches: 0.0198
trigger times: 5
Loss after 307153200 batches: 0.0184
trigger times: 6
Loss after 307284300 batches: 0.0185
trigger times: 7
Loss after 307415400 batches: 0.0173
trigger times: 8
Loss after 307546500 batches: 0.0189
trigger times: 9
Loss after 307677600 batches: 0.0200
trigger times: 0
Loss after 307808700 batches: 0.0187
trigger times: 1
Loss after 307939800 batches: 0.0199
trigger times: 2
Loss after 308070900 batches: 0.0176
trigger times: 3
Loss after 308202000 batches: 0.0180
trigger times: 4
Loss after 308333100 batches: 0.0176
trigger times: 5
Loss after 308464200 batches: 0.0178
trigger times: 0
Loss after 308595300 batches: 0.0182
trigger times: 1
Loss after 308726400 batches: 0.0177
trigger times: 2
Loss after 308857500 batches: 0.0180
trigger times: 3
Loss after 308988600 batches: 0.0181
trigger times: 4
Loss after 309119700 batches: 0.0175
trigger times: 5
Loss after 309250800 batches: 0.0184
trigger times: 6
Loss after 309381900 batches: 0.0178
trigger times: 7
Loss after 309513000 batches: 0.0175
trigger times: 8
Loss after 309644100 batches: 0.0181
trigger times: 9
Loss after 309775200 batches: 0.0173
trigger times: 10
Loss after 309906300 batches: 0.0180
trigger times: 11
Loss after 310037400 batches: 0.0173
trigger times: 12
Loss after 310168500 batches: 0.0171
trigger times: 13
Loss after 310299600 batches: 0.0169
trigger times: 14
Loss after 310430700 batches: 0.0169
trigger times: 15
Loss after 310561800 batches: 0.0165
trigger times: 16
Loss after 310692900 batches: 0.0172
trigger times: 17
Loss after 310824000 batches: 0.0177
trigger times: 18
Loss after 310955100 batches: 0.0171
trigger times: 19
Loss after 311086200 batches: 0.0168
trigger times: 20
Early stopping!
Start to test process.
Loss after 311217300 batches: 0.0169
Time to train on one home:  988.0106737613678
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884]]
Round_7_results:  [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884]
trigger times: 0
Loss after 311348400 batches: 0.1105
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2461 < 2462; dropping {'Training_Loss': 0.11047211767367597, 'Validation_Loss': 0.23280753195285797, 'Training_R2': 0.8887145030659694, 'Validation_R2': 0.7838383101095489, 'Training_F1': 0.81840950411911, 'Validation_F1': 0.7313603151568655, 'Training_NEP': 0.3630286208086276, 'Validation_NEP': 0.5175878111896822, 'Training_NDE': 0.08354399786900885, 'Validation_NDE': 0.1721397750577541, 'Training_MAE': 12.022787957791605, 'Validation_MAE': 14.19440512311074, 'Training_MSE': 367.58054, 'Validation_MSE': 635.70746}.
trigger times: 0
Loss after 311479500 batches: 0.0316
trigger times: 0
Loss after 311610600 batches: 0.0235
trigger times: 0
Loss after 311741700 batches: 0.0202
trigger times: 1
Loss after 311872800 batches: 0.0184
trigger times: 2
Loss after 312003900 batches: 0.0171
trigger times: 0
Loss after 312135000 batches: 0.0160
trigger times: 1
Loss after 312266100 batches: 0.0154
trigger times: 2
Loss after 312397200 batches: 0.0152
trigger times: 3
Loss after 312528300 batches: 0.0147
trigger times: 0
Loss after 312659400 batches: 0.0143
trigger times: 1
Loss after 312790500 batches: 0.0142
trigger times: 0
Loss after 312921600 batches: 0.0132
trigger times: 1
Loss after 313052700 batches: 0.0131
trigger times: 2
Loss after 313183800 batches: 0.0128
trigger times: 0
Loss after 313314900 batches: 0.0128
trigger times: 1
Loss after 313446000 batches: 0.0123
trigger times: 2
Loss after 313577100 batches: 0.0124
trigger times: 0
Loss after 313708200 batches: 0.0121
trigger times: 1
Loss after 313839300 batches: 0.0121
trigger times: 2
Loss after 313970400 batches: 0.0117
trigger times: 3
Loss after 314101500 batches: 0.0119
trigger times: 4
Loss after 314232600 batches: 0.0115
trigger times: 5
Loss after 314363700 batches: 0.0115
trigger times: 6
Loss after 314494800 batches: 0.0112
trigger times: 7
Loss after 314625900 batches: 0.0110
trigger times: 8
Loss after 314757000 batches: 0.0108
trigger times: 9
Loss after 314888100 batches: 0.0109
trigger times: 10
Loss after 315019200 batches: 0.0107
trigger times: 11
Loss after 315150300 batches: 0.0106
trigger times: 12
Loss after 315281400 batches: 0.0104
trigger times: 13
Loss after 315412500 batches: 0.0105
trigger times: 14
Loss after 315543600 batches: 0.0105
trigger times: 15
Loss after 315674700 batches: 0.0102
trigger times: 16
Loss after 315805800 batches: 0.0101
trigger times: 17
Loss after 315936900 batches: 0.0102
trigger times: 18
Loss after 316068000 batches: 0.0103
trigger times: 19
Loss after 316199100 batches: 0.0101
trigger times: 20
Early stopping!
Start to test process.
Loss after 316330200 batches: 0.0101
Time to train on one home:  287.5787479877472
trigger times: 0
Loss after 316432800 batches: 0.1970
trigger times: 0
Loss after 316535400 batches: 0.0849
trigger times: 0
Loss after 316638000 batches: 0.0519
trigger times: 1
Loss after 316740600 batches: 0.0421
trigger times: 2
Loss after 316843200 batches: 0.0357
trigger times: 3
Loss after 316945800 batches: 0.0328
trigger times: 4
Loss after 317048400 batches: 0.0319
trigger times: 5
Loss after 317151000 batches: 0.0288
trigger times: 6
Loss after 317253600 batches: 0.0266
trigger times: 7
Loss after 317356200 batches: 0.0260
trigger times: 8
Loss after 317458800 batches: 0.0290
trigger times: 9
Loss after 317561400 batches: 0.0265
trigger times: 10
Loss after 317664000 batches: 0.0244
trigger times: 0
Loss after 317766600 batches: 0.0229
trigger times: 1
Loss after 317869200 batches: 0.0222
trigger times: 2
Loss after 317971800 batches: 0.0225
trigger times: 0
Loss after 318074400 batches: 0.0278
trigger times: 0
Loss after 318177000 batches: 0.0222
trigger times: 0
Loss after 318279600 batches: 0.0222
trigger times: 1
Loss after 318382200 batches: 0.0212
trigger times: 2
Loss after 318484800 batches: 0.0218
trigger times: 3
Loss after 318587400 batches: 0.0211
trigger times: 4
Loss after 318690000 batches: 0.0213
trigger times: 5
Loss after 318792600 batches: 0.0200
trigger times: 6
Loss after 318895200 batches: 0.0236
trigger times: 7
Loss after 318997800 batches: 0.0216
trigger times: 8
Loss after 319100400 batches: 0.0206
trigger times: 9
Loss after 319203000 batches: 0.0205
trigger times: 10
Loss after 319305600 batches: 0.0212
trigger times: 11
Loss after 319408200 batches: 0.0199
trigger times: 12
Loss after 319510800 batches: 0.0189
trigger times: 13
Loss after 319613400 batches: 0.0196
trigger times: 14
Loss after 319716000 batches: 0.0188
trigger times: 15
Loss after 319818600 batches: 0.0188
trigger times: 16
Loss after 319921200 batches: 0.0193
trigger times: 17
Loss after 320023800 batches: 0.0201
trigger times: 18
Loss after 320126400 batches: 0.0187
trigger times: 19
Loss after 320229000 batches: 0.0187
trigger times: 20
Early stopping!
Start to test process.
Loss after 320331600 batches: 0.0185
Time to train on one home:  236.22870302200317
trigger times: 0
Loss after 320462700 batches: 0.1500
trigger times: 0
Loss after 320593800 batches: 0.0504
trigger times: 1
Loss after 320724900 batches: 0.0370
trigger times: 2
Loss after 320856000 batches: 0.0322
trigger times: 3
Loss after 320987100 batches: 0.0293
trigger times: 4
Loss after 321118200 batches: 0.0273
trigger times: 5
Loss after 321249300 batches: 0.0258
trigger times: 6
Loss after 321380400 batches: 0.0245
trigger times: 7
Loss after 321511500 batches: 0.0242
trigger times: 8
Loss after 321642600 batches: 0.0233
trigger times: 9
Loss after 321773700 batches: 0.0226
trigger times: 10
Loss after 321904800 batches: 0.0217
trigger times: 11
Loss after 322035900 batches: 0.0214
trigger times: 12
Loss after 322167000 batches: 0.0206
trigger times: 13
Loss after 322298100 batches: 0.0202
trigger times: 14
Loss after 322429200 batches: 0.0202
trigger times: 15
Loss after 322560300 batches: 0.0197
trigger times: 16
Loss after 322691400 batches: 0.0194
trigger times: 17
Loss after 322822500 batches: 0.0189
trigger times: 18
Loss after 322953600 batches: 0.0186
trigger times: 19
Loss after 323084700 batches: 0.0183
trigger times: 20
Early stopping!
Start to test process.
Loss after 323215800 batches: 0.0182
Time to train on one home:  167.19987607002258
trigger times: 0
Loss after 323346900 batches: 0.3027
trigger times: 0
Loss after 323478000 batches: 0.0900
trigger times: 1
Loss after 323609100 batches: 0.0656
trigger times: 2
Loss after 323740200 batches: 0.0564
trigger times: 0
Loss after 323871300 batches: 0.0504
trigger times: 1
Loss after 324002400 batches: 0.0469
trigger times: 2
Loss after 324133500 batches: 0.0440
trigger times: 3
Loss after 324264600 batches: 0.0418
trigger times: 4
Loss after 324395700 batches: 0.0399
trigger times: 5
Loss after 324526800 batches: 0.0384
trigger times: 6
Loss after 324657900 batches: 0.0371
trigger times: 7
Loss after 324789000 batches: 0.0359
trigger times: 8
Loss after 324920100 batches: 0.0350
trigger times: 9
Loss after 325051200 batches: 0.0343
trigger times: 10
Loss after 325182300 batches: 0.0334
trigger times: 11
Loss after 325313400 batches: 0.0322
trigger times: 12
Loss after 325444500 batches: 0.0316
trigger times: 13
Loss after 325575600 batches: 0.0313
trigger times: 14
Loss after 325706700 batches: 0.0309
trigger times: 15
Loss after 325837800 batches: 0.0302
trigger times: 16
Loss after 325968900 batches: 0.0294
trigger times: 17
Loss after 326100000 batches: 0.0292
trigger times: 18
Loss after 326231100 batches: 0.0288
trigger times: 19
Loss after 326362200 batches: 0.0286
trigger times: 20
Early stopping!
Start to test process.
Loss after 326493300 batches: 0.0276
Time to train on one home:  188.272625207901
trigger times: 0
Loss after 326621940 batches: 0.1137
trigger times: 0
Loss after 326750580 batches: 0.0324
trigger times: 1
Loss after 326879220 batches: 0.0256
trigger times: 2
Loss after 327007860 batches: 0.0222
trigger times: 3
Loss after 327136500 batches: 0.0203
trigger times: 0
Loss after 327265140 batches: 0.0194
trigger times: 1
Loss after 327393780 batches: 0.0185
trigger times: 2
Loss after 327522420 batches: 0.0172
trigger times: 3
Loss after 327651060 batches: 0.0172
trigger times: 4
Loss after 327779700 batches: 0.0167
trigger times: 5
Loss after 327908340 batches: 0.0156
trigger times: 6
Loss after 328036980 batches: 0.0154
trigger times: 7
Loss after 328165620 batches: 0.0151
trigger times: 8
Loss after 328294260 batches: 0.0149
trigger times: 9
Loss after 328422900 batches: 0.0147
trigger times: 10
Loss after 328551540 batches: 0.0146
trigger times: 11
Loss after 328680180 batches: 0.0142
trigger times: 12
Loss after 328808820 batches: 0.0140
trigger times: 13
Loss after 328937460 batches: 0.0137
trigger times: 14
Loss after 329066100 batches: 0.0135
trigger times: 15
Loss after 329194740 batches: 0.0135
trigger times: 16
Loss after 329323380 batches: 0.0131
trigger times: 17
Loss after 329452020 batches: 0.0129
trigger times: 18
Loss after 329580660 batches: 0.0131
trigger times: 19
Loss after 329709300 batches: 0.0129
trigger times: 20
Early stopping!
Start to test process.
Loss after 329837940 batches: 0.0129
Time to train on one home:  192.3378505706787
trigger times: 0
Loss after 329969040 batches: 0.2580
trigger times: 0
Loss after 330100140 batches: 0.0681
trigger times: 1
Loss after 330231240 batches: 0.0497
trigger times: 0
Loss after 330362340 batches: 0.0426
trigger times: 1
Loss after 330493440 batches: 0.0393
trigger times: 2
Loss after 330624540 batches: 0.0364
trigger times: 0
Loss after 330755640 batches: 0.0345
trigger times: 1
Loss after 330886740 batches: 0.0335
trigger times: 0
Loss after 331017840 batches: 0.0316
trigger times: 1
Loss after 331148940 batches: 0.0306
trigger times: 0
Loss after 331280040 batches: 0.0299
trigger times: 1
Loss after 331411140 batches: 0.0293
trigger times: 2
Loss after 331542240 batches: 0.0283
trigger times: 3
Loss after 331673340 batches: 0.0278
trigger times: 0
Loss after 331804440 batches: 0.0274
trigger times: 1
Loss after 331935540 batches: 0.0270
trigger times: 2
Loss after 332066640 batches: 0.0260
trigger times: 3
Loss after 332197740 batches: 0.0258
trigger times: 0
Loss after 332328840 batches: 0.0256
trigger times: 1
Loss after 332459940 batches: 0.0251
trigger times: 2
Loss after 332591040 batches: 0.0250
trigger times: 3
Loss after 332722140 batches: 0.0248
trigger times: 4
Loss after 332853240 batches: 0.0246
trigger times: 5
Loss after 332984340 batches: 0.0241
trigger times: 6
Loss after 333115440 batches: 0.0238
trigger times: 7
Loss after 333246540 batches: 0.0236
trigger times: 8
Loss after 333377640 batches: 0.0237
trigger times: 9
Loss after 333508740 batches: 0.0229
trigger times: 10
Loss after 333639840 batches: 0.0236
trigger times: 11
Loss after 333770940 batches: 0.0227
trigger times: 12
Loss after 333902040 batches: 0.0225
trigger times: 13
Loss after 334033140 batches: 0.0225
trigger times: 14
Loss after 334164240 batches: 0.0220
trigger times: 15
Loss after 334295340 batches: 0.0219
trigger times: 16
Loss after 334426440 batches: 0.0218
trigger times: 17
Loss after 334557540 batches: 0.0217
trigger times: 18
Loss after 334688640 batches: 0.0216
trigger times: 19
Loss after 334819740 batches: 0.0216
trigger times: 20
Early stopping!
Start to test process.
Loss after 334950840 batches: 0.0213
Time to train on one home:  287.54644656181335
trigger times: 0
Loss after 335081940 batches: 0.2425
trigger times: 0
Loss after 335213040 batches: 0.0886
trigger times: 0
Loss after 335344140 batches: 0.0540
trigger times: 0
Loss after 335475240 batches: 0.0430
trigger times: 0
Loss after 335606340 batches: 0.0377
trigger times: 0
Loss after 335737440 batches: 0.0340
trigger times: 1
Loss after 335868540 batches: 0.0330
trigger times: 0
Loss after 335999640 batches: 0.0313
trigger times: 0
Loss after 336130740 batches: 0.0298
trigger times: 1
Loss after 336261840 batches: 0.0291
trigger times: 2
Loss after 336392940 batches: 0.0295
trigger times: 3
Loss after 336524040 batches: 0.0278
trigger times: 4
Loss after 336655140 batches: 0.0276
trigger times: 5
Loss after 336786240 batches: 0.0258
trigger times: 0
Loss after 336917340 batches: 0.0249
trigger times: 0
Loss after 337048440 batches: 0.0263
trigger times: 1
Loss after 337179540 batches: 0.0265
trigger times: 2
Loss after 337310640 batches: 0.0254
trigger times: 3
Loss after 337441740 batches: 0.0252
trigger times: 4
Loss after 337572840 batches: 0.0255
trigger times: 5
Loss after 337703940 batches: 0.0240
trigger times: 0
Loss after 337835040 batches: 0.0232
trigger times: 0
Loss after 337966140 batches: 0.0227
trigger times: 1
Loss after 338097240 batches: 0.0232
trigger times: 0
Loss after 338228340 batches: 0.0235
trigger times: 1
Loss after 338359440 batches: 0.0244
trigger times: 2
Loss after 338490540 batches: 0.0232
trigger times: 0
Loss after 338621640 batches: 0.0230
trigger times: 1
Loss after 338752740 batches: 0.0221
trigger times: 2
Loss after 338883840 batches: 0.0224
trigger times: 3
Loss after 339014940 batches: 0.0235
trigger times: 4
Loss after 339146040 batches: 0.0230
trigger times: 5
Loss after 339277140 batches: 0.0219
trigger times: 0
Loss after 339408240 batches: 0.0219
trigger times: 0
Loss after 339539340 batches: 0.0217
trigger times: 1
Loss after 339670440 batches: 0.0218
trigger times: 2
Loss after 339801540 batches: 0.0228
trigger times: 3
Loss after 339932640 batches: 0.0212
trigger times: 4
Loss after 340063740 batches: 0.0218
trigger times: 5
Loss after 340194840 batches: 0.0210
trigger times: 6
Loss after 340325940 batches: 0.0206
trigger times: 7
Loss after 340457040 batches: 0.0202
trigger times: 8
Loss after 340588140 batches: 0.0208
trigger times: 9
Loss after 340719240 batches: 0.0208
trigger times: 10
Loss after 340850340 batches: 0.0217
trigger times: 11
Loss after 340981440 batches: 0.0204
trigger times: 0
Loss after 341112540 batches: 0.0211
trigger times: 1
Loss after 341243640 batches: 0.0208
trigger times: 2
Loss after 341374740 batches: 0.0194
trigger times: 3
Loss after 341505840 batches: 0.0205
trigger times: 0
Loss after 341636940 batches: 0.0206
trigger times: 1
Loss after 341768040 batches: 0.0201
trigger times: 2
Loss after 341899140 batches: 0.0198
trigger times: 3
Loss after 342030240 batches: 0.0194
trigger times: 4
Loss after 342161340 batches: 0.0196
trigger times: 5
Loss after 342292440 batches: 0.0196
trigger times: 6
Loss after 342423540 batches: 0.0190
trigger times: 7
Loss after 342554640 batches: 0.0194
trigger times: 8
Loss after 342685740 batches: 0.0191
trigger times: 9
Loss after 342816840 batches: 0.0203
trigger times: 10
Loss after 342947940 batches: 0.0205
trigger times: 11
Loss after 343079040 batches: 0.0191
trigger times: 12
Loss after 343210140 batches: 0.0189
trigger times: 13
Loss after 343341240 batches: 0.0187
trigger times: 14
Loss after 343472340 batches: 0.0196
trigger times: 15
Loss after 343603440 batches: 0.0189
trigger times: 16
Loss after 343734540 batches: 0.0184
trigger times: 17
Loss after 343865640 batches: 0.0188
trigger times: 18
Loss after 343996740 batches: 0.0192
trigger times: 19
Loss after 344127840 batches: 0.0189
trigger times: 20
Early stopping!
Start to test process.
Loss after 344258940 batches: 0.0186
Time to train on one home:  515.3774588108063
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887, 0.018152045403586472]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884], [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987]]
Round_8_results:  [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2722 < 2723; dropping {'Training_Loss': 0.1296657995795304, 'Validation_Loss': 0.2618938949373033, 'Training_R2': 0.8694530893894586, 'Validation_R2': 0.7568621370285493, 'Training_F1': 0.8018032671356319, 'Validation_F1': 0.7008426803055061, 'Training_NEP': 0.3964144524560092, 'Validation_NEP': 0.5446325429794999, 'Training_NDE': 0.09800388300658763, 'Validation_NDE': 0.19362217727451925, 'Training_MAE': 13.128460490709038, 'Validation_MAE': 14.936083870506629, 'Training_MSE': 431.20175, 'Validation_MSE': 715.0414}.
trigger times: 0
Loss after 344390040 batches: 0.1297
trigger times: 0
Loss after 344521140 batches: 0.0308
trigger times: 0
Loss after 344652240 batches: 0.0228
trigger times: 0
Loss after 344783340 batches: 0.0203
trigger times: 1
Loss after 344914440 batches: 0.0184
trigger times: 0
Loss after 345045540 batches: 0.0163
trigger times: 0
Loss after 345176640 batches: 0.0161
trigger times: 1
Loss after 345307740 batches: 0.0156
trigger times: 2
Loss after 345438840 batches: 0.0149
trigger times: 3
Loss after 345569940 batches: 0.0143
trigger times: 4
Loss after 345701040 batches: 0.0138
trigger times: 5
Loss after 345832140 batches: 0.0133
trigger times: 6
Loss after 345963240 batches: 0.0131
trigger times: 7
Loss after 346094340 batches: 0.0132
trigger times: 0
Loss after 346225440 batches: 0.0128
trigger times: 1
Loss after 346356540 batches: 0.0126
trigger times: 2
Loss after 346487640 batches: 0.0126
trigger times: 3
Loss after 346618740 batches: 0.0124
trigger times: 4
Loss after 346749840 batches: 0.0120
trigger times: 5
Loss after 346880940 batches: 0.0116
trigger times: 6
Loss after 347012040 batches: 0.0115
trigger times: 7
Loss after 347143140 batches: 0.0115
trigger times: 8
Loss after 347274240 batches: 0.0115
trigger times: 9
Loss after 347405340 batches: 0.0114
trigger times: 10
Loss after 347536440 batches: 0.0113
trigger times: 0
Loss after 347667540 batches: 0.0113
trigger times: 1
Loss after 347798640 batches: 0.0111
trigger times: 2
Loss after 347929740 batches: 0.0109
trigger times: 3
Loss after 348060840 batches: 0.0107
trigger times: 4
Loss after 348191940 batches: 0.0105
trigger times: 5
Loss after 348323040 batches: 0.0105
trigger times: 0
Loss after 348454140 batches: 0.0105
trigger times: 1
Loss after 348585240 batches: 0.0106
trigger times: 2
Loss after 348716340 batches: 0.0106
trigger times: 3
Loss after 348847440 batches: 0.0103
trigger times: 4
Loss after 348978540 batches: 0.0102
trigger times: 5
Loss after 349109640 batches: 0.0099
trigger times: 6
Loss after 349240740 batches: 0.0098
trigger times: 7
Loss after 349371840 batches: 0.0098
trigger times: 8
Loss after 349502940 batches: 0.0097
trigger times: 9
Loss after 349634040 batches: 0.0098
trigger times: 10
Loss after 349765140 batches: 0.0097
trigger times: 11
Loss after 349896240 batches: 0.0096
trigger times: 12
Loss after 350027340 batches: 0.0098
trigger times: 0
Loss after 350158440 batches: 0.0094
trigger times: 1
Loss after 350289540 batches: 0.0094
trigger times: 2
Loss after 350420640 batches: 0.0095
trigger times: 3
Loss after 350551740 batches: 0.0093
trigger times: 4
Loss after 350682840 batches: 0.0096
trigger times: 5
Loss after 350813940 batches: 0.0095
trigger times: 6
Loss after 350945040 batches: 0.0094
trigger times: 7
Loss after 351076140 batches: 0.0094
trigger times: 8
Loss after 351207240 batches: 0.0091
trigger times: 9
Loss after 351338340 batches: 0.0090
trigger times: 10
Loss after 351469440 batches: 0.0090
trigger times: 11
Loss after 351600540 batches: 0.0089
trigger times: 12
Loss after 351731640 batches: 0.0089
trigger times: 13
Loss after 351862740 batches: 0.0088
trigger times: 14
Loss after 351993840 batches: 0.0086
trigger times: 15
Loss after 352124940 batches: 0.0087
trigger times: 16
Loss after 352256040 batches: 0.0088
trigger times: 17
Loss after 352387140 batches: 0.0087
trigger times: 18
Loss after 352518240 batches: 0.0088
trigger times: 19
Loss after 352649340 batches: 0.0087
trigger times: 20
Early stopping!
Start to test process.
Loss after 352780440 batches: 0.0089
Time to train on one home:  472.56394052505493
trigger times: 0
Loss after 352883040 batches: 0.1877
trigger times: 0
Loss after 352985640 batches: 0.0777
trigger times: 1
Loss after 353088240 batches: 0.0481
trigger times: 2
Loss after 353190840 batches: 0.0367
trigger times: 3
Loss after 353293440 batches: 0.0346
trigger times: 4
Loss after 353396040 batches: 0.0311
trigger times: 5
Loss after 353498640 batches: 0.0281
trigger times: 6
Loss after 353601240 batches: 0.0280
trigger times: 7
Loss after 353703840 batches: 0.0280
trigger times: 8
Loss after 353806440 batches: 0.0288
trigger times: 0
Loss after 353909040 batches: 0.0264
trigger times: 1
Loss after 354011640 batches: 0.0251
trigger times: 2
Loss after 354114240 batches: 0.0243
trigger times: 0
Loss after 354216840 batches: 0.0250
trigger times: 1
Loss after 354319440 batches: 0.0236
trigger times: 2
Loss after 354422040 batches: 0.0239
trigger times: 3
Loss after 354524640 batches: 0.0221
trigger times: 4
Loss after 354627240 batches: 0.0216
trigger times: 5
Loss after 354729840 batches: 0.0208
trigger times: 6
Loss after 354832440 batches: 0.0205
trigger times: 0
Loss after 354935040 batches: 0.0202
trigger times: 0
Loss after 355037640 batches: 0.0209
trigger times: 1
Loss after 355140240 batches: 0.0199
trigger times: 2
Loss after 355242840 batches: 0.0194
trigger times: 3
Loss after 355345440 batches: 0.0200
trigger times: 4
Loss after 355448040 batches: 0.0189
trigger times: 5
Loss after 355550640 batches: 0.0195
trigger times: 6
Loss after 355653240 batches: 0.0193
trigger times: 7
Loss after 355755840 batches: 0.0190
trigger times: 0
Loss after 355858440 batches: 0.0191
trigger times: 1
Loss after 355961040 batches: 0.0199
trigger times: 2
Loss after 356063640 batches: 0.0189
trigger times: 3
Loss after 356166240 batches: 0.0185
trigger times: 4
Loss after 356268840 batches: 0.0192
trigger times: 5
Loss after 356371440 batches: 0.0182
trigger times: 6
Loss after 356474040 batches: 0.0193
trigger times: 0
Loss after 356576640 batches: 0.0231
trigger times: 1
Loss after 356679240 batches: 0.0187
trigger times: 2
Loss after 356781840 batches: 0.0175
trigger times: 3
Loss after 356884440 batches: 0.0178
trigger times: 4
Loss after 356987040 batches: 0.0183
trigger times: 5
Loss after 357089640 batches: 0.0187
trigger times: 0
Loss after 357192240 batches: 0.0214
trigger times: 1
Loss after 357294840 batches: 0.0178
trigger times: 2
Loss after 357397440 batches: 0.0186
trigger times: 3
Loss after 357500040 batches: 0.0187
trigger times: 4
Loss after 357602640 batches: 0.0181
trigger times: 5
Loss after 357705240 batches: 0.0166
trigger times: 0
Loss after 357807840 batches: 0.0168
trigger times: 1
Loss after 357910440 batches: 0.0173
trigger times: 2
Loss after 358013040 batches: 0.0175
trigger times: 3
Loss after 358115640 batches: 0.0178
trigger times: 4
Loss after 358218240 batches: 0.0184
trigger times: 5
Loss after 358320840 batches: 0.0176
trigger times: 6
Loss after 358423440 batches: 0.0167
trigger times: 7
Loss after 358526040 batches: 0.0176
trigger times: 8
Loss after 358628640 batches: 0.0167
trigger times: 9
Loss after 358731240 batches: 0.0163
trigger times: 10
Loss after 358833840 batches: 0.0166
trigger times: 11
Loss after 358936440 batches: 0.0163
trigger times: 12
Loss after 359039040 batches: 0.0159
trigger times: 13
Loss after 359141640 batches: 0.0158
trigger times: 14
Loss after 359244240 batches: 0.0153
trigger times: 15
Loss after 359346840 batches: 0.0158
trigger times: 16
Loss after 359449440 batches: 0.0161
trigger times: 17
Loss after 359552040 batches: 0.0160
trigger times: 18
Loss after 359654640 batches: 0.0157
trigger times: 19
Loss after 359757240 batches: 0.0164
trigger times: 20
Early stopping!
Start to test process.
Loss after 359859840 batches: 0.0158
Time to train on one home:  408.0396454334259
trigger times: 0
Loss after 359990940 batches: 0.1347
trigger times: 1
Loss after 360122040 batches: 0.0467
trigger times: 0
Loss after 360253140 batches: 0.0356
trigger times: 1
Loss after 360384240 batches: 0.0307
trigger times: 2
Loss after 360515340 batches: 0.0280
trigger times: 3
Loss after 360646440 batches: 0.0261
trigger times: 4
Loss after 360777540 batches: 0.0253
trigger times: 5
Loss after 360908640 batches: 0.0240
trigger times: 6
Loss after 361039740 batches: 0.0228
trigger times: 7
Loss after 361170840 batches: 0.0222
trigger times: 8
Loss after 361301940 batches: 0.0218
trigger times: 9
Loss after 361433040 batches: 0.0211
trigger times: 10
Loss after 361564140 batches: 0.0206
trigger times: 11
Loss after 361695240 batches: 0.0200
trigger times: 12
Loss after 361826340 batches: 0.0198
trigger times: 13
Loss after 361957440 batches: 0.0193
trigger times: 14
Loss after 362088540 batches: 0.0190
trigger times: 15
Loss after 362219640 batches: 0.0189
trigger times: 16
Loss after 362350740 batches: 0.0190
trigger times: 17
Loss after 362481840 batches: 0.0185
trigger times: 18
Loss after 362612940 batches: 0.0181
trigger times: 19
Loss after 362744040 batches: 0.0181
trigger times: 20
Early stopping!
Start to test process.
Loss after 362875140 batches: 0.0178
Time to train on one home:  174.655033826828
trigger times: 0
Loss after 363006240 batches: 0.2725
trigger times: 0
Loss after 363137340 batches: 0.0815
trigger times: 1
Loss after 363268440 batches: 0.0606
trigger times: 2
Loss after 363399540 batches: 0.0524
trigger times: 3
Loss after 363530640 batches: 0.0472
trigger times: 4
Loss after 363661740 batches: 0.0434
trigger times: 5
Loss after 363792840 batches: 0.0411
trigger times: 6
Loss after 363923940 batches: 0.0396
trigger times: 7
Loss after 364055040 batches: 0.0376
trigger times: 8
Loss after 364186140 batches: 0.0365
trigger times: 9
Loss after 364317240 batches: 0.0352
trigger times: 10
Loss after 364448340 batches: 0.0340
trigger times: 11
Loss after 364579440 batches: 0.0335
trigger times: 12
Loss after 364710540 batches: 0.0325
trigger times: 13
Loss after 364841640 batches: 0.0320
trigger times: 14
Loss after 364972740 batches: 0.0315
trigger times: 15
Loss after 365103840 batches: 0.0306
trigger times: 16
Loss after 365234940 batches: 0.0301
trigger times: 17
Loss after 365366040 batches: 0.0294
trigger times: 18
Loss after 365497140 batches: 0.0290
trigger times: 19
Loss after 365628240 batches: 0.0283
trigger times: 20
Early stopping!
Start to test process.
Loss after 365759340 batches: 0.0283
Time to train on one home:  167.72526335716248
trigger times: 0
Loss after 365887980 batches: 0.1538
trigger times: 0
Loss after 366016620 batches: 0.0393
trigger times: 0
Loss after 366145260 batches: 0.0279
trigger times: 0
Loss after 366273900 batches: 0.0236
trigger times: 1
Loss after 366402540 batches: 0.0222
trigger times: 2
Loss after 366531180 batches: 0.0208
trigger times: 3
Loss after 366659820 batches: 0.0191
trigger times: 4
Loss after 366788460 batches: 0.0186
trigger times: 5
Loss after 366917100 batches: 0.0176
trigger times: 0
Loss after 367045740 batches: 0.0172
trigger times: 1
Loss after 367174380 batches: 0.0166
trigger times: 2
Loss after 367303020 batches: 0.0163
trigger times: 3
Loss after 367431660 batches: 0.0159
trigger times: 4
Loss after 367560300 batches: 0.0157
trigger times: 5
Loss after 367688940 batches: 0.0155
trigger times: 6
Loss after 367817580 batches: 0.0149
trigger times: 7
Loss after 367946220 batches: 0.0148
trigger times: 8
Loss after 368074860 batches: 0.0145
trigger times: 9
Loss after 368203500 batches: 0.0142
trigger times: 10
Loss after 368332140 batches: 0.0138
trigger times: 11
Loss after 368460780 batches: 0.0137
trigger times: 12
Loss after 368589420 batches: 0.0137
trigger times: 13
Loss after 368718060 batches: 0.0138
trigger times: 14
Loss after 368846700 batches: 0.0133
trigger times: 15
Loss after 368975340 batches: 0.0131
trigger times: 16
Loss after 369103980 batches: 0.0130
trigger times: 17
Loss after 369232620 batches: 0.0129
trigger times: 18
Loss after 369361260 batches: 0.0130
trigger times: 19
Loss after 369489900 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 369618540 batches: 0.0127
Time to train on one home:  220.6766266822815
trigger times: 0
Loss after 369749640 batches: 0.2658
trigger times: 0
Loss after 369880740 batches: 0.0668
trigger times: 0
Loss after 370011840 batches: 0.0488
trigger times: 1
Loss after 370142940 batches: 0.0425
trigger times: 0
Loss after 370274040 batches: 0.0389
trigger times: 1
Loss after 370405140 batches: 0.0359
trigger times: 2
Loss after 370536240 batches: 0.0341
trigger times: 3
Loss after 370667340 batches: 0.0330
trigger times: 4
Loss after 370798440 batches: 0.0316
trigger times: 5
Loss after 370929540 batches: 0.0304
trigger times: 6
Loss after 371060640 batches: 0.0298
trigger times: 7
Loss after 371191740 batches: 0.0288
trigger times: 8
Loss after 371322840 batches: 0.0281
trigger times: 9
Loss after 371453940 batches: 0.0273
trigger times: 10
Loss after 371585040 batches: 0.0269
trigger times: 11
Loss after 371716140 batches: 0.0265
trigger times: 12
Loss after 371847240 batches: 0.0261
trigger times: 0
Loss after 371978340 batches: 0.0257
trigger times: 1
Loss after 372109440 batches: 0.0255
trigger times: 0
Loss after 372240540 batches: 0.0251
trigger times: 1
Loss after 372371640 batches: 0.0247
trigger times: 2
Loss after 372502740 batches: 0.0243
trigger times: 3
Loss after 372633840 batches: 0.0241
trigger times: 4
Loss after 372764940 batches: 0.0239
trigger times: 5
Loss after 372896040 batches: 0.0236
trigger times: 6
Loss after 373027140 batches: 0.0233
trigger times: 7
Loss after 373158240 batches: 0.0230
trigger times: 8
Loss after 373289340 batches: 0.0227
trigger times: 9
Loss after 373420440 batches: 0.0228
trigger times: 10
Loss after 373551540 batches: 0.0225
trigger times: 11
Loss after 373682640 batches: 0.0224
trigger times: 12
Loss after 373813740 batches: 0.0224
trigger times: 13
Loss after 373944840 batches: 0.0220
trigger times: 14
Loss after 374075940 batches: 0.0219
trigger times: 15
Loss after 374207040 batches: 0.0215
trigger times: 16
Loss after 374338140 batches: 0.0216
trigger times: 17
Loss after 374469240 batches: 0.0215
trigger times: 18
Loss after 374600340 batches: 0.0212
trigger times: 19
Loss after 374731440 batches: 0.0213
trigger times: 20
Early stopping!
Start to test process.
Loss after 374862540 batches: 0.0209
Time to train on one home:  294.9764316082001
trigger times: 0
Loss after 374993640 batches: 0.2235
trigger times: 0
Loss after 375124740 batches: 0.0734
trigger times: 0
Loss after 375255840 batches: 0.0477
trigger times: 0
Loss after 375386940 batches: 0.0400
trigger times: 0
Loss after 375518040 batches: 0.0365
trigger times: 0
Loss after 375649140 batches: 0.0322
trigger times: 0
Loss after 375780240 batches: 0.0308
trigger times: 1
Loss after 375911340 batches: 0.0293
trigger times: 2
Loss after 376042440 batches: 0.0296
trigger times: 0
Loss after 376173540 batches: 0.0290
trigger times: 0
Loss after 376304640 batches: 0.0271
trigger times: 1
Loss after 376435740 batches: 0.0259
trigger times: 0
Loss after 376566840 batches: 0.0254
trigger times: 1
Loss after 376697940 batches: 0.0254
trigger times: 2
Loss after 376829040 batches: 0.0248
trigger times: 0
Loss after 376960140 batches: 0.0248
trigger times: 1
Loss after 377091240 batches: 0.0258
trigger times: 2
Loss after 377222340 batches: 0.0250
trigger times: 0
Loss after 377353440 batches: 0.0243
trigger times: 1
Loss after 377484540 batches: 0.0247
trigger times: 2
Loss after 377615640 batches: 0.0242
trigger times: 0
Loss after 377746740 batches: 0.0224
trigger times: 1
Loss after 377877840 batches: 0.0219
trigger times: 2
Loss after 378008940 batches: 0.0227
trigger times: 3
Loss after 378140040 batches: 0.0218
trigger times: 0
Loss after 378271140 batches: 0.0230
trigger times: 1
Loss after 378402240 batches: 0.0224
trigger times: 2
Loss after 378533340 batches: 0.0221
trigger times: 0
Loss after 378664440 batches: 0.0222
trigger times: 1
Loss after 378795540 batches: 0.0222
trigger times: 2
Loss after 378926640 batches: 0.0212
trigger times: 3
Loss after 379057740 batches: 0.0212
trigger times: 4
Loss after 379188840 batches: 0.0209
trigger times: 5
Loss after 379319940 batches: 0.0208
trigger times: 6
Loss after 379451040 batches: 0.0212
trigger times: 7
Loss after 379582140 batches: 0.0215
trigger times: 8
Loss after 379713240 batches: 0.0213
trigger times: 9
Loss after 379844340 batches: 0.0216
trigger times: 10
Loss after 379975440 batches: 0.0204
trigger times: 0
Loss after 380106540 batches: 0.0205
trigger times: 0
Loss after 380237640 batches: 0.0201
trigger times: 1
Loss after 380368740 batches: 0.0203
trigger times: 2
Loss after 380499840 batches: 0.0203
trigger times: 3
Loss after 380630940 batches: 0.0203
trigger times: 4
Loss after 380762040 batches: 0.0206
trigger times: 5
Loss after 380893140 batches: 0.0208
trigger times: 0
Loss after 381024240 batches: 0.0205
trigger times: 0
Loss after 381155340 batches: 0.0209
trigger times: 1
Loss after 381286440 batches: 0.0201
trigger times: 2
Loss after 381417540 batches: 0.0191
trigger times: 0
Loss after 381548640 batches: 0.0188
trigger times: 1
Loss after 381679740 batches: 0.0198
trigger times: 0
Loss after 381810840 batches: 0.0192
trigger times: 1
Loss after 381941940 batches: 0.0197
trigger times: 2
Loss after 382073040 batches: 0.0197
trigger times: 3
Loss after 382204140 batches: 0.0200
trigger times: 4
Loss after 382335240 batches: 0.0190
trigger times: 5
Loss after 382466340 batches: 0.0199
trigger times: 6
Loss after 382597440 batches: 0.0200
trigger times: 7
Loss after 382728540 batches: 0.0183
trigger times: 8
Loss after 382859640 batches: 0.0182
trigger times: 9
Loss after 382990740 batches: 0.0189
trigger times: 10
Loss after 383121840 batches: 0.0193
trigger times: 11
Loss after 383252940 batches: 0.0189
trigger times: 12
Loss after 383384040 batches: 0.0194
trigger times: 13
Loss after 383515140 batches: 0.0186
trigger times: 14
Loss after 383646240 batches: 0.0177
trigger times: 15
Loss after 383777340 batches: 0.0183
trigger times: 16
Loss after 383908440 batches: 0.0188
trigger times: 17
Loss after 384039540 batches: 0.0187
trigger times: 18
Loss after 384170640 batches: 0.0197
trigger times: 19
Loss after 384301740 batches: 0.0191
trigger times: 20
Early stopping!
Start to test process.
Loss after 384432840 batches: 0.0182
Time to train on one home:  529.7435781955719
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887, 0.018152045403586472, 0.017512356438894854]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884], [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987], [0.5804732673698001, 0.3723540197761146, 0.45230155418961127, 1.0682024720651153, 0.514164189884338, 25.236666271720736, 1587.2568]]
Round_9_results:  [0.5804732673698001, 0.3723540197761146, 0.45230155418961127, 1.0682024720651153, 0.514164189884338, 25.236666271720736, 1587.2568]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3044 < 3045; dropping {'Training_Loss': 0.09506145576542278, 'Validation_Loss': 0.25157426132096183, 'Training_R2': 0.9044020187821883, 'Validation_R2': 0.7663802491925454, 'Training_F1': 0.8308422819398606, 'Validation_F1': 0.7159782191379304, 'Training_NEP': 0.338557797984331, 'Validation_NEP': 0.5208514601987564, 'Training_NDE': 0.07176710136700766, 'Validation_NDE': 0.18604245448592005, 'Training_MAE': 11.212362836725744, 'Validation_MAE': 14.283907918989872, 'Training_MSE': 315.764, 'Validation_MSE': 687.0497}.
trigger times: 0
Loss after 384563940 batches: 0.0951
trigger times: 0
Loss after 384695040 batches: 0.0289
trigger times: 1
Loss after 384826140 batches: 0.0209
trigger times: 0
Loss after 384957240 batches: 0.0179
trigger times: 0
Loss after 385088340 batches: 0.0162
trigger times: 0
Loss after 385219440 batches: 0.0155
trigger times: 1
Loss after 385350540 batches: 0.0146
trigger times: 2
Loss after 385481640 batches: 0.0142
trigger times: 3
Loss after 385612740 batches: 0.0134
trigger times: 4
Loss after 385743840 batches: 0.0131
trigger times: 5
Loss after 385874940 batches: 0.0126
trigger times: 6
Loss after 386006040 batches: 0.0126
trigger times: 7
Loss after 386137140 batches: 0.0120
trigger times: 8
Loss after 386268240 batches: 0.0122
trigger times: 9
Loss after 386399340 batches: 0.0117
trigger times: 0
Loss after 386530440 batches: 0.0116
trigger times: 1
Loss after 386661540 batches: 0.0113
trigger times: 2
Loss after 386792640 batches: 0.0113
trigger times: 3
Loss after 386923740 batches: 0.0113
trigger times: 4
Loss after 387054840 batches: 0.0108
trigger times: 5
Loss after 387185940 batches: 0.0108
trigger times: 0
Loss after 387317040 batches: 0.0104
trigger times: 1
Loss after 387448140 batches: 0.0106
trigger times: 2
Loss after 387579240 batches: 0.0103
trigger times: 3
Loss after 387710340 batches: 0.0105
trigger times: 4
Loss after 387841440 batches: 0.0102
trigger times: 5
Loss after 387972540 batches: 0.0102
trigger times: 6
Loss after 388103640 batches: 0.0102
trigger times: 7
Loss after 388234740 batches: 0.0099
trigger times: 8
Loss after 388365840 batches: 0.0101
trigger times: 9
Loss after 388496940 batches: 0.0100
trigger times: 10
Loss after 388628040 batches: 0.0098
trigger times: 11
Loss after 388759140 batches: 0.0095
trigger times: 12
Loss after 388890240 batches: 0.0095
trigger times: 13
Loss after 389021340 batches: 0.0094
trigger times: 14
Loss after 389152440 batches: 0.0094
trigger times: 15
Loss after 389283540 batches: 0.0094
trigger times: 16
Loss after 389414640 batches: 0.0093
trigger times: 17
Loss after 389545740 batches: 0.0092
trigger times: 18
Loss after 389676840 batches: 0.0093
trigger times: 0
Loss after 389807940 batches: 0.0089
trigger times: 1
Loss after 389939040 batches: 0.0090
trigger times: 2
Loss after 390070140 batches: 0.0089
trigger times: 3
Loss after 390201240 batches: 0.0088
trigger times: 4
Loss after 390332340 batches: 0.0088
trigger times: 5
Loss after 390463440 batches: 0.0087
trigger times: 6
Loss after 390594540 batches: 0.0086
trigger times: 7
Loss after 390725640 batches: 0.0086
trigger times: 8
Loss after 390856740 batches: 0.0086
trigger times: 9
Loss after 390987840 batches: 0.0086
trigger times: 10
Loss after 391118940 batches: 0.0085
trigger times: 11
Loss after 391250040 batches: 0.0084
trigger times: 12
Loss after 391381140 batches: 0.0084
trigger times: 0
Loss after 391512240 batches: 0.0084
trigger times: 1
Loss after 391643340 batches: 0.0082
trigger times: 2
Loss after 391774440 batches: 0.0085
trigger times: 3
Loss after 391905540 batches: 0.0085
trigger times: 4
Loss after 392036640 batches: 0.0084
trigger times: 5
Loss after 392167740 batches: 0.0082
trigger times: 6
Loss after 392298840 batches: 0.0082
trigger times: 7
Loss after 392429940 batches: 0.0081
trigger times: 8
Loss after 392561040 batches: 0.0082
trigger times: 9
Loss after 392692140 batches: 0.0082
trigger times: 10
Loss after 392823240 batches: 0.0082
trigger times: 11
Loss after 392954340 batches: 0.0079
trigger times: 12
Loss after 393085440 batches: 0.0080
trigger times: 13
Loss after 393216540 batches: 0.0079
trigger times: 14
Loss after 393347640 batches: 0.0078
trigger times: 15
Loss after 393478740 batches: 0.0079
trigger times: 16
Loss after 393609840 batches: 0.0078
trigger times: 17
Loss after 393740940 batches: 0.0078
trigger times: 18
Loss after 393872040 batches: 0.0077
trigger times: 19
Loss after 394003140 batches: 0.0078
trigger times: 20
Early stopping!
Start to test process.
Loss after 394134240 batches: 0.0078
Time to train on one home:  537.6725113391876
trigger times: 0
Loss after 394236840 batches: 0.1887
trigger times: 0
Loss after 394339440 batches: 0.0874
trigger times: 0
Loss after 394442040 batches: 0.0477
trigger times: 0
Loss after 394544640 batches: 0.0372
trigger times: 0
Loss after 394647240 batches: 0.0334
trigger times: 1
Loss after 394749840 batches: 0.0282
trigger times: 2
Loss after 394852440 batches: 0.0268
trigger times: 3
Loss after 394955040 batches: 0.0256
trigger times: 0
Loss after 395057640 batches: 0.0245
trigger times: 1
Loss after 395160240 batches: 0.0246
trigger times: 2
Loss after 395262840 batches: 0.0236
trigger times: 3
Loss after 395365440 batches: 0.0223
trigger times: 4
Loss after 395468040 batches: 0.0221
trigger times: 5
Loss after 395570640 batches: 0.0226
trigger times: 6
Loss after 395673240 batches: 0.0244
trigger times: 0
Loss after 395775840 batches: 0.0214
trigger times: 1
Loss after 395878440 batches: 0.0217
trigger times: 2
Loss after 395981040 batches: 0.0207
trigger times: 3
Loss after 396083640 batches: 0.0204
trigger times: 4
Loss after 396186240 batches: 0.0198
trigger times: 0
Loss after 396288840 batches: 0.0207
trigger times: 1
Loss after 396391440 batches: 0.0201
trigger times: 2
Loss after 396494040 batches: 0.0218
trigger times: 3
Loss after 396596640 batches: 0.0200
trigger times: 4
Loss after 396699240 batches: 0.0182
trigger times: 5
Loss after 396801840 batches: 0.0191
trigger times: 6
Loss after 396904440 batches: 0.0185
trigger times: 7
Loss after 397007040 batches: 0.0193
trigger times: 8
Loss after 397109640 batches: 0.0191
trigger times: 9
Loss after 397212240 batches: 0.0178
trigger times: 10
Loss after 397314840 batches: 0.0182
trigger times: 11
Loss after 397417440 batches: 0.0180
trigger times: 12
Loss after 397520040 batches: 0.0177
trigger times: 13
Loss after 397622640 batches: 0.0176
trigger times: 14
Loss after 397725240 batches: 0.0189
trigger times: 15
Loss after 397827840 batches: 0.0170
trigger times: 16
Loss after 397930440 batches: 0.0167
trigger times: 17
Loss after 398033040 batches: 0.0168
trigger times: 18
Loss after 398135640 batches: 0.0170
trigger times: 0
Loss after 398238240 batches: 0.0188
trigger times: 1
Loss after 398340840 batches: 0.0181
trigger times: 2
Loss after 398443440 batches: 0.0182
trigger times: 3
Loss after 398546040 batches: 0.0182
trigger times: 4
Loss after 398648640 batches: 0.0170
trigger times: 5
Loss after 398751240 batches: 0.0163
trigger times: 0
Loss after 398853840 batches: 0.0159
trigger times: 1
Loss after 398956440 batches: 0.0165
trigger times: 2
Loss after 399059040 batches: 0.0161
trigger times: 3
Loss after 399161640 batches: 0.0162
trigger times: 4
Loss after 399264240 batches: 0.0160
trigger times: 5
Loss after 399366840 batches: 0.0168
trigger times: 6
Loss after 399469440 batches: 0.0163
trigger times: 7
Loss after 399572040 batches: 0.0169
trigger times: 8
Loss after 399674640 batches: 0.0220
trigger times: 9
Loss after 399777240 batches: 0.0170
trigger times: 10
Loss after 399879840 batches: 0.0159
trigger times: 11
Loss after 399982440 batches: 0.0153
trigger times: 12
Loss after 400085040 batches: 0.0162
trigger times: 13
Loss after 400187640 batches: 0.0162
trigger times: 14
Loss after 400290240 batches: 0.0202
trigger times: 15
Loss after 400392840 batches: 0.0165
trigger times: 16
Loss after 400495440 batches: 0.0154
trigger times: 17
Loss after 400598040 batches: 0.0154
trigger times: 18
Loss after 400700640 batches: 0.0149
trigger times: 19
Loss after 400803240 batches: 0.0150
trigger times: 20
Early stopping!
Start to test process.
Loss after 400905840 batches: 0.0150
Time to train on one home:  391.3877856731415
trigger times: 0
Loss after 401036940 batches: 0.1320
trigger times: 0
Loss after 401168040 batches: 0.0456
trigger times: 1
Loss after 401299140 batches: 0.0345
trigger times: 0
Loss after 401430240 batches: 0.0305
trigger times: 1
Loss after 401561340 batches: 0.0277
trigger times: 2
Loss after 401692440 batches: 0.0255
trigger times: 3
Loss after 401823540 batches: 0.0248
trigger times: 0
Loss after 401954640 batches: 0.0233
trigger times: 1
Loss after 402085740 batches: 0.0225
trigger times: 2
Loss after 402216840 batches: 0.0218
trigger times: 3
Loss after 402347940 batches: 0.0212
trigger times: 4
Loss after 402479040 batches: 0.0208
trigger times: 5
Loss after 402610140 batches: 0.0203
trigger times: 6
Loss after 402741240 batches: 0.0199
trigger times: 7
Loss after 402872340 batches: 0.0194
trigger times: 8
Loss after 403003440 batches: 0.0191
trigger times: 9
Loss after 403134540 batches: 0.0187
trigger times: 10
Loss after 403265640 batches: 0.0183
trigger times: 11
Loss after 403396740 batches: 0.0182
trigger times: 12
Loss after 403527840 batches: 0.0180
trigger times: 13
Loss after 403658940 batches: 0.0178
trigger times: 14
Loss after 403790040 batches: 0.0176
trigger times: 15
Loss after 403921140 batches: 0.0172
trigger times: 16
Loss after 404052240 batches: 0.0172
trigger times: 17
Loss after 404183340 batches: 0.0171
trigger times: 18
Loss after 404314440 batches: 0.0168
trigger times: 19
Loss after 404445540 batches: 0.0168
trigger times: 20
Early stopping!
Start to test process.
Loss after 404576640 batches: 0.0165
Time to train on one home:  210.09755063056946
trigger times: 0
Loss after 404707740 batches: 0.2572
trigger times: 1
Loss after 404838840 batches: 0.0776
trigger times: 0
Loss after 404969940 batches: 0.0586
trigger times: 1
Loss after 405101040 batches: 0.0498
trigger times: 2
Loss after 405232140 batches: 0.0456
trigger times: 3
Loss after 405363240 batches: 0.0418
trigger times: 4
Loss after 405494340 batches: 0.0403
trigger times: 5
Loss after 405625440 batches: 0.0382
trigger times: 6
Loss after 405756540 batches: 0.0370
trigger times: 0
Loss after 405887640 batches: 0.0352
trigger times: 1
Loss after 406018740 batches: 0.0340
trigger times: 2
Loss after 406149840 batches: 0.0334
trigger times: 3
Loss after 406280940 batches: 0.0324
trigger times: 4
Loss after 406412040 batches: 0.0317
trigger times: 5
Loss after 406543140 batches: 0.0310
trigger times: 6
Loss after 406674240 batches: 0.0303
trigger times: 7
Loss after 406805340 batches: 0.0297
trigger times: 8
Loss after 406936440 batches: 0.0291
trigger times: 9
Loss after 407067540 batches: 0.0288
trigger times: 10
Loss after 407198640 batches: 0.0281
trigger times: 11
Loss after 407329740 batches: 0.0277
trigger times: 12
Loss after 407460840 batches: 0.0274
trigger times: 13
Loss after 407591940 batches: 0.0269
trigger times: 14
Loss after 407723040 batches: 0.0266
trigger times: 15
Loss after 407854140 batches: 0.0265
trigger times: 16
Loss after 407985240 batches: 0.0264
trigger times: 17
Loss after 408116340 batches: 0.0261
trigger times: 18
Loss after 408247440 batches: 0.0259
trigger times: 19
Loss after 408378540 batches: 0.0256
trigger times: 20
Early stopping!
Start to test process.
Loss after 408509640 batches: 0.0250
Time to train on one home:  224.59927654266357
trigger times: 0
Loss after 408638280 batches: 0.1087
trigger times: 0
Loss after 408766920 batches: 0.0314
trigger times: 0
Loss after 408895560 batches: 0.0238
trigger times: 1
Loss after 409024200 batches: 0.0211
trigger times: 2
Loss after 409152840 batches: 0.0200
trigger times: 0
Loss after 409281480 batches: 0.0184
trigger times: 1
Loss after 409410120 batches: 0.0181
trigger times: 2
Loss after 409538760 batches: 0.0171
trigger times: 3
Loss after 409667400 batches: 0.0165
trigger times: 4
Loss after 409796040 batches: 0.0158
trigger times: 5
Loss after 409924680 batches: 0.0155
trigger times: 6
Loss after 410053320 batches: 0.0151
trigger times: 7
Loss after 410181960 batches: 0.0150
trigger times: 8
Loss after 410310600 batches: 0.0145
trigger times: 9
Loss after 410439240 batches: 0.0144
trigger times: 10
Loss after 410567880 batches: 0.0142
trigger times: 11
Loss after 410696520 batches: 0.0140
trigger times: 12
Loss after 410825160 batches: 0.0138
trigger times: 13
Loss after 410953800 batches: 0.0134
trigger times: 14
Loss after 411082440 batches: 0.0134
trigger times: 15
Loss after 411211080 batches: 0.0133
trigger times: 16
Loss after 411339720 batches: 0.0130
trigger times: 17
Loss after 411468360 batches: 0.0131
trigger times: 18
Loss after 411597000 batches: 0.0127
trigger times: 19
Loss after 411725640 batches: 0.0127
trigger times: 20
Early stopping!
Start to test process.
Loss after 411854280 batches: 0.0125
Time to train on one home:  193.47132325172424
trigger times: 0
Loss after 411985380 batches: 0.2147
trigger times: 0
Loss after 412116480 batches: 0.0584
trigger times: 0
Loss after 412247580 batches: 0.0434
trigger times: 0
Loss after 412378680 batches: 0.0383
trigger times: 0
Loss after 412509780 batches: 0.0352
trigger times: 0
Loss after 412640880 batches: 0.0336
trigger times: 1
Loss after 412771980 batches: 0.0321
trigger times: 2
Loss after 412903080 batches: 0.0305
trigger times: 0
Loss after 413034180 batches: 0.0292
trigger times: 1
Loss after 413165280 batches: 0.0288
trigger times: 2
Loss after 413296380 batches: 0.0275
trigger times: 3
Loss after 413427480 batches: 0.0270
trigger times: 0
Loss after 413558580 batches: 0.0266
trigger times: 1
Loss after 413689680 batches: 0.0261
trigger times: 2
Loss after 413820780 batches: 0.0256
trigger times: 0
Loss after 413951880 batches: 0.0253
trigger times: 0
Loss after 414082980 batches: 0.0250
trigger times: 1
Loss after 414214080 batches: 0.0244
trigger times: 2
Loss after 414345180 batches: 0.0241
trigger times: 0
Loss after 414476280 batches: 0.0240
trigger times: 1
Loss after 414607380 batches: 0.0237
trigger times: 2
Loss after 414738480 batches: 0.0235
trigger times: 3
Loss after 414869580 batches: 0.0230
trigger times: 4
Loss after 415000680 batches: 0.0226
trigger times: 5
Loss after 415131780 batches: 0.0224
trigger times: 6
Loss after 415262880 batches: 0.0226
trigger times: 7
Loss after 415393980 batches: 0.0220
trigger times: 0
Loss after 415525080 batches: 0.0218
trigger times: 1
Loss after 415656180 batches: 0.0216
trigger times: 2
Loss after 415787280 batches: 0.0215
trigger times: 3
Loss after 415918380 batches: 0.0213
trigger times: 0
Loss after 416049480 batches: 0.0212
trigger times: 1
Loss after 416180580 batches: 0.0211
trigger times: 2
Loss after 416311680 batches: 0.0212
trigger times: 3
Loss after 416442780 batches: 0.0207
trigger times: 4
Loss after 416573880 batches: 0.0205
trigger times: 5
Loss after 416704980 batches: 0.0208
trigger times: 6
Loss after 416836080 batches: 0.0206
trigger times: 7
Loss after 416967180 batches: 0.0206
trigger times: 8
Loss after 417098280 batches: 0.0199
trigger times: 9
Loss after 417229380 batches: 0.0201
trigger times: 10
Loss after 417360480 batches: 0.0200
trigger times: 11
Loss after 417491580 batches: 0.0198
trigger times: 12
Loss after 417622680 batches: 0.0197
trigger times: 0
Loss after 417753780 batches: 0.0199
trigger times: 1
Loss after 417884880 batches: 0.0196
trigger times: 2
Loss after 418015980 batches: 0.0197
trigger times: 3
Loss after 418147080 batches: 0.0193
trigger times: 4
Loss after 418278180 batches: 0.0191
trigger times: 5
Loss after 418409280 batches: 0.0190
trigger times: 6
Loss after 418540380 batches: 0.0189
trigger times: 7
Loss after 418671480 batches: 0.0187
trigger times: 8
Loss after 418802580 batches: 0.0185
trigger times: 9
Loss after 418933680 batches: 0.0187
trigger times: 10
Loss after 419064780 batches: 0.0188
trigger times: 11
Loss after 419195880 batches: 0.0187
trigger times: 12
Loss after 419326980 batches: 0.0185
trigger times: 13
Loss after 419458080 batches: 0.0183
trigger times: 14
Loss after 419589180 batches: 0.0182
trigger times: 15
Loss after 419720280 batches: 0.0183
trigger times: 16
Loss after 419851380 batches: 0.0182
trigger times: 17
Loss after 419982480 batches: 0.0183
trigger times: 18
Loss after 420113580 batches: 0.0181
trigger times: 19
Loss after 420244680 batches: 0.0181
trigger times: 20
Early stopping!
Start to test process.
Loss after 420375780 batches: 0.0179
Time to train on one home:  470.5475115776062
trigger times: 0
Loss after 420506880 batches: 0.1546
trigger times: 0
Loss after 420637980 batches: 0.0657
trigger times: 0
Loss after 420769080 batches: 0.0476
trigger times: 0
Loss after 420900180 batches: 0.0364
trigger times: 0
Loss after 421031280 batches: 0.0329
trigger times: 1
Loss after 421162380 batches: 0.0307
trigger times: 0
Loss after 421293480 batches: 0.0293
trigger times: 0
Loss after 421424580 batches: 0.0289
trigger times: 1
Loss after 421555680 batches: 0.0278
trigger times: 2
Loss after 421686780 batches: 0.0258
trigger times: 3
Loss after 421817880 batches: 0.0252
trigger times: 4
Loss after 421948980 batches: 0.0252
trigger times: 0
Loss after 422080080 batches: 0.0260
trigger times: 1
Loss after 422211180 batches: 0.0246
trigger times: 2
Loss after 422342280 batches: 0.0237
trigger times: 3
Loss after 422473380 batches: 0.0248
trigger times: 4
Loss after 422604480 batches: 0.0234
trigger times: 0
Loss after 422735580 batches: 0.0233
trigger times: 1
Loss after 422866680 batches: 0.0241
trigger times: 0
Loss after 422997780 batches: 0.0227
trigger times: 1
Loss after 423128880 batches: 0.0223
trigger times: 2
Loss after 423259980 batches: 0.0233
trigger times: 3
Loss after 423391080 batches: 0.0216
trigger times: 4
Loss after 423522180 batches: 0.0209
trigger times: 5
Loss after 423653280 batches: 0.0210
trigger times: 0
Loss after 423784380 batches: 0.0211
trigger times: 0
Loss after 423915480 batches: 0.0222
trigger times: 0
Loss after 424046580 batches: 0.0214
trigger times: 1
Loss after 424177680 batches: 0.0218
trigger times: 2
Loss after 424308780 batches: 0.0211
trigger times: 0
Loss after 424439880 batches: 0.0213
trigger times: 1
Loss after 424570980 batches: 0.0215
trigger times: 2
Loss after 424702080 batches: 0.0217
trigger times: 3
Loss after 424833180 batches: 0.0206
trigger times: 4
Loss after 424964280 batches: 0.0211
trigger times: 5
Loss after 425095380 batches: 0.0205
trigger times: 6
Loss after 425226480 batches: 0.0213
trigger times: 7
Loss after 425357580 batches: 0.0202
trigger times: 8
Loss after 425488680 batches: 0.0199
trigger times: 9
Loss after 425619780 batches: 0.0192
trigger times: 10
Loss after 425750880 batches: 0.0197
trigger times: 11
Loss after 425881980 batches: 0.0193
trigger times: 12
Loss after 426013080 batches: 0.0193
trigger times: 0
Loss after 426144180 batches: 0.0197
trigger times: 1
Loss after 426275280 batches: 0.0207
trigger times: 2
Loss after 426406380 batches: 0.0205
trigger times: 0
Loss after 426537480 batches: 0.0197
trigger times: 1
Loss after 426668580 batches: 0.0184
trigger times: 2
Loss after 426799680 batches: 0.0183
trigger times: 3
Loss after 426930780 batches: 0.0186
trigger times: 4
Loss after 427061880 batches: 0.0187
trigger times: 5
Loss after 427192980 batches: 0.0190
trigger times: 6
Loss after 427324080 batches: 0.0192
trigger times: 7
Loss after 427455180 batches: 0.0191
trigger times: 8
Loss after 427586280 batches: 0.0187
trigger times: 9
Loss after 427717380 batches: 0.0186
trigger times: 10
Loss after 427848480 batches: 0.0187
trigger times: 11
Loss after 427979580 batches: 0.0188
trigger times: 12
Loss after 428110680 batches: 0.0187
trigger times: 13
Loss after 428241780 batches: 0.0192
trigger times: 14
Loss after 428372880 batches: 0.0192
trigger times: 15
Loss after 428503980 batches: 0.0189
trigger times: 16
Loss after 428635080 batches: 0.0181
trigger times: 17
Loss after 428766180 batches: 0.0179
trigger times: 18
Loss after 428897280 batches: 0.0185
trigger times: 19
Loss after 429028380 batches: 0.0181
trigger times: 20
Early stopping!
Start to test process.
Loss after 429159480 batches: 0.0178
Time to train on one home:  485.4668712615967
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887, 0.018152045403586472, 0.017512356438894854, 0.016091731943637777]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884], [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987], [0.5804732673698001, 0.3723540197761146, 0.45230155418961127, 1.0682024720651153, 0.514164189884338, 25.236666271720736, 1587.2568], [0.5693913300832113, 0.3843477905525722, 0.4653277545238437, 1.0599366084402062, 0.5043389577801884, 25.04138228090006, 1556.9258]]
Round_10_results:  [0.5693913300832113, 0.3843477905525722, 0.4653277545238437, 1.0599366084402062, 0.5043389577801884, 25.04138228090006, 1556.9258]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3400 < 3401; dropping {'Training_Loss': 0.10273093238191784, 'Validation_Loss': 0.23578675091266632, 'Training_R2': 0.8965662330569674, 'Validation_R2': 0.7810969849616284, 'Training_F1': 0.8241894257313125, 'Validation_F1': 0.7020048283025182, 'Training_NEP': 0.35173465444833524, 'Validation_NEP': 0.5653292975846275, 'Training_NDE': 0.07764956479634307, 'Validation_NDE': 0.1743228218990439, 'Training_MAE': 11.648754190289274, 'Validation_MAE': 15.503674747354241, 'Training_MSE': 341.6459, 'Validation_MSE': 643.7694}.
trigger times: 0
Loss after 429290580 batches: 0.1027
trigger times: 0
Loss after 429421680 batches: 0.0279
trigger times: 0
Loss after 429552780 batches: 0.0203
trigger times: 1
Loss after 429683880 batches: 0.0175
trigger times: 0
Loss after 429814980 batches: 0.0159
trigger times: 1
Loss after 429946080 batches: 0.0150
trigger times: 2
Loss after 430077180 batches: 0.0142
trigger times: 3
Loss after 430208280 batches: 0.0134
trigger times: 4
Loss after 430339380 batches: 0.0128
trigger times: 5
Loss after 430470480 batches: 0.0129
trigger times: 6
Loss after 430601580 batches: 0.0125
trigger times: 0
Loss after 430732680 batches: 0.0121
trigger times: 0
Loss after 430863780 batches: 0.0119
trigger times: 0
Loss after 430994880 batches: 0.0114
trigger times: 1
Loss after 431125980 batches: 0.0115
trigger times: 2
Loss after 431257080 batches: 0.0111
trigger times: 3
Loss after 431388180 batches: 0.0110
trigger times: 4
Loss after 431519280 batches: 0.0108
trigger times: 5
Loss after 431650380 batches: 0.0109
trigger times: 6
Loss after 431781480 batches: 0.0106
trigger times: 7
Loss after 431912580 batches: 0.0106
trigger times: 8
Loss after 432043680 batches: 0.0100
trigger times: 9
Loss after 432174780 batches: 0.0103
trigger times: 10
Loss after 432305880 batches: 0.0102
trigger times: 11
Loss after 432436980 batches: 0.0099
trigger times: 12
Loss after 432568080 batches: 0.0097
trigger times: 13
Loss after 432699180 batches: 0.0096
trigger times: 14
Loss after 432830280 batches: 0.0097
trigger times: 15
Loss after 432961380 batches: 0.0097
trigger times: 16
Loss after 433092480 batches: 0.0098
trigger times: 0
Loss after 433223580 batches: 0.0095
trigger times: 1
Loss after 433354680 batches: 0.0094
trigger times: 2
Loss after 433485780 batches: 0.0092
trigger times: 3
Loss after 433616880 batches: 0.0092
trigger times: 4
Loss after 433747980 batches: 0.0092
trigger times: 5
Loss after 433879080 batches: 0.0090
trigger times: 6
Loss after 434010180 batches: 0.0091
trigger times: 7
Loss after 434141280 batches: 0.0089
trigger times: 8
Loss after 434272380 batches: 0.0089
trigger times: 9
Loss after 434403480 batches: 0.0087
trigger times: 10
Loss after 434534580 batches: 0.0088
trigger times: 11
Loss after 434665680 batches: 0.0087
trigger times: 12
Loss after 434796780 batches: 0.0088
trigger times: 13
Loss after 434927880 batches: 0.0085
trigger times: 14
Loss after 435058980 batches: 0.0087
trigger times: 15
Loss after 435190080 batches: 0.0086
trigger times: 16
Loss after 435321180 batches: 0.0085
trigger times: 17
Loss after 435452280 batches: 0.0083
trigger times: 0
Loss after 435583380 batches: 0.0084
trigger times: 1
Loss after 435714480 batches: 0.0083
trigger times: 2
Loss after 435845580 batches: 0.0082
trigger times: 3
Loss after 435976680 batches: 0.0084
trigger times: 4
Loss after 436107780 batches: 0.0083
trigger times: 5
Loss after 436238880 batches: 0.0082
trigger times: 6
Loss after 436369980 batches: 0.0080
trigger times: 7
Loss after 436501080 batches: 0.0082
trigger times: 8
Loss after 436632180 batches: 0.0082
trigger times: 9
Loss after 436763280 batches: 0.0081
trigger times: 10
Loss after 436894380 batches: 0.0079
trigger times: 11
Loss after 437025480 batches: 0.0079
trigger times: 12
Loss after 437156580 batches: 0.0081
trigger times: 13
Loss after 437287680 batches: 0.0078
trigger times: 14
Loss after 437418780 batches: 0.0079
trigger times: 15
Loss after 437549880 batches: 0.0078
trigger times: 16
Loss after 437680980 batches: 0.0077
trigger times: 17
Loss after 437812080 batches: 0.0079
trigger times: 18
Loss after 437943180 batches: 0.0077
trigger times: 19
Loss after 438074280 batches: 0.0075
trigger times: 20
Early stopping!
Start to test process.
Loss after 438205380 batches: 0.0075
Time to train on one home:  500.6470408439636
trigger times: 0
Loss after 438307980 batches: 0.2132
trigger times: 0
Loss after 438410580 batches: 0.0633
trigger times: 0
Loss after 438513180 batches: 0.0444
trigger times: 1
Loss after 438615780 batches: 0.0346
trigger times: 2
Loss after 438718380 batches: 0.0309
trigger times: 0
Loss after 438820980 batches: 0.0298
trigger times: 1
Loss after 438923580 batches: 0.0283
trigger times: 0
Loss after 439026180 batches: 0.0265
trigger times: 1
Loss after 439128780 batches: 0.0254
trigger times: 2
Loss after 439231380 batches: 0.0246
trigger times: 3
Loss after 439333980 batches: 0.0233
trigger times: 0
Loss after 439436580 batches: 0.0226
trigger times: 0
Loss after 439539180 batches: 0.0236
trigger times: 1
Loss after 439641780 batches: 0.0226
trigger times: 0
Loss after 439744380 batches: 0.0229
trigger times: 1
Loss after 439846980 batches: 0.0214
trigger times: 0
Loss after 439949580 batches: 0.0232
trigger times: 1
Loss after 440052180 batches: 0.0199
trigger times: 2
Loss after 440154780 batches: 0.0204
trigger times: 3
Loss after 440257380 batches: 0.0194
trigger times: 0
Loss after 440359980 batches: 0.0194
trigger times: 1
Loss after 440462580 batches: 0.0219
trigger times: 2
Loss after 440565180 batches: 0.0207
trigger times: 0
Loss after 440667780 batches: 0.0192
trigger times: 1
Loss after 440770380 batches: 0.0184
trigger times: 2
Loss after 440872980 batches: 0.0177
trigger times: 3
Loss after 440975580 batches: 0.0190
trigger times: 0
Loss after 441078180 batches: 0.0189
trigger times: 1
Loss after 441180780 batches: 0.0178
trigger times: 2
Loss after 441283380 batches: 0.0189
trigger times: 3
Loss after 441385980 batches: 0.0179
trigger times: 4
Loss after 441488580 batches: 0.0176
trigger times: 5
Loss after 441591180 batches: 0.0186
trigger times: 0
Loss after 441693780 batches: 0.0192
trigger times: 1
Loss after 441796380 batches: 0.0178
trigger times: 0
Loss after 441898980 batches: 0.0179
trigger times: 1
Loss after 442001580 batches: 0.0171
trigger times: 2
Loss after 442104180 batches: 0.0169
trigger times: 3
Loss after 442206780 batches: 0.0168
trigger times: 4
Loss after 442309380 batches: 0.0163
trigger times: 0
Loss after 442411980 batches: 0.0186
trigger times: 1
Loss after 442514580 batches: 0.0179
trigger times: 2
Loss after 442617180 batches: 0.0169
trigger times: 3
Loss after 442719780 batches: 0.0163
trigger times: 4
Loss after 442822380 batches: 0.0158
trigger times: 5
Loss after 442924980 batches: 0.0156
trigger times: 6
Loss after 443027580 batches: 0.0158
trigger times: 7
Loss after 443130180 batches: 0.0166
trigger times: 8
Loss after 443232780 batches: 0.0165
trigger times: 0
Loss after 443335380 batches: 0.0165
trigger times: 1
Loss after 443437980 batches: 0.0155
trigger times: 2
Loss after 443540580 batches: 0.0162
trigger times: 3
Loss after 443643180 batches: 0.0156
trigger times: 4
Loss after 443745780 batches: 0.0173
trigger times: 5
Loss after 443848380 batches: 0.0159
trigger times: 6
Loss after 443950980 batches: 0.0171
trigger times: 7
Loss after 444053580 batches: 0.0159
trigger times: 8
Loss after 444156180 batches: 0.0164
trigger times: 9
Loss after 444258780 batches: 0.0197
trigger times: 10
Loss after 444361380 batches: 0.0164
trigger times: 11
Loss after 444463980 batches: 0.0153
trigger times: 12
Loss after 444566580 batches: 0.0150
trigger times: 0
Loss after 444669180 batches: 0.0158
trigger times: 1
Loss after 444771780 batches: 0.0152
trigger times: 2
Loss after 444874380 batches: 0.0159
trigger times: 0
Loss after 444976980 batches: 0.0148
trigger times: 1
Loss after 445079580 batches: 0.0146
trigger times: 2
Loss after 445182180 batches: 0.0148
trigger times: 3
Loss after 445284780 batches: 0.0150
trigger times: 4
Loss after 445387380 batches: 0.0177
trigger times: 5
Loss after 445489980 batches: 0.0148
trigger times: 6
Loss after 445592580 batches: 0.0154
trigger times: 7
Loss after 445695180 batches: 0.0153
trigger times: 8
Loss after 445797780 batches: 0.0148
trigger times: 9
Loss after 445900380 batches: 0.0148
trigger times: 10
Loss after 446002980 batches: 0.0153
trigger times: 0
Loss after 446105580 batches: 0.0142
trigger times: 1
Loss after 446208180 batches: 0.0141
trigger times: 2
Loss after 446310780 batches: 0.0141
trigger times: 3
Loss after 446413380 batches: 0.0137
trigger times: 4
Loss after 446515980 batches: 0.0136
trigger times: 5
Loss after 446618580 batches: 0.0139
trigger times: 0
Loss after 446721180 batches: 0.0139
trigger times: 1
Loss after 446823780 batches: 0.0140
trigger times: 0
Loss after 446926380 batches: 0.0138
trigger times: 1
Loss after 447028980 batches: 0.0150
trigger times: 2
Loss after 447131580 batches: 0.0147
trigger times: 3
Loss after 447234180 batches: 0.0160
trigger times: 4
Loss after 447336780 batches: 0.0146
trigger times: 5
Loss after 447439380 batches: 0.0145
trigger times: 6
Loss after 447541980 batches: 0.0144
trigger times: 7
Loss after 447644580 batches: 0.0146
trigger times: 0
Loss after 447747180 batches: 0.0145
trigger times: 1
Loss after 447849780 batches: 0.0182
trigger times: 2
Loss after 447952380 batches: 0.0145
trigger times: 3
Loss after 448054980 batches: 0.0142
trigger times: 4
Loss after 448157580 batches: 0.0158
trigger times: 5
Loss after 448260180 batches: 0.0138
trigger times: 0
Loss after 448362780 batches: 0.0134
trigger times: 1
Loss after 448465380 batches: 0.0136
trigger times: 2
Loss after 448567980 batches: 0.0137
trigger times: 3
Loss after 448670580 batches: 0.0140
trigger times: 4
Loss after 448773180 batches: 0.0136
trigger times: 5
Loss after 448875780 batches: 0.0132
trigger times: 6
Loss after 448978380 batches: 0.0138
trigger times: 7
Loss after 449080980 batches: 0.0133
trigger times: 8
Loss after 449183580 batches: 0.0140
trigger times: 9
Loss after 449286180 batches: 0.0138
trigger times: 10
Loss after 449388780 batches: 0.0134
trigger times: 11
Loss after 449491380 batches: 0.0137
trigger times: 12
Loss after 449593980 batches: 0.0127
trigger times: 13
Loss after 449696580 batches: 0.0131
trigger times: 14
Loss after 449799180 batches: 0.0137
trigger times: 15
Loss after 449901780 batches: 0.0130
trigger times: 16
Loss after 450004380 batches: 0.0139
trigger times: 0
Loss after 450106980 batches: 0.0144
trigger times: 1
Loss after 450209580 batches: 0.0132
trigger times: 2
Loss after 450312180 batches: 0.0126
trigger times: 3
Loss after 450414780 batches: 0.0132
trigger times: 4
Loss after 450517380 batches: 0.0136
trigger times: 5
Loss after 450619980 batches: 0.0139
trigger times: 6
Loss after 450722580 batches: 0.0130
trigger times: 7
Loss after 450825180 batches: 0.0132
trigger times: 8
Loss after 450927780 batches: 0.0130
trigger times: 9
Loss after 451030380 batches: 0.0132
trigger times: 10
Loss after 451132980 batches: 0.0142
trigger times: 11
Loss after 451235580 batches: 0.0157
trigger times: 12
Loss after 451338180 batches: 0.0133
trigger times: 13
Loss after 451440780 batches: 0.0130
trigger times: 14
Loss after 451543380 batches: 0.0128
trigger times: 15
Loss after 451645980 batches: 0.0128
trigger times: 16
Loss after 451748580 batches: 0.0127
trigger times: 17
Loss after 451851180 batches: 0.0130
trigger times: 18
Loss after 451953780 batches: 0.0131
trigger times: 19
Loss after 452056380 batches: 0.0128
trigger times: 0
Loss after 452158980 batches: 0.0127
trigger times: 1
Loss after 452261580 batches: 0.0140
trigger times: 2
Loss after 452364180 batches: 0.0136
trigger times: 3
Loss after 452466780 batches: 0.0137
trigger times: 4
Loss after 452569380 batches: 0.0140
trigger times: 5
Loss after 452671980 batches: 0.0131
trigger times: 6
Loss after 452774580 batches: 0.0142
trigger times: 7
Loss after 452877180 batches: 0.0127
trigger times: 8
Loss after 452979780 batches: 0.0130
trigger times: 9
Loss after 453082380 batches: 0.0131
trigger times: 10
Loss after 453184980 batches: 0.0138
trigger times: 11
Loss after 453287580 batches: 0.0135
trigger times: 12
Loss after 453390180 batches: 0.0149
trigger times: 0
Loss after 453492780 batches: 0.0136
trigger times: 1
Loss after 453595380 batches: 0.0142
trigger times: 2
Loss after 453697980 batches: 0.0126
trigger times: 3
Loss after 453800580 batches: 0.0128
trigger times: 4
Loss after 453903180 batches: 0.0132
trigger times: 5
Loss after 454005780 batches: 0.0130
trigger times: 6
Loss after 454108380 batches: 0.0127
trigger times: 7
Loss after 454210980 batches: 0.0127
trigger times: 0
Loss after 454313580 batches: 0.0129
trigger times: 1
Loss after 454416180 batches: 0.0153
trigger times: 2
Loss after 454518780 batches: 0.0130
trigger times: 3
Loss after 454621380 batches: 0.0123
trigger times: 4
Loss after 454723980 batches: 0.0121
trigger times: 5
Loss after 454826580 batches: 0.0120
trigger times: 6
Loss after 454929180 batches: 0.0118
trigger times: 7
Loss after 455031780 batches: 0.0118
trigger times: 8
Loss after 455134380 batches: 0.0117
trigger times: 9
Loss after 455236980 batches: 0.0130
trigger times: 10
Loss after 455339580 batches: 0.0118
trigger times: 11
Loss after 455442180 batches: 0.0121
trigger times: 12
Loss after 455544780 batches: 0.0122
trigger times: 13
Loss after 455647380 batches: 0.0120
trigger times: 14
Loss after 455749980 batches: 0.0123
trigger times: 15
Loss after 455852580 batches: 0.0136
trigger times: 16
Loss after 455955180 batches: 0.0119
trigger times: 17
Loss after 456057780 batches: 0.0123
trigger times: 18
Loss after 456160380 batches: 0.0121
trigger times: 19
Loss after 456262980 batches: 0.0115
trigger times: 20
Early stopping!
Start to test process.
Loss after 456365580 batches: 0.0118
Time to train on one home:  1029.4937343597412
trigger times: 0
Loss after 456496680 batches: 0.1938
trigger times: 0
Loss after 456627780 batches: 0.0552
trigger times: 0
Loss after 456758880 batches: 0.0387
trigger times: 1
Loss after 456889980 batches: 0.0329
trigger times: 0
Loss after 457021080 batches: 0.0296
trigger times: 0
Loss after 457152180 batches: 0.0275
trigger times: 1
Loss after 457283280 batches: 0.0260
trigger times: 2
Loss after 457414380 batches: 0.0244
trigger times: 3
Loss after 457545480 batches: 0.0238
trigger times: 4
Loss after 457676580 batches: 0.0230
trigger times: 0
Loss after 457807680 batches: 0.0222
trigger times: 0
Loss after 457938780 batches: 0.0218
trigger times: 1
Loss after 458069880 batches: 0.0208
trigger times: 2
Loss after 458200980 batches: 0.0205
trigger times: 3
Loss after 458332080 batches: 0.0203
trigger times: 4
Loss after 458463180 batches: 0.0199
trigger times: 5
Loss after 458594280 batches: 0.0193
trigger times: 6
Loss after 458725380 batches: 0.0190
trigger times: 7
Loss after 458856480 batches: 0.0190
trigger times: 8
Loss after 458987580 batches: 0.0185
trigger times: 9
Loss after 459118680 batches: 0.0181
trigger times: 10
Loss after 459249780 batches: 0.0182
trigger times: 11
Loss after 459380880 batches: 0.0178
trigger times: 12
Loss after 459511980 batches: 0.0175
trigger times: 13
Loss after 459643080 batches: 0.0174
trigger times: 14
Loss after 459774180 batches: 0.0171
trigger times: 15
Loss after 459905280 batches: 0.0170
trigger times: 16
Loss after 460036380 batches: 0.0168
trigger times: 17
Loss after 460167480 batches: 0.0168
trigger times: 18
Loss after 460298580 batches: 0.0166
trigger times: 19
Loss after 460429680 batches: 0.0164
trigger times: 20
Early stopping!
Start to test process.
Loss after 460560780 batches: 0.0163
Time to train on one home:  238.11657238006592
trigger times: 0
Loss after 460691880 batches: 0.2354
trigger times: 0
Loss after 460822980 batches: 0.0722
trigger times: 0
Loss after 460954080 batches: 0.0547
trigger times: 1
Loss after 461085180 batches: 0.0471
trigger times: 2
Loss after 461216280 batches: 0.0429
trigger times: 3
Loss after 461347380 batches: 0.0407
trigger times: 4
Loss after 461478480 batches: 0.0377
trigger times: 5
Loss after 461609580 batches: 0.0362
trigger times: 6
Loss after 461740680 batches: 0.0349
trigger times: 7
Loss after 461871780 batches: 0.0334
trigger times: 8
Loss after 462002880 batches: 0.0327
trigger times: 9
Loss after 462133980 batches: 0.0318
trigger times: 10
Loss after 462265080 batches: 0.0313
trigger times: 11
Loss after 462396180 batches: 0.0300
trigger times: 12
Loss after 462527280 batches: 0.0297
trigger times: 13
Loss after 462658380 batches: 0.0293
trigger times: 14
Loss after 462789480 batches: 0.0285
trigger times: 15
Loss after 462920580 batches: 0.0280
trigger times: 16
Loss after 463051680 batches: 0.0279
trigger times: 17
Loss after 463182780 batches: 0.0270
trigger times: 18
Loss after 463313880 batches: 0.0270
trigger times: 19
Loss after 463444980 batches: 0.0265
trigger times: 20
Early stopping!
Start to test process.
Loss after 463576080 batches: 0.0262
Time to train on one home:  174.70649909973145
trigger times: 0
Loss after 463704720 batches: 0.1045
trigger times: 1
Loss after 463833360 batches: 0.0301
trigger times: 0
Loss after 463962000 batches: 0.0235
trigger times: 0
Loss after 464090640 batches: 0.0206
trigger times: 1
Loss after 464219280 batches: 0.0192
trigger times: 2
Loss after 464347920 batches: 0.0180
trigger times: 3
Loss after 464476560 batches: 0.0175
trigger times: 4
Loss after 464605200 batches: 0.0168
trigger times: 0
Loss after 464733840 batches: 0.0161
trigger times: 1
Loss after 464862480 batches: 0.0158
trigger times: 2
Loss after 464991120 batches: 0.0152
trigger times: 3
Loss after 465119760 batches: 0.0150
trigger times: 4
Loss after 465248400 batches: 0.0145
trigger times: 5
Loss after 465377040 batches: 0.0142
trigger times: 6
Loss after 465505680 batches: 0.0143
trigger times: 7
Loss after 465634320 batches: 0.0140
trigger times: 8
Loss after 465762960 batches: 0.0135
trigger times: 9
Loss after 465891600 batches: 0.0134
trigger times: 10
Loss after 466020240 batches: 0.0131
trigger times: 11
Loss after 466148880 batches: 0.0128
trigger times: 12
Loss after 466277520 batches: 0.0130
trigger times: 13
Loss after 466406160 batches: 0.0128
trigger times: 14
Loss after 466534800 batches: 0.0126
trigger times: 15
Loss after 466663440 batches: 0.0124
trigger times: 16
Loss after 466792080 batches: 0.0122
trigger times: 17
Loss after 466920720 batches: 0.0124
trigger times: 18
Loss after 467049360 batches: 0.0122
trigger times: 19
Loss after 467178000 batches: 0.0122
trigger times: 20
Early stopping!
Start to test process.
Loss after 467306640 batches: 0.0119
Time to train on one home:  213.5753800868988
trigger times: 0
Loss after 467437740 batches: 0.1942
trigger times: 0
Loss after 467568840 batches: 0.0546
trigger times: 0
Loss after 467699940 batches: 0.0415
trigger times: 0
Loss after 467831040 batches: 0.0359
trigger times: 0
Loss after 467962140 batches: 0.0324
trigger times: 1
Loss after 468093240 batches: 0.0311
trigger times: 2
Loss after 468224340 batches: 0.0295
trigger times: 3
Loss after 468355440 batches: 0.0286
trigger times: 4
Loss after 468486540 batches: 0.0273
trigger times: 5
Loss after 468617640 batches: 0.0267
trigger times: 6
Loss after 468748740 batches: 0.0264
trigger times: 7
Loss after 468879840 batches: 0.0254
trigger times: 8
Loss after 469010940 batches: 0.0249
trigger times: 9
Loss after 469142040 batches: 0.0245
trigger times: 10
Loss after 469273140 batches: 0.0240
trigger times: 0
Loss after 469404240 batches: 0.0237
trigger times: 1
Loss after 469535340 batches: 0.0232
trigger times: 2
Loss after 469666440 batches: 0.0231
trigger times: 3
Loss after 469797540 batches: 0.0227
trigger times: 4
Loss after 469928640 batches: 0.0227
trigger times: 5
Loss after 470059740 batches: 0.0222
trigger times: 6
Loss after 470190840 batches: 0.0221
trigger times: 7
Loss after 470321940 batches: 0.0216
trigger times: 8
Loss after 470453040 batches: 0.0214
trigger times: 9
Loss after 470584140 batches: 0.0215
trigger times: 10
Loss after 470715240 batches: 0.0211
trigger times: 11
Loss after 470846340 batches: 0.0209
trigger times: 12
Loss after 470977440 batches: 0.0206
trigger times: 13
Loss after 471108540 batches: 0.0208
trigger times: 14
Loss after 471239640 batches: 0.0205
trigger times: 15
Loss after 471370740 batches: 0.0202
trigger times: 16
Loss after 471501840 batches: 0.0202
trigger times: 17
Loss after 471632940 batches: 0.0202
trigger times: 18
Loss after 471764040 batches: 0.0199
trigger times: 19
Loss after 471895140 batches: 0.0200
trigger times: 20
Early stopping!
Start to test process.
Loss after 472026240 batches: 0.0197
Time to train on one home:  266.5103099346161
trigger times: 0
Loss after 472157340 batches: 0.1892
trigger times: 0
Loss after 472288440 batches: 0.0581
trigger times: 0
Loss after 472419540 batches: 0.0413
trigger times: 0
Loss after 472550640 batches: 0.0349
trigger times: 1
Loss after 472681740 batches: 0.0316
trigger times: 2
Loss after 472812840 batches: 0.0299
trigger times: 3
Loss after 472943940 batches: 0.0296
trigger times: 0
Loss after 473075040 batches: 0.0273
trigger times: 0
Loss after 473206140 batches: 0.0268
trigger times: 0
Loss after 473337240 batches: 0.0260
trigger times: 1
Loss after 473468340 batches: 0.0249
trigger times: 0
Loss after 473599440 batches: 0.0245
trigger times: 1
Loss after 473730540 batches: 0.0240
trigger times: 0
Loss after 473861640 batches: 0.0240
trigger times: 1
Loss after 473992740 batches: 0.0234
trigger times: 0
Loss after 474123840 batches: 0.0232
trigger times: 1
Loss after 474254940 batches: 0.0229
trigger times: 2
Loss after 474386040 batches: 0.0227
trigger times: 3
Loss after 474517140 batches: 0.0231
trigger times: 4
Loss after 474648240 batches: 0.0233
trigger times: 5
Loss after 474779340 batches: 0.0233
trigger times: 6
Loss after 474910440 batches: 0.0217
trigger times: 7
Loss after 475041540 batches: 0.0224
trigger times: 8
Loss after 475172640 batches: 0.0216
trigger times: 9
Loss after 475303740 batches: 0.0208
trigger times: 10
Loss after 475434840 batches: 0.0216
trigger times: 11
Loss after 475565940 batches: 0.0210
trigger times: 12
Loss after 475697040 batches: 0.0206
trigger times: 13
Loss after 475828140 batches: 0.0201
trigger times: 0
Loss after 475959240 batches: 0.0195
trigger times: 1
Loss after 476090340 batches: 0.0203
trigger times: 0
Loss after 476221440 batches: 0.0200
trigger times: 1
Loss after 476352540 batches: 0.0206
trigger times: 0
Loss after 476483640 batches: 0.0202
trigger times: 1
Loss after 476614740 batches: 0.0204
trigger times: 2
Loss after 476745840 batches: 0.0206
trigger times: 3
Loss after 476876940 batches: 0.0203
trigger times: 4
Loss after 477008040 batches: 0.0203
trigger times: 5
Loss after 477139140 batches: 0.0204
trigger times: 6
Loss after 477270240 batches: 0.0198
trigger times: 7
Loss after 477401340 batches: 0.0201
trigger times: 8
Loss after 477532440 batches: 0.0199
trigger times: 9
Loss after 477663540 batches: 0.0189
trigger times: 10
Loss after 477794640 batches: 0.0191
trigger times: 11
Loss after 477925740 batches: 0.0206
trigger times: 12
Loss after 478056840 batches: 0.0199
trigger times: 13
Loss after 478187940 batches: 0.0190
trigger times: 14
Loss after 478319040 batches: 0.0194
trigger times: 15
Loss after 478450140 batches: 0.0198
trigger times: 16
Loss after 478581240 batches: 0.0184
trigger times: 17
Loss after 478712340 batches: 0.0189
trigger times: 18
Loss after 478843440 batches: 0.0192
trigger times: 19
Loss after 478974540 batches: 0.0188
trigger times: 20
Early stopping!
Start to test process.
Loss after 479105640 batches: 0.0188
Time to train on one home:  394.694260597229
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887, 0.018152045403586472, 0.017512356438894854, 0.016091731943637777, 0.01601899988306293]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884], [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987], [0.5804732673698001, 0.3723540197761146, 0.45230155418961127, 1.0682024720651153, 0.514164189884338, 25.236666271720736, 1587.2568], [0.5693913300832113, 0.3843477905525722, 0.4653277545238437, 1.0599366084402062, 0.5043389577801884, 25.04138228090006, 1556.9258], [0.5662658578819699, 0.38771059287123977, 0.46820783457394766, 1.0579149875211409, 0.5015841683218024, 24.993620762090103, 1548.4215]]
Round_11_results:  [0.5662658578819699, 0.38771059287123977, 0.46820783457394766, 1.0579149875211409, 0.5015841683218024, 24.993620762090103, 1548.4215]
trigger times: 0
Loss after 479236740 batches: 0.0845
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 3820 < 3821; dropping {'Training_Loss': 0.08454024802260804, 'Validation_Loss': 0.22176167203320396, 'Training_R2': 0.9148758426004949, 'Validation_R2': 0.7939308725129632, 'Training_F1': 0.8416015710208746, 'Validation_F1': 0.7382564680773954, 'Training_NEP': 0.31662427982196356, 'Validation_NEP': 0.49862863725566164, 'Training_NDE': 0.06390421591594417, 'Validation_NDE': 0.16410259038011513, 'Training_MAE': 10.485968214045219, 'Validation_MAE': 13.674465917045497, 'Training_MSE': 281.16852, 'Validation_MSE': 606.02637}.
trigger times: 0
Loss after 479367840 batches: 0.0251
trigger times: 1
Loss after 479498940 batches: 0.0188
trigger times: 0
Loss after 479630040 batches: 0.0164
trigger times: 1
Loss after 479761140 batches: 0.0149
trigger times: 2
Loss after 479892240 batches: 0.0140
trigger times: 3
Loss after 480023340 batches: 0.0135
trigger times: 0
Loss after 480154440 batches: 0.0128
trigger times: 1
Loss after 480285540 batches: 0.0121
trigger times: 2
Loss after 480416640 batches: 0.0117
trigger times: 0
Loss after 480547740 batches: 0.0116
trigger times: 0
Loss after 480678840 batches: 0.0111
trigger times: 1
Loss after 480809940 batches: 0.0111
trigger times: 2
Loss after 480941040 batches: 0.0110
trigger times: 3
Loss after 481072140 batches: 0.0105
trigger times: 0
Loss after 481203240 batches: 0.0103
trigger times: 1
Loss after 481334340 batches: 0.0103
trigger times: 2
Loss after 481465440 batches: 0.0100
trigger times: 3
Loss after 481596540 batches: 0.0103
trigger times: 0
Loss after 481727640 batches: 0.0099
trigger times: 1
Loss after 481858740 batches: 0.0098
trigger times: 2
Loss after 481989840 batches: 0.0096
trigger times: 3
Loss after 482120940 batches: 0.0096
trigger times: 4
Loss after 482252040 batches: 0.0093
trigger times: 5
Loss after 482383140 batches: 0.0094
trigger times: 6
Loss after 482514240 batches: 0.0094
trigger times: 7
Loss after 482645340 batches: 0.0091
trigger times: 0
Loss after 482776440 batches: 0.0093
trigger times: 1
Loss after 482907540 batches: 0.0093
trigger times: 2
Loss after 483038640 batches: 0.0092
trigger times: 3
Loss after 483169740 batches: 0.0090
trigger times: 4
Loss after 483300840 batches: 0.0090
trigger times: 5
Loss after 483431940 batches: 0.0087
trigger times: 6
Loss after 483563040 batches: 0.0087
trigger times: 7
Loss after 483694140 batches: 0.0088
trigger times: 8
Loss after 483825240 batches: 0.0087
trigger times: 9
Loss after 483956340 batches: 0.0087
trigger times: 10
Loss after 484087440 batches: 0.0086
trigger times: 11
Loss after 484218540 batches: 0.0084
trigger times: 12
Loss after 484349640 batches: 0.0086
trigger times: 13
Loss after 484480740 batches: 0.0083
trigger times: 14
Loss after 484611840 batches: 0.0083
trigger times: 15
Loss after 484742940 batches: 0.0084
trigger times: 16
Loss after 484874040 batches: 0.0082
trigger times: 17
Loss after 485005140 batches: 0.0082
trigger times: 18
Loss after 485136240 batches: 0.0083
trigger times: 19
Loss after 485267340 batches: 0.0081
trigger times: 20
Early stopping!
Start to test process.
Loss after 485398440 batches: 0.0079
Time to train on one home:  351.38043308258057
trigger times: 0
Loss after 485501040 batches: 0.1305
trigger times: 0
Loss after 485603640 batches: 0.0614
trigger times: 0
Loss after 485706240 batches: 0.0412
trigger times: 0
Loss after 485808840 batches: 0.0314
trigger times: 0
Loss after 485911440 batches: 0.0264
trigger times: 0
Loss after 486014040 batches: 0.0265
trigger times: 1
Loss after 486116640 batches: 0.0238
trigger times: 2
Loss after 486219240 batches: 0.0229
trigger times: 3
Loss after 486321840 batches: 0.0234
trigger times: 0
Loss after 486424440 batches: 0.0236
trigger times: 0
Loss after 486527040 batches: 0.0209
trigger times: 1
Loss after 486629640 batches: 0.0202
trigger times: 2
Loss after 486732240 batches: 0.0200
trigger times: 0
Loss after 486834840 batches: 0.0203
trigger times: 1
Loss after 486937440 batches: 0.0193
trigger times: 0
Loss after 487040040 batches: 0.0203
trigger times: 1
Loss after 487142640 batches: 0.0192
trigger times: 0
Loss after 487245240 batches: 0.0239
trigger times: 1
Loss after 487347840 batches: 0.0251
trigger times: 2
Loss after 487450440 batches: 0.0189
trigger times: 3
Loss after 487553040 batches: 0.0177
trigger times: 4
Loss after 487655640 batches: 0.0173
trigger times: 5
Loss after 487758240 batches: 0.0171
trigger times: 6
Loss after 487860840 batches: 0.0172
trigger times: 7
Loss after 487963440 batches: 0.0174
trigger times: 8
Loss after 488066040 batches: 0.0168
trigger times: 9
Loss after 488168640 batches: 0.0183
trigger times: 10
Loss after 488271240 batches: 0.0177
trigger times: 11
Loss after 488373840 batches: 0.0162
trigger times: 12
Loss after 488476440 batches: 0.0159
trigger times: 13
Loss after 488579040 batches: 0.0157
trigger times: 14
Loss after 488681640 batches: 0.0154
trigger times: 15
Loss after 488784240 batches: 0.0150
trigger times: 16
Loss after 488886840 batches: 0.0147
trigger times: 17
Loss after 488989440 batches: 0.0152
trigger times: 18
Loss after 489092040 batches: 0.0153
trigger times: 19
Loss after 489194640 batches: 0.0151
trigger times: 20
Early stopping!
Start to test process.
Loss after 489297240 batches: 0.0155
Time to train on one home:  229.84824800491333
trigger times: 0
Loss after 489428340 batches: 0.1293
trigger times: 0
Loss after 489559440 batches: 0.0416
trigger times: 1
Loss after 489690540 batches: 0.0319
trigger times: 2
Loss after 489821640 batches: 0.0276
trigger times: 0
Loss after 489952740 batches: 0.0255
trigger times: 1
Loss after 490083840 batches: 0.0240
trigger times: 2
Loss after 490214940 batches: 0.0230
trigger times: 3
Loss after 490346040 batches: 0.0224
trigger times: 4
Loss after 490477140 batches: 0.0214
trigger times: 5
Loss after 490608240 batches: 0.0207
trigger times: 6
Loss after 490739340 batches: 0.0203
trigger times: 7
Loss after 490870440 batches: 0.0197
trigger times: 8
Loss after 491001540 batches: 0.0192
trigger times: 9
Loss after 491132640 batches: 0.0186
trigger times: 10
Loss after 491263740 batches: 0.0184
trigger times: 11
Loss after 491394840 batches: 0.0181
trigger times: 12
Loss after 491525940 batches: 0.0179
trigger times: 13
Loss after 491657040 batches: 0.0177
trigger times: 14
Loss after 491788140 batches: 0.0175
trigger times: 15
Loss after 491919240 batches: 0.0171
trigger times: 16
Loss after 492050340 batches: 0.0169
trigger times: 17
Loss after 492181440 batches: 0.0170
trigger times: 18
Loss after 492312540 batches: 0.0166
trigger times: 19
Loss after 492443640 batches: 0.0164
trigger times: 20
Early stopping!
Start to test process.
Loss after 492574740 batches: 0.0161
Time to train on one home:  188.26864981651306
trigger times: 0
Loss after 492705840 batches: 0.2394
trigger times: 0
Loss after 492836940 batches: 0.0704
trigger times: 1
Loss after 492968040 batches: 0.0530
trigger times: 0
Loss after 493099140 batches: 0.0465
trigger times: 1
Loss after 493230240 batches: 0.0422
trigger times: 2
Loss after 493361340 batches: 0.0398
trigger times: 3
Loss after 493492440 batches: 0.0373
trigger times: 4
Loss after 493623540 batches: 0.0361
trigger times: 5
Loss after 493754640 batches: 0.0344
trigger times: 6
Loss after 493885740 batches: 0.0334
trigger times: 7
Loss after 494016840 batches: 0.0322
trigger times: 8
Loss after 494147940 batches: 0.0315
trigger times: 9
Loss after 494279040 batches: 0.0314
trigger times: 10
Loss after 494410140 batches: 0.0302
trigger times: 11
Loss after 494541240 batches: 0.0289
trigger times: 12
Loss after 494672340 batches: 0.0286
trigger times: 13
Loss after 494803440 batches: 0.0284
trigger times: 14
Loss after 494934540 batches: 0.0279
trigger times: 15
Loss after 495065640 batches: 0.0273
trigger times: 16
Loss after 495196740 batches: 0.0268
trigger times: 17
Loss after 495327840 batches: 0.0265
trigger times: 18
Loss after 495458940 batches: 0.0262
trigger times: 19
Loss after 495590040 batches: 0.0258
trigger times: 20
Early stopping!
Start to test process.
Loss after 495721140 batches: 0.0255
Time to train on one home:  181.66136717796326
trigger times: 0
Loss after 495849780 batches: 0.1173
trigger times: 0
Loss after 495978420 batches: 0.0326
trigger times: 0
Loss after 496107060 batches: 0.0241
trigger times: 0
Loss after 496235700 batches: 0.0214
trigger times: 0
Loss after 496364340 batches: 0.0193
trigger times: 0
Loss after 496492980 batches: 0.0182
trigger times: 0
Loss after 496621620 batches: 0.0174
trigger times: 0
Loss after 496750260 batches: 0.0166
trigger times: 0
Loss after 496878900 batches: 0.0161
trigger times: 1
Loss after 497007540 batches: 0.0159
trigger times: 2
Loss after 497136180 batches: 0.0153
trigger times: 3
Loss after 497264820 batches: 0.0151
trigger times: 4
Loss after 497393460 batches: 0.0145
trigger times: 5
Loss after 497522100 batches: 0.0144
trigger times: 6
Loss after 497650740 batches: 0.0143
trigger times: 7
Loss after 497779380 batches: 0.0139
trigger times: 8
Loss after 497908020 batches: 0.0137
trigger times: 9
Loss after 498036660 batches: 0.0135
trigger times: 10
Loss after 498165300 batches: 0.0132
trigger times: 11
Loss after 498293940 batches: 0.0131
trigger times: 12
Loss after 498422580 batches: 0.0128
trigger times: 13
Loss after 498551220 batches: 0.0128
trigger times: 14
Loss after 498679860 batches: 0.0123
trigger times: 15
Loss after 498808500 batches: 0.0126
trigger times: 16
Loss after 498937140 batches: 0.0125
trigger times: 17
Loss after 499065780 batches: 0.0122
trigger times: 18
Loss after 499194420 batches: 0.0120
trigger times: 19
Loss after 499323060 batches: 0.0118
trigger times: 20
Early stopping!
Start to test process.
Loss after 499451700 batches: 0.0118
Time to train on one home:  214.7140395641327
trigger times: 0
Loss after 499582800 batches: 0.1960
trigger times: 0
Loss after 499713900 batches: 0.0532
trigger times: 0
Loss after 499845000 batches: 0.0398
trigger times: 1
Loss after 499976100 batches: 0.0355
trigger times: 2
Loss after 500107200 batches: 0.0331
trigger times: 3
Loss after 500238300 batches: 0.0309
trigger times: 4
Loss after 500369400 batches: 0.0296
trigger times: 5
Loss after 500500500 batches: 0.0283
trigger times: 6
Loss after 500631600 batches: 0.0274
trigger times: 7
Loss after 500762700 batches: 0.0266
trigger times: 0
Loss after 500893800 batches: 0.0257
trigger times: 1
Loss after 501024900 batches: 0.0251
trigger times: 2
Loss after 501156000 batches: 0.0246
trigger times: 3
Loss after 501287100 batches: 0.0244
trigger times: 4
Loss after 501418200 batches: 0.0239
trigger times: 5
Loss after 501549300 batches: 0.0239
trigger times: 0
Loss after 501680400 batches: 0.0232
trigger times: 1
Loss after 501811500 batches: 0.0229
trigger times: 2
Loss after 501942600 batches: 0.0224
trigger times: 3
Loss after 502073700 batches: 0.0223
trigger times: 4
Loss after 502204800 batches: 0.0222
trigger times: 5
Loss after 502335900 batches: 0.0218
trigger times: 6
Loss after 502467000 batches: 0.0213
trigger times: 7
Loss after 502598100 batches: 0.0214
trigger times: 8
Loss after 502729200 batches: 0.0212
trigger times: 9
Loss after 502860300 batches: 0.0212
trigger times: 10
Loss after 502991400 batches: 0.0208
trigger times: 11
Loss after 503122500 batches: 0.0205
trigger times: 12
Loss after 503253600 batches: 0.0205
trigger times: 13
Loss after 503384700 batches: 0.0205
trigger times: 14
Loss after 503515800 batches: 0.0200
trigger times: 15
Loss after 503646900 batches: 0.0203
trigger times: 16
Loss after 503778000 batches: 0.0201
trigger times: 17
Loss after 503909100 batches: 0.0201
trigger times: 18
Loss after 504040200 batches: 0.0198
trigger times: 19
Loss after 504171300 batches: 0.0196
trigger times: 0
Loss after 504302400 batches: 0.0197
trigger times: 1
Loss after 504433500 batches: 0.0194
trigger times: 2
Loss after 504564600 batches: 0.0195
trigger times: 3
Loss after 504695700 batches: 0.0193
trigger times: 4
Loss after 504826800 batches: 0.0193
trigger times: 0
Loss after 504957900 batches: 0.0188
trigger times: 1
Loss after 505089000 batches: 0.0187
trigger times: 2
Loss after 505220100 batches: 0.0187
trigger times: 3
Loss after 505351200 batches: 0.0186
trigger times: 0
Loss after 505482300 batches: 0.0182
trigger times: 1
Loss after 505613400 batches: 0.0183
trigger times: 2
Loss after 505744500 batches: 0.0185
trigger times: 3
Loss after 505875600 batches: 0.0183
trigger times: 4
Loss after 506006700 batches: 0.0182
trigger times: 5
Loss after 506137800 batches: 0.0180
trigger times: 6
Loss after 506268900 batches: 0.0179
trigger times: 7
Loss after 506400000 batches: 0.0179
trigger times: 8
Loss after 506531100 batches: 0.0176
trigger times: 9
Loss after 506662200 batches: 0.0178
trigger times: 0
Loss after 506793300 batches: 0.0177
trigger times: 1
Loss after 506924400 batches: 0.0175
trigger times: 2
Loss after 507055500 batches: 0.0174
trigger times: 3
Loss after 507186600 batches: 0.0177
trigger times: 4
Loss after 507317700 batches: 0.0176
trigger times: 5
Loss after 507448800 batches: 0.0172
trigger times: 6
Loss after 507579900 batches: 0.0171
trigger times: 7
Loss after 507711000 batches: 0.0173
trigger times: 0
Loss after 507842100 batches: 0.0172
trigger times: 1
Loss after 507973200 batches: 0.0170
trigger times: 2
Loss after 508104300 batches: 0.0169
trigger times: 3
Loss after 508235400 batches: 0.0169
trigger times: 4
Loss after 508366500 batches: 0.0169
trigger times: 5
Loss after 508497600 batches: 0.0169
trigger times: 0
Loss after 508628700 batches: 0.0169
trigger times: 0
Loss after 508759800 batches: 0.0168
trigger times: 1
Loss after 508890900 batches: 0.0165
trigger times: 2
Loss after 509022000 batches: 0.0165
trigger times: 3
Loss after 509153100 batches: 0.0164
trigger times: 4
Loss after 509284200 batches: 0.0166
trigger times: 5
Loss after 509415300 batches: 0.0163
trigger times: 6
Loss after 509546400 batches: 0.0165
trigger times: 7
Loss after 509677500 batches: 0.0161
trigger times: 8
Loss after 509808600 batches: 0.0163
trigger times: 9
Loss after 509939700 batches: 0.0163
trigger times: 10
Loss after 510070800 batches: 0.0164
trigger times: 11
Loss after 510201900 batches: 0.0161
trigger times: 12
Loss after 510333000 batches: 0.0160
trigger times: 13
Loss after 510464100 batches: 0.0160
trigger times: 14
Loss after 510595200 batches: 0.0159
trigger times: 15
Loss after 510726300 batches: 0.0160
trigger times: 16
Loss after 510857400 batches: 0.0159
trigger times: 17
Loss after 510988500 batches: 0.0159
trigger times: 18
Loss after 511119600 batches: 0.0158
trigger times: 19
Loss after 511250700 batches: 0.0159
trigger times: 20
Early stopping!
Start to test process.
Loss after 511381800 batches: 0.0158
Time to train on one home:  657.4989607334137
trigger times: 0
Loss after 511512900 batches: 0.1577
trigger times: 0
Loss after 511644000 batches: 0.0603
trigger times: 0
Loss after 511775100 batches: 0.0440
trigger times: 0
Loss after 511906200 batches: 0.0342
trigger times: 0
Loss after 512037300 batches: 0.0301
trigger times: 1
Loss after 512168400 batches: 0.0301
trigger times: 2
Loss after 512299500 batches: 0.0288
trigger times: 0
Loss after 512430600 batches: 0.0280
trigger times: 0
Loss after 512561700 batches: 0.0264
trigger times: 1
Loss after 512692800 batches: 0.0247
trigger times: 2
Loss after 512823900 batches: 0.0251
trigger times: 0
Loss after 512955000 batches: 0.0245
trigger times: 1
Loss after 513086100 batches: 0.0242
trigger times: 2
Loss after 513217200 batches: 0.0238
trigger times: 3
Loss after 513348300 batches: 0.0239
trigger times: 0
Loss after 513479400 batches: 0.0224
trigger times: 1
Loss after 513610500 batches: 0.0233
trigger times: 2
Loss after 513741600 batches: 0.0224
trigger times: 3
Loss after 513872700 batches: 0.0219
trigger times: 4
Loss after 514003800 batches: 0.0216
trigger times: 5
Loss after 514134900 batches: 0.0221
trigger times: 0
Loss after 514266000 batches: 0.0215
trigger times: 1
Loss after 514397100 batches: 0.0212
trigger times: 2
Loss after 514528200 batches: 0.0210
trigger times: 3
Loss after 514659300 batches: 0.0214
trigger times: 4
Loss after 514790400 batches: 0.0213
trigger times: 5
Loss after 514921500 batches: 0.0220
trigger times: 6
Loss after 515052600 batches: 0.0209
trigger times: 7
Loss after 515183700 batches: 0.0202
trigger times: 8
Loss after 515314800 batches: 0.0197
trigger times: 9
Loss after 515445900 batches: 0.0204
trigger times: 0
Loss after 515577000 batches: 0.0207
trigger times: 1
Loss after 515708100 batches: 0.0210
trigger times: 2
Loss after 515839200 batches: 0.0204
trigger times: 3
Loss after 515970300 batches: 0.0206
trigger times: 4
Loss after 516101400 batches: 0.0203
trigger times: 5
Loss after 516232500 batches: 0.0200
trigger times: 6
Loss after 516363600 batches: 0.0199
trigger times: 7
Loss after 516494700 batches: 0.0193
trigger times: 8
Loss after 516625800 batches: 0.0194
trigger times: 9
Loss after 516756900 batches: 0.0193
trigger times: 10
Loss after 516888000 batches: 0.0185
trigger times: 11
Loss after 517019100 batches: 0.0196
trigger times: 12
Loss after 517150200 batches: 0.0189
trigger times: 0
Loss after 517281300 batches: 0.0187
trigger times: 1
Loss after 517412400 batches: 0.0185
trigger times: 2
Loss after 517543500 batches: 0.0187
trigger times: 3
Loss after 517674600 batches: 0.0191
trigger times: 4
Loss after 517805700 batches: 0.0188
trigger times: 0
Loss after 517936800 batches: 0.0188
trigger times: 0
Loss after 518067900 batches: 0.0197
trigger times: 1
Loss after 518199000 batches: 0.0186
trigger times: 2
Loss after 518330100 batches: 0.0183
trigger times: 3
Loss after 518461200 batches: 0.0182
trigger times: 4
Loss after 518592300 batches: 0.0183
trigger times: 5
Loss after 518723400 batches: 0.0178
trigger times: 6
Loss after 518854500 batches: 0.0182
trigger times: 7
Loss after 518985600 batches: 0.0187
trigger times: 8
Loss after 519116700 batches: 0.0173
trigger times: 9
Loss after 519247800 batches: 0.0175
trigger times: 10
Loss after 519378900 batches: 0.0179
trigger times: 11
Loss after 519510000 batches: 0.0186
trigger times: 12
Loss after 519641100 batches: 0.0186
trigger times: 13
Loss after 519772200 batches: 0.0178
trigger times: 14
Loss after 519903300 batches: 0.0182
trigger times: 15
Loss after 520034400 batches: 0.0192
trigger times: 16
Loss after 520165500 batches: 0.0170
trigger times: 17
Loss after 520296600 batches: 0.0179
trigger times: 18
Loss after 520427700 batches: 0.0172
trigger times: 19
Loss after 520558800 batches: 0.0184
trigger times: 20
Early stopping!
Start to test process.
Loss after 520689900 batches: 0.0172
Time to train on one home:  516.3542585372925
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887, 0.018152045403586472, 0.017512356438894854, 0.016091731943637777, 0.01601899988306293, 0.015690088165884984]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884], [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987], [0.5804732673698001, 0.3723540197761146, 0.45230155418961127, 1.0682024720651153, 0.514164189884338, 25.236666271720736, 1587.2568], [0.5693913300832113, 0.3843477905525722, 0.4653277545238437, 1.0599366084402062, 0.5043389577801884, 25.04138228090006, 1556.9258], [0.5662658578819699, 0.38771059287123977, 0.46820783457394766, 1.0579149875211409, 0.5015841683218024, 24.993620762090103, 1548.4215], [0.5593101216687096, 0.3952721834915385, 0.47154332581377506, 1.051822130314305, 0.49538975421253684, 24.84967482675361, 1529.299]]
Round_12_results:  [0.5593101216687096, 0.3952721834915385, 0.47154332581377506, 1.051822130314305, 0.49538975421253684, 24.84967482675361, 1529.299]
trigger times: 0
Loss after 520821000 batches: 0.1078
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4146 < 4147; dropping {'Training_Loss': 0.10781999765280283, 'Validation_Loss': 0.2591360757748286, 'Training_R2': 0.8913428512660461, 'Validation_R2': 0.759456656157803, 'Training_F1': 0.818661076223017, 'Validation_F1': 0.716383085253863, 'Training_NEP': 0.36303846435583503, 'Validation_NEP': 0.5259984669410568, 'Training_NDE': 0.0815708502219583, 'Validation_NDE': 0.19155603900774718, 'Training_MAE': 12.023113956553258, 'Validation_MAE': 14.425060197486673, 'Training_MSE': 358.8989, 'Validation_MSE': 707.41113}.
trigger times: 0
Loss after 520952100 batches: 0.0254
trigger times: 0
Loss after 521083200 batches: 0.0185
trigger times: 1
Loss after 521214300 batches: 0.0163
trigger times: 0
Loss after 521345400 batches: 0.0148
trigger times: 1
Loss after 521476500 batches: 0.0141
trigger times: 2
Loss after 521607600 batches: 0.0135
trigger times: 3
Loss after 521738700 batches: 0.0129
trigger times: 4
Loss after 521869800 batches: 0.0122
trigger times: 5
Loss after 522000900 batches: 0.0120
trigger times: 6
Loss after 522132000 batches: 0.0119
trigger times: 7
Loss after 522263100 batches: 0.0114
trigger times: 8
Loss after 522394200 batches: 0.0113
trigger times: 9
Loss after 522525300 batches: 0.0110
trigger times: 10
Loss after 522656400 batches: 0.0108
trigger times: 0
Loss after 522787500 batches: 0.0106
trigger times: 1
Loss after 522918600 batches: 0.0104
trigger times: 2
Loss after 523049700 batches: 0.0103
trigger times: 3
Loss after 523180800 batches: 0.0103
trigger times: 4
Loss after 523311900 batches: 0.0100
trigger times: 5
Loss after 523443000 batches: 0.0099
trigger times: 6
Loss after 523574100 batches: 0.0097
trigger times: 0
Loss after 523705200 batches: 0.0095
trigger times: 1
Loss after 523836300 batches: 0.0095
trigger times: 2
Loss after 523967400 batches: 0.0096
trigger times: 3
Loss after 524098500 batches: 0.0095
trigger times: 4
Loss after 524229600 batches: 0.0093
trigger times: 5
Loss after 524360700 batches: 0.0091
trigger times: 6
Loss after 524491800 batches: 0.0093
trigger times: 7
Loss after 524622900 batches: 0.0092
trigger times: 8
Loss after 524754000 batches: 0.0090
trigger times: 9
Loss after 524885100 batches: 0.0092
trigger times: 10
Loss after 525016200 batches: 0.0089
trigger times: 11
Loss after 525147300 batches: 0.0088
trigger times: 12
Loss after 525278400 batches: 0.0089
trigger times: 13
Loss after 525409500 batches: 0.0087
trigger times: 14
Loss after 525540600 batches: 0.0088
trigger times: 15
Loss after 525671700 batches: 0.0087
trigger times: 16
Loss after 525802800 batches: 0.0088
trigger times: 17
Loss after 525933900 batches: 0.0088
trigger times: 18
Loss after 526065000 batches: 0.0086
trigger times: 19
Loss after 526196100 batches: 0.0085
trigger times: 20
Early stopping!
Start to test process.
Loss after 526327200 batches: 0.0084
Time to train on one home:  317.17352414131165
trigger times: 0
Loss after 526429800 batches: 0.1588
trigger times: 0
Loss after 526532400 batches: 0.0685
trigger times: 0
Loss after 526635000 batches: 0.0384
trigger times: 0
Loss after 526737600 batches: 0.0311
trigger times: 1
Loss after 526840200 batches: 0.0274
trigger times: 2
Loss after 526942800 batches: 0.0249
trigger times: 0
Loss after 527045400 batches: 0.0241
trigger times: 1
Loss after 527148000 batches: 0.0224
trigger times: 2
Loss after 527250600 batches: 0.0222
trigger times: 0
Loss after 527353200 batches: 0.0215
trigger times: 1
Loss after 527455800 batches: 0.0209
trigger times: 2
Loss after 527558400 batches: 0.0207
trigger times: 3
Loss after 527661000 batches: 0.0201
trigger times: 0
Loss after 527763600 batches: 0.0200
trigger times: 0
Loss after 527866200 batches: 0.0197
trigger times: 1
Loss after 527968800 batches: 0.0199
trigger times: 2
Loss after 528071400 batches: 0.0188
trigger times: 3
Loss after 528174000 batches: 0.0186
trigger times: 0
Loss after 528276600 batches: 0.0193
trigger times: 1
Loss after 528379200 batches: 0.0187
trigger times: 2
Loss after 528481800 batches: 0.0192
trigger times: 3
Loss after 528584400 batches: 0.0184
trigger times: 0
Loss after 528687000 batches: 0.0174
trigger times: 1
Loss after 528789600 batches: 0.0175
trigger times: 2
Loss after 528892200 batches: 0.0179
trigger times: 0
Loss after 528994800 batches: 0.0206
trigger times: 1
Loss after 529097400 batches: 0.0190
trigger times: 2
Loss after 529200000 batches: 0.0174
trigger times: 3
Loss after 529302600 batches: 0.0169
trigger times: 4
Loss after 529405200 batches: 0.0200
trigger times: 0
Loss after 529507800 batches: 0.0215
trigger times: 1
Loss after 529610400 batches: 0.0165
trigger times: 2
Loss after 529713000 batches: 0.0166
trigger times: 3
Loss after 529815600 batches: 0.0161
trigger times: 4
Loss after 529918200 batches: 0.0157
trigger times: 5
Loss after 530020800 batches: 0.0158
trigger times: 6
Loss after 530123400 batches: 0.0159
trigger times: 7
Loss after 530226000 batches: 0.0160
trigger times: 8
Loss after 530328600 batches: 0.0165
trigger times: 9
Loss after 530431200 batches: 0.0165
trigger times: 0
Loss after 530533800 batches: 0.0154
trigger times: 1
Loss after 530636400 batches: 0.0152
trigger times: 2
Loss after 530739000 batches: 0.0152
trigger times: 3
Loss after 530841600 batches: 0.0170
trigger times: 4
Loss after 530944200 batches: 0.0164
trigger times: 5
Loss after 531046800 batches: 0.0163
trigger times: 6
Loss after 531149400 batches: 0.0162
trigger times: 7
Loss after 531252000 batches: 0.0162
trigger times: 8
Loss after 531354600 batches: 0.0172
trigger times: 9
Loss after 531457200 batches: 0.0153
trigger times: 10
Loss after 531559800 batches: 0.0148
trigger times: 11
Loss after 531662400 batches: 0.0146
trigger times: 12
Loss after 531765000 batches: 0.0154
trigger times: 13
Loss after 531867600 batches: 0.0155
trigger times: 14
Loss after 531970200 batches: 0.0179
trigger times: 15
Loss after 532072800 batches: 0.0161
trigger times: 16
Loss after 532175400 batches: 0.0160
trigger times: 17
Loss after 532278000 batches: 0.0151
trigger times: 18
Loss after 532380600 batches: 0.0141
trigger times: 19
Loss after 532483200 batches: 0.0143
trigger times: 20
Early stopping!
Start to test process.
Loss after 532585800 batches: 0.0144
Time to train on one home:  362.8677384853363
trigger times: 0
Loss after 532716900 batches: 0.1155
trigger times: 1
Loss after 532848000 batches: 0.0397
trigger times: 2
Loss after 532979100 batches: 0.0307
trigger times: 0
Loss after 533110200 batches: 0.0272
trigger times: 1
Loss after 533241300 batches: 0.0251
trigger times: 2
Loss after 533372400 batches: 0.0236
trigger times: 3
Loss after 533503500 batches: 0.0226
trigger times: 4
Loss after 533634600 batches: 0.0215
trigger times: 0
Loss after 533765700 batches: 0.0210
trigger times: 0
Loss after 533896800 batches: 0.0203
trigger times: 1
Loss after 534027900 batches: 0.0199
trigger times: 2
Loss after 534159000 batches: 0.0197
trigger times: 3
Loss after 534290100 batches: 0.0191
trigger times: 0
Loss after 534421200 batches: 0.0186
trigger times: 1
Loss after 534552300 batches: 0.0181
trigger times: 0
Loss after 534683400 batches: 0.0181
trigger times: 1
Loss after 534814500 batches: 0.0175
trigger times: 2
Loss after 534945600 batches: 0.0175
trigger times: 3
Loss after 535076700 batches: 0.0171
trigger times: 4
Loss after 535207800 batches: 0.0171
trigger times: 5
Loss after 535338900 batches: 0.0166
trigger times: 6
Loss after 535470000 batches: 0.0163
trigger times: 7
Loss after 535601100 batches: 0.0163
trigger times: 8
Loss after 535732200 batches: 0.0162
trigger times: 9
Loss after 535863300 batches: 0.0162
trigger times: 10
Loss after 535994400 batches: 0.0160
trigger times: 11
Loss after 536125500 batches: 0.0156
trigger times: 12
Loss after 536256600 batches: 0.0156
trigger times: 13
Loss after 536387700 batches: 0.0155
trigger times: 14
Loss after 536518800 batches: 0.0156
trigger times: 15
Loss after 536649900 batches: 0.0151
trigger times: 16
Loss after 536781000 batches: 0.0150
trigger times: 17
Loss after 536912100 batches: 0.0150
trigger times: 18
Loss after 537043200 batches: 0.0149
trigger times: 19
Loss after 537174300 batches: 0.0150
trigger times: 20
Early stopping!
Start to test process.
Loss after 537305400 batches: 0.0146
Time to train on one home:  266.26967000961304
trigger times: 0
Loss after 537436500 batches: 0.2367
trigger times: 0
Loss after 537567600 batches: 0.0674
trigger times: 0
Loss after 537698700 batches: 0.0516
trigger times: 1
Loss after 537829800 batches: 0.0456
trigger times: 0
Loss after 537960900 batches: 0.0414
trigger times: 0
Loss after 538092000 batches: 0.0389
trigger times: 1
Loss after 538223100 batches: 0.0365
trigger times: 0
Loss after 538354200 batches: 0.0346
trigger times: 1
Loss after 538485300 batches: 0.0334
trigger times: 2
Loss after 538616400 batches: 0.0321
trigger times: 3
Loss after 538747500 batches: 0.0316
trigger times: 4
Loss after 538878600 batches: 0.0312
trigger times: 5
Loss after 539009700 batches: 0.0304
trigger times: 6
Loss after 539140800 batches: 0.0290
trigger times: 7
Loss after 539271900 batches: 0.0285
trigger times: 8
Loss after 539403000 batches: 0.0281
trigger times: 0
Loss after 539534100 batches: 0.0281
trigger times: 1
Loss after 539665200 batches: 0.0271
trigger times: 2
Loss after 539796300 batches: 0.0269
trigger times: 3
Loss after 539927400 batches: 0.0264
trigger times: 4
Loss after 540058500 batches: 0.0260
trigger times: 5
Loss after 540189600 batches: 0.0255
trigger times: 6
Loss after 540320700 batches: 0.0254
trigger times: 7
Loss after 540451800 batches: 0.0251
trigger times: 8
Loss after 540582900 batches: 0.0251
trigger times: 9
Loss after 540714000 batches: 0.0248
trigger times: 10
Loss after 540845100 batches: 0.0243
trigger times: 11
Loss after 540976200 batches: 0.0241
trigger times: 12
Loss after 541107300 batches: 0.0240
trigger times: 13
Loss after 541238400 batches: 0.0236
trigger times: 14
Loss after 541369500 batches: 0.0236
trigger times: 15
Loss after 541500600 batches: 0.0232
trigger times: 16
Loss after 541631700 batches: 0.0231
trigger times: 17
Loss after 541762800 batches: 0.0231
trigger times: 18
Loss after 541893900 batches: 0.0228
trigger times: 19
Loss after 542025000 batches: 0.0225
trigger times: 20
Early stopping!
Start to test process.
Loss after 542156100 batches: 0.0225
Time to train on one home:  273.7620165348053
trigger times: 0
Loss after 542284740 batches: 0.1013
trigger times: 0
Loss after 542413380 batches: 0.0287
trigger times: 0
Loss after 542542020 batches: 0.0226
trigger times: 0
Loss after 542670660 batches: 0.0198
trigger times: 0
Loss after 542799300 batches: 0.0188
trigger times: 1
Loss after 542927940 batches: 0.0174
trigger times: 0
Loss after 543056580 batches: 0.0170
trigger times: 0
Loss after 543185220 batches: 0.0165
trigger times: 1
Loss after 543313860 batches: 0.0158
trigger times: 2
Loss after 543442500 batches: 0.0153
trigger times: 3
Loss after 543571140 batches: 0.0149
trigger times: 4
Loss after 543699780 batches: 0.0147
trigger times: 5
Loss after 543828420 batches: 0.0141
trigger times: 6
Loss after 543957060 batches: 0.0141
trigger times: 7
Loss after 544085700 batches: 0.0137
trigger times: 0
Loss after 544214340 batches: 0.0135
trigger times: 1
Loss after 544342980 batches: 0.0133
trigger times: 2
Loss after 544471620 batches: 0.0130
trigger times: 3
Loss after 544600260 batches: 0.0133
trigger times: 4
Loss after 544728900 batches: 0.0131
trigger times: 5
Loss after 544857540 batches: 0.0125
trigger times: 6
Loss after 544986180 batches: 0.0125
trigger times: 7
Loss after 545114820 batches: 0.0123
trigger times: 8
Loss after 545243460 batches: 0.0124
trigger times: 9
Loss after 545372100 batches: 0.0120
trigger times: 10
Loss after 545500740 batches: 0.0121
trigger times: 11
Loss after 545629380 batches: 0.0121
trigger times: 12
Loss after 545758020 batches: 0.0117
trigger times: 13
Loss after 545886660 batches: 0.0117
trigger times: 14
Loss after 546015300 batches: 0.0118
trigger times: 15
Loss after 546143940 batches: 0.0115
trigger times: 16
Loss after 546272580 batches: 0.0115
trigger times: 17
Loss after 546401220 batches: 0.0115
trigger times: 18
Loss after 546529860 batches: 0.0112
trigger times: 19
Loss after 546658500 batches: 0.0112
trigger times: 20
Early stopping!
Start to test process.
Loss after 546787140 batches: 0.0112
Time to train on one home:  262.5428376197815
trigger times: 0
Loss after 546918240 batches: 0.1830
trigger times: 0
Loss after 547049340 batches: 0.0489
trigger times: 0
Loss after 547180440 batches: 0.0371
trigger times: 0
Loss after 547311540 batches: 0.0327
trigger times: 0
Loss after 547442640 batches: 0.0300
trigger times: 0
Loss after 547573740 batches: 0.0284
trigger times: 1
Loss after 547704840 batches: 0.0276
trigger times: 0
Loss after 547835940 batches: 0.0261
trigger times: 0
Loss after 547967040 batches: 0.0254
trigger times: 1
Loss after 548098140 batches: 0.0247
trigger times: 2
Loss after 548229240 batches: 0.0241
trigger times: 3
Loss after 548360340 batches: 0.0237
trigger times: 4
Loss after 548491440 batches: 0.0232
trigger times: 5
Loss after 548622540 batches: 0.0227
trigger times: 6
Loss after 548753640 batches: 0.0226
trigger times: 7
Loss after 548884740 batches: 0.0225
trigger times: 0
Loss after 549015840 batches: 0.0218
trigger times: 0
Loss after 549146940 batches: 0.0216
trigger times: 1
Loss after 549278040 batches: 0.0212
trigger times: 2
Loss after 549409140 batches: 0.0210
trigger times: 3
Loss after 549540240 batches: 0.0206
trigger times: 4
Loss after 549671340 batches: 0.0205
trigger times: 5
Loss after 549802440 batches: 0.0204
trigger times: 6
Loss after 549933540 batches: 0.0200
trigger times: 7
Loss after 550064640 batches: 0.0199
trigger times: 8
Loss after 550195740 batches: 0.0198
trigger times: 9
Loss after 550326840 batches: 0.0195
trigger times: 10
Loss after 550457940 batches: 0.0195
trigger times: 11
Loss after 550589040 batches: 0.0194
trigger times: 12
Loss after 550720140 batches: 0.0191
trigger times: 13
Loss after 550851240 batches: 0.0191
trigger times: 14
Loss after 550982340 batches: 0.0189
trigger times: 0
Loss after 551113440 batches: 0.0188
trigger times: 1
Loss after 551244540 batches: 0.0187
trigger times: 0
Loss after 551375640 batches: 0.0187
trigger times: 1
Loss after 551506740 batches: 0.0185
trigger times: 2
Loss after 551637840 batches: 0.0186
trigger times: 3
Loss after 551768940 batches: 0.0184
trigger times: 4
Loss after 551900040 batches: 0.0184
trigger times: 5
Loss after 552031140 batches: 0.0182
trigger times: 6
Loss after 552162240 batches: 0.0180
trigger times: 7
Loss after 552293340 batches: 0.0176
trigger times: 8
Loss after 552424440 batches: 0.0176
trigger times: 9
Loss after 552555540 batches: 0.0176
trigger times: 10
Loss after 552686640 batches: 0.0177
trigger times: 11
Loss after 552817740 batches: 0.0176
trigger times: 12
Loss after 552948840 batches: 0.0177
trigger times: 13
Loss after 553079940 batches: 0.0172
trigger times: 14
Loss after 553211040 batches: 0.0171
trigger times: 15
Loss after 553342140 batches: 0.0173
trigger times: 16
Loss after 553473240 batches: 0.0170
trigger times: 17
Loss after 553604340 batches: 0.0172
trigger times: 18
Loss after 553735440 batches: 0.0169
trigger times: 19
Loss after 553866540 batches: 0.0168
trigger times: 20
Early stopping!
Start to test process.
Loss after 553997640 batches: 0.0169
Time to train on one home:  400.6784167289734
trigger times: 0
Loss after 554128740 batches: 0.1421
trigger times: 0
Loss after 554259840 batches: 0.0556
trigger times: 0
Loss after 554390940 batches: 0.0393
trigger times: 0
Loss after 554522040 batches: 0.0338
trigger times: 1
Loss after 554653140 batches: 0.0309
trigger times: 2
Loss after 554784240 batches: 0.0286
trigger times: 3
Loss after 554915340 batches: 0.0265
trigger times: 0
Loss after 555046440 batches: 0.0269
trigger times: 1
Loss after 555177540 batches: 0.0261
trigger times: 2
Loss after 555308640 batches: 0.0251
trigger times: 3
Loss after 555439740 batches: 0.0254
trigger times: 4
Loss after 555570840 batches: 0.0248
trigger times: 0
Loss after 555701940 batches: 0.0238
trigger times: 0
Loss after 555833040 batches: 0.0233
trigger times: 0
Loss after 555964140 batches: 0.0223
trigger times: 1
Loss after 556095240 batches: 0.0220
trigger times: 2
Loss after 556226340 batches: 0.0221
trigger times: 3
Loss after 556357440 batches: 0.0213
trigger times: 4
Loss after 556488540 batches: 0.0214
trigger times: 5
Loss after 556619640 batches: 0.0204
trigger times: 6
Loss after 556750740 batches: 0.0206
trigger times: 7
Loss after 556881840 batches: 0.0201
trigger times: 8
Loss after 557012940 batches: 0.0205
trigger times: 9
Loss after 557144040 batches: 0.0213
trigger times: 10
Loss after 557275140 batches: 0.0211
trigger times: 0
Loss after 557406240 batches: 0.0205
trigger times: 0
Loss after 557537340 batches: 0.0198
trigger times: 1
Loss after 557668440 batches: 0.0202
trigger times: 2
Loss after 557799540 batches: 0.0202
trigger times: 3
Loss after 557930640 batches: 0.0200
trigger times: 4
Loss after 558061740 batches: 0.0203
trigger times: 5
Loss after 558192840 batches: 0.0196
trigger times: 6
Loss after 558323940 batches: 0.0194
trigger times: 7
Loss after 558455040 batches: 0.0198
trigger times: 8
Loss after 558586140 batches: 0.0198
trigger times: 9
Loss after 558717240 batches: 0.0190
trigger times: 0
Loss after 558848340 batches: 0.0192
trigger times: 1
Loss after 558979440 batches: 0.0190
trigger times: 2
Loss after 559110540 batches: 0.0192
trigger times: 3
Loss after 559241640 batches: 0.0196
trigger times: 4
Loss after 559372740 batches: 0.0190
trigger times: 5
Loss after 559503840 batches: 0.0185
trigger times: 6
Loss after 559634940 batches: 0.0192
trigger times: 7
Loss after 559766040 batches: 0.0188
trigger times: 8
Loss after 559897140 batches: 0.0179
trigger times: 0
Loss after 560028240 batches: 0.0189
trigger times: 1
Loss after 560159340 batches: 0.0180
trigger times: 2
Loss after 560290440 batches: 0.0186
trigger times: 3
Loss after 560421540 batches: 0.0182
trigger times: 4
Loss after 560552640 batches: 0.0184
trigger times: 5
Loss after 560683740 batches: 0.0176
trigger times: 6
Loss after 560814840 batches: 0.0181
trigger times: 7
Loss after 560945940 batches: 0.0182
trigger times: 8
Loss after 561077040 batches: 0.0185
trigger times: 9
Loss after 561208140 batches: 0.0178
trigger times: 10
Loss after 561339240 batches: 0.0174
trigger times: 11
Loss after 561470340 batches: 0.0171
trigger times: 12
Loss after 561601440 batches: 0.0174
trigger times: 13
Loss after 561732540 batches: 0.0184
trigger times: 14
Loss after 561863640 batches: 0.0173
trigger times: 15
Loss after 561994740 batches: 0.0179
trigger times: 16
Loss after 562125840 batches: 0.0174
trigger times: 17
Loss after 562256940 batches: 0.0176
trigger times: 18
Loss after 562388040 batches: 0.0178
trigger times: 19
Loss after 562519140 batches: 0.0174
trigger times: 20
Early stopping!
Start to test process.
Loss after 562650240 batches: 0.0176
Time to train on one home:  478.7352213859558
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887, 0.018152045403586472, 0.017512356438894854, 0.016091731943637777, 0.01601899988306293, 0.015690088165884984, 0.01509131066983643]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884], [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987], [0.5804732673698001, 0.3723540197761146, 0.45230155418961127, 1.0682024720651153, 0.514164189884338, 25.236666271720736, 1587.2568], [0.5693913300832113, 0.3843477905525722, 0.4653277545238437, 1.0599366084402062, 0.5043389577801884, 25.04138228090006, 1556.9258], [0.5662658578819699, 0.38771059287123977, 0.46820783457394766, 1.0579149875211409, 0.5015841683218024, 24.993620762090103, 1548.4215], [0.5593101216687096, 0.3952721834915385, 0.47154332581377506, 1.051822130314305, 0.49538975421253684, 24.84967482675361, 1529.299], [0.5649060971207089, 0.3891939618618432, 0.4673667174574867, 1.0599116662790036, 0.5003690004733905, 25.04079301340178, 1544.6703]]
Round_13_results:  [0.5649060971207089, 0.3891939618618432, 0.4673667174574867, 1.0599116662790036, 0.5003690004733905, 25.04079301340178, 1544.6703]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4480 < 4481; dropping {'Training_Loss': 0.08540360281630507, 'Validation_Loss': 0.2267482744322883, 'Training_R2': 0.9139747804668799, 'Validation_R2': 0.7895410639671316, 'Training_F1': 0.8413430755103258, 'Validation_F1': 0.7323701641547471, 'Training_NEP': 0.317322397471331, 'Validation_NEP': 0.5247546917846824, 'Training_NDE': 0.06458065925352645, 'Validation_NDE': 0.16759840250116687, 'Training_MAE': 10.509088486075674, 'Validation_MAE': 14.390950722591851, 'Training_MSE': 284.14478, 'Validation_MSE': 618.9362}.
trigger times: 0
Loss after 562781340 batches: 0.0854
trigger times: 0
Loss after 562912440 batches: 0.0225
trigger times: 1
Loss after 563043540 batches: 0.0173
trigger times: 0
Loss after 563174640 batches: 0.0155
trigger times: 0
Loss after 563305740 batches: 0.0144
trigger times: 0
Loss after 563436840 batches: 0.0135
trigger times: 0
Loss after 563567940 batches: 0.0129
trigger times: 1
Loss after 563699040 batches: 0.0123
trigger times: 2
Loss after 563830140 batches: 0.0118
trigger times: 3
Loss after 563961240 batches: 0.0114
trigger times: 4
Loss after 564092340 batches: 0.0114
trigger times: 0
Loss after 564223440 batches: 0.0110
trigger times: 1
Loss after 564354540 batches: 0.0107
trigger times: 2
Loss after 564485640 batches: 0.0106
trigger times: 3
Loss after 564616740 batches: 0.0104
trigger times: 0
Loss after 564747840 batches: 0.0104
trigger times: 1
Loss after 564878940 batches: 0.0099
trigger times: 2
Loss after 565010040 batches: 0.0096
trigger times: 3
Loss after 565141140 batches: 0.0097
trigger times: 4
Loss after 565272240 batches: 0.0097
trigger times: 5
Loss after 565403340 batches: 0.0095
trigger times: 6
Loss after 565534440 batches: 0.0095
trigger times: 0
Loss after 565665540 batches: 0.0095
trigger times: 1
Loss after 565796640 batches: 0.0094
trigger times: 2
Loss after 565927740 batches: 0.0093
trigger times: 3
Loss after 566058840 batches: 0.0091
trigger times: 4
Loss after 566189940 batches: 0.0094
trigger times: 5
Loss after 566321040 batches: 0.0090
trigger times: 6
Loss after 566452140 batches: 0.0094
trigger times: 7
Loss after 566583240 batches: 0.0090
trigger times: 8
Loss after 566714340 batches: 0.0088
trigger times: 9
Loss after 566845440 batches: 0.0088
trigger times: 10
Loss after 566976540 batches: 0.0086
trigger times: 11
Loss after 567107640 batches: 0.0088
trigger times: 12
Loss after 567238740 batches: 0.0090
trigger times: 13
Loss after 567369840 batches: 0.0086
trigger times: 14
Loss after 567500940 batches: 0.0085
trigger times: 15
Loss after 567632040 batches: 0.0087
trigger times: 16
Loss after 567763140 batches: 0.0085
trigger times: 17
Loss after 567894240 batches: 0.0083
trigger times: 18
Loss after 568025340 batches: 0.0082
trigger times: 19
Loss after 568156440 batches: 0.0082
trigger times: 20
Early stopping!
Start to test process.
Loss after 568287540 batches: 0.0082
Time to train on one home:  317.09520149230957
trigger times: 0
Loss after 568390140 batches: 0.1332
trigger times: 0
Loss after 568492740 batches: 0.0508
trigger times: 0
Loss after 568595340 batches: 0.0376
trigger times: 0
Loss after 568697940 batches: 0.0288
trigger times: 1
Loss after 568800540 batches: 0.0260
trigger times: 2
Loss after 568903140 batches: 0.0240
trigger times: 3
Loss after 569005740 batches: 0.0224
trigger times: 0
Loss after 569108340 batches: 0.0225
trigger times: 0
Loss after 569210940 batches: 0.0218
trigger times: 0
Loss after 569313540 batches: 0.0215
trigger times: 1
Loss after 569416140 batches: 0.0201
trigger times: 2
Loss after 569518740 batches: 0.0194
trigger times: 3
Loss after 569621340 batches: 0.0191
trigger times: 0
Loss after 569723940 batches: 0.0195
trigger times: 1
Loss after 569826540 batches: 0.0186
trigger times: 2
Loss after 569929140 batches: 0.0184
trigger times: 3
Loss after 570031740 batches: 0.0185
trigger times: 4
Loss after 570134340 batches: 0.0192
trigger times: 5
Loss after 570236940 batches: 0.0184
trigger times: 6
Loss after 570339540 batches: 0.0187
trigger times: 0
Loss after 570442140 batches: 0.0187
trigger times: 1
Loss after 570544740 batches: 0.0191
trigger times: 2
Loss after 570647340 batches: 0.0183
trigger times: 3
Loss after 570749940 batches: 0.0184
trigger times: 4
Loss after 570852540 batches: 0.0167
trigger times: 5
Loss after 570955140 batches: 0.0170
trigger times: 6
Loss after 571057740 batches: 0.0171
trigger times: 7
Loss after 571160340 batches: 0.0170
trigger times: 0
Loss after 571262940 batches: 0.0170
trigger times: 1
Loss after 571365540 batches: 0.0171
trigger times: 2
Loss after 571468140 batches: 0.0164
trigger times: 3
Loss after 571570740 batches: 0.0166
trigger times: 4
Loss after 571673340 batches: 0.0160
trigger times: 5
Loss after 571775940 batches: 0.0167
trigger times: 6
Loss after 571878540 batches: 0.0160
trigger times: 7
Loss after 571981140 batches: 0.0163
trigger times: 8
Loss after 572083740 batches: 0.0157
trigger times: 9
Loss after 572186340 batches: 0.0151
trigger times: 10
Loss after 572288940 batches: 0.0156
trigger times: 11
Loss after 572391540 batches: 0.0155
trigger times: 12
Loss after 572494140 batches: 0.0157
trigger times: 13
Loss after 572596740 batches: 0.0174
trigger times: 14
Loss after 572699340 batches: 0.0158
trigger times: 15
Loss after 572801940 batches: 0.0144
trigger times: 16
Loss after 572904540 batches: 0.0161
trigger times: 17
Loss after 573007140 batches: 0.0170
trigger times: 18
Loss after 573109740 batches: 0.0154
trigger times: 19
Loss after 573212340 batches: 0.0146
trigger times: 20
Early stopping!
Start to test process.
Loss after 573314940 batches: 0.0145
Time to train on one home:  292.7639400959015
trigger times: 0
Loss after 573446040 batches: 0.1530
trigger times: 1
Loss after 573577140 batches: 0.0429
trigger times: 2
Loss after 573708240 batches: 0.0320
trigger times: 3
Loss after 573839340 batches: 0.0280
trigger times: 4
Loss after 573970440 batches: 0.0255
trigger times: 5
Loss after 574101540 batches: 0.0241
trigger times: 6
Loss after 574232640 batches: 0.0228
trigger times: 7
Loss after 574363740 batches: 0.0218
trigger times: 8
Loss after 574494840 batches: 0.0212
trigger times: 9
Loss after 574625940 batches: 0.0204
trigger times: 10
Loss after 574757040 batches: 0.0200
trigger times: 11
Loss after 574888140 batches: 0.0195
trigger times: 12
Loss after 575019240 batches: 0.0191
trigger times: 13
Loss after 575150340 batches: 0.0187
trigger times: 14
Loss after 575281440 batches: 0.0185
trigger times: 15
Loss after 575412540 batches: 0.0178
trigger times: 16
Loss after 575543640 batches: 0.0176
trigger times: 17
Loss after 575674740 batches: 0.0173
trigger times: 18
Loss after 575805840 batches: 0.0173
trigger times: 19
Loss after 575936940 batches: 0.0167
trigger times: 20
Early stopping!
Start to test process.
Loss after 576068040 batches: 0.0167
Time to train on one home:  160.3841106891632
trigger times: 0
Loss after 576199140 batches: 0.2063
trigger times: 0
Loss after 576330240 batches: 0.0624
trigger times: 1
Loss after 576461340 batches: 0.0484
trigger times: 2
Loss after 576592440 batches: 0.0427
trigger times: 3
Loss after 576723540 batches: 0.0384
trigger times: 4
Loss after 576854640 batches: 0.0367
trigger times: 5
Loss after 576985740 batches: 0.0348
trigger times: 6
Loss after 577116840 batches: 0.0332
trigger times: 7
Loss after 577247940 batches: 0.0320
trigger times: 8
Loss after 577379040 batches: 0.0308
trigger times: 9
Loss after 577510140 batches: 0.0301
trigger times: 10
Loss after 577641240 batches: 0.0294
trigger times: 11
Loss after 577772340 batches: 0.0288
trigger times: 12
Loss after 577903440 batches: 0.0281
trigger times: 13
Loss after 578034540 batches: 0.0274
trigger times: 14
Loss after 578165640 batches: 0.0272
trigger times: 15
Loss after 578296740 batches: 0.0266
trigger times: 16
Loss after 578427840 batches: 0.0264
trigger times: 17
Loss after 578558940 batches: 0.0259
trigger times: 18
Loss after 578690040 batches: 0.0254
trigger times: 19
Loss after 578821140 batches: 0.0250
trigger times: 20
Early stopping!
Start to test process.
Loss after 578952240 batches: 0.0249
Time to train on one home:  167.54405450820923
trigger times: 0
Loss after 579080880 batches: 0.0993
trigger times: 0
Loss after 579209520 batches: 0.0281
trigger times: 0
Loss after 579338160 batches: 0.0220
trigger times: 1
Loss after 579466800 batches: 0.0196
trigger times: 0
Loss after 579595440 batches: 0.0184
trigger times: 0
Loss after 579724080 batches: 0.0174
trigger times: 0
Loss after 579852720 batches: 0.0165
trigger times: 1
Loss after 579981360 batches: 0.0157
trigger times: 2
Loss after 580110000 batches: 0.0150
trigger times: 0
Loss after 580238640 batches: 0.0145
trigger times: 1
Loss after 580367280 batches: 0.0141
trigger times: 0
Loss after 580495920 batches: 0.0140
trigger times: 1
Loss after 580624560 batches: 0.0140
trigger times: 2
Loss after 580753200 batches: 0.0137
trigger times: 3
Loss after 580881840 batches: 0.0132
trigger times: 4
Loss after 581010480 batches: 0.0133
trigger times: 5
Loss after 581139120 batches: 0.0130
trigger times: 6
Loss after 581267760 batches: 0.0127
trigger times: 7
Loss after 581396400 batches: 0.0126
trigger times: 8
Loss after 581525040 batches: 0.0123
trigger times: 9
Loss after 581653680 batches: 0.0122
trigger times: 10
Loss after 581782320 batches: 0.0122
trigger times: 11
Loss after 581910960 batches: 0.0121
trigger times: 12
Loss after 582039600 batches: 0.0119
trigger times: 13
Loss after 582168240 batches: 0.0119
trigger times: 14
Loss after 582296880 batches: 0.0118
trigger times: 15
Loss after 582425520 batches: 0.0117
trigger times: 16
Loss after 582554160 batches: 0.0115
trigger times: 17
Loss after 582682800 batches: 0.0113
trigger times: 18
Loss after 582811440 batches: 0.0116
trigger times: 19
Loss after 582940080 batches: 0.0112
trigger times: 20
Early stopping!
Start to test process.
Loss after 583068720 batches: 0.0112
Time to train on one home:  234.83755207061768
trigger times: 0
Loss after 583199820 batches: 0.1637
trigger times: 0
Loss after 583330920 batches: 0.0459
trigger times: 0
Loss after 583462020 batches: 0.0348
trigger times: 0
Loss after 583593120 batches: 0.0312
trigger times: 1
Loss after 583724220 batches: 0.0287
trigger times: 2
Loss after 583855320 batches: 0.0275
trigger times: 0
Loss after 583986420 batches: 0.0261
trigger times: 1
Loss after 584117520 batches: 0.0255
trigger times: 2
Loss after 584248620 batches: 0.0246
trigger times: 3
Loss after 584379720 batches: 0.0242
trigger times: 0
Loss after 584510820 batches: 0.0235
trigger times: 0
Loss after 584641920 batches: 0.0230
trigger times: 1
Loss after 584773020 batches: 0.0225
trigger times: 2
Loss after 584904120 batches: 0.0223
trigger times: 3
Loss after 585035220 batches: 0.0217
trigger times: 4
Loss after 585166320 batches: 0.0214
trigger times: 5
Loss after 585297420 batches: 0.0212
trigger times: 6
Loss after 585428520 batches: 0.0210
trigger times: 7
Loss after 585559620 batches: 0.0209
trigger times: 0
Loss after 585690720 batches: 0.0203
trigger times: 1
Loss after 585821820 batches: 0.0204
trigger times: 2
Loss after 585952920 batches: 0.0199
trigger times: 3
Loss after 586084020 batches: 0.0199
trigger times: 4
Loss after 586215120 batches: 0.0197
trigger times: 0
Loss after 586346220 batches: 0.0196
trigger times: 1
Loss after 586477320 batches: 0.0193
trigger times: 2
Loss after 586608420 batches: 0.0192
trigger times: 3
Loss after 586739520 batches: 0.0190
trigger times: 4
Loss after 586870620 batches: 0.0190
trigger times: 5
Loss after 587001720 batches: 0.0185
trigger times: 6
Loss after 587132820 batches: 0.0185
trigger times: 7
Loss after 587263920 batches: 0.0186
trigger times: 8
Loss after 587395020 batches: 0.0184
trigger times: 9
Loss after 587526120 batches: 0.0185
trigger times: 10
Loss after 587657220 batches: 0.0182
trigger times: 11
Loss after 587788320 batches: 0.0180
trigger times: 12
Loss after 587919420 batches: 0.0182
trigger times: 13
Loss after 588050520 batches: 0.0180
trigger times: 14
Loss after 588181620 batches: 0.0182
trigger times: 15
Loss after 588312720 batches: 0.0179
trigger times: 16
Loss after 588443820 batches: 0.0175
trigger times: 17
Loss after 588574920 batches: 0.0177
trigger times: 18
Loss after 588706020 batches: 0.0178
trigger times: 0
Loss after 588837120 batches: 0.0176
trigger times: 1
Loss after 588968220 batches: 0.0176
trigger times: 2
Loss after 589099320 batches: 0.0173
trigger times: 3
Loss after 589230420 batches: 0.0172
trigger times: 4
Loss after 589361520 batches: 0.0170
trigger times: 5
Loss after 589492620 batches: 0.0168
trigger times: 6
Loss after 589623720 batches: 0.0170
trigger times: 7
Loss after 589754820 batches: 0.0169
trigger times: 8
Loss after 589885920 batches: 0.0169
trigger times: 9
Loss after 590017020 batches: 0.0166
trigger times: 10
Loss after 590148120 batches: 0.0166
trigger times: 11
Loss after 590279220 batches: 0.0167
trigger times: 12
Loss after 590410320 batches: 0.0166
trigger times: 13
Loss after 590541420 batches: 0.0167
trigger times: 14
Loss after 590672520 batches: 0.0164
trigger times: 15
Loss after 590803620 batches: 0.0165
trigger times: 16
Loss after 590934720 batches: 0.0164
trigger times: 17
Loss after 591065820 batches: 0.0162
trigger times: 18
Loss after 591196920 batches: 0.0162
trigger times: 19
Loss after 591328020 batches: 0.0161
trigger times: 20
Early stopping!
Start to test process.
Loss after 591459120 batches: 0.0160
Time to train on one home:  465.3634457588196
trigger times: 0
Loss after 591590220 batches: 0.1640
trigger times: 0
Loss after 591721320 batches: 0.0520
trigger times: 0
Loss after 591852420 batches: 0.0383
trigger times: 1
Loss after 591983520 batches: 0.0324
trigger times: 2
Loss after 592114620 batches: 0.0302
trigger times: 0
Loss after 592245720 batches: 0.0277
trigger times: 0
Loss after 592376820 batches: 0.0262
trigger times: 1
Loss after 592507920 batches: 0.0265
trigger times: 2
Loss after 592639020 batches: 0.0241
trigger times: 3
Loss after 592770120 batches: 0.0241
trigger times: 0
Loss after 592901220 batches: 0.0238
trigger times: 1
Loss after 593032320 batches: 0.0232
trigger times: 2
Loss after 593163420 batches: 0.0227
trigger times: 3
Loss after 593294520 batches: 0.0226
trigger times: 4
Loss after 593425620 batches: 0.0224
trigger times: 0
Loss after 593556720 batches: 0.0228
trigger times: 1
Loss after 593687820 batches: 0.0216
trigger times: 0
Loss after 593818920 batches: 0.0217
trigger times: 0
Loss after 593950020 batches: 0.0219
trigger times: 1
Loss after 594081120 batches: 0.0218
trigger times: 2
Loss after 594212220 batches: 0.0209
trigger times: 3
Loss after 594343320 batches: 0.0206
trigger times: 4
Loss after 594474420 batches: 0.0201
trigger times: 5
Loss after 594605520 batches: 0.0202
trigger times: 0
Loss after 594736620 batches: 0.0211
trigger times: 1
Loss after 594867720 batches: 0.0203
trigger times: 2
Loss after 594998820 batches: 0.0196
trigger times: 3
Loss after 595129920 batches: 0.0194
trigger times: 4
Loss after 595261020 batches: 0.0200
trigger times: 5
Loss after 595392120 batches: 0.0197
trigger times: 6
Loss after 595523220 batches: 0.0205
trigger times: 7
Loss after 595654320 batches: 0.0189
trigger times: 8
Loss after 595785420 batches: 0.0190
trigger times: 9
Loss after 595916520 batches: 0.0194
trigger times: 10
Loss after 596047620 batches: 0.0194
trigger times: 11
Loss after 596178720 batches: 0.0181
trigger times: 12
Loss after 596309820 batches: 0.0183
trigger times: 13
Loss after 596440920 batches: 0.0182
trigger times: 14
Loss after 596572020 batches: 0.0192
trigger times: 15
Loss after 596703120 batches: 0.0186
trigger times: 16
Loss after 596834220 batches: 0.0189
trigger times: 17
Loss after 596965320 batches: 0.0194
trigger times: 18
Loss after 597096420 batches: 0.0187
trigger times: 19
Loss after 597227520 batches: 0.0189
trigger times: 20
Early stopping!
Start to test process.
Loss after 597358620 batches: 0.0179
Time to train on one home:  329.8620536327362
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887, 0.018152045403586472, 0.017512356438894854, 0.016091731943637777, 0.01601899988306293, 0.015690088165884984, 0.01509131066983643, 0.015631624726830377]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884], [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987], [0.5804732673698001, 0.3723540197761146, 0.45230155418961127, 1.0682024720651153, 0.514164189884338, 25.236666271720736, 1587.2568], [0.5693913300832113, 0.3843477905525722, 0.4653277545238437, 1.0599366084402062, 0.5043389577801884, 25.04138228090006, 1556.9258], [0.5662658578819699, 0.38771059287123977, 0.46820783457394766, 1.0579149875211409, 0.5015841683218024, 24.993620762090103, 1548.4215], [0.5593101216687096, 0.3952721834915385, 0.47154332581377506, 1.051822130314305, 0.49538975421253684, 24.84967482675361, 1529.299], [0.5649060971207089, 0.3891939618618432, 0.4673667174574867, 1.0599116662790036, 0.5003690004733905, 25.04079301340178, 1544.6703], [0.5509053071339926, 0.40439992136570346, 0.47811920698222654, 1.044031858684061, 0.48791236074962885, 24.665626867270383, 1506.2158]]
Round_14_results:  [0.5509053071339926, 0.40439992136570346, 0.47811920698222654, 1.044031858684061, 0.48791236074962885, 24.665626867270383, 1506.2158]
trigger times: 0
Loss after 597489720 batches: 0.0788
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 4756 < 4757; dropping {'Training_Loss': 0.07878145125677001, 'Validation_Loss': 0.22034531169467503, 'Training_R2': 0.9206759706928723, 'Validation_R2': 0.7953841852327725, 'Training_F1': 0.846806414586779, 'Validation_F1': 0.7582140249016801, 'Training_NEP': 0.30626259574630377, 'Validation_NEP': 0.4866536114812588, 'Training_NDE': 0.05954995680456298, 'Validation_NDE': 0.16294524874014493, 'Training_MAE': 10.142809786894775, 'Validation_MAE': 13.346061028972748, 'Training_MSE': 262.0105, 'Validation_MSE': 601.75226}.
trigger times: 0
Loss after 597620820 batches: 0.0232
trigger times: 0
Loss after 597751920 batches: 0.0172
trigger times: 0
Loss after 597883020 batches: 0.0150
trigger times: 1
Loss after 598014120 batches: 0.0141
trigger times: 2
Loss after 598145220 batches: 0.0134
trigger times: 3
Loss after 598276320 batches: 0.0123
trigger times: 4
Loss after 598407420 batches: 0.0123
trigger times: 5
Loss after 598538520 batches: 0.0117
trigger times: 0
Loss after 598669620 batches: 0.0112
trigger times: 1
Loss after 598800720 batches: 0.0110
trigger times: 2
Loss after 598931820 batches: 0.0108
trigger times: 3
Loss after 599062920 batches: 0.0105
trigger times: 4
Loss after 599194020 batches: 0.0106
trigger times: 5
Loss after 599325120 batches: 0.0103
trigger times: 6
Loss after 599456220 batches: 0.0103
trigger times: 7
Loss after 599587320 batches: 0.0100
trigger times: 8
Loss after 599718420 batches: 0.0099
trigger times: 9
Loss after 599849520 batches: 0.0097
trigger times: 10
Loss after 599980620 batches: 0.0096
trigger times: 11
Loss after 600111720 batches: 0.0095
trigger times: 12
Loss after 600242820 batches: 0.0093
trigger times: 13
Loss after 600373920 batches: 0.0093
trigger times: 14
Loss after 600505020 batches: 0.0091
trigger times: 0
Loss after 600636120 batches: 0.0091
trigger times: 1
Loss after 600767220 batches: 0.0092
trigger times: 2
Loss after 600898320 batches: 0.0091
trigger times: 3
Loss after 601029420 batches: 0.0088
trigger times: 4
Loss after 601160520 batches: 0.0088
trigger times: 5
Loss after 601291620 batches: 0.0088
trigger times: 6
Loss after 601422720 batches: 0.0087
trigger times: 7
Loss after 601553820 batches: 0.0087
trigger times: 8
Loss after 601684920 batches: 0.0085
trigger times: 9
Loss after 601816020 batches: 0.0086
trigger times: 10
Loss after 601947120 batches: 0.0083
trigger times: 11
Loss after 602078220 batches: 0.0083
trigger times: 0
Loss after 602209320 batches: 0.0083
trigger times: 1
Loss after 602340420 batches: 0.0082
trigger times: 2
Loss after 602471520 batches: 0.0080
trigger times: 3
Loss after 602602620 batches: 0.0081
trigger times: 4
Loss after 602733720 batches: 0.0081
trigger times: 5
Loss after 602864820 batches: 0.0081
trigger times: 6
Loss after 602995920 batches: 0.0080
trigger times: 7
Loss after 603127020 batches: 0.0080
trigger times: 8
Loss after 603258120 batches: 0.0080
trigger times: 9
Loss after 603389220 batches: 0.0080
trigger times: 10
Loss after 603520320 batches: 0.0078
trigger times: 11
Loss after 603651420 batches: 0.0078
trigger times: 12
Loss after 603782520 batches: 0.0077
trigger times: 13
Loss after 603913620 batches: 0.0078
trigger times: 14
Loss after 604044720 batches: 0.0081
trigger times: 15
Loss after 604175820 batches: 0.0075
trigger times: 16
Loss after 604306920 batches: 0.0078
trigger times: 17
Loss after 604438020 batches: 0.0078
trigger times: 18
Loss after 604569120 batches: 0.0076
trigger times: 19
Loss after 604700220 batches: 0.0075
trigger times: 20
Early stopping!
Start to test process.
Loss after 604831320 batches: 0.0077
Time to train on one home:  415.19696283340454
trigger times: 0
Loss after 604933920 batches: 0.1220
trigger times: 0
Loss after 605036520 batches: 0.0542
trigger times: 0
Loss after 605139120 batches: 0.0364
trigger times: 1
Loss after 605241720 batches: 0.0287
trigger times: 0
Loss after 605344320 batches: 0.0249
trigger times: 1
Loss after 605446920 batches: 0.0233
trigger times: 2
Loss after 605549520 batches: 0.0244
trigger times: 0
Loss after 605652120 batches: 0.0228
trigger times: 0
Loss after 605754720 batches: 0.0223
trigger times: 0
Loss after 605857320 batches: 0.0214
trigger times: 1
Loss after 605959920 batches: 0.0201
trigger times: 2
Loss after 606062520 batches: 0.0198
trigger times: 3
Loss after 606165120 batches: 0.0200
trigger times: 4
Loss after 606267720 batches: 0.0190
trigger times: 5
Loss after 606370320 batches: 0.0193
trigger times: 6
Loss after 606472920 batches: 0.0187
trigger times: 7
Loss after 606575520 batches: 0.0177
trigger times: 8
Loss after 606678120 batches: 0.0181
trigger times: 9
Loss after 606780720 batches: 0.0173
trigger times: 10
Loss after 606883320 batches: 0.0185
trigger times: 11
Loss after 606985920 batches: 0.0185
trigger times: 12
Loss after 607088520 batches: 0.0197
trigger times: 13
Loss after 607191120 batches: 0.0187
trigger times: 14
Loss after 607293720 batches: 0.0185
trigger times: 15
Loss after 607396320 batches: 0.0170
trigger times: 16
Loss after 607498920 batches: 0.0167
trigger times: 17
Loss after 607601520 batches: 0.0167
trigger times: 18
Loss after 607704120 batches: 0.0172
trigger times: 0
Loss after 607806720 batches: 0.0166
trigger times: 1
Loss after 607909320 batches: 0.0173
trigger times: 2
Loss after 608011920 batches: 0.0162
trigger times: 3
Loss after 608114520 batches: 0.0175
trigger times: 4
Loss after 608217120 batches: 0.0175
trigger times: 5
Loss after 608319720 batches: 0.0161
trigger times: 6
Loss after 608422320 batches: 0.0163
trigger times: 7
Loss after 608524920 batches: 0.0162
trigger times: 8
Loss after 608627520 batches: 0.0156
trigger times: 9
Loss after 608730120 batches: 0.0160
trigger times: 0
Loss after 608832720 batches: 0.0158
trigger times: 1
Loss after 608935320 batches: 0.0154
trigger times: 2
Loss after 609037920 batches: 0.0151
trigger times: 3
Loss after 609140520 batches: 0.0144
trigger times: 4
Loss after 609243120 batches: 0.0152
trigger times: 5
Loss after 609345720 batches: 0.0156
trigger times: 6
Loss after 609448320 batches: 0.0150
trigger times: 7
Loss after 609550920 batches: 0.0152
trigger times: 8
Loss after 609653520 batches: 0.0147
trigger times: 9
Loss after 609756120 batches: 0.0150
trigger times: 10
Loss after 609858720 batches: 0.0144
trigger times: 11
Loss after 609961320 batches: 0.0143
trigger times: 12
Loss after 610063920 batches: 0.0139
trigger times: 13
Loss after 610166520 batches: 0.0147
trigger times: 14
Loss after 610269120 batches: 0.0149
trigger times: 15
Loss after 610371720 batches: 0.0148
trigger times: 0
Loss after 610474320 batches: 0.0172
trigger times: 1
Loss after 610576920 batches: 0.0159
trigger times: 2
Loss after 610679520 batches: 0.0145
trigger times: 3
Loss after 610782120 batches: 0.0138
trigger times: 4
Loss after 610884720 batches: 0.0144
trigger times: 5
Loss after 610987320 batches: 0.0141
trigger times: 6
Loss after 611089920 batches: 0.0137
trigger times: 0
Loss after 611192520 batches: 0.0137
trigger times: 1
Loss after 611295120 batches: 0.0151
trigger times: 0
Loss after 611397720 batches: 0.0145
trigger times: 1
Loss after 611500320 batches: 0.0148
trigger times: 2
Loss after 611602920 batches: 0.0143
trigger times: 3
Loss after 611705520 batches: 0.0138
trigger times: 4
Loss after 611808120 batches: 0.0136
trigger times: 5
Loss after 611910720 batches: 0.0147
trigger times: 6
Loss after 612013320 batches: 0.0138
trigger times: 7
Loss after 612115920 batches: 0.0136
trigger times: 8
Loss after 612218520 batches: 0.0137
trigger times: 9
Loss after 612321120 batches: 0.0136
trigger times: 10
Loss after 612423720 batches: 0.0135
trigger times: 11
Loss after 612526320 batches: 0.0142
trigger times: 12
Loss after 612628920 batches: 0.0151
trigger times: 13
Loss after 612731520 batches: 0.0133
trigger times: 14
Loss after 612834120 batches: 0.0130
trigger times: 15
Loss after 612936720 batches: 0.0130
trigger times: 16
Loss after 613039320 batches: 0.0128
trigger times: 17
Loss after 613141920 batches: 0.0135
trigger times: 18
Loss after 613244520 batches: 0.0130
trigger times: 19
Loss after 613347120 batches: 0.0143
trigger times: 20
Early stopping!
Start to test process.
Loss after 613449720 batches: 0.0145
Time to train on one home:  495.0030539035797
trigger times: 0
Loss after 613580820 batches: 0.1063
trigger times: 0
Loss after 613711920 batches: 0.0368
trigger times: 0
Loss after 613843020 batches: 0.0292
trigger times: 1
Loss after 613974120 batches: 0.0262
trigger times: 0
Loss after 614105220 batches: 0.0242
trigger times: 0
Loss after 614236320 batches: 0.0228
trigger times: 1
Loss after 614367420 batches: 0.0218
trigger times: 2
Loss after 614498520 batches: 0.0210
trigger times: 3
Loss after 614629620 batches: 0.0204
trigger times: 4
Loss after 614760720 batches: 0.0199
trigger times: 5
Loss after 614891820 batches: 0.0193
trigger times: 6
Loss after 615022920 batches: 0.0187
trigger times: 7
Loss after 615154020 batches: 0.0184
trigger times: 8
Loss after 615285120 batches: 0.0180
trigger times: 9
Loss after 615416220 batches: 0.0176
trigger times: 10
Loss after 615547320 batches: 0.0176
trigger times: 11
Loss after 615678420 batches: 0.0174
trigger times: 12
Loss after 615809520 batches: 0.0170
trigger times: 13
Loss after 615940620 batches: 0.0167
trigger times: 14
Loss after 616071720 batches: 0.0165
trigger times: 15
Loss after 616202820 batches: 0.0162
trigger times: 16
Loss after 616333920 batches: 0.0162
trigger times: 17
Loss after 616465020 batches: 0.0159
trigger times: 18
Loss after 616596120 batches: 0.0159
trigger times: 19
Loss after 616727220 batches: 0.0157
trigger times: 20
Early stopping!
Start to test process.
Loss after 616858320 batches: 0.0156
Time to train on one home:  195.6532497406006
trigger times: 0
Loss after 616989420 batches: 0.2018
trigger times: 0
Loss after 617120520 batches: 0.0609
trigger times: 0
Loss after 617251620 batches: 0.0474
trigger times: 1
Loss after 617382720 batches: 0.0413
trigger times: 2
Loss after 617513820 batches: 0.0386
trigger times: 3
Loss after 617644920 batches: 0.0361
trigger times: 4
Loss after 617776020 batches: 0.0345
trigger times: 5
Loss after 617907120 batches: 0.0325
trigger times: 6
Loss after 618038220 batches: 0.0315
trigger times: 7
Loss after 618169320 batches: 0.0308
trigger times: 8
Loss after 618300420 batches: 0.0301
trigger times: 9
Loss after 618431520 batches: 0.0294
trigger times: 10
Loss after 618562620 batches: 0.0286
trigger times: 11
Loss after 618693720 batches: 0.0279
trigger times: 12
Loss after 618824820 batches: 0.0277
trigger times: 13
Loss after 618955920 batches: 0.0269
trigger times: 14
Loss after 619087020 batches: 0.0265
trigger times: 15
Loss after 619218120 batches: 0.0261
trigger times: 16
Loss after 619349220 batches: 0.0259
trigger times: 17
Loss after 619480320 batches: 0.0253
trigger times: 18
Loss after 619611420 batches: 0.0246
trigger times: 19
Loss after 619742520 batches: 0.0248
trigger times: 20
Early stopping!
Start to test process.
Loss after 619873620 batches: 0.0244
Time to train on one home:  174.52837347984314
trigger times: 0
Loss after 620002260 batches: 0.0874
trigger times: 0
Loss after 620130900 batches: 0.0270
trigger times: 0
Loss after 620259540 batches: 0.0210
trigger times: 0
Loss after 620388180 batches: 0.0190
trigger times: 0
Loss after 620516820 batches: 0.0179
trigger times: 1
Loss after 620645460 batches: 0.0171
trigger times: 0
Loss after 620774100 batches: 0.0159
trigger times: 1
Loss after 620902740 batches: 0.0152
trigger times: 0
Loss after 621031380 batches: 0.0149
trigger times: 1
Loss after 621160020 batches: 0.0146
trigger times: 2
Loss after 621288660 batches: 0.0141
trigger times: 3
Loss after 621417300 batches: 0.0137
trigger times: 4
Loss after 621545940 batches: 0.0134
trigger times: 5
Loss after 621674580 batches: 0.0133
trigger times: 6
Loss after 621803220 batches: 0.0130
trigger times: 7
Loss after 621931860 batches: 0.0128
trigger times: 8
Loss after 622060500 batches: 0.0128
trigger times: 9
Loss after 622189140 batches: 0.0125
trigger times: 10
Loss after 622317780 batches: 0.0126
trigger times: 11
Loss after 622446420 batches: 0.0122
trigger times: 12
Loss after 622575060 batches: 0.0123
trigger times: 13
Loss after 622703700 batches: 0.0122
trigger times: 14
Loss after 622832340 batches: 0.0119
trigger times: 15
Loss after 622960980 batches: 0.0118
trigger times: 16
Loss after 623089620 batches: 0.0115
trigger times: 17
Loss after 623218260 batches: 0.0118
trigger times: 18
Loss after 623346900 batches: 0.0116
trigger times: 19
Loss after 623475540 batches: 0.0116
trigger times: 20
Early stopping!
Start to test process.
Loss after 623604180 batches: 0.0113
Time to train on one home:  213.71127223968506
trigger times: 0
Loss after 623735280 batches: 0.1523
trigger times: 0
Loss after 623866380 batches: 0.0422
trigger times: 1
Loss after 623997480 batches: 0.0333
trigger times: 0
Loss after 624128580 batches: 0.0297
trigger times: 0
Loss after 624259680 batches: 0.0273
trigger times: 1
Loss after 624390780 batches: 0.0263
trigger times: 2
Loss after 624521880 batches: 0.0249
trigger times: 3
Loss after 624652980 batches: 0.0245
trigger times: 4
Loss after 624784080 batches: 0.0237
trigger times: 5
Loss after 624915180 batches: 0.0227
trigger times: 6
Loss after 625046280 batches: 0.0222
trigger times: 7
Loss after 625177380 batches: 0.0219
trigger times: 8
Loss after 625308480 batches: 0.0215
trigger times: 9
Loss after 625439580 batches: 0.0213
trigger times: 10
Loss after 625570680 batches: 0.0212
trigger times: 11
Loss after 625701780 batches: 0.0207
trigger times: 12
Loss after 625832880 batches: 0.0207
trigger times: 0
Loss after 625963980 batches: 0.0203
trigger times: 1
Loss after 626095080 batches: 0.0198
trigger times: 2
Loss after 626226180 batches: 0.0199
trigger times: 3
Loss after 626357280 batches: 0.0194
trigger times: 4
Loss after 626488380 batches: 0.0192
trigger times: 5
Loss after 626619480 batches: 0.0193
trigger times: 6
Loss after 626750580 batches: 0.0191
trigger times: 7
Loss after 626881680 batches: 0.0192
trigger times: 8
Loss after 627012780 batches: 0.0189
trigger times: 9
Loss after 627143880 batches: 0.0186
trigger times: 10
Loss after 627274980 batches: 0.0187
trigger times: 11
Loss after 627406080 batches: 0.0182
trigger times: 12
Loss after 627537180 batches: 0.0185
trigger times: 13
Loss after 627668280 batches: 0.0181
trigger times: 14
Loss after 627799380 batches: 0.0180
trigger times: 15
Loss after 627930480 batches: 0.0179
trigger times: 16
Loss after 628061580 batches: 0.0179
trigger times: 17
Loss after 628192680 batches: 0.0179
trigger times: 18
Loss after 628323780 batches: 0.0176
trigger times: 19
Loss after 628454880 batches: 0.0176
trigger times: 20
Early stopping!
Start to test process.
Loss after 628585980 batches: 0.0174
Time to train on one home:  281.1373918056488
trigger times: 0
Loss after 628717080 batches: 0.1389
trigger times: 1
Loss after 628848180 batches: 0.0496
trigger times: 2
Loss after 628979280 batches: 0.0388
trigger times: 0
Loss after 629110380 batches: 0.0314
trigger times: 1
Loss after 629241480 batches: 0.0284
trigger times: 2
Loss after 629372580 batches: 0.0281
trigger times: 3
Loss after 629503680 batches: 0.0263
trigger times: 0
Loss after 629634780 batches: 0.0257
trigger times: 0
Loss after 629765880 batches: 0.0246
trigger times: 1
Loss after 629896980 batches: 0.0240
trigger times: 2
Loss after 630028080 batches: 0.0234
trigger times: 3
Loss after 630159180 batches: 0.0223
trigger times: 4
Loss after 630290280 batches: 0.0231
trigger times: 5
Loss after 630421380 batches: 0.0225
trigger times: 6
Loss after 630552480 batches: 0.0228
trigger times: 7
Loss after 630683580 batches: 0.0215
trigger times: 8
Loss after 630814680 batches: 0.0225
trigger times: 0
Loss after 630945780 batches: 0.0223
trigger times: 1
Loss after 631076880 batches: 0.0209
trigger times: 2
Loss after 631207980 batches: 0.0212
trigger times: 3
Loss after 631339080 batches: 0.0208
trigger times: 4
Loss after 631470180 batches: 0.0210
trigger times: 5
Loss after 631601280 batches: 0.0201
trigger times: 6
Loss after 631732380 batches: 0.0206
trigger times: 7
Loss after 631863480 batches: 0.0205
trigger times: 8
Loss after 631994580 batches: 0.0200
trigger times: 9
Loss after 632125680 batches: 0.0194
trigger times: 10
Loss after 632256780 batches: 0.0194
trigger times: 11
Loss after 632387880 batches: 0.0196
trigger times: 12
Loss after 632518980 batches: 0.0189
trigger times: 13
Loss after 632650080 batches: 0.0191
trigger times: 14
Loss after 632781180 batches: 0.0205
trigger times: 15
Loss after 632912280 batches: 0.0201
trigger times: 16
Loss after 633043380 batches: 0.0197
trigger times: 17
Loss after 633174480 batches: 0.0194
trigger times: 18
Loss after 633305580 batches: 0.0187
trigger times: 19
Loss after 633436680 batches: 0.0183
trigger times: 20
Early stopping!
Start to test process.
Loss after 633567780 batches: 0.0186
Time to train on one home:  281.2483971118927
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887, 0.018152045403586472, 0.017512356438894854, 0.016091731943637777, 0.01601899988306293, 0.015690088165884984, 0.01509131066983643, 0.015631624726830377, 0.015631806442569732]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884], [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987], [0.5804732673698001, 0.3723540197761146, 0.45230155418961127, 1.0682024720651153, 0.514164189884338, 25.236666271720736, 1587.2568], [0.5693913300832113, 0.3843477905525722, 0.4653277545238437, 1.0599366084402062, 0.5043389577801884, 25.04138228090006, 1556.9258], [0.5662658578819699, 0.38771059287123977, 0.46820783457394766, 1.0579149875211409, 0.5015841683218024, 24.993620762090103, 1548.4215], [0.5593101216687096, 0.3952721834915385, 0.47154332581377506, 1.051822130314305, 0.49538975421253684, 24.84967482675361, 1529.299], [0.5649060971207089, 0.3891939618618432, 0.4673667174574867, 1.0599116662790036, 0.5003690004733905, 25.04079301340178, 1544.6703], [0.5509053071339926, 0.40439992136570346, 0.47811920698222654, 1.044031858684061, 0.48791236074962885, 24.665626867270383, 1506.2158], [0.5444808701674143, 0.41136524952900755, 0.4884585927112777, 1.0362181287288137, 0.4822064016178801, 24.481024696450934, 1488.6012]]
Round_15_results:  [0.5444808701674143, 0.41136524952900755, 0.4884585927112777, 1.0362181287288137, 0.4822064016178801, 24.481024696450934, 1488.6012]
trigger times: 0
Loss after 633698880 batches: 0.0947
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5051 < 5052; dropping {'Training_Loss': 0.09465459716629307, 'Validation_Loss': 0.24606725407971275, 'Training_R2': 0.9046530839462301, 'Validation_R2': 0.7714666863915381, 'Training_F1': 0.8321657830654261, 'Validation_F1': 0.735382539357012, 'Training_NEP': 0.335594598570084, 'Validation_NEP': 0.5072530718900817, 'Training_NDE': 0.07157862229194777, 'Validation_NDE': 0.18199188402765007, 'Training_MAE': 11.114227548784013, 'Validation_MAE': 13.910983695308795, 'Training_MSE': 314.9347, 'Validation_MSE': 672.09106}.
trigger times: 0
Loss after 633829980 batches: 0.0225
trigger times: 0
Loss after 633961080 batches: 0.0170
trigger times: 0
Loss after 634092180 batches: 0.0148
trigger times: 1
Loss after 634223280 batches: 0.0142
trigger times: 2
Loss after 634354380 batches: 0.0132
trigger times: 3
Loss after 634485480 batches: 0.0123
trigger times: 4
Loss after 634616580 batches: 0.0120
trigger times: 5
Loss after 634747680 batches: 0.0117
trigger times: 6
Loss after 634878780 batches: 0.0112
trigger times: 7
Loss after 635009880 batches: 0.0109
trigger times: 8
Loss after 635140980 batches: 0.0107
trigger times: 9
Loss after 635272080 batches: 0.0106
trigger times: 10
Loss after 635403180 batches: 0.0103
trigger times: 11
Loss after 635534280 batches: 0.0102
trigger times: 12
Loss after 635665380 batches: 0.0101
trigger times: 13
Loss after 635796480 batches: 0.0097
trigger times: 14
Loss after 635927580 batches: 0.0098
trigger times: 15
Loss after 636058680 batches: 0.0097
trigger times: 16
Loss after 636189780 batches: 0.0093
trigger times: 17
Loss after 636320880 batches: 0.0095
trigger times: 0
Loss after 636451980 batches: 0.0094
trigger times: 0
Loss after 636583080 batches: 0.0090
trigger times: 1
Loss after 636714180 batches: 0.0089
trigger times: 2
Loss after 636845280 batches: 0.0091
trigger times: 3
Loss after 636976380 batches: 0.0091
trigger times: 4
Loss after 637107480 batches: 0.0089
trigger times: 5
Loss after 637238580 batches: 0.0088
trigger times: 0
Loss after 637369680 batches: 0.0086
trigger times: 1
Loss after 637500780 batches: 0.0086
trigger times: 2
Loss after 637631880 batches: 0.0087
trigger times: 3
Loss after 637762980 batches: 0.0087
trigger times: 4
Loss after 637894080 batches: 0.0085
trigger times: 5
Loss after 638025180 batches: 0.0083
trigger times: 6
Loss after 638156280 batches: 0.0085
trigger times: 7
Loss after 638287380 batches: 0.0086
trigger times: 8
Loss after 638418480 batches: 0.0088
trigger times: 9
Loss after 638549580 batches: 0.0082
trigger times: 10
Loss after 638680680 batches: 0.0081
trigger times: 11
Loss after 638811780 batches: 0.0083
trigger times: 12
Loss after 638942880 batches: 0.0083
trigger times: 13
Loss after 639073980 batches: 0.0082
trigger times: 14
Loss after 639205080 batches: 0.0080
trigger times: 15
Loss after 639336180 batches: 0.0079
trigger times: 16
Loss after 639467280 batches: 0.0079
trigger times: 17
Loss after 639598380 batches: 0.0079
trigger times: 18
Loss after 639729480 batches: 0.0078
trigger times: 19
Loss after 639860580 batches: 0.0077
trigger times: 20
Early stopping!
Start to test process.
Loss after 639991680 batches: 0.0078
Time to train on one home:  359.27115392684937
trigger times: 0
Loss after 640094280 batches: 0.1461
trigger times: 0
Loss after 640196880 batches: 0.0461
trigger times: 1
Loss after 640299480 batches: 0.0313
trigger times: 2
Loss after 640402080 batches: 0.0262
trigger times: 3
Loss after 640504680 batches: 0.0246
trigger times: 4
Loss after 640607280 batches: 0.0231
trigger times: 5
Loss after 640709880 batches: 0.0217
trigger times: 6
Loss after 640812480 batches: 0.0208
trigger times: 0
Loss after 640915080 batches: 0.0204
trigger times: 1
Loss after 641017680 batches: 0.0218
trigger times: 0
Loss after 641120280 batches: 0.0198
trigger times: 1
Loss after 641222880 batches: 0.0191
trigger times: 2
Loss after 641325480 batches: 0.0194
trigger times: 3
Loss after 641428080 batches: 0.0186
trigger times: 4
Loss after 641530680 batches: 0.0181
trigger times: 5
Loss after 641633280 batches: 0.0177
trigger times: 6
Loss after 641735880 batches: 0.0172
trigger times: 7
Loss after 641838480 batches: 0.0168
trigger times: 8
Loss after 641941080 batches: 0.0175
trigger times: 9
Loss after 642043680 batches: 0.0171
trigger times: 10
Loss after 642146280 batches: 0.0180
trigger times: 11
Loss after 642248880 batches: 0.0174
trigger times: 12
Loss after 642351480 batches: 0.0178
trigger times: 13
Loss after 642454080 batches: 0.0173
trigger times: 14
Loss after 642556680 batches: 0.0178
trigger times: 15
Loss after 642659280 batches: 0.0185
trigger times: 0
Loss after 642761880 batches: 0.0186
trigger times: 1
Loss after 642864480 batches: 0.0172
trigger times: 2
Loss after 642967080 batches: 0.0157
trigger times: 0
Loss after 643069680 batches: 0.0168
trigger times: 1
Loss after 643172280 batches: 0.0181
trigger times: 2
Loss after 643274880 batches: 0.0159
trigger times: 3
Loss after 643377480 batches: 0.0157
trigger times: 4
Loss after 643480080 batches: 0.0154
trigger times: 5
Loss after 643582680 batches: 0.0149
trigger times: 6
Loss after 643685280 batches: 0.0150
trigger times: 7
Loss after 643787880 batches: 0.0147
trigger times: 8
Loss after 643890480 batches: 0.0148
trigger times: 9
Loss after 643993080 batches: 0.0146
trigger times: 10
Loss after 644095680 batches: 0.0145
trigger times: 11
Loss after 644198280 batches: 0.0142
trigger times: 12
Loss after 644300880 batches: 0.0144
trigger times: 13
Loss after 644403480 batches: 0.0140
trigger times: 14
Loss after 644506080 batches: 0.0146
trigger times: 0
Loss after 644608680 batches: 0.0147
trigger times: 1
Loss after 644711280 batches: 0.0152
trigger times: 2
Loss after 644813880 batches: 0.0142
trigger times: 3
Loss after 644916480 batches: 0.0145
trigger times: 4
Loss after 645019080 batches: 0.0141
trigger times: 5
Loss after 645121680 batches: 0.0140
trigger times: 6
Loss after 645224280 batches: 0.0142
trigger times: 7
Loss after 645326880 batches: 0.0152
trigger times: 8
Loss after 645429480 batches: 0.0146
trigger times: 9
Loss after 645532080 batches: 0.0140
trigger times: 10
Loss after 645634680 batches: 0.0154
trigger times: 11
Loss after 645737280 batches: 0.0137
trigger times: 12
Loss after 645839880 batches: 0.0140
trigger times: 13
Loss after 645942480 batches: 0.0130
trigger times: 14
Loss after 646045080 batches: 0.0132
trigger times: 15
Loss after 646147680 batches: 0.0129
trigger times: 16
Loss after 646250280 batches: 0.0134
trigger times: 17
Loss after 646352880 batches: 0.0141
trigger times: 18
Loss after 646455480 batches: 0.0143
trigger times: 19
Loss after 646558080 batches: 0.0140
trigger times: 20
Early stopping!
Start to test process.
Loss after 646660680 batches: 0.0141
Time to train on one home:  384.94070649147034
trigger times: 0
Loss after 646791780 batches: 0.1118
trigger times: 0
Loss after 646922880 batches: 0.0361
trigger times: 1
Loss after 647053980 batches: 0.0288
trigger times: 2
Loss after 647185080 batches: 0.0257
trigger times: 3
Loss after 647316180 batches: 0.0239
trigger times: 4
Loss after 647447280 batches: 0.0226
trigger times: 5
Loss after 647578380 batches: 0.0213
trigger times: 6
Loss after 647709480 batches: 0.0207
trigger times: 0
Loss after 647840580 batches: 0.0201
trigger times: 1
Loss after 647971680 batches: 0.0197
trigger times: 2
Loss after 648102780 batches: 0.0190
trigger times: 3
Loss after 648233880 batches: 0.0186
trigger times: 4
Loss after 648364980 batches: 0.0183
trigger times: 5
Loss after 648496080 batches: 0.0179
trigger times: 6
Loss after 648627180 batches: 0.0176
trigger times: 7
Loss after 648758280 batches: 0.0170
trigger times: 8
Loss after 648889380 batches: 0.0170
trigger times: 9
Loss after 649020480 batches: 0.0169
trigger times: 10
Loss after 649151580 batches: 0.0164
trigger times: 11
Loss after 649282680 batches: 0.0162
trigger times: 12
Loss after 649413780 batches: 0.0162
trigger times: 13
Loss after 649544880 batches: 0.0161
trigger times: 14
Loss after 649675980 batches: 0.0159
trigger times: 15
Loss after 649807080 batches: 0.0155
trigger times: 16
Loss after 649938180 batches: 0.0153
trigger times: 17
Loss after 650069280 batches: 0.0154
trigger times: 18
Loss after 650200380 batches: 0.0153
trigger times: 19
Loss after 650331480 batches: 0.0152
trigger times: 20
Early stopping!
Start to test process.
Loss after 650462580 batches: 0.0150
Time to train on one home:  216.39190578460693
trigger times: 0
Loss after 650593680 batches: 0.1957
trigger times: 0
Loss after 650724780 batches: 0.0587
trigger times: 0
Loss after 650855880 batches: 0.0455
trigger times: 1
Loss after 650986980 batches: 0.0412
trigger times: 2
Loss after 651118080 batches: 0.0378
trigger times: 3
Loss after 651249180 batches: 0.0350
trigger times: 4
Loss after 651380280 batches: 0.0340
trigger times: 5
Loss after 651511380 batches: 0.0326
trigger times: 6
Loss after 651642480 batches: 0.0311
trigger times: 7
Loss after 651773580 batches: 0.0305
trigger times: 8
Loss after 651904680 batches: 0.0294
trigger times: 9
Loss after 652035780 batches: 0.0288
trigger times: 10
Loss after 652166880 batches: 0.0282
trigger times: 11
Loss after 652297980 batches: 0.0279
trigger times: 12
Loss after 652429080 batches: 0.0272
trigger times: 13
Loss after 652560180 batches: 0.0267
trigger times: 14
Loss after 652691280 batches: 0.0262
trigger times: 15
Loss after 652822380 batches: 0.0259
trigger times: 16
Loss after 652953480 batches: 0.0258
trigger times: 17
Loss after 653084580 batches: 0.0254
trigger times: 18
Loss after 653215680 batches: 0.0249
trigger times: 19
Loss after 653346780 batches: 0.0247
trigger times: 20
Early stopping!
Start to test process.
Loss after 653477880 batches: 0.0240
Time to train on one home:  174.3699402809143
trigger times: 0
Loss after 653606520 batches: 0.0890
trigger times: 0
Loss after 653735160 batches: 0.0266
trigger times: 0
Loss after 653863800 batches: 0.0205
trigger times: 1
Loss after 653992440 batches: 0.0186
trigger times: 2
Loss after 654121080 batches: 0.0172
trigger times: 3
Loss after 654249720 batches: 0.0163
trigger times: 0
Loss after 654378360 batches: 0.0156
trigger times: 0
Loss after 654507000 batches: 0.0153
trigger times: 1
Loss after 654635640 batches: 0.0148
trigger times: 2
Loss after 654764280 batches: 0.0141
trigger times: 3
Loss after 654892920 batches: 0.0140
trigger times: 4
Loss after 655021560 batches: 0.0139
trigger times: 5
Loss after 655150200 batches: 0.0136
trigger times: 6
Loss after 655278840 batches: 0.0133
trigger times: 7
Loss after 655407480 batches: 0.0129
trigger times: 8
Loss after 655536120 batches: 0.0126
trigger times: 9
Loss after 655664760 batches: 0.0124
trigger times: 10
Loss after 655793400 batches: 0.0125
trigger times: 11
Loss after 655922040 batches: 0.0121
trigger times: 12
Loss after 656050680 batches: 0.0121
trigger times: 13
Loss after 656179320 batches: 0.0120
trigger times: 14
Loss after 656307960 batches: 0.0120
trigger times: 15
Loss after 656436600 batches: 0.0120
trigger times: 16
Loss after 656565240 batches: 0.0116
trigger times: 17
Loss after 656693880 batches: 0.0115
trigger times: 18
Loss after 656822520 batches: 0.0115
trigger times: 19
Loss after 656951160 batches: 0.0113
trigger times: 20
Early stopping!
Start to test process.
Loss after 657079800 batches: 0.0112
Time to train on one home:  206.86361932754517
trigger times: 0
Loss after 657210900 batches: 0.1507
trigger times: 0
Loss after 657342000 batches: 0.0416
trigger times: 1
Loss after 657473100 batches: 0.0332
trigger times: 2
Loss after 657604200 batches: 0.0296
trigger times: 3
Loss after 657735300 batches: 0.0275
trigger times: 4
Loss after 657866400 batches: 0.0260
trigger times: 0
Loss after 657997500 batches: 0.0251
trigger times: 1
Loss after 658128600 batches: 0.0241
trigger times: 0
Loss after 658259700 batches: 0.0232
trigger times: 1
Loss after 658390800 batches: 0.0231
trigger times: 2
Loss after 658521900 batches: 0.0226
trigger times: 3
Loss after 658653000 batches: 0.0221
trigger times: 0
Loss after 658784100 batches: 0.0215
trigger times: 0
Loss after 658915200 batches: 0.0212
trigger times: 1
Loss after 659046300 batches: 0.0212
trigger times: 2
Loss after 659177400 batches: 0.0210
trigger times: 0
Loss after 659308500 batches: 0.0206
trigger times: 1
Loss after 659439600 batches: 0.0202
trigger times: 2
Loss after 659570700 batches: 0.0199
trigger times: 3
Loss after 659701800 batches: 0.0196
trigger times: 4
Loss after 659832900 batches: 0.0194
trigger times: 5
Loss after 659964000 batches: 0.0191
trigger times: 6
Loss after 660095100 batches: 0.0192
trigger times: 7
Loss after 660226200 batches: 0.0193
trigger times: 8
Loss after 660357300 batches: 0.0191
trigger times: 9
Loss after 660488400 batches: 0.0187
trigger times: 10
Loss after 660619500 batches: 0.0190
trigger times: 11
Loss after 660750600 batches: 0.0186
trigger times: 12
Loss after 660881700 batches: 0.0184
trigger times: 0
Loss after 661012800 batches: 0.0183
trigger times: 1
Loss after 661143900 batches: 0.0182
trigger times: 2
Loss after 661275000 batches: 0.0180
trigger times: 3
Loss after 661406100 batches: 0.0177
trigger times: 0
Loss after 661537200 batches: 0.0180
trigger times: 1
Loss after 661668300 batches: 0.0178
trigger times: 2
Loss after 661799400 batches: 0.0176
trigger times: 3
Loss after 661930500 batches: 0.0176
trigger times: 4
Loss after 662061600 batches: 0.0176
trigger times: 5
Loss after 662192700 batches: 0.0174
trigger times: 6
Loss after 662323800 batches: 0.0177
trigger times: 7
Loss after 662454900 batches: 0.0175
trigger times: 8
Loss after 662586000 batches: 0.0171
trigger times: 9
Loss after 662717100 batches: 0.0171
trigger times: 0
Loss after 662848200 batches: 0.0169
trigger times: 1
Loss after 662979300 batches: 0.0169
trigger times: 0
Loss after 663110400 batches: 0.0167
trigger times: 1
Loss after 663241500 batches: 0.0167
trigger times: 2
Loss after 663372600 batches: 0.0168
trigger times: 3
Loss after 663503700 batches: 0.0167
trigger times: 4
Loss after 663634800 batches: 0.0164
trigger times: 5
Loss after 663765900 batches: 0.0165
trigger times: 0
Loss after 663897000 batches: 0.0164
trigger times: 0
Loss after 664028100 batches: 0.0167
trigger times: 1
Loss after 664159200 batches: 0.0162
trigger times: 2
Loss after 664290300 batches: 0.0161
trigger times: 3
Loss after 664421400 batches: 0.0160
trigger times: 4
Loss after 664552500 batches: 0.0161
trigger times: 5
Loss after 664683600 batches: 0.0160
trigger times: 6
Loss after 664814700 batches: 0.0161
trigger times: 0
Loss after 664945800 batches: 0.0160
trigger times: 1
Loss after 665076900 batches: 0.0159
trigger times: 2
Loss after 665208000 batches: 0.0159
trigger times: 3
Loss after 665339100 batches: 0.0158
trigger times: 4
Loss after 665470200 batches: 0.0157
trigger times: 5
Loss after 665601300 batches: 0.0155
trigger times: 6
Loss after 665732400 batches: 0.0157
trigger times: 7
Loss after 665863500 batches: 0.0155
trigger times: 8
Loss after 665994600 batches: 0.0157
trigger times: 9
Loss after 666125700 batches: 0.0152
trigger times: 10
Loss after 666256800 batches: 0.0155
trigger times: 11
Loss after 666387900 batches: 0.0156
trigger times: 12
Loss after 666519000 batches: 0.0152
trigger times: 13
Loss after 666650100 batches: 0.0152
trigger times: 14
Loss after 666781200 batches: 0.0153
trigger times: 15
Loss after 666912300 batches: 0.0152
trigger times: 16
Loss after 667043400 batches: 0.0151
trigger times: 17
Loss after 667174500 batches: 0.0153
trigger times: 18
Loss after 667305600 batches: 0.0150
trigger times: 19
Loss after 667436700 batches: 0.0151
trigger times: 20
Early stopping!
Start to test process.
Loss after 667567800 batches: 0.0149
Time to train on one home:  577.9420123100281
trigger times: 0
Loss after 667698900 batches: 0.1384
trigger times: 1
Loss after 667830000 batches: 0.0483
trigger times: 2
Loss after 667961100 batches: 0.0366
trigger times: 0
Loss after 668092200 batches: 0.0323
trigger times: 0
Loss after 668223300 batches: 0.0298
trigger times: 1
Loss after 668354400 batches: 0.0279
trigger times: 2
Loss after 668485500 batches: 0.0268
trigger times: 0
Loss after 668616600 batches: 0.0256
trigger times: 0
Loss after 668747700 batches: 0.0255
trigger times: 0
Loss after 668878800 batches: 0.0246
trigger times: 0
Loss after 669009900 batches: 0.0237
trigger times: 1
Loss after 669141000 batches: 0.0236
trigger times: 2
Loss after 669272100 batches: 0.0237
trigger times: 3
Loss after 669403200 batches: 0.0224
trigger times: 4
Loss after 669534300 batches: 0.0212
trigger times: 5
Loss after 669665400 batches: 0.0213
trigger times: 6
Loss after 669796500 batches: 0.0211
trigger times: 7
Loss after 669927600 batches: 0.0211
trigger times: 8
Loss after 670058700 batches: 0.0206
trigger times: 9
Loss after 670189800 batches: 0.0210
trigger times: 0
Loss after 670320900 batches: 0.0206
trigger times: 1
Loss after 670452000 batches: 0.0210
trigger times: 0
Loss after 670583100 batches: 0.0198
trigger times: 1
Loss after 670714200 batches: 0.0195
trigger times: 2
Loss after 670845300 batches: 0.0199
trigger times: 3
Loss after 670976400 batches: 0.0196
trigger times: 4
Loss after 671107500 batches: 0.0197
trigger times: 5
Loss after 671238600 batches: 0.0202
trigger times: 6
Loss after 671369700 batches: 0.0199
trigger times: 7
Loss after 671500800 batches: 0.0207
trigger times: 8
Loss after 671631900 batches: 0.0195
trigger times: 9
Loss after 671763000 batches: 0.0182
trigger times: 10
Loss after 671894100 batches: 0.0192
trigger times: 0
Loss after 672025200 batches: 0.0196
trigger times: 1
Loss after 672156300 batches: 0.0196
trigger times: 2
Loss after 672287400 batches: 0.0189
trigger times: 3
Loss after 672418500 batches: 0.0187
trigger times: 4
Loss after 672549600 batches: 0.0196
trigger times: 0
Loss after 672680700 batches: 0.0190
trigger times: 1
Loss after 672811800 batches: 0.0191
trigger times: 2
Loss after 672942900 batches: 0.0183
trigger times: 3
Loss after 673074000 batches: 0.0184
trigger times: 4
Loss after 673205100 batches: 0.0181
trigger times: 5
Loss after 673336200 batches: 0.0181
trigger times: 6
Loss after 673467300 batches: 0.0187
trigger times: 7
Loss after 673598400 batches: 0.0181
trigger times: 8
Loss after 673729500 batches: 0.0184
trigger times: 9
Loss after 673860600 batches: 0.0181
trigger times: 10
Loss after 673991700 batches: 0.0175
trigger times: 11
Loss after 674122800 batches: 0.0175
trigger times: 12
Loss after 674253900 batches: 0.0172
trigger times: 13
Loss after 674385000 batches: 0.0168
trigger times: 14
Loss after 674516100 batches: 0.0173
trigger times: 15
Loss after 674647200 batches: 0.0183
trigger times: 16
Loss after 674778300 batches: 0.0174
trigger times: 17
Loss after 674909400 batches: 0.0171
trigger times: 18
Loss after 675040500 batches: 0.0171
trigger times: 19
Loss after 675171600 batches: 0.0178
trigger times: 20
Early stopping!
Start to test process.
Loss after 675302700 batches: 0.0173
Time to train on one home:  428.63509225845337
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887, 0.018152045403586472, 0.017512356438894854, 0.016091731943637777, 0.01601899988306293, 0.015690088165884984, 0.01509131066983643, 0.015631624726830377, 0.015631806442569732, 0.01489884440801886]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884], [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987], [0.5804732673698001, 0.3723540197761146, 0.45230155418961127, 1.0682024720651153, 0.514164189884338, 25.236666271720736, 1587.2568], [0.5693913300832113, 0.3843477905525722, 0.4653277545238437, 1.0599366084402062, 0.5043389577801884, 25.04138228090006, 1556.9258], [0.5662658578819699, 0.38771059287123977, 0.46820783457394766, 1.0579149875211409, 0.5015841683218024, 24.993620762090103, 1548.4215], [0.5593101216687096, 0.3952721834915385, 0.47154332581377506, 1.051822130314305, 0.49538975421253684, 24.84967482675361, 1529.299], [0.5649060971207089, 0.3891939618618432, 0.4673667174574867, 1.0599116662790036, 0.5003690004733905, 25.04079301340178, 1544.6703], [0.5509053071339926, 0.40439992136570346, 0.47811920698222654, 1.044031858684061, 0.48791236074962885, 24.665626867270383, 1506.2158], [0.5444808701674143, 0.41136524952900755, 0.4884585927112777, 1.0362181287288137, 0.4822064016178801, 24.481024696450934, 1488.6012], [0.5505051612854004, 0.4048088230706274, 0.48190953965979827, 1.0477392844571842, 0.48757739068612305, 24.753216129990033, 1505.1818]]
Round_16_results:  [0.5505051612854004, 0.4048088230706274, 0.48190953965979827, 1.0477392844571842, 0.48757739068612305, 24.753216129990033, 1505.1818]
trigger times: 0
Loss after 675433800 batches: 0.0804
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5384 < 5385; dropping {'Training_Loss': 0.08035061023426505, 'Validation_Loss': 0.22249128917853037, 'Training_R2': 0.9190449121361817, 'Validation_R2': 0.7934245466550116, 'Training_F1': 0.8474123633171604, 'Validation_F1': 0.7485286508744581, 'Training_NEP': 0.3051516831823586, 'Validation_NEP': 0.4959121765375842, 'Training_NDE': 0.060774421414405075, 'Validation_NDE': 0.1645058016028711, 'Training_MAE': 10.106018565954097, 'Validation_MAE': 13.599969294250652, 'Training_MSE': 267.3979, 'Validation_MSE': 607.51544}.
trigger times: 0
Loss after 675564900 batches: 0.0211
trigger times: 0
Loss after 675696000 batches: 0.0163
trigger times: 1
Loss after 675827100 batches: 0.0142
trigger times: 0
Loss after 675958200 batches: 0.0134
trigger times: 1
Loss after 676089300 batches: 0.0124
trigger times: 0
Loss after 676220400 batches: 0.0121
trigger times: 0
Loss after 676351500 batches: 0.0114
trigger times: 0
Loss after 676482600 batches: 0.0113
trigger times: 1
Loss after 676613700 batches: 0.0109
trigger times: 2
Loss after 676744800 batches: 0.0105
trigger times: 3
Loss after 676875900 batches: 0.0104
trigger times: 4
Loss after 677007000 batches: 0.0101
trigger times: 0
Loss after 677138100 batches: 0.0101
trigger times: 1
Loss after 677269200 batches: 0.0099
trigger times: 2
Loss after 677400300 batches: 0.0097
trigger times: 3
Loss after 677531400 batches: 0.0095
trigger times: 4
Loss after 677662500 batches: 0.0093
trigger times: 0
Loss after 677793600 batches: 0.0094
trigger times: 1
Loss after 677924700 batches: 0.0092
trigger times: 2
Loss after 678055800 batches: 0.0092
trigger times: 3
Loss after 678186900 batches: 0.0093
trigger times: 4
Loss after 678318000 batches: 0.0087
trigger times: 5
Loss after 678449100 batches: 0.0088
trigger times: 6
Loss after 678580200 batches: 0.0090
trigger times: 7
Loss after 678711300 batches: 0.0088
trigger times: 8
Loss after 678842400 batches: 0.0087
trigger times: 9
Loss after 678973500 batches: 0.0084
trigger times: 10
Loss after 679104600 batches: 0.0086
trigger times: 11
Loss after 679235700 batches: 0.0084
trigger times: 12
Loss after 679366800 batches: 0.0086
trigger times: 13
Loss after 679497900 batches: 0.0081
trigger times: 14
Loss after 679629000 batches: 0.0084
trigger times: 15
Loss after 679760100 batches: 0.0083
trigger times: 16
Loss after 679891200 batches: 0.0081
trigger times: 17
Loss after 680022300 batches: 0.0080
trigger times: 18
Loss after 680153400 batches: 0.0081
trigger times: 19
Loss after 680284500 batches: 0.0079
trigger times: 20
Early stopping!
Start to test process.
Loss after 680415600 batches: 0.0079
Time to train on one home:  287.60518503189087
trigger times: 0
Loss after 680518200 batches: 0.1199
trigger times: 0
Loss after 680620800 batches: 0.0422
trigger times: 0
Loss after 680723400 batches: 0.0307
trigger times: 0
Loss after 680826000 batches: 0.0269
trigger times: 0
Loss after 680928600 batches: 0.0276
trigger times: 1
Loss after 681031200 batches: 0.0232
trigger times: 2
Loss after 681133800 batches: 0.0222
trigger times: 3
Loss after 681236400 batches: 0.0207
trigger times: 4
Loss after 681339000 batches: 0.0212
trigger times: 5
Loss after 681441600 batches: 0.0238
trigger times: 0
Loss after 681544200 batches: 0.0216
trigger times: 1
Loss after 681646800 batches: 0.0193
trigger times: 2
Loss after 681749400 batches: 0.0196
trigger times: 3
Loss after 681852000 batches: 0.0186
trigger times: 4
Loss after 681954600 batches: 0.0184
trigger times: 5
Loss after 682057200 batches: 0.0176
trigger times: 6
Loss after 682159800 batches: 0.0171
trigger times: 7
Loss after 682262400 batches: 0.0168
trigger times: 8
Loss after 682365000 batches: 0.0170
trigger times: 0
Loss after 682467600 batches: 0.0168
trigger times: 0
Loss after 682570200 batches: 0.0180
trigger times: 1
Loss after 682672800 batches: 0.0183
trigger times: 2
Loss after 682775400 batches: 0.0184
trigger times: 0
Loss after 682878000 batches: 0.0164
trigger times: 1
Loss after 682980600 batches: 0.0160
trigger times: 2
Loss after 683083200 batches: 0.0155
trigger times: 3
Loss after 683185800 batches: 0.0152
trigger times: 4
Loss after 683288400 batches: 0.0152
trigger times: 5
Loss after 683391000 batches: 0.0173
trigger times: 6
Loss after 683493600 batches: 0.0153
trigger times: 7
Loss after 683596200 batches: 0.0157
trigger times: 8
Loss after 683698800 batches: 0.0163
trigger times: 9
Loss after 683801400 batches: 0.0155
trigger times: 10
Loss after 683904000 batches: 0.0169
trigger times: 11
Loss after 684006600 batches: 0.0169
trigger times: 12
Loss after 684109200 batches: 0.0178
trigger times: 13
Loss after 684211800 batches: 0.0162
trigger times: 14
Loss after 684314400 batches: 0.0151
trigger times: 15
Loss after 684417000 batches: 0.0154
trigger times: 16
Loss after 684519600 batches: 0.0148
trigger times: 17
Loss after 684622200 batches: 0.0147
trigger times: 18
Loss after 684724800 batches: 0.0155
trigger times: 19
Loss after 684827400 batches: 0.0142
trigger times: 20
Early stopping!
Start to test process.
Loss after 684930000 batches: 0.0153
Time to train on one home:  264.1491115093231
trigger times: 0
Loss after 685061100 batches: 0.1020
trigger times: 0
Loss after 685192200 batches: 0.0355
trigger times: 1
Loss after 685323300 batches: 0.0279
trigger times: 2
Loss after 685454400 batches: 0.0247
trigger times: 3
Loss after 685585500 batches: 0.0232
trigger times: 4
Loss after 685716600 batches: 0.0223
trigger times: 5
Loss after 685847700 batches: 0.0212
trigger times: 6
Loss after 685978800 batches: 0.0203
trigger times: 7
Loss after 686109900 batches: 0.0197
trigger times: 8
Loss after 686241000 batches: 0.0192
trigger times: 9
Loss after 686372100 batches: 0.0189
trigger times: 10
Loss after 686503200 batches: 0.0181
trigger times: 11
Loss after 686634300 batches: 0.0177
trigger times: 12
Loss after 686765400 batches: 0.0177
trigger times: 13
Loss after 686896500 batches: 0.0173
trigger times: 14
Loss after 687027600 batches: 0.0171
trigger times: 15
Loss after 687158700 batches: 0.0170
trigger times: 16
Loss after 687289800 batches: 0.0166
trigger times: 17
Loss after 687420900 batches: 0.0162
trigger times: 18
Loss after 687552000 batches: 0.0161
trigger times: 19
Loss after 687683100 batches: 0.0161
trigger times: 20
Early stopping!
Start to test process.
Loss after 687814200 batches: 0.0157
Time to train on one home:  167.1015830039978
trigger times: 0
Loss after 687945300 batches: 0.1970
trigger times: 1
Loss after 688076400 batches: 0.0576
trigger times: 0
Loss after 688207500 batches: 0.0456
trigger times: 1
Loss after 688338600 batches: 0.0404
trigger times: 2
Loss after 688469700 batches: 0.0372
trigger times: 3
Loss after 688600800 batches: 0.0354
trigger times: 4
Loss after 688731900 batches: 0.0342
trigger times: 5
Loss after 688863000 batches: 0.0320
trigger times: 0
Loss after 688994100 batches: 0.0315
trigger times: 0
Loss after 689125200 batches: 0.0304
trigger times: 1
Loss after 689256300 batches: 0.0292
trigger times: 2
Loss after 689387400 batches: 0.0289
trigger times: 3
Loss after 689518500 batches: 0.0282
trigger times: 4
Loss after 689649600 batches: 0.0278
trigger times: 5
Loss after 689780700 batches: 0.0270
trigger times: 6
Loss after 689911800 batches: 0.0266
trigger times: 7
Loss after 690042900 batches: 0.0258
trigger times: 8
Loss after 690174000 batches: 0.0256
trigger times: 9
Loss after 690305100 batches: 0.0252
trigger times: 10
Loss after 690436200 batches: 0.0249
trigger times: 11
Loss after 690567300 batches: 0.0245
trigger times: 12
Loss after 690698400 batches: 0.0242
trigger times: 13
Loss after 690829500 batches: 0.0242
trigger times: 14
Loss after 690960600 batches: 0.0238
trigger times: 15
Loss after 691091700 batches: 0.0234
trigger times: 16
Loss after 691222800 batches: 0.0233
trigger times: 17
Loss after 691353900 batches: 0.0230
trigger times: 18
Loss after 691485000 batches: 0.0226
trigger times: 19
Loss after 691616100 batches: 0.0228
trigger times: 20
Early stopping!
Start to test process.
Loss after 691747200 batches: 0.0227
Time to train on one home:  223.55525255203247
trigger times: 0
Loss after 691875840 batches: 0.0910
trigger times: 0
Loss after 692004480 batches: 0.0260
trigger times: 0
Loss after 692133120 batches: 0.0204
trigger times: 1
Loss after 692261760 batches: 0.0180
trigger times: 2
Loss after 692390400 batches: 0.0170
trigger times: 0
Loss after 692519040 batches: 0.0163
trigger times: 0
Loss after 692647680 batches: 0.0155
trigger times: 0
Loss after 692776320 batches: 0.0151
trigger times: 1
Loss after 692904960 batches: 0.0146
trigger times: 2
Loss after 693033600 batches: 0.0142
trigger times: 3
Loss after 693162240 batches: 0.0137
trigger times: 0
Loss after 693290880 batches: 0.0134
trigger times: 1
Loss after 693419520 batches: 0.0133
trigger times: 2
Loss after 693548160 batches: 0.0131
trigger times: 3
Loss after 693676800 batches: 0.0129
trigger times: 4
Loss after 693805440 batches: 0.0128
trigger times: 5
Loss after 693934080 batches: 0.0125
trigger times: 6
Loss after 694062720 batches: 0.0124
trigger times: 7
Loss after 694191360 batches: 0.0122
trigger times: 8
Loss after 694320000 batches: 0.0120
trigger times: 9
Loss after 694448640 batches: 0.0120
trigger times: 10
Loss after 694577280 batches: 0.0120
trigger times: 11
Loss after 694705920 batches: 0.0119
trigger times: 12
Loss after 694834560 batches: 0.0114
trigger times: 13
Loss after 694963200 batches: 0.0114
trigger times: 14
Loss after 695091840 batches: 0.0114
trigger times: 15
Loss after 695220480 batches: 0.0110
trigger times: 16
Loss after 695349120 batches: 0.0111
trigger times: 17
Loss after 695477760 batches: 0.0109
trigger times: 18
Loss after 695606400 batches: 0.0109
trigger times: 19
Loss after 695735040 batches: 0.0108
trigger times: 20
Early stopping!
Start to test process.
Loss after 695863680 batches: 0.0111
Time to train on one home:  234.201997756958
trigger times: 0
Loss after 695994780 batches: 0.1407
trigger times: 0
Loss after 696125880 batches: 0.0400
trigger times: 1
Loss after 696256980 batches: 0.0309
trigger times: 2
Loss after 696388080 batches: 0.0279
trigger times: 3
Loss after 696519180 batches: 0.0262
trigger times: 4
Loss after 696650280 batches: 0.0252
trigger times: 5
Loss after 696781380 batches: 0.0236
trigger times: 6
Loss after 696912480 batches: 0.0234
trigger times: 7
Loss after 697043580 batches: 0.0224
trigger times: 8
Loss after 697174680 batches: 0.0218
trigger times: 9
Loss after 697305780 batches: 0.0216
trigger times: 10
Loss after 697436880 batches: 0.0211
trigger times: 11
Loss after 697567980 batches: 0.0209
trigger times: 12
Loss after 697699080 batches: 0.0207
trigger times: 13
Loss after 697830180 batches: 0.0201
trigger times: 14
Loss after 697961280 batches: 0.0196
trigger times: 15
Loss after 698092380 batches: 0.0197
trigger times: 16
Loss after 698223480 batches: 0.0193
trigger times: 17
Loss after 698354580 batches: 0.0196
trigger times: 18
Loss after 698485680 batches: 0.0191
trigger times: 19
Loss after 698616780 batches: 0.0188
trigger times: 20
Early stopping!
Start to test process.
Loss after 698747880 batches: 0.0186
Time to train on one home:  166.9121916294098
trigger times: 0
Loss after 698878980 batches: 0.1418
trigger times: 0
Loss after 699010080 batches: 0.0509
trigger times: 1
Loss after 699141180 batches: 0.0368
trigger times: 0
Loss after 699272280 batches: 0.0312
trigger times: 0
Loss after 699403380 batches: 0.0286
trigger times: 0
Loss after 699534480 batches: 0.0270
trigger times: 0
Loss after 699665580 batches: 0.0254
trigger times: 0
Loss after 699796680 batches: 0.0245
trigger times: 1
Loss after 699927780 batches: 0.0239
trigger times: 2
Loss after 700058880 batches: 0.0242
trigger times: 0
Loss after 700189980 batches: 0.0233
trigger times: 1
Loss after 700321080 batches: 0.0231
trigger times: 2
Loss after 700452180 batches: 0.0226
trigger times: 3
Loss after 700583280 batches: 0.0221
trigger times: 0
Loss after 700714380 batches: 0.0216
trigger times: 1
Loss after 700845480 batches: 0.0213
trigger times: 0
Loss after 700976580 batches: 0.0214
trigger times: 1
Loss after 701107680 batches: 0.0210
trigger times: 2
Loss after 701238780 batches: 0.0204
trigger times: 3
Loss after 701369880 batches: 0.0206
trigger times: 4
Loss after 701500980 batches: 0.0203
trigger times: 5
Loss after 701632080 batches: 0.0202
trigger times: 6
Loss after 701763180 batches: 0.0207
trigger times: 7
Loss after 701894280 batches: 0.0208
trigger times: 8
Loss after 702025380 batches: 0.0191
trigger times: 9
Loss after 702156480 batches: 0.0189
trigger times: 10
Loss after 702287580 batches: 0.0194
trigger times: 0
Loss after 702418680 batches: 0.0195
trigger times: 1
Loss after 702549780 batches: 0.0193
trigger times: 2
Loss after 702680880 batches: 0.0189
trigger times: 3
Loss after 702811980 batches: 0.0190
trigger times: 4
Loss after 702943080 batches: 0.0193
trigger times: 5
Loss after 703074180 batches: 0.0198
trigger times: 6
Loss after 703205280 batches: 0.0193
trigger times: 7
Loss after 703336380 batches: 0.0182
trigger times: 8
Loss after 703467480 batches: 0.0180
trigger times: 9
Loss after 703598580 batches: 0.0181
trigger times: 10
Loss after 703729680 batches: 0.0183
trigger times: 11
Loss after 703860780 batches: 0.0187
trigger times: 0
Loss after 703991880 batches: 0.0186
trigger times: 1
Loss after 704122980 batches: 0.0182
trigger times: 2
Loss after 704254080 batches: 0.0175
trigger times: 3
Loss after 704385180 batches: 0.0179
trigger times: 4
Loss after 704516280 batches: 0.0173
trigger times: 5
Loss after 704647380 batches: 0.0178
trigger times: 6
Loss after 704778480 batches: 0.0179
trigger times: 7
Loss after 704909580 batches: 0.0176
trigger times: 8
Loss after 705040680 batches: 0.0178
trigger times: 9
Loss after 705171780 batches: 0.0182
trigger times: 10
Loss after 705302880 batches: 0.0178
trigger times: 11
Loss after 705433980 batches: 0.0172
trigger times: 12
Loss after 705565080 batches: 0.0174
trigger times: 13
Loss after 705696180 batches: 0.0169
trigger times: 14
Loss after 705827280 batches: 0.0176
trigger times: 15
Loss after 705958380 batches: 0.0173
trigger times: 16
Loss after 706089480 batches: 0.0167
trigger times: 17
Loss after 706220580 batches: 0.0167
trigger times: 18
Loss after 706351680 batches: 0.0174
trigger times: 19
Loss after 706482780 batches: 0.0169
trigger times: 20
Early stopping!
Start to test process.
Loss after 706613880 batches: 0.0171
Time to train on one home:  435.71342039108276
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887, 0.018152045403586472, 0.017512356438894854, 0.016091731943637777, 0.01601899988306293, 0.015690088165884984, 0.01509131066983643, 0.015631624726830377, 0.015631806442569732, 0.01489884440801886, 0.015460750691055835]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884], [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987], [0.5804732673698001, 0.3723540197761146, 0.45230155418961127, 1.0682024720651153, 0.514164189884338, 25.236666271720736, 1587.2568], [0.5693913300832113, 0.3843477905525722, 0.4653277545238437, 1.0599366084402062, 0.5043389577801884, 25.04138228090006, 1556.9258], [0.5662658578819699, 0.38771059287123977, 0.46820783457394766, 1.0579149875211409, 0.5015841683218024, 24.993620762090103, 1548.4215], [0.5593101216687096, 0.3952721834915385, 0.47154332581377506, 1.051822130314305, 0.49538975421253684, 24.84967482675361, 1529.299], [0.5649060971207089, 0.3891939618618432, 0.4673667174574867, 1.0599116662790036, 0.5003690004733905, 25.04079301340178, 1544.6703], [0.5509053071339926, 0.40439992136570346, 0.47811920698222654, 1.044031858684061, 0.48791236074962885, 24.665626867270383, 1506.2158], [0.5444808701674143, 0.41136524952900755, 0.4884585927112777, 1.0362181287288137, 0.4822064016178801, 24.481024696450934, 1488.6012], [0.5505051612854004, 0.4048088230706274, 0.48190953965979827, 1.0477392844571842, 0.48757739068612305, 24.753216129990033, 1505.1818], [0.53266022933854, 0.4241441989768663, 0.5002133610335463, 1.022931215967613, 0.47173795539587554, 24.16711662012189, 1456.2844]]
Round_17_results:  [0.53266022933854, 0.4241441989768663, 0.5002133610335463, 1.022931215967613, 0.47173795539587554, 24.16711662012189, 1456.2844]
trigger times: 0
Loss after 706744980 batches: 0.0815
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5633 < 5634; dropping {'Training_Loss': 0.08150953050913676, 'Validation_Loss': 0.23303252127435473, 'Training_R2': 0.9178528715544232, 'Validation_R2': 0.7835603524577044, 'Training_F1': 0.8463794870581428, 'Validation_F1': 0.7469792175914284, 'Training_NEP': 0.30708069559250445, 'Validation_NEP': 0.49885660792471137, 'Training_NDE': 0.061669307437884326, 'Validation_NDE': 0.1723611258793929, 'Training_MAE': 10.169903631333993, 'Validation_MAE': 13.680717818583211, 'Training_MSE': 271.33533, 'Validation_MSE': 636.5249}.
trigger times: 0
Loss after 706876080 batches: 0.0207
trigger times: 1
Loss after 707007180 batches: 0.0161
trigger times: 0
Loss after 707138280 batches: 0.0146
trigger times: 1
Loss after 707269380 batches: 0.0131
trigger times: 0
Loss after 707400480 batches: 0.0126
trigger times: 0
Loss after 707531580 batches: 0.0118
trigger times: 1
Loss after 707662680 batches: 0.0114
trigger times: 2
Loss after 707793780 batches: 0.0112
trigger times: 3
Loss after 707924880 batches: 0.0110
trigger times: 4
Loss after 708055980 batches: 0.0107
trigger times: 0
Loss after 708187080 batches: 0.0105
trigger times: 1
Loss after 708318180 batches: 0.0103
trigger times: 2
Loss after 708449280 batches: 0.0099
trigger times: 3
Loss after 708580380 batches: 0.0098
trigger times: 0
Loss after 708711480 batches: 0.0098
trigger times: 1
Loss after 708842580 batches: 0.0096
trigger times: 2
Loss after 708973680 batches: 0.0093
trigger times: 3
Loss after 709104780 batches: 0.0094
trigger times: 4
Loss after 709235880 batches: 0.0092
trigger times: 5
Loss after 709366980 batches: 0.0089
trigger times: 6
Loss after 709498080 batches: 0.0090
trigger times: 0
Loss after 709629180 batches: 0.0090
trigger times: 1
Loss after 709760280 batches: 0.0089
trigger times: 2
Loss after 709891380 batches: 0.0088
trigger times: 3
Loss after 710022480 batches: 0.0088
trigger times: 4
Loss after 710153580 batches: 0.0088
trigger times: 5
Loss after 710284680 batches: 0.0087
trigger times: 6
Loss after 710415780 batches: 0.0085
trigger times: 7
Loss after 710546880 batches: 0.0087
trigger times: 8
Loss after 710677980 batches: 0.0085
trigger times: 9
Loss after 710809080 batches: 0.0085
trigger times: 10
Loss after 710940180 batches: 0.0084
trigger times: 11
Loss after 711071280 batches: 0.0083
trigger times: 12
Loss after 711202380 batches: 0.0083
trigger times: 13
Loss after 711333480 batches: 0.0081
trigger times: 14
Loss after 711464580 batches: 0.0082
trigger times: 15
Loss after 711595680 batches: 0.0081
trigger times: 16
Loss after 711726780 batches: 0.0081
trigger times: 17
Loss after 711857880 batches: 0.0081
trigger times: 18
Loss after 711988980 batches: 0.0083
trigger times: 19
Loss after 712120080 batches: 0.0079
trigger times: 20
Early stopping!
Start to test process.
Loss after 712251180 batches: 0.0079
Time to train on one home:  315.6715829372406
trigger times: 0
Loss after 712353780 batches: 0.1085
trigger times: 0
Loss after 712456380 batches: 0.0437
trigger times: 1
Loss after 712558980 batches: 0.0318
trigger times: 2
Loss after 712661580 batches: 0.0279
trigger times: 3
Loss after 712764180 batches: 0.0240
trigger times: 4
Loss after 712866780 batches: 0.0216
trigger times: 5
Loss after 712969380 batches: 0.0225
trigger times: 6
Loss after 713071980 batches: 0.0211
trigger times: 7
Loss after 713174580 batches: 0.0201
trigger times: 0
Loss after 713277180 batches: 0.0199
trigger times: 1
Loss after 713379780 batches: 0.0201
trigger times: 2
Loss after 713482380 batches: 0.0191
trigger times: 3
Loss after 713584980 batches: 0.0193
trigger times: 4
Loss after 713687580 batches: 0.0190
trigger times: 5
Loss after 713790180 batches: 0.0181
trigger times: 6
Loss after 713892780 batches: 0.0201
trigger times: 7
Loss after 713995380 batches: 0.0188
trigger times: 8
Loss after 714097980 batches: 0.0176
trigger times: 9
Loss after 714200580 batches: 0.0170
trigger times: 0
Loss after 714303180 batches: 0.0170
trigger times: 1
Loss after 714405780 batches: 0.0171
trigger times: 0
Loss after 714508380 batches: 0.0164
trigger times: 1
Loss after 714610980 batches: 0.0165
trigger times: 2
Loss after 714713580 batches: 0.0161
trigger times: 3
Loss after 714816180 batches: 0.0161
trigger times: 4
Loss after 714918780 batches: 0.0176
trigger times: 5
Loss after 715021380 batches: 0.0173
trigger times: 6
Loss after 715123980 batches: 0.0158
trigger times: 7
Loss after 715226580 batches: 0.0157
trigger times: 8
Loss after 715329180 batches: 0.0160
trigger times: 9
Loss after 715431780 batches: 0.0167
trigger times: 10
Loss after 715534380 batches: 0.0167
trigger times: 11
Loss after 715636980 batches: 0.0156
trigger times: 0
Loss after 715739580 batches: 0.0159
trigger times: 1
Loss after 715842180 batches: 0.0160
trigger times: 2
Loss after 715944780 batches: 0.0156
trigger times: 3
Loss after 716047380 batches: 0.0155
trigger times: 4
Loss after 716149980 batches: 0.0154
trigger times: 5
Loss after 716252580 batches: 0.0144
trigger times: 6
Loss after 716355180 batches: 0.0138
trigger times: 7
Loss after 716457780 batches: 0.0143
trigger times: 8
Loss after 716560380 batches: 0.0148
trigger times: 9
Loss after 716662980 batches: 0.0166
trigger times: 10
Loss after 716765580 batches: 0.0141
trigger times: 11
Loss after 716868180 batches: 0.0144
trigger times: 12
Loss after 716970780 batches: 0.0141
trigger times: 13
Loss after 717073380 batches: 0.0138
trigger times: 0
Loss after 717175980 batches: 0.0135
trigger times: 1
Loss after 717278580 batches: 0.0135
trigger times: 2
Loss after 717381180 batches: 0.0137
trigger times: 3
Loss after 717483780 batches: 0.0138
trigger times: 4
Loss after 717586380 batches: 0.0138
trigger times: 5
Loss after 717688980 batches: 0.0137
trigger times: 6
Loss after 717791580 batches: 0.0139
trigger times: 7
Loss after 717894180 batches: 0.0141
trigger times: 8
Loss after 717996780 batches: 0.0138
trigger times: 9
Loss after 718099380 batches: 0.0134
trigger times: 10
Loss after 718201980 batches: 0.0129
trigger times: 11
Loss after 718304580 batches: 0.0138
trigger times: 12
Loss after 718407180 batches: 0.0139
trigger times: 0
Loss after 718509780 batches: 0.0140
trigger times: 1
Loss after 718612380 batches: 0.0134
trigger times: 2
Loss after 718714980 batches: 0.0133
trigger times: 3
Loss after 718817580 batches: 0.0136
trigger times: 4
Loss after 718920180 batches: 0.0136
trigger times: 5
Loss after 719022780 batches: 0.0145
trigger times: 6
Loss after 719125380 batches: 0.0136
trigger times: 7
Loss after 719227980 batches: 0.0132
trigger times: 8
Loss after 719330580 batches: 0.0128
trigger times: 9
Loss after 719433180 batches: 0.0132
trigger times: 10
Loss after 719535780 batches: 0.0138
trigger times: 11
Loss after 719638380 batches: 0.0128
trigger times: 12
Loss after 719740980 batches: 0.0133
trigger times: 13
Loss after 719843580 batches: 0.0140
trigger times: 14
Loss after 719946180 batches: 0.0132
trigger times: 15
Loss after 720048780 batches: 0.0133
trigger times: 16
Loss after 720151380 batches: 0.0136
trigger times: 17
Loss after 720253980 batches: 0.0127
trigger times: 0
Loss after 720356580 batches: 0.0123
trigger times: 1
Loss after 720459180 batches: 0.0125
trigger times: 2
Loss after 720561780 batches: 0.0125
trigger times: 3
Loss after 720664380 batches: 0.0148
trigger times: 4
Loss after 720766980 batches: 0.0133
trigger times: 5
Loss after 720869580 batches: 0.0128
trigger times: 6
Loss after 720972180 batches: 0.0130
trigger times: 7
Loss after 721074780 batches: 0.0135
trigger times: 8
Loss after 721177380 batches: 0.0125
trigger times: 9
Loss after 721279980 batches: 0.0120
trigger times: 10
Loss after 721382580 batches: 0.0127
trigger times: 11
Loss after 721485180 batches: 0.0124
trigger times: 12
Loss after 721587780 batches: 0.0125
trigger times: 13
Loss after 721690380 batches: 0.0123
trigger times: 14
Loss after 721792980 batches: 0.0121
trigger times: 15
Loss after 721895580 batches: 0.0117
trigger times: 16
Loss after 721998180 batches: 0.0124
trigger times: 17
Loss after 722100780 batches: 0.0124
trigger times: 18
Loss after 722203380 batches: 0.0126
trigger times: 19
Loss after 722305980 batches: 0.0120
trigger times: 20
Early stopping!
Start to test process.
Loss after 722408580 batches: 0.0124
Time to train on one home:  573.5316495895386
trigger times: 0
Loss after 722539680 batches: 0.0905
trigger times: 0
Loss after 722670780 batches: 0.0337
trigger times: 1
Loss after 722801880 batches: 0.0272
trigger times: 0
Loss after 722932980 batches: 0.0247
trigger times: 1
Loss after 723064080 batches: 0.0228
trigger times: 2
Loss after 723195180 batches: 0.0219
trigger times: 3
Loss after 723326280 batches: 0.0208
trigger times: 4
Loss after 723457380 batches: 0.0202
trigger times: 5
Loss after 723588480 batches: 0.0195
trigger times: 6
Loss after 723719580 batches: 0.0189
trigger times: 7
Loss after 723850680 batches: 0.0187
trigger times: 8
Loss after 723981780 batches: 0.0181
trigger times: 9
Loss after 724112880 batches: 0.0177
trigger times: 10
Loss after 724243980 batches: 0.0175
trigger times: 11
Loss after 724375080 batches: 0.0172
trigger times: 12
Loss after 724506180 batches: 0.0170
trigger times: 13
Loss after 724637280 batches: 0.0168
trigger times: 14
Loss after 724768380 batches: 0.0165
trigger times: 15
Loss after 724899480 batches: 0.0163
trigger times: 16
Loss after 725030580 batches: 0.0163
trigger times: 17
Loss after 725161680 batches: 0.0159
trigger times: 18
Loss after 725292780 batches: 0.0158
trigger times: 19
Loss after 725423880 batches: 0.0156
trigger times: 20
Early stopping!
Start to test process.
Loss after 725554980 batches: 0.0153
Time to train on one home:  181.43233704566956
trigger times: 0
Loss after 725686080 batches: 0.1801
trigger times: 0
Loss after 725817180 batches: 0.0559
trigger times: 1
Loss after 725948280 batches: 0.0433
trigger times: 2
Loss after 726079380 batches: 0.0385
trigger times: 3
Loss after 726210480 batches: 0.0360
trigger times: 0
Loss after 726341580 batches: 0.0344
trigger times: 1
Loss after 726472680 batches: 0.0327
trigger times: 2
Loss after 726603780 batches: 0.0313
trigger times: 3
Loss after 726734880 batches: 0.0303
trigger times: 4
Loss after 726865980 batches: 0.0294
trigger times: 5
Loss after 726997080 batches: 0.0284
trigger times: 0
Loss after 727128180 batches: 0.0279
trigger times: 1
Loss after 727259280 batches: 0.0276
trigger times: 2
Loss after 727390380 batches: 0.0271
trigger times: 3
Loss after 727521480 batches: 0.0268
trigger times: 4
Loss after 727652580 batches: 0.0260
trigger times: 5
Loss after 727783680 batches: 0.0253
trigger times: 6
Loss after 727914780 batches: 0.0249
trigger times: 7
Loss after 728045880 batches: 0.0245
trigger times: 8
Loss after 728176980 batches: 0.0240
trigger times: 9
Loss after 728308080 batches: 0.0241
trigger times: 10
Loss after 728439180 batches: 0.0241
trigger times: 11
Loss after 728570280 batches: 0.0238
trigger times: 12
Loss after 728701380 batches: 0.0231
trigger times: 13
Loss after 728832480 batches: 0.0231
trigger times: 14
Loss after 728963580 batches: 0.0228
trigger times: 15
Loss after 729094680 batches: 0.0227
trigger times: 16
Loss after 729225780 batches: 0.0225
trigger times: 17
Loss after 729356880 batches: 0.0226
trigger times: 18
Loss after 729487980 batches: 0.0222
trigger times: 19
Loss after 729619080 batches: 0.0218
trigger times: 20
Early stopping!
Start to test process.
Loss after 729750180 batches: 0.0217
Time to train on one home:  238.48735737800598
trigger times: 0
Loss after 729878820 batches: 0.0816
trigger times: 0
Loss after 730007460 batches: 0.0254
trigger times: 0
Loss after 730136100 batches: 0.0196
trigger times: 0
Loss after 730264740 batches: 0.0179
trigger times: 1
Loss after 730393380 batches: 0.0168
trigger times: 0
Loss after 730522020 batches: 0.0158
trigger times: 1
Loss after 730650660 batches: 0.0151
trigger times: 2
Loss after 730779300 batches: 0.0148
trigger times: 3
Loss after 730907940 batches: 0.0141
trigger times: 4
Loss after 731036580 batches: 0.0139
trigger times: 5
Loss after 731165220 batches: 0.0137
trigger times: 0
Loss after 731293860 batches: 0.0133
trigger times: 1
Loss after 731422500 batches: 0.0133
trigger times: 2
Loss after 731551140 batches: 0.0131
trigger times: 3
Loss after 731679780 batches: 0.0126
trigger times: 4
Loss after 731808420 batches: 0.0125
trigger times: 5
Loss after 731937060 batches: 0.0123
trigger times: 6
Loss after 732065700 batches: 0.0125
trigger times: 7
Loss after 732194340 batches: 0.0122
trigger times: 0
Loss after 732322980 batches: 0.0118
trigger times: 1
Loss after 732451620 batches: 0.0117
trigger times: 2
Loss after 732580260 batches: 0.0116
trigger times: 3
Loss after 732708900 batches: 0.0116
trigger times: 4
Loss after 732837540 batches: 0.0112
trigger times: 5
Loss after 732966180 batches: 0.0111
trigger times: 6
Loss after 733094820 batches: 0.0111
trigger times: 7
Loss after 733223460 batches: 0.0111
trigger times: 8
Loss after 733352100 batches: 0.0110
trigger times: 9
Loss after 733480740 batches: 0.0108
trigger times: 10
Loss after 733609380 batches: 0.0108
trigger times: 11
Loss after 733738020 batches: 0.0108
trigger times: 12
Loss after 733866660 batches: 0.0107
trigger times: 13
Loss after 733995300 batches: 0.0106
trigger times: 14
Loss after 734123940 batches: 0.0106
trigger times: 15
Loss after 734252580 batches: 0.0105
trigger times: 16
Loss after 734381220 batches: 0.0103
trigger times: 17
Loss after 734509860 batches: 0.0104
trigger times: 18
Loss after 734638500 batches: 0.0104
trigger times: 19
Loss after 734767140 batches: 0.0103
trigger times: 20
Early stopping!
Start to test process.
Loss after 734895780 batches: 0.0102
Time to train on one home:  290.7180597782135
trigger times: 0
Loss after 735026880 batches: 0.1440
trigger times: 0
Loss after 735157980 batches: 0.0389
trigger times: 0
Loss after 735289080 batches: 0.0311
trigger times: 0
Loss after 735420180 batches: 0.0282
trigger times: 1
Loss after 735551280 batches: 0.0266
trigger times: 2
Loss after 735682380 batches: 0.0251
trigger times: 3
Loss after 735813480 batches: 0.0244
trigger times: 4
Loss after 735944580 batches: 0.0236
trigger times: 5
Loss after 736075680 batches: 0.0227
trigger times: 0
Loss after 736206780 batches: 0.0224
trigger times: 1
Loss after 736337880 batches: 0.0219
trigger times: 2
Loss after 736468980 batches: 0.0214
trigger times: 3
Loss after 736600080 batches: 0.0212
trigger times: 4
Loss after 736731180 batches: 0.0210
trigger times: 5
Loss after 736862280 batches: 0.0206
trigger times: 6
Loss after 736993380 batches: 0.0204
trigger times: 7
Loss after 737124480 batches: 0.0202
trigger times: 8
Loss after 737255580 batches: 0.0200
trigger times: 0
Loss after 737386680 batches: 0.0196
trigger times: 1
Loss after 737517780 batches: 0.0192
trigger times: 2
Loss after 737648880 batches: 0.0191
trigger times: 3
Loss after 737779980 batches: 0.0189
trigger times: 4
Loss after 737911080 batches: 0.0187
trigger times: 5
Loss after 738042180 batches: 0.0185
trigger times: 6
Loss after 738173280 batches: 0.0185
trigger times: 7
Loss after 738304380 batches: 0.0182
trigger times: 8
Loss after 738435480 batches: 0.0183
trigger times: 0
Loss after 738566580 batches: 0.0184
trigger times: 1
Loss after 738697680 batches: 0.0184
trigger times: 2
Loss after 738828780 batches: 0.0179
trigger times: 3
Loss after 738959880 batches: 0.0176
trigger times: 4
Loss after 739090980 batches: 0.0177
trigger times: 5
Loss after 739222080 batches: 0.0174
trigger times: 6
Loss after 739353180 batches: 0.0174
trigger times: 7
Loss after 739484280 batches: 0.0175
trigger times: 8
Loss after 739615380 batches: 0.0172
trigger times: 9
Loss after 739746480 batches: 0.0170
trigger times: 10
Loss after 739877580 batches: 0.0173
trigger times: 11
Loss after 740008680 batches: 0.0170
trigger times: 12
Loss after 740139780 batches: 0.0169
trigger times: 13
Loss after 740270880 batches: 0.0171
trigger times: 14
Loss after 740401980 batches: 0.0169
trigger times: 15
Loss after 740533080 batches: 0.0167
trigger times: 16
Loss after 740664180 batches: 0.0166
trigger times: 17
Loss after 740795280 batches: 0.0166
trigger times: 18
Loss after 740926380 batches: 0.0167
trigger times: 19
Loss after 741057480 batches: 0.0163
trigger times: 20
Early stopping!
Start to test process.
Loss after 741188580 batches: 0.0165
Time to train on one home:  350.8291094303131
trigger times: 0
Loss after 741319680 batches: 0.1192
trigger times: 0
Loss after 741450780 batches: 0.0472
trigger times: 0
Loss after 741581880 batches: 0.0358
trigger times: 0
Loss after 741712980 batches: 0.0310
trigger times: 1
Loss after 741844080 batches: 0.0286
trigger times: 0
Loss after 741975180 batches: 0.0261
trigger times: 1
Loss after 742106280 batches: 0.0238
trigger times: 2
Loss after 742237380 batches: 0.0242
trigger times: 0
Loss after 742368480 batches: 0.0241
trigger times: 1
Loss after 742499580 batches: 0.0238
trigger times: 2
Loss after 742630680 batches: 0.0226
trigger times: 3
Loss after 742761780 batches: 0.0233
trigger times: 0
Loss after 742892880 batches: 0.0221
trigger times: 0
Loss after 743023980 batches: 0.0216
trigger times: 0
Loss after 743155080 batches: 0.0222
trigger times: 1
Loss after 743286180 batches: 0.0207
trigger times: 2
Loss after 743417280 batches: 0.0210
trigger times: 3
Loss after 743548380 batches: 0.0204
trigger times: 4
Loss after 743679480 batches: 0.0212
trigger times: 5
Loss after 743810580 batches: 0.0201
trigger times: 6
Loss after 743941680 batches: 0.0204
trigger times: 7
Loss after 744072780 batches: 0.0202
trigger times: 8
Loss after 744203880 batches: 0.0205
trigger times: 9
Loss after 744334980 batches: 0.0199
trigger times: 10
Loss after 744466080 batches: 0.0200
trigger times: 11
Loss after 744597180 batches: 0.0189
trigger times: 12
Loss after 744728280 batches: 0.0190
trigger times: 13
Loss after 744859380 batches: 0.0187
trigger times: 14
Loss after 744990480 batches: 0.0189
trigger times: 15
Loss after 745121580 batches: 0.0189
trigger times: 16
Loss after 745252680 batches: 0.0188
trigger times: 17
Loss after 745383780 batches: 0.0186
trigger times: 18
Loss after 745514880 batches: 0.0186
trigger times: 0
Loss after 745645980 batches: 0.0186
trigger times: 1
Loss after 745777080 batches: 0.0185
trigger times: 2
Loss after 745908180 batches: 0.0177
trigger times: 3
Loss after 746039280 batches: 0.0181
trigger times: 4
Loss after 746170380 batches: 0.0179
trigger times: 0
Loss after 746301480 batches: 0.0183
trigger times: 1
Loss after 746432580 batches: 0.0178
trigger times: 2
Loss after 746563680 batches: 0.0177
trigger times: 3
Loss after 746694780 batches: 0.0172
trigger times: 4
Loss after 746825880 batches: 0.0179
trigger times: 5
Loss after 746956980 batches: 0.0185
trigger times: 6
Loss after 747088080 batches: 0.0182
trigger times: 7
Loss after 747219180 batches: 0.0177
trigger times: 8
Loss after 747350280 batches: 0.0181
trigger times: 9
Loss after 747481380 batches: 0.0176
trigger times: 10
Loss after 747612480 batches: 0.0174
trigger times: 11
Loss after 747743580 batches: 0.0182
trigger times: 12
Loss after 747874680 batches: 0.0170
trigger times: 13
Loss after 748005780 batches: 0.0170
trigger times: 14
Loss after 748136880 batches: 0.0171
trigger times: 15
Loss after 748267980 batches: 0.0175
trigger times: 16
Loss after 748399080 batches: 0.0171
trigger times: 17
Loss after 748530180 batches: 0.0163
trigger times: 18
Loss after 748661280 batches: 0.0163
trigger times: 19
Loss after 748792380 batches: 0.0168
trigger times: 20
Early stopping!
Start to test process.
Loss after 748923480 batches: 0.0172
Time to train on one home:  430.1760559082031
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887, 0.018152045403586472, 0.017512356438894854, 0.016091731943637777, 0.01601899988306293, 0.015690088165884984, 0.01509131066983643, 0.015631624726830377, 0.015631806442569732, 0.01489884440801886, 0.015460750691055835, 0.014456455932877739]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884], [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987], [0.5804732673698001, 0.3723540197761146, 0.45230155418961127, 1.0682024720651153, 0.514164189884338, 25.236666271720736, 1587.2568], [0.5693913300832113, 0.3843477905525722, 0.4653277545238437, 1.0599366084402062, 0.5043389577801884, 25.04138228090006, 1556.9258], [0.5662658578819699, 0.38771059287123977, 0.46820783457394766, 1.0579149875211409, 0.5015841683218024, 24.993620762090103, 1548.4215], [0.5593101216687096, 0.3952721834915385, 0.47154332581377506, 1.051822130314305, 0.49538975421253684, 24.84967482675361, 1529.299], [0.5649060971207089, 0.3891939618618432, 0.4673667174574867, 1.0599116662790036, 0.5003690004733905, 25.04079301340178, 1544.6703], [0.5509053071339926, 0.40439992136570346, 0.47811920698222654, 1.044031858684061, 0.48791236074962885, 24.665626867270383, 1506.2158], [0.5444808701674143, 0.41136524952900755, 0.4884585927112777, 1.0362181287288137, 0.4822064016178801, 24.481024696450934, 1488.6012], [0.5505051612854004, 0.4048088230706274, 0.48190953965979827, 1.0477392844571842, 0.48757739068612305, 24.753216129990033, 1505.1818], [0.53266022933854, 0.4241441989768663, 0.5002133610335463, 1.022931215967613, 0.47173795539587554, 24.16711662012189, 1456.2844], [0.5479287935627831, 0.40760351741064715, 0.4862655487793294, 1.038368061796194, 0.4852879922089788, 24.531817635754958, 1498.1144]]
Round_18_results:  [0.5479287935627831, 0.40760351741064715, 0.4862655487793294, 1.038368061796194, 0.4852879922089788, 24.531817635754958, 1498.1144]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 5978 < 5979; dropping {'Training_Loss': 0.08254065758214807, 'Validation_Loss': 0.2367265439695782, 'Training_R2': 0.9168418718416305, 'Validation_R2': 0.7802714888818465, 'Training_F1': 0.8461375141661299, 'Validation_F1': 0.7366319695455826, 'Training_NEP': 0.3077710943035596, 'Validation_NEP': 0.5055058851283867, 'Training_NDE': 0.06242828286755047, 'Validation_NDE': 0.17498020346169138, 'Training_MAE': 10.192768267435849, 'Validation_MAE': 13.863068585669268, 'Training_MSE': 274.67468, 'Validation_MSE': 646.197}.
trigger times: 0
Loss after 749054580 batches: 0.0825
trigger times: 0
Loss after 749185680 batches: 0.0206
trigger times: 1
Loss after 749316780 batches: 0.0156
trigger times: 2
Loss after 749447880 batches: 0.0137
trigger times: 3
Loss after 749578980 batches: 0.0128
trigger times: 4
Loss after 749710080 batches: 0.0123
trigger times: 0
Loss after 749841180 batches: 0.0117
trigger times: 1
Loss after 749972280 batches: 0.0113
trigger times: 2
Loss after 750103380 batches: 0.0110
trigger times: 3
Loss after 750234480 batches: 0.0107
trigger times: 4
Loss after 750365580 batches: 0.0103
trigger times: 5
Loss after 750496680 batches: 0.0103
trigger times: 0
Loss after 750627780 batches: 0.0100
trigger times: 1
Loss after 750758880 batches: 0.0098
trigger times: 0
Loss after 750889980 batches: 0.0097
trigger times: 1
Loss after 751021080 batches: 0.0097
trigger times: 2
Loss after 751152180 batches: 0.0095
trigger times: 3
Loss after 751283280 batches: 0.0093
trigger times: 4
Loss after 751414380 batches: 0.0092
trigger times: 5
Loss after 751545480 batches: 0.0092
trigger times: 6
Loss after 751676580 batches: 0.0089
trigger times: 7
Loss after 751807680 batches: 0.0090
trigger times: 0
Loss after 751938780 batches: 0.0088
trigger times: 1
Loss after 752069880 batches: 0.0087
trigger times: 2
Loss after 752200980 batches: 0.0087
trigger times: 3
Loss after 752332080 batches: 0.0086
trigger times: 4
Loss after 752463180 batches: 0.0085
trigger times: 5
Loss after 752594280 batches: 0.0086
trigger times: 6
Loss after 752725380 batches: 0.0084
trigger times: 7
Loss after 752856480 batches: 0.0084
trigger times: 8
Loss after 752987580 batches: 0.0083
trigger times: 9
Loss after 753118680 batches: 0.0084
trigger times: 10
Loss after 753249780 batches: 0.0082
trigger times: 11
Loss after 753380880 batches: 0.0083
trigger times: 12
Loss after 753511980 batches: 0.0081
trigger times: 13
Loss after 753643080 batches: 0.0080
trigger times: 14
Loss after 753774180 batches: 0.0079
trigger times: 15
Loss after 753905280 batches: 0.0080
trigger times: 16
Loss after 754036380 batches: 0.0080
trigger times: 17
Loss after 754167480 batches: 0.0080
trigger times: 18
Loss after 754298580 batches: 0.0078
trigger times: 19
Loss after 754429680 batches: 0.0078
trigger times: 20
Early stopping!
Start to test process.
Loss after 754560780 batches: 0.0078
Time to train on one home:  315.8553488254547
trigger times: 0
Loss after 754663380 batches: 0.1033
trigger times: 0
Loss after 754765980 batches: 0.0452
trigger times: 1
Loss after 754868580 batches: 0.0327
trigger times: 2
Loss after 754971180 batches: 0.0251
trigger times: 0
Loss after 755073780 batches: 0.0225
trigger times: 0
Loss after 755176380 batches: 0.0215
trigger times: 1
Loss after 755278980 batches: 0.0213
trigger times: 2
Loss after 755381580 batches: 0.0199
trigger times: 3
Loss after 755484180 batches: 0.0194
trigger times: 4
Loss after 755586780 batches: 0.0189
trigger times: 0
Loss after 755689380 batches: 0.0201
trigger times: 0
Loss after 755791980 batches: 0.0216
trigger times: 1
Loss after 755894580 batches: 0.0206
trigger times: 0
Loss after 755997180 batches: 0.0186
trigger times: 1
Loss after 756099780 batches: 0.0173
trigger times: 2
Loss after 756202380 batches: 0.0174
trigger times: 0
Loss after 756304980 batches: 0.0169
trigger times: 0
Loss after 756407580 batches: 0.0166
trigger times: 1
Loss after 756510180 batches: 0.0164
trigger times: 2
Loss after 756612780 batches: 0.0158
trigger times: 3
Loss after 756715380 batches: 0.0154
trigger times: 4
Loss after 756817980 batches: 0.0156
trigger times: 5
Loss after 756920580 batches: 0.0157
trigger times: 6
Loss after 757023180 batches: 0.0171
trigger times: 7
Loss after 757125780 batches: 0.0159
trigger times: 8
Loss after 757228380 batches: 0.0147
trigger times: 0
Loss after 757330980 batches: 0.0148
trigger times: 0
Loss after 757433580 batches: 0.0163
trigger times: 1
Loss after 757536180 batches: 0.0173
trigger times: 2
Loss after 757638780 batches: 0.0149
trigger times: 3
Loss after 757741380 batches: 0.0153
trigger times: 0
Loss after 757843980 batches: 0.0157
trigger times: 1
Loss after 757946580 batches: 0.0145
trigger times: 2
Loss after 758049180 batches: 0.0148
trigger times: 3
Loss after 758151780 batches: 0.0142
trigger times: 4
Loss after 758254380 batches: 0.0143
trigger times: 5
Loss after 758356980 batches: 0.0154
trigger times: 6
Loss after 758459580 batches: 0.0205
trigger times: 0
Loss after 758562180 batches: 0.0160
trigger times: 1
Loss after 758664780 batches: 0.0166
trigger times: 2
Loss after 758767380 batches: 0.0166
trigger times: 3
Loss after 758869980 batches: 0.0163
trigger times: 4
Loss after 758972580 batches: 0.0154
trigger times: 5
Loss after 759075180 batches: 0.0140
trigger times: 6
Loss after 759177780 batches: 0.0143
trigger times: 7
Loss after 759280380 batches: 0.0145
trigger times: 8
Loss after 759382980 batches: 0.0141
trigger times: 9
Loss after 759485580 batches: 0.0143
trigger times: 10
Loss after 759588180 batches: 0.0155
trigger times: 11
Loss after 759690780 batches: 0.0141
trigger times: 12
Loss after 759793380 batches: 0.0154
trigger times: 13
Loss after 759895980 batches: 0.0177
trigger times: 14
Loss after 759998580 batches: 0.0146
trigger times: 15
Loss after 760101180 batches: 0.0140
trigger times: 16
Loss after 760203780 batches: 0.0143
trigger times: 17
Loss after 760306380 batches: 0.0145
trigger times: 18
Loss after 760408980 batches: 0.0133
trigger times: 19
Loss after 760511580 batches: 0.0134
trigger times: 20
Early stopping!
Start to test process.
Loss after 760614180 batches: 0.0133
Time to train on one home:  346.14921736717224
trigger times: 0
Loss after 760745280 batches: 0.0979
trigger times: 0
Loss after 760876380 batches: 0.0337
trigger times: 1
Loss after 761007480 batches: 0.0271
trigger times: 0
Loss after 761138580 batches: 0.0246
trigger times: 1
Loss after 761269680 batches: 0.0228
trigger times: 0
Loss after 761400780 batches: 0.0214
trigger times: 1
Loss after 761531880 batches: 0.0205
trigger times: 2
Loss after 761662980 batches: 0.0202
trigger times: 3
Loss after 761794080 batches: 0.0194
trigger times: 4
Loss after 761925180 batches: 0.0188
trigger times: 5
Loss after 762056280 batches: 0.0185
trigger times: 6
Loss after 762187380 batches: 0.0180
trigger times: 7
Loss after 762318480 batches: 0.0176
trigger times: 8
Loss after 762449580 batches: 0.0172
trigger times: 9
Loss after 762580680 batches: 0.0169
trigger times: 10
Loss after 762711780 batches: 0.0168
trigger times: 11
Loss after 762842880 batches: 0.0164
trigger times: 12
Loss after 762973980 batches: 0.0163
trigger times: 13
Loss after 763105080 batches: 0.0161
trigger times: 14
Loss after 763236180 batches: 0.0159
trigger times: 15
Loss after 763367280 batches: 0.0157
trigger times: 16
Loss after 763498380 batches: 0.0155
trigger times: 17
Loss after 763629480 batches: 0.0154
trigger times: 18
Loss after 763760580 batches: 0.0154
trigger times: 19
Loss after 763891680 batches: 0.0151
trigger times: 20
Early stopping!
Start to test process.
Loss after 764022780 batches: 0.0149
Time to train on one home:  196.08321046829224
trigger times: 0
Loss after 764153880 batches: 0.1841
trigger times: 0
Loss after 764284980 batches: 0.0537
trigger times: 0
Loss after 764416080 batches: 0.0422
trigger times: 0
Loss after 764547180 batches: 0.0382
trigger times: 1
Loss after 764678280 batches: 0.0354
trigger times: 2
Loss after 764809380 batches: 0.0336
trigger times: 3
Loss after 764940480 batches: 0.0320
trigger times: 4
Loss after 765071580 batches: 0.0305
trigger times: 5
Loss after 765202680 batches: 0.0293
trigger times: 6
Loss after 765333780 batches: 0.0283
trigger times: 7
Loss after 765464880 batches: 0.0281
trigger times: 8
Loss after 765595980 batches: 0.0273
trigger times: 9
Loss after 765727080 batches: 0.0266
trigger times: 10
Loss after 765858180 batches: 0.0261
trigger times: 11
Loss after 765989280 batches: 0.0257
trigger times: 12
Loss after 766120380 batches: 0.0254
trigger times: 13
Loss after 766251480 batches: 0.0249
trigger times: 14
Loss after 766382580 batches: 0.0247
trigger times: 15
Loss after 766513680 batches: 0.0243
trigger times: 16
Loss after 766644780 batches: 0.0241
trigger times: 17
Loss after 766775880 batches: 0.0236
trigger times: 18
Loss after 766906980 batches: 0.0235
trigger times: 19
Loss after 767038080 batches: 0.0231
trigger times: 20
Early stopping!
Start to test process.
Loss after 767169180 batches: 0.0225
Time to train on one home:  181.14300060272217
trigger times: 0
Loss after 767297820 batches: 0.0862
trigger times: 0
Loss after 767426460 batches: 0.0244
trigger times: 0
Loss after 767555100 batches: 0.0194
trigger times: 1
Loss after 767683740 batches: 0.0176
trigger times: 2
Loss after 767812380 batches: 0.0162
trigger times: 0
Loss after 767941020 batches: 0.0158
trigger times: 0
Loss after 768069660 batches: 0.0149
trigger times: 0
Loss after 768198300 batches: 0.0146
trigger times: 1
Loss after 768326940 batches: 0.0142
trigger times: 2
Loss after 768455580 batches: 0.0136
trigger times: 0
Loss after 768584220 batches: 0.0134
trigger times: 1
Loss after 768712860 batches: 0.0127
trigger times: 0
Loss after 768841500 batches: 0.0129
trigger times: 1
Loss after 768970140 batches: 0.0124
trigger times: 0
Loss after 769098780 batches: 0.0123
trigger times: 1
Loss after 769227420 batches: 0.0123
trigger times: 2
Loss after 769356060 batches: 0.0120
trigger times: 0
Loss after 769484700 batches: 0.0120
trigger times: 1
Loss after 769613340 batches: 0.0117
trigger times: 2
Loss after 769741980 batches: 0.0115
trigger times: 3
Loss after 769870620 batches: 0.0114
trigger times: 4
Loss after 769999260 batches: 0.0112
trigger times: 5
Loss after 770127900 batches: 0.0114
trigger times: 6
Loss after 770256540 batches: 0.0114
trigger times: 7
Loss after 770385180 batches: 0.0113
trigger times: 8
Loss after 770513820 batches: 0.0109
trigger times: 9
Loss after 770642460 batches: 0.0108
trigger times: 10
Loss after 770771100 batches: 0.0108
trigger times: 11
Loss after 770899740 batches: 0.0107
trigger times: 12
Loss after 771028380 batches: 0.0107
trigger times: 13
Loss after 771157020 batches: 0.0105
trigger times: 14
Loss after 771285660 batches: 0.0103
trigger times: 15
Loss after 771414300 batches: 0.0106
trigger times: 16
Loss after 771542940 batches: 0.0104
trigger times: 17
Loss after 771671580 batches: 0.0103
trigger times: 18
Loss after 771800220 batches: 0.0102
trigger times: 19
Loss after 771928860 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 772057500 batches: 0.0103
Time to train on one home:  276.0646240711212
trigger times: 0
Loss after 772188600 batches: 0.1414
trigger times: 0
Loss after 772319700 batches: 0.0386
trigger times: 0
Loss after 772450800 batches: 0.0311
trigger times: 0
Loss after 772581900 batches: 0.0281
trigger times: 0
Loss after 772713000 batches: 0.0265
trigger times: 1
Loss after 772844100 batches: 0.0253
trigger times: 2
Loss after 772975200 batches: 0.0242
trigger times: 3
Loss after 773106300 batches: 0.0234
trigger times: 4
Loss after 773237400 batches: 0.0226
trigger times: 5
Loss after 773368500 batches: 0.0225
trigger times: 6
Loss after 773499600 batches: 0.0219
trigger times: 7
Loss after 773630700 batches: 0.0211
trigger times: 8
Loss after 773761800 batches: 0.0206
trigger times: 9
Loss after 773892900 batches: 0.0203
trigger times: 10
Loss after 774024000 batches: 0.0200
trigger times: 11
Loss after 774155100 batches: 0.0200
trigger times: 12
Loss after 774286200 batches: 0.0198
trigger times: 13
Loss after 774417300 batches: 0.0194
trigger times: 14
Loss after 774548400 batches: 0.0193
trigger times: 15
Loss after 774679500 batches: 0.0194
trigger times: 16
Loss after 774810600 batches: 0.0190
trigger times: 17
Loss after 774941700 batches: 0.0188
trigger times: 18
Loss after 775072800 batches: 0.0186
trigger times: 19
Loss after 775203900 batches: 0.0186
trigger times: 20
Early stopping!
Start to test process.
Loss after 775335000 batches: 0.0186
Time to train on one home:  188.67720556259155
trigger times: 0
Loss after 775466100 batches: 0.1266
trigger times: 0
Loss after 775597200 batches: 0.0465
trigger times: 0
Loss after 775728300 batches: 0.0348
trigger times: 0
Loss after 775859400 batches: 0.0323
trigger times: 0
Loss after 775990500 batches: 0.0261
trigger times: 0
Loss after 776121600 batches: 0.0254
trigger times: 0
Loss after 776252700 batches: 0.0243
trigger times: 0
Loss after 776383800 batches: 0.0242
trigger times: 1
Loss after 776514900 batches: 0.0231
trigger times: 0
Loss after 776646000 batches: 0.0228
trigger times: 1
Loss after 776777100 batches: 0.0228
trigger times: 2
Loss after 776908200 batches: 0.0225
trigger times: 3
Loss after 777039300 batches: 0.0225
trigger times: 4
Loss after 777170400 batches: 0.0212
trigger times: 5
Loss after 777301500 batches: 0.0206
trigger times: 0
Loss after 777432600 batches: 0.0201
trigger times: 1
Loss after 777563700 batches: 0.0204
trigger times: 2
Loss after 777694800 batches: 0.0201
trigger times: 3
Loss after 777825900 batches: 0.0198
trigger times: 0
Loss after 777957000 batches: 0.0203
trigger times: 1
Loss after 778088100 batches: 0.0196
trigger times: 2
Loss after 778219200 batches: 0.0195
trigger times: 3
Loss after 778350300 batches: 0.0198
trigger times: 4
Loss after 778481400 batches: 0.0189
trigger times: 5
Loss after 778612500 batches: 0.0187
trigger times: 6
Loss after 778743600 batches: 0.0191
trigger times: 7
Loss after 778874700 batches: 0.0182
trigger times: 8
Loss after 779005800 batches: 0.0190
trigger times: 9
Loss after 779136900 batches: 0.0186
trigger times: 10
Loss after 779268000 batches: 0.0189
trigger times: 11
Loss after 779399100 batches: 0.0186
trigger times: 12
Loss after 779530200 batches: 0.0187
trigger times: 13
Loss after 779661300 batches: 0.0189
trigger times: 14
Loss after 779792400 batches: 0.0187
trigger times: 15
Loss after 779923500 batches: 0.0181
trigger times: 16
Loss after 780054600 batches: 0.0179
trigger times: 17
Loss after 780185700 batches: 0.0178
trigger times: 18
Loss after 780316800 batches: 0.0182
trigger times: 19
Loss after 780447900 batches: 0.0173
trigger times: 20
Early stopping!
Start to test process.
Loss after 780579000 batches: 0.0181
Time to train on one home:  294.3324246406555
train_results:  [0.07637831982700095, 0.07267687724368596, 0.0436565930669719, 0.029268322745462223, 0.025150398871445344, 0.02038352680435321, 0.0188122097154291, 0.018674868325335887, 0.018152045403586472, 0.017512356438894854, 0.016091731943637777, 0.01601899988306293, 0.015690088165884984, 0.01509131066983643, 0.015631624726830377, 0.015631806442569732, 0.01489884440801886, 0.015460750691055835, 0.014456455932877739, 0.015049014330570792]
test_results:  [[0.8822027544180552, 0.04570113494395134, 0.22752954083873828, 1.4977325004479, 0.78175646513989, 35.38446714604627, 2413.3306], [0.7681049572096931, 0.16927827289015485, 0.29622865770810647, 1.1198001873704007, 0.6805227425919177, 26.455680789656963, 2100.816], [0.6697042551305559, 0.27588773795312793, 0.30142567630112715, 1.0339700061482742, 0.5931888458328671, 24.427911994705063, 1831.2107], [0.6139205263720618, 0.33617805049226224, 0.38676158894245427, 1.0577863267320318, 0.5437993481203187, 24.990581104832312, 1678.7424], [0.5977684723006355, 0.35368631251092997, 0.41156664498505463, 1.0738790183729128, 0.5294566746375702, 25.37077671285171, 1634.4658], [0.6143038438426124, 0.33577019671758734, 0.4084100666461499, 1.0865966524322412, 0.5441334597250368, 25.671235375805995, 1679.7738], [0.61226025223732, 0.3379813280360544, 0.415055508185782, 1.0788816810808035, 0.5423221129166304, 25.488966412398977, 1674.1821], [0.5902653634548187, 0.3617836219021716, 0.4423079406237756, 1.0690006806449037, 0.5228234026107094, 25.255524235515185, 1613.9884], [0.5733900434441037, 0.3800483204031151, 0.45050852320984747, 1.0475445872603122, 0.5078610604558702, 24.74861633892982, 1567.7987], [0.5804732673698001, 0.3723540197761146, 0.45230155418961127, 1.0682024720651153, 0.514164189884338, 25.236666271720736, 1587.2568], [0.5693913300832113, 0.3843477905525722, 0.4653277545238437, 1.0599366084402062, 0.5043389577801884, 25.04138228090006, 1556.9258], [0.5662658578819699, 0.38771059287123977, 0.46820783457394766, 1.0579149875211409, 0.5015841683218024, 24.993620762090103, 1548.4215], [0.5593101216687096, 0.3952721834915385, 0.47154332581377506, 1.051822130314305, 0.49538975421253684, 24.84967482675361, 1529.299], [0.5649060971207089, 0.3891939618618432, 0.4673667174574867, 1.0599116662790036, 0.5003690004733905, 25.04079301340178, 1544.6703], [0.5509053071339926, 0.40439992136570346, 0.47811920698222654, 1.044031858684061, 0.48791236074962885, 24.665626867270383, 1506.2158], [0.5444808701674143, 0.41136524952900755, 0.4884585927112777, 1.0362181287288137, 0.4822064016178801, 24.481024696450934, 1488.6012], [0.5505051612854004, 0.4048088230706274, 0.48190953965979827, 1.0477392844571842, 0.48757739068612305, 24.753216129990033, 1505.1818], [0.53266022933854, 0.4241441989768663, 0.5002133610335463, 1.022931215967613, 0.47173795539587554, 24.16711662012189, 1456.2844], [0.5479287935627831, 0.40760351741064715, 0.4862655487793294, 1.038368061796194, 0.4852879922089788, 24.531817635754958, 1498.1144], [0.5347844892077975, 0.42185707035004694, 0.4988119711271065, 1.0259957673535647, 0.47361155878795097, 24.239517745023132, 1462.0682]]
Round_19_results:  [0.5347844892077975, 0.42185707035004694, 0.4988119711271065, 1.0259957673535647, 0.47361155878795097, 24.239517745023132, 1462.0682]