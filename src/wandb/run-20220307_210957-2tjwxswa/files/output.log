LSTM(
  (conv1): Conv1d(1, 30, kernel_size=(10,), stride=(1,))
  (conv2): Conv1d(30, 30, kernel_size=(8,), stride=(1,))
  (conv3): Conv1d(30, 40, kernel_size=(6,), stride=(1,))
  (conv4): Conv1d(40, 50, kernel_size=(5,), stride=(1,))
  (conv5): Conv1d(50, 50, kernel_size=(5,), stride=(1,))
  (linear1): Linear(in_features=23500, out_features=1024, bias=True)
  (linear2): Linear(in_features=1024, out_features=1, bias=True)
  (relu): ReLU()
  (leaky): LeakyReLU(negative_slope=0.01)
  (dropout): Dropout(p=0.2, inplace=False)
)
Window Length:  499
trigger times: 0
Loss after 131100 batches: 0.7280
trigger times: 0
Loss after 262200 batches: 0.3629
trigger times: 0
Loss after 393300 batches: 0.2568
trigger times: 0
Loss after 524400 batches: 0.2024
trigger times: 1
Loss after 655500 batches: 0.1657
trigger times: 0
Loss after 786600 batches: 0.1402
trigger times: 1
Loss after 917700 batches: 0.1195
trigger times: 2
Loss after 1048800 batches: 0.1024
trigger times: 3
Loss after 1179900 batches: 0.0887
trigger times: 4
Loss after 1311000 batches: 0.0780
trigger times: 0
Loss after 1442100 batches: 0.0721
trigger times: 0
Loss after 1573200 batches: 0.0642
trigger times: 1
Loss after 1704300 batches: 0.0596
trigger times: 0
Loss after 1835400 batches: 0.0538
trigger times: 0
Loss after 1966500 batches: 0.0494
trigger times: 1
Loss after 2097600 batches: 0.0467
trigger times: 2
Loss after 2228700 batches: 0.0444
trigger times: 3
Loss after 2359800 batches: 0.0419
trigger times: 0
Loss after 2490900 batches: 0.0395
trigger times: 1
Loss after 2622000 batches: 0.0386
trigger times: 2
Loss after 2753100 batches: 0.0362
trigger times: 0
Loss after 2884200 batches: 0.0339
trigger times: 1
Loss after 3015300 batches: 0.0336
trigger times: 2
Loss after 3146400 batches: 0.0313
trigger times: 3
Loss after 3277500 batches: 0.0308
trigger times: 4
Loss after 3408600 batches: 0.0299
trigger times: 5
Loss after 3539700 batches: 0.0288
trigger times: 6
Loss after 3670800 batches: 0.0276
trigger times: 7
Loss after 3801900 batches: 0.0260
trigger times: 0
Loss after 3933000 batches: 0.0260
trigger times: 0
Loss after 4064100 batches: 0.0249
trigger times: 0
Loss after 4195200 batches: 0.0246
trigger times: 1
Loss after 4326300 batches: 0.0242
trigger times: 2
Loss after 4457400 batches: 0.0234
trigger times: 3
Loss after 4588500 batches: 0.0224
trigger times: 4
Loss after 4719600 batches: 0.0219
trigger times: 5
Loss after 4850700 batches: 0.0217
trigger times: 6
Loss after 4981800 batches: 0.0214
trigger times: 7
Loss after 5112900 batches: 0.0206
trigger times: 8
Loss after 5244000 batches: 0.0204
trigger times: 9
Loss after 5375100 batches: 0.0201
trigger times: 10
Loss after 5506200 batches: 0.0193
trigger times: 0
Loss after 5637300 batches: 0.0190
trigger times: 1
Loss after 5768400 batches: 0.0186
trigger times: 2
Loss after 5899500 batches: 0.0182
trigger times: 3
Loss after 6030600 batches: 0.0178
trigger times: 4
Loss after 6161700 batches: 0.0180
trigger times: 5
Loss after 6292800 batches: 0.0172
trigger times: 6
Loss after 6423900 batches: 0.0170
trigger times: 7
Loss after 6555000 batches: 0.0167
trigger times: 0
Loss after 6686100 batches: 0.0165
trigger times: 0
Loss after 6817200 batches: 0.0162
trigger times: 0
Loss after 6948300 batches: 0.0159
trigger times: 1
Loss after 7079400 batches: 0.0159
trigger times: 0
Loss after 7210500 batches: 0.0154
trigger times: 1
Loss after 7341600 batches: 0.0154
trigger times: 2
Loss after 7472700 batches: 0.0150
trigger times: 0
Loss after 7603800 batches: 0.0147
trigger times: 1
Loss after 7734900 batches: 0.0148
trigger times: 2
Loss after 7866000 batches: 0.0146
trigger times: 0
Loss after 7997100 batches: 0.0147
trigger times: 0
Loss after 8128200 batches: 0.0149
trigger times: 1
Loss after 8259300 batches: 0.0140
trigger times: 2
Loss after 8390400 batches: 0.0141
trigger times: 3
Loss after 8521500 batches: 0.0137
trigger times: 4
Loss after 8652600 batches: 0.0133
trigger times: 5
Loss after 8783700 batches: 0.0133
trigger times: 6
Loss after 8914800 batches: 0.0131
trigger times: 7
Loss after 9045900 batches: 0.0135
trigger times: 8
Loss after 9177000 batches: 0.0129
trigger times: 9
Loss after 9308100 batches: 0.0128
trigger times: 10
Loss after 9439200 batches: 0.0128
trigger times: 11
Loss after 9570300 batches: 0.0127
trigger times: 12
Loss after 9701400 batches: 0.0126
trigger times: 13
Loss after 9832500 batches: 0.0120
trigger times: 14
Loss after 9963600 batches: 0.0128
trigger times: 0
Loss after 10094700 batches: 0.0119
trigger times: 1
Loss after 10225800 batches: 0.0119
trigger times: 2
Loss after 10356900 batches: 0.0116
trigger times: 3
Loss after 10488000 batches: 0.0118
trigger times: 4
Loss after 10619100 batches: 0.0117
trigger times: 5
Loss after 10750200 batches: 0.0119
trigger times: 6
Loss after 10881300 batches: 0.0116
trigger times: 7
Loss after 11012400 batches: 0.0114
trigger times: 8
Loss after 11143500 batches: 0.0115
trigger times: 9
Loss after 11274600 batches: 0.0113
trigger times: 0
Loss after 11405700 batches: 0.0107
trigger times: 1
Loss after 11536800 batches: 0.0111
trigger times: 2
Loss after 11667900 batches: 0.0109
trigger times: 3
Loss after 11799000 batches: 0.0107
trigger times: 4
Loss after 11930100 batches: 0.0107
trigger times: 5
Loss after 12061200 batches: 0.0111
trigger times: 6
Loss after 12192300 batches: 0.0105
trigger times: 7
Loss after 12323400 batches: 0.0105
trigger times: 8
Loss after 12454500 batches: 0.0107
trigger times: 9
Loss after 12585600 batches: 0.0108
trigger times: 10
Loss after 12716700 batches: 0.0106
trigger times: 11
Loss after 12847800 batches: 0.0102
trigger times: 0
Loss after 12978900 batches: 0.0101
trigger times: 1
Loss after 13110000 batches: 0.0100
trigger times: 2
Loss after 13241100 batches: 0.0101
trigger times: 3
Loss after 13372200 batches: 0.0104
trigger times: 4
Loss after 13503300 batches: 0.0098
trigger times: 5
Loss after 13634400 batches: 0.0096
trigger times: 6
Loss after 13765500 batches: 0.0097
trigger times: 7
Loss after 13896600 batches: 0.0098
trigger times: 8
Loss after 14027700 batches: 0.0097
trigger times: 0
Loss after 14158800 batches: 0.0098
trigger times: 0
Loss after 14289900 batches: 0.0098
trigger times: 1
Loss after 14421000 batches: 0.0098
trigger times: 2
Loss after 14552100 batches: 0.0094
trigger times: 3
Loss after 14683200 batches: 0.0096
trigger times: 4
Loss after 14814300 batches: 0.0095
trigger times: 5
Loss after 14945400 batches: 0.0094
trigger times: 6
Loss after 15076500 batches: 0.0093
trigger times: 7
Loss after 15207600 batches: 0.0092
trigger times: 8
Loss after 15338700 batches: 0.0093
trigger times: 9
Loss after 15469800 batches: 0.0093
trigger times: 10
Loss after 15600900 batches: 0.0092
trigger times: 0
Loss after 15732000 batches: 0.0088
trigger times: 1
Loss after 15863100 batches: 0.0092
trigger times: 0
Loss after 15994200 batches: 0.0090
trigger times: 1
Loss after 16125300 batches: 0.0090
trigger times: 2
Loss after 16256400 batches: 0.0090
trigger times: 3
Loss after 16387500 batches: 0.0089
trigger times: 4
Loss after 16518600 batches: 0.0088
trigger times: 5
Loss after 16649700 batches: 0.0088
trigger times: 6
Loss after 16780800 batches: 0.0087
trigger times: 7
Loss after 16911900 batches: 0.0086
trigger times: 8
Loss after 17043000 batches: 0.0087
trigger times: 9
Loss after 17174100 batches: 0.0086
trigger times: 10
Loss after 17305200 batches: 0.0087
trigger times: 11
Loss after 17436300 batches: 0.0085
trigger times: 12
Loss after 17567400 batches: 0.0084
trigger times: 13
Loss after 17698500 batches: 0.0083
trigger times: 14
Loss after 17829600 batches: 0.0081
trigger times: 15
Loss after 17960700 batches: 0.0081
trigger times: 16
Loss after 18091800 batches: 0.0081
trigger times: 17
Loss after 18222900 batches: 0.0081
trigger times: 18
Loss after 18354000 batches: 0.0079
trigger times: 19
Loss after 18485100 batches: 0.0084
trigger times: 20
Early stopping!
Start to test process.
Loss after 18616200 batches: 0.0081
Time to train on one home:  1014.7865488529205
trigger times: 0
Loss after 18718800 batches: 1.0045
trigger times: 0
Loss after 18821400 batches: 0.8749
trigger times: 1
Loss after 18924000 batches: 0.7389
trigger times: 2
Loss after 19026600 batches: 0.6341
trigger times: 0
Loss after 19129200 batches: 0.5636
trigger times: 0
Loss after 19231800 batches: 0.5225
trigger times: 1
Loss after 19334400 batches: 0.5011
trigger times: 2
Loss after 19437000 batches: 0.4813
trigger times: 3
Loss after 19539600 batches: 0.4101
trigger times: 4
Loss after 19642200 batches: 0.3746
trigger times: 5
Loss after 19744800 batches: 0.3536
trigger times: 6
Loss after 19847400 batches: 0.3315
trigger times: 7
Loss after 19950000 batches: 0.3138
trigger times: 8
Loss after 20052600 batches: 0.3341
trigger times: 9
Loss after 20155200 batches: 0.2925
trigger times: 10
Loss after 20257800 batches: 0.2833
trigger times: 11
Loss after 20360400 batches: 0.2580
trigger times: 12
Loss after 20463000 batches: 0.2392
trigger times: 13
Loss after 20565600 batches: 0.2193
trigger times: 14
Loss after 20668200 batches: 0.2094
trigger times: 15
Loss after 20770800 batches: 0.1976
trigger times: 16
Loss after 20873400 batches: 0.1977
trigger times: 17
Loss after 20976000 batches: 0.1876
trigger times: 18
Loss after 21078600 batches: 0.1774
trigger times: 19
Loss after 21181200 batches: 0.1747
trigger times: 20
Early stopping!
Start to test process.
Loss after 21283800 batches: 0.1759
Time to train on one home:  159.5872106552124
trigger times: 0
Loss after 21414900 batches: 0.7713
trigger times: 0
Loss after 21546000 batches: 0.4998
trigger times: 1
Loss after 21677100 batches: 0.3965
trigger times: 2
Loss after 21808200 batches: 0.3291
trigger times: 3
Loss after 21939300 batches: 0.2802
trigger times: 4
Loss after 22070400 batches: 0.2426
trigger times: 5
Loss after 22201500 batches: 0.2045
trigger times: 6
Loss after 22332600 batches: 0.1759
trigger times: 7
Loss after 22463700 batches: 0.1498
trigger times: 8
Loss after 22594800 batches: 0.1300
trigger times: 9
Loss after 22725900 batches: 0.1162
trigger times: 10
Loss after 22857000 batches: 0.1032
trigger times: 11
Loss after 22988100 batches: 0.0958
trigger times: 12
Loss after 23119200 batches: 0.0864
trigger times: 13
Loss after 23250300 batches: 0.0803
trigger times: 14
Loss after 23381400 batches: 0.0731
trigger times: 15
Loss after 23512500 batches: 0.0697
trigger times: 16
Loss after 23643600 batches: 0.0671
trigger times: 17
Loss after 23774700 batches: 0.0618
trigger times: 18
Loss after 23905800 batches: 0.0590
trigger times: 19
Loss after 24036900 batches: 0.0571
trigger times: 20
Early stopping!
Start to test process.
Loss after 24168000 batches: 0.0536
Time to train on one home:  167.3715090751648
train_results:  [0.07919007795897719]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987]]
Round_0_results:  [0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 191 < 192; dropping {'Training_Loss': 0.3045882911614652, 'Validation_Loss': 0.3032881369193395, 'Training_R2': 0.6936827269189383, 'Validation_R2': 0.7179464256969492, 'Training_F1': 0.6907807197545345, 'Validation_F1': 0.6406860174448489, 'Training_NEP': 0.6259234490722397, 'Validation_NEP': 0.6975963341467304, 'Training_NDE': 0.2299578140419766, 'Validation_NDE': 0.22461259837193528, 'Training_MAE': 20.729343293216917, 'Validation_MAE': 19.130985632207658, 'Training_MSE': 1011.7783, 'Validation_MSE': 829.48816}.
trigger times: 0
Loss after 24299100 batches: 0.3046
trigger times: 0
Loss after 24430200 batches: 0.0951
trigger times: 1
Loss after 24561300 batches: 0.0501
trigger times: 2
Loss after 24692400 batches: 0.0360
trigger times: 0
Loss after 24823500 batches: 0.0297
trigger times: 0
Loss after 24954600 batches: 0.0254
trigger times: 0
Loss after 25085700 batches: 0.0232
trigger times: 1
Loss after 25216800 batches: 0.0207
trigger times: 2
Loss after 25347900 batches: 0.0196
trigger times: 0
Loss after 25479000 batches: 0.0186
trigger times: 1
Loss after 25610100 batches: 0.0177
trigger times: 0
Loss after 25741200 batches: 0.0168
trigger times: 1
Loss after 25872300 batches: 0.0165
trigger times: 2
Loss after 26003400 batches: 0.0159
trigger times: 3
Loss after 26134500 batches: 0.0148
trigger times: 0
Loss after 26265600 batches: 0.0145
trigger times: 1
Loss after 26396700 batches: 0.0138
trigger times: 2
Loss after 26527800 batches: 0.0133
trigger times: 3
Loss after 26658900 batches: 0.0133
trigger times: 4
Loss after 26790000 batches: 0.0132
trigger times: 5
Loss after 26921100 batches: 0.0128
trigger times: 6
Loss after 27052200 batches: 0.0125
trigger times: 7
Loss after 27183300 batches: 0.0124
trigger times: 8
Loss after 27314400 batches: 0.0123
trigger times: 9
Loss after 27445500 batches: 0.0119
trigger times: 10
Loss after 27576600 batches: 0.0118
trigger times: 11
Loss after 27707700 batches: 0.0121
trigger times: 12
Loss after 27838800 batches: 0.0116
trigger times: 13
Loss after 27969900 batches: 0.0111
trigger times: 14
Loss after 28101000 batches: 0.0114
trigger times: 15
Loss after 28232100 batches: 0.0112
trigger times: 16
Loss after 28363200 batches: 0.0107
trigger times: 17
Loss after 28494300 batches: 0.0111
trigger times: 18
Loss after 28625400 batches: 0.0108
trigger times: 19
Loss after 28756500 batches: 0.0106
trigger times: 20
Early stopping!
Start to test process.
Loss after 28887600 batches: 0.0105
Time to train on one home:  266.60394740104675
trigger times: 0
Loss after 28990200 batches: 0.6893
trigger times: 0
Loss after 29092800 batches: 0.4906
trigger times: 1
Loss after 29195400 batches: 0.3884
trigger times: 2
Loss after 29298000 batches: 0.2970
trigger times: 3
Loss after 29400600 batches: 0.2395
trigger times: 4
Loss after 29503200 batches: 0.2036
trigger times: 5
Loss after 29605800 batches: 0.1960
trigger times: 6
Loss after 29708400 batches: 0.1684
trigger times: 7
Loss after 29811000 batches: 0.1367
trigger times: 8
Loss after 29913600 batches: 0.1317
trigger times: 9
Loss after 30016200 batches: 0.1278
trigger times: 10
Loss after 30118800 batches: 0.1082
trigger times: 11
Loss after 30221400 batches: 0.1050
trigger times: 12
Loss after 30324000 batches: 0.0974
trigger times: 13
Loss after 30426600 batches: 0.0938
trigger times: 14
Loss after 30529200 batches: 0.0848
trigger times: 15
Loss after 30631800 batches: 0.0795
trigger times: 16
Loss after 30734400 batches: 0.0761
trigger times: 17
Loss after 30837000 batches: 0.0750
trigger times: 18
Loss after 30939600 batches: 0.0699
trigger times: 19
Loss after 31042200 batches: 0.0707
trigger times: 20
Early stopping!
Start to test process.
Loss after 31144800 batches: 0.0665
Time to train on one home:  136.6146092414856
trigger times: 0
Loss after 31275900 batches: 0.5100
trigger times: 0
Loss after 31407000 batches: 0.2834
trigger times: 1
Loss after 31538100 batches: 0.1878
trigger times: 2
Loss after 31669200 batches: 0.1282
trigger times: 3
Loss after 31800300 batches: 0.0973
trigger times: 4
Loss after 31931400 batches: 0.0803
trigger times: 5
Loss after 32062500 batches: 0.0697
trigger times: 6
Loss after 32193600 batches: 0.0617
trigger times: 7
Loss after 32324700 batches: 0.0561
trigger times: 8
Loss after 32455800 batches: 0.0523
trigger times: 9
Loss after 32586900 batches: 0.0492
trigger times: 10
Loss after 32718000 batches: 0.0455
trigger times: 11
Loss after 32849100 batches: 0.0426
trigger times: 12
Loss after 32980200 batches: 0.0401
trigger times: 0
Loss after 33111300 batches: 0.0387
trigger times: 0
Loss after 33242400 batches: 0.0366
trigger times: 1
Loss after 33373500 batches: 0.0354
trigger times: 2
Loss after 33504600 batches: 0.0336
trigger times: 3
Loss after 33635700 batches: 0.0325
trigger times: 4
Loss after 33766800 batches: 0.0316
trigger times: 5
Loss after 33897900 batches: 0.0305
trigger times: 6
Loss after 34029000 batches: 0.0294
trigger times: 7
Loss after 34160100 batches: 0.0285
trigger times: 8
Loss after 34291200 batches: 0.0278
trigger times: 9
Loss after 34422300 batches: 0.0271
trigger times: 10
Loss after 34553400 batches: 0.0262
trigger times: 11
Loss after 34684500 batches: 0.0257
trigger times: 12
Loss after 34815600 batches: 0.0254
trigger times: 13
Loss after 34946700 batches: 0.0250
trigger times: 14
Loss after 35077800 batches: 0.0242
trigger times: 15
Loss after 35208900 batches: 0.0237
trigger times: 16
Loss after 35340000 batches: 0.0232
trigger times: 0
Loss after 35471100 batches: 0.0227
trigger times: 1
Loss after 35602200 batches: 0.0221
trigger times: 2
Loss after 35733300 batches: 0.0220
trigger times: 3
Loss after 35864400 batches: 0.0217
trigger times: 4
Loss after 35995500 batches: 0.0211
trigger times: 5
Loss after 36126600 batches: 0.0206
trigger times: 0
Loss after 36257700 batches: 0.0208
trigger times: 1
Loss after 36388800 batches: 0.0201
trigger times: 2
Loss after 36519900 batches: 0.0197
trigger times: 3
Loss after 36651000 batches: 0.0200
trigger times: 4
Loss after 36782100 batches: 0.0191
trigger times: 5
Loss after 36913200 batches: 0.0192
trigger times: 6
Loss after 37044300 batches: 0.0188
trigger times: 7
Loss after 37175400 batches: 0.0185
trigger times: 8
Loss after 37306500 batches: 0.0183
trigger times: 9
Loss after 37437600 batches: 0.0180
trigger times: 10
Loss after 37568700 batches: 0.0178
trigger times: 11
Loss after 37699800 batches: 0.0175
trigger times: 12
Loss after 37830900 batches: 0.0172
trigger times: 13
Loss after 37962000 batches: 0.0175
trigger times: 14
Loss after 38093100 batches: 0.0171
trigger times: 15
Loss after 38224200 batches: 0.0170
trigger times: 16
Loss after 38355300 batches: 0.0170
trigger times: 17
Loss after 38486400 batches: 0.0168
trigger times: 18
Loss after 38617500 batches: 0.0164
trigger times: 19
Loss after 38748600 batches: 0.0168
trigger times: 20
Early stopping!
Start to test process.
Loss after 38879700 batches: 0.0165
Time to train on one home:  429.7223072052002
train_results:  [0.07919007795897719, 0.031169424880564872]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998]]
Round_1_results:  [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 308 < 309; dropping {'Training_Loss': 0.0975435845694452, 'Validation_Loss': 0.22463444703155094, 'Training_R2': 0.9018097980743647, 'Validation_R2': 0.7910455608557687, 'Training_F1': 0.8321790099728124, 'Validation_F1': 0.7082560188237624, 'Training_NEP': 0.33695319733596724, 'Validation_NEP': 0.5406455839995447, 'Training_NDE': 0.07371312746435965, 'Validation_NDE': 0.16640030048727011, 'Training_MAE': 11.159221645518171, 'Validation_MAE': 14.82674491439669, 'Training_MSE': 324.32623, 'Validation_MSE': 614.5117}.
trigger times: 0
Loss after 39010800 batches: 0.0975
trigger times: 0
Loss after 39141900 batches: 0.0277
trigger times: 0
Loss after 39273000 batches: 0.0210
trigger times: 0
Loss after 39404100 batches: 0.0185
trigger times: 0
Loss after 39535200 batches: 0.0172
trigger times: 1
Loss after 39666300 batches: 0.0154
trigger times: 0
Loss after 39797400 batches: 0.0148
trigger times: 1
Loss after 39928500 batches: 0.0140
trigger times: 0
Loss after 40059600 batches: 0.0136
trigger times: 1
Loss after 40190700 batches: 0.0134
trigger times: 2
Loss after 40321800 batches: 0.0128
trigger times: 3
Loss after 40452900 batches: 0.0123
trigger times: 4
Loss after 40584000 batches: 0.0124
trigger times: 5
Loss after 40715100 batches: 0.0122
trigger times: 6
Loss after 40846200 batches: 0.0117
trigger times: 0
Loss after 40977300 batches: 0.0116
trigger times: 1
Loss after 41108400 batches: 0.0113
trigger times: 2
Loss after 41239500 batches: 0.0111
trigger times: 0
Loss after 41370600 batches: 0.0110
trigger times: 1
Loss after 41501700 batches: 0.0109
trigger times: 2
Loss after 41632800 batches: 0.0107
trigger times: 3
Loss after 41763900 batches: 0.0105
trigger times: 4
Loss after 41895000 batches: 0.0108
trigger times: 5
Loss after 42026100 batches: 0.0102
trigger times: 6
Loss after 42157200 batches: 0.0102
trigger times: 7
Loss after 42288300 batches: 0.0100
trigger times: 8
Loss after 42419400 batches: 0.0102
trigger times: 9
Loss after 42550500 batches: 0.0102
trigger times: 10
Loss after 42681600 batches: 0.0102
trigger times: 11
Loss after 42812700 batches: 0.0100
trigger times: 12
Loss after 42943800 batches: 0.0099
trigger times: 13
Loss after 43074900 batches: 0.0094
trigger times: 14
Loss after 43206000 batches: 0.0094
trigger times: 15
Loss after 43337100 batches: 0.0094
trigger times: 16
Loss after 43468200 batches: 0.0093
trigger times: 17
Loss after 43599300 batches: 0.0094
trigger times: 18
Loss after 43730400 batches: 0.0089
trigger times: 19
Loss after 43861500 batches: 0.0090
trigger times: 0
Loss after 43992600 batches: 0.0093
trigger times: 1
Loss after 44123700 batches: 0.0092
trigger times: 2
Loss after 44254800 batches: 0.0089
trigger times: 3
Loss after 44385900 batches: 0.0088
trigger times: 4
Loss after 44517000 batches: 0.0088
trigger times: 5
Loss after 44648100 batches: 0.0086
trigger times: 6
Loss after 44779200 batches: 0.0087
trigger times: 7
Loss after 44910300 batches: 0.0087
trigger times: 8
Loss after 45041400 batches: 0.0086
trigger times: 9
Loss after 45172500 batches: 0.0086
trigger times: 10
Loss after 45303600 batches: 0.0087
trigger times: 11
Loss after 45434700 batches: 0.0086
trigger times: 12
Loss after 45565800 batches: 0.0084
trigger times: 13
Loss after 45696900 batches: 0.0084
trigger times: 14
Loss after 45828000 batches: 0.0084
trigger times: 15
Loss after 45959100 batches: 0.0082
trigger times: 16
Loss after 46090200 batches: 0.0082
trigger times: 17
Loss after 46221300 batches: 0.0081
trigger times: 18
Loss after 46352400 batches: 0.0082
trigger times: 19
Loss after 46483500 batches: 0.0082
trigger times: 20
Early stopping!
Start to test process.
Loss after 46614600 batches: 0.0081
Time to train on one home:  429.7892141342163
trigger times: 0
Loss after 46717200 batches: 0.4202
trigger times: 1
Loss after 46819800 batches: 0.1954
trigger times: 2
Loss after 46922400 batches: 0.1352
trigger times: 3
Loss after 47025000 batches: 0.1032
trigger times: 4
Loss after 47127600 batches: 0.0895
trigger times: 5
Loss after 47230200 batches: 0.0885
trigger times: 6
Loss after 47332800 batches: 0.0705
trigger times: 7
Loss after 47435400 batches: 0.0642
trigger times: 8
Loss after 47538000 batches: 0.0613
trigger times: 9
Loss after 47640600 batches: 0.0557
trigger times: 10
Loss after 47743200 batches: 0.0527
trigger times: 11
Loss after 47845800 batches: 0.0507
trigger times: 12
Loss after 47948400 batches: 0.0490
trigger times: 13
Loss after 48051000 batches: 0.0535
trigger times: 14
Loss after 48153600 batches: 0.0450
trigger times: 15
Loss after 48256200 batches: 0.0487
trigger times: 16
Loss after 48358800 batches: 0.0656
trigger times: 17
Loss after 48461400 batches: 0.0435
trigger times: 18
Loss after 48564000 batches: 0.0422
trigger times: 19
Loss after 48666600 batches: 0.0394
trigger times: 20
Early stopping!
Start to test process.
Loss after 48769200 batches: 0.0361
Time to train on one home:  130.88262724876404
trigger times: 0
Loss after 48900300 batches: 0.1950
trigger times: 0
Loss after 49031400 batches: 0.0617
trigger times: 0
Loss after 49162500 batches: 0.0438
trigger times: 1
Loss after 49293600 batches: 0.0355
trigger times: 0
Loss after 49424700 batches: 0.0318
trigger times: 1
Loss after 49555800 batches: 0.0286
trigger times: 2
Loss after 49686900 batches: 0.0270
trigger times: 0
Loss after 49818000 batches: 0.0250
trigger times: 1
Loss after 49949100 batches: 0.0238
trigger times: 2
Loss after 50080200 batches: 0.0228
trigger times: 3
Loss after 50211300 batches: 0.0220
trigger times: 0
Loss after 50342400 batches: 0.0211
trigger times: 1
Loss after 50473500 batches: 0.0208
trigger times: 2
Loss after 50604600 batches: 0.0200
trigger times: 3
Loss after 50735700 batches: 0.0200
trigger times: 4
Loss after 50866800 batches: 0.0192
trigger times: 5
Loss after 50997900 batches: 0.0187
trigger times: 6
Loss after 51129000 batches: 0.0187
trigger times: 7
Loss after 51260100 batches: 0.0183
trigger times: 0
Loss after 51391200 batches: 0.0178
trigger times: 1
Loss after 51522300 batches: 0.0175
trigger times: 2
Loss after 51653400 batches: 0.0171
trigger times: 3
Loss after 51784500 batches: 0.0168
trigger times: 4
Loss after 51915600 batches: 0.0166
trigger times: 0
Loss after 52046700 batches: 0.0163
trigger times: 0
Loss after 52177800 batches: 0.0161
trigger times: 1
Loss after 52308900 batches: 0.0159
trigger times: 2
Loss after 52440000 batches: 0.0156
trigger times: 3
Loss after 52571100 batches: 0.0156
trigger times: 4
Loss after 52702200 batches: 0.0155
trigger times: 5
Loss after 52833300 batches: 0.0156
trigger times: 6
Loss after 52964400 batches: 0.0153
trigger times: 7
Loss after 53095500 batches: 0.0150
trigger times: 8
Loss after 53226600 batches: 0.0148
trigger times: 9
Loss after 53357700 batches: 0.0149
trigger times: 10
Loss after 53488800 batches: 0.0145
trigger times: 11
Loss after 53619900 batches: 0.0145
trigger times: 12
Loss after 53751000 batches: 0.0144
trigger times: 13
Loss after 53882100 batches: 0.0142
trigger times: 14
Loss after 54013200 batches: 0.0140
trigger times: 15
Loss after 54144300 batches: 0.0139
trigger times: 16
Loss after 54275400 batches: 0.0137
trigger times: 17
Loss after 54406500 batches: 0.0139
trigger times: 18
Loss after 54537600 batches: 0.0136
trigger times: 19
Loss after 54668700 batches: 0.0137
trigger times: 20
Early stopping!
Start to test process.
Loss after 54799800 batches: 0.0135
Time to train on one home:  338.0108013153076
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401]]
Round_2_results:  [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401]
trigger times: 0
Loss after 54930900 batches: 0.0595
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 434 < 435; dropping {'Training_Loss': 0.05947338273081015, 'Validation_Loss': 0.2181053799059656, 'Training_R2': 0.9401770490935984, 'Validation_R2': 0.7971717487341072, 'Training_F1': 0.8649525262810296, 'Validation_F1': 0.7489468237865927, 'Training_NEP': 0.27084695460531705, 'Validation_NEP': 0.48895102814687835, 'Training_NDE': 0.044910151104459776, 'Validation_NDE': 0.16152172739750018, 'Training_MAE': 8.969913989095447, 'Validation_MAE': 13.40906572534192, 'Training_MSE': 197.59767, 'Validation_MSE': 596.4953}.
trigger times: 1
Loss after 55062000 batches: 0.0203
trigger times: 0
Loss after 55193100 batches: 0.0159
trigger times: 1
Loss after 55324200 batches: 0.0141
trigger times: 2
Loss after 55455300 batches: 0.0132
trigger times: 3
Loss after 55586400 batches: 0.0123
trigger times: 0
Loss after 55717500 batches: 0.0120
trigger times: 1
Loss after 55848600 batches: 0.0115
trigger times: 0
Loss after 55979700 batches: 0.0109
trigger times: 1
Loss after 56110800 batches: 0.0108
trigger times: 2
Loss after 56241900 batches: 0.0102
trigger times: 3
Loss after 56373000 batches: 0.0101
trigger times: 4
Loss after 56504100 batches: 0.0100
trigger times: 5
Loss after 56635200 batches: 0.0100
trigger times: 6
Loss after 56766300 batches: 0.0099
trigger times: 7
Loss after 56897400 batches: 0.0097
trigger times: 8
Loss after 57028500 batches: 0.0095
trigger times: 9
Loss after 57159600 batches: 0.0094
trigger times: 10
Loss after 57290700 batches: 0.0096
trigger times: 11
Loss after 57421800 batches: 0.0093
trigger times: 0
Loss after 57552900 batches: 0.0090
trigger times: 1
Loss after 57684000 batches: 0.0093
trigger times: 2
Loss after 57815100 batches: 0.0091
trigger times: 3
Loss after 57946200 batches: 0.0090
trigger times: 4
Loss after 58077300 batches: 0.0089
trigger times: 5
Loss after 58208400 batches: 0.0086
trigger times: 6
Loss after 58339500 batches: 0.0085
trigger times: 7
Loss after 58470600 batches: 0.0085
trigger times: 8
Loss after 58601700 batches: 0.0085
trigger times: 9
Loss after 58732800 batches: 0.0085
trigger times: 10
Loss after 58863900 batches: 0.0084
trigger times: 11
Loss after 58995000 batches: 0.0084
trigger times: 12
Loss after 59126100 batches: 0.0083
trigger times: 13
Loss after 59257200 batches: 0.0083
trigger times: 14
Loss after 59388300 batches: 0.0083
trigger times: 15
Loss after 59519400 batches: 0.0082
trigger times: 16
Loss after 59650500 batches: 0.0081
trigger times: 17
Loss after 59781600 batches: 0.0078
trigger times: 18
Loss after 59912700 batches: 0.0081
trigger times: 19
Loss after 60043800 batches: 0.0077
trigger times: 20
Early stopping!
Start to test process.
Loss after 60174900 batches: 0.0079
Time to train on one home:  302.6247043609619
trigger times: 0
Loss after 60277500 batches: 0.3116
trigger times: 1
Loss after 60380100 batches: 0.1222
trigger times: 2
Loss after 60482700 batches: 0.0827
trigger times: 3
Loss after 60585300 batches: 0.0625
trigger times: 4
Loss after 60687900 batches: 0.0542
trigger times: 5
Loss after 60790500 batches: 0.0496
trigger times: 6
Loss after 60893100 batches: 0.0495
trigger times: 7
Loss after 60995700 batches: 0.0485
trigger times: 8
Loss after 61098300 batches: 0.0424
trigger times: 9
Loss after 61200900 batches: 0.0415
trigger times: 10
Loss after 61303500 batches: 0.0385
trigger times: 11
Loss after 61406100 batches: 0.0408
trigger times: 12
Loss after 61508700 batches: 0.0363
trigger times: 13
Loss after 61611300 batches: 0.0341
trigger times: 14
Loss after 61713900 batches: 0.0354
trigger times: 15
Loss after 61816500 batches: 0.0338
trigger times: 16
Loss after 61919100 batches: 0.0310
trigger times: 17
Loss after 62021700 batches: 0.0302
trigger times: 18
Loss after 62124300 batches: 0.0329
trigger times: 19
Loss after 62226900 batches: 0.0293
trigger times: 20
Early stopping!
Start to test process.
Loss after 62329500 batches: 0.0298
Time to train on one home:  130.83192348480225
trigger times: 0
Loss after 62460600 batches: 0.1085
trigger times: 1
Loss after 62591700 batches: 0.0370
trigger times: 0
Loss after 62722800 batches: 0.0282
trigger times: 0
Loss after 62853900 batches: 0.0245
trigger times: 1
Loss after 62985000 batches: 0.0228
trigger times: 0
Loss after 63116100 batches: 0.0209
trigger times: 1
Loss after 63247200 batches: 0.0201
trigger times: 2
Loss after 63378300 batches: 0.0192
trigger times: 3
Loss after 63509400 batches: 0.0185
trigger times: 4
Loss after 63640500 batches: 0.0178
trigger times: 5
Loss after 63771600 batches: 0.0175
trigger times: 6
Loss after 63902700 batches: 0.0171
trigger times: 7
Loss after 64033800 batches: 0.0164
trigger times: 8
Loss after 64164900 batches: 0.0160
trigger times: 9
Loss after 64296000 batches: 0.0159
trigger times: 10
Loss after 64427100 batches: 0.0158
trigger times: 11
Loss after 64558200 batches: 0.0154
trigger times: 12
Loss after 64689300 batches: 0.0152
trigger times: 0
Loss after 64820400 batches: 0.0151
trigger times: 1
Loss after 64951500 batches: 0.0148
trigger times: 2
Loss after 65082600 batches: 0.0144
trigger times: 0
Loss after 65213700 batches: 0.0144
trigger times: 1
Loss after 65344800 batches: 0.0142
trigger times: 2
Loss after 65475900 batches: 0.0138
trigger times: 3
Loss after 65607000 batches: 0.0141
trigger times: 4
Loss after 65738100 batches: 0.0136
trigger times: 5
Loss after 65869200 batches: 0.0138
trigger times: 6
Loss after 66000300 batches: 0.0137
trigger times: 7
Loss after 66131400 batches: 0.0137
trigger times: 8
Loss after 66262500 batches: 0.0132
trigger times: 9
Loss after 66393600 batches: 0.0132
trigger times: 10
Loss after 66524700 batches: 0.0132
trigger times: 11
Loss after 66655800 batches: 0.0130
trigger times: 12
Loss after 66786900 batches: 0.0132
trigger times: 13
Loss after 66918000 batches: 0.0128
trigger times: 14
Loss after 67049100 batches: 0.0128
trigger times: 15
Loss after 67180200 batches: 0.0127
trigger times: 16
Loss after 67311300 batches: 0.0128
trigger times: 0
Loss after 67442400 batches: 0.0126
trigger times: 0
Loss after 67573500 batches: 0.0124
trigger times: 1
Loss after 67704600 batches: 0.0123
trigger times: 2
Loss after 67835700 batches: 0.0122
trigger times: 3
Loss after 67966800 batches: 0.0123
trigger times: 4
Loss after 68097900 batches: 0.0123
trigger times: 5
Loss after 68229000 batches: 0.0123
trigger times: 0
Loss after 68360100 batches: 0.0121
trigger times: 1
Loss after 68491200 batches: 0.0119
trigger times: 2
Loss after 68622300 batches: 0.0118
trigger times: 3
Loss after 68753400 batches: 0.0121
trigger times: 4
Loss after 68884500 batches: 0.0117
trigger times: 5
Loss after 69015600 batches: 0.0117
trigger times: 6
Loss after 69146700 batches: 0.0116
trigger times: 7
Loss after 69277800 batches: 0.0116
trigger times: 8
Loss after 69408900 batches: 0.0115
trigger times: 9
Loss after 69540000 batches: 0.0116
trigger times: 10
Loss after 69671100 batches: 0.0113
trigger times: 11
Loss after 69802200 batches: 0.0114
trigger times: 12
Loss after 69933300 batches: 0.0113
trigger times: 13
Loss after 70064400 batches: 0.0111
trigger times: 14
Loss after 70195500 batches: 0.0112
trigger times: 15
Loss after 70326600 batches: 0.0111
trigger times: 16
Loss after 70457700 batches: 0.0112
trigger times: 17
Loss after 70588800 batches: 0.0109
trigger times: 18
Loss after 70719900 batches: 0.0109
trigger times: 19
Loss after 70851000 batches: 0.0110
trigger times: 20
Early stopping!
Start to test process.
Loss after 70982100 batches: 0.0111
Time to train on one home:  478.34333205223083
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542]]
Round_3_results:  [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542]
trigger times: 0
Loss after 71113200 batches: 0.0523
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 562 < 563; dropping {'Training_Loss': 0.05227339200956641, 'Validation_Loss': 0.23759647127654818, 'Training_R2': 0.9473553542641141, 'Validation_R2': 0.7790758566553153, 'Training_F1': 0.8744771451566642, 'Validation_F1': 0.7417715432264068, 'Training_NEP': 0.2516507749404651, 'Validation_NEP': 0.4983899827441927, 'Training_NDE': 0.03952126999783948, 'Validation_NDE': 0.17593234194021276, 'Training_MAE': 8.334174588725006, 'Validation_MAE': 13.667921020224103, 'Training_MSE': 173.88739, 'Validation_MSE': 649.71326}.
trigger times: 0
Loss after 71244300 batches: 0.0171
trigger times: 1
Loss after 71375400 batches: 0.0137
trigger times: 2
Loss after 71506500 batches: 0.0122
trigger times: 3
Loss after 71637600 batches: 0.0112
trigger times: 4
Loss after 71768700 batches: 0.0110
trigger times: 5
Loss after 71899800 batches: 0.0108
trigger times: 0
Loss after 72030900 batches: 0.0104
trigger times: 0
Loss after 72162000 batches: 0.0099
trigger times: 1
Loss after 72293100 batches: 0.0096
trigger times: 2
Loss after 72424200 batches: 0.0096
trigger times: 3
Loss after 72555300 batches: 0.0094
trigger times: 4
Loss after 72686400 batches: 0.0089
trigger times: 5
Loss after 72817500 batches: 0.0090
trigger times: 6
Loss after 72948600 batches: 0.0089
trigger times: 7
Loss after 73079700 batches: 0.0090
trigger times: 8
Loss after 73210800 batches: 0.0086
trigger times: 9
Loss after 73341900 batches: 0.0086
trigger times: 10
Loss after 73473000 batches: 0.0086
trigger times: 11
Loss after 73604100 batches: 0.0085
trigger times: 12
Loss after 73735200 batches: 0.0082
trigger times: 13
Loss after 73866300 batches: 0.0080
trigger times: 0
Loss after 73997400 batches: 0.0084
trigger times: 1
Loss after 74128500 batches: 0.0082
trigger times: 2
Loss after 74259600 batches: 0.0081
trigger times: 0
Loss after 74390700 batches: 0.0081
trigger times: 1
Loss after 74521800 batches: 0.0082
trigger times: 2
Loss after 74652900 batches: 0.0082
trigger times: 3
Loss after 74784000 batches: 0.0080
trigger times: 4
Loss after 74915100 batches: 0.0079
trigger times: 5
Loss after 75046200 batches: 0.0079
trigger times: 6
Loss after 75177300 batches: 0.0078
trigger times: 7
Loss after 75308400 batches: 0.0078
trigger times: 8
Loss after 75439500 batches: 0.0081
trigger times: 9
Loss after 75570600 batches: 0.0077
trigger times: 10
Loss after 75701700 batches: 0.0079
trigger times: 11
Loss after 75832800 batches: 0.0076
trigger times: 12
Loss after 75963900 batches: 0.0075
trigger times: 13
Loss after 76095000 batches: 0.0074
trigger times: 14
Loss after 76226100 batches: 0.0074
trigger times: 15
Loss after 76357200 batches: 0.0077
trigger times: 16
Loss after 76488300 batches: 0.0073
trigger times: 17
Loss after 76619400 batches: 0.0073
trigger times: 18
Loss after 76750500 batches: 0.0071
trigger times: 19
Loss after 76881600 batches: 0.0072
trigger times: 20
Early stopping!
Start to test process.
Loss after 77012700 batches: 0.0074
Time to train on one home:  337.1034438610077
trigger times: 0
Loss after 77115300 batches: 0.2759
trigger times: 1
Loss after 77217900 batches: 0.1013
trigger times: 2
Loss after 77320500 batches: 0.0602
trigger times: 3
Loss after 77423100 batches: 0.0495
trigger times: 4
Loss after 77525700 batches: 0.0439
trigger times: 5
Loss after 77628300 batches: 0.0406
trigger times: 6
Loss after 77730900 batches: 0.0394
trigger times: 7
Loss after 77833500 batches: 0.0400
trigger times: 8
Loss after 77936100 batches: 0.0371
trigger times: 9
Loss after 78038700 batches: 0.0374
trigger times: 10
Loss after 78141300 batches: 0.0339
trigger times: 11
Loss after 78243900 batches: 0.0320
trigger times: 12
Loss after 78346500 batches: 0.0343
trigger times: 13
Loss after 78449100 batches: 0.0323
trigger times: 14
Loss after 78551700 batches: 0.0336
trigger times: 15
Loss after 78654300 batches: 0.0314
trigger times: 16
Loss after 78756900 batches: 0.0300
trigger times: 17
Loss after 78859500 batches: 0.0284
trigger times: 18
Loss after 78962100 batches: 0.0284
trigger times: 19
Loss after 79064700 batches: 0.0257
trigger times: 20
Early stopping!
Start to test process.
Loss after 79167300 batches: 0.0264
Time to train on one home:  131.0628125667572
trigger times: 0
Loss after 79298400 batches: 0.0826
trigger times: 0
Loss after 79429500 batches: 0.0277
trigger times: 1
Loss after 79560600 batches: 0.0212
trigger times: 0
Loss after 79691700 batches: 0.0189
trigger times: 1
Loss after 79822800 batches: 0.0175
trigger times: 2
Loss after 79953900 batches: 0.0165
trigger times: 3
Loss after 80085000 batches: 0.0162
trigger times: 4
Loss after 80216100 batches: 0.0154
trigger times: 5
Loss after 80347200 batches: 0.0149
trigger times: 6
Loss after 80478300 batches: 0.0146
trigger times: 7
Loss after 80609400 batches: 0.0143
trigger times: 8
Loss after 80740500 batches: 0.0141
trigger times: 9
Loss after 80871600 batches: 0.0138
trigger times: 10
Loss after 81002700 batches: 0.0136
trigger times: 11
Loss after 81133800 batches: 0.0133
trigger times: 12
Loss after 81264900 batches: 0.0132
trigger times: 13
Loss after 81396000 batches: 0.0131
trigger times: 14
Loss after 81527100 batches: 0.0131
trigger times: 15
Loss after 81658200 batches: 0.0129
trigger times: 16
Loss after 81789300 batches: 0.0128
trigger times: 17
Loss after 81920400 batches: 0.0125
trigger times: 18
Loss after 82051500 batches: 0.0125
trigger times: 19
Loss after 82182600 batches: 0.0124
trigger times: 20
Early stopping!
Start to test process.
Loss after 82313700 batches: 0.0123
Time to train on one home:  181.3170599937439
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237]]
Round_4_results:  [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237]
trigger times: 0
Loss after 82444800 batches: 0.0402
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 653 < 654; dropping {'Training_Loss': 0.04020831507261632, 'Validation_Loss': 0.21981719467375013, 'Training_R2': 0.9595367676517637, 'Validation_R2': 0.7955937559627656, 'Training_F1': 0.888856284042159, 'Validation_F1': 0.7268172076278613, 'Training_NEP': 0.22265946825292657, 'Validation_NEP': 0.5099869805961597, 'Training_NDE': 0.03037646674730822, 'Validation_NDE': 0.162778357658113, 'Training_MAE': 7.374040007194792, 'Validation_MAE': 13.985958814321881, 'Training_MSE': 133.65169, 'Validation_MSE': 601.136}.
trigger times: 0
Loss after 82575900 batches: 0.0147
trigger times: 1
Loss after 82707000 batches: 0.0117
trigger times: 0
Loss after 82838100 batches: 0.0108
trigger times: 1
Loss after 82969200 batches: 0.0102
trigger times: 2
Loss after 83100300 batches: 0.0099
trigger times: 3
Loss after 83231400 batches: 0.0095
trigger times: 4
Loss after 83362500 batches: 0.0094
trigger times: 0
Loss after 83493600 batches: 0.0091
trigger times: 1
Loss after 83624700 batches: 0.0086
trigger times: 2
Loss after 83755800 batches: 0.0087
trigger times: 3
Loss after 83886900 batches: 0.0089
trigger times: 4
Loss after 84018000 batches: 0.0086
trigger times: 5
Loss after 84149100 batches: 0.0081
trigger times: 6
Loss after 84280200 batches: 0.0080
trigger times: 7
Loss after 84411300 batches: 0.0082
trigger times: 8
Loss after 84542400 batches: 0.0080
trigger times: 9
Loss after 84673500 batches: 0.0080
trigger times: 10
Loss after 84804600 batches: 0.0079
trigger times: 0
Loss after 84935700 batches: 0.0080
trigger times: 1
Loss after 85066800 batches: 0.0079
trigger times: 2
Loss after 85197900 batches: 0.0078
trigger times: 0
Loss after 85329000 batches: 0.0079
trigger times: 1
Loss after 85460100 batches: 0.0077
trigger times: 2
Loss after 85591200 batches: 0.0076
trigger times: 3
Loss after 85722300 batches: 0.0076
trigger times: 4
Loss after 85853400 batches: 0.0075
trigger times: 5
Loss after 85984500 batches: 0.0076
trigger times: 6
Loss after 86115600 batches: 0.0076
trigger times: 7
Loss after 86246700 batches: 0.0074
trigger times: 8
Loss after 86377800 batches: 0.0074
trigger times: 9
Loss after 86508900 batches: 0.0077
trigger times: 10
Loss after 86640000 batches: 0.0073
trigger times: 11
Loss after 86771100 batches: 0.0074
trigger times: 12
Loss after 86902200 batches: 0.0074
trigger times: 0
Loss after 87033300 batches: 0.0073
trigger times: 1
Loss after 87164400 batches: 0.0072
trigger times: 2
Loss after 87295500 batches: 0.0073
trigger times: 3
Loss after 87426600 batches: 0.0071
trigger times: 4
Loss after 87557700 batches: 0.0072
trigger times: 5
Loss after 87688800 batches: 0.0069
trigger times: 6
Loss after 87819900 batches: 0.0069
trigger times: 0
Loss after 87951000 batches: 0.0069
trigger times: 1
Loss after 88082100 batches: 0.0070
trigger times: 2
Loss after 88213200 batches: 0.0069
trigger times: 0
Loss after 88344300 batches: 0.0067
trigger times: 1
Loss after 88475400 batches: 0.0067
trigger times: 2
Loss after 88606500 batches: 0.0069
trigger times: 3
Loss after 88737600 batches: 0.0069
trigger times: 4
Loss after 88868700 batches: 0.0068
trigger times: 5
Loss after 88999800 batches: 0.0065
trigger times: 6
Loss after 89130900 batches: 0.0069
trigger times: 7
Loss after 89262000 batches: 0.0066
trigger times: 8
Loss after 89393100 batches: 0.0064
trigger times: 9
Loss after 89524200 batches: 0.0067
trigger times: 10
Loss after 89655300 batches: 0.0065
trigger times: 11
Loss after 89786400 batches: 0.0068
trigger times: 12
Loss after 89917500 batches: 0.0065
trigger times: 13
Loss after 90048600 batches: 0.0066
trigger times: 14
Loss after 90179700 batches: 0.0066
trigger times: 15
Loss after 90310800 batches: 0.0063
trigger times: 16
Loss after 90441900 batches: 0.0064
trigger times: 17
Loss after 90573000 batches: 0.0063
trigger times: 18
Loss after 90704100 batches: 0.0064
trigger times: 19
Loss after 90835200 batches: 0.0063
trigger times: 20
Early stopping!
Start to test process.
Loss after 90966300 batches: 0.0064
Time to train on one home:  479.3026819229126
trigger times: 0
Loss after 91068900 batches: 0.2187
trigger times: 1
Loss after 91171500 batches: 0.0647
trigger times: 2
Loss after 91274100 batches: 0.0519
trigger times: 3
Loss after 91376700 batches: 0.0413
trigger times: 4
Loss after 91479300 batches: 0.0405
trigger times: 5
Loss after 91581900 batches: 0.0361
trigger times: 6
Loss after 91684500 batches: 0.0340
trigger times: 7
Loss after 91787100 batches: 0.0337
trigger times: 8
Loss after 91889700 batches: 0.0339
trigger times: 9
Loss after 91992300 batches: 0.0305
trigger times: 10
Loss after 92094900 batches: 0.0351
trigger times: 11
Loss after 92197500 batches: 0.0307
trigger times: 12
Loss after 92300100 batches: 0.0285
trigger times: 13
Loss after 92402700 batches: 0.0287
trigger times: 14
Loss after 92505300 batches: 0.0297
trigger times: 15
Loss after 92607900 batches: 0.0281
trigger times: 16
Loss after 92710500 batches: 0.0264
trigger times: 17
Loss after 92813100 batches: 0.0256
trigger times: 18
Loss after 92915700 batches: 0.0256
trigger times: 19
Loss after 93018300 batches: 0.0245
trigger times: 20
Early stopping!
Start to test process.
Loss after 93120900 batches: 0.0243
Time to train on one home:  130.79898476600647
trigger times: 0
Loss after 93252000 batches: 0.0739
trigger times: 0
Loss after 93383100 batches: 0.0249
trigger times: 1
Loss after 93514200 batches: 0.0195
trigger times: 2
Loss after 93645300 batches: 0.0176
trigger times: 3
Loss after 93776400 batches: 0.0166
trigger times: 0
Loss after 93907500 batches: 0.0157
trigger times: 1
Loss after 94038600 batches: 0.0152
trigger times: 2
Loss after 94169700 batches: 0.0147
trigger times: 3
Loss after 94300800 batches: 0.0143
trigger times: 4
Loss after 94431900 batches: 0.0140
trigger times: 5
Loss after 94563000 batches: 0.0140
trigger times: 6
Loss after 94694100 batches: 0.0134
trigger times: 7
Loss after 94825200 batches: 0.0133
trigger times: 8
Loss after 94956300 batches: 0.0130
trigger times: 9
Loss after 95087400 batches: 0.0130
trigger times: 10
Loss after 95218500 batches: 0.0129
trigger times: 11
Loss after 95349600 batches: 0.0128
trigger times: 12
Loss after 95480700 batches: 0.0125
trigger times: 13
Loss after 95611800 batches: 0.0123
trigger times: 14
Loss after 95742900 batches: 0.0121
trigger times: 15
Loss after 95874000 batches: 0.0121
trigger times: 16
Loss after 96005100 batches: 0.0122
trigger times: 17
Loss after 96136200 batches: 0.0120
trigger times: 18
Loss after 96267300 batches: 0.0118
trigger times: 19
Loss after 96398400 batches: 0.0115
trigger times: 20
Early stopping!
Start to test process.
Loss after 96529500 batches: 0.0117
Time to train on one home:  195.47806811332703
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822]]
Round_5_results:  [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 766 < 767; dropping {'Training_Loss': 0.040211222696838515, 'Validation_Loss': 0.21818232287963232, 'Training_R2': 0.9595523302287293, 'Validation_R2': 0.7971668849770356, 'Training_F1': 0.8903730726956111, 'Validation_F1': 0.7507479772239111, 'Training_NEP': 0.21963529283927033, 'Validation_NEP': 0.48741026157637724, 'Training_NDE': 0.030364783644543963, 'Validation_NDE': 0.1615256006372435, 'Training_MAE': 7.273885315081075, 'Validation_MAE': 13.366811513729848, 'Training_MSE': 133.60028, 'Validation_MSE': 596.5095}.
trigger times: 0
Loss after 96660600 batches: 0.0402
trigger times: 0
Loss after 96791700 batches: 0.0135
trigger times: 1
Loss after 96922800 batches: 0.0108
trigger times: 2
Loss after 97053900 batches: 0.0099
trigger times: 3
Loss after 97185000 batches: 0.0096
trigger times: 4
Loss after 97316100 batches: 0.0090
trigger times: 5
Loss after 97447200 batches: 0.0085
trigger times: 0
Loss after 97578300 batches: 0.0083
trigger times: 0
Loss after 97709400 batches: 0.0083
trigger times: 1
Loss after 97840500 batches: 0.0081
trigger times: 2
Loss after 97971600 batches: 0.0079
trigger times: 3
Loss after 98102700 batches: 0.0077
trigger times: 4
Loss after 98233800 batches: 0.0077
trigger times: 5
Loss after 98364900 batches: 0.0077
trigger times: 6
Loss after 98496000 batches: 0.0075
trigger times: 7
Loss after 98627100 batches: 0.0076
trigger times: 8
Loss after 98758200 batches: 0.0074
trigger times: 9
Loss after 98889300 batches: 0.0071
trigger times: 10
Loss after 99020400 batches: 0.0072
trigger times: 11
Loss after 99151500 batches: 0.0072
trigger times: 12
Loss after 99282600 batches: 0.0071
trigger times: 0
Loss after 99413700 batches: 0.0071
trigger times: 1
Loss after 99544800 batches: 0.0072
trigger times: 2
Loss after 99675900 batches: 0.0071
trigger times: 3
Loss after 99807000 batches: 0.0072
trigger times: 0
Loss after 99938100 batches: 0.0072
trigger times: 1
Loss after 100069200 batches: 0.0070
trigger times: 2
Loss after 100200300 batches: 0.0069
trigger times: 3
Loss after 100331400 batches: 0.0069
trigger times: 4
Loss after 100462500 batches: 0.0069
trigger times: 5
Loss after 100593600 batches: 0.0068
trigger times: 6
Loss after 100724700 batches: 0.0068
trigger times: 7
Loss after 100855800 batches: 0.0068
trigger times: 8
Loss after 100986900 batches: 0.0071
trigger times: 9
Loss after 101118000 batches: 0.0068
trigger times: 10
Loss after 101249100 batches: 0.0068
trigger times: 11
Loss after 101380200 batches: 0.0067
trigger times: 12
Loss after 101511300 batches: 0.0067
trigger times: 13
Loss after 101642400 batches: 0.0064
trigger times: 0
Loss after 101773500 batches: 0.0066
trigger times: 1
Loss after 101904600 batches: 0.0066
trigger times: 2
Loss after 102035700 batches: 0.0064
trigger times: 3
Loss after 102166800 batches: 0.0066
trigger times: 4
Loss after 102297900 batches: 0.0062
trigger times: 5
Loss after 102429000 batches: 0.0064
trigger times: 6
Loss after 102560100 batches: 0.0065
trigger times: 7
Loss after 102691200 batches: 0.0064
trigger times: 8
Loss after 102822300 batches: 0.0064
trigger times: 9
Loss after 102953400 batches: 0.0064
trigger times: 10
Loss after 103084500 batches: 0.0065
trigger times: 11
Loss after 103215600 batches: 0.0064
trigger times: 12
Loss after 103346700 batches: 0.0064
trigger times: 13
Loss after 103477800 batches: 0.0063
trigger times: 14
Loss after 103608900 batches: 0.0062
trigger times: 15
Loss after 103740000 batches: 0.0062
trigger times: 16
Loss after 103871100 batches: 0.0061
trigger times: 17
Loss after 104002200 batches: 0.0063
trigger times: 18
Loss after 104133300 batches: 0.0063
trigger times: 0
Loss after 104264400 batches: 0.0063
trigger times: 1
Loss after 104395500 batches: 0.0060
trigger times: 2
Loss after 104526600 batches: 0.0061
trigger times: 3
Loss after 104657700 batches: 0.0060
trigger times: 4
Loss after 104788800 batches: 0.0061
trigger times: 5
Loss after 104919900 batches: 0.0060
trigger times: 6
Loss after 105051000 batches: 0.0060
trigger times: 7
Loss after 105182100 batches: 0.0060
trigger times: 8
Loss after 105313200 batches: 0.0060
trigger times: 9
Loss after 105444300 batches: 0.0059
trigger times: 0
Loss after 105575400 batches: 0.0059
trigger times: 1
Loss after 105706500 batches: 0.0060
trigger times: 2
Loss after 105837600 batches: 0.0059
trigger times: 3
Loss after 105968700 batches: 0.0059
trigger times: 4
Loss after 106099800 batches: 0.0059
trigger times: 0
Loss after 106230900 batches: 0.0058
trigger times: 1
Loss after 106362000 batches: 0.0059
trigger times: 2
Loss after 106493100 batches: 0.0058
trigger times: 3
Loss after 106624200 batches: 0.0058
trigger times: 4
Loss after 106755300 batches: 0.0059
trigger times: 5
Loss after 106886400 batches: 0.0059
trigger times: 6
Loss after 107017500 batches: 0.0057
trigger times: 7
Loss after 107148600 batches: 0.0057
trigger times: 8
Loss after 107279700 batches: 0.0058
trigger times: 9
Loss after 107410800 batches: 0.0058
trigger times: 0
Loss after 107541900 batches: 0.0058
trigger times: 1
Loss after 107673000 batches: 0.0056
trigger times: 2
Loss after 107804100 batches: 0.0057
trigger times: 3
Loss after 107935200 batches: 0.0056
trigger times: 4
Loss after 108066300 batches: 0.0058
trigger times: 5
Loss after 108197400 batches: 0.0056
trigger times: 6
Loss after 108328500 batches: 0.0056
trigger times: 7
Loss after 108459600 batches: 0.0057
trigger times: 8
Loss after 108590700 batches: 0.0056
trigger times: 9
Loss after 108721800 batches: 0.0057
trigger times: 10
Loss after 108852900 batches: 0.0056
trigger times: 11
Loss after 108984000 batches: 0.0055
trigger times: 12
Loss after 109115100 batches: 0.0054
trigger times: 13
Loss after 109246200 batches: 0.0054
trigger times: 14
Loss after 109377300 batches: 0.0055
trigger times: 15
Loss after 109508400 batches: 0.0054
trigger times: 16
Loss after 109639500 batches: 0.0055
trigger times: 17
Loss after 109770600 batches: 0.0056
trigger times: 18
Loss after 109901700 batches: 0.0055
trigger times: 19
Loss after 110032800 batches: 0.0055
trigger times: 20
Early stopping!
Start to test process.
Loss after 110163900 batches: 0.0057
Time to train on one home:  748.3434693813324
trigger times: 0
Loss after 110266500 batches: 0.2047
trigger times: 0
Loss after 110369100 batches: 0.0589
trigger times: 1
Loss after 110471700 batches: 0.0465
trigger times: 2
Loss after 110574300 batches: 0.0378
trigger times: 3
Loss after 110676900 batches: 0.0358
trigger times: 0
Loss after 110779500 batches: 0.0355
trigger times: 1
Loss after 110882100 batches: 0.0331
trigger times: 2
Loss after 110984700 batches: 0.0375
trigger times: 3
Loss after 111087300 batches: 0.0306
trigger times: 4
Loss after 111189900 batches: 0.0291
trigger times: 5
Loss after 111292500 batches: 0.0285
trigger times: 6
Loss after 111395100 batches: 0.0264
trigger times: 7
Loss after 111497700 batches: 0.0250
trigger times: 8
Loss after 111600300 batches: 0.0245
trigger times: 9
Loss after 111702900 batches: 0.0259
trigger times: 10
Loss after 111805500 batches: 0.0268
trigger times: 11
Loss after 111908100 batches: 0.0250
trigger times: 12
Loss after 112010700 batches: 0.0261
trigger times: 13
Loss after 112113300 batches: 0.0237
trigger times: 14
Loss after 112215900 batches: 0.0230
trigger times: 15
Loss after 112318500 batches: 0.0236
trigger times: 16
Loss after 112421100 batches: 0.0230
trigger times: 17
Loss after 112523700 batches: 0.0222
trigger times: 18
Loss after 112626300 batches: 0.0223
trigger times: 19
Loss after 112728900 batches: 0.0210
trigger times: 20
Early stopping!
Start to test process.
Loss after 112831500 batches: 0.0221
Time to train on one home:  160.914644241333
trigger times: 0
Loss after 112962600 batches: 0.0654
trigger times: 0
Loss after 113093700 batches: 0.0219
trigger times: 1
Loss after 113224800 batches: 0.0182
trigger times: 2
Loss after 113355900 batches: 0.0168
trigger times: 3
Loss after 113487000 batches: 0.0158
trigger times: 4
Loss after 113618100 batches: 0.0151
trigger times: 5
Loss after 113749200 batches: 0.0145
trigger times: 6
Loss after 113880300 batches: 0.0141
trigger times: 7
Loss after 114011400 batches: 0.0136
trigger times: 8
Loss after 114142500 batches: 0.0133
trigger times: 9
Loss after 114273600 batches: 0.0131
trigger times: 10
Loss after 114404700 batches: 0.0130
trigger times: 11
Loss after 114535800 batches: 0.0128
trigger times: 12
Loss after 114666900 batches: 0.0126
trigger times: 13
Loss after 114798000 batches: 0.0126
trigger times: 14
Loss after 114929100 batches: 0.0125
trigger times: 15
Loss after 115060200 batches: 0.0124
trigger times: 16
Loss after 115191300 batches: 0.0122
trigger times: 17
Loss after 115322400 batches: 0.0121
trigger times: 18
Loss after 115453500 batches: 0.0118
trigger times: 19
Loss after 115584600 batches: 0.0118
trigger times: 20
Early stopping!
Start to test process.
Loss after 115715700 batches: 0.0116
Time to train on one home:  168.8577105998993
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105]]
Round_6_results:  [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105]
trigger times: 0
Loss after 115846800 batches: 0.0343
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 918 < 919; dropping {'Training_Loss': 0.03425085811682467, 'Validation_Loss': 0.2074775074919065, 'Training_R2': 0.9655077318184351, 'Validation_R2': 0.8070741150101897, 'Training_F1': 0.8961600045546687, 'Validation_F1': 0.7559585422003914, 'Training_NEP': 0.2080687742627831, 'Validation_NEP': 0.47626868707884973, 'Training_NDE': 0.025893957962609807, 'Validation_NDE': 0.15363600488964876, 'Training_MAE': 6.890825158707701, 'Validation_MAE': 13.061263317446555, 'Training_MSE': 113.92936, 'Validation_MSE': 567.37354}.
trigger times: 0
Loss after 115977900 batches: 0.0115
trigger times: 1
Loss after 116109000 batches: 0.0094
trigger times: 2
Loss after 116240100 batches: 0.0085
trigger times: 3
Loss after 116371200 batches: 0.0082
trigger times: 4
Loss after 116502300 batches: 0.0079
trigger times: 0
Loss after 116633400 batches: 0.0076
trigger times: 1
Loss after 116764500 batches: 0.0074
trigger times: 2
Loss after 116895600 batches: 0.0073
trigger times: 0
Loss after 117026700 batches: 0.0071
trigger times: 0
Loss after 117157800 batches: 0.0069
trigger times: 1
Loss after 117288900 batches: 0.0069
trigger times: 2
Loss after 117420000 batches: 0.0068
trigger times: 3
Loss after 117551100 batches: 0.0067
trigger times: 4
Loss after 117682200 batches: 0.0067
trigger times: 5
Loss after 117813300 batches: 0.0066
trigger times: 6
Loss after 117944400 batches: 0.0067
trigger times: 7
Loss after 118075500 batches: 0.0064
trigger times: 8
Loss after 118206600 batches: 0.0065
trigger times: 9
Loss after 118337700 batches: 0.0064
trigger times: 10
Loss after 118468800 batches: 0.0064
trigger times: 11
Loss after 118599900 batches: 0.0064
trigger times: 0
Loss after 118731000 batches: 0.0064
trigger times: 1
Loss after 118862100 batches: 0.0064
trigger times: 2
Loss after 118993200 batches: 0.0064
trigger times: 3
Loss after 119124300 batches: 0.0064
trigger times: 4
Loss after 119255400 batches: 0.0063
trigger times: 5
Loss after 119386500 batches: 0.0063
trigger times: 6
Loss after 119517600 batches: 0.0061
trigger times: 7
Loss after 119648700 batches: 0.0061
trigger times: 8
Loss after 119779800 batches: 0.0062
trigger times: 9
Loss after 119910900 batches: 0.0061
trigger times: 10
Loss after 120042000 batches: 0.0062
trigger times: 11
Loss after 120173100 batches: 0.0063
trigger times: 12
Loss after 120304200 batches: 0.0063
trigger times: 13
Loss after 120435300 batches: 0.0062
trigger times: 14
Loss after 120566400 batches: 0.0061
trigger times: 15
Loss after 120697500 batches: 0.0060
trigger times: 16
Loss after 120828600 batches: 0.0062
trigger times: 0
Loss after 120959700 batches: 0.0059
trigger times: 1
Loss after 121090800 batches: 0.0059
trigger times: 2
Loss after 121221900 batches: 0.0059
trigger times: 3
Loss after 121353000 batches: 0.0060
trigger times: 4
Loss after 121484100 batches: 0.0058
trigger times: 5
Loss after 121615200 batches: 0.0058
trigger times: 6
Loss after 121746300 batches: 0.0060
trigger times: 7
Loss after 121877400 batches: 0.0059
trigger times: 8
Loss after 122008500 batches: 0.0058
trigger times: 9
Loss after 122139600 batches: 0.0057
trigger times: 10
Loss after 122270700 batches: 0.0057
trigger times: 11
Loss after 122401800 batches: 0.0056
trigger times: 12
Loss after 122532900 batches: 0.0057
trigger times: 13
Loss after 122664000 batches: 0.0057
trigger times: 14
Loss after 122795100 batches: 0.0056
trigger times: 15
Loss after 122926200 batches: 0.0057
trigger times: 16
Loss after 123057300 batches: 0.0057
trigger times: 0
Loss after 123188400 batches: 0.0057
trigger times: 1
Loss after 123319500 batches: 0.0057
trigger times: 2
Loss after 123450600 batches: 0.0057
trigger times: 3
Loss after 123581700 batches: 0.0055
trigger times: 4
Loss after 123712800 batches: 0.0055
trigger times: 5
Loss after 123843900 batches: 0.0056
trigger times: 6
Loss after 123975000 batches: 0.0054
trigger times: 7
Loss after 124106100 batches: 0.0055
trigger times: 8
Loss after 124237200 batches: 0.0055
trigger times: 9
Loss after 124368300 batches: 0.0056
trigger times: 10
Loss after 124499400 batches: 0.0055
trigger times: 11
Loss after 124630500 batches: 0.0056
trigger times: 12
Loss after 124761600 batches: 0.0055
trigger times: 13
Loss after 124892700 batches: 0.0054
trigger times: 14
Loss after 125023800 batches: 0.0053
trigger times: 15
Loss after 125154900 batches: 0.0053
trigger times: 16
Loss after 125286000 batches: 0.0054
trigger times: 17
Loss after 125417100 batches: 0.0055
trigger times: 18
Loss after 125548200 batches: 0.0053
trigger times: 19
Loss after 125679300 batches: 0.0052
trigger times: 20
Early stopping!
Start to test process.
Loss after 125810400 batches: 0.0053
Time to train on one home:  557.1407649517059
trigger times: 0
Loss after 125913000 batches: 0.1861
trigger times: 0
Loss after 126015600 batches: 0.0572
trigger times: 1
Loss after 126118200 batches: 0.0414
trigger times: 0
Loss after 126220800 batches: 0.0442
trigger times: 0
Loss after 126323400 batches: 0.0338
trigger times: 0
Loss after 126426000 batches: 0.0308
trigger times: 1
Loss after 126528600 batches: 0.0296
trigger times: 2
Loss after 126631200 batches: 0.0300
trigger times: 3
Loss after 126733800 batches: 0.0280
trigger times: 4
Loss after 126836400 batches: 0.0259
trigger times: 5
Loss after 126939000 batches: 0.0249
trigger times: 6
Loss after 127041600 batches: 0.0240
trigger times: 7
Loss after 127144200 batches: 0.0244
trigger times: 8
Loss after 127246800 batches: 0.0235
trigger times: 9
Loss after 127349400 batches: 0.0238
trigger times: 10
Loss after 127452000 batches: 0.0245
trigger times: 11
Loss after 127554600 batches: 0.0237
trigger times: 12
Loss after 127657200 batches: 0.0221
trigger times: 13
Loss after 127759800 batches: 0.0221
trigger times: 14
Loss after 127862400 batches: 0.0214
trigger times: 15
Loss after 127965000 batches: 0.0216
trigger times: 16
Loss after 128067600 batches: 0.0215
trigger times: 17
Loss after 128170200 batches: 0.0216
trigger times: 18
Loss after 128272800 batches: 0.0211
trigger times: 19
Loss after 128375400 batches: 0.0203
trigger times: 20
Early stopping!
Start to test process.
Loss after 128478000 batches: 0.0201
Time to train on one home:  159.26458168029785
trigger times: 0
Loss after 128609100 batches: 0.0684
trigger times: 1
Loss after 128740200 batches: 0.0224
trigger times: 2
Loss after 128871300 batches: 0.0185
trigger times: 3
Loss after 129002400 batches: 0.0169
trigger times: 4
Loss after 129133500 batches: 0.0160
trigger times: 5
Loss after 129264600 batches: 0.0149
trigger times: 6
Loss after 129395700 batches: 0.0144
trigger times: 7
Loss after 129526800 batches: 0.0139
trigger times: 8
Loss after 129657900 batches: 0.0140
trigger times: 9
Loss after 129789000 batches: 0.0135
trigger times: 10
Loss after 129920100 batches: 0.0131
trigger times: 11
Loss after 130051200 batches: 0.0131
trigger times: 12
Loss after 130182300 batches: 0.0127
trigger times: 13
Loss after 130313400 batches: 0.0128
trigger times: 14
Loss after 130444500 batches: 0.0124
trigger times: 15
Loss after 130575600 batches: 0.0123
trigger times: 16
Loss after 130706700 batches: 0.0122
trigger times: 17
Loss after 130837800 batches: 0.0120
trigger times: 18
Loss after 130968900 batches: 0.0119
trigger times: 19
Loss after 131100000 batches: 0.0117
trigger times: 20
Early stopping!
Start to test process.
Loss after 131231100 batches: 0.0119
Time to train on one home:  160.13653421401978
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012]]
Round_7_results:  [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012]
trigger times: 0
Loss after 131362200 batches: 0.0296
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1042 < 1043; dropping {'Training_Loss': 0.02956815390794907, 'Validation_Loss': 0.23187558435731465, 'Training_R2': 0.9702539526929019, 'Validation_R2': 0.7844855847532879, 'Training_F1': 0.9061089784264168, 'Validation_F1': 0.7398278658494576, 'Training_NEP': 0.18805940175776203, 'Validation_NEP': 0.5043627581827438, 'Training_NDE': 0.02233088570659646, 'Validation_NDE': 0.17162431965198688, 'Training_MAE': 6.228154424206161, 'Validation_MAE': 13.831719302276557, 'Training_MSE': 98.2524, 'Validation_MSE': 633.8039}.
trigger times: 0
Loss after 131493300 batches: 0.0106
trigger times: 0
Loss after 131624400 batches: 0.0086
trigger times: 0
Loss after 131755500 batches: 0.0076
trigger times: 0
Loss after 131886600 batches: 0.0072
trigger times: 1
Loss after 132017700 batches: 0.0072
trigger times: 2
Loss after 132148800 batches: 0.0070
trigger times: 0
Loss after 132279900 batches: 0.0069
trigger times: 0
Loss after 132411000 batches: 0.0066
trigger times: 1
Loss after 132542100 batches: 0.0067
trigger times: 0
Loss after 132673200 batches: 0.0064
trigger times: 1
Loss after 132804300 batches: 0.0064
trigger times: 2
Loss after 132935400 batches: 0.0063
trigger times: 3
Loss after 133066500 batches: 0.0062
trigger times: 4
Loss after 133197600 batches: 0.0063
trigger times: 5
Loss after 133328700 batches: 0.0065
trigger times: 6
Loss after 133459800 batches: 0.0065
trigger times: 7
Loss after 133590900 batches: 0.0064
trigger times: 8
Loss after 133722000 batches: 0.0063
trigger times: 9
Loss after 133853100 batches: 0.0062
trigger times: 10
Loss after 133984200 batches: 0.0061
trigger times: 11
Loss after 134115300 batches: 0.0061
trigger times: 12
Loss after 134246400 batches: 0.0060
trigger times: 13
Loss after 134377500 batches: 0.0058
trigger times: 14
Loss after 134508600 batches: 0.0060
trigger times: 15
Loss after 134639700 batches: 0.0058
trigger times: 16
Loss after 134770800 batches: 0.0057
trigger times: 17
Loss after 134901900 batches: 0.0058
trigger times: 18
Loss after 135033000 batches: 0.0060
trigger times: 19
Loss after 135164100 batches: 0.0059
trigger times: 0
Loss after 135295200 batches: 0.0058
trigger times: 1
Loss after 135426300 batches: 0.0057
trigger times: 2
Loss after 135557400 batches: 0.0057
trigger times: 3
Loss after 135688500 batches: 0.0057
trigger times: 4
Loss after 135819600 batches: 0.0057
trigger times: 5
Loss after 135950700 batches: 0.0055
trigger times: 6
Loss after 136081800 batches: 0.0057
trigger times: 7
Loss after 136212900 batches: 0.0056
trigger times: 8
Loss after 136344000 batches: 0.0055
trigger times: 9
Loss after 136475100 batches: 0.0056
trigger times: 10
Loss after 136606200 batches: 0.0056
trigger times: 11
Loss after 136737300 batches: 0.0055
trigger times: 12
Loss after 136868400 batches: 0.0056
trigger times: 13
Loss after 136999500 batches: 0.0056
trigger times: 14
Loss after 137130600 batches: 0.0056
trigger times: 15
Loss after 137261700 batches: 0.0055
trigger times: 16
Loss after 137392800 batches: 0.0055
trigger times: 17
Loss after 137523900 batches: 0.0056
trigger times: 18
Loss after 137655000 batches: 0.0056
trigger times: 19
Loss after 137786100 batches: 0.0056
trigger times: 20
Early stopping!
Start to test process.
Loss after 137917200 batches: 0.0056
Time to train on one home:  372.90989112854004
trigger times: 0
Loss after 138019800 batches: 0.1589
trigger times: 0
Loss after 138122400 batches: 0.0464
trigger times: 0
Loss after 138225000 batches: 0.0378
trigger times: 0
Loss after 138327600 batches: 0.0362
trigger times: 1
Loss after 138430200 batches: 0.0330
trigger times: 2
Loss after 138532800 batches: 0.0291
trigger times: 3
Loss after 138635400 batches: 0.0271
trigger times: 0
Loss after 138738000 batches: 0.0282
trigger times: 0
Loss after 138840600 batches: 0.0260
trigger times: 1
Loss after 138943200 batches: 0.0253
trigger times: 2
Loss after 139045800 batches: 0.0291
trigger times: 3
Loss after 139148400 batches: 0.0244
trigger times: 4
Loss after 139251000 batches: 0.0232
trigger times: 5
Loss after 139353600 batches: 0.0223
trigger times: 6
Loss after 139456200 batches: 0.0229
trigger times: 7
Loss after 139558800 batches: 0.0220
trigger times: 8
Loss after 139661400 batches: 0.0215
trigger times: 9
Loss after 139764000 batches: 0.0212
trigger times: 10
Loss after 139866600 batches: 0.0216
trigger times: 11
Loss after 139969200 batches: 0.0220
trigger times: 12
Loss after 140071800 batches: 0.0215
trigger times: 13
Loss after 140174400 batches: 0.0200
trigger times: 14
Loss after 140277000 batches: 0.0191
trigger times: 15
Loss after 140379600 batches: 0.0192
trigger times: 16
Loss after 140482200 batches: 0.0194
trigger times: 17
Loss after 140584800 batches: 0.0191
trigger times: 18
Loss after 140687400 batches: 0.0195
trigger times: 19
Loss after 140790000 batches: 0.0187
trigger times: 20
Early stopping!
Start to test process.
Loss after 140892600 batches: 0.0199
Time to train on one home:  176.93543243408203
trigger times: 0
Loss after 141023700 batches: 0.0695
trigger times: 0
Loss after 141154800 batches: 0.0221
trigger times: 1
Loss after 141285900 batches: 0.0183
trigger times: 2
Loss after 141417000 batches: 0.0167
trigger times: 3
Loss after 141548100 batches: 0.0158
trigger times: 4
Loss after 141679200 batches: 0.0150
trigger times: 5
Loss after 141810300 batches: 0.0143
trigger times: 6
Loss after 141941400 batches: 0.0141
trigger times: 7
Loss after 142072500 batches: 0.0137
trigger times: 8
Loss after 142203600 batches: 0.0135
trigger times: 9
Loss after 142334700 batches: 0.0132
trigger times: 10
Loss after 142465800 batches: 0.0131
trigger times: 11
Loss after 142596900 batches: 0.0127
trigger times: 12
Loss after 142728000 batches: 0.0125
trigger times: 13
Loss after 142859100 batches: 0.0124
trigger times: 14
Loss after 142990200 batches: 0.0122
trigger times: 15
Loss after 143121300 batches: 0.0120
trigger times: 16
Loss after 143252400 batches: 0.0120
trigger times: 17
Loss after 143383500 batches: 0.0120
trigger times: 18
Loss after 143514600 batches: 0.0119
trigger times: 19
Loss after 143645700 batches: 0.0117
trigger times: 20
Early stopping!
Start to test process.
Loss after 143776800 batches: 0.0117
Time to train on one home:  167.4551341533661
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526, 0.012410333956078289]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012], [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167]]
Round_8_results:  [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167]
trigger times: 0
Loss after 143907900 batches: 0.0339
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1144 < 1145; dropping {'Training_Loss': 0.03391252503783073, 'Validation_Loss': 0.22164464410808352, 'Training_R2': 0.9658455302474769, 'Validation_R2': 0.7940277357831551, 'Training_F1': 0.8998940769179615, 'Validation_F1': 0.7534963558373033, 'Training_NEP': 0.20050111490287872, 'Validation_NEP': 0.4851599833947236, 'Training_NDE': 0.025640366685996657, 'Validation_NDE': 0.1640254535777953, 'Training_MAE': 6.640199288994551, 'Validation_MAE': 13.30509955015661, 'Training_MSE': 112.81358, 'Validation_MSE': 605.7415}.
trigger times: 0
Loss after 144039000 batches: 0.0106
trigger times: 1
Loss after 144170100 batches: 0.0083
trigger times: 2
Loss after 144301200 batches: 0.0076
trigger times: 3
Loss after 144432300 batches: 0.0073
trigger times: 4
Loss after 144563400 batches: 0.0070
trigger times: 5
Loss after 144694500 batches: 0.0067
trigger times: 6
Loss after 144825600 batches: 0.0067
trigger times: 7
Loss after 144956700 batches: 0.0065
trigger times: 8
Loss after 145087800 batches: 0.0063
trigger times: 9
Loss after 145218900 batches: 0.0063
trigger times: 10
Loss after 145350000 batches: 0.0063
trigger times: 11
Loss after 145481100 batches: 0.0062
trigger times: 12
Loss after 145612200 batches: 0.0062
trigger times: 13
Loss after 145743300 batches: 0.0062
trigger times: 14
Loss after 145874400 batches: 0.0061
trigger times: 15
Loss after 146005500 batches: 0.0059
trigger times: 16
Loss after 146136600 batches: 0.0060
trigger times: 17
Loss after 146267700 batches: 0.0059
trigger times: 18
Loss after 146398800 batches: 0.0057
trigger times: 19
Loss after 146529900 batches: 0.0059
trigger times: 20
Early stopping!
Start to test process.
Loss after 146661000 batches: 0.0059
Time to train on one home:  167.44896411895752
trigger times: 0
Loss after 146763600 batches: 0.1398
trigger times: 0
Loss after 146866200 batches: 0.0447
trigger times: 1
Loss after 146968800 batches: 0.0342
trigger times: 2
Loss after 147071400 batches: 0.0303
trigger times: 0
Loss after 147174000 batches: 0.0289
trigger times: 1
Loss after 147276600 batches: 0.0261
trigger times: 2
Loss after 147379200 batches: 0.0252
trigger times: 0
Loss after 147481800 batches: 0.0248
trigger times: 1
Loss after 147584400 batches: 0.0238
trigger times: 2
Loss after 147687000 batches: 0.0224
trigger times: 3
Loss after 147789600 batches: 0.0232
trigger times: 4
Loss after 147892200 batches: 0.0220
trigger times: 5
Loss after 147994800 batches: 0.0233
trigger times: 6
Loss after 148097400 batches: 0.0231
trigger times: 7
Loss after 148200000 batches: 0.0218
trigger times: 8
Loss after 148302600 batches: 0.0230
trigger times: 9
Loss after 148405200 batches: 0.0208
trigger times: 10
Loss after 148507800 batches: 0.0196
trigger times: 11
Loss after 148610400 batches: 0.0197
trigger times: 12
Loss after 148713000 batches: 0.0188
trigger times: 13
Loss after 148815600 batches: 0.0191
trigger times: 14
Loss after 148918200 batches: 0.0203
trigger times: 15
Loss after 149020800 batches: 0.0205
trigger times: 16
Loss after 149123400 batches: 0.0196
trigger times: 17
Loss after 149226000 batches: 0.0211
trigger times: 18
Loss after 149328600 batches: 0.0197
trigger times: 19
Loss after 149431200 batches: 0.0188
trigger times: 20
Early stopping!
Start to test process.
Loss after 149533800 batches: 0.0189
Time to train on one home:  171.40813779830933
trigger times: 0
Loss after 149664900 batches: 0.0603
trigger times: 1
Loss after 149796000 batches: 0.0205
trigger times: 2
Loss after 149927100 batches: 0.0170
trigger times: 3
Loss after 150058200 batches: 0.0157
trigger times: 4
Loss after 150189300 batches: 0.0149
trigger times: 5
Loss after 150320400 batches: 0.0144
trigger times: 6
Loss after 150451500 batches: 0.0141
trigger times: 7
Loss after 150582600 batches: 0.0136
trigger times: 8
Loss after 150713700 batches: 0.0133
trigger times: 9
Loss after 150844800 batches: 0.0129
trigger times: 10
Loss after 150975900 batches: 0.0126
trigger times: 11
Loss after 151107000 batches: 0.0125
trigger times: 12
Loss after 151238100 batches: 0.0126
trigger times: 13
Loss after 151369200 batches: 0.0125
trigger times: 14
Loss after 151500300 batches: 0.0124
trigger times: 15
Loss after 151631400 batches: 0.0119
trigger times: 16
Loss after 151762500 batches: 0.0120
trigger times: 17
Loss after 151893600 batches: 0.0118
trigger times: 18
Loss after 152024700 batches: 0.0116
trigger times: 19
Loss after 152155800 batches: 0.0115
trigger times: 20
Early stopping!
Start to test process.
Loss after 152286900 batches: 0.0113
Time to train on one home:  160.33352303504944
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526, 0.012410333956078289, 0.01202659437884182]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012], [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167], [0.3772937340868844, 0.5928655797026701, 0.6636011186022571, 0.7875590196524677, 0.3335223135047167, 18.60636411918107, 1029.6041]]
Round_9_results:  [0.3772937340868844, 0.5928655797026701, 0.6636011186022571, 0.7875590196524677, 0.3335223135047167, 18.60636411918107, 1029.6041]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1215 < 1216; dropping {'Training_Loss': 0.029388488252770226, 'Validation_Loss': 0.21561385028892094, 'Training_R2': 0.9703952169014631, 'Validation_R2': 0.7995955264914276, 'Training_F1': 0.9058011402370493, 'Validation_F1': 0.7507669322424725, 'Training_NEP': 0.18860760635499013, 'Validation_NEP': 0.48462025913324086, 'Training_NDE': 0.022224836157785907, 'Validation_NDE': 0.15959155855885637, 'Training_MAE': 6.246309873259404, 'Validation_MAE': 13.290298071727959, 'Training_MSE': 97.785805, 'Validation_MSE': 589.3672}.
trigger times: 0
Loss after 152418000 batches: 0.0294
trigger times: 1
Loss after 152549100 batches: 0.0099
trigger times: 0
Loss after 152680200 batches: 0.0080
trigger times: 1
Loss after 152811300 batches: 0.0074
trigger times: 0
Loss after 152942400 batches: 0.0071
trigger times: 0
Loss after 153073500 batches: 0.0068
trigger times: 1
Loss after 153204600 batches: 0.0067
trigger times: 2
Loss after 153335700 batches: 0.0066
trigger times: 3
Loss after 153466800 batches: 0.0065
trigger times: 4
Loss after 153597900 batches: 0.0064
trigger times: 5
Loss after 153729000 batches: 0.0061
trigger times: 6
Loss after 153860100 batches: 0.0062
trigger times: 7
Loss after 153991200 batches: 0.0062
trigger times: 8
Loss after 154122300 batches: 0.0062
trigger times: 9
Loss after 154253400 batches: 0.0062
trigger times: 10
Loss after 154384500 batches: 0.0059
trigger times: 11
Loss after 154515600 batches: 0.0059
trigger times: 12
Loss after 154646700 batches: 0.0059
trigger times: 13
Loss after 154777800 batches: 0.0060
trigger times: 14
Loss after 154908900 batches: 0.0059
trigger times: 15
Loss after 155040000 batches: 0.0059
trigger times: 16
Loss after 155171100 batches: 0.0059
trigger times: 0
Loss after 155302200 batches: 0.0058
trigger times: 1
Loss after 155433300 batches: 0.0058
trigger times: 0
Loss after 155564400 batches: 0.0057
trigger times: 1
Loss after 155695500 batches: 0.0056
trigger times: 2
Loss after 155826600 batches: 0.0059
trigger times: 3
Loss after 155957700 batches: 0.0058
trigger times: 4
Loss after 156088800 batches: 0.0057
trigger times: 5
Loss after 156219900 batches: 0.0056
trigger times: 6
Loss after 156351000 batches: 0.0056
trigger times: 7
Loss after 156482100 batches: 0.0056
trigger times: 8
Loss after 156613200 batches: 0.0058
trigger times: 9
Loss after 156744300 batches: 0.0057
trigger times: 10
Loss after 156875400 batches: 0.0057
trigger times: 11
Loss after 157006500 batches: 0.0056
trigger times: 12
Loss after 157137600 batches: 0.0056
trigger times: 13
Loss after 157268700 batches: 0.0055
trigger times: 14
Loss after 157399800 batches: 0.0056
trigger times: 15
Loss after 157530900 batches: 0.0056
trigger times: 0
Loss after 157662000 batches: 0.0057
trigger times: 1
Loss after 157793100 batches: 0.0056
trigger times: 2
Loss after 157924200 batches: 0.0054
trigger times: 3
Loss after 158055300 batches: 0.0057
trigger times: 4
Loss after 158186400 batches: 0.0055
trigger times: 5
Loss after 158317500 batches: 0.0054
trigger times: 6
Loss after 158448600 batches: 0.0053
trigger times: 7
Loss after 158579700 batches: 0.0053
trigger times: 8
Loss after 158710800 batches: 0.0054
trigger times: 9
Loss after 158841900 batches: 0.0054
trigger times: 10
Loss after 158973000 batches: 0.0054
trigger times: 11
Loss after 159104100 batches: 0.0053
trigger times: 12
Loss after 159235200 batches: 0.0054
trigger times: 13
Loss after 159366300 batches: 0.0053
trigger times: 14
Loss after 159497400 batches: 0.0051
trigger times: 15
Loss after 159628500 batches: 0.0053
trigger times: 16
Loss after 159759600 batches: 0.0054
trigger times: 17
Loss after 159890700 batches: 0.0052
trigger times: 18
Loss after 160021800 batches: 0.0052
trigger times: 19
Loss after 160152900 batches: 0.0052
trigger times: 20
Early stopping!
Start to test process.
Loss after 160284000 batches: 0.0052
Time to train on one home:  444.2574965953827
trigger times: 0
Loss after 160386600 batches: 0.1117
trigger times: 0
Loss after 160489200 batches: 0.0362
trigger times: 0
Loss after 160591800 batches: 0.0284
trigger times: 1
Loss after 160694400 batches: 0.0263
trigger times: 2
Loss after 160797000 batches: 0.0250
trigger times: 3
Loss after 160899600 batches: 0.0250
trigger times: 0
Loss after 161002200 batches: 0.0233
trigger times: 1
Loss after 161104800 batches: 0.0222
trigger times: 2
Loss after 161207400 batches: 0.0224
trigger times: 3
Loss after 161310000 batches: 0.0237
trigger times: 0
Loss after 161412600 batches: 0.0227
trigger times: 1
Loss after 161515200 batches: 0.0232
trigger times: 2
Loss after 161617800 batches: 0.0220
trigger times: 3
Loss after 161720400 batches: 0.0223
trigger times: 4
Loss after 161823000 batches: 0.0214
trigger times: 5
Loss after 161925600 batches: 0.0196
trigger times: 6
Loss after 162028200 batches: 0.0195
trigger times: 7
Loss after 162130800 batches: 0.0186
trigger times: 8
Loss after 162233400 batches: 0.0222
trigger times: 9
Loss after 162336000 batches: 0.0194
trigger times: 10
Loss after 162438600 batches: 0.0205
trigger times: 11
Loss after 162541200 batches: 0.0200
trigger times: 12
Loss after 162643800 batches: 0.0187
trigger times: 13
Loss after 162746400 batches: 0.0185
trigger times: 14
Loss after 162849000 batches: 0.0182
trigger times: 15
Loss after 162951600 batches: 0.0176
trigger times: 16
Loss after 163054200 batches: 0.0175
trigger times: 17
Loss after 163156800 batches: 0.0177
trigger times: 18
Loss after 163259400 batches: 0.0196
trigger times: 19
Loss after 163362000 batches: 0.0174
trigger times: 20
Early stopping!
Start to test process.
Loss after 163464600 batches: 0.0165
Time to train on one home:  187.89508724212646
trigger times: 0
Loss after 163595700 batches: 0.0531
trigger times: 0
Loss after 163726800 batches: 0.0187
trigger times: 1
Loss after 163857900 batches: 0.0160
trigger times: 2
Loss after 163989000 batches: 0.0147
trigger times: 3
Loss after 164120100 batches: 0.0143
trigger times: 4
Loss after 164251200 batches: 0.0137
trigger times: 5
Loss after 164382300 batches: 0.0133
trigger times: 6
Loss after 164513400 batches: 0.0130
trigger times: 7
Loss after 164644500 batches: 0.0130
trigger times: 8
Loss after 164775600 batches: 0.0124
trigger times: 9
Loss after 164906700 batches: 0.0125
trigger times: 10
Loss after 165037800 batches: 0.0122
trigger times: 11
Loss after 165168900 batches: 0.0121
trigger times: 12
Loss after 165300000 batches: 0.0118
trigger times: 13
Loss after 165431100 batches: 0.0118
trigger times: 14
Loss after 165562200 batches: 0.0115
trigger times: 15
Loss after 165693300 batches: 0.0117
trigger times: 16
Loss after 165824400 batches: 0.0115
trigger times: 17
Loss after 165955500 batches: 0.0113
trigger times: 18
Loss after 166086600 batches: 0.0113
trigger times: 19
Loss after 166217700 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 166348800 batches: 0.0110
Time to train on one home:  167.42702746391296
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526, 0.012410333956078289, 0.01202659437884182, 0.010893938768271265]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012], [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167], [0.3772937340868844, 0.5928655797026701, 0.6636011186022571, 0.7875590196524677, 0.3335223135047167, 18.60636411918107, 1029.6041], [0.36474793487124973, 0.6064658220614043, 0.6684562959467083, 0.7573153685201283, 0.3223810685753463, 17.891847021135376, 995.2104]]
Round_10_results:  [0.36474793487124973, 0.6064658220614043, 0.6684562959467083, 0.7573153685201283, 0.3223810685753463, 17.891847021135376, 995.2104]
trigger times: 0
Loss after 166479900 batches: 0.0284
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1329 < 1330; dropping {'Training_Loss': 0.028385130881841452, 'Validation_Loss': 0.22806672751903534, 'Training_R2': 0.9714246606784349, 'Validation_R2': 0.7880455625830912, 'Training_F1': 0.9070255974385082, 'Validation_F1': 0.7537656496223915, 'Training_NEP': 0.1861433471252298, 'Validation_NEP': 0.48569755370900536, 'Training_NDE': 0.021452014441757844, 'Validation_NDE': 0.16878934096939283, 'Training_MAE': 6.164698494722787, 'Validation_MAE': 13.319841958416832, 'Training_MSE': 94.3855, 'Validation_MSE': 623.33435}.
trigger times: 0
Loss after 166611000 batches: 0.0093
trigger times: 0
Loss after 166742100 batches: 0.0078
trigger times: 0
Loss after 166873200 batches: 0.0072
trigger times: 0
Loss after 167004300 batches: 0.0067
trigger times: 1
Loss after 167135400 batches: 0.0066
trigger times: 2
Loss after 167266500 batches: 0.0065
trigger times: 3
Loss after 167397600 batches: 0.0063
trigger times: 0
Loss after 167528700 batches: 0.0062
trigger times: 1
Loss after 167659800 batches: 0.0061
trigger times: 2
Loss after 167790900 batches: 0.0062
trigger times: 3
Loss after 167922000 batches: 0.0061
trigger times: 4
Loss after 168053100 batches: 0.0060
trigger times: 5
Loss after 168184200 batches: 0.0060
trigger times: 6
Loss after 168315300 batches: 0.0059
trigger times: 0
Loss after 168446400 batches: 0.0058
trigger times: 1
Loss after 168577500 batches: 0.0058
trigger times: 2
Loss after 168708600 batches: 0.0058
trigger times: 3
Loss after 168839700 batches: 0.0059
trigger times: 4
Loss after 168970800 batches: 0.0057
trigger times: 5
Loss after 169101900 batches: 0.0057
trigger times: 0
Loss after 169233000 batches: 0.0056
trigger times: 1
Loss after 169364100 batches: 0.0055
trigger times: 2
Loss after 169495200 batches: 0.0058
trigger times: 3
Loss after 169626300 batches: 0.0056
trigger times: 0
Loss after 169757400 batches: 0.0055
trigger times: 1
Loss after 169888500 batches: 0.0055
trigger times: 2
Loss after 170019600 batches: 0.0057
trigger times: 3
Loss after 170150700 batches: 0.0055
trigger times: 4
Loss after 170281800 batches: 0.0055
trigger times: 5
Loss after 170412900 batches: 0.0054
trigger times: 6
Loss after 170544000 batches: 0.0055
trigger times: 7
Loss after 170675100 batches: 0.0055
trigger times: 8
Loss after 170806200 batches: 0.0054
trigger times: 9
Loss after 170937300 batches: 0.0053
trigger times: 10
Loss after 171068400 batches: 0.0055
trigger times: 11
Loss after 171199500 batches: 0.0054
trigger times: 12
Loss after 171330600 batches: 0.0054
trigger times: 13
Loss after 171461700 batches: 0.0055
trigger times: 14
Loss after 171592800 batches: 0.0053
trigger times: 15
Loss after 171723900 batches: 0.0054
trigger times: 16
Loss after 171855000 batches: 0.0055
trigger times: 17
Loss after 171986100 batches: 0.0054
trigger times: 18
Loss after 172117200 batches: 0.0054
trigger times: 19
Loss after 172248300 batches: 0.0053
trigger times: 0
Loss after 172379400 batches: 0.0053
trigger times: 1
Loss after 172510500 batches: 0.0054
trigger times: 2
Loss after 172641600 batches: 0.0053
trigger times: 3
Loss after 172772700 batches: 0.0053
trigger times: 4
Loss after 172903800 batches: 0.0052
trigger times: 5
Loss after 173034900 batches: 0.0051
trigger times: 6
Loss after 173166000 batches: 0.0051
trigger times: 7
Loss after 173297100 batches: 0.0052
trigger times: 8
Loss after 173428200 batches: 0.0053
trigger times: 9
Loss after 173559300 batches: 0.0050
trigger times: 10
Loss after 173690400 batches: 0.0053
trigger times: 11
Loss after 173821500 batches: 0.0052
trigger times: 12
Loss after 173952600 batches: 0.0051
trigger times: 13
Loss after 174083700 batches: 0.0052
trigger times: 14
Loss after 174214800 batches: 0.0051
trigger times: 15
Loss after 174345900 batches: 0.0050
trigger times: 16
Loss after 174477000 batches: 0.0050
trigger times: 17
Loss after 174608100 batches: 0.0050
trigger times: 18
Loss after 174739200 batches: 0.0049
trigger times: 19
Loss after 174870300 batches: 0.0049
trigger times: 20
Early stopping!
Start to test process.
Loss after 175001400 batches: 0.0050
Time to train on one home:  479.4320180416107
trigger times: 0
Loss after 175104000 batches: 0.1119
trigger times: 0
Loss after 175206600 batches: 0.0317
trigger times: 0
Loss after 175309200 batches: 0.0259
trigger times: 1
Loss after 175411800 batches: 0.0247
trigger times: 2
Loss after 175514400 batches: 0.0236
trigger times: 3
Loss after 175617000 batches: 0.0243
trigger times: 4
Loss after 175719600 batches: 0.0231
trigger times: 0
Loss after 175822200 batches: 0.0217
trigger times: 1
Loss after 175924800 batches: 0.0221
trigger times: 0
Loss after 176027400 batches: 0.0220
trigger times: 1
Loss after 176130000 batches: 0.0204
trigger times: 2
Loss after 176232600 batches: 0.0204
trigger times: 0
Loss after 176335200 batches: 0.0200
trigger times: 1
Loss after 176437800 batches: 0.0193
trigger times: 2
Loss after 176540400 batches: 0.0187
trigger times: 3
Loss after 176643000 batches: 0.0185
trigger times: 4
Loss after 176745600 batches: 0.0184
trigger times: 5
Loss after 176848200 batches: 0.0182
trigger times: 6
Loss after 176950800 batches: 0.0177
trigger times: 7
Loss after 177053400 batches: 0.0174
trigger times: 8
Loss after 177156000 batches: 0.0175
trigger times: 9
Loss after 177258600 batches: 0.0174
trigger times: 10
Loss after 177361200 batches: 0.0169
trigger times: 11
Loss after 177463800 batches: 0.0165
trigger times: 12
Loss after 177566400 batches: 0.0166
trigger times: 13
Loss after 177669000 batches: 0.0188
trigger times: 14
Loss after 177771600 batches: 0.0168
trigger times: 15
Loss after 177874200 batches: 0.0176
trigger times: 16
Loss after 177976800 batches: 0.0228
trigger times: 17
Loss after 178079400 batches: 0.0183
trigger times: 18
Loss after 178182000 batches: 0.0170
trigger times: 19
Loss after 178284600 batches: 0.0171
trigger times: 20
Early stopping!
Start to test process.
Loss after 178387200 batches: 0.0171
Time to train on one home:  200.0735001564026
trigger times: 0
Loss after 178518300 batches: 0.0555
trigger times: 0
Loss after 178649400 batches: 0.0184
trigger times: 1
Loss after 178780500 batches: 0.0158
trigger times: 2
Loss after 178911600 batches: 0.0146
trigger times: 3
Loss after 179042700 batches: 0.0141
trigger times: 4
Loss after 179173800 batches: 0.0134
trigger times: 5
Loss after 179304900 batches: 0.0133
trigger times: 6
Loss after 179436000 batches: 0.0129
trigger times: 7
Loss after 179567100 batches: 0.0126
trigger times: 8
Loss after 179698200 batches: 0.0124
trigger times: 9
Loss after 179829300 batches: 0.0121
trigger times: 10
Loss after 179960400 batches: 0.0120
trigger times: 11
Loss after 180091500 batches: 0.0119
trigger times: 12
Loss after 180222600 batches: 0.0117
trigger times: 13
Loss after 180353700 batches: 0.0117
trigger times: 14
Loss after 180484800 batches: 0.0115
trigger times: 15
Loss after 180615900 batches: 0.0114
trigger times: 16
Loss after 180747000 batches: 0.0114
trigger times: 17
Loss after 180878100 batches: 0.0113
trigger times: 18
Loss after 181009200 batches: 0.0114
trigger times: 19
Loss after 181140300 batches: 0.0112
trigger times: 20
Early stopping!
Start to test process.
Loss after 181271400 batches: 0.0110
Time to train on one home:  167.6700723171234
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526, 0.012410333956078289, 0.01202659437884182, 0.010893938768271265, 0.01106632367073798]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012], [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167], [0.3772937340868844, 0.5928655797026701, 0.6636011186022571, 0.7875590196524677, 0.3335223135047167, 18.60636411918107, 1029.6041], [0.36474793487124973, 0.6064658220614043, 0.6684562959467083, 0.7573153685201283, 0.3223810685753463, 17.891847021135376, 995.2104], [0.3664713038338555, 0.6046035803580228, 0.6709737285237666, 0.7533363636113402, 0.3239066068994288, 17.797841603995224, 999.9198]]
Round_11_results:  [0.3664713038338555, 0.6046035803580228, 0.6709737285237666, 0.7533363636113402, 0.3239066068994288, 17.797841603995224, 999.9198]
trigger times: 0
Loss after 181402500 batches: 0.0287
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1450 < 1451; dropping {'Training_Loss': 0.02869643332471825, 'Validation_Loss': 0.21925933245155546, 'Training_R2': 0.9711019167062221, 'Validation_R2': 0.7962534114272862, 'Training_F1': 0.9067438077200478, 'Validation_F1': 0.7589164558632155, 'Training_NEP': 0.1867031366974896, 'Validation_NEP': 0.47909316425328263, 'Training_NDE': 0.021694304070412434, 'Validation_NDE': 0.16225304281931913, 'Training_MAE': 6.183237615173596, 'Validation_MAE': 13.138722199607493, 'Training_MSE': 95.451546, 'Validation_MSE': 599.196}.
trigger times: 0
Loss after 181533600 batches: 0.0092
trigger times: 1
Loss after 181664700 batches: 0.0074
trigger times: 0
Loss after 181795800 batches: 0.0071
trigger times: 1
Loss after 181926900 batches: 0.0067
trigger times: 2
Loss after 182058000 batches: 0.0063
trigger times: 3
Loss after 182189100 batches: 0.0063
trigger times: 4
Loss after 182320200 batches: 0.0061
trigger times: 5
Loss after 182451300 batches: 0.0060
trigger times: 0
Loss after 182582400 batches: 0.0059
trigger times: 0
Loss after 182713500 batches: 0.0059
trigger times: 1
Loss after 182844600 batches: 0.0059
trigger times: 2
Loss after 182975700 batches: 0.0057
trigger times: 3
Loss after 183106800 batches: 0.0057
trigger times: 4
Loss after 183237900 batches: 0.0058
trigger times: 5
Loss after 183369000 batches: 0.0057
trigger times: 0
Loss after 183500100 batches: 0.0056
trigger times: 1
Loss after 183631200 batches: 0.0056
trigger times: 2
Loss after 183762300 batches: 0.0056
trigger times: 3
Loss after 183893400 batches: 0.0054
trigger times: 4
Loss after 184024500 batches: 0.0056
trigger times: 5
Loss after 184155600 batches: 0.0055
trigger times: 6
Loss after 184286700 batches: 0.0055
trigger times: 7
Loss after 184417800 batches: 0.0055
trigger times: 8
Loss after 184548900 batches: 0.0054
trigger times: 9
Loss after 184680000 batches: 0.0054
trigger times: 10
Loss after 184811100 batches: 0.0055
trigger times: 11
Loss after 184942200 batches: 0.0054
trigger times: 12
Loss after 185073300 batches: 0.0054
trigger times: 13
Loss after 185204400 batches: 0.0054
trigger times: 14
Loss after 185335500 batches: 0.0052
trigger times: 15
Loss after 185466600 batches: 0.0053
trigger times: 16
Loss after 185597700 batches: 0.0053
trigger times: 17
Loss after 185728800 batches: 0.0054
trigger times: 18
Loss after 185859900 batches: 0.0055
trigger times: 19
Loss after 185991000 batches: 0.0052
trigger times: 20
Early stopping!
Start to test process.
Loss after 186122100 batches: 0.0052
Time to train on one home:  273.70863914489746
trigger times: 0
Loss after 186224700 batches: 0.1032
trigger times: 0
Loss after 186327300 batches: 0.0306
trigger times: 1
Loss after 186429900 batches: 0.0251
trigger times: 2
Loss after 186532500 batches: 0.0239
trigger times: 0
Loss after 186635100 batches: 0.0247
trigger times: 1
Loss after 186737700 batches: 0.0221
trigger times: 2
Loss after 186840300 batches: 0.0210
trigger times: 0
Loss after 186942900 batches: 0.0212
trigger times: 0
Loss after 187045500 batches: 0.0193
trigger times: 1
Loss after 187148100 batches: 0.0200
trigger times: 2
Loss after 187250700 batches: 0.0192
trigger times: 0
Loss after 187353300 batches: 0.0193
trigger times: 1
Loss after 187455900 batches: 0.0193
trigger times: 2
Loss after 187558500 batches: 0.0185
trigger times: 3
Loss after 187661100 batches: 0.0179
trigger times: 4
Loss after 187763700 batches: 0.0176
trigger times: 5
Loss after 187866300 batches: 0.0188
trigger times: 6
Loss after 187968900 batches: 0.0197
trigger times: 7
Loss after 188071500 batches: 0.0203
trigger times: 8
Loss after 188174100 batches: 0.0175
trigger times: 9
Loss after 188276700 batches: 0.0166
trigger times: 10
Loss after 188379300 batches: 0.0175
trigger times: 11
Loss after 188481900 batches: 0.0170
trigger times: 12
Loss after 188584500 batches: 0.0180
trigger times: 13
Loss after 188687100 batches: 0.0177
trigger times: 14
Loss after 188789700 batches: 0.0166
trigger times: 15
Loss after 188892300 batches: 0.0168
trigger times: 16
Loss after 188994900 batches: 0.0187
trigger times: 17
Loss after 189097500 batches: 0.0162
trigger times: 18
Loss after 189200100 batches: 0.0159
trigger times: 19
Loss after 189302700 batches: 0.0155
trigger times: 20
Early stopping!
Start to test process.
Loss after 189405300 batches: 0.0154
Time to train on one home:  193.54378938674927
trigger times: 0
Loss after 189536400 batches: 0.0541
trigger times: 1
Loss after 189667500 batches: 0.0182
trigger times: 2
Loss after 189798600 batches: 0.0158
trigger times: 3
Loss after 189929700 batches: 0.0146
trigger times: 4
Loss after 190060800 batches: 0.0137
trigger times: 5
Loss after 190191900 batches: 0.0133
trigger times: 6
Loss after 190323000 batches: 0.0130
trigger times: 7
Loss after 190454100 batches: 0.0128
trigger times: 8
Loss after 190585200 batches: 0.0127
trigger times: 9
Loss after 190716300 batches: 0.0123
trigger times: 10
Loss after 190847400 batches: 0.0122
trigger times: 11
Loss after 190978500 batches: 0.0120
trigger times: 12
Loss after 191109600 batches: 0.0118
trigger times: 13
Loss after 191240700 batches: 0.0117
trigger times: 14
Loss after 191371800 batches: 0.0115
trigger times: 15
Loss after 191502900 batches: 0.0115
trigger times: 16
Loss after 191634000 batches: 0.0115
trigger times: 17
Loss after 191765100 batches: 0.0111
trigger times: 18
Loss after 191896200 batches: 0.0112
trigger times: 19
Loss after 192027300 batches: 0.0111
trigger times: 20
Early stopping!
Start to test process.
Loss after 192158400 batches: 0.0110
Time to train on one home:  160.14844346046448
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526, 0.012410333956078289, 0.01202659437884182, 0.010893938768271265, 0.01106632367073798, 0.010575175447393387]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012], [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167], [0.3772937340868844, 0.5928655797026701, 0.6636011186022571, 0.7875590196524677, 0.3335223135047167, 18.60636411918107, 1029.6041], [0.36474793487124973, 0.6064658220614043, 0.6684562959467083, 0.7573153685201283, 0.3223810685753463, 17.891847021135376, 995.2104], [0.3664713038338555, 0.6046035803580228, 0.6709737285237666, 0.7533363636113402, 0.3239066068994288, 17.797841603995224, 999.9198], [0.36360657546255326, 0.6076820853508372, 0.6711735820918979, 0.7556768178669199, 0.32138471227163135, 17.85313567465779, 992.1346]]
Round_12_results:  [0.36360657546255326, 0.6076820853508372, 0.6711735820918979, 0.7556768178669199, 0.32138471227163135, 17.85313567465779, 992.1346]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1540 < 1541; dropping {'Training_Loss': 0.026014210041260945, 'Validation_Loss': 0.21143768313858244, 'Training_R2': 0.9737964662403482, 'Validation_R2': 0.8035948165991013, 'Training_F1': 0.912118681393364, 'Validation_F1': 0.7499339824398057, 'Training_NEP': 0.17588448177439445, 'Validation_NEP': 0.48966102721698346, 'Training_NDE': 0.019671457906815452, 'Validation_NDE': 0.15640673473612196, 'Training_MAE': 5.824945219827008, 'Validation_MAE': 13.428536845449905, 'Training_MSE': 86.55133, 'Validation_MSE': 577.6057}.
trigger times: 0
Loss after 192289500 batches: 0.0260
trigger times: 0
Loss after 192420600 batches: 0.0085
trigger times: 1
Loss after 192551700 batches: 0.0073
trigger times: 2
Loss after 192682800 batches: 0.0066
trigger times: 3
Loss after 192813900 batches: 0.0066
trigger times: 4
Loss after 192945000 batches: 0.0062
trigger times: 5
Loss after 193076100 batches: 0.0061
trigger times: 6
Loss after 193207200 batches: 0.0060
trigger times: 7
Loss after 193338300 batches: 0.0059
trigger times: 0
Loss after 193469400 batches: 0.0059
trigger times: 1
Loss after 193600500 batches: 0.0056
trigger times: 0
Loss after 193731600 batches: 0.0059
trigger times: 1
Loss after 193862700 batches: 0.0057
trigger times: 2
Loss after 193993800 batches: 0.0056
trigger times: 0
Loss after 194124900 batches: 0.0057
trigger times: 1
Loss after 194256000 batches: 0.0056
trigger times: 2
Loss after 194387100 batches: 0.0055
trigger times: 3
Loss after 194518200 batches: 0.0054
trigger times: 4
Loss after 194649300 batches: 0.0056
trigger times: 5
Loss after 194780400 batches: 0.0055
trigger times: 6
Loss after 194911500 batches: 0.0053
trigger times: 0
Loss after 195042600 batches: 0.0054
trigger times: 1
Loss after 195173700 batches: 0.0055
trigger times: 2
Loss after 195304800 batches: 0.0054
trigger times: 3
Loss after 195435900 batches: 0.0053
trigger times: 4
Loss after 195567000 batches: 0.0053
trigger times: 5
Loss after 195698100 batches: 0.0055
trigger times: 6
Loss after 195829200 batches: 0.0055
trigger times: 7
Loss after 195960300 batches: 0.0053
trigger times: 8
Loss after 196091400 batches: 0.0053
trigger times: 9
Loss after 196222500 batches: 0.0053
trigger times: 10
Loss after 196353600 batches: 0.0052
trigger times: 11
Loss after 196484700 batches: 0.0053
trigger times: 0
Loss after 196615800 batches: 0.0054
trigger times: 1
Loss after 196746900 batches: 0.0053
trigger times: 2
Loss after 196878000 batches: 0.0052
trigger times: 3
Loss after 197009100 batches: 0.0050
trigger times: 4
Loss after 197140200 batches: 0.0052
trigger times: 5
Loss after 197271300 batches: 0.0051
trigger times: 6
Loss after 197402400 batches: 0.0052
trigger times: 7
Loss after 197533500 batches: 0.0053
trigger times: 8
Loss after 197664600 batches: 0.0051
trigger times: 9
Loss after 197795700 batches: 0.0051
trigger times: 10
Loss after 197926800 batches: 0.0052
trigger times: 11
Loss after 198057900 batches: 0.0051
trigger times: 12
Loss after 198189000 batches: 0.0050
trigger times: 13
Loss after 198320100 batches: 0.0050
trigger times: 14
Loss after 198451200 batches: 0.0051
trigger times: 15
Loss after 198582300 batches: 0.0050
trigger times: 16
Loss after 198713400 batches: 0.0050
trigger times: 17
Loss after 198844500 batches: 0.0049
trigger times: 18
Loss after 198975600 batches: 0.0050
trigger times: 0
Loss after 199106700 batches: 0.0050
trigger times: 1
Loss after 199237800 batches: 0.0051
trigger times: 2
Loss after 199368900 batches: 0.0050
trigger times: 3
Loss after 199500000 batches: 0.0050
trigger times: 4
Loss after 199631100 batches: 0.0049
trigger times: 5
Loss after 199762200 batches: 0.0050
trigger times: 6
Loss after 199893300 batches: 0.0049
trigger times: 7
Loss after 200024400 batches: 0.0049
trigger times: 8
Loss after 200155500 batches: 0.0050
trigger times: 9
Loss after 200286600 batches: 0.0048
trigger times: 10
Loss after 200417700 batches: 0.0048
trigger times: 11
Loss after 200548800 batches: 0.0048
trigger times: 12
Loss after 200679900 batches: 0.0049
trigger times: 13
Loss after 200811000 batches: 0.0048
trigger times: 14
Loss after 200942100 batches: 0.0048
trigger times: 15
Loss after 201073200 batches: 0.0048
trigger times: 16
Loss after 201204300 batches: 0.0048
trigger times: 17
Loss after 201335400 batches: 0.0047
trigger times: 18
Loss after 201466500 batches: 0.0047
trigger times: 19
Loss after 201597600 batches: 0.0047
trigger times: 20
Early stopping!
Start to test process.
Loss after 201728700 batches: 0.0046
Time to train on one home:  527.2968792915344
trigger times: 0
Loss after 201831300 batches: 0.0946
trigger times: 1
Loss after 201933900 batches: 0.0283
trigger times: 0
Loss after 202036500 batches: 0.0229
trigger times: 1
Loss after 202139100 batches: 0.0227
trigger times: 0
Loss after 202241700 batches: 0.0216
trigger times: 1
Loss after 202344300 batches: 0.0199
trigger times: 2
Loss after 202446900 batches: 0.0197
trigger times: 0
Loss after 202549500 batches: 0.0195
trigger times: 1
Loss after 202652100 batches: 0.0192
trigger times: 2
Loss after 202754700 batches: 0.0175
trigger times: 3
Loss after 202857300 batches: 0.0183
trigger times: 0
Loss after 202959900 batches: 0.0190
trigger times: 1
Loss after 203062500 batches: 0.0186
trigger times: 2
Loss after 203165100 batches: 0.0178
trigger times: 0
Loss after 203267700 batches: 0.0182
trigger times: 1
Loss after 203370300 batches: 0.0171
trigger times: 2
Loss after 203472900 batches: 0.0177
trigger times: 3
Loss after 203575500 batches: 0.0181
trigger times: 4
Loss after 203678100 batches: 0.0166
trigger times: 5
Loss after 203780700 batches: 0.0168
trigger times: 6
Loss after 203883300 batches: 0.0177
trigger times: 7
Loss after 203985900 batches: 0.0202
trigger times: 8
Loss after 204088500 batches: 0.0174
trigger times: 9
Loss after 204191100 batches: 0.0164
trigger times: 10
Loss after 204293700 batches: 0.0158
trigger times: 11
Loss after 204396300 batches: 0.0160
trigger times: 12
Loss after 204498900 batches: 0.0158
trigger times: 13
Loss after 204601500 batches: 0.0159
trigger times: 14
Loss after 204704100 batches: 0.0154
trigger times: 15
Loss after 204806700 batches: 0.0148
trigger times: 16
Loss after 204909300 batches: 0.0149
trigger times: 17
Loss after 205011900 batches: 0.0161
trigger times: 18
Loss after 205114500 batches: 0.0150
trigger times: 19
Loss after 205217100 batches: 0.0148
trigger times: 20
Early stopping!
Start to test process.
Loss after 205319700 batches: 0.0161
Time to train on one home:  210.2454993724823
trigger times: 0
Loss after 205450800 batches: 0.0483
trigger times: 1
Loss after 205581900 batches: 0.0172
trigger times: 2
Loss after 205713000 batches: 0.0148
trigger times: 3
Loss after 205844100 batches: 0.0142
trigger times: 4
Loss after 205975200 batches: 0.0137
trigger times: 5
Loss after 206106300 batches: 0.0132
trigger times: 6
Loss after 206237400 batches: 0.0128
trigger times: 7
Loss after 206368500 batches: 0.0124
trigger times: 8
Loss after 206499600 batches: 0.0122
trigger times: 9
Loss after 206630700 batches: 0.0121
trigger times: 10
Loss after 206761800 batches: 0.0119
trigger times: 11
Loss after 206892900 batches: 0.0118
trigger times: 12
Loss after 207024000 batches: 0.0116
trigger times: 13
Loss after 207155100 batches: 0.0115
trigger times: 14
Loss after 207286200 batches: 0.0115
trigger times: 15
Loss after 207417300 batches: 0.0113
trigger times: 16
Loss after 207548400 batches: 0.0111
trigger times: 17
Loss after 207679500 batches: 0.0110
trigger times: 18
Loss after 207810600 batches: 0.0109
trigger times: 19
Loss after 207941700 batches: 0.0107
trigger times: 20
Early stopping!
Start to test process.
Loss after 208072800 batches: 0.0108
Time to train on one home:  160.25111746788025
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526, 0.012410333956078289, 0.01202659437884182, 0.010893938768271265, 0.01106632367073798, 0.010575175447393387, 0.010503794982589784]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012], [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167], [0.3772937340868844, 0.5928655797026701, 0.6636011186022571, 0.7875590196524677, 0.3335223135047167, 18.60636411918107, 1029.6041], [0.36474793487124973, 0.6064658220614043, 0.6684562959467083, 0.7573153685201283, 0.3223810685753463, 17.891847021135376, 995.2104], [0.3664713038338555, 0.6046035803580228, 0.6709737285237666, 0.7533363636113402, 0.3239066068994288, 17.797841603995224, 999.9198], [0.36360657546255326, 0.6076820853508372, 0.6711735820918979, 0.7556768178669199, 0.32138471227163135, 17.85313567465779, 992.1346], [0.3657842063241535, 0.6053299347838157, 0.6728032514002775, 0.7532968651349748, 0.32331158128519083, 17.7969084383339, 998.08295]]
Round_13_results:  [0.3657842063241535, 0.6053299347838157, 0.6728032514002775, 0.7532968651349748, 0.32331158128519083, 17.7969084383339, 998.08295]
trigger times: 0
Loss after 208203900 batches: 0.0254
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1669 < 1670; dropping {'Training_Loss': 0.025432304514326015, 'Validation_Loss': 0.20718032866716385, 'Training_R2': 0.9743925668696153, 'Validation_R2': 0.8075454455515272, 'Training_F1': 0.9125068405980299, 'Validation_F1': 0.7667188051328132, 'Training_NEP': 0.17513254618612442, 'Validation_NEP': 0.4639232956709966, 'Training_NDE': 0.019223954583622253, 'Validation_NDE': 0.15326066209229738, 'Training_MAE': 5.800042604392581, 'Validation_MAE': 12.72270146715171, 'Training_MSE': 84.58238, 'Validation_MSE': 565.98737}.
trigger times: 1
Loss after 208335000 batches: 0.0083
trigger times: 2
Loss after 208466100 batches: 0.0071
trigger times: 3
Loss after 208597200 batches: 0.0066
trigger times: 4
Loss after 208728300 batches: 0.0062
trigger times: 0
Loss after 208859400 batches: 0.0060
trigger times: 1
Loss after 208990500 batches: 0.0059
trigger times: 2
Loss after 209121600 batches: 0.0059
trigger times: 3
Loss after 209252700 batches: 0.0059
trigger times: 4
Loss after 209383800 batches: 0.0058
trigger times: 5
Loss after 209514900 batches: 0.0056
trigger times: 6
Loss after 209646000 batches: 0.0056
trigger times: 7
Loss after 209777100 batches: 0.0054
trigger times: 8
Loss after 209908200 batches: 0.0054
trigger times: 9
Loss after 210039300 batches: 0.0053
trigger times: 10
Loss after 210170400 batches: 0.0053
trigger times: 11
Loss after 210301500 batches: 0.0054
trigger times: 12
Loss after 210432600 batches: 0.0053
trigger times: 13
Loss after 210563700 batches: 0.0053
trigger times: 14
Loss after 210694800 batches: 0.0053
trigger times: 15
Loss after 210825900 batches: 0.0054
trigger times: 16
Loss after 210957000 batches: 0.0052
trigger times: 17
Loss after 211088100 batches: 0.0051
trigger times: 18
Loss after 211219200 batches: 0.0053
trigger times: 19
Loss after 211350300 batches: 0.0051
trigger times: 20
Early stopping!
Start to test process.
Loss after 211481400 batches: 0.0052
Time to train on one home:  196.92795181274414
trigger times: 0
Loss after 211584000 batches: 0.0928
trigger times: 0
Loss after 211686600 batches: 0.0318
trigger times: 0
Loss after 211789200 batches: 0.0267
trigger times: 1
Loss after 211891800 batches: 0.0218
trigger times: 2
Loss after 211994400 batches: 0.0213
trigger times: 3
Loss after 212097000 batches: 0.0219
trigger times: 4
Loss after 212199600 batches: 0.0195
trigger times: 5
Loss after 212302200 batches: 0.0187
trigger times: 6
Loss after 212404800 batches: 0.0203
trigger times: 7
Loss after 212507400 batches: 0.0181
trigger times: 8
Loss after 212610000 batches: 0.0180
trigger times: 9
Loss after 212712600 batches: 0.0178
trigger times: 10
Loss after 212815200 batches: 0.0176
trigger times: 11
Loss after 212917800 batches: 0.0174
trigger times: 0
Loss after 213020400 batches: 0.0164
trigger times: 1
Loss after 213123000 batches: 0.0159
trigger times: 2
Loss after 213225600 batches: 0.0167
trigger times: 3
Loss after 213328200 batches: 0.0165
trigger times: 4
Loss after 213430800 batches: 0.0160
trigger times: 5
Loss after 213533400 batches: 0.0163
trigger times: 6
Loss after 213636000 batches: 0.0160
trigger times: 7
Loss after 213738600 batches: 0.0160
trigger times: 8
Loss after 213841200 batches: 0.0160
trigger times: 9
Loss after 213943800 batches: 0.0159
trigger times: 10
Loss after 214046400 batches: 0.0156
trigger times: 11
Loss after 214149000 batches: 0.0152
trigger times: 12
Loss after 214251600 batches: 0.0165
trigger times: 13
Loss after 214354200 batches: 0.0151
trigger times: 14
Loss after 214456800 batches: 0.0155
trigger times: 15
Loss after 214559400 batches: 0.0147
trigger times: 16
Loss after 214662000 batches: 0.0158
trigger times: 17
Loss after 214764600 batches: 0.0165
trigger times: 18
Loss after 214867200 batches: 0.0168
trigger times: 19
Loss after 214969800 batches: 0.0145
trigger times: 20
Early stopping!
Start to test process.
Loss after 215072400 batches: 0.0144
Time to train on one home:  211.59133172035217
trigger times: 0
Loss after 215203500 batches: 0.0549
trigger times: 1
Loss after 215334600 batches: 0.0179
trigger times: 2
Loss after 215465700 batches: 0.0153
trigger times: 3
Loss after 215596800 batches: 0.0142
trigger times: 4
Loss after 215727900 batches: 0.0137
trigger times: 5
Loss after 215859000 batches: 0.0133
trigger times: 6
Loss after 215990100 batches: 0.0127
trigger times: 7
Loss after 216121200 batches: 0.0125
trigger times: 8
Loss after 216252300 batches: 0.0124
trigger times: 9
Loss after 216383400 batches: 0.0122
trigger times: 10
Loss after 216514500 batches: 0.0119
trigger times: 11
Loss after 216645600 batches: 0.0117
trigger times: 12
Loss after 216776700 batches: 0.0117
trigger times: 13
Loss after 216907800 batches: 0.0115
trigger times: 14
Loss after 217038900 batches: 0.0113
trigger times: 15
Loss after 217170000 batches: 0.0112
trigger times: 16
Loss after 217301100 batches: 0.0111
trigger times: 17
Loss after 217432200 batches: 0.0110
trigger times: 18
Loss after 217563300 batches: 0.0108
trigger times: 19
Loss after 217694400 batches: 0.0108
trigger times: 20
Early stopping!
Start to test process.
Loss after 217825500 batches: 0.0108
Time to train on one home:  161.42833542823792
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526, 0.012410333956078289, 0.01202659437884182, 0.010893938768271265, 0.01106632367073798, 0.010575175447393387, 0.010503794982589784, 0.01012047204975843]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012], [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167], [0.3772937340868844, 0.5928655797026701, 0.6636011186022571, 0.7875590196524677, 0.3335223135047167, 18.60636411918107, 1029.6041], [0.36474793487124973, 0.6064658220614043, 0.6684562959467083, 0.7573153685201283, 0.3223810685753463, 17.891847021135376, 995.2104], [0.3664713038338555, 0.6046035803580228, 0.6709737285237666, 0.7533363636113402, 0.3239066068994288, 17.797841603995224, 999.9198], [0.36360657546255326, 0.6076820853508372, 0.6711735820918979, 0.7556768178669199, 0.32138471227163135, 17.85313567465779, 992.1346], [0.3657842063241535, 0.6053299347838157, 0.6728032514002775, 0.7532968651349748, 0.32331158128519083, 17.7969084383339, 998.08295], [0.361652703748809, 0.6098542734072183, 0.6776885777608854, 0.7497262707738148, 0.31960526757274665, 17.71255186676552, 986.6414]]
Round_14_results:  [0.361652703748809, 0.6098542734072183, 0.6776885777608854, 0.7497262707738148, 0.31960526757274665, 17.71255186676552, 986.6414]
trigger times: 0
Loss after 217956600 batches: 0.0278
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1751 < 1752; dropping {'Training_Loss': 0.027802979310025584, 'Validation_Loss': 0.20686008863978916, 'Training_R2': 0.9720044877822125, 'Validation_R2': 0.8077712558789807, 'Training_F1': 0.9090166774376977, 'Validation_F1': 0.7547365745013623, 'Training_NEP': 0.1821547140908732, 'Validation_NEP': 0.48235776882031806, 'Training_NDE': 0.021016727943004923, 'Validation_NDE': 0.15308083865090374, 'Training_MAE': 6.032602878937187, 'Validation_MAE': 13.228251200850291, 'Training_MSE': 92.47031, 'Validation_MSE': 565.3233}.
trigger times: 0
Loss after 218087700 batches: 0.0085
trigger times: 0
Loss after 218218800 batches: 0.0071
trigger times: 0
Loss after 218349900 batches: 0.0066
trigger times: 1
Loss after 218481000 batches: 0.0063
trigger times: 2
Loss after 218612100 batches: 0.0060
trigger times: 3
Loss after 218743200 batches: 0.0060
trigger times: 0
Loss after 218874300 batches: 0.0058
trigger times: 0
Loss after 219005400 batches: 0.0058
trigger times: 1
Loss after 219136500 batches: 0.0057
trigger times: 2
Loss after 219267600 batches: 0.0056
trigger times: 3
Loss after 219398700 batches: 0.0057
trigger times: 4
Loss after 219529800 batches: 0.0055
trigger times: 0
Loss after 219660900 batches: 0.0054
trigger times: 1
Loss after 219792000 batches: 0.0055
trigger times: 2
Loss after 219923100 batches: 0.0053
trigger times: 3
Loss after 220054200 batches: 0.0054
trigger times: 4
Loss after 220185300 batches: 0.0054
trigger times: 0
Loss after 220316400 batches: 0.0054
trigger times: 1
Loss after 220447500 batches: 0.0054
trigger times: 2
Loss after 220578600 batches: 0.0053
trigger times: 3
Loss after 220709700 batches: 0.0054
trigger times: 4
Loss after 220840800 batches: 0.0052
trigger times: 5
Loss after 220971900 batches: 0.0052
trigger times: 6
Loss after 221103000 batches: 0.0052
trigger times: 7
Loss after 221234100 batches: 0.0052
trigger times: 8
Loss after 221365200 batches: 0.0051
trigger times: 9
Loss after 221496300 batches: 0.0051
trigger times: 10
Loss after 221627400 batches: 0.0051
trigger times: 11
Loss after 221758500 batches: 0.0050
trigger times: 12
Loss after 221889600 batches: 0.0051
trigger times: 13
Loss after 222020700 batches: 0.0050
trigger times: 14
Loss after 222151800 batches: 0.0051
trigger times: 15
Loss after 222282900 batches: 0.0050
trigger times: 16
Loss after 222414000 batches: 0.0052
trigger times: 17
Loss after 222545100 batches: 0.0050
trigger times: 18
Loss after 222676200 batches: 0.0050
trigger times: 19
Loss after 222807300 batches: 0.0050
trigger times: 20
Early stopping!
Start to test process.
Loss after 222938400 batches: 0.0050
Time to train on one home:  289.41951394081116
trigger times: 0
Loss after 223041000 batches: 0.0818
trigger times: 0
Loss after 223143600 batches: 0.0248
trigger times: 1
Loss after 223246200 batches: 0.0209
trigger times: 2
Loss after 223348800 batches: 0.0200
trigger times: 3
Loss after 223451400 batches: 0.0192
trigger times: 4
Loss after 223554000 batches: 0.0179
trigger times: 5
Loss after 223656600 batches: 0.0176
trigger times: 6
Loss after 223759200 batches: 0.0172
trigger times: 7
Loss after 223861800 batches: 0.0171
trigger times: 8
Loss after 223964400 batches: 0.0172
trigger times: 9
Loss after 224067000 batches: 0.0164
trigger times: 10
Loss after 224169600 batches: 0.0164
trigger times: 11
Loss after 224272200 batches: 0.0162
trigger times: 12
Loss after 224374800 batches: 0.0174
trigger times: 13
Loss after 224477400 batches: 0.0169
trigger times: 14
Loss after 224580000 batches: 0.0161
trigger times: 15
Loss after 224682600 batches: 0.0152
trigger times: 16
Loss after 224785200 batches: 0.0154
trigger times: 17
Loss after 224887800 batches: 0.0162
trigger times: 18
Loss after 224990400 batches: 0.0156
trigger times: 19
Loss after 225093000 batches: 0.0164
trigger times: 20
Early stopping!
Start to test process.
Loss after 225195600 batches: 0.0172
Time to train on one home:  137.31241726875305
trigger times: 0
Loss after 225326700 batches: 0.0479
trigger times: 1
Loss after 225457800 batches: 0.0165
trigger times: 0
Loss after 225588900 batches: 0.0143
trigger times: 1
Loss after 225720000 batches: 0.0134
trigger times: 2
Loss after 225851100 batches: 0.0130
trigger times: 3
Loss after 225982200 batches: 0.0128
trigger times: 4
Loss after 226113300 batches: 0.0126
trigger times: 5
Loss after 226244400 batches: 0.0122
trigger times: 6
Loss after 226375500 batches: 0.0117
trigger times: 7
Loss after 226506600 batches: 0.0118
trigger times: 8
Loss after 226637700 batches: 0.0116
trigger times: 9
Loss after 226768800 batches: 0.0113
trigger times: 10
Loss after 226899900 batches: 0.0113
trigger times: 11
Loss after 227031000 batches: 0.0112
trigger times: 12
Loss after 227162100 batches: 0.0111
trigger times: 13
Loss after 227293200 batches: 0.0110
trigger times: 14
Loss after 227424300 batches: 0.0110
trigger times: 15
Loss after 227555400 batches: 0.0110
trigger times: 16
Loss after 227686500 batches: 0.0108
trigger times: 17
Loss after 227817600 batches: 0.0109
trigger times: 18
Loss after 227948700 batches: 0.0105
trigger times: 19
Loss after 228079800 batches: 0.0106
trigger times: 20
Early stopping!
Start to test process.
Loss after 228210900 batches: 0.0104
Time to train on one home:  175.3965060710907
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526, 0.012410333956078289, 0.01202659437884182, 0.010893938768271265, 0.01106632367073798, 0.010575175447393387, 0.010503794982589784, 0.01012047204975843, 0.010866048209303356]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012], [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167], [0.3772937340868844, 0.5928655797026701, 0.6636011186022571, 0.7875590196524677, 0.3335223135047167, 18.60636411918107, 1029.6041], [0.36474793487124973, 0.6064658220614043, 0.6684562959467083, 0.7573153685201283, 0.3223810685753463, 17.891847021135376, 995.2104], [0.3664713038338555, 0.6046035803580228, 0.6709737285237666, 0.7533363636113402, 0.3239066068994288, 17.797841603995224, 999.9198], [0.36360657546255326, 0.6076820853508372, 0.6711735820918979, 0.7556768178669199, 0.32138471227163135, 17.85313567465779, 992.1346], [0.3657842063241535, 0.6053299347838157, 0.6728032514002775, 0.7532968651349748, 0.32331158128519083, 17.7969084383339, 998.08295], [0.361652703748809, 0.6098542734072183, 0.6776885777608854, 0.7497262707738148, 0.31960526757274665, 17.71255186676552, 986.6414], [0.35913565589321983, 0.6125684953219025, 0.6786061046593334, 0.7338401978555779, 0.31738179167088215, 17.337237700125648, 979.7774]]
Round_15_results:  [0.35913565589321983, 0.6125684953219025, 0.6786061046593334, 0.7338401978555779, 0.31738179167088215, 17.337237700125648, 979.7774]
trigger times: 0
Loss after 228342000 batches: 0.0244
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1835 < 1836; dropping {'Training_Loss': 0.024446326167375413, 'Validation_Loss': 0.23068606522348192, 'Training_R2': 0.9753746610154299, 'Validation_R2': 0.785651964211727, 'Training_F1': 0.9144365412935632, 'Validation_F1': 0.7552161213717902, 'Training_NEP': 0.1711805657560215, 'Validation_NEP': 0.49448488539316016, 'Training_NDE': 0.018486679076161132, 'Validation_NDE': 0.17069547653594064, 'Training_MAE': 5.669160850169911, 'Validation_MAE': 13.560827049602317, 'Training_MSE': 81.33849, 'Validation_MSE': 630.3737}.
trigger times: 0
Loss after 228473100 batches: 0.0082
trigger times: 1
Loss after 228604200 batches: 0.0068
trigger times: 0
Loss after 228735300 batches: 0.0063
trigger times: 1
Loss after 228866400 batches: 0.0063
trigger times: 2
Loss after 228997500 batches: 0.0059
trigger times: 3
Loss after 229128600 batches: 0.0059
trigger times: 4
Loss after 229259700 batches: 0.0057
trigger times: 5
Loss after 229390800 batches: 0.0056
trigger times: 6
Loss after 229521900 batches: 0.0056
trigger times: 7
Loss after 229653000 batches: 0.0054
trigger times: 8
Loss after 229784100 batches: 0.0055
trigger times: 9
Loss after 229915200 batches: 0.0053
trigger times: 10
Loss after 230046300 batches: 0.0054
trigger times: 11
Loss after 230177400 batches: 0.0054
trigger times: 0
Loss after 230308500 batches: 0.0053
trigger times: 1
Loss after 230439600 batches: 0.0053
trigger times: 0
Loss after 230570700 batches: 0.0053
trigger times: 1
Loss after 230701800 batches: 0.0052
trigger times: 2
Loss after 230832900 batches: 0.0052
trigger times: 3
Loss after 230964000 batches: 0.0052
trigger times: 4
Loss after 231095100 batches: 0.0053
trigger times: 5
Loss after 231226200 batches: 0.0051
trigger times: 6
Loss after 231357300 batches: 0.0050
trigger times: 7
Loss after 231488400 batches: 0.0050
trigger times: 8
Loss after 231619500 batches: 0.0050
trigger times: 9
Loss after 231750600 batches: 0.0051
trigger times: 10
Loss after 231881700 batches: 0.0050
trigger times: 0
Loss after 232012800 batches: 0.0051
trigger times: 1
Loss after 232143900 batches: 0.0053
trigger times: 2
Loss after 232275000 batches: 0.0051
trigger times: 3
Loss after 232406100 batches: 0.0051
trigger times: 4
Loss after 232537200 batches: 0.0051
trigger times: 5
Loss after 232668300 batches: 0.0050
trigger times: 6
Loss after 232799400 batches: 0.0050
trigger times: 7
Loss after 232930500 batches: 0.0050
trigger times: 0
Loss after 233061600 batches: 0.0049
trigger times: 1
Loss after 233192700 batches: 0.0050
trigger times: 2
Loss after 233323800 batches: 0.0049
trigger times: 3
Loss after 233454900 batches: 0.0050
trigger times: 4
Loss after 233586000 batches: 0.0049
trigger times: 5
Loss after 233717100 batches: 0.0049
trigger times: 6
Loss after 233848200 batches: 0.0050
trigger times: 7
Loss after 233979300 batches: 0.0049
trigger times: 8
Loss after 234110400 batches: 0.0049
trigger times: 9
Loss after 234241500 batches: 0.0050
trigger times: 10
Loss after 234372600 batches: 0.0050
trigger times: 11
Loss after 234503700 batches: 0.0049
trigger times: 12
Loss after 234634800 batches: 0.0048
trigger times: 13
Loss after 234765900 batches: 0.0048
trigger times: 14
Loss after 234897000 batches: 0.0047
trigger times: 0
Loss after 235028100 batches: 0.0048
trigger times: 1
Loss after 235159200 batches: 0.0049
trigger times: 2
Loss after 235290300 batches: 0.0048
trigger times: 3
Loss after 235421400 batches: 0.0048
trigger times: 4
Loss after 235552500 batches: 0.0049
trigger times: 5
Loss after 235683600 batches: 0.0048
trigger times: 6
Loss after 235814700 batches: 0.0048
trigger times: 7
Loss after 235945800 batches: 0.0048
trigger times: 8
Loss after 236076900 batches: 0.0047
trigger times: 9
Loss after 236208000 batches: 0.0047
trigger times: 10
Loss after 236339100 batches: 0.0046
trigger times: 11
Loss after 236470200 batches: 0.0047
trigger times: 0
Loss after 236601300 batches: 0.0047
trigger times: 1
Loss after 236732400 batches: 0.0047
trigger times: 2
Loss after 236863500 batches: 0.0047
trigger times: 3
Loss after 236994600 batches: 0.0046
trigger times: 4
Loss after 237125700 batches: 0.0048
trigger times: 5
Loss after 237256800 batches: 0.0046
trigger times: 6
Loss after 237387900 batches: 0.0046
trigger times: 7
Loss after 237519000 batches: 0.0046
trigger times: 8
Loss after 237650100 batches: 0.0046
trigger times: 9
Loss after 237781200 batches: 0.0046
trigger times: 10
Loss after 237912300 batches: 0.0047
trigger times: 11
Loss after 238043400 batches: 0.0046
trigger times: 12
Loss after 238174500 batches: 0.0045
trigger times: 13
Loss after 238305600 batches: 0.0047
trigger times: 14
Loss after 238436700 batches: 0.0045
trigger times: 15
Loss after 238567800 batches: 0.0046
trigger times: 16
Loss after 238698900 batches: 0.0045
trigger times: 17
Loss after 238830000 batches: 0.0045
trigger times: 18
Loss after 238961100 batches: 0.0046
trigger times: 19
Loss after 239092200 batches: 0.0045
trigger times: 20
Early stopping!
Start to test process.
Loss after 239223300 batches: 0.0047
Time to train on one home:  610.6657104492188
trigger times: 0
Loss after 239325900 batches: 0.0829
trigger times: 0
Loss after 239428500 batches: 0.0239
trigger times: 1
Loss after 239531100 batches: 0.0208
trigger times: 0
Loss after 239633700 batches: 0.0202
trigger times: 1
Loss after 239736300 batches: 0.0191
trigger times: 2
Loss after 239838900 batches: 0.0186
trigger times: 3
Loss after 239941500 batches: 0.0194
trigger times: 4
Loss after 240044100 batches: 0.0190
trigger times: 5
Loss after 240146700 batches: 0.0175
trigger times: 6
Loss after 240249300 batches: 0.0175
trigger times: 7
Loss after 240351900 batches: 0.0175
trigger times: 8
Loss after 240454500 batches: 0.0169
trigger times: 9
Loss after 240557100 batches: 0.0189
trigger times: 10
Loss after 240659700 batches: 0.0163
trigger times: 11
Loss after 240762300 batches: 0.0166
trigger times: 12
Loss after 240864900 batches: 0.0156
trigger times: 13
Loss after 240967500 batches: 0.0159
trigger times: 14
Loss after 241070100 batches: 0.0170
trigger times: 15
Loss after 241172700 batches: 0.0169
trigger times: 16
Loss after 241275300 batches: 0.0149
trigger times: 17
Loss after 241377900 batches: 0.0147
trigger times: 18
Loss after 241480500 batches: 0.0150
trigger times: 19
Loss after 241583100 batches: 0.0144
trigger times: 20
Early stopping!
Start to test process.
Loss after 241685700 batches: 0.0149
Time to train on one home:  148.2063024044037
trigger times: 0
Loss after 241816800 batches: 0.0424
trigger times: 0
Loss after 241947900 batches: 0.0153
trigger times: 1
Loss after 242079000 batches: 0.0137
trigger times: 2
Loss after 242210100 batches: 0.0129
trigger times: 3
Loss after 242341200 batches: 0.0127
trigger times: 4
Loss after 242472300 batches: 0.0123
trigger times: 5
Loss after 242603400 batches: 0.0121
trigger times: 6
Loss after 242734500 batches: 0.0118
trigger times: 7
Loss after 242865600 batches: 0.0119
trigger times: 8
Loss after 242996700 batches: 0.0114
trigger times: 9
Loss after 243127800 batches: 0.0112
trigger times: 10
Loss after 243258900 batches: 0.0111
trigger times: 11
Loss after 243390000 batches: 0.0110
trigger times: 12
Loss after 243521100 batches: 0.0109
trigger times: 13
Loss after 243652200 batches: 0.0110
trigger times: 14
Loss after 243783300 batches: 0.0108
trigger times: 15
Loss after 243914400 batches: 0.0107
trigger times: 16
Loss after 244045500 batches: 0.0107
trigger times: 17
Loss after 244176600 batches: 0.0105
trigger times: 18
Loss after 244307700 batches: 0.0104
trigger times: 19
Loss after 244438800 batches: 0.0105
trigger times: 20
Early stopping!
Start to test process.
Loss after 244569900 batches: 0.0105
Time to train on one home:  167.28789925575256
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526, 0.012410333956078289, 0.01202659437884182, 0.010893938768271265, 0.01106632367073798, 0.010575175447393387, 0.010503794982589784, 0.01012047204975843, 0.010866048209303356, 0.009993974293054141]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012], [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167], [0.3772937340868844, 0.5928655797026701, 0.6636011186022571, 0.7875590196524677, 0.3335223135047167, 18.60636411918107, 1029.6041], [0.36474793487124973, 0.6064658220614043, 0.6684562959467083, 0.7573153685201283, 0.3223810685753463, 17.891847021135376, 995.2104], [0.3664713038338555, 0.6046035803580228, 0.6709737285237666, 0.7533363636113402, 0.3239066068994288, 17.797841603995224, 999.9198], [0.36360657546255326, 0.6076820853508372, 0.6711735820918979, 0.7556768178669199, 0.32138471227163135, 17.85313567465779, 992.1346], [0.3657842063241535, 0.6053299347838157, 0.6728032514002775, 0.7532968651349748, 0.32331158128519083, 17.7969084383339, 998.08295], [0.361652703748809, 0.6098542734072183, 0.6776885777608854, 0.7497262707738148, 0.31960526757274665, 17.71255186676552, 986.6414], [0.35913565589321983, 0.6125684953219025, 0.6786061046593334, 0.7338401978555779, 0.31738179167088215, 17.337237700125648, 979.7774], [0.3646826330158446, 0.6065532684133498, 0.676905662030514, 0.7423034996622564, 0.3223094329971329, 17.537186238757172, 994.98926]]
Round_16_results:  [0.3646826330158446, 0.6065532684133498, 0.676905662030514, 0.7423034996622564, 0.3223094329971329, 17.537186238757172, 994.98926]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 1965 < 1966; dropping {'Training_Loss': 0.023718108915073692, 'Validation_Loss': 0.21248875061670938, 'Training_R2': 0.9761168158406471, 'Validation_R2': 0.8026674451691039, 'Training_F1': 0.9152909213573291, 'Validation_F1': 0.7630878324667462, 'Training_NEP': 0.16951740010946564, 'Validation_NEP': 0.47307455505168383, 'Training_NDE': 0.017929530275601926, 'Validation_NDE': 0.15714524445741276, 'Training_MAE': 5.614080102369132, 'Validation_MAE': 12.973666965619637, 'Training_MSE': 78.88713, 'Validation_MSE': 580.33307}.
trigger times: 0
Loss after 244701000 batches: 0.0237
trigger times: 0
Loss after 244832100 batches: 0.0078
trigger times: 0
Loss after 244963200 batches: 0.0066
trigger times: 1
Loss after 245094300 batches: 0.0060
trigger times: 2
Loss after 245225400 batches: 0.0058
trigger times: 3
Loss after 245356500 batches: 0.0058
trigger times: 4
Loss after 245487600 batches: 0.0058
trigger times: 5
Loss after 245618700 batches: 0.0055
trigger times: 6
Loss after 245749800 batches: 0.0054
trigger times: 7
Loss after 245880900 batches: 0.0052
trigger times: 8
Loss after 246012000 batches: 0.0053
trigger times: 9
Loss after 246143100 batches: 0.0051
trigger times: 10
Loss after 246274200 batches: 0.0051
trigger times: 11
Loss after 246405300 batches: 0.0053
trigger times: 12
Loss after 246536400 batches: 0.0053
trigger times: 13
Loss after 246667500 batches: 0.0051
trigger times: 14
Loss after 246798600 batches: 0.0049
trigger times: 15
Loss after 246929700 batches: 0.0051
trigger times: 16
Loss after 247060800 batches: 0.0050
trigger times: 17
Loss after 247191900 batches: 0.0050
trigger times: 18
Loss after 247323000 batches: 0.0050
trigger times: 19
Loss after 247454100 batches: 0.0049
trigger times: 20
Early stopping!
Start to test process.
Loss after 247585200 batches: 0.0051
Time to train on one home:  174.95709538459778
trigger times: 0
Loss after 247687800 batches: 0.0954
trigger times: 0
Loss after 247790400 batches: 0.0254
trigger times: 0
Loss after 247893000 batches: 0.0208
trigger times: 1
Loss after 247995600 batches: 0.0191
trigger times: 0
Loss after 248098200 batches: 0.0187
trigger times: 0
Loss after 248200800 batches: 0.0182
trigger times: 0
Loss after 248303400 batches: 0.0174
trigger times: 1
Loss after 248406000 batches: 0.0172
trigger times: 2
Loss after 248508600 batches: 0.0165
trigger times: 3
Loss after 248611200 batches: 0.0166
trigger times: 4
Loss after 248713800 batches: 0.0170
trigger times: 5
Loss after 248816400 batches: 0.0194
trigger times: 6
Loss after 248919000 batches: 0.0167
trigger times: 7
Loss after 249021600 batches: 0.0156
trigger times: 8
Loss after 249124200 batches: 0.0160
trigger times: 9
Loss after 249226800 batches: 0.0171
trigger times: 10
Loss after 249329400 batches: 0.0151
trigger times: 0
Loss after 249432000 batches: 0.0157
trigger times: 0
Loss after 249534600 batches: 0.0153
trigger times: 1
Loss after 249637200 batches: 0.0149
trigger times: 2
Loss after 249739800 batches: 0.0149
trigger times: 3
Loss after 249842400 batches: 0.0151
trigger times: 4
Loss after 249945000 batches: 0.0151
trigger times: 5
Loss after 250047600 batches: 0.0160
trigger times: 6
Loss after 250150200 batches: 0.0170
trigger times: 7
Loss after 250252800 batches: 0.0143
trigger times: 8
Loss after 250355400 batches: 0.0150
trigger times: 9
Loss after 250458000 batches: 0.0155
trigger times: 10
Loss after 250560600 batches: 0.0142
trigger times: 11
Loss after 250663200 batches: 0.0143
trigger times: 12
Loss after 250765800 batches: 0.0156
trigger times: 13
Loss after 250868400 batches: 0.0179
trigger times: 14
Loss after 250971000 batches: 0.0144
trigger times: 15
Loss after 251073600 batches: 0.0145
trigger times: 16
Loss after 251176200 batches: 0.0150
trigger times: 17
Loss after 251278800 batches: 0.0141
trigger times: 18
Loss after 251381400 batches: 0.0135
trigger times: 19
Loss after 251484000 batches: 0.0136
trigger times: 20
Early stopping!
Start to test process.
Loss after 251586600 batches: 0.0136
Time to train on one home:  234.17669677734375
trigger times: 0
Loss after 251717700 batches: 0.0473
trigger times: 0
Loss after 251848800 batches: 0.0158
trigger times: 1
Loss after 251979900 batches: 0.0140
trigger times: 2
Loss after 252111000 batches: 0.0131
trigger times: 3
Loss after 252242100 batches: 0.0126
trigger times: 4
Loss after 252373200 batches: 0.0125
trigger times: 5
Loss after 252504300 batches: 0.0121
trigger times: 6
Loss after 252635400 batches: 0.0118
trigger times: 7
Loss after 252766500 batches: 0.0115
trigger times: 8
Loss after 252897600 batches: 0.0114
trigger times: 9
Loss after 253028700 batches: 0.0113
trigger times: 10
Loss after 253159800 batches: 0.0113
trigger times: 11
Loss after 253290900 batches: 0.0112
trigger times: 12
Loss after 253422000 batches: 0.0108
trigger times: 13
Loss after 253553100 batches: 0.0108
trigger times: 14
Loss after 253684200 batches: 0.0107
trigger times: 15
Loss after 253815300 batches: 0.0105
trigger times: 16
Loss after 253946400 batches: 0.0106
trigger times: 17
Loss after 254077500 batches: 0.0104
trigger times: 18
Loss after 254208600 batches: 0.0104
trigger times: 19
Loss after 254339700 batches: 0.0102
trigger times: 20
Early stopping!
Start to test process.
Loss after 254470800 batches: 0.0102
Time to train on one home:  168.52236437797546
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526, 0.012410333956078289, 0.01202659437884182, 0.010893938768271265, 0.01106632367073798, 0.010575175447393387, 0.010503794982589784, 0.01012047204975843, 0.010866048209303356, 0.009993974293054141, 0.009646852175512956]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012], [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167], [0.3772937340868844, 0.5928655797026701, 0.6636011186022571, 0.7875590196524677, 0.3335223135047167, 18.60636411918107, 1029.6041], [0.36474793487124973, 0.6064658220614043, 0.6684562959467083, 0.7573153685201283, 0.3223810685753463, 17.891847021135376, 995.2104], [0.3664713038338555, 0.6046035803580228, 0.6709737285237666, 0.7533363636113402, 0.3239066068994288, 17.797841603995224, 999.9198], [0.36360657546255326, 0.6076820853508372, 0.6711735820918979, 0.7556768178669199, 0.32138471227163135, 17.85313567465779, 992.1346], [0.3657842063241535, 0.6053299347838157, 0.6728032514002775, 0.7532968651349748, 0.32331158128519083, 17.7969084383339, 998.08295], [0.361652703748809, 0.6098542734072183, 0.6776885777608854, 0.7497262707738148, 0.31960526757274665, 17.71255186676552, 986.6414], [0.35913565589321983, 0.6125684953219025, 0.6786061046593334, 0.7338401978555779, 0.31738179167088215, 17.337237700125648, 979.7774], [0.3646826330158446, 0.6065532684133498, 0.676905662030514, 0.7423034996622564, 0.3223094329971329, 17.537186238757172, 994.98926], [0.36132707529597813, 0.610169510531575, 0.6799456397665353, 0.7368694861698217, 0.3193470270266852, 17.408805722318668, 985.8442]]
Round_17_results:  [0.36132707529597813, 0.610169510531575, 0.6799456397665353, 0.7368694861698217, 0.3193470270266852, 17.408805722318668, 985.8442]
trigger times: 0
Loss after 254601900 batches: 0.0258
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2049 < 2050; dropping {'Training_Loss': 0.025828670894832543, 'Validation_Loss': 0.21889766802390417, 'Training_R2': 0.974000888275128, 'Validation_R2': 0.7967131750612644, 'Training_F1': 0.9114487651642197, 'Validation_F1': 0.7634640219076034, 'Training_NEP': 0.17728031042586948, 'Validation_NEP': 0.46976508606179784, 'Training_NDE': 0.019517994656809617, 'Validation_NDE': 0.16188691129724928, 'Training_MAE': 5.871172296537142, 'Validation_MAE': 12.882907595771188, 'Training_MSE': 85.87611, 'Validation_MSE': 597.84393}.
trigger times: 1
Loss after 254733000 batches: 0.0078
trigger times: 2
Loss after 254864100 batches: 0.0065
trigger times: 3
Loss after 254995200 batches: 0.0062
trigger times: 4
Loss after 255126300 batches: 0.0060
trigger times: 5
Loss after 255257400 batches: 0.0057
trigger times: 0
Loss after 255388500 batches: 0.0057
trigger times: 1
Loss after 255519600 batches: 0.0055
trigger times: 2
Loss after 255650700 batches: 0.0054
trigger times: 3
Loss after 255781800 batches: 0.0054
trigger times: 4
Loss after 255912900 batches: 0.0053
trigger times: 5
Loss after 256044000 batches: 0.0052
trigger times: 6
Loss after 256175100 batches: 0.0053
trigger times: 7
Loss after 256306200 batches: 0.0052
trigger times: 8
Loss after 256437300 batches: 0.0052
trigger times: 9
Loss after 256568400 batches: 0.0051
trigger times: 10
Loss after 256699500 batches: 0.0051
trigger times: 11
Loss after 256830600 batches: 0.0050
trigger times: 0
Loss after 256961700 batches: 0.0051
trigger times: 1
Loss after 257092800 batches: 0.0050
trigger times: 2
Loss after 257223900 batches: 0.0050
trigger times: 0
Loss after 257355000 batches: 0.0050
trigger times: 1
Loss after 257486100 batches: 0.0050
trigger times: 2
Loss after 257617200 batches: 0.0050
trigger times: 3
Loss after 257748300 batches: 0.0049
trigger times: 4
Loss after 257879400 batches: 0.0049
trigger times: 5
Loss after 258010500 batches: 0.0049
trigger times: 6
Loss after 258141600 batches: 0.0049
trigger times: 7
Loss after 258272700 batches: 0.0048
trigger times: 8
Loss after 258403800 batches: 0.0049
trigger times: 9
Loss after 258534900 batches: 0.0047
trigger times: 10
Loss after 258666000 batches: 0.0048
trigger times: 11
Loss after 258797100 batches: 0.0049
trigger times: 12
Loss after 258928200 batches: 0.0048
trigger times: 13
Loss after 259059300 batches: 0.0048
trigger times: 14
Loss after 259190400 batches: 0.0048
trigger times: 15
Loss after 259321500 batches: 0.0049
trigger times: 16
Loss after 259452600 batches: 0.0048
trigger times: 17
Loss after 259583700 batches: 0.0049
trigger times: 18
Loss after 259714800 batches: 0.0049
trigger times: 19
Loss after 259845900 batches: 0.0048
trigger times: 20
Early stopping!
Start to test process.
Loss after 259977000 batches: 0.0047
Time to train on one home:  310.9911251068115
trigger times: 0
Loss after 260079600 batches: 0.0784
trigger times: 1
Loss after 260182200 batches: 0.0238
trigger times: 0
Loss after 260284800 batches: 0.0212
trigger times: 0
Loss after 260387400 batches: 0.0184
trigger times: 0
Loss after 260490000 batches: 0.0183
trigger times: 1
Loss after 260592600 batches: 0.0169
trigger times: 2
Loss after 260695200 batches: 0.0165
trigger times: 3
Loss after 260797800 batches: 0.0160
trigger times: 4
Loss after 260900400 batches: 0.0176
trigger times: 5
Loss after 261003000 batches: 0.0184
trigger times: 0
Loss after 261105600 batches: 0.0180
trigger times: 1
Loss after 261208200 batches: 0.0166
trigger times: 0
Loss after 261310800 batches: 0.0157
trigger times: 0
Loss after 261413400 batches: 0.0162
trigger times: 1
Loss after 261516000 batches: 0.0165
trigger times: 2
Loss after 261618600 batches: 0.0158
trigger times: 3
Loss after 261721200 batches: 0.0148
trigger times: 4
Loss after 261823800 batches: 0.0152
trigger times: 5
Loss after 261926400 batches: 0.0162
trigger times: 6
Loss after 262029000 batches: 0.0148
trigger times: 7
Loss after 262131600 batches: 0.0146
trigger times: 8
Loss after 262234200 batches: 0.0141
trigger times: 9
Loss after 262336800 batches: 0.0144
trigger times: 10
Loss after 262439400 batches: 0.0145
trigger times: 11
Loss after 262542000 batches: 0.0139
trigger times: 12
Loss after 262644600 batches: 0.0138
trigger times: 13
Loss after 262747200 batches: 0.0139
trigger times: 14
Loss after 262849800 batches: 0.0137
trigger times: 15
Loss after 262952400 batches: 0.0141
trigger times: 16
Loss after 263055000 batches: 0.0142
trigger times: 17
Loss after 263157600 batches: 0.0135
trigger times: 18
Loss after 263260200 batches: 0.0135
trigger times: 19
Loss after 263362800 batches: 0.0145
trigger times: 20
Early stopping!
Start to test process.
Loss after 263465400 batches: 0.0136
Time to train on one home:  205.2563512325287
trigger times: 0
Loss after 263596500 batches: 0.0425
trigger times: 0
Loss after 263727600 batches: 0.0150
trigger times: 0
Loss after 263858700 batches: 0.0132
trigger times: 1
Loss after 263989800 batches: 0.0125
trigger times: 2
Loss after 264120900 batches: 0.0123
trigger times: 3
Loss after 264252000 batches: 0.0119
trigger times: 4
Loss after 264383100 batches: 0.0116
trigger times: 5
Loss after 264514200 batches: 0.0112
trigger times: 6
Loss after 264645300 batches: 0.0113
trigger times: 7
Loss after 264776400 batches: 0.0110
trigger times: 8
Loss after 264907500 batches: 0.0109
trigger times: 9
Loss after 265038600 batches: 0.0110
trigger times: 10
Loss after 265169700 batches: 0.0109
trigger times: 11
Loss after 265300800 batches: 0.0106
trigger times: 12
Loss after 265431900 batches: 0.0106
trigger times: 13
Loss after 265563000 batches: 0.0105
trigger times: 14
Loss after 265694100 batches: 0.0105
trigger times: 15
Loss after 265825200 batches: 0.0102
trigger times: 16
Loss after 265956300 batches: 0.0103
trigger times: 17
Loss after 266087400 batches: 0.0103
trigger times: 18
Loss after 266218500 batches: 0.0101
trigger times: 19
Loss after 266349600 batches: 0.0101
trigger times: 20
Early stopping!
Start to test process.
Loss after 266480700 batches: 0.0099
Time to train on one home:  174.50862097740173
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526, 0.012410333956078289, 0.01202659437884182, 0.010893938768271265, 0.01106632367073798, 0.010575175447393387, 0.010503794982589784, 0.01012047204975843, 0.010866048209303356, 0.009993974293054141, 0.009646852175512956, 0.009412276137404883]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012], [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167], [0.3772937340868844, 0.5928655797026701, 0.6636011186022571, 0.7875590196524677, 0.3335223135047167, 18.60636411918107, 1029.6041], [0.36474793487124973, 0.6064658220614043, 0.6684562959467083, 0.7573153685201283, 0.3223810685753463, 17.891847021135376, 995.2104], [0.3664713038338555, 0.6046035803580228, 0.6709737285237666, 0.7533363636113402, 0.3239066068994288, 17.797841603995224, 999.9198], [0.36360657546255326, 0.6076820853508372, 0.6711735820918979, 0.7556768178669199, 0.32138471227163135, 17.85313567465779, 992.1346], [0.3657842063241535, 0.6053299347838157, 0.6728032514002775, 0.7532968651349748, 0.32331158128519083, 17.7969084383339, 998.08295], [0.361652703748809, 0.6098542734072183, 0.6776885777608854, 0.7497262707738148, 0.31960526757274665, 17.71255186676552, 986.6414], [0.35913565589321983, 0.6125684953219025, 0.6786061046593334, 0.7338401978555779, 0.31738179167088215, 17.337237700125648, 979.7774], [0.3646826330158446, 0.6065532684133498, 0.676905662030514, 0.7423034996622564, 0.3223094329971329, 17.537186238757172, 994.98926], [0.36132707529597813, 0.610169510531575, 0.6799456397665353, 0.7368694861698217, 0.3193470270266852, 17.408805722318668, 985.8442], [0.36047327684031594, 0.6110878050586475, 0.6782920101137582, 0.7353344564641495, 0.31859476512034884, 17.372540095330994, 983.52185]]
Round_18_results:  [0.36047327684031594, 0.6110878050586475, 0.6782920101137582, 0.7353344564641495, 0.31859476512034884, 17.372540095330994, 983.52185]
[34m[1mwandb[39m[22m: [33mWARNING[39m Step must only increase in log calls.  Step 2148 < 2149; dropping {'Training_Loss': 0.025318678248814255, 'Validation_Loss': 0.22252258989546034, 'Training_R2': 0.9745056338452052, 'Validation_R2': 0.7932872849311422, 'Training_F1': 0.9136854852945724, 'Validation_F1': 0.7627009794555419, 'Training_NEP': 0.1727134516716662, 'Validation_NEP': 0.47197162258671593, 'Training_NDE': 0.019139073198105025, 'Validation_NDE': 0.16461510960413103, 'Training_MAE': 5.71992699165548, 'Validation_MAE': 12.943419981643718, 'Training_MSE': 84.20892, 'Validation_MSE': 607.91907}.
trigger times: 0
Loss after 266611800 batches: 0.0253
trigger times: 1
Loss after 266742900 batches: 0.0077
trigger times: 0
Loss after 266874000 batches: 0.0065
trigger times: 1
Loss after 267005100 batches: 0.0061
trigger times: 2
Loss after 267136200 batches: 0.0060
trigger times: 3
Loss after 267267300 batches: 0.0058
trigger times: 0
Loss after 267398400 batches: 0.0057
trigger times: 1
Loss after 267529500 batches: 0.0055
trigger times: 2
Loss after 267660600 batches: 0.0053
trigger times: 3
Loss after 267791700 batches: 0.0053
trigger times: 4
Loss after 267922800 batches: 0.0053
trigger times: 5
Loss after 268053900 batches: 0.0052
trigger times: 6
Loss after 268185000 batches: 0.0053
trigger times: 7
Loss after 268316100 batches: 0.0052
trigger times: 8
Loss after 268447200 batches: 0.0051
trigger times: 9
Loss after 268578300 batches: 0.0051
trigger times: 0
Loss after 268709400 batches: 0.0051
trigger times: 1
Loss after 268840500 batches: 0.0050
trigger times: 2
Loss after 268971600 batches: 0.0051
trigger times: 3
Loss after 269102700 batches: 0.0050
trigger times: 0
Loss after 269233800 batches: 0.0050
trigger times: 1
Loss after 269364900 batches: 0.0050
trigger times: 2
Loss after 269496000 batches: 0.0050
trigger times: 3
Loss after 269627100 batches: 0.0049
trigger times: 4
Loss after 269758200 batches: 0.0049
trigger times: 5
Loss after 269889300 batches: 0.0049
trigger times: 6
Loss after 270020400 batches: 0.0049
trigger times: 7
Loss after 270151500 batches: 0.0049
trigger times: 8
Loss after 270282600 batches: 0.0049
trigger times: 9
Loss after 270413700 batches: 0.0048
trigger times: 10
Loss after 270544800 batches: 0.0047
trigger times: 11
Loss after 270675900 batches: 0.0050
trigger times: 12
Loss after 270807000 batches: 0.0049
trigger times: 13
Loss after 270938100 batches: 0.0047
trigger times: 14
Loss after 271069200 batches: 0.0048
trigger times: 15
Loss after 271200300 batches: 0.0049
trigger times: 16
Loss after 271331400 batches: 0.0048
trigger times: 17
Loss after 271462500 batches: 0.0047
trigger times: 18
Loss after 271593600 batches: 0.0047
trigger times: 19
Loss after 271724700 batches: 0.0047
trigger times: 20
Early stopping!
Start to test process.
Loss after 271855800 batches: 0.0047
Time to train on one home:  303.17834186553955
trigger times: 0
Loss after 271958400 batches: 0.0827
trigger times: 0
Loss after 272061000 batches: 0.0329
trigger times: 0
Loss after 272163600 batches: 0.0200
trigger times: 1
Loss after 272266200 batches: 0.0179
trigger times: 2
Loss after 272368800 batches: 0.0175
trigger times: 3
Loss after 272471400 batches: 0.0163
trigger times: 4
Loss after 272574000 batches: 0.0157
trigger times: 5
Loss after 272676600 batches: 0.0159
trigger times: 0
Loss after 272779200 batches: 0.0155
trigger times: 1
Loss after 272881800 batches: 0.0152
trigger times: 2
Loss after 272984400 batches: 0.0151
trigger times: 3
Loss after 273087000 batches: 0.0146
trigger times: 4
Loss after 273189600 batches: 0.0141
trigger times: 5
Loss after 273292200 batches: 0.0147
trigger times: 6
Loss after 273394800 batches: 0.0141
trigger times: 7
Loss after 273497400 batches: 0.0143
trigger times: 0
Loss after 273600000 batches: 0.0143
trigger times: 1
Loss after 273702600 batches: 0.0146
trigger times: 2
Loss after 273805200 batches: 0.0149
trigger times: 3
Loss after 273907800 batches: 0.0146
trigger times: 4
Loss after 274010400 batches: 0.0159
trigger times: 5
Loss after 274113000 batches: 0.0145
trigger times: 6
Loss after 274215600 batches: 0.0134
trigger times: 7
Loss after 274318200 batches: 0.0133
trigger times: 8
Loss after 274420800 batches: 0.0135
trigger times: 9
Loss after 274523400 batches: 0.0131
trigger times: 10
Loss after 274626000 batches: 0.0134
trigger times: 11
Loss after 274728600 batches: 0.0134
trigger times: 12
Loss after 274831200 batches: 0.0141
trigger times: 13
Loss after 274933800 batches: 0.0134
trigger times: 14
Loss after 275036400 batches: 0.0130
trigger times: 15
Loss after 275139000 batches: 0.0133
trigger times: 16
Loss after 275241600 batches: 0.0127
trigger times: 17
Loss after 275344200 batches: 0.0126
trigger times: 18
Loss after 275446800 batches: 0.0130
trigger times: 19
Loss after 275549400 batches: 0.0126
trigger times: 20
Early stopping!
Start to test process.
Loss after 275652000 batches: 0.0136
Time to train on one home:  223.49897122383118
trigger times: 0
Loss after 275783100 batches: 0.0412
trigger times: 1
Loss after 275914200 batches: 0.0146
trigger times: 2
Loss after 276045300 batches: 0.0129
trigger times: 3
Loss after 276176400 batches: 0.0124
trigger times: 4
Loss after 276307500 batches: 0.0120
trigger times: 5
Loss after 276438600 batches: 0.0117
trigger times: 6
Loss after 276569700 batches: 0.0116
trigger times: 7
Loss after 276700800 batches: 0.0112
trigger times: 8
Loss after 276831900 batches: 0.0112
trigger times: 9
Loss after 276963000 batches: 0.0109
trigger times: 10
Loss after 277094100 batches: 0.0108
trigger times: 11
Loss after 277225200 batches: 0.0107
trigger times: 12
Loss after 277356300 batches: 0.0106
trigger times: 13
Loss after 277487400 batches: 0.0105
trigger times: 14
Loss after 277618500 batches: 0.0104
trigger times: 15
Loss after 277749600 batches: 0.0104
trigger times: 16
Loss after 277880700 batches: 0.0102
trigger times: 17
Loss after 278011800 batches: 0.0102
trigger times: 18
Loss after 278142900 batches: 0.0101
trigger times: 19
Loss after 278274000 batches: 0.0101
trigger times: 20
Early stopping!
Start to test process.
Loss after 278405100 batches: 0.0101
Time to train on one home:  161.29216527938843
train_results:  [0.07919007795897719, 0.031169424880564872, 0.019239323559002663, 0.016296673694128457, 0.015343315850142264, 0.014087723190141727, 0.013113040729183465, 0.012445205791905526, 0.012410333956078289, 0.01202659437884182, 0.010893938768271265, 0.01106632367073798, 0.010575175447393387, 0.010503794982589784, 0.01012047204975843, 0.010866048209303356, 0.009993974293054141, 0.009646852175512956, 0.009412276137404883, 0.009467769760957005]
test_results:  [[0.8365190492735969, 0.09510281736686543, 0.25583247433187156, 1.5064095748295083, 0.7412868742841652, 35.58946613838168, 2288.3987], [0.41471103496021694, 0.5522172986870829, 0.6034741159783167, 0.8347555394737183, 0.36682116530508213, 19.721398816313794, 1132.3998], [0.416481077671051, 0.5503032269292987, 0.6108686967359516, 0.8717537655029887, 0.36838916252920273, 20.595495167299838, 1137.2401], [0.40785420934359234, 0.5596741915007764, 0.6223451573341271, 0.8665839963551935, 0.36071251907231366, 20.473357518214907, 1113.542], [0.38579129841592574, 0.5836443594168326, 0.6447384240893203, 0.7999958698352181, 0.34107628725329614, 18.90018916241157, 1052.9237], [0.3810204996003045, 0.5888408409373886, 0.6508218730887105, 0.7898426439430315, 0.3368193577174573, 18.66031556155092, 1039.7822], [0.36352380944622886, 0.6078535067064823, 0.6692406741949329, 0.7629925856940172, 0.32124428482489964, 18.025973311718218, 991.70105], [0.3770870649152332, 0.5930997180662239, 0.6636952771326541, 0.7897413607964617, 0.3333305086245605, 18.65792271091092, 1029.012], [0.36467647386921775, 0.6065404299813688, 0.6678829260700717, 0.7598771785316129, 0.32231995017112447, 17.952370700870365, 995.02167], [0.3772937340868844, 0.5928655797026701, 0.6636011186022571, 0.7875590196524677, 0.3335223135047167, 18.60636411918107, 1029.6041], [0.36474793487124973, 0.6064658220614043, 0.6684562959467083, 0.7573153685201283, 0.3223810685753463, 17.891847021135376, 995.2104], [0.3664713038338555, 0.6046035803580228, 0.6709737285237666, 0.7533363636113402, 0.3239066068994288, 17.797841603995224, 999.9198], [0.36360657546255326, 0.6076820853508372, 0.6711735820918979, 0.7556768178669199, 0.32138471227163135, 17.85313567465779, 992.1346], [0.3657842063241535, 0.6053299347838157, 0.6728032514002775, 0.7532968651349748, 0.32331158128519083, 17.7969084383339, 998.08295], [0.361652703748809, 0.6098542734072183, 0.6776885777608854, 0.7497262707738148, 0.31960526757274665, 17.71255186676552, 986.6414], [0.35913565589321983, 0.6125684953219025, 0.6786061046593334, 0.7338401978555779, 0.31738179167088215, 17.337237700125648, 979.7774], [0.3646826330158446, 0.6065532684133498, 0.676905662030514, 0.7423034996622564, 0.3223094329971329, 17.537186238757172, 994.98926], [0.36132707529597813, 0.610169510531575, 0.6799456397665353, 0.7368694861698217, 0.3193470270266852, 17.408805722318668, 985.8442], [0.36047327684031594, 0.6110878050586475, 0.6782920101137582, 0.7353344564641495, 0.31859476512034884, 17.372540095330994, 983.52185], [0.3567118975851271, 0.6151724921344071, 0.6804043761108612, 0.7307689794520935, 0.31524861157613326, 17.26467906454558, 973.192]]
Round_19_results:  [0.3567118975851271, 0.6151724921344071, 0.6804043761108612, 0.7307689794520935, 0.31524861157613326, 17.26467906454558, 973.192]